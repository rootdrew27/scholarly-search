Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:19<00:57, 19.24s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:36<00:36, 18.17s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:56<00:18, 18.96s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:59<00:00, 12.51s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:59<00:00, 14.80s/it]
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Traceback (most recent call last):
  File "/data/groups/classes/2024/fall/cs426/group_5/SchSearch/scholarly-search/./py_code/main.py", line 82, in <module>
    top_paper_ids = embLib.search_papers(prompt, response, n_results=5)
  File "/data/groups/classes/2024/fall/cs426/group_5/SchSearch/scholarly-search/py_code/embedding_library.py", line 143, in search_papers
    r_secs = self.preprocess_llm_response(prompt, response)
  File "/data/groups/classes/2024/fall/cs426/group_5/SchSearch/scholarly-search/py_code/embedding_library.py", line 134, in preprocess_llm_response
    response = response.replace(prompt, '').strip()
TypeError: replace() argument 1 must be str, not tuple
