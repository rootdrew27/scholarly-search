{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import umap    \n",
    "import umap.plot\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sentence_transformers.losses as losses\n",
    "from sentence_transformers import SentenceTransformer, SimilarityFunction\n",
    "from sentence_transformers.trainer import SentenceTransformerTrainer, SentenceTransformerTrainingArguments\n",
    "from sentence_transformers.training_args import BatchSamplers\n",
    "from datasets import Dataset\n",
    "import sys\n",
    "import os\n",
    "import logging\n",
    "import re\n",
    "\n",
    "SCRIPT_DIR = Path.cwd().parent\n",
    "sys.path.insert(0, str(SCRIPT_DIR))\n",
    "\n",
    "from embeddings.embedding_library import EmbeddingLibrary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_papers = Path(r'../data/papers')\n",
    "path_to_embs = Path(r'../data/embeddings')\n",
    "\n",
    "# should all be lower case\n",
    "end_of_paper_words = [\n",
    "    'references',\n",
    "    'acknowledgement', \n",
    "    'appendix',\n",
    "]\n",
    "\n",
    "papers_to_skip = [\n",
    "    '2311.11329v2_content.txt',\n",
    "    '2411.09324v2_content.txt',\n",
    "    '2411.14259v1_content.txt',\n",
    "    '2309.01837v3_content.txt'\n",
    "    #'2403.12778v2_content.txt' \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'all-distilroberta-v1'\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(model_name, device=device)\n",
    "\n",
    "# https://huggingface.co/sentence-transformers/all-distilroberta-v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_weights = r'C:\\Users\\rooty\\UWEC\\Fall 24\\CS 426\\GroupProject\\GroupProject\\scholarly-search\\finetune\\semsim'\n",
    "\n",
    "model = SentenceTransformer(path_to_weights, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.15it/s]\n"
     ]
    }
   ],
   "source": [
    "emb_lib = EmbeddingLibrary(\n",
    "    path_to_papers=path_to_papers,\n",
    "    path_to_embs=path_to_embs,\n",
    "    model=model,\n",
    "    end_of_paper_words=end_of_paper_words,\n",
    "    papers_to_skip=papers_to_skip,\n",
    "    norm_embs=True,\n",
    "    name=model_name,\n",
    "    log_lvl=logging.INFO,\n",
    "    path_to_log=Path(\"../log\")\n",
    ")\n",
    "\n",
    "model.similarity_fn_name = SimilarityFunction.COSINE # TODO fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 153.60it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  6.63it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.12it/s]\n",
      "Batches: 100%|██████████| 5/5 [00:00<00:00, 20.40it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 162.09it/s]\n",
      "Batches: 100%|██████████| 6/6 [00:00<00:00, 17.25it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 196.07it/s]\n",
      "Batches: 100%|██████████| 4/4 [00:00<00:00, 22.08it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 87.89it/s]\n",
      "Batches: 100%|██████████| 7/7 [00:00<00:00, 30.97it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 139.23it/s]\n",
      "Batches: 100%|██████████| 3/3 [00:00<00:00, 15.17it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.44it/s]\n",
      "Batches: 100%|██████████| 8/8 [00:00<00:00, 37.29it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 168.42it/s]\n",
      "Batches: 100%|██████████| 6/6 [00:00<00:00, 29.96it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 117.75it/s]\n",
      "Batches: 100%|██████████| 8/8 [00:00<00:00, 14.43it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 138.20it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  6.05it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 144.12it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00, 10.19it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 161.11it/s]\n",
      "Batches: 100%|██████████| 9/9 [00:00<00:00, 47.45it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 130.61it/s]\n",
      "Batches: 100%|██████████| 4/4 [00:00<00:00,  9.63it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 201.07it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  6.27it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 115.41it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00, 17.91it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 145.41it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.34it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 198.52it/s]\n",
      "Batches: 100%|██████████| 3/3 [00:00<00:00,  5.18it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 160.03it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  8.55it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 123.42it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  5.86it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 143.00it/s]\n",
      "Batches: 100%|██████████| 3/3 [00:00<00:00,  9.40it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 156.83it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.07it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 163.41it/s]\n",
      "Batches: 100%|██████████| 3/3 [00:00<00:00,  8.85it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 124.99it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  5.08it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 144.84it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  6.55it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 153.38it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.91it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.94it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  3.42it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 145.23it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  3.55it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 138.42it/s]\n",
      "Batches: 100%|██████████| 7/7 [00:00<00:00, 15.19it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.09it/s]\n",
      "Batches: 100%|██████████| 8/8 [00:00<00:00, 13.00it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 167.50it/s]\n",
      "Batches: 100%|██████████| 5/5 [00:00<00:00, 24.50it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 142.96it/s]\n",
      "Batches: 100%|██████████| 4/4 [00:00<00:00,  6.42it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 142.82it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.04it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 164.20it/s]\n",
      "Batches: 100%|██████████| 3/3 [00:00<00:00,  7.28it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 165.97it/s]\n",
      "Batches: 100%|██████████| 4/4 [00:00<00:00, 13.75it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 143.91it/s]\n",
      "Batches: 100%|██████████| 7/7 [00:00<00:00, 24.72it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 163.29it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  6.21it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 133.16it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.74it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 116.65it/s]\n",
      "Batches: 100%|██████████| 3/3 [00:00<00:00,  5.82it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 169.60it/s]\n",
      "Batches: 100%|██████████| 12/12 [00:00<00:00, 31.46it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 169.49it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.41it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.82it/s]\n",
      "Batches: 100%|██████████| 3/3 [00:00<00:00, 12.55it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 164.48it/s]\n",
      "Batches: 100%|██████████| 3/3 [00:00<00:00, 10.62it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 136.47it/s]\n",
      "Batches: 100%|██████████| 5/5 [00:00<00:00,  7.70it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 147.25it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.11it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 199.30it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  6.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 60.82it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.09it/s]\n"
     ]
    }
   ],
   "source": [
    "emb_lib.multi_sec_embed(skip_existing=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Embedding Lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs = emb_lib.paper_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper = umap.UMAP().fit(embs)\n",
    "p = umap.plot.points(mapper, labels=pd.Series([i for i in range(embs.shape[0])]), show_legend=False)\n",
    "umap.plot.show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.1973, 0.3841,  ..., 0.5426, 0.3374, 0.3162],\n",
       "        [0.1973, 1.0000, 0.3558,  ..., 0.2475, 0.3143, 0.4754],\n",
       "        [0.3841, 0.3558, 1.0000,  ..., 0.3466, 0.4973, 0.3639],\n",
       "        ...,\n",
       "        [0.5426, 0.2475, 0.3466,  ..., 1.0000, 0.3382, 0.3323],\n",
       "        [0.3374, 0.3143, 0.4973,  ..., 0.3382, 1.0000, 0.2821],\n",
       "        [0.3162, 0.4754, 0.3639,  ..., 0.3323, 0.2821, 1.0000]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity(embs, embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Next-generation (NextG) networks promise to support ultra-reliable and low-latency communication for rapidly evolving wireless devices. Emerging networks are thus challenged to establish new features (e.g., adaptive coding and enhanced modulation) to overcome rapidly changing channel conditions and to achieve more efficient use of spectrum.', 'Machine Learning (ML) overcomes this barrier by revolutionizing the entire wireless network protocol stack.', 'Recent research introduces joint source-channel coding (JSCC), an end-to-end wireless communication system leveraging deep neural networks (DNNs) for both transmitter and receiver. This ML approach jointly optimizes source and channel coding in a cross-layer framework to handle diverse and challenging channel conditions.', 'To effectively cope with the multipath fading effects, the JSCC-encoded data can be further modulated into continuous signal waveforms through orthogonal frequency division multiplexing (OFDM).', 'The DNN models for JSCC are tailored to specific modalities (e.g., texts, images, etc.), so as to convey semantic information more accurately than traditional communication systems (seesectionII-B).', 'The advantages of such ML-based communication systems are increasingly recognized by standardization bodies such as the Third Generation Partnership Project (3GPP).Industry leaders, such as Apple, Huawei, Nokia Bell Labs, Qualcomm, and ZTE are also investigating AI-native 6G communications. NVIDIA has established an ML-based, GPU-accelerated communication signal processing framework for 6G applications. These developments underscore the growing consensus that ML-based wireless communications will play a crucial role in shaping the future of 6G technology.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.31it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['2311.00207v3',\n",
       " '2406.08298v5',\n",
       " '2411.09101v1',\n",
       " '2410.23279v3',\n",
       " '2411.13037v1']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Paper: 2311.00207v3\n",
    "\n",
    "prompt = \"\"\n",
    "response = '''SECTION: IIntroduction\n",
    "\n",
    "Next-generation (NextG) networks promise to support ultra-reliable and low-latency communication for rapidly evolving wireless devices[29]. Emerging networks are thus challenged to establish new features (e.g., adaptive coding and enhanced modulation) to overcome rapidly changing channel conditions and to achieve more efficient use of spectrum[102,63].\n",
    "Machine Learning (ML) overcomes this barrier by revolutionizing the entire wireless network protocol stack[67].'''\n",
    "\n",
    "emb_lib.search_papers(prompt, response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from 2411.06091\n",
    "\n",
    "prompt = \"\"\n",
    "response = '''Remote sensing technology, employing aerial or satellite platforms, facilitates the acquisition of earth observation data from afar, marking its utility across diverse sectors such as agriculture monitoring, environmental surveillance evaluation, urban planning, and disaster management. At the heart of these applications lies the interpretation of remote sensing imagery, a task that has been considerably challenging[1]. The advent of deep learning technology[2]has significantly enhanced the precision and automation of remote sensing image analysis, encompassing object detection, land cover classification, and change detection. Nonetheless, the intrinsic complexity of remote sensing scenarios—attributable to variations in sensor technology, atmospheric conditions, and image resolution—results in the performance disparity of analytical models across different contexts. The prohibitive costs associated with manual annotation further exacerbate this issue, rendering the re-labeling of samples for specific scenarios impractical. Consequently, there is a growing demand for models endowed with superior generalization capabilities, capable of adeptly navigating the multifaceted landscape of remote sensing applications.\n",
    "\n",
    "Seeking a method that discerns both the semantic information of different scene images and the semantic details within each area of the images is essential for effectively processing scenographic remote sensing imagery. It has been observed that scenographic images in remote sensing typically exhibit specific distribution patterns, where natural elements (such as mountains, bodies of water, forests, lakes, and grasslands) and man-made elements (such as buildings, farmlands, plowed lands, and gardens) often display pronounced clustering. This tendency for feature aggregation leads to the formation of discernible geographic pattern clusters among individual image patches and their neighbors, thereby establishing intricate spatial relationships across the varying patches and their adjacent areas.'''\n",
    "\n",
    "emb_lib.search_papers(prompt, response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**These results are sub-par**\n",
    "\n",
    "### Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The digital image forensics based research works in literature classifying natural and computer generated images primarily focuses on binary tasks. These tasks typically involve the classification ofnatural images versus computer graphics imagesonly ornatural images versus GAN generated imagesonly, but not natural images versus both types of generated images simultaneously. Furthermore, despite the support of advanced convolutional neural networks and transformer based architectures that can achieve impressive classification accuracies for this forensic classification task of distinguishing natural and computer generated images, these models are seen to fail over the images that have undergone post-processing operations intended to deceive forensic algorithms, such as JPEG compression, Gaussian noise addition, etc. In this digital image forensic based work to distinguish between natural and computer-generated images encompassing both computer graphics and GAN generated images, we propose a robust forensic classifier framework leveraging enriched vision transformers. By employing a fusion approach for the networks operating in RGB and YCbCr color spaces, we achieve higher classification accuracy and robustness against the post-processing operations of JPEG compression and addition of Gaussian noise. Our approach outperforms baselines, demonstrating 94.25% test accuracy with significant performance gains in individual class accuracies. Visualizations of feature representations and attention maps reveal improved separability as well as improved information capture relevant to the forensic task. This work advances the state-of-the-art in image forensics by providing a generalized and resilient solution to distinguish between natural and generated images.',\n",
       " 'The widespread availability of image acquiring devices has led to images being captured easily. Apart from the camera-captured‘natural or real images’, there exists‘computer graphics images’, i.e., images generated through computer graphics software. The other category of computer generated images includes the Generative Adversarial Network‘(GAN) images’. Beyond the artistic and entertaining purposes of these computer generated images, they also have a significant possibility to easily propagate misinformation via online platforms, especially when associated with fake news. Hence there exists a high necessity for computational aids to distinguish natural and computer generated images. Most works in the literature that distinguish natural and computer generated images consider either computer graphics images only or GAN images only in the category of computer generated images. That is, these works focus only on one category of computer generated images and not both. However, these forensic algorithms do not suit the real-world scenario where the process of image generation is unknown. Also, there could be cases if a generated image is not computer graphics then maybe it could be a GAN image or vice-versa. Hence a single forensic classification system is required to investigate images and understand whether they are natural/real photographs or the ones created by computer graphics or GAN algorithms, with high accuracy.',\n",
       " 'With the advent of convolutional neural network (CNN) architectures and transformer based architectures, the image classification systems are achieving high classification accuracies. Utilizing these techniques the forensic algorithms or tools to distinguish natural and computer generated images while moving to attain some success have been mostly vulnerable to different variations in images caused due to image quality, resolution, compression quality, or color which are capable enough to dramatically modify or restructure the underlying image properties. Image forgeries are almost always followed by these operations like adding some noise or applying JPEG compression to obscure any traces of image generation or forgeries and thereby deceiving image forensic algorithms and tools. Therefore, besides producing a high performance system for classifying photographs and computer generated images of both categories (computer graphics and GAN), it is equally essential to review the robustness of these forensic systems towards various post-processing operations. Hence, unlike the works in the literature that deals the forensic task of distinguishing natural and computer generated images as a binary-class classification task either by considering only computer graphics or only GAN images, our work proposes a generalized forensic algorithm, i.e., a three-class classification task classifying real, computer graphics and GAN images. Our methodology focuses on building a classification model such that it achieves high accuracy and is highly robust against post-processing operations.The major contributions of our work are:',\n",
       " 'We propose a robust image forensic classification framework for distinguishing natural and computer generated images including both computer graphics and GAN images, utilizing a vision transformer based approach and employing a fusion of two color space transformations RGB and YCbCr.',\n",
       " 'We devise a novel methodology by exploiting the vulnerability of the decrease in classification accuracies of image classes with the increase in compression factors, to majorly focus on building a forensic classifier system that is highly robust against the post-processing operations.',\n",
       " 'We compare the performance of our proposed model with a set of baselines and could observe that the performance of our approach outperforms the baselines with significant performance gains in total and individual class accuracies, is highly robust against post-processing operations of image compression and addition of gaussian noise, and is also generalizable to unseen test data.',\n",
       " 'We visualize the feature representations of our model and compare with the input image features and feature representations of the baseline method, where we could observe that our model provides much better separability between the different classes, helping towards improved classification performance.',\n",
       " 'We also analyze the attention maps of the networks of our fused model which shows that our novel methodology has the capability of improved information capture relevant to the forensic task of classifying natural and generated images.',\n",
       " 'The rest of this paper is organized as follows. Section2presents a brief survey of related works and delineates our proposed work from the related works in the literature. Section3discusses the methodology of the proposed work along with the dataset used in this study, motivation and detailed description of the proposed model. Section4presents the experimental settings and details of the baseline models used for comparing our proposed model. Section5presents the results and discussion including the results of the experiments for robustness, generalizability, feature visualization, and the analysis of attention maps. Section6finally draws the conclusions.',\n",
       " 'Image processing has a wide range of applicability in various research areas, including computer vision, computer graphics, medical image processing, digital image forensics, etc. Digital Image Forensics is a research area that examines digital images using scientific approaches to provide evidence of their authenticity in different contexts, such as criminal cases, civil cases, etc. There are various different sub-tasks, which this area of research focuses on. These mainly include the development of computational algorithms to detect computer-generated images such as graphics and GAN images, detecting recaptured images, forged images, etc.',\n",
       " 'Image forensic based works that distinguish natural and computer graphics images can be seen since the 1990s. These include statistical feature based works that utilize features based on local binary patterns, co-occurrence matrices, histogram features, etc., transform domain features based works utilizing wavelet features, quaternion wavelet features, etc., device based features exploiting features based on intrinsic fingerprints, demosaicing, etc.. Also, there have been many works utilizing deep learning architectures such as CNNs.Works that distinguish natural and GAN images are comparatively much more recent than the category described above of natural versus computer graphics images. These works in the literature that attempt natural versus GAN images, mostly employ deep learning based approaches such as CNNs, XceptionNet, InceptionNet, etc., than the handcrafted feature based approaches. Few works are also seen to utilize transformer based architectures to detect GAN generated videos, but these works also only consider the single category of generation, that is natural versus GAN only. Works detecting GAN images generally consider image content based feature analysis or some works are also found to consider residual features, biometric features, etc. GAN image detection algorithms have applicability in numerous categories such as detection of generated faces, generated obscene images, generated medical images, etc. Works in the literature classifying natural and computer-generated images are also found to study and conduct robustness experimentations towards post-processing operations, mainly JPEG compression. Table1shows a snapshot of works classifying natural and computer-generated images.',\n",
       " 'To the best of our knowledge, there is only a single work that distinguishes natural versus computer generated images by considering both computer graphics and GAN images in the category of computer generated images, and utilizes an EfficientNet CNN architecture for the three-class classification task. But, to the best of our knowledge, no works are seen to be reported using the high performance recent deep learning architectures i.e., transformer based architectures for classifying natural images and both categories of computer generated images, i.e., Graphics and GAN generated images. Our digital image forensic analysis based work utilizes vision transformers and proposes a fusion based approach to distinguish between natural versus computer generated images of both categories, i.e., Graphics and GAN images. Our methodology is based on the novel idea of exploiting the vulnerability of decline in classification accuracies of different classes with the increase in compression, that helps to primarily focus on developing a forensic classifier system that is highly robust against post-processing operations such as JPEG compression, addition of gaussian noise, etc. Despite other works in the literature that utilize certain color space transformations for the task of distinguishing natural images from computer-generated images of either of the category such as, our work utilizes two color spaces in our methodology, based on our rigorous set of experiments, one for improving the overall accuracy of the task of distinguishing natural images from computer-generated images of both categories and other color space that deals with improving the robustness of the proposed model against post-processing operations. Our methodology follows a transfer learning approach which helps to improve the accuracy of the task without involving the burdensome process of pre-training and increasing the training complexity.',\n",
       " 'In this work, the forensic task of distinguishing natural and computer generated images is formulated as a three-class classification task, where the three classes are GAN, Graphics, and Real. GAN and Graphics images, even though both being computer generated, are maintained as separate classes as they have different generation processes. For this three-class classification problem, we utilize transformer based deep neural networks to find the best-fit mapping functionfor the train datawhere for each imagein the training set,, indicating either of the classes GAN, Graphics or Real, respectively.',\n",
       " 'In this work, we utilize the GAN, Graphics, and Real images from the dataset used in the study. The dataset comprises 12000 images in total, where each of the classes contains 4000 images. The class GAN consists of images collected from a variety of generative algorithms such as ProgressiveGAN, StyleGAN, StyleGAN2, and StyleGAN2-ADA. Whereas, the images for the classes Graphics and Real are obtained from the Computer Graphics versus Photographs dataset. The entire dataset is challenging and contains a diverse category of images in terms of image content (indoor and outdoor scenes, animals, objects, etc.), origin (different generative/graphics algorithms or cameras, etc), and quality. Also, the computer generated images i.e., both GAN and graphics are photorealistic, i.e., they are not manually easy to predict as computer generated ones. There are no any pre-processing operations performed on the images in the dataset. For the training, validation, and testing the dataset is proportionally split in the ratio 60:20:20 respectively.',\n",
       " 'The images transmitted through social media and online platforms are subject to image compression, mostly JPEG compression, where different online platforms follow different JPEG compression standards. Also in the cases of intentionally propagating fake images to support some piece of fake news to convince the audience, these images are seen to be mostly subject to multiple image compressions. Hence, any model proposed for distinguishing natural and computer generated images, apart from obtaining high classification accuracy must also be highly robust towards compression. But most of the computational models in the literature proposed for the forensic task of distinguishing natural and computer generated images, are less robust to post-processing operations, particularly the JPEG compression. That is, as the compression factor of the images increases (or the quality factor of the images decreases), the accuracy of the computational models to distinguish natural and computer generated images decreases. We analyze the classification accuracies of different computational models to distinguish GAN, graphics, and real images, at varying levels of JPEG compression, including the state-of-the-art vision transformer based architectures ViT-Base and ViT-Large, state-of-the-art CNN based architecture InceptionResNet and a few other baselines that classify natural and computer generated images. Figure1shows the classification accuracies of these models for original images (uncompressed images or zero compression factor) and for various compression quality factors in descending order. As can be seen from the figure, the accuracy of all the models decreases from the original or uncompressed scenario, as the compression increases.',\n",
       " 'To propose a robust model for the task, hence, we further analyze the class accuracies of these models. Figure2shows the individual class accuracies of the models for varying levels of JPEG compression i.e., accuracies of the class GAN generated images at varying levels of compression factors in figure2a, graphics generated images in figure2b, and real images in figure2c. We could observe that, for original or uncompressed images, class accuracies of GAN generated images are always higher than the graphics and real images, for all the models. But at the same time, the class GAN is the most affected as the compression increases (as shown in figure2a). That is, there is a very high drop/decay in the class GAN accuracies of the models with even small increments in JPEG compression. For the class Real in figure2c, the drop in class accuracies of the models as the compression increases are comparatively very less and for some compression rates the accuracies even gets an increase in values. Whereas, for the class Graphics in figure2b, there is negligible drop in class accuracies of the models, it has almost stable accuracies as the compression increases. Altogether, analyzing the class accuracies shows that the rate of decrease in class accuracies with the increase in compression differs for different classes. And hence, how we can exploit this vulnerability of the decrease in accuracy with an increase in compression, which differs for each class, is the motivation for our proposed approach of a robust classification model for the forensic task of distinguishing natural and computer generated images.',\n",
       " 'We devise our methodology in such a way that the proposed model should obtain high classification accuracy and should also be highly robust. We propose a Multi-Colorspace fused and Enriched Vision Transformer (MCE-ViT) model by parallely combining two transformer based networks that operates in two different color spaces, one for obtaining high classification accuracy and the other network dedicated to improve the robustness of classification. The entire architecture of our proposed model is shown in figure3. For obtaining high classification accuracies we choose as the first network of the fused model, one of the recent state-of-the-art transformer based models ViT. This network takes as input the RGB images, and we name this first ViT network asRGB ViT network. We follow a transfer learning strategy where we choose the ViT network pre-trained on the Imagenet 21-k dataset and fine-tune the network using the task specific GAN, Graphics and Real images dataset (detailed in section3.1). Even though this firstRGB ViT networkis found to obtain high classification accuracies for uncompressed images, it also has a performance decay when the images are JPEG compressed, as already seen in figure1. Since the rate of performance decay is different for different classes (as detailed in section3.2), allowing the model to also learn about these differences in performance decay among the classes, would help to better identify these classes in compressed scenarios. Therefore we design a strategy for the second network in our fused model in such a way as to make the model learn these differences and thereby improve the robustness.',\n",
       " 'As the firstRGB ViT networkof the proposed fused model primarily focuses on high classification accuracies, the second network of the fused model is designed primarily to focus on the task of improving the robustness of the entire classification. Hence our strategy for the second network of the fused model is motivated by the fact that, since the rate of differences in accuracies between original and compressed versions of images are different for different classes, enriching the images in each class with this information on their corresponding rate of differences would be highly beneficial to distinguish natural and generated images with high classification accuracies, even for images in post-processed scenarios thereby improving robustness of the model. Accordingly, in the second network of our fused model, the input RGB images from the dataset are initially converted to the YCbCr color space. The motivation for choosing YCbCr color space is that it is the color space commonly used in the process of JPEG compression. The process of JPEG compression mainly downsamples the chrominance planes of this colorspace, i.e., Cb and Cr, because the changes brought up in these planes are less visually discernible. Hence enriching the network with the error of chrominance planes between the original images and their corresponding JPEG compressed versions, would enable the network to learn the distinguishable feature of the rate of differences in classification accuracies based on compression that differs for different classes. On this basis, our secondEnriched YCbCr ViT networkenriches the YCbCr images with the information on the error of chrominance planes between the original images and their corresponding JPEG compressed versions as given in the following equation, and this second pre-trained ViT network is fine-tuned using these enriched YCbCr images.',\n",
       " 'Both the firstRGB ViT networkand the secondEnriched YCbCr ViT networkproduce an output feature vector. Even though the feature vector from the first RGB ViT network is very powerful to classify gan, graphics, and real images with high accuracy in uncompressed scenarios, in order to decrease its effect towards performance decay in compressed scenarios, we perform average pooling on this feature vector to obtain only the representative feature of the RGB ViT network. Thus, pooling the feature vector from the RGB ViT network helps to not affect the robustness of the model in compressed scenarios, as well as to maintain high model accuracies. Later, to build the fused model, the feature vectors from both the ViT networks are concatenated to form a single feature vector which is passed to a fully connected neural network with a dense layer and a 3-class output layer that determines the class label of the images.',\n",
       " 'Our entire methodology including the conversion to YCbCr color space and enriching the YCbCr images with the error of chrominance planes between the original and their corresponding JPEG compressed versions can be applied irrespective of any image formats such as .png, .jpeg, etc. For example, for a .png image format, the firstRGB ViT networktakes as input the RGB image as such. For the secondEnriched YCbCr ViT network, the .png image would be converted from RGB to YCbCr, which is later enriched with the error of chrominance planes of this YCbCr image and its corresponding JPEG compressed version. Furthermore, our novel methodology can also be applied to compressed input images as well (the major aim of this work being to develop a robust model towards post-processed images). This is because our methodology focuses on the rate of difference in accuracies as compression increases, where we could observe from the figure2that these rates are different for different classes. This rate of difference not only occurs between an original/uncompressed version of an image and its corresponding compressed version with a compression quality factor of 90, but also between compressed images with quality factors of 90 and 80, 80 and 70, etc. Accordingly, when a compressed image is presented to our proposed model, the secondEnriched YCbCr ViT networkwould perform its action of conversion to YCbCr, again compressing it, and enriching the YCbCr with the error of chrominance planes of these two. That is, since our methodology is centered on the rate of difference in classification accuracies as compression increases which differs for different classes, our methodology can handle uncompressed or compressed input images of different formats, thereby making the proposed model highly robust.',\n",
       " 'The proposed study utilizes the vision transformer ‘vit-large’ with patch size 16 and input image size 224 x 224, for bothRGBand theEnriched YCbCrnetworks of the fused model. TheEnriched YCbCr ViT networkutilizes JPEG compression with a quality factor of 90. The study follows transfer learning strategy where both the ‘vit-large’ networks are pre-trained on ImageNet-21k and fine-tuned on the task specific dataset used in the study. Each ViT network produces an output feature vector of size 1024. After manually analyzing the results of average pooling of the feature vector from the firstRGB ViT networkwith various kernel sizes and strides, we use a kernel size of 16 and a stride of 16, as a representative setting, and the resultant feature vector of size 64 is concatenated with the feature vector from the secondEnriched YCbCr ViT network. The entire concatenated feature vector of size 1088 is fed to a dense layer of 512 neurons withReLUactivation function, followed by an output dense layer of 3 neurons withsoftmaxactivation function, making 559,107 number of trainable parameters. The other hyperparameters are batch size 16,categorical crossentropyloss function,Adamoptimizer, and 50 epochs. The experiments are conducted on the deep learning workstation equipped with Intel Xeon Silver 4208 CPU at 2.10 GHz, 256 GB RAM, and two GPUs of NVIDIA Quadro RTX 5000 (16GB each), using the libraries Tensorflow (version 2.8.0), Keras (version 2.8.0), Torch (version 1.13.1+cu116), PyTorch Lightning (version 1.9.0), Transformer (version 4.17.0), and Albumentations (version 1.3.1).',\n",
       " 'The model performance analysis experiments in this work focus on analyzing the performance of the proposed model based on classification accuracy and comparing it with the baselines. The classification accuracy a multi-class (3-class in this work) classifier model is computed as follows:',\n",
       " 'where,andindicate the original/ground-truth class and predicted class of an ithimage respectively, andindicates the total number of images.',\n",
       " 'For the purpose of model performance comparison with baselines, we consider state-of-the-art works in the literature that classify natural and generated images. To the best of our knowledge, there is only a single work classifying GAN, graphics, and real images and therefore, we choose this work for our baseline comparison experiments. We also perform model performance comparison with works that classify natural and either of the computer generated images (i.e., GAN or graphics only), and an off-the-shelf deep neural network architecture. We detail these baselines below.',\n",
       " 'MC-EffNet: Multi-colorspace fused EfficientNet networks proposed in. This baseline work proposes a fused model by parallely combining three EfficientNet-B0 networks that operate in three different color spaces RGB, LCH, and HSV. After converting the images into LCH and HSV color spaces, the values are rescaled into the range 0 to 255 in order to supply into the EfficientNet network. The pre-trained EfficientNet-B0 networks are fine-tuned using the GAN, Graphics, and Real images of the dataset used in. The feature vectors from all three EfficientNet networks are then concatenated and supplied to an output dense layer with 3 neurons and asoftmaxactivation function. This forms the MC-EffNet-1 network. Whereas, the MC-EffNet-2 network includes an additional preprocessing layer of laplacian of gaussian operation, before performing the color conversions to LCH and HSV. For both the networks the hyperparameters are,categorical cross-entropyas loss function,Adamoptimizer with learning rate 0.001, 256 as batch size, and 100 epochs.',\n",
       " 'The other baselines chosen for this study include InceptionResNet, Quan et al., Nataraj et al. and Rezende et al. that were chosen as the baselines in the work of MC-EffNet. InceptionResNet is one of the high-accuracy models in the ImageNet classification task. We follow a transfer learning approach over the InceptionResNet-V2 architecture where the final output layer of 1000 neurons is replaced with 3 neurons to suit this 3-class classification task. The state-of-the-art works in the literature classify natural images and either of the types of computer generated images (i.e., either only GAN images or only computer graphics images). Quan et al. propose a CNN based model to classify natural and computer graphics images. They predict results through local-to-global strategy where classification results of local patches are computed initially, and later the global prediction results of the whole image is acquired via majority voting. Nataraj et al. proposes a CNN based model to classify natural and GAN images by incorporating a co-occurrence feature of the RGB colorspace. Rezende et al. classify natural and graphics images using a transfer learning approach on the ResNet-50 model in combination with an SVM classifier. All these baselines are observed obtaining accuracy gains in their paper over many other related works in the literature. The experimental settings of all the baselines are followed in the same way as mentioned in the work of MC-EffNet. Since all these baselines are not originally designed as 3-class classification tasks, for performance comparison experiments, the top dense layer of all these baselines is replaced using a final dense layer of 3 neurons withsoftmaxactivation function, to conform to the 3-class classification task. Later, we fine-tune all the baselines with the dataset used in this study. This provides a general setting to analyze and compare the potential of the models in detecting generated images including both GAN and graphics.',\n",
       " 'Our proposed model achieves a test accuracy of 94.25 percent. The confusion matrix and detection error trade-off (DET) curve of the test results of our proposed model is shown in figure4. Table2presents a comparison of the test results of our model and the baselines in terms of total accuracy and class-wise accuracy. The test results indicate that our proposed model outperforms the baselines in terms of overall test accuracy and even in the case of individual class-wise accuracy. Our model obtains a gain of 4.87 percentage points in terms of overall accuracy when compared to MC-EffNet-2, the best performing model among the baselines.',\n",
       " 'Among the three classes, GAN obtains the highest individual class accuracy of 98.75 percent, achieving a gain of 2 percentage points when compared to the highest class accuracy of 96.75 percent obtained by MC-EffNet-2 amongst the baselines. Class Graphics achieves an accuracy of 91.00 percent, a very high gain of 7.25 percentage points when compared to the highest class accuracy of 83.75 percent for the MC-EffNet-2 model amongst the baselines. Class Real achieves 93.00 percentage accuracy, i.e., a gain of 5.37 percentage points when compared to the highest class accuracy of 87.63 percent obtained by MC-EffNet-2 amongst the baselines.',\n",
       " 'Besides achieving high classification accuracies on original images or images that are not being post-processed, an efficient image forensic algorithm should also provide robust classification output over post-processed images. Hence, the robustness of the proposed modelMCE-ViTis evaluated towards the typical post-processing operation of JPEG compression and is compared against the baselines. Apart from the baseline models discussed in section4.1, we also analyze the robustness of the proposed model over the off-the-shelf pre-trained vision transformer networks, ViT-B/16 and ViT-L/16 that are fine-tuned using the dataset used in this study. This is because we have observed in section3.2that despite improved accuracies achievable by ViT they are prone to decline in performance with an increase in compression. So here, we also inspect whether our novel methodology of exploiting the vulnerability of the decrease in classification accuracies of image classes with the increase in compression, has helped to improve robustness. Here, the results of ViT-L/16 will serve as an ablation study for our methodology that incorporates colorspace fusion and enrichment of YCbCr with the error of chrominance planes information using ViT-L/16.',\n",
       " 'Every model trained on uncompressed data (or original train data in the dataset) is tested separately over ten different test data created using various JPEG compression quality factors within the range 100 to 10, in steps of 10. Figure5shows the robustness test results, where it can be observed that compared to the baselines, the proposed model achieves improved robustness towards post-processing based on JPEG compression, for all the compression quality factors. Here also, similar to the classification results of original uncompressed images (table2), MC-EffNet-2 is the baseline model that shows the next improved robustness among all the baselines for different compression quality factors. When looking at the results of uncompressed images the accuracy of the proposed model is less than the ViT-L (ablation) and the ViT-B networks. We provide in table3, the accuracies of our model and the ViT-L and ViT-B networks, for a better comparison. From the table, we can observe that for uncompressed images the proposed model has accuracy less than both ViT-L and ViT-B networks. This could be because, the feature vector from the ViT-L network even though is very powerful in classifying gan, graphics, and real images with high accuracy in uncompressed scenarios, since they fail in compressed scenarios our model has only taken a representative feature of this network through average pooling so that it provides better accuracy and also decrease its effect towards performance decay in compressed scenarios. But even then, our model has very significant robustness starting from a very low compression (quality factor 90) to a very high compression (quality factor 10). That is, even being built with a backbone of the ViT network, our methodology of exploiting the vulnerability of accuracy decline in image classes with an increase in compression, plays a vital role in providing significant robustness.',\n",
       " 'Besides analyzing robustness against JPEG compression in terms of overall accuracy, we also analyze the robustness of our proposed model specifically for the class GAN, since it is the class that is most affected by post-processing operations and also the major contributing factor towards the decrease in overall accuracy, as already discussed in section3.2. Figure6shows the robustness test results of our proposed model and the baselines, specifically for the class GAN. We can observe that our proposed model achieves very high class accuracy for all the compression factors. The class accuracy obtained for our model while classifying original images (no compression) is 98.75 percent. At the same time, for the images compressed at a compression factor of 10 (very high compression), the class accuracy of our model is 85.13 percent. That is, for a very high compression factor of 10, our model obtains a very high gain of 41.32 percentage points when compared to the next higher accuracy of 43.81 percent at compression factor 10 for the baseline Rezende et al..',\n",
       " 'The class accuracy of the baseline ViT-L network (ablation) at a compression factor 10 is only 34.00 percent. That is, our proposed model even being built using the ViT-L network, but our methodology of color space fusion and the enrichment of YCbCr with the error of chrominance planes information between original and compressed images has proven to be highly advantageous in improving the robustness of the proposed model thereby achieving a class accuracy of 85.13 percent even at a compression factor of 10, i.e., a very huge gain of 51.13 percentage points when compared to the ViT-L network (ablation).',\n",
       " 'We also find the robustness of our model against another post-processing operation of addition of Gaussian noise and compare it with the baselines. The accuracy of our proposed model and the baselines at various standard deviation () values of Gaussian noise is shown in figure7. We can observe that compared to the baselines our model achieves better robustness against Gaussian noising also. Thus, this shows that, even if the images are post-processed with the compression or addition of gaussian noise, our novel methodology of exploiting the rate of difference in accuracies as compression increases, is powerful enough to distinguish between GAN, graphics, and real images.',\n",
       " 'This study also analyzes the generalizability of our proposed modelMCE-ViTby testing over unseen data. The proposed modelMCE-ViTwhich is fine-tuned on the dataset used in this study is tested over three other combinations of unseen GAN, Graphics, Real images, i.e., PG2versus PRCG versus PIM-Google, PG2versus PRCG versus PIM-Personal, and Cycle GAN versus Raise versus Level-Design datasets, with 160 images in each of the class of the three datasets, as experimented in the baseline work. The generalizability test results are shown in table4, where it can be observed that compared to the baselines our proposed model is able to obtain higher test accuracies. The results hence prove that our model has better generalizability, which can even help towards the future challenges in generated image categories.',\n",
       " 'The dimension of images that are input to the proposedMCE-ViTmodel is 2242243, i.e., the length of the raw image features is 150528.MCE-ViTprojects these raw image features into a smaller dimension of length 1088, with an aim to provide better separability between the three classes. We analyze the separability potential of the feature vector of our model by comparing against the feature vectors of raw input image pixels and the best baseline model MC-EffNet-2 that obtains the next higher classification accuracy among all the baselines. The t-Distributed Stochastic Neighbor Embedding (t-SNE) dimensionality reduction technique can visualize high dimensional features into a two-dimensional plane and thereby helps to easily compare the separability potential of the feature vectors. Using the t-SNE technique we project the raw image features, feature vector output from the baseline model MC-EffNet-2 and feature vector output from our proposed modelMCE-ViT, into two-dimensional plots, shown in figure8. In each of the plots, three different colors are used to indicate three different classes, green circles are used to represent GAN image class, blue squares are used to represent Graphics images class, and pink diamonds are used to represent the class of Real images. From the t-SNE visualizations, it is clearly understandable that the raw image features do not have the potential to separate the classes, all the classes are clustered together, more particularly towards the center of the plot (figure8a). The output features projected from the baseline model MC-EffNet-2 (figure8b) seem to produce separability between the classes better than the raw image pixels. The output features projected from our proposed modelMCE-ViT(figure8c) seem to produce much better separability between the three classes when compared to both raw pixels and the best baseline MC-EffNet-2. This demonstrates that our proposed modelMCE-ViThas better capability to transform the raw image pixels to a much better separable feature space for the forensic task of distinguishing natural images from computer-generated images including computer graphics and GAN images.',\n",
       " 'While visualizing the attention maps of both the networks of our fused model i.e., theRGB networkand theEnriched YCbCr network, we could observe that compared to theRGB network, theEnriched YCbCr networkcould potentially identify more regions or information in an image that are helpful for classifying images into the classes GAN, Graphics or Real. Table5shows a comparison of the attention map visualizations of both theRGB networkand theEnriched YCbCr networkfor a set of GAN, Graphics and Real images. For example in the first generated image of a cat in the set of GAN images, the RGB network mainly captures the edges of the cat, whereas theEnriched YCbCr networkcaptures some of the information captured by theRGB networkand even more including from the background, irrespective of the regions of the object in the image. Similarly, in the second generated face image, the forehead, right eye, and the image background are given more attention by theEnriched YCbCr networkthan theRGB network.',\n",
       " 'Similarly in the set of graphics images also, we can observe that theEnriched YCbCr networkis able to capture more relevant image information in detail than theRGB network, such as the regions of uneven illuminations in the image. Also, mostly, the attention given to the captured image regions by theEnriched YCbCr networkare higher than the attention given by theRGB network.',\n",
       " 'In the real class of images also we can observe that theEnriched YCbCr networkcould capture more image information than theRGB network. For example in the first image of the dog and the second image of the cow, the attention given to the image regions are higher forthe Enriched YCbCr networkthan theRGB network. Also, more image regions relevant for classification are seen to be captured by the attention maps of theEnriched YCbCr network.',\n",
       " 'Thus for all the classes we can observe that the image information captured by theEnriched YCbCr networkfor this forensic task is comparatively much more relevant than theRGB network. This, in fact, shows the powerful nature of our methodology incorporating the YCbCr color space transformation and enriching the color space with the information on the rate of differences between original and corresponding compressed versions of images in different categories, to capture image information that is highly relevant to the forensic task of classifying natural and generated images.',\n",
       " 'In this work a robust approach towards distinguishing natural and computer generated images including both, computer graphics and GAN generated images is proposed, unlike the works in the literature to distinguish natural and computer generated images that consider only either of the generated images category. Our novel methodology focuses on exploiting the vulnerability of decrease in classification accuracies of image classes with the increase in compression factors, to majorly focus on building a forensic classifier system that is highly robust. The proposed work utilized a fusion of two vision transformers one of them operating in RGB color space and the other in YCbCr color space. Transformer based architectures can provide good classification performance, while our methodology focuses on improving the robustness of the proposed model against post-processing operations such as JPEG compression by maintaining a high model accuracy because usually the forensic algorithms and tools even with high performance accuracies are fooled using these post-processed images. Experiments are conducted to analyze the performance of the model and are compared against a set of baselines. The proposed model achieves higher accuracy than the baselines and is found to be highly robust and generalizable. Visualizing the features showed better separability capability of the proposed model than the baseline. The work also studied the attention map visualizations of the networks of the fused model and observed that the proposed methodology could capture more image information relevant to the forensic task of classifying natural and generated images. To aid future research, the relevant materials of this study including the source code are made publicly available at. In the future, we plan to use and integrate our methodology to develop a web tool where users can test in real-time any image doubted for the authenticity of whether it is natural or computer generated. Additionally, we plan to explore other challenges in the area of digital image forensics, such as identifying recaptured images and computer generated videos.',\n",
       " 'This work was supported by the Women Scientist Scheme-A (WOS-A) for Research in Basic/Applied Science from the Department of Science and Technology (DST) of the Government of India under the Grant SR/WOS-A/PM-62/2018.',\n",
       " 'A Robust Image Forensic Framework Utilizing Multi-Colorspace Enriched Vision Transformer for Distinguishing Natural and Computer-Generated Images',\n",
       " 'Motivated by the emergence of decentralized machine learning (ML) ecosystems, we study the delegation of data collection. Taking the field of contract theory as our starting point, we design optimal and near-optimal contracts that deal with two fundamental information asymmetries that arise in decentralized ML: uncertainty in the assessment of model quality and uncertainty regarding the optimal performance of any model. We show that a principal can cope with such asymmetry via simple linear contracts that achievefraction of the optimal utility.',\n",
       " 'To address the lack of a priori knowledge regarding the optimal performance, we give a convex program that can adaptively and efficiently compute the optimal contract. We also study linear contracts and derive the optimal utility in the more complex setting of multiple interactions.',\n",
       " 'The design of machine learning pipelines is increasingly a cooperative, distributed endeavor, in which the expertise needed for the design of various components of an overall pipeline is spread across many stakeholders. Such',\n",
       " 'expertise pertains in part to classical design choices such as how much and what kind of data to use for training, how much test data to use for verification,',\n",
       " 'how to train a model, and how to tune hyper-parameters, but, more broadly, expertise may reflect experience, access to certain resources, or knowledge of local conditions. To the extent that there is a central designer, their role may in large part be that of setting requirements, developing coordination mechanisms, and providing incentives.',\n",
       " 'Overall, we are seeing a flourishing new industry at the intersection of',\n",
       " 'ML and operations which makes use of specialization and decentralization to',\n",
       " 'achieve high performance and operational efficiency. Such an ML ecosystem creates',\n",
       " 'a need for new design tools and insights that are not focused merely on how the',\n",
       " 'designer could perform a task in this pipeline, but rather how she should delegate',\n",
       " 'it to agents who are willing and capable of performing the task on her behalf.How should the designer interact with this ecosystem? How should she evaluate and compensate other agents for their work? How does the outcome of the delegated pipeline compare with the outcome if the designer were to perform the task by herself?In this work, we initiate the study of delegating machine learning pipelines through the lens ofcontract theoryand take a step towards answering these questions.',\n",
       " 'Contract theory provides a principal-agent perspective, where the principal—who is the designer interested in the outcome of the learning pipeline—can create a contractual arrangement—a menu of services and compensations—with an agent. At the heart of the issue is creating contracts that incentivize the agents, who may be more knowledgeable and skilled than the principal, to take the appropriate actions. The uncertain and data-centric nature of machine learning tasks brings to light interesting sources of knowledge asymmetry between the principal and the agent and requires extensions of classical contract theory.',\n",
       " 'Consider a scenario where a firm delegates a predictive task to an ML service provider. In this context, the service provider may offer the firm either a dataset for learning or a pre-trained predictive model based on that dataset. To ensure aligned incentives, the firm needs to assess the dataset or predictive model and design the payment structure for the service provider accordingly. Since the accuracy of the model is crucial to the firm as it directly influences revenue, a natural evaluation approach involves directly measuring the accuracy of the model that the service provider produces for the firm. Several challenges arise during this evaluation process. Firstly, the firm generally only has limited data in the form of historical data or data acquired shortly after deploying the model for evaluation. So there is inherent noise in the evaluation. Secondly, the firm lacks knowledge about the baseline accuracy that is realistically achievable. This makes it harder for the firm to reward the service provider in a way that yields accuracy close to the optimal accuracy. These challenges are due to two sources of uncertainty and asymmetry that we study in this work:',\n",
       " 'Hidden actions(aka Moral Hazard): Contracts must compensate the agent for his effort towards creating a good outcome for the principal. These contracts therefore must depend on observable and verifiable outcome quality, such as the true accuracy of a classifier.',\n",
       " 'This is particularly challenging in machine learning pipelines, where the accuracy of the learned model is unknown a priori and random.',\n",
       " 'The principal may be able to invest in resources, such as large test sets, that reduce this uncertainty at a cost and better evaluate the agent’s effort.',\n",
       " 'An important consideration here is determining',\n",
       " 'whether the principal must accurately verify the outcome or instead incentivize the agent in the first place to ensure a high-quality outcome.',\n",
       " 'Hidden state(aka Adverse Selection): Effective contracts use the knowledge of the best achievable outcome to incentivize the agents to work towards such outcomes.',\n",
       " 'This is challenging in machine learning pipelines where the true error of the best model is unknown. Furthermore, generic methods that estimate the optimal accuracy',\n",
       " 'tend to use almost as many resources as are needed to learn a model of that accuracy. Here again, we must ask whether contracts exist that appropriately incentivize the agent to perform his best while knowing very little about the optimal possible accuracy.',\n",
       " 'We consider performance-based contracts where the agent is compensated as a function of the estimated accuracy of the learned model. The principal’s utility is the true accuracy of the learned classifier minus the monetary transfer she makes to the agent. We model the agent in two delegation settings. In each we contrast the principal’s utility through contracting with and without information asymmetry. Borrowing terminology from the contract theory literature, we refer to the hypothetical scenario without information asymmetry as thefirst-best scenarioand the resulting optimal utility as thefirst-best utility, which we use as a benchmark.',\n",
       " 'We address both types of information asymmetry—hidden state and hidden action—creating contracts specific to each while also evaluating their efficacy when both asymmetries coexist. For hidden actions, our linear contract based on a single test point (Proposition2) ensures at leastfraction of the first-best utility. This guarantee continues to hold even with hidden state if the agent’s sampling cost is low (Theorem1).',\n",
       " 'For the hidden state challenge withpossible states, we derive an optimal contract by solving a convex optimization problem withconstraints. Section4describes how this contract’s optimality improves as the principal’s test set size increases.',\n",
       " 'In Section5, we analyze a multi-round delegation setting where the agent is uncertain about the delegated task and uses feedback over rounds to learn the principal’s requirements and collect relevant samples. We define the principal’s regret and establish a tightbound on this regret through repeated linear contracts overrounds. ',\n",
       " 'This shows that linear contracts are also powerful approximations of optimal utility in multi-round settings.',\n",
       " 'In comparison, we obtain a strictly better regret offor multi-round first-best contracts.',\n",
       " 'There is a rich literature on contract theory in economics(see, e.g., Laffont and Martimort,2009; Bolton and Dewatripont,2004).',\n",
       " 'More recently, there has been work on algorithmic and statistical aspects of contract theory(Carroll,2015; Dütting et\\xa0al.,2019;2020; Bates et\\xa0al.,2022; Alon et\\xa0al.,2022)which include results on approximation by simple contracts. These results hold for either finite actions or outcomes, and thus are not directly applicable to our setting, which involves infinite actions and a continuous space of outcomes. Working in such spaces requires utilizing the structure of our problem, and specifically exploiting fundamental results on statistical minimax rates.',\n",
       " 'Thepricingof data has been considered for various purposes and considerations(Bergemann and Bonatti,2019; Acemoglu et\\xa0al.,2022; Cai et\\xa0al.,2015; Ho et\\xa0al.,2016)including in learning problems(Agarwal et\\xa0al.,2019; Chen et\\xa0al.,2022). The latter study the pricing of previously collected data to incentivize the seller and buyer to be forthright about the valuation and quality of their data, respectively. We are interested instead in pricing for the purpose of incentivizing the data collecting agent to exert effort to collect data. Some of these papers also consider incentivizing high-quality data labelling by relying on multiple labellers who can be compared. We study delegation of learning in the setting of a single agent.',\n",
       " 'An adversarial perspective on the delegation problem has been considered for machine learning from the lens of interactive proofs. In this line of work(Goldwasser et\\xa0al.,2021; Chiesa and Gur,2018), the principal wants to fully verify the effort of an agent who may be an adversary that is interested in getting his effort verified. While they deal with similar challenges, such as not knowing the optimal achievable error, they do not consider incentivizing the agent (via contracts and compensations) to improve the outcome.',\n",
       " 'Concurrent work bySaig et\\xa0al. (2023)studies a similar setting of incentivizing data collection for classification. They characterize the optimal contract for a given test set size, under the hidden action challenge, as a threshold contract when the agent has two choices for actions. They provide conditions which make the threshold contract optimal even for additional actions. They study empirically the effect of the hidden state challenge. We provide results for an arbitrary number of actions and propose a contract that is based on a single test sample that is optimal relative to what is achievable without the hidden action and hidden state challenges. We show that this contract is robust to hidden state challenges in many cases and describe other approaches of dealing with hidden state challenges outside of these cases.',\n",
       " 'We have a task distributionrepresenting the joint distribution over the domain and label set. The principal aims to learn a classifierthat achieves high accuracy on, denoted by. To accomplish this, the principal delegates the task to an agent who selects the number of samples to collect and trains a classifier. We prioritize the collection of samples as the primary effort, considering it more significant than classifier training. The principal’s primary objective is to incentivize high-quality data collection, leading to the development of an accurate classifier. To evaluate the performance of the model obtained through delegation, the principal possesses an independent test set consisting of independently and identically distributed (i.i.d.) points drawn from the distribution. The principal utilizes this test set to evaluate the learned classifier’s accuracy.',\n",
       " 'As in many other delegation settings, the principal faces the hidden state and hidden action challenges when delegating learning. While the principal desires to construct a contract based on the true accuracy of the learned model,, they can only obtain a noisy estimate of this value using test data. Our focus is on scenarios where the size of the test dataset is not excessively large. If the test dataset is too large, it becomes more beneficial for the principal to learn a model using their own test data rather than delegating the data collection process. Even when the estimate of the learned model’s accuracy has negligible noise, the principal still faces the hidden state challenge, i.e., the principal does not know how to value the accuracy since she does not know the optimal error achievable. We useto indicate the optimal accuracy achievable on.',\n",
       " 'Assigning a low payment for the model’s accuracy when the optimal error,, is high would result in negative agent utility, discouraging agent participation. Conversely, assigning a high payment when the optimal error is low might incentivize the agent to collect a smaller dataset than is optimal for the principal.',\n",
       " 'The delegation process begins with the principal publishing a contract which is a mapping from test accuracy to payment for the agent. Seeing the contract, the agent collects data and provides a classifier to the principal. The principal then executes the contract by evaluating the classifier on her own test set. The principal pays the agent the amount specified by the contract for the measured test accuracy. We assume that the principal can commit to a test set in advance and that this test set is not accessible to the agent until the contract is executed after the agent’s data collection.',\n",
       " 'Utilities.Upon receiving a classifier with accuracyfor the task distributionand paying the agent, the principal gets utilityfor some constant. The agent exerts effortper sample it collects. So the utility for the agent receiving paymentby collectingsamples is.',\n",
       " 'Outcome as a function of the agent’s action.We assume that when the agent collectssamples,111We will consider agent’s action as continuous and the true sample size is a rounding of the action.the classifier’s observed accuracy on the principal’s test set drawn fromis drawn from a distribution with meanand variance that is determined by the size of the test set.',\n",
       " 'The constantdepends on the complexity of the training algorithm and the constantdescribes the rate of decay of the excess error. These rates are motivated in part by minimax statistical rates and scaling laws.',\n",
       " 'Even though minimax rates are typically upper bounds, we treat them as exact rates in the main body and defer the discussion on the implications of treating them as upper bounds to AppendixB.1.',\n",
       " 'An algorithm that PAC-learns a function classwith VC dimensionusingi.i.d.\\xa0samples drawn fromand returns a classifiersatisfying, where. This is minimax-optimal as there is a distributionsuch that.',\n",
       " 'In a-dimensional linear model with covariatesand outcomes, wherefor, the Ordinary Least Squares (OLS) estimatorsatisfies the property.',\n",
       " 'First-best contracts.As a benchmark for the best performance we can hope to achieve, we first consider the problem in an idealized setting without the hidden state and hidden action challenges. This is when the principal knows the optimal errorand the mapping between the agent’s actionand the test accuracy of the resulting model is deterministic (i.e., is exactly). The optimal contract in this idealized setting is called thefirst-best contract. The next proposition provides a closed form for this contract.',\n",
       " 'For any set of problem parameters, the first-best contract offers paymentwhen the test accuracy is at least, where.',\n",
       " 'One way to interpret the first-best contract is that it asks the agent to collectsamples and compensates the agent exactly forsamples.',\n",
       " 'Without hidden state or hidden action, the first-best contract yields zero utility to the agent. In this idealized scenario, the principal’s utility due to the first-best contract is called thefirst-best utilityand serves as a benchmark for comparison in our analysis of delegation. While first-best utility is used as a benchmark, the first-best contract itself may not be optimal due to existing randomness in test accuracy (hidden action).',\n",
       " 'Additionally, each optimal error valueleads to a different first-best contract, which is not implementable when the principal doesn’t know theparameter exactly (hidden state).',\n",
       " 'When dealing only with hidden action but known, the principal’s goal is to set up a contract specified forthat deals with the randomness in the test accuracy to recover some fraction of the first-best utility. However, when both actions and states are unknown (uncertainty inand test accuracy) the contract must ensure good principal utility for a range of possible states.',\n",
       " 'Linear contracts.As opposed to first-best contracts that can be quite complex,linear contractsare simple contracts that compensate an agent by a linear function of the test accuracy. That is, a-linear contract for parameterassigns paymentwhen the test accuracy is.',\n",
       " 'Linear contracts must have non-negative parameter, since the principal cannot make negative payments to the agent.',\n",
       " 'In this section, we aim to find near-optimal contracts in the realistic scenario with hidden state and hidden actions, recognizing that the first-best contract may not be optimal. Our main result is that',\n",
       " 'a linear contract compensating the agent based on the test (and not true) accuracy is approximately optimal across all possible contracts for the principal.',\n",
       " 'Moreover, the slope of the linear contract has an explicit value that is the same across a wide range ofmaking it possible to deal with both hidden state and hidden state challenges.',\n",
       " 'A crucial advantage of our linear contract is that it works with any unbiased estimator of the accuracy of the learned model. Therefore, even a test set of size one suffices to enact this contract. We state our main results in this section and defer their formal proofs to AppendixA.',\n",
       " 'Consider the hidden action (but known state) challenge where the principal knowsbut not the random mapping from the agent’s actionto test accuracy. This mapping has a mean) and a variance dependent on the test set size. The variance is finite but possibly arbitrarily large. This setting includes the delegation problem where the principal has as little as just a single samplein her test set. Furthermore, we assume that beyond knowing the mean',\n",
       " 'of the distribution of the test accuracy, the distribution can be arbitrary and unknown to the principal.',\n",
       " 'Our main result is that we can design an approximately optimal linear contract.',\n",
       " 'Furthermore, under a wide range of problem parameters, the principal does not even need to know the optimal error to construct this contract. This allows us to deal with both hidden state and hidden action challenges.',\n",
       " 'Our results in fact show a stronger comparison, that linear contracts approximate not just the optimal utility but also the first-best utility. This is quite a strong guarantee as there is often no contract that can achieve the first-best utility in presence of the hidden action challenge.',\n",
       " 'Before we state our main theorem, we start with the following proposition which deals only with the hidden action challenge while assuming that optimal erroris known to the principal.',\n",
       " 'Our main result in Theorem1follows from this proposition and shows that the linear contract in this proposition is also a good choice in more general settings.',\n",
       " 'For any set of problem parameters, if the principal knows(but not the distribution of the test error) she can construct a linear contract that brings an expected utility that is at leasttimes the first-best utility. Furthermore this contract only requires a single test sample.',\n",
       " 'The linear contractthat achieves this approximately optimal utility is the following:',\n",
       " 'At a finer level, this linear contract approximates the first-best utility by a factor of',\n",
       " 'Let us first note that previous work(Alon et\\xa0al.,2022; Dütting et\\xa0al.,2019)has provided constant approximation guarantees',\n",
       " 'but is limited to settings where the agent’s action set is a finite set or where the ratio of the maximum and minimum reward for the principal is bounded by. In the former case, an approximation ratio ofis obtained and in the latter the ratio is. Neither of these conditions hold in our settings, as the action is the number of samples collected and is unbounded and the reward can take any value in. Instead, we use the structure of first-best contract (Proposition1), the linearity of contracts, and the structure of the utility functions to obtain thisapproximation guarantees.',\n",
       " 'The full details are deferred to the appendices; here we provide some intuition and a proof sketch.',\n",
       " 'Underlying the proof is the linearity of expectation and the fact that the agent is expectation-maximizing. Under a linear contract, the expectation-maximizing agent aims to maximize, whereis the test-set accuracy of a model trained onsamples drawn from an unknown distribution with mean. The only distribution-dependent quantity in this maximizing objective is the expected accuracy. So the agent’s action and hence the principal’s contract design only depends on the expectation of the test accuracy and not on the exact distribution of the test accuracy. Next we sketch a proof for the approximation result and use the structured way the expected accuracy depends on the number of samples drawn.',\n",
       " 'Note thatis the maximum of two terms. Let us denote these terms by.',\n",
       " 'Given a linear contract with parameter, the agent’s best response is to chooseso as to maximize. The maximizing value is. By settinglarge enough, we havewhereis the threshold above which this holds. So the value ofis set to ensure the agent gets non-negative utility from participating.',\n",
       " 'When,satisfies the participation constraint. By computing the principal’s utility from the linear contractusing the expression for the agent’s best response, we see that it istimes the first-best utility. Moreover, we have. It turns out the same upper bound holds for the approximation ratio of the linear contractto the first-best utility when. This upper bound is decreasing inand the limit asis.',\n",
       " '∎',\n",
       " 'Importantly, by inspecting the contract in Proposition2, we see that in many cases it does not depend on problem-specific parameters like the optimum error. This makesdeployable in practice.',\n",
       " 'The optimal-error-parameter-agnostic linear contract is appropriate when the cost per sample collection is small enough and when the optimal error is low enough. As a result, whenis small, we can relax the assumption that the principal knows the exact optimum errorto that the principal knows thatlies in a certain range. Moreover, even under this relaxation, linear contracts are still approximately optimal. This is stated as the following theorem.',\n",
       " 'For any, consider the linear contract. For any, suppose the optimum erroris any value inand that. Then,has utility at leasttimes the first-best utility.',\n",
       " 'Note thatis constructed based on(error decay rate) and(how the principal values accuracy relative to payment). The principal knows these quantities. In contrast, the optimal contract requires additional knowledge, such as(optimum error) and(agent’s cost per sample). The theorem demonstrates a simple contract that requires less knowledge but remains approximately optimal in utility.',\n",
       " 'Medium test set regime.In our analysis, we have examined the impact of the hidden action challenge when dealing with a small test set size. The significance of the hidden action challenge diminishes as the test set size increases, as the principal can obtain highly accurate estimates of the model’s accuracy. However, when the test set becomes too large, delegation loses its value since the principal can independently learn an accurate model without delegation. Is there a regime in which the test set size is large enough for hidden action to not be significant while also being small enough for the principal to benefit from delegating data collection? In this section, we demonstrate the existence of such a regime, referred to as the “medium test set regime.” Later, we outline how we can capitalize on the larger size of the test set to achieve stronger results.',\n",
       " 'The sample complexity for learning an-optimal model is. In particular, this bound is linear in the training algorithm’s complexity which can be problematic when using highly complex training algorithms. We say that the medium test set regime exists, if the sample complexity for hidden action is significantly smaller than,',\n",
       " 'wherecaptures the significance level of hidden action which we will make precise in the following definition.',\n",
       " 'In a finite test set setting with hidden action, for any optimal error parameter, letdenote the optimal expected utility of contracting. We say that hidden action is insignificant at level, for any, if the expected utility of the first-best contract based onin this setting is at least.',\n",
       " 'We next state a theorem giving the sample complexity of the principal’s test set to achieve insignificance of the hidden action.',\n",
       " 'The sample complexity stated in the theorem is logarithmic inwhile learning would have required a number of samples linear in. This demonstrates the existence of a medium test set regime where it is possible to employ delegation without considering hidden action.',\n",
       " 'For any, if the principal has a test set of size, then hidden action is insignificant at the level.',\n",
       " 'By ignoring hidden action in the medium test set regime, we can hope to design contracts with stronger guarantees. Previously we were able to design contracts with high utility when the optimal error lies in a particular range given in Theorem1. By ignoring hidden action, we can design contracts with utility guarantees for when the optimal error lies in any arbitrary set. When the principal holds a finitely supported prior belief over the optimal error value, we show how to compute the optimal contract by setting up a convex optimization problem. We also describe some qualitative properties of the optimal contract in this setting.',\n",
       " 'When we ignore the hidden action challenge, we can assume that the observed accuracy is deterministic in the agent’s action. That is, when the agent collectssamples, the observed accuracy is. We assume that the principal holds a prior belief on the optimal error but does not know the exact value. The agent knows more about the optimal error since he collects data that informs him more about the optimal error. We assume that the agent knows the exact optimal error. We start by analyzing the optimal contract in this setting. Later in Section4.2, we discuss how to design contracts in the more realistic setting of the agent learning the optimal error instead of knowing this value exactly. And we show that the utility guarantees by making the perfectly aware agent assumption still hold approximately in the more realistic case with a learning agent.',\n",
       " 'Let us analyze the optimization problem for computing the optimal contract. Let the finite support of the prior over optimal error be. The principal puts forth a contract of accuracy-payment pairswith the pairintended for when the optimal error is.222This is implied by the revelation principle that states that, with hidden state, any delegation mechanism is equivalent to anincentive compatiblemechanism where all agents inform their private information to a planner who then recommends actions.Let us denote the expected accuracy from collectingwhen optimal error isby. Hereis the number of samples the agent would collect to achieve accuracywhen optimal error is. The principal optimizes over. The constraints of the optimization problem for the principal’s contract design for hidden state are one of two types. The first type of constraint is the participation constraint, which ensures that the agent is adequately compensated for his effort when he chooses the contract intended for the optimal error. For each, the participation constraintcan be expressed as, whererepresents the compensation rate.',\n",
       " 'The second type of constraint is the incentive compatibility constraint to ensure that the agent chooses the option intended in the contract for the optimal error. For any, the corresponding incentive compatibility constraint is that when the optimal error is, the utility of choosingis worse for the agent than choosing. The number of samples the agent would choose to achieveaccuracy under optimal errorissuch that.333Note that all accuracies cannot be achieved for all optimal errors. If no suchexists, an incentive compatibility constraint is not needed.The constraintis. Due to the structure of, the IC constraints are convex.',\n",
       " 'The principal’s expected utility which it maximizes is. So the contract design problem is the following optimization problem:',\n",
       " 'Qualitative insights on the optimal contract.We derive the following insights (see Figure1) when there are two values for the optimal error,, in the AppendixB.3. These properties also hold more generally for finitely supported beliefs and have been studied for classical contract design for many other delegation problemsLaffont and Martimort (2009).',\n",
       " 'Decreased utility. The principal gets lower utility than the first-best utility and this utility decreases asincreases.',\n",
       " 'Information rent.In the first-best contract, the agent gets no more payment than to compensate his effort. That is,. Under hidden state, for problems with lower optimal error, the agent gets positive utility. This information rent is to incentivize the agent to not pretend the problem is harder and exert lower effort to achieve an accuracy that requires more effort if the problem was harder.',\n",
       " 'Downward distortion.The first-best contract calls for the agent to collect a particular number of samples regardless of the optimal error. Under hidden state, when the problem is harder, agents are asked to collect fewer samples compared to the first-best contract. When the problem is the easiest in the support, the agent is asked to collect the same number of samples as in the first contract.',\n",
       " 'In Section4.1and particularlyOpt, we assumed perfect knowledge of the hidden state () by the agent. However, in reality, the agent does not know the optimal error beforehand. Instead, as the agent executes the contract, he learns more about the optimal error and adapts his actions accordingly. To design a contract for such a state-learning agent, the principal would need to predict the agent’s response to the contract. However, this is challenging for arbitrary contracts since the principal would require knowledge of the agent’s exact learning strategy, which is often unreasonable. Therefore, we focus on analyzing simple contracts for which we can easily derive the agent’s response. We demonstrate numerically that the utility achieved with these simple contracts is close to the utility we previously derived for state-aware agents, which we refer to as “state-aware utility.”',\n",
       " 'This provides evidence that qualitative insights we derived about the state-aware utility in Section2and Section3are applicable in the more realistic case of a state-learning agent.',\n",
       " 'We focus on the case where the optimal error can take one of two possible values,, but these design principles also extend to more possible values of the optimal error. The simple contract we consider, which we call thestate-learning contract, is the best of two simple contracts: optimalpoolingandseparatingcontracts.',\n",
       " 'Separating contract.Separating contracts allow the agent to perfectly infer the hidden state while executing the contract. These are incentive-compatible contracts that ask agents to collectsamples under optimal errorsrespectively. Additionally,are such that the agent can successfully infer the optimal error after collectingsamples. The agent’s response to this contract would be to first collectsamples and decide whether to collect more depending on the inferred optimal error. The agent’s successful inference of the optimal error makes computing optimal separating contracts similar to the contract design problem against a state-aware agent, which was solving the optimization problemOpt. The new optimization problem that yields optimal separating contracts has the same objective and constraints asOptwith the added constraint thatfor somethat we will describe soon. The additional constraint ensures that the agent knows the optimal error (with high probability) after collectingsamples.',\n",
       " 'To determine the value of, we rely on assumptions about the agent’s learning strategy. We assume that the agent can distinguish betweenwith high probability usingsamples. Hereandis a constant reflecting the degree of assumption made about the agent’s efficiency. A lower value ofis a stronger assumption, assuming a more efficient agent. This assumption on the agent’s learning strategy is more reasonable compared to assuming precise knowledge of the agent’s learning strategy.',\n",
       " 'Whenis small enough that the added constraintis not active, the state-learning agent is behaving exactly as the state-aware agent, so our results from Section4.1apply. On the other hand, ifis large (which happens whenis small) the additional constraint becomes too restrictive and the utility becomes low. In this case, another approach works well.',\n",
       " 'Pooling contract.In pooling contracts, the agent has no incentive to learn the optimal error. The pooling contract asks the agent to achieve one accuracy levelregardless of the optimal error. The payment for this accuracy is set to ensure the agent can get nonnegative utility regardless of the optimal error. It is again straightforward to understand the agent’s response to this contract. Supposecan be achieved by collectingsamples under optimal errorsrespectively. To execute this contract, the agent starts collectingand sees if it achievesaccuracy. If it does not, he collectsmore samples since this action is guaranteed to yield nonnegative utility. Furthermore, collecting fewer or no additional samples results in less thanexpected accuracy and hence zero payment even though the agent exerted effort.',\n",
       " 'A pooling contract does not let agents differentiate actions for different optimal errors and would be sub-optimal for this reason. However, when the difference in both problems is not significant i.e.,is low, the benefit to the principal for distinguishing the agents is low. In summary, the separating contract has good utility whenis large and the pooling contract has good utility whenis small. By deploying the contract of the two with the higher utility, we can hope to have good utility for all values of.',\n",
       " 'Numerical results.We compute the utility difference between the state-aware contract and the state-learning contract, varying problem parametersand. We highlight a few observations (see Fig2), that reflect the intuition we used to design the approach for state-learning contracts.',\n",
       " 'Figure2ashows that the state-learning contract is pooling whenis less than some threshold and is separating otherwise. For small and large values of, the state-learning contract has utility close to the state-aware utility.',\n",
       " 'Figure2bshows that when it is more difficult to distinguish between, the pooling contract is better than the separating contract for more values of.',\n",
       " 'Figure2cshows that the worst-case sub-optimality over allvalues of the state-learning contract compared to the state-aware utility increases asincreases. When the agent can test more efficiently, the state-learning contract has greater utility for the principal.',\n",
       " 'So far, we analyzed delegated learning that occurs through a single round of interaction between the principal and the agent. However, delegation often occurs over multiple rounds to allow the agent to learn more about the principal’s requirements. Here we model such a scenario and analyze what happens when the principal uses a linear contract in each round. We introduce a notion of regret and show that repeated linear contracting overrounds results inregret for the principal which is worse than theregret achievable without delegation i.e., thefirst-best regret.',\n",
       " 'We provide proof sketches of our results in the main body and provide the full proofs in the appendices.',\n",
       " 'To model uncertainty about the principal’s requirements, we assume that the target distributionbelongs to a class. The agent knows the classbut does not knowapriori. The principal contracts and deploys classifiers forrounds.',\n",
       " 'For each round:',\n",
       " 'The principal announces payment rulewhich is a randomized mapping from a classifier to a positive, real-valued payment to the agent.',\n",
       " 'The agent chooses a target number of i.i.d.\\xa0samples to collect from each distribution. Denote this number bywhereis the number of samples the agent draws from distributionin round. This choice is not observed by the principal.',\n",
       " 'The agent provides classifierto the principal.',\n",
       " 'The principal deploys a classifier.',\n",
       " 'The principal paysto the agent according to the announced payment rule.',\n",
       " 'The principal’s and agent’s action in each round can be chosen adaptively depending on the actions and outcomes of previous rounds.',\n",
       " 'Over theserounds, the agent’s utility is the sum of payments minus sample collection costs:. The principal’s utility is the sum of accuracies minus payments:.',\n",
       " 'Our results deal with contracts based on test accuracy of the deployed classifier. In roundwhere the principal deploys the classifier, the payment is a function of the test accuracy, whereis a mean-zero, random variable resulting in the principal’s randomness of testing. A linear contractin this round offers paymenttimes this test accuracy in this round.',\n",
       " 'In these repeated interactions, the payments serve as feedback to the agent to learn the principal’s requirements as long as the principal’s deployed classifiers depend on the classifiers provided by the agent. We focus onfeedback-providingcontracts which satisfy the following property. A contract is feedback-providing if for every roundwith, the principal deploys the agent-provided classifier, and the payment for the round is thereforetimes the test accuracy of the agent-provided classifier.',\n",
       " 'We prove positive results on the utility the principal can achieve when contracting with a rational agent.',\n",
       " 'We introduce a notion of regret for the principal and the agent in this online setting. The regret notion is defined relative to a class of classifiers. It compares utility to the utility obtained by deploying the best classifier inin every round, without any sample collection or payments.',\n",
       " 'Letbe any class of classifiers. Letbe the sequence of actions by the principal and agent.',\n",
       " 'The principal’s-regret () is the difference in the utility of the sequence and the utility of deploying the most accurate classifier inwithout payments:',\n",
       " 'The agent’s-regret () is the difference in the utility of the sequence and the utility of deploying the highest expected payment yielding classifier inand collecting no samples:',\n",
       " 'We consider a model of rationality for the agent in repeated linear contracting. This is an assumption on the agent’s-Regret rate.',\n",
       " 'Letbe the sequence of linear contracts employed by the principal. A rational agent achieves-regret sub-linear in.',\n",
       " 'We discuss the implications of contracting with a rational agent first and then justify the rationality assumption by providing an algorithm for the agent to achieve the rationality criteria.',\n",
       " 'In the following proposition, we show how contracting with such a rational agent provides the principal a sub-linear-regret. The proof of the proposition constructs a contract that the principal can use to achieve sub-linear-regret. This contract makes use of an upper bound on the agent’s regret rate.',\n",
       " 'Suppose the agent achievesagent’s-regret with, for any sequence of contracts. Then, the principal can achieve-regret.',\n",
       " 'The contract that allows this regret for the principal is described here. The principal contracts withfor the firstrounds. In these rounds, the principal deploys the agent-provided classifier. The principal shuts down contracting for the remaining rounds. That is,for. In rounds, the principal deploys a classifierthat is uniformly, at random picked from the classifiersdeployed in the firstrounds.',\n",
       " 'In the firstrounds, the principal’s-regret is bounded bytrivially. In the remainingrounds, the principal’s-regret is bounded above by just the suboptimality of the deployed classifiers since no payments are offered. The regret in the lastrounds is at most',\n",
       " 'where the last step follows from the rationality assumption that the agent achieves a-regret of.',\n",
       " 'The cumulative principal’s-regret overall allrounds is therefore. Since, principal’s-regret is.',\n",
       " '∎',\n",
       " 'The above proposition shows how the principal can achieve sublinear-regret under the agent’s rationality assumption. Now we show that the agent’s rationality can be achieved. In our proof, we provide an algorithm for the agent to achieve-regret sublinear in.',\n",
       " 'For any sequence of feedback-providing contracts, the agent can achieve-regret.',\n",
       " 'We will describe the agent’s algorithm here and defer the full analysis of this algorithm to the appendices. For any round, let us denoteby. Since the agent incurs cost linear in the number of samples collected, the algorithm must limit the number of samples to. To do this, at round, the agents collectssamples. As a result, for any, the number of samples collected up to roundis.To select the distribution to sample from, the agent picks one out ofuniformly at random. This fully describes how the agent’s algorithm collects samples by describing the number of samples and distributions to sample from.',\n",
       " 'The remaining component of the agent’s algorithm is classifier selection. The classifier selection includes exploration and exploitation where the exploration is to determine which ofprovides samples to train an accurate classifier. Since we knowis one of thesedistributions, we are guaranteed that there exists a distribution providing relevant samples.',\n",
       " 'We assign rounds of sample collection to be rounds of exploration. These rounds are suitable for exploration for two reasons. The first is because the principal deploys the agent-provided classifiers in these rounds. By the algorithm’s design, the agent only collects samples in rounds with, and in feedback-providing contracts, agent-provided classifiers are deployed in these rounds. Secondly, this choice also leads to the right number of exploration rounds for the optimal exploration-exploitation tradeoff. Letdenote the indices of rounds of exploration. This set’s size is at most. A phased exploration is done in these rounds.',\n",
       " 'The phased exploration divides the indicesinto phases where phaseis of length. So at leastsamples are collected in phase. The number of phases is therefore.',\n",
       " 'Each phaseuniformly explores classifiers learned based on samples from each distribution, collected up to phase. Letbe the set of samples collected fromin phases. And let. In each round of phase, the agent picks one ofuniformly at random. Letbe the classifier of phasewith the highest sum of test accuracies. The agent can compute the test accuracy by dividing payment by the contract coefficient (which is non-zero in exploration phases).',\n",
       " 'The agent treats all rounds other thanas exploitation rounds. For an exploitation round, suppose the last collected sample was in phase. Then in round, the agent selects the best classifier in phase:. Due to the completion of theexploration phase,is guaranteed to have some optimality guarantees.',\n",
       " 'The regret analysis for this algorithm is presented in AppendixA.3.',\n",
       " '∎',\n",
       " 'Propositions3and4together imply that the principal can achieve-Regret through linear contracting with a rational agent. We next show that the principal cannot achieve better-regret rates when the agent’s-regret rate is.',\n",
       " 'For any sequence of linear contracts, if the principal contracts with an agent with-regret on all problem instances, the principal’s-regret isfor some problem instance.',\n",
       " 'Let us denoteby. An agent with-regret collectssamples since the agent incurs cost linear in the number of samples collected. This upper bound on the number of samples provides a lower bound on the excess errors of the deployed classifiers and therefore a lower bound on the principal’s regret.',\n",
       " 'Usual sample complexity lower bounds provide lower bounds on the error of learning using a number of i.i.d samples. However, in our setting, the agent has more than just the samples he collects to learn classifiers. Through the linear payments he receives, he also has access to estimates of the accuracy of classifiers he provides in each round. This is a form of (noisy) query access to the distribution.',\n",
       " 'We provide a min-max lower bound on learning using both i.i.d samples and queries of the form answering the expected error onof a classifier in the following proposition. We show that in a min-max sense, the queries do not allow for more accurate classifiers compared to using just the i.i.d samples.',\n",
       " 'Consider a learning algorithm that usesi.i.d samples andqueries of accuracies of classifiers. Then there exists a distributionfor which the expected error of the learned classifier ismore than the optimal error of a one-dimensional halfspace on.',\n",
       " 'We prove this lemma in AppendixA.4. The main intuition for this lower bound is that without any structure on the distribution, it is hard to know what classifiers are useful to query. Therefore the queries are not useful.',\n",
       " 'Due to the above proposition, there is a problem instance for which the classifiers the principal deploys have an average excess error ofresulting in regret.',\n",
       " 'The principal also incurs a cost of orderdue to the contracting. Due to the sub-linearity of agent’s regret in, the payments have to beresulting in this term of the principal’s regret.',\n",
       " 'The optimal choice ofto balance these two terms isresulting in regret of order.',\n",
       " '∎',\n",
       " 'Comparison with non-delegation benchmark.We have shown that the principal’s-regret through delegating with an-regret agent is. Since there is an algorithm for the agent to achieve-regret, a rational agent is likely to achieve this regret upper bound.',\n",
       " 'If the principal instead performs this learning without delegation, the achievable regret is the same as the agent’s achievable regret when the contract coefficient isin every round. By Proposition4,regret is achievable. Note that this means the-regret rate is strictly worse for the principal through delegation compared to without delegation.',\n",
       " 'Different parts of the learning pipeline are increasingly being delegated to autoML services and firms. Focusing on the delegation of data collection in such settings, we developed a principal-agent model capturing the practical challenges of hidden action and hidden state arising due to information asymmetry. We hope our work inspires future research exploring and addressing other incentive-theoretic obstacles that arise in this domain and in the delegation of other parts of the learning pipeline.',\n",
       " 'We identified the practicality of linear contracts under many problem parameters. We also obtained insights that hold for many other delegation settings, including the decreased utility and information rent the principal faces due to not knowing the hidden state. We also addressed a more realistic form of hidden state information asymmetry where the agent gradually learns the hidden state while executing the contract.',\n",
       " 'Many of our results rely on a specific structure for the dependence of error rates on number of samples used for training. These rates are exact for some learning tasks (see Remark2) but are generally upper bounds. From the principal’s perspective, the contracts we designed continue to have good accuracy guarantees even if the error rates are just upper bounds and not exact. The agent’s perspective is more complicated. Since the agent can learn more about the true error curve during learning and not rely on the upper bound, analyzing how the error curve learning occurs is needed to allow us to design truly incentive-compatible contracts in this more general setting. However, learning the error curve shape is learning from a much broader class and is likely to be more challenging. How this challenge impacts the utility of contracts would be an interesting direction for future work.',\n",
       " 'Delegating Data Collection in Decentralized Machine Learning',\n",
       " 'Machine Learning (ML) has been instrumental in enabling joint transceiver optimization by merging all physical layer blocks of the end-to-end wireless communication systems. Although there have been a number of adversarial attacks on ML-based wireless systems,',\n",
       " 'the existing methods do not provide a comprehensive view including multi-modality of the source data, common physical layer protocols, and wireless domain constraints. This paper proposes Magmaw,a novel wireless attack methodologycapable of generating universal adversarial perturbations for any multimodal signal transmitted over a wireless channel. We further introduce new objectives for adversarial attacks on downstream applications. We adopt the widely-used defenses to verify the resilience of Magmaw.',\n",
       " 'For proof-of-concept evaluation, we build a real-time wireless attack platform using a software-defined radio system.',\n",
       " 'Experimental results demonstrate that Magmaw causes significant performance degradation even in the presence of strong defense mechanisms.Furthermore, we validate the performance of Magmaw in two case studies: encrypted communication channel and channel modality-based ML model. Our code is available at.',\n",
       " 'Next-generation (NextG) networks promise to support ultra-reliable and low-latency communication for rapidly evolving wireless devices. Emerging networks are thus challenged to establish new features (e.g., adaptive coding and enhanced modulation) to overcome rapidly changing channel conditions and to achieve more efficient use of spectrum.',\n",
       " 'Machine Learning (ML) overcomes this barrier by revolutionizing the entire wireless network protocol stack.',\n",
       " 'Recent research introduces joint source-channel coding (JSCC), an end-to-end wireless communication system leveraging deep neural networks (DNNs) for both transmitter and receiver. This ML approach jointly optimizes source and channel coding in a cross-layer framework to handle diverse and challenging channel conditions.',\n",
       " 'To effectively cope with the multipath fading effects, the JSCC-encoded data can be further modulated into continuous signal waveforms through orthogonal frequency division multiplexing (OFDM).',\n",
       " 'The DNN models for JSCC are tailored to specific modalities (e.g., texts, images, etc.), so as to convey semantic information more accurately than traditional communication systems (seesectionII-B).',\n",
       " 'The advantages of such ML-based communication systems are increasingly recognized by standardization bodies such as the Third Generation Partnership Project (3GPP).Industry leaders, such as Apple, Huawei, Nokia Bell Labs, Qualcomm, and ZTE are also investigating AI-native 6G communications. NVIDIA has established an ML-based, GPU-accelerated communication signal processing framework for 6G applications. These developments underscore the growing consensus that ML-based wireless communications will play a crucial role in shaping the future of 6G technology.',\n",
       " 'Unfortunately, ML is vulnerable to adversarial attacks, where small, imperceptible changes to input can yield substantial changes in the model’s output.',\n",
       " 'The susceptibility of the models to adversarial examples raises serious concerns for the safety of ML adoption in NextG.',\n",
       " 'Traditional jamming or overshadowing attacks have been dedicated to developing a malicious RF device to disrupt legitimate wireless communications. However, these approaches typically rely on high-power transmissions to cause large-scale disruptions in the spectrum, leading spectrum owners to respond swiftly.',\n",
       " 'Highly effective attacks that use low signal strengths are missing in this literature.',\n",
       " 'There have been recent works on small signal manipulations designed to target ML-based wireless systems.',\n",
       " 'However, they make unrealistic assumptions about the attacker’s capabilities. For example, even though JSCC has a modality-specific structure, they assume that only a single modality (e.g., one-hot vector message or image) is wirelessly communicated. They also assume that the adversary knows which modality is sent by the transmitter.',\n",
       " 'In practice, the above assumptions are not valid for the following reasons: 1) the transmitter typically incorporates data from all modalities into the data blocks and then sends them to the receiver; 2) if the adversary wants to recognize the modality of the signal, it needs to have access to the target ML model that carries out JSCC, and this is not always feasible, and 3) even if the adversary can detect the modality, high latency occurs until perturbations are generated and added to the victim signal.',\n",
       " 'We propose Magmaw, a new hardware-driven wireless attack framework that creates universal adversarial perturbations (UAPs) to subvert ML-based wireless systems. We show for the first time that modulated multimodal data can be perturbed by adversaries, resulting in failure to restore the original data as well as subversion of downstream services. We consider examples of downstream services such as video classification (VC), which analyzes human activity from video, and audio-visual event recognition (AVE) which predicts the event label based on representations over multiple input modalities.',\n",
       " 'Magmaw can cause significant disruptions or threaten user safety in quality-sensitive applications, e.g., remote surgery and autonomous driving, as illustrated in Figure1. Emerging applications (e.g., XR) would suffer even more from the corruption of multiple input modalities.',\n",
       " 'Magmaw must address four main design challenges.Firstly, we assume that the adversary lacks prior knowledge about the data’s modality and the exact channel model. Additionally, the attacker’s ability to adjust its transmit signal pattern effectively depends on knowing the channel matrix between the sender and receiver ().',\n",
       " 'However, sincevaries due to factors like link distance, mobility, and environment, not having this information makes crafting an effective attack challenging.',\n",
       " 'We solve the above challenges by designing a perturbation generator model (PGM) trained to create input- and channel-agnostic perturbations on surrogate wireless models. We adopt an ensemble learning approach that utilizes surrogate multimodal JSCC models to learn UAPs.',\n",
       " 'Secondly, previous attacks do not consider that the input signal can be adjusted by physical layer protocols (seesectionII-C). They only focus on the scenarios where the adversary has prior knowledge of the protocol’s full setup. In a practical scenario, the attacker does not know the constellation mapping or how the OFDM system assigns the complex symbols to multiple subcarriers.It is possible to design an attacker that recognizes the protocol from the transmitted signals. However, since wireless protocols change rapidly depending on the channel state, the analyzed output quickly becomes obsolete. A protocol-agnostic attack is required.We address this challenge by incorporating multiple controllable parameters inside the ensemble learning to optimize perturbations generalizable across all modulated data.',\n",
       " 'Thirdly, an adversarial wireless device may not be precisely synchronized with a legitimate transmitter or receiver in the time or frequency domain, reducing the effectiveness of perturbations.',\n",
       " 'We address de-synchronization issues between the adversarial device and the legitimate transmitter/receiver using our offline training procedure. Specifically, we train the PGM using time shift and phase rotation functions, ensuring that UAPs remain effective even with varying offsets.',\n",
       " 'Finally, previous studies are vulnerable to adaptive defenses. For instance, a perturbation detector can exploit traces of perturbations to predict whether the input is perturbed. This is because their perturbations are overly rigid and lacking in variability due to overfitting. To craft robust and diverse perturbations, we introduce a discriminator and diversity loss to regularize the learning process explicitly.',\n",
       " 'After integrating the above solutions, we implement Magmaw on the software-defined radio platform and validate its attack feasibility, as shown in AppendixA. Our experiments show that Magmaw degrades the Peak Signal-to-Noise Ratio (PSNR) by up to 8.04dB and 8.29dB for image and video transmission, respectively, where PSNR is a representative image quality score. For speech transmission, Magmaw prevents receivers from recognizing the speech content, increasing the mean square error (MSE) by up to 3.91compared to baseline attacks. Furthermore, Magmaw reduces the bilingual evaluation understudy (BLEU) score to 0.338 points for text transmission, indicating that the received text exhibits significant semantic errors and grammatical inaccuracies. Notably, we achieve up to 91.2attack success rate on the downstream tasks.',\n",
       " 'In our case study, we establish an encryption-based secure image transmission and prove that Magmaw leads to a reduction of up to 5.88dB in PSNR.We also evaluate Magmaw with channel modality-based ML models. Magmaw introduces up tomore error in the ML results than the baseline.',\n",
       " 'In summary, we make the following contributions:',\n",
       " 'We introduce Magmaw,a novel wireless attack frameworkimplemented over software-defined radio against ML-based multimodal communication systems and underlying downstream applications.',\n",
       " 'We adopt an ensemble learning on a set of surrogate JSCCs to craft our UAP input- and protocol-agnostic, i.e., oblivious to the modality, constellation, coding rate, OFDM specifications, and channel conditions.',\n",
       " 'We evaluate Magmaw against various defense techniques, including adaptive ones. Extensive results from case studies further show Magmaw’s efficacy.',\n",
       " 'Current communication standards (e.g., 4G LTE, IEEE 802.11 family, 5G NR) follow separate source and channel coding designs and require independent optimization of each component. The source encoder transforms the source data into the embedded source bits. The channel encoder adds redundancy to the transmitted signal, allowing the receiver to correct errors caused by noise. However, these conventional systems suffer from dramatic performance degradation due to the cliff effect where the receiver’s error correction algorithm cannot recover the transmitted data if channel conditions are worse than a certain threshold.',\n",
       " 'ML-driven wireless systems aim to train a robust JSCC encoder and decoder on wireless channels infused with channel conditions similar to the physical world. The JSCC encoder directly maps the source to complex-valued symbols, and the JSCC decoder recovers its estimate directly from the noisy channel output. To adopt the widely used wireless standards, the JSCC models can be concatenated with OFDM to increase the spectral efficiency and reduce the multipath channel effects. Since multipath fading channels and OFDM blocks can be represented as differentiable layers, ML-based wireless systems are trained end-to-end.As such, JSCC can be built without modifying standard radio hardware (e.g., field test 6G with JSCC on 4G LTE). Furthermore, ML-based wireless communication can significantly save channel bandwidth costs compared to conventional systems while achieving the same end-to-end wireless transmission performance.',\n",
       " 'Existing JSCC systems adopt modality-specific structures, with each modality requiring a specialized approach for accurate symbol recovery at the receiver.',\n",
       " 'We consider four state-of-the-art JSCC models for image, video, speech, and text transmission.',\n",
       " 'Figure2depicts the commonly used structures for each modality.',\n",
       " 'The image JSCC is trained to minimize distortion on a frame-by-frame basis. The video JSCC leverages spatiotemporal similarities between successive frames to remove the redundancy. To achieve this, the video JSCC adopts the temporal coding structure, which clusters each consecutive sequence of pictures into a group of pictures (GOP). Each frame within the GOP is entered into the video JSCC in coding order rather than display order. This means that the video JSCC encoder compresses frames in a specific order.',\n",
       " 'For a total offrames included in the GOP, the coding order of each frame is determined by the mapping function, where.',\n",
       " 'On the other hand, speech signals contain speaker characteristics such as speech rate and tone. The attention mechanism is utilized for speech JSCC to identify the essential features to help accurately recover speech signals at the receiver. The text JSCC is designed to precisely encode context information and cope with semantic distortion based on Transformers. The text features recovered by the receiver are decoded into the text sentence through a greedy decoder. A cross-entropy loss is used to understand semantic meaning while maximizing system capacity.',\n",
       " 'Modulation.Wireless standards commonly adopt QPSK, 16-QAM, and 64-QAM to map bits to complex symbols. Therefore, the JSCC-encoded data are mapped to elements in a two-dimensional finite constellation diagram.An adaptive modulation scheme can change the modulation type to balance reliability and spectral efficiency. For example, changes the modulation',\n",
       " 'based on the threshold of the channel state to meet the bit error rate (BER) requirement.',\n",
       " 'OFDM.To achieve high spectral efficiency, the OFDM transmitter may assign modulated symbols arbitrarily to the subcarriers rather than in a fixed order. Therefore, each subcarrier carries symbol vectors with a different distribution.',\n",
       " 'Coding Rate.Adaptive encoding is essential to guarantee the reliability of wireless communications. The JSCC encoder estimates the available bandwidth based on the channel state and employs adaptive algorithms to choose an optimal coding rate for efficient real-time streaming.',\n",
       " ': a channel matrix between the sender and the receiver; ✓: the item is supported;WB: White Box.',\n",
       " 'RT: Robust Training (Adversarial Training, Defensive Distillation, and Randomized Smoothing);PS: Perturbation Subtraction;PD: Perturbation Detection;OD: Oracle Defense.',\n",
       " ': the attack can compromise the defense;: the defense was considered, but the attack was ineffective.;: not mentioned in the paper.',\n",
       " 'Jamming Attacks.RF jamming transmits radio signals indiscriminately across a range of frequencies, causing interference and disrupting communication. Jamming can be broadly categorized as active jamming and reactive jamming. Active jamming continuously emits powerful interference signals, but its continuous operation leaves detectable traces, making it vulnerable to defensive techniques. Reactive jamming adjusts its jamming behavior according to observed signals in the environment. It remains silent when the channel is idle but initiates high-power signal transmission upon detecting activity on the channel.',\n",
       " 'The drawback of these approaches is that spectrum owners may promptly detect the presence of an attack and respond accordingly.',\n",
       " 'Overshadowing Attacks.Cellular networks are vulnerable to overshadowing attacks. Recent works can force the victims to receive the attacker’s symbols/subframes by sending high-powered signals to a base station. However, the adversary must have the capability to receive and decode the messages transmitted by the victim. These attacks typically require signal strengths ranging from -3.4dB to +3dB over the benign signal.',\n",
       " 'Since the adversary’s signal strength is comparable to or even stronger than the legitimate signals, it becomes easier for the legitimate nodes to identify the attack.',\n",
       " 'Adversarial ML has been studied to analyze the robustness of the model across multiple areas, such as image classification, speech recognition, activity recognition, video compression, etc.',\n",
       " 'Most studies provide an attacker with the capabilities to perform a man-in-the-middle attack where he/she intercepts data in the middle and then injects small perturbations. These are not physically feasible and only expose theoretical vulnerabilities. As the demand for physically feasible attacks grows, recent studies define practical approaches so that attacks can be realized in the real world. SLAP applies a projector to superimpose light onto an object, causing the model to misclassify the object. Compared to wireless domains, physical attacks in vision domains are less susceptible to signal distortion and have relatively fewer domain constraints.',\n",
       " 'There are two types of target wireless systems: (1) wireless networking, which concentrates on efficient dataflow management between networked devices,',\n",
       " 'and (2) wireless communication and sensing for restoring and analyzing radio signals at the physical layer.',\n",
       " 'In this paper, we focus on the second point.',\n",
       " 'Attackers targeting wireless networking seek to deceive the ML-based network devices into making wrong decisions (e.g., for resource allocation). Certain attacks operated in a white-box setting with complete knowledge of the target ML model. In contrast, Apruzzeseet al. devised a realistic threat model by assuming a constrained attacker and demonstrated their performance across various ML systems.',\n",
       " 'When attacking wireless communication and sensing, it is crucial to design physically realizable perturbations.',\n",
       " 'TableIsummarizes existing attacks in two categories: offline attacks and online attacks. Offline attacks are impractical as they allow attackers unlimited access to inputs and models. Online attacks address this by adding UAPs to victim signals.',\n",
       " 'Several works studied methods for crafting UAPs against radio signal classifiers but aimed to identify theoretical vulnerabilities rather than design physically feasible attacks.',\n",
       " 'Flowerset al. identified victim’s transmissions by sniffing the signal strength of the target channel, but sniffing does not provide an accurate time offset due to latency and cannot reveal the modality and wireless protocol.Bahramaliet al. adopted a generative model to produce diverse UAPs, but they made the unrealistic assumption that the target system sends only one-hot vector messages. Their attacks are evaluated individually on each physical layer component rather than on an end-to-end system.',\n",
       " 'RAFA designed a practically feasible UAP in a limited-knowledge setting. They solely target the publicly-known preambles, so their attacks are not applicable to JSCC which transmits unknown data symbols. In addition, the JSCC-encoded data are modulated by various protocols (e.g., modulation, coding rate, and OFDM). Furthermore, due to the lack of diversity in its perturbations, RAFA can be directly mitigated by the adaptive defense with high accuracy (seesectionIX-B).Additionally, a recent study attacked wireless sensing systems by assuming that an adversary could install malicious firmware on the victim transmitter and change pilot packets. However, we are interested in a more realistic scenario where an adversarial signal is injected into the target channel.',\n",
       " 'Magmaw is targeted towards radio signals created by front-end sources that are used to transmit the multimodal source to back-end user(s). The attacker deploys commercial off-the-shelf (COTS) hardware (e.g., software-defined radios) to send the attack signals. We focus on vulnerabilities unique to ML in wireless environments, leading to the failure of the receiver’s JSCC decoder to correctly decode the received packet. Note that we exclude the jamming effect, a brute-force solution that disrupts all communication within the medium.',\n",
       " 'Multiple transmitters and receivers can share the spectrum. As described in AppendixA, the standard Wi-Fi protocol ensures only one device uses the wireless channel at a time within a cell to avoid collision. Magmaw can thus inject adversarial signals to target different transmitter-receiver pairs sequentially.',\n",
       " 'Magmaw can also be positioned in a selective attack that only targets a specific wireless device, leaving other devices unaffected. Specifically, Magmaw can identify the victim by sniffing the MAC address in the packet, and launch the attack whenever the victim device transmits the packet. Please note that all the above cases are equivalent to applying Magmaw’s adversarial perturbation to a single transmitter-receiver pair (as shown in Appendix Figure22).',\n",
       " 'Magmaw aims to transmit well-crafted perturbations over the target wireless channel to prevent legitimate receivers from recovering the source data and performing target downstream tasks.',\n",
       " 'To ensure stealthiness, Magmaw sends adversarial signals with a small magnitude. As a result, the victim cannot differentiate between adversarial perturbations and natural noise from wireless channels. Following the previous studies, we utilize a perturbation-to-signal ratio (PSR) metric to compare the power of the perturbation at the receiver with the received legitimate signal power. The PSR is set to be [-20,-10] dB so that the perturbation is not distinguishable from the expected natural noise in the channel.',\n",
       " 'We envision a constrained attacker with limited knowledge of ML-based wireless systems as described below.',\n",
       " 'Wireless System.We assume that the adversary has no prior knowledge about the ML model architecture/parameters, but knows the category of target models (e.g., autoencoder which is the de facto model for JSCC)',\n",
       " 'and the physical layer techniques being used (e.g., OFDM modulation which is specified in the communication standard).This is a realistic assumption for the following reasons: 1) standard documentation usually describes the core technology and is open to the public, and 2)specialized operations (seesectionII-B)for each modality have already been widely known in the ML community. The adversary trains surrogate ML-based JSCC models using a large amount of publicly available data.Note that the attacker cannot access the target JSCC model or observe the output.',\n",
       " 'Knowledge about Input and Protocols.We assume that the adversary does not know the modality and the constellation mapping method due to the following reasons: 1) all the application-layer source data, regardless of modalities, need to multiplex the transmitter radio and wireless channel, 2) the transmitter can adapt several types of modulation techniques according to channel conditions. Additionally, the JSCC model can dynamically adjust the coding rate in real time based on the current channel conditions, so the adversary has no prior knowledge about the number of OFDM symbols encoded by the JSCC model in the transmitted signal.',\n",
       " 'However, we assume that the adversary can refer to the possible coding rates specified in the standards documents. Lastly, we do not assume that the adversary knows how the transmitter maps the OFDM symbol to the subcarriers.',\n",
       " 'Target Wireless Channel.We consider a real-world attack scenario where the attacker cannot have access to the channel matrix between the transmitter and the receiver, i.e.,. In addition, we do not assume that the adversary is synchronized with either the transmitter or the receiver, leading',\n",
       " 'to random time and frequency offsets. Furthermore, we assume that the attacker can determine the carrier frequency used by the targeted channel. The attacker can overhear the victim’s signals by arbitrarily adjusting its waveform bandwidth and carrier frequency using a software-defined radio.',\n",
       " 'Attacker’s Wireless Channel.The attacker employs a single antenna to send the adversarial signal. We denote the channel matrix for the attacker as.',\n",
       " 'According to the Wi-Fi protocol, the receiver periodically sends beacons to wireless devices within the range. An attacker can overhear this transmission and estimate the channel matrix from the receiver to itself. Due to the principle of reciprocity, this channel is the same as. In contrast to recent work, we relax the assumption that the adversary knows the exact channel matrix between the attacker and the receiver. We make a weaker assumption that the adversary has limited information,',\n",
       " 'i.e., the distribution of the channel between the attacker and the receiver.',\n",
       " 'Figure3illustrates the core processing blocks in the victim communication link along with the Magmaw attacker.',\n",
       " 'ML-based Transmitter.We consider OFDM-based JSCC over a multipath fading channel withpaths. The multimodal data are transmitted usingOFDM symbols withOFDM subcarriers. Note thathas different values depending on the modality and the coding rate. For channel estimation, the sender transmits a preamble (according to the wireless communication standards) on the subcarriers. We denote the source data aswith modalityat time step, wheredenote the image, video, speech, and text, respectively. We describe a JSCC encoder for processing a modalitywith a given coding rateand modulation schemeas a function, whereis the transmitter’s',\n",
       " 'reference frame buffer used for the video JSCC, as depicted in Figure2(b). We definecontaining the previously decoded frameas:',\n",
       " 'Recall thatis a function that finds the coding order of the-th image in the given GOP structure.=when=1. This is because the first frame is coded by the image JSCC. Note thatis reconstructed as the output of a video JSCC decoder that takes encoded video sequenceas input.is the coding rate to control the number of symbols.',\n",
       " 'Then, a constellation mappermoves symbols to the nearest points in a finite constellation diagram. The modulated symbol,, can then be obtained as:',\n",
       " 'Without loss of generality, we assume the target transmitter/receiver uses a single antenna following the 802.11a/g/n Wi-Fi standard. We splitinto a number of signal vectors with dimension of. Afterwards, an OFDM transmitter allocates divided signals on each subcarrier. Each OFDM symbol passes through an inverse discrete Fourier transform (IFFT), then a cyclic prefix (CP) is added and transmitted to the receiver over a multipath fading channel.',\n",
       " 'ML-based Receiver.The receiver obtains the complex-valued symbols from the channel output by removing the CP and applying FFT with an OFDM receiver. The received signal of the-th subcarrier in the-th OFDM symbol is given by:',\n",
       " 'whereis the frequency-domain channel matrix, which is a diagonal matrix, andis the frequency-domain AWGN matrix.',\n",
       " 'Given the FFT output of the pilot signals, the channel estimation and equalization are performed to compensate the channel-induced transformation. We adopt a least squares (LS) algorithm to predict the channel state information. After equalizing all of the divided signals with the channel equalizer, we quantize the phase and amplitude of the signal on each subcarrier with. Finally, we employ the decoderto reconstruct an estimateof the original signal. We express the entire process after OFDM receiver as follows:',\n",
       " 'whereis the receiver’s decoded frame buffer for the video JSCC.=, where=when=1.=for other modalities. For simplicity, we denote all processes after the OFDM receiver as.',\n",
       " 'The framework of Magmaw is illustrated in Figure3.',\n",
       " 'Our attack methodology follows a hardware/algorithm co-design to ensure Magmaw is robust against various signal distortions.',\n",
       " 'General Attack Formulation.Our adversary aims to find an input-agnostic perturbation, with a magnitude bounded by the attacker’s power budget. Whenis injected into the victim wireless channel, the receiver obtains the frequency-domain channel outputas:',\n",
       " 'whereandrepresent the frequency-domain perturbed response and the adversarial perturbation at the-th subcarrier of the-th OFDM symbol, respectively.is the channel matrix between the attacker and the receiver. The attacker can obtainby leveraging channel reciprocity. However, the attacker does not have access to the target wireless system and therefore does not know modality, modulation scheme,,, and JSCC models.One method to address such a lack of knowledge is to utilize a set of surrogate models with different configurations (i.e.,and diverse channel matrix.Specifically, we seek to generatefrom a set of surrogate JSCC models, train UAPs using ensemble learning, and transfer the learned UAPs to the target system.',\n",
       " 'During this offline UAP training, we randomly samplefrom multipath fading model to makechannel-agnostic.',\n",
       " 'Using Equation\\xa0(4), the receiver in the surrogate model then feeds this perturbed signalto the remaining physical layer elements to reconstruct the source with modalityas:',\n",
       " 'whereis the perturbed decoded frame buffer to be used in the video JSCC model.=, where=when=1.=for other modalities.',\n",
       " 'As mentioned before, we aim to find the adversarial signals in alimited-knowledge setting (sectionIV-C). A representative way to handle this is to exploit the fact that adversarial examples exhibit good transferability between different ML models. By adopting the attack transferability, we first train a surrogate JSCC model for each modality using publicly available datasets that have different distributions from the target model’s training data. Then we use an ensemble learning approach to find a modality-agnostic adversarial perturbationby solving the following optimization problem:',\n",
       " 'whereis a set of all wireless signals that can be created by physical layer elements.is the loss function of ML-based JSCC model whenis sampled from.',\n",
       " 'However, this attack formulation is not suitable for making the UAPs physically realizable for the following reasons. First, having a singleas the UAP allows the receiver to estimate the perturbation signal using OFDM pilot signals, resulting in low robustness and persistence of adversarial attacks. Second, the adversary has no prior knowledge of the number of OFDM symbols in the target signal and thus is unable to defineas a matrix of the same size as the transmitted signal. Third, the video JSCC model has a network structure that forms a temporal chain between all video frames within the same GOP, so the model encodes current source data based on previous encoding results. This constructs the inter-frame dependency within a video sequence and it should be considered in crafting the UAPs. Fourth, the adversary does not know the distribution of the channel inputs carried by each OFDM subcarrier. Finally, when the perturbation signal overlaps with the benign signal, time or phase offsets may occur.',\n",
       " 'Practical Attack Formulation.To address the problem of Equation\\xa0(7), we construct a Perturbation Generator Model (PGM)that generates a UAP signal by receiving a',\n",
       " 'random triggerat time step. We adopt a ResNet-based generator. The adversary changesand injects a new perturbation signal into the target channel each time. Compared with using a singleas the UAP, the adversary creates an extremely large set of perturbations, which makes it difficult for the receiver to predict the perturbations. The following equation holds for frequency-domain complex-valued symbols at the receiver in the attacker’s surrogate models:',\n",
       " 'wheredenotes a UAP which containsdata symbols. Since the attacker does not know the number of target symbols,may not be equal to. We define a novel transformation functionwhich enables the PGM-generated wireless signals to model the distribution of real wireless data.',\n",
       " 'The transformation function consists of several steps: 1) symbol extension model, 2) symbol shuffling model, 3) time rotation, and 4) frequency rotation.',\n",
       " 'The symbol extension model concatenates multiple PGM-generated perturbations such that the symbol-extended perturbations can perturb all OFDM symbols of the target radio signal. The symbol shuffling model makes our attack robust against unknown target symbols by randomly shuffling symbols between the OFDM subcarriers of the adversarial signal. The time and phase rotation changes the offset of the adversarial signal during offline training so that the adversarial signals are agnostic to random time and phase shifts in the real world.',\n",
       " 'We also incorporate the power normalization into the transformation to make Magmaw undetectable from natural noise. The wireless properties controlled by the transformation function are parameterized with. Figure3shows all the modules included in the transformation function. With the help of, the PGM can be optimized to produce the perturbation signals that are resilient to real-world transformations. InsectionVI-B, we explain the internal mechanisms of.',\n",
       " 'We define an optimization problem to train the PGM that generates a hardware-implementable perturbation signal as:',\n",
       " 'whereis a set containing all radio signals that can be generated by the surrogate ML models. The perturbed signals at the receiver are computed from Equation\\xa0(6). We use mean-squared error (MSE) loss as the distortion function. We train the PGM to maximize distortion on a frame-by-frame basis for the image JSCC model. For the video JSCC model, we consider the inter-frame dependency between adjacent frames as the sum of the distortions over all frames within the GOP. This allows the PGM to adapt to any GOP without the need to reconfigure the attack. As for speech, we transform the speech data into a one-dimensional vector via the deframing functionbefore the loss is calculated.',\n",
       " 'Since the text JSCC model completes sentence restoration by sequentially finding the probabilities that words will appear with a greedy decoder, we use a cross-entropy lossbetween the predicted sentenceand the ground truth sentence.',\n",
       " 'Downstream Attack Formulation.Figure3depicts downstream tasks appended to the wireless communication pipeline. We consider two ML models as examples: 1) VC and 2) AVE. Letdenote a discriminant function for the receiver’s downstream task. After the receiver demodulates incoming perturbed signals into data, the discriminant function takes the dataand outputs a probability distribution over a setof class labels. Note that the VC takes a video clipconsisting ofconsecutive frames and the AVE receivesas two inputs. A classifier for task,, pointsto the class with the maximum probability:, whereis the probability of the perturbed input belonging to a specific class.',\n",
       " 'We define a lossto subvert classifiers:',\n",
       " 'wheredenotes the reconstructed data when there is no attack.and. The attack succeeds when. With the ensemble learning, we find UAPs that maximizefor the surrogate model with different architectures from the target model. We then fool the downstream services by',\n",
       " 'transferring the attacks calculated from the surrogate model to the target model.',\n",
       " 'Stealthy Attack Formulation.Existing works have a problem that an adaptive defender can devise an anomaly classifier that identifies the attacks by analyzing the perturbation’s statistical behavior. To enforce the generator to produce undetectable perturbations, we explicitly regularize our PGM with the discriminative loss:',\n",
       " 'whereis a discriminator that distinguishes clean signals from perturbed signals. We aim to minimizefor forcing our PGM to explore the latent space and discover robust adversarial examples. To guarantee that the PGM properly produces the diversified perturbation, we utilize the diversity sensitive loss:',\n",
       " 'whereandare two different random latent codes.',\n",
       " 'Unified Attack Formulation.Finally, we integrate all losses into the objective function so that UAPs generated by the PGM can perturb wireless communication and downstream services simultaneously. Specifically, our goal is to solve the following objective function:',\n",
       " 'where,, andweigh the relative importance of each term and. PGM generates a perturbation conditioned on the latent code and multiple controllable parameters of the wireless protocols, whiletries to distinguish between perturbed and clean signals.',\n",
       " 'In Algorithm1, we outline the training process.Please refer tosectionVII-Afor the parameters selected in the experiment.Our goal is to train the PGMthat generates UAPs to subvert ML-based JSCC models. We ensemble outputs of multimodal JSCC models to find generalizable adversarial signals that can transfer between modalities and protocols. The ML model used for training is a surrogate model that is different from the target model. We utilize the transformation functionto change the outputs of PGM to practically feasible adversarial signals. At each training iteration, the algorithm selects a batch from the training datasetwith a different distribution from the training dataset of the target model.We ensure that the PGM learns effective UAPs leveraging ensemble learning, which integrates a set of JSCC models with different. The loss values derived from each JSCC model are jointly backpropagated to optimize the PGM using the Adam optimizer.As a result, we solve four technical challenges described insectionI: (1) multi-modality and unknown, (2) unknown protocols, (3) de-synchronization, and (4) susceptibility to adaptive defense.',\n",
       " 'To cope with challenging real-world scenarios, the adversary should craft input-agnostic UAP signals regardless of synchronization with the legitimate receiver. The transformation functionhelps PGM learn to produce perturbations with a distribution similar to that of adversarial signals that can be realized in the real environment.',\n",
       " 'Therefore, our adversarial signals are agnostic to 1) inconsistency of the number of data symbols between the benign signal and the adversarial signal, 2) unknown symbol allocation across the OFDM subcarriers, 3) time misalignment, and 4) unknown phase rotation. We additionally include a power regularization for undetectability. The modules included in the transformation function are shown in Figure3and detailed below.',\n",
       " 'Symbol Extension Model.The number of OFDM symbols varies greatly depending on the modality and coding rate. Furthermore, the coding rate of the JSCC encoder determines the amount of data compressed. In an online attack, the modality and coding rate are unknown. This leads the adversary to make the attack signal invariant to the number of OFDM symbols contained in the target signal.',\n",
       " 'As the information about the coding rate is publicly available (seesectionIV-C), we can find the maximum value of.As shown in Figure4, we concatenate the PGM-generated signal multiple times through functionsuch thatis equal to the maximum value of, whereis a parameter to adjust the number of symbols.Hence, our symbol-extended perturbations can perturb all symbols of the target signal without prior knowledge of the target signal’s symbol count.',\n",
       " 'Then, we ensure that the concatenated perturbations achieve high generalizability for multiple coding rates. In Algorithm1, we randomly select the coding ratefor each training epoch.',\n",
       " 'Symbol Shuffling Model.Previous works make the assumption that the adversary knows how the target wireless system allocates symbols to each subcarrier. This is infeasible in practice, because standard wireless communications often randomize the allocation to prevent consecutive repetition of the same symbols.',\n",
       " 'Our adversary aims to make a subcarrier-invariant perturbation that is universally applicable to any symbol distribution of subcarriers. We define a functionthat randomly shuffles the symbols assigned to the subcarrier based on a seed,as shown in Figure5. Consequently, we train the PGM to generate the attack signal that is robust to the unknown symbol distribution across OFDM subcarriers.',\n",
       " 'Time and Frequency Rotation.Due to time and frequency misalignment, a random phase rotation occurs in each OFDM subcarrier. In order to enforce our perturbation to learn shift-invariant properties, we employ a phase rotation functionfrom the previous approaches, whereandare time difference and phase offset between the benign signal and the adversarial perturbation, respectively.',\n",
       " 'Power Normalization.is a power normalization function that adjusts the perturbation signal according to, which is the upper bound on the attacker’s signal power. We follow the existing power remapping function to preserve the power constraint of the perturbations as follows:',\n",
       " 'wherePSR is the ratio of the power of the attack signal to the power of the victim signal.is defined as, whereis the time-domain signal in Equation\\xa0(3).is the output of the symbol extension and symbol shuffling models.',\n",
       " 'Transformation Function.Consequently, we obtain the converted perturbation signal transmitted from the-th subcarrier of the-th OFDM symbol through the',\n",
       " 'transformation functionas follows:',\n",
       " 'Here, the transformation function is controlled by various parameters.',\n",
       " 'Figure6shows real-world attack scenarios in which the attacker (red device) sends a perturbation signal to the receiver. To thoroughly study radio signal propagation, we classify the physical environment into Line Of Sight (LoS) or NLoS (Non Line of Sight) between the transmitter and receiver. We obtain the experimental results from both Tx-Rx scenarios and then indicate the distribution of the results. We further showcase the difference in efficiency for each scenario insectionVII-D.',\n",
       " 'Target Wireless System.We first implement the ML-based wireless communication system depicted in Figure3through USRP B210, a software-defined radio widely used in designing wireless communication systems. We drive the USRP B210 using GNURadio software package that provides a graphical programming interface for configuring transceivers and allows us to model the customized blocks. The transmitter and receiver consist of a USRP B210 and a Linux laptop, respectively, and they communicate through a single antenna, where the carrier frequency is set to 2.4 GHz. The number of cyclic prefixes and subcarriersis 16 and 64, respectively. Of the 64 subcarriers, 48 are used to carry symbols for ML-based JSCC, 4 of which are used for pilot symbols.',\n",
       " 'Attack System.We build an adversarial transmitter using a USRP N310 device with a single antenna and a Linux desktop.',\n",
       " 'We randomly move the antenna to collect 2000 random realizations of the channelbetween the adversarial transmitter and receiver. Following the previous work, we set the range of PSR to [-20,-10] dB. To perform the UAP attack, we adopt surrogate models with different architectures and parameters from the target wireless communication system and the downstream classifier. We train the PGM offline according to the Algorithm1. The hyperparametersare all set to 1.',\n",
       " 'ML Models.We consider four state-of-the-art JSCC models that deliver multimodal data over the wireless channel and re-implement them based on several open-source resources. In Appendix TableIII, we show the surrogate JSCC models111We also evaluate how less similar surrogate models affect the attack performance in the supplementary document..We use constellation mapping schemesadopted in wireless standards and coding rateschosen from existing literature. The coding rate is computed as channel usage per source.Note that each JSCC model has different model weights based on the variations ofand.',\n",
       " 'Downstream Tasks.We also consider scenarios where the receiver applies the demodulated data to ML-based downstream services, such as VC and AVE. For the VC task, we benchmark three state-of-the-art models, namely, I3D, SlowFast and TPN. As a benchmark model for a multimodal task, we choose the AVE proposed by. Appendix TableIVdepicts surrogate models to craft transferable attacks.',\n",
       " 'Dataset.We choose popular multimodal datasets to train and evaluate JSCC models. For training the image and video JSCC models, we adopt the Vimeo90K dataset, which is widely used in evaluating image and video processing tasks. To facilitate efficient training, the video sequences are cropped to a resolution of. We then evaluate the image and video JSCC models using the UCF-101 dataset.',\n",
       " 'For the speech JSCC model, we use the speech dataset from Edinburgh DataShare, which contains more than 10,000 training data and 800 test data with a sampling rate of 16 KHz. We truncate the speech sample sequence to have 128 frames with a frame length of 128 after framing. For the text JSCC model, we select the proceedings of the European Parliament, which includes about 2 million sentences and 53 million words. We pre-process the dataset to have sentence lengths between 4 and 30 words. We then split it into training and test sets. We also select widely used datasets as benchmarks to evaluate VC and AVE downstream tasks. We adopt the UCF-101 human activity dataset to verify Magmaw on the VC model. For evaluating the AVE model, we adopt the audio-visual event dataset which contains 4,143 video clips with 28 events.',\n",
       " 'Evaluation Metrics.We use evaluation metrics that effectively reflect the semantic information of each modality. In the image and video domains, we select the PSNR as the representative picture quality measurement. In the speech domain, the MSE reflects the quality of the received speech. For the text domain, the BLEU score is widely used to compare the difference between the original sentence and the reconstructed one. We measure the experimental results from the two Tx-Rx scenarios (see Figure6), and then plot the distribution of the results in the figure. We use the black dotted line as the quality threshold for each experimental result, indicating that the result below it is not properly restored, which can pose a serious threat to back-end users.',\n",
       " 'Baseline Attacks.We compare Magmaw with four types of baseline attacks: (1) Random Attack, (2) Vanilla UAP Attack, (3) Sync-Free UAP Attack, and (4)One-hot Vector Modality-based (OVM) UAP Attack. We design the random attack to transmit randomly sampled Gaussian noise into the air. It resembles classic jamming, as Gaussian jamming is widely used. The vanilla UAP is an entry-level attack where multi-modality, protocol, and synchronization are not considered in crafting perturbations. The sync-free UAP attacker knows the perturbation undergoes time and phase shifts and tries to exploit such knowledge to devise shift-invariant attacks.Following previous work, OVM UAP is trained with a dataset consisting of one-hot vector messages.For downstream tasks, we compare Magmaw to random and white-box attacks.',\n",
       " 'Analysis of Magmaw.Figure7presents the reconstruction performance of the ML-based wireless transmission systems under adversarial attacks. We sweep PSR from -20dB to -10dB with steps of 2dB. We compare the performance of Magmaw to that of the baseline attacks. As shown in Figure7, Magmaw dramatically deteriorates the performance metrics in the range of all PSRs. Note that “no attack” shows the original performance of the benign model. When applying the adversarial attacks on the image JSCC model, the PSNR drops by up to 8.04dB. For the video JSCC model, PSNR is lowered by 8.29dB on average by Magmaw. We see that the video model is more vulnerable to our adversarial signals than the image JSCC model. The main reason is that the video JSCC model encodes the current frame based on the previously decoded frame, thus propagating the reconstruction distortion to the next frame. For the speech model, we find that Magmaw degrades MSE loss by 3.91 times more than the baseline. We also observe that the BLEU score of the text JSCC model drops to a minimum of 0.338 points under Magmaw.',\n",
       " 'Comparison with Baselines.As depicted in Figure7, Magmaw outperforms the baselines by a large margin. Against the image JSCC model, Magmaw lowers PSNR by up to 5.68dB more than the vanilla UAP attack and up to 4.85dB more than the sync-free UAP attack.We see that OVM UAP attacks have similar results to random attacks.Without considering the multi-modality, wireless protocols, and vulnerabilities of the model, the evaluated baselines cannot critically hurt the JSCC.',\n",
       " 'Attack Visualization.As shown in Figure8, we visualize the attack effect on the multimodal data reconstruction at the receiver. As seen, the JSCC decoder fails to retain semantic information. Specifically, the restored images and videos have noise-like artifacts, which dramatically reduce the users’ quality of experience (QoE). Furthermore, the user cannot hear the speaker’s voice in a speech sequence due to noticeable noise. The text JSCC decoder generates sentences with incorrect grammar and context, so the user cannot understand the sender’s message.In Figure9, we present the differences in complex-valued symbols before and after the attack. We observe that Magmaw’s low PSR results in minimal changes to the original signal. Additionally, as shown in Figure10, the variation in channel state information (CSI) due to perturbation is extremely low.',\n",
       " 'Analysis of Modulation.In Appendix Figure20, we further demonstrate the attack results of Magmaw for different constellation mapping methods. We confirm that Magmaw severely degrades the performance of JSCC models regardless of constellation type. As 64-QAM has slightly higher recovery performance than other modulations (16-QAM, QPSK) in all modalities, we confirm that the higher order of the modulation helps to increase the robustness.',\n",
       " 'Analysis of Magmaw.We evaluate the accuracy of each classifier when Magmaw is directed to a downstream classifier. Then, we provide a comparison with other baseline attacks. Figure11shows the attack results for the video classifiers I3D, SlowFast, and TPN and the audio-visual event classifier AVE. We compare the performance of Magmaw to white-box and random attack scenarios. We see that the changes made in random attacks are not optimized to subvert the model. In the white-box attack scenario, the attacker has complete knowledge of the classification model. Figure11presents the accuracy of each baseline for different PSRs. As shown, transmitting randomly sampled perturbations performs very poorly compared to Magmaw. As our attack consistently achieves comparable attack performance compared to the white-box attacks, we confirm that our UAP signals are successfully transferable to unseen downstream models. Specifically, Magmaw achieves an average attack success rate of 81.6, which is only 8.7lower on average than white-box attacks.',\n",
       " 'Analysis of Modulation.To analyze the influence of different constellation mapping techniques on the downstream tasks, we illustrate the attack results on the downstream classifiers when different constellation mapping methods are applied to ML-based wireless communication systems in Appendix Figure20. Although 64-QAM can increase accuracy slightly more than other modulations, we observe that our protocol-agnostic attack defeats all modulation techniques.',\n",
       " 'Analysis of Targeted Attacks.We investigate targeted UAPs aimed at flipping the prediction of inputs to a target class. To accomplish this, we define the loss function as below:',\n",
       " 'whereis a target class. We train the PGM by replacingin Equation\\xa0(13). Targeted attacks gain success if and only if.',\n",
       " 'As shown in Figure12, the targeted UAPs achieve up to 82accuracy in AVE when PSR is -10dB.',\n",
       " 'Compared to untargeted UAPs, the fooling ratio is relatively low because it is more challenging to trick the predictions of all samples into a specific class.',\n",
       " 'Impact of Multi-Modality.To understand the importance, we study the transferability of adversarial perturbations between different modalities. For each modality, we learn a modality-specific perturbation signal and then conduct an experiment in which we inject the learned perturbation into the radio signals of other modalities. As shown in Figure13(a), we see that the lack of learning generalized adversarial features limits both the cross-modal and cross-model transferability.',\n",
       " 'Effect of Modulation.To verify the effectiveness of protocol-agnostic attacks, we conduct an ablation study on attacking JSCC without considering the constellation mapping method. As shown in Figure13(b), eliminating knowledge of the physical layer protocol has a significant impact on the effectiveness of the attack. We enable the transferability of adversarial examples by creating diverse modulated signals.',\n",
       " 'Impact of Tx-Rx Placement.Each Tx-Rx scenario has different amounts of multipath because the power via the LoS path is stronger than power via the reflection path. To investigate the influence of multipath, we first compare the performance of JSCC in the two scenarios when there is no attack. As shown in Figure14(a), we see that the NLoS path makes the interference issue in wireless communication, reducing the performance of JSCC by 5. We then inject our perturbations into the channel to analyze the effect of the NLoS path. As shown in Figure14(b), we confirm that Magmaw is effective regardless of the location of Tx-Rx. A slight decrease in attack performance when the Tx/Rx path is NLoS is due to the degradation of the original performance of JSCC.',\n",
       " 'The defense performance depends on what information the defender knows about the attack formulation. FromsectionVIII-AtosectionVIII-C, we present multiple expert defenders who know the PGM’s model architecture, the channel distribution between the attacker and the receiver, and attack mechanisms illustrated in Algorithm1. InsectionVIII-D, we test Magmaw against an oracle defender who knows every detail about Magmaw.',\n",
       " 'The defender aims to obtain a robust ML-based JSCC model for each modality to protect the physical layer from the Magmaw. Since we assume that the defender knows the model architecture of the PGM, adversarial training extends the training dataset to include all adversarial examples and then trains a JSCC model on the augmented dataset. Algorithm2shows detailed steps of our adversarial training. We refer to the target JSCC models as, and denote the PGM as, which is identical to the attacker’s model architecture but with different model parameters.',\n",
       " 'The defender trains an ML-based JSCC model by selecting a',\n",
       " 'batch from the training datasetand generating the adversarial signals controlled by several parameters of the transformation function. We then expand the training dataset to include all adversarial examples and train the model on the augmented training dataset.',\n",
       " 'ML-based Wireless System.We validate Magmaw against the ML-based wireless communication systems, whose resiliency has been improved by adversarial training. As shown in Figure15(a), incorporating adversarial examples inside the model training process results in a lower ability to restore source data even if the underlying victim model is not attacked. Moreover, we observe that adversarial training cannot protect ML-based wireless communication from Magmaw. The reason is that the JSCC model has to be trained on a huge set of perturbations that the defender generates with PGM. Yet it is not feasible for the defender to train JSCC models that are resilient to all possible perturbations. Another reason is that the defender uses a PGM with different parameters from the attacker’s model, so the distribution of adversarial signals generated by the two models is different.',\n",
       " 'Downstream Tasks.Figure15(b) shows the accuracy of the downstream models trained by adversarial training. Adversarial training significantly reduces the accuracy of benign models, hindering their applicability. We observe that Magmaw still achieves a high attack success rate even though the benign model undergoes adversarial training. This is because training a model that is universally robust to different types of perturbed signals, while being able to correctly classify input data, is a fundamentally challenging problem.',\n",
       " 'This defense scheme can be performed at the physical layer before the signal is passed through the OFDM receiver. Defenders aim to mitigate the effects of perturbations and reconstruct the originally transmitted signal. As we assume that the defender has knowledge of Magmaw’s model architecture, the receiver generates a perturbation signal via the defender’s PGM and then subtracts it from the received wireless signal.',\n",
       " 'ML-based Wireless System.The defense results are summarized in Figure15(a). We observe that the source data restored by each JSCC model is more degraded than before the defense. This is because the cancellation of the adversarial signal fails and further amplifies the power of the perturbation. Even if the defender knows the structure of the PGM, the defender cannot generate exactly the same perturbation signal if the model parameters of the PGM are different.',\n",
       " 'Downstream Tasks.As shown in Figure15(b), applying perturbation signal subtraction reduces the accuracy of the downstream services by an average of 3.6. We see that the defender cannot increase the accuracy of the downstream classifier by simply subtracting an estimate of the perturbation. The accuracy of the classifier tends to depend heavily on the quality of the input source.',\n",
       " 'We define an input-level detection that aims to correctly find adversarially manipulated signals at the receiver side. The underlying hypothesis for this defense follows previous studies that show that UAPs may leave signatures observable by ML-based anomaly detection algorithms. Based on this, we design a perturbation detector that acts as a discriminator to distinguish the clean signalfrom the perturbed signal. Leveraging the trace of UAPs, we design the binary classifier as follows. First, we train the detector offline using the training dataset constructed from the defender’s PGM. In the online process, we label the received signals as adversarial attacks when the efficiency of JSCC deteriorates and include them in the training data. Finally, we fine-tune a well-trained model with newly collected data.',\n",
       " 'Appendix Figure21(a) summarizes the detection accuracy and false positive rate of our perturbation detector. It shows that Magmaw can bypass detection, even though the fine-tuning improves the accuracy of the detector. This is because Magmaw is trained to generate perturbed signals, which are indistinguishable from the clean signal, as shown in Equation\\xa0(11) and Equation\\xa0(12). For example, the fine-tuned detector only obtains up to 12accuracy to detect perturbed radio signals in the text transmission. The results in Appendix Figure21(b) have shown the detection rate of the perturbation detector when Magmaw conducted the training without regularization loss. The detector achieves about 75detection rate after fine-tuning. We verify that',\n",
       " 'ML-based detectors can offer strong generalization capability in distinguishing PGM-generated perturbations. In order to train undetectable and robust UAPs, we should leverage a discriminator to enforce stealthiness.',\n",
       " 'As shown in TableII, we report the Area Under Curve (AUC) of Receiver Operation Characteristic Curve (ROC) of the perturbation detection. The AUC metric shows the probability that the detector will assign a higher score to a perturbed signal than to a clean signal. We verify that the AUC results are close to the random guess, which means that Magmaw can achieve high undetectability. Another drawback of malware classifiers is that when an attacker changes position, the channel matrix between the attacker and the receiver also changes, requiring the defender to collect new datasets to adapt to the new environment.',\n",
       " 'It is crucial to identify the lower bound of the effectiveness of attacks. We define two strong defenders as follows:',\n",
       " 'Oracle Defenderknows complete details of Magmaw, including PGM architecture and parameters,,',\n",
       " 'the time and frequency offsets between Magmaw and receiver, and how Magmaw assigns symbols to OFDM subcarriers.',\n",
       " 'Oracle Defender without Sync Assumptionis aware of all details except the time/frequency offset. This is practical because otherwise, the receiver has to coordinate with the attacker to estimate the time/frequency offset and convey the information to the defender.',\n",
       " 'These defenders reconstruct the signal by removing the attack effect from the received wireless signal by utilizing the same perturbations generated by Magmaw.',\n",
       " 'ML-based Wireless System.The oracle defender can completely neutralize Magmaw, as shown in Figure16. These results are consistent with those reported in, which also points out that this defense is impractical.',\n",
       " 'We further measure defense performance by eliminating the assumption that the attacker and receiver are synchronized. The oracle defense without sync assumption can only reduce the efficiency of Magmaw by up to 21.42. This is because lack of synchronization causes inaccuracies in the results of perturbation removal. See detailed results in Figure16.',\n",
       " 'Downstream Tasks.We also investigate the adversarial robustness of downstream tasks in the presence of oracle defenders. We verify that addressing synchronization robustness is essential to increasing the effectiveness of the oracle defender. Specifically, without sync assumption, the oracle defender only improved the robustness of the classifier by at most 16.8. Detailed results can be found in Figure16.',\n",
       " 'Encryption schemes are commonly applied in the communication pipeline to protect users’ private data. While the robustness of privacy-preserving communications with ML-based JSCC has not been investigated before, we add the encryption and decryption blocks in image JSCC to examine the impact of Magmaw on secure transmission, and analyze the vulnerability of encrypted signals.',\n",
       " 'Experiment Design.ML-based JSCC encoder directly maps the source to the complex-valued symbols without converting it to bits. To handle this new feature, public-key encryption with LWE rather than classical AES-based schemes is applied in JSCC. In public-key encryption, any user can send encrypted messages to the receiver using the public key. Thus, we assume that the adversary knows the public key, but does not know the secret key.',\n",
       " 'Attack Results.Figure17shows the attack results on the secure communication system. We see that the OFDM symbols carrying the ciphertext of the image data are vulnerable to our perturbation signal. Specifically, Magmaw lowers the performance of secure image transmission by up to 5.88dB. This is because the decrypted output of ciphertext operations in LWE is similar to performing plaintext operations on the original plaintext data. By showing that secure communication does not provide adversarial robustness, we promote the need for new defense techniques against Magmaw.',\n",
       " 'Standard-defined preambles are widely used in ML-driven wireless systems to obtain the CSI.',\n",
       " 'We consider two ML models that are also used as target models in RAFA: (a) DLoc performs localization task via CSI received from four fixed access points, and (b) FIRE takes the CSI of the uplink channel as input and then predicts the downlink CSI. It can address the overhead of feedback exchange in the Frequency Domain Duplex (FDD) system.',\n",
       " 'Experiment Design.In the experiment setup, the sender allocates preamblesto OFDM subcarriers (i.e., 64 subcarriers for 20MHz) and then transmits them to the receiver. Here,denotes the preamble. Since Magmaw injects attack signals into the channel according to Equation\\xa0(8), the received preamble is. Thus, the receiver acquires the perturbed CSI viaand feeds it into the target ML model.',\n",
       " 'We re-implement the target models via details provided in their papers, as well as open source and the dataset. We further improve the robustness of DLoc and FIRE through adversarial training proposed by RAFA. For a fair comparison, we utilize surrogate models used in RAFA to train Magmaw.',\n",
       " 'Attack Results.We compare our attack with RAFA for a comprehensive evaluation. As shown in Figure18(a), DLoc achieves 0.71m and 1.03m localization errors at the 90th and 99th percentile. However, when Magmaw is present, the results go up to 2.7m and 9.8m. We can observe that Magmaw outperforms RAFA byon average. Figure18(b) describes the accuracy of channel estimation by FIRE. The SNR measures the similarity between the estimated downlink channel and the ground truth. We confirm that Magmaw drops the SNR of the predicted channel by 3.8dB more than RAFA. The observed underperformance of RAFA can be due to the lack of consideration for improving the robustness of adversarial attacks during the training. In contrast, Magmaw achieves high robustness by leveraging a discriminator and diversity loss that increases the variability of perturbation patterns.',\n",
       " 'To provide an intuition of the attack effect, we visualize an example of channel estimation in Figure19.',\n",
       " 'Figure19(b) shows that Magmaw causes subcarriers to have symbols that are significantly different from the actual symbols.',\n",
       " 'We present Magmaw, a novel attack framework to subvert semantic communication for AI-native networks.',\n",
       " 'Our results show that the Magmaw is feasible in the real world, and can degrade the performance of both wireless communication and downstream tasks simultaneously. Magmaw maintains a high attack success rate by evading several defenses.',\n",
       " 'In case studies, we evaluate Magmaw on encrypted communication andCSI modality-based models, proving that Magmaw is transferable.',\n",
       " 'While Magmaw demonstrates great success, the perturbation designed in this work needs to be powered by software-defined radios for flexible generation of the UAPs. Promising future work is to explore new attack methods,',\n",
       " 'e.g., intelligent reflecting surfaces, to induce small adversarial perturbations.',\n",
       " 'Another area of future research is to establish practical defense techniques to prevent the proposed attacks.',\n",
       " 'Magmaw: Modality-Agnostic Adversarial Attacks onMachine Learning-Based Wireless Communication Systems',\n",
       " 'In this paper, we calibrate the luminosity relation of gamma-ray bursts (GRBs) with the machine learning (ML) algorithms',\n",
       " 'from the Pantheon+ sample of type Ia supernovae in a cosmology-independent way. By using K-Nearest Neighbors (KNN) and Random Forest (RF) selected with the best performance in the ML algorithms, we calibrate the Amati relation (-) relation with the A219 sample to construct the Hubble diagram of GRBs.',\n",
       " 'Via the Markov Chain Monte Carlo numerical method with GRBs at high redshift and latest observational Hubble data, we find the results of constraints on cosmological models by using KNN and RF algorithms are consistent with those obtained from GRBs calibrated by using the Gaussian Process.',\n",
       " 'Type Ia supernovae (SNe Ia) have been often used as a standard candle with the maximum redshift observed at. Therefore, observations of luminous objects at higher redshift than SNe Ia are required to explore the cosmic evolution at the high-redshift region.',\n",
       " 'Gamma-ray bursts (GRBs) are the most intense bursts of high-energy gamma rays in a short time at high redshifts (the maximum redshift of GRB can reach at). Utilizing the GRB’s luminosity relations, which are connections between measurable properties of the instantaneous emission and the luminosity or energy,',\n",
       " 'GRBs have been used as cosmic probe',\n",
       " 'to study the evolutionary history of our universe and the properties of dark energy.',\n",
       " 'In the early studies of GRB cosmology, the luminosity relations of GRBs had usually been calibrated by assuming a certain cosmological model. Thus, the so-called circularity problem is encountered.',\n",
       " 'In order to avoid this circularity problem, proposed a cosmological model-independent method to calibrate GRBs at low redshift interpolated from SNe Ia and built the GRB Hubble diagram at high redshift. Following the interpolation method used in,',\n",
       " 'many works have constrained cosmological models with GRBs without any cosmological assumption, see, i. e..',\n",
       " 'On the other hand, the simultaneous method in which the parameters of the relationship and the cosmological model fitting simultaneously has been proposed to avoid the circularity problem.',\n",
       " 'Recently, it is found that the GRB relation parameters are almost identical in all cosmological models, which seems to indicate that GRBs can be standardized within error bars.',\n",
       " 'It should be notice that GRB luminosity relations can be calibrated by using other observations. For example, proposed to calibrate GRB correlations by using the observational Hubble data (OHD) obtained with the cosmic chronometers (CC) method fitted by the Bézier parametric, and built up a Hubble diagram consisting of 193 GRBs with the Amati relation (the-correlation).111Besides the calibration method by using SN Ia and OHD, the mock data of gravitational waves (GWs), quasar sample and the angular diameter distances of galaxy clusters have also been used to calibrate GRBs.Following this method,',\n",
       " 'several works have constrained cosmological models with the Amati relation calibrated by OHD.222For recent GRB luminosity relations and the applications in cosmology, see e.g., and,',\n",
       " 'and. For reviews, see.',\n",
       " 'The reconstruction from cosmological data in the calibration of GRBs can be constructed in several ways.',\n",
       " 'Similar to the interpolation method used in and the Bézier parametric used in, GRBs are calibrated from the local data by using the polynomial fitting, an iterative procedure, the local regression, the cosmography methods, a two-steps method minimizing the use of SNe Ia, and the Padé approximation method.',\n",
       " 'Recently, the non-parametric method has been addressed to reconstruction of the dark',\n",
       " 'energy, which can effectively reduce the errors of reconstructed results compared to the approaches mentioned in the above. studied the evolution of the cosmological',\n",
       " 'equation of state in a nonparametric way with high redshift GRBs.',\n",
       " 'Gaussian Process (GP) is a powerful nonlinear interpolating tool without the need of specific models or parameters, which is a fully Bayesian approach that describes a distribution over functions with a generalization of Gaussian distributions to function space. GP approach has been used in various cosmological studies, see, e.g.. However, in GP analysis, it is typically assumed that the errors in observational data follow a Gaussian distribution, which may pose a substantial limitation when reconstructing functions from data. found that GP exhibit sensitivity to the fiducial Hubble constantand the results are significantly impacted by. proposed that GP should be used with caution for the reconstruction of OHD and SNe Ia.',\n",
       " 'Furthermore, the results can be affected by the choose of the kernel functions, and there are a lot of kernel functions available that we can choose. Machine Learning (ML) algorithms are a set of technologies that learn to make predictions and decisions by training with a large amount of the observational data, which are a collection of processing units designed to identify underlying relationships in input data; therefore, when an appropriate network is chosen, the model created using ML can accurately depict the distribution of the input data in a completely data-driven way.',\n",
       " 'The ML methods have shown outstanding performance in solving cosmological problems in both accuracy and efficiency to provide powerful tools and methods for cosmological research.',\n",
       " 'Genetic Algorithms (GA) has been used to investigate the redshift evolution of the Cosmic Microwave Background (CMB) temperature, the distance duality relations (DDR) with gravitational wave (GW),',\n",
       " 'and the late-time cosmological tensions using redshift-space distortion data in the low-redshift background to show that phantom dark energy is more preferable than the cosmological constant. proposed a new nonparametric approach for reconstructing a function from observational data using an Artificial Neural Network (ANN). used the Recurrent Neural Networks (RNN) and the Bayesian Neural Networks (BNN) methods to reduce the computation load of expensive codes for dark energy models; these methods have subsequently been used to calibrate the GRB relations.',\n",
       " 'Recently, explored three machine learning treatments (linear regression, neural network, and random forest) based on Bézier polynomials to alleviate the circularity problem with the Amati relation.',\n",
       " 'The main issue in the calibration of GRBs is that we do not know a priorithe the correct curve to fitting data.',\n",
       " 'The overall advantage on using machine learning has been discussed in:',\n",
       " 'i) Healing degeneracy and over-fitting issues.',\n",
       " 'Multiple models can fit the same data will lead to degeneracy in fitting data approaches, and the overall approach of ML overcomes those issues due to interpolation, polynomials with generic over-fitting treatments.',\n",
       " 'ii) Speeding up the process of data adaption.',\n",
       " 'ML can maintain the consistency of data, which automatically encapsulates data without postulation over the shapes and orders.',\n",
       " 'The complexity of ML models turns out to intimately related to the number of data points.',\n",
       " 'Therefore, the overall process of calibration can be improved. deployed ML algorithms to measure thethrough regression analysis as Extra-Trees, ANN, Gradient Boosting, and Support Vector Machines (SVM), and found that the SVM exhibits the best performance in terms of bias-variance tradeoff in most cases, showing itself a competitive cross-check to GP.',\n",
       " 'More recently, compiled a total 220 GRBs (the A220 sample) to derive the correlation and cosmological model parameter constraints simultaneously.',\n",
       " 'By using the GP method, calibrated the Amati relation with the A219 GRB sample333Removed GRB051109A, which are counted twice in the A220 sample.from the Pantheon sample which contains 1048 SNe, and constrained cosmological models in flat space with GRBs at high redshift and OHD via the Markov Chain Monte Carlo (MCMC) numerical method. calibrated GRBs from the latest OHD to construct the GRB Hubble diagram and constrained Dark Energy models with GRBs at high redshift and SNe Ia in a flat space. used the Pantheon+ sample, which contains 1701 SNe light curves of 1550 spectroscopically confirmed SNe Ia, for calibrating the Amati relation to reconstruct cosmography parameters. used the Pantheon+ sample to calibrate the Amati relation from the latest 221 GRB sample.',\n",
       " 'In this work, we calibrate the Amati relation with the A219 GRB data at low redshift from the Pantheon+ SNe Ia sample using the ML algorithms.',\n",
       " 'Combining the high redshift GRB data with the latest OHD, we constrain cosmological models in a flat space with MCMC numerical method. We also compare the results of ML and GP methods.',\n",
       " 'In this section, we use ML algorithms to fit SNe Ia data set to reconstruct the apparent magnitude-redshift () relation. To implement the regression models of ML algorithms, we use scikit-learn, which offers a variety of ML classification and regression models to choose, e.g.,',\n",
       " 'Linear Regression(LR), Lasso Regression(Lasso), Random Forest (RF), Support Vector Regression (SVR), and K-Nearest Neighbors (KNN). In order to estimate the performance of different ML algorithms that reduce the residuals between real and fitting data,',\n",
       " 'we use the values of',\n",
       " 'the mean squared error (MSE),',\n",
       " 'which are given by',\n",
       " 'We also select MSE as the evaluation index and utilize the hyperparameter optimization method (GridSearchCV',\n",
       " 'provided by scikit-learn to determine optimal hyperparameters for the ML algorithms.666In grid search, we assess various hyperparameter combinations for each ML algorithms and utilize the 5-fold cross-validation method to select the one that minimizes MSE as the final configuration',\n",
       " 'The Pantheon+ dataset consists of 1701 light curves of 1550 unique spectroscopically confirmed SNe Ia (= 0.00122 to 2.26137), with a table of size 1701x47 (Pantheon+SH0ES.dat.',\n",
       " 'It also consists of a 1701x1701 covariance matrixwhich represents the covariance between SN Ia due to systematic and statistical uncertainties. Although ML algorithms in scikit-learn are able to predict apparent magnitude of SN Ia at a given redshift, they do not provide their uncertainties. Following, we first develop a Monte Carlo-bootstrap (MC-bootstrap) method to generate 1000 instances of data () with the initial candidate sample being drawn from the distribution of Pantheon+’s apparent magnitude () and covariance matrix.',\n",
       " 'Based on Monte Carlo sampling datasets, 1000 iterations of ML algorithms training are conducted to predict each redshift of the testing set by the objective functions. The average of the prediction results of the objective functions is taken to obtain thevalue, and the standard deviation is taken to obtain the errorvalue.',\n",
       " 'The data flow diagram of ML in our study is shown in Fig.1.',\n",
       " 'It should be noted that the Pantheon+ sample do not use SNe from SNLS atdue to sensitivity to theband in model training, therefore the Pantheon+ statistics betweenare lower than that of Pantheon and the Joint Light-curve Analysis (JLA,). We selected a series of different redshift splits (from 0.6 to 2.26, taking a point every 0.2) in the training phase of each ML algorithms to determine the reliable redshift splits of the Pantheon+ sample to calibrate GRBs.',\n",
       " 'For each redshift split, we take the SNe Iato calculate MSE, and the results with the ML algorithms are shown in Fig.2.',\n",
       " 'The results with the ML algorithms at critical redshifts (e.g.) are listed in Tab.1.',\n",
       " 'From Fig.2, Tab.1(the subgraph of Fig.2), we find that KNN and RF methods show the relatively better performances in redshift splits from 0.6 to 2.26; and all ML methods except SVM achieve the best values at, which are consistent with the redshift point empirically chosen in and from Pantheon+ sample.',\n",
       " 'In Fig.3, we plot the reconstruction of the apparent magnitude from Pantheon+ by KNN and RF methods with the relatively better performances reconstructed using the optimal hyperparameters888When training machine learning algorithms based on each of 1000 Monte Carlo sampled datasets, we employ the GridSearchCV method to determine the optimal hyperparameter values..',\n",
       " 'For GRB data set, we use the A219 sample with one point GRB051109A removed in the A220 sample, which includes the A118 data set with the smallest intrinsic dispersion, as well as 102 data set (A102) from 193 GRBs analyzed by and. We divide A219 sample into two subsamples, i.e., the low-redshift GRB sample (), which consists of 37 GRBs, and the high-redshift sample (), which consists of 182 GRBs.',\n",
       " 'The Amati relation which connects the spectral peak energy () and the isotropic equivalent radiated energy () is expressed as',\n",
       " 'where,andare free coefficients;andcan be respectively expressed as:',\n",
       " 'whereis the observational value of GRB spectral peak energy andis observational value of bolometric fluence. The luminosity distance () is related the distance modulus (),.',\n",
       " 'In order to express the GRB relation direct from the apparent magnitude, we introduce a new coefficientto rewrite the Amati relation by',\n",
       " 'where,andare free coefficients needing to be calibrated from the GRBs observed data in the formula. Therefore, we can calibrate the Amati relation without assuming any prior values of.',\n",
       " 'We use likelihood function methods to fit the parameters of Amati relation',\n",
       " 'which can be written as[104,95]',\n",
       " 'Here, the intrinsic scatter, in whichandare the intrinsic scatter along the-axis and-axis.',\n",
       " 'The likelihood function proposed by has the advantage of not requiring the arbitrary choice of an independent variable fromand.999The use of the likelihood (here,is the intrinsic scatter of GRBs,, and.) may introduce a subjective bias on the choice of the independent variable in the analysis. The Bivariate Correlated Errors and intrinsic Scatter (BCES) method used in recent Fermi data take into account the possible intrinsic scatter of the data.',\n",
       " 'The python packageemcee is used to implement the MCMC numerical fitting.',\n",
       " 'The best fitting parameters,,by KNN and RF methods',\n",
       " 'from the A219 sample at redshiftare shown in Fig.4and Tab.2For comparison, we also use theGaPPpackage the well-known Gaussian process with the squared exponential covariance function.',\n",
       " 'From Tab.2,',\n",
       " 'We find that the results of GP are consistent with previous analyses that obtained in using GaPP from SNe Ia at; and the fitting results by GP are consistent with that by KNN and RF methods in 1uncertainty, which indicate that ML methods are competitive to GP method.',\n",
       " 'In order to derive the luminosity distances of GRBs at high-redshift to build the GRB Hubble diagram, we assume that the calibration results of the Amati relation at low-redshift are valid at high-redshift.111111It should be noted that whether the luminosity relations of GRB are redshift dependent or not is still under debate.',\n",
       " 'The possible evolutionary effects in GRB relations have been discussed in many works. found that the Amati relation is independent of redshift within the error bars. proposed the improved Amati relation by accounting for evolutionary effects via copula, and found that a redshift evolutionary correlation is slightly favored. found no statistically significant evidence for the redshift evolution with the Amati relation from the analysis of data in different redshift intervals with the 221 GRB sample. calibrated the Amati relation into five redshift bins and find that GRBs seem to evolve with redshift.',\n",
       " 'Further examinations of possible evolutionary effects should be required for considering GRBs as standard candles for a cosmological probe.We utilize the calibration results obtained through the likelihood to construct the GRB Hubble diagrams atfor avoiding any bias in the selection of independent variables.',\n",
       " 'The Hubble diagram (the apparent magnitude verse the redshift) of A219 GRB sample by KNN and RF is plotted in Fig.5.',\n",
       " 'The uncertainty of the apparent magnitude with the Amati relation can be expressed as',\n",
       " 'where',\n",
       " 'Here, and the inverse of covariance matrix from the fitting coefficients is.',\n",
       " 'We use the GRB data in the Hubble diagram at high-redshift to constrain cosmological models.',\n",
       " 'The cosmological parameters can be fitted by minimizing thestatistic.',\n",
       " 'Thefunction for the GRB data can be expressed as',\n",
       " 'Here,= 182 is the number of GRBs atin the A219 sample,is the observational value of the apparent magnitude with its error, andis the theoretical value of the apparent magnitude calculated from the cosmological parametersP,',\n",
       " 'where,,is the Hubble constant, the unanchored luminosity',\n",
       " 'distance.',\n",
       " 'We consider three the dark energy (DE) models for a flat space121212The cosmological models have been usually constrained with flat spatial curvature. It should be noted that recently works constrain nonspatially flat models with GRBs and results are promising., theCDM model with the Equation of State (EoS), theCDM model (), and the CPL model evolving with redshift with a parametrization EoS (). In a flat space,',\n",
       " 'hereis the speed of light,, and, which is determined by,',\n",
       " 'The OHD can be obtained from the galactic age differential method, which have advantages to constrain cosmological parameters and distinguish dark energy models.',\n",
       " 'In our analysis, we also use the latest OHD to constrain cosmological models,',\n",
       " 'including the 31 Hubble parameter measurements at, and a new point atproposed by in a similar approach. In this work, we also use the 31 OHD atand one point atfrom.131313It should be noted that obtained another new OHD at. Considering these two measurements are not fully independent and their covariance is not clear, we only use the point, which takes advantage of thefraction of systematic uncertainty. One could either use the data from alternatively with other 31 OHD to investigate cosmology.The total OHD contain 32 data, including 15 correlated measurements with the covariance matrix. Thefunction for the OHD is,',\n",
       " 'Here the difference vector for the 15 correlated measurements between the observed data () and the theoretical values () is:;is the inverse of the covariance matrix; and thefunction for the 17 uncorrelated measurements is',\n",
       " '.',\n",
       " 'The totalwith the joint data of GRB+OHD can be expressed asThe python packageemcee for the MCMC numerical fitting is used to constrain DE models from the GRB. The cosmological parameters can be fitted by using the minimizationmethod through MCMC method. The joint results from 182 GRBs (A219)with 32 OHD are shown in Fig. 6 (CDM), Fig. 7 (CDM) and Fig. 8 (CPL).',\n",
       " 'We find the joint results by the KNN method are most identical with results by the RF algorithm with 182 GRBs atin the A219 sample and 32 OHD. By the KNN method, we obtained=,=for the flatCDM model;=,=,=for the flatCDM model; and=,=,=,=for the CPL model at the 1confidence level, which favor a possible DE evolution ().',\n",
       " 'For comparison, we also use the calibration results ofGaPPto constrain cosmological models, which are consistent with',\n",
       " 'the results by KNN and RF with slight difference.',\n",
       " 'We also find that the results by GP from the Pantheon+ data atare consistent with previous analyses that obtained in using GP from the Pantheon data atfor theCDM model and theCDM model.',\n",
       " 'For the well-knowntension,with a redshift evolving is an interesting idea141414See e. g.Wong et al. ,Krishnan et al. ,Krishnan et al. ,Dainotti et al.  for earlier work.. found thatvalue is consistent with that measured from the local data at low redshift and drops to the value measured from the CMB at high redshift.',\n",
       " 'Moreover, found the evolving (,) values abovein Pantheon+ sample.',\n",
       " 'Compared to the fitting results from CMB data based on theCDM model at very high-redshift (= 67.36 km,= 0.315) and SNe Ia at very low-redshift (= 74.3 km,= 0.298), we find that thevalue with GRBs by ML atand OHD atseems to favor the one from the Planck observations, and thevalue of our results for the flatCDM model is consistent with the CMB observations at the 1confidence level.',\n",
       " 'In order to compare the different cosmological models and ML algorithms, we compute the values of the Akaike information criterion (AIC;) and the Bayesian information criterion (BIC;), respectively:,;',\n",
       " 'whereis the maximum value of the likelihood function,is the number of free parameters in a model, andis the number of data.',\n",
       " 'We find that the results ofandby KNN, RF and GP methods indicate that theCDM model is favoured respect to theCDM model and the CPL model, which are consistent with the previous analyses obtained from the 193 GRBs by using the OHD atthrough the Bézier parametric curve combined with 740 SNe Ia.',\n",
       " 'GRB relations of the prompt emission phase involving the X-ray afterglow plateau phase exist less variability in its features. In this section, we also investigate the Dainotti relation151515 proposed the relation between the plateau luminosity and the end time of the plateau in X-ray afterglows (2D Dainotti relation) to constrain for cosmological parameters.',\n",
       " 'Furthermore, proposed the 3D Dainotti relation among the rest-frame time and X-ray luminosity at the end of the plateau emission and the peak prompt luminosity with small intrinsic scatter. investigated the 2D and 3D Dainotti relation standardized with the Platinum sample including 50 GRB data.by the ML algorithms for comparison.',\n",
       " 'The Platinum sample listed in Table A1 of are used to calibrate the Dainotti relation by the KNN and RF methods. The 2D Dainotti relation which connects the X-ray luminosityand the rest-frame time at the end of the plateau emissionis expressed as[18]',\n",
       " 'whereandare free coefficients,can be calculated by',\n",
       " 'whereis the measured gamma-ray energy flux at,is the X-ray spectral index of the plateau phase in the X-ray band;is related with the reconstructed apparent magnitude by using the ML algorithms and the absolute magnitude161616Following, we fix the absolute magnitude..',\n",
       " 'We use sub-sample atfrom the Platinum sample, which consists of 50 GRBs () to calibrate the 2D Dainotti relation.',\n",
       " 'The results by KNN and RF algorithms are summarized in Table 4. We find that the calibration results by ML from likelihood function are consistent with those in the current works calibrated with sub-sample atof the Platinum sample from SNe Ia by neural networks:; and from OHD by a Gaussian Processes Bayesian reconstruction tool:.',\n",
       " 'We combine GRB data at high-redshift () with the calibrated 2D Dainotti relation by likelihood function to constrain cosmological parameters. The joint results from the high-redshift GRBs and OHD are summarized in Table 5. We find that the results are consistent with analyses that obtained in the calibration with the Amati relation by likelihood function in Tab. 3.',\n",
       " 'In this paper, we use the ML algorithms to calibrate the Amati relation from the Pantheon+ sample to obtain the GRB Hubble diagram with the A219 sample. The KNN and RF algorithms are selected to calibrate Amati relations due to the best performances.',\n",
       " 'By the KNN algorithm with GRBs atin the A219 sample and 32 OHD, we obtained=,=for the flatCDM model;=,=,=for the flatCDM model; and=,=,=,=for the CPL model at the 1confidence level, which are most identical with results by the RF algorithm. These results favor a possible DE evolution () at the 1-confidence region for both cases.',\n",
       " 'We also find that theCDM model is favoured respect to theCDM model and the CPL model from the results ofand.',\n",
       " 'Our results with GRBs atare consistent with previous analyses that obtained in using GP from the Pantheon data and OHD at. Compared ML to GP, we find that KNN and RF methods with the lowest values in terms of MSE are competitive technics to GP in precision.',\n",
       " 'Furthermore, we also investigate the Dainotti relation by the ML algorithms for comparison. We find that calibration results of the 2D Dainotti relation are consistent with those in the current works; and constrain results at the high-redshift from the Dainotti relation are consistent with that obtained from the Amati relation.',\n",
       " 'It should be noted that recent observations from the Dark Energy Spectroscopic Instrument',\n",
       " '(DESI) collaboration display slight deviations fromCDM model, see e.g.,.',\n",
       " 'In future, GRBs could be used to set tighter constraints on cosmological models by the ML technics from recent Fermi data with much smaller scatters, as well as the data from the Chinese-French mission SVOM (the Space-based multiband astronomical Variable Objects Monitor), which will provide a substantial enhancement of the number of GRBs with measured redshift and spectral parameters.',\n",
       " 'We thank Zhen Huang, Xin Luo and Prof. Jianchao Feng, Prof. Junjin Peng for kind help and discussions.',\n",
       " 'This project was supported by the Guizhou Provincail Science and Technology Foundation: QKHJC-ZK[2021] Key 020 and QKHJC-ZK[2024] general 443.',\n",
       " 'P. Wu was supported by the NSFC under Grants Nos. 12275080, 12073069,',\n",
       " 'and by the innovative research group of Hunan Province under Grant No. 2024JJ1006, and cultivation project for FAST scientific payoff and research achievement of CAMS-CAS.',\n",
       " 'Data AvailabilityData are available at the following references:',\n",
       " 'the A219 sample of GRB data set',\n",
       " 'from,',\n",
       " 'the Pantheon+ SNe Ia sample from,',\n",
       " 'and the latest OHD obtained with the CC method from and.',\n",
       " 'Competing interestsThe authors declare no competing interests.',\n",
       " 'Ethics approvalNot applicable.',\n",
       " 'Model-independent Gamma-Ray Bursts Constraints on Cosmological Models Using Machine Learning',\n",
       " 'We introduce a comprehensive method for establishing stochastic orders among order statistics in the i.i.d.\\xa0case. This approach relies on the assumption that the underlying distribution is linked to a reference distribution through a transform order. Notably, this method exhibits broad applicability, particularly since several well-known nonparametric distribution families can be defined using relevant transform orders, including the convex and the star transform orders.',\n",
       " 'Moreover, for convex-ordered families, we show that an application of Jensen’s inequality gives bounds for the probability that a random variable exceeds the expected value of its corresponding order statistic.',\n",
       " 'Keywords: stochastic orders; hazard rate; odds; convex; star-shaped; exceedance probability',\n",
       " '2020 Mathematics Subject Classification: Primary 60E15; Secondary 62G30 62G15',\n",
       " 'Order statistics are fundamental tools in probability, statistics, and reliability theory. Especially in the context of reliability, a major issue consists of comparing order statistics with different ranks and sample sizes. To be more specific, letbe a random variable (RV) and denote withthe-th order statistic corresponding to an i.i.d.\\xa0random sample of sizefrom. Ifrepresents the lifetime of some component, thenis the lifetime of a-out-of-system, that is, a system that fails if and only if at leastcomponents stop functioning.',\n",
       " 'The ageing and reliability properties of such systems, described in terms of their stochastic behaviour, are an important aspect. Hence, the issue of comparing, in some stochastic sense, the order statisticsand, corresponding to systems with a different number of components and different functioning requirements, naturally arises. This problem can be addressed by the theory ofstochastic orders(seeShaked and Shanthikumar (2007)for general results and relationships). In particular, several results on the stochastic comparison between order statistics have been obtained, for example, byArnold and Villaseñor (1991); Arnold and Nagaraja (1991); Kochar (2006); Kochar and Xu (2009); Kochar (2012); Lando et\\xa0al. (2021).',\n",
       " 'This paper focuses on establishing conditions under whichdominatesin the sense thatfor every functionin some class. Relationships of this kind are referred to asintegral stochastic orderswith respect to a generator class, as defined byMüller (1997). These orders include comparisons of expected order statistics whencontains the identity function. Significant examples of integral stochastic orders include theincreasing concave(ICV),increasing convex(ICX), and thestar-shaped(SS) orders (seeShaked and Shanthikumar (2007)).',\n",
       " 'In contrast to numerous methods found in the literatureArnold and Nagaraja (1991); Wilfling (1996); Kundu and Chowdhury (2016), our approach does not assume a known parametric form for the cumulative distribution function (CDF)of the RV.',\n",
       " 'Instead, we opt for a more flexible approach, by making nonparametric assumptions about. Specifically, we assume that, whereis a carefully chosen cumulative distribution function, andrepresents a set of increasing functions. In other words, we assume thatis related tovia atransform orderLando et\\xa0al. (2023b), whereis referred to as the generator class.',\n",
       " 'Interesting examples of distributions satisfying transform order assumptions are theincreasing hazard rate(IHR),increasing hazard rate average(IHRA),increasing odds rate(IOR),decreasing density(DD),decreasing density on average(DDA),decreasing reversed hazard rate(DRHR) families (seeShaked and Shanthikumar (2007); Marshall and Olkin (2007); Lando et\\xa0al. (2022)).',\n",
       " 'In this paper, we show that a key step for deriving appealing probabilistic inequalities between order statistics within transform-ordered families involves combining integral and transform orders with the same generator class. Additionally, we illustrate the application of this approach by deriving bounds for expected values of order statistics.',\n",
       " 'Our method’s general behaviour aligns with expectations: stronger assumptions onlead to more applicable ordering conditions betweenand, or more stringent bounds, and vice versa.',\n",
       " 'The paper is organized as follows. In Section2, we present formal definitions and outline our general approach. Although our result is of general form, its application extends seamlessly to well-known classes of distributions, discussed in Section4. Section5delves into the derivation of conditions for the ICV and ICX order between order statistics from convex-ordered families, extending some recent results ofLando et\\xa0al. (2021).',\n",
       " 'Moving on to Section6, we establish conditions for the SS order between order statistics within star-ordered families. The approach is heuristically extended toincreasing anti-star-shaped(IAS) order, introduced in Section2, based on a simulation algorithm. Finally, Section7provides bounds for the probability thatexceeds its expected order statistic, that is, the probability that a single component surpasses the expected lifetime of the system. As a byproduct of this general result, we provide a new characterisation of the log-logistic distribution (with shape parameter 1).',\n",
       " 'Throughout this paper, “increasing” and “decreasing” are taken as “non-decreasing” and “non-increasing”, respectively, and the generalised inverse of an increasing functionis denoted as.',\n",
       " 'Moreover, the beta function with parametersis denoted with.',\n",
       " 'Finally, given an absolutely continuous CDF, its density function is denoted by the corresponding lowercase letter.',\n",
       " 'We shall consider two general families of stochastic orders, characterised either by integration or shape assumptions, which are shown to be crucial for establishing comparisons between expectations of order statistics.',\n",
       " 'Letbe some family of functions. We say thatdominatesin the-integral stochastic order, denoted as, iffor every, provided that the integrals exist.is referred to as the generator of the integral order.',\n",
       " 'Setting particular choices for the generator class we obtain some well known stochastic orders. We recall some relevant classes of functions before presenting the translation of the previous definition into specific ordering relations.',\n",
       " 'A non-negative function, defined forand such that, is said to be',\n",
       " 'star-shaped at the origin if every segment joining the origin with the graph ofalways stays above the graph, or, equivalently, ifis increasing;',\n",
       " 'anti-star-shaped (at the origin) if every segment that joins the origin with the graph ofis always below the graph, or, equivalently, ifis decreasing.',\n",
       " 'We will focus on the following integral stochastic orders, obtained from Definition2.1for particular generator classes.',\n",
       " 'Assume that. We say thatdominatesin',\n",
       " 'the usual stochastic order, denoted as, ifis the family of increasing functions;',\n",
       " 'the increasing concave (ICV) order, denoted as, ifis the family of increasing concave functions;',\n",
       " 'the increasing convex (ICX) order, denoted as, ifis the family of increasing convex functions;',\n",
       " 'the star-shaped (SS) order, denoted as, ifis the family of star-shaped functions;',\n",
       " 'the increasing anti-star-shaped (IAS) order, denoted as, ifis the family of increasing anti-star-shaped functions.',\n",
       " 'The ICV, ICX, and SS orders are well known (see for instanceShaked and Shanthikumar (2007)). Differently, the IAS order seems not to have been studied.',\n",
       " 'As we will discuss in Section6.2, the IAS order has the disadvantage, unlike the others, that an easy to check characterization is not available.',\n",
       " 'The relationships among classes of functions yield the following implications (see Theorem\\xa04.A.55 inShaked and Shanthikumar (2007)for the first line, while the second is proved later in Proposition6.6):',\n",
       " 'All these orders imply inequality of the means,since the identity function belongs to each of the above classes.',\n",
       " 'We now introduce a second general family of stochastic orders.',\n",
       " 'Letbe some family of increasing functions. We say thatdominatesin the-transform order, denoted as, or, equivalently,, if.is referred to as the generator of the transform order.',\n",
       " 'Similarly to the integral stochastic orders defined earlier, the following transform orders may be obtained from Definition2.4by takingas the class of convex and star-shaped functions, respectively.',\n",
       " 'Assume that. We say thatdominatesin',\n",
       " 'the convex transform order, denoted as, ifis the family of (increasing) convex functions;',\n",
       " 'the star order, denoted as, ifis the family of star-shaped functions.',\n",
       " 'We should note that the standard stochastic order may be seen both as an integral and a transform order. In fact,if, for every.',\n",
       " 'In this article, we show that a useful approach to obtaining interesting stochastic inequalities consists in a suitable combination of integral and transform orderings based on a common generator class.',\n",
       " 'We now address the comparison of order statistics with respect to integral stochastic orders. We shall be takingas the CDF of interest, andsome suitably chosen reference CDF. It is well known that the CDF ofis given by, whereis the CDF of a beta random variable with parametersand, that is,(seeJones (2004)).',\n",
       " 'Equivalently, one can write. This representation renders it difficult to establish conditions for a stochastic comparison between two different order statistics, sayand, since the result depends on the four parametersand on the analytical form of. In a parametric framework,is assumed to be known up to defining several real parameters, so the problem boils down to a mathematical exercise, which may still be analytically complicated.',\n",
       " 'However, ifis in some nonparametric class, the problem is more complicated, and, as we show in the sequel, it can be solved just by adding some shape constraints on. In this nonparametric framework, results may still be obtained by applying a simple decomposition trick: writeand assume thatis related to some knownby a suitable transform order. Indeed, in this case, the analytical form ofbeing known, the problem reduces to a simpler comparison between known RVs, namelyand.',\n",
       " 'For the sake of convenience and flexibility in the applications of our main result, we introduce the following notation.',\n",
       " 'Letbe some CDF andsome family of increasing functions. We define, that is, the family of CDFs that dominatewith respect to the-transform order.',\n",
       " 'We may now state our main result, which establishes sufficient conditions for comparing expected order statistics.',\n",
       " 'Letbe a class of increasing functions. If, for some given CDF,,andis preserved undertransformations, then.',\n",
       " 'Writing, the result follows easily from the definitions above. In fact, the orderis preserved under-transformations, whereas the assumptionensures thatis an-transformation. Therefore, applying the transformationto both sides of the stochastic inequality, we obtain, which implies the desired result by definition of integral stochastic orders, taking into account that.',\n",
       " '∎',\n",
       " 'Note that, ifis closed under the composition of functions, the preservation assumption of theorder is automatically fulfilled. Despite the simplicity of Theorem3.2, its applications are remarkably',\n",
       " 'interesting, showcasing the profound implications of the interplay between integral and transform orders.',\n",
       " 'Definitions2.1and2.4become particularly interesting when the generator classes are chosen as well-known and popular families. We will now show that some of the already mentioned classes are encompassed within this framework, and add a number of further interesting families of distributions that can also be addressed. Indeed, according to the choice of the classand of the reference CDFin Theorem3.2, these choices yield different families of the type, which, we recall, are defined via a transform order.',\n",
       " 'As shown below, whenis the class of increasing convex or concave functions,may be characterised using the convex transform order. Hence, we will refer to these choices ofasconvex-orderedfamilies.',\n",
       " 'Similarly, whenis the class of star-shaped or (increasing) anti-star-shaped functions,may be characterised via the star transform order, so we will refer to these choices asstar-orderedfamilies.',\n",
       " 'For the sake of simplicity, besides the already defined classesof convex functions, andof functions that are star-shaped at the origin, we shall defineas the class of concave functions, andas the class of increasing anti-star-shaped functions. Bear in mind that a function is convex if and only if its inverse is concave, so thatis equivalent to. The same relation holds between star-shaped and increasing anti-star-shaped functions, namely,is equivalent to. This is stated as follows.',\n",
       " 'is star-shaped if and only ifis increasing anti-star-shaped.',\n",
       " 'Letbe star-shaped, sois increasing. Note thatis strictly increasing by construction, but it may have jumps, corresponding to intervals at which the generalised inverseis constant. Proceeding by composition,is increasing, even in those intervals whereis constant. So the ratiois decreasing, concluding the proof.',\n",
       " '∎',\n",
       " 'The results that follow from Theorem3.2depend obviously on the choice of. In particular, we will consider the uniform distribution on the unit interval, with CDF,, the exponential distribution, with CDF,, the standard logistic distribution with CDF,, and the log-logistic distribution with shape parameter equal to 1, hereafter LL1, with CDF,. These reference distributions, as described below, lead to several well-known families of distributions.',\n",
       " 'We will also consider the corresponding “negative” versions: in general, ifthen, where. Note that, due to symmetry, for the logistic distribution we have.',\n",
       " 'Combining the classes,,, orwith the choices ofdiscussed above, we may generate several different families of distributions, some of them well known in the literature. An application of Theorem3.2will derive inequalities that hold for each of the constructed classes of distributions. Naturally, some of these are more interesting than others. Hereafter we will focus on the following ones.',\n",
       " 'The class of concave CDFs, also known asdecreasing density(DD) class, as it requires the existence of a decreasing PDF (except, possibly, at the right-endpoint of its support). This may be obtained by. This class has received much attention in the literature, for instance, it is a typical assumption for shape-constrained statistical inference (see, for example,Groeneboom and Jongbloed (2014)). Among known parametric models, the gamma, the log-logistic, and the Weibull distributions, with shape parameters less than or equal to 1, belong to this class.',\n",
       " 'The class of convex CDFs, also known asincreasing density(ID) class, as it requires the existence of an increasing PDF (except, possibly, at the right-endpoint of its support). This may be obtained by. This class is generally less applicable than the DD one, as it requires bounded support, and contains few known parametric models.',\n",
       " 'The class of star-shaped CDFs. In the case of absolutely continuous distributions, this is also known as the class of distribution withincreasing density on average(IDA). This may be obtained as. This class extends the applicability of the ID class.',\n",
       " 'The class of anti-star-shaped CDFs. In the case of absolutely continuous distributions, this is also known as the class of distribution withdecreasing density on average(DDA), as it requires theto be decreasing. This may be obtained by. This is an interesting class, as it extends the applicability of the popular DD class, allowing for non-monotonicity of the PDF and jumps in the CDF.',\n",
       " 'The class of distributions with a convexhazard function,, that is, the well-knownincreasing hazard rate(IHR) classMarshall and Olkin (2007), as it requires the existence of an increasing hazard rate function(except, possibly, at the right-endpoint of the support). This may be obtained by. The properties and applicability of IHR models are well known.',\n",
       " 'The class of distributions with a star-shaped hazard function. This is denoted as theIHR on average(IHRA) class, as it requires, in the absolutely continuous case, to be increasing. This class may be obtained as. This is a relevant class (seeMarshall and Olkin (2007); Shaked and Shanthikumar (2007)) which extends the applicability of the IHR class (in the non-negative case).',\n",
       " 'The class of distributions with a concave hazard function, that is, thedecreasing hazard rate(DHR) classMarshall and Olkin (2007), as it requires the existence of a decreasing hazard rate function. Analogously, to the previous example, this class may be obtained by.',\n",
       " 'The class of distributions with an anti-star-shaped hazard function. This is denoted as theDHR on average(DHRA) class, as it requires,',\n",
       " 'in the absolutely continuous case, to be decreasing. This class may be obtained as. It extends the applicability of the DHR class (in the non-negative case).',\n",
       " 'The class of CDFs such thatis concave, also characterised bybeing decreasing, known as thedecreasing reversed hazard rate(DRHR) class. This is a rather broad class of distributions. One may obtain this class takingand, the class of functions that dominatew.r.t.\\xa0the convex transform order.',\n",
       " 'The class of distributions with a convexodds function,, that is, theincreasing odds rate(IOR) classLando et\\xa0al. (2022), as it requires the existence of an increasing odds rate function(except, possibly, at the right-endpoint of the support). This may be obtained by. The properties and applicability of IOR models are discussed byLando et\\xa0al. (2022)andLando et\\xa0al. (2023a).',\n",
       " 'The class with a concave odds function may be similarly defined as thedecreasing odds rate(DOR) class, which may be obtained as.',\n",
       " 'The class of distributions with a convexlog-odds function,, that is, theincreasing log-odds rate(ILOR) classZimmer et\\xa0al. (1998), as it requires the existence of an increasing log-odds rate function. This may be obtained by.',\n",
       " 'The class with a concave log-odds function may be similarly defined as thedecreasing log-odds rate(DLOR) class, which may be obtained as.',\n",
       " 'In this section, we apply Theorem3.2to families of distributions which may be obtained through the convex transform order, extending some recent results ofLando et\\xa0al. (2021). All results are summarised in the following corollaries. Although some cases are already proved inLando et\\xa0al. (2021), we report them here for the sake of completeness.',\n",
       " 'If, any of the following conditions imply.',\n",
       " 'is ID and;',\n",
       " 'is IHR and;',\n",
       " 'is IOR class and;',\n",
       " 'is ILOR and.',\n",
       " 'The flexibility concerning the choice of thefamily in Theorem3.2allows for the following extension. Note that the first 4 cases of this corollary follow trivially from the previous result, and the fact thatif and only if(Theorem\\xa04.A.1 inShaked and Shanthikumar (2007)).',\n",
       " 'If, any of the following conditions imply.',\n",
       " 'is DD class and;',\n",
       " 'is DHR and;',\n",
       " 'is DOR class and;',\n",
       " 'is DLOR and;',\n",
       " 'Ifis DRHR and;',\n",
       " 'is DROR and.',\n",
       " 'Note that ifis increasing concave, thenis increasing convex, and that the ICX order is obviously preserved under increasing convex transformations.',\n",
       " 'As follows from Theorem\\xa04.A.63 inShaked and Shanthikumar (2007)(remark that the ICX order is, inShaked and Shanthikumar (2007), referred as 2-icx) in conjunction with Lemma\\xa02.6 inLando et\\xa0al. (2021), a sufficient condition foris thatand.',\n",
       " 'Then, settingas the uniform, unit exponential, LL1, standard logistic, negative exponential, and negative LL1, we obtain conditions 1–6, respectively.',\n",
       " 'We verify only case 5., the less obvious one, corresponding to, where we need to compute',\n",
       " 'using repeatedly (6.44) inViola (2016), where,, whererepresents the Euler gamma function, is thedigammafunction (we refer the reader toViola (2016), for properties of).',\n",
       " '∎',\n",
       " 'Note that, since both the ICV and the ICX orders imply the inequality between the means, Corollaries5.1and5.2provide assumptions implying that. Furthermore, we may derive conditions for the comparison with the mean of their parent distribution by settingor, respectively.',\n",
       " 'In this section we deal with families of distribution of the form, which include the family of anti-star-shaped CDFs and the DHRA family, using the SS order. Then, we move to families of the form, which include the family of star-shaped distributions and the IHRA family, using the new IAS order.',\n",
       " 'Let us start with some preliminary discussion. As starshapedness refers only to functions with domain, in this section, we will consider only non-negative RVs. First, a simple preservation property.',\n",
       " 'if and only if, for every star-shaped function.',\n",
       " 'It is also useful to remark that a functionis star-shaped if and only if its generalized inverseis increasing anti-star-shaped, as proved in Lemma4.1. We now recall the following characterization of the SS order.',\n",
       " 'if and only if, for every,',\n",
       " 'In the following subsections, we will frequently deal with transformations of beta RVs using the result stated next. The proof is omitted since it follows straightforwardly, requiring a simple observation of the shape of the graphical representation of the function considered in each case.',\n",
       " 'Let, where,, definethe set of roots of the equationthat are in, and represent byits cardinality. Then (i) if,; (ii) if,; (iii) if,.',\n",
       " 'The previous lemma means that when,has at most two elements.',\n",
       " 'Using the above lemmas, it is not difficult to apply Theorem3.2to wide families of distributions, as discussed in the next subsections.',\n",
       " 'The next theorem deals with the case of anti-star-shaped CDFs, denoted as DDA distributions.',\n",
       " 'Assume thatis anti-star-shaped.',\n",
       " 'Denote by',\n",
       " 'Ifand for every, it holds that, then.',\n",
       " 'Sinceis star-shaped, the result holds by Theorem3.2and Lemma6.1, provided that, which, taking into account Theorem6.2and the distribution of the beta order statistics mentioned before (andJones (2004)), is equivalent to',\n",
       " 'It is easily seen that (2) is equivalent to, for every. Now, the extreme points ofare at 0, 1, or among the solutions of, hence the result follows immediately from the assumptions.',\n",
       " '∎',\n",
       " 'The results of Theorem6.4can be compared with part 1. of Corollary5.2. Assume thator, equivalently, that. Ifis concave (DD class), thenfor. Ifis increasing anti-star-shaped (yielding the wider DDA class), then the stronger orderholds ifforin the described set. Recall that the ICX order is necessary for the SS order, andis necessary for the ICX order. So, the condition, forin the set defined in Theorem6.4, is stronger than just.',\n",
       " 'We may use Theorem6.4to get a complete geometric description of the-comparability of order statistics when F is DDA.',\n",
       " 'Assume the sample sizesare given. Based on Theorem\\xa01 inArab et\\xa0al. (2021)we know that, which implies, wheneverand, that is, whenever.',\n",
       " 'Likewise, this result also implies that, implying, wheneverand, which is equivalent to(see Figure1). For the regionwe have no-comparability.',\n",
       " 'The linecorresponds to points such that, whereis given by (1). Above this line we havehence, according to Theorem6.2, there is no-comparability. Finally, we are left with the region whereand, the region not shaded in Figure1, where actual verification of (2) is needed.',\n",
       " 'Forin the unshaded region it is easily seen thatwheneveris close to 0 or 1. Moreover, as Lemma6.3implies thathas two extreme points in, the monotonicity ofis “”.',\n",
       " 'A numerical verification shows that the initial interval whereis decreasing is rather small, sowill remain nonnegative wheneveris large enough. Therefore, we expect that pointsnot satisfying the assumption in Theorem6.4will be close to the top border of the unshaded region. A few examples illustrating this behaviour are shown in Figure2.',\n",
       " 'We now extend our approach to the family of DHRA distributions.',\n",
       " 'Assume thatis DHRA. Let',\n",
       " 'Ifandfor everythen.',\n",
       " 'Sinceis star-shaped, the result holds by Theorem3.2and Lemma6.1, provided that, or, equivalently,. This may be expressed as',\n",
       " 'Using the binomial Theorem, we obtain',\n",
       " 'and similarly for the second term, hence (4) is equivalent to, for every. Note that, due to the exponential terms. Now, the functionis continuous on, so it is nonnegative if and only if',\n",
       " 'its minimal value inis nonnegative.',\n",
       " 'The extreme points ofare easily seen to be among the solutions of, hence the result follows immediately from the assumptionfor every.',\n",
       " '∎',\n",
       " 'A complete geometric picture of the-comparability for DHRA distributions produces a plot similar to the one in Figure1. The shaded regions where one has comparability are the same, but the directions of the-comparability are reversed, taking into account thatandstill havedistributions with the parameters swapped.',\n",
       " 'Moreover, the red line in Figure1is now replaced by setting to 0 the two terms appearing in part 2. of Corollary5.2, that is, for eachgoing through the coordinatesandsuch thatandhave opposite signs.',\n",
       " 'The region below this curve, corresponding to, and above the diagonal is seen to be where we have no-comparability. The remaining region needs numerical verification. Hence, with respect to Figure1, one reverses the direction of the comparisons, swaps the unshaded and shaded areas between the two straight lines, and, the separating red line is no longer straight.',\n",
       " 'Our method can be applied to classes of the formusing the IAS order. This includes the important IHRA class, obtained for, and also the IDA class, where we take, the uniform distribution.',\n",
       " 'In some sense the IAS order behaves like the SS order, with the disadvantage that it does not seem to have a simple characterization based on a transformation of the CDFs, analogous to Theorem6.2, which makes it difficult to check.',\n",
       " 'The IAS order satisfies the following properties.',\n",
       " 'Letandbe nonnegative random variables with CDFsand, respectively.',\n",
       " '.',\n",
       " 'implies, for every increasing anti-star-shaped.',\n",
       " 'Letbe a random variable, and letandbe the conditional CDFs ofandwith respect to the event. Iffor every possible realizationof, then.',\n",
       " 'The first implication follows from the fact that all increasing anti star-shaped functions are increasing. Letbe an increasing concave function. Ifis also anti-star-shaped andimplies that. If, define, which is still increasing concave, and proceed similarly.',\n",
       " 'Letbe any increasing anti-star-shaped function. Taking into account thatis increasing, the quotient',\n",
       " 'is the product of two decreasing functions, hence it is decreasing itself. That is,is increasing anti-star-shaped. Therefore,implies thator, equivalently,.',\n",
       " 'This follows directly from the tower law of conditional expectations.',\n",
       " '∎',\n",
       " 'Properties 2. and 3. mean that the IAS order is closed under composition and mixtures, respectively. Property 1. shows why the IAS order can be useful. In fact, it measures size and dispersion at the same time. The IAS order implies the inequality of the means, moreover, if, thenimplies that. Below, we provide examples showing that neither of the implications in part 1. of Proposition6.6is an equivalence. The above properties suggests that the IAS order can be used as a valid (and stronger) alternative to the commonly-used ICV order, whenever we deal with star-ordered families. However, for technical reasons, the verification of the IAS order is complicated, as discussed in the next subsection.',\n",
       " 'Let,, andbe given, and consider the random variablesandwith distributions, and. As, the CDFs ofandcross, then. However, given any increasing anti-star-shaped function, so thatandis decreasing, we have. Hence, showing that the first implication in part 1. of Proposition6.6is indeed not an equivalence.',\n",
       " 'To show that also the second implication is not an equivalence, takeexponentially distributed, with CDF, andwith Weibull distribution, with shape and scale parameters equal to 2 and, respectively. Therefore,and their CDFs cross once. According to Theorem\\xa04.A.22 inShaked and Shanthikumar (2007),. On the other hand, considering the increasing anti-star-shaped function',\n",
       " 'it is easy to verify that, so.',\n",
       " 'From a practical point of view, a simple characterization of the IAS order, described in distributions terms, seems unavailable and remains an open problem. An alternative approach may be based on Theorem6.9below. To state our result, we need some additional notations. Givendefine',\n",
       " 'Moreover, given sequencesand, define',\n",
       " 'Letandbe absolutely continuous nonnegative random variables with density functionsand, respectively.if and only if, for every positive integer, and for everyand, and everydefined according to (6), we have:',\n",
       " 'As proved in Lemma4.1,is increasing anti-star-shaped if and only if, whereis star-shaped. By the change of variable,, and equivalently for the integration with respect to. Hence, we need to prove thatfor every star-shaped function. Now, every star-shaped functioncan be approximated by a sequence, where the sequencesandsatisfy the given assumptions. Indeed, this follows directly from approximating the increasing functionby an increasing step-function.',\n",
       " 'Therefore, by monotonous approximation,if and only iffor every integer, and everyandas given. Asandhave discontinuities at, andandhave densitiesand, respectively, we obtain',\n",
       " 'and similarly for.',\n",
       " '∎',\n",
       " 'Although Theorem6.9provides a necessary and sufficient condition, it requires the verification of infinitely many inequalities, thus reducing its usability for a direct verification of the IAS order. To address this difficulty we propose the following simulation algorithm to check if. Letbe the total number of repetitions; for every:',\n",
       " 'randomly generate, say, from a discrete distribution with infinite support;',\n",
       " 'randomly generate the sequencesandand defineaccording to (6);',\n",
       " 'compute',\n",
       " 'and',\n",
       " 'ifwe have that, otherwise, ifwe have an indication thatmay dominatein the IAS order.',\n",
       " 'The procedure described above can be used to check whether.',\n",
       " 'Assume that. If for every positive integer, and for every choice ofand,',\n",
       " 'then.',\n",
       " 'According to Theorem6.9, (8) is equivalent to. Then, the result follows from the fact thatis increasing anti-star-shaped and this class is closed under composition, noting that, as we are integrating with respect to a beta distribution, we only need to consider the approximation in .',\n",
       " '∎',\n",
       " 'Similarly to the previous applications, choosing a particularleads to conditions for, whenbelongs to the appropriate family of distributions. For example, takingwe find conditions that apply whenis star-shaped (or in the IDA class, referring to the families described in Section4), while the choicegives conditions whenis IHRA.',\n",
       " 'Take,,,. In this case we havewhich means that. Hence, as the strongest of the stochastic order fails to hold, we may be interested in checking that some weaker order, such as ICV or IAS, holds. Ifis convex, these values satisfyand, then.',\n",
       " 'However, ifis not convex but only star-shaped, we can check the condition of Corollary6.10for, using the proposed algorithm. Takingand random generatingfrom a Poisson distribution with parameter, we obtain, suggesting, although not actually proving, that. This would imply that, although, as the identity is an increasing anti-star-shaped function. Note thatmeans thatfor every increasing concave function, a large subset of the class of increasing anti-star-shaped functions. Nevertheless, the proposed algorithm was not able to identify an increasing anti-star-shaped function violating (8). Although this, of course, it is not guaranteed that such a function does not exist.',\n",
       " 'Now, take,,,. Even in this case,. Ifis convex we still have. However, applying again our algorithm (with the same settings), we obtain, so in this case we know that. Accordingly, one cannot conclude thatdominatesin the IAS order, although we assume thatis star-shaped. Similar examples can be provided for the IHRA case.',\n",
       " 'Consider a scenario where we represent the lifetime of a-out-of-system as. A notable challenge in reliability analysis involves determining the probability that the individual component’s lifetime falls below or exceeds the expected lifetime of the entire system, denoted as. In a parametric setting, this probability can be precisely computed using the mathematical formula of the parent CDF. However, when the exact form ofis unknown, we can leverage information about its overall shape to establish upper or lower bounds for this probability. This would follow from the application of Jensen’s inequality, under the assumption thatbelongs to a convex-ordered familyor. We remark that the case in whichis the uniform has been already discussed byAli and Chan (1965).',\n",
       " 'Given a CDF, define. For everyandsuch thatis defined, the following holds.',\n",
       " 'LetThen,.',\n",
       " 'LetThen,.',\n",
       " 'In particular, given a pair of CDFsand, if, then.',\n",
       " 'We prove just case 1., as case 2. is dealt similarly. We are assuming thatis concave, henceis convex. Therefore, Jensen’s inequality gives.',\n",
       " 'Now, taking into account thatapplyingto both sides we obtain.',\n",
       " 'Note thatimplies that, so that, hence the last statement follows.',\n",
       " '∎',\n",
       " 'Put otherwise, the above result means that, if, the expected order statisticis always smaller than or equal to the-quantile of. Similarly, if, the expected order statisticis always greater than or equal to the-quantile of, that is,. This result also enables a useful characterization of the LL1 distribution. Indeed, generally one may approximatewith: forand(constant),. This result is exact forfinite if and only ifhas an LL1 distribution.',\n",
       " 'if and only if, for any scale parameter.',\n",
       " 'First, note that, for the LL1 distribution, the expectationsare finite for everywhile.belongs to bothandif and only if. Without loss of generality, let. In this case, it is easy to verify that, so. This means that the-quantile of the LL1 is.',\n",
       " '∎',\n",
       " 'Common choices ofyield the following explicit expressions of:',\n",
       " 'If.',\n",
       " 'If,.',\n",
       " 'If,.',\n",
       " 'If,.',\n",
       " 'Table1shows thebounds forand some choices of.',\n",
       " 'The application of our results is quite straightforward. For instance, if we know that the CDF of interest,, is IHR and has a decreasing density, as is the case, for example, of the Gompertz distributions (for suitably chosen parameters), then the probability of havingis always betweenand, that is,',\n",
       " 'Ifand, this means that.',\n",
       " 'Similarly, ifis IOR and DRHR, then',\n",
       " 'As these classes are wider than the previously considered, these bounds are generally weaker, so, forand, we now find.',\n",
       " 'The bounds, with respect to the families of distributions, are sharp, as illustrated in Figure3, where we plotted true probabilities for two distributions that are both IRH and DRHR (Weibull with shape parameter larger than 1, and power distribution), and the inverted power distribution with exponent, which is not IHR, hence violates the lower bound.',\n",
       " 'The paperNichols and Padgett (2006)provides a table containing a sample of sizeof breaking stress for carbon fibers. Applying the tests ofLando (2023)andLando et\\xa0al. (2023a), respectively, it can be tested that this dataset is likely to come from an IOR distribution. Moreover, the dataset also seems to satisfy the DRHR assumption.',\n",
       " 'A straightforward interval foris then obtained by plugging in the appropriate bounds described above to the empirical CDF, that is. Taking, as an example,, this interval reduces to a single point, as we get. However, we may use instead estimators that take into account the available information about the shape of the distribution: we may use, introduced byLando et\\xa0al. (2023a), as an IOR estimator of the CDF, and, proposed bySengupta and Paul (2005), as a DRHR estimator. Differently from the empirical CDF, these estimators are continuous. Hence, an interval formay be given by. For this sample, thus taking into account the knowledge about the shape of the CDF, this leads to.',\n",
       " 'T.L. was supported by the Italian funds ex MURST 60% 2022. I.A. and P.E.O. were partially supported by the Centre for Mathematics of the University of Coimbra UID/MAT/00324/2020, funded by the Portuguese Government through FCT/MCTES and co-funded by the European Regional Development Fund through the Partnership Agreement PT2020.',\n",
       " 'Inequalities and bounds for expected order statistics from transform-ordered families',\n",
       " 'In the ever-evolving landscape of machine learning, seamless translation of natural language descriptions into executable code remains a formidable challenge. This paper introduces Linguacodus, an innovative framework designed to tackle this challenge by deploying a dynamic pipeline that iteratively transforms natural language task descriptions into code through high-level data-shaping instructions.',\n",
       " 'The core of Linguacodus is a fine-tuned large language model, empowered to evaluate diverse solutions for various problems and select the most fitting one for a given task. This paper details the fine-tuning process and sheds light on how natural language descriptions can be translated into functional code. Linguacodus represents a substantial leap towards automated code generation, effectively bridging the gap between task descriptions and executable code. It holds great promise for advancing machine learning applications across diverse domains. Additionally, we propose an algorithm capable of transforming a natural description of an ML task into code with minimal human interaction. In extensive experiments on a vast machine learning code dataset originating from Kaggle, we showcase the effectiveness of Linguacodus. The investigations highlight its potential applications across diverse domains, emphasizing its impact on applied machine learning in various scientific fields.',\n",
       " 'Automated code generation from natural language, a field often referred to as natural language programming (NLP), holds the promise of simplifying programming tasks and enhancing the software development process(Lei et al.,,2013),(Desai et al.,,2016),(Wang Sh. et al.,,2023), particularly in the field of machine learning (ML)(Chandel et al.,,2022). The demand for efficient ML solutions is continuously rising, showcasing the significance of this technology in streamlining programming tasks and enhancing software development processes. ML has transformed human lives and significantly impacted scientific research and engineering(Alpaydin,,2021). It has emerged as a standard tool in various domains, revolutionizing the way tasks are approached and problems are solved(Jung,,2022). With the increasing reliance on ML solutions, the ability to swiftly and accurately translate ambiguous task descriptions into functional code has become increasingly vital.',\n",
       " 'Early endeavors in code generation from natural language primarily rely on rule-based systems and template-based approaches(Gulwani et al.,,2023). These methods suffer from limited expressiveness and scalability as they struggle to accommodate the variability and complexity of human and coding languages(Allamanis et al.,,2018).',\n",
       " 'Vaswani A. et al., (2017)introduce the Transformer architecture, a cornerstone in many natural language processing tasks, including code generation. Transformer-based models excel in capturing long-range dependencies and contextual information, leading to significant improvements in code generation quality. The synergy of deep learning techniques and the availability of extensive training data has transformed the landscape of code generation from natural language(Vaithilingam et al.,,2021), paving the way for the development of Large Language Models (LLMs). These LLMs exhibit the capability to learn intricate mappings between textual inputs and executable code.',\n",
       " 'While significant progress has been made in code generation from natural language, there remains a substantial gap in effectively transforming complex machine learning task descriptions into precise, executable code(Yin et al.,,2022),(Wen. et al.,,2024). Current generative models often produce common yet suboptimal code snippets based on textual input, failing to capture the nuanced requirements of specific ML tasks. This gap exists primarily due to the complexity and variability of ML tasks, which often require domain-specific knowledge and customized approaches. The challenge also lies in converting detailed ML task narratives into a structured series of code components, as LLMs excel more with direct instructions. By ”instructions” we mean the high-level guidance provided to the model for generating specific outputs (see Methodology Section). Moreover, the difficulty is in maintaining coherence and logical flow throughout longer code sequences necessary for complete ML solutions. Addressing this knowledge gap can accelerate the development and prototyping of ML solutions, democratize ML development, and enhance the reproducibility and standardization of ML research.',\n",
       " 'Our approach, Linguacodus, seeks a more accurate and flexible solution. It involves a two-step process: first, it transforms the human-provided ML task descriptions into explicit, high-level instructions. This step ensures the instructions are clear, verifiable, and understandable to the user, laying a solid foundation for the next phase. Then, these high-level instructions are translated into machine-compilable code representation, specifically Python code in our case, with the potential for extension to other programming languages (Fig.1). This method not only accommodates the intricate nature of ML tasks but also enhances the control and precision in the code generation process, meeting the need for understanding and controlled production of code in ML applications.',\n",
       " 'By converting human language into executable code, Linguacodus enables quick prototyping, ease iteration, and facilitate the deployment of ML models, potentially democratizing software development. This breakthrough allows individuals without extensive coding skills to engage in creating complex ML tasks, promoting innovation across various disciplines. The drive for such technology underlines a vision to broaden ML’s reach and impact, simplifying the development process and inviting a wider audience to contribute to technological advancements. Portions of this text were previously published as part of a preprint(Trofimova et al.,,2024).',\n",
       " 'Our main contributions can be summarized as follows:',\n",
       " 'A Controllable Transformation Framework:We present a framework for the controlled transformation of ML task descriptions into solution instructions, involving fine-tuning the Llama 2 model using pairs of ML task descriptions and instructions.',\n",
       " 'Instruction-Based Sequential Generation:We demonstrate the efficacy of executing instructions for sequential generation, producing compilable code with promising results based on evaluation metrics.',\n",
       " 'The rest of the paper is organised as follows. Section ’Related Work’ explores the application of Large Language Models (LLMs) in code generation, text-to-code conversion, controllable generation, and automating problem-solving tasks, shedding light on the limitations of LLMs in ML code synthesis. Section ’Methodology’ provides an overview of the Linguacodus framework. Section ’Experimantal Results and Analysis’ describes the experiments and validation of the approach, highlighting the effectiveness of Linguacodus in transforming plain English descriptions of ML tasks into executable code. Sections ’Discussion’ and ’Limitations’ discusses and critically examines the limitations our approach. ’Future work’ suggest the future perspectives of the work. Finally, Section ’Conclusion’ summarizes and concludes the paper.',\n",
       " 'Code generation from developer’s requirements has emerged as a compelling area of research, bridging the realms of NLP and programming languages(Liu et al.,,2020). Traditional methodologies for code synthesis from human language have historically leaned on formal semantic representations of natural language(Winograd.,,1972),(Harel et al.,,1990),(Buse and Weimer,,2012). However, formal specifications require manual creation and maintenance, making them labor-intensive and difficult to scale for large codebases or complex systems(Raychev et al.,,2014).',\n",
       " 'Ling et al., (2016)automatically predict code snippets directly from natural language inputs by proposing Latent Predictor Networks (LPN). LPN encapsulates the latent variable model for capturing the underlying structure of the input natural descriptions, and the predictor network for mapping the latent representations to corresponding code snippets.',\n",
       " 'Meanwhile,Rabinovich et al., (2017),Yin and Neubig, (2017)andYin and Neubig, (2018)emphasize the importance of incorporating syntax awareness into the neural network architectures. The researchers leverage the Abstract Syntax Tree to capture the well-defined structure in the target programming syntax. Additionally, Long Short-Term Memory (LSTM) networks are employed to capture long dependencies in natural language sequences. However, these methods predominantly rely on a single NL statement.',\n",
       " 'In contrast,Agashe et al., (2019)tackle the task of interactive general-purpose code generation by incorporating a full sequence of previous natural language and code blocks as context within a Python Jupyter notebook environment(Kluyver et al.,,2016). Still, the work is limited to the domain defined by the JuICe dataset, consisting of code snippets and corresponding markdowns, and does not utilize general task descriptions as inputs for code generation.',\n",
       " 'Utilizing vast amounts of code and natural language data has been made possible through pre-training techniques(Radford et al.,,2018),(Devlin et al.,,2018). By leveraging pre-trained models, like CodeBERT(Feng et al.,,2020), researchers strive to capture comprehensive representations of both code and language semantics. This enables the models to produce code from natural language descriptions that are not only more accurate but also contextually relevant. Such models offer versatility in code-related tasks, including code generation, summarization, and recommendation.',\n",
       " 'CoditT5(Zhang et al.,,2022)is another language model that generates edit-based output sequences from corrupted input sequences. Models like CoditT5 enhance code generation capabilities, aligning them more closely with user requirements.',\n",
       " 'Modern code generation approaches often rely on general-purpose transformers, exemplified by GPT-3. Codex(Chen et al.,,2021), a notable model in this category, showcases the potential to generate code snippets directly from natural language prompts. AlphaCode(Li et al.,,2022)extends this foundation, emphasizing the significance of code diversity and improving contextual understanding in LLMs.',\n",
       " 'In parallel, text-to-code conversion has gained prominence. PaLM-Coder(Chowdhery et al.,,2023)presents a method for converting natural language descriptions into code, focusing on Java code generation. OpenAI models(Achiam et al.,,2023),(Bubeck et al.,,2023)have further extended the capabilities of LLMs in understanding and generating code from textual prompts.',\n",
       " 'Controllable code generation is an emerging subfield with significant potential. CTRL(Keskar et al.,,2019)is a conditional language model for controlled code generation. The model focuses on allowing users to specify conditions that influence the generated code, providing a level of control over the output. Texygen(Zhu et al.,,2018)is a benchmarking platform for evaluating text generation models, including those designed for code generation. This platform facilitates the assessment of controllable code generation models by offering standardized evaluation metrics and tasks.',\n",
       " 'In automating problem-solving tasks, researchers have actively explored solutions such as AutoGluonTabular(Erickson et al.,,2020)and H2O AutoML(LeDell and Poirier,,2020). These frameworks offer automated machine learning capabilities to streamline the model development process and improve prediction accuracy.',\n",
       " 'In particular, LightAutoML(Vakhrushev et al.,,2021)tailors itself to the distinctive needs of large financial services companies companies. It provides solutions for handling large datasets with diverse types, non-stationary data, and specific validations, making it well-suited for complex financial analysis tasks.',\n",
       " 'Another recent AutoML framework, HuggingGPT(Shen et al.,,2024), utilizes ChatGPT for task planning, model selection, subtask execution, and result summarization. HuggingGPT demonstrates versatility across a wide range of AI tasks, including natural language understanding and automated problem-solving.',\n",
       " 'Nair et al., (2023)present the dialog-enabled resolving agents (DERA), aiming for accurate output generation. DERA enhances the conversational abilities of LLMs by incorporating two distinct agent types: a Researcher, responsible for processing information and identifying critical problem components, and a Decider, capable of autonomously integrating the Researcher’s information and making judgments on the final output. Although the DERA paradigm was initially used in healthcare, one can notice the potential applicability of multi-agent LLM in various training fields.',\n",
       " 'While automated machine learning offers structured workflow optimization, ML code generation based on natural language descriptions provides seamless integration into existing systems and customization for domain-specific tasks.',\n",
       " 'The recent advancements in code generation driven by LLMs have witnessed notable progress. Thus, OpenAI GPT models(Achiam et al.,,2023),(Bubeck et al.,,2023), although not explicitly designed for code generation, have demonstrated proficiency in generating code snippets and understanding programming-related prompts. The generative capabilities of GPT models make them versatile tools for interpreting and translating natural language descriptions into executable code.',\n",
       " 'Google’s PaLM 2(Anil et al.,,2023)undergoes pre-training on a vast dataset encompassing web pages and source code, making it valuable for code debugging, completion, and generation across multiple programming languages. The model’s dual focus on semantic parsing and language model pre-training enhances its ability to comprehend and generate code based on diverse natural language inputs.',\n",
       " 'One of the leading publicly available LLMs for code generation is Code Llama(Rozière et al.,,2024). An extension of Llama 2(Touvron et al.,,2023), Code Llama comes in two variations: a code producer and its instruction-specific refinement, Code Llama - Instruct. Code Llama - Instruct surpasses Code Llama in providing more helpful and secure responses in natural language, ensuring a more dependable performance. However, the generated instructions are generally broad-purpose and lack easy assessability regarding their suitability for specific tasks.',\n",
       " 'While OpenAI’s ChatGPT and similar LLMs have demonstrated remarkable capabilities in various natural language understanding tasks, they do have some inherent limitations in the context of ML code generation:',\n",
       " 'Lack of Specificity: LLMs often generate code snippets that lack specificity for specific ML tasks. The generated code may be overly general and not finely tailored to the requirements of complex machine learning workflows.',\n",
       " 'Limited Control Over Code Generation: Users have limited control over the fine-tuning process of LLMs, making it challenging to enforce specific guidelines or constraints during the generation of ML code. This lack of control may result in variations in code quality and suitability for different tasks.',\n",
       " 'Handling Ambiguity: Natural language descriptions of ML tasks can be inherently ambiguous. LLMs may struggle to disambiguate between multiple potential interpretations, leading to code snippets that may not accurately capture the intended meaning of the task.',\n",
       " 'Inability to Learn Task-Specific Patterns: While proficient in learning patterns from diverse data, LLMs may face challenges in capturing task-specific patterns relevant to ML code generation. This limitation can result in generated code that lacks the specificity required for specialized tasks.',\n",
       " 'Evaluation Metrics and Validation: The evaluation metrics for assessing the quality of generated code may not always align with the specific requirements of ML tasks. LLMs may prioritize generating syntactically correct code without necessarily ensuring the semantic correctness or optimization of the generated solutions.',\n",
       " 'Addressing these challenges requires a hybrid approach involving specialized ML code datasets and dimensional reduction within the learning space for LLM fine-tuning. The Code4ML(Drozdova et al.,,2023)is a comprehensive corpus comprising of a) Kaggle challenge descriptions in natural language, b) Jupyter notebooks and their scores, c) Python code snippets, and d) competition-related metadata. This metadata includes formal descriptions of challenge datasets and scoring metrics. Code4ML relies on a knowledge taxonomy tree (Fig.2) to categorize various Jupyter notebook code snippets. A description of a challenge solution in terms of the classes of this taxonomy significantly reduces the dimensionality of a code generation problem compared to the direct generation of code by using task description as a prompt. However, Code4ML lacks annotation for all code snippets. This limitation is addressed through the taxonomy-based categorization introduced by(Berezovskiy et al.,,2023).',\n",
       " 'This section presents a comprehensive overview of the Linguacodus. Fig.3depicts the two stages of the framework. Initially, utilizing the fine-tuned Llama 2, we generate the most appropriate instruction, encapsulating the high-level core information of a generalized ML solution, tailored to a specific ML task. Subsequently, this instruction undergoes a sequential transformation into programming code through prompts with GPT-3.5.',\n",
       " 'To extract the high-level code instructions, we’ve devised a four-stage framework:',\n",
       " '1.High-Level Solution Representation: We begin by creating high-level representations of ML solutions. To refine the quality of our dataset, the solutions undergo a ranking process based on their scores. Each solution is intricately linked to the natural language description of the ML task. Linguacodus utilizes the LLM to extract critical information regarding data preprocessing, model architecture, and the training procedure from existing code solutions. This information forms the high-level ML instruction. Fig.5illustrates the precise input prompt presented to the model.',\n",
       " '2.Llama 2 Fine-Tuning: Then, we utilize the acquired instructions as inputs for fine-tuning the open-source Llama 2 7b model. To ensure the relevance of the instructions to the machine learning (ML) task, we leverage the original code’s quality evaluation in the form of a score. The retrieved instructions are ranked based on their significance to the ML task. Furthermore, we furnish the Llama 2 model with essential information presented as prompts, including the task description, metric details, and data type information. The prompt-completion pair used in this stage is visually depicted in Fig.5, with the separation marked by the [/INST] token. This comprehensive approach enhances the fine-tuning process, incorporating the quality ranking of instructions and pertinent task details for optimal model adaptation. Llama models have been pre-trained on vast amounts of data. By fine-tuning, we leverage this extensive knowledge and adapt it to specific tasks, often achieving state-of-the-art results with less data and time. The fine-tuning details are summarised in AppendixA.',\n",
       " '3.Llama 2 Inference: Next, we infer Llama 2 to select the top 3 most valuable instructions by specifying their rank using a dedicated prompt, as shown in Fig.6.',\n",
       " '4.Iterative enhancing LLM responses through multi-agent LLM: The inferred instructions then undergo further refinement with the assistance of multi-agent LLM.',\n",
       " 'The primary goal of multi-agent LLM is to identify any logical errors in the provided instructions and subsequently choose the best option from the three variants, thereby enhancing the overall quality of the instructions. This intelligent processing is elucidated in Fig.8, and8.',\n",
       " 'The second stage of our approach centers on the actual code generation, building upon the instructions obtained in the previous step. In this phase, we harness the capabilities of language models to transform these instructions into functional and well-structured code that aligns with the underlying ML tasks.',\n",
       " 'Fig.9precisely represents the sequential pipeline involved in the instruction-to-code transformation. We have separated the code synthesis into the stages of Data Preprocessing, Model Architecture, and Model Training. Additionally, we have also introduced a submission block to enable the testing of results on the Kaggle platform. The next step in this pipeline involves integrating all the generated code segments. To mitigate the possible execution problems, Linguacodus employs an error-fixing procedure, running it up to three times. In this process, the same LLM agent, responsible for integrating all code components iteratively, inputs the errors without any additional specifications.',\n",
       " 'This phase forms the critical bridge between the high-level ML instructions and the executable code, ensuring that the generated code adheres to the provided instructions and produces practical solutions for the intended ML tasks.',\n",
       " 'Our research relies on the Code4ML dataset, focusing on Kaggle competitions encompassing all metric categories except ’points,’ ’significance,’ and ’custom loss.’ We curate the top 75 solutions for retrieving high-level instructions from these competitions. It is essential to highlight that specific contests may have fewer than 75 solutions available for selection.',\n",
       " 'As a result, our training dataset comprises 395 natural language ML task descriptions paired with 7023 corresponding',\n",
       " 'Kaggle solutions. Fig.11overviews the prevalent models featured in the selected solutions. Fig.11illustrates the diversity of data types used in the chosen Kaggle competitions. This work emphasizes ML tasks involving tabular data. However, we do not restrict competitions to numeric tabular formats and consider those involving time series or text data presented with tables.',\n",
       " 'To assess the effectiveness of our approach, we employ Kaggle competitions that are recent and popular, featuring more than 500 participating teams, ensuring that the tasks were unseen by our model. To approximate the distribution of the training competition space, we randomly select ten machine learning tasks, with a majority operating on numerical data and one each for text, time series, and image data.',\n",
       " 'Linguacodus generated instructions validation extends beyond the Kaggle platform, encompassing ML competitions hosted on CodaLab(Pavao et al.,,2023). All the data used for validation and testing is not included in the training set.',\n",
       " 'The overall comparative model for our framework is vanilla GPT-3.5, considering its prominence as a leading tool in natural language generation tasks. While other models exist, such as CodeBERT, CoditT5, PalM-Coder, and CTRL, their suitability for generating code from natural language task descriptions may be limited. Specifically, CodeBERT and CoditT5 are primarily trained for synthesizing code snippets rather than entire pipelines or comprehensive solutions. Therefore, GPT-3.5 is a more relevant and established benchmark in transforming natural language into complete machine learning pipelines. Additionally, GPT-3.5 demonstrates greater efficiency compared to Llama 2(Zheng et al.,,2024)and does not require payment, as GPT-4. Code Llama - Instruct is used as a reference model for the Linguacodus Instruction Creation phase.',\n",
       " 'In our experiments, we use GPT-3.5 for retrieving instructions from the ML solutions, finding and improving the best instruction, and code generation. The selection of GPT-3.5 is driven by the consideration of balancing quality and inference time using the OpenAI API. However, the framework is generally agnostic to the choice of large language model, allowing for flexibility in utilizing different models based on specific requirements or preferences.',\n",
       " 'To underscore the significance of the research, we compare the instructions generated by the fine-tuned Llama 2 model and those inferred from Code Llama - Instruct. Our evaluation extends beyond the Kaggle platform, encompassing ML competitions hosted on CodaLab(Pavao et al.,,2023)to ensure a thorough analysis. All the data used for validation and testing is not included in the training set. We use the selected by Linguacodus best instruction from the top three inferred by Llama 2. Additionally, we include examples of instructions automatically improved with the multi-agent LLM technique through the proposition of more advanced models for training.',\n",
       " 'Instructions produced by Code Llama - Instruct generally focus on the high-level approach and conceptual steps involved in training a model. They emphasize data preprocessing, model architecture, and training goals without delving into specific implementation details. In contrast, the fine-tuned Llama 2 instructions provide detailed, step-by-step breakdowns of the data preprocessing, model architecture, and model training processes. While the former offers a broader overview suitable for understanding the overall flow of the task, the latter caters to individuals seeking a more granular understanding, providing a comprehensive guide with specific library references and functions used at each stage (see AppendixB).',\n",
       " 'Generating complete and functional code solutions using LLM requires providing the model with a detailed prompt or context outlining the task or problem statement. Hence, well-suited task instructions are vital for code generation. Our pipeline, enhanced by multi-agent LLM, can synthesize code via instructions of predefined quality, making our approach unique and promising for assisting in ML code generation. AppendixCpresents sample code generated by vanilla GPT-3.5 with automatically improved instructions and plain task descriptions. Raw GPT-3.5 output often contains code that cannot be compiled without further specific model training, whereas Linguacodus produces ready-to-run code.',\n",
       " 'Table1reports the Kaggle scores and percentiles obtained for code generated by Linguacodus and vanilla GPT-3.5 across a selection of randomly chosen machine learning tasks. TableD.19provides an overview of the mapping between task IDs and corresponding Kaggle competition names. The percentiles reported in Table1reflect the relative standing on the Kaggle competition leaderboards, where lower percentiles indicate superior performance. The 0 percentile represents the top ranking, while higher percentiles indicate lower positions on the leaderboard. This comparison provides insight into how the generated solutions perform relative to the broader Kaggle community for each specific competition.',\n",
       " 'The use of Kaggle leaderboard percentiles provides a comprehensive assessment of the generated models. Unlike traditional code evaluation metrics, such as comparing Abstract Syntax Trees(Knuth,,1968)or using code similarity measures(Song et al.,,2024), ML task performance requires a more nuanced approach. This is because the goal is to find the most effective solution for a given ML task, which can vary significantly in implementation while achieving similar results. Optimal solutions often emerge from novel combinations of existing ML techniques, making direct code comparison less relevant. Moreover, the effectiveness of generated code can only be truly measured by its performance on the specific ML task.',\n",
       " 'As shown in Table1, Linguacodus consistently produces compilable code, outperforming vanilla GPT-3.5 solutions across specified machine learning metrics. Both Linguacodus and vanilla GPT-3.5 receive natural language descriptions and necessary metadata for each machine learning task as input. To ensure a fair and unbiased comparison, the code generated by both approaches undergoes up to three iterations of error treatment.',\n",
       " 'Kaggle, as a competitive platform, traditionally demands significant investment of time and expertise from its participants. Engaging in Kaggle competitions often requires deep understanding of the field and substantial time commitment. Our pipeline for transforming ML task descriptions into code offers a markedly more efficient alternative.',\n",
       " 'This approach significantly reduces the time and expertise required to bridge the gap between task descriptions and executable code, making machine learning development more accessible. While the OpenAI GPT-3.5 API generates a default solution (without error treatment process) in approximately 6 seconds, our pipeline averages 44 seconds on an A100 GPU. This process involves generating three instructions, correcting them, and sequentially generating code. Despite the longer processing time compared to GPT-3.5, our approach consistently yields superior results.',\n",
       " 'As mentioned in ’Related Work’, the recent advancements in code generation driven by LLMs have made significant strides, yet several challenges remain. Table2discusses how these issues are addressed with Linguacodus.',\n",
       " 'Despite the advancements presented by Linguacodus in addressing the challenges outlined in the section ’Related Work’, there are several limitations that warrant consideration. The Code4ML dataset used to train Llama 2, which forms the foundation of Linguacodus, includes competitions only up to 2021. This temporal limitation means that the model may not fully cover the entire range of ML tasks and techniques, particularly recent emergent methods, potentially affecting its performance on cutting-edge problems.',\n",
       " 'Multi-agent LLM occasionally exhibits suboptimal performance compared to unprocessed Linguacodus instructions, emphasizing the role of context in task’s complexity. Ethical considerations surrounding biases and potential misuse of generated code highlight the need for responsible deployment. Linguacodus faces challenges when tasks deviate significantly from those fine-tuned on Llama 2, suggesting a need for dataset enrichment.',\n",
       " 'Insufficiently detailed instructions arise when tasks lack comprehensive descriptions, calling for more explicit task information. Recognizing that multi-agent LLM may not consistently outperform initially inferred instructions, human intervention is proposed to select the best instruction. This highlights the need for a balanced approach that combines the strengths of automated models with human judgment in refining outputs.',\n",
       " 'The temporal limitation of the training dataset underscores the importance of ongoing model updates and the potential for performance gaps in very recent or rapidly evolving areas of machine learning. This observation points to a development of a dynamic framework for enriching the ML data corpus. Such a framework would allow for continuous integration of new ML techniques, datasets, and competition results, ensuring that models like Linguacodus remain current and effective across the evolving landscape of machine learning tasks.',\n",
       " 'Another promising direction for future work involves exploring alternative, more deterministic approaches to constructing high-level instructions. One such approach is the development of a graph-instruction methodology. This could enable a more structured representation of the ML task, allowing for better assessment of intermediate generation steps and interpretability. By mapping the natural task description to a graph-based representation, we could potentially achieve greater transparency in the instruction generation process, facilitating easier evaluation and refinement of the model’s outputs.',\n",
       " 'In this paper, we introduce a comprehensive approach to transforming unstructured ML task descriptions into executable code, presenting the novel Linguacodus model. Leveraging the Code4ML dataset, which encompasses a rich collection of Python code snippets, contest summaries, and data descriptions from Kaggle competitions, our methodology capitalizes on the dataset’s valuable competition-related metadata, data types, and scoring metrics. Inspired by the knowledge taxonomy tree introduced inDrozdova et al., (2023), we adopt a similar organizational framework to achieve dimensional reduction in our ML task description-to-code synthesis approach. However, our approach differs in that it focuses on high-level information extraction rather than individual code snippet classification. This strategic shift simplifies and streamlines the code generation process, making it more efficient and adaptable.',\n",
       " 'Linguacodus is structured into two phases: synthesizing high-level ML solution instructions and transforming these instructions into functional code. To generate instructions, the Llama 2 model is fine-tuned on the Code4ML corpus. The top three instructions are then inferred and further refined with the assistance of multi-agent LLM, ensuring the highest quality instructions for subsequent code generation. The second phase involves translating these refined instructions into well-structured and executable code segments, encompassing data preprocessing, model architecture, model training, and submission block generation. This transformation bridges the gap between high-level ML instructions and practical code, ensuring alignment with the underlying ML tasks.',\n",
       " 'Our approach’s effectiveness is validated through experiments on Kaggle competitions that are not part of our training data. The results demonstrate that the generated code is compilable and aligns well with the evaluation metrics. We also compare the performance of multi-agent LLM and unprocessed Code Llama - Instructions, highlighting the need for further refinement in multi-agent LLM’s algorithmic approach to achieve superior solution quality consistently.',\n",
       " 'In summary, the research provides an innovative and efficient solution for code generation from ML task descriptions, showcasing the capabilities of Linguacodus. By capitalizing on the Code4ML dataset’s wealth of resources and introducing a structured approach to instruction synthesis and code generation, we bridge the gap between natural language task descriptions and executable code, making machine learning development more accessible and efficient.',\n",
       " 'CRediT authorship contribution statementEkaterina Trofimova: Conceptualization, Investigation, Methodology, Software, Validation, Formal analysis, Writing - Original Draft, Writing - Review & Editing, Visualization, Supervision, Project administration. Emil Sataev: Investigation, Software, Methodology, Data Curation, Formal analysis. Andrey E. Ustyuzhanin: Conceptualization, Supervision, Formal analysis, Funding acquisition, Methodology, Writing – Review & Editing.',\n",
       " 'AcknowledgmentsWe would like to express our appreciation to Denis Derkach and Artem Maevskiy for their invaluable comments and support.',\n",
       " 'Linguacodus: A Synergistic Framework for Transformative Code Generation in Machine Learning Pipelines',\n",
       " 'Gaze following aims to interpret human-scene interactions by predicting the person’s focal point of gaze.',\n",
       " 'Prevailing approaches often adopt a two-stage framework, whereby multi-modality information is extracted in the initial stage for gaze target prediction.',\n",
       " 'Consequently, the efficacy of these methods highly depends on the precision of the preceding modality extraction.',\n",
       " 'Others use a single-modality approach with complex decoders, increasing network computational load.',\n",
       " 'Inspired by the remarkable success of pre-trained plain vision transformers (ViTs),',\n",
       " 'we introduce a novel single-modality gaze following framework called ViTGaze.',\n",
       " 'In contrast to previous methods, it creates a novel gaze following framework based mainly on powerful encoders (relative decoder parameters less than 1%).',\n",
       " 'Our principal insight is that the inter-token interactions within self-attention can be transferred to interactions between humans and scenes.',\n",
       " 'Leveraging this presumption, we formulate a framework consisting of a 4D interaction encoder and a 2D spatial guidance module to extract human-scene interaction information from self-attention maps.',\n",
       " 'Furthermore, our investigation reveals that ViT with self-supervised pre-training has an enhanced ability to extract correlation information.',\n",
       " 'Many experiments have been conducted to demonstrate the performance of the proposed method.',\n",
       " 'Our method achieves state-of-the-art (SOTA) performance among all single-modality methods',\n",
       " '(3.4% improvement in the area under curve (AUC) score, 5.1% improvement in the average precision (AP))',\n",
       " 'and very comparable performance against multi-modality methods with 59% number of parameters less.',\n",
       " 'Gaze following is the task of predicting a person’s gaze target in an image.',\n",
       " 'Specifically, given an image and a bounding box of a person’s head, it aims to predict the location of the point where the person is watching.',\n",
       " 'It is widely applied in the fields of human-computer interactionapp_interactand neuroscienceapp_Neural.',\n",
       " 'Gaze following through RGB images has been a longstanding topic of research, with numerous related studies developed over time.',\n",
       " 'Previous works have taken two approaches.',\n",
       " 'One approach introduces multi-modality frameworks to improve the prediction performance.',\n",
       " 'These methods often adopt a two-stage methodology.',\n",
       " 'In the initial stage, task-specific modality predictors are employed to extract supplementary information, including depth and poses, as additional inputs to compensate for the absence of human-scene interactionsFang_DAM_2021_CVPR;Bao_ESC_2022_CVPR;Gupta_MM_2022_CVPR.',\n",
       " 'In the second stage, the visual features and the multi-modality input extracted in the initial stage are combined and utilized by the decoder to regress the gaze target heatmap.',\n",
       " 'Another approachTu_HGGTR_2022_CVPR;Tonini_GOT_2023_ICCV;tu2023jointinvolves a query-based decoder and utilizes additional object bounding boxes to improve performance by learning person-object interactions.',\n",
       " 'These methods employ a convolutional backbone for image feature extraction.',\n",
       " 'A transformer decoderdetris subsequently used to mix global information and provide gaze target predictions corresponding to the gaze queries.',\n",
       " 'However, both methods have drawbacks:',\n",
       " 'The additional information results in a multi-modal design with a two-stage framework. The accuracy depends on the performance of the prior predictions.',\n",
       " 'The query-based methods require a heavy decoder, which increases the complexity of the whole design.',\n",
       " 'We posit that these drawbacks stem from a shared design flaw: the absence of a sufficiently robust encoder for feature extraction of human-scene interactions.',\n",
       " 'Recently, the pre-trained plain vision transformers (ViTs)vithave demonstrated remarkable visual modeling capabilities.',\n",
       " 'A few works have explored the use of plain ViT on several downstream tasksvitdet;vitpose;vitmatteand achieved impressive results, highlighting the capacity of encoding rich features of the task-agnostic pre-trained ViTdino;dinov2.',\n",
       " 'Inspired by these previous works, it would be interesting to raise the following question: Can a pre-trained ViT provide an effective interactive representation between humans and the scene to describe a gazing relationship?',\n",
       " 'We propose ViTGaze, a concise single-modality and lightweight gaze following method based on pre-trained plain ViTs.',\n",
       " 'Our principal observation is that the inter-token interactions within self-attention can be transferred to interactions between humans and scenes.',\n",
       " 'The dot product operation in the self-attention mechanism inherently encodes token correlations.',\n",
       " 'Therefore, we hypothesize that the interactions between humans and scenes are also embedded in these self-attentions. This assumption of the self-attention map is consistent with the observations of previous studiesLOST;weaktr.',\n",
       " 'On this basis, we design a concise 4D interaction encoder to extract the interaction information from multi-level and multi-head attention maps and design a 2D spatial guidance module to guide it.',\n",
       " 'Owing to the strong ability of pre-trained ViTs to extract interactions between objects, ViTGaze does not introduce any decoder design.',\n",
       " 'In contrast to previous methods, ViTGaze creates a brand-new paradigm of gaze following, which is mainly based on encoders (decoder parameters account for less than 1%).',\n",
       " 'Furthermore, we also observe that self-supervised pre-trained ViTs are better at understanding token interactions. In contrast to previous ViT-based methodsvitdet;vitpose;vitmatte;cellvit, our method further investigates the importance of attention maps while using the feature map for gaze following.',\n",
       " 'Compared with existing state-of-the-art methods, our method has advantages in both performance and efficiency.',\n",
       " 'We conduct experiments on the two most widely-used benchmarks, the GazeFollowRecasens_GazeFollow_2015_NIPSand VideoAttentionTargetChong_VideoAttn_2020_CVPR.',\n",
       " 'The experimental results demonstrate the superiority of our method.',\n",
       " 'Specifically, our method achieves a 3.4% improvement in the area under curve (AUC) and a 5.1% improvement in the average precision (AP) among single-modality methods, resulting in a new state-of-the-art (SOTA) performance as shown in Fig.1.',\n",
       " 'In addition, our method achieves comparable performance to multi-modal methods (1.8% lower in distance error but 2.7% higher in AUC) with 59% fewer parameters. These results provide ample evidence of the validity of our method, demonstrating that a single-modality lightweight framework built upon pre-trained ViTs could also achieve SOTA performance in gaze following.',\n",
       " 'Our contributions can be summarized as follows:',\n",
       " 'We propose ViTGaze, a single-modality lightweight gaze following framework based on pre-trained vision transformers.',\n",
       " 'To the best of our knowledge, this is the first gaze-following method built upon the pre-training of ViTs.',\n",
       " 'We demonstrate the feasibility of extracting human-scene interaction information from inter-tokens interactions in self-attentions and design a 4D interaction module guided by a 2D spatial guidance module.',\n",
       " 'We evaluate our method on GazeFollowRecasens_GazeFollow_2015_NIPSand VideoAttentionTargetChong_VideoAttn_2020_CVPRbenchmarks.',\n",
       " 'Our method achieves SOTA performance (3.4% improvement in the AUC and 5.1% improvement in the AP) among the single-modality methods and very comparable performance with multi-modality methods with 59% fewer parameters.',\n",
       " 'Research on gaze behavior has drawn significant academic interest across multiple domains.',\n",
       " 'One prominent area is gaze estimationzhong2024uncertainty, which involves inferring gaze direction from facial cues.',\n",
       " 'Another key domain is scan path predictionzhong2024spformer;xia2020evaluation, which focuses on modeling the sequence of fixations across an image.',\n",
       " 'Unlike these works, gaze followingRecasens_GazeFollow_2015_NIPSaims to interpret human-scene interactions by locating the gaze target of a person to provide an understanding of how individuals interact with their surroundings.',\n",
       " 'Previous research on gaze following can be classified into two principal categories: single-modality methods and multi-modality methods.',\n",
       " 'The ground-breaking workRecasens_GazeFollow_2015_NIPSdesigns a single modality architecture using only RGB images as the input.',\n",
       " 'In this work, gaze following is formulated as the combination of the individual’s gaze field and the global saliency mapsaliency1;saliency2;saliency3.',\n",
       " 'The subsequent series of worksChong_Connect_2018_ECCV;Lian_ACCV_2019;zhaoLearningDrawSight2020focus on providing human-scene interactions through auxiliary tasks such as sight line estimationLian_ACCV_2019;zhaoLearningDrawSight2020.',\n",
       " 'GaTectorWang_GaTector_CVPR_2022proposes a unified framework to extract features from both head crops and scenes with a shared backbone.',\n",
       " 'HGTTRTu_HGGTR_2022_CVPRproposes a transformer-based encoder-decoder architecture, which provides implicit interaction clues through the global modeling of self-attention.',\n",
       " 'Despite the concise input of these methods, they often achieve unsatisfactory performance compared with multi-modality methods.',\n",
       " 'To further improve prediction performance, a few methods utilize additional modality information besides RGB images.',\n",
       " 'Fang et al.Fang_DAM_2021_CVPRproposed a depth-assisted architecture to estimate spatial relationships between the person and the scene through an extra depth estimator.',\n",
       " 'Furthermore, the human pose estimation sub-task is incorporated into the frameworkBao_ESC_2022_CVPR;Gupta_MM_2022_CVPR;Jian_Enhanced_ICMM_2020, offering a more precise 3D spatial interaction estimation.',\n",
       " 'The temporal information is utilized in Refs.Chong_VideoAttn_2020_CVPR;Miao_PDP_2023_WACVto improve prediction performance.',\n",
       " 'Additional object segmentation masks are incorporated in Refs.Chen_TPNet_TCSVT_2022;Hu_GazeTargetEstimation_TCSVT_2022to model object interactions.',\n",
       " 'Samy et al.sharingandirectly embeded the head position along with the coarse sight direction into a token and utilized a ViT to predict precise gaze targets.',\n",
       " 'GTRtu2023jointincorporates gaze object detection into the HGTTR framework, providing object-level supervision for accurate gaze localization.',\n",
       " 'Francesco et al.Tonini_GOT_2023_ICCVexpanded DETRdetrto detect objects and gaze points simultaneously.',\n",
       " 'However, these methods mostly adopt a two-stage design and their performance is dependent on the prediction results from the first stage.',\n",
       " 'Hence, how to design a concise and high-performance single-modality gaze following framework remains an unsolved problem.',\n",
       " 'Pre-training can improve ViTvitin learning transferable visual representations.',\n",
       " 'Recently, researchersbeit;maefocus on self-supervised pre-training employing masked image modeling (MIM).',\n",
       " 'Furthermore, the integration of CLIPclipwith MIM by Refs.mvp;cae;eva;eva02results in a notable enhancement in the performance of pre-training.',\n",
       " 'Notably, approaches such as those in Refs.zhou2021ibot;dinov2leverage online tokenizers to further optimize the performance of pre-training.',\n",
       " 'The pre-trained representations of ViTs can significantly enhance the performance of downstream tasks.',\n",
       " 'ViTDetvitdetapplies pre-trained ViT to object detection by constructing a simple feature pyramid.',\n",
       " 'ViTPosevitposeinvestigates the performance of ViT in pose estimation tasks via a similar approach.',\n",
       " 'Additionally, ViT is employed in ViTMattevitmattein image matting tasks by designing a detail capture module.',\n",
       " 'In the medical field, pre-trained ViT can also further enhance the performance of cell segmentationcellvit.',\n",
       " 'In contrast, WeakTrweaktrleverages attention maps from a pre-trained ViT for weakly supervised image segmentation.',\n",
       " 'However, it remains an unexplored question whether the pre-trained representations contain enough interaction information to facilitate gaze following.',\n",
       " 'We first review the dot-product self-attention mechanism proposed in Ref.transformer.',\n",
       " 'The input token sequenceis first transformed into keys, queries, and values, whereis the length of the sequence andis the number of channels.',\n",
       " 'Then the dot product performed between queries and keys captures token correlations which are normalized and used as weights to aggregate the values.',\n",
       " 'where,, andare queries, keys, and values respectively, andrefers to the softmax operation.',\n",
       " 'The normalized dot product, which is also known as the attention map, adaptively determines the relationship between each pair of tokens.',\n",
       " 'In the vision region, an image with a size ofis first flattened to a patch sequence and then generates an attention map, which inherently follows a 4D paradigm that effectively indicates the interactions among patches.',\n",
       " 'This 4D interaction feature map has proven to be particularly effective in capturing patch correlations in an image, resulting in advanced performance in various computer vision tasks, such as weakly-supervised segmentationweaktrand unsupervised object localizationLOST.',\n",
       " 'The overall structure of our method is shown in Fig.2.',\n",
       " 'It takes an image and a bounding box of the person’s head as the inputs.',\n",
       " 'It outputs a heatmap that locates the gaze target with the highest response, and the probability that the person watches outside the scene.',\n",
       " 'Our method is composed of three components: a pre-trained vision transformer encoder that extracts a feature map with rich semantics and multi-level 4D interaction features, a 2D spatial guidance module to determine the spatial weights corresponding to each patch for person-specific interaction feature aggregation, and two prediction heads for heatmaps and in-out prediction.',\n",
       " 'These components are described in detail in the following subsections.',\n",
       " 'Inspired by the capacity of self-attention to capture patch-level interactions, we propose a 4D interaction encoder for efficient human-scene interaction estimation.',\n",
       " 'In Fig.3, we visualize the attention map of the token overlapped with the head of a person and the feature map output by the last block of a DINOv2dinov2pre-trained ViT.',\n",
       " 'Rich semantic information in the feature map enables effective distinction between objects.',\n",
       " 'However, it cannot represent interactions between image regions.',\n",
       " 'In contrast, a specific token’s attention map is capable of acquiring interaction information between it and other regions.',\n",
       " 'Therefore, we propose the interaction encoder to leverage attention maps, which are referred to as 4D interaction features, from pre-trained ViTs with a simple adaptation (Fig.4(a)).',\n",
       " 'Compared with the encoders used in the previous tasks (Fig.4(b)) such as object detectionvitdetand image mattingvitmattethat utilize feature maps to distinguish objects, the interaction feature-based encoder inherently facilitates the explicit capture of patch relations, which is required by gaze following.',\n",
       " 'Given an image with a resolution of. The interaction encoder extracts 4D features, which explicitly describe the interactions between image patches. The correlations among them are represented as a 4D tensor with a size of, whereand, anddenotes the patch-size of the ViT.',\n",
       " 'Different from the final feature map leveraged in other regionsvitdet;vitpose;vitmatte, this 4D representation reflects the inner-token relations which are more effective in gaze following.',\n",
       " 'Transformers pre-trained with masked image modeling capture correlations at multiple scales in multiple layersdark_secret.',\n",
       " 'To this end, we extract multi-level and multi-head 4D features to capture correlations between tokens with multiple distances.',\n",
       " 'Specifically, we extract features fromtransformer layers, and each feature is divided intosub-features based on different attention heads.',\n",
       " 'These attention maps represent patch interactions at both local and global levels and are combined to create multi-level 4D interaction features, whereandrefer to the number of semantic levels and the number of heads in the multi-head self-attention, respectively.',\n",
       " 'The extracted features are then guided by spatial information and used for heatmap prediction.',\n",
       " 'We propose a 2D spatial guidance module for aggregating the 4D interaction features with the head position to obtain person-specific interaction features.',\n",
       " 'The insight between this module has two parts:',\n",
       " '1) The attention map describes how each image patch interacts with all other patches in a 4D paradigm, while the gaze feature for following the specific person’s gaze must be a 2D feature map to predict the spatial distribution of gazing probability.',\n",
       " '2) Due to the rich interaction features being fully extracted by the ViT encoder, it is unnecessary to design a heavy branch to extract head features and gaze cones, which is widely adopted in the previous literatureRecasens_GazeFollow_2015_NIPS;Miao_PDP_2023_WACV;Tonini_GOT_2023_ICCV;Gupta_MM_2022_CVPR.',\n",
       " 'Therefore, we formulate the aggregation as a simple weighted sum of 4D interaction features in two spatial dimensions on the basis of the head position and head features.',\n",
       " 'The module is constructed with a simple two-layer multi-layer perceptron (MLP) with softmax activation, which is used to calculate the weights of each patch.',\n",
       " 'Background patches are masked before the softmax operation to guarantee that the weights are unique to the target individual.',\n",
       " 'Built upon this 2D spatial guidance, we proceed to feature aggregation with guidance.',\n",
       " 'We obtain the person-specific interaction featuresthrough a weighted sum of 4D interaction featureswith 2D guidanceas weights.',\n",
       " 'This can be expressed in the form of matrix multiplication as Eq.\\xa0(3).',\n",
       " 'The output interaction featuresrepresent multi-level person-specific interactions between the target person and each patch.',\n",
       " 'With this simple aggregation, we transfer the abundant interaction information in the whole image to the person-scene interaction corresponding to the specific person.',\n",
       " 'Unlike the interaction feature, the in-out prediction feature focuses more on the global semantics instead of the geometric comprehension of the image,',\n",
       " 'which is provided in the final ViT feature maps.',\n",
       " 'Therefore, we use image token features, guided by 2D guidance.',\n",
       " 'It is expressed by Eq.\\xa0(4),',\n",
       " 'whilerepresents the image token features from the last layer of ViT.',\n",
       " 'To further engage the module in feature extraction of heads, we introduce an auxiliary head that predicts whether a patch overlaps with the heads of any people.',\n",
       " 'The auxiliary head shares the stem with the main branch and introduces an extra prediction layer to predict the patch-level probability of overlapping with the head of a person to provide more supervision of head features.',\n",
       " 'Notably, in gaze-following settings, this auxiliary task does not require any supplementary input modalities such as depth, human poses, or object segmentation.',\n",
       " 'This component uses the person-specific interaction feature and the in-out prediction feature to predict a gaze target heatmapand a valueindicating the probability that the person watches outside the scene.',\n",
       " 'These two parts are detailed below.',\n",
       " 'Gaze heatmap head.This module employs three groups of bilinear interpolation and convolutional layers to predict the gaze location.',\n",
       " 'It converts person-specific interaction features to a logit map, where the highest activation point refers to the predicted gaze target.',\n",
       " 'In-out prediction head.We design an MLP head to predict whether the person’s gaze point is located in the image.',\n",
       " 'The head vector is fed into the two-layer MLP followed by sigmoid activation to obtain the probability that the person watches outside.',\n",
       " 'We train our model in an end-to-end paradigm with the training objective as a weighted sum of all tasks.',\n",
       " 'The loss consists of three parts: gaze heatmap loss, gaze in-out loss, and auxiliary head regression loss.',\n",
       " 'Gaze heatmap lossmeasures the error of gaze target prediction with a mean square error between the predicted gaze heatmap and the ground truth generated by a Gaussian blob centered at the coordinate of the gaze target.',\n",
       " 'Gaze in-out lossmeasures the prediction error of whether the person watches outside.',\n",
       " 'It is achieved by a focal lossfocal_lossbetween the predicted probability and the ground truth label.',\n",
       " 'Auxiliary head regression lossis designed to constrain the 2D spatial guidance.',\n",
       " 'It is defined as the binary cross-entropy loss between the predicted head occurrence and the ground truth heatmap which is a combination of Gaussian blobs centered at head bounding boxes in annotations.',\n",
       " 'The final loss is a linear combination of the three losses:',\n",
       " 'where,, andare weights of,, and.',\n",
       " 'Datasets.We train and test ViTGaze on the GazefollowRecasens_GazeFollow_2015_NIPSdataset and VideoAttentionTargetChong_VideoAttn_2020_CVPRdataset.',\n",
       " 'GazefollowRecasens_GazeFollow_2015_NIPScontains over 130 K annotations of people and their gaze targets in 122 K images.',\n",
       " 'The dataset is annotated with head bounding boxes, gaze points, and in-out-of-frame labels provided by Ref.Chong_Connect_2018_ECCV.',\n",
       " 'GazeFollow focuses on gaze targets inside the images, therefore only gaze heatmap regression is evaluated.',\n",
       " 'VideoAttentionTargetChong_VideoAttn_2020_CVPRcontains 1331 video clips with 165 K annotations.',\n",
       " 'We evaluate both gaze heatmap regression and watching-outside prediction.',\n",
       " 'Evaluation metrics.We adopt the following metrics to evaluate the performance of the proposed method.',\n",
       " 'AUC reflects the prediction confidence of gaze heatmaps.',\n",
       " 'Distance (Dist.) refers to the relative Euclidean distance between the ground truth and the predicted position with the highest confidence.',\n",
       " 'Since 10 annotations are provided in GazeFollow for each instance, we report both the minimum distance (Min. Dist.) and average distance (Avg. Dist.).',\n",
       " 'We use AP to evaluate the performance for watching-outside prediction in VideoAttentionTarget.',\n",
       " 'Model structure.We adopt ViT-Svitas the transformer encoder.',\n",
       " 'The encoder consists of 12 blocks with a multi-head self-attention of 6 heads.',\n",
       " 'The multi-level 4D interaction is constructed with attention maps from the 3-rd, 6-th, 9-th, and 12-th blocks.',\n",
       " 'We use an efficient implementation of multi-head attention which is available in the xFormersxFormers2022library.',\n",
       " 'Unbiased data processing.The coordinate encoding process (i.e. transforming ground-truth coordinates to heatmaps) used in current methods introduces quantization error of gaze targets and thus degrades model performance.',\n",
       " 'To address this dilemma, we follow DARKdarkposeto generate gaze heatmaps with a Gaussian kernel using the real gaze target as the center without quantization of the center coordinates.',\n",
       " 'We also adopt the post-processing method proposed in Ref.darkposeto infer the final gaze target location.',\n",
       " 'Model training.For GazeFollowRecasens_GazeFollow_2015_NIPS, we initialize the model with weights of DINOv2dinov2.',\n",
       " 'The model is trained for 15 epochs via the AdamWadamwoptimizer with a batch size of 48.',\n",
       " 'The initial learning rate is set as, and decays towith a cosine scheduler.',\n",
       " 'The weight decay is set as 0.1.',\n",
       " 'We follow DINOv2dinov2to increase the resolution of images toin the last epoch.',\n",
       " 'In addition to basic data augmentations, we follow DINOv2dinov2to apply random masks to the images.',\n",
       " 'Specifically, we replace patches in the background that occupy less than 50% of the area with a mask token with a probability of 0.5.',\n",
       " 'The loss coefficients are= 2 in the focal loss,= 100 for the heatmap regression,= 1 for the watching-outside prediction, and= 1 for auxiliary head regression.',\n",
       " 'For VideoAttentionTarget, following Refs.Fang_DAM_2021_CVPR;Gupta_MM_2022_CVPR, we finetune the model initialized with weights learned from GazeFollow.',\n",
       " 'We train for 1 epoch with the learning rate ofon VideoAttentionTarget.',\n",
       " 'The detailed training configurations are outlined in AppendixA.',\n",
       " 'We present the quantitative results for the GazeFollow and VideoAttentionTarget datasets in Tab.1.',\n",
       " 'To ensure a fair comparison, we also annotate the modalities used in each model.',\n",
       " 'Additionally, we include the parameter counts and floating point operations (FLOPs) of each model to facilitate the comparison of their efficiency, where the additional modality extractors required by multi-modality methods are not taken into account.',\n",
       " 'Accuracy.As demonstrated in Tab.1, we compare ViTGaze with previous multi-modality and single-modality methods.',\n",
       " 'ViTGaze achieves new SOTA performance among single-modality methods.',\n",
       " 'Compared with the previous SOTA methodTu_HGGTR_2022_CVPR, ViTGaze outperforms 3.4% in terms of the AUC in GazeFollow and 5.1% in terms of AP in VideoAttentionTarget.',\n",
       " 'This improvement demonstrates that ViTGaze is efficient in leveraging single-modality data to achieve better performance.',\n",
       " 'Additionally, our method shows performance that is comparable to multi-modality methods without extra input used during prediction.',\n",
       " 'For instance, compared with the previous SOTA multi-modality methodTonini_GOT_2023_ICCV, our method is only 1.8% lower in terms of the distance but achieves 2.7% higher AUC on the GazeFollow benchmark.',\n",
       " 'This indicates that even with fewer modalities, ViTGaze can match and even exceed the performance of complex multi-modality methods.',\n",
       " 'This highlights the efficiency and robustness of ViTGaze to extract and utilize interaction features from single-modality data.',\n",
       " 'Efficiency.As displayed in Tab.1, our method achieves the SOTA performance with only 22 M parameters and a computational cost of only 4.62 GFLOPs.',\n",
       " 'Even the computational overhead associated with the modality extraction stage required by multi-modality methods is not considered, our approach still achieves a reduction of over 50% in the number of parameters and over 25% in the computational demand in comparison to existing methods.',\n",
       " 'This result demonstrates that 4D patch-level interaction features are capable of extracting the appropriate clues for accurate gaze target detection.',\n",
       " 'Besides, our method is a novel architecture based mainly on encoders (relative decoder parameters less than 1%).',\n",
       " 'We conduct a comprehensive comparison of the parameter distribution in ViTGaze with those of existing SOTA methods as illustrated in Tab.2.',\n",
       " 'Specifically, we list the parameter distributions of both the SOTA single-modality (RGB-only) methodTu_HGGTR_2022_CVPRand the SOTA multi-modality methodTonini_GOT_2023_ICCV.',\n",
       " 'The top-performing RGB-only method employs a hybrid architecture as the encoder for feature extraction, and it adopts a decoder constituting 26% of the total parameter count for facilitating further information processing.',\n",
       " 'The most effective multi-modality method incorporates an additional object decoder constituting 18% of the total parameter count for extra object detection.',\n",
       " 'This, in turn, leads to a two-stage decoder structure that accounts for 40% of the total parameter count.',\n",
       " 'Our method harnesses the robust representational capabilities of pre-trained ViT, requiring only a prediction head that constitutes less than 1% of the total parameter count to accomplish gaze following.',\n",
       " 'Additionally, pre-training allows us to achieve more robust interaction information extraction with a transformer-based encoder that occupies only 70% of the parameter count compared with other methods.',\n",
       " 'This demonstrates that the complex and heavy design may not be necessary for gaze following.',\n",
       " 'In this section, we conduct experiments on the GazeFollowRecasens_GazeFollow_2015_NIPSbenchmark to validate the effectiveness of our proposed method.',\n",
       " 'All the experiments are conducted with ViT-smalldinov2as the backbone.',\n",
       " 'Multi-level 4D interaction features.To demonstrate the effectiveness of multi-level 4D interaction features (M-4D features) for gaze following, we conduct a study on the features used for gaze prediction.',\n",
       " 'We build a variant via 2D feature maps i.e. the final output of the ViT for prediction.',\n",
       " 'We also compare our method to a single-level variant (S-4D features) that uses only attention maps in the last block to verify the effect of multi-level relation fusion.',\n",
       " 'The detailed results of the study on GazeFollow are presented in Tab.3.',\n",
       " 'According to the experimental results, 4D interaction features are more effective in capturing person-scene relations, whereas 2D features are incapable of capturing patch interactions and cannot directly determine the gaze targets in an end-to-end training paradigm.',\n",
       " 'Furthermore, our multi-level approach outperforms its single-level counterpart, highlighting the significance of capturing relationships at different levels.',\n",
       " 'Single-level 4D interactions fail to provide adequate local representation and cannot predict precise gaze targets.',\n",
       " '2D spatial guidance.A comparative study is conducted to validate the significance of 2D spatial guidance, as illustrated in Tab.4.',\n",
       " 'Direct pooling of the interaction features in the head region achieves an acceptable result, indicating that it is unnecessary for the complex design of head feature extraction.',\n",
       " ...]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create paper chunks and Dataset\n",
    "\n",
    "paper_chunks = []\n",
    "\n",
    "for paper_path in emb_lib.paper_paths:\n",
    "    with open(paper_path, 'r', encoding='utf-8') as p:\n",
    "        paper = p.read()\n",
    "        title, chunks = emb_lib.preprocess_paper(paper)\n",
    "        paper_chunks.extend(chunks)\n",
    "        paper_chunks.append(title)\n",
    "\n",
    "dataset = Dataset.from_dict({\n",
    "    \"anchor\": paper_chunks,\n",
    "    \"positive\": paper_chunks,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a tokenizer\n",
    "\n",
    "batch_size = 1000\n",
    "all_texts = np.random.shuffle([dataset[i : i + batch_size][\"anchor\"] for i in range(0, len(dataset), batch_size)])\n",
    "\n",
    "def batch_iterator():\n",
    "    for i in range(0, len(dataset), batch_size):\n",
    "        yield dataset[i : i + batch_size][\"anchor\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.tokenizer = model.tokenizer.train_new_from_iterator(batch_iterator(), vocab_size=55000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set dropout in model\n",
    "\n",
    "from torch.nn.modules.dropout import Dropout\n",
    "\n",
    "def set_dropout(model, p):\n",
    "\n",
    "    def set_d(module):\n",
    "        for c in module.children():\n",
    "            if len(list(module.children())) == 0: return\n",
    "            if isinstance(c, Dropout):\n",
    "                c.p = p\n",
    "            set_d(c)\n",
    "\n",
    "    for m in model.modules():\n",
    "        set_d(m)\n",
    "\n",
    "set_dropout(model, 0.5)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the MultipleNegativesRankingLoss\n",
    "train_loss = losses.MultipleNegativesRankingLoss(model)\n",
    "\n",
    "args = SentenceTransformerTrainingArguments(\n",
    "    output_dir=f\"training/{model_name}\",\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=16,\n",
    "    #per_device_eval_batch_size=16,\n",
    "    warmup_ratio=0.1,\n",
    "    fp16=True,  # Set to False if your GPU can't handle FP16\n",
    "    bf16=False,  # Set to True if your GPU supports BF16\n",
    "    batch_sampler=BatchSamplers.NO_DUPLICATES,  # Losses using \"in-batch negatives\" benefit from no duplicates\n",
    "    # Optional tracking/debugging parameters:\n",
    "    # eval_strategy=\"steps\",\n",
    "    # eval_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=100,\n",
    "    run_name=f\"{model_name}\",  # Used in W&B if `wandb` is installed\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SentenceTransformerTrainer(\n",
    "    model = model,\n",
    "    args = args,\n",
    "    train_dataset = dataset,\n",
    "    loss = train_loss,\n",
    "    tokenizer = model.tokenizer,  \n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 34%|███▎      | 518/1535 [05:09<05:38,  3.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.454, 'grad_norm': 5.565038204193115, 'learning_rate': 3.14935064935065e-05, 'epoch': 0.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "                                                  \n",
      " 34%|███▎      | 518/1535 [06:17<05:38,  3.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0543, 'grad_norm': 3.9539802074432373, 'learning_rate': 4.844315713251267e-05, 'epoch': 0.65}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 34%|███▎      | 518/1535 [06:59<05:38,  3.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0377, 'grad_norm': 0.3020555377006531, 'learning_rate': 4.4822592324402606e-05, 'epoch': 0.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 34%|███▎      | 518/1535 [07:50<05:38,  3.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0253, 'grad_norm': 0.2598021626472473, 'learning_rate': 4.1202027516292545e-05, 'epoch': 1.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 34%|███▎      | 518/1535 [08:58<05:38,  3.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0087, 'grad_norm': 0.3064405024051666, 'learning_rate': 3.758146270818248e-05, 'epoch': 1.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 34%|███▎      | 518/1535 [09:40<05:38,  3.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0179, 'grad_norm': 0.1715710610151291, 'learning_rate': 3.3960897900072416e-05, 'epoch': 1.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 34%|███▎      | 518/1535 [10:30<05:38,  3.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0134, 'grad_norm': 0.22755272686481476, 'learning_rate': 3.034033309196235e-05, 'epoch': 2.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 34%|███▎      | 518/1535 [11:42<05:38,  3.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0056, 'grad_norm': 0.04436185956001282, 'learning_rate': 2.671976828385228e-05, 'epoch': 2.61}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 34%|███▎      | 518/1535 [12:24<05:38,  3.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0088, 'grad_norm': 4.521520614624023, 'learning_rate': 2.3099203475742217e-05, 'epoch': 2.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 34%|███▎      | 518/1535 [13:10<05:38,  3.01it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.012, 'grad_norm': 0.3633098006248474, 'learning_rate': 1.9478638667632152e-05, 'epoch': 3.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 34%|███▎      | 518/1535 [14:24<05:38,  3.01it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0034, 'grad_norm': 0.2215462327003479, 'learning_rate': 1.5858073859522085e-05, 'epoch': 3.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 34%|███▎      | 518/1535 [15:06<05:38,  3.01it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0068, 'grad_norm': 0.11929313838481903, 'learning_rate': 1.223750905141202e-05, 'epoch': 3.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 34%|███▎      | 518/1535 [15:52<05:38,  3.01it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0089, 'grad_norm': 0.25609907507896423, 'learning_rate': 8.616944243301955e-06, 'epoch': 4.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 34%|███▎      | 518/1535 [17:07<05:38,  3.01it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0036, 'grad_norm': 0.027053426951169968, 'learning_rate': 4.99637943519189e-06, 'epoch': 4.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 34%|███▎      | 518/1535 [17:48<05:38,  3.01it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0063, 'grad_norm': 1.005142092704773, 'learning_rate': 1.3758146270818247e-06, 'epoch': 4.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      "100%|██████████| 1535/1535 [13:45<00:00,  1.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 825.2674, 'train_samples_per_second': 29.706, 'train_steps_per_second': 1.86, 'train_loss': 0.04364390678048522, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1535, training_loss=0.04364390678048522, metrics={'train_runtime': 825.2674, 'train_samples_per_second': 29.706, 'train_steps_per_second': 1.86, 'total_flos': 0.0, 'train_loss': 0.04364390678048522, 'epoch': 5.0})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_weights = r'C:\\Users\\rooty\\UWEC\\Fall 24\\CS 426\\GroupProject\\GroupProject\\scholarly-search\\weights\\semsim'\n",
    "model.save(path_to_weights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GroupProject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
