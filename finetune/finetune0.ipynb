{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning SemSim Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rooty\\UWEC\\Fall 24\\CS 426\\GroupProject\\GroupProject\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\rooty\\UWEC\\Fall 24\\CS 426\\GroupProject\\GroupProject\\lib\\site-packages\\dask\\dataframe\\__init__.py:49: FutureWarning: \n",
      "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
      "\n",
      "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
      "This will raise in a future version.\n",
      "\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "c:\\Users\\rooty\\UWEC\\Fall 24\\CS 426\\GroupProject\\GroupProject\\lib\\site-packages\\numba\\np\\ufunc\\dufunc.py:343: NumbaWarning: \u001b[1mCompilation requested for previously compiled argument types ((uint32,)). This has no effect and perhaps indicates a bug in the calling code (compiling a ufunc more than once for the same signature\u001b[0m\n",
      "  warnings.warn(msg, errors.NumbaWarning)\n",
      "c:\\Users\\rooty\\UWEC\\Fall 24\\CS 426\\GroupProject\\GroupProject\\lib\\site-packages\\numba\\np\\ufunc\\dufunc.py:343: NumbaWarning: \u001b[1mCompilation requested for previously compiled argument types ((uint32,)). This has no effect and perhaps indicates a bug in the calling code (compiling a ufunc more than once for the same signature\u001b[0m\n",
      "  warnings.warn(msg, errors.NumbaWarning)\n",
      "c:\\Users\\rooty\\UWEC\\Fall 24\\CS 426\\GroupProject\\GroupProject\\lib\\site-packages\\numba\\np\\ufunc\\dufunc.py:343: NumbaWarning: \u001b[1mCompilation requested for previously compiled argument types ((uint32,)). This has no effect and perhaps indicates a bug in the calling code (compiling a ufunc more than once for the same signature\u001b[0m\n",
      "  warnings.warn(msg, errors.NumbaWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import umap    \n",
    "import umap.plot\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sentence_transformers.losses as losses\n",
    "from sentence_transformers import SentenceTransformer, SimilarityFunction\n",
    "from sentence_transformers.trainer import SentenceTransformerTrainer, SentenceTransformerTrainingArguments\n",
    "from sentence_transformers.training_args import BatchSamplers\n",
    "from datasets import Dataset\n",
    "import sys\n",
    "import os\n",
    "import logging\n",
    "import re\n",
    "\n",
    "SCRIPT_DIR = Path.cwd().parent\n",
    "sys.path.insert(0, str(SCRIPT_DIR))\n",
    "\n",
    "from embeddings.embedding_library import EmbeddingLibrary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init Model & Embedding Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_papers = Path(r'../data/paper_texts')\n",
    "path_to_embs = Path(r'../data/embeddings')\n",
    "\n",
    "# should all be lower case\n",
    "end_of_paper_words = [\n",
    "    'references',\n",
    "    'acknowledgement', \n",
    "    'appendix',\n",
    "]\n",
    "\n",
    "papers_to_skip = [\n",
    "    '2311.11329v2_content.txt',\n",
    "    '2411.09324v2_content.txt',\n",
    "    '2411.14259v1_content.txt',\n",
    "    '2309.01837v3_content.txt'\n",
    "    #'2403.12778v2_content.txt' \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'all-distilroberta-v1'\n",
    "device = 'cpu' #'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(model_name, device=device)\n",
    "\n",
    "# https://huggingface.co/sentence-transformers/all-distilroberta-v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.roberta.tokenization_roberta_fast.RobertaTokenizerFast"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RobertaTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_weights = r'C:\\Users\\rooty\\UWEC\\Fall 24\\CS 426\\GroupProject\\GroupProject\\scholarly-search\\weights\\all_distilroberta_v1'\n",
    "\n",
    "model = SentenceTransformer(path_to_weights, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_lib = EmbeddingLibrary(\n",
    "    path_to_papers=path_to_papers,\n",
    "    path_to_embs=path_to_embs,\n",
    "    model=model,\n",
    "    end_of_paper_words=end_of_paper_words,\n",
    "    papers_to_skip=papers_to_skip,\n",
    "    norm_embs=True,\n",
    "    name=model_name,\n",
    "    log_lvl=logging.INFO,\n",
    "    path_to_log=Path(\"../log\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 60.32it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:02<00:00,  1.31s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 65.62it/s]\n",
      "Batches: 100%|██████████| 6/6 [00:02<00:00,  2.11it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 46.31it/s]\n",
      "Batches: 100%|██████████| 4/4 [00:01<00:00,  2.68it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 70.86it/s]\n",
      "Batches: 100%|██████████| 7/7 [00:01<00:00,  3.97it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 73.26it/s]\n",
      "Batches: 100%|██████████| 3/3 [00:01<00:00,  1.94it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 55.14it/s]\n",
      "Batches: 100%|██████████| 8/8 [00:01<00:00,  4.88it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 63.92it/s]\n",
      "Batches: 100%|██████████| 6/6 [00:01<00:00,  3.84it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 57.09it/s]\n",
      "Batches: 100%|██████████| 8/8 [00:04<00:00,  1.79it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 46.25it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:02<00:00,  1.36s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 39.48it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:01<00:00,  1.29it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 55.48it/s]\n",
      "Batches: 100%|██████████| 9/9 [00:01<00:00,  6.41it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 55.79it/s]\n",
      "Batches: 100%|██████████| 4/4 [00:03<00:00,  1.13it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 63.90it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:02<00:00,  1.39s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 48.47it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  2.23it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 65.13it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:02<00:00,  1.16s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 32.41it/s]\n",
      "Batches: 100%|██████████| 3/3 [00:03<00:00,  1.06s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 56.95it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:01<00:00,  1.60it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 63.05it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:01<00:00,  1.14it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 73.47it/s]\n",
      "Batches: 100%|██████████| 3/3 [00:01<00:00,  1.77it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 54.22it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.59s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 54.26it/s]\n",
      "Batches: 100%|██████████| 3/3 [00:01<00:00,  1.66it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 66.45it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:02<00:00,  1.04s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 54.82it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:01<00:00,  1.35it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 55.45it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:01<00:00,  1.66it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 46.16it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:03<00:00,  1.51s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 62.24it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:02<00:00,  1.45s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 55.43it/s]\n",
      "Batches: 100%|██████████| 7/7 [00:02<00:00,  3.24it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 57.80it/s]\n",
      "Batches: 100%|██████████| 8/8 [00:02<00:00,  2.86it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 52.21it/s]\n",
      "Batches: 100%|██████████| 5/5 [00:00<00:00,  5.74it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 55.73it/s]\n",
      "Batches: 100%|██████████| 4/4 [00:03<00:00,  1.28it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 56.25it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:01<00:00,  1.49it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 53.23it/s]\n",
      "Batches: 100%|██████████| 3/3 [00:01<00:00,  1.51it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 59.29it/s]\n",
      "Batches: 100%|██████████| 4/4 [00:01<00:00,  2.97it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 56.78it/s]\n",
      "Batches: 100%|██████████| 7/7 [00:01<00:00,  5.53it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 59.63it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:01<00:00,  1.33it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 55.31it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.01s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 53.30it/s]\n",
      "Batches: 100%|██████████| 3/3 [00:02<00:00,  1.18it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 19.60it/s]\n",
      "Batches: 100%|██████████| 12/12 [00:01<00:00,  6.99it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 68.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.04s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 63.15it/s]\n",
      "Batches: 100%|██████████| 3/3 [00:01<00:00,  2.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 65.67it/s]\n",
      "Batches: 100%|██████████| 3/3 [00:01<00:00,  2.29it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 77.05it/s]\n",
      "Batches: 100%|██████████| 5/5 [00:03<00:00,  1.63it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 61.00it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.14s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 56.60it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:01<00:00,  1.42it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 54.17it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:01<00:00,  1.53it/s]\n"
     ]
    }
   ],
   "source": [
    "emb_lib.embed_papers(skip_existing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Embedding Lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 768)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.load(path_to_embs/'2411.14427v1.npy').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mapper = umap.UMAP().fit(emb_lib.paper_embs)\n",
    "p = umap.plot.points(mapper, labels=pd.Series([i for i in range(emb_lib.paper_embs.shape[0])]), show_legend=False)\n",
    "umap.plot.show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.3841, 0.3510,  ..., 0.5426, 0.3374, 0.3162],\n",
       "        [0.3841, 1.0000, 0.3399,  ..., 0.3466, 0.4973, 0.3639],\n",
       "        [0.3510, 0.3399, 1.0000,  ..., 0.6894, 0.2107, 0.3106],\n",
       "        ...,\n",
       "        [0.5426, 0.3466, 0.6894,  ..., 1.0000, 0.3382, 0.3323],\n",
       "        [0.3374, 0.4973, 0.2107,  ..., 0.3382, 1.0000, 0.2821],\n",
       "        [0.3162, 0.3639, 0.3106,  ..., 0.3323, 0.2821, 1.0000]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity(emb_lib.paper_embs, emb_lib.paper_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 16.05it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['2311.00207v3',\n",
       " '2407.05941v4',\n",
       " '2411.13037v1',\n",
       " '2406.08298v5',\n",
       " '2411.09101v1']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Paper: 2311.00207v3\n",
    "\n",
    "prompt = \"\"\n",
    "response = '''SECTION: IIntroduction\n",
    "\n",
    "Next-generation (NextG) networks promise to support ultra-reliable and low-latency communication for rapidly evolving wireless devices[29]. Emerging networks are thus challenged to establish new features (e.g., adaptive coding and enhanced modulation) to overcome rapidly changing channel conditions and to achieve more efficient use of spectrum[102,63].\n",
    "Machine Learning (ML) overcomes this barrier by revolutionizing the entire wireless network protocol stack[67].'''\n",
    "\n",
    "emb_lib.search_papers(prompt, response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.22it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['2411.06091v1',\n",
       " '2411.14354v1',\n",
       " '2411.09420v1',\n",
       " '2411.09101v1',\n",
       " '2308.07279v2']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from 2411.06091\n",
    "\n",
    "prompt = \"\"\n",
    "response = '''Remote sensing technology, employing aerial or satellite platforms, facilitates the acquisition of earth observation data from afar, marking its utility across diverse sectors such as agriculture monitoring, environmental surveillance evaluation, urban planning, and disaster management. At the heart of these applications lies the interpretation of remote sensing imagery, a task that has been considerably challenging[1]. The advent of deep learning technology[2]has significantly enhanced the precision and automation of remote sensing image analysis, encompassing object detection, land cover classification, and change detection. Nonetheless, the intrinsic complexity of remote sensing scenarios—attributable to variations in sensor technology, atmospheric conditions, and image resolution—results in the performance disparity of analytical models across different contexts. The prohibitive costs associated with manual annotation further exacerbate this issue, rendering the re-labeling of samples for specific scenarios impractical. Consequently, there is a growing demand for models endowed with superior generalization capabilities, capable of adeptly navigating the multifaceted landscape of remote sensing applications.\n",
    "\n",
    "Seeking a method that discerns both the semantic information of different scene images and the semantic details within each area of the images is essential for effectively processing scenographic remote sensing imagery. It has been observed that scenographic images in remote sensing typically exhibit specific distribution patterns, where natural elements (such as mountains, bodies of water, forests, lakes, and grasslands) and man-made elements (such as buildings, farmlands, plowed lands, and gardens) often display pronounced clustering. This tendency for feature aggregation leads to the formation of discernible geographic pattern clusters among individual image patches and their neighbors, thereby establishing intricate spatial relationships across the varying patches and their adjacent areas.'''\n",
    "\n",
    "emb_lib.search_papers(prompt, response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**These results are sub-par**\n",
    "\n",
    "### Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_lib.update_paper_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create paper chunks and Dataset\n",
    "\n",
    "paper_chunks = []\n",
    "\n",
    "for paper_path in emb_lib.paper_paths:\n",
    "    with open(paper_path, 'r', encoding='utf-8') as p:\n",
    "        paper = p.read()\n",
    "        title, chunks = emb_lib.preprocess_paper(paper)\n",
    "        paper_chunks.extend(chunks)\n",
    "        paper_chunks.append(title)\n",
    "\n",
    "dataset = Dataset.from_dict({\n",
    "    \"anchor\": paper_chunks,\n",
    "    \"positive\": paper_chunks,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a tokenizer\n",
    "\n",
    "batch_size = 1000\n",
    "all_texts = np.random.shuffle([dataset[i : i + batch_size][\"anchor\"] for i in range(0, len(dataset), batch_size)])\n",
    "\n",
    "def batch_iterator():\n",
    "    for i in range(0, len(dataset), batch_size):\n",
    "        yield dataset[i : i + batch_size][\"anchor\"]\n",
    "\n",
    "model.tokenizer = model.tokenizer.train_new_from_iterator(batch_iterator(), vocab_size=60000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "317"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(Path('../data/paper_texts').iterdir()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set dropout in model\n",
    "\n",
    "from torch.nn.modules.dropout import Dropout\n",
    "\n",
    "def set_dropout(model, p):\n",
    "\n",
    "    def set_d(module):\n",
    "        for c in module.children():\n",
    "            if len(list(module.children())) == 0: return\n",
    "            if isinstance(c, Dropout):\n",
    "                c.p = p\n",
    "            set_d(c)\n",
    "\n",
    "    for m in model.modules():\n",
    "        set_d(m)\n",
    "\n",
    "set_dropout(model, 0.5)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SentenceTransformer(\n",
       "   (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: RobertaModel \n",
       "   (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       "   (2): Normalize()\n",
       " ),\n",
       " Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: RobertaModel ,\n",
       " RobertaModel(\n",
       "   (embeddings): RobertaEmbeddings(\n",
       "     (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "     (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "     (token_type_embeddings): Embedding(1, 768)\n",
       "     (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "     (dropout): Dropout(p=0.5, inplace=False)\n",
       "   )\n",
       "   (encoder): RobertaEncoder(\n",
       "     (layer): ModuleList(\n",
       "       (0-5): 6 x RobertaLayer(\n",
       "         (attention): RobertaAttention(\n",
       "           (self): RobertaSdpaSelfAttention(\n",
       "             (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "             (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "             (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "             (dropout): Dropout(p=0.5, inplace=False)\n",
       "           )\n",
       "           (output): RobertaSelfOutput(\n",
       "             (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "             (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "             (dropout): Dropout(p=0.5, inplace=False)\n",
       "           )\n",
       "         )\n",
       "         (intermediate): RobertaIntermediate(\n",
       "           (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "           (intermediate_act_fn): GELUActivation()\n",
       "         )\n",
       "         (output): RobertaOutput(\n",
       "           (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "           (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "           (dropout): Dropout(p=0.5, inplace=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (pooler): RobertaPooler(\n",
       "     (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "     (activation): Tanh()\n",
       "   )\n",
       " ),\n",
       " RobertaEmbeddings(\n",
       "   (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "   (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "   (token_type_embeddings): Embedding(1, 768)\n",
       "   (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "   (dropout): Dropout(p=0.5, inplace=False)\n",
       " ),\n",
       " Embedding(50265, 768, padding_idx=1),\n",
       " Embedding(514, 768, padding_idx=1),\n",
       " Embedding(1, 768),\n",
       " LayerNorm((768,), eps=1e-05, elementwise_affine=True),\n",
       " Dropout(p=0.5, inplace=False),\n",
       " RobertaEncoder(\n",
       "   (layer): ModuleList(\n",
       "     (0-5): 6 x RobertaLayer(\n",
       "       (attention): RobertaAttention(\n",
       "         (self): RobertaSdpaSelfAttention(\n",
       "           (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "           (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "           (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "           (dropout): Dropout(p=0.5, inplace=False)\n",
       "         )\n",
       "         (output): RobertaSelfOutput(\n",
       "           (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "           (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "           (dropout): Dropout(p=0.5, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (intermediate): RobertaIntermediate(\n",
       "         (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "         (intermediate_act_fn): GELUActivation()\n",
       "       )\n",
       "       (output): RobertaOutput(\n",
       "         (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "         (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "         (dropout): Dropout(p=0.5, inplace=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " ModuleList(\n",
       "   (0-5): 6 x RobertaLayer(\n",
       "     (attention): RobertaAttention(\n",
       "       (self): RobertaSdpaSelfAttention(\n",
       "         (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "         (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "         (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "         (dropout): Dropout(p=0.5, inplace=False)\n",
       "       )\n",
       "       (output): RobertaSelfOutput(\n",
       "         (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "         (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "         (dropout): Dropout(p=0.5, inplace=False)\n",
       "       )\n",
       "     )\n",
       "     (intermediate): RobertaIntermediate(\n",
       "       (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "       (intermediate_act_fn): GELUActivation()\n",
       "     )\n",
       "     (output): RobertaOutput(\n",
       "       (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "       (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "       (dropout): Dropout(p=0.5, inplace=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " RobertaLayer(\n",
       "   (attention): RobertaAttention(\n",
       "     (self): RobertaSdpaSelfAttention(\n",
       "       (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (dropout): Dropout(p=0.5, inplace=False)\n",
       "     )\n",
       "     (output): RobertaSelfOutput(\n",
       "       (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "       (dropout): Dropout(p=0.5, inplace=False)\n",
       "     )\n",
       "   )\n",
       "   (intermediate): RobertaIntermediate(\n",
       "     (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "     (intermediate_act_fn): GELUActivation()\n",
       "   )\n",
       "   (output): RobertaOutput(\n",
       "     (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "     (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "     (dropout): Dropout(p=0.5, inplace=False)\n",
       "   )\n",
       " ),\n",
       " RobertaAttention(\n",
       "   (self): RobertaSdpaSelfAttention(\n",
       "     (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "     (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "     (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "     (dropout): Dropout(p=0.5, inplace=False)\n",
       "   )\n",
       "   (output): RobertaSelfOutput(\n",
       "     (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "     (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "     (dropout): Dropout(p=0.5, inplace=False)\n",
       "   )\n",
       " ),\n",
       " RobertaSdpaSelfAttention(\n",
       "   (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "   (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "   (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "   (dropout): Dropout(p=0.5, inplace=False)\n",
       " ),\n",
       " Linear(in_features=768, out_features=768, bias=True),\n",
       " Linear(in_features=768, out_features=768, bias=True),\n",
       " Linear(in_features=768, out_features=768, bias=True),\n",
       " Dropout(p=0.5, inplace=False),\n",
       " RobertaSelfOutput(\n",
       "   (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "   (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "   (dropout): Dropout(p=0.5, inplace=False)\n",
       " ),\n",
       " Linear(in_features=768, out_features=768, bias=True),\n",
       " LayerNorm((768,), eps=1e-05, elementwise_affine=True),\n",
       " Dropout(p=0.5, inplace=False),\n",
       " RobertaIntermediate(\n",
       "   (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "   (intermediate_act_fn): GELUActivation()\n",
       " ),\n",
       " Linear(in_features=768, out_features=3072, bias=True),\n",
       " GELUActivation(),\n",
       " RobertaOutput(\n",
       "   (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "   (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "   (dropout): Dropout(p=0.5, inplace=False)\n",
       " ),\n",
       " Linear(in_features=3072, out_features=768, bias=True),\n",
       " LayerNorm((768,), eps=1e-05, elementwise_affine=True),\n",
       " Dropout(p=0.5, inplace=False),\n",
       " RobertaLayer(\n",
       "   (attention): RobertaAttention(\n",
       "     (self): RobertaSdpaSelfAttention(\n",
       "       (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (dropout): Dropout(p=0.5, inplace=False)\n",
       "     )\n",
       "     (output): RobertaSelfOutput(\n",
       "       (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "       (dropout): Dropout(p=0.5, inplace=False)\n",
       "     )\n",
       "   )\n",
       "   (intermediate): RobertaIntermediate(\n",
       "     (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "     (intermediate_act_fn): GELUActivation()\n",
       "   )\n",
       "   (output): RobertaOutput(\n",
       "     (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "     (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "     (dropout): Dropout(p=0.5, inplace=False)\n",
       "   )\n",
       " ),\n",
       " RobertaAttention(\n",
       "   (self): RobertaSdpaSelfAttention(\n",
       "     (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "     (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "     (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "     (dropout): Dropout(p=0.5, inplace=False)\n",
       "   )\n",
       "   (output): RobertaSelfOutput(\n",
       "     (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "     (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "     (dropout): Dropout(p=0.5, inplace=False)\n",
       "   )\n",
       " ),\n",
       " RobertaSdpaSelfAttention(\n",
       "   (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "   (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "   (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "   (dropout): Dropout(p=0.5, inplace=False)\n",
       " ),\n",
       " Linear(in_features=768, out_features=768, bias=True),\n",
       " Linear(in_features=768, out_features=768, bias=True),\n",
       " Linear(in_features=768, out_features=768, bias=True),\n",
       " Dropout(p=0.5, inplace=False),\n",
       " RobertaSelfOutput(\n",
       "   (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "   (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "   (dropout): Dropout(p=0.5, inplace=False)\n",
       " ),\n",
       " Linear(in_features=768, out_features=768, bias=True),\n",
       " LayerNorm((768,), eps=1e-05, elementwise_affine=True),\n",
       " Dropout(p=0.5, inplace=False),\n",
       " RobertaIntermediate(\n",
       "   (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "   (intermediate_act_fn): GELUActivation()\n",
       " ),\n",
       " Linear(in_features=768, out_features=3072, bias=True),\n",
       " GELUActivation(),\n",
       " RobertaOutput(\n",
       "   (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "   (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "   (dropout): Dropout(p=0.5, inplace=False)\n",
       " ),\n",
       " Linear(in_features=3072, out_features=768, bias=True),\n",
       " LayerNorm((768,), eps=1e-05, elementwise_affine=True),\n",
       " Dropout(p=0.5, inplace=False),\n",
       " RobertaLayer(\n",
       "   (attention): RobertaAttention(\n",
       "     (self): RobertaSdpaSelfAttention(\n",
       "       (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (dropout): Dropout(p=0.5, inplace=False)\n",
       "     )\n",
       "     (output): RobertaSelfOutput(\n",
       "       (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "       (dropout): Dropout(p=0.5, inplace=False)\n",
       "     )\n",
       "   )\n",
       "   (intermediate): RobertaIntermediate(\n",
       "     (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "     (intermediate_act_fn): GELUActivation()\n",
       "   )\n",
       "   (output): RobertaOutput(\n",
       "     (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "     (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "     (dropout): Dropout(p=0.5, inplace=False)\n",
       "   )\n",
       " ),\n",
       " RobertaAttention(\n",
       "   (self): RobertaSdpaSelfAttention(\n",
       "     (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "     (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "     (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "     (dropout): Dropout(p=0.5, inplace=False)\n",
       "   )\n",
       "   (output): RobertaSelfOutput(\n",
       "     (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "     (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "     (dropout): Dropout(p=0.5, inplace=False)\n",
       "   )\n",
       " ),\n",
       " RobertaSdpaSelfAttention(\n",
       "   (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "   (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "   (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "   (dropout): Dropout(p=0.5, inplace=False)\n",
       " ),\n",
       " Linear(in_features=768, out_features=768, bias=True),\n",
       " Linear(in_features=768, out_features=768, bias=True),\n",
       " Linear(in_features=768, out_features=768, bias=True),\n",
       " Dropout(p=0.5, inplace=False),\n",
       " RobertaSelfOutput(\n",
       "   (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "   (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "   (dropout): Dropout(p=0.5, inplace=False)\n",
       " ),\n",
       " Linear(in_features=768, out_features=768, bias=True),\n",
       " LayerNorm((768,), eps=1e-05, elementwise_affine=True),\n",
       " Dropout(p=0.5, inplace=False),\n",
       " RobertaIntermediate(\n",
       "   (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "   (intermediate_act_fn): GELUActivation()\n",
       " ),\n",
       " Linear(in_features=768, out_features=3072, bias=True),\n",
       " GELUActivation(),\n",
       " RobertaOutput(\n",
       "   (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "   (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "   (dropout): Dropout(p=0.5, inplace=False)\n",
       " ),\n",
       " Linear(in_features=3072, out_features=768, bias=True),\n",
       " LayerNorm((768,), eps=1e-05, elementwise_affine=True),\n",
       " Dropout(p=0.5, inplace=False),\n",
       " RobertaLayer(\n",
       "   (attention): RobertaAttention(\n",
       "     (self): RobertaSdpaSelfAttention(\n",
       "       (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (dropout): Dropout(p=0.5, inplace=False)\n",
       "     )\n",
       "     (output): RobertaSelfOutput(\n",
       "       (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "       (dropout): Dropout(p=0.5, inplace=False)\n",
       "     )\n",
       "   )\n",
       "   (intermediate): RobertaIntermediate(\n",
       "     (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "     (intermediate_act_fn): GELUActivation()\n",
       "   )\n",
       "   (output): RobertaOutput(\n",
       "     (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "     (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "     (dropout): Dropout(p=0.5, inplace=False)\n",
       "   )\n",
       " ),\n",
       " RobertaAttention(\n",
       "   (self): RobertaSdpaSelfAttention(\n",
       "     (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "     (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "     (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "     (dropout): Dropout(p=0.5, inplace=False)\n",
       "   )\n",
       "   (output): RobertaSelfOutput(\n",
       "     (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "     (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "     (dropout): Dropout(p=0.5, inplace=False)\n",
       "   )\n",
       " ),\n",
       " RobertaSdpaSelfAttention(\n",
       "   (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "   (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "   (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "   (dropout): Dropout(p=0.5, inplace=False)\n",
       " ),\n",
       " Linear(in_features=768, out_features=768, bias=True),\n",
       " Linear(in_features=768, out_features=768, bias=True),\n",
       " Linear(in_features=768, out_features=768, bias=True),\n",
       " Dropout(p=0.5, inplace=False),\n",
       " RobertaSelfOutput(\n",
       "   (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "   (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "   (dropout): Dropout(p=0.5, inplace=False)\n",
       " ),\n",
       " Linear(in_features=768, out_features=768, bias=True),\n",
       " LayerNorm((768,), eps=1e-05, elementwise_affine=True),\n",
       " Dropout(p=0.5, inplace=False),\n",
       " RobertaIntermediate(\n",
       "   (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "   (intermediate_act_fn): GELUActivation()\n",
       " ),\n",
       " Linear(in_features=768, out_features=3072, bias=True),\n",
       " GELUActivation(),\n",
       " RobertaOutput(\n",
       "   (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "   (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "   (dropout): Dropout(p=0.5, inplace=False)\n",
       " ),\n",
       " Linear(in_features=3072, out_features=768, bias=True),\n",
       " LayerNorm((768,), eps=1e-05, elementwise_affine=True),\n",
       " Dropout(p=0.5, inplace=False),\n",
       " RobertaLayer(\n",
       "   (attention): RobertaAttention(\n",
       "     (self): RobertaSdpaSelfAttention(\n",
       "       (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (dropout): Dropout(p=0.5, inplace=False)\n",
       "     )\n",
       "     (output): RobertaSelfOutput(\n",
       "       (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "       (dropout): Dropout(p=0.5, inplace=False)\n",
       "     )\n",
       "   )\n",
       "   (intermediate): RobertaIntermediate(\n",
       "     (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "     (intermediate_act_fn): GELUActivation()\n",
       "   )\n",
       "   (output): RobertaOutput(\n",
       "     (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "     (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "     (dropout): Dropout(p=0.5, inplace=False)\n",
       "   )\n",
       " ),\n",
       " RobertaAttention(\n",
       "   (self): RobertaSdpaSelfAttention(\n",
       "     (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "     (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "     (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "     (dropout): Dropout(p=0.5, inplace=False)\n",
       "   )\n",
       "   (output): RobertaSelfOutput(\n",
       "     (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "     (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "     (dropout): Dropout(p=0.5, inplace=False)\n",
       "   )\n",
       " ),\n",
       " RobertaSdpaSelfAttention(\n",
       "   (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "   (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "   (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "   (dropout): Dropout(p=0.5, inplace=False)\n",
       " ),\n",
       " Linear(in_features=768, out_features=768, bias=True),\n",
       " Linear(in_features=768, out_features=768, bias=True),\n",
       " Linear(in_features=768, out_features=768, bias=True),\n",
       " Dropout(p=0.5, inplace=False),\n",
       " RobertaSelfOutput(\n",
       "   (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "   (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "   (dropout): Dropout(p=0.5, inplace=False)\n",
       " ),\n",
       " Linear(in_features=768, out_features=768, bias=True),\n",
       " LayerNorm((768,), eps=1e-05, elementwise_affine=True),\n",
       " Dropout(p=0.5, inplace=False),\n",
       " RobertaIntermediate(\n",
       "   (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "   (intermediate_act_fn): GELUActivation()\n",
       " ),\n",
       " Linear(in_features=768, out_features=3072, bias=True),\n",
       " GELUActivation(),\n",
       " RobertaOutput(\n",
       "   (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "   (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "   (dropout): Dropout(p=0.5, inplace=False)\n",
       " ),\n",
       " Linear(in_features=3072, out_features=768, bias=True),\n",
       " LayerNorm((768,), eps=1e-05, elementwise_affine=True),\n",
       " Dropout(p=0.5, inplace=False),\n",
       " RobertaLayer(\n",
       "   (attention): RobertaAttention(\n",
       "     (self): RobertaSdpaSelfAttention(\n",
       "       (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (dropout): Dropout(p=0.5, inplace=False)\n",
       "     )\n",
       "     (output): RobertaSelfOutput(\n",
       "       (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "       (dropout): Dropout(p=0.5, inplace=False)\n",
       "     )\n",
       "   )\n",
       "   (intermediate): RobertaIntermediate(\n",
       "     (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "     (intermediate_act_fn): GELUActivation()\n",
       "   )\n",
       "   (output): RobertaOutput(\n",
       "     (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "     (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "     (dropout): Dropout(p=0.5, inplace=False)\n",
       "   )\n",
       " ),\n",
       " RobertaAttention(\n",
       "   (self): RobertaSdpaSelfAttention(\n",
       "     (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "     (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "     (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "     (dropout): Dropout(p=0.5, inplace=False)\n",
       "   )\n",
       "   (output): RobertaSelfOutput(\n",
       "     (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "     (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "     (dropout): Dropout(p=0.5, inplace=False)\n",
       "   )\n",
       " ),\n",
       " RobertaSdpaSelfAttention(\n",
       "   (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "   (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "   (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "   (dropout): Dropout(p=0.5, inplace=False)\n",
       " ),\n",
       " Linear(in_features=768, out_features=768, bias=True),\n",
       " Linear(in_features=768, out_features=768, bias=True),\n",
       " Linear(in_features=768, out_features=768, bias=True),\n",
       " Dropout(p=0.5, inplace=False),\n",
       " RobertaSelfOutput(\n",
       "   (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "   (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "   (dropout): Dropout(p=0.5, inplace=False)\n",
       " ),\n",
       " Linear(in_features=768, out_features=768, bias=True),\n",
       " LayerNorm((768,), eps=1e-05, elementwise_affine=True),\n",
       " Dropout(p=0.5, inplace=False),\n",
       " RobertaIntermediate(\n",
       "   (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "   (intermediate_act_fn): GELUActivation()\n",
       " ),\n",
       " Linear(in_features=768, out_features=3072, bias=True),\n",
       " GELUActivation(),\n",
       " RobertaOutput(\n",
       "   (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "   (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "   (dropout): Dropout(p=0.5, inplace=False)\n",
       " ),\n",
       " Linear(in_features=3072, out_features=768, bias=True),\n",
       " LayerNorm((768,), eps=1e-05, elementwise_affine=True),\n",
       " Dropout(p=0.5, inplace=False),\n",
       " RobertaPooler(\n",
       "   (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "   (activation): Tanh()\n",
       " ),\n",
       " Linear(in_features=768, out_features=768, bias=True),\n",
       " Tanh(),\n",
       " Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True}),\n",
       " Normalize()]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.modules())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "loss = nn.KLDivLoss(reduction='batchmean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[0.5, 1, 2], [1, 0, 0.2]], dtype=torch.float32)\n",
    "b = torch.tensor([[1, 0, 0], [1, 0, 0]], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, std = a.mean(0), a.std(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3536, 0.7071, 1.2728])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.7500, 0.5000, 1.1000]), tensor([0.6464, 0.2929, 0.2728]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(mean - 0).abs(), (std - 1).abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = F.log_softmax(a, dim=1)\n",
    "b = F.softmax(b, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: RobertaModel \n",
       "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       "  (2): Normalize()\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = losses.MultipleNegativesRankingLoss(model, kl_factor=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "args = SentenceTransformerTrainingArguments(\n",
    "    output_dir=f\"training/{model_name}\",\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=64,\n",
    "    #per_device_eval_batch_size=16,\n",
    "    warmup_ratio=0.1,\n",
    "    fp16=True,  # Set to False if your GPU can't handle FP16\n",
    "    bf16=False,  # Set to True if your GPU supports BF16\n",
    "    batch_sampler=BatchSamplers.NO_DUPLICATES,  # Losses using \"in-batch negatives\" benefit from no duplicates\n",
    "    # Optional tracking/debugging parameters:\n",
    "    # eval_strategy=\"steps\",\n",
    "    # eval_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=100,\n",
    "    run_name=f\"{model_name}\",  # Used in W&B if `wandb` is installed\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rooty\\UWEC\\Fall 24\\CS 426\\GroupProject\\GroupProject\\lib\\site-packages\\torch\\nn\\modules\\module.py:1118\u001b[0m, in \u001b[0;36mModule.cpu\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcpu\u001b[39m(\u001b[38;5;28mself\u001b[39m: T) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[0;32m   1110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Move all model parameters and buffers to the CPU.\u001b[39;00m\n\u001b[0;32m   1111\u001b[0m \n\u001b[0;32m   1112\u001b[0m \u001b[38;5;124;03m    .. note::\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1116\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[0;32m   1117\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rooty\\UWEC\\Fall 24\\CS 426\\GroupProject\\GroupProject\\lib\\site-packages\\torch\\nn\\modules\\module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rooty\\UWEC\\Fall 24\\CS 426\\GroupProject\\GroupProject\\lib\\site-packages\\torch\\nn\\modules\\module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[1;31m[... skipping similar frames: Module._apply at line 900 (1 times)]\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\rooty\\UWEC\\Fall 24\\CS 426\\GroupProject\\GroupProject\\lib\\site-packages\\torch\\nn\\modules\\module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rooty\\UWEC\\Fall 24\\CS 426\\GroupProject\\GroupProject\\lib\\site-packages\\torch\\nn\\modules\\module.py:927\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    923\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    924\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    925\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    926\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 927\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    928\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    930\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rooty\\UWEC\\Fall 24\\CS 426\\GroupProject\\GroupProject\\lib\\site-packages\\torch\\nn\\modules\\module.py:1118\u001b[0m, in \u001b[0;36mModule.cpu.<locals>.<lambda>\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcpu\u001b[39m(\u001b[38;5;28mself\u001b[39m: T) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[0;32m   1110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Move all model parameters and buffers to the CPU.\u001b[39;00m\n\u001b[0;32m   1111\u001b[0m \n\u001b[0;32m   1112\u001b[0m \u001b[38;5;124;03m    .. note::\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1116\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[0;32m   1117\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(\u001b[38;5;28;01mlambda\u001b[39;00m t: \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2980 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 8\u001b[0m\n\u001b[0;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m SentenceTransformerTrainer(\n\u001b[0;32m      2\u001b[0m     model \u001b[38;5;241m=\u001b[39m model,\n\u001b[0;32m      3\u001b[0m     args \u001b[38;5;241m=\u001b[39m args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      6\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtokenizer,  \n\u001b[0;32m      7\u001b[0m )\n\u001b[1;32m----> 8\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rooty\\UWEC\\Fall 24\\CS 426\\GroupProject\\GroupProject\\lib\\site-packages\\transformers\\trainer.py:2123\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2121\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2124\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rooty\\UWEC\\Fall 24\\CS 426\\GroupProject\\GroupProject\\lib\\site-packages\\transformers\\trainer.py:2481\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2475\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2476\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m   2477\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2478\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[0;32m   2479\u001b[0m )\n\u001b[0;32m   2480\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m-> 2481\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2483\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2484\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2485\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2486\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2487\u001b[0m ):\n\u001b[0;32m   2488\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2489\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\rooty\\UWEC\\Fall 24\\CS 426\\GroupProject\\GroupProject\\lib\\site-packages\\transformers\\trainer.py:3579\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[0;32m   3576\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   3578\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 3579\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3581\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[0;32m   3582\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   3583\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   3584\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   3585\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\rooty\\UWEC\\Fall 24\\CS 426\\GroupProject\\GroupProject\\lib\\site-packages\\sentence_transformers\\trainer.py:393\u001b[0m, in \u001b[0;36mSentenceTransformerTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    387\u001b[0m     model \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m model \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel  \u001b[38;5;66;03m# Only if the model is wrapped\u001b[39;00m\n\u001b[0;32m    389\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(loss_fn, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Only if the loss stores the model\u001b[39;00m\n\u001b[0;32m    390\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m loss_fn\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m!=\u001b[39m model  \u001b[38;5;66;03m# Only if the wrapped model is not already stored\u001b[39;00m\n\u001b[0;32m    391\u001b[0m ):\n\u001b[0;32m    392\u001b[0m     loss_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moverride_model_in_loss(loss_fn, model)\n\u001b[1;32m--> 393\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_outputs:\n\u001b[0;32m    395\u001b[0m     \u001b[38;5;66;03m# During prediction/evaluation, `compute_loss` will be called with `return_outputs=True`.\u001b[39;00m\n\u001b[0;32m    396\u001b[0m     \u001b[38;5;66;03m# However, Sentence Transformer losses do not return outputs, so we return an empty dictionary.\u001b[39;00m\n\u001b[0;32m    397\u001b[0m     \u001b[38;5;66;03m# This does not result in any problems, as the SentenceTransformerTrainingArguments sets\u001b[39;00m\n\u001b[0;32m    398\u001b[0m     \u001b[38;5;66;03m# `prediction_loss_only=True` which means that the output is not used.\u001b[39;00m\n\u001b[0;32m    399\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss, {}\n",
      "File \u001b[1;32mc:\\Users\\rooty\\UWEC\\Fall 24\\CS 426\\GroupProject\\GroupProject\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\rooty\\UWEC\\Fall 24\\CS 426\\GroupProject\\GroupProject\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\rooty\\UWEC\\Fall 24\\CS 426\\GroupProject\\GroupProject\\lib\\site-packages\\sentence_transformers\\losses\\MultipleNegativesRankingLoss.py:103\u001b[0m, in \u001b[0;36mMultipleNegativesRankingLoss.forward\u001b[1;34m(self, sentence_features, labels)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, sentence_features: Iterable[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Tensor]], labels: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 103\u001b[0m     reps \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(sentence_feature)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentence_embedding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m sentence_feature \u001b[38;5;129;01min\u001b[39;00m sentence_features]\n\u001b[0;32m    104\u001b[0m     embeddings_a \u001b[38;5;241m=\u001b[39m reps[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    105\u001b[0m     embeddings_b \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(reps[\u001b[38;5;241m1\u001b[39m:])\n",
      "File \u001b[1;32mc:\\Users\\rooty\\UWEC\\Fall 24\\CS 426\\GroupProject\\GroupProject\\lib\\site-packages\\sentence_transformers\\losses\\MultipleNegativesRankingLoss.py:103\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, sentence_features: Iterable[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Tensor]], labels: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 103\u001b[0m     reps \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence_feature\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentence_embedding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m sentence_feature \u001b[38;5;129;01min\u001b[39;00m sentence_features]\n\u001b[0;32m    104\u001b[0m     embeddings_a \u001b[38;5;241m=\u001b[39m reps[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    105\u001b[0m     embeddings_b \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(reps[\u001b[38;5;241m1\u001b[39m:])\n",
      "File \u001b[1;32mc:\\Users\\rooty\\UWEC\\Fall 24\\CS 426\\GroupProject\\GroupProject\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\rooty\\UWEC\\Fall 24\\CS 426\\GroupProject\\GroupProject\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\rooty\\UWEC\\Fall 24\\CS 426\\GroupProject\\GroupProject\\lib\\site-packages\\accelerate\\utils\\operations.py:823\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\rooty\\UWEC\\Fall 24\\CS 426\\GroupProject\\GroupProject\\lib\\site-packages\\accelerate\\utils\\operations.py:811\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 811\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n",
      "File \u001b[1;32mc:\\Users\\rooty\\UWEC\\Fall 24\\CS 426\\GroupProject\\GroupProject\\lib\\site-packages\\torch\\amp\\autocast_mode.py:44\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[1;32m---> 44\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\rooty\\UWEC\\Fall 24\\CS 426\\GroupProject\\GroupProject\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:690\u001b[0m, in \u001b[0;36mSentenceTransformer.forward\u001b[1;34m(self, input, **kwargs)\u001b[0m\n\u001b[0;32m    688\u001b[0m     module_kwarg_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_kwargs\u001b[38;5;241m.\u001b[39mget(module_name, [])\n\u001b[0;32m    689\u001b[0m     module_kwargs \u001b[38;5;241m=\u001b[39m {key: value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m module_kwarg_keys}\n\u001b[1;32m--> 690\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodule_kwargs)\n\u001b[0;32m    691\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\rooty\\UWEC\\Fall 24\\CS 426\\GroupProject\\GroupProject\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\rooty\\UWEC\\Fall 24\\CS 426\\GroupProject\\GroupProject\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\rooty\\UWEC\\Fall 24\\CS 426\\GroupProject\\GroupProject\\lib\\site-packages\\sentence_transformers\\models\\Transformer.py:393\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, features, **kwargs)\u001b[0m\n\u001b[0;32m    390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m features:\n\u001b[0;32m    391\u001b[0m     trans_features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m--> 393\u001b[0m output_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrans_features, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    394\u001b[0m output_tokens \u001b[38;5;241m=\u001b[39m output_states[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    396\u001b[0m \u001b[38;5;66;03m# If the AutoModel is wrapped with a PeftModelForFeatureExtraction, then it may have added virtual tokens\u001b[39;00m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;66;03m# We need to extend the attention mask to include these virtual tokens, or the pooling will fail\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rooty\\UWEC\\Fall 24\\CS 426\\GroupProject\\GroupProject\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\rooty\\UWEC\\Fall 24\\CS 426\\GroupProject\\GroupProject\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\rooty\\UWEC\\Fall 24\\CS 426\\GroupProject\\GroupProject\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:912\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    909\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    910\u001b[0m         token_type_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(input_shape, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m--> 912\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    913\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    914\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    918\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    920\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    921\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones((batch_size, seq_length \u001b[38;5;241m+\u001b[39m past_key_values_length), device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\Users\\rooty\\UWEC\\Fall 24\\CS 426\\GroupProject\\GroupProject\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\rooty\\UWEC\\Fall 24\\CS 426\\GroupProject\\GroupProject\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\rooty\\UWEC\\Fall 24\\CS 426\\GroupProject\\GroupProject\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:129\u001b[0m, in \u001b[0;36mRobertaEmbeddings.forward\u001b[1;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[0;32m    127\u001b[0m     position_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embeddings(position_ids)\n\u001b[0;32m    128\u001b[0m     embeddings \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m position_embeddings\n\u001b[1;32m--> 129\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLayerNorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    130\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(embeddings)\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\n",
      "File \u001b[1;32mc:\\Users\\rooty\\UWEC\\Fall 24\\CS 426\\GroupProject\\GroupProject\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\rooty\\UWEC\\Fall 24\\CS 426\\GroupProject\\GroupProject\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\rooty\\UWEC\\Fall 24\\CS 426\\GroupProject\\GroupProject\\lib\\site-packages\\torch\\nn\\modules\\normalization.py:217\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 217\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\n\u001b[0;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rooty\\UWEC\\Fall 24\\CS 426\\GroupProject\\GroupProject\\lib\\site-packages\\torch\\nn\\functional.py:2900\u001b[0m, in \u001b[0;36mlayer_norm\u001b[1;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[0;32m   2890\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[0;32m   2891\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m   2892\u001b[0m         layer_norm,\n\u001b[0;32m   2893\u001b[0m         (\u001b[38;5;28minput\u001b[39m, weight, bias),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2898\u001b[0m         eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[0;32m   2899\u001b[0m     )\n\u001b[1;32m-> 2900\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2901\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\n\u001b[0;32m   2902\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "trainer = SentenceTransformerTrainer(\n",
    "    model = model,\n",
    "    args = args,\n",
    "    train_dataset = dataset,\n",
    "    loss = train_loss,\n",
    "    tokenizer = model.tokenizer,  \n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 34%|███▎      | 518/1535 [05:09<05:38,  3.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.454, 'grad_norm': 5.565038204193115, 'learning_rate': 3.14935064935065e-05, 'epoch': 0.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "                                                  \n",
      " 34%|███▎      | 518/1535 [06:17<05:38,  3.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0543, 'grad_norm': 3.9539802074432373, 'learning_rate': 4.844315713251267e-05, 'epoch': 0.65}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 34%|███▎      | 518/1535 [06:59<05:38,  3.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0377, 'grad_norm': 0.3020555377006531, 'learning_rate': 4.4822592324402606e-05, 'epoch': 0.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 34%|███▎      | 518/1535 [07:50<05:38,  3.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0253, 'grad_norm': 0.2598021626472473, 'learning_rate': 4.1202027516292545e-05, 'epoch': 1.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 34%|███▎      | 518/1535 [08:58<05:38,  3.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0087, 'grad_norm': 0.3064405024051666, 'learning_rate': 3.758146270818248e-05, 'epoch': 1.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 34%|███▎      | 518/1535 [09:40<05:38,  3.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0179, 'grad_norm': 0.1715710610151291, 'learning_rate': 3.3960897900072416e-05, 'epoch': 1.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 34%|███▎      | 518/1535 [10:30<05:38,  3.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0134, 'grad_norm': 0.22755272686481476, 'learning_rate': 3.034033309196235e-05, 'epoch': 2.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 34%|███▎      | 518/1535 [11:42<05:38,  3.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0056, 'grad_norm': 0.04436185956001282, 'learning_rate': 2.671976828385228e-05, 'epoch': 2.61}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 34%|███▎      | 518/1535 [12:24<05:38,  3.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0088, 'grad_norm': 4.521520614624023, 'learning_rate': 2.3099203475742217e-05, 'epoch': 2.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 34%|███▎      | 518/1535 [13:10<05:38,  3.01it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.012, 'grad_norm': 0.3633098006248474, 'learning_rate': 1.9478638667632152e-05, 'epoch': 3.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 34%|███▎      | 518/1535 [14:24<05:38,  3.01it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0034, 'grad_norm': 0.2215462327003479, 'learning_rate': 1.5858073859522085e-05, 'epoch': 3.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 34%|███▎      | 518/1535 [15:06<05:38,  3.01it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0068, 'grad_norm': 0.11929313838481903, 'learning_rate': 1.223750905141202e-05, 'epoch': 3.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 34%|███▎      | 518/1535 [15:52<05:38,  3.01it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0089, 'grad_norm': 0.25609907507896423, 'learning_rate': 8.616944243301955e-06, 'epoch': 4.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 34%|███▎      | 518/1535 [17:07<05:38,  3.01it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0036, 'grad_norm': 0.027053426951169968, 'learning_rate': 4.99637943519189e-06, 'epoch': 4.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 34%|███▎      | 518/1535 [17:48<05:38,  3.01it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0063, 'grad_norm': 1.005142092704773, 'learning_rate': 1.3758146270818247e-06, 'epoch': 4.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      "100%|██████████| 1535/1535 [13:45<00:00,  1.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 825.2674, 'train_samples_per_second': 29.706, 'train_steps_per_second': 1.86, 'train_loss': 0.04364390678048522, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1535, training_loss=0.04364390678048522, metrics={'train_runtime': 825.2674, 'train_samples_per_second': 29.706, 'train_steps_per_second': 1.86, 'total_flos': 0.0, 'train_loss': 0.04364390678048522, 'epoch': 5.0})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_weights = r'C:\\Users\\rooty\\UWEC\\Fall 24\\CS 426\\GroupProject\\GroupProject\\scholarly-search\\weights\\all_distilroberta_v1'\n",
    "model.save(path_to_weights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GroupProject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
