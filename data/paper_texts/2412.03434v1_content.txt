SECTION: BIMCaP: BIM-based AI-supported LiDAR-Camera Pose Refinement

Abstract.

This paper introduces BIMCaP, a novel method to integrate mobile 3D sparse LiDAR data and camera measurements with pre-existing building information models (BIMs), enhancing fast and accurate indoor mapping with affordable sensors.
BIMCaP refines sensor poses by leveraging a 3D BIM and employing a bundle adjustment technique to align real-world measurements with the model.
Experiments using real-world open-access data show that BIMCaP achieves superior accuracy, reducing translational error by over 4 cm compared to current state-of-the-art methods.
This advancement enhances the accuracy and cost-effectiveness of 3D mapping methodologies like SLAM.
BIMCaP’s improvements benefit various fields, including construction site management and emergency response, by providing up-to-date, aligned digital maps for better decision-making and productivity. Link to the repository:https://github.com/MigVega/BIMCaP.

SECTION: 1.   Introduction

This research explores the convergence of 3D Building Information Modeling (BIM) with real-world 3D reconstruction, utilizing cost-effective RGB and LiDAR sensors. Traditional 3D data acquisition methods, such as terrestrial laser scanning, present challenges in terms of high costs and time-intensive procedures, particularly in the context of construction site monitoring or disaster relief. The practical implementation of faster 3D data acquisition methods has been made possible by the advancement of simultaneous localization and mapping (SLAM). However, state-of-the-art algorithms still encounter challenges to accurately map complex and dynamic environments, such as construction sites.

BIM has become a revolutionary technology within the Architecture, Engineering, and Construction (AEC) domain, offering comprehensive geometric and semantic information throughout a building’s life cycle. In this context, BIMs provide a valuable foundation for rectifying data acquired through SLAM algorithms in real time using low-cost sensors.
The automatic alignment of RGB and LiDAR data with the BIM holds significant potential to rapidly create precise 3D maps in GPS-denied environments (such as indoors).
This alignment not only facilitates safety monitoring and quality management but also contributes to the quick development of a digital twin, providing an accurate 3D representation of actual asset states.

In this research, we address several critical questions central to advancing state-of-the-art technologies in the field of sensor pose correction for accurate 3D reconstruction.
By utilizing the geometric and semantic information inherent in a BIM, we examine the potential of a bundle adjustment module to refine drifted sensor poses.
Additionally, we explore methodologies for effectively combining calibrated sparse LiDAR data with RGB images to produce detailed depth maps, aiming for a comprehensive reconstruction.
Furthermore, we evaluate semantic segmentation algorithms in complex indoor construction settings and develop a strategy for improving their performance.
We demonstrated the improvement in performance through extensive experiments on the publicly available ConSLAM dataset(Trzeciak et al.,2023).
In this research, we aim to contribute to the development of robust and efficient techniques for 3D reconstruction, which has implications for a range of applications, including construction site management, emergency response, and beyond.

As we delve into the intricacies of our methodology, it is essential to acknowledge the scope and assumptions of our method.
Since our goal is to have a robust method using sensors with a reduced field of view (FoV), such as solid-state LiDARs or RGB-D cameras, we only use the LiDAR information in the FoV of the camera, ignoring the rest of the available points.
In our approach, we also assume that there is an initial rough alignment of the drifted trajectory with the reference map.

This paper is structured as follows: Section2offers an in-depth exploration of related research efforts.
Section3delineates the proposed framework split into three main steps.
Subsequently, Section4presents the findings derived from numerous experiments conducted on real-world construction site data.
Finally, Section5provides the culmination of this work, offering conclusive insights and avenues for future research.

SECTION: 2.   Related Work

Several studies have approached the alignment of RGB images with BIMs in two main ways: (1) as a global localization problem and (2) as a pose-tracking problem.

In the global localization problem,Acharya et al.(2022)introduced BIM-PoseNet, utilizing synthetic images from a 3D indoor model to achieve a 2-meter accurate camera pose without an initial position.

In the pose-tracking approach,Kropp et al.(2018)focused on image-to-4D BIM registration using line segments as features, with manual intervention for initial registration.Boniardi et al.(2019)proposed a clutter-handling method using a convolutional neural network for layout prediction and a particle filter algorithm for pose tracking using a floor plan as a reference map.
The method proposed byDantas et al.(2022)aims at quickly correcting camera poses using vanishing points, lines, and synthetic renders created from a BIM.

Other methods addressed the challenge of creating a coherent 3D map of the environment aligned with a given reference map.Vega Torres et al.(2023)used a BIM to align and correct 360-degree LiDAR measurements, which initial poses were calculated with a LiDAR-based SLAM algorithm.Sokolova et al.(2022)presented the Floorplan-Aware Camera Poses Refinement (FACaP) method, aligning Visual-SLAM maps with floor plans using semantic segmentation and an optimization model considering geometric, floor-to-plane and wall-to-floorplan terms for map correction.

However, most of these methods were tested in indoor residential apartments without the level of clutter, dynamic elements, and changing lighting conditions present in real-world construction sites.
Furthermore, the literature review shows accuracy metrics for diverse methods; however, all were evaluated on separate, unrelated case studies.
This fact highlights the necessity for a standardized dataset (i.e., BIM and synchronized sensor information) tailored explicitly to real-world construction site environments, enabling fair and consistent ranking.

SECTION: 3.   Methodology

We propose a framework designed to align a sequence of synchronized LiDAR scans and RGB images with a 3D BIM, thereby refining the initial approximated camera poses, which inherently suffer from drift owing to the characteristics of SLAM algorithms. Our framework can be divided into three significant steps.Step 1.The initial step of our methodology involves fusing camera images and sparse LiDAR scans in precise depth maps. This process is facilitated through a hybrid approach employing interpolation and a deep learning (DL) technique, which then allows the projection of the pixel information (such as semantic information) into the 3D space.Step 2.Subsequently, in the second step, semantic segmentation is applied to the images, enabling the detection of permanent elements such as walls, columns, and floors within the reconstructed 3D map. Simultaneously, a point cloud and a vectorized floor plan with semantic information are created from the BIM. This vectorized semantic floor plan will be used as a reference map for the alignment of the real-world data.Step 3.In the third step, we employ a statistical approach to generate initial synthetic camera poses. These poses are then refined through a bundle adjustment (BA) module, which integrates custom cost functions. These functions are designed to iteratively enhance the accuracy of sensor poses, thereby ensuring optimal alignment between the generated map and the semantically vectorized floor plan from the BIM. This refinement process selectively considers only permanent elements, which are identified through semantic segmentation in real-world images and projected into three-dimensional space using the previously estimated depth maps.
Fig.1illustrates the proposed semantic-aware pose optimization framework.

SECTION: 3.1.   Step 1: LiDAR and camera fusion

To fuse the information from the LiDAR and the camera, we first project the visible point cloud (in the FoV of the camera) into the image, and then we aim to generate a dense map that is coherent with the image and LiDAR information.
The projection of the LiDAR points to the camera image is made with the intrinsic and extrinsic parameters of the camera and with the package provided byTrzeciak et al.(2023); this package ensures the camera image is undistorted, and only the corresponding (timestamped synchronized) LiDAR points of the small FoV of the camera are projected from the 3D space to the 2D image.
Upon this step, we now have depth information for several pixels of the image. This depth is, however, very sparse since we are working with a 360∘LiDAR.
It is essential to mention that to ensure that the method works appropriately with sensors with a reduced FoV (such as solid-state LiDARs or RGB-D cameras), we only use the LiDAR information in the FoV of the camera.

The sparsity of the point cloud would not be sufficient to leverage all the information from the image in the 3D space; therefore, we subsequently aim to create a dense depth map using the point cloud and the corresponding camera image.

Currently, numerous DL methods serve for depth estimation, yet many of them are optimized for outdoor environments (such as the KITTI dataset). Therefore, their accuracy tends to decline in indoor settings, which constitutes the focus of our investigation.
Following extensive experimentation with various methodologies, we chose to adopt a hybrid method that combines linear interpolation with CompletionFormer (CF)(Zhang et al.,2023).
Fig.2illustrates the results of this hybrid approach, showing the original CF output alongside the refined outcome involving an initial linear interpolation.

SECTION: 3.2.   Step 2: Semantically enriched maps

In this step, we aim to create maps that will allow the sensor pose correction in the subsequent step. This step is divided into two sub-steps: Firstly, we create a reference semantic vectorized floor plan from the BIM, and secondly, we enrich the 3D map created with real-world data with semantic information.
This semantic enrichment serves a pivotal role in distinguishing permanent elements within real-world data, such as walls, columns, and floors, which can be reliably aligned with the BIM.

To prepare for implementing the pose correction module, we simplify the 3D BIM into a 2D semantic vectorized floor plan. Since walls and columns are perpendicular to the XY plane, this reduction not only retains all vertical structural element information but also allows efficient pose optimization in subsequent stages.

To generate the 2D semantic vectorized floor plan, the BIM undergoes conversion from Industry Foundation Classes (IFC) format to OBJ format using ifcConvert.
Following this, distinct OBJ files are generated for each entity within the model (e.g., walls, columns, floor, ceiling, windows, and doors). Then, uniform point cloud sampling is applied to each OBJ file, and the resulting semantically enriched synthetic point clouds are merged into a single one.
An illustration of such a point cloud can be observed in Fig.3b.

The created synthetic 3D point cloud is projected vertically into 2D images within a specified height range, typically within20 cm from the floor level.
Semantic labels are utilized to filter each element in the point cloud.
Subsequently, image processing methods such as contour and line detection are employed to identify line segments representing individual elements in the 2D projection.
These detected lines are then consolidated, including their start and end points, to form the vectorized semantic floor plan.
A resulting floor plan is depicted in Fig.3c.

To filter permanent elements that we can match from the real-world data with the BIM, we leverage state-of-the-art image semantic segmentation algorithms.
More specifically, we use a modified version of Grounding DINO(Liu et al.,2023).
However, for the object detection task, we replace the DINO algorithm with a tiny version of the RTMDet algorithm(Lyu et al.,2022)pre-trained with the COCO dataset and 250 labeled images of the ConSLAM dataset, which contains custom classes typical of a construction site.
These images were labeled semi-automatically using the Computer Vision Annotation Tool (CVAT).
Thus, our approach enables the detection of objects of interest, expanding beyond the foreground elements identified by the original Grounding DINO version.
Fig.4illustrates the results of the semantic enrichment before and after the proposed enhancement, and Fig.5ashows the top view of the resulting semantically enriched 3D point cloud after projecting the semantic labels to the 3D space with the previously generated depth maps.
In this last figure, it is also visible that we can now filter walls, columns, floor, and ceiling points in the depth maps, which can reliably be used for registration with the BIM and, therefore, for camera pose optimization.
It is worth mentioning that the floor and ceiling predicted labels were also used to optimize the depth maps, creating smoother surfaces in these regions with blurring operations in the 2D depth maps.

SECTION: 3.3.   Step 3: Sensor pose calculation and refinement

The initial approximations of sensor poses are ideally determined using a Visual-SLAM framework.
However, our experimentation with cutting-edge SLAM algorithms such as DROID-SLAM or Go-SLAM yielded unsatisfactory results when applied to the ConSLAM dataset, functioning correctly only for limited segments of the trajectory.
Despite these limitations, advancements in odometry systems suggest that addressing this challenge will become feasible in the future.
Therefore, and since we aim to refine slightly drifted poses and experiment under different magnitudes of drift, we opted to create a synthetic trajectory instead, simulating the output of a SLAM framework.
This process is explained in the following subsection.
Subsequently, we introduce the method that is used to improve the accuracy of these poses.

To make sure our approach matches the usual trajectory patterns seen in existing SLAM systems and to have the flexibility to study how stable our method is when it comes to convergence with different starting positions, we carefully engineer synthetic trajectories.
When creating these trajectories, we focus on replicating the gradual drifting feature of SLAM-generated trajectories. In other words, we want the error at each pose to slowly increase over time.

Therefore, we model the translation offset from the original sensor poseas a normally distributed random variable with meanand variance.
Formally,withwhereis the previously sampled offset value, and the varianceis an adjustable hyper-parameter which would determine the offset of the trajectory from the ground truth poses.
Regarding the camera rotation, we randomly sample degree offsetsaround pitch and yaw directions. Fig.5bpresents the resulting map using a synthetically drifted trajectory.

Inspired by the FaCAP framework(Sokolova et al.,2022), we consider several terms in our cost function to achieve sensor pose refinement with the BIM using BA.
To create a consistent 3D map from the real-world sequential images, we use a geometric term that encapsulates the divergence between 3D point estimations from two distinct viewpoints.
In other words, the geometric term considers photogrammetric constraints, and in our case, we use COLMAP to obtain features and correspondences among sequential images. Fig.5band5cvisualize some of these features.

Furthermore, we use a floor term designed to ensure that segmented points corresponding to the floor lie within a single plane and close to the floor surface defined in the BIM.
In a parallel manner, we introduce a ceiling term that incorporates information from the model’s ceiling for optimization purposes.
In addition, we include wall and column terms. These elements are crucial for correcting rotations around the vertical axis (i.e., yaw variations) and horizontal translations.

SECTION: 4.   Experiments and results

SECTION: 4.1.   Dataset and evaluation details

To ensure reproducibility and enable benchmarking, we tested the developed method on the ConSLAM dataset(Trzeciak et al.,2023).
ConSLAM represents a pioneering effort, offering the first open-access dataset acquired in an indoor cluttered construction site.
This dataset encompasses sequences of RGB and LiDAR data together with Terrestrial Laser Scanning (TLS) point clouds. The latter was leveraged as a resource for the generation of a BIM with centimeter-level accuracy.
The GT poses of ConSLAM were calculated using SLAM2REF(Vega-Torres et al.,2024), an enhanced version of BIM-SLAM(Vega Torres et al.,2023)and OGM2PGBM(Vega Torres et al.,2022)for large-scale maps, which is robust to LiDAR motion distortion and Scan-Map deviations.

To quantify the quality of the whole trajectory before and after pose optimization, we used the standardized root mean square error (RMSE) of the absolute trajectory error (ATE) in position (also referred to as translation) and in rotation.
Moreover, for better comparison, we incorporated some of the metrics introduced in(Sokolova et al.,2022)including the Map Mean Entropy (MME), Mean Plane Variance (MPV), and the Nearest Neighbor Distance (NND).
The MME serves to assess the quality of 3D maps, with a higher MME signifying favorable alignment between the input cloud and the reference map.
The MPV evaluates the variance among planes within the map, with lower MPV values indicating more uniform and well-defined surfaces. The NND quantifies the average distance between adjacent points in the point cloud, with smaller NND values indicating denser point clouds.
The MME value of 0.761, calculated using the GT poses (as shown in1), represents the optimal alignment between the real-world point cloud and the BIM model. For understanding, this value would be zero if no deviations between the actual environment and the model (Scan-BIM deviations) exist.

SECTION: 4.2.   Pose refinement results

The results of our framework are compared against the state-of-the-art FaCAP pipeline(Sokolova et al.,2022)and evaluated meticulously with three different experiments.
The first experiment consists of a synthetic trajectory that has an offset of around 1.4 meters in translation and 10 degrees in rotation (Exp. 1), the second one has an offset of only 30.3 cm in translation and 8.82 deg in rotation (Exp. 2), and the third one only has rotation offset of 9.6 degrees (Exp. 3).

Table1shows initial metrics based on ground truth and synthetic poses for Exp. 1, along with results after pose optimization using various terms of the FACaP pipeline and the proposed BIMCaP framework.

The findings from Exp. 1 (Tab.1) emphasize the efficacy of utilizing all terms for optimizing translational errors.
However, this approach may not consistently yield optimal results when addressing rotational errors.
Notably, while BIMCaP demonstrates a superior reduction in translational error by 4 cm compared to FACaP, both methodologies become trapped in a local minimum, impeding the accurate optimization of the poses.
This issue can be attributed to the substantial difference between the synthetic poses and the ground truth.

Exp. 2 and 3 results (Table2and Fig.6) indicate superior performance of both methods in optimizing rotational errors over translational errors. Notably, BIMCaP significantly enhances yaw and pitch angles during the pose optimization process.
Fig.7illustrates how BIMCaP aligns the floor and ceiling points to the correct planes, contrary to FACaP, which tries to fit a plane among the given measurements without any reference.

Exp. 3 exposes a limitation in our approach, as optimizing trajectory with only rotational offsets resulted in unintended translations.
This could be due to the simultaneous optimization of both translation and rotation, causing discrepancies. Additionally, the challenge of accurately calculating sensor poses is intensified by the reduced FoV and the sparse ground truth poses.

SECTION: 5.   Conclusion and future directions

We have demonstrated that BIMCaP enables alignment and correction of a sequence of the camera and reduced FoV LiDAR measurements with a BIM; the technique takes into consideration only reliable selected semantic landmarks (in our case, floor, walls, columns, and ceiling) for the drift correction.
Moreover, we evaluated our technique in the open-access ConSLAM dataset and compared it against a state-of-the-art method, ensuring reproducibility and benchmarking.
In future work, we aim to improve the accuracy of the optimization process and add global registration and change detection capabilities to our framework.

SECTION: 6.   Acknowledgement

This research is part of the INTREPID project funded by EU’s Horizon 2020 program (Grant agreement ID: 883345) and the Research Unit 5672 funded by the German Research Foundation (DFG) (Grant ID: 517965147). This work has also benefited from the collaboration with the NYUAD Center for Interacting Urban Networks (CITIES), funded by Tamkeen under the NYUAD Research Institute Award CG001.
Shaowen Qi’s contributions to data labeling and to the semantic segmentation step are also acknowledged.

SECTION: References