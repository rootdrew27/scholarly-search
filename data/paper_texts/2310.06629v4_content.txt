SECTION: EViT: An Eagle Vision Transformer with Bi-Fovea Self-Attention
Owing to advancements in deep learning technology, Vision Transformers (ViTs) have demonstrated impressive performance in various computer vision tasks. Nonetheless, ViTs still face some challenges, such as high computational complexity and the absence of desirable inductive biases. To alleviate these issues, the potential advantages of combining eagle vision with ViTs are explored. We summarize a Bi-Fovea Visual Interaction (BFVI) structure inspired by the unique physiological and visual characteristics of eagle eyes. A novel Bi-Fovea Self-Attention (BFSA) mechanism and Bi-Fovea Feedforward Network (BFFN) are proposed based on this structural design approach, which can be used to mimic the hierarchical and parallel information processing scheme of the biological visual cortex, enabling networks to learn feature representations of targets in a coarse-to-fine manner. Furthermore, a Bionic Eagle Vision (BEV) block is designed as the basic building unit based on the BFSA mechanism and BFFN. By stacking BEV blocks, a unified and efficient family of pyramid backbone networks called Eagle Vision Transformers (EViTs) is developed. Experimental results show that EViTs exhibit highly competitive performance in various computer vision tasks, such as image classification, object detection and semantic segmentation. Compared with other approaches, EViTs have significant advantages, especially in terms of performance and computational efficiency. The developed code is available at

SECTION: 
Over the past decade, Convolutional Neural Networks (CNNs) have made extraordinary contributions to the field of computer vision in accordance with the development of deep learning technologies and hardware computing units. This success can be attributed to the pyramidal structural design of CNNs and their inherent inductive biases, such as translation invariance and local sensitivity. However, it is challenging for CNNs to explicitly model the importance of global contextual information because of the limited receptive fields possessed by convolutional kernels. This issue restricts the further development and applications of CNNs. Moreover, inspired by the success of Transformersin the Natural Language Processing (NLP) field, a question has been raised: What happens when transformers are applied in the field of computer vision? The original Vision Transformer (ViT)is a significant milestone that first incorporated a Transformer into vision tasks, which demonstrates superior performance in computer vision tasks, and have attracted widespread attention worldwide.

Following the release of the ViT, various vision transformer variantshave been proposed, offering new paradigms and solutions for completing computer vision tasks. Nonetheless, ViTs also face several challenges, including the following. (1) The process of computing self-attention matrices in ViTs has proven to be their major efficiency bottleneck. This is because calculating the importance of each token in an input sequence to the other tokens is necessary, which leads to quadratic computational complexity with respect to the length of the sequence. Consequently, the maximum input sequence length is limited by the amount of available memory. This issue is especially notable when addressing high-resolution images and videos. (2) ViTs tend to focus on the overall information of objects when processing target features, thus lacking sensitivity to local features and details. This limitation impairs their performance in dense prediction tasks. (3) Due to the absence of appropriate inductive biases, ViTs require more data for training. In scenarios with limited available data, this increases the risk of overfitting.

To alleviate these issues, we draw inspiration from biological vision and develop a unified pyramid backbone network. Figureillustrates the physiological structure and photoreceptor cell density distribution of the bi-fovea of eagle eyes. Although eagle vision and vision transformer originate from biological science and computer science, respectively, we identify three analogous properties between them. (1) Attention mechanisms: Eagle vision is renowned for its fast focusing ability, which enables eagles to efficiently capture prey in complex environments. Similarly, the self-attention mechanism in vision transformers allows the network to dynamically allocate attention scores to different regions, enabling them to encode the critical feature representations of targets. (2) Multilevel feature extraction capabilities: Eagles process visual information at multiple levels, beginning with photoreceptor cells and ultimately reaching the cerebral cortex. In the same way, vision transformers encode the target features in a layer-by-layer manner by stacking the Multi-Head Self-Attention (MHSA) and a Multi-Layer Perceptron (MLP). (3) Global information awareness: Eagle vision involves a wide field of view for perceiving prey and predators at high altitudes and from long distances. Furthermore, vision transformers can encode the target features observed across the spatial domain of the entire input image. This global perception ability allows vision transformers to model and integrate the contextual information among different locations within an image, which helps them obtain a more accurate understanding of the image and process it better.

According to the above discussions, we revisit the potential advantages of combining eagle vision with vision transformer. A Bi-Fovea Visual Interaction (BFVI) structure is derived from the unique physiological and visual characteristics of eagle eyes, which can integrate the benefits of both cascade and parallel structures, including hierarchical organization and parallel information processing. Thereafter, a novel Bi-Fovea Self-Attention (BFSA) mechanism and a Bi-Fovea Feedforward Network (BFFN) are proposed based on this design approach, which mimics the hierarchical and parallel information processing scheme of the biological visual cortex. As improved variants of self-attention and feedforward networks, respectively, the BFSA and BFFN can be used to help networks extract features in a coarse-to-fine manner, resulting in high computational efficiency and scalability. Furthermore, a Bionic Eagle Vision (BEV) block is designed as the basic building unit based on BFSA and BFFN. Following the mainstream hierarchical design concept, a general family of pyramid vision backbone networks called Eagle Vision Transformers (EViTs) is developed by stacking BEV blocks. These EViTs include four variants, namely, EViT-Tiny, EViT-Small, EViT-Base and EViT-Large, enhancing their applicability in various computer vision tasks. Figurepresents the results of a performance comparison between the EViTs and other convolutional neural networks and vision transformers on the ImageNetdataset. To our knowledge, this is the first work to integrate eagle vision with vision transformers on large-scale datasets such as ImageNet, and it is also the first study to propose a general family of vision backbone networks on the basis of eagle vision.

The main contributions of this study are as follows.

Benefiting from biological vision, a structural design approach called Bi-Fovea Vision Interaction (BFVI) is derived from the unique physiological and visual characteristics of eagle eyes, which integrates the advantages of both cascade and parallel connections, featuring hierarchical organization and parallel information processing.

Based on the BFVI design approach, a novel Bi-Fovea Self-Attention (BFSA) and a Bi-Fovea Feedforward Network (BFFN) are proposed to construct a Bionic Eagle Vision (BEV) block. These components are used to simulate the shallow and deep fovea of eagle vision, enabling the network to learn the feature representations of the target from coarse-to-fine.

Following the hierarchical design concept, a general and efficient family of pyramid backbone networks called EViTs is developed. Experimental results demonstrate that the EViTs perform excellently across various vision tasks and exhibit significant competitive advantages in terms of computational efficiency and scalability.

The remainder of this paper is structured as follows. Section 2 summarizes the related work on biological eagle vision and vision transformer, respectively. Section 3 describes the process of designing EViTs. Section 4 presents the experimental results produced by the EViTs in various vision tasks. Section 5 presents the conclusion.

SECTION: 
SECTION: 
Eagles possess an excellent natural visual system, allowing them to observe their surroundings with remarkable sensitivity, which is attributed to their unique bi-fovea visual structure. The physiological structure and photoreceptor cell density distribution of eagle eyes are illustrated in Figure. The deep fovea is located at the centre of the retina and has a high photoreceptor cells density. This is crucial for enhancing the visual resolution of eagle eyes, allowing them to recognize and capture prey at long distances. The shallow fovea is located in the peripheral area of the retina, where the photoreceptor cell density is lower, but it provides a wider field of view. This structure not only enhances the efficiency of eagles in terms of processing visual information, but also ensures their survival in complex environments.

Figure(a) illustrates the visual neural pathway of an eagle. As mentioned above, eagles process visual information at multiple levels, beginning with photoreceptor cells and ultimately reaching the cerebral cortex. Although one eagle eye cannot simultaneously use both the deep and shallow fovea for imaging, two eyes can collaborate to alternate between them. For example, when an eagle is looking forward, the deep fovea of one eye can be used for fine target recognition, whereas the shallow fovea of the other eye can be used to perceive the surrounding environment. Inspired by the unique physiological and visual characteristics of eagle eyes, a Bi-Fovea Vision Interaction (BFVI) structure is summarized in this work. The BFVI structure, which is illustrated in Figure(b), integrates the benefits of cascaded and parallel structures, including hierarchical organization and parallel information processing.

SECTION: 
ViTis a pioneering work that first introduces transformers into vision tasks. The core idea is to split images into non-overlapping patches and then feed them as sequences into self-attention mechanisms and multilayer perceptrons for feature encoding. Following the ViT, a series of improvement methods have been proposed. The ViT is extended in various directions, including self-attention mechanisms, positional encoding, and computational efficiency. Compared with CNNs, vision transformers are excellent at modelling long-range dependencies and capturing global contextual information. These strengths enable vision transformers to exhibit potential surpassing that of CNNs in vision tasks such as image classification, object detection, and semantic segmentation. The success of vision transformers can be attributed primarily to their innovative architectural designs, which include self-attention mechanisms, multi-layer perceptrons, and skip connections. These components allow networks to learn the interactions between the spatial locations of features in raw data.

The CF-ViTis a coarse-to-fine visual neural network that achieves satisfactory performance. The difference between this network and our method is that we do not employ the two-stage approach for network inference. Instead, we draw inspiration from eagle vision to enhance the ability of our model to focus on image details and the global context. This is beneficial for the expansion and application of EViTs across various visual tasks. Moreover, vision mamba models, which are state-space models with linear computational complexity, have also demonstrated tremendous potential in representation learning tasks, gaining significant attention because of their efficiency and ability to model long-term dependencies in sequences. MLLAexplores the relationship between state-space models and linear attention, reinterpreting vision mamba as a variant of the linear attention transformer. Representative vision mamba models include EfficientVMamba, MSVMamba, and VMamba. In the experiments, these models are compared with EViTs in detail. Our work demonstrates the potential of combining eagle vision with vision transformers, showing that EViTs can lead to more performance breakthroughs in vision tasks.

SECTION: 
SECTION: 
Inspired by eagle vision, a novel family of pyramid backbone networks called Eagle Vision Transformers (EViTs) is proposed. The overall pipeline of an EViT, which is illustrated in Figure, consists of a convolutional stem, severalconvolutional layers and Bionic Eagle Vision (BEV) blocks. Here, the stride of theseconvolutional layers is 2, and the layers are used for patch embedding. As general backbone networks for various vision tasks, EViTs follow the mainstream hierarchical design concept, which comprises four stages, each of which possesses a similar architecture. The difference is that the resolutions of the features output from stage 1 to stage 4 are divided by factors of 4, 8, 16 and 32, respectively, while the corresponding channel dimensions are increased to,,and. Given an input image of size, it is first fed into the convolutional stem to obtain the low-level feature representations. The convolutional stem consists of three successiveconvolutional layers, where the first convolutional layer has a stride of 2, which is used to stabilize the training process in the early stages of the network. These low-level representations are then processed through a series ofconvolutional layers and BEV blocks to generate hierarchical representations of the targets. Finally, in image classification tasks, a normalization layer, an average pooling layer, and a fully connected layer are used as classifiers to output the predictions.

SECTION: 
As the basic building units of EViTs, BEV blocks integrates the advantages of convolutions and vision transformers. A BEV block consists of three key components: a Convolutional Position Embedding (CPE), a Bi-Fovea Self-Attention (BFSA) and a Bi-Fovea Feedforward Network (BFFN). The complete mathematical definition of a BEV block is shown as

where, LN represents the layer normalization function, which is used to normalize the feature tensors. Stage 1 is taken as an example. Given an input tensor, it is first processed by the CPE, which is used to introduce position information into all tokens. The structure of the CPE is shown in Figure 4, and it is defined as

The position information plays a crucial role in describing visual representations. In previous works, most vision transformer models encode position information through Absolute Position Embedding (APE)and Relative Position Embedding (RPE). These embeddings are typically defined by a series of sinusoidal functions with varying frequencies or learnable parameters. The difference is that APE is designed for specific input sizes and is not robust to changes in the resolution of feature tokens, requiring adjustment to accommodate these variations. On the other hand, the distances among the feature tokens in an input sequence are considered by RPE, which exhibits translational invariance. However, additional computational costs are introduced when calculating the relative distances among feature tokens. Importantly, visual tasks still require absolute position information, and RPE cannot provide this information.

For the above reasons, Convolutional Position Embedding (CPE) is introduced in EViTs to encode the position information of feature tokens, which is composed of a depth-wise convolution. Compared with APE and RPE, there are two advantages of CPE as follows. (1) CPE can flexibly learn the position information of features at arbitrary resolutions through zero padding with a depth-wise convolution, providing it with plug-and-play properties. (2) CPE further integrates the necessary inductive bias into EViTs through a depth-wise convolution, which helps improve their performance ceiling. Additionally, this BEV block employs BFSA to simulate the shallow fovea and deep fovea of eagle vision for modelling the global feature dependencies and local fine-grained feature representations of images. Finally, the BFFN is used to complement the local information, and to improve the information interaction and local feature extraction capabilities of the BEV blocks.

SECTION: 
Eagle eyes possess unique bi-fovea physiological and visual characteristics. The shallow fovea of eagle vision is utilized for coarse-grained environmental perception, and the deep fovea is used for fine-grained prey recognition. Inspired by this fact, a Bi-Fovea Self-Attention (BFSA) is proposed. An illustration of this BFSA is shown in Figure. The BFSA consists of a Shallow Fovea Attention (SFA) and a Deep Fovea Attention (DFA). In terms of the structural design of BFSA, the SFA and DFA are not simply connected in parallel or in cascade. Instead, the Bi-Fovea Vision Interaction (BFVI) structure derived from eagle vision is adopted. This BFVI structural combines the advantages of both parallel and cascaded connections, allowing the SFA to model the global feature dependencies of images while enabling the DFA to capture the fine-grained feature representations of the targets.

Given an input tensor, it is first projected to a Query, a Keyand a Value. Intuitively, adopting MHSA for the feature maps to calculate the global dependencies among tokens is a redundant and time-consuming approach. To mitigate the computational complexity and memory requirements of SFA, DWConv is employed to reduce the lengths ofand. The dimensionality ofremains unchanged, which directly affects the ability of self-attention to focus on input the features. Reducing the dimensionality ofcan degrade the ability to fully capture the information possessed by the input, thereby impacting the effectiveness of the attention mechanism. Specifically,,and. This is different from the designs of LSRAand SRA, which use average pooling and standard convolution, respectively. Compared with LSRA and SRA, our design achieves a better trade-off between reducing the computational complexity of the network and preserving the independence of spatial and channel features. Furthermore, SFA is used to encode the global feature dependencies among all tokens to produce attention scores. The compact matrix form of the SFA is defined as

whereis the output of theattention head, and the weight matrixis used to construct all the heads.

The mathematical definitions of DFA and SFA are identical, and the difference is that the output of SFA serves as the input for DFA. This design approach fully utilizes the advantages of self-attention, enhancing the ability of the network to extract complex features through a layer-by-layer refinement process. The complete mathematical definition of the DFA is shown as

where

Finally, the outputs of the SFA and DFA are summed and fed into the next layer. This design follows the visual characteristics of the information interaction and fusion processes implemented in the bi-fovea of eagle eyes, and is used to simulate the working mechanisms of information processing in biological visual systems. The process is represented as

To conduct an in-depth analysis of the BFSA, the activation mapping approach is employed to generate visual attention maps, which are shown in Figure, wherein the BFSA allocates varying attention scores to different regions and targets while processing image features. Regions with higher attention values are crucial for the current task, reflecting the feature selection priorities of the network. According to the numerical distribution of these visual attention maps, the BFSA pays more attention to the foreground objects of interest, effectively concentrating on the task-relevant features while suppressing unnecessary background information. This improves the performance of the model, providing richer feature information for visual tasks.

SECTION: 
In this subsection, a computational complexity analysis is conducted on the standard MHSA and BFSA. For simplicity, the attention heads are excluded. Given input feature maps with dimensions of, the computational complexity of the standard MHSA is, whererepresents the computational complexity of generating,and, which involves three linear projections.denotes the complexity of computing the attention weights, which includes matrix multiplyingand, along with a Softmax operation. The computational complexity of self-attention is quadratic with respect to the sequence length. In SFA, the computational complexity of the linear projections of,andis, which is consistent with that of MHSA. The difference is that SFA uses a DWConv to reduce the spatial sizes ofand. The computational complexity of this operation is. The computational complexity of the attention matrix and the softmax operation in SFA is, whereis the reduction factor for the spatial sizes ofand. Overall, the computational complexity of this SFA is. The computational complexity of BFSA is twice that of SFA. Since the sequence length of the feature tokens is reduced by the BFSA through depth-wise convolution, its overall computational complexity and memory cost remain low, even though self-attention calculations are performed twice. Additionally, an inductive bias is further introduced to the EViT by using DWConv to processand. This step enhances the ability of the network to model local feature information, which helps to improve the robustness and generalizability of the network in complex scenarios.

SECTION: 
As essential components of vision transformers, the feedforward networks are employed to enhance the representation of image features through nonlinear transformations. However, the feedforward network lacks sensitivity to local features. The common practice is to introduce convolutional operations between two fully connected layers or useconvolutions to replace the fully connected layers, which is considered inefficient. To this end, we are inspired by the ability of the biological visual cortex to process information and believe that an efficient feed forward network should satisfy the imposed hierarchical structure and parallel information processing requirements. Therefore, the design approach of the BFVI structure is followed, and a Bi-Fovea Feedforward Network (BFFN) is proposed. The structure of the BFFN is illustrated in Figure. As already emphasized, the BFFN combines the characteristics of the hierarchical structure and parallel information processing, which helps to increase the receptive field of each network layer and improve the multi-scale feature representations of networks with finer granularity. In our experiments, ablation studies are conducted on the connection structure and effectiveness of the BFFN.

SECTION: 
The EViTs include four variations: EViT-Tiny, EViT-Small, EViT-Base and EViT-Large. These variants have different parameter scales and computational complexity. To obtain multiscale feature representations, each stage of these variants uses a 2 × 2 convolution with a stride of 2 for connection purposes. This convolution is applied for patch embedding. At the beginning of the next stage, the spatial sizes of the feature maps are halved, and the number of dimensions is doubled. Therefore, the four stages of EViTs can output feature maps with four different sizes, providing rich hierarchical feature representations of the targets. These hierarchical representations are beneficial for enhancing the performance of the EViTs in dense prediction tasks. The configuration details of the EViTs are shown in Table. To facilitate comparison with other mainstream networks, the input image resolutions of EViT-Tiny, EViT-Small, EViT-Base and EViT-Large are all.

SECTION: 
In this section, experiments are conducted with the EViTs on a series of mainstream computer vision tasks, including ImageNet-1Kclassification (Section 4.1), COCO 2017object detection and instance segmentation (Section 4.2), ADE20Ksemantic segmentation (Section 4.3), and other transfer learning tasks (Section 4.4). Specifically, the EViTs are trained first from scratch on the ImageNet-1K dataset to implement image classification and obtain their pretraining parameters. The pretraining parameters are subsequently fine-tuned on object detection, semantic segmentation and other vision tasks through transfer learning, which is done to validate the generalization performance of the EViTs. Additionally, a robustness evaluation and an ablation study are conducted on EViTs in Sections 4.5 and 4.6, respectively, to demonstrate the effectiveness of the BFSA and BFFN.

SECTION: 
In this section, the EViTs are first evaluated on the ImageNet-1Kdataset, which includes 1000 classes and approximately 1.33M images in total. Among these, the training dataset contains approximately 1.28M images and the validation dataset contains approximately 50K images. For fairness, the same training strategy as that employed by DeiTand PVTis followed to facilitate comparisons with other networks. Specifically, the AdamW is taken as the parameter optimizer and the weight decay rate is set to 0.05. All networks are trained 300 epochs and the initial learning rate is set to 0.001 with the cosine decay strategy. We employ the same data augmentation techniques as those used in DeiT, including random flipping, random cropping, random erasing, CutMix, Mixupand label smoothing. Unless otherwise specified, the input image resolutions of the EViTs are allduring the training process.

Tableshows the performance achieved by the EViTs in the ImageNet classification task. For ease of comparison, similar networks are grouped based on their model parameters and performance. The experimental results show that the EViTs achieve better accuracy and speed trade-offs than the other models do with similar model parameters. Specifically, EViT-Tiny and EViT-Small achieve 79.9% and 82.6% classification accuracies with small model scales, respectively. Compared with the latest network, although EViT-Tiny is not as strong as RepViT-M1.5in terms of performance, EViT-Small demonstrates competitive advantages at larger parameter scales. Compared with ConvNext-S, Focal-Sand ResT-Large, EViT-Base attains impressive performance with the lowest computational cost. Specifically, EViT-Base achieves 83.9% Top-1 accuracy with 7.2 GFLOPs, representing a performance improvement of 0.3% over the three above mentioned methods while simultaneously reducing the required computational complexity by nearly 0.7 to 1.9 GFLOPs. At larger parameter scales, EViT-Large maintains significant competitive advantages over the other networks. In particular, for a fair comparison with the other networks, two image classification experiments are conducted on EViT-Large with image sizes ofand. Under the same settings, EViT-Large can achieve 0.8% and 0.6% performance gains over PVTv2-B4and ConvNext-B, respectively. When the input image size is set to, the computational complexity and number of model parameters required by EViT-Large are only 13.7 GFLOPs and 61.9 M, respectively, but it achieves 84.9% classification accuracy. Additionally, the EViTs are compared with representative vision mamba models, including EfficientVMamba, MSVMamba, and VMamba. The numerical results indicate that our method still demonstrates a competitive advantage over these linear attention models. It enhances the context-awareness of the model to better capture long-range dependencies. As described, the EViTs exhibit significant competitive advantages, especially in terms of their low computational complexity and high scalability. They can be flexibly scaled to form smaller or larger models according to specific task requirements.

SECTION: 
In this section, object detection and instance segmentation experiments are conducted on the COCO 2017dataset to evaluate the EViTs. The COCO 2017 dataset contains 80 classes, 118k training images, 5k validation images and 20k test images. Two representative frameworks, RetinaNetand the Mask R-CNNare used to evaluate the performance of the EViTs. Specifically, the EViTs are used as backbone networks and then plugged into the RetinaNet and the Mask R-CNN frameworks. Before starting the training process, each backbone network is initialized by using the parameters determined during ImageNet-1k pretraining, whereas the other layers are randomly initialized. For fairness, the same settings as those of PVTv2are followed. AdamW is selected as the optimizer and the training schedule is set to 1 × 12 epochs. The weight decay rate and the initial learning rate are set to 0.05 and 0.0001, respectively.

Tableshows the performance comparison between the EViTs and other backbone networks in terms of object detection and instance segmentation tasks implemented on the COCO 2017 dataset. In the RetinaNet framework, the mean Average Precision (mAP), Average Precision at 50% and 75% Intersection over Union (IoU) thresholds (,), and average precision across three object sizes (small, medium, and large, denoted as,, and, respectively) are used as the evaluation metrics for model performance assessments. The numerical results show that the EViTs have significant competitive advantages over the other networks. Specifically, the average accuracies of EViT-Small and EViT-Base are at least 8% higher than those of ResNet-50 and ResNet-101, and they outperform the advanced PVTv2-B2 and PVTv2-B3 by 0.5% and 0.6%, respectively. In the Mask R-CNN framework, the bounding box Average Precision () and mask Average Precision () at the mean and different IoU thresholds (50%, 75%) are used as the evaluation metrics. These results also show that EViT-Small and EViT-Base significantly outperform the other networks. Specifically, in terms of theandmetrics, EViT-Small exceeds PVTv2-B2 by 0.7% and 0.5%, while EViT-Base exceeds PVTv2-B3 by 0.5% and 0.6% respectively.

SECTION: 
Semantic segmentation experiments are conducted for the EViTs on the ADE20Kdataset. The ADE20K dataset is widely used for semantic segmentation tasks and comprises 150 different semantic categories, with approximately 20K training images, 2K validation images, and 3K test images. To facilitate comparisons with other networks, the EViTs are used as backbones and integrated into the Semantic FPN, enabling us to evaluate the performance of the EViTs in semantic segmentation tasks. Specifically, the same parameter settings as those of PVTare followed. AdamW is chosen as the parameter optimizer, and the learning rate is set to 0.0001. The learning rate is decayed by following the polynomial decay schedule with a power parameter of 0.9, and the number of training iterations is 80k.

Tableshows the results of a performance comparison between the EViTs with other backbone networks in terms of a semantic segmentation task implemented on the ADE20Kdataset. In this experiment, the model parameters, computational complexity (FLOPs), and mean Intersection over Union (mIoU) are used as evaluation metrics. Specifically, EViT-Small and EViT-Base achieve mean IoUs (mIoUs) of 46.1% and 48.5%, respectively. Under similar parameter counts and FLOPs, the EViT-Small and EViT-Base outperform the LITv2and PVTv2approaches by at least 1.8% and 0.9%, respectively. Additionally, other advanced networks, such as FaViT-B3, CrossFormer-B, and ScalableViT-Sare subjected to a more comprehensive comparison. Compared with these networks, EViT-Base leads in segmentation performance by 1.3%, 0.8%, and 0.1%, respectively. The numerical results indicate that the EViTs demonstrate a significant competitive advantage over these networks in dense prediction tasks.

SECTION: 
In this section, other transfer learning experiments are conducted to evaluate the performance of the EViTs in different downstream vision tasks. These vision tasks consist of different application scenarios and datasets, including fine-grained visual classification (Standford Cars, Oxford-102 Flowersand Oxford-IIIT-Pets), long-tailed classification (iNaturalist18, iNaturalist19) and superordinate classification tasks (CIFAR-10, CIFAR-100). The details of these datasets are listed in Table.

Before beginning the training process, we use the parameters determined during ImageNet-1K pretraining to initialize the EViT backbones, while the other layers are randomly initialized. Tableshows the results of a performance comparison between the EViTs and other backbone networks in the aforementioned vision tasks. The numerical results indicate that the EViTs demonstrate highly competitive performance. In the particular, EViTs achieve comparable or even superior performance relative to that of EfficientNet-B5 and EfficientNet-B7 at a lower computational cost. This demonstrates the superiority and generality of the EViTs based on the bi-foveal eagle vision designed in this paper.

SECTION: 
In this section, the robustness of the EViTs is evaluated on the ImageNet-Cdataset, which consists of various types of image corruptions, categorized into noise, blur, weather, and digital. Each type of corruption has five severity levels, with varying intensities of degradation. The test results obtained on this benchmark are used to reflect the overall robustness of the developed networks. For performance evaluation purposes, the mean corruption error (mCE) is used as the metric. A smaller mCE indicates greater model robustness under corruptions.

Tableshows the experimental results produced by EViT-Small on the ImageNet-C benchmark dataset. Several convolutional neural networks and vision transformers used for comparison, where RVT is a specially designed vision transformer network that achieves high robustness. The numerical results show that the EViT-Small achieves similar performance to that of RVT-S in the robustness tests. It demonstrates good robustness under various types of corruptions and further validating the practicality of the proposed method. Although our focus is not on designing a robust visual transformer akin to RVT, several design principles derived from RVT are naturally incorporated into the EViTs, such as the convolutional stem, position embedding, pyramidal network architecture, and local information exchange in the feedforward network. These key designs ensure that the EViTs exhibit robust performance under various types of image corruption.

SECTION: 
In this section, the ablation experiments are conducted on the ImageNet-1Kdataset to demonstrate the effectiveness of the BFVI connection structure and the BFFN. Specifically, the ablation study uses EViT-Base as the baseline. The training strategy outlined in Section 4.1 is followed.

The Bi-Fovea Self-Attention (BFSA) and Bi-Fovea Feedforward Network (BFFN) are the main contributions of our work. They are the basic components used to building the EViTs, ensuring that these EViTs can achieve competitive performance in various vision tasks. We attribute the advantages of the EViTs to the unique visual structure derived from eagle vision, which is referred to as Bi-Foveal Visual Interaction (BFVI). Taking BFSA as an example, Figure(c) illustrates the unique connection approach employed by the Shallow Fovea Attention (SFA) and Deep Fovea Attention (DFA) in the BFSA via simplification. The BFSA does not simply connect the SFA and DFA in parallel or cascade, it is more similar to the combination of them. Therefore, the ablation studies are first conducted on BFVI to demonstrate the effectiveness of this connection approach.

In the implementation details, the Parallel BFSA and Cascade BFSA are implemented by connecting the SFA and DFA in parallel and in a cascaded manner, respectively. Similarly, a Parallel BFFN and a Cascaded BFFN are also constructed, and they are used to conduct a comparison with the proposed BFSA and BFFN. Tableshows the performance comparison conducted among three connection approaches for the BFSA and BFFN. Note that although these three connection modes have the same number of parameters and computational complexity, our BFVI connection approach achieves superior performance. Specifically, the BFSA outperforms the Parallel BFSA and Cascade BFSA by 1.4% and 0.5%, respectively, in terms of the Top-1 classification accuracy metric. The performance of the BFFN exceeds that of the Parallel BFFN and Cascade BFFN by 0.7% and 0.2%, respectively. This demonstrates that the BFVI combines the advantages of parallel and cascaded patterns, and can attain more competitive performance in vision tasks.

To demonstrate the effectiveness of the BFFN, an ablation experiment is conducted in this section. Specifically, the Feed Forward Network (FFN) obtained from the ViTand the Convolutional Feed Forward Network (CFFN) from PVTare selected as the controls. The BFFN is compared with the FFN and the CFFN, respectively. Tablepresents the performance comparison conducted between the BFFN, FFN and CFFN. The numerical results show that the BFFN outperforms the FFN and CFFN by 0.9% and 0.4%, respectively, while its computational cost remains negligible. This finding demonstrates that the BFFN more efficiently complements the local detail information contained in the feed forward network, which is critical for computer vision tasks.

SECTION: 
In this paper, a novel Bi-Fovea Self-Attention (BFSA) and Bi-Fovea Feedforward Network (BFFN) are proposed. Their core ideas are derived from the unique bi-fovea structure of eagle eyes, which is referred to as Bi-Foveal Visual Interaction (BFVI). The BFSA and BFFN can facilitate networks to model the global feature dependencies of images while extracting fine-grained feature representations of the targets. Additionally, a Bionic Eagle Vision (BEV) block is designed on the basis of the BFSA and BFFN. This BEV block combines the advantages of convolutions and transformers. Furthermore, a general family of pyramidal vision backbone networks called Eagle Vision Transformers (EViTs) is constructed by stacking BEV blocks. Experimental results show that the EViTs demonstrate excellent performance in image classification, object detection and semantic segmentation tasks. In the future, we will further explore the advantages of combining eagle vision with vision transformers and conduct studies on linear attention models to increase the applicability and interpretability of the EViTs.

SECTION: References