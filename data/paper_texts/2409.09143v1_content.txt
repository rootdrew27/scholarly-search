SECTION: DomURLs_BERT: Pre-trained BERT-based Model for Malicious Domains and URLs Detection and Classification

Detecting and classifying suspicious or malicious domain names and URLs is fundamental task in cybersecurity. To leverage such indicators of compromise, cybersecurity vendors and practitioners often maintain and update blacklists of known malicious domains and URLs. However, blacklists frequently fail to identify emerging and obfuscated threats. Over the past few decades, there has been significant interest in developing machine learning models that automatically detect malicious domains and URLs, addressing the limitations of blacklists maintenance and updates. In this paper, we introduce DomURLs_BERT, a pre-trained BERT-based encoder adapted for detecting and classifying suspicious/malicious domains and URLs. DomURLs_BERT is pre-trained using the Masked Language Modeling (MLM) objective on a large multilingual corpus of URLs, domain names, and Domain Generation Algorithms (DGA) dataset. In order to assess the performance of DomURLs_BERT, we have conducted experiments on several binary and multi-class classification tasks involving domain names and URLs, covering phishing, malware, DGA, and DNS tunneling. The evaluations results show that the proposed encoder outperforms state-of-the-art character-based deep learning models and cybersecurity-focused BERT models across multiple tasks and datasets. The pre-training dataset111https://hf.co/datasets/amahdaouy/Web_DomURLs, the pre-trained DomURLs_BERT222https://hf.co/amahdaouy/DomURLs_BERTencoder, and the experiments source code333https://github.com/AbdelkaderMH/DomURLs_BERTare publicly available.

SECTION: 1Introduction

Domain names and Uniform Resource Locators (URLs) are fundamental components in navigating and identifying resources on the Internet. Nevertheless, they are frequently exploited for various malicious activities in cyberspace, such as phishing campaigns, malware distribution, spam dissemination, and Command and Control (C&C) server operations, among others[1,2,3,4,5].Thus, detecting and flagging malicious domains and URLs is crucial for network security. Traditionally, cybersecurity vendors and practitioners rely on blacklists and heuristic methods to identify malicious domain names and URLs[6,7,8,9]. While blacklists are essential for blocking known threats, they are reactive by nature, posing challenges in maintenance and being vulnerable to evasion techniques. On the other hand, heuristic methods, which use patterns and behavioral analysis to identify potential threats, offer a more proactive approach to detection[7,10,11]. However, they are prone to false positives and require continuous updates to remain effective against evolving obfuscation tactics[4,5,10,12,13].

To overcome the limitations of blacklisting and heuristic-based methods, a growing body of research has focused on developing Machine Learning (ML) techniques for detecting malicious URLs and domain names[2,8,12]. The goal of these approaches is to automatically train models that can distinguish between legitimate and malicious threats based on data. Traditional ML-based techniques rely heavily on hand-engineered features, where the learning process involves identifying patterns in the data to guide the model’s decision-making. Consequently, numerous studies have proposed various feature sets for ML-based classification of malicious domains and URLs[2,4,9,11]. Although ML-based methods have demonstrated promising results across different domain name and URL classification tasks, the manual feature engineering process is both costly and time-consuming[6,7].

Recently, a considerable amount of literature has been published on the use of Deep Learning (DL) for detecting and classifying malicious domain names and URLs[5,6,7,13]. These studies leverage the representation learning capabilities of deep neural networks, which can automatically learn hierarchical features at different levels of abstraction from raw input data[14]. As a result, various neural network architectures have been explored. Typically, these architectures either use hand-engineered features or learn representations of characters, n-grams, and sub-words for classifying malicious domain names and URLs[6,13,15,16,17].

The introduction of the transformer architecture[18]has resulted in significant breakthroughs and advancements in Artificial Intelligence. Beyond natural language processing, transformers have been employed in various fields such as computer vision, data science, robotics, and cybersecurity[19,20,21,22]. Particularly, self-supervised pre-training of stacked transformer blocks—whether in encoder, decoder, or encoder-decoder configurations—has demonstrated state-of-the-art performance when fine-tuned for downstream tasks[23,24]. In line with the pretrain-finetune paradigm, researchers have proposed fine-tuning or adapting pre-trained Bidirectional Encoder Representations from Transformers (BERT)[25]for cybersecurity tasks[5,21,22,26,27,28,29,30,31,32]. Following the domain-adaptive pre-training approach, several BERT-based encoders have been pre-trained using the Masked Language Modeling (MLM) objective on domain-specific corpora for the classification of malicious and phishing URLs[22,29,33,31]. However, these models have not been explicitly pre-trained on both domain names and URLs, and much of the existing research has focused primarily on phishing URLs.

In this paper, we introduce DomURLs_BERT, a BERT-based encoder pre-trained on a large-scale corpus using the MLM objective. The pre-training corpus includes multilingual URLs, domain names, and Domain Generation Algorithms (DGA) datasets. Additionally, we propose a lightweight preprocessing method for the input data and train our model’s tokenizer from scratch using SentencePiece tokenization. To evaluate the performance of DomURLs_BERT in detecting malicious URLs and domain names, we conducted a comprehensive evaluation on a diverse set of datasets covering DGA, DNS tunneling techniques, malware classification, and phishing/malicious URL classification. The overall results show that our model outperforms six character-based deep learning models and four BERT-based models on multiple classification tasks. To summarize, the main contributions of this paper are as follows:

We introduce DomURLs_BERT, a specialized BERT-based encoder pre-trained on a large-scale multilingual corpus of URLs, domain names, and DGA datasets.

We propose a light preprocessing tailored to the characteristics of URLs and domain names, and train domain-specific tokenizer.

We evaluate DomURLs_BERT on various malicious URLs and domain names classification tasks, including DGA, DNS tunneling, malware classification, and phishing.

We conduct our experiments on both binary and multi-class classification tasks.

We compare our model with several state-of-the-art deep learning models, including character-based models and pre-trained cybersecurity BERT models.

The rest of the paper is organized as follows: Section2reviews related work in the field of malicious domain and URL detection. In Section3, we describe the proposed method for DomURLs_BERT pre-training. Sections4presents the experimental results. Finally, Section5concludes the paper and outlines potential directions for future research.

SECTION: 2Related Work

The field of natural language processing is currently undergoing a revolutionary transformation, driven by the advent of large pre-trained language models (PLMs) based on the groundbreaking Transformer architecture[18]. However, applying these models to domain-specific tasks poses challenges, as general models often fail to represent domain-specific terms and contexts not covered in their training data. To address this issue, domain-specific PLMs have been developed, such as BioBERT[34]for biomedical text and SciBERT[35]for scientific literature. Similarly, in the cybersecurity domain, several models based on the BERT architecture[25]have been created to capture domain-specific language and improve performance on cybersecurity-related tasks[22].

For instance, CyBERT[36]is a domain-specific variant of BERT pre-trained on a large cybersecurity corpus using MLM. It focuses on generating contextualized embeddings specifically designed for cybersecurity tasks like cyber threat intelligence and malware detection. In the same context, CySecBERT[21]is a domain-adapted version of BERT pre-trained on large cybersecurity corpora. It is designed to improve performance across multiple cybersecurity tasks, including classification and named entity recognition while addressing challenges like catastrophic forgetting during domain adaptation. CySecBERT has demonstrated superior performance compared to both BERT and CyBERT in several cybersecurity tasks. Additionally, SecureBERT[20], based on the RoBERTa architecture, incorporates continual pre-training with a specialized tokenizer and fine-tuned pre-trained weights to capture both general and cybersecurity-specific language. Evaluated on MLM and NER tasks, SecureBERT has shown promising results in comprehending cybersecurity text. On the other hand, SecBERT[19], developed from scratch, is trained on various cybersecurity corpora, such as "APTnotes" and "CASIE," and targets a broad range of cybersecurity data.

More recently, several models have focused on specific tasks within the cybersecurity domain. For example, MalBERT[37], a BERT-based model, is specialized in detecting malicious software. Similarly, Li et al.[29]introduced URLBERT, the first pre-trained model specifically designed for URL classification and detection tasks. URLBERT incorporates novel pre-training techniques, such as self-supervised contrastive learning and virtual adversarial training, to enhance its understanding of URL structures and robustness, achieving state-of-the-art results in phishing detection and web page classification. Motivated by the success of PLMs in cybersecurity tasks, we propose DomURLs_BERT, a specialized BERT-based encoder pre-trained on a large multilingual corpus of URLs, domain names, and DGA datasets. This paper contextualizes DomURLs_BERT by reviewing recent studies on the classification of malicious domain names and URLs. For a detailed review of existing large language models in cybersecurity, readers can refer to the recent study by Xu et al.[22].

SECTION: 2.1Malicious domain names classification

Detecting malicious domains, especially those generated by domain generation algorithms, is a crucial task in cybersecurity. Early work by Yadav et al.[1]laid the foundation by focusing on detecting algorithmically generated malicious domain names through linguistic analysis. Building on this, Cucchiarelli et al.[9]proposed using n-gram features, enhancing the ability to capture linguistic patterns in DGA-generated domains. Liew and Law[17]further advanced the field by introducing subword tokenization techniques for DGA classification, a method that allows more granular token analysis, improving model robustness against unseen domain variations. Shi et al.[8]explored machine learning techniques, particularly extreme machine learning, for detecting malicious domain names. This approach demonstrates the effectiveness of using machine learning models to identify domains that exhibit abnormal patterns. Tian et al.[32]introduced Dom-bert, a pre-trained model designed to detect malicious domains, leveraging contextual information embedded in domain names to enhance detection performance. In the broader context, Kang[3]reviewed various malicious domain detection techniques, while Hamroun et al.[11]focused specifically on lexical-based methods, emphasizing the importance of features derived from the domain names themselves. Together, these works underscore the importance of both lexical features and advanced machine-learning techniques in detecting DGA-generated and malicious domains.

SECTION: 2.2Malicious URLs classification

In the field of malicious URLs detection, machine learning and deep learning approaches have been extensively studied and developed, with significant advancements in recent years. These approaches can be broadly categorized into traditional machine learning methods, neural network-based methods, and transformer-based models. Traditional machine learning methods, which rely on manually engineered features, were initially prominent in malicious URL detection. Sahoo et al.[2]provided a comprehensive survey of these early efforts, highlighting how machine learning techniques such as support vector machines, decision trees, naive Bayes, and random forests were applied to extract statistical and lexical features from URLs. Similarly, Aljabri et al.[4]reviewed more recent methods and highlighted the shift toward deep learning techniques due to their ability to automate feature extraction and improve detection performance.

The shift towards deep learning led to the development of several promising models. In this context, Le et al.[6]proposed URLNet, a deep learning-based method that captures both character- and word-level representations of URLs to improve classification accuracy. Vazhayil et al.[15]performed a comparative study between shallow and deep networks, concluding that deep networks outperform traditional machine learning models by capturing more complex patterns in URLs. Afzal et al.[16]took this further by introducing Urldeepdetect, a deep learning model that integrates semantic vector models to enhance URL representation.

More recently, transformer-based models have emerged as a dominant approach, driven by their capacity to understand the semantic and contextual information of URLs. As previously mentioned, the BERT model and its variants have been particularly influential in this area. Chang et al.[26]and Otieno et al.[27]explored the application of BERT for URL detection, demonstrating that transformer models outperform traditional methods in terms of both accuracy and robustness. Building on these efforts, Su et al.[28]and Yu et al.[5]proposed modified BERT variants that further enhance semantic understanding for malicious URL detection. The introduction of URLBERT by Li et al.[29], a contrastive and adversarial pre-trained model, continues this trend, pushing the boundaries of transformer-based URL classification. Several other transformer-based models have also been proposed, focusing on improving phishing URL detection. For example, URLTran[33]applies transformers specifically to phishing detection, while Bozkir et al.[13]introduced GramBeddings, a neural network that utilizes n-gram embeddings to enhance the identification of phishing URLs. This direction was further extended by Liu et al.[30], who combined a pre-trained language model with multi-level feature attention for improved detection accuracy.

Overall, the progression from traditional machine learning approaches to advanced deep learning and transformer-based models has significantly improved the ability to classify malicious URLs. The integration of semantic understanding, n-gram embeddings, and pre-trained models has pushed the state-of-the-art, enabling more accurate and robust detection of malicious URLs across different attack types.

SECTION: 3Methodology

This section presents our methodology for pre-training the DomURLs_BERT encoder, focusing on the collection of pre-training data, preprocessing of domain names and URLs, tokenizer training, and domain-adaptive pre-training.

SECTION: 3.1Pre-training data

We have collected a large-scale pre-training corpus of domain names and URLs from the following datasets:

mC4: The multilingual colossal Common Crawl Corpus444https://hf.co/datasets/legacy-datasets/mc4. This is a cleaned version of the Common Crawl’s web corpus, curated by the Allen Institute for Artificial Intelligence[38], containing approximately 170 million URLs.

falcon-refinedweb: An English large-scale dataset curated for large language model pre-training. This dataset is compiled from CommonCrawl, using strict filtering and extensive deduplication[39], and contains around 128 million URLs555https://hf.co/datasets/tiiuae/falcon-refinedweb.

CBA Web tracking datasets: A dataset compiled by the Broadband Communications Systems and Architectures Research Group666https://cba.upc.edu/downloads/category/29-web-tracking-datasets#, containing 76M URLs and 1.5M domain names.

Tranco top 1M: is a dataset of top 1M domain names compiled and ranked by Tranco777https://tranco-list.eu/[40].

UTL_DGA22: A Domain Generation Algorithm botnet dataset, containing 4.3 million entries from 76 DGA families[12].

UMUDGA: A dataset for profiling DGA-based botnets, consisting of 30 million manually labeled DGA entries[41].

Since the pre-training dataset is curated from multiple sources, the data cleaning process includes deduplication based on exact matching. The final pre-training dataset contains 375,057,861 samples for model training and 19,739,888 samples for development. Table1provides details on the collected dataset, which is publicly available on Hugging Face Datasets888https://hf.co/datasets/amahdaouy/Web_DomURLs.

SECTION: 3.2Pre-training Procedure

A URL consists of several components, which can be grouped into three main parts: the scheme (protocol), the domain name, and the path. Figure1illustrates the overall structure of a URL (source999https://www.seoforgooglenews.com/p/everything-urls-news-publishers). Our proposed input preprocessing method involves removing the protocol identifier and splitting the URL into two parts: the domain name and the path. These two parts are delimited by special tokens, [DOMAIN] and [PATH], indicating the start of the domain name and the URL path, respectively. Additionally, if the input URL contains an IP address instead of a domain name, we use the [IP] and [IPv6] special tokens in place of [DOMAIN] for IPv4 and IPv6 addresses, respectively. Finally, the [CLS] and [SEP] tokens are appended to the start and end of the input URL or domain, as follows:

After cleaning and preprocessing the data, we trained our tokenizer from scratch using the SentencePiece tokenization method, which employs the Byte Pair Encoding (BPE) algorithm[42]. SentencePiece is language-agnostic and does not require any pre-tokenization, as it processes input as a sequence of Unicode characters. For tokenizer training, we utilized the HuggingFace tokenizers library101010https://github.com/huggingface/tokenizers. The vocabulary size was set to 32,000.

Domain-adaptive pre-training has been shown to enhance the contextualized word embeddings of existing domain-generic Pre-trained Language Models (PLMs)[43]. This improvement has also been demonstrated in cybersecurity applications, where several domain-adapted models have been proposed[20,21,29,33,31,22]. Following this trend, we continued the pre-training of the BERT-base encoder introduced in[25]. The model consists of approximately 110 million parameters, with 12 transformer layers, a hidden dimension size of 768, and 12 attention heads.

Pre-training is performed using the MLM objective on our dataset, following the guidelines of Devlin et al. (2019)[25], where 15% of the input sequence’s subwords are randomly selected for masking. The model is trained to minimize the cross-entropy loss between the predicted sequence and the original sequence. We use the HuggingFace transformers111111https://github.com/huggingface/transformerslibrary for training on a server equipped with 4 NVIDIA A100 GPUs, each with 80GB of RAM. The maximum sequence length, per-device batch size, and learning rate are set to 128, 768, and, respectively. The model is trained for 260,000 steps. Our pre-trained model is publicly available on HuggingFace Models121212https://hf.co/amahdaouy/DomURLs_BERT.

SECTION: 4Experiments and Results

In this section, we present the evaluation datasets, the deep learning models used for comparison, the experimental settings, and the evaluation metrics. We then discuss and analyze the obtained results.

SECTION: 4.1Evaluation Datasets

To evaluate the effectiveness of our model, we employed several domain name and URL classification datasets. For malicious domain name classification, we used theDNS Tunnelingdataset[44],UMUDGA[41], andUTL_DGA22[12]. Additionally, we collected a malware domain names dataset,ThreatFox_MalDom, from the ThreatFox131313https://threatfox.abuse.ch/database in June 2024. For legitimate domain names, we used the Tranco list.

For malicious URL classification, we utilized several datasets, includingMendeley AK Singh[45],Kaggle Malicious URLs[46],Grambedding[47],LNU_Phish[48],PhiUSIIL[49], andPhishCrawl[50]. We also curated a malware URL dataset,ThreatFox_MalURLs, from the ThreatFox database in June 2024. For legitimate URLs, we employed the benign URLs from theKaggle Malicious URLsdataset[46].

All the used datasets have been divided into 60%, 20%, and 20% for training, validation, and testing, respectively. Table2summarizes the characteristics of the evaluation datasets.

SECTION: 4.2Comparison methods

We compared our model with several state-of-the-art deep learning models, including six character-based RNN and CNN models, as described below:

CharCNN: This model employs an embedding layer followed by three one-dimensional convolutional layers with kernel sizes of 3, 4, and 5, respectively. The final convolutional layer is followed by a dropout layer and a classification layer.

CharGRU: This model uses an embedding layer and multiple GRU layers. The last GRU layer is followed by a dropout layer and a classification layer.

CharLSTM: This model utilizes an embedding layer and multiple LSTM layers. The final LSTM layer is followed by a dropout layer and a classification layer.

CharBiGRU: This model uses an embedding layer and multiple bidirectional GRU layers. The last BiGRU layer is followed by a dropout layer and a classification layer.

CharBiLSTM: This model employs an embedding layer and multiple bidirectional LSTM layers. The final BiLSTM layer is followed by a dropout layer and a classification layer.

CharCNNBiLSTM: This model uses CNN layers to extract local features from character embeddings, which are then passed into a BiLSTM layer to capture contextual dependencies.

Moreover, we compared our model with five state-of-the-art domain-generic and domain-specific BERT-based PLMs, includingBERT[25],SecBERT[19],SecureBERT[20],CySecBERT[21], andURLBERT[29].

SECTION: 4.3Experiments settings

We implemented our model and the other state-of-the-art models using Pytorch141414https://pytorch.org/deep learning framework, Lightning151515https://lightning.ai/, and HuggingFace transformers161616https://github.com/huggingface/transformerslibrary. All our experiments have been conducted on a Dell PowerEdge XE8545 server, having 4 NVIDIA A100-SXM4-80GB GPUs, 1000 GiB RAM, and 2 AMD EPYC 7713 64-Core Processor 1.9GHz.

All models are trained using AdamW optimizer[51]. We used a batch size of 128 and the maximum sequence length is fixed to 128 and 64 for URLs and domain names, respectively. For character-based deep learning models, the number of epochs, the learning rate, the weight decay, the number of RNN layers, the hidden dimensions size are fixed to,,, 3, 128, respectively. For BERT-based models, the number of epochs, the learning rate, and the weight decay are fixed to,,, respectively. For all models, weight decay is applied to all the layers weights except biases and Layer Normalization. For all models and dataset, we utilized the following performance measures:

Accuracy:is The proportion of all correct predictions (both true positives and true negatives) out of all predictions.

True Positive Rate (TPR) / Recall / Sensitivity/ Detection Rate:is the proportion of actual positives that are correctly identified by the model.

Specificity (SPC) / True Negative Rate:is the proportion of actual negatives that are correctly identified by the model.

Positive Predictive Value (PPV) / Precision:is the proportion of predicted positives that are actually positive.

Negative Predictive Value (NPV):is the proportion of predicted negatives that are actually negative.

F1 score:is the harmonic mean of precision and recall.

Weighted F1 Score (F1_wted):whereis the proportion of the total number of samples belonging to class, andis the F1 score for class. This score considers each class’s importance by weighting their respective F1 scores according to their frequency.

Micro F1 Score (F1_mic):aggregates the contributions of all classes to compute precision and recall, treating all instances equally, regardless of the class.

Diagnostic Efficiency (DE):is the product of sensitivity (TPR) and specificity (SPC). Indicates the overall diagnostic ability of the test. Higher values indicate better performance.

False Positive Rate (FPR):is the proportion of actual negatives that are incorrectly identified as positive by the model. Lower values indicate better performance.

False Discovery Rate (FDR):is the proportion of predicted positives that are actually negative. Lower values indicate better performance.

False Negative Rate (FNR):is the proportion of actual positives that are incorrectly identified as negative by the model. Lower values indicate better performance.

For all evaluation measures, we report the macro-average performances (except F1_wted, F1_mic, and Accuracy).

SECTION: 4.4Results

In this section, we present the results of our model alongside state-of-the-art character-based and BERT-based models. All evaluated models are compared on both binary and multi-class classification tasks for domain names and URLs.

Table3summarizes the obtained results for binary classification of domain names. The aim of this task is to detect malicious domain names and DNS tunneling. The overall results show that DomURLs_BERT achieves the best performance across all datasets for most evaluation metrics. However, on the DNS tunneling dataset, the CharCNN model outperforms in Specificity (SPC) and diagnostic efficiency (DE). Additionally, the results indicate that fine-tuning BERT and cybersecurity-specific BERT-based models (BERT, SecureBERT, and CySecBERT) outperforms character-based deep learning models in most datasets and metrics.

Table4presents the obtained results for domain names multi-class classification tasks. The aim of these tasks is to classify domain names into a set of predefined class labels. The overall results show that DomURLs_BERT model outperform the other state-of-the-art models domain generation algorithm classification datasets on most evaluation measures. Nevertheless, the CharBiLSTM achieves better F1_macro and precision (PPV) on the UTL_DGA22 dataset. For malware domain names classification (ThreathFox_MalDomains dataset), DomURLs_BERT model yields better Accuracy, F1_wted, F1_mic, NPV, specificity (SPC), and false positive rate (FPR). However, the best F1_macro, precision (PPV), diagnostic efficiency (DE), recall (TPR) and false negative rate (FNR) are achieved by CharCNN, CharCNN, BERT, and
CharBiGRU, respectively. For DNS tunneling, all models achieve nearly similar performances. However, the best results are obtained using CharBiGRU, CharGRU, and SecureBERT models.

Tables5and6summarizes the obtained results for URLs binary classification tasks (malicious URLs detection). The overall obtained results show that DomURLs_BERT outperforms the state-of-the-art character-based models and
the evaluated BERT-based models on Grambedding, PhishCrawl, and kaggle malicious urls datasets. Besides, it achieves comparable or nearly similar performances on LNU_Phish, Mendely AK Singh, and PhiUSIIL datasets. For LNU_Phish, the top performances are achieved by CharGRU, CySecBERT, SecBERT, SecureBERT, BERT, and DomUrlsBERT models. For Mendely AK Singh dataset, DomURLs_BERT yields the best accuracy, F1_macro, F1_wted, and F1_micro. Whereas, SecureBERT obtains better recall (TPR), specificity (SPC), diagnostic efficiency (DE), false positive rate (FPR) and false negative rate (FNR) performances. Additionally, the CharCNN model yields the best precision (PPV), NPV, and false detection rate (FDR). For PhiUSIIL dataset, CySecBERT achieves the best overall performances, while the other models,including DomURLs_BERT, obtain comparable results. For ThreatFox_MalURLs dataset, all models yield nearly perfect performances, while the top results are achieved by CySecBERT, SecureBERT, and BERT models. In accordance with domain names classification tasks, BERT model and the other cybersecurity BERT-based models obtain state-of-the-art perfornaces on all datasets.

Table7summarizes the obtained results for malicious URLs multi-class classification. The overall obtained results show that DomURLs_BERT achieve comparable results to the best performing models (SecureBERT and CySecBERT). For kaggle malicious urls dataset, DomURLs_BERT yields slightly better Accuracy, F1_wted, F1_micro, NPV, specificity (SPC), and false positive rate (FPR) performances, while SecureBERT obtains slightly better F1_macro, recall (TPR), precison, diagnostic efficiency (DE), false detection rate (FDR), and false negative rate (FNR). Although CySecBERT outperforms all evaluated models on the ThreatFox_MalURLs dataset, DomURLs_BERT, BERT, SecureBERT also demonstrate performances that are closely comparable. In accordance with the previously reported results, most BERT-based models, especially those adapted to cybersecurity domain, yields state-of-the-art performances on malicious URLs classification tasks.

SECTION: 5Conclusion

In this work, we introduced DomURLs_BERT, a novel state-of-the-art pre-trained language model for detecting and classifying malicious or suspicious domain names and URLs. DomURLs_BERT is pre-trained using the masked language modeling objective on a large-scale, multilingual corpus comprising URLs, domain names, and domain generation algorithm datasets. We presented a detailed methodology for data collection, preprocessing, and domain-adaptive pre-training.

To evaluate the performance of our model, we conducted experiments on several binary and multi-class classification tasks related to domain names and URLs, including phishing and malware detection, DGA identification, and DNS tunneling. Our results demonstrate that DomURLs_BERT achieves state-of-the-art performance across multiple datasets. Furthermore, the findings highlight that character-based deep learning models, such as RNNs, CNNs, and their combinations, serve as strong end-to-end baselines for URL and domain name classification. In comparison, fine-tuning a domain-generic pre-trained BERT model and adapting BERT-based models to the cybersecurity domain consistently outperforms the baseline character-based models on most benchmark datasets. Future work includes evaluating the model performance on other domain names and URLs classification tasks and exploring robust fine-tuning approaches for dealing with adversarial attacks.

SECTION: References