SECTION: DeiSAM: Segment Anything with Deictic Prompting

Large-scale, pre-trained neural networks have demonstrated strong capabilities in various tasks, including zero-shot image segmentation.
To identify concrete objects in complex scenes, humans instinctively rely ondeicticdescriptions in natural language,i.e., referring to something depending on the context, such as “The object that is on the desk and behind the cup”. However, deep learning approaches cannot reliably interpret such deictic representations as they have limited reasoning capabilities, particularly in complex scenarios.
Therefore, we propose DeiSAM—a combination of large pre-trained neural networks with differentiable logic reasoners—for deictic promptable segmentation.
Given a complex, textual segmentation description, DeiSAM leverages Large Language Models (LLMs) to generate first-order logic rules and performs differentiable forward reasoning on generated scene graphs. Subsequently, DeiSAM segments objects by matching them to the logically inferred image regions.
As part of our evaluation, we propose the Deictic Visual Genome (DeiVG) dataset, containing paired visual input and complex, deictic textual prompts.
Our empirical results demonstrate that DeiSAM is a substantial improvement over purely data-driven baselines for deictic promptable segmentation.

SECTION: 1Introduction

Recently, large-scale neural networks have substantially advanced various tasks at the intersection of vision and language. One such challenge is grounded image segmentation, wherein objects within a scene are identified through textual descriptions. For instance, Grounding Dino(Liu et al.,2023c), combined with the Segment Anything Model (SAM)(Kirillov et al.,2023), excels at this task if provided with appropriate prompts.
However, a well-documented limitation of data-driven neural approaches is their lack of reasoning capabilities(Shi et al.,2023; Huang et al.,2024).
Consequently, they often fail to understand complex prompts that require high-level reasoning on relations and attributes of multiple objects, as demonstrated in Fig.1.

In contrast, humans identify objects through structured descriptions of complex scenes referring to an object,e.g., “An object that is on the boat and holding an umbrella”. These descriptions are referred to asdeictic representationsand were introduced to artificial intelligence research motivated by linguistics(Agre & Chapman,1987), and
subsequently applied in reinforcement learning(Finney et al.,2002).
A deictic expression refers to an object depending on the agent using it and the overall context.
Although deictic representations play a central role in human comprehension of scenes,
current approaches fail to interpret them faithfully due to their poor reasoning capabilities.

To remedy these issues, we propose DeiSAM, which is a
combination of large pre-trained neural networks
with differentiable logic reasoners for deictic
promptable object detection and segmentation.
The DeiSAM pipeline is highly modular and fully differentiable, sophisticatedly integrating large pre-trained networks and neuro-symbolic reasoners.
Specifically, we leverage Large Language Models (LLMs) to generate logic rules for a given deictic prompt and perform differentiable forward reasoning(Shindo et al.,2023,2024)with scene graph generators(Zellers et al.,2018). The reasoner is efficiently combined with neural networks by leveraging forward propagation on computational graphs. The result of this reasoning step is used to ground a segmentation model that reliably identifies the objects best matching the input.

In summary, we make the following contributions:
1) We propose DeiSAM111Code:https://github.com/ml-research/deictic-segment-anything, a modular, neuro-symbolic framework using LLMs and scene graphs for object segmentation with complex textual prompts.
2) We introduce a novel Deictic Visual Genome (DeiVG) benchmark that contains visual scenes paired with deictic representations,i.e., complex textual identifications of objects in the scene.
To further investigate the challenging nature of abstract prompts, we curate a new DeiRefCOCO+ benchmark. It is a deictic variant of RefCOCO+, an established reference object detection benchmark.
3) We empirically demonstrate that DeiSAM strongly outperforms neural baselines for deictic segmentation.
4) We showcase that DeiSAM can perform end-to-end training via differentiable reasoning to improve the segmentation quality adapting to complex downstream reasoning tasks.

SECTION: 2Related Work

Multi-modal Large Language Models.The recent achievements of large language models (LLMs)(Brown et al.,2020)have led to the development of multi-modal models, including vision-language models(Radford et al.,2021; Alayrac et al.,2022; Li et al.,2022a; Liu et al.,2023b),
which take visual and textual inputs.
However, these large models’ reasoning capabilities are limited(Huang et al.,2024), often inferring wrong conclusions when confronted with complex reasoning tasks. DeiSAM addresses these issues by combining large models with (differentiable) reasoners.

Additionally, DeiSAM is related to prior work using LLMs for program generation.
For example, LLMs have been applied to generate probabilistic programs(Wong et al.,2023), Answer Set Programs(Ishay et al.,2023; Yang et al.,2023), and programs for visual reasoning(Surís et al.,2023; Stanić et al.,2024).
These works have demonstrated that LLMs are powerful program generators and outperform simple zero-shot reasoning.
With DeiSAM we propose the usage of LLMs to generate differentiable logic programs for image segmentation and object detection.

Scene Graph Generation.Scene Graph Generators (SGGs) encode complex visual relations to a summary graph using the comprehensive contextual knowledge of relation encoders(Lu et al.,2016; Zellers et al.,2018; Tang et al.,2019). Recently, the focus has shifted to transformer-based SGGs that use attention to capture global context while improving visual and semantic fusion(Lin et al.,2020; Lu et al.,2021; Dong et al.,2022). Lately, attention has also been used to capture object-level relation cues using visual and geometric features(Sudhakaran et al.,2023).
The modularity of DeiSAM allows for using any SGG to obtain graph representations of input visual scenes.
Scene graphs are essential for segmentation models to be faithful reasoners. Without them, models may develop shortcuts, resulting in apparent answers through flawed scene understanding(Marconato et al.,2023).

Visual Reasoning and Segmentation.Visual Reasoning has been a fundamental problem in machine learning research, resulting in multiple benchmarks(Antol et al.,2015; Johnson et al.,2017; Yi et al.,2020)to address this topic
and subsequent frameworks(Yi et al.,2018; Mao et al.,2019; Amizadeh et al.,2020; Hsu et al.,2023)that perform reasoning using symbolic programs and multi-modal transformers(Tan & Bansal,2019).
These benchmarks are primarily developed to answer queries written in natural language texts paired with visual inputs. Our proposed dataset, DeiVG, is the first to integrate complex textual prompts into the task of image segmentation with natural images.
In a similar vein, to tackle visual reasoning tasks,
neuro-symbolic rule learning frameworks have been proposed, where discrete rule structures are learned via backpropagation(Evans & Grefenstette,2018; Minervini et al.,2020; Shindo et al.,2021,2023,2024; Zimmer et al.,2023).
These works have primarily been tested on visual arithmetic tasks and dedicated synthetic environments for reasoning(Stammer et al.,2021).
DeiSAM is a unique neuro-symbolic framework that addresses image segmentation in natural images and utilizes differentiable reasoning for program learning.

Semantic segmentation aims to generate objects’ segmentation masks given visual input(Wang et al.,2018; Guo et al.,2018).
Multiple datasets and tasks have been proposed that assess a model’s reasoning ability to identify objects(Kazemzadeh et al.,2014; Yu et al.,2016).
Recently, Segment Anything Model (SAM)(Kirillov et al.,2023)has been released,
achieving strong results on zero-shot image segmentation tasks.
Grounded SAM(Ren et al.,2024)combines Grounding DINO(Liu et al.,2023c)with SAM, allowing for objects described by textual prompts.
Moreover, LISA(Lai et al.,2023)fine-tunes multi-modal LLMs to perform low-level reasoning over image segmentation. However, LISA still requires strong prior information on the type target object (e.g.“thepersonthat is wearing green shoes”) and breaks down for more abstract tasks (cf.Sec.5.5).
In contrast, DeiSAM encodes the reasoning process explicitly as a differentiable function, thus avoiding spurious neural networks’ behavior.
Consequently, DeiSAM is capable of high-level reasoning on arbitrarily abstract prompts (e.g.“anobject”) utilizing structured representation of scene graphs.
To this end, frameworks that enhance the transformer (or attention) architecture for various segmentation tasks have been proposed(Liu et al.,2023a; Wu et al.,2024a,b).
These approaches rely on transformers (or attentions) as their core reasoning pipeline. In contrast, DeiSAM explicitly encodes logical reasoning processes to guarantee accurate and faithful interpretation of abstract and complex prompts.

SECTION: 3DeiSAM — The Deictic Segment Anything Model

DeiSAM uses first-order logic as its language, and we provide its formal definition in App.A. Let us start by outlining the DeiSAM pipeline with a brief overview of its modules, before describing essential components in more detail.

SECTION: 3.1Overview: Deictic Segmentation

We show a schematic overview of the proposed DeiSAM workflow in Fig.2.
First, an input image is transferred into a graphical representation using a(1) Scene Graph Generator.
Specifically, a scene graph comprises a set of triplets, where entitiesandhave relation.
For example, aperson() isholding() anumbrella().
Consequently, each tripletin a scene graph can be interpreted as a fact,, whereis a 2-ary predicate andandare constants in first-order logic.
The textual deictic prompt needs to be interpreted as a structured logical expression to perform reasoning on these facts.

For this step, DeiSAM leverages(2) Large Language Models, which can generate logic rules for deictic descriptions, given sufficiently restrictive prompts as we demonstrate.
In our example, the LLM would translate “An object that is on the boat, and that is holding an umbrella” into the rules (Program 1) in ListingLABEL:lst:program1.
The first two rules define the conditions described in the prompt, and the last rule identifies corresponding objects.
However, users often use terminology different from that of the SGG,e.g.,boatandbargetarget the same concept but will not be trivially matched. To bridge the semantic gap, we introduce a(3) semantic unifier. This module leverages word embeddings of labels, entities, and relations in the generated scene graphs and rules to match synonymous terms by modifying rules accordingly.
The semantically unified rules are then compiled to a(4) forward reasoner, which computes logical entailment using forward chaining(Shindo et al.,2023).
The reasoner identifies the targeted objects and their bounding boxes from the scene graph. Lastly, we segment the object by feeding the cropped images to a(5) segmentation model.

Now, let us investigate the two core modules of DeiSAM in detail:
rule generation and reasoning.

SECTION: 3.2LLMs as Logic Generators

To perform reasoning on textual prompts, we need to identify corresponding rules.
We use LLMs to parse textual descriptions to logic rules using the system prompt specifying the rule format to be generated. The complete prompt is provided in App.B.
DeiSAM uses a specific rule format describing object and attribute relations.
For example, a facton(person,boat)in a scene graph would be decomposed into multiple factson(X,Y),type(X,person), andtype(Y,boat)to account for several entities with the same attribute in the scene.

The computational and memory cost of forward reasoning is determined by the number of variables over all rules and the number of conditions. Naive formatting of rules(Shindo et al.,2024)leads to an exponential resource increase with the growing complexity of deictic prompts. Since the representations used in the forward reasoner are pre-computed and kept in memory, non-optimized approaches will quickly lead to exhaustive memory consumption(Evans & Grefenstette,2018).
In our format, however, we restrict the used variables toandand only increase the number of rules with growing prompt complexity. Thus resulting inlinearscaling of computational costs instead.

SECTION: 3.3Reasoning with Deictic Prompting

DeiSAM performs differentiable forward reasoning as follows.
We build a reasoning functionwhereis a set of facts representing a scene graph,is a set of rules generated by an LLM, andis a set of facts representing identified target objects in the scene.

(Differentiable) Forward Reasoning.For a visual input, DeiSAM utilizes scene graph generators(Zellers et al.,2018)to obtain a logical graph representation, where each factrepresents an edge in the scene graph.
Each fact in a given setis mapped to a confidence score using avaluation vector.
A SGG is a functionthat produces a valuation vector out of a visual input.
DeiSAM builds on the neuro-symbolic message-passing reasoner (NEUMANN)(Shindo et al.,2024)to perform reasoning. For a given set of rules, DeiSAM constructs aforward reasoning graph, which is a bi-directional graph representation of a logic program.
Given an initial valuation vector produced by an SGG, DeiSAM computes logical consequences in a differentiable manner
by performing bi-directional message passing on the constructed reasoning graph using soft-logic operations (cf.App.A.1). DeiSAM identifies target objects to be segmented using confidence scores over facts representing targets,e.g.,target(obj1), and extracts the corresponding bounding boxes from the scene graph.

Semantic Unifier.DeiSAM unifies diverging semantics in the generated rules and scene graph
using concept embeddings similar to neural theorem provers(Rocktäschel & Riedel,2017).
We rewrite the corresponding rulesof a prompt by identifying the most similar terms in the scene graph for each predicate and constant.
If rulecontains a term, which does not appear in scene graph, we compute the most similar term aswhereis an embedding model for texts.
We apply this procedure to terms and predicates individually.

SECTION: 4The Deictic Visual Genome

To facilitate a thorough evaluation of the novel deictic object segmentation tasks, we introduce the Deictic Visual Genome (DeiVG) dataset. Building on Visual Genome(Krishna et al.,2017), we construct pairs of deictic prompts and corresponding object annotations for real-world images, as shown in Fig.3.
Our analysis of the scene graphs in Visual Genome found the annotations to often be noisy and ambiguous, which aligns with observations from previous research(Hudson & Manning,2019). Consequently, we substantially filtered and cleaned potential candidates to produce a sound dataset.

First, we restricted ourselves to 19 commonly occurring relations and ensured that prompts were unambiguous, with only one kind of target object satisfying the prompt. Specifically, DeiVG contains prompts requiring the correct identification of multiple objects, but these are guaranteed to be the same type according to Visual Genomes synset annotations. We automatically synthesize prompts from the filtered scene graphs using textual templates,e.g., the relationshas(cooler,handle)andon(cooler,bench)would yield a prompt “An object that has a handle and that is on a bench” targeting the cooler.
Entries in the DeiVG dataset can be categorized by the number of relations they use in their object description. We introduce three subsets with 1-3 relations, which we denote as DeiVG1, DeiVG2, and DeiVG3, respectively. Each dataset is distinct,e.g., DeiVG2contains only prompts using 2 relations. For each set, we randomly select 10k samples that we make publicly available to encourage further research.

SECTION: 5Experimental Evaluation

With the methodology of DeiSAM and our novel evaluation benchmark DeiVG established, we now provide empirical and qualitative experiments.
Our results outline DeiSAM’s benefits over purely neural approaches, supplemented by ablation studies of each module.
Additionally, we investigate RefCOCO(Yu et al.,2016), a low-level reasoning benchmark for segmentation tasks, and demonstrate the robustness of DeiSAM for abstract prompts.
Lastly, we show that DeiSAM is end-to-end trainable and can thus be leveraged to improve the performance of the neural components in the pipeline.

SECTION: 5.1Experimental Setup

We base our experiments on the three subsets of DeiVG.
As an evaluation metric, we use mean average precision (mAP) over objects. Since the object segmentation quality largely depends on the used segmentation model, we focus on assessing the object identification preceding the segmentation step. The default DeiSAM configuration for the subsequent experiments uses the ground truth scene graphs from Visual Genome(Krishna et al.,2017),gpt-3.5-turbo222https://openai.com/blog/introducing-chatgpt-and-whisper-apisas LLM for rule generation,ada-002333https://openai.com/blog/new-and-improved-embedding-modelas embedding model for semantic unification, and SAM(Kirillov et al.,2023)for object segmentation. Additionally, we provide few-shot examples of deictic prompts and paired rules in the input context of the LLM, which improves performance (cf.App.E). We present detailed ablations on each component of the DeiSAM pipeline in Sec.5.4.

We compare DeiSAM to multiple purely neural approaches, both empirically as well as qualitatively. We include three baselines that use one model for object identification and subsequently segment the grounded image using SAM(Kirillov et al.,2023), similar to the grounding in DeiSAM.
Our comparison includes the following models for visual grounding: 1) One-For-All (OFA)(Wang et al.,2022), a unified transformer-based sequence-to-sequence model for vision and language tasks of which we use a dedicated visual grounding checkpoint444https://modelscope.cn/models/damo/ofa_visual-grounding_refcoco_large_en/summary. 2) Grounded Language-Image Pre-training (GLIP)(Li et al.,2022b)a model for specifically designed for object-aware and semantically-rich object detection and grounding. 3) GroundingDino(Liu et al.,2023c)an open-set object detector combining transformer-based detection with grounded pre-training.
Moreover, we compare to an end-to-end semantic segmentation model supporting textual prompts withSEEM(Zou et al.,2023).
Lastly, we compare to LISA(Lai et al.,2023), a state-of-the-art neural reasoning segmentation model.

SECTION: 5.2Empirical Evidence

The results on DeiVG of all baselines compared to DeiSAM are summarized in Tab.1.
DeiSAM clearly outperforms all purely neural approaches by a large margin on all splits of DeiVG. The performance of most methods improves with more descriptive deictic prompts,i.e., more relations being used. We attribute this effect to two distinct causes. For one,
additional information describing the target object contributes to higher accuracy in object detection. On the other hand, DeiVG1contains significantly more samples with multiple target objects than DeiVG2or DeiVG3. Consequently, cases in which a method identifies only one out of multiple objects will have a higher impact on the overall performance.
Overall, the large gap between DeiSAM and all baselines highlights the lack of complex reasoning capabilities in prevalent models and DeiSAM’s large advantage.
We further provide a runtime comparison and its analysis in App.F, showcasing that DeiSAM’s runtime is comparable to the baselines, and the bottleneck is in the LLMs, not in the reasoning pipeline.

SECTION: 5.3Qualitative Evaluation

After empirically demonstrating DeiSAM’s capabilities, we look into some qualitative examples. In Fig.4, we demonstrate the efficacy of the semantic unifier. All examples use terminology in the deictic prompt diverging from the scene graph entity names. Nonetheless, the unification step successfully maps synonymous terms and still produces the correct segmentation masks, overcoming the limitation of off-the-shelf symbolic logic reasoners.

In Fig.5, we further compare DeiSAM with the purely neural baselines. DeiSAM produces the correct segmentation mask even for complicated shapes (e.g., partially occluded cable) or complex scenarios (e.g., multiple people, only some holding umbrellas). All baseline methods, however, regularly fail to identify the correct object.
A common failure mode is confounding nouns in the deictic prompt. For example, when describing an object in relation to a ‘boat’, the boat itself is identified instead of the target object.
These examples strongly illustrate the improvements of DeiSAM over the pure neural approach on abstract reasoning tasks.

SECTION: 5.4Ablations

The modular nature of the DeiSAM pipeline enables easy component variations. Next, we investigate the performance of key modules in isolation and their overall influence on the pipeline.

LLM Rule Generation.One of the key steps for DeiSAM is the translation of deictic prompts posed in natural language into syntactically and semantically sound logic rules. We observed that the performance of instruction-tuned LLMs on this task heavily depends on the employed prompting technique. Consequently, we leverage the well-known methods of few-shot prompting(Brown et al.,2020)and chain-of-thought (CoT)(Wei et al.,2022).

To that end, we first let the model extract all predicates from a deictic prompt, which we subsequently provide as additional context for the rule generation. For both cases, we provide multiple few-shot examples.We evaluate all prompting approaches with LLama-2-13B(Touvron et al.,2023)in Tab.2. Clearly, few-shot examples are imperative to perform rule generation successfully.
Additionally, CoT for predicate decomposition further improves the rule generation for complex prompts.

With the best prompting technique identified, we additionally evaluated multiple open and closed-source language models of different sizes (cf.App.F).
In general, all instruction-tuned models can generate logic rules from deictic prompts. However, larger models strongly outperform smaller ones, especially for more complex inputs.
The overall best-performing model was gpt-3.5-turbo producing correct rules for DeiVG forof all samples.

Semantic Unification.Next, we take a more detailed look into the semantic unification module. At this step, we bridge the semantic gap between differing formulations in the deictic prompt and the scene graph generator. To evaluate this task, we created an exemplary benchmark based on synonyms in the visual genome. For 2.5k scenes in DeiVG, we considered all objects in the scene graph and identified one object name that differed from its synset entry. Based on that synonym, the task is to identify the one, unique, synonymous object in the scene. For example, in an image containing a ‘table’, ‘couch’, ‘chair’, and ‘cupboard’ the query ’sofa’ should identify the ’couch’ as most likely synoynm.
Overall, the task is considerably more challenging than it may appear at first glance, with the best model only achieving a success rate of 72% (cf.AppF). We observed, for example, the query ‘sofa’ is matched with ‘pillow’ instead of the targeted ‘couch’ or ‘trousers’ with ‘jacket’ instead of ‘pants’. These results motivate further research into the semantic unification process.

SECTION: 5.5Solving Reference Expression

In addition to experiments on DeiVG, we also consider the RefCOCO dataset(Yu et al.,2016), which comprises referring expressions for object segmentation. Thus, this dataset is the most similar setup to the deictic segmentation task in prior work.
The key difference between RefCOCO and DeiVG is that the latter is built to evaluate models’abstractreasoning capabilities in complex visual scenes. In contrast, RefCOCO mainly evaluates descriptive object identifications,i.e., objects with names and properties,e.g.“old man or child in green short”, compared toe.g.“an object on a table and next to a computer” in DeiVG.
Consequently, deictic prompts are more challenging than the reference texts in RefCOCO, since DeiVG prompts donotinclude explicit names and properties of target objects.

Since there were no publicly available scene graphs (or SGGs) for MSCOCO images, we used GPT3 to convert the reference text to a structured scene graph representation with additional annotations.
Tab.4shows the mAP of LISA, GroudnedSAM, and DeiSAM on RefCOCO.
LISA achieved better overall performances than GroundedSAM, showing its strong capability on the reference task. DeiSAM, however, remains competitive with LISA and achieves better results on all splits.

To further investigate the challenging nature of abstract prompts, we curated a DeiRefCOCO+ benchmark that contains more abstract textual references.
Specifically, we turned the reference texts in RefCOCO+ into deictic prompts by removing any description of the target object.
For example, the prompt “kid wearing navy shirt” is modified to “an object that is wearing navy shirt”.
Tab.4again shows the mAP of DeiSAM, GroundedSAM, and LISA on the modified DeiRefCOCO+ dataset. Importantly, DeiSAM retains a similar performance on both types of prompt formulations. In comparison, we observe a strong drop in performance for GroundedSAM and LISA with the absence of confounding object descriptions. These results further highlight the strength and abstraction level of the performed reasoning performed by DeiSAM555In App.F, we demonstrate experiments on DeiVG with prompts in the RefCOCO’s reference format, highlighting the robustness of DeiSAM against neural baselines..

SECTION: 5.6DeiCLEVR – Abstract Reasoning Segmentation

DeiSAM excels in high-level abstract reasoning, where purely neural pipelines often struggle. To demonstrate, we developed DeiCLEVR, an abstract reasoning segmentation task based on CLEVR(Johnson et al.,2017). This task challenges models with abstract concepts and relationships.

Task.The task is to segment objects given prompts where the answers are derived by the reasoning over abstract list operations. We consider 2 operations: delete and sort. The input is a pair of an image and a prompt, e.g. “Segment the second left-most object after deleting a gray object?”. Examples are shown in Fig.6. To solve this task, models need to understand the visual scenes and perform high-level abstract reasoning to segment.

Dataset.(Image)Each scene contains at most 3 objects with different attributes: (i) colors of cyan, gray, red, and yellow, (ii) shapes of sphere, cube, and cylinder, (iii) materials of metal and matte. We excluded color duplications in a single image.(Prompts)We generated prompts using a templates: “The [Position] object after [Operation]?”, where [Position] can take either of: left-most first, second, or third. [Operation] can take either of: (I) delete an object, and (II) sort the objects in the order of: cyan < gray < red < yellow (alphabetical order). We generated 10k examples for each operation.

Models.We used DeiSAM with Slot Attention(Locatello et al.,2020)pretrained on the visual Inductive Logic Programming (ILP) dataset(Shindo et al.,2024), which contains positive and negative visual scenes for list operations. We used GroundedSAM and LISA for neural baselines (cf.App.E).

Result.In Table5, we present the mean Average Precision (mAP) for each baseline evaluated. The purely neural baselines struggle to accurately deduce segmentations in response to abstract reasoning prompts, while DeiSAM excels at identifying and segmenting the object specified by the prompt.

Moreover, Fig.6provides qualitative examples illustrating that DeiSAM effectively segments objects requiring high-level reasoning. In contrast, the neural baselines frequently fail to segment the correct target object. These findings indicate that existing neural baselines are inadequate for addressing abstract reasoning prompts. We demonstrate that integrating differentiable logic reasoners can significantly enhance reasoning capabilities.

SECTION: 5.7End-to-End Training of DeiSAM

Since DeiSAM employs a differentiable forward reasoner, a meaningful gradient signal can be back-propagated through the entire pipeline. Consequently, DeiSAM enables end-to-end learning on complex object detection and segmentation tasks with logical reasoning explicitly modeled during training.
To illustrate this property, we show that DeiSAM can learn weighted mixtures of scene graph generators by propagating gradients through the reasoning module.

Task.We consider 2 distinct scene graph generators and compose a weighted mixture of them. We show an example for the deictic prompt “An object that has hair and that is on a surfboard” in ListingLABEL:lst:program2.
The first 3 rules compute the target object for each SGG, similarly toProgram 1, and the last 2 rules produce a weighted merge of both predictions.
Importantly,Program 2utilizes different SGGs (i.e.variableSG) and merges the results using learnable weights (i.e.w_1andw_2).
Consequently, the learning task is the optimization of weightsfor downstream deictic segmentation.
The differentiability of the DeiSAM pipeline allows efficient gradient-based optimizations.

Experimental Setup.We used VETO(Sudhakaran et al.,2023), which outperforms other SGGs on biased datasets where only some relations appear frequently. As the second ‘SGG’ for our weighted mixture model, we relied on ground-truth scene graphs from Visual Genome.
We consider the following baselines: DeiSAM-VETO that only uses a pre-trained VETO model(Sudhakaran et al.,2023), DeiSAM-Mixture (naive) that uses a mixture of VETO and VG scene graphs with randomly initialized weights. We compare those approaches to DeiSAM-Mixture*, which uses the trained mixture.
We extracted instances from DeiVG datasets not used in VETO training (ca.samples), which we divided into a training, validation, and test split.
For rule generation, we use the same system prompt and models as in Sec.5.1, adapting the generated programs for weight learning.

We minimize the binary cross entropy loss with respect to rule weightsw_1andw_2.
To calculate this loss, we provide labels for predicted masks in the model,i.e., a binary label.
For each instance in DeiVG, DeiSAM predicts segmentation masks in the forward pass, and gradients are backpropagated through the differentiable forward reasoner (cf.App.E.1for details).

Result.In Tab.5.7, we compare the mAP on the test split.
The trained model DeiSAM-Mixture∗clearly outperforms the naive baseline, demonstrating successful training of the DeiSAM pipeline using gradients via differentiable reasoning.
DeiSAM-VETO weak performance can be attributed to objects that appear only on prompts but not its training data (cf.App.D).

Fig7shows examples of segmentation masks and their confidence scores produced by DeiSAM-Mixture models before and after training.
Before learning, wrong or incomplete regions are segmented with low confidence scores because the reasoner fails to identify correct objects with low-quality scene graphs that miss critical objects and relations.
After learning, DeiSAM produces faithful segmentation masks and increased confidence scores.
This experiment highlights that DeiSAM improves the quality of scene graphs and the subsequent segmentation masks by learning using gradients,i.e., it is a fully trainable pipeline with a strong capacity for complex logic reasoning.

SECTION: 6Conclusion

Before concluding, let us discuss the limitations and future research directions.
Our investigation of DeiSAM’s components highlights some clear avenues for future research.
While LLMs perform well at parsing deictic prompts into logic rules with few-shot prompting,
their performance could be improved further by,e.g., syntactically constrained sampling666https://github.com/IsaacRe/Syntactically-Constrained-Samplingor dedicated fine-tuning. Further, the observed challenges in semantic unification could be addressed by querying LLMs instead of using embedding models or providing multiple weighted candidates to the reasoner.

Upon manual inspection of the DeiVG dataset, we identified some inconsistent examples annotated with erroneous scene graphs in Visual Genome that cannot be automatically cleaned up without external object identification (cf.App.D).
Our results support the assessment that generating rich scene graphs is key but difficult to achieve in a zero-shot fashion.
However, as we demonstrated, the differentiable pipeline of DeiSAM can be utilized for meaningful training on complex downstream tasks. Thus allowing for the incorporation of real-world use cases in the training of SGGs in an end-to-end fashion. Further, DeiSAM can provide valuable information on general performance and failure cases of SGGs by investigating deictic segmentation tasks.
Furthermore, the modularity of DeiSAM allows for easy integration of potential improvements to any of its components.

To conclude, we proposed DeiSAM to perform deictic object segmentation in complex scenes.
DeiSAM effectively combines large-scale neural networks with differentiable forward reasoning in a modular pipeline. DeiSAM allows users to intuitively describe objects in complex scenes by their relations to other objects. Moreover, we introduced the novel Deictic Visual Genome (DeiVG) benchmark for segmentation with complex deictic prompts.
In our extensive experiments, we demonstrated that DeiSAM strongly outperforms neural baselines
highlighting its strong reasoning capabilities on visual scenes with complex textual prompts.
To this end, our empirical results revealed open research questions and important future avenues of visual scene understanding.

Acknowledgements.The authors thank Maurice Kraus and Felix Friedrich for their valuable feedback on the manuscript. This work was partly supported by the EU ICT-48 Network of AI Research Excellence Center “TAILOR” (EU Horizon 2020, GA No 952215), and the Collaboration Lab “AI in Construction” (AICO). The work has also benefited from the Federal Ministry of Education and Research (BMBF) Competence Center for AI and Labour (“KompAKI”, FKZ 02L19C150) and from the Hessian Ministry of Higher Education, Research, Science and the Arts (HMWK) cluster projects “The Third Wave of AI” and “The Adaptive Mind”.
We gratefully acknowledge support by the German Center for Artificial Intelligence (DFKI) project “SAINT”. This work also benefited the HMWK / BMBF ATHENE project “AVSV” and the National High Performance Computing Center for Computational Engineering Science (NHR4CES).
The Eindhoven University of Technology authors received support from their Department of Mathematics and Computer Science and the Eindhoven Artificial Intelligence Systems Institute.

SECTION: References

SECTION: Appendix AFirst-Order Logic and Differentiable Reasoning.

We provide a formal definition of first-order logic (FOL).
Anatomis a formula, whereis a predicate symbol (e.g.) andare terms.
A term is a variable or a constant.
Aground atomor simply afactis an atom with no variables (e.g.).
Aliteralis an atom () or its negation (), and aclauseis a finite disjunction () of literals.
Adefinite clauseis a clause with exactly one positive literal.
Ifare atoms, thenis a definite clause.
We write definite clauses in the form of, and
refer to them asrulesfor simplicity in this paper.Forward Reasoningis a data-driven approach of reasoning in FOL(Russell & Norvig,2010),i.e., given a set of facts and a set of rules, new facts are deduced by applying the rules to the facts.
Differentiable forward reasoners compute logical entailment using tensor representations(Evans & Grefenstette,2018; Shindo et al.,2023)or graph neural networks(Shindo et al.,2024), and perform rule learning using gradients given labeled examples in the form of inductive logic programming(Cropper & Dumancic,2022).

DeiSAM employs a graph neural network-based differentiable forward reasoner(Shindo et al.,2024), and we briefly explain the reasoning process.
We represent a set of (weighted) rules as a directed bipartite graph. For example, Fig.8is a reasoning graph that representsProgram 1.

SECTION: A.1Details of Differentiable Forward Reasoning

We provide the details of differentiable forward reasoning.

AForward Reasoning Graphis a bipartite directed graph, whereis a set of nodes representing ground atoms (atom nodes),is set of nodes representing conjunctions (conjunction nodes),is set of edges from atom to conjunction nodes andis a set of edges from conjunction to atom nodes.

DeiSAM performs forward-chaining reasoning by passing messages on the reasoning graph.
Essentially, forward reasoning consists oftwosteps: (1) computing conjunctions of body atoms for each rule and (2) computing disjunctions for head atoms deduced by different rules. These two steps can be efficiently computed on bi-directional message-passing on the forward reasoning graph.
We now describe each step in detail.

(Direction) From Atom to Conjunction.First, messages are passed to the conjunction nodes from atom nodes.
For conjunction node, the node features are updated:

whereis a soft implementation ofconjunction, andis a soft implementation ofdisjunction.
Intuitively, probabilistic truth values for bodies of all ground rules are computed softly by Eq.1.

(Direction) From Conjunction to Atom.Following the first message passing, the atom nodes are then updated using the messages from conjunction nodes.
For atom node, the node features are updated:

whereis a weight of edge.
We assume that each rulehas its weight, andif edgeon the reasoning graph is produced by rule.
Intuitively, in Eq.2, new atoms are deduced by gathering values from different ground rules and from the previous step.

We used product for conjunction, andlog-sum-expfunction for disjunction:

whereis a smooth parameter. Eq.3approximates the maximum value given input.

Prediction.The probabilistic logical entailment is computed by the bi-directional message-passing.
Letbe input node features, which map a fact to a scalar value,be the reasoning graph,be the rule weights,be background knowledge, andbe the infer step.
For fact, DeiSAM computes the probability as:

whereis the node features of atom nodes after-steps of the bi-directional message-passing.

By optimizing the cross-entropy loss, the differentiable forward reasoner can solve Inductive Logic Programming (ILP) problems with propositional encoding(Shindo et al.,2018), where the task is to find classification rules given positive and negative examples.
It has been extensively applied to solve complex visual patterns(Helff et al.,2023), image generation(Deiseroth et al.,2022), meta-level reasoning and learning(Ye et al.,2022), predicate invention(Sha et al.,2024,2023), and self-explanatory learning(Stammer et al.,2024).

SECTION: Appendix BSystem Prompts for Rule Generation

To generate logic rules using LLMs, we used the following system prompt.

In practice, this system prompt combined with few-shot examples for a downstream task (cf.App.E).

SECTION: Appendix CDeiVG Datasets

We generated DeiVG dataset using Visual Genome dataset(Krishna et al.,2017).
We used the entire Visual Genome dataset to generate deictic prompts and answers out of scene graphs, and we randomly downsampledk examples.
We only considered the following relations:

‘on’

‘wears’

‘has’

‘parked on’

‘behind’

‘holding’

‘against’

‘wearing’

‘near’

‘along’

‘in front of’

‘at’

‘under’

‘sitting on’

‘made of’

‘above’

‘carrying’

‘riding’

‘over’

The prompts are synthetically generated by extracting relations that shares the same subject in the scene. For example, with a pair of VG relations,"person is holding an umbrella"and"person on a boat", we generate a deictic prompt,"an object that is holding an umbrella, and that is on a boat". Subsequently, the corresponding answer is extracted from the scene graph.
We provide two instances in the generated DeiVG2dataset in ListingLABEL:lst:deivg_examples.

SECTION: Appendix DAdditional Analysis on VG Scene Graphs

We provide a further investigation of Visual Genome (VG) scene graph annotations.
We demonstrate that (1) VG annotations have multiple versions and there is a non-trivial discrepancy between their relation distributions, and (2) VG annotations contain incomplete and erroneous scene graphs that cannot be automatically cleaned up without external object identification.

We investigated the scene graph generator module, going beyond the ground truth scene graphs of Visual Genome.
One of the key challenges in using a pre-trained SGG is the potential mismatch between the set of objects and annotations in DeiVG and the training data of the SGG.
While we built on the latest version of Visual Genome with extended annotations (VG),
an older version(Krishna et al.,2017)has been commonly used for benchmarking different SGGs by excluding non-frequent object types and relations(Xu et al.,2017).
This preprocessed older version (VG) is still considered the standard benchmark for SGGs, which lacks many crucial objects and attributes in DeiVG built upon VG.
We illustrate the discrepancy in Fig.9, where we compare the Kernel Density Estimate (KDE) of VGand VG.
It shows that the newer version contains more types of objects in the top-and-middle frequency range.
Additionally, many object types of VGin the middle-frequency range are not contained at all in VG. Consequently, when parsing scenes from DeiVG with SGGs pre-trained on VG, the objects in the deictic prompt may not be contained in the generated scene graph at all.
For example, for a given deictic prompt “an object that is wearing a black shirt”, we observed that the pre-trained SGG failed to detectblack shirtbecause it was not included in its training data.
This discrepancy leads to sub-par performance of DeiSAM with a pre-trained SGG, as shown in Tab.5.7.
While training the SGG specifically on the relevant scene distribution can partially address this issue, it is crucial to highlight that DeiSAM can be leveraged to improve SGGs as well. Our subsequent demonstration in Section5.7provides a glimpse of DeiSAM’s capabilities, showcasing its differentiable forward reasoner’s ability to perform end-to-end training, thereby unlocking new horizons in scene understanding and reasoning.

In Fig.10, the plot on the left depicts the distribution of the least frequent tail object types on the latest extended version (VG) in comparison to the older version (VG)(Krishna et al.,2017).
Object types are sorted with respect to the number of occurrences (x-axis).
In the non-frequent range of 0 to 1000, VGcontains object types that never appear on the older version (VG),
revealing these tail classes of object types are completely missing in the processed older annotations(Xu et al.,2017), which are commonly used for benchmarking SGGs. Moreover, the bar plot on the right indicates a notable increase in the number of object types and total object counts in VGin comparison to VG, making it difficult for SGGs that are pre-trained on the older version to achieve high performance on the DeiVG dataset built upon VG.

Upon manual inspection of the DeiVG dataset, we identified some inconsistent examples resulting from incomplete and erroneous scene graphs in Visual Genome that cannot be automatically cleaned up without external object identification. For example, the annotations shown in Fig.11lead to a DeiVG sample with two missing target objects and an incorrect one. We plan on building a more consistent deictic segmentation benchmark using a cleanup process similar to GQA(Hudson & Manning,2019).

SECTION: Appendix EDetails of Experiments

We provide details of the models used in the evaluation. For all methods using SAM for segmentation—including DeiSAM—we use the same publicly available SAM checkpoint777https://huggingface.co/spaces/abhishek/StableSAM/blob/main/sam_vit_h_4b8939.pth.

DeiSAM.We used NEUMANN(Shindo et al.,2024)withfor soft-logic operations, and the number of inference steps is set to.
We set the box threshold toand the text threshold tofor the SAM model.
All generated rules are assigned a weight of.
If no targets are detected, DeiSAM produces a mask of a randomly chosen object in the scene.

For LLMs, we provided few-shot examples of deictic prompts and desired outputs as shown in ListingLABEL:lst:fewshot_examples.
These few-shot examples improved the quality of the rule generation that follows a certain format.
These are combined with the system prompt in App.Bto generate rules by LLMs.

GroundedSAM.We used a publicly available GroundedDino version888https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha2/groundingdino_swinb_cogcoor.pthwith Swin-B backbone, pre-trained on COCO, O365, GoldG, Cap4M, OpenImage, ODinW-35 and RefCOCO.
We set the box threshold toand the text threshold tofor the SAM model.

GLIPWe used a publicly available version of GLIP-L999https://huggingface.co/GLIPModel/GLIP/blob/main/glip_large_model.pthwith a Swin-L backbone and pre-trained on FourODs, GoldG, CC3M+12M, SBU. We set the prediction threshold to.

OFAWe used a publicly available version of OFA-Large101010https://modelscope.cn/models/iic/ofa_visual-grounding_refcoco_large_en/summaryfine-tuned for visual grounding.

SEEMWe used a publicly available checkpoint of SEEM111111https://huggingface.co/xdecoder/SEEM/resolve/main/seem_focall_v0.ptwith Focal-L backbone.

Evaluation Metric.We used mean average precision (mAP) to evaluate segmentation models.
Segmentation masks are converted to corresponding bounding boxes by computing their contours, and then mAP is computed by comparing them with the ground truth bounding boxes provided by Visual Genome.

SECTION: E.1Learning to Segment Better

We describe the experimental setting in more detail.

Method.Letbe a DeiVG dataset, whereis an input image, andis a deictic prompt.
For each input, DeiSAM produces segmentation maskswith corresponding confidence score,i.e.,.
For each mask, we consider a binary labelusing its corresponding bounding box and comparing with ground-truth masks with the IoU score, which assesses the quality of bounding boxes.
For each instance, we compute the binary-cross entropy loss:We minimize the lossthrough gradient descent with respect to the rule weights in the differentiable programs.

Task.Given SGGs,
we compose a DeiSAM model with a mixture of them,i.e.withand, whereis a set of facts to describe scene graphs.
For example,Program 2shown on the right represents the mixture of scene graph generators for the deictic prompt “An object that has hair and that is on a surfboard”.
In contrast toProgram 1, the program utilizes different SGGs and merges the results using learnable weights.
The learning task is the optimization of weightsfor downstream deictic segmentation.
The differentiable reasoning pipeline in DeiSAM allows efficient gradient-based optimizations with automatic differentiation.

Dataset.Letbe a DeiVG dataset, whereis an input image, andis a deictic prompt andbe the answers, whereare correct target objects to be segmented specified by the prompt.
For each input, DeiSAM produces segmentation masks with their confidence:

whereis the-th predicted segmentation mask andis the confidence score.
For each mask, we consider a binary labelby computing its corresponding bounding box and using the IoU score, which assesses the quality of bounding boxes:

whereis a threshold,is a bounding box computed from mask, andis a mask that represents answer object.
We extracted instances from DeiVG datasets not used in VETO training (ca.samples), which are divided into training, validation, and test splits that contain,, andinstances, respectively.
If no targets are detected by the forward reasoner, DeiSAM produces a mask of a randomly chosen object in the scene,i.e., if the maximum confidence score for targets is less than, DeiSAM was set to sample a random object in the scene and segment with a confidence score randomly sampled from a uniform distribution with a range of.

Optimization.We used the RMSProp optimizer with a learning rate of, and performedsteps of weight updates with a batch size of.
The reasoners’ inference step was set to. We used IoU score’s threshold.

SECTION: Appendix FAblations

Subsequently, we present detailed results corresponding to the ablation studies in Sec.5.4.

LLMs for Rule Generation and Semantic Unification.First, we evaluated multiple open and closed-source language models of different sizes on rule generation.
The results on correct predicate identification and rule generation are reported in Tab.7.
In general, all instruction-tuned models can generate logic rules from deictic prompts. However, larger models strongly outperform smaller ones, especially for more complex inputs.
Interestingly, the types of failure cases differ significantly between models. For example, both Llama models and GPT-3.5-turbo rarely make syntactical errors in rule generation (), whereas most of Mistral-7B’s incorrect rules are already syntactically unsound.

For the semantic unification task we compare various semantic embedding models as shown in Table8.

Runtime Analysis.We provide comparisons of the runtime of Grounded-SAM, LISA, and DeiSAM in Tab.10. It shows the inference time per instance (a visual input with a textual prompt), averaged over 1000 examples for each dataset. GroundedSAM achieved remarkably faster inferences since it does not encode the reasoning process. Both LISA and DeiSAM approaches require about 6 to 10 seconds per example.
For more analysis, we provide the running time for each component of DeiSAM in Tab.10. It shows the inference time per instance of rule generation, semantic unification, differentiable forward reasoning, and segmentation. We observe that semantic unification is the most time-consuming process in the DeiSAM pipeline. In contrast, the differentiable forward reasoning and segmentation only make up a negligible fraction of the runtime. However, as outlined in the paper, the rule generation and semantic unification step rely on the OpenAI API and could be significantly reduced by running a local model instead.

DeiVG in the ReCOCO’s reference format.We conducted additional experiments by modifying the deictic prompts in DeiVG datasets to the ones with targets’ labels, resulting in prompts similar to reference texts in RefCOCOs(Yu et al.,2016). Table11shows the mAP on DeiVG datasets with the modified prompts using LISA, GroundedSAM, and DeiSAM.represents the gain compared to the original performance with deictic prompts (shown in Tab.1). Neural baselines gained the performance remarkably by the modified prompts since they contain labels for targets (e.g.person, kid), on which neural models highly rely to identify targets.

SECTION: Appendix GAdditional Segmentation Results

We provide supplementary results of the segmentation on the DeiVG2dataset in
Fig.12.