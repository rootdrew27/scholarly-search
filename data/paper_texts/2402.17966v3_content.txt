SECTION: STC-ViT: Spatio Temporal Continuous Vision Transformer for Weather Forecasting
Operational weather forecasting system relies on computationally expensive physics-based models. Recently, transformer based models have shown remarkable potential in weather forecasting achieving state-of-the-art results. However, transformers are discrete and physics-agnostic models which limit their ability to learn the continuous spatio-temporal features of the dynamical weather system. We address this issue with, a Spatio-Temporal Continuous Vision Transformer for weather forecasting. STC-ViT incorporates the continuous time Neural ODE layers with multi-head attention mechanism to learn the continuous weather evolution over time. The attention mechanism is encoded as a differentiable function in the transformer architecture to model the complex weather dynamics. Further, we define a customised physics informed loss for STC-ViT which penalize the model’s predictions for deviating away from physical laws. We evaluate STC-ViT against operational Numerical Weather Prediction (NWP) model and several deep learning based weather forecasting models. STC-ViT, trained on 1.5-degree 6-hourly data, demonstrates computational efficiency and competitive performance compared to state-of-the-art data-driven models trained on higher-resolution data for global forecasting.

SECTION: Introduction
Operational weather forecasting systems rely on physics based Numerical Weather Prediction (NWP) models. While highly accurate, these system tend to be extremely slow with aggregated approximation errors and requiring high computational resources.

Weather forecasting can be formulated as a continuously evolving physical phenomenon which can be typically expressed as time-dependent partial differential equations (PDEs). These PDEs capture the dynamic evolution of atmospheric variables over time, enabling the simulation of complex weather patterns and phenomena. Encoding these physical priors leads to more interpretable and physically consistent forecasting models. Recent works have shown that using deep learning models as surrogates to model complex multi-scale spatio-temporal phenomena can lead to training-efficient models. Vision Transformer byhas emerged one of such models.

However, transformers only process discrete sequences of input data and are fundamentally discrete in naturewhich limit their ability in modelling the continuous evolving dynamics of weather system. Additionally, these models ignore the fundamental physical laws of our atmosphere which is essential for faithful weather modelling.
In this work, we address theproblem in Vision Transformer (ViT) which arise in weather forecasting systems where data is present in discrete form. The distinctive and continuously evolving characteristics of weather data pose significant challenge in producing accurate forecasts. Additionally, we penalise the model for not following physical dynamics in the form a customized physics informed loss function.

We build upon the core ideas of Neural Ordinary Differential Equation (NODE) byand ViT.The idea is to parameterize the attention mechanism by converting it into a differentiable function. Continuous temporal attention is calculated sample-wise and combined with the patch wise spatial attention to learn the spatio-temporal mapping of weather variables in the embedding space of the vision transformer. Furthermore, we add derivation as a pre-processing step to prepare the discrete data for continuous model and explore the role of normalization in continuous modelling.

In this paper we focus on the following research question:. In summary, our contributions are as follows:

We propose continuous spatio-temporal continuous attention to effectively learn and interpolate the spatio-temporal information for weather forecasting.

We introduce derivation as a pre-processing step to ensure better feature extraction for continuous spatio-temporal models.

We introduce physical constraints in our model via soft penalties in the form of a custom physics informed loss based on three fundamental physical laws of thermodynamics, kinetic enrgy and potential energy.

We perform extensive experiments on both WeatherBench and WeatherBench 2 to show the competitive performance of STC-ViT against state-of-the-art forecasting models.

SECTION: Background
SECTION: Neural ODE
Neural ODEs are the continuous time models which learn the evolution of a system over time using Ordinary Differential Equations (ODE). The key idea behind Neural ODE is to model the derivative of the hidden state using a neural network. Consider a hidden stateat timeIn a traditional neural network, the transformation from one layer to the next could be considered as moving from timeto. In Neural ODEs, instead of discrete steps, the change inover time is defined by an ordinary differential equation parameterized by a neural network:

whereis the derivative of the hidden state with respect to time,is a neural network with parametersandis the time variable, allowing the dynamics ofto change with time.

To see the connection between ResNets and Neural ODEs, consider a ResNet with layers updating the hidden state as:

In the limit, as the number of layers goes to infinity and the updates become infinitesimally small, this equation resembles the Euler method for numerical ODE solving, where:

Reducingto an infinitesimally small value transforms the discrete updates into a continuous model described by the ODE given earlier. To compute the output of a Neural ODE, integration is used as a backpropagation technique. This is done using numerical ODE solvers, such as Euler, Runge-Kutta, or more sophisticated adaptive methods, which can efficiently handle the potentially complex dynamics encoded by.

SECTION: Physics Constrained Models
In weather and climate modeling, incorporating physical constraints ensures that the model adheres to the governing physical laws. These constraints can be added in two ways: hard constraints and soft constraints.For a given physical constraintwhereis the governing physical law, a hard constraint would mean that the machine learning model’s predictionmust always satisfy equation.

This constraint can be embedded into model’s architecture as a constrained layer or optimizer.

Soft constraints adds a penalty term to the loss function that minimizes the violation of a physical lawas shown in equation

wherecontrols the weight of the penalty for violating the physical constraintandmeasures the degree of violation (e.g., deviation from mass conservation).

SECTION: Related Work
Integrated Forecasting System (IFS)is the operational NWP based weather forecasting system which generates forecasts at a high resolution of 1km. IFS combines data assimilation and earth system model to generate accurate forecasts using high computing super computers. In contrast, data-driven methodologies have now outperformed NWP models with much less computational complexities.

WeatherBench byprovides a benchmark platform to evaluate data-driven systems for effective development of weather forecasting models. Current state-of-the-art data-driven forecasting models are mostly based on Graph Neural Networks (GNNs) and Transformers.implemented a message passing GNN based forecasting model which was further extended by GraphCastwhich used multi-mesh GNN to achieve state-of-the-art results. FourCastNet (FCN)used a combination of Fourier Neural Operator (FNO) and ViT and was reported to be 80,000 times faster than NWP models. Several more transformer based models emerged including Pangu-Weather, ClimaX, FengWu, FuXiand Stormerall showcased remarkable capabilities for short to medium range forecasting.

While being highly accurate and showcasing remarkable scaling capabilities, these models are discrete space-time models and do not account for the continuous dynamics of weather system. Recently,proposed ClimODE which used Neural ODE to incorporate a continuous-time process that models weather evolution and advection, enabling it to capture the dynamics of weather transport across the globe effectively. However it currently yields less precise forecast results compared to state-of-the-art models, offering significant potential for further enhancements. Further,proposed Neural GCM, which integrates a differentiable solver with neural networks resulting in physically consistent models.

SECTION: Methodology
Consider a modelreceives weather data as input of the format timeand predicts the weather information at timeas shown in equationwhereis the number of weather variables such as temperature, wind speed, pressure, etc. andrefers to the spatial resolution of the variable.

The objective of the model is to learn the continuous temporal dependencies between consecutive time steps while accounting for spatial correlations within thegrid. Since the weather changes continuously over time, it is essential to capture the continuous change within the provided fixed time step data. The main aim of STC-ViT is to to learn the continuous latent representation of the weather data using Spatio-Temporal Continuous Attention and Neural ODEs. The evolution of the weather system fromtocan be represented as:

learnable model parameters

Weather information is highly variational and complex at both temporal and spatial levels. Temporal derivatives of each weather variable are calculated to preserve weather dynamics and ensure better feature extraction from discretized data. We perform sample wise derivation at pixel level to capture the continuous changes in weather events over time as shown in equation.

whereis pixel value of weather variable at time t andis the pixel value of same variable at t-1.

SECTION: Spatio-Temporal Continuous Vision Transformer
In the STC-ViT architecture, we enhance vision transformer-based weather forecasting by introducing Temporal Continuous Attention (TCA) and Spatial Attention (SA) mechanisms to capture dynamically evolving weather patterns. We build upon the variable tokenization and aggregation scheme proposed byfollowed by Continuous attention mechanism to enhance interpolated feature learning. The detailed architecture of STC-ViT is shown in figure.

We model the temporal dynamics with attention by incorporating derivatives (temporal changes) over time directly into the query, key, and value representations.

We formulateas the "current" time step’s information and is designed to seek relevant patterns or changes reflecting the model’s focus on upcoming changes equation.

models the "context" of prior time step states, which provide historical dynamics incorporating past time step derivatives equation.

represents the updated information allowing the model to interpret them in the context of previous and current changes.
This modification allows the model to learn the transitional changes from one time step to another which is important for capturing unprecedented changes in weather.

To compute TCA and capture the temporal change for the same variable, we calculate the attention betweenandfocusing on the temporal evolution. Assume we have two input samples of sizedenoted asat timeandat timewhereis the number of weather variables such as temperature, wind speed, pressure, etc.,refers to the spatial resolution of the variable. The attention mechanism to capture the temporal continuity between time stepsandis given by:

The resulting output represents the attention weighted sum of values for similar variables across input samples at timeand. This approach models the dynamics of each variable independently over time, allowing the model to capture temporal dependencies and changes effectively.

For each variable i, we calculate the attention between spatial positionsandgiven by equation:

where Q, K are matrices formed from all queries, keys, and values, respectively,is the dimensionality of the keys and queries, and the division byis a scaling factor to deal with gradient vanishing problems during training.
Use the spatial attention weights to obtain the spatially-enhanced representation for each variable i by weighting the values:

We concatenate the outputs of the TCA and SA along the feature dimension (as shown in equation) to provide a comprehensive representation, considering both temporal and spatial dependencies. This dual attention approach allows the Vision Transformer to effectively model complex spatio-temporal dependencies in the weather reanalysis data.

This output captures both the temporal evolution of each variable over time and the spatial interactions within each time step, enhancing the model’s ability to understand the continuous, spatiotemporal dynamics of the data.

Neural ODEs model the transformation of data through a continuous dynamical system. Instead of discrete layers, the transformation is modeled by a differential equation. The idea is to treat the depth of the network as a continuous time variable t. As explained in section, Neural ODEs can be seen as a continuous form of ResNets.

In STC-ViT, instead of adding the output of an attention mechanism discretely as, we model the transformation using a Neural ODE. The attention output is treated as a continuous evolution of the input statetransforming equationas equation. Solving this ODE fromtogives us the updated state, which incorporates the attention mechanism’s effect continuously over time.

To compute the output of a Neural ODE, we solve the initial value problem:

This integral is computed using Runge-Kuttanumerical ODE solver. The solver adaptively determines the number of steps required, making the process efficient and accurate. This continuous approach allows for smoother transformations and improve model performance.

SECTION: Latitude Weighted Physics Informed Loss Function
We use latitude weighted mean squared error to compute loss for the predicted variables.

whereaccounts for Latitude weights:

Additionally, we account for three fundamental physical laws i.e. Potential Energy, Kinetic Energy and Thermodynamic Balance in our loss function. Kinetic energy in atmospheric science refers to the energy associated with the motion of air masses. Geopotential height is often used in meteorology to express the potential energy of an air parcel in the Earth’s gravitational field. It represents the energy per unit mass of an air parcel at height z above sea level and thermodynamic balance equation describes the evolution of temperature in an air parcel due to processes like heat addition and pressure changes.
These fundamental principles, coupled with wind components and thermodynamic variables like temperature and geopotential height, form the core of atmospheric dynamics used in climate and weather models.

where u is the eastward wind component (m/s) and v is the northward wind component (m/s).

where g is the acceleration due to gravity approximatelyand z is the height or geopotential height (m).

whereis the temporal change of temperature (model output),represent the advection of temperature by wind in the x and y directions, respectively.

where,andare the weight factors for physics based loss. The resulting loss aims to ensure that the predicted temperature and wind fields satisfy the physical laws of thermodynamics (like energy conservation) on their own. By setting up the equation to balance energy and temperature gradients, this loss constrains the predictions to maintain internal consistency with physical laws, regardless of the specific values in the ground truth.

SECTION: Experiments and Results
SECTION: Dataset
We train STC-ViT on ERA5 datasetprovided by the European Center for Medium-Range Weather Forecasting (ECMWF). We compare STC-ViT against several weather forecasting models by training it at two different resolutions of(32 x 64 grid points) provided by WeatherBenchand(121 x 240 grid points) provided by WeatherBench2.

SECTION: Training Details.
We consider weather forecasting as a continuous spatio temporal forecasting problem i.e a tensor of shapeat timeis fed to the pre-processing layer of the model where it passes through pre-derivation to STC-ViT and outputs a tensor ofat future time step. Complete training details of the model are given in the in Appendix.

SECTION: WeatherBench
We train STC-ViT on hourly data with following set of variables: Land Sea Mask (LSM), Orography, 10-meter U and V wind components (U10 and V10) and 2-meter temperature (T2m) in addition to 6 atmospheric variables: geopotential (Z), temperature (T), U and V wind components, specific humidity (Q) and relative humidity (R) at 7 pressure levels: 50 250 500 600 700 850 925. We use data from 1979-2015 for training, 2016 for validation and 2017-2018 for testing phase. We compare STC-ViT with ClimaX, and ClimODE on ERA5 dataset atresolution provided by WeatherBench. To ensure fairness, we retrained ClimaX from scratch without any pre-training.

STC-Vit outperforms ClimaX and ClimODE at all lead times which shows that replacing regular attention with continuous attention in ViT architecture derives improved feature extraction by mapping the changes occurring between successive time steps. Additionally, enforcing physical constraints in the model lead to improved prediction scores. The RMSE and ACC results are shown in Table.

SECTION: WeatherBench2
To keep training consistent with WeatherBench 2, we utilize the training data from 1979 to 2018, validation data from 2019, and test data from 2020 year. We train STC-ViT on 6 hourly data for following variables: T2m, u10 and v10 wind components and mean sea-level pressure (MSLP) along with five atmospheric variables: geopotential height (Z), temperature (T), U and V wind components, and specific humidity (Q). These atmospheric variables are considered at 13 pressure levels: 50, 100, 150, 200, 250, 300, 400, 500, 600, 700, 850, 925, 1000 hPa. We also compare our results with both versions of IFS, IFS-HRES which is state-of-the-art forecasting model at high resolution ofand IFS-ENS, ensemble version trained at. Additionally, we compare STC-ViT against GraphCast and PanguWeather which are trained at a higher resolution ofand finally with Neural GCM trained at.

STC-ViT shows competitive performance, outperforming Pangu and GraphCast particularly for geopotential, temperature and wind variables. This is due to the physics loss penalizing the model for not obeying physics and encouraging outputs that balance temperature evolution and respect thermodynamic laws by penalizing deviations from these energy fluxes. We find out that Neural GCM is the best performing model on atmospheric variables which could be due to the physics embedded in the dynamical core. We believe that STC-ViT could also benefit from high resolution training and physics directly incorporated in the model architecture which we will explore in future. The RMSE and ACC results are shown in Figuresandrespectively.

SECTION: Ablation Studies
Vision Transformer has emerged as a powerful architecture which captures long-term dependencies better than any model. For this ablation study, we simply train a basic ViT architecture on ERA5 dataset. Compared with STC-ViT, ViT under performs for prediction accuracy showing the superiority of continuous models in weather forecasting systems.

Another ablation study is done using vanilla Neural ODE model. We replace the transformer with Neural ODE architecture as proposed in the original paper. While Neural ODEs are computationally efficient, they only aid in interpolating the temporal irregularities and ignore the spatial continuity. This study proves that STC-ViT learns both spatio-temporal continuous features from the discrete data and is better at representing dynamical forecast systems.

To understand the importance of continuous attention, we remove the neural ODE layer and provide the output of attention mechanism to the basic feed forward network. While continuous attention only model does not outperform the STC-ViT, it provides better prediction accuracy than the traditional attention model.

We perform another ablation study to compare how well Neural ODE capture continuity in weather information when simply added as a layer in the transformer architecture. Neural ODEis a continuous depth neural network designed to learn continuous information in the process as well. For this study, we replace the continuous attention with vanilla attention and add a Neural ODE layer in the feed forward block. The feed forward solution is approximated using Runge-Kutta (RK) numerical solver.

SECTION: Conclusion and Future Work
In this paper, we present STC-ViT, a novel technique designed to capture continuous dynamics of weather data while obeying the fundamental physical laws of Earth’s atmosphere. STC-ViT achieves competitive results in weather forecasting which shows that vision transformers can model continuous nature of spatio-temporal dynamic systems with carefully designed attention mechanism.

While STC-ViT performs competitively with state-of-the-art data-driven weather forecasting models, it is important to address its limitations in weather forecasting systems. STC-ViT is a deterministic model which does not account for model uncertainty which can produce unrealistic results. Additionally, predictions on longer lead times result in blurry forecasts. Extending STC-ViT to a probabilistic model can be addressed in future works. Also, scaling STC-ViT to better accommodate the multi-modal high resolution training and evaluation presents an opportunity as future research. Finally, addressing the black-box problem of STC-ViT can shed light on model learning insights which is equally important for climate science community.

SECTION: Societal Impact
Our research focuses on modelling the continuous dynamics of weather forecasting system through the integration of deep learning (DL) techniques. The study shows that leveraging data-driven approaches can achieve significantly improved forecast results with compute efficient resources. The environmental benefits of compute-efficient forecasting systems are significant. Lowering the carbon footprint of computational processes contributes to global efforts in combating climate change. By integrating ML to improve accuracy while optimizing computational efficiency, we can create a sustainable and inclusive approach to weather forecasting that serves the global community more effectively.

SECTION: References
SECTION: Experiment Details
SECTION: Evaluation Metrics.
We used Root Mean Square Error (RMSE) and Anomaly Correlation Coefficient to evaluate our model predictions. The formula used for RMSE is:

whereis the spatial resolution of the weather input and N is the number of total samples used for training or testing. L(i) is used to account for non-uniformity in grid cells.

Whereandand C is the temporal mean of the entire test set

SECTION: Optimization
We use the AdamW optimizerwith a learning rate of 5e-5. We utilize a CosineAnnealing learning rate scheduler is adopted which progressively lowers the learning rate to zero following the warm-up period which is 10% of the total epochs. We train STC-ViT for 50 epochs forand 100 epochs forresolution data .

SECTION: Hyperparameters
SECTION: Normalization
In our experiments, we normalize all inputs during training and re-scale them to their original range before computing final predictions. We perform z-score normalization for every variable, at each atmospheric pressure level, we calculate the mean and standard deviation to standardize them to zero mean and unit variance.

SECTION: Software and Hardware Requirements
We use PyTorch, Pytorch Lightning, torchdiffeq, and xarrayto implement our model. We use 2 NVIDIA DGX A100 devices with 80GB RAM for training STC-ViT atand 4 NVIDIA Tesla Volta V100-SXM2-32GB for the training at resolution of.