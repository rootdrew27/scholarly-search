SECTION: Towards Training Without Depth Limits:Batch Normalization Without Gradient Explosion

Normalization layers are one of the key building blocks for deep neural networks. Several theoretical studies have shown that batch normalization improves the signal propagation, by avoiding the representations from becoming collinear across the layers. However, results on mean-field theory of batch normalization also conclude that this benefit comes at the expense of exploding gradients in depth. Motivated by these two aspects of batch normalization, in this study we pose the following question:
“Can a batch-normalized network keep the optimal signal propagation properties, butavoidexploding gradients?” We answer this question in the affirmative by giving a particular construction of anMulti-Layer Perceptron (MLP) with linear activationsand batch-normalization that provably hasbounded gradientsat any depth. Based on Weingarten calculus, we develop a rigorous and non-asymptotic theory for this constructed MLP that gives a precise characterization of forward signal propagation, while proving that gradients remain bounded for linearly independent input samples, which holds in most practical settings. Inspired by our theory, we also design an activation shaping scheme that empirically achieves the same properties for certain non-linear activations.

SECTION: 1Introduction

What if we could train even deeper neural networks? Increasing depth empowers neural networks, by turning them into powerful data processing machines. For example, increasing depth allows large language models (LLMs) to capture longer structural dependencies(Devlin et al.,2018; Liu et al.,2019; Brown et al.,2020; Goyal et al.,2021; Raffel et al.,2020). Also, sufficiently deep convolutional networks can outperform humans in image classification(Liu et al.,2022; Woo et al.,2023; Wu et al.,2021). Nevertheless, increasing depth imposes an inevitable computational challenge: deeper networks are harder to optimize. In fact, standard optimization methods exhibit a slower convergence when training deep neural networks. Hence, computation has become a barrier in deep learning, demanding extensive research and engineering.

A critical problem is that deeper networks suffer from the omnipresent issue of rank collapse at initialization: the outputs become collinear for different inputs as the network grows in depth(Saxe et al.,2013a). Rank collapse is not only present in MLPs and convolutional networks(Feng et al.,2022; Saxe et al.,2013a; Daneshmand et al.,2020), but also in transformer architectures(Dong et al.,2021; Noci et al.,2022). This issue significantly contributes to the training slowdown of deep neural networks. Hence, it has become the focus of theoretical and experimental studies(Saxe et al.,2013a; Feng et al.,2022; Daneshmand et al.,2021; Noci et al.,2022).

One of the most successful methods to avoid rank collapse is Batch Normalization (BN)(Ioffe and Szegedy,2015), as proven in a number of theoretical studies(Yang et al.,2019; Daneshmand et al.,2020,2021). Normalization imposes a particular bias across the layers of neural networks(Joudaki et al.,2023a). More precisely, the representations of a batch of inputs become more orthogonal after each normalization(Joudaki et al.,2023a). This orthogonalization effect precisely avoids the rank collapse of deep neural networks at initialization(Yang et al.,2019; Joudaki et al.,2023a; Daneshmand et al.,2021; Joudaki et al.,2023b).

While batch normalization effectively avoids rank collapse, it causes numerical issues. The existing literature proves that batch normalization layers cause exploding gradients in MLPs in an activation-independent manner(Yang et al.,2019). Gradient explosion limits increasing depth by causing numerical issues during backpropagation. For networks without batch normalization, there are effective approaches to avoid gradient explosion and vanishing, such as tuning the variance of the random weights based on the activation and the network width(He et al.,2015; Glorot and Bengio,2010). However, such methods cannot avoid gradient explosion in the presence of batch normalization(Yang et al.,2019). Thus, the following important question remains unanswered:

Is there any network with batch normalization without gradient explosion and rank collapse issues?

We answer the above question affirmatively by giving a specific MLP construction initialized with orthogonal random weight matrices, rather than Gaussian. To show that the MLP still has optimal signal propagation, we prove that the MLP output embeddings become isometric (equation5), implying the output representations becomes more orthogonal with depth. For a batch of linearly independent inputs, we prove

whereis a constant depending only on the network width and input and the expectation is taken over the random weight matrices. Thus, for sufficiently deep networks, the representations rapidly approach an orthogonal matrix. WhileDaneshmand et al. (2021)prove that the outputs converge to within an-ball close to orthogonality, we prove that the output representations become perfectly orthogonal in the infinite depth limit. This perfect orthogonalization turns out to be key in proving our result about avoiding gradient explosion. In fact, for MLPs initialized with Gaussian weights and BN,Yang et al. (2019, Theorem 3.9)prove that the gradients explode at an exponential rate in depth. In a striking contrast, we prove that gradients of an MLP with BN and orthogonal weights remain bounded as

Thus, the gradient is bounded by a constant that only depends on the network width where the expectation is taken over the random weight matrices. It is worth noting that both isometry and log-norm gradient bounds are derivednon-asymptotically. Thus, in contrast to the previously studied mean-field or infinite width regime, our theoretical results hold in practical settings where the width is finite.

The limitation of our theory is that it holds for a simplification in the BN module and linear activations. However, our results provide guidelines to avoid gradient explosion in MLPs with non-linear activations. We experimentally show that it is possible to avoid gradient explosion for certain non-linear activations with orthogonal random weights together with “activation shaping”(Martens et al.,2021). Finally, we experimentally demonstrate that avoiding gradient explosion stabilizes the training of deep MLPs with BN.

SECTION: 2Related work

Large depth poses challenges for the optimization of neural networks, which becomes slower by increasing the number of layers. This depth related slowdown is mainly attributed to: (i) gradient vanishing/explosion, and (ii) the rank collapse of hidden representations. (i) Gradient vanishing and explosion is a classic problem in neural networks(Hochreiter,1998). For some neural architectures, this issue can be effectively solved. For example,He et al. (2015)propose a particular initialization scheme that avoids gradient vanishing/explosion for neural networks with rectifier non-linearities whileGlorot and Bengio (2010)study the effect of initialization on sigmoidal activations. However, such initializations cannot avoid gradient explosion for networks with batch normalization(Yang et al.,2019; Lubana et al.,2021). (ii)Saxe et al. (2013a)demonstrate that outputs become independent from inputs with growing depth, which is called the rank collapse issue(Daneshmand et al.,2020; Dong et al.,2021). Various techniques have been developed to avoid rank collapse such as batch normalization(Ioffe and Szegedy,2015), residual connections(He et al.,2016), and self-normalizing activations(Klambauer et al.,2017).
Here, we focus on batch normalization since our primary goal is to avoid the systemic issue of gradient explosion for batch normalization.

Saxe et al. (2013a)propose initializing the weights with random orthogonal matrices for linear networks without normalization layers. Orthogonal matrices avoid the rank collapse issue in linear networks, thereby enabling a depth-independent training convergence.Pennington et al. (2017)show that MLPs with sigmoidal activations achieve dynamical isometry when initialized with orthogonal weights. Similar benefits have been achieved by initializing CNNs with orthogonal or almost orthogonal kernels(Xiao et al.,2018; Mishkin and Matas,2015), and by initializing RNN transition matrices with elements from the orthogonal and unitary ensembles(Arjovsky et al.,2016; Le et al.,2015; Henaff et al.,2016). Similarly, we use orthogonal random matrices to avoid gradient explosion. What sets our study apart from this literature is that our focus is on batch normalization and the issue of gradient explosion.

Due to its analytical simplicity, the identity function has been widely used in theoretical studies for neural networks. Studies on identity activations date back to at least two decades.Fukumizu (1998)studies batch gradient descent in linear neural networks and its effect on overfitting and generalization.Baldi and Hornik (1995)provide an overview over various theoretical manuscripts studying linear neural networks. Despite linearity, asSaxe et al. (2013a,b)observe, the gradient dynamics in a linear MLP are highly nonlinear. In a line of work,Saxe et al. (2013b,a,2019)study the training dynamics of deep neural networks with identity activations and introduce the notion of dynamical isometry.Baldi and Hornik (1989)andYun et al. (2017)study the mean squared error optimization landscape in linear MLPs. More recently, the optimum convergence rate of gradient descent in deep linear neural networks has been studied byArora et al. (2018)andShamir (2019).Du and Hu (2019)prove that under certain conditions on the model width and input degeneracy, linear MLPs with Xavier initialized weights(Glorot and Bengio,2010)converge linearly to the global optimum. Akin to these studies, we also analyze networks with linear activations. However, batch normalization is a non-linear function, hence the network we study in this paper is a highly non-linear function of its inputs.

The existing analyses for random networks often rely on mean-field regimes where the network width tends to infinity(Pennington et al.,2017; Yang et al.,2019; Li et al.,2022; Pennington and Worah,2017). However, there is a discrepancy between mean-field regimes and the practical regime of finite width. While some analyses attempt to bridge this gap(Joudaki et al.,2023b; Daneshmand et al.,2021), their results rely on technical assumptions that are hard to validate. In contrast, our non-asymptotic results hold for standard neural networks used in practice. Namely, our main assumption for avoiding rank collapse and gradient explosion is that samples in the input batch are not linearly dependent, which we will show is necessary. To go beyond mean-field regimes, we leverage recent theoretical advancements in Weingarten calculus(Weingarten,1978; Collins,2003; Banica et al.,2011; Collins and Śniady,2006; Collins et al.,2022).

SECTION: 3Main results

We will develop our theory by constructing networks that do not suffer from gradient explosion (Sec.3.3) and still orthogonalize (Sec.3.1). The construction is similar to the network studied byDaneshmand et al. (2021): an MLP with batch normalization and linear activations. Formally, letdenote the representation ofsamples inat layer, then

whereare random weights. Analogous to recent theoretical studies of batch normalization(Daneshmand et al.,2021,2020), we define the BN operatoras

Note that compared to the standard BN operator, mean reduction in equation4is omitted. Our motivation for this modification, similar toDaneshmand et al. (2021), is purely technical and to streamline our theory. We will experimentally show that using standard BN modules instead does not influence our results on gradient explosion and signal propagation (for more details see FigureG6). A second minor difference is that in the denominator, we have omitted afactor. However, this only amounts to a constant scaling of the representations and does not affect our results.

Compared toDaneshmand et al. (2021), we need two main modifications to avoid gradient explosion: (i), and (ii)are randomorthogonalmatrices. More precisely, we assume the distribution ofis the Haar measure over the orthogonal group denoted by(Collins and Śniady,2006). Such an initialization scheme is widely used in deep neural networks without batch normalization(Saxe et al.,2013a; Xiao et al.,2018; Pennington et al.,2017). For MLP networks with BN, we prove such initialization avoids the issue of gradient explosion, while simultaneously orthogonalizing the inputs.

SECTION: 3.1Tracking signal propagation via orthogonality

As discussed, batch normalization has an important orthogonalization bias that influences training. Without normalization layers, representations in many architectures face the issue of rank-collapse, which happens when network outputs become collinear for arbitrary inputs, hence their directions become insensitive to the changes in the input. In contrast, the outputs in networks with batch normalization become increasingly orthogonal through the layers, thereby enhancing the signal propagation in depth(Daneshmand et al.,2021). Thus, it is important to check whether the constructed network maintains the important property of orthogonalization.

Our analysis relies on the notion ofisometry gap,, introduced byJoudaki et al. (2023a). Isometry gap is defined as

One can readily check thatand it is zero whenis an orthogonal matrix, i.e.,. Theisometrydenoted byis defined as.

While the formula for isometry gap may seem enigmatic at first, it has a simple geometric interpretation that makes it intuitively understandable. The determinantis the squared volume of the parallelepiped spanned by the columns of, whileis the sum squared-norms of the columns of. Thus, the ratio between the two provides a scale-invariant notion of volume and isometry. On the one hand, if there is any collinearity between the columns, the volume will vanish and the isometry gap will be infinity,. On the other hand,impliesis a scaled identity matrix. We will proveserves as a Lyapunov function for the chain of hidden representations.

The following theorem establishes the link between orthogonality of representations and depth.

There is an absolute constantsuch that for any layerwe have

Theorem1states that if the samples in the input batch are not linearly dependent, representations approach orthogonality at an exponential rate in depth. The orthogonalization in depth ensures the avoidance of the rank collapse of representations, which is a known barrier to training deep neural networks(Daneshmand et al.,2020; Saxe et al.,2013a; Bjorck et al.,2018).

Figure1compares the established theoretical decay rate ofwith the practical rate. Interestingly, the plot confirms that the rate depends on width in practice, akin to the theoretical rate in Theorem1.

It is worth mentioning that the condition on the input samples to not be linearly dependent is necessary to establish this result. One can readily check that starting from a rank-deficient input, neither matrix products, nor batch-normalization operations can increase the rank of the representations. Since this assumption is quantitative, we can numerically verify it by randomly drawing many input mini-batches and check if they are linearly dependent. For CIFAR10, CIFAR100, MNIST and FashionMNIST, we empirically tested that most batches across various batch sizes are full-rank (see SectionDfor details on the average rank of a batch in these datasets).

Theorem1distinguishes itself from the existing orthogonalization results in the literature(Yang et al.,2019; Joudaki et al.,2023a)as it is non-asymptotic and holds for networks with finite width. Since practical networks have finite width and depth, non-asymptotic results are crucial for their applicability to real-world settings. WhileDaneshmand et al. (2021)provide a non-asymptotic bound for orthogonalization, the main result relies on an assumption that is hard to verify.

Proof idea of Theorem1.We leverage a recent result established byJoudaki et al. (2023a), proving that the isometry gap does not decrease with BN layers.
For all non-degenerate matrices, the following holds

Using the above result, we can prove that matrix multiplication with orthogonal weights also does not decrease isometry as stated in the next lemma.

Letandbe an orthogonal matrix and; then,

It is straightforward to check that there exists at least an orthogonal matrixfor which(see CorollaryA.3). Thus,strictly increases for some weight matrices, as long asis not orthogonal. When the distribution ofis the Haar measure over the orthogonal group, we can leverage recent developments in Weingarten calculus(Weingarten,1978; Banica et al.,2011; Collins and Śniady,2006; Collins et al.,2022)to calculate a rate for the isometry increase in expectation:

Supposeis a matrix drawn fromsuch that the distribution ofandare the same for all orthogonal matrices. Letbe the eigenvalues of. Then,

holds for all, with equality for orthogonal matrices.

The structure ininduced byBnensures its eigenvalues lie in the interval, in that the multiplicative factor in the above inequality is always greater than one. In other words,increases by a constant factor in expectation that depends on how closeis to an orthogonal matrix.

The connection between Theorem3and the main isometry gap bound stated in Theorem1is established in the following Corollary (recall).

Suppose the same setup as in Theorem3, where. Then, we have:

Notice that the term, yielding.
The rest of proof is based on an induction over the layers, presented in AppendicesAandB.

SECTION: 3.2Orthogonalization and gradient explosion

There is a subtle connection between orthogonalization and gradient explosion. Suppose the input batch is rank-deficient, i.e., degenerate. As elaborated above, since all operations in our MLP can be formulated as matrix products, they cannot recover the rank of the representations, which thus remain degenerate. By perturbing the input such that it becomes full-rank, the output matrix becomes orthogonal, hence non-degenerate at an exponential rate in depth as proven in Theorem1.

Thus, a slight change in inputs leads to a significant change in outputs from degeneracy to orthogonality. Considering that the gradient measures changes in the loss for infinitesimal inputs changes, the large changes in outputs potentially lead to gradient explosion. While this is only an intuitive argument, we observe that in practice the gradient does explode for degenerate inputs, as shown in Figure2.

Nonetheless, in Figure2we observe that for non-degenerate inputs the gradient norm does not explode. In fact, we observe that inputs are often non-degenerate in practice (see TableD1for details). Thus, an important question is whether the gradient norm remains bounded for non-degenerate input batches. Remarkably, we can not empirically verify that forall degenerate inputsthe gradient norm remains bounded. Therefore, a theoretical guarantee is necessary to ensure avoiding gradient explosion.

SECTION: 3.3Avoiding gradient explosion in depth

So far, we have proven that the constructed network maintains the orthogonalization property of BN. Now, we turn our focus to the gradient analysis. The next theorem proves that the constructed network does not suffer from gradient explosion in depth for non-degenerate input matrices.

Let the loss functionbe-Lipschitz, and input batchbe non-degenerate. Then, there exists an absolute constantsuch that for allit holds

where the expectation is over the random orthogonal weight matrices.

For degenerate inputsholds, in that the bound becomes vacuous.

The-Lipschitz condition holds in many practical settings. For example, in a classification setting, MSE and cross entropy losses obey the-Lipschitz condition (see LemmaC.2).

Note that the bound is stated for the expected value of log-norm of the gradients, which can be interpreted as bits of precision needed to store the gradient matrices. Thus, the fact that depth does not appear in any form in the upper bound of Theorem5points out that training arbitrarily deep MLPs with orthogonal weights will not face numerical issues that arise with Gaussian weights(Yang et al.,2019)as long as the inputs are non-degenerate. Such guarantees are necessary to ensure backpropagation will not face numerical issues.

Theorem5states that as long as the input samples are not linearly dependent, the gradients remain bounded for any arbitrary depth. As discussed in the previous section and evidenced in Figure2, this is necessary to avoid gradient explosion. Therefore, the upper bound provided in Theorem5is tight in terms of inputs constraints. Furthermore, as mentioned before, random batches sampled from commonly used benchmarks, such as CIFAR10, CIFAR100, MNIST, and FashionMNIST, are non-degenerate in most practical cases (see SectionDfor more details). Thus, the assumptions and thereby assertions of the theorem are valid for all practical purposes.

To the best of our knowledge, Theorem5is the first non-asymptotic gradient analysis that holds for networks with batch normalization and finite width. Previous results heavily rely on mean field analyses in asymptotic regimes, where the network width tends to infinity(Yang et al.,2019). While mean-field analyses have brought many insights about the rate of gradient explosion, they are often specific to Gaussian weights. Here, we show that non-Gaussian weights can avoid gradient explosion, which has previously been considered “unavoidable”(Yang et al.,2019). Figure3illustrates this pronounced discrepancy.

Proof idea of Theorem5.The first important observation is that, due to the chain rule, we can bound the log-norm of the gradient of a composition of functions, by bounding the summation of the log-norms of the input-output Jacobian of each layer, plus two additional terms corresponding to the loss term and the gradient of the first layer in the chain. If we discount the effect of the first and last terms, the bulk of the analysis is dedicated to bounding the total sum of log-norms of per layer input-output Jacobian, i.e., the fully connected and batch normalization layers. The second observation is that because the weights are only rotations, their Jacobian has eigenvalues equal toThus, the log-norm of gradients corresponding to fully connected layers vanish. What remains is to show that for any arbitrary depththe log-norm of gradients of batch normalization layers also remains bounded. The main technical novelty for proving this step is showing that the log-norm of the gradient ofBnlayers is upper bounded by the isometry gap of pre-normalization matrices. Thus, we can invoke the exponential decay in isometry gap stated in Theorem1to establish a bound on the log-norm of the gradient of these layers. Finally, since the decay in isometry gap is exponentially fast, the bound on the total sum of log-norm of the gradients amounts to a geometric sum that remains bounded for any arbitrary depth

SECTION: 4Implications on training

In this section, we experimentally validate the benefits of avoiding gradient explosion and rank collapse for training.
Thus far, we have proved that our constructed neural network with BN does not suffer from gradient explosion in Theorem5, and does not have the rank collapse issue in depth via the orthogonalization property established in Theorem1. We find that the constructed MLP is therefore less prone to numerical issues that arise when training deep networks.

By avoiding gradient explosion and rank collapse in depth, we observe that the optimization with vanilla minibatch Stochastic Gradient Descent (SGD) exhibits an almost depth-independent convergence rate for linear MLPs. In other words, the number of iterations to get to a certain accuracy does not vary widely between networks with different depths. Figure4(c) shows the convergence of SGD for CIFAR10, with learning rate, for MLPs with widthand batch sizeWhile the SGD trajectory strongly diverges from the initial conditions that we analyze theoretically, Figure4shows that the gradients remain stable during training, as well as the fact that different depths exhibit largely similar accuracy curves.

While the empirical evidence for our MLP with linear activation is encouraging, non-linear activations are essential parts of feature learning(Nair and Hinton,2010; Klambauer et al.,2017; Hendrycks and Gimpel,2016; Maas et al.,2013). However, introducing non-linearity violates one of the key parts of our theory, in that it prevents representations from reaching perfect isometry (see FigureG5in the Appendix for details on the connection between non-linearities and gradient explosion in depth). Intuitively, this is due to the fact that non-linear layers, as opposed to rotations and batch normalization, perturb the isometry of representations and prevent them from reaching zero isometry gap in depth. This problem turns out to be not just a theoretical nuisance, but to play a direct role in the gradient explosion behavior.
While the situation may seem futile at first, it turns out that activation shaping(Li et al.,2022; Zhang et al.,2022; Martens et al.,2021; He et al.,2023; Noci et al.,2023)can alleviate this problem, which is discussed next. For the remainder of this section, we focus on the training of MLPs with non-linear activations, as well as standard batch normalization and fully connected layers.

SECTION: 5Activation shaping based on the theoretical analysis

In recent years, several works have attempted to overcome the challenges of training very deep networks by parameterizing activation functions.
In a seminal work,Martens et al. (2021)proposedeep kernel shaping, which is aimed at facilitating the training of deep networks without relying on skip connections or normalization layers, and was later extended to LeakyReLU intailored activation transformations(Zhang et al.,2022). In a similar direction,Li et al. (2022)proposeactivation shapingin order to avoid a degenerate output covariance.Li et al. (2022)propose shaping the negative slope of LeakyReLU towards identity to ensure that the output covariance matrix remains non-degenerate when the networks becomes very deep.

Since kernel and activation shaping aim to replace normalization, they have not been used in conjunction with normalization layers.
In fact, in networks with batch normalization, even linear activations have non-degenerate outputs(Daneshmand et al.,2021; Yang et al.,2019)and exploding gradients(Yang et al.,2019). Thus, shaping activations towards identity in the presence of normalization layers may seem fruitless. Remarkably, we empirically demonstrate that we can leverage activation shaping to avoid gradient explosion in depth by using a pre-activation gain at each layer.

Inspired by our theory, we develop a novel activation shaping scheme for networks with BN. The main strategy consists of shaping the activation function towards a linear function across the layers. Our activation shaping consists of tuning the gain of the activation, i.e., tuningforWe consider non-linear activations

The special property that bothandactivations have in common is that they are centered,are differentiable around the originand have bounded gradients. Therefore, by tuning the per-layer pre-activation gaintowards, the non-linearities behave akin to the identity function. This observation inspires us to study the relationship between the rate of gradient explosion for each layer as a function of the gain parameter. Formally, we consider an MLP with shaped activations using gainfor theth layer, that has the update rule

Since the gradient norm has an exponential growth in depth, as shown in Figure5, we can compute the slope of the linear growth rate of log-norm of gradients in depth. We define the rate of explosion for a model of depthand gainat layeras the slope of the log norm of the gradients. We show in Figure5that by tuning the gain properly, we are able to reduce the exponential rate of the log-norm of the gradients by diminishing the slope of the rate curve and achieve networks trainable at arbitrary depths, while still maintaining the benefits of the non-linear activation. The main idea for our activation shaping strategy is to have a bounded total sum of rates across layers, by ensuring faster decay than a harmonic series (see App.Efor more details on activation shaping). Figure5illustrates that this activation shaping strategy effectively avoids gradient explosion while maintaining the signal propagation and orthogonality of the outputs in depth. Furthermore, Figure4shows that the training accuracy remains largely depth-independent. For further experiments using activation shaping, see AppendixG.

SECTION: 6Discussion

Optimization over orthogonal matrices has been an effective approach for training deep neural networks. Enforcing orthogonality during training ensures that the spectrum of the weight matrices remains bounded, which prevents gradient vanishing and explosion in depth.Vorontsov et al. (2017)study how different orthogonality constraints affect training performance in RNNs. For exampleLezcano-Casado and Martınez-Rubio (2019)leverage the exponential map on the orthogonal group,Jose et al. (2018)decompose RNN transition matrices in Kronecker factors and impose soft constraints on each factor andMhammedi et al. (2017)introduce a constraint based on Householder matrices.

While these studiesenforceorthogonality constraints, one of our most striking empirical observations is that when our MLP grows very deep, the middle layers remain almost orthogonal even after many steps of SGD. As shown in Figure6, forlayer networks, the middle layers remain orthogonal during training. One could hypothesize that this is due to small gradients in these layers. In Figure6, we observe that the gradients of these middle layers are not negligible. Thus, in our MLP construction, both with linear activation and with activation shaping, the gradient dynamics have animplicit biasto optimize over the space of orthogonal matrices. The mechanisms underlying this implicit orthogonality bias will be an ample direction for future research.

Alexandru Meterez: proofs of SectionA, driving experiments, and writing the paper. Amir Joudaki: proofs of SectionBand SectionC, designing the activation shaping scheme, and writing the paper. Francesco Orabona: proposing the idea of using orthogonal weights to achieve prefect isometry, reading the proofs, help with writing. Alexander Immer: reading the proofs, help with experiments and paper writing. Gunnar Rätsch: help with experimental designs for activation shaping and paper writing. Hadi Daneshmand: proposed using orthogonal weights to avoid gradients explosion, leading the proofs help with paper writing.

Amir Joudaki is funded through Swiss National Science Foundation Project Grant #200550 to Andre Kahles. We acknowledge support from the NSF TRIPODS program (award DMS-2022448). Alexander Immer acknowledges support from the Max Planck ETH Center for Learning Systems. Amir Joudaki and Alexander Immer were partially funded by ETH Core funding (to G.R.).

SECTION: References

SECTION: Table of contents

The appendix is structured in the following sections:

AppendixA: contains the proof based on Weingarten calculus for the conditional isometry gap decay rate, conditioned on the previous layer in the network.

AppendixB: contains the proof for the isometry gap exponential rate decay in depth, based on the conditional rate established in AppendixA.

AppendixC: contains the proof for the log-norm gradient bound at arbitrary depths, based on the orthogonality result established in AppendixB.

AppendixD: contains the numerical verification of the average rank of the input batch in common datasets.

AppendixE: contains the full derivation and additional explanations of the activation shaping heuristic.

AppendixF: contains further results on CIFAR10, MNIST, FashionMNIST and CIFAR100, showcasing the implicit orthogonality bias of SGD during training.

AppendixG: contains supplemental experiments, including train and test results on MNIST, FashionMNIST, CIFAR100, as well as the effect of mean reduction and non-linearities without shaping on the gradient log-norms.

SECTION: Appendix AConditional orthogonalization

Our analysis is based on[Joudaki et al.,2023a, Corollary 3], which we restate in the following Lemma:

For all non-degenerate matrices, we have:

LemmaA.1proves isometry bias ofBn. The next lemma proves that isometry does not change under rotation

Letandbe a random orthogonal matrix and. Then,

Using properties of the determinant, we have

where the last equation holds sinceis an orthogonal matrix. Furthermore,

Combining the last two equations with LemmaA.1concludes the proof.
∎

The last lemma proves the isometry bias does not decrease with rotation andBn. However, this does not prove a strict decrease in isometry withBnand rotation. The next lemma proves there exists an orthogonal matrix for which the isometry is strictly increasing.

Letand denote its singular value decomposition, whereandare orthogonal matrices. Then, we have:

Letbe the diagonal matrix containing the singular values of. Then, we have:

which has maximum isometry 1 since it’s an orthonormal matrix.
∎

Thus, there exists a rotation that increases isometry withBnfor each non-orthogonal matrix.
The proof of the last corollary is based on a straightforward application of Lemma2.

The isometric is non-decreasing in Lemma2and provably increases for a certain rotation matrix (as stated in the last corollary). Hence, it is possible to increase isometry with random orthogonal matrices.

Supposeis a matrix drawn fromsuch thatfor all orthogonal matrices. Letbe the eigenvalues of. Then,

holds for all, with equality for orthogonal matrices.

Note that the assumption oncan be viewed as an induction hypothesis, in that we can recursively apply this theorem to arrive at a quantitative rate at depth.

Notably,if. Hence, one can expect thatfor all full random matricesin form of.

We need to compute the variance/mean ratio in Lemma2.
Lethave SVD decompositionwhereandare orthogonal matrix and. Since the distribution ofis invariant to transformations with orthogonal matrices, the distribution ofequates those of. It easy to check that

Thus,

where the last equality holds due to the batch normalization.
Thus, we have

We need to estimate

Since square root function is concave, we have.
Thus

where in the first equality we have used the fact that the cross terms reduce, where the expectations are applications of Weingarten calculus:

The main quantity we must compute is an expectation of polynomials taken over the Haar measure of the Orthogonal group. To carry out the computation, we make use of Weingarten calculus[Banica et al.,2011, Collins and Śniady,2006, Weingarten,1978]. More specifically, we make use of the of the Weingarten formula, studied by[Collins and Śniady,2006, Collins et al.,2022]:

whereis the Haar measure of Orthogonal group. For an in depth explanation of each quantity in the Weingarten formula, we refer the reader toCollins et al. [2022, Section 5.2].

The quantity we focus on is. We will do the computation on multiple cases, based on the equalities of. Notice thatin all cases. It suffices if we focus on the two distinct cases:and.

We first compute.

Following the procedure fromCollins et al. [2022, Section 5.2], we take the index sequences to beand. Similarly, we getonly ifand.

Consderingas permutations, we get:

wherehas coset-type. Finally, we plug the results back into the formula and we obtain:

where the last equality is based on the results in Section 7 of[Collins and Matsumoto,2009].

We compute.

Similar to the previous expression, we take the index sequences to be to beand. Thus, we obtainonly ifand,,. Similarly, we computefor. Notice thatis the identity permutation, thus yielding, with the coset-typesrespectively, for each.

Plugging back into the original equation, we obtain:

Thus, plugging back into the original inequality, we obtain:

where we have used thatin the last equality.

Thus, we obtain:

∎

Suppose the same setup as in Theorem3. Then, we have:

Notice that the term, yielding.

From Lemma2, we know that:

where in inequality60we have used the fact thatand in inequality61we have used the bound obtained in proof Theorem3, equation55.
Thus, we obtain:

∎

SECTION: Appendix BIsometry gap decay rate in depth

Before we start with the main part of our analysis, let us establish a simple result on the relation between isometry gap and orthogonality:

Ifthen eigenvalues ofare within

Note that, in order to simplify the calculations, we use the fact thatin the following proofs.

Based on the conditional expectation in CorollaryA.5, we have:

Now, we prove a lemma that is conditioned on the previous layer isometry gap being smaller or larger than.

Forbeing the representations of an MLP under our setting, we have:

Let, and assume without loss of generality that. Then, using the numerical inequality, whenwe have:

Altogether, we have

Now, we can restate the condition in terms of an inequality on the isometry gap. Thus, we can write:

where we used the fact thatimplyingand also used the inequalitywhenfor’s. Note that because, we getall terms on the right-hand side are positive, implying that each term is bounded by the upper bound:

By construction, we haveandSince, we can multiply both sides byand conclude. We can now solve the quadratic equation and obtainwhich numerically becomes, implying.

By solving the quadratic equation above we can guarantee that

Furthermore, we can restate the condition on maximum using:

and conclude that

Using this statement, we have

If we negate and flip the two sides we arrive at

Thus, we can simplify the recurrence

as follows

where we used equation72in the first one and equation76in the second one.
∎

From LemmaA.1, we know that, for any layer. Thus, we get using equation66:

where in the last step we have used the fact that.

Thus, we can combine equation65and equation66and obtain:

By iterated expectations overwe get:

Note that since, we can conclude the proof.
∎

SECTION: Appendix CGradient norm bound

In the following section, we denote bythe pre-normalization values. Moreover, we define as, whereis the number of output classes, as the functional composition of anlayers MLP, following the update rule defined in equation3, i.e.:

Let us restate the theorem for completeness of the appendix:

For any-Lipschitz loss functionand non-degenerate input, we have:

holds for all, where possibly.

In particular, the following lemma guarantees that the Lipschitz conditions are met for practical loss functions:

In a classification setting, cross entropy and mean squared error losses are-Lipschitz.

The main idea for the feasibility of this theorem is the presence of perfectly isometric weight matrices that are orthonormal, and the linear activation that does not lead to vanishing or exploding gradients. The only remaining layers to be analyzed are the batch normalization layers. Thus, our main goal is to show that the sum of log-norm gradient ofBnlayers remains bounded even if the network has infinite depth. To do so, we shall relate the norm of the gradient of those layers to the isometry gap of representations, and use the bounds from the previous section to establish that the log-norm sum is bounded.

Now, considering anlayer deep model, wherecan possibly be, we can finalize the proof of Theorem5. Consider an MLP model as defined in equation3. Letbe the logits of the model, where,,is an orthogonal matrix andis the number of output classes. Denote asthe loss of model for an input matrix, with ground truth. Then, applying the chain rule, we have:

By taking the logarithm of the norm of each factor and applying LemmaC.12, we get:

where, since the orthogonal matrixhas operator norm.

Sinceandis batch normalized, this means that.
Thus, the main part is to bound the Jacobian log-norms, which is provided by the following lemma:

We have

Finally, we can plug the bound from LemmaC.3in equation96and obtain the conclusion:

Note that, for, we get:

In order to conclude the bound, it suffices to show that the norm of the gradient of the loss with respect to the logits is bounded, which is the objective of LemmaC.2.
∎

The proof of this lemma is chiefly relying on the following bound on the Jacobian of batch normalization layers, which we will state and prove beforehand.

Ifis the input to aBnlayer, its Jacobian operator norm is bounded by

Furthermore, if, then we have

Based on the lemma above, we shall defineas the hitting time, corresponding to the first layer in our case, that the isometry gap drops below the critical value of:

So, we first bound the total log-grad norm for layersup to, and subsequentlyup to:

where we have used thatas an upper bound on.

Thus, taking expectation we get:

Note thatis a random variable, which is why the expectation over the number of layers appears at the last line. Thus, we can bound the log-norm by boundingand the summation separately.

We haveif, andif.

We have

Thus, we have the following 2 cases, based on whetheris below or over thethreshold. If we plug the bounds in equation107we get the following.

If, then:

and ifthen:

In fact, the maximum of the two bounds is

∎

By the bound in LemmaB.2, we have

Since we assumed, the conditional inequality always holds and thus we have the Markov bound

We define as failure the eventwith probability, and conversely as success the eventwith probability.
In other words, the probability thatdoes not decrease by at least a factor ofis bounded by the failure probability

Since, then under the assumption thatwe can upper boundwithin case of failure with probability, and within case of success with probability:

Thus, for, we have

The summation starts from belowand will decay by ratewhich is upper bounded by the geometric series:

∎

By LemmaB.2we have

Thus, we have

Sinceis a non-negative integer valued random variable, we can thus boundas:

∎

SECTION: 1Proof of LemmaC.4: Bounding BN grad-norm with isometry gap

The proof of the Lemma relies on two main observations that are crystallized in the following lemmas that first establish a bound on Jacobian operator norm based on the inverse of smallest eigenvalue, and then establish a lower bound for the smallest eigenvalue using the isometry gap.

Letand letbe the eigenvalues of. Then, we have that:

whereis the Jacobian of theoperator.

Using the above lemma we have. The following lemma upper bounds this quantity using isometry gap:

The minimum eigenvalue of a Gram matrix that is trace-normalized is lower-bounded by the isometry gap asFurthermore, ifthen.

Plugging these two values we have the bounds

Now we can turn our attention to the proof of the Lemmas used in the proof. The proof of relationship between minimum eigenvalue and isometry gap is obtained by merely a few numerical inequalities:

Letbe the eigenvalues of. Since the matrix is trace-normalized, we have.

The arithmetic mean of the topvalues can be written as

Thus, we have that their geometric mean is bounded by the same value.
Therefore, we have the following bound:

where in the second inequality we have taken logarithm of both sides. Now, we can apply the numerical inequalityto conclude:

Sinceis non-negative, this clearly implies the first inequality:.

For the second inequality first we use the numerical inequalityto conclude

We can now use the inequalityforto conclude that

when, which is equivalent to.

∎

For proving LemmaC.4, we first analyzeBnoperator on a row, and then invoke this bound and the special structureto derive the main proof.

Letdefined asbe the elementwise normalization of the. Then:

whereis the outer product.

To begin, notice that forwe have.
Denote by. Then the Jacobian entries become:

Assembling the equations into matrix form, we obtain:

∎

Thehas the eigenvaluewith multiplicityandwith multiplicity.

Let, whereis the eigendecomposition of. Then, we have that.

where in the last equality we have used the fact thatorthogonal.
Since this is true for all rows, we get.
∎

Having established the above properties, we are equipped to prove that the Jacobian of a BN layer is bounded by the inverse of the minimum eigenvalue of its input Gram matrix.

To begin, notice that since, implies that. Denote. Since the normalization happens on each row independently of the other rows, the only non-zero derivatives in the Jacobian correspond to changes in output rowwith regards to the same input row, i.e.:

This creates a block-diagonal structure in, withblocks on the main diagonal, where each block has sizeand is equal to, whereis as defined in LemmaC.9. Therefore, due to the block-diagonal structure, we know that:

wheredenotes the eigenvalue spectrum. From CorollaryC.10, we know that

with their respective multiplicites. Finally, using LemmaC.11, this implies:

∎

Assuming the the logits are passed through a softmax layer, we analyze the case of Cross Entropy Loss for one samplein a-classes classification problem. Denoting, we have:

whereis the probability vector for sampleafter passing through the softmax function.

Computing the partial derivatives, we obtain:

Finally, we can compute the gradient of the loss with respect to the logits:

Since the gradient of the loss with regard to each sample is bounded, we can conclude that the operator norm of the Jacobian of the loss with regards to the logits matrixis also bounded.

In a similar analysis, we now shift our attention towards the Mean Squared Error (MSE) loss:

We want to compute the gradient of the loss with respect to each logit:

By substituting these derivatives into the gradient equation, we can derive the gradient for each logit with respect to the MSE loss.

∎

Letbe the hidden representations of layeras defined in equation3. Then, we have that:

By definition, we have that:

Therefore, applying the chain rule, we get:

Taking the logarithm of the norm of this quantity, we reach the conclusion:

where we have used the fact that the spectrum of the orthogonal matrixcontains only the singular valuewith multiplicity.
∎

SECTION: Appendix DLinear independence in common datasets

In this section, we empirically verify the assumption that popular datasets do not suffer from rank collapse in most practical settings.

We provide empirical evidence for CIFAR10, MNIST, FashionMNIST and CIFAR100. We test this assumption by randomly samplinginput batches of sizesfrom each of these datasets and then measuring the rank of the Gram matrix of these randomly sampled batches using thematrix_rank()function provided in PyTorch. We stop at sizesince we approach the dimensionality of some datasets, i.e. FashionMNIST, MNIST. We show in TableD1the average rank with the standard deviation for each, overrandomly sampled batches.

We would like to remark that these datasets are fairly simple in terms of dimensionality and semantics, which can lead to correlated samples. Furthermore, the rank degeneracy can be alleviated even in the larger batch sizes through various data augmentations techniques. Note that these datasets have a high degree of correlation between samples. Most notably, the average cosine similarity between samples in asize batch isfor CIFAR10, MNIST, FashionMNIST and CIFAR100 respectively.

SECTION: Appendix EActivation shaping

In this section, we explain the full procedure for shaping the activation, as well as expand on the heuristic we use to choose the pre-activation gain. Under the functional structure of the MLP in  equation11, letbe the pre-activation gain.

More formally, since the gradient norm has an exponential growth in depth, as shown in FigureG5, we can compute the linear growth rate of log-norm of gradients in depth. We define the rate of explosion for a model of depthand gainat layeras the slope of the log norm of the gradients:

Since the rate function is not perfectly linear and has noisy peaks, we measure the slope with alayer gap in order to capture the true behaviour instead of the noise.

Our goal is to choosesuch that the sum of the rates across the layers in depth is bounded by a constant that does not depend on the depth of the model, i.e., whereis independent of. One choice to achieve this is to pick a gain such that the sum of the rates behaves like a decaying harmonic sum in depth.

To this end, we measure the rate of explosion at multiple layers in alayer deep model, for various gainswhich are constant across the layers in FigureE1and notice that it behaves as. In order to have the sum of rates across layers behave like a bounded harmonic series in depth, we must choose the gain such that it decays roughly aswhereresults in convergence. Therefore, we can obtain a heuristic for picking a gain such that the gradients remain bounded in depth as, where we refer toas the gain exponent.

This reduces the problem to picking the exponent such that the sum stays bounded. We show how the behaviour of the explosion rate at the early layers, for various models, is impacted by the exponent in FigureE2. Note that for several exponent values, we able to reduce the exponential explosion rate and obtain trainable models, which we show in AppendixG.

SECTION: Appendix FImplicit orthogonality during training

In this section, we provide empirical evidence that our architecture during training maintains orthogonality across depths, while maintaining bounded gradients. FigureF1shows the evolution of the isometry gap of the weight matricesduring training, for models at different depths and different nonlinearities. In order to show that these weights are updated gradient descent, we also show the evolution of the norm of the loss gradients with regards to matricesin FigureF2.

These experiments are performed on an MLP with orthogonal weight matrices and batch normalization, with sin and tanh activations. The width is set to, batch sizeand learning rate. The gain exponent is set to a fixed value for all experiments. The measurements are performed on a single batch of sizefrom CIFAR10, after each epoch of training on the same dataset.

SECTION: Appendix GOther experiments

In this section we provide the train and test accuracies of deep MLPs on 4 popular image datasets, namely MNIST, FashionMNIST, CIFAR10, CIFAR100. Hyperparameters and measurements procedure are described in Section4.

SECTION: 1Supplementary train and test results on MNIST, FashionMNIST, CIFAR10, CIFAR100

SECTION: 2Supplemental figures

We present empirical results in Figure2showing that degenerate input batches are a hard constraint for orthogonalization without gradient explosion. For MLPs with different depths, we show that by repeating samples in a batch of sizewe get an exponential gradient explosion, which is unavoidable theoretically.

Furthermore, we show how non-linearities affect the gradient explosion rate in FigureG5. Using standard batch normalization and fully connected layers from PyTorch we show that non-linearities maintain a large isometry gap. This is a critical issue for our theoretical framework, since we take advantage of the fact that the identity activation achieves perfect orthogonality in order to prove that the gradients remain bounded in depth.

SECTION: 3Influence of mean reduction on the gradient bound

In this section, we compare whether adding mean reduction and the additional factor ofin the denominator of the batch normalization module influences our gradient bound. As expected, we show in FigureG6that in both cases, for the identity activation, the result remains similar, with the gradients remaining bounded in depth.