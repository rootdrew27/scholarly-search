SECTION: Practical Considerations for Agentic LLM Systems

As the strength of Large Language Models (LLMs) has grown over recent years, so too has interest in their use as the underlying models for autonomous agents. Although LLMs demonstrate emergent abilities and broad expertise across natural language domains, their inherent unpredictability makes the implementation of LLM agents challenging, resulting in a gap between related research and the real-world implementation of such systems. To bridge this gap, this paper frames actionable insights and considerations from the research community in the context of established application paradigms to enable the construction and facilitate the informed deployment of robust LLM agents. Namely, we position relevant research findings into four broad categories—Planning, Memory, Tools, and Control Flow—based on common practices in application-focused literature and highlight practical considerations to make when designing agentic LLMs for real-world applications, such as handling stochasticity and managing resources efficiently. While we do not conduct empirical evaluations, we do provide the necessary background for discussing critical aspects of agentic LLM designs, both in academia and industry.

SECTION: 1.Introduction

In academia, the concept of ”agents” has been well-defined for decades (e.g.,(Wooldridge and Jennings,1995)), and thus the proposition of agents based on LLMs comes with predefined criteria and expectations. As such, agentic LLMs in the research community have come to be defined as autonomous systems with capabilities of beliefs(Sclar et al.,2023; Li et al.,2023a; Park et al.,2023), reasoning(Zhang et al.,2024c), planning(Huang et al.,2024; Shen et al.,2023), and control(Shen et al.,2023). Under this definition, the ability to plan, reason, and interact with an environment have emerged as the key considerations for success(Xi et al.,2023).

For LLM agents in industry and real-world deployment, the history and breadth of agents has been condensed to a definition along the lines of ”a system that can use an LLM to reason through a problem, create a plan to solve the problem, and execute the plan with the help of a set of tools”(Varshney,2023). Most industry discussions follow this form, introducing the LLM as the central reasoning engine and adding planning, memory, and tools as three necessary modules (e.g.,(SAA,2024; PGA,2024; TFA,2024; Varshney,2023; Weng,2023)). Indeed, most industry resources focusing on deployable agentic LLM systems are accompanied by a diagram similar to Figure1, focusing largely on single agents. While this description is helpful for the most basic of LLM agents, it glosses over some of the more nuanced considerations that must be made for the informed construction of robust agentic LLM systems.

The prevailing view of LLM agents in industry brings to light the disparity between (1) research into LLMs and agents and (2) the application of agentic LLM systems in real-world scenarios. To bridge this gap, we propose framing relevant findings from the research community in the common industry view of LLM agents. To that end, we organize this work into four main sections—Planning, Memory, Tools, and Control Flow—that correspond to, respectively, planning, memory, tools, and the central reasoning engine components referenced above. We tailor the contents of this paper to black-box LLM-based single-agent systems typical of that industry perspective. By doing so, we hope to create an actionable and approachable survey that enables information exchange between academia and industry within a manageable scope.

A diagram showing five boxes containing the texts ”User Input,” ”Agent,” ”Planning,” ”Tools,” and ”Memory. The ”Agent” box sits between ”User Input” and the other three boxes.

SECTION: 2.Related Work

Many surveys discussing LLM-based agents focus on multi-agent frameworks and related ideas(Guo et al.,2024; Zhang et al.,2024c). While similar, the challenges facing multi-agent systems are distinct from the real-world deployment of a single LLM agent. Here, we focus more on deliberately crafting a robust agent rather than the orchestration of many.

Another approach, taken by(Yang et al.,2024), focuses on methods for improving agentic LLM performance starting at the underlying model, looking into data composition and training methodologies. We focus on implementation considerations that improve agentic LLM system performance from a black-box perspective, which lends itself more to real-world deployment. Others focus on creating unified taxonomies(Li,2024; Xi et al.,2023)or target a single component of LLM agents, such as planning(Huang et al.,2024).

The most similar work is(Wang et al.,2024c), providing a comprehensive survey of works relating to LLMs as agents as well as reviewing aspects of their design, application, and evaluation. While(Wang et al.,2024c)develops a valuable unified framework based on extant research, we leverage research findings to provide practical application-focused insights and frame our review in the context of the LLM agent paradigm that has developed organically in industry.

To the best of our knowledge, this is the first work that coalesces research relevant to LLM agents through the lens of common industry practices. We expand that contribution by not just presenting existing research but by extrapolating actionable insights and best practices from it.

SECTION: 3.Applied Scenario

To help illustrate some of the following points, we propose the example outlined in Figure2of applying an LLM agent as a pescetarian111Someone that does not eat meat, aside from fish and other seafood.meal assistant. We will refer to this as the primary example222While we attempt to select a simple example with some relevance to the real world, it is still a contrived example to demonstrate the points outlined in this work and may not fully reflect the complexities of the real world.throughout this work for consistency, citing specifics from it by the codes assigned in Figure2(e.g.,2.R1 to refer to ”Pescetarian recipe book”).

A diagram showing the user, task, and environment definition relating to a pescetarian meal assistant example.

SECTION: 4.Glossary

This glossary serves to briefly introduce the following terms that will be used across subsequent sections. Later sections will provide additional contextualization and examples of their utility but not necessarily explicit definitions.

Persona.A persona (also referred to as a ”role” or a ”profile”) is the identity assigned to the LLM, often as part of the system prompt. The persona is the lens through which the LLM will interpret and respond to prompts. The persona (e.g., Figure2.Pe2) can be defined and refined by an occupation (e.g., ”professional chef”), level and domain of expertise (e.g., ”specializing in pescetarian dishes”), and personality traits (e.g., ”friendly and understanding”) but can be further customized by adding details such as age, race, gender, and nationality(Argyle et al.,2023; Wang et al.,2024c).

Tool.Tools are the means by which an LLM can interact with its environment (beyond basic textual exchange) and access external resources(Qin et al.,2024a).Retrieval Augmented Generation(RAG) is commonly used as a tool (Figure2.T1) but is limited in its utility as it exclusivelyretrievesinformation about the environment. The true power of tools is realized when they are used to perform actions in the environment, such as the example in Figure2.T2 that would allow the pescetarian meal assistant to not only recommend recipes but also order the ingredients to prepare them. Other examples of tools include ground-truth verification methods (e.g., code execution and calculator usage) and real-time environment querying (e.g., requesting trending recipes)(Wang et al.,2024b).

Hyperparameters.This section includes a brief overview of hyperparameters we will reference but does not explore their technical details333See(Gem,2024b; Ope,[n.d.]; Ant,2024a)for common commercial API support for these hyperparameters..

Seed.Some LLM interfaces will have a “seed” parameter that should, provided all other parameters remain constant, produce the same output.

Temperature.Temperature corresponds to the degree to which randomness will be employed in selecting the output tokens. This usually plays out in higher temperature responses being more creative and rambling while lower temperature responses are more predictable and straight-to-the-point. Thus, for more consistent results, a lower temperature (e.g., 0.0 to 0.5) can be used.

Top-p.Top-p (also known as ”nucleus sampling;” introduced in(Holtzman et al.,2020)) corresponds to the probability threshold for selecting tokens that can form part of the output, bounded 0.0 to 1.0. Lower top-p values restrict the pool of tokens the LLM can choose from, resulting in more reproducible outputs.

SECTION: 5.Planning

Planning has long been a core component of agent research(Nau et al.,2004; Wooldridge and Jennings,1995); it allows more complex tasks to be handled in smaller, more manageable steps. Planning can also enhance the interpretability of an LLM agent, as the steps of the plan and the stopping criteria will be defined in a interpretable format.

SECTION: 5.1.LLMs and Planning

Despite anecdotal applications showing signs of successful LLM planning(Song et al.,2023), more holistic reviews suggest that LLMs make poor planners(Valmeekam et al.,2022,2024; Kambhampati,2024; Liu et al.,2023; Dagan et al.,2024). As such, if an LLM agent is to be deployed in an environment with a consistent task, manually curating a plan can alleviate the pains of poor LLM planning as well as provide an opportunity to manually craft relevant roles and prompts. Another option is to augment the LLM agent with an external planning tool, which has shown promise(Liu et al.,2023; Dagan et al.,2024). Because LLM planning remains an open area of research, the example in Figure2.P1 simply assumes planning capabilities without subscribing to a specific approach, for illustrative purposes.

To describe current approaches to planning in LLM agents, we categorize them intoimplicitandexplicitplanning. Forimplicit planning, some agents will rely on the LLM to iteratively determine the immediate next step until the task is complete, without ever eliciting a plan(Zhang et al.,2024a; Gur et al.,2024). This approach relies on the idea that, when provided with an end goal, the LLM can maintain an internal plan whose steps are revealed iteratively without any explicit plan formalization. This approach can be viable when the environment is dynamic or only partially observable, such as interacting with a webpage(Gur et al.,2024). The other form of implicit planning is the creation and execution of a plan in a single inference, as seen in prompting strategies such as Plan-and-Solve(Wang et al.,2023b)and, to a degree, zero-shot Chain-of-Thought(Kojima et al.,2022). These approaches rely on conditioning subsequent token generation (i.e., the ”execution”) on a plan by first generating said plan. Due to the single-hop nature of this approach, it is not recommended for complex tasks, particularly those that would benefit from feedback during execution.

Explicit planningis characterized by the explicit formalization of a multi-step plan, typically executed in a multi-hop fashion. The most basic form is to simply request the formulation of a plan and then execute it, as demonstrated by the Least-to-Most prompting strategy(Zhou et al.,2023). More advanced approaches will first develop a plan and then iteratively refine the plan as steps are executed(Liu et al.,2024a). Both of these require long-term planning, which is where LLMs tend to demonstrate lackluster performance(Valmeekam et al.,2022,2024; Kambhampati,2024).

SECTION: 5.2.Task Decomposition

It is important to understand the limitations of an LLM before formulating a plan for it to execute. Agentic LLM systems are often applied to problems that a single LLM call cannot resolve but a sequence of calls can. Tasks can typically be decomposed into smaller pieces that, when solved individually, can be reconstructed to produce the final solution(Zhou et al.,2023).

Returning to Figure2, the request made in Figure2.I1 is composed of multiple subtasks, namely: (1) retrieve recipes that contain rice, beans, and tomato that the user will like and (2) order any missing ingredients. It is also reasonable to decompose (1) further, into a retrieval of recipes that contain the required ingredients and separately a request to select the one that best fits the user’s tastes.

If decomposing a well-defined task manually, iteratively decomposing the task into subtasks and testing an LLM on them can provide valuable insight into what the LLM can consistently handle. Breaking down the problem logically is simple enough, but ascertaining which tasks an LLM can perform well and which require further decomposition can be challenging, particularly when dealing with stochasticity and prompt changes. It is recommended to evaluate the LLM agent frequently and systematically during this process, as discussed in Section9.2. It may be easier to start at the most basic building blocks of the tasks and combine them than to find the minimum number of viable tasks to start.

While it may be intuitive to assume that the more atomic the task the better, this is not always the case. It has been shown that LLMs not only possess the ability to solve multiple distinct tasks in a single query(Xiong et al.,2024; Son et al.,2024; Laskar et al.,2023)but that composing multiple tasks into a single prompt canincreaseperformance on all constituent tasks, as well as decreasing overall context usage(Son et al.,2024). However, the degree to which tasks may be combined should be the subject of rigorous experimentation for the specific task and environment in which it is considered.

SECTION: 5.3.Plan Adherence

One of the responsibilities of the LLM agent is to oversee the application of the plan. It should decide if a step needs to be repeated (e.g., forError Handling) or skipped for a given input (e.g., to iterate on the plan(Liu et al.,2024a)). One of the major concerns of LLMs as planners is their inability to identify whether or not they can complete a given task(Kambhampati,2024). As such, it is often impossible for an LLM agent to know if a step will be successful until it has been attempted. Thus, it follows logically that an evaluation of the success of each step should take place following execution. Similarly, the overall success of the plan should be evaluated upon completion of all steps. If unsuccessful, the LLM agent may need to adjust or rerun the plan, based on the results of each step and the overall plan (see Section8.2for a discussion on incorporating feedback).

SECTION: 6.Memory

SECTION: 6.1.Retrieval Augmented Generation

Retrieval augmented generation (RAG) (introduced in(Lewis et al.,2020)) has emerged as a staple of agentic LLM applications in industry(Gao et al.,2024). The basis is simple: a system that can provide external context relevant to a natural language input. Typically, an incoming input will be compared against a ground-truth data store and the most relevant piece(s) of information will be provided to the LLM as context upon which it will base its response. This can be done either implicitly, where a user’s input is always used for retrieval for a given LLM call, or explicitly, where the LLM uses RAG as a tool. This has a number of benefits for LLM systems:

Grounding.Rather than relying on the LLM “remembering” relevant context from its training data correctly, we can provide the LLM with accurate relevant information. Providing grounded text as context significantly reduces LLM hallucinations and fills knowledge gaps in the training data(Shuster et al.,2021; Es et al.,2024; Lewis et al.,2020).

Explainability.Rather than relying on an LLM opaquely referencing information it has been trained on, adherence to context supplied as part of RAG provides insight into exactly where an LLM is getting its information(Gao et al.,2024).

Timeliness.While LLMs can reference information from their static training data, the LLM will be subject to a hard information cutoff (the latest date training data was scraped) and a soft information cutoff (events close to its hard information cutoff that have limited coverage). Rather than turning to the infeasible prospect of retraining with updated data, we canprovideupdated information that is relevant to the query as context(Gao et al.,2024).

Outsourcing.Depending on the content, quality, and reliability of the RAG database, aspects of the query can be implicitly outsourced to the context returned, such as reasoning and decision-making.

Alignment.The vast amount of training data used for LLMs is the source of their natural language understanding but should not necessarily be relied on for unbiased, trustworthy, and safe generation. Typically, aligning LLM ouputs with human preferences is seen as a data collection and training problem(Wang et al.,2023c; Bai et al.,2022)but can also be addressed post-hoc with RAG. By augmenting an LLM’s natural language capabilities and tendencies with context derived from a more refined dataset that adheres to a desired set of human preferences, its output can be guided to conform to a desired set of content and attitudes. This requires careful curation of the data store but is a viable method for black-box alignment.

To exemplify the points above, consider the RAG sources referenced in Figure2.R1 and2.R2. Figure2.R1 can be quoted to avoid recipe hallucinations (grounding) and be updated with new recipes (timeliness). Figure2.R2 could be useful in responding to Figure2.I2; where there is no general consensus, we can supply our own ground truth rather than require the LLM to answer a potentially moral question (outsourcing). The tone and terminology of both Figure2.R1 and2.R2 will guide the ideals, content, and terminology used by the LLM (alignment).

There are two main approaches to RAG: knowledge graphs(Edge et al.,2024)and vector databases(Gao et al.,2024; Barron et al.,2024), with the latter seeing far greater adoption due to its simplicity. For a discussion on implementing RAG and the extant commercial and open-source offerings, see(Gao et al.,2024).

SECTION: 6.2.Long-Term Memory

Sometimes, key information is gained during a conversation that may be helpful across all contexts, such as a useful piece of external knowledge or information about a user or task. In those instances, it may be advantageous to store that information in a way accessible to the agent so that its impact is not limited to the current context. This is commonly referred to as ”long-term memory”(Qian et al.,2024a; Wang et al.,2024c; Zhong et al.,2024)444A real-world implementation of long-term memory is OpenAI’s “Memory”(OAI,2024). During conversations with ChatGPT, the LLM will save information that it deems particularly useful to its memory. That memory is then made available to the LLM in future conversations. A clear benefit of this is that it reduces repetition on the part of the user and allows the LLM to better fulfill its objective of providing relevant responses..

We want to be selective with the information that is stored in long-term memory so that it is generally useful and not excessively large. Some common approaches are to store prior solutions to queries(Qian et al.,2024a), global summaries and insights(Zhong et al.,2024), and acquired tools(Wang et al.,2023a).

Long-term memory can be enhanced with reflection, consolidation, forgetting, revision, and other mechanisms designed to mimic long-term memory in humans (see(Zhong et al.,2024)for a discussion on advanced long-term memory implementation). For simplicity, we focus on a simple version of long-term memory, where information is simply stored and retrieved, and any edits are manual. For this simple variation, we derive the following three criteria from existing literature on long-term memory in LLM agents(Qian et al.,2024a; Wang et al.,2024c; Zhong et al.,2024; OAI,2024; Wang et al.,2023a)to use as a litmus test for what information should be stored:

Independent.The information should not have any implicit dependencies, such as input values.

Relevant to a consistency.The information should be relevant toconsistenciesin the agentic LLM system, which may include a task, user, or environment.

Applicable long-term.The information should consistently be applicable to contexts to which the LLM agent may be exposed.

See Table1for the above criteria applied to examples drawn from Figure2.

If the three criteria above are ensured, then the gathered in-context information can be a useful starting place for prompt improvements. It will be information that has been identified as generally and consistently useful to the LLM agent’s environment and may be appropriately suited to permanent inclusion in user or system prompts. It can also be valuable to review long-term memory when making prompt, task, persona, hyperparameter, or model updates; reordering LLM calls; or adjusting tool functionality as such changes may impact the validity of the three criteria.

A common approach to extracting information that belongs in long-term memory is to leverage an external conversation moderator(Zhong et al.,2024; Shinn et al.,2024). The external moderator (e.g., an LLM with a separate role) that reviews conversations (either whole or in pieces) and can be tasked with extracting information it deems compliant with the three criteria above. This is an instance where care must be taken with phrasing as the subjectivity of the task may make the LLM prone to framing bias in its response (e.g., if we ask if there is anything useful to pull out, the LLM will likely pull out some information)(Echterhoff et al.,2024).

Once a piece of information has been deemed worthy of long-term memory, it should be stored. Some approaches include embedding and storing the information in a vector database (similar toRAG)(Zhong et al.,2024; Lin et al.,2023)and natural language storage, although interacting with the latter quickly becomes unwieldy as the amount of long-term memory increases. The structure of the vector database allows us to easily query relevant information(Lin et al.,2023; Wang et al.,2023a; Zhong et al.,2024).

Once information is stored in long-term memory, we must decide when to expose it to the LLM agent. It is key to understand what information is relevant in the current scope. LLM agents may be composed of many LLM calls with different purposes and contexts; not all information from long-term memory will apply to every LLM call. For example, if the user from Figure2decides to plan meals once a week (per Figure2.I5), that would be a valuable long-term memory for Figure2.Pe1 but not necessarily2.Pe2, which is mainly used for its culinary expertise. In such instances, the relevance afforded by retrieval from a vector database is valuable(Lin et al.,2023; Wang et al.,2023a; Zhong et al.,2024). Once relevant information has been retrieved from long-term memory, it can be shared in an LLM call via the user or system prompt.

SECTION: 7.Tools

SECTION: 7.1.Using Tools

To enable the LLM to use tools, tool descriptions and methods of invocation need to be exposed with the LLM (similar to traditional software engineering documentation). If the number of tools in use is small, they can be introduced in natural language. The method for invoking a tool should be clear and easily parsable. A common way to do this is by defining JSON schemas or function signatures, although the latter has been shown to be better for LLM agents(Roucher and Petrov,2024; Wang et al.,2024a).

Tools can be called either explicitly or implicitly, with the former being the de facto approach in practice. Explicit usage simply entails the invocation of a tool as part of the LLM agent’s output(Schick et al.,2023; Qin et al.,2024b). Once the tools are defined and passed as context, the agent will have the means to perform such an invocation in the specified parsable format. Tools can also be implicitly invoked by the implementor in response to an LLM agent’s action or inaction. For example, if a transition between personas occurs, it may be the case that the system will always benefit from a summarization of preceding dialogue. Rather than rely on the LLM agent to invoke a summarization at every persona change, every such transition can trigger a summarization behind the scenes. See Section9.3for a discussion on incorporating implicit tool calling.

SECTION: 7.2.Managing Multiplicity

As the number of tools grows, defining tools in natural language quickly becomes unwieldy and a structured approach is necessary. To do so, we can leverage LLMs’ convenient understanding of code by creating more concise tool definitions using JSON schemas or function signatures in conjunction with condensed natural language descriptions555See https://python.langchain.com/docs/concepts/#tools for a discussion on tool definition and(PCo,[n.d.])for an implementation..

Often, distinct tools can be placed into distinct groups based on similar core functionality (i.e., if they can reasonably be seen as inheriting from the same base class). These groups can be called “toolsets” or ”toolkits”666See https://python.langchain.com/docs/concepts/#toolkits.and are helpful for determining if tools can be combined behind a single interface or introduced together in the prompt. For example, the tools Figure2.T1 and2.T2 introduced in the example would not belong in the same toolset but2.T2 and2.T3 would.

SECTION: 7.3.Adding Tools Dynamically

Sometimes the tools that are available in the environment in which an agentic LLM system will be deployed are not known beforehand. In this case, we can add “tool identification” as a task for the system(Schick et al.,2023; Wang et al.,2023a). A compelling example and implementation of this can be observed in the Voyager paper, where an LLM-based agent autonomously traverses the world of Minecraft777https://www.minecraft.netand dynamically assembles a set of tools based on interactions with the environment, which are then stored in long-term memory(Wang et al.,2023a).

SECTION: 8.Control Flow

In the context of LLM agents, control flow refers to the ability to determine what needs to be done in order to respond to a query. Tasking an LLM with control flow is what enables LLM-based agents to accomplish complex tasks that elude the capacity of a single inference. This endows the LLM agent with the autonomy to incorporate advanced techniques such as planning, tool usage, and multi-step reasoning as it sees fit(Shen et al.,2023; Wang et al.,2024c).

In practice, this may look like the LLM agent receiving user input (i.e., observing the environment) and selecting the immediate next action. The agent continues to take actions until it decides to stop. For this to be possible, the LLM agent needs to be aware of the action space(Yao et al.,2023), such as the stopping criteria, available tools (e.g., Figure2.T1,2.T2, and2.T3), available planning options (2.P1), the ability to take a turn to think out loud(Yao et al.,2023), and utilizing other personas (e.g.,2.Pe2).

Consider an LLM agent receiving Figure2.I1. Rather than simply providing an output, the agent can opt to leverage2.P1, the planning module, to decompose the complex task and generate a multi-step plan that it can then administer. Once the plan is complete, the agent can decide if it has enough information to provide the final output to2.I1 or if it needs to take additional actions.

Here, we present practical considerations for ensuring the LLM agent can interact with its environment smoothly and without interruption.

SECTION: 8.1.Output Processing

When chaining together multiple LLM inputs and outputs, it is often advantageous to process the text before handing it off to the next step. Although natural language is human-readable, it is advisable to use a more structured format (such as JSON or executable code) that is easily parsable(Wang et al.,2024a). While weaker models may struggle with instruction following, most commercial models have been optimized to adhere to desired output formats specified in the user or system prompt888Output processing documentation for common commercial models: Anthropic(Ant,[n.d.]); OpenAI(OAI,[n.d.]b); Google(Gem,2024c).

Because we approach LLMs from a black-box perspective, we do not discuss the underlying approaches to constraining LLMs to output a specific format. However, it is important to note that the reasoning capabilities demonstrated by an LLM may be negatively (and inadvertently) impacted by constrained generation, depending on the implementation(Beurer-Kellner et al.,2024; Tam et al.,2024). Because of this, it has been shown that requiring code outputs instead of a specific structure can yield better agents(Wang et al.,2024a).

SECTION: 8.2.Error Handling

Error handling is one of the most important yet elusive parts of building a robust agentic LLM system. Because LLMs are inherently stochastic, chaining several LLM calls together compounds the risk of failure to the point of near inevitability for long sequences. As such, every LLM call in an agentic LLM system should be treated as a potential point of failure and supported by appropriate error handling.

We provide Figure3of an erroneous tool call to demonstrate several approaches to error handling, where specific responses will be referenced by the codes assigned in the figure (e.g.,3.UI to refer to ”Order me an onion.”). The system prompt, containing role and tool information, is excluded for simplicity.

A diagram showing an erroneous tool call from the agent.

The simplest approach to handling a problematic output is to retry the LLM call with the same prompt. While otherhyperparametersmay stay the same, the seed shouldalwayschange between static retries to avoid completely duplicate calls. If using a low temperature or a high top-p, then it may also make sense to adjust those values appropriately so as to receive a different output. In the context of Figure3, this might look like3.UI simply being rerun with a different seed.

For low-context calls that yield output that is easily verifiable (e.g., parsing the output into a JSON object), it is a simple yet valuable addition to attempt a few static retries in case verification fails. For outputs that are more difficult to verify, such as natural language instructions that are interpreted downstream, static retries are less helpful as the cost of verification increases.

A more informed approach is to append the LLM’s output to the history, add another user message indicating that the output was unsuccessful, and try again. This should be supplemented with specific error messages or additional directions(Kamoi et al.,2024; Tyen et al.,2024). In the Figure3example, an informed retry might look like sending the following list of messages:3.UI,3.AO,3.TR, and ”Attempting the above code yielded the provided error. Please provide an updated output that achieves the initial instruction.”.

Rather than asking for an informed retry from the same context, we can pull out pieces of the history and provide it to an LLM in a separate context to either fix the previous output or generate a new one. This will likely require significant context from the original call but can be supplemented and differentiated by using a different role, different instructions, and error information. Often, shifting roles from, for example, a software engineer to a code reviewer can provide the impetus the LLM needs to fix or generate the correct output. While it has been shown that the explanations from external LLM-based error systems are frequently unreliable and sensitive to prompt changes(Kamoi et al.,2024), having access to task-specific roles, detailed error information (e.g., the error raised by a piece of generated Python code), and background context helps mitigate those issues(Tyen et al.,2024).

In the Figure3example, an external retry might look like sending the following list of messages: ”You are an expert debugger. You have access to {tool information}.” and ”When attempting to fulfill the request, ’{3.UI}’, a helper tried to run the code ‘{3.AO}‘, which yielded ‘{3.TR}‘. Please provide an updated output that achieves the initial instruction.”.

It should be noted that LLMs struggle to locate errors(Kamoi et al.,2024; Tyen et al.,2024)but demonstrate strong error correction capabilities if provided sufficient context, specifically error location(Tyen et al.,2024). As such, when employingInformed RetryorExternal Retry, care should be taken to include error information that pinpoints the source, such as diagnostic error messages and tracebacks from APIs and runtime environments.

SECTION: 8.3.Stopping

As the control flow of the agentic LLM system is controlled by an LLM, a clear stopping method needs to be defined. This will likely take the form of a predetermined stop token or phrase inserted into the system prompt, such as ”TERMINATE”(Wu et al.,2024a). It should be a token or phrase that is easily parsable and not otherwise likely, to avoid accidental stopping.

SECTION: 8.4.Multiple Personas

Often, the role that an LLM is assigned has a significant impact on its performance on a given task. This has been observed in LLM literature generally, becoming a key ingredient of effective prompting(Karmaker Santu and Feng,2023; Kong et al.,2024), and in recent LLM multi-agent research, emerging as a necessary component for agent multiplicity in many such architectures(Hong et al.,2024; Wu et al.,2024a; Li et al.,2023b; Guo et al.,2024; Wang et al.,2024d; Park et al.,2023). For example, while the Figure2.Pe1 role is good for answering most of the user’s queries, the Figure2.Pe2 role may be better at answering Figure2.I4 because it requires specialist culinary knowledge.

Because there are likely to be many distinct tasks that form part of an agentic LLM system, there is usually room for multiple roles to be used. An overview of approaches to defining personas for LLMs, or ”profiling” them, is detailed in(Wang et al.,2024c), categorizing them as handcrafted (e.g.,(Qian et al.,2024a; Park et al.,2023)), LLM-generated (e.g,(Wang et al.,2024d; Xu et al.,2023)), or dataset-aligned (i.e., derived from a pertinent dataset). The roles should be informed by the task that the call is handling. This is dependent on the overall context of the agentic LLM system but can largely be addressed in the following ways:

If the tasks are well-defined, handcraft specialist roles for each task (e.g., Figure2.Pe1 and2.Pe2).

If the tasks are not well-defined but generally correspond to a single topic, use the most specific handcrafted role for that topic (e.g., the catch-all Figure2.Pe1).

If the tasks are truly undefined to start (e.g., an assistant that helps with anything) or the topic is very broad:

Define several distinct roles to which the LLM agent can route subsequent calls as it sees fit(Si et al.,2023). Once the agent is in use, a more informed set of personas can be defined according to the most frequently ones. This may also be thought of as the dataset alignment approach(Wang et al.,2024c), where the dataset is constructed in the environment under an interim set of personas.

Leverage an LLM to create the role that it deems would be best able to respond to the prompt(Wang et al.,2024d; Xu et al.,2023). This is more expensive as generating the role requires LLM usage but is certainly more robust to unforeseen scenarios. This approach may be used in conjunction with the above point (e.g., if no suitable predefined role is found, create one).

SECTION: 8.5.Managing Relevant Context

Managing the context that is sent to an LLM is an effective method of increasing the efficiency (speed and cost) and performance of an LLM system, as inference time is dependent on the number of input tokens(Vaswani et al.,2017; Pope et al.,2023)and LLMs perform worse in long-context scenarios, particularly for complex tasks(Li et al.,2024; Liu et al.,2024b). Additionally, careful context management is a necessity given that LLMs have limited context windows999E.g., Claude 3: 200k(Ant,2024b); Gemini 1.5 Pro: 2M+(Gem,2024a); GPT-4o: 128k(OAI,[n.d.]a). Even for ”long-”context LLMs (¿100k token limit), many tasks quickly become unwieldy if not properly managed (e.g., working with HTML, where single webpages can be hundreds of thousands of tokens). This is a key consideration to make duringtask decomposition; the more specific the task, the more extraneous context (e.g., prior messages) can be trimmed(Qian et al.,2024b). As such, the context that a specific LLM call receives should be tailored to the task as much as possible. Even if an LLM call requires past messages, it is often possible to strip out certain pieces of context or summarize them, leaving the parts the subsequent call relies on intact and maintaining the overall meaning. Significant adjustments can be made to the context between calls to decrease the overall token count and remove extraneous context, thus reducing LLM confusion and increasing performance for the LLM call(Qian et al.,2024b).

SECTION: 9.Additional Considerations

SECTION: 9.1.Model Size

The size of the model to use is typically driven by three concerns: cost, speed, and performance. Usually, the bigger the model, the higher the cost, the lower the speed, and the better the performance (although this is not a hard-and-fast rule). It can be tempting to build an agentic LLM system around the weakest model that will adequately do the job so that all three conditions are optimized from the start. However, attempting to build out a functional system from a smaller model first will likely be more time consuming and expensive than starting at the strongest model possible and downgrading the models used for specific calls once the LLM agent has demonstrated competence in the environment. Due to the influence one call can have on subsequent ones, it is infeasible to understand what is possible for a given use case if not all the pieces are working optimally. By starting with stronger models, there will be a gold-standard baseline to compare against so the performance impact of downgrading a model for a specific call can be measured101010See(Benram,2024; ljunkai,2023)for discussions on these points from an industry perspective.. It is recommended that the correct model is selected on a per-task basis and evaluated both individually and in the context of the entire agentic LLM system.

SECTION: 9.2.Evaluation

Evaluating an agentic LLM system can be challenging due to the potential for long sequences, non-determinism in LLMs, interactions with external entities, and tasks that may not have obviously correct solutions. Nonetheless, it is essential to have an approach to evaluation defined before deployment to (1) have a baseline to compare against and (2) measure performance changes over time and in response to changes.

When creating a dataset for evaluating an LLM agent, the most important consideration is that it accurately resembles the environment in which is will be deployed. There are many LLM agent benchmarks available targeting specific domains(Deng et al.,2024; Yao et al.,2022; Liu et al.,2024c; Zhang et al.,2024b)as well as general purpose application(Srivastava et al.,2023; Wu et al.,2024b; Mialon et al.,2024), but many agentic LLM systems applied to a specific task will be too niche to benefit from a broader benchmark. However, insomuch as an established benchmark fits the application of the LLM system, it can be a strong starting point for evaluation and refinement. Whether an existing benchmark is used or not, it is advisable to collect informative agent interactions (e.g., long sequences, short sequences, incorrect outputs, correct outputs, etc.) and related metadata (e.g., hyperparameters) in the deployment environment. Doing so will allow the creation of a dataset, comprised of reproducible input and output pairs, that is derived from the environment. Even a dataset with a few samples will provide a baseline to compare against to ensure prompt engineering addresses failed executions, identify the effects of model and prompt changes, and avoid regression in the system111111See(Lan,2024)for an industry approach to evaluating deployed LLM systems..

While traditional metrics (e.g., precision, recall, etc.) are useful to track, metrics specific to the agent can help reveal changes in the system that higher-level metrics fail to reflect(Chang et al.,2024; Kapoor et al.,2024). For example, an LLM agent that arrives at the same answer when presented with two different prompts is superficially consistent but a difference in the number of intermediate steps to reach that conclusion may indicate that the system is overly sensitive to prompt changes. Building from(Liu et al.,2023; Kapoor et al.,2024; Mehta et al.,2024)that suggest types of alternative evaluation, we provide sample metrics below to use as a starting place, although useful metrics should be chosen in accordance with the design of the LLM agent and the environment in which it is implemented121212Note that the following are focused primarily on evaluating agentic LLM systems but that external components should also be evaluated, such as the RAG system (e.g., the quality of retrievals and the fidelity of embedded documents)(Salemi and Zamani,2024; Es et al.,2024)and tools (e.g., reliability and consistency of their output)..

No matter how well an agentic LLM system might do along the way or what emergent capabilities it might demonstrate, the final output will determine whether the system is accomplishing its task or not. It is impossible to tell how a composition of LLM calls will perform without running them end-to-end; thus, evaluating an LLM agent should primarily rely on holistic metrics to determine if it is performing as expected.

Sample Metrics.

Across X distinct prompts, how many correct answers does the agent produce?

For input X across N trials, how many distinct answers does the agent produce?

For input X across N trials, what is the average number of steps executed by the agent?

For input X across N trials, what is the average number of tools used by the agent?

For input X (that requires LLM planning) across N trials, what is the average number of steps in each plan?

For input X across N trials, what is the average cost/time?

Measuring the performance of a single or a subset of LLM calls that completes a definable task is a viable method of diagnosing problems in or making changes to the system. However, due to the influence a single LLM call can have downstream in an LLM agent, isolated piecemeal evaluation of an agentic LLM system should never be considered a substitute forHolisticmeasures.

Sample Metrics.

For call X with N trials, how many distinct answers are produced?

For N synonymous versions of input A to call X, how many distinct top-K documents are provided by RAG from each embedded version of A?

For call X with tool access across N trials, how many distinct tools are used?

For call X across N trials, what is the average cost/time?

SECTION: 9.3.Integration with Traditional Engineering

Because LLMs are inherently stochastic, it is often easier to offload as much of the agent’s responsibility onto traditional engineering as possible. This allows outsourcing parts of the system that require determinism to methods that can be deterministic. By crafting an LLM agent according to software engineering best practices, we can ensure that key components that are necessary for a given task are always completed or included, rather than relying on the agent to make a request or execute an action. This can take the form of automatically managing context between calls,output processing, combining tools into toolsets (e.g., putting Figure2.T2 and2.T3 behind a ”delivery” interface), incorporating information from long-term memory permanently into the prompts (e.g., Figure2.E1), setting callbacks on certain transitions and calls (e.g., to generate a summary of the most recent conversation to use as context when transitioning from Figure2.Pe1 to2.Pe2), and adding an evaluation after each step of a plan (seePlan Adherence). (The last two can be thought of as implicit tool usage; see Section7.1). However, care should be taken not to limit the autonomy of the agent in doing so. One way to return autonomy to the agent while still leveraging the benefits of traditional engineering is to allow the agent to short-circuit.

Short-circuiting (from the world of software engineering: the idea of evaluating an expression only so far as to guarantee a single answer) is an integral technique for agentic LLM systems. This can be as simple as includingstoppingcriteria into the LLM agent’s instructions (see Section8.3for examples) or allowing the LLM agent to produce a final output in a single turn. If an agentic LLM system does not short-circuit when it obviously should, the system may have an overreliance on external engineering (i.e., the flow (or parts of the flow) of the agent being hard-coded)131313A recent example of this is OpenAI’s GPT-o1(OpenAI,2024). The initial implementation has no short-circuiting, meaning even simple queries that a much weaker model can handle or that require no significant output still incur a full traversal of the agentic LLM system. For example, asking GPT-o1 to “Do nothing” will still pass through the planning, thinking, and alignment stages of the system..

As an example, the query presented in Figure2.I3 demonstrates an instance when an LLM agent may want to short-circuit. The query poses a simple question-answering scenario that most current models could satisfactorily respond to. Allowing the agent the autonomy to determine what step to take next (as opposed to, for example, implicitly calling Figure2.P1 for every input) would permit it to simply provide an answer, thus short-circuiting any other components.

SECTION: 10.Limitations

Although we present some practical methods for the evaluation of deployed systems, we do not explore human-in-the-loop evaluation as human-computer interaction represents a rich field of study that exceeds the scope of this work.

An important follow-up to evaluation ishowto compare and respond to changes in a deployed agentic LLM system, such as prompt, model, and environment changes. These considerations remain largely underexplored in current literature and represent some of the key challenges to deploying real-world LLM agents. We do not discuss these considerations as agent maintenance does not fall into the scope of this work but suggest that they are prominent directions for future work.

We explore one aspect of cost for agentic LLM systems, model size, but leave other considerations (such as whether to use an out-of-the-box model or to finetune one on a specific task(Bucher and Martini,2024; Lehman et al.,2023), to leverage increasingly strong open-source models(Dubey et al.,2024; Jiang et al.,2023; Bai et al.,2023)or to rely on aligned commercial models(OpenAI et al.,2024; Team et al.,2024; cla,2024), and, similarly, to self-host or to use a 3rd party provider) for future work as cost and feasibility of proposed agent architectures warrant a review on their own. See(Kapoor et al.,2024)for a discussion on the need for cost-informed LLM agent research.

While we approach the agent’s underlying LLM from a black-box perspective for simplicity and relevance to many industry applications, approaching it as whitebox opens up additional complexities and opportunities. We deem that considering model specifics exceeds the scope of this review but recognize the value of future work highlighting practical considerations for the real-world deployment of whitebox LLM agents.

SECTION: 11.Conclusion

In this review, we present relevant research into LLM agents and derive actionable insights from it that can be utilized when implementing and deploying agentic LLM systems in the real world. We ascribe relevant research and insights to the four main components of LLM agents from application-focused literature—Planning,Memory,Tools, andControl Flow—to provide a review that is mutually accessible to both industry and academia. Namely, forPlanning, we explore how poor LLM planning capabilities hinder current LLM agent applications and the practical benefits to be derived from task decomposition; forMemory, we explore the benefits of and practical considerations to make when leveraging RAG and long-term memory in an LLM agent; forTools, we discuss how to present and manage tools for an LLM agent; forControl Flow, we provide practical insights for promoting an uninterrupted LLM agent execution and managing agent internals, such as personas and context usage; and, lastly, suggest additional considerations, such as model size, evaluation, and integrating an LLM agent with traditional engineering.

SECTION: References