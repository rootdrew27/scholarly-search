SECTION: 360U-Former: HDR Illumination Estimation with Panoramic Adapted Vision Transformers
Recent illumination estimation methods have focused on enhancing the resolution and improving the quality and diversity of the generated textures. However, few have explored tailoring the neural network architecture to the Equirectangular Panorama (ERP) format utilised in image-based lighting. Consequently, high dynamic range images (HDRI) results usually exhibit a seam at the side borders and textures or objects that are warped at the poles. To address this shortcoming we propose a novel architecture, 360U-Former, based on a U-Net style Vision-Transformer which leverages the work of PanoSWIN, an adapted shifted window attention tailored to the ERP format. To the best of our knowledge, this is the first purely Vision-Transformer model used in the field of illumination estimation. We train 360U-Former as a GAN to generate HDRI from a limited field of view low dynamic range image (LDRI). We evaluate our method using current illumination estimation evaluation protocols and datasets, demonstrating that our approach outperforms existing and state-of-the-art methods without the artefacts typically associated with the use of the ERP format.

SECTION: Introduction
To believably composite an object into a virtual scene it must be lit consistently with the target scene’s illumination conditions. The field of illumination estimation has sought to capture a scene’s lighting through the constraints of a Limited Field of View (LFOV) Low Dynamic Range image (LDR(I)) such as that taken from a mobile phone camera.
Several methods have been utilised to represent these conditions such as regression-based methods, like Spherical Harmonics (SH)and Spherical Gaussians (SG), and Image Based Lighting (IBL)methods that render lighting from an Equirectangular Panorama (ERP) High Dynamic Range image (HDR(I)). IBL has become the leading way to represent lighting conditionsdue to its ability to capture high-frequency textures, as well as global illumination, meaning it can be used to light a range of surfaces from rough diffuse to mirror reflective. The current trends in IBL and illumination estimation are to increase the resolution, details and accuracy of the generated ERP images.

Neural networks have used the ERP image format in various tasks, such as panoramic outpainting and illumination estimation. Due to mapping the surface of a sphere to a 2D image, warping occurs at the top and bottom of ERP images. When using ERP images in a standard neural network this warping has to be learnt by the network or accounted for by adapting the architecture. The sides of the image also have to be considered connected, unlike regular LFOV images. Not adapting a neural network architecture to work with the ERP format can lead to artefacts such as obvious seams at the side borders of the image and badly generated objects at the top and bottom (poles) of the ERP where it is most warped. In the field of illumination estimation, only a few models have made adjustments. These few approaches are typically limited to either changes to the loss functions or changes to the image at network inference.

Vision Transformers (ViT) have shown their ability to understand relationships of both global and local information in an image for a variety of image processing tasks such as object classification, image restorationand outpainting, as well as being able to change the window attention network to better process ERPs.

We propose 360U-Former, a U-Net style ViT adapted for the ERP format by leveraging PanoSWINattention blocks.
We use 360U-Former to generate the ERP HDR environment map of a scene from a single LFOV LDRI.
Our model does not generate a border seam or any form of warping artefacts at the poles of the output HDRI. It can recreate a variety of both indoor and outdoor environments, outperforming the state of the art for both specialised scene types.
We compare our model against the current state of the art in LFOV illumination estimation using the evaluation method outlined by Weber et al.. We also compare against previous methods from a dataset kept updated by Dastjerdi et al.. Our model outperforms state-of-the-art models at removing ERP artefacts and illuminating objects with diffuse surfaces.
To summarise our contributions are as follows:

First use of a purely Vision-Transformer network to approach the illumination estimation problem;

The use of a Vision-Transformer network with global self attention creates more accurate diffuse lighting than current state-of-the-art methods;

Incorporation of PanoSWIN attention modules and circular padding to better encode and generate ERPs by removing the warping artefacts that appear at the poles of panoramic images.

SECTION: Related Work
We first review the literature for methods that adapt neural networks to the ERP image format. We then review inverse tone-mapping techniques, a fundamental component that makes up the large majority of IBL illumination estimation papers and models. As our proposed network is based on ViTs, we briefly review transformer literature and focus on current Generative Adversarial Network (GAN) ViT architectures. Lastly, we investigate the current state-of-the-art illumination estimation papers and compare the benefits and compromises of each method.

SECTION: Equirectangular Panoramas and Neural Networks
Various techniques have been proposed to overcome the artefacts created by the ERP format. Rotating the input image by 180∘, effectively turning it inside-out, has been used byto assist with panoramic outpainting by converting the problem from an unidirectional outpainting task to a part bidirectional inpainting task. This also helps reduce the seam generated at the sides of the panorama. To more reliably reduce the border and homogenise generation at the sides of the ERP, Akimoto et al.proposed a circular inference method for their transformer, removing the border seam but increasing inference time. Other methods have used circular padding either before and after or throughout the networkallowing for homogenous generation and continuity at the sides of the ERP.
Several approaches have been adopted to overcome the warping at the top and bottom of the ERP. Conversion from ERP to cube map is one approach to remove warping, as used by. However, as noted byadditional requirements are needed to account for the seams generated at the intersections of the cube’s faces. Fenget al.also observe that while the cube map format does well with local details it does not perform as well at capturing global context.
HEALPix, a framework to give equal area weighting to spherical images, has been adopted byin their patch-embedding method to spherically encode the input image shape to a ViT. PAVERalso uses a form of deform patch-embedding for their transformer architecture. Although these methods have been show to improve ViTs above baseline for the ERP format we found they do not remove border and warping artefacts in our tests.
A few papers have adapted loss functions to work with the ERP format. EnvMapNetuses a spherical warping on the L1 loss and Akimoto et al.instead use the same spherical warping but on the Perceptual Loss. Another approach is to train the discriminator to identify ERP artefacts, by rotating the output ERP by 180∘the border seam can then be detected by the discriminator. This has been implemented by.
Other approaches have aimed to change the architecture of neural networks to better handle ERP. Su and Graumandevelop a Spherical Convolution that adapts traditional CNNs to work with ERP. PanoSWIN, which outperformed Spherical Convolutions in benchmark metrics, uses a ViT architecture and observes that attention windows at the poles of the ERP should be considered connected. This changes the shifted window attention layout and implements a pitch attention module to further account for the ERP warping at the poles.

SECTION: Inverse Tone Mapping
Inverse tone mapping is the process of converting an LDRI to an HDRI. It is commonly used in illumination estimation methods as HDR is an accurate way of capturing the lighting through an image. IBL illumination estimation methods perform inverse tone mapping as part of the extrapolation, however, some methodshave used a separate network to do the LDR to HDR conversion so that the main network can focus on field of view extrapolation.

Neural networks tend to be trained on LDRI with a range of [-1, 1] which works with the activation functions used, such as tanh activation with an output range of [-1, 1]. HDRIs have a much larger range (for example [0, 100000]) to capture the difference in light. A method for handling the range without biasing the loss functions to the larger values needs to be implemented. Gardner et al.separate the lighting into LDR and light log intensity. Other methodsuse gamma correction, withandto map the HDR values visible in an LDRI to the same scale as an LDR. DeepLightuses the natural log space to represent the illumination. Similarly, Li et al.use the log(x+1) space. Hilliard et al.use a gamma compression withandso that the HDR is compressed to [0, 2] so that it can be better used with a tanh activation function, whilst increasing the range of the LDR values and decreasing the HDR values.

SECTION: Vision Transformers
The transformer architecture was created to improve natural language processing by focusing on the relationships between each word and its surrounding words in a sentence. The Self-Attention and Feed Forward neural networks were later applied to the field of image processing with the ViT, which used global self-attention to learn the relationship between each section (called a window) of the image and the image as a whole. Liu et al. proposed the SWIN Transformer, which used a hierarchical design with shifted attention windows and provided connections between them, allowing the network to use various scales and image sizes. ViTs were initially used for image classification, object detection and semantic segmentation but were later adapted into the GAN architecture for use in image generation and manipulation tasks. VQGANused a CNN-based vector quantised variational autoencoder to train a ViT to learn a codebook of context-rich visual parts. RFormercombined ViTs with the U-Net style of architecture, using a ViT for the encoder and decoder, with shortcut connections between them, for image restoration. Gao et al.developed a similar U-Net style ViT called U-Transformer for image outpainting. This method used masked shortcut connections to connect only the known regions of the input image to the decoder layers.

SECTION: Illumination Estimation
Modern methods for illumination estimation extrapolated from an LFOV LDRI use deep learning to infer the lighting conditions. The format with which the lighting conditions are represented can be put into two distinct categories: regression based methods such as SHand SGwhich use a basis function to reduce the lighting to a set of coefficients, and, as more recent methodsare opting to use, an IBL approach by generating an ERP HDRI known as an environment map. Regression-based methods have the advantage of using less memory and are more efficient at rendering. However, IBL methods have been favoured because of the key advantage that they can be used to light mirror reflective surfaces due to their ability to represent high-frequency textures. The recent developments in IBL methods have aimed to create higher resolution and more plausible images that can generalise to a variety of indoor and outdoor scenes, whilst retaining accurate light positions and colour for lighting purposes.
The illumination estimation methods that make adaptions for ERP are. These methods have only focused on changing the loss functions rather than the architecture of the network. Consequently, they are unable to generate the poles of the ERP without noticeable artefacts.

SECTION: Methodology
We present 360U-Former, a U-Net style ViT-based architecture trained as a GAN to generate HDR 360∘ERP images from LFOV LDR images that can be applied to both indoor and outdoor scenes. By using PanoSWIN attention layers our model generates ERP images without warping at the poles and a seam at the sides of the image. The architecture for the discriminator is based on RALSGAN. The pipeline for the whole model is featured in.

SECTION: Network Architecture
The input to the network is an LFOV LDR image converted to ERP format. The output of the model is an HDR ERP image.
The 360U-Former,, can be split into three distinct parts: the encoder, the bottleneck and the decoder.

The encoder uses 4 PanoSWINblocks with layer sizes of 3, 3, 7 and 2. The PanoSWIN block consists of a standard window multi-head self-attention layer (W-MSA),shows the input, followed by a panoramically warped shifted window multi-head self-attention layer (PSW-MSA),shows the rotations done and the final input to the layer, that ensures that the regions along the top or bottom of the panorama are considered adjacent rather than distant. The final layer of each block, except the last block, is a Pitch Attention Module (PAM). The PAM performs cross-attention on the windows from the default orientation and the associated windows from the image pitched and rotated by 90∘,. This method allows the network to learn the spatial distortion at the poles of the ERP. For the bottleneck, we use a block of two PanoSWIN layers without the PAM. We found that using the PAM in the bottleneck, the last block of the encoder and the first block of the decoder would lead to artefacts at the sides of the generated image. This is likely due to the size of the tensor at these blocks, 8 by 16 and 4 by 8, and the way they are interpolated that affects the edge of the output image.

The decoder is similar to the encoder but uses upsampling instead of down-sampling to increase the resolution and reduce the channel size. We incorporate shortcut connections from the encoder by further upsampling the channels of the tensor from the previous block and concatenating them with the associated output from the encoder. When reversing the patch embedding to get the output image we incorporate circular padding on either side of the image and reflection padding at the top and bottom instead of the default padding used by convolutional layers. This further ensures that no border artefacts are produced.

SECTION: Loss Functions
To train our network we use three loss functions. We choose L1 loss for pixel-wise accuracy. To measure semantic similarity to the ground truth we use the perceptual loss.

whereandare the positions on the feature (size) in theth layer of the VGG feature extractor.

We leverage the RalSGANto train our GAN because we found it to generate higher-quality images with fewer artefacts.

The ICN overall objective with weighted loss functions is

whererepresents the loss weight for each loss. We found the optimal loss weights to be,andby testing a range of values and comparing the performance of the metrics and appearance of the generated output.

SECTION: Implementation Details
We train our network on both indoor and outdoor data. For the indoor dataset, we use the Structured3Dsynthetic dataset which contains 21,843 photo-realistic panoramas. For the outdoor dataset, we use the 360 Sun Positionsdataset which contains 19,093 street view images of both urban roads and rural environments.

To create the HDR ground truth pairs for these LDR datasets, we convert to HDR using an LDR-to-HDR network. This network is based on the network design from Bolducet al.without the exposure and illuminance branch using the ERP LDRI as input and the ERP HDRI as output. It is trained on the Laval Photometric Indoor HDR dataset, Laval Outdoor HDR datasetand the dataset from Chenget al.. Due to the difference in calibration between each dataset, we base the values on the average mean and median values from the Laval Photometric dataset scaled by a factor 0.01. This scaling factor is based on the factor needed to create a plausible render when using the Cycles rendering engine in Blender. Using this method we used a scaling factor of 340 for the Laval HDR Outdoor dataset and that the other dataset did not need to be scaled.

The network is trained on an image resolution of 256 by 512. We augment both datasets by horizontally rotating each panorama 8 times randomly between 20∘and 340∘at intervals of 40∘with a 20% chance of being vertically flipped. After augmentation our dataset consists of 368,370 images pairs which we split into a train/test ratio of 99:1. We ensure that all augmented versions of each pair exist only in the train or the test subset to prevent over-fitting. Both networks are trained with a range of LFOV sizesfor the masked input. The mask size is randomly chosen for each input image.
The network is trained for 50 epochs on an A100 GPU at 5.5 hours per epoch. There are a total of 220 million parameters in the generator network. We use the ADAM optimiser with betas 0 and 0.9, weight decay of 0.0001 and learning rate of 0.0001 for the generator and 0.0004 for the discriminator.

SECTION: Evaluation
We evaluate our model against the latest state-of-the-art illumination estimation methods using the protocol first outlined by Weber et al.and used again by EverLight. We compare quantitative results for indoor and outdoor methods separately as some previous methods focused on one or the other. This also helps to highlight the performance in different domains. To highlight the ability of our network to adapt to the ERP format, we rotate the outputs to show the border seams and the quality of the poles generated. We also conduct an ablation study to compare the PanoSWIN against the SWIN attention blocks. The results of the diffuse render tests have been generously contributed to the community by Dastjerdiet al.and can be found on the project website of EverLight.

SECTION: Indoor Images
We follow the evaluation protocol of Everlight, testing using a two-fold method. First, evaluating the ability of the method to produce accurate light positions, colours and intensities on a diffuse surface. This is conducted using the test split of the Laval HDR Indoor datasetand 10 extracted views of 50∘. The generated environment maps are rendered to light a scene with 9 diffuse spheres on a ground plane. We use RMSE, scale-invariant RMSE (siRMSE), RGB angular error and PSNR metrics to measure the diffuse lighting accuracy. Second, the plausibility of the generated HDRIs is measured with Fréchet Inception Distance (FID). We evaluate the FID score in the same way as, using additional datasets to remove the potential bias of using just one dataset. These are the 305 images from the Laval HDR Indoor test setand the 192 indoor images from. However, as the 360Cities dataset is not publicly available we calculate the FID score for our work without these images. As with the rendered evaluation, we take 10 images with a field of view of 50∘extracted from the ground truth panorama as input to the network.

Following the protocol, we evaluate against the following methods. Two versions ofare compared: the original (3) where 3 light sources are estimated, and a version (1) trained to predict a single parametric light. We also compare to Lighthouse, which expects a stereo pair as input, instead a second image is generated with a small baseline using(visual inspection confirmed this yields results comparable to the published work). For, the coordinates of the image centre for the object position are selected. For, the proposed “Cluster ID loss”, and tonemapping are used with a pix2pixHDnetwork architecture. We compare against EMLight, StyleLight, EverLight,and. Finally,as a state-of-the-art (LDR) LFOV extrapolation method.

The results for the Indoor quantitative comparison are shown in. In terms of si-RMSE and RMSE, our method outperforms all other methods indicating that the colours are more accurate. In terms of light position accuracy, the methods that incorporate parametric or spherical Gaussian lighting as the main form of lighting representation perform better. Our method performs competitively in terms of PSNR and FID scores.

In, we present a selection of panoramas rotated by 180∘about the vertical axis to observe the side borders of the generated ERP. We also feature a rotation of 90∘by 90∘to show the poles of the panorama side by side. To be concise, we do not include all of the methods from the quantitative comparison and base our choice on the type of network and the quality of the output to prevent comparing against too many similar methods. These results show that our model completely removes the side border and any warping at the ERP’s poles. Other methods are not able to remove the side border and, although it is not obvious, EverLight does not completely remove it. However, the plausibility of our generated textures and structures does not compete with the current state of the art. This reflects the quantitative results in. Our method retains the information from the input LFOV LDRI as does EverLight.

SECTION: Outdoor Images
We conduct the quantitative comparison on the outdoor scenes similarly to the indoor scenes but change the input to 3 perspective crops at azimuth spacing {0, 120 and 240} of 90∘field of view. We use the 893 outdoor panoramas from thedataset giving a total of 2,517 images for evaluation. Unlike the indoor evaluation, all metrics are tested on this dataset. We compare our results with the works of Zhanget al., Everlightand ImmerseGAN. It should be noted that Zhanget al.proposed a method for predicting the parameters of an outdoor sun+sky light model.

The quantitative results are shown in. Similarly to the indoor comparison, our method outperforms all over methods when comparing the diffuse render showing a better understanding of light position and colour. As with the indoor method our FID scores perform worse than EverLight and ImmerseGAN, showing that our method does not reproduce plausible textures.

As with the indoor scenes we choose the same method to compare the quality of our generated images. We display images from the outdoor evaluation dataset from urban and natural scenes, similar to the ones in the 360 Sun Positions dataset. We also include some scene types from the evaluation dataset that are not included in our dataset to demonstrate our model’s ability to generalise. As with the indoor results our method removes the side border and warped poles commonly seen in illumination estimation methods. EverLight and ImmerseGAN have reduced the ERP warping to subtle artefacts only noticed when zooming in. We can see that the quality of the colours and features are similar to that of the ground truth. However, textures are not plausible and do not compete with the current state of the art. It is also worth noting that the model generates more plausible and accurate outdoor scenes with fewer artefacts compared to the indoor scenes.

SECTION: Ablation Study
We conduct an ablation study to demonstrate the effectiveness of our adaptions, the PanoSWIN attention blocks and patch embedding with circular padding, at removing the artefacts produced when using the ERP format. We use the same methods of comparison as the indoor and outdoor quantitative and qualitative studies. The results can be seen inandand show that not only does making adaptations to the network architecture remove the artefacts caused by using ERPs, it also improves the ability of a network to generate quality images. Most notable is the improvement in the RGB angular error, suggesting that adapting the network architecture has helped position light sources more accurately. The results shown inare from our test set. it should be noted that there is a significant improvement in the quality of the generation when our network uses images from the same dataset as the training set compared to the datasets used for the benchmark. This could be a limitation of the datasets we use to train the model, suggesting that there is not a diverse enough distribution of data in the Structured3Dand 360 Sun Positionsdatasets.

SECTION: Conclusion
This paper proposes a method for removing artefacts caused by using the ERP format and a Vision-Transformer architecture for estimating the illumination conditions of indoor and outdoor scenes from an LFOV image. By utilising a Vision-Transformer architecture with PanoSWIN attention layers the network can account for the warping at the poles of the ERP and allow for seamless and homogenised generation at the borders. We demonstrate this through a qualitative comparison that rotates the generated environment maps, highlighting warping at the edges of the image. In general, the results of this paper could be used as a guide when constructing any neural network that makes use of the ERP image format to improve the quality of the generation and remove artefacts.
Although the ERP artefacts have been addressed, the network lacks the ability to generate plausible high-resolution textures competitive with that of other state-of-the-art methods. This could potentially be resolved by increasing the size of the dataset or using a different training method.

Further analysis could be carried out by mapping the attention and latent space to understand how the PanoSWIN attention layers improve the ViT network. We could also compare our models’ complexity with other approaches.
Based on the direction the field of illumination estimation is heading towards we highlight three key areas that could extend the functionality of our method. Firstly it would be useful for a user to have a variety of mediums to edit the generated output, such as providing a diffuse environment map to the ICN to change the lighting conditions or by using a text prompt to change the details of the environment map. Secondly, adapting the method to incorporate spatial variance to shift the panorama so that it can accurately light objects not at its centre. This feature would be integral to XR applications. Finally, a large-scale standardised HDR panoramic dataset that has calibrated values for light sources and a range of outdoor and indoor scenes with various textures.

SECTION: Acknowledgements
This work was supported by the UKRI EPSRC Doctoral Training Partnership Grants EP/N509772/1 and EP/R513350/1 (studentship reference 2437074)

SECTION: References