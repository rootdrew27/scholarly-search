SECTION: Learning Where to Edit Vision Transformers
Model editing aims to data-efficiently correct predictive errors of large pre-trained models while ensuring generalization to neighboring failures and locality to minimize unintended effects on unrelated examples. While significant progress has been made in editing Transformer-based large language models, effective strategies for editing vision Transformers (ViTs) in computer vision remain largely untapped. In this paper, we take initial steps towards correcting predictive errors of ViTs, particularly those arising from subpopulation shifts. Taking a locate-then-edit approach, we first address the “where-to-edit” challenge by meta-learning a hypernetwork on CutMix-augmented data generated for editing reliability. This trained hypernetwork produces generalizable binary masks that identify a sparse subset of structured model parameters, responsive to real-world failure samples. Afterward, we solve the “how-to-edit” problem by simply fine-tuning the identified parameters using a variant of gradient descent to achieve successful edits. To validate our method, we construct an editing benchmark that introduces subpopulation shifts towards natural underrepresented images and AI-generated images, thereby revealing the limitations of pre-trained ViTs for object recognition. Our approach not only achieves superior performance on the proposed benchmark but also allows for adjustable trade-offs between generalization and locality. Our code is available at.

SECTION: Introduction
In many scientific and engineering disciplines, computational models serve as approximations of complex real-world phenomena. As a consequence, they are inherently prone to predictive errors, aptly encapsulated by George Box’s adage: “.” Model editinghas emerged as a promising technique to make (large) pre-trained modelsby enabling targeted updates to model behavior on specific inputs or tasks
in a data-efficient manner without pre-training again from scratch.
An ideal model editing method should satisfy three major desiderata: 1), ensuring the model behavior is effectively updated for the current sample; 2), so that the changes extend to neighboring samples; and 3), meaning the edit should have minimal impact on the model behavior on unrelated samples.

Model editing has allowed many fascinating applications, including error correction, factual knowledge update, bias mitigation, policy compliance, and personalization, though most of them have predominantly been within large language models (LLMs)in the
natural language processing (NLP) community.
With the enormous and often inaccessible pre-training datasets and the ever-growing model sizes that make retraining computationally demanding,
the need for effectively editing computer vision (CV) models is also becoming urgent.
Adapting model editing techniques from NLP to CV is non-trivial and presents unique challenges. From the data perspective, NLP deals with one-dimensional, discrete signals that are highly semantic and information-dense, whereas CV requires processing high-dimensional continuous sensor data that is spatially redundant. From the model perspective, lots of model editing methods in NLP are specially designed for LLMs with(, autoregressive) attention, such as GPT-3and GPT-4. In contrast, CV models have primarily been based on convolutional networks, with more recent implementations using vision Transformers (ViTs)that otherwise employattention. These differences in data formats and model structures make targeted edits more challenging to implement in CV models, and when such edits are achieved, they often result in suboptimal performance.

In this paper, we take initial steps towards editing pre-trained ViTs for object recognition, aiming to correct predictive errors without the need for costly and time-consuming retraining. Specifically, we take a locate-then-edit approach, which breaks down
model editing into two key subproblems: where-to-edit and how-to-edit. Moreover, we prioritize learning where to edit rather than how to edit to facilitate a simpler yet better trade-off between generalization and locality, without needing to store previously trained data.

For the where-to-edit phase, we first narrow the editing scope using a greedy search-based heuristic. Next, inspired by the proven effectiveness of meta-learningin optimizing training strategies for individual samples, we meta-train a hypernetwork to generate a binary task, indicating which parameters are critical for the editing sample.
To address the issue of limited data, the hypernetwork is trained solely using pseudo-samples, each comprising a natural image paired with its CutMix version(see Fig.). The optimization objective is to align the predicted probability distribution of the CutMix sample to that of the original. By controlling the sizes of patches used in CutMix and randomly varying their locations, we simulate distribution shifts in backgrounds, contextual objects, and object attributes, creating opportunities to learn generalizable binary masks that effectively respond to real-world failures. Additionally, we apply a sparsity constraint to the binary masks, acting as an indirect, data-free regularizer to promote locality. Once the where-to-edit problem is solved, the how-to-edit phase becomes straightforward: we simply fine-tune the selected parameters using a variant of gradient descent to apply targeted edits.

To validate our method, we construct an editing benchmark that exposes the weaknesses of pre-trained ViTs by introducing two types of subpopulation shifts.
The first is a natural subpopulation shift,
with underrepresented natural images of certain categories efficiently identified by the maximum discrepancy (MAD) competition. The second is an artificial subpopulation shift, introduced by synthesized images from high-quality text-to-image generative models like Stable Diffusion.

In summary, our key contributions are as follows:

a first-of-its-kind model editing method for pre-trained ViTs that leverages meta-learning to prioritize the where-to-edit phase;

an editing benchmark that provides valuable resources for future model editing research in CV;

an extensive experimental demonstration that our method achieves the best Pareto front between generalization and locality on the proposed benchmark, while offering flexible trade-offs in the how-to-edit phase.

SECTION: Related Work
In this section, we provide a brief overview of current model editing methods in NLP and CV.

SECTION: Model Editing in NLP
rely on external mechanisms, such as wrappersand caches, to store factual updates without modifying the internal model parameters. A common theme in these studies is the use of a gating mechanism to determine whether a test sample falls within the editing scope; if so, the base model behavior is overridden. For instance, SERACand GRACEemploy a scope classifier as a form of hard gating, while Murtyutilized a soft gating function, allowing for smoother integration. More recent approaches like IKEand MeLLoalter the input prompts of an LLM for knowledge update, where the gating mechanism is implicitly embedded within the LLM itself. Generally, memory-based methods offer advantages such as non-destructive updates, modularity, and suitability for continual and few-shot learning settings. However, they face scalability issues when handling a large number of edits. Additionally, the editing success heavily depends on the accuracy of the gating mechanism.

modify the internal model parameters, which offers a more fine-grained approach to editing. These methods can roughly be categorized into two subgroups: locate-then-edit approaches and hypernetwork-based approaches. Locate-then-edit methods focus on identifying a subset of key parameters for editing. For instance, ROME, MEMIT, and MEMITleverage causal mediation analysis (, representation denoising) to locate hidden states (, intermediate representations, not model parameters) responsible for knowledge storage. The theory of associative memoryis then applied to transfer the state localization results to model parameters. Recent studiessuggest that knowledge localization may not reliably inform successful edits. Furthermore, the very notion that knowledge can be localized may be inherently flawed, as factual information in LLMs may be encoded in a distributed manner.integrated gradient across multiple editing samplesis another commonly used statistic for localization. Here, we adopt a more principled meta-learning strategy to locate key parameters, usinggradient information that more accurately captures the changes in model behavior.

Hypernetwork-based methods, such as KnowledgeEditor, MEND, and MALMEN, train an external network to directly generate parameter updates for the editing sample, which is represented by either feedforward feature representationor backward gradient decomposition. Localization techniques can be applied beforehand to restrict the functional space of the hypernetwork. Existing hypernetwork-based methods emphasize the how-to-edit aspect but treat the where-to-edit superficially, and often result in suboptimal performance, especially when adapting to CV applications. In contrast, our method prioritizes learning where to edit, achieving a better balance between generalization and locality.

SECTION: Model Editing in CV
Limited research on model editing has been conducted in CV. Bautook a locate-then-edit approach to rewrite generative adversarial networks. Santurkaradapted this method for editing image classifiers based on convolutional networks by mapping the representation of the new visual concept to that of a previously learned concept. However, this approach requires prior knowledge of the new visual concept, its location within the image, and the specific target concept for correction. In practical applications, such detailed information may not always be available. In contrast, our method relaxes all these assumptions and is one of the first applied to ViTs.

SECTION: Learning Where to Edit ViTs
In this section, we first present the preliminaries, followed by a detailed description of the proposed method for learning where to edit ViTs. The system diagram of our method is shown in Fig..

SECTION: Preliminaries
Given a base
computational model, parameterized by,
model editing aims to modify the model behavior for specific inputs(or regions of the input space,) while keeping its overall performance intact. Denote the post-edited model as, whererepresents the updated parameter vectorafter editing.
Typically,is evaluated based on three main criteria: reliability, generalization, and locality.

: For any editing sample, the edited model.

: For any neighboringsample,, even ifis not directly used in the editing process.

: For any sample, the model behavior should remain unchanged,,.

An ideal model editing method shall ensure reliable edits while balancing generalization and locality effectively. As initial model editing attempts in CV, we limit our scope to single-example editing.

A ViTfeature extractor, denoted bywith parameter vector, consists of a linear embedding layer followed byattention blocks. Each block is composed of
a multiheaded self-attention (MSA) layer and a feedforward neural network (FFN). The FFN, which underpins most model editing methods, including ours, comprises
two fully-connected (FC) layers:. Here,andare weight matrices, wheredenotes the intermediate dimension.andare bias terms. The activation functionis the Gaussian error linear unit.

An input imageis first partitioned intonon-overlapping, fixed-size patches, each linearly embedded in an-dimensional feature space together with a class token,
yielding a concatenation of patch embeddings of size.
These embeddings are processed through theattention blocks for feature extraction.
A linear classification head,, maps the extracted features to a probability distribution over classes in, represented as, where. For notation simplicity, we omit the parameters in the classification head, as they constitute only a small fraction of the total parameters and are generally frozen during model editing.

SECTION: Model Editing at Training Time: Where-to-edit
The simplest way of editing a ViT is through vanilla fine-tuning, which involves updating all model parameters. However, modern ViTs have millions to billions of parameters, and fine-tuning on a single samplecan lead to overfitting, while incurring substantial computation costs.
To overcome these, prior researchfirst identifies a subset of key parameters, followed by editing:

whereis a binary mask of the same dimension as,represents the parameter update, andis the Hadamard product.

Prevailing localization strategies in NLP rely on casual mediation analysis, integrated gradients, or pure heuristic methods, which may not be ideal for ViTs due to differences in data modalities and model architectures. In this work, we follow the locate-the-edit approach, and decompose model editing into two subproblems: where-to-edit (, computing) and how-to-edit (, computing), with a focus on where-to-edit. Drawing inspiration from the demonstrated success of meta-learningin tailoring training strategies for individual samples, we meta-train a hypernetwork to generate the binary maskfor each editing sample.

Meta-learning, also known as learning-to-learn, involves training models on a collection of training episodesto enable effective generalization and adaptation to novel, unseen episodes. In our context, a training episode corresponds to a single editing example. We employ optimization-based meta-learning approaches, framing where-to edit as a bi-level optimization problem. In the inner loop, key parameters, indicated by, are updated for the editing sample by optimizing a reliability loss via gradient-descent overiterations. In the outer loop, the hypernetwork, parameterized by, is refined to generate. Mathematically, we have

whereis the editing sample.is the updated parameter afteriterations of inner-loop optimization, anddenotes the pre-trained parameters of the base model as initialization. The termis the parameter update after the-th iteration, with. The loss functionmeasures the reliability of the edit. To encourage sparsity in the binary mask, we add an-norm term in the outer-loop objective, which acts as an indirect, data-free regularizer to encourage locality. The scalarcontrols the trade-off between the two terms. In our implementation, the hypernetwork takes the last-stage features corresponding to thetoken from the ViT feature extractoras input,,.

SECTION: Optimization Challenges
Despite mathematical elegance, solving the bi-level optimization problem in () presents three challenges. First, meta-training the hypernetwork necessitates a sizable of high-quality editing samples, which are expensive and time-consuming to collect in practice.
To address this, we generate pseudo-samples using a data augmentation technique known as CutMix. Second, identifying key parameters within the entirety of the ViT presents a vast search space. This combinatorial complexity not only introduces unacceptable computational costs but also makes the localization of key parameters a challenging endeavor. To alleviate this, we shrink the editing scope based on a greedy search-based heuristic.
Third, generating a binary mask typically involves a binarization operation in, which produces zero gradients almost everywhere and is thus ineffective in optimizing. To resolve this, we use a gradient-friendly approximation to binarization.

We employ CutMixto generate pseudo-samples for editing.
Specifically, given a natural image, we apply CutMixto randomly overlay a small patch from another irrelevant image onto, producing a pseudo-sample. This patch-based perturbation tends to alter the predicted probability distribution, resulting in, for. This motivates us to instantiate the reliability lossin Problem () as the Kullback-Leibler (KL) divergencebetweenand:

whereis treated as the soft ground-truth label.

Previous studieshave suggested that modifying FFNs within a Transformer is more effective for achieving successful edits. For example, MENDfocuses on editing the last three FFNs, while ROMEtargets the middle FFNs.
Here, we conduct a similar empirical investigation to
identify a subset of consecutive FFNs in a ViT, by greedy search for the optimal generalization and locality trade-off.
Specifically, we fine-tune ten groups of FFNs (or MSAs) in three consecutive layersof a pre-trained ViT/B-16, denoted as {-,-,,-}. The editing set comprisespredictive failures of the ViT, whereis mistaken for(see Fig.), identified by the MAD competition(see more details in Sec.). The average results across the editing set are shown in Fig., where we see that editing MSAs is not conducive to preserving locality. In contrast, editing the-th to-th FNNs tends to achieve the best trade-off, which are selected as the default layers for subsequent experiments.

To further limit the output space of the hypernetwork, we employ structured tuningby selecting specific rows/columns of the weight matrices in the FFNs for updating. As suggested in, we select the weights along the intermediate dimension, which further reduces the output dimension of the hypernetwork to(, three FFNs with two FCs each).

As a special case of quantization in signal processing, binarization can be approximated to enable gradient-based training through three main approaches: straight-through estimation, uniform noise addition, and soft-to-hard annealing. Here, we use a fixed parametric sigmoid function with favorable gradient behavior as the approximation:

whereis a continuous map computed by the hypernetwork right before binarization, andis a hyperparameter that controls the degree to which the sigmoid curve approximates the desired binarization operation. Empirically, we set. We have also experimented with a soft-to-hard annealing for, and observed comparable results. After adopting Eq. (), we substitutewithand replace the-norm with the-norm in Problem () to facilitate gradient-based optimization.

SECTION: Model Editing at Test Time: How-to-edit
At test time, we solve the how-to-edit problem in a manner similar to the inner-loop optimization. The two minor differences lie in the loss function and the binarization operation.

At test time, we are provided with the editing sampleand its ground-truth label. Therefore, the KL divergence during training reduces the cross-entropy loss during testing:

Also, we can directly employ the threshold-based binarization without approximation to obtain

whereis the positional index, andis a hyperparameter that can be adjusted for different model editing applications. Whenis set to zero, all parameters in the selected FFNs are updated with improved reliability. Asincreases, fewer parameters are updated, which favors locality.

SECTION: Hypernetwork Architecture
Similar to the ViT feature extractor, the hypernetworkcomprises five attention blocks, an FC layer as the projection head, and a binarization operation. As shown in Fig., we introduce six learnable tokens, each corresponding to an FC layer within the three selected FFNs of the base ViT. These tokens are concatenated with the image features derived fromand serve as input to the hypernetwork to compute the binary mask.

SECTION: Editing Benchmark with Subpopulation Shifts
In this section, we establish an editing benchmark that exposes failures of the base ViT in object recognition by introducing subpopulation shifts to underrepresented natural and AI-generated images.

SECTION: Natural Image Subset
To build the natural image subset, we first compile a large dataset of unlabeled images, denoted as, from Flickr, by leveraging keywords relevant to the object categories in ImageNet-1k. Next, we employ the MAD competitionto facilitate failure identification of the base ViT to be edited. Under the principle of model falsification as model comparison, MAD chooses to identify images that best distinguish two classifiers,and, by maximizing their prediction discrepancies. This can be mathematically formulated as

whereis the set ofimages that have been identified.is the multi-hop distance defined over the WordNetto measure prediction discrepancy at a semantic level. Intuitively, if one classifier is weaker, the identified image setis more likely to include its predictive failures, thereby substantially reducing the human effort for failure identification. Moreover, the “ground-truth” labels for these failures can be first suggested by the stronger model and then verified by two of the authors. To leverage this intuition, we pair our base model (, a ViT/B-16 pre-trained on ImageNet-1k) with a stronger one (, the same ViT/B-16 pre-trained using CLIPand fine-tuned on ImageNet), which generally exhibits better generalization to unseen data. In total, we collectMAD-searched natural images, which are partitioned intogroups,,, based on the predictions by the two models. Each group is named according to the format “prediction of the stronger model”-“prediction of the base model,” with the statistics and visual examples given in the Appendix.

SECTION: AI-generated Image Subset
Classifiers pre-trained on natural images often struggle to generalize to AI-generated images. To exploit this, we construct an AI-generated image subset containing two groups of images, denoted as. The-th group includesimages with an art style shift (, oil painting) generated by Textural Inversion, while the-th group comprisesimages with a lighting condition shift (, stage light) produced by PUG. Both Textural Inversion and PUG are text-to-image generators, wherein the “ground-truth” label is embedded in the input text prompt and subsequently verified by two of the authors.
Additional details of the AI-generated image subset can be found in the Appendix.

SECTION: Experiments
In this section, we first describe the experimental
setups
and then present comparison results on the proposed editing benchmark.

SECTION: Experiment Setups
Following, we evaluate all model editing methods on the single-example editing task and compare their performance using three evaluation metrics. The first is the(SR), which indicates the reliability (, accuracy) of the edited model:

whereconsists of all MAD-searched and AI-generated images, and we make it explicit the dependence of the updated parameterson the editing sample.
The second metric is the(GR), which assesses the accuracy of the edited model on neighboring samples that fall within the editing scope:

wheredenotes one of thegroups in the proposed editing benchmark. We further average the GR values across all groups as an overall indicator of generalization.
The third metric is the(LR), which examines whether the edited model maintains its predictions on unrelated samples outside the editing scope:

whereincludes out-of-scope images. Using the validation set from ImageNet-1k asdoes not adequately examine locality, as the majority are easy samples that lie far from the decision boundary. To more closely examine the adverse effects of model editing, we have carefully curatedimages near the decision boundary of the base model from the validation sets of ImageNet-1k, ImageNet-R, and ImageNet-Sketch, whose predictions are more susceptible to change.
Our selection criteria rely on the
predicted probabilities of the pre-trained ViT/B-16 model as follows: 1) the predicted probability for the true label is the highest, and 2) the difference between the top two predicted probabilities is less than, suggesting a highly ambiguous class. We also employ the GR-LR curve to delineate the generalization and locality trade-off.

For all model editing methods, we experiment with two ViT backbones, ViT-B/16 and ViT/S-16, both pre-trained on ImageNet-21k and ImageNet-1k.

We compare our method with several recent model editing approaches as follows. 1) Fine-tuning (FT) updates the-th to-th FFNs, which have been identified as the most effective layers using greedy search (see Fig.). 2) FT-incorporates-norm regularization during fine-tuning. 3) T-Patcheradds and tunes a single neuron in the last FFN. 4) KNand 5) SPTselect key parameters based on integrated gradient information. 6) ROMEis implemented to adjust the second FC layer of the last FFN by solving a constrained least squares problem. 7) LoRAintroduces trainable low-rank matrices to update the queries and values of all MSAs. 8) KEand 9) MENDemploy hypernetworks to generate parameter updates for the last three FFNs. In line with previous work, early stopping is applied when the training loss drops belowor the maximum ofediting steps is reached. Detailed implementations of the competing methods and additional training configurations are provided in the Appendix.

SECTION: Main Results
Fig.shows the GR-LR curves for different editing methods applied to ViT-B/16, averaged acrossgroups in the proposed benchmark. We highlight several interesting observations. First, correcting a single predictive error is generally feasible, as evidenced by a nearlySR for most methods.
Second, achieving high levels of generalization and locality simultaneously proves to be a significant challenge. T-Patcher and ROME utilize previously seen data to maintain locality. Nevertheless, T-Patcher, which relies on an editing scope classifier, exhibits noticeable generalization variability across different editing samples. ROME, being specifically designed for language-based GPT, shows limited promise in generalizing to ViTs. LoRA manages to maintain locality because of its low-rank updates but struggles to generalize. Both KE and MEND exhibit low locality on the MAD-searched natural images and poor generalization to the AI-generated images. Third, our method achieves the new state-of-the-art without relying on previously trained data to explicitly enforce locality. Similar conclusions can be drawn for ViT-S/16, shown in the Appendix.

We then evaluate our method across different parameter sparsity levels in the three FFNs from, corresponding toparameters of the entire model, by adjustingin Eq. (). The competing methods—FT-, KN, and SPT—are adjusted to comparable levels of parameter sparsity by tuning their respective hyperparameters. Note that our method reduces to FT when. The resulting GR-LR curves are shown in Fig.. As expected, increasing the parameter sparsity in KN, SPT, and our method improves locality at the expense of generalization. Notably, our method achieves the best Pareto front among all methods, which we believe arises from our proposed strategy of learning where to edit towards editing success.

SECTION: Ablation Studies
To substantiate that the effectiveness of our method is indeed due to the successful localization of a specific subset of key parameters, rather than merely due to sparsity, we compare the binary masks produced by our hypernetwork to random masks at the same sparsity levels, together with FT-and FT-.
As depicted in Fig., FT-generally surpasses FT-at various regularization levels as-norm is more effective in zeroing out less important parameters. Applying random masks shows effects akin to FT-. When the ratio of editing parameters falls below, the performance of random masking becomes significantly inferior to our method.

To confirm the specificity of the parameters identified by the hypernetwork for different editing samples, we compute the intersection over union (IoU) of the corresponding binary masks at thesparsity level for samples within and outside the same groups in the natural image subset. Fig.illustrates that the identified parameters demonstrate substantial overlaps for images within the same group and much lower overlaps between images from different groups. These findings support that the hypernetwork successfully pinpoints key parameters necessary to correct specific errors while effectively excluding parameters associated with other unrelated samples. This learned mask specificity allows our method to balance effectively between generalization and locality.

We further evaluate our method when multiple editing samples in the same group (, with similar failure causes) are available. As a straightforward extension, we compute the average of the continuous masks generated from each sample, followed by binarization using Eq. (). Fig.presents the results of using one, two, and three samples for model editing. Remarkably, the editing performance improves with more editing samples, which can be attributed to more precise parameter localization as a result of the ensemble of masks.

More ablation studies (, the alternative pseudo-sample generation strategy, the sparsity regularization in the outer loop, the gradient step and learning rate in the inner loop, and the number of attention blocks in the hypernetwork) are in the Appendix.

SECTION: Conclusion and Discussion
We have introduced a model editing method to correct predictive errors in ViTs. Our method prioritizes where-to-edit over how-to-edit by
meta-training a hypernetwork to identify a subset of structured parameters for editing. By applying-norm regularization, our method promotes sparsity in the generated mask, thereby indirectly ensuring locality without needing to retrain on previously used data. Comprehensive tests on the proposed editing benchmark confirm that our method effectively corrects predictive errors in ViTs. Moreover, the introduced edits are not only reliable but also generalize well to neighboring samples, while maintaining a high rate of locality.

Our work is among the early endeavors in CV model editing, and it raises several intriguing questions for future research. First, our approach utilizes the CutMix techniqueto generate cost-effective pseudo-samples for training, but its effectiveness has only been confirmed empirically. The reasons why the hypernetwork trained on such synthetic data achieves reasonable generalization and the identification of optimal synthetic data generation techniques remain wide open.
Second, it would be beneficial to adapt our method to other vision architectures, such as convolutional networks or Swin Transformers, and extend its application to other vision areas like dense prediction, generative modeling, and multimodal LLMs. Third, exploring how to apply our method in a batch-editing setting represents a promising avenue. In such scenarios, the use of a decoupling trick (see more details in the Appendix) may prove essential for effectively reducing computational and memory demands.

SECTION: References
SECTION: More Details about the Editing Benchmark
SECTION: Natural Image Subset
We divide the MAD-searched natural image subset intogroups, whose statistics are listed in Table. Visual examples in each group are shown in Figs.and. These images are sourced from Flickr, prior to the advent of Stable Diffusion, and are licensed under creative commons.

SECTION: AI-generated Image Subset
We adopt Textural Inversionand PUGto construct the AI-generated image subset, encompassing the oil painting and stage light shifts, respectively. The statistics are given in Table.

Specific classes in the oil painting subset include,,,,,,,,,,,,,,,,,,,,, and.

Specific classes in the stage light subset include,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, and.

Visual examples of the oil painting and stage light images are shown in Fig.and Fig., respectively.

SECTION: Potential Dataset Filtering
Recall that the editing benchmark is designed to challenge the ViT/B-16 model. Thus, it is likely that some images might not induce predictive errors in other base models, which vary in terms of training data, model architecture, loss function, and optimization pipeline. For the ViT/S-16 model, the benchmark is subject to an additional filtering process based on its predictions. Consequently,of the natural images andof the AI-generated images are retained.

SECTION: More Experimental Details
In this section, we give more implementation details of the proposed and competing model editing methods. Algorithmpresents the pseudo-code of our method.

SECTION: More Details of Our Method
In meta-learning, optimization of the hypernetwork entails
differentiating the outer-loop loss with respect to the output of the inner loop, and propagating the gradient through the inner-loop optimization to the output of the hypernetwork(approximated by Eq. ()), and finally to the parameters of the hypernetwork,. This extended chain of computation not only demands substantial computational resources but also hampers efficient optimization. To mitigate these, we decouple the pathway of hypernetwork optimization from the meta-learning gradient. Specifically, we introduce an auxiliary variable, matching the dimensionality of, to substitute for the hypernetwork’s output during bi-level optimization. As a result,is now dependent on, rather than.
We first optimize the auxiliary variable:

Subsequently,directs the parameter optimization of the hypernetwork using the element-wise KL divergence averaged across all positions:

whereis the positional index andin our implementation.

When applying CutMix, we vary the sizes of the pasted patches fromto, ensuring the preservation of the primary structural and textural details in the original images, which arein size.

We design the hypernetwork to mirror the architecture of its corresponding base model (, ViT/B-16 or ViT/S-16), with the same input and intermediate dimensions. Nevertheless, we reduce the number of attention blocks to five.

We set the learning rate in the inner loop as, and perform gradient descent for five steps (,). In the outer loop, we apply the Adam optimizer with a learning rate ofto optimizefrom random initialization for a total of ten steps.
For the hypernetwork optimization, RMSPropis utilized with a learning rate of, a minibatch size of eight, and a maximum iteration number of. Training a hypernetwork for the base ViT/B-16 takes approximately 9 hours on a single RTX A6000 GPU (48G).

SECTION: Implementation Details of Competing Methods
For methods that involve updating the base model parameters through backpropagation—including FT, FT-, KN, SPT, and our method—we followand adopt RMSProp as the optimizer, where the learning rate is set tofor ViT/B-16 andfor ViT/S-16, respectively.

T-Patcheradds one neuron in the last FFN, together with a trainable multiplier initialized as. The new parameters are optimized using Adam with a learning rate of.

ROMEemploys Adam with a learning rate ofto obtain the target hidden representations of the last FFN, and then solves a constrained least squares problem to update the second FC layer.

We follow the default setting in LoRA, adding learnable matrices with a rank of eight. These low-rank matrices are optimized by Adam with a learning rate of.

For KEand MEND, we adhere to their training protocols to edit the six FC layers within the last three FFNs. The hypernetworks are meta-trained on editing samples sourced from ImageNet-1k to alter the base model’s predictions to match the top-randomly selected classes. The optimizer is Adamwith a learning rate of.

SECTION: More Experimental Results
SECTION: More Editing Results for ViT/B-16
In the main paper, we report the averaged editing results for ViT/B-16 across the sixteen groups in the natural image subset. Here, we further report the editing results on each group in Fig..

SECTION: Editing Results for ViT/S-16
Fig.presents the editing outcomes for ViT/S-16, where our method continues to exhibit the optimal generation-locality trade-off, demonstrating its adaptability across various model architectures. Meanwhile, Fig.presents the editing results on each group in the natural image subset.

SECTION: More Analysis
We present the training curves of the hypernetwork in Fig.. We find that the mask sparsity increases rapidly at the beginning of training fromto, which poses challenges for successful edits. As training progresses, the mask sparsity stabilizes while the KL divergence decreases.
This suggests that the hypernetwork has effectively located key parameters relevant to successful edits.

SECTION: Ablation Studies
We further compute the averaged IoU results of the binary masks at thesparsity level for editing samples among eight groups in the natural image subset. The results in Fig.show that the identified parameters exhibit substantial overlaps for samples within the same group and much lower overlaps for samples from different groups.

We examine another more computationally expensive pseudo-sample generation strategy,, PGD, which has been validated to capture diverse distribution variations. Given a natural imagewith the labelin the pre-training set, we apply PGDonto obtain the pseudo-samplewith the prediction different from. We set the number of attack steps towith a step size of, under the feasible set of. During training, we employ the cross-entropy lossto correct the prediction of.

Fig.shows the editing results of two hypernetworks meta-trained using the two different pseudo-sample generation approaches. Remarkably, the simple CutMix rivals PGD in simulating distribution shifts, even in the two AI-generated image subsets.

In the outer loop, we introduce a trade-off hyperparameter,, to balance the reliability objective with the sparsity regularizer. Here, we explore the impact ofand observe that the sensitivity of hypernetwork to this trade-off parameter is minimal, as shown in Fig..

For the gradient step,, in the inner loop, we test values of. The performance of ViT/B-16 for each setting is illustrated in Fig., where we find that one gradient step yields slightly inferior results compared to more steps. Five and ten steps perform similarly, yet ten steps have greater training costs. Thus, we opt for five gradient steps as the default.

We explore the impact of the learning rate in the inner loop with values from. The editing results shown in Fig.indicate that a lower learning rate (,) exhibits slightly inferior performance than a larger learning rate. This may arise because a lower learning rate results in minimal updates to the base model within five gradient steps, thereby ineffective in guiding the hypernetwork training.

We additionally conduct ablative experiments to evaluate the impact of the number of attention blocks in the hypernetwork. We test values of, and the editing performance for ViT/B-16 is illustrated in Fig., where we find that a small hypernetwork can achieve comparable performance to larger hypernetworks. Decreasing the number of attention blocks in the hypernetwork from five to three, and to one, does not incur a noticeable performance drop.

SECTION: Limitations
See the Conclusion and Discussion section in the main text.

SECTION: Broader Impact
Model editing has a broad impact by accelerating innovation in AI development through rapid iterations and refinements without extensive retraining, thus conserving resources and reducing environmental impact. The proposed method enables error correction of CV models, thereby enhancing adaptability and accessibility. We believe our method has great potential in addressing ethical concerns by mitigating biases and improving fairness in CV applications, while also increasing the robustness of CV systems against security threats like adversarial attacks.