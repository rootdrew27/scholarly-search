SECTION: Automatic Speech Recognition with BERT and CTC Transformers: A review

This review paper provides a comprehensive analysis of recent advances in automatic speech recognition (ASR) with bidirectional encoder representations from transformers BERT and connectionist temporal classification (CTC) transformers. The paper first introduces the fundamental concepts of ASR and discusses the challenges associated with it. It then explains the architecture of BERT and CTC transformers and their potential applications in ASR. The paper reviews several studies that have used these models for speech recognition tasks and discusses the results obtained. Additionally, the paper highlights the limitations of these models and outlines potential areas for further research. All in all, this review provides valuable insights for researchers and practitioners who are interested in ASR with BERT and CTC transformers.

SECTION: IIntroduction

Traditional methods of speech recognition rely on maximum a posteriori probability estimation, which involves transforming the acoustic speech characteristics into word sequences through four steps: feature extraction, acoustic modeling, language modeling, and word sequence decoding. Feature extraction involves essential data extraction from the input signal using algorithms such as Mel-frequency cepstral coefficients (MFCC)[1]and perceptual line spectral pairs (LSP)[2]. The acoustic modeling stage utilizes deep neural networks and hidden Markov models to map the acoustic frame to the phonetic state at each input time, optimized for the phonetic classification error per frame. Language modeling is designed to model the most probable sequences of words, regardless of acoustics[3]. The use of transformers in speech recognition involves several steps, such as : (i) preprocessing of the audio signal, to extract essential features like log Mel filterbank energies, (ii) an acoustic model based on a self-attention mechanism to model the temporal relationships between acoustic features, and (iii) a language model trained on a large amount of text data to capture long-term dependencies between words. The language model takes a sequence of words as input and predicts the probability distribution of the next word in the sequence. Finally, the decoding process involves finding the most likely word sequence, given the output of both the acoustic and language models.

To sum up, transformers such as BERT, and connectionist temporal classification (CTC) based ASR is a recent advancement in speech recognition that uses the self-attention mechanism to simultaneously perform feature extraction, acoustic modeling, language modeling, and decoding in a single network. On the other hand, the transformer architecture is a neural network model that is designed to process sequential data by attending to relevant context information. Transformers have demonstrated promising outcomes in ASR and are expected to play a crucial role in future advancements in this field.

SECTION: I-ARelated work and our contribution

In[4]the authors explore various methodologies for detecting emotions in text using BERT and its variants. They thoroughly outline their approaches, contributions, achieved accuracies, and also discuss the limitations or weaknesses of their models. However, our review focuses on ASR rather than emotion using BERT. While the authors primarily mention BERT base, BERT large, RoBERTa, DistillBERT, and cross-lingual language model (XLM), which are the BERT variants employed in their research, we expand on their contributions by incorporating additional models such as ALBERT, and ELECTRA.
The work in[5]focuses on providing a tutorial and survey on the attention mechanism, transformers, BERT, and GPT. It explains various concepts such as the attention mechanism, transformers, and their components. However, our review focuses on both BERT and CTC transformer applications, specifically in ASR. In[6]the survey discusses the impressive performance of transformer models like BERT, GPT, RoBERTa, and T5 across various language tasks such as text classification, machine translation, and question answering. Additionally, the article also explores their applications in computer vision. However, our review focuses on BERT and CTC transformers within the domain of ASR, distinguishing our research from other domains discussed in the aforementioned survey. In[7]encompasses various speech-related domains such as ASR, speech synthesis, speech translation, speech para-linguistics, speech enhancement, and other applications. The authors of the survey identify and discuss the challenges that transformers face in these domains. In our review, similar to the survey, we address these challenges, but we extend the scope by including additional transformers like ELECTRA, ALBERT, and CTC Transformers specifically in the context of ASR.

SECTION: I-BPaper structure

The rest of the paper is organized as follows: SectionIIpresents the methodology used to create this review. SectionIIIprovides an overview of the preliminaries on CTC and BERT. SectionIVdefines BERT and categorizes articles based on the utilization of BERT in ASR, highlighting the advancements made in this area. SectionVdefines CTC and classifies articles based on the application of CTC in ASR, showcasing the progress achieved in this domain. SectionVIdiscusses potential future directions. Finally, the paper concludes in sectionVII.

SECTION: IIReview Methodology and Analysis

This review focuses mainly on two distinct categories of papers: The first category examines BERT-based ASR systems, the second category of papers explores CTC-based ASR techniques. The initial search was done using related keywords for transformers in ASR, namely: BERT and ASR, CTC and ASR, or BERT and CTC and ASR.
A search was performed on scientific databases indexed at least in Scopus and available in IEEE Xplore and Springer, science direct, and others. Besides, arXiv papers are taken into consideration that have many citation and impact, which are known for their extensive coverage and relevance to ASR. Additionally, Google Scholar was utilized to include a broader range of publications, including gray literature, which can provide valuable insights for a systematic review. Only the most widely used methods and implementations were included to ensure modularity. The focus is on papers that reported new and unique applications within specific domains, avoiding repetition. Emphasis is given to papers published in high-quality journals with a significant impact factor. The search has been conducted until 2023 to gather the most recent available information at the time of the review.

SECTION: IIIPreliminaries

SECTION: III-ADataset

Several datasets have been utilized in various ASR tasks in the existing literature. TableIprovides a compilation of some of these datasets that have been used specifically for BERT and CTC-based ASR applications, along with their specific characteristics[3].

SECTION: III-BMetrics

The ASR research community has employed several methods to evaluate the quality and generalizability of ASR techniques. Besides the famous metrics that are commonly employed in ML and DL, such as accuracy, F1-score, recall, and precision[24], other metrics are used including word error rate (WER) and character error rate (CER), and real-time factor (RTF), are thoroughly described in[3].

SECTION: IVBERT-based ASR

BERT, developed by Devlin et al. in 2019[25], is a pre-training model for NLP tasks that utilizes transformer encoders[26]. It consists of two phases: pre-training for language understanding and fine-tuning. These latter are for specific tasks like sentiment analysis, question answering, text summarization, and more. During pre-training, BERT employs masked language modeling (MLM) and next-sentence prediction (NSP). MLM involves masking some words in sentences and reconstructing them using the surrounding context during training. NSP helps BERT understand the relationship between two sentences by predicting if the second sentence follows the first. BERT was trained on 16GB of text data from the books corpus datasets and English Wikipedia. After pre-training, the model is fine-tuned for a specific task by replacing BERT’s output layers. This fine-tuning process is faster since only the model parameters, excluding the output parameters, are learned from scratch. There are two versions of BERT: BERT-base and BERT-large. BERT-base consists of 12 transformer encoder blocks with 12-head self-attention layers and 768 hidden layers, resulting in approximately 110 million parameters. BERT-large has 24 transformer encoder blocks with 24-head self-attention layers and around 340 million parameters. BERT-large achieves higher accuracies but requires more computational resources compared to BERT-base[4]. However, BERT has a few notable limitations. Firstly, it is primarily designed for monolingual classifications, meaning its optimal performance is achieved when working with a single language. While it is possible to fine-tune BERT for multilingual tasks, its effectiveness may be somewhat diminished compared to its performance on monolingual tasks. Secondly, the length of input sentences can also present challenges. BERT has a maximum token limit, typically set at 512 tokens, which means longer sentences need to be truncated or split into smaller segments, potentially losing some contextual information[27]. TableIIpresents a summary of the performance achieved in the cited papers compared to other systems, including the metrics used to evaluate the results, and the source code availability. Figure1illustrates BERT variants featuring transformers along with the attention layers they are built upon.

SECTION: IV-ABERT-based SMCQA framework

The authors in[8]developed a framework called MA-BERT for spoken multiple-choice question answering (SMCQA) task, which uses a combination of multi-turn audio-extractor hierarchical CNNs (MA-HCNNs) and BERT to extract acoustic-level and text-level information, respectively, from speech data. The proposed framework outperformed various state-of-the-art systems. However, the scheme in[9]proposes a novel audio-enriched BERT-based (aeBERT) framework for improving performance on the SMCQA task, where syllables, questions, and choices are all given in speech. Besides, the method proposes incorporating acoustic-level information from the speech input to enhance the accuracy of SMCQA systems. The resulting audio-enriched BERT-based SMCQA framework shown to outperform various state-of-the-art systems by a large margin.

SECTION: IV-BBERT-based reranking framework

Chiu et al.[10]propose a BERT n-best reranking framework that incorporates cross-utterance information signals using a graph convolutional network (GCN) to model historical utterances for better ASR performance. The approach addresses the limitations of recurrent neural network (RNN) and LSTM-based language models (LMs) in capturing complex global structural dependencies among utterances. Nevertheless, the study in[28]introduces a new implementation of BERT-based contextualized language models specifically for reranking the n-best hypotheses generated by ASR systems. The approach frames the n-best hypothesis reranking as a prediction problem, aiming to predict the oracle hypothesis with the lowest WER.

SECTION: IV-CBERT-based model for speech summarization

Kano et al.[11]suggest a novel text summarization (TS) method that combines sub-word embedding vectors and posterior values from an ASR system. They incorporate an attention-based fusion module into a pre-trained BERT module for improved summarization. This fusion module aligns and merges multiple ASR hypotheses. The researchers then perform experiments on speech summarization using both the How2 and TED dataset. In[29]The authors of the paper enhance a BERT-based model for speech summarization in three ways: incorporating confidence scores into sentence representations to address ASR errors, augmenting sentence embeddings with additional features, and validating the model’s effectiveness on a benchmark dataset compared to classic summarization methods. The goal is to improve the model’s performance and overcome challenges caused by imperfect ASR.

SECTION: IV-DBERT-based model for distilling the knowledge

Futami et al.[20]propose a method to improve ASR using a combination of a (sequence-to-sequence) seq2seq model and BERT as an external language model. The seq2seq model is enhanced with both left and right context through knowledge distillation from BERT which generates soft labels to guide the training. Additionally, context beyond the current utterance is leveraged as input to BERT. The proposed method is evaluated on the CSJ, showing significant improvements in ASR performance compared to the seq2seq baseline. This method surpasses alternative approaches in LM applications like n-best rescoring and shallow fusion with not requiring any additional inference cost. Jiang, B et al.[15]suggest a method for end-to-end intent classification using speech, which does not rely on an intermediate ASR module. It leverages the transformer distillation method to transfer knowledge from a transformer-based language model BERT to a transformer-based speech model for intent classification. A multi-level transformer-based teacher-student model is designed, and knowledge distillation is performed across attention and hidden sub-layers of different transformer layers. The proposed method achieves a high level of accuracy in intent classification and showcases superior performance and resilience in acoustically degraded conditions when compared to the baseline method.

SECTION: IV-EBERT, RoBERTa, XLM-RoBERTa, and ELECTRA models

Ganesan et al.[12]propose a method to improve the performance of spoken language understanding (SLU) systems by using concatenated n-best ASR alternatives as input to transformer models, such as BERT XLM-RoBERTa on DSTC2 dataset[30].
In their paper, Chen et al.[13]introduce a discriminative self-training method that incorporates weighted loss and discriminative label smoothing for improving punctuation prediction in ASR output transcripts, the authors utilize extensive unlabeled spoken language data, which lacks punctuation, such as transcripts employed for training ASR systems. They employ self-training techniques to enhance robust baseline models built on BERT, RoBERTa, and ELECTRA.

SECTION: IV-FHuBERT and LightHuBERT models

In their study[31], the authors introduce a novel speech pre-training method called ”HuBERT-AP.” This approach utilizes patterns derived from target codes as the training signal to facilitate the model in acquiring improved acoustic features. The patterns, referred to as ”acoustic pieces,” are constructed based on the sentence piece outcomes of the original HuBERT target codes, and are highly relevant to phonemized natural language, making them beneficial for audio-to-text tasks. The proposed method is evaluated on the LibriSpeech ASR task, and is shown to be significantly more effective than previous strong baselines. However, the authors in[32]propose LightHuBERT, a compressed version of the HuBERT model, which is a self-supervised speech representation learning model. LightHuBERT is designed as a once-for-all transformer compression framework. To automatically discover desired architectures through pruning structured parameters, the researchers create a transformer-based supernet that encompasses numerous weight-sharing subnets. They also employ a two-stage distillation strategy to leverage contextualized latent representations from HuBERT. Experimental results on ASR and the SUPERB benchmark demonstrate that LightHuBERT surpasses HuBERT in ASR tasks while reducing parameters by 29%. Furthermore, LightHuBERT achieves a compression ratio of 3.5 times in three SUPERB tasks, albeit with a slight loss in accuracy.

SECTION: IV-GNorBERT and Speech-BERT models

Rugayan et al.[33]propose a robust evaluation metric, aligned semantic distance (ASD), for Norwegian ASR systems. They leverage semantic information modeled by a transformer-based LM and employ dynamic programming techniques to measure the similarity between reference and hypothesis text. ASD utilizes NorBERT embeddings to compute the optimal alignment and obtain the minimum global distance. This distance is then normalized by the length of the reference embedding vector. Additionally, the researchers present results using another metric called semantic distance (SemDist), and they compare the performance of ASD with SemDist. The authors in[34]introduced a neural model called speech-BERT, which combines a bidirectional transformer LM with a neural zero-inflated beta regression approach. This approach is specifically designed to be conditioned on speech features. To fine-tune speech-BERT, the authors utilized a pre-training strategy known as token-level masked language modeling. Additionally, they incorporated a zero-inflated layer into the model to effectively handle the mixture of discrete and continuous outputs.

SECTION: IV-HBERT-based language models

Chang et al.[35]introduce an innovative network called the context-aware transformer transducer (CATT), which enhances the performance of transformer-based ASR systems by leveraging contextual signals. The authors propose a context-biasing network based on multi-head attention, which is trained alongside other sub-networks of the ASR system. Various techniques are explored to encode contextual data and generate the ultimate attention context vectors. To encode the contextual information and facilitate network training, both BLSTM and pre-trained BERT models are utilized. The researchers in[36]propose two deep neural network (DNN) models to improve ASR by modeling long-term semantic relations. They employed as input features to their DNN model two things: (i) dynamic contextual embeddings are derived from BERT, a transformer-based model specifically designed for acoustic tasks. (ii) Additionally, linguistic features are incorporated into the system. Moving forward, the scheme proposed in[37]discusses the linguistic diversity in India and the need for speech recognition in regional languages. The paper suggests the creation of an advanced ASR system based on deep sequence modeling, aiming to address the challenges posed by low-resource languages. The proposed model incorporates an enhanced spell corrector component. The performance of the proposed system is assessed using metrics such as WER and sequence match ratio. Notably, the experimental results demonstrate promising outcomes, with an average WER of 0.62. The latter result proves the importance of spell correction in ASR systems and the use of a transformer-based LM for performance improvement.

SECTION: VCTC-based ASR

CTC is a variant of the transformer architecture that is used in seq-2seq learning tasks, particularly in ASR. The CTC transformer combines the concepts of the CTC loss function and the transformer architecture, which are both powerful tools for sequence modeling. The CTC loss function is commonly used in ASR to align the predicted sequence with the ground truth sequence by taking into account the presence of blank symbols and repeated characters. In the following, a brief summary of the proposed approaches-based CTC transformer. TableIIpresents a summary of the performance achieved in the cited papers compared to others systems, including the metrics used to evaluate the results, and the source code availability. Figure2shows a CTC variation, elucidating its intended purpose and objectives.

SECTION: V-AMask CTC

The proposed method, detailed in[21], consists of a novel non-autoregressive end-to-end ASR called mask CTC. This framework generates a sequence by refining the outputs of the CTC model, which is a popular method used for ASR, while autoregressive models generate one token at a time and require as many iterations as the output length. Non-autoregressive models offer the advantage of generating tokens simultaneously in a fixed number of iterations, resulting in substantial reductions in inference time. The mask CTC model employs a training methodology that combines a transformer encoder-decoder architecture with simultaneous training of mask prediction and CTC during inference. Initially, the target sequence is initialized with the greedy CTC outputs. Afterward, tokens with low confidence are selectively masked using the CTC predictions. By taking into account the conditional interdependence among output tokens, the model predicts the masked low-confidence tokens using the high-confidence tokens.

SECTION: V-BNAR CTC

Inaguma et al in[16]propose a faster version of the multi-decoder (MD) end-to-end speech translation model called Fast-MD. The MD model decomposes the overall speech translation task into ASR and machine translation sub-tasks, but its decoding speed is not fast enough for real-world applications. Fast-MD generates hidden intermediates (HI) by NAR decoding based on CTC outputs followed by an ASR decoder. The scheme employs sampling CTC outputs during training to reduce a mismatch in the ASR decoder. The authors also suggest that adopting the conformer encoder and intermediate CTC loss can further boost the model’s quality without sacrificing decoding speed.
Song et al.[18]propose a solution to the accuracy degradation problem faced by NAR transformer models in ASR. The proposed solution is a CTC-enhanced NAR transformer that refines the predictions of the CTC module to generate the target sequence.
The paper[17]presents improvements to the end-to-end CTC alignment-based single-step non-autoregressive transformer (CASS-NAT) for speech recognition. The proposed methods include applying convolution augmented self-attention blocks to the encoder and decoder modules, expanding the trigger mask for each token to increase CTC alignment robustness, and using iterated loss functions to enhance gradient updates. Fujita et al. in[23]proposed a method for non-autoregressive ASR streaming input or long recording. They used an insertion-based model that jointly trained CTC and achieved better accuracy with fewer iterations using transformer with greedy decoding. The authors suggested combining audio segmentation and non-autoregressive ASR into a single neural network. This integration leverages the CTC component of the insertion-based model, utilizing causal self-attention implemented through block self-attention, similar to the transformer XL. Experimental outcomes demonstrated that the proposed approach achieved a favorable trade-off between accuracy and RTF when compared to both the autoregressive transformer and CTC baseline models.

SECTION: V-CAuxiliary CTC and End-to-end CTC

The method introduced by the authors in[19]offers a means to enhance CTC-based ASR models by loosening the assumption of conditional independence in CTC. The method involves training a CTC-based ASR model with auxiliary CTC losses in intermediate layers. Predictions from these layers are accumulated and conditioned on in subsequent layers, resulting in improved performance compared to a standard CTC model across multiple ASR corpora. Furthermore, the proposed method achieves comparable performance to a strong auto-regressive model with beam search on the TEDLIUM2 corpus and the AISHELL-1 corpus, while being at least 30 times faster in decoding speed. Andrusenko et al. in[38]explore different end-to-end ASR systems for the largest open-source Russian language data set – OpenSTT. They compare existing end-to-end approaches, including joint CTC/Attention, RNN-transducer, and transformer, with a strong hybrid ASR system based on the so-called LF-MMI and TDNN-F acoustic model. Is the performance of each system is evaluated on three available validation sets, including phone calls, YouTube, and books. The paper[39]adapted an end-to-end transformer acoustic model to the speech of children learning to read, with the aim of enhancing ASR performance for this challenging task. They used transfer learning with a small amount of child speech and multi-objective training with a CTC function. They also proposed a method of data augmentation for reading mistakes, where they simulated word-level repetitions and substitutions with phonetically or graphically close words. The authors analyzed the performance of their model and showed that both the CTC multi-objective training and the data augmentation with synthetic repetitions assisted the attention mechanisms better identify children’s disfluencies.

SECTION: V-DCTC-Based Other Approaches

In their work[14], Chen et al. introduce the controllable time-delay transformer (CT-Transformer) model, which addresses both punctuation prediction and disfluency detection tasks in real-time. These tasks are crucial for enhancing transcript readability and enabling subsequent applications. The CT-Transformer model incorporates a mechanism for selectively freezing partial outputs with adjustable time delays to meet the real-time constraints imposed by downstream applications. Experimental results demonstrate that the proposed approach surpasses previous state-of-the-art models in terms of F-scores, while also achieving competitive inference speed on benchmark datasets such as IWSLT2011[40]and an in-house Chinese annotated dataset. Moritz et al. in[22]describe the development and implementation of an ”all-in-one” (AIO) acoustic model based on the transformer architecture. The AIO model is designed to simultaneously solve the problems of ASR audio tagging (AT), and acoustic event detection (AED), using shared parameters across all tasks. The authors argue that this approach more closely mimics the way the human auditory system processes sound signals from different sources. The integration of the transformer model with CTC enables the enforcement of monotonic ordering and the utilization of timing information for both ASR and AED tasks. The AIO transformer model consistently outperforms all baseline systems in recent DCASE challenge tasks, showcasing its aptness for comprehensive transcription of acoustic scenes, encompassing speech recognition and identification of other acoustic events. Xiao et al. in[41]propose a new framework for an automatic voice query service AVQS to improve the accuracy of response for multi-accented Mandarin users. The problem addressed is that many dialect areas in China make it necessary for the AVQS to respond to users with a single acoustic model in ASR, limiting its accuracy. The proposed framework uses a fusion feature comprising i-vector and filter-bank acoustic features to train a transformer-CTC, which is then used to construct an end-to-end ASR. Additionally, a keyword-matching algorithm based on fuzzy mathematics theory is proposed to further enhance the accuracy of the response.

SECTION: VIFuture directions

SECTION: VI-ABERT and ChatGPT

Improved contextual coherence:By combining ChatGPT ability to generate human-like responses with BERT strong contextual understanding, the integration enhances the coherence and relevance of the chat responses by leveraging BERT knowledge of bidirectional dependencies in text.

Enhanced language comprehension:BERT extensive pre-training on a large corpus enables it to understand language nuances effectively. When integrated with ChatGPT, it improves the model’s language understanding capabilities, enabling it to comprehend user inputs, handle complex queries, and provide more accurate and context-aware responses.

Effective handling of ambiguity and multiple meanings:ChatGPT can sometimes struggle with phrases that have multiple interpretations. By incorporating BERT contextual representation, which considers the surrounding context, the integrated model becomes better at disambiguating such phrases and generating responses that are more accurate and appropriate in context.

SECTION: VI-BCTC and ChatGPT

Enhanced language generation:By integrating ChatGPT into CTC, the speech output generated becomes more natural and engaging due to ChatGPT proficiency in generating human-like responses.

Context-aware speech Generation:Incorporating ChatGPT ability to understand contextual cues into CTC enables the model to generate speech that is more coherent and relevant by considering the surrounding context.

Versatile text-to-speech applications:CTC is widely used in text-to-speech systems. Integrating ChatGPT with CTC expands the capabilities of TTS applications, making them more flexible and adaptable. This allows for interactive and dynamic speech generation by leveraging ChatGPT conversational capabilities.

Enhanced personalization and user interaction:ChatGPT excels in personalized conversations, and when combined with CTC, it enables the integrated model to generate speech that adapts to user preferences. This results in more interactive and engaging interactions, leading to a highly personalized user experience.

SECTION: VIIConclusion

Transformers play a crucial role in ASR by capturing contextual information and long-range dependencies. They improve accuracy by considering the entire context and utilizing attention mechanisms to focus on relevant information. Pre-trained models like BERT, RoBERTa, and ELECTRA have proven effective in transfer learning for ASR, benefiting from knowledge acquired on large-scale datasets. Additionally, the CTC method enables end-to-end training, handles variable-length inputs, incorporates label-smoothing regularization, integrates with language models, and supports online and streaming ASR applications. CTC is a flexible and effective approach for transcribing speech signals, contributing to robust and accurate ASR systems applied to diverse domains, such as biomedical[42].

In this survey, the idea of incorporating ChatGPT into the BERT and CTC frameworks is proposed, opening new avenues for research and development. By leveraging ChatGPT’s conversational abilities and natural language understanding, it is suggested to enhance BERT and CTC capabilities. This integration aims to improve ASR performance, accuracy, and contextual understanding, leading to advanced speech recognition applications.

SECTION: References