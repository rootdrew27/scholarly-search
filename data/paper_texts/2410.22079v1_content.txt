SECTION: HRPVT: High-Resolution Pyramid Vision Transformer for medium and small-scale human pose estimation
Human pose estimation on medium and small scales has long been a significant challenge in this field. Most existing methods focus on restoring high-resolution feature maps by stacking multiple costly deconvolutional layers or by continuously aggregating semantic information from low-resolution feature maps while maintaining high-resolution ones, which can lead to information redundancy. Additionally, due to quantization errors, heatmap-based methods have certain disadvantages in accurately locating keypoints of medium and small-scale human figures. In this paper, we propose HRPVT, which utilizes PVT v2 as the backbone to model long-range dependencies. Building on this, we introduce the High-Resolution Pyramid Module (HRPM), designed to generate higher quality high-resolution representations by incorporating the intrinsic inductive biases of Convolutional Neural Networks (CNNs) into the high-resolution feature maps. The integration of HRPM enhances the performance of pure transformer-based models for human pose estimation at medium and small scales. Furthermore, we replace the heatmap-based method with SimCC approach, which eliminates the need for costly upsampling layers, thereby allowing us to allocate more computational resources to HRPM. To accommodate models with varying parameter scales, we have developed two insertion strategies of HRPM, each designed to enhancing the model’s ability to perceive medium and small-scale human poses from two distinct perspectives. Our proposed method achieved scores of 76.3 AP and 75.5 AP on the MS COCO Keypoint validation and test-dev datasets, respectively, while reducing the number of parameters by 60% and the GFLOPs by 62% compared to HRNet-W48. Furthermore, our method achieved the higher score in the APmetric among most state-of-the-art methods, validating its superiority in medium and small-scale human pose estimation.

[1]organization=School of Computer Science and Information Engineering, Shanghai Institute of Technology,
addressline=100 Haiquan Road,
city=Shanghai,
postcode=201418,
country=China

A novel hybrid Vision Transformer architecture has the capability to enhance human pose estimation at medium and small scales.

Two insertion strategies derived from HRPM enhance the estimation of human poses at medium and small scales from two different perspectives.

HRPVT achieves superior performance while reducing the number of parameters by 60% and the GFLOPs by 63% compared to HRNet-W48.

SECTION: Introduction
Human pose estimation (HPE) stands as a cornerstone task in computer vision, involving the detection of keypoint locations on the human body and the categorization of these keypoints for each individual in a given image. Its significance extends to numerous downstream applications, such as activity recognition, human-robot interaction, and video surveillance. However, HPE presents formidable challenges owing to a multitude of factors, including occlusion, truncation, under-exposed imaging, blurry appearances, and the low-resolution nature of person instances.
In the early stages of 2D human pose estimation, regression-based methods were frequently explored. These methods directly regress the keypoint coordinates within a computationally efficient framework. However, due to unsatisfactory performance, only a limited number of existing methods have adopted this scheme. In recent years, heatmap-based methods have emerged as the predominant approach, offering advantages such as reducing false positives and facilitating smoother training through the assignment of probability values to each position. Despite their success, these methods encounter a significant challenge: the persistent quantization error problem, particularly evident in scenarios with low resolution or small-scale persons. This issue arises from the mapping of continuous coordinate values into discretized 2D downscaled heatmaps. Numerous efforts have been made to address this quantization error, with Simccbeing one notable example. Simcc approaches human pose estimation as two classification tasks for horizontal and vertical coordinates, which not only removes costly upsampling layers but also demonstrates superior localization accuracy for medium and small-scale persons compared to heatmap-based methods.

Recently, Vision Transformer (ViT)has revolutionized the use of pure transformer architecture in vision tasks, showcasing promising outcomes. While ViT excels in tasks like image classification, directly adapting it to pixel-level dense predictions, such as human pose estimation, poses significant challenges. For example, ViTPoseachieves superior performance but at the cost of substantial model parameter size and computational complexity. Addressing this, the PVT seriesintroduces pioneering methods, including a progressive shrinking pyramid and spatial-reduction attention to refine the pure transformer architecture and improve its suitability for dense prediction tasks. Although the PVT series significantly enhances the pure transformer architecture, it still lacks the intrinsic inductive biases of CNNs, which excel in modeling local visual structures and handling scale variance. As a pure transformer, it boasts strong capabilities in modeling long-range dependencies but falls short in capturing local feature details inherent to CNNs. For HPE, on one hand, keypoints constraint relationships and visual cues precisely correspond to these two critical abilities; on the other hand, in certain scenarios, such as outdoor extreme sports, the protean nature of human poses makes it difficult to accurately predict medium and small-scale human postures, even when using a top-down paradigm. Moreover, in many previous works, such as Higher-HRNet, high-resolution representations with high quality have been proven to be significantly helpful in addressing this issue. Thus, we wondered if it is possible to eliminate costly upsampling layers through SimCC and instead focus our limited computational resources on integrating the intrinsic inductive biases of CNNs, such as scale-invariance and locality, into the high-resolution feature maps of PVT. This approach may achieve a favorable trade-off between accuracy and efficiency.

Motivated by this, we propose a novel model called the High-Resolution Pyramid Vision Transformer (HRPVT), which uses a combination of PVT v2 and Simcc as the baseline. Building on this foundation, we have designed a module, namely the High-Resolution Pyramid Module (HRPM). The HRPM consists of two sub-modules, HRPM v1 and HRPM v2, which are designed to model the scale-invariance and locality by CNNs in the high-resolution feature maps, thereby enhancing the network’s ability to localize keypoints in medium and small-scale human figures. Moreover, according to the insertion position of HRPM v2, we designed two insertion strategies, namely Layer-wise Insertion and Stage-wise Insertion, to accommodate baselines with different capacity from two perspectives.Our main contributions are summarized as follows:

We propose a novel model called HRPVT, which introduces the intrinsic inductive biases of scale invariance and locality, inherent in CNNs, into the high-resolution feature maps of PVT v2. This enhances the network’s ability to localize keypoints in medium and small-scale human figures.

We develop two insertion strategies—Layer-wise Insertion and Stage-wise Insertion—each designed from a distinct perspective to address baselines of varying complexity.

Our HRPVT model has demonstrated outstanding performance on both the MS COCO and MPII datasets. Notably, on the MS COCO dataset, HRPVT outperformed HRNet-W48 while using only 40% of its parameter count and 37% of its GFLOPS, achieving even better results.

SECTION: Related work
Methods for 2D Human Pose Estimation (HPE) focus on determining the 2D coordinates or spatial positioning of human body keypoints in images or videos. Two main deep learning strategies are utilized: regression and heatmap-based approaches.

SECTION: Regression-based methods
Regression methods employ a comprehensive framework that learns to directly map the input image to the locations of body joints or to parameters defining human body models. One of the trailblazing works in this domain is DeepPose, which pioneered the transformation of the human pose estimation challenge into a keypoint coordinate regression problem, sparking a series of influential subsequent studies e.g.,. However, ongoing research uncovered several issues. First, the extensive numerical range and dispersed distribution of human keypoint coordinates make direct learning by the network difficult. Second, there is a wealth of constraint information both among human keypoints and between humans and their environment, but this information is lost in coordinate regression methods. These deficiencies substantially hinder the effectiveness of coordinate regression techniques and prevented them from outperforming heatmap-based methods. This situation persisted until Li et al.ventured into probabilistic modeling by proposing a normalizing flow model named RLE (Residual Log-likelihood Estimation). This model is designed to capture the distribution of joint locations and seeks optimized parameters through residual log-likelihood estimation.

SECTION: Heatmap-based methods
Heatmap-based approaches in human pose estimation (HPE) diverge from directly pinpointing the 2D coordinates of human joints. Instead, these methods focus on generating 2D heatmaps, which are formulated by superimposing 2D Gaussian kernels over each joint’s location. This strategy not only retains precise spatial information about each joint but also facilitates a smoother training process, offering a distinctive advantage over methods that aim to estimate joint coordinates directly. The use of heatmaps for representing joint locations has seen a surge in interest, prompting the development of CNNs’ architectures tailored for this purpose. Wei et al.introduced the Convolutional Pose Machines, a multi-stage framework that excels in pinpointing keypoint locations by leveraging 2D belief maps from prior stages to enhance prediction accuracy. Concurrently, Newell et al.developed the stacked hourglass network, a creative encoder-decoder architecture that iteratively captures and processes body pose information, supplemented by intermediate supervision. Sun et al.contributed significantly with the High-Resolution Network (HRNet), which maintains high-resolution representations throughout the network by connecting multiresolution subnetworks and performing multi-scale fusions. This innovation greatly enhances the accuracy of keypoint heatmap predictions. Although the heatmap-based approaches are widely used, quantization error still remains a significant challenge, especially with low-resolution inputs. Cheng et al.introduced an enhancement to HRNet, termed Higher Resolution Network, which employs deconvolution on the high-resolution heatmaps produced by HRNet to relatively reduce the quantization error and significantly boosts the detection of medium- and small-scale individuals.

SECTION: Quantization error problem
To mitigate the significant quantization error arising from the discretized 2D downscaled heatmaps. Zhang et al.suggested using a Taylor-expansion based distribution approximation for post-processing. This method effectively incorporates the distribution information of heatmap activations. Gu et al.proposed a method for obtaining coordinate values by normalizing the feature map and then computing the expectations. Li et al.presented a novel approach by reformulating HPE into two distinct classification tasks, one for horizontal coordinates and the other for vertical. They introduced the SimCC method, which significantly improves sub-pixel localization accuracy and minimizes quantization errors through the uniform segmentation of each pixel into several bins.

SECTION: Vision Transformers with inductive bias
ViTstands out as a groundbreaking endeavor in applying a pure transformer approach to vision tasks, yielding promising outcomes. The simultaneous developments of MViT, PVT, and Swinincorporate multi-scale feature hierarchies into the transformer design, mirroring the spatial arrangement found in conventional convolutional architectures like ResNet-50. However, a notable limitation lies in these ViT-like methods’ absence of intrinsic inductive bias in capturing local visual structures, relying instead on implicit learning from extensive data. DeiTpresents a method to distill knowledge from CNNs to transformers during training. Nonetheless, this approach necessitates employing a pre-existing CNN model as a teacher, thereby introducing additional computational overhead during training. Subsequent efforts have sought to imbue vision transformers with the intrinsic inductive bias of CNNs. For instance,adopt a strategy of stacking convolutions and attention layers sequentially, thereby establishing a serial structure conducive to modeling both locality and global dependencies. However, this sequential approach may inadvertently overlook the broader global context while focusing on local features (and vice versa). In contrast, ViTAEoffers a novel approach by concurrently modeling locality and global dependencies through a parallel structure within each transformer layer. This parallel architecture not only enhances computational efficiency but also facilitates comprehensive understanding by capturing both local and global features simultaneously.

SECTION: Methodology
The proposed HRPVT utilizes PVT v2 as the backbone to extract comprehensive representation information and employs SimCC as the 1D coordinate classifier for predicting keypoint coordinates using separate classifiers for horizontal and vertical dimensions. The structure of HRPVT is shown in Figure. We have further optimized this foundation by developing the HRPM, which is tasked with incorporating multi-scale contextual details into tokens and enhancing the modeling of local low-level semantic representations. The subsequent sections will provide detailed introductions to each component of the HRPVT.

SECTION: Revisiting PVT v2 and Simcc
We now give a brief review of our baseline, i.e. the combination of PVT v2 and SimCC. In the first stage, given an input imageof size, overlapping patch embedding (OPE) is utilized to tokenize the images, which divides the image intopatches to model the local continuity information. Subsequently, the flattened patches are fed into a linear projection, resulting in embedded token sequence of size, whererepresents the token dimension in the first stage, respectively. Then, the resulting tokens are fed into the following PVT v2 encoder layers. Each PVT v2 encoder layer is composed of two parts, i.e., a spatial-reduction attention (SRA) layer and a convolutional feed-forward network (CFFN).

In contrast to multi-head self-attention (MHSA), SRA maintains consistency with MHSA in other network architectures, with the exception that SRA reduces the spatial scale of keys and values before the attention by spatial-reduction (SR) operation. The details ofcan be described as follows:

Here,represents the token sequence ofstage, anddenotes the reduction ratio of the attention layers in Stage.is an operation of reshaping the token sequenceback to feature map of size.is a linear projection that reduces the dimension of the token sequence to.refers to layer normalization.

In comparison with the original feed-forward network (FFN), the CFFN incorporates a 3 × 3 depth-wise convolution with a padding size of 1, enabling it to capture the local continuity of the input tensor. Additionally, the introduction of positional information through zero-padding in both the OPE and CFFN allows for the removal of fixed-size positional embeddings previously used in PVT v1.

After passing through multiple PVT v2 encoder layers, the output token sequence is transformed into a feature mapof size. Similarly, by utilizing the token sequence generated from the preceding stage as input, subsequent feature maps,, andare derived, with each having strides of 8, 16, and 32 pixels respectively, relative to the original input image.

Given the feature mapof sizeextracted by PVT v2, wheredepends on the number of keypoints in the dataset, SimCC, which serves as the pose estimation head, first flatteninto embeddings. Then, two linear projections are performed independently for the vertical and horizontal axes to encode the coordinate information of each keypoint forkeypoints. The formula is as follows:

Here,stands for the embedding of thekeypoint,represents a fully connected layer, and,respectively represent encoded Simcc labels for the horizontal and vertical axes, whereis the scaling factor. It should be noted that Gaussian label smoothing is used, with the standard deviation set to 6.0 by default, such thatandfollow a Gaussian distribution. Subsequently, the two generated sequences,are fed into a coordinate classifier to decode the horizontal and vertical coordinate information. Specifically, the decoding process is as follows:

Here,,denotes theclassification bin on,, respectively.indicates the predicted probability of the horizontal or vertical coordinate of thekeypoint.represents the coordinate prediction ofkeypoint. Finally, combining thepairs ofkeypoints results in the predicted coordinates for all keypoints.

SECTION: High-Resolution Pyramid Module
HRPM comprises two submodules, HRPM v1 and HRPM v2. The structure of HRPM v1 is shown in Figure. Unlike PVTv2, which directly splits and flattens images into visual tokens using OPE with strides of 4 pixels in the stem-net, HRPM v1 employs progressive downsampling, i.e., using two convolutional layers each with a stride of 2 pixels, to extract representations of high-resolution images with finer granularity, thereby modeling the locality for high-resolution feature maps. HRPM utilizes hybrid-dilated convolutions (HDC)with a hierarchical structure to capture spatial context information across multiple scales within varying receptive fields and model the scale-invariance, i.e.,

where

Here,represents input image,denotes the GELUactivation function,signifies the concatenation operation,indicates the convolutional layer, which includes convolution, batch normalization, and ReLUactivation function.denotes functions learned bydilated convolution andsymbolizes the HDC structure. Specifically, in HRPM v1, the depth of HDC,, is six layers, while in HRPM v2, it is three layers. We have empirically proven their effectiveness. After the HDC structure, the hierarchical features are concatenated along the channel dimension with GELU activation. Subsequently, passing through a convolutional layer, we obtain. As the input to the first stage,needs to be reshaped into a 1D token sequence, which then enters the stacked SRA and CFFN to further encode the feature information, resulting in the first stage output. The formulas are as follows:

Here,flattens the feature map to a token sequence,represents the token sequence of first stage anddenotes the token sequence obtained afterhas passed throughPVT v2 encoder layers.andindicate theSRA and CFFN operations, respectively.

Compared to directly employing progressive downsampling in HRPMv1, as shown in Figure, HRPMv2 first upsamples the feature map from a 1/4 scale to a 1/2 scale using a deconvolutional layer. It then further extracts high-resolution representations using non-downsampling HDC structure. It should be noted that after HDC, we use an element-wise addition operation to merge the hierarchical features, allowing the network to utilize both the high-resolution information from lower layers and the high semantic information from upper layers while maintaining the transmission of multi-scale features. Finally, progressive downsampling is applied, reducing the feature map from a 1/2 scale to a 1/8 scale. Additionally, a residual branch is utilized to reuse the learned features from the previous stage. The entire process can be described as follows:

Here,denotes function learned by the deconvolutional layer.as the output feature map of HRPM v2, will serve as the input for the second stage for further processing. To this point, our vanilla HRPVT is completed. In addition to this, we have designed two variants. We will further elaborate on these in Sectionand validate their effectiveness in Section.

SECTION: Two insertion strategies
Since PVT v2 includes various models with different capacities, the performance improvement by using only vanilla HRPM is quite limited. Therefore, we designed two insertion strategies in accordance with the number and insertion position of HRPM v2, namely Layer-wise Insertion and Stage-wise Insertion. The structure of vanilla HRPVT and its two other insertion strategies are shown in Figure.

Layer-wise Insertion means inserting HRPM v2 after each PVT v2 encoder layer within the first stage only, aiming to extract richer high-resolution features compared to vanilla HRPVT without introducing excessive model complexity. The formula is as follows:

Here,andrepresents thePVT v2 encoder layer and HRPM v2, respectively.

Stage-wise Insertion involves inserting HRPM v2 after each stage. This approach seeks to incorporate the inductive bias of CNNs into various stages while guiding the network’s learning with higher-resolution representation information compared to the current stage. The formula is as follows:

Here,represents thestage.

SECTION: Experiments
SECTION: Datasets and evaluation metrics
The MS COCO datasetcomprises more than 200,000 images and 250,000 labeled person instances, each marked with 17 keypoints. Our model is trained using the MS COCO train2017 dataset, which includes 57,000 images and 150,000 person instances. We assess our model’s performance on two subsets: the val2017 set, which has 5,000 images, and the test-dev2017 set, which includes 20,000 images. In the MS COCO dataset, the evaluation metrics employed are Average Precision (AP) and Average Recall (AR). These metrics are computed based on the Object Keypoint Similarity (OKS), which measures the alignment between the ground truth and the predicted keypoints. The formula is presented below:

In this metric,represents the Euclidean distance between a detected keypoint and its ground truth counterpart. The visibility of the ground truth keypoint is indicated by, while s denotes the scale of the object, adjusting for size differences. Additionally,is a keypoint-specific constant that influences the rate of decay in the similarity measure based on the distance.

The MPII Human Pose datasetis comprised of approximately 25,000 images featuring full-body pose annotations, captured across a diverse array of real-world activities. It includes around 40,000 subjects, with 12,000 designated for testing and the remainder used for training. The evaluation metric employed in the MPII Human Pose dataset is known as the Percentage of Correct Keypoints with head-based normalization (PCKh). This metric assesses the accuracy of predicted keypoints by determining whether the distance between a predicted keypoint and its corresponding ground truth keypoint falls within a predefined threshold. The approach to data augmentation and training strategies aligns with those employed for the MS COCO dataset, with the exception that images are cropped to a uniform size of 256 × 256 pixels to ensure consistent comparisons across different methods.

SECTION: Implementation details
For the MS COCO Keypoint validation set, we first crop the input image based on the human detection box from SimpleBaseline, and resize the cropped boxes to either 256 × 192 or 384 × 288. Then, we perform data augmentation including horizontal flip, scaling (0.65, 1.35) and random rotation (-45°,+45°), while the MPII dataset uniformly resizes images to 256 × 256 for equitable comparisons with other approaches. Our models, with three different capacities, were trained and tested on two RTX2080Ti GPUs and one RTX3080Ti GPU based on the mmposecodebase. In the case of 256 × 192, the backbones are initialized with PVT v2 official pre-trained weights, except for HRPVT-L, which is initialized with the weights from mmpose. The default training setting in mmpose is utilized for training the HRPVT models, i.e., we use Adamoptimizer with a learning rate of 5e-4. After 170 epochs, the learning rate decreases by a factor of 10 during the subsequent 40 epochs and again in the final 10 epochs. The total training duration spans 210 epochs. In the case of 384 × 288, the backbones are initialized with the weights from the 256 × 192 configuration and fine-tuned for 100 epochs. It is important to highlight that the scaling factorfor our S (small), M (medium), and L (large) models is set to 4.0, 4.0, and 6.0, respectively, which empirically performed better.

SECTION: Experimental results
The results of our method, along with those of other state-of-the-art methods on the MS COCO dataset, are reported in Tableand Table.

As shown in Table, our method not only outperforms state-of-the-art 2D heatmap-based methods but also demonstrates a better trade-off between accuracy and complexity compared to SimCC-based methods. Specifically, our HRPVT-L achieves higher accuracy than HRNet-W48 while saving 60% of the parameters and 63% of the GFLOPs. Furthermore, when compared to recent state-of-the-art methods such as TransPose and TokenPose, HRPVT-L achieves comparable performance with fewer GFLOPs. In comparison to SimCC-based methods like those using HRNet-W32 as the backbone, HRPVT-L achieves a trade-off with only a 0.1 AP accuracy loss while saving 6.2M parameters and 2.4 GFLOPs. Additionally, when the resolution is increased to 384×288, HRPVT-L attains a higher accuracy of 76.3 AP, surpassing MSPose-L with a similar model capacity.

On the other hand, for the smaller model HRPVT-S, at a resolution of 256×192, it achieves 2.6 AP and 0.8 AP higher than MSPose-T and LMFormer-L, respectively. Although HRPVT-S does not outperform the state-of-the-art HRFormer-T at 256×192, it leads by 0.9 AP at 384×288, demonstrating its stronger discriminative ability with higher resolution inputs.
Notably, our method leads all models in Tablein the APmetric, demonstrating HRPVT’s superiority in medium and small-scale human pose estimation.

Tablepresents a comparison of our method with other state-of-the-art methods. From the results, we can see that HRPVT continues to exhibit excellent performance on the more challenging test-dev set. Specifically, HRPVT-L achieves the same accuracy as HRNet-W48, but with only 38% of its computational cost (GFLOPs). Additionally, although HRPVT-S has a 0.2 AP lower accuracy compared to the SimCC-based SimpleBaseline method, it achieves a 0.6 APimprovement.

To evaluate the generalization capability of our method, we also conducted experiments on the MPII dataset, where all models were trained from scratch. As shown in Table, despite having 0.8M more parameters, HRPVT-S achieves the same performance as LMFormer-L while reducing GFLOPS by 21%. Although HRPVT-L does not surpass other state-of-the-art methods in terms of the overall PCKh@0.5 metric, it demonstrates better performance in detecting more challenging keypoints such as shoulders and hips.

SECTION: Ablation study
To verify the effectiveness of our HRPM, we conducted extensive experiments on the MS COCO Keypoint validation set, with a standardized input size of 256 × 192.

We fine-tune HRPM v1 and HRPM v2 separately with a scaling factorof 2.0 to evaluate their independent contributions to our vanilla HRPVT. As shown in Table, there was no significant increase in accuracy with increasing depth until we set the depths of the two sub-modules to 6 and 3, respectively, resulting in a notable improvement in accuracy to 74.86 AP. However, further increases in depth led to a decrease in accuracy. We believe that incorporating multi-scale information appropriately during the high-resolution stage aids in network learning, while an excessive number of low-level features extracted from early stages are less beneficial. It’s worth noting that the depth of HRPM v2 is set to half of HRPM v1 to maintain scale consistency and to prevent a significant increase in computational overhead without a corresponding improvement in accuracy. Additionally, we conducted a study on the width of HDC. As shown in Table, the best accuracy was achieved when HRPM v1 and HRPM v2 were configured with 16 and 32 channels, respectively, they achieve relatively higher accuracy, whereas in other cases, there is a varying degree of decline. This indicates that the width of HDC is not necessarily better when larger. Finally, the combination of HRPM v1 and HRPM v2 resulted in the highest accuracy of 74.86 AP, underscoring the complementary nature of these two modules.

We conducted experiments on the proposed two insertion strategies. To further validate the effectiveness of HRPM, we designed two corresponding variants based on these strategies. Both variants adopt the same design principle: we removed HRPM v1 and replaced all deconvolutional layers and strided convolutional layers in HRPM v2 with point-wise convolutional layersto ensure consistency in the number of channels. All models were trained from scratch using the same training strategy.

As shown in Figure, significant improvement (+2.9 AP) was observed when applying the Stage-wise Insertion strategy to the baseline PVT v2-B0 of HRPVT-S. Although the corresponding variant also performed well, it lacked HRPM and thus had a 0.5 AP gap compared to the original model. Meanwhile, we found that as the model capacity increased, the performance gains from the Stage-wise Insertion strategy diminished. This indicates that the strategy effectively utilizes higher-resolution representation information compared to the current stage to guide the learning of small networks. However, for a model with the capacity like HRPVT-L, richer high-resolution features are more necessary to improve the localization accuracy of medium and small-scale human keypoints, thereby enhancing overall performance. Consequently, a 0.3 AP improvement was observed with the Layer-wise Insertion strategy, while its variant only achieved a 0.1 AP improvement.

SECTION: Limitation and discussion
The experimental results demonstrate the superiority of our proposed method in addressing medium and small-scale human pose estimation. However, when we further increased the model capacity, there was no significant performance improvement. We believe that our model is more adept at handling scenarios with limited computational resources and wide fields of view, such as outdoor sports capture. As shown in Figure, our method performs better in these scenarios.

SECTION: Conclusion
In this paper, we propose HRPVT, a novel hybrid Vision Transformer architecture that uses a combination of PVT v2 and SimCC as its baseline. Building on this foundation, we designed the HRPM, which integrates the intrinsic inductive biases of CNNs into high-resolution feature maps to address the significant challenge of human pose estimation at medium and small scales. To accommodate models with varying parameter scales, we developed two distinct insertion strategies for HRPM, each tailored to enhance the model’s ability to perceive medium and small-scale human poses from different perspectives. We conducted experiments on the MS COCO and MPII datasets, and the results demonstrated the effectiveness of HRPM and the two insertion strategies, showcasing the superiority of HRPVT in medium and small-scale human pose estimation. Future work involves extending HRPM to other hierarchical Vision Transformer architectures and enhancing its performance in models with larger capacity.

SECTION: References