SECTION: Brain-Inspired Stepwise Patch Merging for Vision Transformers
The hierarchical architecture has become a mainstream design paradigm for Vision Transformers (ViTs), with Patch Merging serving as the pivotal component that transforms a columnar architecture into a hierarchical one.
Drawing inspiration from the brain’s ability to integrate global and local information for comprehensive visual understanding, we propose a novel technique called Stepwise Patch Merging (SPM), which enhances the subsequent attention mechanism’s ability to ’see’ better.
SPM comprises two critical modules: Multi-Scale Aggregation (MSA) and Guided Local Enhancement (GLE).
The MSA module integrates multi-scale features to enrich feature representation, while the GLE module focuses on refining local detail extraction, thus achieving an optimal balance between long-range dependency modeling and local feature enhancement.
Extensive experiments conducted on benchmark datasets, including ImageNet-1K, COCO, and ADE20K, demonstrate that SPM significantly improves the performance of various models, particularly in dense prediction tasks such as object detection and semantic segmentation.
These results underscore the efficacy of SPM in enhancing model accuracy and robustness across a wide range of computer vision tasks.

SECTION: Introduction
Transformers have demonstrated remarkable advancements in natural language processing (NLP), and their application has recently extended significantly into the computer vision (CV) domain.
To enhance their adaptability to downstream tasks, hierarchical vision transformers (HVTs)have been developed.
These architectures draw inspiration from the pyramid structure utilized in convolutional neural networks (CNNs).
In HVTs, transformer blocks are segmented into multiple stages, resulting in a progressive reduction of feature map sizes and an increase in the number of channels as the network depth increases.

HVTs commonly utilize either standard convolutional layers or linear projection layers to amalgamate adjacent tokens, with the objective of generating hierarchical feature maps.
Nevertheless, fixed-grid methods can limit the representational capacity of vision transformers in modeling geometric transformations, as not every pixel contributes equally to an output unit.
To overcome this limitation, adaptive methods have been proposed to derive more informative downsampled tokens for subsequent processing.
For example, LIT, drawing inspiration from deformable convolutions, learns a grid of offsets to adaptively adjust spatial sampling locations for merging neighboring patches from a sub-window in a feature map.
Similarly, TCformeremploys a variant of the k-nearest neighbor-based density peaks clustering algorithm (DPC-KNN)to aggregate redundant patches, generating more patches on the target object to capture additional information.
HAFAintegrates the methodologies of LIT and TCformer, predicting offsets to adjust the sampling center of patches in the shallow layers, while employing clustering in the deeper layers to group patches with similar semantics in the feature space.
However, these methods face several common challenges.
They often exhibit limited capacity for modeling long-distance relationships and suffer from a loss of spatial information due to the clustering process.
Additionally, the clustering algorithms used are typically not amenable to end-to-end training, leading to inefficiencies.
The integration of multiple modules, as seen in HAFA, further complicates their generalizability across different applications.

In the brain, the primary visual cortex (V1), integral to initial visual processing, houses neurons with relatively small receptive fields that are crucial for detecting fine, localized visual features such as edges and orientations.
As visual information propagates to higher cortical areas like V2, V3, and V4, neurons with increasingly larger receptive fields integrate these initial perceptions, facilitating the recognition of more complex patterns and broader contextual elements.
Additionally, the visual cortex benefits from a dynamic feedback system where higher-order areas like the inferotemporal cortex (IT) provide contextual modulation to lower areas.
This top-down modulation is essential for refining the perception of local features within their broader environmental matrix, enhancing both the accuracy and relevance of visual processing.

Inspired by the nuanced neurobiological mechanisms of the human visual cortex, particularly the orchestrated activities across various cortical areas, we introduce Stepwise Patch Merging (SPM), a novel approach designed to enhance the receptive field while preserving local details.
SPM framework consists of two sequential stages: Multi-Scale Aggregation (MSA) and Guided Local Enhancement (GLE).
In the MSA stage, spatial dimensions are preserved while channel dimensions are increased to a designated size.
This process aggregates multi-scale information, enriching the semantic content to accommodate the increased capacity of the feature map.
Subsequently, the GLE stage reduces the spatial dimensions of the feature map while maintaining the channel dimensions.
Given that the input to GLE already contains rich semantic information, this stage emphasizes local information, optimizing it for downstream dense prediction tasks such as object detection and semantic segmentation.
The distinct focus and reasonable division of labor between the MSA and GLE modules ensure that the SPM architecture serves as a flexible, drop-in replacement for existing hierarchical vision transformers.

In summary, our contributions are as follows:

We propose an innovative technique termed Stepwise Patch Merging (SPM), which serves as a plug-in replacement within hierarchical vision transformer architectures, leading to substantial performance enhancements.

The SPM framework comprises two distinct modules: Multi-Scale Aggregation (MSA) and Guided Local Enhancement (GLE). MSA enriches feature representation by integrating multi-scale information, while GLE enhances the extraction of local details, achieving an optimal balance between long-range dependency modeling and local feature refinement.

Extensive experiments conducted on benchmark datasets, including ImageNet-1K, COCO, and ADE20K, demonstrate that SPM significantly boosts the performance of various models, particularly in downstream dense prediction tasks such as object detection and semantic segmentation.

SECTION: Related Work
SECTION: Vision Transformer
The Vision Transformer (ViT)revolutionized visual tasks by introducing the transformer architecture to computer vision.
ViT segments images into non-overlapping patches, projects these patches linearly into token sequences, and processes them using a transformer encoder.
ViT models have demonstrated superior performance in image classification and other downstream tasks, surpassing CNNswhen trained with large-scale pretraining datasets and advanced training methodologies.
Motivated by the success of CNNs and the necessity to address dense prediction tasks, researchers have incorporated the feature pyramid structure within transformers.
This innovation has led to the development and widespread adoption of HVTs.

SECTION: Hierarchical Feature Representation
Hierarchical feature representation plays a pivotal role in dense prediction tasks, prompting extensive research in this domain.
Existing approaches can be broadly categorized into fixed-grid and dynamic feature-based methods.
Fixed-grid methods, exemplified by works such as PVTand Swin, merge patches within adjacent windows using 2D convolution.
In contrast, dynamic methods, such as DynamicViTadaptively extract features by eliminating redundant patches and retaining essential ones, thereby forming hierarchical feature maps.
EviTenhances this approach by selecting the top K tokens with the highest average values across all heads for the next stage, merging the remaining tokens.
PS-ViTfurther refines the process by iteratively adjusting patch centers towards the object to enrich object information within the hierarchical feature maps.
Token Mergingemploys cosine similarity to progressively merge similar tokens, thereby increasing model throughput.

Fixed-grid methods are constrained by their singular and relatively small receptive fields, and excessively enlarging the grid size leads to increased computational overhead.
Dynamic feature-based methods, while adaptive, may discard low-scoring tokens that contain valuable information and often lack end-to-end training capabilities.
Our proposed Stepwise Patch Merging approach distinguishes itself from both fixed-grid and dynamic feature-based methods.
The Multi-Scale Aggregation module in Stepwise Patch Merging provides an expanded and enriched receptive field, which is advantageous for long-distance modeling.
Additionally, the Guided Local Enhancement module enhances the extraction of local discriminative features and supports end-to-end training.
Moreover, Stepwise Patch Merging can be directly applied to dense prediction tasks, resulting in improved performance.

SECTION: Methodology
Inspired by the brain’s ability to integrate global and local information when processing visual scenes, we propose the Stepwise Patch Merging framework, as illustrated in Fig..
The framework comprises two primary components: Multi-Scale Aggregation (MSA) and Guided Local Enhancement (GLE), designed to address variations in feature map dimensions.
The MSA module enhances feature diversity by increasing the number of channels and capturing long-range dependencies, akin to how the brain processes information at multiple scales to form a coherent perception.
In contrast, the GLE module optimizes local feature extraction by introducing context-aware guide tokens within local windows, thereby refining and enhancing feature details.
This synergistic design effectively combines the strengths of both global structure processing and local detail enhancement, making it particularly beneficial for downstream dense prediction tasks.

SECTION: Multi-Scale Aggregation
Our proposed Multi-Scale Aggregation (MSA) module draws inspiration from the brain’s remarkable ability to effectively model long-range dependencies when processing visual information.
In the brain, the visual system achieves precise modeling of long-range dependencies through multi-level and multi-scale information processing.
Neurons with small receptive fields process local features, and this information is progressively integrated over larger areas by neurons with larger receptive fields, capturing complex patterns and objects.
Additionally, the brain’s extensive network of long-range neural connections allows for the exchange and integration of data from various parts of the visual field, facilitating a comprehensive understanding of the scene.
Furthermore, neurons within the same level possess receptive fields of varying sizes, enabling the brain to simultaneously process local details and global features.
This sophisticated mechanism of combining local and global information processing in the brain inspired the design of our MSA module, which aims to enhance feature diversity and capture long-range dependencies effectively.

Inspired by these mechanisms, the MSA module first divides the input channelsintodistinct heads, each undergoing depth-wise convolutions with varying receptive fields.
This method not only reduces the parameter count and computational cost but also facilitates the extraction of multi-granularity information, akin to how different neurons in the brain handle information processing.
Subsequently, the MSA module employs larger convolutional kernels to further expand the receptive field, thereby enhancing its capability to model long-range dependencies.
Following this, Channel Shuffleis used to interleave channels containing features of different scales, followed by a series of linear projections to fuse these multi-scale features.
The number of linear projections is, with each projection having unique parameters.
Finally, theheads are concatenated, and a final linear projection adjusts the number of channels to the specified.

By leveraging the brain’s mechanism for effective long-range dependency modeling, the MSA module better captures and integrates key features, significantly enhancing the model’s performance in complex visual recognition tasks.

Our proposed MSA can be formulated as follows:

whererepresents the inputsplit into multiple heads along the channel dimension, anddenotes the-th head. The kernel size of the depth-wise convolution for the-th head is denoted by. Here,represents the-th head after being processed by the depth-wise convolution with, andrepresents the-th channel in the-th head.is the weight matrix of the linear projection.. Finally,is the weight matrix of the linear projection that adjusts the number of channels to the specified.

SECTION: Guided Local Enhancement
Inspired by the brain’s ability to enhance local features through context-aware processing, we developed the Guided Local Enhancement (GLE) module.
In the brain, local feature enhancement is achieved by integrating information from both local and global contexts.
Higher-level cortical areas provide contextual feedback that refines the processing of local features, ensuring that details are interpreted within the broader visual context.
This hierarchical processing involves neurons that respond specifically to local stimuli but are influenced by surrounding contextual information, allowing for more nuanced and precise feature extraction.

Following this principle, the GLE module acts as a local feature enhancer utilizing context-aware guide tokens, as illustrated in Fig..
Specifically, we implement self-attention within a local window and introduce atoken into the input space.
Thistoken undergoes the same self-attention operations as the patch tokens, thereby providing high-level semantic features to the local window during pooling.
By mimicking the brain’s method of using contextual information to refine local feature extraction, the GLE module ensures that the extracted local features are both precise and contextually relevant.

Formally, given an input, thetokens are generated by a large-kernel depth-wise convolution, referred to as the Guide Token Generator (GTG), which can be described as follows:

We focus on the operations performed on a single pixel within the input feature map.
We define a set of pixels within a local window centered at pixelas.
For a fixed window size of,.
In our setup,is equal to the stride of the GTG, both being 2, meaning.
Tokens within a window containing atoken can be represented by the sequence:

then perform the standard self-attention operation on:

where, and we ignore the relative positional relationships between tokens within a window, so positional encoding is not used.
Finally, we select thetoken as the output of the GLE:

It is worth noting that when thetoken is stripped of its semantic information, it degrades into thetoken in the vanilla vision transformer. Experiments show that using ourtokens results in higher performance (see Tab.).

SECTION: Experiments
SECTION: Image Classification on ImageNet-1K
We first evaluate the proposed SPM framework on the ImageNet-1K dataset, which comprises 1.28 million training images and 50,000 validation images spanning 1,000 categories.
To ensure a fair comparison, all models are trained on the training set and report the top-1 error rate on the validation set.
For data augmentation, we apply a suite of techniques including random cropping, random horizontal flipping, label smoothing regularization, mixup, CutMix, and random erasing.
These augmentations are employed to enhance the robustness and generalization ability of the models.
During training, we use the AdamW optimizerwith a momentum parameter of 0.9, a mini-batch size of 128, and a weight decay of.
The initial learning rate is set toand follows a cosine annealing scheduleto gradually reduce the learning rate.
All models are trained from scratch for 300 epochs on eight NVIDIA A100 GPUs.
For evaluation, we adopt the standard center crop strategy on the validation set, where apatch is extracted from each image to assess the classification accuracy.

In Tab., we observe that incorporating the SPM framework into the PVT results in significant improvements in classification accuracy, specifically by 4.4%, 1.9%, and 0.7% in the Tiny, Small, and Medium models, respectively, while adding only a minimal number of parameters compared to the original PVT.
The final experimental results indicate that the accuracy of models of various sizes has been enhanced, with the most notable improvement observed in the Tiny model.
Remarkably, the combination of PVT-Tiny and SPM achieved a top-1 accuracy of 79.5%, which is comparable to the performance of PVT-Small, despite PVT-Small having nearly 70% more parameters than PVT-Tiny.
Furthermore, with the integration of SPM, PVT-Small surpassed PVT-Medium by 0.5%.

SECTION: Object Detection on COCO
Object detection and instance segmentation experiments were conducted on the challenging COCO benchmark.
All models were trained on the training set comprising 118k images and evaluated on the validation set with 5k images.
We validated the effectiveness of different backbones using Mask R-CNN.
Before training, the weights pre-trained on ImageNet-1K were used to initialize the backbone, and the newly added layers were initialized using the Xavier initialization method.
Our models were trained with a batch size of 16 on 8 NVIDIA A100 GPUs and optimized using the AdamW optimizerwith an initial learning rate of.

As shown in Tab., incorporating the SPM framework into the PVT resulted in significant improvements of 4.1%, 2.6%, and 1.3% in the Tiny, Small, and Medium models, respectively, for the object detection task.
Notably, the SPM framework also demonstrated substantial improvements in the instance segmentation task.
Several observations can be made by analyzing the detection results of models with different sizes.
Models integrated with SPM show marked improvements in detecting medium-sized and large objects.
This enhancement is attributed to the original patch merging’s relatively singular and small receptive fields, whereas the MSA module integrates features with diverse receptive fields, enabling the model to more accurately capture long-range dependencies.
Moreover, there is a significant improvement in detecting small objects.
Although larger models are typically better at modeling global relationships, the disruption of local information may hinder small objects from establishing complete semantic information, leading to missed detections.
The GLE module addresses this by enhancing the perception of local discriminative information, resulting in consistent improvements in the detection performance of small objects across models of different sizes.

SECTION: Semantic Segmentation on ADE20K
The ADE20K datasetis a widely utilized benchmark for semantic segmentation, comprising 150 categories with 20,210 images for training, 2,000 images for validation, and 3,352 images for testing.
All the methods compared were evaluated using the Semantic FPN framework.
The backbone network of our method was initialized with the pre-trained ImageNet-1k model, and the newly added layers were initialized using the Xavier initialization method.
The initial learning rate was set to 0.0001, and the model was optimized using the AdamW optimizer.
We trained our models for 40,000 iterations with a batch size of 16 on eight NVIDIA A100 GPUs.
The learning rate followed a polynomial decay schedule with a power of 0.9.
During training, images were randomly resized and cropped topixels.
For testing, images were rescaled to have a shorter side of 512 pixels.

As shown in Tab., the integration of the SPM framework led to a significant enhancement in the semantic segmentation task.
Specifically, the performance of the Tiny, Small, and Large models improved by 5.8%, 6.1%, and 3.7%, respectively.
It is evident that the improvement achieved by SPM in segmentation tasks surpasses that in classification and detection tasks.
Interestingly, in both classification and detection tasks, the relative improvement brought by SPM compared to the base model gradually decreases as the model size increases.
However, on the ADE20K dataset, the performance gain of PVT-Small exceeds that of PVT-Tiny, with improvements of 6.1% and 5.8%, respectively.
This phenomenon can be attributed to the GLE module within SPM, which is specifically designed to capture local information.
Consequently, SPM demonstrates a significant advantage in semantic segmentation tasks, where detailed local feature extraction is crucial.

SECTION: Effectiveness on other Backbones
To further validate the generalizability of the proposed SPM framework, we integrated SPM into various mainstream Transformer backbones and trained them on the ImageNet-1K dataset.
We employed consistent training settings to ensure a fair comparison, and the top-1 accuracies are presented in Tab..
The results demonstrate that the performance improvements conferred by SPM are universal across different backbones, indicating its robust generalization capability.
Specifically, our method significantly enhanced the performance of Swin-T by 1.1%, Shunted-T by 0.8%, and NAT-Mini by 0.4% on ImageNet-1K.
These results underscore the effectiveness of SPM in boosting the performance of various Transformer architectures, highlighting its potential as a versatile enhancement technique in the field of computer vision.

SECTION: Visualization
We compared the visualization results of the attention maps with and without using the SPM framework, as shown in Fig..
Specifically, we replaced the original patch merging block of PVT-Tiny with our SPM and visualized the first block of the second stage for two separate heads.
For example, after employing SPM, the bird’s two wings and tail were successfully linked, whereas the vanilla PVT-Tiny failed to capture the distant tail.
This demonstrates that SPM facilitates the network’s ability to establish long-range relationships at shallower layers, leading to significant improvements in classification performance.

Additionally, we employed the Effective Receptive Field (ERF) methodas a visualization tool to compare the changes in ERF before and after using SPM, as depicted in Fig..
It is readily observed that after integrating SPM, the size of the ERF not only increases significantly but also changes shape from a regular square to a radial decay pattern, which aligns more closely with biological vision.
This pattern can be attributed to our carefully designed MSA and GLE modules, which together achieve an excellent balance between capturing long-range relationships and preserving local detail features.
Consequently, this balance leads to improved performance in classification and downstream dense prediction tasks.

SECTION: Ablation Study
SECTION: The Effectiveness of GLE
To evaluate the effectiveness of the proposed GLE module, we conducted experiments by replacing GLE with two alternative methods, as shown in Tab.).
Replacing GLE with aconvolution or aaverage pooling layer results in a performance decrease of 2.3% and 2.4%, respectively.

SECTION: The Effectiveness of Guide Token
We conducted comparative experiments to evaluate the effectiveness of the proposedtoken against two mainstream methods: thetokenand global average pooling (GAP), as presented in Tab..
The results demonstrate that thetoken improves model performance by approximately 1.7% compared to these methods, without significantly increasing the number of parameters.

Furthermore, an important observation from Tab.and Tab.is that when the local window size of self-attention and the kernel size of convolution are both set to, the self-attention method achieves higher accuracy than the convolution method.
This suggests that the self-attention mechanism has a superior capability in extracting high-frequency features.

SECTION: Selection of GTG’s Kernel Size
To determine the optimal kernel size for GTG, we conducted a series of performance comparison experiments with different kernel sizes.
Ultimately, we set the kernel size of GTG to 7, which achieved the best performance without significantly increasing the number of parameters.
This indicates that information outside the local window positively influences the attention results within the window, highlighting the effectiveness of thetoken.

SECTION: Gradually Applying SPM
Different stages represent varying levels of semantic information.
To validate the generalizability of the SPM framework across different levels of semantic information, we conducted experiments by gradually replacing the original patch merging with SPM (see Tab.).
From the second and third rows, we observe that SPM enhances network performance by 1.3% with low-level features (stage 1) and by 1.6% with high-level features (stage 3), respectively.
Furthermore, from the second, fourth, and fifth rows, it is evident that progressively replacing the original patch merging with SPM linearly improves performance by 1.3%, 3.0%, and 4.4%, respectively.

SECTION: Conclusion
In this work, we introduced the Stepwise Patch Merging framework, inspired by the brain’s ability to integrate global and local information for comprehensive visual understanding.
The proposed SPM framework, comprising Multi-Scale Aggregation and Guided Local Enhancement modules, demonstrates significant improvements in various computer vision tasks, including classification, detection, and segmentation.
Through extensive experiments on ImageNet-1K and COCO benchmarks, as well as on ADE20K for semantic segmentation, we showed that SPM consistently enhances the performance of different backbone models.
The robustness of SPM to different input sizes and its effective generalization to other Transformer architectures further underscore its versatility and potential as a powerful enhancement technique.
Future work will explore the application of SPM in more diverse domains and its integration with other state-of-the-art models to further elevate its impact on the field of computer vision.

SECTION: References