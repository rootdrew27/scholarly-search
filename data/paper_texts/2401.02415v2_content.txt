SECTION: LLaMA Pro: Progressive LLaMA with Block Expansion

Humans generally acquire new skills without compromising the old; however, the opposite holds for Large Language Models (LLMs),e.g., from LLaMA to CodeLLaMA.
To this end, we propose a new post-pretraining method for LLMs with an expansion of Transformer blocks.
We tune the expanded blocks using only new corpus, efficiently and effectively improving the model’s knowledge while mitigating forgetting.
In this paper, we experiment on the corpus of code and math, yieldingLLaMA Pro-8.3B, a versatile foundation model initialized from LLaMA2-7B, excelling in general tasks, programming, and mathematics.LLaMA Proand its instruction-following counterpart (LLaMA Pro - Instruct) achieve advanced performance among various benchmarks,
demonstrating superiority over existing open models in the LLaMA family and the immense potential of reasoning and addressing diverse tasks as an intelligent agent.
Our findings provide valuable insights into integrating natural and programming languages, laying a solid foundation for developing advanced language agents that operate effectively in various environments.

LLaMA Pro: Progressive LLaMA with Block Expansion

Chengyue Wu1,2Yukang Gan2Yixiao Ge2††thanks:Correspondence toyixiaoge@tencent.com.Zeyu Lu3Jiahao Wang1Ye Feng4Ying Shan2Ping Luo11The University of Hong Kong2ARC Lab, Tencent PCG3Shanghai Jiao Tong University4Beijing Language and Culture Universityhttps://github.com/TencentARC/LLaMA-Pro

SECTION: 1Introduction

The advent of Large Language Models (LLMs) has revolutionized the field of natural language processing, exhibiting remarkable proficiency in a variety of real-world tasksOpenAI (2023); Chowdhery et al. (2023).
Despite the versatility, LLMs still fall short in certain domains, for example, programming, mathematics, biomedical, or finance.
This limitation impedes the progress of developing generic language agents for broader applications.

Existing worksLiu et al. (2023); Li et al. (2023a); Wu et al. (2023b)attempted to improve the multi-faceted capabilities of pre-trained LLMs with tailored data recipes.
While feasible, they require substantial computational resources and vast amounts of data, which poses a challenge to the democratization of LLM research.
Consequently, another line of research, known as domain-adaptive pretraining, focuses on post-pretraining with domain-specific corporaGururangan et al. (2020). These approaches have demonstrated efficacy in adapting various LLMs to specific domainsRoziere et al. (2023); Azerbayev et al. (2023); Wu et al. (2023b); Xu et al. (2023b),
resulting in enhanced performance on downstream domain-specific tasks at a reduced computational cost.

Nonetheless, a considerable obstacle emerges in catastrophic forgettingDe Lange et al. (2021). Post-pretraining often leads to a decline in the model’s original general abilities, inhibiting the fine-tuned performance of the model on diverse tasksCheng et al. (2023); Dong et al. (2023). This necessitates a method that can inject domain-specific knowledge into LLMs while preserving their general abilities, thereby enhancing their comprehensive capabilities.

Towards this end, we introduce a simple yet effective post-pretraining method, termedblock expansion.
We expand the off-the-shelf pre-trained LLM using copied Transformer blocks, as illustrated in Figure2.
The newly added blocks, whose linear layers are zero-initialized to enable identity mapping, are further tuned with only domain-specific corpus while the remaining blocks are frozen.
After tuning, the extended pre-trained model excels in both general and domain-specific tasks.

In practice, we extend the pre-trained LLaMA2-7BTouvron et al. (2023)by eight more blocks, yieldingLLaMA Pro, a foundation model with 8.3B parameters, and enhanced performance in programming, coding, and reasoning.
We pre-trainLLaMA Pro’s expanded blocks on 80B tokens using open-source code and math data for 2830 GPU Hours (16 NVIDIA H800 GPUs for about 7 days).
We further perform supervised instruction tuning (fully fine-tuning of all the blocks,akaSFT) onLLaMA Prowith approximately 80M tokens, yieldingLLaMA Pro - Instruct.
It is noted that pre-trained models produced by our block expansion method are well-compatible with the subsequent SFT techniques without specific modification.

As shown in Figure1,LLaMA Pro - Instructreaches state-of-the-art performance across a broad range of general, code (i.e., HumanEval), and math (i.e., GSM8K) tasks.
Furthermore, we assess the capabilities ofLLaMA Pro - Instructas a language agent across various scenarios (i.e., MINT-Bench), with a focus on the tool usage abilities and the capacity to ground in environmental and human feedback.
We also employ GPT-4OpenAI (2023)automatic evaluation to assessLLaMA Pro’s ability to serve as an effective assistant (i.e., MT-Bench).
Comprehensive experimental results indicate the superiority ofLLaMA Pro - Instructover other models from the LLaMA family on both benchmarks and practical applications.
Our contributions are three-fold:

We propose a novel post-pretraining method for LLMs, termed block expansion, enabling the injection of new knowledge while preserving the initial capabilities.

We introduceLLaMA ProandLLaMA Pro - Instruct, versatile LLMs that well integrate natural and programming languages, excelling in general tasks, programming, and mathematics.

We benchmark the family ofLLaMA Proon extensive datasets, including both traditional and agent-oriented tasks, demonstrating its superiority and great potential in broader complex applications.

SECTION: 2Related Work

Recent advancements in large language models have led to significant progress, with model and data scale growth driving state-of-the-art performance across various tasksHoffmann et al. (2022); Kaplan et al. (2020); Chowdhery et al. (2023). The development of generalist models has enabled addressing diverse problems and rapid adaptation to new tasksRadford et al. (2019); Brown et al. (2020). The open-source community has further contributed by releasing powerful models like LLaMATouvron et al. (2023)and CodeLLaMARoziere et al. (2023). Our work builds upon these developments, providing a method for specializing LLMs in the code domain, fostering future research and applications.

Language model applications typically involve a two-step process: general-domain pretraining followed by domain-specific trainingRoziere et al. (2023); Azerbayev et al. (2023). Fine-tuning often aims to enhance instruction-following abilitiesSanh et al. (2021); Wei et al. (2021); Wang et al. (2023d)or align model outputs with human preferencesZiegler et al. (2019); Ouyang et al. (2022); Bai et al. (2022). Some research explores parameter-efficient fine-tuning methods for adapting pretrained models to new domainsHoulsby et al. (2019); Hu et al. (2021); Wu et al. (2023a), while others focus on continual learning post-pretrainingWang et al. (2023b); Gupta et al. (2023); Scialom et al. (2022).
Parameter-efficient tuning methods like adaptor and LoRA are generally applied during the instruction tuning phase rather than the pretraining phase. In contrast, our focus is on enhancing the capacity of LLMs by increasing their depth during continued pretraining. Our work proposes an adaptation strategy that combines continued training with general capability maintenance, allowing LLMs to specialize without sacrificing overall performance.

Progressive training has gained attention for accelerating large-scale model training in computer visionZhang et al. (2023)and NLP researchYao et al. (2023); Li et al. (2023b).Gong et al. (2019)proposed a stacking method doubling model depth successively. CompoundGrowGu et al. (2020)extends stacking with Feed-Forward Network expansion in schedule design.Shen et al. (2022)introduced a staged method supporting hidden size expansion. Bert2BERTChen et al. (2021a)and LiGOWang et al. (2023a)accommodate all growth dimensions. Our method utilizes depth growth to maintain general performance while adapting to specific domains.

SECTION: 3Method

SECTION: 3.1Preliminaries: The LLaMA Block

The LLaMA block consists of a multi-head self-attention (MHSA) mechanism followed by a position-wise feed-forward network (FFN) with residual connections and a Swish-Gated Linear Unit (SwiGLU) operation as Figure3shows. Given an input, the LLaMA block produces an outputas described by the following equations:

The inputhas a dimension of, whereis the sequence length andis the hidden size. The outputhas the same dimension as the input. The MHSA operation is a crucial component of the transformer, defined as:

where,, andare the query, key, and value matrices, respectively, andis the output weight matrix without bias . Each head is computed as:

with,, andbeing the corresponding weight matrices for the-th head.

The FFN block in the LLaMA model utilizes the SwiGLU activation function, which is defined as:

wheredenotes element-wise multiplication,,, andare the weight matrices without bias,.

SECTION: 3.2Block Expansion

Given a model with blocks, the block expansion incorporates an identity blockafter each block in the original model, ensuring that the expanded model maintains the same output after expansion. The identity block is defined asso the input and output are identical.

Suppose we have an initial model withblocks that needs to be expanded toblocks. First, we partition the originalblocks intogroups, with each group containingblocks. For each group, we create identity copies of the topblocks and stack them on top of each group, as depicted in Figure3. We arrange these blocks in an interleaved manner to maintain the structural characteristic of the transformer model, whose prior is that deeper blocks encode more complex informationVan Aken et al. (2019); Tenney et al. (2019). This process leads to an increased depth in the model while maintaining its output behavior.

Shen et al.Shen et al. (2022)proposed the initialization of scale parameters in the Norm modules within the identity blocks to zero for the construction of the identity block. However, this approach may not be effective when applied to the LLaMA block. The reason lies in the fact that the gradient of the loss functionwith respect to the RMSNorm weightduring backpropagation would be zero. This would prevent the training of RMSNorm, implying that when, the following condition will hold:

This equation signifies that the gradient of the loss function with respect to the weight of RMSNorm is zero, which would hinder the training of the RMSNorm module. This is further explained in AppendixA. Referring to the LLaMA block formulation in Equation1, the identity can be achieved as long asand. We initialize theandweight matrices in the identity blocks to zero. Due to the presence of residual connections and the absence of bias terms in the LLaMA block, only the residual flows through the identity block. As a result, the entire block is reduced to an identity block at initialization, preserving the output from the initial model.

The entire training pipeline is depicted in Figure2. Our method concentrates on the post-pretraining stage, targeting specific domain corpora. We begin by initializing our model with large language models trained on extensive unlabeled general corpora, where all blocks will be fine-tuned. To enhance the model’s capacity for accommodating additional domain knowledge while retaining its general knowledge, we employ block expansion to increase the number of blocks in the LLM. During this process, we only fine-tune the newly added blocks while freezing the original blocks, thereby preserving the general abilities of the model.

SECTION: 4Experiments

This section presents our key experimental findings. We begin with experimental settings (described in Sec.4.1), and then verify the effectiveness of block expanded tuning after pretraining (described in Sec.4.2). Next, we give the supervised finetuning (SFT) results (described in Sec.3). Finally, ablation studies of the key design choices are presented (described in Sec.4.5).

SECTION: 4.1Experimental Settings

We construct a dataset that concentrates on code and math. For the code component, we rely on the Stack-dedup dataset, which is a compilation of permissively licensed source codes from GitHub.
Among all the programming languages available in Stack-dedup, we specifically utilize the Python split.
As for the math component, we opt for the Proof-pile-2 datasetAzerbayev et al. (2023), a 55-billion-token amalgamation of scientific papers, web data containing mathematical content, and mathematical code. The details can be found in Appendix3.

We initialize our base model with LLaMA2-7B and expand the number of blocks from 32 to 40 using an interleaved approach. In the block expansion process, we configure the parameters as,, and, resulting in 8 groups where each group expands from 4 blocks to 5 blocks. For the code and math corpus pretraining, we employ a batch size of 1024, a sequence length of 4096, a warmup ratio of 6%, a learning rate of 2e-4, and a Cosine learning rate scheduler. We also use bf16 mixed precision, a weight decay of 0.1, and gradient clipping at 1.0. To speed up the training process, we apply the flash-attention mechanism.

Our experiment is conducted on 16 NVIDIA H800 GPUs.LLaMA Prois trained for a total of 15,900 steps. This training process corresponds to approximately 2830 H800 GPU hours.

We want to highlight that our approach does not incur higher training costs, and it is worth the extra resources to achieve a better performance of the domain specific tasks in the inference.

Training stage cost:Our approach requires fewer computational resources since only the newly added blocks are tuned during training. As illustrated in Figure4, LLaMA Pro-8B (1B parameters tuned for 80B tokens) incurs less training overhead compared to CodeLLaMA-7B (7B parameters tuned for 500B tokens). It also uses fewer resources than training domain-specific models from scratch, such as StarCoder and CrystalCoder. Despite this, our method achieves a better balance of general and domain-specific performance, offering a more cost-effective solution.

Inference stage cost:Although our method requires more resources during inference than the initial LLM, it strikes a balance between performance and efficiency. LLaMA Pro-8B outperforms larger models like LLaMA2-13B and LLaMA2-34B in the code domain while demanding significantly fewer resources during training and inference.

During the instruction fine-tuning phase, we combine five data sources to createLLaMA Pro - Instructas shown in Table7. The final sft dataset consists of approximately 1M samples. To fine-tune the basic models, we employ specific configurations, including a batch size of 128, a sequence length of 4096, 0.03 warmup ratio, a learning rate of 2e-5, a Cosine learning rate scheduler, and bf16 mixed precision.

We conduct a comparative analysis ofLLaMA Prowith the latest state-of-the-art (SOTA) Large Language Models (LLMs). The evaluation is performed on six key general benchmarks using the Eleuther AI Language Model Evaluation Harness111https://github.com/EleutherAI/lm-evaluation-harness, a unified framework designed to test generative language models across a vast array of evaluation tasks. For code-related tasks, we employ the BigCode Evaluation Harness222https://github.com/bigcode-project/bigcode-evaluation-harnessto evaluate HumanEval and MBPP, and we report the pass@1 rate of code tasks with greedy decoding. The evaluation details can be found in AppendixD.

SECTION: 4.2Pretrain Results

We evaluateLLaMA Pro’s performance with benchmark datasets from the Open LLM Leaderboard. Furthermore, we incorporate coding benchmark datasets, including HumanEval pass@1 and MBPP pass@1, as well as the math benchmark GSM8K, to provide a comprehensive evaluation. We compare the performance ofLLaMA Prowith a selection of state-of-the-art pretrained models that were trained around the same period with similar size. This includes general-purpose pretrained models like LLaMA2 and code-oriented pretrained models like CodeLLaMA. The results are presented in Table1.

The results highlight thatLLaMA Proeffectively balances natural language processing and coding capabilities. It not only preserves the general performance of its base model, LLaMA2-7B, but also surpasses it in the average performance of general language tasks. Conversely, CodeLLaMA-7B sacrifices general performance. We attribute this improvement to our expansion design, which freezes the initial LLaMA blocks to maintain their capabilities and increases the blocks to accommodate domain-specific knowledge.

As depicted in Figure4,LLaMA Proshows robust general performance alongside code performance that is on par with code-oriented LLMs. Situated on the Pareto frontier,LLaMA Prohas undergone fine-tuning with an additional 80B tokens in conjunction with LLaMA2, which more than doubles the code tasks average performance. In contrast, CodeLLaMA is fine-tuned with 500B tokens.LLaMA Proexcels in general performance while maintaining code performance that is competitive with code-oriented LLMs, whether they are trained from scratch, such as StarCoder-15B and CrystalCoder, or fine-tuned like CodeLLaMA-7B.

SECTION: 4.3SFT Results

Modern LLMs typically undergo supervised fine-tuning or instruction tuning after pretraining on vast amounts of unlabeled data. In this section, we aim to demonstrate that our expansion strategy can adapt to this widely used training pipeline, just as traditional LLMs do.

Table1presents a comparison of evaluation results among several prominent supervised fine-tuning (SFT) LLMs from the LLaMA community, across general tasks, math tasks, and code tasks benchmarks. As a singular SFT model,LLaMA Pro - Instructattains state-of-the-art performance, even when compared to specifically tuned models such as WizardCoder and WizardMath. This demonstrates its more comprehensive capabilities.

As seen in Figure1,LLaMA Pro - Instructboosts both code and math tasks to SOTA performances while maintaining reliable general performance. We enhance the average performance of LLaMA2-7B-chat and CodeLLaMA-7B-instruct by 13.81% and 14.50% respectively, which highlights the benefits of balancing textual and coding abilities.

To assess the comprehensive conversational performance of theLLaMA Pro - Instructassistant, we evaluate it using the MT-Bench with GPT-4 automatic scoring, as proposed by VicunaZheng et al. (2023). As depicted in Table2,LLaMA Pro - Instructsurpasses widely used chatbots from the LLaMA community. This indicates its potential as a chatbot capable of providing helpful responses, in addition to its impressive performance in traditional benchmarks. The details of MT-Bench can be found in the AppendixF.

We use MINT-BenchWang et al. (2023c)to evaluate our model’s ability to solve multi-turn interactions by using tools. MINT-Bench tests LLMs’ ability to use tools by generating and executing Python code, focusing on tool-augmented task-solving and leveraging natural language feedback. MINT includes eight datasets covering reasoning, code generation, and decision-making. The details of MINT can be found in the AppendixE. The results are shown in TableLABEL:tab:mint.LLaMA Pro - Instructachieves SOTA performance compared to similar size models in multi-turn interactions with the use of tools.

SECTION: 4.4Mistral-Pro Results

We experimented with block expansion on Mistral-7BJiang et al. (2023), training it on code and mathematics datasets. The resulting pretrained performance is detailed in Table4, highlighting superior outcomes across various benchmarks, particularly in the domains of code and math. Notably, it demonstrates competitive results compared to the new open-source model GemmaTeam et al. (2024), while incurring significantly lower training overhead. We further utilized the MetaMath datasetYu et al. (2023)for supervised fine-tuning. Our approach yielded scores of 78.4 for GSM8k and 30.3 for MATH, surpassing Mistral’s scores of 77.7 and 28.2, respectively. Additional details are provided in AppendixC.

SECTION: 4.5Ablation Study

Apart from the aspect of code corpus, we explore our method on another domain: law, with the freelaw subset of Pile dataset as our pretrain corpusGao et al. (2020). We evaluate on UNFAIR-ToSLippi et al. (2019)of the LexGLUE benchmarkChalkidis et al. (2021).

In our experiment, we assess the scalability of our block expansion method in terms of training loss and downstream task performance as we increase the number of added blocks. We also compare our method with the Mixture-of-Expert (MoE) expansion methodFedus et al. (2022)and traditional training strategies, such as fine-tuning and LoRAHu et al. (2021). The details can be found in Appendix12.

We analyze the training loss with varying added blocks (Figure5). The loss consistently decreases during training, regardless of the number of added blocks, and decreases more rapidly with larger models. These findings indicate that our method demonstrates strong scalability with larger models and more data.

However, a lower overall training loss does not necessarily guarantee superior performance on domain-specific tasks. Therefore, we evaluate models of different sizes on both general language tasks and Unfair-ToS, as shown in Table5. All the expanded models effectively preserve the general capabilities of the initial model. For the domain-specific task, larger models achieve better performance. We find that adding eight blocks provides optimal performance with minimal cost compared to larger models, hence we adopt this as our default strategy. The performance of MoE is comparable to our method with four added blocks. Figure9illustrates the differences between traditional training strategies such as fine-tuning and LoRA, and our proposed method. We observe that while LoRA effectively preserves the general ability, it struggles to model the distribution of a new domain, as also evidenced by the training loss depicted in Figure5. In contrast, full fine-tuning results in a more significant drop in general performance. Here we use a rank of 1024 for LoRA, resulting in a number of trainable parameters comparable to our method.

In line with the approach ofLin et al. (2023), we analyze the token distribution between the original LLaMA andLLaMA Proto assess the similarity in their behavior when answering general questions from the Alpaca datasetTaori et al. (2023). As depicted in Figure6, the token distribution shift between LLaMA andLLaMA Prois subtle. Detailed information can be found in AppendixG.

We also analyze the impact of the position where the identity blocks are added, either at the bottom or the top of the model, compared to adding them interleaved, as shown in Table5. We observe that adding blocks at the bottom results in poor evaluation performance, likely because it disrupts the model’s foundation, causing errors to propagate throughout the model. Adding blocks at the top of the modelGong et al. (2019)preserves the initial model’s performance, but its performance on domain-specific tasks is lower than when adding blocks interleaved.

As highlighted in the LIMA studyZhou et al. (2023), the majority of knowledge in large language models is acquired during pretraining, with only a limited amount of instruction tuning data required to generate high-quality output. To investigate the extent of knowledge encoded during pretraining, we conducted a comparative analysis between LLaMA2-7B andLLaMA Prousing the same instruction dataset, as illustrated in Figure7. Our results showed thatLLaMA Proconsistently outperforms LLaMA2-7B across all tasks, indicating that our method effectively enablesLLaMA Proto encode more domain-specific knowledge during the pretraining phase.

SECTION: 5Scope and Limitations

Although our study presents a promising method for balancing general and domain-specific capabilities in LLMs, its scope is limited to the language modality, especially programming language and English. Future research could explore extending the application of our block expansion method to other domains, such as maintaining original language ability in multimodal large language modelsGe et al. (2023); Bai et al. (2023), and multi-lingual domains.

SECTION: 6Conclusion

In this study, we introduced a novel block expansion method for Large Language Models (LLMs) post-pretraining, aiming to enhance domain-specific abilities while preserving the original general capabilities. Our approach effectively balances the model’s performance across both general and domain-specific tasks. We demonstrated the effectiveness of our method throughLLaMA Pro, an LLM initialized from LLaMA2-7B with 8 added blocks, which outperformed other LLaMA-series models on comprehensive benchmarks.

SECTION: 7Ethical Statement

LLaMA ProandLLaMA Pro - Instructare designed for a wide range of NLP tasks, with a focus on programming, mathematics, and general language tasks. It suits scenarios requiring integration of natural and programming languages. While LLaMA-Pro addresses some limitations of previous models in the series, it may still encounter challenges specific to highly specialized domains or tasks. Users should be aware of potential biases in the model and use it responsibly, considering its impact on various applications with the LLaMA-2 license.

SECTION: References

SECTION: Appendix AGradient Derivation

To calculate the gradient of the RMSNorm weight during backpropagation, we first need to consider the forward pass equation for the Llama RMSNorm:

whereis the input tensor,is the weight parameter,is the variance ofacross the last dimension, andis a small constant for numerical stability.

Now, let’s consider the chain rule for the gradient of the loss function with respect to the RMSNorm weight during backpropagation. Denote the loss function as, and the output of the FFN as. We have:

To compute the gradient, we need to find the partial derivative. From the FFN equation, we have:

Taking the derivative with respect to, we get:

Now, let’s differentiate the RMSNorm function with respect to:

Using the chain rule, we can compute the gradient of the loss function with respect to the RMSNorm weight:

Given that, we need to find the derivative of the FFN with respect to. Recall the FFN equation:

Now we want to find the partial derivative of the FFN with respect to. Recall the SwiGLU activation function:

Taking the derivative of the SwiGLU function with respect to, we get:

Now, recall the SiLU activation function:

Thus, the gradient of the FFN with respect towhenis also zero:

In conclusion, when, the gradient of the FFN with respect tois zero, which demonstrates that the gradient is zero when the input to the FFN is zero.

SECTION: Appendix BDataset Details

In this section, we provide detailed information about the dataset used for both pretraining and Supervised Fine-Tuning (SFT). Table6outlines the composition of our pretraining dataset, which comprises approximately 80 billion tokens from both math and code corpora. The specifics of the SFT data are delineated in Table7.

For our proposedLLaMA Pro - Instruct, we employ a blend of multiple instruction datasets spanning general instruction, math, and code for the SFT process. These sources include ShareGPT333https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered, which contains real user and ChatGPT chat history records, and the WizardLM evolution instruction datasetXu et al. (2023a), offering a wealth of instruction data with varying complexity levels. We also incorporate the evolution CodeAlpaca datasetLuo et al. (2023), which includes complex coding tasks generated by ChatGPT and their corresponding solutions. Additionally, we use MetaMathYu et al. (2023), which reframes questions from multiple perspectives, and SlimOrcaLian et al. (2023), a curated subset of our OpenOrca data. SlimOrca provides an efficient route to achieve performance comparable to using larger data slices, while only incorporating approximately 500,000 GPT-4 completions.

SECTION: Appendix CMistal-Pro Details

Mistral-Pro is an advanced version of the original Mistral modelJiang et al. (2023), enhanced through the addition of Transformer blocks. This version excels in combining general language understanding with domain-specific knowledge, particularly in programming and mathematics. It employs the same methodology for creating additional blocks as LLaMA-Pro but utilizes onlyof LLaMA Pro’s learning rate, as recommended by MetaMath-Mistral444https://huggingface.co/spaces/TencentARC/MetaMath-Mistral-Pro. We continued pretraining on code and math datasets, including the automath subset of Cosmopedia555https://huggingface.co/datasets/HuggingFaceTB/cosmopedia, proof-pile-2, and the Python subset of Stack. The supervised fine-tuning (SFT) approach remains consistent with MetaMath-Mistral, except that we switch the base model to our Mistral-Pro. The detailed results of GSM8k and MATH can be found in Table8.

SECTION: Appendix DEvaluation Benchmark

The benchmarks used for evaluation include:

AI2 Reasoning ChallengeClark et al. (2018)(25-shot): a set of grade-school science questions.

HellaSwag(10-shot)Zellers et al. (2019): a test of commonsense inference, which is easy for humans (approximately 95%) but challenging for SOTA models.

MMLU(5-shot)Hendrycks et al. (2020): a test to measure a text model’s multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more.

TruthfulQA(0-shot)Lin et al. (2021): a test to measure a model’s propensity to reproduce falsehoods commonly found online.

Winogrande(5-shot)Sakaguchi et al. (2021): an adversarial and difficult Winograd benchmark at scale, for commonsense reasoning.

GSM8k(5-shot)Cobbe et al. (2021): diverse grade school math word problems to measure a model’s ability to solve multi-step mathematical reasoning problems. Additionally, we assess the models in the context of the Program of Thought (PoT) settingChen et al. (2023a). The PoT setting utilizes Python code to solve mathematical problems, which serves to evaluate the code generation capabilities of the models.

HumanEval(0-shot)Chen et al. (2021b): 164 handwritten Python programming problems with a function signature, docstring, body, and several unit tests.

MBPP(3-shot)Austin et al. (2021): crowd-sourced Python programming problems, designed to be solvable by entry-level programmers. Each problem consists of a task description in English, a code solution and 3 automated test cases.

SECTION: Appendix EMINT-Bench

The MINT-BenchWang et al. (2023c)details are provided in this section. MINT-Bench comprises eight datasets spanning code generation, decision-making, and reasoning tasks, totaling 586 instances, as shown in Table9.

We use theSuccess Rate (SR)as our evaluation metric, which measures the percentage of successful task instances. For an interaction limit of, MINT-Bench starts from scratch and allows each LLM to interact up to the-th turn, measuring the corresponding. Unless specified otherwise, MINT-Bench limits, whereindicates no interaction, andmaximizes interaction turns within the context window (4,096 tokens) of most modern LLMs.

In each turn, the LLM is instructed to perform the following steps:(1)Optionally express its reasoning process (referred to as "Thought," similar toYao et al. (2022));(2)Either interact with tools by generating Python code and executing it through a Python interpreter (referred to as "Execute"), or propose a solution to the user (referred to as "Propose Solution").

Table10displays the success rate for each model evaluated on various task type benchmarks, as well as the micro average when. TheLLaMA Pro - Instructmodel demonstrates robust performance across all task types compared to other models of similar size. Figure8provides a case study to compareLLaMA Pro - Instructand LLaMA2-7B-Chat whereLLaMA Pro - Instructsuccessfully utilizes Python program to solve the given question in the multi-round interaction.

SECTION: Appendix FMT-Bench

MT-bench is a collection of demanding multi-turn open-ended questions designed for evaluating chat assistants. In order to automate the evaluation process, we employ powerful LLMs, such as GPT-4, to act as judges and assess the quality of the models’ responses. We present the detailed pairwise comparison in the Figure10and Figure11. Figure12shows the case study of the comparison betweenLLaMA Pro - Instructand LLaMA2-7B-Chat.

SECTION: Appendix GToken Distribution

We assess the token distribution between LLaMA-2-7B andLLaMA Pro, employing the methodology proposed byLin et al. (2023). Specifically, for a given user query, we input it intoLLaMA Proto obtain its outputusing greedy decoding. For each position, we define a context at this position as. We denote the aligned model’s probability distribution for predicting the next token at this position as, wherehas the highest probability.

By passing the contextinto the base model LLaMA-2-7B, we generate another probability distribution,, for sampling the next token at this position. First, the aligned model with greedy decoding is used to generate a full output. For each position, tokens are ranked according to their probabilityas predicted by the base model. The rank ofin this sorted list is defined as the ’base rank’, denoted as. This categorizes positions into three types: (1) unshifted positions ():is the top-ranked token in bothand, having the highest probability; (2) marginal positions (): althoughis not the top-ranked token in, it is still likely to be sampled for decoding, with the 2nd or 3rd highest probability; (3) shifted positions (): in this case,is rather unlikely to be sampled by, indicating a significant distribution shift fromto.

We conduct a perplexity evaluation of LLaMA-2-7B andLLaMA Proacross general and code corpora. For the general domain, we utilize two different versions of the LAMBADA dataset. For the code domain, we use the Python split of the bigcode/the-stack-smol-xs dataset666https://huggingface.co/datasets/bigcode/the-stack-smol-xs. The results, presented in Table11, indicate thatLLaMA Proeffectively retains the language modeling ability for the general corpus while enhancing its proficiency in the code domain.

SECTION: Appendix HDomain of Law

Table12shows the hyper-parameters we use to do the ablation study in the domain of law. We use the freelaw subset of Pile dataset as our pretrain corpusGao et al. (2020)in the domain of law. This subset has 51.2 GiB raw size and 16.7B tokens with 3.6M documents.

The Unfair-ToS dataset, which we use to evaluate the performance of law, contains Terms of Service (ToS) from online platforms (e.g., YouTube, Ebay, Facebook, etc.). The dataset has been annotated on the sentence-level with 8 types of unfair contractual terms (sentences), meaning terms that potentially violate user rights according to the European consumer law. The UNFAIR-ToS task is a multilabel classification task. To get model predictions for this task, we categorize it as a multiple-choice question as the methodCheng et al. (2023)uses. The accuracy of an individual data example is considered true if the model prediction (i.e., the option with the highest per-token likelihood) belongs to the label(s) set. We evaluate the Unfair-ToS dataset in a 4-shot scenario just likeCheng et al. (2023).

Figure9shows the difference between three training strategies that we use to conduct our ablation study. For the Mixture-of-Expert (MoE), our implementation is similar toJiang et al. (2024). We use 2 experts and for each token, both experts will be activated. Specifically, We extend each FFN for all 32 layers, keep the originalunchanged, learn an additional Linear layer with weights, and at the same time add two new learnable parameters, when forward the output of Linear corresponding towill be weighted and summed withand fed into the next block.