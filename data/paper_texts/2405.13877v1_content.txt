SECTION: On connections between-coloring and Euclidean-means

In the Euclidean-means problems we are given as input a set ofpoints inand the goal is to find a set ofpoints, so as
to minimize the sum of the squared Euclidean distances from each point into its closest
center in.
In this paper, we formally explore connections between the-coloring problem on graphs and the Euclidean-means problem. Our results are as follows:

For all, we provide a simple reduction from the-coloring problem on regular graphs to the Euclidean-means problem. Moreover, our technique extends to enable a reduction from a structured max-cut problem (which may be considered as a partial 2-coloring problem) to the Euclidean-means problem. Thus, we have a simple and alternate proof of the NP-hardness of Euclidean 2-means problem.

In the other direction, we mimic thetime algorithm of Williams [TCS’05] for the max-cut of problem onvertices to obtain an algorithm for the Euclidean 2-means problem with the same runtime, improving on the naive exhaustive search running intime.

We prove similar results and connections as above for the Euclidean-min-sumproblem.

SECTION: 1Introduction

The-means problem333Throughout this paper, we consider the-means problem only in the Euclidean space.is a classic objective for modelling
clustering in a large variety of applications arising in data mining and
machine learning. Given a set ofpoints,
the goal is to find a set ofpoints, calledcenters, so as
to minimize the sum of the squared distances from each point into its closest
center in.
The algorithmic study of the-means problem arguably
started with the seminal work of Lloyd[Llo82].
Since then, the problem has received a tremendous amount of attention[Ber06,WKQ+08].

The-means problem is
known to be NP-Hard, even when the points lie in the Euclidean plane (andis large)[MNV12], or even when(and the dimension is large)[DF09].
On the positive side, a near linear time approximation scheme exists when the dimension is fixed (and the number of clustersis arbitrary)[CFS21], or when the number of clusters is constant (and the dimensionis arbitrary)[KSS10,BJK18].
When bothandare arbitrary,
several groups of researchers have shown hardness
of approximation results[ACKS15,LSW17,CK19,CKL22].

One of the main research directions in Fine-Grained Complexity is to identify the exact complexity of important hard problems, distinguishing, say, between NP-complete problems where exhaustive search is essentially the best possible algorithm, and those that have improved exponential time algorithms[Wil15,Wil16,Wil18].
The importance of high-dimensional Euclidean inputs in statistics and
machine learning applications has led researchers to study the parameterized and fine-grained complexity of the problem. Both the
dimensionality of the input,, and the target number of clusters,, have been studied as parameters, in as early as the
mid 90s.

The-means problems can be solved in timeby simply performing an exhaustive search over the solution space. This can be improved toruntime using dynamic programming[Jen69], which itself can be further improved toruntime using fast max-sum convolution[BHKK07]. The seminal work of Inaba, Katoh, and Imai[IKI94]has shown that one can compute an exact solution to the-means problems in time.
However, this algorithm clearly suffers from the so-called “curse of dimensionality”, the higher the dimension, the
higher the running time. Thus, for high dimensions, say when, the algorithm in[IKI94]is slower than even the exhaustive search over the solution space. Therefore, we ask:

Can we beatruntime for-means problem in high dimensions?444In[FGI+22], the authors provide a better thanruntime algorithm (to be precise anruntime algorithm) for adiscretevariant of the-means problem, where the centers need to be picked from the input point-set. Their result extends to the discrete variant of the-median and-center problems as well. However, the discrete variant is not as natural as the continuous variant in geometric spaces.

The complexity of-means problem increases as the number of clusterincreases. Therefore, if the answer to the above question is in the affirmative, then a natural first step would be to try to beat the exhaustive search algorithm for 2-means problem.

Is there an algorithm for 2-means problemrunning in time, for some?

Additionally, the case offor the-means problem is of practical interest, for example, in medical testing to determine if a patient has certain disease or not, and industrial quality control to decide whether a specification has been met, and also in information retrieval to decide whether a page should be in the result set of a search or not.

Apriori, there is no reason to suspect that an improvement over exhaustive search is even possible, and one might instead be able to prove conditional lower bounds assuming one of the popular fine-grained complexity theoretic hypothesis such as theStrong Exponential Time Hypothesis[IP01,IPZ01]or theSet Cover Conjecture[CDL+16]. In fact, over the last decade, there have been a large number of conditional lower bounds proven under these two assumptions ruling out algorithms which are faster than exhaustive search (for example, see[CDL+16,SV19,KT19,ABHS22,Lam20]). Thus, it comes as a pleasant surprise that our main result is an affirmative answer to the above question on the 2-means problem.

There is an exact algorithm for the 2-means problem running in time, whereis
the number of input points555In this theorem, we assume that the coordinate entries of all points in the input are integral and that the absolute value of any coordinate is bounded by..

We remark that, under the hypothesis that the matrix multiplication constant, our runtime can be improved to.

From a technical point-of-view, the ideas and intuition behind our algorithm provide a lot of conceptual clarifications and insights, and we elaborate more on that below.

There are a lot of connections between clustering problems and graph cut problems (for example, spectral clustering[VL07]or metric Max-Cut[DK01]). The popular graph cut problems that are motivated by geometric clustering tasks are (variants of) the min-cut problem and sparsest cut problem. Intuitively, in these two cut problems, a node corresponds to a point of a clustering problem, and an edge corresponds to similarity (i.e., proximity in the distance measure) between the corresponding points. Such a connection is presented in[Fei14]to prove the NP-hardness of-median in-metric.

One of the key insights, inspired by the embedding in[CKL21,FKK+23], is a connection between the Max-Cut problem and
the (Euclidean) 2-means problem!
In the Max-Cut problem, we are given as input a graph and the goal is to partition the vertex set into two parts such that the number of edges across the parts is maximized. We provide an intimate connection between Max-Cut (with some special guarantees) and 2-means by showing the following:

In Section3, we present a simple embedding of the vertices of a Max-Cut instance (with additional guarantees) into Euclidean space such that any clustering of the resulting pointset into two clusters minimizing the 2-means objective yields a partition of the vertices of the Max-Cut instance maximizing the number of edges across the parts. This gives an alternate view to[DF09]on the hardness of 2-means (see Remark3.2to know more about the technical differences).

In[Wil05], Williams designed an algorithm to solve Max-Cut in better than exhaustive search time. This algorithmic technique can be adopted for the 2-means problem to give us666For the sake of ease of presentation, we use the more generalized algorithm of Williams for Weigthed 2-CSPs given in[Wil07].Theorem1.1. However, we have to take some additional care due to the geometric nature of the problem.

In fact, the structured Max-Cut instances that we identify are quite powerful – in Section6, we show that the structured Max-Cut problem is computationally equivalent to 2-min-sum777-min-sum is another classic clustering objective and is defined in Section2.problem under polynomial time reductions. This yields both hardness of 2-min-sum and also antime algorithm for 2-min-sum (much like Theorem1.1).

Moreover, in Section4we present a general and yetsimpleconnection between-coloring and-means clustering for888It is appropriate to view Max-Cut as almost 2-coloring.. This yields an even simpler proof of NP-hardness of 3-means problems. Additionally, it opens up the below new research direction for future exploration:

There is an inclusion-exclusion based algorithm that runs intime for the-coloring problem onvertex graphs[BHK09]. On the other hand, for fixed, there are techniques different from William’s technique[Wil05]to beat thetime algorithm for-coloring problem[BE05,FGS07,Zam21].

Can we use the algorithmic techniques developed for-coloringto obtain similar runtimes for-means (for small values of)?

Another important fine-grained complexity question is about beating exhaustive search for other popular clustering objectives.

Is there an algorithm for Euclidean 2-center or 2-median problem running in time, for some?

In Section2, we formally introduce the problems of interest to this paper and also prove/recall some basic NP-hardness results. In Section3, we provide a linear size blow up reduction from (a specially structured) Max-cut instance to the 2-means problem. In Section4, we provide a linear time reduction from-coloring problem to the-means problem.
In Section5, we provide an algorithm for 2-means problem that beats exhaustive search. Finally, in Section6, we prove the computational equivalence of 2-min-sum and (structured) Max-Cut problems.

SECTION: 2Preliminaries

In this section, we define the clustering problems studied in this paper. We also define the graph problems and prove/recall their (known) hardness results for the sake of completeness.

SECTION: 2.1Problem Definitions

The input is a set ofpointsand the output is a partition of points into clusters, along with their centerssuch that the following objective, the-means cost, is minimized:

Here,is the Euclidean distance. In the decision version of the problem, a rationalwill be given as part of the input, and the output isYESif there exists a partition such that its-means cost is less than or equal to, andNOotherwise. Moreover in the above formulation of the objective, it can be shown that thecenterof a clusteris the centroid of the points in that cluster. Using this result,can be removed from the above objective function and it simplifies to minimizing:

Letbe a metric space. The input is a set ofpointsand the output is a partition of points into clusterssuch that the following objective, the-min-sum cost, is minimized:

whereis the metric distance measure. In the decision version, an integerwill be given as the part of the input, and the output isYESif there exists a partition such that its-min-sum cost is less than or equal to, andNOotherwise.

In this paper, we study the-meansproblem in Euclidean space, but we study the-min-sumproblem in general metric.

The input is a-regular graphand an integer. For any partition ofinto two parts, an edge is called abad edgeif both of its vertices are in the same part and is agood edgeotherwise. The problem is to distinguish between the following two cases:

YES instance: There exists a balanced cut ofinto, such thatand the total number of bad edges is equal to.

NO instance: For every 2-partition ofinto, the total number of bad edges is strictly greater than.

Note that this problem is slightly different from the conventional way of defining theMax-Cutproblem in two ways. The partition setsandneed to be of same size in theYEScase, and we focus on the bad edges instead of the edges cut (although it is still a Max-Cut problem due to the regularity of the input graph).

The input is a-regular graphand an integerand the output isYES, if there exists a-coloring of the vertices of the graph such that no two adjacent vertices are of the same color; else, the output isNO.

The input is a collection ofclauses onvariables. Each clause contains three variables or negation of variables. The output isYESif and only if there exists an assignment of the variables such that
all three values in every clause are not equal to each other. In other words, every clause has at least one true value and at least one is false.

The input are integersfor a fixed integerindependent of the input, a finite domain, and functions:

where. The output to the Weighted 2-CSP problem isYESif and only if there is a variable assignmentsuch that,

SECTION: 2.2Computational Hardness of Graph Problems

In this subsection, we state the NP-hardness ofBalanced Max-Cutand-coloring.

-coloring is NP-Hard for all.

Balanced Max-Cut is NP-Hard.

The proof of the above theorem is a reduction from an NP-hard structured variant of NAE-3-SAT problem and is deferred to AppendixA.

SECTION: 3Reduction from Balanced Max-Cut to 2-means

In this section, we give a (linear size blow up) reduction fromBalanced Max-Cutto2-meansproblem.

Letbe the input to an instance of the Balanced Max-Cut problem whereis a-regular graph. Letand. We buildpoints inand set the task of checking if the 2-means cost is equal to. Arbitrarily orient the edges of the graph. For every, we have a pointin the point-set where,

For any two-partition of:=, let the number of bad edges in each part berespectively. Then, the 2-means cost of the corresponding 2-clusteringis given by.

Fix.
Let us begin by computing the center of a cluster. Observe that a good edgecontributes aadditive factor to the center on thecoordinate, whereas a bad edge contributes nothing. So, the center of a clusteris given by the following expression:

For any, the cost contributed by a good edgeis given by:

The cost contributed by a bad edgeis given by

We rewrite the2-meanscost in terms of cost contributed by good and bad edges as follows:

The cost contributed by a bad edge in a clusteris given by:, and there aremany such bad edges. On the other hand, the cost contributed by a good edge on all vertices in the clusteris, and there aremany good edges in each part. Putting it together, the total2-meanscost:

∎

In anYES instance,and. So, from Claim3.1, the2-meanscost is:

In aNO instance,and.

Assume, for the sake of contradiction, there exists a clusteringsuch that its2-meanscost is. From Claim3.1, this implies:

Without loss of generality let us suppose that, and thus letand. We may assume that, because otherwise, we immediately arrive at a contradiction as the soundness assumption tells us thatand (1) implies that.

We can now rewrite (1) as follows:

Combining the above with the soundness assumption thatand that, we obtain:

Since bothandare positive, this implies, which is a contradiction.
This completes the soundness analysis of the reduction.

The starting point of the proof of NP-hardness of 2-means given in[DF09]is also NAE-3-SAT (much like the starting point of the NP-hardness proof idea of Balanced Max-Cut). However, in[DF09], the authors directly construct the distance matrix of the input points from the NAE-3-SAT instance and then argue that the distance matrix can indeed be realized in. On the other hand, our proof sheds new light by identifying a clean graph theoretic intermediate problem, namely the Balanced Max-Cut problem, which then admits a very simple embedding to the Euclidean space.

SECTION: 4Reduction from-Coloring to-means

In this section, we present a (linear size blow up) reduction from-coloringto-meansproblem. Invoking Theorem2.1then gives an alternate proof of NP-hardness for the-meansproblem when.

Letbe the input to an instance of the-coloring problem whereis a-regular graph. Letand. We build the point setofpoints inand set the-means cost equal to. Arbitrarily orient the edges of the graph. For every, we have a pointwhere,

Ifis-colorable, thencan be partitioned intosuch that eachis an independent set. Consider the clusters of pointssuch that. Observe that the centerof a clusteris given by

This is because at most one vertex of an edge can be present in a cluster, this is from the definition of an independent set. Moreover, note that there are exactlynon-zero entries in each(recall thatis-regular). Let us compute the cost contributed by a point:

In the above cost contribution, for fixed, the first case happens oncoordinates and the second case oncoordinates, owing to the-regularity of. The-meanscost contributed by one point is then

The total-meanscost by all the points is given by:

This shows that theYESinstance of-coloringto reduced to aYESinstance of-means.

Whenis not-colorable, any-partition ofcontains at least one edge with both its vertices in the same part. Let us call such edgesbad edges, and the rest of the edges (the edges with its vertices in different parts) asgood edges. Consider any-partition of, and the corresponding clusters. Let the number of bad edges in each part berespectively. Since it is aNOinstance,.

The centerof each clusteris given by

The cost contributed by a good edge is

and the cost contributed by a bad edge is

Hence, the-meanscost is given by

The cost contributed by each bad edge on the vertices in the same part is 2, and there aremany such bad edges. The cost contributed by all good edges on all vertices in the same part is, and there aremany good edges in each part. So, the total-means cost is

This shows that any partitioning of the-coloring instance would result in the-means cost strictly greater than. This completes the soundness case.

SECTION: 5Anruntime Algorithm for 2-means

In this section, we discuss the algorithm that beats exhaustive search for the2-meansproblem. The key idea of the algorithm is to reduce the given2-meansinstance to aWeighted 2-CSPinstance (see Section2for it’s definition). Then we use the matrix multiplication based fast algorithm for the Weighted 2-CSP problem999This is a generalization of the max-cut problem[Wil07].to beat thetime bound for 2-means problem. Formally, we prove the following theorem.

There is an exact algorithm for the 2-means problem running in time, whereis the number of input points,is the dimensionality of the Euclidean space, andis the matrix multiplication constant.

There is an exact algorithm for the 2-means problem running in time.

By using the best known value of matrix multiplication constant,[WXXZ24], we upper bound the time complexity in Theorem5.1by.
∎

To prove Theorem5.1, we need the well-known algorithm of Williams[Wil07]which in turns relies on the algorithm of Nešetřil and Poljak,[NP85]to quickly detect a-clique in a graph via Matrix Multiplication.

Weighted 2-CSP instances with weights inare solvable in

whereis the number of variables,is the domain, andis the matrix multiplication constant.

We are now ready to prove Theorem5.1. For the sake of presentation, we will assume that the coordinate entries of all points in the input are integral and that the absolute value of any coordinate is bounded by(although our claims would go through even if the absolute value of any coordinate is bounded above by). Moreover, we will assume that.

For the ease of presentation, assumeis divisible by. Letbe the set of given input points to the 2-means instance (where).
Under the assumption that the input is integral, let the largest absolute value of any coordinate appearing inbe. Then, the squared Euclidean distance between any two pointsis bounded by

Consequently, we obtain that, for any partition oftoand, we have:

for some fixed constant(for large enough).

Arbitrarily partitionintosetsandwithpoints in each set.
For everysuch that, and, such that, we construct an instanceof Weighted 2-CSP on 3 variablesandas follows.

For every, let. Letbe the domain of. Note that. It remains to define the weight functionsto complete the construction of.

For everyand every, we define:

For every, we define.

Next, for everysuch that, and everyand, we define:

For every, we define.

Recall, that the output tomust beYESif and only if

Also note that the number of weighted 2-CSP instances we constructed is, and the construction time of each instance is at most.

We run the algorithm in Theorem5.3on all the above constructed Weighted 2-CSP instances, and letbe an instance whose output isYES, and for whichis minimized (amongst all instances for which the algorithm outputtedYES; it is easy to observe that there is at least one instance whose output by the algorithm isYES).

Letbe the variable assignment towhich satisfies all the constraints. Then we claim that the two clustersandminimizes the 2-means objective for.
We prove this by contradiction as follows.

Suppose (one of) the minimizers of the 2-means objective foris given by the clustering. For the sake of contradiction, we assume that the 2-means cost ofis strictly less than the 2-means cost of, i.e.,

For every, letand. Letand letwhereandare defined as follows:

By construction, we have thatis an assignment tothat satisfies all the constraints.

However from (2), note that:

leading to a contradiction.
∎

SECTION: 6Fine-Grained Complexity of 2-min-sum

In this section, we study the fine-complexity of the2-min-sumproblem in general-metric spaces.

The 2-min-sum and Balanced Max-Cut problems are computationally equivalent in-metrics:

Given a 2-min-sum instance, there is a polytime algorithm to reduce it to a weighted Max-Cut instance.

Given a Balanced Max-Cut instance, there is a polytime algorithm to reduce it to a 2-min-sum instance.

SECTION: 6.1Reduction from Balanced Max-Cut to 2-min-sum

In this subsection, we give a reduction fromBalanced Max-Cutas defined in Section2(and thus the NP-hardness in Theorem2.2applies to 2-min-sum as well).

Letandbe an instance to theBalanced Max-Cutproblem. Letand. We buildpoints inand set the2-min-sumcost equal to

Arbitrarily orient the edges of the graph. For every, we define a pointin the point-set where,

For any two-partition of, let the number of bad edges in each part berespectively. Then, the 2-min-sum cost of the corresponding 2-clustering is

Let us begin by computing the distance between any two pointsand. Each of the pointsandhavenon-zero entries, sinceis-regular. Since we are working in themetric, we remark that

If there is an edgefromtoin, thenandfor exactlyother edges. So, the distance betweenandin this case is.

On the other hand, if there is no edge betweenand, thenfor exactlyedges. Putting it together, we get:

The2-min-sumcost can be computed as follows:

∎

In aYESinstance,and. So, from Claim6.2, the2-min-sumcost is

In aNOinstance,and. Observe that. So the2-min-sumcost is strictly greater than.

SECTION: 6.2Anruntime Algorithm for 2-min-sum

Instead of giving an algorithm to2-min-sumproblem, we will give a reduction from the problem to the well-knownMax-Cutproblem with linear blowup. Then, we will the use the algorithm by Williams[Wil05]that runs in time, whereis the number of points.

Let us begin by recalling the well-known NP-Hard problem, theMax-Cutproblem.

The input is a weighted graph, and the output is a partition of the vertices ofintosuch that the sum of weights of edges with one vertex inand the other inis maximized.

There is a polynomial-time linear-blowup reduction from 2-min-sum problem to Max-Cut problem.

Given a2-min-suminstance, we will construct aMax-Cutinstancewith weight function. Letbe the number of points in. For every point, we will add a vertexto. We will have an edge between every pair of vertices(that is,is an-clique), and the weight of the edge is equal to distance between the pointsand:.

Correctness of the reduction: For any 2-partitioning of the vertex set, observe that the weight of any edge would either contribute toMax-Cutcost or2-min-sumcost. To elaborate, the weight of the edge contributes to the2-min-sumcost if both the vertices of the edge are in eitherorand toMax-Cutcost otherwise. Since the total sum of weights of all the edges in a given instance is constant,2-min-sumcost andMax-Cutcost sums to a constant. Therefore, maximizing theMax-Cutcost is the same task as minimizing the2-min-sumcost.
∎

SECTION: Acknowledgements

We would like to thank Pasin Manurangsi for pointing us to[BHKK07]and informing us that the fast max-sum convolution result of that paper can be used to obtain aruntime algorithm for the-means problem. Also, we would like to thank Vincent Cohen-Addad for suggesting to us the that-min-sum problem might be more naturally connected to the Max-Cut problem. Finally, we would like to thank the anonymous reviewers for helping us improve the presentation of the paper.

SECTION: References

SECTION: Appendix ANP-hardness of Balanced Max-Cut

In this section, we prove that the Balanced Max-Cut problem is NP-hard using a particular NP-hard variant of the not-all-equal 3-SAT (abbreviated to “NAE-3-SAT”) problem. For convenience, we write clauses in an instance of 3-SAT as sets of literals, and then consider the instance as a collection of subsets of literals.

Given a graphand a partition, thecutofcreated byis a subsetof edges with one end-point inand the other in. An edgeisgoodwith respect to a cutif, and calledbadotherwise. We denote the number of bad edges by.

Givenvariables, aliteralis an element of the set. The literalwill represent the variable, andrepresents the negation of, i.e.,.

Aclauseoveris a subsetof literals, and aCNFoveris a collectionwhere eachis a clause overvariables. Anassignmentis a functionsuch thatfor each.

To prove the NP-hardness of Balanced Max-Cut, we give a reduction from Linear 4-Regular NAE-3-SAT, which we define as follows.

The input is an integerand a CNFoverthat satisfies

(3-uniform)for each,

(Linear)for allwith,

(4-regular) For each, the sethas cardinality 4.

The problem outputsYESif there exists an assignmentwithfor each, andNOotherwise.

Essentially,as above does not assign every literal in the same clause with the same value; this is the “not-all-equals” part of the problem above.

We call an instanceof Linear 4-Regular NAE-3-SATnae-satisfiableif there is such an assignment. This problem has been shown to be NP-hard by[DD20]. We focus now on the main result of this section.

SECTION: A.1Balanced Max-Cut is NP-hard

Our goal will be to construct a graphfrom an instanceof Linear 4-Regular NAE-3-SAT by duplicating literals and connecting them by edges in a special way depending on the clauses that contain them.

Fix the number of variablesand letbe an instance of Linear 4-Regular NAE-3-SAT. We construct a graph, depending on. Let the vertex setofbe

i.e., for every literal in,will contain 4 copies of that literal as a vertex.

For each clause, introduce an edge between each pair of vertices,,, for each. This constructs four disjoint 3-cycles infor; denote theth one created fromby

In addition, we also place the same edge relations inwhere we replacewith, respectively. The edges we have added are those in the set

Finally, for simplicity, define, which represents two disjoint triangles in.

Next, for each, insert an undirected edge betweenand, for each. This constructs a copy ofinfor each; denote the edge set of this copy ofby

Formally, we’ve constructed the graphwith edge set

Intuitively, for each clause,contains a triangle between theth copies of the variables in, and between their negations. In total, we have 8 disjoint triangles for each clause. In addition, we connect copies of the variablewith copies of the variableby a copy of.

Note thatis simple (no multiedges) sinceis linear, andis 12-regular: eachhas

sinceis incident to 2 edges for each of the 4 clauses that eitherorare in, andis connected tofor.

For any cut, letandbe the number of bad edges in, respectively, under the cut. The union in (3) is actually a disjoint union, so

(For the ease of presentation, we will writeandto represent the sum notations above.)

The next result gives a useful bound forgiven the above. Before continuing, observe that the vertices involved inandare

correspondingly.

For any cutof,

and, if, equality occurs if and only ifandfor each.

Any cut of a triangle has either 1 or 3 bad edges, and any cutofhas at leastbad edges (these can be shown through casework). Accordingly, for any cutof,

wherefor. Note thatpartitions, so that. Hence, from formula (4) for,

When, equality occurs exactly when, which is only possible when eachand each.
∎

Now we can prove Theorem2.2.

We prove below the completeness and soundness of the reduction detailed above.

Supposenae-satisfies. For everywe have,

By definition of an assignment, if, then. Hence,. We also obtain that eachis not a bad edge, sofor each.

Sincenae-satisfies, the imagefor each. Correspondingly, this implies that only one of the three edges inis a bad edge, and similarly for. Equivalently,for each.
From LemmaA.1for this choice of, we have.

Our proof is by contradiction. Suppose there is some 2-partitionsuch that, whereand, and sinceis 4-Regular NAE-3-SAT formula, we have. By applying LemmaA.1over this partition, we obtain that. Then, we have thatand that

for each. Since, it must be the case that, for eachand each, the verticesandare in different parts of the partition.

Writing, observe that the setsandare contained in different parts of the partition, that is, either

Denote the mapby

is well-defined and, sinceare contained in different parts of the partition.

Finally, if, then exactly one of the edges inis bad, and similarly for. Correspondingly, this implies that the image set.
Therefore,nae-satisfies.
∎