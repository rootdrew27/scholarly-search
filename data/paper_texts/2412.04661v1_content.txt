SECTION: HEAL: Hierarchical Embedding Alignment Loss for Improved Retrieval and Representation Learning

Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by integrating external document retrieval to provide domain-specific or up-to-date knowledge. The effectiveness of RAG depends on the relevance of retrieved documents, which is influenced by the semantic alignment of embeddings with the domain’s specialized content. Although full fine-tuning can align language models to specific domains, it is computationally intensive and demands substantial data. This paper introducesHierarchicalEmbeddingAlignmentLoss (HEAL), a novel method that leverages hierarchical fuzzy clustering with matrix factorization within contrastive learning to efficiently align LLM embeddings with domain-specific content. HEAL computes level/depth-wise contrastive losses and incorporates hierarchical penalties to align embeddings with the underlying relationships in label hierarchies. This approach enhances retrieval relevance and document classification, effectively reducing hallucinations in LLM outputs. In our experiments, we benchmark and evaluate HEAL across diverse domains, including Healthcare, Material Science, Cyber-security, and Applied Maths.

SECTION: 1Introduction

Large Language Models (LLMs), such as GPT-4(OpenAI,2023), have demonstrated exceptional capabilities in natural language understanding and generation. However, LLMs are prone tohallucinations, generating plausible but incorrect or nonsensical content(Ji et al.,2023). Retrieval-Augmented Generation (RAG) frameworks(Lewis et al.,2020)mitigate this issue by integrating external knowledge through document retrieval, enhancing the factual accuracy of LLM outputs. A critical component of RAG systems is the embedding model used for document retrieval. Standard embedding models, however, often fail to capture the hierarchical and semantic relationships within domain-specific corpora, leading to suboptimal retrieval and, consequently, increased hallucinations. This issue is particularly pronounced in domains with increased specificity such as Healthcare, Legal sytem, and Scientific research.

Corpus of documents for a specialized domain inherently exhibit a high degree of semantic coherence, presenting an opportunity to align embedding models for retrieving the most contextually relevant information. Hierarchical Non-negative Matrix Factorization (HNMF)(Eren et al.,2023)is a powerful technique for semantically categorizing documents into clusters that exhibit thematic coherence. By grouping documents into hierarchical clusters of supertopics and subtopics, HNMF provides a rich semantic categorization of the corpus, enabling a deeper understanding of document relationships. Leveraging this semantic knowledge in the form of hierarchical cluster labels, we can align embedding models to preserve hierarchical information within the embedding space. This alignment enhances the embeddings to capture both coarse-grained and fine-grained document similarities, improving contextual relevance in retrieval tasks and enabling better downstream capabilities.

To tackle the challenges of hallucination and suboptimal retrieval in RAG systems, we introduce theHierarchical Embedding Alignment Loss (HEAL), a refined extension of the Hierarchical Multi-label Contrastive Loss(Zhang et al.,2022). HEAL leverages an improved hierarchical weighting scheme to align embeddings more effectively with the underlying hierarchical structure. By incorporating hierarchical label structures, HEAL fine-tunes embedding models to align with document clusters derived from HNMF. The method computes contrastive losses at each hierarchical level, combining them with depth-specific penalties to emphasize distinctions at higher levels of the hierarchy.

Our contributions are summarized as follows:

Introduce a refined contrastive learning framework, named HEAL, that incorporates hierarchical label structures to align embeddings with hierarchical document relationships.

Integrate HEAL into RAG systems, fine-tuning embedding models to improve retrieval accuracy and reduce hallucinations in LLM outputs.

Validate and benchmark HEAL through extensive experiments on domain-specific datasets from specialized scientific sub-domains of Healthcare, Material Science, Tensor Decomposition, and Cyber-security.

Showcase significant improvements in retrieval relevance and downstream tasks compared to baseline method.

SECTION: 2Related Work

Contrastive learning has become a cornerstone of representation learning, particularly in computer vision and natural language processing. Methods like SimCLR(Chen et al.,2020)and MoCo(He et al.,2020)have achieved state-of-the-art performance in unsupervised settings by learning representations that are invariant to data augmentations. In supervised contrastive learning,Khosla et al. (2020)extended the contrastive loss to utilize label information, improving performance on classification tasks. Similarly, the SciNCL framework employs neighborhood contrastive learning to capture continuous similarity among scientific documents, leveraging citation graph embeddings to sample both positive and negative examplesOstendorff et al. (2022). However, these methods generally assume flat label structures and do not exploit hierarchical relationships.

Hierarchical classification has been studied extensively, with approaches such as hierarchical softmax(Goodman,2001)and hierarchical cross-entropy loss(Deng et al.,2014). These methods aim to leverage hierarchical label structures to improve classification efficiency and accuracy. In the context of representation learning,Deng et al. (2011)introduced hierarchical semantic embedding, aligning image embeddings with WordNet hierarchies. More recent works, such asBertinetto et al. (2020), have explored hierarchical prototypes to capture hierarchical relationships.Zhang et al. (2022)propose a hierarchical multi-label contrastive learning framework that preserves hierarchical label relationships through hierarchy-preserving losses. Their method excels in scenarios with hierarchical multi-label annotations, such as biological or product classifications. In contrast, our approach focuses on enhancing information retrieval to mitigate hallucinations.

RAG frameworks combine retrieval models with generative models to enhance the factual accuracy of language generation(Lewis et al.,2020). These systems rely heavily on the quality of the embeddings used for retrieval. Prior work has focused on improving retrieval through better indexing and retrieval algorithms(Karpukhin et al.,2020), but less attention has been given to aligning embeddings with hierarchical document structures.

SECTION: 3Method

In this section, we propose an embedding alignment framework comprising hierarchical label extraction with HNMF, embedding alignment using HEAL, and retrieval with aligned embeddings as outlined in Figure1.

SECTION: 3.1Hierarchical Document Clustering with HNMFk.

Hierarchical Non-negative Matrix Factorization with automatic latent feature estimation (HNMFk)Eren et al. (2023)is an advanced technique for uncovering hierarchical patterns within document collections. It builds on traditional Non-negative Matrix Factorization (NMF)Vangara et al. (2021)by
dynamically and automatically determining the optimal number of latent features at each level.
Effective contrastive learning relies on well-separated document cluster labels to align embeddings effectively. HNMFk’s ability to automatically balance stability and accuracy using a bootstrap approach enhances the quality of clustering results. In this work, we utilize the publicly available HNMFk implementation from the TELF library111TELF is available athttps://github.com/lanl/T-ELF.

Given a Term Frequency-Inverse Document Frequency (TF-IDF) matrix, whererepresents the vocabulary size anddenotes the number of documents, HNMFk performs a sequence of matrix factorizations across hierarchical levels to capture the nested structure of topics. At each level, the factorization is expressed as, whereis the basis matrix representing latent topics, andis the coefficient matrix quantifying the contribution of each topic to the composition of documents. Here,is the number of topics at level, which is determined automatically through stability analysisVangara et al. (2021). This analysis involves bootstrapping the data to create resampled versions of the TF-IDF matrix, applying NMF across a range ofvalues, and evaluating the stability of clusters across the resampled datasets. The optimalis selected as the value that produces the most consistent clustering results, indicating a robust underlying structure in the data.

To construct hierarchical labels for each document, the coefficient matrixis used to determine topic assignments. For each level, the topic for documentis identified by selecting the index of the maximum value in the corresponding column of, expressed as. The hierarchical label for documentis then formed by aggregating the topic assignments across all levels, resulting in. Here,is the total number of hierarchical levels, or hierarchical depth that is the number of NMFk operations from the first one to the leaf.is the label of sampleat level, withcorresponding to theshallowest(most general or root node) level andto thedeepest(most fine-grained, or leaf node) level.

SECTION: 3.2Hierarchical Multilevel Contrastive Loss (HEAL)

Upon the unsupervised data decomposition with HNMFk, the datasets have clusters with hierarchical structures. To incorporate such structures, we propose the HEAL, which extends supervised contrastive loss(Khosla et al.,2020)by introducing level-wise contrastive losses and aggregating them with level-specific penalties.

For a batch ofsamples, whereis the input andis the hierarchical cluster label, we obtain normalized embeddingsusing an encoder network:

For a given level, the set of positive samples for sampleis:

The contrastive loss at levelfor sampleis:

Ifis empty (i.e., no positive samples at levelfor),is excluded from the total loss.

To prioritize discrepancies at shallower levels, we assign penaltiesto each level, where shallower levels have higher penalties. The penalties are defined as:

The penaltiessatisfy:

for, i.e., penalties decrease for deeper levels.

, i.e., the penalties are normalized.

The total HEAL loss is then:

Algorithm1outlines the computation offor a mini-batch.

SECTION: 3.3Fine-tuning Embedding Models with HEAL for RAG

To enhance retrieval performance in RAG systems, we fine-tune the embedding model to align with the hierarchical structure of the document corpus. Given a specialized document corpus, we first apply HNMFk (as described in Section3.1) to the corresponding TF-IDF matrixproducing hierarchical cluster labelsfor each document. Next, we generate embeddings from each documentusing a pretrained embedding model. The embedding model is initialized with pre-trained weights and produces normalized embeddingsfor document.
To align embeddings with the hierarchical structure, we optimize the HEAL presented in3.3.

The embedding model is trained by minimizingusing gradient-based optimization:

whereare the parameters of the embedding model.

After fine-tuning, the updated embeddingsare used to replace the initial embeddings in the vector store. During inference, a queryis embedded usingas, and
retrieves topdocuments based on cosine similarity:

To maximize retrieval performance in RAG systems, it is essential to align the query embeddings with the hierarchically aligned document embeddings. Since queries are typically shorter and may not capture the full semantic richness of the documents, we need to semantically align queries and documents in the embedding space. To achieve this, we generate question-answer (Q&A) pairs using a language model (e.g., LLaMA-3.1 70B) for each document and leverage HEAL to jointly align both query and document embeddings during training.
For each document, we generate a set of queries, whereis the number of queries generated for document. Each queryis associated with the same hierarchical labelsas its source document, since it is derived from the content of.We extend the HEAL framework to include both documents and queries by defining a unified set of samples:

Each samplehas an associated hierarchical label, where:

Based on this dataset, the HEAL is leveraged to finetune the embedding model .

SECTION: 4Experiments

SECTION: 4.1Datasets

We evaluate our method on datasets specifically constructed from scientific publications in the domains of Material Science, Medicine, Tensor Decomposition, and Cybersecurity. To construct our datasets, we leveraged the Bibliographic Utility Network Information Expansion (BUNIE) method, a machine learning-based approach that integrates subject-matter expertise in a human-in-the-loop frameworkSolovyev et al. (2023).
For completeness, we briefly summarize the BUNIE approach in this paper. BUNIE begins with a small core corpus of documents selected by subject-matter experts (SMEs). From this starting point, it constructs a citation network to identify additional relevant documents, leveraging BERT based text embeddings to assess semantic similarity. Through iterative cycles of dataset expansion and pruning—guided by embedding visualization, topic modeling, and expert feedback—the method ensures the corpus is both comprehensive and domain-specific. We apply this procedure to each scientific domain with guidance from SMEs, who provide target keywords/phrases and/or a core set of papers relevant to the sub-topic of interest within the domain. Using this knowledge base, we employ BUNIE to expand the dataset from the initial core papers to a larger collection of domain-specific documents.

Material Science: A collection of 46,862 scientific articles, which explore 73 Transition Metal Dichalcogenides (TMD) compounds, combining transition-metal and chalcogen atoms (S, Se, or Te). With a layered structure similar to graphite, TMDs excel as solid lubricants and exhibit unique quantum phases like superconductivity and charge density waves. Their atomically thin layers offer tunable properties, with applications in spintronics, optoelectronics, energy harvesting, batteries, and flexible electronics.

Healthcare: A collection of 9,639 scientific articles, which examine Pulmonary Hypertension (PH) disease - a rare condition causing elevated pulmonary arterial pressure, right heart strain, and reduced oxygen delivery. The WHO classifies PH into five groups based on causes, including pulmonary arterial hypertension (PAH), which has a prevalence of 15-25 cases per million in the U.S. Treatments such as endothelin receptor antagonists and prostacyclin analogs aim to improve symptoms, but prognosis varies, with untreated PAH having a median survival of less than three years.

Applied Mathematics:A collection of 4,624 scientific articles, which explore tensor network techniques, such as Tensor-Train (TT) decomposition, which recently emerged as a powerful mathematical tool for solving large-scale Partial Differential Equations (PDEs). Tensor network PDE solvers efficiently manage high-dimensional data by mitigating the curse of dimensionality, drastically reducing computational costs and memory usage while maintaining high solution accuracy. These advancements hold significant promise for breakthroughs in scientific computing, including material science, climate modeling, and engineering design optimization.

Cyber-security: We created a dataset of 8,790 scientific publications focusing on the application of tensor decomposition methods in cybersecurity and ML techniques for malware analysis. This dataset serves as a knowledge base covering topics for cyber-security such as ML-based anomaly detection, malware classification, novel malware detection, uncertainty quantification, real-world malware analysis challenges, tensor-based anomaly detection, malware characterization, and user behavior analysis.

SECTION: 4.2Experimental Setup

For training, we used the Adam optimizer with a learning rate of, a batch size of, and early stopping based on validation performance with a patience ofepochs. The experiments were conducted on a high-performance computing cluster, with each node equipped withNVIDIA GH200 GPUs. Document metadata, comprising the title and abstract combined, were used as input. Hierarchical labels were generated using HNMF with dataset-specific factorization depths: Material Science (depth 3), Healthcare (depth 4), Applied Mathematics (depth 3), and Cybersecurity (depth 3). HEAL loss was applied with a temperature parameter of. The embedding base model, SciNCLOstendorff et al. (2022), was chosen for its robust contrastive pretraining on scientific documents, serving as a strong baseline for fine-tuning.
The data was split intotraining,validation, andtest sets, with early stopping monitored on the validation set. Evaluation metrics were reported on the test set, while Q&A retrieval analysis used the entire dataset (train + validation + test) for constructing the vector store.

The efficacy of the RAG system was evaluated at two levels.First, we characterized the embeddings on document-level tasks, including hierarchical classification, retrieval, and hallucination measurement. For hierarchical classification, we used a hierarchical classifier applying random forests to each node(Miranda et al.,2023). The classifier is trained on embeddings corresponding to train dataset and evaluated against the test set. We perform this for embeddings derived from aligned and unaligned embedding model. Retrieval performance was assessed by measuring whether retrieved documents belonged to the same hierarchical class as the query document. Hallucination likelihood was evaluated based on the retrieval of incorrect documents for a given query.Second, we evaluated the performance of the embedding model within a RAG framework. To support retrieval and hallucination analysis, we used the LLaMA-3.1 70B model to generateQ&A pairs per document using abstracts as input, providing a robust test for embedding alignment and retrieval capabilities. Next, we leveraged the questions as queries to the embedding model to retrieve the best metadata and assessed whether the model retrieved the exact document that generated the query during Q&A analysis, as well as the rank of the returned document within the top 10 results. Furthermore, the retrieved documents were augmented with LLaMA-3.1 70B LLM to generate responses, with hallucinations evaluated based on response accuracy and relevance.

Given the specialized nature of our dataset and the requirement for hierarchical labels, fine-tuning is essential. Comparing our method to approaches that do not leverage hierarchical labels is inequitable, as they are inherently less effective for this task. Our approach simplifies training by eliminating HEAL loss hyperparameter tuning, unlike HiMulConZhang et al. (2022), which requires extensive tuning of penalty parameters for optimal results. While HiMulCon focuses on root-level classification in vision datasets, our method aligns embeddings across all hierarchical depths. We optimize hierarchical metrics such as classification, retrieval, and hallucination indirectly through the HEAL loss, ensuring a robust alignment with the hierarchical structure.

For these reasons, we evaluate the performance of HEAL using the baseline model SciNCL, both without and with hierarchical alignment on our diverse specialized datasets.
We evaluate performance using hierarchical metrics to capture nuances of hierarchical label structures in retrieval, classification, and hallucination assessments as presented in Table1.

SECTION: 4.3Results

Table2summarizes the performance metrics for three datasets (Healthcare, Materials, Applied Mathematics, and Cybersecurity) across three tasks: classification, retrieval, and hallucination evaluation. The aligned model corresponds to the embedding model trained using the HEAL loss, whereas the non-aligned model corresponds to the original embedding model without HEAL-based training. The metrics are reported for both non-aligned and aligned SciNCL embeddings, demonstrating the significant impact of HEAL on improving performance.
Figure2illustrates hierarchical embedding alignment achieved through HEAL training, resulting in well-separated super and sub-clusters for the Materials and Healthcare datasets which enhances the performance of downstream tasks.

First, we evaluate the performance on document-level tasks using hierarchical labels. Specifically, we assess the ability of the hierarchical classifier to predict hierarchical labels in the classification task. Additionally, we quantify the retrieval of documents from the same hierarchical category based on a query document to characterize retrieval accuracy and evaluate hallucinations.
The results presented in table2demonstrate that HEAL significantly improves hierarchical classification metrics across all datasets.
For the Healthcare dataset, the Hierarchical F1 Score improves from 0.5164 to 0.6588, reflecting a more accurate representation of hierarchical labels. Similarly, the Materials dataset achieves near perfect classification metrics (F1 Score, Precision, Recall = 0.99) with aligned embeddings, while the most challenging Healthcare dataset (4 depth cluster label) sees improvements in F1 Score from 0.5164 to 0.6588. In retrieval tasks, HEAL aligned embeddings consistently outperform non-aligned embeddings across all metrics. For the Healthcare dataset, Hierarchical MRR improves from 1.6259 to 2.2525, and nDCG@k increases from 0.3752 to 0.5908 where, indicating better ranking and retrieval relevance. The Materials dataset achieves a dramatic increase in retrieval precision, with Precision@k rising from 0.4787 to 0.9707, while nDCG@k reaches 0.99, showcasing near-perfect retrieval performance. For the Cyber dataset, aligned embeddings yield an MRR improvement from 2.7538 to 3.1482 and a corresponding nDCG@k increase from 0.6781 to 0.7908. Hallucination metrics further underscore the superiority of HEAL. Aligned embeddings reduce hallucination rates significantly across all datasets. For the Healthcare dataset, FPR@k drops from 0.9386 to 0.8771, and severity decreases from 0.7306 to 0.5533, indicating fewer irrelevant or misleading retrievals. The Materials dataset shows the most striking improvement, with FPR@k reduced from 0.8534 to 0.0878 and severity declining from 0.6041 to 0.0644, nearly eliminating hallucination tendencies. For the Cyber dataset, aligned embeddings lower FPR@k from 0.7968 to 0.6236 and severity from 0.4402 to 0.3654.

Next, we evaluate the performance of aligned RAG in retrieving the correct documents for generated queries to augment the LLM and minimize hallucinations. From each test dataset, we randomly sampled 100 documents and generated 10 Q&A pairs per document using the LLAMA-3.1 70B model, resulting in a total of 1,000 Q&A pairs for each dataset. Each Q&A pair was tagged with the corresponding document from which it was generated.
The prompt used for Q&A generation was as follows:“First, provide a concise summary of the following abstract that emphasizes its key concepts and hierarchical relationships. Then, based on this summary, generate 10 unique, nuanced Q&A pairs. Focus on creating questions that delve into specialized details of the hierarchical concepts discussed.”The generated queries were used to fetch documents via both aligned and unaligned models. We assessed the ability of each model to correctly retrieve the original document and evaluated the rank/order of retrieval. On average, the unaligned model achieved an MRR of 0.273 and a Recall@10 of 0.415. These metrics represent regular retrieval scores, not hierarchical scores. In contrast, the aligned model significantly improved performance, achieving an MRR of 0.514 and a Recall@10 of 0.731, demonstrating its superior ability to retrieve the correct set of documents.
Furthermore, when integrating RAG with LLAMA-3.1 70B for generating answers from the queries and retrieved documents, the unaligned model produced a ROUGE score of 0.42, while the aligned model achieved a ROUGE score of 0.68. This highlights the impact of alignment on improving the quality and relevance of generated responses.

SECTION: 5Conclusion

In this work, we introduced HEAL, a novel framework for aligning embeddings in RAG systems through hierarchical fuzzy clustering and matrix factorization, integrated within a contrastive learning paradigm. HEAL effectively computes level-specific contrastive losses and applies hierarchical penalties to align embeddings with domain-specific structures, enhancing both retrieval relevance and classification performance. Experimental results across diverse domains — Healthcare, Materials Science, Cybersecurity, and Applied Mathematics — demonstrate HEAL’s capability to significantly improve retrieval accuracy and mitigate hallucinations in LLM-based systems. By bridging hierarchical semantics with contrastive alignment, HEAL establishes itself as a versatile and robust tool for advancing RAG methodologies, enabling more precise, reliable, and domain-adaptive applications of large language models.

SECTION: References