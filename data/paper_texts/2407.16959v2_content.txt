SECTION: Dynamic Graph Transformer with Correlated Spatial-Temporal Positional Encoding
Learning effective representations for Continuous-Time Dynamic Graphs (CTDGs) has garnered significant research interest, largely due to its powerful capabilities in modeling complex interactions between nodes.
A fundamental and crucial requirement for representation learning in CTDGs is the appropriate estimation and preservation of proximity.
However, due to the sparse and evolving characteristics of CTDGs, the spatial-temporal properties inherent in high-order proximity remain largely unexplored.
Despite its importance, this property presents significant challenges due to the computationally intensive nature of personalized interaction intensity estimation and the dynamic attributes of CTDGs.
To this end, we propose a novel Correlated Spatial-Temporal Positional encoding that incorporates a parameter-free personalized interaction intensity estimation under the weak assumption of the Poisson Point Process.
Building on this, we introduce theynamicraphransformer withrelated Spatial-Temporal Positional Encoding (), which efficiently retains the evolving spatial-temporal high-order proximity for effective node representation learning in CTDGs. Extensive experiments on seven small and two large-scale datasets demonstrate the superior performance and scalability of the proposed CorDGT. The code is available at:.

SECTION: Introduction
Graph Neural Networks (GNNs)have become a potent tool for analyzing diverse graph structures due to their effectiveness in learning low-dimensional graph representations. While early GNN research focused on static graphs, many real-world network data exhibit evolving graph structures, such as the World Wide Web and recommendation systems.
The increasing prevalence of dynamic graph data has spurred researchers to adapt GNNs to handle dynamic graphs.
Dynamic graphs can be categorized into Discrete-Time Dynamic Graphs (DTDGs) and Continuous-Time Dynamic Graphs (CTDGs).
Recently, CTDGs have garnered more research attention compared to DTDGs, attributed to their flexible generalization capabilities and proficiency in modeling intricate and dynamic node interactions.
To learn effective representations for CTDGs, researchers have explored various techniques, including temporal message passingand the incorporation of temporal intervals into random walks, etc.

The effectiveness of graph representation fundamentally hinges on the preservation of proximity between nodes.
This implies that nodes that are proximate in the graph should also maintain closeness in the low-dimensional space.
The first-order proximity, represented by the direct interactions between node pairs observed in the CTDG, can be easily maintained by making the embeddings of adjacent nodes close. However, considering the sparsity and the evolving nature of CTDGs, the preservation of first-order proximity alone does not provide a comprehensive measure of proximity.
The high-order proximityin CTDGs, evaluated by the closeness degree between target nodes and auxiliary nodes, embodies the.
Figureillustrates this spatial-temporal duality of high-order proximity in CTDGs using a social network example. Suppose the model is predicting the interaction between usersandat time. As there is not direct interaction between the target nodes, their shared neighbors should be taken into consideration. Unlike the static graphs, although usersandhave three shared neighbors in both subgraphs (a) and (b), they have more frequent and recent interactions with their shared neighbors in subgraph (a) than (b), indicating higher probability of connections between node pairsand.
Although important, most existing methodsindependently aggregate neighbor information of target nodes without considering dependencies between target nodes.
Some existing worksencode the appearance positions of auxiliary nodes on the random walks path, which fail to address the spatial-temporal duality property inherent in high-order proximity.

Despite its paramount importance, estimating and preserving comprehensive spatial-temporal proximity in CTDGs poses several challenges for the following reasons:, CTDGs entail multiple interactions occurring between node pairs at varying timestamps. The, which represents the count of interactions within a specified time interval, is instrumental in characterizing the degree of connection between node pairs. Consequently, measuring higher-order proximity in CTDGs necessitates a personalized intensity estimation between the target node pair and the auxiliary nodes at any given timestamp, a process that is computationally demanding. Several existing worksemploy the Hawkes process and learn node representations to estimate pairwise intensity. Nonetheless, integrating these methods to estimate high-order proximity necessitates pre-training of the node embeddings or neural networks, thereby incurring significant computational costs., the dynamic nature of CTDGs leads to varying proximity between node pairs across different timestamps, influenced by both spatial and temporal factors. This necessitates efficient adaptation in proximity estimation, thereby introducing unique challenges in the modeling of CTDGs.

To address the aforementioned challenges, in this paper, we propose, aynamicraphransformer withrelated Spatial-Temporal Positional Encoding. Inspired by the effectiveness of Poisson process in modeling counting process, we propose a novel Temporal Distance that incorporates a parameter-free interaction intensity estimation by leveraging the weak assumption of the Poisson Point Process. This approach circumvents the computational cost of pretraining, thereby enhancing efficiency. Based on Temporal Distance, we further propose athat models the spatial-temporal duality of evolving high-order proximity in CTDGs. Equipped with the STPE-C, we propose a dynamic graph Transformer that adaptively preserves the comprehensive proximity for effective learning of node embeddings in CTDGs, which are subsequently leveraged for downstream tasks. Consistent performance improvement over eight baselines are observed on both link prediction and node classification tasks, demonstrating the superiority of the proposed CorDGT. Additionally, experiments on large-scale datasets demonstrate the superior effectiveness-efficiency trade-off of proposed CorDGT. In summary, the main contributions of this paper are as follows:

We propose a novel estimation of comprehensive proximity that incorporates an efficient parameter-free personalized intensity to encode the evolving spatial-temporal high-order proximity on CTDGs.

We propose Dynamic Graph Transformer with Correlated Spatial-Temporal Positional Encoding (CorDGT), which efficiently preserves the evolving comprehensive proximity for effective node representation learning in CTDGs.

Extensive experiments conducted on seven small and two large-scale datasets demonstrate the superiority and the scalability of the proposed CorDGT.

SECTION: Related Works
Existing dynamic graphs can be categorized into Discrete-Time Dynamic Graphs (DTDGs) and Continuous-Time Dynamic Graphs (CTDGs) based on whether the timestamps in the dynamic graphs are discrete or continuous. Early works on DTDGs learn the graph representation of each snapshot, which is then fed into a sequential model such as a Recurrent Neural Network or Transformer to learn temporal representation. Some recent works learn the representation of successive snapshots selected by Bernoulli samplingor a sliding window. In contrast, the graph proximity of CTDGs is highly coupled with the time series. Most CTDG models leverage a unified model to learn the node representation via temporal graph neural networks, random walks, or Transformers. However, due to the sparsity and evolving nature of CTDGs, addressing the spatial-temporal property of high-order proximity is important for proximity measurement, which is overlooked by existing works. Different from existing works, the proposed model addresses the spatial-temporal high-order proximity in CTDGs by incorporating the spatial-temporal distance between the target and the auxiliary nodes.

The Temporal Point Process (TPP) is a mathematical model used to represent a sequence of events in time, which has been employed to model the interaction intensity of Continuous-Time Dynamic Graphs (CTDGs). These works have utilized a parametric network or temporal node embeddings to model the interaction intensity of TPPs, such as the Hawkes process. However, the pre-training process of these methods can be time-consuming, making them unsuitable for high-order proximity modeling. Instead, we propose a parameter-free intensity estimation method based on the Poisson Point Process.

Several studies have explored the application of the pure Transformer model in static graph representation learning, wherein graph-specific information is incorporated as a soft inductive bias via positional encodings such as eigenvectors of the graph Laplacian matrix, diagonals of the random walk matrix. pairwise shortest path length. In the context of DTDGs, several works have proposed utilizing the Transformer model to capture the temporal evolution following spatial graph convolution within each graph snapshot. Given the Transformer model’s capability to learn long-term dependencies, it has been adopted for learning on CTDGs. APANemploys the Transformer to model asynchronous mail messages from other temporal neighbors.
Our proposed model differs from these methods in terms of input tokens, positional encodings, and Transformer architecture.

SECTION: Problem Definition
The Continuous-Time Dynamic Graph is defined as a set of interaction events, whereis the number of events, and the tuplerepresents that nodesandinteract at time, andis the feature vector associated with the-th interaction event. The node feature matrix of the CTDG is denoted aswhereis the total number of nodes andis the raw node feature of the-th node. Two nodes may have multiple interactions at different timestamps in a CTDG. The objective of representation learning on CTDG is to learn a embedding function for each nodeat time:. Since most of the CTDG datasets do not have node labels, the CTDG models are typically trained based on the future link prediction task. Future link prediction on CTDGs aims to predict the occurrence probability of the linkbased on all the historical interactions happening before, which can be categorized into transductive and inductive settings based on whether the testing nodes are visible in the training stage.
The network parameters trained based on transductive link prediction can be utilized for other downstream tasks such as node classification.

Given a nodeat time, the collection of its (1-hop) temporal neighbors is defined as the set of nodes that have interaction withbefore:. The collection of-hop () temporal neighbors of the nodeat timecan be recursively defined as all the temporal neighbor of its-hop temporal neighbors, denoted as. The-hop temporal neighborhood of a nodeat timeis defined as.

SECTION: Model
SECTION: Overall Framework
In this section, we introduce the proposedynamicraphrans-former withrelated Spatial-Temporal Positional Encoding().
The general framework of CorDGT is presented in Figure.
Suppose we are predicting the interaction probability of the target nodesandat time, CorDGT begins by sampling theirfrom their-hop temporal neighborhood, denoted asand, respectively. As stated in Section, the spatial-temporal high-order proximity between the target nodesandis characterized by their first-order proximity to these contextual nodes. Therefore, we first introduce Spatial Distance and Temporal Distance to characterize the first-order proximity on the CTDG. Then, the Correlated Spatial-Temporal Positional Encodings (STPE-C) for each contextual nodewith respect to the target node pairsandat timeis proposed to encode the spatial-temporal high-order proximity. Further, the network architecture of CorDGT is presented, which modifies the original Transformer to incorporate structural information of the CTDG.

SECTION: Contextual Nodes Sampling
A simple tree-based sampling strategy is adopted to obtain contextual nodes set. Specifically, given the nodeat time, its contextual nodes set is initialized as. At the first iteration, we uniformly sampletemporal neighbors ofand add them to. At the-th iteration (), we uniformly sampletemporal neighbors for each nodes sampled at theiteration and add them to. Note that the sampling numbers(are predefined hyper-parameters.

SECTION: Correlated Spatial-Temporal Positional Encoding
The spatial distance can be characterized by the shortest path length on the topological structure of the CTDG. However, since the topological structure of the CTDG is continuously evolving as new interaction events happen, online updating the shortest path length between any two nodes on the entire CTDG is time consuming. Alternatively, we use the shortest path length on the sampled K-hop temporal neighborhood as a proxy. Specifically, to compute Spatial Distance (SD) oftoat time, denoted as, we firstly sample K-hop temporal neighborhood from the root node, thenis defined as:

wheredenotes the hop number offrom the root node. Note that if If,is set as 0. The computation of SD is based on the closestto the root node, sincemay occur multiple times in the contextual node set of. Ifis not in, we setas infinity.

In contrast to static graphs, the node pairs may have multiple interactions at different timestamps in the CTDG, thus the proximity between two nodes is associated with their interaction history. We propose Temporal Distance to characterize the proximity originated from the interaction history between two nodes. Suppose current timestamp is, and the timestamps sequence that nodesandinteracted prior tois denoted aswithand. Then, the Temporal Distance (TD) betweenandat time, denoted asis defined as:

whereis an arbitrary function which maps the interaction timestamps sequence to a scalar. Moreover, the temporal distance should satisfy following properties: (Recentness) If the most recent interaction between two nodes is closer to current time, then the temporal distance between them should be smaller; (Intensity) If the interaction intensity of these two nodes is higher, then the temporal distance between them should be smaller. The recentness property can be easily characterized by the difference betweenand.
However, estimating the interaction intensity between two nodes is not straightforward since there is no prior knowledge to its distribution and the interactions patterns of different node pairs may be highly divergent.
Most existing works adopt Hawkes process in modeling the interaction intensity.
However, these methods require to pre-train the node embeddings to obtain the intensity, thus is time-consuming.
Instead, we propose a parameter-free approach to estimate the interaction intensity at any time.
Specifically, we employ the Poisson point process assumption for its simplicity and generalization ability, which is a commonly used weak assumption for an unknown counting process.
We provide following Theoremwith the Poisson point process assumption to evaluate maximum likelihood estimation of interaction intensity given the interaction sequence.

With the Poisson Point Process assumption, the probability of the interaction sequencehappens is:

wheredenotes the number of interactions within the rangeand. Therefore, the likelihood function of the intensitycan be written as:

Thus, the log-likelihood ofis:

By setting the derivative, we get the maximum likelihood estimation of:

which concludes the proof.
∎

Lemmaenables us to characterize the interaction intensity by. By integrating both the intensity property and recentness property, we specify the temporal distance defined in Eq. () as:

whereare hyper-parameters. Note that ifanddo not have interaction before, we setas a very large value. If,is set as 0. For implementation, due to the sparsity of CTDGs, we only need to store the most recent interaction timestampand the interaction times countfor the node pairs that have interactions before, which leads to the memory complexity significantly less than. To prevent the problem of information leakage, when computing the temporal distance in Eq. (), we use the recordedanduntil themini-batch of interactions. After the complete forward propagation of the current mini-batch, we updateandrecords using the interactions of the current mini-batch.

The aforementioned spatial distance and temporal distance encode the direct proximity between two nodes on CTDG in scalars. However, the expressiveness of self-attention will be restricted if we directly use the scalar distance as inputs. In addition, this encoding function should learn the difference of spatial-temporal distance among contextual nodes more effectively. Inspired by, we use the sinusoidal functionas the encoding function:

whereis used to amplify the influence ofon different positions of the encoding, and we setin this work. Further, the Unitary Spatial-Temporal Positional Encoding (STPE-U) of the contextual nodewith respect to the single target nodeat timeis defined as:

wheredenotes the concatenation operation, and MLP denotes Multi-Layer Perceptions.
The above defined STPE-U can characterize the spatial-temporal first-order proximity of the contextual node to the single target. To address the spatial-temporal high-order proximity between the target node pair, the contextual nodesare leveraged as the auxiliary nodes for the target node pair. Specifically, we propose the Correlated Spatial-Temporal Positional Encoding (STPE-C) of the contextual nodeas the combination of its STPE-U to both target nodes:

SECTION: Network Architecture
In this section, we present the network architecture of CorDGT, which is a Transformer based model incorporating the structural information of CTDG. The input of CorDGT is the node embeddings of contextual nodeswhereis the size of contextual nodes set. The node embeddingis the concatenation of raw node feature ofand its STPE-C with respect to the target linkat time.
In addition, some CTDG datasets may provide features associated with interaction events, which contains important semantic information about the correlations between the contextual node pairs. Therefore, we modify the self-attention module of the original Transformer to incorporate the event feature. Specifically, the event feature matrix is defined as, whereis the event feature if nodeandare interacted during contextual node sampling period otherwise a zero vector. The input node embeddingis denoted as. Then, the self-attention module of CorDGT (CorDGTAttn) is defined as:

wheredenotes the-th row of the matrix.,are weight matrices. For simplicity, we setfor the intermediate layers. In addition,in Eq. () is a masking matrix defined as follow:

whereandare the timestamps and hop numbers obtained in contextual node sampling period. This masking matrix ensures that messages can only pass from the history to future, and from farther temporal neighbors to the closer temporal neighbors.

Following the common practice of Transformer models, we adopt the the Layer Normalization (LN)and residual connectionin our CorDGT layer. For easier optimization, we adopt a Pre-Norm architecturewhere the Layer Normalization is applied before CorDGTAttn and Feed-Forwad Networks (FFN). Formally, the CorDGT layer is defined as follows:

whereis the total number of layers. Multi-head self attentioncan also be adopted to further enhance the expressive power of CorDGT. The output of CorDGT layerare the embeddings of contextual nodes. The embedding of the root nodeat time, denoted as, is obtained by applying mean pooling on the node embeddings of its associated contextual nodes:

SECTION: Training Objective
Given the target link, the node embeddingsandcan be computed using Eq. (). Then, the predicted score ofis computed as:

wheredenotes the Sigmoid function.Finally, the Binary Cross Entropy (BCE) loss is adopted to train CorDGT:

wheredenotes a uniform sampling distribution on the node set. The overall training pipeline is in Appendix.

SECTION: Complexity Analysis
In this section, we analyze the time and spatial complexity of the proposed CorDGT model. For time complexity, given a mini-batch of interactions of size, sampling the contextual nodes requires to binary-search the insertion point of the timestamp and costs, whereis the average degree of nodes. Computing the Temporal Distance costscomplexity, whereis the number of contextual nodes. Computing the Spatial Distance costscomplexity whereis the maximum hop of contextual nodes. Forwarding the model costsdue to self-attention operation, whereis the number of attention heads andis the hidden dimension of weights. Therefore, the time complexity of training CorDGT is. For spatial complexity, storing the statistics of interactions cost, whereis the total number of interactions. This spatial complexity is inevitable for learning CTDG models due to the CTDG data loading. The complexity comparison with other methods is presented in Appendix.

SECTION: Experiments
SECTION: Experimental setup
We evaluate the proposed model on nine Continuous-Time Dynamic Graph (CTDG) datasets: Reddit, Wikipedia, LastFM, UCI, Enron, Social Evolution, Flights, Gowalla-Food, and Gowalla-Outdoors. Among these, Reddit and Wikipedia constitute bipartite networks abundant in node/edge attributes, while LastFM represents a bipartite network devoid of node features. UCI, Enron, Social Evolution, and Flights are non-bipartite communication networks, also lacking attributes. Gowalla-Food and Gowalla-Outdoors are two large-scale datasets derived from the primary Gowalla dataset. For datasets without meaningful node features (i.e., LastFM, UCI, and Enron), we employ zero vectors as node features. Further details regarding these datasets are provided in Appendix.

We compare the proposed CorDGT with three types of CTDG models: (1) GNN-based: DyRep, TGAT, TGNand Graphmixer. (2) Random walk based: CTDNEand CAW. (3) Transformer based: TCLand TGSRec. More introductions about the baseline methods and tuned hyper-parameters are presented in Appendix.

Our evaluation protocols closely follow. In specific, we adopt transductive/inductive link prediction and dynamic node classification tasks for evaluation. Forlink prediction task, we split the total time rangeinto three seperate intervals,andwithandfixed. Then, we allocate the interactions happening within each interval to generate the training, validation and testing set. The inductive link prediction task follows the same splitting protocol as the transductive experiments. However, we randomly select 10% of the nodes as ”masking nodes”, excluding any links associated with them in the training set, and removing any links not associated with them in the validation and testing sets.

We train all the models for 50 epochs and adopt the early stopping strategies. We adopt Adam optimizer and learning rate of 0.001 for all the tasks. Early stopping strategy is adopted. The batch size is set as 100. We sample 2-hop temporal neighbors for all datasets. More details about hyper-parameter of CorDGT and other baselines are presented in Appendix.

SECTION: Results and Discussion
The Average Precision (AP) scores of transductive and inductive link prediction experiments are presented in Table. The Area Under the receiver operating Characteristic (AUC) results are presented in Appendix.
As can be seen from Table, our CorDGT achieves the best AP performance on all the datasets for both transductive and inductive settings. Specifically, the transductive AP and inductive AP of CorDGT show an average improvement of 1.95% and 3.21%, respectively, demonstrating the effectiveness of CorDGT.

In addition, we make the following observations: (1) Our proposed model demonstrates robust performance across both attributed networks (Reddit and Wikipedia) and non-attributed datasets (LastFM, UCI, MOOC, and UCI). In contrast, the performance of several baseline methods, which lack node encodings designed for the evolving laws of CTDGs (such as Jodie, DyRep, and TGAT), significantly decreases on non-attributed datasets. This suggests the efficacy of our proposed STPE-C in modeling the evolving nature of CTDGs. (2) On the LastFM and Social Evolution datasets, our model improves the inductive Average Precision (AP) by 6.71% and 9.52% over the strongest baseline, respectively. This may be attributed to the significantly higher average interaction intensity of the Social Evolution () and LastFM () datasets compared to others. As such, the intensity may play a more crucial role in proximity estimation. Our proposed model excels at capturing long-term interaction intensity via temporal distance. (3) When compared to the parametric intensity-based method (DyRep), our proposed model displays significantly improved performance. This improvement can be attributed to the incorporation of parametric-free intensity into high-order proximity encoding. The results of node classification is presented in Appendix.

SECTION: Ablation Studies
In this subsection, we conduct ablation studies to evaluate the effectiveness of different modules of CorDGT . The inductive AP and AUC results are shown in Table. The results are analyzed as follows: (1) In Ablations 1 and 2, we eliminate the recentness term and intensity term from the temporal distance calculation, respectively. As evidenced by the results, the removal of either term leads to a performance degradation across all datasets, especially on LastFM. This demonstrate the significance of both recentness and intensity in computing temporal distance. (2) In Ablations 3 and 4, we exclude the spatial distance and temporal distance from the STPE-C component, respectively. The results indicate that the performance across all datasets, particularly non-attributed ones, is compromised when either spatial distance or temporal distance is removed. This highlights the importance of modeling both spatial and temporal proximity in learning on CTDGs. (3) In Ablation 5, we substitute the binary STPE-C with STPE-U as defined in Eq.. In this scenario, the high-order proximity between the target nodes fails to be captured. We observe a significant drop in performance across all datasets, underscoring the critical role of modeling spatial-temporal high-order proximity in learning the evolving patterns of CTDGs. (4) In Ablation 6, we remove the masking matrix utilized in the Transformer model, which also leads to a performance decline across all datasets. (5) In Ablation 7, we replace the uniform contextual nodes sampling with the most recent sampling (i.e., the most recent interacted neighbors are sampled as the contextual nodes). The performance of CorDGT drops on all four datasets. The reason may be that the diversity of neighbors decrease when replaced with most recent sampling strategy, thus the high-order proximity estimation may be less accurate.

SECTION: Scalability Analysis
In this section, we evaluate the performance and efficiency of the proposed CorDGT on large-scale datasets. We adoptandsubsets from the large Gowalladataset for evaluation. The Outdoors dataset contains around 0.22M nodes and 1.19M edges. The Food dataset has around 0.67M nodes and 2.71M edges. We run all the models for one epoch and compare the performance. The inductive AP metrics and the training speed per epoch of the Food dataset are presented in Figure. The results of Outdoors are presented in Appendix.

As can be seen from Figure, both two configurations of CorDGT can achieve consistently outperform the baselines in terms of inductive AP. In addition, training one epoch of CorDGTwith 10 contextual neighbors takes 3319 seconds, which is significantly faster than memory-based models (JODIE, DyRep and TGN). It is because the memory-based models need to store the memory state for each node. Given the vast number of nodes in large-scale datasets, the process of storing and managing the memory state incurs a considerable computational cost. Our proposed CorDGT is marginally slower than CAWN (3234 seconds), but has significant better inductive AP performance.

SECTION: Visualization
One motivation of the proposed CorDGT is to capture the spatial-temporal high-order proximity in CTDGs by considering the distance of the contextual nodes to both ends of target nodes.
To further demonstrate the interpretability of the proposed CorDGT, we visualize the link prediction score by the contextual nodes with different Temporal Distance (TD) and Spatial Distance (SD) to the target nodes.
Specifically, we replace Eq. () as:

whereis a trainable linear projector, and train the model.
In this way, the contribution of each contextual nodeto the link prediction score can be decomposed as.
After training, we randomly select a mini-batch of contextual nodes. To see the influence of temporal distance to the predicted score, we evenly split the range of TD to target node pairs in this mini-batch to 5 groups, which formulates total 25 buckets. Then, we allocate the contextual nodes into these buckets based on their TD to target node pairs. Similarly, we can allocate the contextual nodes into different buckets based on their SD to target node pairs. Finally, we compute the average prediction score of the contextual nodes in each bucket. The visualization results on UCI and Enron are shown in Figure. As can be seen from Figure, the contextual nodes have closer temporal distance (closer to top-left corner) to both ends of the target link will give higher prediction score of the link (closer to yellow). This indicates that the proposed CorDGT will give higher prediction if the contextual nodes have smaller temporal distance to target node pairs. Similar observations are also seen from spatial distance. Therefore, the proposed CorDGT may be capable to capture the spatial-temporal high-order proximity.

SECTION: Conclusions
This paper introduces the Dynamic Graph Transformer with Correlated Spatial-Temporal Positional Encoding (CorDGT), a novel approach for representation learning on Continuous-Time Dynamic Graphs (CTDGs). We employ the Poisson Point Process assumption and sampled temporal neighborhood to achieve comprehensive proximity estimation on CTDGs. Subsequently, we propose Correlated Spatial-Temporal Positional Encodings (STPE-C), which utilizes the comprehensive proximity to capture spatial-temporal high-order proximity. Extensive experiments conducted on seven small and two large-scale datasets demonstrate the performance superiority and scalability of the proposed CorDGT model. A potential future direction for this work could involve designing more sophisticated spatial-temporal distances for improved proximity estimation and preservation.

SECTION: ACKNOWLEDGMENTS
This work is supported by the National Natural Science Foundation of China (62476244), Zhejiang Provincial Natural Science Foundation of China (Grant No: LTGG23F030005), National Natural Science Foundation of China (62372399,62476245) and the advanced computing resources provided by the Supercomputing Center of Hangzhou City University.

SECTION: Ethical Considerations
The proposed CorDGT is used to learn the temporal embeddings which can be leveraged for downstream tasks such as link prediction and node classification. The direct negative societal effects of this research, encompassing fairness, privacy, and security considerations, are minimal. Nevertheless, akin to other predictive models, a few erroneous predictions by the model could impact system functionality. Despite extensive experimental validation of the model’s efficacy, occasional inaccurate predictions, particularly on outlier data, remain plausible. Therefore, enhancing data quality through measures such as data cleaning prior to model application is advised.

SECTION: References
SECTION: Overall Algorithm of CorDGT
SECTION: More discussion on other Stochastic process
The intensity is the number of happening times within a certain time interval of a counting process. Therefore, non-counting processes, such as Gaussian process and Wiener process, are not applicable for intensity estimation. Poisson process is the most well-known and commonly used counting process. It assumes the intensity is a constant, which provides great mathematical tractability and computation efficiency. Most of other counting processes are generalized from Poisson processes, such as non-homogeneous Poisson process, Hawkes process and Markovian arrival process, which includes more undetermined parameters for flexibility. Estimating the parameters of these process requires significantly higher computation budget than Poisson process.

SECTION: Complexity Comparison with existing models
In Section 4, we analyze the computation complexity of CorDGT . In this section, we also analyze the computation complexity of two baseline methods, i.e., TGATand TGN. For these models, given a mini-batch of interactions of size, the historical neighbor sampling process costs, whererepresents the average degree of nodes. The forward process, costswheredenote the number of sampled neighbors, attention heads, and hidden dimensions, respectively. Thus, the overall time complexity of TGAT and TGN is. The size of network parameters is denoted as. In addition, TGN requires a memory and costs the spatial complexity ofwhereanddenote the number of nodes and hidden dimension of the memory. We summarize the time and space complexity in Table.

SECTION: Experimental Setting
SECTION: Datasets
Our experiments section includes seven public datasets: Reddit, Wikipedia, UCI, LastFM, Enron, Social Evolutionand Flights. Reddit network is an user action datasets which consists of subreddits posted by different users in one month on Reddit website. It is a bipartite dataset consisting of 10000 most active users and 984 subreddits with rich interaction feature provided. Wikipedia network records the clicking actions on wikipedia pages by different users. It is a bipartite network consisted by clicking actions on 1000 pages in one month made by users with rich interaction feature provided. UCI network is non-bipartite network which contains sent messages between the users of an online community of students from the University of California, Irvine. The nodes represent students and the edges represent the communicated messages among them. Enron is a non-bipartite dataset which consists of approximately 0.5M emails that were exchanged between employees of the Enron energy company over a span of three years. Social Evolution is a mobile phone proximity network which tracks the everyday life of a whole undergraduate dormitory from October 2008 to May 2009. Flights is a directed dynamic flight network illustrating the development of the air traffic during the COVID-19 pandemic, which was extracted and cleaned for the purpose of this study. Each node represents an airport and each edge is a tracked flight. The edge weights specify the number of flights between two given airports in a day. In addition, we select the large-scale Gowalladataset for scalability evaluation. Gowalla is a social network for users check-ins at various locations, containing about 36 million check-ins made by 0.32 million users over 2.8 million locations. These check-in records are in the time span of Jan 2009 - June 2011.
The locations are grouped into 7 main fields. We select a subset of Outdoors and Food field for experiments. Specifically, we choose the part of the Outdoors data from Jan. 2009 to Dec. 2010 and the part of the Food data from Jan. 2011 to June 2011.
Detailed statistics of aforementioned datasets are presented in Table.

SECTION: Baselines
The brief introduction of baseline methods in the Experiments section are as follows:

CTDNE extends DeepWalkto dynamic graphs which leverages a SkipGram model on the temporal random walk sequence and learn the node embeddings.

Jodie updates node embeddings in an interaction via two coupled RNNs, which are leveraged for future link prediction via a temporal projector.

DyRep updates the node embeddings involved in an interaction by a recurrent model considering the messages from 2-hop temporal neighbors.

TGAT extends GATand GraphSAGEto dynamic graphs, which samples and recursively aggregates the messages of k-hop temporal neighbors. The temporal representation is obtained by Fourier transformation on the time interval.

TGN proposes a generalized message-passing networks by extending Jodie and TGAT with a per-node memory mechanism for long-time interactions.

TGSRec proposes a Temporal Collaborative Transformer which simultaneously captures the collaborative signals from users and items as well as temporal dynamics.

proposes a dynamic-graph-topology-aware Transformer with a two-stream encoder for semantic inter-dependency modeling. Contrastive learning is adopted to maximize mutual information between future interaction nodes.

CAWN samples temporal random walks andanonymize the node identities via Causal Anonymous Walks (CAW). The node encodings on temporal random walks are learned via a sequential model.

Graphmixer proposes a conceptually simple architecture that leverage MLP and mean-pooling to aggregate the temporal information and node features of K most recently interacted neighbors. Note that the original Graphmixer leverages one-hot node encoding as input, thus can not be applied for inductive experiment. In this work, we replace the one-hot encodings as the node encodings used by other baselines for fair comparison.

SECTION: Hyper-parameters Tuning
For all the baselines, we set the dimension of time encoding and hidden unit as 100 and 172, respectively. For Jodie, DyRep and TGN, we adopt the implementationfor evaluation. In specific, we set the memory dimension as 32 (for UCI) or 172 (for Reddit, Wikipedia and LastFM). We adopt a one-layer model with 10 temporal neighbors being sampled. The official implementations of TGATand TGSRecare adopted. The number of layers, attention heads and the sampled temporal neighbors are set as 2, 2 and 20, respective.

For CAWN, we adopt its official implementation. We grid search following hyper-parameters: the time scaling factor is set as, the random walk length inand the number of walks in.

For Graphmixer, we adopt the original implementation. The time gap is set as 2000. The number of MLP-Mixer layers is set as 2. For TCL, we adopt the implementation of DyGLib. The number of layers, attention heads and the sampled temporal neighbors are set as 2, 2 and 20, respective.

SECTION: Implementation details of CorDGT
For fair comparison, we train all the models for 50 epochs with early stopping executed if there is no improvement on validation AP for 3 epochs. In addition, we use the batch size of 100 for all models. We repeat the methods for 3 runs and report the mean and standard deviation of statistics. For the proposed CorDGT, we set the learning rate as 0.001 and optimizer as Adam for all datasets. The attention heads, layer number and hidden dimension of main CorDGT encoder is set as 6, 2, 64, respectively, for all datasets. We sample 2-hop temporal neighbors for all datasets. For LastFM and UCI, the sampling numbers of contextual neighbors are {32,1}, and for other datasets, the sampling numbers of contextual neighbors are {20,1}. We setandfor UCI andandfor LastFM. We setandfor other datasets. We set the dimension of Correlated Spatial-Temporal Positional Encoding as 200, where the dimension of Spatial Distance encoding and Temporal distance is set as 100 and 100, respectively. All the experiments are run on a Linux Ubuntu 18.04 Server with a NVIDIA RTX2080Ti GPU.

SECTION: Additional Experimental results
SECTION: Node classification results
The experiments settings of node classification are as follows. We initially train a model using the transductive link prediction task. Subsequently, we load the trained model and freeze its parameters, then append a classifier on top of it for the purpose of classification. The AUC results of dynamic node classification are presented in Table. Note that we replace STPE-C with STPE-U as the positional encodings, since the node classification task only considers single node rather than interaction. Our CorDGT achieves the best performance on both Wikipedia and Reddit datasets compared with other baselines.

SECTION: AUC results of link prediction
The AUC results of link prediction are presented in Table. As can be seen, our proposed CorDGT consistently outperforms other baselines on both transductive and inductive experiments.

SECTION: Scalability on Gowalla Outdoors
We further evaluate the scalability of the proposed CorDGT on Gowalla-Food datasets. We train all the models one epoch. The inductive AP and training time per epoch results are presented in Figure. We observe that CorDGT with 10 or 20 contextual nodes can obtain the highest inductive AP among baselines. In addition, the training speed of CorDGT with 10 contextual nodes (1511 seconds per epoch) is significantly faster than TGN (5046 seconds per epoch) on Food datasets. This result demonstrate the scalability of the proposed CorDGT .

SECTION: Parameters Sensitivity
In this subsection, we evaluate the sensitivity of the proposed CorDGT with respect to some key hyperparameters. The results are illustrated in Figure, and the observations are as follows:
(1) Equation () employs coefficientsandto balance recentness and intensity. We evaluate the influence ofandon UCI and Wikipedia. As shown in Figure(a), for UCI dataset, varying settings ofandminimally influence the performance of CorDGT. On the other hand, for Enron dataset (Figure(b)), the smaller() and larger() enhance the performance of CorDGT.
(2) We also investigate the influence of sampled 1-hop contextual node numbers on UCI and Enron datasets, as depicted in Figure(c). On the UCI dataset, the model’s performance improves with an increasing number of sampled contextual neighbors. In contrast, on the Enron dataset, the model achieves optimal performance with 20 sampled neighbors, possibly due to Enron’s smaller node number.
(3) Furthermore, we explore the impact of different batch sizes on UCI and Enron datasets, as shown in Figure(d). The model’s performance remains stable on UCI. However, on the Enron dataset, performance generally deteriorates with increasing sampled neighbor numbers. This discrepancy may stem from Enron’s higher average interaction intensity, suggesting that more frequent updates on recent interactions and counts could benefit proximity evaluation.