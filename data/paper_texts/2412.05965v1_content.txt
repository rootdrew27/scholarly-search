SECTION: Quasi-Optimal Least Squares: Inhomogeneous boundary conditions, and application with machine learning
We construct least squares formulations of PDEs with inhomogeneous essential boundary conditions, where boundary residuals are not measured in unpractical fractional Sobolev norms, but which formulations nevertheless are shown to yield a quasi-best approximations from the employed trial spaces. Dual norms do enter the least-squares functional, so that solving the least squares problem amounts to solving a saddle point or minimax problem. For finite element applications we construct uniformly stable finite element pairs, whereas for Machine Learning applications we employ adversarial networks.

SECTION: Introduction
This paper is about Least Squares discretizations of boundary value problems (BVPs). A comprehensive monograph on this topic is. In an abstract framework we consider variational formulations of BVPs in the form, where for some Hilbert spacesand,is a linear operatorfor whichis a norm onthat is equivalent to. In particular,is injective, butnotnecessarily surjective.

Given a closedlinear subspace, typically of finite element type, from an (infinite) familyof such linear subspaces,

is thebestapproximation tofromw.r.t., and so aquasi-bestapproximation w.r.t..

Not each BVP can be formulated in above form with an evaluable normas whenis an-type space.
Such formulations are scarce in particular for the case of having essential inhomogeneous boundary conditions. The only exception we are aware of isfor an inhomogeneous Robin boundary condition.
In general,is a product of spaces, some with and others without evaluable dual norms. Below we describe the approach to deal with the non-evaluable norms from our recent work.

SECTION: Approach from
It suffices to consider the case that, and soand, wherecan andcannot be evaluated.
Then, for a sufficiently large closedlinear subspace, typically of finite element type, in the Least Squares minimization overone replaces the normby the discretized dual-norm.
Assuming the pairsatisfies an (uniform) inf-sup or LBB condition, the resultingis still aquasi-bestapproximation tofrom.
Thiscan be computed as the 2nd component of the pairthat solves the saddle-point system

A problem, however, arises when bothandcannot be evaluated, as whenis a fractional Sobolev space. Such spaces naturally arise with the imposition of inhomogeneous essential boundary conditions.
Forbeing such thatis uniformly equivalent to, a solution is given byreplacingin above saddle-point system by.
The resulting solutionis then stillquasi-best, and by eliminating, it can be computed as the solution of the symmetric positive definite system

We will refer toas apreconditionerfor.
Such preconditioners, whose application, moreover, can be performed in linear time, are available for fractional Sobolev spaces of positive and negative order. It is, however, fair to say that their implementation is demanding. A related first work in this direction was.

SECTION: Current work
Here we propose a different approach to deal with Sobolev spaces on the boundary with smoothness indices.
It is based on the observation that for a domain, andbeing the normal trace operator,

withdenoting the standard trace operator.

Withequipped with the graph norm, an alternative for () is

Although not attractive for finite element computations, the use of the smaller scalar spaceinstead of the vectorialshows to be beneficial in machine learning applications.

By applying these alternative expressions () (or ()) and () for the Sobolev norms with smoothness indicesand, for both 2nd order elliptic equations and the stationary Stokes equations on a domain, we obtain variational formulations of the form, withequivalent to, where bothandare products ofmerely functions spaces on, which are either Sobolev spaces with smoothness indices in, or are equal to. In the case of mixed boundary conditions some of these spaces are restricted by the incorporation of homogeneous boundary conditions on part of the boundary.
For either mixed or non-mixed boundary conditions, the operatorwill not be surjective, which for least squares discretizations does not hurt assuming.

For finite element spaces, and those factorsof the product spacethat are not-spaces, we will construct finite element spacessuch thatsatisfies the required LBB stability, so thatcan be replaced bywhilst maintainingquasi-optimalityof the obtained Least Squares approximation.

Compared to our earlier work, the advantage of this approach is that it does not require the application of preconditioners for fractional Sobolev norms on the boundary.

SECTION: Application with Machine Learning
The approximation of the solution of a BVP using Neural Nets requires its formulation as a minimization problem.
Forsymmetric positive definite problemsa possibility is to use theEnergy functional, whereas ageneral applicableapproach is to use aLeast Squares functional. The imposition of essential boundary conditions, in particular inhomogeneous ones causes problems.

Let us illustrate this by considering the simple model problem

where with Least Squares methods non-symmetric first order terms can be added.
The solutionof the above model problem is the minimizer overof theEnergy functional.

When, one can approximateby the minimizerof this functional over, whereis asetof Neural Net functions, andis a, preferably smooth function, with.
Thisis the best approximation tofromw.r.t..
For general domains, the construction ofis, however, not obvious.
Seefor an extensive discussion.
Moreover, for non-smooththe best approximation error fromcan be significantly larger than that from(cf.).

For, one can minimize the Energy functional over, whereis an (approximate) extension ofcomputed by transfinite interpolation (see), or by using a second neural net (). The appropriate norm for controllingis, which is, however, difficult to implement and therefore never used.

Instead of incorporating the essential boundary condition in the trial space, following, one may approximately enforce it by minimizing overthe modified Energy functional, whereis some empirically chosen constant. The method is known as theDeep Ritz method. Because of the use of the practical feasible-norm instead of the mathematically correct-norm, generally it will not produce a quasi-best approximation to the solution from.

Following, another way of imposing the essential boundary condition is by adding a penalty term to the Energy functional. Then for someone minimizes

over. This penalty method is called theDeep Nitsche method().
Based on direct and inverse inequalities, for a finite element trial space an appropriate local choice foris a sufficiently large constant times the reciprocal of the local mesh-size. For Neural Nets some empirically found global constantis applied. In that case, however, it cannot be expected that an appropriate, solution-independent choice for this penalty parameter exists.

ExistingLeast Squaresbased Neural Net approximations include minimization overofas with thePhysics Informed Neural Network(PINN) method.

The weak solution of Poisson’s problem minimizesover. Invoking a second set of Neural Net functions, this leads to theWeak Adversarial Network(WAN) methodof computing

Because of the use of the-norm in which the boundary residual is measured, both PINN and WAN are not guaranteed to yield quasi-best approximations from the employed set.

Ina first order system formulation is employed. Setting, the pairis the minimizer overof. For a set of Neural Net functions, one approach would be to minimize this functional overanalogously to the method fromdiscussed above, and with similar difficulties. Alternatively, one can minimize

over, which would give a quasi-best approximation fromtoin the norm on.
As noted in, however, forthe computation of the fractional norm is unfeasible, and the numerical experiments are restricted to the one-dimensional case. In, in a slightly different context, it is proposed to approximately compute the Sobolev-Slobodeckij fractional norm, which because of the singular kernel is difficult and in any case expensive.

A recent work on the use in Machine Learning of well-posed first order system formulations for homogeneous boundary conditions is.

TheQuasi-Optimal Least Squares(QOLS) method that is introduced in this work solves the problem to correctly impose essential (inhomogeneous) boundary conditions.
It measures the residuals of the PDE and the boundary conditions in correct norms, so that residual minimization is equivalent to error minimization, whereas it avoids the use of the non-evaluable fractional Sobolev norms. Notice that the approach of replacing such norms on test spaces by equivalent ones defined in terms of preconditioners is restricted tolineartest spaces, and so does not apply in the Machine Learning setting.

For the model problem in a first order formulation, givensetsof Neural Net functionsand, the QOLS method computes

The computed minimum will be shown to be aquasi-bestapproximation tofromw.r.t. the norm onunder the condition thatis sufficiently large in relation to, akin to the LBB condition for linear trial- and test-spaces.

The QOLS method can also be applied to the second order weak formulation. In that case, for, and, it computes

Again, forsufficiently large, only dependent on, the computed minimum is aquasi-bestapproximation tofromin the norm on.
For larger, this second order formulation has the advantage that no spaces of-dimensional vector fields enter. On the other hand, one needs to construct
a functionwithto obtain test functions. Notice that a reduced approximation property as a consequence of the multiplication withis irrelevant at the test side.

SECTION: Organization
In Sect., in an abstract setting we discuss least squares principles for the numerical approximation fromlinear subspacesof the solution of a well-posed operator equation. In particular, we discuss the treatment of dual norms in a least squares functional.

In Sect.we present well-posed formulations of the model elliptic second order boundary value problem on a domainwith inhomogeneous boundary conditions.

The operators corresponding to the formulations from Sect.map into a product of Hilbert spaces, some of which being fractional Sobolev spaces on the boundary of.
In Sect., well-posed modified formulations are constructed where all arising Hilbert spaces are integer order Sobolev spaces on, or duals of those.
Because of the presence of dual spaces, least-squares discretizations lead to saddle-point problems. The necessary uniform inf-sup stability conditions are verified for pairs of finite element spaces.

The least squares approach is not restricted to the model second order problem, and in Sect.+ Appendixwe consider the application to the stationary Stokes equations.

In Sect.we consider the discretisation of least squares formulations by(deep) neural nets. Viewing a saddle-point problem as a minimax problem we employ adversarial networks. We derive a sufficient condition on the adversarial ‘dual’ network for obtaining a quasi-best approximation from the ‘primal’ network.

In Sect.we present numerical results for finite element and neural network discretizations, the latter in comparison with familiar neural net discretizations of PDEs.
Conclusions are formulated in Sect..

SECTION: Notations
With the notation, we mean thatcan be bounded by a multiple of, independently of parameters whichandmay depend on, as the discretization index.
Obviously,is defined as, andasand.

For Hilbert spacesand,will denote the space of bounded linear mappingsendowed with the operator norm. The subset of invertible operators in, thus with inverses in,
will be denoted as. The subset of operatorsfor whichdefines a scalar product onwill be denoted by.

SECTION: Least squares principles in an abstract setting
SECTION: Continuous problem
For some Hilbert spacesand, letbe a homeomorphism with its range, i.e.

or, equivalently,

Notice thatis injective, butnotnecessarily surjective.
Equippingwith norm, we have.

Given, consider the Least Squares problem of finding

Necessarily, thissolves the corresponding Euler-Lagrange equations

meaning thatis the-orthogonal projection ofonto, and therefore.
Wheneverhas a solution, i.e.,, it is the unique solution of (), and.

SECTION: Discretization
Forfrom some (infinite) index set, letbe a closed, e.g. finite dimensionallinear subspace, typically of finite element type. Then

is thebestapproximation tofromw.r.t..
Thisis computable when(and) is evaluable, as whenis an-space.

Variational formulations of PDEs in the operator form, where () holdsandis evaluable are relatively scarce, and in any case they are not available in the case of inhomogeneous essential boundary conditions.

In general,is a product of Hilbert spaces some of them with, and others without evaluable norms. To analyze this situation, it suffices to consider the case that,and so, and, where

Given a closed linear subspacethat is sufficiently large such that

wereplacethe non-computable Least Squares approximation () by

which, as we will see,iscomputable. For the latter, we will assume that, or a uniformly equivalent norm, is evaluable.

As is well-known, an inf-sup condition like () relates to existence of a ‘Fortin interpolator’. The following formulation of this relation does not require injectivity ofwhich is not guaranteed. It is used that, being a consequence of.

For datum in the range of the operator, next we show that if, thenis aquasi-bestapproximation to.

Using Footnote, one infers that

With, it holds that

Withdefined by(,), we have

so thatis the solution inof

From, we have, and we conclude thatis a projector onto.
By substituting, we infer that

and so

From,, we have(), so that

SECTION: Implementation
With the Riesz’ liftdefined by

an equivalent expression fordefined in () is

and so it solves the corresponding Euler-Lagrange equations

One may verify that

Introducing,is the 2nd component of the pairthat solves thesaddle-point problem

To solve () efficiently using an iterative method, one needs ‘uniform’ preconditioners for the ‘upper-left block’ and the Schur complement equation, which is ().
So one needs

withuniformly bounded norms, anduniformly bounded norms of their inverses,
and whose applications can be efficiently computed, preferablyin linear time.
The last requirement relates to the basis that is applied onor, since constructing a preconditioner amounts to constructing an approximate inverse of the stiffness matrix corresponding toor.
In our applications preconditioners that satisfy these requirements will be available.

Having such, a most likely even more efficient strategy is toreplacein (), or, equivalently, in (), the scalar productonby, and to solve the resultingfrom the Schur complement equation

What is more, when bothandare not evaluable, as whenis a fractional Sobolev space, then this is theonly strategythat leads to a computable quasi-best least squares approximation.

By comparing () with (), notice that forequipped with scalar product, the
Riesz’ liftis the operator.

Thesymmetric positive definite system() can be efficiently solved using Preconditioned Conjugate Gradients with preconditioner.

SECTION: A posteriori error estimation
An obvious modification ofthat takes into account that in the current workis not necessarily surjective shows the following result.

Ifand, then by takingsuch, we conclude that.
The oscillation term is, however, not computable.

As we will see, in our applicationscan be chosen such that it allows for the construction of Fortin interpolators
that are both uniformly bounded, and for which, for sufficiently smooth,
data oscillation is of ahigher orderthan what, in view of the order of, can be expected for the best approximation error. In that case the estimatoris thus not only efficient, but in any case asymptotically also reliable.

Forbeing the solution of (), one has available() as well as the preconditioned residual, so thatcan be computed efficiently.

Propositionis also applicable withreading as the Riesz’ liftfrom () (cf. Remark), in which case.
This case is relevant whenis evaluable, andis computed by solving the saddle-point system (). Then the quantityis available as, and so. In applications this (squared) error estimator splits into a sum of local error indicators which suggests an adaptive refinement procedure.

SECTION: Application
SECTION: Model elliptic second order boundary value problem
On a bounded Lipschitz domain, where, and closed, withand, consider the following elliptic second order boundary value problem

where, andsatisfies().
We assume that the matrix, and the first order operatorare such that

SECTION: Well-posed first order system reformulations
We consider two consistent first order system formulations–of ().From the formulations given, one easily derives the expressions for the operator ‘’, solution ‘’, right-hand side ‘’, and spaces ‘’ and ‘’. Implicitly we will assume that the data,, andare such that ‘’ is in dual of ‘’.

() 
Findsuch that

whereandare the trace or normal trace operators onand, respectively,is the interpolation space,, where.
The dual ofis denoted by.

() 
Findsuch that

Inboth formulations have been shown to bewell-posedin the strong sense that ‘’ isboundedly invertiblebetween ‘’ and the dual of ‘’, which is () together with surjectivity of ‘’.

Although not the focus of this work, we mention that forhomogeneous(essential) boundary conditions simplified formulations can be applied.
In, replacebyand omit the last two equations; and in, replacebyand omit the last equation.

Formulationhas the advantage that both ‘field residuals’ are measured in-spaces. A disadvantage is that it requires that, whereasis allowed in.
Datumis, however, covered when it is given as

for someand. In that case, replace the two equationsandbyand.

Anycan be written in the form (), but in general it requires solving a PDE to find such a decomposition. In a finite element setting, an alternative approach is to replaceinby a suitable projection onto the space of piecewise polynomials. It was shown that the then resulting solutionis still quasi-optimal
(seefor the lowest order case, andfor the extension to general orders).

The key to derive well-posedness of–in the aforementioned strong sense was the following abstract lemma.

This lemma shows that it suffices to find a surjective trace operator that corresponds to the essential boundary conditions, and to show well-posedness of the problem with homogeneous essential boundary conditions.

Inor, the spacereads asor as.
To apply to–the Least Squares discretisation ()/(), or the modified one (),
for those factorsinthat are unequal to-spaces, one has to select test spacesfor which(), being the (uniform) inf-sup stability condition ().

Furthermore, (uniform) preconditioners in, and for aforementioned, inhave to be found, preferably of linear computational complexity. Ifis a fractional Sobolev space, then having such a (uniform) preconditioner is evenindispensibleto circumvent the evaluation of the fractional norm (see the paragraph preceding Theorem).

For both formulations–, intrial- and test-spaces of finite element type that give (uniform) inf-sup stability have been constructed, and suitable preconditioners are known.

The construction of test-spaces on the boundary, and the implementation of preconditioners for fractional Sobolev spaces on the boundary require quite some effort.
Therefore, in the following subsection we present modified formulations, that are well-posed in the sense of (), in which all function spaces on the boundary, specifically fractional Sobolev spaces, disappeared.
Although the operators ‘’ will not be surjective anymore, recall that for consistent data Least Squares discretisations yield quasi-optimal approximations.

SECTION: Avoidance of fractional Sobolev norms
SECTION: Modified first order formulations in terms of field variables only
With the trace operator, it is known that

From, it follows that on,

Similarly, for the normal trace operator, it holds that

see e.g.(there withdenoted by).
Recalling that,
it follows that that on,

From the well-posedness of, and () and (), we conclude that following modified formulationis well-posed in the sense that the corresponding operatoris a homeomorphism with its range, i.e., () is valid.

() 
Findsuch that

Knowing that the operator corresponding tois surjective, the range of the current operator is.

A subset of the above arguments applied to the formulationshows that following modified formulationis well-posed in the sense that the corresponding operator is a homeomorphism with its range, i.e., satisfies ().

() 
Findsuch that

The range of the corresponding operator is.

Although not very relevant for finite element computations, for the application with machine learning, in particular whenis a higher dimensional space, we give the following alternative for (). It avoids the introduction of the spaceof-dimensional vector fields, at the expense of requiring smoother test functions.

It holds that. Given, defineby

Then

Sincevanishes on, () shows that, so that.
Furthermore,

orand, and so. We conclude that.

Conversely, for arbitraryand,, or.
∎

Consequently, inand, the equationincan alternatively be replaced byin. This will be applied in Sect..

SECTION: Necessary inf-sup conditions
To apply toor tothe Least Squares discretisation ()/() or (),
one has to realize the following (uniform) inf-sup conditions.

Forand:
Given a family of trial spaces, we need a corresponding family of test spaceswith

For: Given, we needwith

For: Given, we needwith

In the following subsections, for the three cases–uniformly inf-sup stable pairs of finite element trial- and test spaces will be constructed.
Forand, the test spacesandwill be finite element spaces w.r.t. conforming partitions ofwhose intersections withorcoincide with the underlying partition of the trial space, but which are maximally coarsened when moving away fromor, respectively. Although such highly non-uniform partitions give the smallest appropriate test spaces, clearly for convenience one may apply larger test spaces without jeopardizing the inf-sup stability.

Dependent on the formulation and the type of boundary condition (Dirichlet, Neumann, or mixed), an efficient iterative solution of the resulting Least Squares discretisations requires preconditioners for stiffness matrices of trial- or test- finite element spaces w.r.t. scalar products on,,, and. Such preconditioners of linear complexity are available.

SECTION: Verification of inf-sup condition
Letbe a polytope,be a conforming, (uniformly) shape regular partition ofinto (closed)-simplices, and letdenote the set of (closed) facets of. Assume thatis the union of some. For some, let

being the space of continuous piecewise polynomials of degreew.r.t..

In order to prove inf-sup stability for the ‘original’ mild and mild-weak formulationsand, ina uniformly bounded ‘Fortin’ interpolatorhas been constructed (there denoted by), with, being the space piecewise polynomials of degreew.r.t. the partition,, and

for allfor which the right-hand side is finite.

To verify, we will construct a (uniformly) bounded right-inverse of the normal trace operatorthat mapsinto a finite element space that will be used as the test space.

Heredenotes the space of Raviart-Thomas functions of orderw.r.t.whose normal components vanish at.
Before we prove this lemma, we use it to demonstrate.
We set

Thenis uniformly bounded, andas a consequence of, meaning thatis a valid Fortin interpolator. From Theoremwe conclude the following result.

Because we are only interested in consistent data, the data-oscillation term to be estimated reads as, where.
From, we have

for allfor which the right-hand side is finite.
We conclude that the data-oscillation term is of order, which, as desired cf. Remark, exceeds the orderof best approximation ofin.

For, ina projectorhas been constructed with the properties that, whereis the-orthogonal projector onto, and

(,), only dependent onand the shape regularity of.

Given, let,be such that

where

Obviouslycan be taken to be a linear map, and so can. For example, one may takewheresolveson,on,on.

We define

It satisfies

, and so

The proof is completed by

Letbe the family of all conforming partitions ofthat can be constructed by Newest Vertex Bisection starting from an initial partitionsuch thatis the union of some. Then a frugal way to constructis by a sequence of consecutive MARK and REFINE steps starting from, where in each iteration only those simplices are marked for refinement that have an edge onthat is not in.
The total number of simplices that will be marked is bounded by an absolute multiple of.
Consequently, an application offor, or its extensionfor, shows that.
That is, the number of elements in the domain mesh(minus the number of elements in) can be bounded by a constant multiple of the number of faces of elements inthat are on.

SECTION: Verification of inf-sup condition
Letandbe as in Sect..
Let

so that.

Ina uniformly bounded projectorhas been constructed (there denoted by), with,, and

for allfor which the right-hand side is finite.

Given, letsolveon,on. Then.
Now letbe the usual Scott-Zhang quasi-interpolant fromofin, which interpolator is uniformly bounded inand preserves boundary data in.
∎

The operator

is uniformly bounded, andas a consequence of.
From Theoremwe conclude the following result.

The data-oscillation term to be estimated reads asfor.
From, we have

for allfor which the right-hand side is finite.
We conclude that the data-oscillation term is of order, which exceeds the orderof best approximation ofin.

Finally, as we have seen in Sect.,can be constructed with.

SECTION: Verification of inf-sup condition
Let,, andbe as in Sect.. Take, and assume that. Writing

the arguments used for the construction ininshow thatis satisfied for.

As follows from, forsuch that for all,, with thisthe data-oscillation term is order, which exceeds the orderof best approximation ofin.

SECTION: Another application: Stokes equations
Our approach to append inhomogeneous essential boundary conditions to a Least Squares functional is not restricted to elliptic problems of second order.
As an example we consider the Stokes equations.
On a bounded Lipschitz domain, where,we seekthat, for given data,, and, and viscosity, satisfy

This proposition generalizeswhich considersand(see also).
Its proof is postponed to Appendix.

Analogously to Sect., using () we have the following corollary.

To apply to () the Least Squares discretisation ()/() or (), one has to realize the following inf-sup conditions.

Given familiesand, one needs a familywith

where.

Given a family, one needswith

Concerning: Forbeing as in Sect., letand.
Then by writing for,

the arguments used for the construction ininshow thatis satisfied for.

As follows from, by taking,
foranddata-oscillation is of orderwhich exceeds the orderof best approximation ofinandin.

Concerning: This inf-sup condition has been discussed in Sect.for the ‘scalar case’.
The results given there show that if, andissomeuniformly shape regular partition ofinto (closed)-simplices with, thenis satisfied for.

Similar to Remark, for(), data-oscillation is of orderwhich exceeds the orderof best approximation ofin.

SECTION: Application with Machine Learning
SECTION: Abstract setting
We return to the abstract setting to approximate the solution of, wherewith, and.
Given aset, we aim to minimizeover. As setwe have in mind a collection of (Deep) Neural Net functions.

Recall the problem that in most applicationsis not evaluable. As before, to analyze this situation it suffices to consider the setting thatwithevaluable, andnot being evaluable.

Earlier, forbeing a closedlinear subspaceof, we solved this problem by replacingby the discretized dual norm, whereis a closedlinear subspaceofthat satisfies the (uniform) inf-sup condition (). As has been shown in Theorem, the resulting Least-Squares approximationis a quasi-best approximation tow.r.t..

Forsubsetsand, a substitute for Theoremis the following Proposition.
It requires thatis closed under scalar multiplication, which, by the absence of an activation function in the output layer, holds true forbeing a collection of ‘adversarial’ (Deep) Neural Net functions (possibly with components multiplied by a functionwithproportional to the distance ofto (part of) the boundary to enforce an homogeneous boundary condition) .

Propositionis based on, where () is milder than the corresponding condition in.

Given an, for any, there exists awithand, where we used thatis closed under scalar multiplication. Also, there exists awithand.
So with,

and so

Now for any,

where the one but last inequality holds true by definition ofand Footnote.
∎

In the setting of Proposition, we set the (squared) error estimator by

Clearly it holds that, i.e., the estimator isefficient.

Now letbe such thatis inf-sup stable in the sense of () with constant, and such that for some constant,

known as asaturation assumption. Then, as in the proof of Proposition, we have

From, we conclude that, i.e., the estimator isreliable.

For the case that,, andare, a similar technique to construct an efficient and reliable a posteriori estimator was applied in.

A straighforward (approximate) computation of, required for the Least Squares approximation (as well as for the a posteriori error estimator), turns out to be unstable as can be understood from the fact that for, anyis a supremizer.
A stable computation is provided by the following result.

Let us denoteby.
Given, letbe such thatand. Then for,

so that.

On the other hand,

Above results show how to avoid the unfeasible computation of.
When also the computation ofis unfeasible, forbeing alinear subspacethe approach from, which was recalled in Sect., is to replaceby an onequivalent norm defined in terms of an (optimal) preconditioner.
This approach doesnotapply in the current setting, and so we will avoid the situation that bothandare non-evaluable norms, as whenis a fractional Sobolev norm.

SECTION: Application to model elliptic second order boundary value problem
For the model elliptic second order boundary value problem (), we apply Least Squares to themodified mild, ormodified mild-weakfirst order system formulationsorfrom Sect., where we replace the imposition of the Dirichlet boundary condition by means of () by that from Lemma.
For the formulation, forand, using Lemmait results in the problem of finding

Obvious adaptations are required when eitheror(so thator).

Analogously, one derives the Least Squares problem resulting fromwith () replaced by Lemma.

For large, the approximation of the-dimensional vector fieldis computational demanding. In that case one may resort to the standardsecond ordervariational formulation. From Lemmaone infers that findingsuch that

defines a homeomorphism betweenand its range in.
Forand, it leads to the Least Squares problem of finding

For this second order formulation, and, in the case of mixed boundary conditions, for the modified mild first order formulation, one has to enforce homogeneous boundary conditions in the test set by multiplying Neural Net functions by a function that is proportional to the distance to the corresponding part of the boundary.

Propositionshows thatifin above formulationsis sufficiently large in relation to, i.e., independently from the data,thenthe Least Squares solution fromis aquasi-best approximationfromto the exact solution in the norm on.

A similar conclusion can be drawn for the Least Squares approximation to the Stokes equations based on their formulation from Corollary.

In above examples we have seen that for sufficiently large, the Least Squares solution fromis a quasi-best approximation to the exact solution from. Notice, however, that other than for the finite element setting discussed in Sections-, and, so far for sets of Neural Net functionsandthe condition ofbeing sufficiently large, i.e., to satisfy (), hasnotbeen verified.

SECTION: Numerical results
SECTION: Experiments with Finite Elements
We take an example from. On a

We prescribe the solutionin polar coordinates, and determine the data correspondingly. Then,, andon, buton the remaining part of.
It is known thatfor all, but().

We consider above problem in themodified mild formulation.
For finite element spacesand, a Least Squares discretization using discretized dual-norms as presented in Sect.-leads to the saddle-point problem to findfor which

Letdenote the collection of all conforming triangulations that can be created by newest vertex bisections starting from the initial triangulation that consists of 8 triangles created by first cuttingalong the y-axis into two equal parts, and then cutting the resulting two squares along their diagonals. The interior vertex of the initial triangulation of both squares are labelled as the ‘newest vertex’ of all four triangles in both squares.

Forand a family, we take

Now for, andbeing the coarsest triangulations withand, we take

Then as follows from Theorems,, and,is aquasi-bestapproximation tofromw.r.t. the norm on.

As follows from Remark,

is an efficient, and, by Remarks,, and, asymptotically reliable estimator for the error.

Because of the limited smoothness of the solution, the asymptotic convergence rate for uniform refinements cannot be expected to exceed.
To drive an adaptive scheme, the error estimator needs to be split into element-wise contributions.
While it is natural to splitandinto contributions corresponding to elements, a similar splitting ofand, although possible, would require additional work sinceandare finite element functions w.r.t. partitionsand, which generally are coarser than. Since we expect, however, thatandhave their largest values at elements at the Dirichlet or Neuman boundary, we will ignore their contributions to the estimator associated to other elements.
Making use of the fact thatand, forwe define the local estimator as follows

The results given in Figureindicate that the adaptive routine driven bywith bulk chasing parameterconverges with the best possible rate for bothand.

An example of the triangulation, and corresponding triangulationsandis given in Figure.

SECTION: Experiments using Machine Learning
Considering the elliptic model problem

we will compare our newly introduced methods to three prevalent archetypes of Machine Learning approaches: Deep Ritz Method (DRM), Physics Informed Neural Network (PINN), and Weak Adversarial Network (WAN). The first of these methods (DRM) minimises
the energy functional, the second method (PINN) minimises the
squared-norm of the residual, while the latter (approximately) minimises the squared-norm of the residual. All three however deal with the essential boundary condition through the addition of a multiple of the squared-norm of the boundary residual to the functional to be minimised .

In general, all these methods minimise a loss functionalover some (deep) neural network, finding an approximate solution. In the aforementioned examples,and the loss functions are as follows

wherewith,some chosen constant, anda neural network like. Note that in the WAN case, due to the need of evaluating a dual-norm, there is an additional supremum of. For reasons explained in Sect., it is best to rewrite the WAN loss function using Lemmain the form

We compare above methods with the following four newly introduced least square loss functions whose minimum over the neural networkwill produce a quasi-best solution from, given that the neural networkat the test side is big enough:

andare the first order system and second order least squares formulations from () and (), specialized to the case that,and, andandare the corresponding formulations where the Dirichlet boundary condition is enforced by means of () instead of ().

As with the WAN method, these formulations involve solving a minimax problem. To solve this in practise, one therefore needs to switch between minimising the loss function over the test spaceforsteps, and maximising over the trial spaceforsteps. One might consider a more intelligent switching between minimizing and maximizing, but this is beyond the scope of this paper.

As the given integrals so far will most often not have a closed form, these must be approximated. This can either be done with Monte Carlo integration or quadrature integration, each having its pros and cons.

Monte Carlo integration has two main benefits. Firstly, it induces some stochastic property in our integral, which combined with gradient descent (like) algorithms, gives rise to a stochastic gradient like approach. This makes sure that the solution is not trained to minimise for specific grid points and helps prevent getting stuck in local minima. The second advantage is that the amount of required samples in Monte Carlo methods do not scale exponentially with dimension as opposed to traditional quadrature schemes for which one requires a meshing of the domain, the so-called ‘curse of dimensionality’. We will use simple uniform sampling of the domain for our Monte Carlo integration, which results in the following approximations

where theandare sampled uniformly from their respective domains. It is possible to adopt different sampling strategies to decrease the variance (such as importance samplingor quasi-Monte Carlo strategies), but this is not the focus of this paper and we will therefore stick to the straightforward uniform sampling.

The other option is to use some sort of quadrature rule to approximate the integrals. Its main advantage is that for smaller dimensions it gives a much better convergence rate, and in general gives more control over the quality of the approximation. Its downsides are that it does not scale well up to higher dimensions and one needs to make it an adaptive scheme to counteract overfitting on the quadrature points. For the numerical experiments on a domainthat is the union of-dimensional hypercubes, an adaptive tensor product Gauss-Legendre quadrature scheme was implemented. This was done by initially subdividing the domainintohypercubes. The scheme was then made adaptive by dividing eachintohypercubes, and computing the following criterium

where withwe denote the Gauss-Legendre tensor product quadrature of a functionover a hypercube. If the criterium holds true, the subdomainis accepted and does not to be refined further, otherwise it is rejected and gets replaced by its refined subdomains. Ideally the refinements stops when all subdomains are accepted, in which case one expects either an absolute error of the orderor a relative error of the order, whereandare some chosen constants. Unfortunately this can be expensive (i.e. in the case of a singular function), so for practical reasons the refinements will automatically be stopped after 1000 rejections and consequent refinements of the domain. This checking and refining can be done completely in parallel and can thus be expected to be similarly fast (if not faster due to potentially requiring fewer samples) as Monte Carlo integration if implemented well.

This then leaves us with defining the deep neural networksandin which our trial and test solution will lie. For the architecture of these networks, we have opted for the so-called Residual Neural Networks (ResNet). These were introduced in, and were designed to avoid the vanishing gradient problem, making them easier to train. A graphical representation of the ResNet we used, is given in Figure. This network is built out of two main components: the ResNet blocks and linear transformations, one to get the input vector to same width as the ResNet block and one to get the output of the last ResNet block to the correct output dimension.

Using the same notation as in Figure, the network first calculates

in whichand. Note that the notationsignifies the’th layer of the’th ResNet block. With, we have entered the first ResNet block. This is a fully connected neural network (FCNN) of widthand depth. This means that to get from layertofor, one calculates

where,andis some non-linear activation function which acts coordinate wise.
To ensure that this architecture does not suffer from a vanishing gradient,
the final output of the’th ResNet block is given by the addition of the first layer of its FCNN with the last

We can therefore define

which maps from the initial layer of the’th ResNet block, to its output. The last step is a linear transformation from the final ResNet block outputto the output vector, which is given by

whereand. We therefore find that

In the context of neural networks, this functionis often called the ‘forward’ function of the network.

The set of all possible different parameters of our ResNet will be denoted with

This allows us to define our neural network space of functions as

wheredenotes the forward functionas defined in () for a specific choice of parameters. Note that the number of degrees of freedom for our ResNet is given by.

Having now defined all necessary components, we can formulate the general algorithm for solving our model problem

This algorithm was implemented for each of the different QOLS formulations along with the DRM, WAN and PINN in Python using PyTorch.

SECTION: Conclusion
We have introduced least-squares formulations of second order elliptic equations and the stationary Stokes equations with possibly inhomogeneous boundary conditions, whose minimization over a finite element space or a Neural Net yield approximations that are quasi-best. This is due to the fact that the least-squares residual is equivalent to the (squared) error in a canonical norm.
Despite of that, the use of fractional Sobolev norms on the boundary, or even any function space on the boundary has been avoided.
Such spaces were replaced by the ranges of trace operators of standard function spaces on the domain. Some parts of the residual are measured in dual norms. For sufficiently large test finite element spaces or adversarial Neural Nets, they are replaced by discretized dual norms whilst preserving quasi-optimal approximations. Numerical results both for finite element spaces and for Neural Nets are presented. The advantage of our approach compared to usual Machine Learning algorithms is apparent for solutions that have singularities. It is however fair to say that known problems with Machine Learning algorithms for solving PDEs, as quadrature issues or the painful minimization of a non-convex functional, are not solved by the use of our well-posed least-squares functionals.

SECTION: Proof of Proposition
By making the substitutions

the modified variables satisfy the Stokes system () with.

By testing the first and second equation of this system withand, respectively, and applying integration by parts we obtain

From, and soon, regardless of the boundary datumthe Stokes solutionsatisfies

().
The operator defined by the left hand side is in. Furthermore, it is well-known that this operator is in.

To arrive at a first order system, we introduce the vorticityas an additional variable. Then () is equivalent to the system

().
With,, and, the operator defined by the left hand side, which we will denote by, satisfies.
To see that, replace the zero at the right hand side of the second equation by. By substitutingin the first equation, by the well-posedness of () one finds a solutionwhose norm is bounded by the norm of the data in.

A solutionof () satisfies. Sincesatisfies, we conclude that

().
The operator defined by the left hand side, which we will denote by, satisfies.
The squared normreads as

andreads as

From, applications of the triangle inequality show.
From, it follows that for,, i.e.,is a homeomorphism with its range.
For arbitrary, there exists a, with, and so, meaning that.

Finally, as we have seen, a consistent formulation of Stokes problem withfor, with, is given by

It holds that,is surjective, with kernel equal to, and. By an application of Lemmawe conclude that.

Upon substituting () and, the proof of Propositionis completed.

SECTION: References