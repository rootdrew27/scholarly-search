SECTION: Monocular Lane Detection Based onDeep Learning: A Survey

Lane detection plays an important role in autonomous driving perception systems. As deep learning algorithms gain popularity, monocular lane detection methods based on them have demonstrated superior performance and emerged as a key research direction in autonomous driving perception. The core designs of these algorithmic frameworks can be summarized as follows: (1) Task paradigm, focusing on lane instance-level discrimination; (2) Lane modeling, representing lanes as a set of learnable parameters in the neural network; (3) Global context supplementation, enhancing inference on the obscure lanes; (4) Perspective effect elimination, providing accurate 3D lanes for downstream applications. From these perspectives, this paper presents a comprehensive overview of existing methods, encompassing both the increasingly mature 2D lane detection approaches and the developing 3D lane detection works. Besides, this paper compares the performance of mainstream methods on different benchmarks and investigates their inference speed under a unified setting for fair comparison. Moreover, we present some extended works on lane detection, including multi-task perception, video lane detection, online high-definition map construction, and lane topology reasoning, to offer readers a comprehensive roadmap for the evolution of lane detection. Finally, we point out some potential future research directions in this field. We exhaustively collect the papers and codes of existing works athttps://github.com/Core9724/Awesome-Lane-Detectionand will keep tracing the research.

SECTION: IIntroduction

Lane detection seeks to obtain the semantic and positional information of each lane line from the front view (FV) image captured by the onboard monocular camera. It is an indispensable part of the perception module in autonomous driving system, providing necessary prerequisites for subsequent decision-making and planning processes. Early lane detection methods depend on manually crafted operators for feature extraction[1,2,3,4,5,6], which have limitations in accuracy and robustness when facing the complex scenes[7,8,9,10,11,12,13,14,15]. Later on, deep learning-based methods gradually dominate this field due to their strong feature representation ability and superior performance. Nowadays, deep learning-based monocular lane detection becomes a key research topic in autonomous driving perception, attracting great attention from both academia and industry.

Existing deep learning-based monocular lane detection methods can be divided into 2D lane detection and 3D lane detection methods. As shown in Figure1, a complete lane detection process can be described as: given a FV image, the ultimate goal is to obtain 3D lane information in the ego vehicle coordinate system, i.e., the bird’s eye view (BEV) space. Because of the inherent perspective distortion in the camera imaging process, parallel lanes in the BEV plane intersect in the FV image, making it challenging to restore their true geometry. An effective solution involves building a 2D lane detection network[16,17,18,19,20]to obtain the 2D lanes from the FV image, then the intrinsic and extrinsic parameters of the camera are combined to project these 2D lanes onto the ground through inverse perspective mapping (IPM)[21], obtaining the final 3D lanes. With the rapid progress of deep learning-based methods, 2D lane detection achieve impressive results. However, IPM assumes that the ground is always flat and does not account for conditions like uphill, downhill, and rough road surfaces. It means that the IPM projection will lead the wrong 3D lane results, even if the 2D lane detection in FV is accurate. Therefore, researchers gradually shift their focus on designing 3D lane detection networks[22,23,24,25,26,27], which directly predict 3D lanes using FV images as input.

Precise localization and real-time processing are essential for lane detection. Apart from the above, autonomous vehicles must adapt to complex road environments where lanes may be obscure due to occlusion by nearby vehicles or adverse weather conditions. Lastly, to better connect downstream tasks like planning and control, each lane instance should be distinguished and presented in a vectorized format, such as an ordered set of points or a curve equation. This is because downstream requires calculating the driving planning lines, i.e., the centerlines, based on the lanes around the vehicle. It is difficult to perform calculations without distinguishing different lane instances or lacking a vectorized representation.

Based on the complete process and the challenges of lane detection, the core design of the lane detection algorithmic frameworks can be summarized as follows: (1)Task paradigm, focusing on lane instance-level discrimination; (2)Lane modeling, representing lanes as a set of learnable parameters in the neural network; (3)Global context supplementation, enhancing the inference on the obscure lanes; (4)Perspective effect elimination, providing accurate 3D lanes for downstream applications. A comprehensive survey on lane detection should systematically explore these four key perspectives while integrating insights from both 2D and 3D lane detection methods. This dual focus is necessary to provide readers with a comprehensive understanding of advancements in lane detection technology, and help them to bridge the gap between conceptual design and practical applications.

Related surveys.Early reviews on monocular lane detection mainly focus on traditional methods[28,29]. Despite the existence of relevant summaries based on deep learning, the related surveys[30,31,32]exhibit relatively narrow focus. On the one hand, only 2D or 3D lane detection methods are summarized. The close connection between 2D and 3D lane detection is ignored. On the other hand, the network structures or loss functions are paid too much attention in these papers, which are significant in deep learning but not the crux of addressing the lane detection challenges.

Contribution.This paper conducts a comprehensive investigation into the latest developments in monocular lane detection methods based on deep learning, focusing on the core designs of the lane detection algorithm frameworks. Compared to the related surveys[30,31,32], ours not only covers state-of-the-art 2D and 3D lane detection methods but also provides a higher-level summary. The main contributions of this survey can be summarized as follows:

We present a comprehensive survey of deep learning-based monocular lane detection methods. This is the first survey that covers both 2D lane detection and 3D lane detection.

This survey firstly introduces the four core designs of lane detection algorithms: task paradigm (Distinguishing different lane instances), lane modeling (representing lanes as network learnable parameters), global information supplementation (identifying obscure lanes), and eliminating perspective effects (obtaining available 3D lanes for downstream). Then we investigate the existing methods systematically from the above perspectives and summarize a general pipeline for each categorization.

In addition to reporting the performance of representative methods, we also reevaluate their efficiency in a unified environment. This enables readers to more easily compare different methods and select the most suitable baselines for their applications.

Moreover, some extended works are surveyed, including multi-task perception, video lane detection, online high-definition (HD) map construction, and lane topology reasoning. They can be regarded as an upgrade of monocular lane detection in terms of task flow. Based on these introductions, readers can receive a roadmap for the development of lane detection research focus.

Organization.The rest of the survey is organized as follows: SectionIIexplains the datasets and evaluation metrics commonly used for Lane detection algorithms. SectionIIIand SectionIVreviews the existing 2D and 3D lane detection methods, respectively, where we summarize existing methods from the perspective of the core designs in lane detection algorithms. SectionVreports on the performance of representative methods on typical datasets and efficiency comparisons in a unified environment, and analyzes them in conjunction with the core designs of lane detection algorithms. Some expanded works of lane detection are introduced in SectionVI. The possible future challenges are discussed in SectionVIIand the conclusions are provided in SectionVIII. The sturcture of this paper is shown in Figure2.

SECTION: IIDatasets and Metrics

SECTION: II-ADatasets

TableIsummarizes the main statistics of prevailing lane detection benchmarks which is publicly available. Next, we provide a detailed introduction to some popular datasets.

Tusimple.The TuSimple[35]dataset is collected with stable lighting conditions in highways, including different levels of occlusion, different types of lanes, and different road conditions. It consists of 6,408 images, which are split into 3,268 training, 358 validation, and 2,782 test images. For each image, lanes are annotated by the 2D coordinates of sampling points with a uniform height interval of 10 pixels. Each annotated image has a size of 1280720 pixels.

CULane.CULane[36]is a large-scale 2D lane detection dataset with 88,880 training images and 34,680 testing images. In addition to different weather conditions and light levels, there are eight challenging lane detection scenarios, such as traffic congestion, shadow occlusion, missing lanes, and lane curves. All the images have 1640590 pixels.

LLAMAS.The annotations of LLAMAS[39]are automatically generated from HD maps. This dataset contains over 100k images from about 350 km of recorded drives. In contrast to other datasets, LLAMAS presents a small and variable number of pixels marking each lane, reflecting real-world conditions more accurately. The resolution of all images is 1280717 pixels.

CurveLanes.CurveLanes[40]contains 100K, 20K, and 30K images for training, validation, and testing, respectively. It features an abundance of curved lanes and difficult scenarios such as S-curves, fork lines, nighttime conditions, and multi-lane configurations. In comparison to existing datasets like the first three, each image within CurveLanes encompasses a greater number of lanes and has a higher resolution.

Apollo 3DLane.The Apollo 3DLane dataset[23]is generated using the game engine, including 10,500 discrete frames of monocular RGB images and their corresponding 3D lanes ground truth, which is split into three scenes: balanced, rarely observed, and visual variation scenes. Each scene contains independent training sets and test sets.

ONCE-3DLanes.ONCE-3DLanes[44]is a large-scale real-world 3D lane detection dataset, which is constructed based on the ONCE dataset[47]. It contains 211K images comprising diverse weather conditions (sunny, cloudy, rainy) and varied geographical locations (urban centers, suburban areas, highways, bridges, and tunnels). Only intrinsics of the camera are provided in ONCE-3DLanes.

OpenLane.OpenLane[24]is another large-scale but more comprehensive benchmark for real-world 3D lane detection based on Waymo Open Dataset[48]. The dataset includes 200K images captured in a variety of weather, terrain, and brightness conditions. In OpenLane, the lane annotation not only contains the 3D position of a lane but also several attributes and tracking id. The intrinsics and extrinsics of the camera are provided for each frame, and category and scene labels (e.g., weather and location) are also provided, providing a realistic and diverse set of challenges for 3D lane detection algorithms.

SECTION: II-BEvaluation Metrics

Despite that different datasets may utilize ununified evaluation metrics, below we mainly introduce the common evaluation metrics adopted by all datasets. More evaluation metrics can be found in the Section-Aof Appendix.

F1 score serves as the primary metric, taking into account both accuracy and recall. The calculation of recall and correct rate is closely related to the determination of true positive (TP). Different datasets determine TP in different ways. The F1 score is calculated as follows:

Tusimple[35]and CULane[36], which are the representive 2D lane detection benchmarks, adopts two different ways to determine TP. Tusimple[35]focuses on point-by-point evaluation. The predicted point is considered correct if the horizontal distance from the true value point is less than 20 pixels when the longitudinal coordinates are the same. Furthermore, the line prediction is viewed as TP when it contains no less than 85% of the true value points. In contrast, CULane[36]emphasizes line-by-line evaluation, treating each lane as a mask of several pixels wide, and calculating the intersection (IoU) between the predicted lane and the annotated lane. The prediction with IoU larger than 75% is viewed as TP.

For 3D lane detection, there are also two main ways to determine the TP, represented by the evaluation methods in OpenLane[24]and ONCE-3DLanes[44], respectively. OpenLane[24]follows the evaluation metric designed by[23]. The matching between prediction and ground truth is built upon edit distance, where one predicted lane is considered to be a TP only if 75% of its covered y-positions have a point-wise distance less than the max-allowed distance (1.5m). The ONCE-3DLanes[44]dataset employs a two-stage evaluation metric for lane detection. First, the IoU method from CULane[36]is utilized on the z-x plane (i.e., top view) to assess the alignment between the prediction and ground truth. Second, if the IoU exceeds a predefined threshold, the curve matching error in camera coordinates is computed using unilateral chamfer distance. If this unilateral chamfer distance falls below the specified threshold, the prediction is classified as TP.

SECTION: IIIMethods of 2D Lane Detection

This section reviews the existing 2D lane detection methods. We first explain the ground for classifying existing methods in SectionIII-A, and then discuss the existing methods accordingly in SectionIII-Band SectionIII-C. Lastly, the process of converting 2D lanes to 3D lanes using IPM and its deficiencies are reviewed in SectionIII-D.

SECTION: III-AClassification Framework

Previous summaries[30,31]primarily focus on the design of network structures and loss functions. It is noteworthy that the instance-level discrimination and vectorized result representation, which are necessary prerequisites for guiding downstream applications, are overlooked. By contrast, our classification of 2D lane detection methods is primarily according to the above two aspects.

As shown in Figure3, first, for lane instance-level discrimination, 2D lane detection methods can be divided into two types of paradigms based on the number of stages required to complete the task: (a)Segmentation-based methods(two-stage), which complete the lane localization and instance discrimination in a certain order. Figure4summarizes the general pipelines for instance-level discrimination in such methods. (b)Object detection-based methods(one-stage), which perform instance discrimination and localization concurrently. This advantage arises from the general pipeline of object detection algorithms, which execute both classification and regression tasks on a set of candidate proposals in parallel.

Second, vectorized result representation requires algorithms to consider how to model lanes as a set of values for neural network learning, i.e., lane modeling. In terms of lane modeling, the segmentation-based methods can be further divided intomask-based modeling,grids-based modeling, andkeypoints-based modeling. For object detection-based methods, adopting a bounding box to model a narrow and long lane is often not reasonable. This is because the bounding boxes generated by object detection methods may be mutually occluded, and a bounding box may contain multiple lane instances. To align with the general object detection paradigm, these methods design unique ”bounding boxes” to model lanes, includingline anchor-based modelingandcurve-based modeling. The details of each lane modeling are described in Figure5.

Furthermore, most existing 2D lane detection datasets provide complete annotations for lanes, even the lanes are severely occluded by vehicles or affected by extreme weather conditions. To better identify such obscure lanes, many algorithms meticulously design special structures within their networks, thus the measures they use are also described in this section. We elaborately compare the representative 2D lane detection works according to the above classification criteria in TableII.

SECTION: III-BSegmentation-based Methods

Given an image, the ultimate goal of the network is to predict a set of masks of the same size as the input image. In the early stages, fully convolutional segmentation networks represented by[49]are used to segment lanes[50]. The encoder extracts high-level semantic information into feature maps, and then the decoder upsamples these feature maps to their original size for pixel-wise prediction. However, even with more powerful general segmentation networks[51,52,53,54,55], the lane segmentation performance remains unsatisfactory. This is mainly because these networks do not account for the elongated nature of lanes. Moreover, when lanes are occluded by factors like vehicle or lighting, relying solely on annotations of complete lanes for supervision is not an effective solution. Traditional encoders often fail to capture these subtle features. Consequently, numerous studies introduce specialized structures before pixel-wise prediction to enhance feature representation.

VPGNet[34]predicts the disappearance points of lanes as the global geometric background to improve the performance of lane detection. SCNN[36]develops a novel convolutional layer for specially shaped objects, such as lanes and utility poles, allowing information to pass between image layers, which is similar to recurrent neural networks. However, there is still some room for improvement in computational speed. SAD[56]employs a self-attention distillation mechanism that contextually aggregates high-level and low-level attention to obtain finer lane features. IntRA-KD[57]uses a teacher-student distillation mechanism to represent lane structure knowledge as an interregional affinity map, capturing the similarity of lane feature distribution across different scene regions. EL-GAN[58]uses the generative adversarial network (GAN) to obtain more realistic and structurally rich lane segmentation results. Then, Zhang et al.[59]select a GAN with better performance[60]and modify its structure to extract subtle lane features. Xu et al.[61]design a channel attention module that enhances lane features and suppresses background noise, and propose a pyramid deformation convolution module to obtain more structural information of lanes. RESA[16]further proposes a recurrent aggregator on top of SCNN[36]that fully exploits the lane shape prior to enable the network to aggregate global features for improved performance and efficiency. PriorLane[62]obtains more comprehensive features based on a Mixed Transformer[63]and improves network performance by fusing image features with low-cost local prior knowledge, enhancing lane segmentation.

While semantic segmentation provides semantic categories at the pixel level, it is insufficient for distinguishing different instances within the same category. An intuitive approach is to apply top-down instance segmentation frameworks, such as Mask R-CNN[64]or YOLACT[65], to achieve instance-level discrimination and segmentation of lanes. However, the bounding boxes generated by object detection methods may contain multiple lane instances, which complicates distinguishing them in the subsequent semantic segmentation process.t

SCNN[36]proposes a top-down process that is different from the above. Specifically, each lane is treated as a separate category so that multi-category semantic segmentation is performed. Meanwhile, a parallel classification branch is incorporated to predict the existence of lanes at each position. Finally, the classification and segmentation results are combined to obtain the final lanes. The subsequent works[58,56,16,62]follow this way. This manner facilitates instance differentiation but introduces certain limitations: it requires defining a maximum number of lanes in advance to determine the number of possible instances. Additionally, the correspondence between lanes and classes is established by annotations. When vehicles switch between lanes, this predefined labeling may lead to ambiguity.

To solve the above problem, some studies adopt a bottom-up approach for instance segmentation, i.e., cluster the binary segmentation results of lanes/backgrounds. VPGNet[34]clusters lanes using a modified density-based clustering method. LaneNet[66]utilizes instance embedding to cluster the results of semantic segmentation, achieving lane instance segmentation. This method offers high clustering accuracy but is time-consuming, which limits its applicability for real-time processing. FastDraw[71]constructs a learnable decoder that not only segments lanes but also identifies pixels belonging to the same lane. To address the inefficiency of pixel-embedding clustering, LaneAF[67]introduces an affinity vector field to associate pixels belonging to the same lane. Although these methods are more flexible, the algorithm execution efficiency remains suboptimal due to the high complexity of bottom-up clustering and the low efficiency of mask-based modeling in classifying all pixels.

The lane masks obtained from the segmentation network usually contain a large number of irrelevant areas. In order to be used for ego-vehicle motion prediction and planning, it is necessary to further denoise the mask to obtain vectorized results. Usually, for each lane mask, the highest response is sampled sequentially at equidistant heights, and then curve fitting is performed.

To address the inefficiency of pixel-wise prediction in semantic segmentation, UFLD[69]proposes a grids-based modeling approach. It divides the image intorows andcolumns, creatinggrids with equal spacing along the height and width. Lane detection is then described as a row-wise prediction process. For each lane instance, a grid that is most likely to belong to it is predicted in each row. In this way, the original pixel-by-pixel classification requires a time complexity of, while this method reduces the complexity to, where C is the number of classes. It is clear thatand. Therefore the ultra-fast inference is enabled. To supplement global context, the network selects a large fully connected (FC) layer to output the classification probabilities for each grid, thereby increasing the receptive field.

For lane instance discrimination, the premise is the existence of known instances, which means it cannot be performed in the bottom-up manner. UFLD[69]follows SCNN[36]by treating each instance as a category, which is not robust. To solve the instance discrimination problem, and inspired by instance segmentation methods like CondInst[72]and SOLOv2[73], CondLaneNet[17]learns the probabilistic heatmaps of the lane starting points to obtain lane instances and generates dynamic kernels based on the features of the starting points. Then, conditional convolution[74]is applied to the kernel and the entire feature map for row-wise classification. Additionally, a recurrent instance module based on LSTM[75]is proposed to address dense lines and forked line scenarios.

The strategy of row-wise classification leverages the vertical and slender nature of lanes. Unfortunately, it is not well-suited for some curved or near-horizontal lanes, where several meshes in a row may correspond to the same lane. For this reason, UFLD-V2[76]extends row-wise classification to row/column-wise classification to address the issue that row-wise classification cannot handle horizontal lanes. However, it still employs a multi-classification strategy[36]for lane instance discrimination, which results in an overly simplistic choice between row and column classification, thereby limiting its generalizability in real-world scenarios. CANet[77]further optimizes this approach. It employs U-shaped guidelines to constrain lane instance kernel generation based on CondLaneNet[17]. An adaptive decoder is designed, which dynamically chooses between row-by-row or column-by-column classification for each instance.

Since the networks output the classification probability of each row/column grid, rather than the vectorized format, the post-processing is also required. Specifically, each point’s coordinate is calculated as the expectation of locations (grids from the same row/column), i.e., a weighted average by probability. Compared to the post-processing of lane masks obtained from semantic segmentation, it is easier to implement.

The mask-based modeling methods often involve predicting numerous irrelevant regions, so some research efforts attempt to directly predict keypoints of lanes. This, like grids-based modeling, can be seen as a sparse version of mask-based modeling, but it directly provides the vectorized expression required by the downstream.

Some works follow a bottom-up approach. PINet[79]uses a stacked hourglass network to predict keypoint locations and feature embeddings, and clusters different lane instances based on the similarity of feature embeddings. FOLOLane[68]estimates the existence and offset of local lane keypoints, and designs a decoder module with low-level operators that integrates the local information into curve instances. Only keypoints on adjacent boundaries are paired, allowing the network to better focus on detailed features, but the lack of global features causes poor performance under the obscure lanes scene. To add global information, GANet[18]adopts a more efficient post-processing method to cluster points by directly calculating the offset between the keypoint and the start point to globally return to the keypoint. Additionally, a lane-aware feature aggregator based on deformable convolution (DCN)[91]is proposed to improve the shape of lanes and better capture the local context on the lane. RCLane[85]sparsifies the binary segmentation results to obtain the keypoints of all lanes. It decodes the channel in a chained mode using distance and transmission head to predict the keypoints with their continuous relationships in the channel. It also proposes a bilateral prediction method for learning complex topology and global shape information, which can adapt to lanes with complex structures, such as Y-shaped and forked lanes. LanePtrNet[92]designs a centrality farthest point sampling method to determine the lane center point. Then a grouping head performs clustering based on the center point position and lane point embeddings to obtain the final lane.

Others adopt a top-down manner. For each lane, Chougule et al.[93]directly regress the position of keypoints, and Yoo et al.[94]predict a feature map and searches for keypoints of the lane on each row. However, both of them distinguish instances using a multi-classification strategy, as described in SectionIII-B1, which is not flexible. CondLSTR[70]improves the instance-obtaining method in CondLaneNet[17]. It leverages Transformer to generate dynamic and offset kernels for each lane, enhancing global knowledge. Then, these kernels are dynamically convolved with the entire feature map to predict the heatmap and offset maps of lane keypoints.

SECTION: III-CObject Detection based Methods

A lane on the image can be represented by equidistant 2D points. Specifically, the lane is expressed as a sequence of points, i.e.,. The y-coordinates of points are equally sampled through the image vertically, i.e.,, whereis the height of the image. Accordingly, the x-coordinate is associated with the respective. Withand, the positions of points that form a lane can be located.

We can initialize a set of two-dimensional points with equal vertical spacing as line anchors. When a line anchor is matched to its corresponding GT, the network only needs to predict the count of valid y-coordinates and the horizontal offset of each valid y-coordinate’s x-coordinate relative to the GT. Through this process, the final lane can be reconstructed.

Line-CNN[78]uses a large number of predefined straight lines as line anchors. However, it predicts the scores, lengths, and transverse coordinate offsets of all anchors based on the local features of each start point, which implies that the feature map from the backbone must have a sufficiently high number of channels. To solve this problem, LaneATT[81]proposes a line anchor feature pooling method that allows the use of a lightweight backbone and presents a high-performance with efficient attention aggregation mechanism to better detect obscure lanes. PointLaneNet[95]and CurveLane-NAS[40]separate images into non-overlapping grids and regress lanes based on vertical line anchors. In particular, CurveLane-NAS uses network architecture search[96]to find a better network to capture more accurate information, which is beneficial for detecting curved lanes. SGNet[82]introduces a novel vanish point-oriented anchor generator and adds multiple structural guides to the performance. Jin et al.[43]introduce data-driven descriptors called eigenlanes, and use lower-order approximations of the lane matrix to obtain line anchors that can better regress curved lanes. Non-maximum suppression (NMS) post-processing is also unavoidable due to the limitations of a large number of predefined anchors and early positive and negative sample matching strategies in object detection.

With the widespread application of Transformers in object detection, research in this field gradually shifts from dense prediction paradigms, such as YOLO[97]and Faster R-CNN[98], to set prediction paradigms like DETR[99]. Similarly, lane detection based on object detection shifts from a fixed dense approach to a dynamic sparse approach. Based on Deformable DETR[100], Laneformer[84]introduces two novel row and column self-attention operations in the encoder to effectively capture lane context. The binary matching strategy enables an NMS-free approach. Inspired by Sparse R-CNN[101], CLRNet[19]uses multi-scale feature maps at the pyramid level to iteratively adjust the positions of a small set of preset line anchors[102]. It presents ROIGather to enhance lane feature extraction, effectively addressing challenges such as occlusion and lighting variations. Additionally, Line IoU Loss is introduced for global lane regression, enhancing positioning accuracy. CLRNet achieves state-of-the-art results on 2D lane detection datasets. The improvements to Line IoU Loss and label matching are made in[103]and[104], respectively. O2SFormer[105]proposes a one-to-many label allocation strategy and incorporates lane anchor points into position queries[106], providing explicit positional priors that accelerate model convergence. ADNet[87]removes the limitation of anchor starting points by learning heatmaps for these points and their related directions, enabling the network to adapt to diverse lane types across different datasets. It puts forward a module based on a hybrid CNN&Transformer architecture[107][108]to expand the receptive field, and proposes the Generalized Line IoU Loss to address the limitations of Line IoU Loss[19]. Similarly, SRLane[88]generates sparse line anchors by predicting local directional heatmaps and develops a lane segment association module to adjust non-fitting line anchors. Sparse Laneformer[109]designs learnable lane and angle queries to generate sparse line anchors. It employs a two-stage Transformer decoder to refine lane predictions. GSENet[90]designs a global feature extraction module based on dilated convolution[51]and SimAm[110]to obtain accurate and comprehensive global features, and further enhances semantic representation using ViT[111]. HGLNet[89]leverages the large receptive field of dilated convolution to enhance the representation of local features. It designs a global extraction head based on deformable-attention[100]to extract global feature of lanes adaptively.

Several studies model lanes as curve equations in image space, predicting the parameters of the modeled curves. This idea was first reflected in the works of Gansbeke et al.[112]. They propose a differentiable least squares fitting module, which fits the cubic polynomial curves (e.g.) to the points predicted by deep neural networks. Then, PolyLaneNet[80]directly learns to predict polynomial coefficients with simple fully connected layers. LSTR[83]uses Transformer to predict polynomials in an end-to-end manner based on DETR[99]. However, the performance of these methods remains suboptimal due to the difficulty of the curve’s parameter learning and challenges in transformer training. PGA-Net[86]introduces an improved supervised strategy to accelerate transformer convergence and proposes a Mean Curvature Loss to constrain the curvature of predicted lanes, enhancing the predictive accuracy for curved lanes.

Feng et al.[20]argue that the polynomials are abstract and the coefficients are challenging to optimize, recommending third-order Bézier curves for lane modeling. Their network predicts four control points to determine lane positions, proving more robust than direct regression of polynomial coefficients. They also consider the pseudo-symmetry of lanes in images and propose a feature-flipping fusion module based on DCN[113]to enhance feature representation in vehicle front-view images. Subsequently, Chen et al.[114]model lanes as more flexible B-spline curves and propose a novel curve-distance calculation method to improve control points prediction supervision.

SECTION: III-D2D Lanes to 3D Lanes

Once we obtain the 2D lane coordinates from FV, we need to use IPM to project it into BEV to obtain the 3D lanes for downstream use. We briefly review the general IPM process here. Firstly, the relationship between each pixel coordinatesand camera coordinatescan be described as:

where the matrixrepresents the camera intrinsics. Then each camera coordinates and ego vehicle coordinatescan be linked as:

where,refers to a rotation and a translation matrix, respectively. Their combination denotes the camera extrinsics.

With Eqn.3and Eqn.4, we can establish a transformation from each pixel coordinates to ego vehicle coordinates:

Due to the characteristics of perspective projection, objects in 3D space may lose depth information when imaged by the camera onto the image plane. It means that objects at different distances may be projected onto the same position. So when we only haveand camera intrinsics and extrinsics, we cannot obtain. We have to assume that the ground is flat, i.e.is a constant. Letand, then according to Eqn.5,can be calculated as:

Finally, substituting Eqn.6into Eqn.5can obtain the ego vehicle coordinates. It is cumbersome to transform every pixel coordinate according to this manner. We can select four points on FV as Region Of Interest (ROI) and calculate the corresponding positions in the ego vehicle coordinate system using the above method. Then we can establish a system of ternary linear equations:

By using the known four point pairs, we can solve the inverse perspective transformation matrix. Then we can useto obtain the position of ROI corresponding to the ego vehicle coordinate system in the image.

Although there are better IPM processes available[115,116,117], the assumption of flat ground is inevitable due to the perspective effect. As shown in Figure6, the lanes would diverge/converge during uphill/downhill, potentially leading to improper action decisions in the planning and control module if the height is ignored. This is why there has been a focus on directly predicting 3D lanes from FV[24,26,25,27].

SECTION: IVMethods of 3D Lane Detection

This section reviews recent 3D lane detection methods. We first explain the ground for classifying existing methods in SectionIV-A, and then discuss the existing methods accordingly in SectionIV-Band SectionIV-C.

SECTION: IV-AClassification Framework

As an upgrade to 2D lane detection, 3D lane detection primarily focuses on how to utilize neural networks to reconstruct the missing 3D information from 2D FV images.

As shown in Figure7, existing 3D lane detection methods can be divided into two categories: (a)BEV-based methods, which utilize camera parameters and convert the extracted FV features into BEV features with height information in some way. This process of constructing an intermediate proxy is usually referred to as view transformation[118]. In this way, the 3D lane detection task can be simplified to 2D lane detection in BEV, and then combining it with the corresponding height values estimated by a height estimation head yields the final three-dimensional lanes. Therefore, the performance of this type of method depends not only on the 2D lane detection results in BEV but also on the adopted view transformation method. (b)BEV-free methods, which do not hinge on BEV features. It can be further divided into two types. One is to detect 2D lanes in the FV image while predicting their depth, and then project them onto the 3D space. The other is to directly model lanes in the 3D space. With the initialized 3D information, it is possible to project it onto FV based on camera parameters. This approach enables direct interactions between the 3D lane and FV features, ultimately refining and updating the 3D lane.

Under the classification framework, for each specific method, instance discrimination and lane modeling are still discussed. We list a comparison of representative works in TableIII.

SECTION: IV-BBEV-based methods

The pipeline follows the established process of dense BEV perception methods[127,128,129]. Since BEV features inherently conceal height information, the subsequent lane decoding process only needs to consider the 2D BEV plane, which can naturally be integrated with 2D lane detection methods. The view transformation between FV and BEV features can be formulated as:

wheredenotes the FV feature.denotes the BEV feature which contains height information.,,denote coordinates in 3D space.denotes view transformation module.,denote corresponding pixel coordinates in terms of,,.andare camera extrinsics and intrinsics.

3D-LaneNet[22]is the first method which uses deep learning to predict 3D lanes directly from monocular images. The network first predicts the camera pitch angle and height to generate a differentiable IPM, combining the original FV feature map to create the BEV feature map. 3D-LaneNet demonstrates promising results in detecting 3D lanes from monocular images. Then Gen-LaneNet[23]directly uses 2D lane segmentation results as input for IPM, allowing for the utilization of extensive 2D lane data and enhancing the model’s generalization. In contrast to 3D-LaneNet, Gen-LaneNet offers more reliable supervision by using the camera pitch angle and height as GT. Li et al.[119]propose a new loss function based on Gen LaneNet to better extract the height information of 3D lanes from 2D lane representations. On the BEV plane, the above work uses vertical line anchors to model lanes. However, matching line anchors to ground truth is performed by measuring the distance at a predefined, which may result in missed detections for short lanes. Therefore, 3D-LaneNet+[130]avoids this issue using a bottom-up segmentation approach. Liu et al.[131]believe that the model can be independent of the ground truth camera pose provided by the benchmark. They design a two-stage network based on Transformer, which first predicts camera pose, i.e., the required parameters for IPM, then extracts BEV features, and finally regresses polynomial coefficients. 3D-SplineNet[123]treats lanes as B-spline curves on the BEV plane.

These methods offer valuable guidance for the initial exploration of 3D lane detection. However, their acquisition of BEV features relies on IPM. As discussed in SecIII-D, this rigid mapping lacks robustness. Issues such as improper feature transformation and suboptimal performance tend to arise during bending or squeezing turns.

To reduce the inherent errors caused by this rigid transformation, some researchers adopt a more flexible approach. They use neural networks to learn the transformation process from FV to BEV features. PersFormer[24]leverages deformable attention[100]to learn the spatial transformation from FV to BEV. It references the coordinate transformation matrix of IPM to generate BEV feature representations, focusing on relevant regions in the FV features. In the lane decoding stage, PersFormer adopts a unified 2D/3D line anchor design, achieving unified 2D and 3D lane detection. BEV-LaneDet[26]integrates the MLP based view transformation method VPN[121]into FPN[132]to obtain BEV features. It constructs a virtual camera module to project all images onto a standard virtual camera view, ensuring consistent image distribution. For lane modeling, BEV-LaneDet models lanes as keypoints. It adopts an embedding-clustering-based instance segmentation method[66]and refers to YOLO[97]to divide the BEV plane into grids, predicting the offset of each grid’s center point relative to GT. Due to its concise architecture, it is well-suited for deployment. Yao et al.[122]add the coarse-to-fine mechanism[19]based on PersFormer[24]. They fuse the local and global information referring to the coordinates of sparse points and jointly refine the global and local structures of lanes. Chen et al.[133]decompose the cross-attention between FV and BEV features into separate cross-attentions: one between FV and lane features, and another between BEV and lane features. Dynamic kernels are then used to convolve FV and BEV feature maps, generating 2D and 3D lane keypoint offset maps. GroupLane[134]uses the depth-estimation-based view transformation method LSS[127]to obtain BEV features. It models BEV lanes as grids[69]and establishes vertical and horizontal group heads to identify horizontal and vertical lanes, respectively. LaneCPP[126]also completes BEV transformation based on LSS. It models lanes as B-spline curves and uses prior knowledge of road geometry to enhance view transformation and lane prediction.

SECTION: IV-CBEV-free Methods

Similar to depth-assisted methods[135,136,137]in monocular 3D object detection, SALAD[44]decouples 3D lane detection into 2D lane segmentation and dense depth estimation tasks. With the help of estimated depth, the 2D lane coordinates can be projected into 3D space. Due to the availability of depth information, this method is independent of camera extrinsics.

The 3D object detection methods based on sparse BEV representations, such as DETR3D[138]and PETR[139], guide this approach. CurveFormer[120]constructs curve queries by modeling 3D lanes as 3D line anchors to provide explicit positional priors[140]. It designs a curve cross-attention mechanism to predict polynomial parameters for 3D lanes. Inspired by the line anchor feature pooling mechanism in LaneATT[81], Anchor3DLane[25]utilizes camera intrinsics and extrinsics to accurately project 3D line anchor points onto FV features. This facilitates anchor feature sampling, allowing the network to predict 3D coordinates and lane classification results based on the sampled anchor features. LATR[27]decomposes 3D line anchors into dynamically generated point-level and lane-level queries. It uses dynamic 3D ground position embeddings to interact with FV features, updating the lane query to bridge 3D space and 2D images. Dong et al.[125]model lanes as 3D Bézier curves and predict the curve control points via Transformer. Han et al.[141]express the 3D lane as a polynomial in 3D space. They design two Transformer structures to learn the 2D polynomial with the height of the X-O-Z plane and project the resulting 3D lane onto FV for supervised alignment. PVALane[124]generates sparser 3D line anchors than Anchor3DLane[25]by predicting 2D lanes in FV. It also introduces a module to align sampled FV and BEV features for more accurate 3D lane detection.

SECTION: VBenchmark Results

This section reports the performance of representative lane detection methods on commonly used public datasets. For each reviewed area, the most widely used datasets are selected for benchmarking in SectionV-A. Because of the high efficiency requirement for lane detection, speed tests are also conducted on representative open-source lane detection methods in a unified environment which are shown in SectionV-B. Note that we only list published works for reference. Following the performance and efficiency comparisons, SectionV-Crevisits the existing methods according to the four core designs of lane detection.

SECTION: V-AMain Results on Lane Detection Datasets

CULane[36]and OpenLane[24]are currently the most widely used 2D and 3D lane detection datasets. We report the performance of representative methods on these two benchmarks in TableIVand TableVseparately. All results are derived from the data in the original paper. More benchmark results are reported in Section-Bof Appendix.

SECTION: V-BEfficiency Comparison

Since the different methods are implemented on different platforms for the experiment, it is unfair to directly compare the speeds reported in their original papers. Therefore, we retest representative methods in a unified environment. TableVIshows the work efficiency of these methods. The representative open-source methods are reevaluated according to their settings on the CULane or OpenLane dataset. To ensure fairness, only the inference speed of the model is tested to report the frames per second (FPS). The backbone, input size, model’s output, and possible post-processing (whether the model’s output reflects a vectorized representation of each unique lane instance) of each method are also described. All tests are conducted on a single Nvidia GeForce RTX 3090 GPU.

SECTION: V-CDiscussion

In the two previous chapters, the overview of existing methods is presented from four aspects: task paradigm, lane modeling, global context supplementation, and perspective effect elimination. Combining performance and efficiency comparisons, we continue to discuss their importance for lane detection, as an empirical recipe provided to readers.

Task Paradigm.Segmentation-based methods achieve instance-level discrimination and lane positioning in a two-stage approach. The majority of the algorithm’s runtime is occupied by independent instance discrimination processes. This makes them overall less efficient than object detection-based methods which are achieved in one-stage. For object detection-based methods, it is necessary to consider the matching strategy of the positive and negative samples during the network training. This will determine whether NMS is needed for post-processing after the network inference.

Lane modeling.In mask-based modeling methods[36,56,16], each pixel is classified, which can lead to inaccurate segmentation masks that subsequently hinder vectorized fitting. Thus, achieving optimal performance and efficiency remains challenging. In contrast, keypoints-based modeling, line anchor-based modeling, and curve-based modeling methods learn fewer points or parameters, directly yielding the vectorized results for downstream use.

Keypoints-based modeling methods[18,70]demonstrate strong performance, benefiting from high-precision attitude estimation techniques. However, the overall efficiency of these algorithms is constrained by the instance discrimination step inherent in their segmentation paradigms.

Line anchor-based modeling methods[81,19,25,27]leverage the vertical and elongated characteristics of lanes in monocular images to strike a good balance between performance and efficiency. Nonetheless, these methods, which learn the horizontal offsets of equidistant points, are unsuitable for U-shaped or nearly horizontal lanes. This corner case is further discussed in subsequent sections.

Curve-based modeling methods[83,20]exhibit decent efficiency but fall short in terms of competitive performance on 2D lane detection benchmarks. Interestingly, this kind of method achieves strong results in 3D lane detection[131,126]. As analyzed by Han et al.[141], this discrepancy is due to the ground height influence, which makes fitting irregular lanes challenging in FV. In contrast, these lanes appear smooth in BEV, where they can be more easily fitted.

Finally, grids-based modeling methods like UFLD[69]achieve the highest efficiency; however, this comes at the cost of reduced computational load, resulting in suboptimal performance. These methods often require more advanced operators to compensate for this trade-off[17].

Global context supplementing.Regardless of the genre, most methods converge on the consensus that supplementing global information significantly enhances lane detection performance, particularly for detecting occluded lanes. Additionally, it is crucial to ensure that these specially designed structures achieve a balance between efficient processing and effective results. While this aspect has received limited attention in existing 3D lane detection benchmarks and methods, in practical applications, certain solutions in 2D lane detection can provide valuable references or be seamlessly integrated into 3D lane detection frameworks.

Perspective effect elimination.The ultimate goal persists in obtaining precise 3D lanes to support downstream applications. Using IPM to project 2D lane detection results into 3D space is feasible. However, the assumption of a flat ground often yields incorrect results in BEV, even if predictions are accurate in FV. While projecting the 2D lane detection results into 3D space based on depth values[44]is straightforward, this approach depends heavily on depth estimation and cannot be optimized in an end-to-end manner.

Early 3D lane detection methods[22,23,131], which still assume a flat ground, leverage IPM to construct BEV features. Some later approaches[24,26,126]improve on this by incorporating learnable ways, leading to enhanced performance. Alternatively, other methods[120,25,27]avoid BEV feature construction entirely, modeling 3D lanes directly and employing a 3D-to-2D forward projection to circumvent the inherent errors introduced by IPM. It should be pointed out that Transformer[142]has strong abilities in view transformation of BEV features[24]or interaction between 3D lanes and FV features[27]. This conclusion is also widely confirmed in related 3D object detection works[129,139]. Nonetheless, the hardware deployment of advanced operators, such as deformable attention[100], remains a problem worth of optimization.

SECTION: VIExtended Works of Lane Detection

There are also some works that have received widespread attention in recent years, which are closely related to lane detection. In terms of task flow, they can be regarded as an upgrade on monocular image lane detection. We provide a brief introduction to them in this section. Figure8depicts a roadmap of the evolution from lane detection to its expansion works.

SECTION: VI-AMulti-task Perception

In autonomous driving, multiple perception tasks often need to be processed synchronously, in real-time, and in parallel. A shared backbone can save computation costs and improve efficiency greatly. Thus, leveraging a unified framework to conduct multiple perception tasks simultaneously gradually becomes a research hotspot. Early works[144,145,161,162,163,164]connect multiple specific task heads after the feature extractor to simultaneously complete three tasks on the BDD100K[38]dataset: object detection, drivable area segmentation, and lane detection. These methods achieve impressive results in each task, which benefit from the powerful and efficient encoder and carefully designed multi-task learning strategy. However, the labels of lanes in BDD100K are only semantic-level annotations, and only binary segmentation methods can be used. Further post-processing is needed to distinguish each lane instance. Recent researches mainly focus on multi-task 3D perception. PETRv2[146]designs detection query, segmentation query, and lane query to support 3D object detection, BEV segmentation, and 3D lane detection simultaneously. Li et al.[147]propose a unified representation method for multiple perception tasks. They represent 3D objects and 3D lanes as a kind of 3D vector field, which allows them to leverage a single-head unified model to achieve multi-task perception.

SECTION: VI-BVideo Lane Detection

As mentioned in the previous chapters, current works attempt to supplement more global information to better detect lanes with unclear visual clues. However, these methods rely on detectors that use single images. In autonomous driving systems, video frames are captured continuously. Therefore, the correlation between frames can be used to more reliably detect obscure lanes in the current frame. For 2D lane detection, Zou et al.[165]and Zhang et al.[166]use recursive neural networks to fuse the features of the current frame with those of several past frames. Zhang et al.[41]aggregate the features of the current frame and multiple past frames based on Transformer. Tabelini et al.[167]extract lane features from video frames using LaneATT[81]and combine these features. Wang et al.[168]utilize spatiotemporal information from adjacent video frames by extending the feature aggregation module in RESA[16]. Jin et al. put RVLD[46], which includes an intra-frame lane detector to locate lanes in stationary frames and a predictive lane detector to use information from the previous frame for lane detection in the current frame. OMR[154]employs vehicle masks occupying lanes to interact with historical frames, further improving the accuracy of lane detection in the current frame. For 3D lane detection, STLane3D[169]proposes a multi-frame pre-alignment layer under the BEV space, which uniformly projects features from different frames onto the same ROI region. Anchor3DLane-T[25]incorporates temporal information by projecting the 3D anchors of the current frame onto previous frames to sample features. CurveFormer++[170]designs a temporary Curve Cross Attention module based on CurveFormer[120], which can selectively utilize historical curve query and keypoints to propagate historical information frame by frame.

SECTION: VI-COnline HD Map Construction

HD maps are an essential module for autonomous driving. Although the traditional offline method of building HD maps can generate accurate map information and is adopted by many autonomous driving companies, it requires a lot of manual annotation costs. As an alternative, an increasing number of works try to design a novel HD map learning framework that makes use of on-car sensors and computation to estimate vectorized local semantic maps.

From a process perspective, they typically follow the general pipeline of BEV perception tasks[118], taking multi-camera images as input, extracting image features using a 2D encoder, then obtaining BEV features through a view transform module, and finally outputting various map elements from the BEV perspective through a specific map element decoder. Due to the increase in the number of sensors and the fact that the map elements to be detected include but are not limited to lanes, pedestrian crossings, lane separations, and lane boundaries, the task is more challenging than monocular lane detection. Similarly, the key to this type of work is how to model map elements with different shapes, such as lines and polygons, into a set of values that can be learned through neural networks.

HDMapNet[160]adopts a basic bottom-up segmentation approach to perform semantic segmentation on all map elements. Then it combines instance embedding and post-processing clustering to obtain each map element instance. This rasterization result still requires post-processing for downstream use, therefore, subsequent work attempts to predict vectorized maps end-to-end. BeMapNet[171]models map elements as segmented Bézier curves. It detects map elements first and then regresses the detailed points with a piecewise Bézier head. VectorMapNet[172]uses a line to represent all map elements and defines a hierarchical query representation. The points of the map elements are autoregressively output through the transformer decoder. However, it outputs the point set through autoregression, which leads to low efficiency. To solve this problem, MapTR[150]designs a unique representation method for map elements. It uses lines and polygons with uniform sampling points to represent line and area elements, respectively. Therefore, all map elements are represented as sets with the same number of points and different arrangement orders. Owing to its unified permutation-equivalent modeling approach and hierarchical query design, MapTR achieves advanced performance and efficiency on the nuScenes[148]dataset solely with camera input, providing a solid baseline for follow-up research. Afterward, MapTRv2[173]improves self-attention and cross-attention in the decoder of MapTR, further enhancing both the accuracy and performance. PivotNet[151]proposes an end-to-end framework for representing map elements using pivot points. The purpose is to address the issue of shape information loss caused by using a fixed number and consistent position of points to represent complex map elements in MapTR. HIMap[152]meticulously designs feature extractors for MapTR’s hierarchical query, enabling the model to better learn instance-level features. StreamMapNet[174]improves MapTR in terms of timing. It overlays information from all historical frames together and implements a memory mechanism using recurrent late embedding. Then MapTracker[153]formalizes the online HD map construction as a tracking task and uses the history of memory latents to ensure consistency in reconstruction over time.

SECTION: VI-DLane Topology Reasoning

Topology reasoning aims to comprehensively understand road scenes and present drivable routes in autonomous driving. It requires detecting road centerlines and traffic elements, further reasoning their topology relationship, i.e., lane-lane topology, and lane-traffic topology. Directly using vehicle-mounted sensors to detect lane topology has become popular due to their practical value.

The early topology reasoning works mainly focus on lane-lane topology, i.e., detecting the centerlines to construct a lane graph. Extraction of the lane topology task is first proposed by STSU[175], which predicts the centreline and lane connectivity relationships. TopoRoad[176]uses a set of directed lane curves and their interactions to represent road topology. Can et al.[177]provide additional supervision of the relationship by considering the centerlines as cluster centers to assign objects. LaneGAP[158]utilizes the shortest path algorithm in graph theory to transform lane topology into a series of overlapping paths and directly obtains information about these complete paths through end-to-end learning. CenterLineDet[178]regards centerlines as vertices and designs a graph model to update centerline topology. Recently, lane-traffic topology is additionally introduced by the OpenLane-V2[155]dataset, further improving the understanding of scene structure. Aiming at a complete and diverse driving scene graph, TopoNet[179]explicitly models the connectivity of centerlines within the network and incorporates traffic elements into the task. TopoMLP[156]leverages position embedding[139]to enhance topology modeling. LaneSegNet[157]proposes a unified representation for integrating lanes and centerlines. It introduces a lane attention mechanism to facilitate the learning of topological relationships between centerlines and lanes. To improve lane topology inference, TopoLogic[159]introduces an efficient post-processing that integrates the geometric distance between centerline endpoints and the semantic similarity of lane queries within a high-dimensional space.

SECTION: VIIFuture Direction

This section outlines potential directions for future research in lane detection. The scope of our discussion includes the improvable issues within the field, underexplored subfields, and relevant tasks outside this area that hold significant research value.

General and Unified Lane Modeling.Effectively modeling lanes of arbitrary shapes without compromising efficiency remains a significant challenge. When the scenario extends from a monocular camera’s front view to multi-camera surround views, the presence of numerous U-shaped or nearly horizontal lanes becomes common. In such cases, modeling approaches that heavily rely on prior knowledge, such as grids-based modeling for row-wise classification, or line anchor-based modeling for learning the longitudinal equidistant offset points, are unsuitable. In contrast, mask-based modeling methods are more reliable despite their lower performance and efficiency. The existing vectorized map element modeling methods[172,150]provide valuable guidance on this issue. Recently, Lane2Seq[143]unifies 2D lane detection through sequence generation. This approach also serves as a promising direction for subsequent studies, although its efficiency still requires further improvement.

Multi-modal Lane Detection.In recent years, LiDAR-based lane detection benchmarks and methods[180,181,182,183,184,185,186]gain attention as another minority approach. Although the 3D information can be directly offered by LiDAR, its shorter perception range and high cost have made camera-based methods more prevalent. However, LiDAR provides the advantages by remaining unaffected by lighting changes and delivering accurate depth information, effectively compensating for the limitations of cameras. The integration of LiDAR and camera data demonstrates significant effectiveness in enhancing performance, which is widely validated in the domain of 3D perception[187,188,189,190]. However, there are notably few dedicated works focusing on multi-modal lane detection[191,192,193].

Label Efficient Lane Detection.The existing lane detection methods mainly focus on supervised learning, which needs a lot of annotations for training, which leads to huge manual costs. Thus, developing annotation-efficient lane detection algorithms is necessary. WS-3D-Lane[194]uses 2D lane labels to weakly supervise 3D lane detection, which is valuable for research and mass production. Furthermore, unsupervised lane detection[195,196,197,198]is also a promising direction, although the related works are limited.

Lane Detection in End-to-End Autonomous Driving.The CVPR Best Paper, UniAD[199], attracts significant interest[200,201,202,203,204]in both academia and industry regarding the development of end-to-end autonomous driving systems. Unlike the conventional modular architecture of ’perception-prediction-planning’, end-to-end autonomous driving directly outputs vehicle motion planning results from sensor data in a fully differentiable manner. Within this framework, lane detection no longer outputs explicit lane coordinate values but instead functions as a module providing intermediate representations of lanes. However, this approach often encounters challenges such as limited interpretability and inadequate generalization, particularly in complex road scenarios, including curved roads and multi-lane switching. Future research might explore hybrid architectures that incorporate specific lane detection outputs, such as lane centerlines, lane width, and curvature, as prior knowledge into intermediate representations within end-to-end models. This integration can enable the network to better capture the structural characteristics of roads and ensure the preservation of critical lane-level information in the decision-making process.

Visual Reasoning for Lane Detection.The advent of large language models (LLMs) and vision-language Models (VLMs) unlock significant potential for multimodal artificial intelligence systems to perceive the real world, make decisions, and control tools with a capability akin to human cognition. Recently, LLMs and VLMs are integrated into autonomous driving systems, primarily focusing on visual reasoning related to dynamic objects, the generation of future trajectories, and the detailed control signals of ego-vehicles[205,206,207]. In contrast, relatively few studies explore visual inference concerning static objects, such as lanes. Fortunately, a new benchmark[208]specifically designed for large-scale visual reasoning in understanding maps and traffic scenarios is emerged. By training on extensive traffic scene data, the models can derive insights from complex multi-modal driving resources, including map data, traffic regulations, and incident reports. This enables them to enhance vehicle navigation and planning with safety and efficiency parameters, while also adapting to dynamic road conditions with an understanding that closely resembles human intuition.

Roadside Lane Detection.The current perception capabilities in autonomous driving primarily focus on ego vehicles. While vehicle-based perception systems capture the immediate surrounding environment, their range is limited to short distances. In contrast, roadside cameras, mounted on utility poles several meters above ground, enable remote perception with minimal visual obstructions. Recently, roadside 3D object detection datasets[209,210]and corresponding methods[211,212,213]are developed to promote 3D perception tasks in roadside scenes, facilitating potential collaboration between vehicles and infrastructure. However, there are still no established benchmarks or methods specifically for roadside lane detection. Roadside lane detection can effectively substitute for manual monitoring of lane violations or illegal lane changes, offering significant potential for applications in security.

SECTION: VIIIConclusion

This survey comprehensively reviews the latest progress in monocular lane detection based on deep learning, covering both 2D and 3D lane detection methods in recent years.
Four core designs in lane detection algorithms are identified through theoretical analysis and experimental evaluation: (1) Task paradigm, focusing on lane instance-level discrimination; (2) Lane modeling, representing lanes as a set of learnable parameters in the neural network; (3) Global information supplementation, enhancing the inference on the obscure lanes; (4) Perspective effect elimination, providing accurate 3D lanes for downstream applications. From these perspectives, this paper presents a comprehensive overview of existing methods, encompassing both the increasingly mature 2D lane detection approaches and the developing 3D lane detection works.
In addition, this article also reviews extended works on monocular lane detection to provide readers with a more comprehensive understanding of the development of lane detection.
Finally, the future research directions for lane detection are pointed out.

Overview.In this appendix, we provide more details as a supplementary adjunct to the main paper.

More descriptions on task metrics. (Section-A)

More benchmark results. (Section-B)

Correspondence between 3D lanes and images. (Section-C)

SECTION: -AMore Task Metrics

In this section, we present detailed descriptions of more indicators for lane detection task metrics.

Accuracy (Acc).For Tusimple[35]dataset, accuracy will also be used as an indicator, and the evaluation formula is

whereare the number of correct points and the number of ground truth points of an image respectively.

Average Precision (AP).It is more often used to evaluate Apollo 3DLane[23]. As described in SectionII-B, the TP under different thresholds can be obtained by selecting the decision criteria for TP and iterating the lane confidence thresholds. Then the exact recall curve can be generated and the AP can be obtained by calculating the area under this curve.

X Error and Z Error in 3D Lane Detection.When GT matches the corresponding predicted lane,is defined as

Whereis thecoordinate of the GT sampling point,is thecoordinate of the matched prediction point, andis the number of points on the lane.

Chamfer Distance (CD).This metric proposed by ONCE-3DLanes[44]is used to calculate the curve matching error in the camera coordinate system. The curve matching errorbetweenandis calculated as follows:

whereandare point ofandrespectively, andis the nearest point to the specific point.represents the number of points token at an equal distance from the ground-truth lane.

SECTION: -BMore Benchmark Results

Results on other 2D lane detection datasets.TableVIIshows the performance comparison on Tusimple[35], LLAMAS[39]and CurveLanes[40].

Results on other 3D lane detection datasets.We report the performance comparison on ONCE-3DLanes[44]in TableIX, and Apollo 3DLane[23]in TableVIII.

SECTION: -CCorrespondence between 3D Lanes and Images.

This section introduces the correspondence between 3D lanes and images. Figure9depicts the imaging process of 3D lanes in the camera. Utilizing the commonly employed pinhole camera projection as an illustration, the projection process encompasses transformation between the ego-vehicle, camera, image, and pixels.

The transformation from the ego-vehicle coordinate system to the camera coordinate system involves translation and rotation exclusively. Consider,as the homogenous coordinates of a 3D pointin the ego-vehicle and camera coordinate systems, respectively. Their relationship is elucidated as follows:

where,refer to a rotation matrix and a translation matrix respectively.

The image coordinate system is employed to represent the perspective projection from the camera coordinate system onto the image plane. When the camera distortion is disregarded, the relationship between a 3D point and its image plane projection can be simplified using a pinhole model. The image coordinatesare determined by Eqn.14:

whererepresents the focal length of the camera.

The translation and scaling transformation links the image coordinate framework with the pixel coordinate framework.
Letanddenote the scaling factors for the x-axis and y-axis, respectively, whileandrepresent the translation values shifting the origin of the coordinate system. The pixel coordinatescan be mathematically formulated as shown in Eqn.15:

With Eqn.14and Eqn.15, setting,, we could derive Eqn.16:

To sum up, the relationship between the 3D pointin the ego-vehicle coordinate system and its corresponding projectionin the pixel coordinate system can be described as:

The matrixis known as the camera intrinsics, while the matrixrepresents the camera extrinsics. By leveraging the intrinsics and extrinsics along with the ego-vehicle coordinates of 3D points, it is possible to calculate their projections onto the image plane through the corresponding transformation.

SECTION: References