SECTION: Local Padding in Patch-Based GANs for Seamless Infinite-Sized Texture Synthesis

Texture models based on Generative Adversarial Networks (GANs) use zero-padding to implicitly encode positional information of the image features. However, when extending the spatial input to generate images at large sizes, zero-padding can often lead to degradation in image quality due to the incorrect positional information at the center of the image. Moreover, zero-padding can limit the diversity within the generated large images. In this paper, we propose a novel approach for generating stochastic texture images at large arbitrary sizes using GANs based on patch-by-patch generation. Instead of zero-padding, the model useslocal paddingin the generator that shares border features between the generated patches; providing positional context and ensuring consistency at the boundaries. The proposed models are trainable on a single texture image and have a constant GPU scalability with respect to the output image size, and hence can generate images of infinite sizes. We show in the experiments that our method has a significant advancement beyond existing GANs-based texture models in terms of the quality and diversity of the generated textures. Furthermore, the implementation of local padding in the state-of-the-art super-resolution models effectively eliminates tiling artifacts enabling large-scale super-resolution. Our code is available athttps://github.com/ai4netzero/Infinite_Texture_GANs.

Keywords:Generative Adversarial Networks (GANs), Texture Synthesis, Large-scale generation.

SECTION: 1Introduction

Texture synthesis refers to the process of generating arbitrary-size textures that are visually similar to a given input example, while also being diverse and not a simple duplication of the input. This problem has numerous applications in fields such as gaming, virtual reality and graphic design, where high-quality textures are critical for creating realistic and visually appealing environments. The use of Generative Adversarial Networks (GANs)[1]in generating realistic textures has been widely explored. Several methods have been proposed for this task, including Spatial GAN[2], PSGAN[3], adversarial expansion[4]and SinGAN[5]. However, using these models to generate arbitrary large-scale textures while maintaining their quality and diversity is not trivial.

Increasing the input latent space size in these models is limited by GPU memory, which restricts the size of the output image. In addition, when using zero-padding, extending the input dimensions results in degraded quality due to the propagation of incorrect positional information. Furthermore, zero-padding can cause limited variability at the corners of the generated images and can often result in tiling or seaming artifacts between the generated patches in incremental generation, i.e., patch-by-patch generation. This problem is more pronounced in image translational tasks, where several works have tried to alleviate the discontinuities at the patch borders either by post-training tiling[6]or integrating the tiles into training[7].

To address these challenges, we propose a patch-based GAN model with a novel padding method that is capable of synthesizing stochastic textures of infinite size and that is trainable on a single texture image. Because of the self-similarity and homogeneity within texture images, it is sufficient to maintain their structure by sharing local information only. Relying on this concept, our model useslocal padding, instead of zero-padding, where the inputs to the generatorâ€™s convolutional layers are padded with content from neighbouring patches to ensure seamless concatenation. During training, the model generates small, locally correlated patches, which it learns to seamlessly stitch together using padded information. Using incremental generation, at the inference time, we can extend the generation to arbitrary large sizes while maintaining the texture structure and diversity.

The main contributions of the paper:

We proposelocal paddingas a new way of padding the inputs before convolutional operations that allows seamless patch-by-patch texture synthesis.

We demonstrate that our trained models are capable of generating higher-quality texture images than existing models while maintaining the fine details and variability exhibited by the original examples and that they are scalable to any resolution by incremental generation.

We also show that local padding can be used in the state-of-the-art super-resolution models, such as Real-ESRGAN[8], to avoid tiling artifacts when super-resolving large inputs.

The paper is organized as follows: in section2, we present a review of related work. In section3, we discuss the proposed patch-by-patch generation with local padding. In section4, we present several applications of the developed method and discuss the results. Finally, the conclusions of this manuscript are presented in section5.

SECTION: 2Literature Review

GANs for texture synthesis.Generative Adversarial Networks (GANs)[1]have gained significant attention in recent years due to their ability to generate realistic images. They have found numerous applications in computer vision, ranging from image-to-image translational tasks[9,10,11], super-resolution[12,8,13], and image in-painting[14,15]. GANs have also shown great potential for generating realistic textures, where the challenge is to generate samples of arbitrarily large sizes while preserving the coherence and consistency of the given example. Spatial GAN[2]builds upon the DCGANs architecture[16]by transforming the generator and discriminator into fully convolutional networks, where the output texture image can be expanded in size by expanding the spatial input. Periodic Spatial GAN (PSGAN)[3]proposes to generate textures with periodic patterns by incorporating a periodic input into the generator network. Adversarial expansion[4]trains a GAN model to double the size of the input texture image. However, expanding the input size of the latent space leads to incorrect positional encoding in the generated images and hence degrades the quality. Moreover, adversarial expansion models do not parametrize the stochasticity of the texture, and instead performed diversification by shuffling and cropping.

SinGAN[5]trains multi-scale generators to generate realistic images, including textures, from a single input image. TileGAN[17]designed a tiling framework to synthesize large-scale texture images based on a neighbourhood similarity search. While these models successfully generated high-resolution images of textures, the zero-padding used leads to limited spatial variability around the boundaries of the generated images in case of SinGAN and visible artifacts between the tiles with TileGAN. In addition, the TileGAN method requires storing latents representations of a large number of generated examples to be searched for similarity matching, which is time-consuming.

Incremental Generation.In patch-by-patch generation, the model synthesizes one small patch at a time, then correlated patches are assembled to form a larger image. This allows the model to generate images with an infinite size and avoid the problem of limited resources and the training instabilities associated with generating a large image in a single forward pass[18]. COCO-GAN[19]trains a GAN model that conditions image patches on coordinates and then assembles patches that share a global latent vector. This allows for limited extrapolation of the images by extending the coordinates. InfinityGAN[20]then extended the method to natural images by employing a padding-free generator. The authors removed zero-padding in their generator and instead padded the inputs with neighbouring content, which allowed for seamless concatenation. To model the position of the patches, (e.g., sky, land), they employed an implicit neural function with CoordConv[21], where the hidden representations are concatenated with positional embeddings.

LocoGAN[22]trains fully convolutional GANs to generate sub-images instead of the full image and uses coordinates to inform the model which part of the image is being generated. The main drawback of their model is that it is limited to periodic texture due to the nature of the periodic coordinates used. ALIS[23]used a spatially-equivariant generator where they modified the AdaIN algorithm[24]such that the modulating parameters are spatially-interpolated. This approach enabled the generation and assembly of vertical patches of natural scenes. However, the model suffers from content repetition when the global anchors do not change fast enough to allow for variations. In addition, the padding used in the generation leads to blocky artifacts and discontinuity between the generated patches. Unlike natural images that require global coordination between the different patches, texture images exhibit a high degree of locality and self-similarity. Applying local padding at every layer enables our model to capture local texture structure without explicit global coordination between patches.

SECTION: 3Methodology

The proposed method relies on homogeneity and self-similarity within texture images. This property allows us to synthesize texture patches using only information from the neighbouring patches. An overview of the method is presented in Figure1. Following[2], both the generator and discriminator are fully convolutional neural networks. Multiple patches are randomly cropped from the single texture image and are fed into the discriminator. Unlike traditional approaches that expand spatial noise to generate larger images, we limit the generator network to simultaneously produce,small-size patches, each patch is of size. In the demonstration Figure1,andare set to 3 and 128, respectively. The model learns to seamlessly assemble these patches into one image, whereis a simple concatenation function. The assembled image is then passed to the discriminator. The seamless assembly is facilitated by shared information between patches, achieved through a novel padding technique we calllocal padding.

Local Padding.Since most texture images are stationary and homogenous, capturing the local spatial structure can be achieved by sharing border features of the patches. Therefore, we pad the input to the convolutional layers in the generator with content from the neighbouring patches, instead of the conventional zero-padding. We introducelocal paddingas a type of padding used in patch-based generation, where the inputs to the convolutional operations at all levels in the generator are padded with the boundary content of neighbouring patches.

Typically, padding involves adding extra rows and columns of zeros around the input to a convolutional layer to ensure that the convolutional filters can be applied to the edges. However, using zero-padding in a patch-by-patch generator results in visible seams between the concatenated patches. This mismatch occurs because the pixels at the boundaries may not align perfectly with those in the neighbouring patches. Local padding addresses this issue by filling in the padded pixels with values taken from the layer input of the neighbouring patches, as depicted in Figure2. The figure shows a 1-dimensional generation problem, where the rows on the left side represent inputs corresponds to 3 different image patches. that are assembled together horizontally to form one image. Local padding is applied to the inputs of all convolutional layers in the generator network.

Global operations such as batch-normalization and nearest neighbour up-sampling are still performed on each patch independently. This ensures that the convolutional filters can be applied to the edges of the feature maps without losing border information. In addition, sharing the padding between the patches and assembling them together during training allows the model to learn to seamlessly stitch the patches based on the shared padding. Padding from neighbouring patches is similar to[20], however we perform the padding at all layers in the generator not just at the input layer. This provides the neighbouring patches context for all levels when generating the local patch and passes the positional information without the need for explicit coordination.

The amount of padding applied depends on the size and the number of the convolutional filters used. For example, ainput is padded with 1 value at both dimensions to be of sizebefore passing it to aconvolution. For the outer patches, we use replicate padding to extend the input along the edges, as we do not have neighbouring patches to provide padding values. This approach lends itself to an easy extension to infinite image generation as we will detail next.

Scaling to Infinite Sizes.We explain the scaling process for 1-dimensional patches in the horizontal direction in Figure3. First, using patch generation with local padding, we can generate an imagewhich is a concatenation of the patchesand. The same process is repeated to generate a new image. However, the rightmost patch in(i.e.,), which was generated using replicate padding, will be regenerated inusing local padding, i.e., by padding from features inand, so that the regenerated patchis consistent with both images. The patchis then dropped and the remaining patches are concatenated with the patches into form a larger image. In the last two rows of the figure, we show examples in which the leftmost patch inis similar to the rightmost patch in, except that the right side has been modified with local padding to match the interior patches in. This incremental process can be repeated in the two dimensions to generate images of infinite sizes while maintaining constant GPU memory and avoiding visible seams or artifacts between the patches.

SECTION: 4Experiments

SECTION: 4.1Experiment Setup

The training examples used in the experiments were obtained from the supplementary materials in[4]. We selected the images such that they are homogeneous and stationary in nature. This is because the developed patch-by-patch approach cannot handle non-stationary patterns.

The generator network is built using ResNets blocks[25]with batch-normalization and nearest neighbour upsampling operations. In addition, we modified the generator to be a fully convolutional network similar to[2,3], removed all zero-padding following[20], and used local padding before every convolutional layer instead. The architecture of the generator is shown in Figure4, where the spatial inputis passed through successive residual blocks interleaved by upsampling layers. We used a PatchGAN discriminator[26,27], which is designed to provide a more fine-grained evaluation of the generated images by focusing on the local features of the image rather than the global features. Similar to[3], we removed the normalization layers in the discriminator and found this to be more stable and boosted performance when training on a single image. The GANs model is trained with the non-saturating logistic loss with spectral normalization[28]applied to the discriminator weights. The learning rate of both the generator and discriminator is set to 0.0002 and Adam[29]is used for optimization withand.

During training, we set. While other combinations are possible,is the simplest combination in which the generator can learn the local information between patches (i.e., 1 central and 8 neighbouring patches). During inference, one can setto be larger to maximize GPU utilization. The choice of the cropping size depends on the size of the training image, with larger cropping size leading to less diversity, since we are training on a single image. Moreover, the receptive field (RF) of the PatchGAN discriminator is selected to be larger than the size of the texture objects presented in the image so that the model can evaluate them. We present in Table1, the hyper-parameters used in some of the experiments, where the first column refers to the training image name in the supplementary materials of[4].

SECTION: 4.2Results

First, we present the visual quality of some generated textures by the proposed method in Figure5. As observed, the method generates textures with fine details, preserving the overall structure and diversity of the original examples. In Figure6, we compare the proposed approach against a number of published methods including: Adversarial Expansion[4], PSGAN[3], and SinGAN[5]in terms of the quality of the generated texture. As shown, both adversarial expansion and PSGAN generate texture of lower quality since expanding the spatial input of the model changes the positional encoding.

While SinGAN generates reasonable results, it tends to produce smooth images due to the multi-scale training scheme. As a result, some of the high-frequency details in the original example are lost, as shown in the last row. Moreover, because of the zero-padding used in SinGAN, the generated examples exhibit limited variability around the boundaries as shown in Figure7, where we plot the standard deviation of 50 generated samples computed per pixel.

To generate regular texture, we incorporated periodic inputs proposed in PSGAN in the latent space of our models. In Figure8, we compare between our method with periodic input, Adversarial Expansion and PSGAN based on generated regular textures. As shown adversarial expansion tries to duplicate the structure presented in the original examples with minimal stochastic variations across the spatial domain. In addition, the zero-padding creates boundary artifacts in the generated textures. PSGAN tends to generate repetitive patterns, diminishing its capacity to produce diverse outputs. On the other hand, the proposed method was able to maintain the regularity of the texture as well as generate diverse and stochastic outputs.

Table2presents a quantitive comparison using SIFID (Single Image FID)[5], a metric used to assess the quality and the diversity of the generated images by computing the FID distance between feature statistics of the real image and those of the generated samples. Average SIFID values are calculated over 50 images for PSGAN[3], SinGAN[5], and our method. We also show, in Table3, FID values calculated for random image patches selected from the real image and the generated images. The results show that the texture images generated by our method consistently achieve lower SIFID and FID values across all cases, indicating superior quality and diversity compared to those produced by PSGAN and SinGAN. SinGAN models, in particular, tend to have large SIFID values due to limited variability around the corners of the generated images. Finally, in Table4, we compare between the developed method and other related GANs work based on GPU scalability, the use of coordinates, the use of zero-padding, and whether they are trainable on single images.

Local Padding in Super-Resolution models.Super-resolution based on deep learning models has been an active area of research in the last few years, where the models learn to reproduce the high-frequency details lost in compressed, noisy or blurry images. However, super-resolving large images is limited by the GPU memory, and hence tiling is often used where the large input image is broken down into smaller overlapping tiles or patches. Each tile is processed independently, and the outputs are stitched back together to form the final large image. However, seaming artifacts can appear when assembling the patches together due to mismatches around the boundaries. To address this problem, we apply local padding in the state-of-the-art Real-ESRGAN[8]model.

Since no global operations is used in the Real-ESRGAN generator (e.g., no batch normalization), we can apply the method directly to the pre-trained model by dropping all the zero paddings and instead use local padding. In Figure9, we used Real-ESRGAN to super-resolve images using both tiling with different overlapping sizes and local padding. Although increasing the overlapping size between the patches smooths down the discontinuities, the cutting lines are inevitable. On the other hand, local padding resulted in no discontinuity between the patches and produced details similar to the single forward pass to the model.

Ablation study.To evaluate the effectiveness of the proposed method, we conducted an ablation study where we ablated the local padding and replaced it with the conventional zero-padding. Figure10shows examples of images generated using zero-padding where noticeable seams and discontinuities become apparent between patches, disrupting the visual coherency of the generated output. In contrast, the utilization of local padding in the generator effectively mitigates these issues, resulting in a visually consistent and coherent image as shown in Figure5.

We also studied the effect of having a fully convolutional generator instead of a traditional generator that uses a fully connected input layer followed by convolutional layers similar to[18]. The results in Figure11show that the fully convolutional generator is able to generate diverse and visually appealing textures, while the generator with a fully connected layer suffers from spatial mode collapse, producing less varied textures.

SECTION: 5Conclusion

We have presented a novel approach for synthesizing textures of infinite size, trained on a single image. The proposed patch-based generation with local padding addresses the limitation of memory scalability and the generation of high-quality, diverse, large size textures, that have challenged previous methods. The trained models can successfully generate visually appealing texture images with intricate details and seamless transitions between patches. Large-scale textures can also be synthesized incrementally in a scalable mannar without a proportional growth in GPU memory usage. Nevertheless, this approach has its limitations, including the requirements to cache a fraction of the feature maps in the scaling steps and the inability to handle non-stationary textures. Future work could address the latter problem by incorporating global inputs to capture long term relationships in non-stationary patterns.

SECTION: 6Acknowledgment

This work was partially funded by the EPSRC grant reference EP/Y006143/1. The first author acknowledges TotalEnergies for supporting the early part of this work conducted during his PhD studies at Heriot-Watt University.

SECTION: References