SECTION: Enhancing Robustness and Efficiency of Least Square Twin SVM via Granular Computing
[inst1]organization=Department of Mathematics, Indian Institute of Technology Indore,addressline=Simrol,
city=Indore,
postcode=453552,
state=Madhya Pradesh,
country=India

SECTION: Introduction
Support vector machine (SVM)is a powerful model in machine learning that uses kernels to precisely determine the best hyperplane between classes in classification tasks. SVM provides a deterministic classification result. Hence, its application can be found in various domains such as health care, anomaly detection, web mining, electroencephalogram (EEG) signal classification, Alzheimer’s disease diagnosis, and so on. SVM implements the structural risk minimization (SRM) principle and, hence, shows better generalization performance. SVM solves a convex quadratic programming problem (QPP) to find the optimal separating hyperplane. However, the effectiveness and efficiency of SVM are limited when dealing with large datasets due to the increase in computational complexity. Further, SVM is sensitive to noise, especially along the decision boundary, and is unstable to resampling. To mitigate the effects of noise and outliers in data points, fuzzy SVM (FSVM)was proposed. The incorporation of pinball loss in SVMled to a better classifier that has the same computational complexity as SVM but is insensitive to noise and stable to resampling. To overcome the issue of the computational complexity of SVM, some non-parallel hyperplane-based classifiers have been proposed, such as generalized eigenvalue proximal SVM (GEPSVM)and twin SVM (TSVM). The GEPSVM and TSVM generate non-parallel hyperplanes that position themselves closer to the samples of one class while maximizing their distance from samples belonging to the other class. GEPSVM solves two generalized eigenvalue problems, and its solutions are determined by selecting the eigenvectors corresponding to the smallest eigenvalue. However, TSVM solves two smaller QPPs, which makes its learning process approximately four times faster than SVM. Also, TSVM shows better generalization performance than GEPSVM. The efficiency of TSVM may decrease due to its vulnerability to noise and outliers, potentially leading to unsatisfactory results. Moreover, its substantially high computational complexity and reliance on matrix inversions pose significant challenges, especially when dealing with large datasets, thereby impeding its real-time applications. Also, TSVM does not adhere to the SRM principle, making the model susceptible to overfitting.proposed least squares TSVM (LSTSVM). Unlike TSVM, LSTSVM incorporates an equality constraint in the primal formulation instead of an inequality constraint. This modification allows LSTSVM to train much faster than TSVM, as it solves a system of linear equations to determine the optimal nonparallel separating hyperplanes. However, despite its success in reducing training time, the methodology of LSTSVM involves the computation of matrix inverses, which limits its applicability to large datasets. Additionally, the LSTSVM’s ability to learn decision boundaries can be significantly affected by the presence of noisy data and outliers. Some recent advancements in TSVM and LSTSVM include
capped-norm metric-based robust LSTSVM for pattern

classification,
the Laplaciannorm LSTSVM, sparse solution of least-squares twin multi-class support vector machine usingand-norm for classification and feature selection, the inverse free reduced universum TSVM for imbalanced data classification, symmetric LINEX loss TSVM for robust classification and its fast iterative algorithm, and intuitionistic fuzzy weighted least squares TSVM (IFW-LSTSVM). These advancements contribute to the ongoing progress in TSVM and LSTSVM techniques. A comprehensive overview of the various versions of TSVM can be found in.

The concept of “large-scale priority” aligns with the natural information-processing mechanism of the human brain. Granulation, or breaking down information into smaller, more manageable parts, is a fundamental aspect of the learning process. Our brains are wired to absorb and process information in layers, starting with a broader concept and then delving into the specifics as needed. This approach allows us to grasp the big picture first and then gradually fill in the details, leading to a more comprehensive understanding. Drawing inspiration from the brain’s functioning, granular computing explores problems at various levels of detail. Coarser granularity emphasizes important components, thereby enhancing the effectiveness of learning and resistance to noise. Conversely, finer granularity offers intricate insights that deepen knowledge.
In contrast, a majority of machine learning models rely on pixels or data points for training at the lowest possible resolution. Consequently, they are often more susceptible to outliers and noise. This approach lacks the efficiency and scalability of the brain’s adaptable granulating capabilities. The novel classifier based on granular computing and SVMwas developed to incorporate the concept of granular balls. This classifier utilizes hyper-balls to partition datasets into different sizes of granular balls. As highlighted in, this approach, which imitates cognitive processes observed in the human brain, offers a scalable, dependable, and efficient solution within the realm of granular computing by introducing larger granularity sizes. However, this transition may compromise the accuracy of fine details. Conversely, finer granularity enhances the focus on specific features, potentially improving precision but also introducing challenges in terms of robustness and efficiency in noisy scenarios. Consequently, striking the right balance between granularity and size becomes imperative.
Researchers persistently explore novel applications, refine existing methodologies, and bridge interdisciplinary gaps to harness the potential of granular computing across diverse domains.

Recently, the granular ball SVM (GBSVM)has been proposed, which integrates the concepts of SVM with granular computing. GBSVM addresses a single quadratic programming problem using the PSO algorithm, which can sometimes lead to convergence at local minima. To overcome this limitation,introduced the Granular Ball TSVM (GBTSVM) and its large-scale variant. These models solve two complex quadratic programming problems, improving the performance and robustness of the model. GBTSVM exhibit good performance in effectively managing datasets that are contaminated with noise and outliers. Several other variants of the GB-based classification models have been proposed, such as GBTSVM based on robust loss function, enhanced feature-based GBTSVM, and randomized neural networks based on granular computing.
Motivated by the robustness and efficiency demonstrated by the GBTSVM, we incorporate the concept of granular computing into the LSTSVM and propose a novel model called the granular ball least square twin support vector machine (GBLSTSVM). This integration aims to address the inherent drawbacks and complexities associated with LSTSVM. GBLSTSVM uses granular balls as input to construct non-parallel separating hyperplanes by solving a system of linear equations like in the case of LSTSVM. The granular balls are characterized by their center and radius. The construction of granular balls based on granular computing is elaborated in Section II (B). The essence of a granular ball lies in its center, which encapsulates all the relevant information of the data points that lie within the ball. In comparision to LSTSVM, GBLSTSVM provides enhanced efficiency, a heightened resistance to noise and outliers, robustness to resampling, and is trained using a substantially reduced number of training instances, thereby significantly reducing the training time. However, GBLSTSVM lacks the SRM principle, which can lead to the potential risk of overfitting. To address this, we further propose the novel large-scale GBLSTSVM (LS-GBLSTSVM) model. LS-GBLSTSVM incorporates the regularization terms in its primal form of the optimization problem which eliminates the need for matrix inversions and also helps to mitigate the risk of overfitting. The main highlights of this paper are as follows:

We propose the novel GBLSTSVM by incorporating granular computing in LSTSVM. The GLSTSVM is trained by feeding granular balls as input instead of data points for constructing optimal non-parallel separating hyperplanes. The use of granular balls reduces the training time by a substantial amount, amplifies the model’s performance, and elevates the robustness against noise and outliers.

We propose the novel LS-GBLSTSVM by implementing the SRM principle through the inclusion of regularization terms in the primal formulation of GBLSTSVM. LS-GBLSTSVM does not require matrix inversion, making it suitable for large-scale problems. In addition, LS-GBLSTSVM offers robust overfitting control, noise and outlier resilience, and improved generalization performance.

We present the meticulous mathematical frameworks for both GBLSTSVM and LS-GBLSTSVM on linear and Gaussian kernel spaces. The formulation integrates the centers and radii of all granular balls used in training into the LSTSVM model. These models excel in capturing complex data patterns and relationships through sophisticated nonlinear transformations.

We conducted the experiments of our proposed GBLSTSVM and LS-GBLSTSVM models using 34 UCI and KEEL datasets with and without label noise. Our comprehensive statistical analyses demonstrate the significantly superior generalization abilities of our proposed models compared to LSTSVM and the other baseline models. Further, we performed experiments on NDC datasets of sample sizes ranging from 10,000 to 5 million to determine scalability. The results demonstrated that our proposed models surpass the baseline models in terms of accuracy, efficiency, robustness, and scalability.

The subsequent sections of this paper are structured as follows: in Section, an overview of related work is provided. In Sections III and IV, we present the mathematical framework of our proposed novel GBLSTSVM and LS-GBLSTSVM in both linear and Gaussian kernel spaces, and the computational complexity is discussed in Section V. The experimental results and discussions are presented in Section VI, followed by the conclusions and recommendations for future research are provided in Section.

SECTION: Related Works
This section begins with establishing notations and then reviews the concept of granular computing. Also, we briefly outline the mathematical formulations along with their solutions of GBSVM and LSTSVM models.

SECTION: Notations
Let the training dataset, whererepresent the feature vector of each data sample andrepresents the label of the individual data. Let the number of granular balls generated onbe. Letandbe the center and radius of the granular ball. Letandbe the feature matrices of the centers of the granular balls with labelclass andclass, respectively, such thatis the total number of granular balls generated on. Letandbe the column vectors containing the radius of all granular balls having labeland, respectively.is the transpose operator andrepresents the column vectors of ones of appropriate dimensions.

SECTION: Granular Computing
In 1996, Lin and Zadeh proposed the concept of “granular computing”. It becomes computationally expensive to process every data point in the space when dealing with large datasets. The objective of granular computing is to reduce the number of training data points required for machine learning models. The core idea behind granular computing is to use granular balls to completely or partially cover the sample space. This captures the spirit of data simplification while maintaining representativeness during the learning process. Granular balls, characterized by two parametric simple representations, a centerand a radius, are the most appropriate choice for effectively handling high-dimensional data. Given a granular ballcontaining the datapoints, where, the centerof ais the center of gravity for all sample points in the ball, andis equal to the average distance fromto all other points in. Mathematically, they can be calculated as:and.
The average distance is utilized to calculate the radiusas it remains unaffected by outliers and aligns appropriately with the distribution of the data. The label assigned to a granular ball is determined by the labels of the data points that have the maximum frequency within the ball. To quantitatively assess the amount of splitting within a granular ball, the concept of “threshold purity” is introduced. This threshold purity represents the percentage of the majority of samples within the granular ball that possess the same label. The number of granular balls generated onis given by the following optimization problem:

whereandare weight coefficients.is the threshold purity.represents the cardinality of a granular ball, andandrepresent the number of samples inand the number of granular balls generated on, respectively.

The quality of each granular ball is adaptive. Initially, the whole dataset is considered as a single granular ball, which fails to accurately represent the dataset’s distribution and exhibits the lowest level of purity. In the cases where the purity of this granular ball falls below the specified threshold, it is necessary to divide it multiple times until all sub-granular balls achieve a purity level equal to or higher than the threshold purity. As the purity of the granular balls increases, their alignment with the original dataset’s data distribution improves. Fig. 1 depicts the procedure of granular ball generation.

SECTION: Least Square Twin Support Vector Machine (LSTSVM)
Suppose matrixandcontain all the training data points belonging to theandclass, respectively. The primal problem of LSTSVMcan be expressed as:

and

whereandare slack variables.
Substituting the equality constraint into the primal problem, we get

and

Taking gradient of (4) with respect toandand solving, we get

Similarly,

whereand

Once the optimal values ofandare calculated. The
categorization of a new input data point xinto either theorclass can be determined as follows:

SECTION: Granular Ball Support Vector Machine (GBSVM)
The basic idea of GBSVMis to mimic the classical SVMusing granular balls during the training process instead of data points. This makes GBSVM efficient and robust in comparison to SVM. The parallel hyperplanes in GBSVM are constructed using supporting granular ballshaving support centerand support radius. Fig. 2 depicts the construction of inseparable GBSVM using supporting granular balls.

The inseparable GBSVM model can be expressed as:

The dual of inseparable GBSVM formulation is:

where’s are Lagrange multipliers.

SECTION: THE PROPOSED GRANULAR BALL LEAST SQUARE TWIN SUPPORT VECTOR MACHINE (GBLSTSVM)
In this section, we introduce a novel GBLSTSVM to tackle the binary classification problem. We propose the use of granular balls that encompass either the complete sample space or a fraction of it during the training process. These granular balls, derived from the training dataset, are coarse and represent only a small fraction of the total training data points. This coarse nature renders our proposed model less susceptible to noise and outliers.

By leveraging the granular balls, we aim to generate separating hyperplanes that are nonparallel and can effectively classify the original data points. In the construction of optimal separating hyperplanes, we aim to utilize the maximum information stored in all the training data points while simultaneously decreasing the data points required to find optimal separating hyperplanes.

Hence, we incorporate both the centers and radii of all the granular balls generated through granular computing into the primal formulation of LSTSVM. The integration of granular balls in the training process not only enhances the LSTSVM’s robustness against noise and outliers but also significantly reduces the training time, offering enhanced robustness and reduced computational complexity. The geometrical depiction of GBLSTSVM is shown in Fig. 3.

SECTION: Linear GBLSTSVM:
The optimized formulation of linear GBLSTSVM is given by:

and

whereandare slack variables andandare tunable parameters.andare the hyperplane parameters. In equations () and (), the incorporation of centers and radii of granular balls is represented using matricesand, along with column vectorsand, respectively.
To solve (), we substitute the equality constraint into the primal problem

Taking gradient with respect toandand equating to 0, we get

and

Converting the system of linear equations into matrix form and solving forandwe get

whereand.

where

Solving () in a similar way, we get

whereand.

Once the optimal values ofandare calculated. The
categorization of a new input data point xinto either theorclass can be determined as follows:

SECTION: Nonlinear GBLSTSVM
To generalize our proposed model to the nonlinear case, we introduce the map, whererepresents a Hilbert space. We define, wheredenotes the training dataset. The granular balls that are generated on the setare denoted by, whererepresents the number of granular balls. Let the matricesand, along with column vectorsand, represent the features of the centers and radii of the granular balls belonging to the positive and negative class, respectively.

The optimization problem for nonlinear GBLSTSVM is given as:

and

The solutions of () and () can be derived similarly as in the linear case. The solutions are:

and

where

The classification of data points to classoris done similarly to the linear case of the GBLSTSVM model.

SECTION: THE PROPOSED LARGE SCALE GRANULAR BALL LEAST SQUARE TWIN SUPPORT VECTOR MACHINE (LS-GBLSTSVM)
Granular computing significantly reduces the number of training instances, leading to a substantial reduction in computational requirements. However, the scalability of the GBLSTSVM may decrease when confronted with large datasets due to its reliance on matrix inversion for solving the system of linear equations. Additionally, like LSTSVM, GBLSTSVM lacks the SRM principle. To address these issues, we introduce a regularization term into the primal formulation of GBLSTSVM. This inclusion results in an additional equality constraint in the primal formulation, effectively eliminating the necessity for matrix inversions in obtaining optimal nonparallel hyperplanes in GBLSTSVM. This removal of matrix inversions significantly reduces the computational complexity of LS-GBLSTSVM, making it well-suited for handling large datasets. Moreover, the integration of the regularization terms implements the SRM principle in GBLSTSVM.

SECTION: Linear LS-GBLSTSVM
The optimized primal problem of linear LS-GBLSTSVM is given by:

and

Introducing Lagrange multipliersandin (), we get

Applying the K.K.T. necessary and sufficient conditions for () we obtain the following:

From () and (), we get

Substituting (), (), and () in () and simplifying, we get the dual of ():

Here,is the matrix of ones of appropriate dimensions, andandare the identity matrices.
Similarly, the Wolfe Dual of () is:

Thenis given by:

The categorization of a new input data point xinto either theorclass can be determined as follows:

SECTION: Nonlinear LS-GBLSTSVM
The optimization problem of nonlinear LS-GBLSTSVM is given as follows:

and

Calculating Lagrangian as in the Linear LS-GBLSTSVM, we get

The dual of the optimization problem () and () are,

and

Thenis given by:

To solve the optimization problem of type (38) and (44), we use Sequential Minimal Optimization (SMO). The classification of test data points to classoris done in the same manner as in the linear case of the LS-GBLSTSVM model.

SECTION: Computational Complexity
We initiate the computational analysis by treating the training datasetas the initial granular ball set (). This setundergoes a binary split using the-means clustering algorithm, initially resulting in computational complexity of. In subsequent iterations, if both resultant granular balls remain impure, they are further divided into four granular balls, maintaining a maximum computational complexity ofper iteration. This iterative process continues for a total ofiterations. Consequently, the overall computational complexity of generating granular balls is approximatelyor less, depending on the purity of the generated granular balls and the number of iterations required.
Suppose thatis the number oflabeled data samples andis the number oflabeled data samples with. The LSTSVM model requires the calculation of two matrix inverses of order. However, using the Sherman-Morrison-Woodbury (SMW) formula, the calculation involves solving three inverses of reduced dimensions. Therefore, in the LSTSVM model, the time complexity includes two inversions of sizeand one inversion of sizeif. Conversely, if, the complexity involves two inversions of sizeand one inversion of size. GBLSTSVM computes the inverses of two matrices with order, whererepresents the total number of granular balls generated on a training dataset. Hence, the total time complexity of the GBLSTSVM model is approximately less than or equal to. Given thatrepresents the number of iterations, it follows thatis considerably smaller than, and also,is significantly less than. Thus,. Hence, the computational complexity of GBLSTSVM is substantially lower than that of LSTSVM.
The computational complexity of the SMO algorithm isto. Therefore, the complexity of each optimization problem in the LS-GBLSTSVM model falls approximately betweenand. Therefore, the computational complexity of LS-GBLSTSVM is considerably lower than that of LSTSVM.

SECTION: Experimental Results and Discussions
In this section, we assess the efficacy of the proposed GBLSTSVM and LS-GBLSTSVM models. We evaluate their performance against LSTSVMand various other baseline models over UCIand KEELbenchmark datasets with and without label noise to ensure comprehensive testing. Furthermore, we conduct experiments on NDC datasets. Moreover, we provide a sensitivity analysis of the hyperparameters and granular ball computing parameters.

SECTION: Experimental Setup
To evaluate the performance of the GBLSTSVM and LS-GBLSTSVM models, a series of experiments are conducted. These experiments are carried out on a PC with an Intel(R) Xeon(R) Gold 6226R processor running at 2.90GHz and 128 GB of RAM. The PC is operating on Windows 11 and utilizes Python 3.11. To solve the dual of QPP in GBSVM, the “QP solvers” function from the CVXOPT package is employed. The dataset is randomly split, with 70% samples are used for training and 30% are for testing purposes. The hyperparameters are tuned using the grid search method and five-fold cross-validation.The hyperparameterswere tuned within the range. For the nonlinear case, a Gaussian kernel is utilized, defined as, wherevaried within the range. In the proposed LS-GBLSTSVM model, the values ofandare set to be equal, as well as the values ofand, for both linear and nonlinear cases.

SECTION: Experments on Real World UCI and KEEL Datasets on the Linear Kernel
In this subsection, we conduct extensive statistical analyses to compare the proposed GBLSTSVM and LS-GBLSTSVM models with LSTSVMalong with several other baseline models, namely SVM, TSVM, and GBSVM. To solve the optimization problem associated with GBSVM, we employ the PSO algorithm. Our experimental investigation encompasses diverse scenarios, encompassing both linear and nonlinear cases, and involves meticulous numerical experimentation.

We conduct experiments on 34 UCIand KEELbenchmark datasets. Table I of Supplementary Material shows the detailed experimental results of every model over each dataset. All the experimental results discussed in this subsection are obtained at a 0% noise level for both linear and Gaussian kernels. The average accuracy (ACC) and average rank of the linear case are presented in Table. The average ACC of the GBLSTSVM model is 88.26%, while the LS-GBLSTSVM model achieves an average ACC of 86.79%. On the other hand, the average ACC of the SVM, TSVM, GBSVM, and LSTSVM models are 81.58%, 71.52%, 73.71%, and 86.67%, respectively. In terms of average ACC, our proposed GBLSTSVM and LS-GBLSTSVM models outperform the baseline SVM, TSVM, GBSVM, and LSTSVM models. To further evaluate the performance of our proposed models, we employ the ranking method. In this method, each model is assigned a rank for each dataset, with the best-performing model receiving the lowest rank and the worst-performing model receiving the highest rank. The average rank of a model is calculated as the average of its ranks across all datasets. If we consider a set ofdatasets, wheremodels are evaluated on each dataset, we can represent the position of themodel on thedataset as. In this case, the average rank of themodel is calculated as. The average rank of SVM, TSVM, GBSVM, and LSTSVM models are 3.88, 5.26, 5.15, and 2.59, respectively. On the other hand, the average rank of the proposed GBLSTSVM and LS-GBLSTSVM models are 1.62 and 2.50, respectively. Based on the average rank, our proposed models demonstrate a superior performance compared to the baseline models. This indicates that our proposed models exhibit better generalization ability.

To assess the statistical significance of the proposed models, we employ the Friedman test. The purpose of this test is to assess the presence of significant disparities among the compared models by examining the average ranks assigned to each model. By evaluating the rankings, we can determine if there are statistically significant differences among the given models. The null hypothesis in this test assumes that all models have the same average rank, indicating an equivalent level of performance. The Friedman test follows the chi-squared distribution () withdegrees of freedom and is given by. The Friedman statisticis given by, where,-distribution hasanddegrees of freedom. Forand, we getandatlevel significance. From the statistical-distribution table, we find that. Since, we reject the null hypothesis, indicating a significant statistical difference among the compared models.

To further establish the statistical significance of our proposed GBLSTSVM and LS-GBLSTSVM models with the baseline models, we conduct the Wilcoxon signed rank test. This test calculates the differences in accuracy between pairs of models on each dataset. These differences are then ranked in ascending order based on their absolute values, with tied ranks being averaged. Subsequently, the sum of positive ranks () and the sum of negative ranks () are computed. The null hypothesis in this test typically assumes that there is no significant difference between the performances of the models, meaning that the median difference in accuracy is zero. However, if the difference betweenandis sufficiently large, indicating a consistent preference for one model over the other across the datasets. If the resulting-value from the test is less than, then the null hypothesis is rejected. The rejection of the null hypothesis signifies that there exists a statistically significant difference in performance between the compared models.

Tablepresents the results, demonstrating that our proposed GBLSTSVM model outperforms the baseline SVM, TSVM, GBSVM, and LSTSVM models. Furthermore, Tableillustrates that the proposed LS-GBLSTSVM model exhibits superior performance compared to the SVM, TSVM, and GBSVM models. The Wilcoxon test strongly suggests that the proposed GBLSTSVM and LS-GBLSTSM models possess a comprehensive statistical advantage over the baseline models.

Moreover, we employ a pairwise win-tie-loss sign test. This test is conducted under the assumption that both models are equal and each model wins ondatasets, wheredenotes the total number of datasets. To establish statistical significance, the model must win on approximatelydatasets over the other model. In cases where there is an even number of ties between the compared models, these ties are evenly distributed between the models. However, if the number of ties is odd, one tie is disregarded, and the remaining ties are divided among the specified models. In our case, with, if one of the models achieves a minimum ofwins, it indicates a significant distinction between the models.
The results presented in Tableclearly show that our proposed models have outperformed the baseline models in the majority of the UCI and KEEL datasets.

SECTION: Experiments on Real World UCI and KEEL Datasets on the Gaussian Kernel
Supplementary Material’s Table II demonstrates that our proposed GBLSTSVM and LS-GBLSTSVM models outperform the baseline models in Gaussian kernel space in most of the datasets. Tablepresents the average accuracy (ACC) and average rank of the proposed GBLSTSVM and LS-GBLSTSVM models, as well as the baseline models using the Gaussian kernel. Our proposed GBSLSTVM and LS-GBLSTSVM models achieve an average accuracy of 83.64% and 84.55%, respectively, which is superior to the baseline models. Additionally, the average rank of our proposed GBLSTSVM and LS-GBLSTSVM models is 2.90 and 2.84, respectively, indicating a lower rank compared to the baseline models. This suggests that our proposed models exhibit better generalization ability than the baseline models.

Furthermore, we conduct the Friedman testand Wilcoxon signed rank testfor the Gaussian kernel. Forand, we obtainedandat a significance level of 5%. Referring to the statistical-distribution table, we find that. Since, we reject the null hypothesis. Consequently, there exists a significant statistical difference among the compared models.

Moreover, the Wilcoxon signed test presented in Tablefor GBLSTSVM and Tablefor LS-GBLSTSVM demonstrate that our proposed models possess a significant statistical advantage over the baseline models. The pairwise win-tie-loss results presented in Tablefurther emphasize the superiority of our proposed models over the baseline models.

SECTION: Experiments on Real World UCI and KEEL Datasets with Added Label Noise on Linear and Gaussian Kernel
The proposed GBTSVM and LS-GBTSVM models are experimentally evaluated using UCI and KEEL benchmark datasets. To assess their performance, label noise is introduced at varying levels of 5%, 10%, 15%, and 20%. The results, presented in Table I and Table II of Supplementary Material, demonstrate the effectiveness of these models compared to baseline models in both linear and nonlinear cases. Throughout the evaluations, the GBLSTSVM and LS-GBLSTSVM models consistently outperformed the baseline models. The proposed GBLSTSVM demonstrates a superior average ACC compared to the baseline models, with an improvement of up to 3% when increasing the label noise from 5% to 20%, for the linear kernel. Similarly, our proposed LS-GBLSTSVM has a better average ACC than the baseline models with linear kernel. The average ACC of LS-GBLSTSVM at noise levels of 5%, 10%, 15%, and 20% are 85.59, 85.50, 84.93, and 83.85, respectively, which is higher than all baseline models. Additionally, the proposed models outperform the baseline models in terms of average rank, even when considering various levels of label noise. For the Gaussian kernel, our proposed GBLSTSVM has up to 3% better average ACC, and LS-GBLSTSVM has up to 2% better average ACC compared to baseline models on increasing the levels of label noise from 5% to 20%. Also, our proposed models have a lower average rank than the baseline models in increasingly noisy conditions as well. This can be attributed to the incorporation of granular balls within these models, which exhibit a coarser granularity and possess the ability to mitigate the effects of label noise. The key feature of these granular balls is their strong influence of the majority label within them, effectively reducing the impact of noise points from minority labels on the classification results. This approach significantly enhances the models’ resistance to label noise contamination. The consistent superiority of the GBLSTSVM and LS-GBLSTSVM models over the baseline models highlights their potential effectiveness in real-world scenarios where noise is commonly encountered in datasets.

SECTION: Experiment on NDC Datasets
The previous comprehensive analyses have consistently shown the superior performance of the proposed GBLSTSVM and LS-GBLSTSVM models compared to the baseline models across the majority of UCI and KEEL benchmark datasets. Furthermore, we conduct an experiment using the NDC datasetsto highlight the enhanced training speed and scalability of our proposed models. For this, all hyperparameters are set to, which is the lowest value among the specified ranges. These NDC datasets’ sample sizes vary from 10k to 5m with 32 features. The results presented in Tableshow the efficiency and scalability of the proposed GBLSTSVM and LS-GBLSTSVM models. Across the NDC datasets, our models consistently outperform the baseline models in terms of both accuracy and training times, thus confirming their robustness and efficiency, particularly when dealing with large-scale datasets. In the context of ACC, our GBLSTSVM model demonstrates superior accuracy, with an increase of up to 3% when the NDC dataset scale is expanded from 10k to 5m. Additionally, GBLSTSVM demonstrates reduced training time across all ranges of NDC datasets compared to LSTSVM. Specifically, for the NDC 5m dataset, the LS-GBLSTSVM model achieves an impressive accuracy of 84.99%, which stands as the highest accuracy. The experimental results demonstrate a significant reduction of 100 to 1000 times in the training duration of the GBLSTSVM and LS-GBLSTSVM models compared to the baseline models. This exceptional decrease in training time can be attributed to the significantly lower count of generated granular balls on a dataset in comparison to the total number of samples.

SECTION: Sensitivity Analysis of Hyperparameters
To thoroughly understand the subtle effects of the hyperparameters on the model’s generalization ability, we systematically explore the hyperparameter space by varying the values ofand. This exploration allows us to identify the configuration that maximizes predictive accuracy and enhances the model’s resilience to previously unseen data. The graphical representations in Fig. 4 provide visual insights into the impact of parameter tuning on the accuracy (ACC) of our GBLSTSVM model for linear case. These visuals demonstrate an apparent variation in the model’s accuracy across a range ofandvalues, highlighting the sensitivity of our model’s performance to these hyperparameters. In Fig. 4(a), it is evident that lower values ofcombined with higher values ofresult in improved accuracy. Similarly, Fig. 4(d) shows that optimal accuracy is achieved when bothandare set to mid-range values. It has been observed that lower values ofand higher values ofgive the best generalization performance.

SECTION: Sensitivity Analysis of Granular Parameters
In the context of granular computing, as elaborated in Section II (B), we can ascertain the minimum number of granular balls to be generated on the training dataset, denoted as. For our binary classification problem, we establish the minimum value forat 2. Hence, our goal is to generate at least two granular balls for each dataset. The purityof a granular ball is a crucial characteristic. By adjusting the purity level of the granular balls, we can simplify the distribution of data points in space and effectively capture the data points distribution using these granular balls. To analyze the impact ofandon the generalization performance of GBLSTSVM, we have tunedwithin the range of, andwithin the range of. The visual depictions in Fig. 5 offer valuable visualizations of how this tuning affects the accuracy (ACC) of our GBLSTSVM model. Careful examination of these visuals depicts that there exists an optimal value ofandusing which our proposed GBLSTSVM mode gives the optimal generalization performance. In the case of Fig. 5(a), when bothandare increased simultaneously, a significant rise in ACC can be observed. This suggests that the accuracy of our proposed model improves as the purity increases and the minimum number of granular balls generated increases. Similar patterns can be observed in other figures in Fig. 5. This aligns with the principle of granular computing. As the value ofrises, the minimum count of granular balls required to cover our sample space also increases. With an increase in, these granular balls divide even more, resulting in a greater generation of granular balls. This process effectively captures the data patterns, ultimately leading to the best possible generalization performance. Tablesandillustrate the variation in ACC for linear GBLSTSVM and LS-GBLSTSVM model and the number of granular balls generated () while tuning purity levels across various UCI and KEEL datasets whenis fixed at 2.

SECTION: Conclusion
In this paper, we have proposed two novel models, the granular ball least square twin support vector machine (GBLSTSVM) and the large-scale granular ball least square twin support vector machine (LS-GBLSTSVM), by incorporating the concept of granular computing in LSTSVM. This incorporation enables our proposed models to achieve the following: (i) Robustness: Both GBLSTSVM and LS-GBLSTSVM demonstrate robustness due to the coarse nature of granular balls, making them less susceptible to noise and outliers.
(ii) Efficiency: The efficiency of GBLSTSVM and LS-GBLSTSVM stems from the significantly lower number of coarse granular balls compared to finer data points, enhancing computational efficiency.
(iii) Scalability: Our proposed models are well suited for large-scale problems, primarily due to the significantly reduced number of generated granular balls compared to the total training data points. The proposed LS-GBLSTSVM model demonstrates exceptional scalability since it does not necessitate matrix inversion for determining optimal parameters. This is evidenced by experiments conducted on the NDC dataset, showcasing their ability to handle large-scale datasets effectively. An extensive series of experiments and statistical analyses have supported the above claims, including ranking schemes, the Friedman Test, the Wilcoxon Signed Rank Test, and the win-tie-loss sign test. The key findings from our experiments include:
(i) Both linear and nonlinear versions of GBLSTSVM and LS-GBLSTSVM demonstrate superior efficiency and generalization performance compared to baseline models, with an average accuracy improvement of up to 15%. (ii) When exposed to labeled noise in the UCI and KEEL datasets, our models exhibit exceptional robustness, achieving up to a 10% increase in average accuracy compared to baseline models under noisy conditions. (iii) Evaluating our proposed models on NDC datasets ranging from 10k to 5m samples underscores their scalability, surpassing various baseline models in training speed by up to 1000 times, particularly beyond the NDC-50k threshold, where memory limitations often hinder baseline models. These findings collectively highlight the effectiveness, robustness, and scalability of our proposed GBLSTSVM and LS-GBLSTSVM models, particularly in handling large-scale and noisy datasets.

While our proposed models have demonstrated outstanding performance in binary classification problems, their evaluation on multiclass problems has not been conducted. A crucial area for future research would involve adapting these models to be suitable for multi-class problems.

SECTION: Acknowledgment
This project is supported by the Indian government through grants from the Department of Science and Technology (DST) and the Ministry of Electronics and Information Technology (MeitY). The funding includes DST/NSM/R&D_HPC_Appl/2021/03.29 for the National Supercomputing Mission and MTR/2021/000787 for the Mathematical Research Impact-Centric Support (MATRICS) scheme. Furthermore, Md Sajid’s research fellowship is funded by the Council of Scientific and Industrial Research (CSIR), New Delhi, under the grant 09/1022(13847)/2022-EMR-I. The authors express their gratitude for the resources and support provided by IIT Indore.

SECTION: References
SECTION: Supplementary Material
SECTION: Classification accuracies of the proposed GBLSTSVM and LS-GBLSTSVM models along with baseline models with linear and non-linear kernel
Here, we provide a comprehensive analysis, comparing the performance of our proposed models with baseline models utilizing linear and Gaussian kernels across label noise (0%, 5%, 10%, 15%, and 20%). Notably, our proposed models consistently outshine baseline models, achieving the highest average accuracy (ACC) and the lowest average rank at various levels of noise. The best results are highlighted in boldface for clear reference.