SECTION: Diffusion Auto-regressive Transformer for Effective Self-supervised Time Series Forecasting
Self-supervised learning has become a popular and effective approach for enhancing time series forecasting, enabling models to learn universal representations from unlabeled data. However, effectively capturing both the global sequence dependence and local detail features within time series data remains challenging. To address this, we propose a novel generative self-supervised method called, denotingiffusionuto-regressiveransformer forime series forecasting. In TimeDART, we treat time series patches as basic modeling units. Specifically, we employ an self-attention based Transformer encoder to model the dependencies of inter-patches. Additionally, we introduce diffusion and denoising mechanisms to capture the detail locality features of intra-patch. Notably, we design a cross-attention-based denoising decoder that allows for adjustable optimization difficulty in the self-supervised task, facilitating more effective self-supervised pre-training. Furthermore, the entire model is optimized in an auto-regressive manner to obtain transferable representations. Extensive experiments demonstrate that TimeDART achieves state-of-the-art fine-tuning performance compared to the most advanced competitive methods in forecasting tasks. Our code is publicly available.

SECTION: Introduction
Time series forecastingis crucial in a wide array of domains, including finance, healthcare, energy management. Accurate predictions of future data points could enable better decision-making, resource allocation, and risk management, ultimately leading to significant operational improvements and strategic advantages. Among the various methods developed for time series forecasting, deep neural networkshave emerged as a popular and effective solution paradigm.

To further enhance the performance of time series forecasting, self-supervised learning has become an increasingly popular research paradigm. This approach allows models to learn transferable representations from unlabeled data by self-supervised pre-training, which can then be fine-tuned for forecasting tasks. Scrutinizing previous studies, existing methods primarily fall into two categories. The first category is masked autoencoders, with representative methods including TST, TimeMAE, and SimMTM. These methods focus on reconstructing masked or corrupted parts of the input data, encouraging the model to learn meaningful representations that capture the underlying structure of the time series. The second category comprises contrastive-based discriminative methods, such as TS-TCC, TS2Vec, and TNC. These approaches leverage contrastive learning to distinguish between similar and dissimilar time series segments, thereby enhancing the model’s ability to capture essential patterns and temporal dynamics.

Despite advancements in self-supervised methods, notable limitations persist when applying them to time series forecasting. First, masked methods easily introduce a significant gap between pre-training and fine-tuning due to altered data distribution, which hinders effective representation transfer. Second, contrastive learning methods might face challenges in constructing positive and negative pairs, given time series’ temporal dependencies and ambiguity in defining similarity. In addition, these methods also prioritize learning discriminative features over modeling the generative aspects needed for forecasting, limiting their ability to capture nuanced temporal dependencies.

Despite the recent advancements in self-supervised learning methods for time series, we argue that an ideal approach should possess the following two key characteristics. First, the gap between the pre-training objective and the downstream fine-tuning task should be minimized as much as possible. As we know, the widely used one-step generationapproach essentially employs an inductive bias of using the past to predict the future. In fact, auto-regressive generative optimizationaligns well with this paradigm, yet it has rarely been adopted in the field of time series self-supervised learning. Second, it is crucial to model both long-term dependencies and local patterns during self-supervised pre-training of time series. However, existing self-supervised methods often struggle to effectively capture these aspects simultaneously, which significantly limits their ability to learn comprehensive and expressive representations of time series data. In this context, developing a novel approach that can effectively address the challenges discussed above is crucial to fully exploit the intricate temporal relationships present in time series.

Building upon this analysis above, in this work, we propose a novel self-supervised time series method called TimeDART. The key feature of TimeDART lies in its elegant integration of two advanced generative self-supervised approaches within a unified framework, allowing for effective self-supervised learning by simultaneously capturing both long-term dependencies and fine-grained local features in time series data. Specifically, we treat time series patches as the fundamental modeling units. To capture inter-patch dependencies, we employ a self-attention-based Transformer encoder. Concurrently, we introduce a forward diffusion and reverse denoising process to reconstruct the detailed features of individual patches, thereby effectively modeling local relational dependencies. Notably, within the diffusion module, we design a novel cross-attention-based denoising network that enables more flexible and adaptive noise reduction. Through this design, the TimeDART framework aims to shorten the gap between pre-training and fine-tuning tasks, while effectively modeling both global dependencies and local feature representations during the self-supervised learning process. Finally, we evaluate the effectiveness of our method on public datasets, demonstrating its superior performance over existing competitive approaches. We hope that TimeDART’s strong performance can inspire more research work in this area.The main contribution of this work can be summarized as:

We propose a novel generative self-supervised learning framework, TimeDART, which integrates diffusion and auto-regressive modeling to effectively learn both global sequence dependencies and local detail features from time series data, addressing the challenges of capturing comprehensive temporal characteristics.

We design a cross-attention-based denoising decoder within the diffusion mechanism, which enables adjustable optimization difficulty during the self-supervised task. This design significantly enhances the model’s ability to capture localized intra-patch features, improving the effectiveness of pre-training for time series forecasting.

We conduct extensive experiments to validate that TimeDART achieves more superior performance on time series forecasting tasks. We also report some insight findings to understand the proposed TimeDART.

SECTION: Related Work
In recent years, deep learning-based modelshave significantly advanced time series forecasting by addressing long-range dependencies. Informerintroduced ProbSparse attention to reduce complexity fromto, combined with attention distillation to handle ultra-long inputs. Autoformerproposed a decomposition architecture with an auto-correlation mechanism to improve efficiency and accuracy. FEDformerintegrated seasonal-trend decomposition with frequency-enhanced attention, further reducing complexity to. Crossformeraddressed multivariate time series forecasting by capturing both temporal and cross-dimensional dependencies through dual-stage attention. PatchTSTintroduced a patching strategy with channel independence and self-supervised pretraining, while iTransformerapplied attention and feedforward networks along reversed dimensions without altering the Transformer architecture. SimMTMemployed manifold learning to restore masked time points, improving semantic recovery, and GPHTintroduced a mixed dataset pretraining approach, enabling large-scale training and autoregressive forecasting without custom heads. Diffusion-TSuses an encoder-decoder transformer to generate high-quality multivariate time series in a diffusion-based framework. These methods collectively enhance the efficiency, scalability, and accuracy of time series forecasting using Transformer architectures.

Self-supervised learning has emerged as a powerful paradigm for pretraining in many domains, including natural language processing (NLP) and computer vision (CV). Unlike supervised learning, where models are trained with labeled data, self-supervised methods rely on the structure within the data itself to generate supervision, typically through pretext tasks. In the domain of time series, self-supervised learning faces unique challenges due to the sequential nature and temporal dependencies of the data. Current approaches can be broadly categorized into two paradigms: discriminative and generative methods.

Discriminative methods, such as contrastive learning, focus on distinguishing between positive and negative instance pairs. These methods learn representations by pulling similar instances (positive pairs) closer and pushing dissimilar instances (negative pairs) apart. For instance, TNCleverages the local smoothness of time series signals to define positive neighborhoods, while TS2Vecintroduces a hierarchical contrastive learning framework that operates at both the instance and patch levels. Similarly, CoSTincorporates both time and frequency domain information to capture seasonal and trend representations, improving the discriminative power of the learned features.

On the other hand, generative methods typically involve reconstructing masked or corrupted inputs, encouraging the model to learn meaningful representations. Masked time series modeling, first introduced by TST, predicts missing time points based on the available data. This approach has since been extended by methods like STEP, PatchTSTand CrossTimeNet, which operate on sub-series to reduce computational costs while improving local information capture. More recent works, such as TimeMAE, enhance this framework by introducing decoupled masked autoencoders, achieving state-of-the-art performance in time series classification tasks.

SECTION: Methodology
SECTION: Problem Definition
Given an input multivariate time series, whererepresents the number of channels anddenotes the look-back window length, the objective is to predict future valuesover a predicted window. Here,consists ofinput vectors, whilerepresents the predicted values. Initially, we pretrain on the look-back window, and subsequently, both the look-back and prediction windows are employed for the forecasting task.

SECTION: The Proposed TimeDART
Our design philosophy centers on integrating two powerful generative approaches: auto-regressive generation and the denoising diffusion model. These two methods complement each other, each leveraging their respective strengths. Auto-Regressive Generation captures the high-level global dependencies within sequence data, while the Denoising Diffusion Model focuses on modeling lower-level local regions. Through their combined efforts, the model learns the deep structures and intrinsic patterns within time series data, ultimately improving prediction accuracy and generalization capability. In the following sections, we will detail the technical aspects of our method.

Before feeding the input multivariate time series data into the representation network, we apply instance normalization to each time series instance, normalizing it to have zero mean and unit standard deviation. After prediction, the original mean and standard deviation are restored to ensure consistency in the final forecast.

The inputis split tounivariate serieswhere. Each of them is fed independently into Transformer encoder. Then the denoising patch decoder will provide resultsaccordingly. Channel-independenceallows universal pre-training across datasets and is common in time series forecasting, enabling different channels to share embedding weights.

Unlike previous works, we use patches instead of points as the basic modeling unit. This is because patches capture more information and features from local regions, providing richer representations compared to individual points. Additionally, diffusion model operate on these modeling units. Applying noise and denoising to individual points could lead to excessive sensitivity to inherent noise in the dataset, while using patches mitigates this issue by offering a more stable representation. To prevent information leakage and preserve the model’s auto-regressive property, we set the patch lengthequal to the stride. This ensures that each patch contains only non-overlapping segments of the original sequence, avoiding access to future time steps and maintaining the auto-regressive assumption. For simplicity, we assume, the time series length, is divisible by, resulting inpatches, which significantly lowers computational complexity and enables the model to process longer sequences.

Each patch (referred to as a clean patch) is then passed through a linear embedding layer, transforming it into a high-dimensional representation. The patch embeddings are expressed as: (we omit the channel indexfor simplicity):

We initialize a vanilla Transformer encoder as the representation network, aligning with existing self-supervised methods. During pre-training, we prepend a learnable start-of-sequence (SOS) embedding to the clean patch representations, while excluding the final one. To further incorporate positional information, we apply sinusoidal positional encoding after the embedding layer. Following this, we use a causal maskin the self-attention layer, limiting each patch’s visibility to itself and prior patches. Letrepresent the Transformer encoder’s processing of the input sequence with the causal mask, resulting in the final contextualized representations. Consequently, the causal Transformer encoder network can be expressed as follows:

Different from previous self-supervised learning (SSL) approaches, our work innovatively incorporates the diffusion model into self-supervised prediction. The diffusion model consists of two key steps: the forward process and the reverse denoising. The forward process gradually adds noise to the data, while the reverse process reconstructs the original data by removing the noise. Below, we detail the techniques of this approach.

For each patch, the forward processgradually adds noise to the patch, whereis the noise scheduler. Letbe the cumulative product ofover time steps, where, the forward process can be rewrite given the original clean patch:

As shown in Figure, we independently add noise to each patch at time step, enabling the model to learn varying denoising scales across the sequence. This prevents oversimplification of the task, ensuring robust pre-training. The resulting sequence of noisy patches is represented as:

In DDPM, the noise schedulertypically decreases linearly asincreases. Instead, we use a cosine scheduling approach, where. This smoother transition emphasizes the early and later stages of diffusion, improving model stability and better capturing the data distribution.

The noise-added and clean patches share the same embedding layer and weights. Both also use sinusoidal positional encoding. The deep representation of the noise-added patches is as follows:

The reverse process is handled by the denoising patch decoder, which is a Transformer Decoder block. It takes the Transformer encoder output as keys and values, while the noise-added patch embeddings act as queries.

A mask is applied to the decoder to ensure that the-th input in the noise-added sequence can only attend to the-th output from the Transformer encoder. The encoder’s output at position, informed by the causal mask and start-of-sequence (SOS) embedding, aggregates information from clean patches at positionsto, enabling auto-regressive generation. Finally, deep representations are mapped back to the original space via flattening and linear projection. Although the linear layer concatenates the generated sequence and projects it into the input space, this does not imply that the auto-regressive mechanism is irrelevant. We will demonstrate the effectiveness of the auto-regressive mechanism through subsequent experiments by removing the Causal Mask in the Transformer encoder and the mask in the denoising patch decoder in Section.

Letdenote the processing of the two inputs by the denoising patch decoder. The reverse process is then expressed as follows::

SECTION: Self-supervised Generative Optimization
Instead of using a masked optimization approach, we adopt an auto-regressive generative scheme for several reasons. First, generative models are better suited for prediction tasks. For example, GPTis favored over BERTin conversational models due to its superior performance, making it a better fit for generating future outcomes. Second, while masked modeling captures bidirectional context, it introduces inconsistencies between pre-training and downstream tasks. Masked token embeddings exist only in pre-training, causing a mismatch during fine-tuning. Additionally, pre-training exposes the model to partial data (with masked tokens), whereas downstream tasks use full sequences, further exacerbating this discrepancy.

We also replace the conventional MSE loss with a denoising diffusion model and its diffusion loss. Diffusion loss helps the model capture multimodal distributions, better suited for the complexity of time series data. In contrast, MSE assumes predicted values center around a single mean, often resulting in overly smooth predictions that fail to capture the multimodal patterns in time series data.

Our self-supervised optimization objective minimizes the diffusion loss, equivalent to the Evidence Lower Bound (ELBO). The final loss is:

The detailed derivation process can be found in Appendix.

After pre-training on the look-back window, fine-tuning is performed on both the look-back and predicted windows by re-initializing a new prediction head for the downstream task and removing the denoising patch decoder. During fine-tuning, the model is optimized for one-step prediction using MSE loss. This approach maintains structural consistency between pre-training and downstream tasks, while keeping their objectives distinct.

SECTION: Experiments
SECTION: Experimental Setup
To evaluate TimeDART, we conduct experiments on 8 popular datasets, including 4 ETT datasets (ETTh1, ETTh2, ETTm1, ETTm2), Weather, Exchange, Electricity, and Traffic. The statistics of these datasets are summarized in Table. Following standard protocol, we split each dataset into training, validation, and testing sets in chronological order. The split ratio isfor the ETT datasets andfor the others.

Since we adopted the channel-independence setting, we can perform general pre-training across all eight datasets. Therefore, we conducted two experimental settings: in-domain and cross-domain. In the in-domain setting, both pre-training and fine-tuning were performed on the same dataset, whereas in the cross-domain setting, we pre-trained on five datasets (ETTh1, ETTh2, ETTm1, ETTm2, Electricity) from the Energy domain and fine-tuned on a specific dataset.

We compared our approach against several state-of-the-art baseline methods. In the in-domain setting, we selected six competitive methods, along with results from a randomly initialized model for comparison. Among them,proposes recovering masked time points by weighted aggregation of multiple neighbors outside the manifold, while also utilizing contrastive learning to optimize the self-supervised process.in its self-supervised version leverages subseries-level patches and channel-independence to retain local semantics, reduce computation, and enhance long-term forecasting accuracy. Additionally,utilizes decoupled masked autoencoders to learn robust representations for regression.is a time series forecasting framework that uses contrastive learning to disentangle seasonal and trend representations. Furthermore, we compared against supervised methods, such as the supervised version of the Transformer-basedand the linear-basedmodel, to further demonstrate the effectiveness of TimeDART.

In the cross-domain setting, we perform mixed pre-training on five datasets [ETTh1, ETTh2, ETTm1, ETTm2, Electricity] from the Energy domain, followed by fine-tuning on a specific dataset from these five. The cross-domain baseline includes the results from a randomly initialized model and the performance of TimeDART in the in-domain setting.

To ensure experimental fairness, we used a unified encoder for all representation networks in the in-domain setting, except for DLinear. Specifically, we adopted a vanilla Transformer encoder with a channel-independent configuration, while DLinear retained its native linear encoder settings. All implementations are based on their official repositories.

Similarly, to ensure fairness, we set the lookback window lengthand the predicted window, following the standard protocol. To highlight the differences introduced by pre-training, we also include a random init setting, where the representation network is randomly initialized and then fine-tuned on the same downstream tasks without any pre-training. This setup clearly demonstrates the significant improvements brought by pre-training.

SECTION: Main Result
The experimental results for the in-domain setting are shown in Table, while the results for the cross-domain setting are shown in Table.

After downstream fine-tuning, TimeDART outperforms its competing baselines in most experimental settings, achieving the best results in approximatelyof the 64 evaluation metrics. Specifically, TimeDART surpasses the best baselines across all metrics in the ETTh2 and ETTm2 datasets, consistently outperforming both self-supervised and supervised methods. TimeDART also demonstrates significant advantages due to pre-training, as seen in its superior performance compared to non-pre-trained baselines across all datasets and prediction horizons. Although it may not always achieve the top result, TimeDART consistently ranks as either the best or second-best method in nearly all settings, with only four exceptions. The method shows relatively weaker performance on the Exchange dataset, primarily due to the uneven distribution between the look-back and predicted windows, which limits its ability to fully exploit its strengths in balancing upstream and downstream input data. Furthermore, the marked differences in data trends between the validation and test sets in this dataset lead to overfitting, necessitating more effective generalization strategies for such cases. To clearly demonstrate the effectiveness of our method, the visualized prediction results will be presented in Section.

As shown in Table, the overall effectiveness of TimeDART in cross-domain scenarios is evident. TimeDART consistently outperforms the random initialization baseline, demonstrating its strong ability to generalize across diverse time series datasets. The use of cross-domain pre-training leads to improved forecasting accuracy by learning robust representations from multiple datasets. For instance, on the ETTh2 dataset, TimeDART’s cross-domain pre-training significantly surpasses in-domain training, illustrating the benefits of leveraging varied temporal patterns and dependencies from different datasets. In contrast, the ETTm2 dataset presents a more challenging scenario, where the distinct characteristics of the data make cross-domain pre-training less effective. However, even in this case, the performance difference between cross-domain and in-domain training remains minimal, showing that TimeDART maintains competitive performance even in more difficult settings. Overall, the experiments demonstrate TimeDART’s ability to enhance generalization across datasets while handling varying distributional characteristics.

SECTION: Ablation Study
We investigated the effectiveness of two key modules: the auto-regressive generation and the denoising diffusion model. Four experimental settings were considered: the original model, named TimeDART, the model with the auto-regressive generation removed, namedAR, the model without the denoising diffusion process, nameddiff, and the model with both modules removed, namedAR-diff. Specifically, in the auto-regressive removal experiment, we eliminated both the causal mask in the Transformer encoder and the mask in the denoising patch decoder. In the denoising patch decoder removal experiment, we bypassed the noise addition and denoising process, allowing the output of the representation network to directly pass into the linear projection layer.

Tabledemonstrate that both the auto-regressive generation and the denoising diffusion model play crucial roles in the effectiveness of this approach. Notably, removing the auto-regressive mechanism leads to performance that is even worse than random initialization, further confirming our claim in the method section that the final linear projection layer does not diminish the impact of the auto-regressive mechanism.

SECTION: Hyperparameter Sensitivity Analysis
In our hyperparameter sensitivity experiments, we first investigate two key parameters: the total number of diffusion stepsand the noise scheduler, comparing cosine and linear schedules. The number of diffusion steps reflects the pre-training difficulty, with highervalues making it harder to recover clean patches. The noise scheduler controls the smoothness of noise addition, with the cosine scheduler providing smoother transitions than the linear one. These experiments are conducted on both the ETTh2 and ETTm2 datasets, as shown in Table. For brevity, we report the results as the mean across four prediction lengths.

As shown in Table, the total number of noise steps does not significantly impact the difficulty of pre-training. However, calculations indicate that models pre-trained with different noise steps still outperform those with random initialization. Notably, the cosine noise scheduler performs substantially better than the linear scheduler. In some cases, using the linear scheduler even leads to results worse than those from random initialization. This highlights the critical importance of the noise scheduler, as insufficiently smooth noise addition can result in significantly poorer outcomes.

We then evaluate the impact of the number of layers in the denoising patch decoder across the ETTh2, ETTm2, and Electricity datasets. The number of layers, selected from, reflects the relative size of the denoising network compared to the representation network, which is fixed at 2 layers for all datasets. A decoder with 0 layers represents an ablation case where the denoising patch decoder is removed in Section. As can be observed in Figure, allocating too many layers to the denoising patch decoder can lead to under-training of the representation network, as the majority of the model’s parameters are concentrated in the denoising component.

Finally we examine the effect of patch length, selected from, which controls the amount of local segment information each patch carries. Patch length determines the scale of intra-patch information, and its optimal value depends on the redundancy between neighboring data points within each dataset. For example, in datasets like Electricity, which exhibit higher redundancy between consecutive data points, larger patch lengths may be more effective for modeling. Conversely, for datasets with less redundancy between adjacent data points, shorter patch lengths may be preferred to capture finer-grained temporal dynamics. Figureindicates that different datasets require different levels of intra-patch analysis, reinforcing the need for adaptive patch length selection based on dataset characteristics.

SECTION: Conclusion
In this paper, we proposed TimeDART, a novel generative self-supervised method for time series forecasting that effectively captures both global sequence dependencies and local detail features. By treating time series patches as basic modeling units, TimeDART employs a self-attention-based Transformer encoder to model the sequence dependencies between patches. Simultaneously, it incorporates diffusion and denoising mechanisms to capture the locality features within each patch. Notably, our design of a cross-attention-based flexible denoising network allows for adjustable optimization difficulty in the self-supervised task, enhancing the model’s learning effectiveness. Extensive experiments demonstrate that TimeDART achieves state-of-the-art fine-tuning performance compared to existing advanced time series pre-training methods in forecasting tasks.

SECTION: References
SECTION: Implementation Details
SECTION: Dataset Descriptions
We conducted extensive experiments on eight real-world datasets to evaluate the effectiveness of the proposed TimeDART method under both in-domain and cross-domain settings. These datasets cover a variety of application scenarios, including power systems, transportation networks, and weather forecasting. For detailed descriptions of the datasets and their respective divisions, please refer to Table.

: This dataset comprises time series data of oil temperature and power load collected from electricity transformers spanning July 2016 to July 2018. It is divided into four subsets, each with different recording intervals: ETTh1 and ETTh2 have hourly recordings, while ETTm1 and ETTm2 are recorded every 15 minutes.

: This dataset captures the electricity consumption of 321 clients on an hourly basis from 2012 to 2014, with measurements taken every 15 minutes (in kW). Time stamps follow Portuguese time. Each day includes 96 measurements (244), and during time changes in March (where one hour is skipped), the values between 1:00 am and 2:00 am are set to zero. Conversely, in October (with an extra hour), consumption between 1:00 am and 2:00 am represents the aggregated values of two hours.

: Road occupancy rates, measured hourly, were collected from 862 sensors located along the San Francisco Bay area freeways. The data spans from January 2015 to December 2016.

: This dataset contains meteorological time series featuring 21 indicators. The data was collected every 10 minutes in 2020 by the Weather Station at the Max Planck Biogeochemistry Institute.

: This dataset collects the daily exchange rates of eight countries—Australia, the UK, Canada, Switzerland, China, Japan, New Zealand, and Singapore—from 1990 to 2016.

SECTION: Implementation Details
All experiments were implemented using PyTorchand executed on a single NVIDIA RTX 4090 16GB GPU. For both pre-training and fine-tuning, we employed the ADAM optimizer, with initial learning rates selected from, and optimized the model using L2 loss. For in-domain pre-training, we set the batch size to 16 for all datasets except Traffic, where it is reduced to 8 due to memory and time limitations. The representation network consists of 2 layers across most datasets, while for Traffic, it has 3 layers. The pre-training process spans 50 epochs, except for Traffic, where it is limited to 30 epochs. In downstream tasks, the settings remain largely the same, except that fine-tuning is performed for 10 epochs. The sequence representation dimension is chosen from. For cross-domain experiments, the settings mirror those of the Electricity dataset.

SECTION: Optimization Objective Derivation Details
The self-supervised optimization objective we employ follows the classical form of diffusion loss, which is designed to maximize the marginal likelihood of the data. In this context, we assume thatrepresents the reverse denoising process, where the model learns to reconstruct the original datafrom its noisy versions. This denoising process is modeled as a gradual reverse transformation of the corrupted data, recovering the underlying clean distribution. The ideal loss function for this process can be formally expressed as:

However, since directly optimizing the exact marginal likelihood is intractable, we instead minimize the Evidence Lower Bound (ELBO), given by:

Following a series of derivations, the final loss function is:

SECTION: Cross Domain Full Result
The results in Tabledemonstrate that TimeDART consistently outperforms random initialization across all datasets and prediction lengths. For ETTh2, TimeDART (CD) achieves the lowest MSE of 0.280 at the 96-step window and maintains superior performance over longer horizons, consistently surpassing both random initialization and in-domain training. At the 192-step window, it records an MSE of 0.342 and MAE of 0.380, compared to random initialization’s MSE of 0.358 and MAE of 0.398, further emphasizing the benefits of cross-domain pre-training. For ETTm2, cross-domain pre-training provides a distinct advantage, particularly at the 336-step horizon, where TimeDART (CD) outperforms TimeDART (ID) by 0.05 in MSE. This highlights the model’s robustness in longer forecasting windows. While the cross-domain approach generally surpasses in-domain training, certain datasets, such as ETTm1, present challenges due to distributional differences. However, the performance gap remains small.

SECTION: Ablation Study
The results in Tableunderscore the critical roles of both the auto-regressive generation and denoising diffusion components in TimeDART. Removing the auto-regressive mechanism (w/o AR) leads to a significant performance decline, particularly in ETTm2. At the 96-step horizon, MSE increases from 0.165 to 0.184, and at 336 steps, it rises from 0.279 to 0.307. This illustrates the crucial role of the auto-regressive mechanism in enhancing the model’s forecasting ability, especially across various time horizons. Similarly, eliminating the denoising diffusion module (w/o Diff) results in noticeable performance degradation, as observed in ETTh2. At the 96-step horizon, MSE increases from 0.283 to 0.288, and at the 336-step horizon, it rises from 0.365 to 0.372. These findings highlight the essential contribution of the denoising diffusion process to improving the model’s learning and overall performance.

When both components are removed (w/o AR-Diff), the model’s performance deteriorates significantly across all datasets. For instance, in Electricity, at the 336-step horizon, MSE jumps from 0.166 to 0.199, clearly showing the combined importance of both modules for achieving optimal performance.

In summary, both modules are indispensable for TimeDART’s success. The auto-regressive mechanism is particularly important for long-term predictions, as evidenced in ETTm2, while the denoising diffusion process significantly improves accuracy and learning, especially in datasets like ETTh2.

SECTION: Hyperparameter Sensitivity Analysis
SECTION: Hyperparameter Sensitivity In Forward Process
The results in Tablesuggest that varying the total number of diffusion steps () has a relatively minor impact on model performance across datasets. Whetheris set to 750, 1000, or 1250, the model’s effectiveness remains consistent, with minimal variation in MSE values. This indicates that once a sufficient number of diffusion steps are reached, further increases offer little additional benefit.

In contrast, the noise scheduler plays a more critical role in shaping model performance. The cosine scheduler consistently outperforms the linear scheduler, with the gap in performance widening as the prediction horizon increases. For instance, in the ETTh2 dataset, the cosine scheduler shows significantly better results at longer horizons compared to the linear scheduler, highlighting its ability to facilitate smoother noise transitions. These results emphasize the importance of selecting an appropriate noise scheduler, as it greatly influences the model’s ability to effectively denoise during pre-training.

SECTION: Hyperparameter Sensitivity In Reverse Process
The results in Tableindicate that increasing the number of layers in the denoising patch decoder does not consistently improve performance. While a single decoder layer generally provides the best balance between model complexity and accuracy, adding more layers tends to offer diminishing returns. In fact, beyond one or two layers, performance gains become negligible, and excessive layers can even hinder the training process by shifting capacity away from the representation network. This suggests that an overly complex decoder may underutilize the model’s capacity, leading to suboptimal pre-training outcomes. Overall, the results emphasize the importance of maintaining a balanced architecture, where one decoder layer appears to be sufficient for effective performance across datasets.

SECTION: Hyperparameter Sensitivity In Input Process
The results in Tabledemonstrate that patch length significantly affects model performance, with each dataset benefiting from different levels of information density. For instance, datasets like, which exhibit high redundancy between data points, perform best with larger patches (e.g., patch length 8), achieving the lowest average MSE of 0.163 and MAE of 0.254. In contrast, other datasets may require shorter patch lengths to capture more localized patterns. However, using smaller patches increases the computational complexity considerably, making training much more difficult and resource-intensive. Thus, determining the optimal patch length depends not only on the dataset’s characteristics but also on the balance between performance and computational feasibility.

SECTION: Visualization
In this visualization (Figure), TimeDART is compared against SimMTM and PatchTST-SSL, the self-supervised version of PatchTST. The ground truth, input data, and predictions are plotted together. The look-back window is set to 336 for all datasets, while the predicted window varies: 192 for ETTh2, 96 for Traffic, and 720 for ETTm2. This setup ensures that different datasets are forecasted over appropriate future horizons based on their unique characteristics. TimeDART consistently shows more accurate and smoother predictions, closely matching the ground truth compared to the baseline models.