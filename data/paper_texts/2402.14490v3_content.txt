SECTION: IIntroduction

Imbalanced data refers to the true underlying groups of data having different sizes, which is common in datasets of medical diagnosis, fraud detection, and anomaly detection. Imbalanced data poses a challenge for learning algorithms because these algorithms tend to be biased towards the majority group[1]. While there is a considerable amount of research on supervised learning (e.g., classification) from imbalanced data[2,3,4], unsupervised learning has not been as thoroughly explored, because the unknown cluster sizes make the task more difficult[5]. Methods like resampling and boosting frequently used in supervised learning cannot be applied in unsupervised learning due to the lack of labels.

Clustering is an important unsupervised learning task involving grouping data into clusters based on similarity. K-means (KM) is the most popular clustering technique, valued for its simplicity, scalability, and effectiveness with real datasets. It can also be used as an initialization method for more advanced clustering techniques, such as the Gaussian mixture model (GMM)[6,7]. KM starts with an initial set of centroids (cluster centers) and iteratively refines them to increase cluster compactness. The hard KM (HKM, or Lloyd’s algorithm)[8,9]and fuzzy KM (FKM, or Bezdek’s algorithm)[10]are the two most representative KM algorithms.
HKM assigns a data point to only one cluster, while FKM assigns a data point to multiple clusters with varying degrees of membership.

HKM and FKM stimulate many subsequent research. For example, the possibilistic K-means (PKM)[11]was proposed with reformulated membership more meaningful in terms of typicality. Later, the possibilistic fuzzy K-means (PFKM)[12]was proposed to address the noise sensitivity defect of FKM and overcome the coincident clusters problem of PKM. FKM-[13]was proposed to improve FKM’s performance on data points with uneven variations or non-spherical shapes in individual clusters. Fuzzy local information K-means[14]was designed to promote FKM’s performance in image segmentation. Recently, feature-weighted PKM (FWPKM)[15,16]was proposed to give non-uniform importance to features. Research of combining K-means and kernel mechanisms[17,18,19]has garnered interest, which non-linearly mapped data from low-dimensional space to high-dimensional space through kernel functions to enhance data separability. Conversely, deep clustering, a modern technique that integrates deep neural networks (DNNs) and K-means (or other clustering algorithms), has been proposed to cluster high-dimensional data by mapping them to a low-dimensional space to overcome the curse of dimensionality[20,21,22,23].

Although many variations derived from HKM and FKM have been developed to deal with different situations, most display degraded effectiveness in cases of imbalanced data. This is due to the so-called “uniform effect” that causes the clusters generated by these algorithms to have similar sizes even when input data has highly varying group sizes[24]. It is reported that FKM has a stronger uniform effect than HKM[25]. An illustration of the uniform effect of HKM and FKM is given in Fig.1where we can observe that the centroids of HKM and FKM crowd together in the large cluster.

SECTION: I-AExisting Efforts to Overcome Uniform Effect

There are two popular methods to overcome the uniform effect. The first method is to introduce more weight on the data points in small clusters at each updating iteration, biasing learning towards them, like a modified FKM called cluster-size insensitive FKM (csiFKM) developed by Noordam et al.[26]. Later, Lin et al.[27]proposed a size-insensitive integrity-based FKM (siibFKM) based on csiFKM to reduce the sensitivity of csiFKM to the distance between adjacent clusters. However, weighting based on cluster size inadvertently increases the influence of outliers, making these algorithms sensitive to noise[28].

The second method is called multiprototype clustering. This method first groups data into multiple subclusters with similar sizes and the final clusters are obtained by merging adjacent subclusters. Liang et al.[29]proposed a multiprototype clustering algorithm that employs FKM to generate subclusters. Later, Lu et al.[5]proposed a self-adaptive multiprototype clustering algorithm that automatically adjusts the number of subclusters. However, multiprototype clustering algorithms have a complex process and high time complexity of, whereis the number of data points in the dataset. Thus, they are computationally expensive for large datasets. We should additionally mention that Zeng et al.[30]recently proposed a soft multiprototype clustering algorithm with time complexity linear to. However, their clustering process remains complex and is aimed at clustering high-dimensional and complex-structured data rather than imbalanced data.

SECTION: I-BOur Contributions

In this paper, we propose equilibrium K-means (EKM), which addresses the imbalanced data clustering issue by a new centroid repulsion mechanism without the above limitations. Our contributions can be summarized in three main aspects.

First, we reformulate HKM, FKM, the maximum-entropy fuzzy clustering (MEFC)[31,32], and EKM in a general form of gradient descent. We show that these algorithms aim to optimize different approximations of the same objective. This general form facilitates the uniform study of KM algorithms.

Second, we develop EKM based on the first contribution where the Boltzmann operator is utilized as an approximation method. Such approximation brings a brand-new centroid repulsion mechanism. Data surrounding centroids repel other centroids, and large clusters repel more as they contain more data, successfully reducing the uniform effect by preventing centroids from crowding together in a large cluster (see Fig.1dfor an example). Hence, EKM is robust to imbalanced data and not sensitive to noise. Similar to HKM and FKM, EKM is a two-step alternating algorithm that iteratively computes centroids. In addition to having the same time () and space complexity as FKM, EKM has a batch-learning version that can be applied to large datasets. A comprehensive study is conducted using four synthetic and 16 real datasets (hence 20 datasets in total) to demonstrate the effectiveness of EKM on balanced and imbalanced data.

Finally, we investigate the combination of DNNs and EKM. We find that mapping high-dimensional data via DNNs to an EKM-friendly space can result in more discriminative low-dimensional representation than mapping to an HKM-friendly space. Compared to the combination of DNNs and HKM, using EKM improves clustering accuracy by 35% on an imbalanced dataset derived from MNIST.

SECTION: I-COrganization

We introduce HKM, FKM, and MEFC in SectionII. In SectionIII, we derive their general form and the proposed EKM. The properties of EKM and its centroid repulsion mechanism are studied in SectionIV. We evaluate the performance of EKM on classic clustering tasks in SectionVand on deep clustering in SectionVI. Finally, we conclude in SectionVII.

SECTION: IIK-Means and Its Variations

SECTION: II-AThe Hard K-Means Algorithm (Lloyd’s Algorithm)

KM aims to partitiondata points intoclusters, minimizing the sum of the variances within each cluster. Mathematically, the objective can be expressed as:

whererepresents the set of data points in the-th cluster,denotes a data point belonging to,is the centroid of, expressed by

signifies the size of, andis thenorm. This optimization problem is NP-hard[33]and is commonly solved heuristically by Lloyd‘s algorithm[9]. Givenpoints (as initial centroids, Lloyd’s algorithm alternates between two steps:

Assign each data point to the nearest centroid, formingclusters:

Recalculate the centroid of each cluster by taking the mean of all data points assigned to it:

Lloyd’s algorithm converges when the assignment of data points to clusters ceases to change, or when a maximum number of iterations is reached. The time complexity of one iteration of the above two steps is. The HKM algorithm mentioned in this paper refers to Lloyd’s algorithm.

SECTION: II-BThe Uniform Effect of K-Means

The uniform effect refers to the propensity to generate clusters of similar sizes. It is essentially a learning bias towards large clusters and is implicitly implied in the objective (1) of KM. For simplicity, kindly consider a case with two clusters (i.e.,). Minimizing the objective function (1) is equivalent to maximizing the following objective function[34]:

whereanddenote the sizes ofand, respectively. By isolating the effect of, maximizing the above objective leads to, indicating KM tends to produce equally sized clusters.

SECTION: II-CThe Fuzzy K-Means Algorithm (Bezdek’s Algorithm)

FKM attempts to minimize the sum of weighted distances between data points and centroids, with the following objective and constraints[10]:

whererepresents the-th data point,denotes the-th centroid,is a coefficient called membership that indicates the degree ofbelonging to the-th cluster, andis a hyperparameter controlling the degree of fuzziness level. Similar to the HKM algorithm, the FKM algorithm operates by alternating two steps:

Calculate the membership value of the-th data point belonging to the-th cluster:

Recalculate the weighted centroid of the-th cluster by:

The time complexity of one iteration of the above two steps is. The higher time complexity of FKM than HKM is due to the extra membership calculation.

SECTION: II-DMaximum-Entropy Fuzzy Clustering

Karayiannis[31]added an entropy term to the objective function of FKM, resulting in MEFC. The new objective function is given as follows:

whereis a hyperparameter, controlling the transition from maximization of the entropy to the minimization of centroid-data distances. MEFC is similar to FKM but with a different definition of membership:

where. The time complexity of MEFC is the same as FKM, which isfor one iteration.

SECTION: IIISmooth K-Means - A Unified Framework

SECTION: III-AObjective of Smooth K-Means

We propose a novel framework called smooth K-means (SKM) and demonstrate that the three KM algorithms introduced in the previous section are special cases of SKM.

Denote the squared Euclidean distance111Other distance metrics, such as the absolute difference and the angle between points, can also be used to define. Considering the derivation process is the same, we use the Euclidean distance in the paper for simplicity.between the-th centroid and the-th data point as

and define the within-cluster sum of squares (WCSS) as the sum of squared Euclidean distances between data points and their nearest centroids, i.e.,

The goal of SKM is to findcentroids that minimize an approximated WCSS, resulting in the following optimization problem:

whereis a smooth approximation toand is referred to as the smooth minimum function. Below we reveal the relationship between the SKM and the GMM.

SECTION: III-BThe Relationship Between SKM and GMM

The centroids obtained by minimizing WCSS are maximum likelihood estimators (MLEs) of parameters of “hard” GMM. The derivation is as follows. Assuming that the datasetis sampled fromindependent multivariate normal distributions. We denote the mean vector and covariance matrix of the-th normal distribution asand, respectively. The standard GMM has the following well-known likelihood

whereis the probability that the-th data point is generated from the-th normal distribution. The standard GMM assumes that data points are generated from multiple normal distributions based on a certain probability distribution. On the other hand, the hard GMM assumes that a data point is generated from a single normal distribution. Accordingly, for all,for oneandif. We further assume that all normal distributions have the same covariance matrix, specifically an identity matrix. Under these assumptions, the MLEs ofareif, andotherwise. By substitutingandinto (14) and taking the logarithmic value, we obtain the log-likelihood:

Now it is clear that the MLEs ofare the centroids minimizing WCSS.

The hard GMM model simplifies the standard GMM by only considering the impact of a data point on its closest centroid. However, as seen in the success of FKM, it can be more advantageous to consider the impact of a data point on all centroids. This can be accomplished in SKM by applying an approximation function to smooth WCSS. Hence, SKM can be viewed as a model between hard and standard GMM. In the following sections, we will introduce three common smooth minimum functions and explore the resulting distinct clustering algorithms.

SECTION: III-CThree Common Smooth Minimum Functions

Assume a monotonically increasing and differentiable functionsatisfies

or equivalently

whereis the first derivative of. Let, a smooth minimum functioncan be constructed by

Letbe defined as, then the functiontakes on a specific form known as LogSumExp:

whereis a parameter controlling the degree of approximation, withas.

Define, we can have another common smooth minimum function, called p-Norm, which has the following specific form

and converges toas. An additional definition is necessary for the domain of the p-Norm function to legally include zeros:if somewhere.

Smooth minimum functions can also be constructed by

A specific example is the Boltzmann operator, where. The Boltzmann operator takes on the form of

and converges to the minimum function as.

SECTION: III-DClustering by Minimizing Smoothed WCSS

Approximating WCSS (12) by LogSumExp (19), we have the following objective:

The differentiable objective functionhas the first-order partial derivative:

The minimizer ofcan be found by gradient descent iteration:

whereis the learning rate at the-th iteration. This updating procedure (25) is equivalent222It should be noted that the equivalence mentioned in this paper is in the sense of the algorithm level, not in the criterion level.to the MEFC algorithm if one set

Such learning rate value is related to the second-order partial derivative of, which we will discuss later. The membership (10) of MEFC is identical to.

In the limit of,if, andotherwise333We assume, if, then.. In this case, the learning ratewhereis the number of data points closest to, and the updating procedure (25) approaches to Lloyd’s algorithm.

The smooth objective functionpossesses the second-order partial derivative given by

whereand. By employing Newton’s method, the minimizer ofcan be iteratively found using

As,, and the updating procedure (28) approaches Lloyd’s algorithm. This indicates that Lloyd’s algorithm is essentially Newton’s method, which aligns with the perspective presented in[35]. There is a close relationship between the learning rate value (26) and the second-order partial derivative of(27): the gradient descent with the learning rate of  (26) is equivalent to an approximated Newton’s method in the sense that one term (the rank-1 matrix) in (27) is ignored. A discussion of the pros and cons of optimizingusing gradient descent or Newton’s method would be valuable, but it is beyond the scope of this paper. In the following section, we will observe the same connection between Newton’s method and FKM.

By substituting the minimum function in WCSS with the-Norm function (20), we have

The objectivehas the first partial derivative of

The minimizer ofcan be located using gradient descent iteration

When settingand

this gradient descent iteration is equivalent to the FKM algorithm. The membership (7) of FKM is equivalent to a power exponent () of.

The connection between FKM and Newton’s method becomes evident by taking the second-order partial derivative of, which is

where. Hence, FKM can also be viewed as an approximated Newton’s method that ignores the term of the rank-1 matrix.

SECTION: III-EFrom Boltzmann Operator to A Novel Clustering Algorithm

Employing the Boltzmann operator to smooth WCSS results in

The objectivepossesses the first-order partial derivative of

The minimizer ofcan be found using gradient descent iteration

where

This updating procedure which we call EKM can be reformulated into a two-step iteration procedure akin to FKM:

Calculate the weight value of the-th data point to the-th cluster by

Recalculate the weighted centroid of the-th cluster by

EKM converges when centroids cease to change or the maximum number of iterations is reached. The time complexity of one iteration of the above two steps is. For convenience, we summarise the complete procedure of EKM in Algorithm1.

SECTION: III-FPhysical Interpretation of Equilibrium K-Means

The second law of thermodynamics asserts that in a closed system with constant external parameters (e.g., volume) and fixed entropy, the internal energy will reach its minimum value at the state of thermal equilibrium. The objective (34) of EKM follows this minimum energy principle. This connection can be established by envisioning data points as particles with discrete/quantized energy levels, where the number of energy levels is equivalent to the number of centroids, and the energy value corresponds to the squared Euclidean distance between a data point and a centroid.

Boltzmann’s law tells that at the state of thermodynamic equilibrium, the probability of a particle occupying a specific energy level decreases exponentially with the increase of the energy value of that level. Hence, the objective function (34) equals the expectation of the entire system’s energy, and EKM seeks centroids to minimize this energy expectation. Due to this connection, we refer to the proposed Algorithm1as equilibrium K-means.

SECTION: III-GMembership Defined in Equilibrium K-Means

Membership can be defined in EKM. Although the sum of weights (38) of EKM equals one, i.e.,

it is worth noting that these weights cannot be interpreted as probabilities or memberships since some weight values are negative. We define membership of EKM from its physical perspective. From the physical interpretation of EKM, the exponential termcan be interpreted as the unnormalized probability of the-th data point belonging to the-th clusters. Hence, the membership of the-th data point to the-th cluster can be defined as

Since we have no intention in this paper to discuss the membership (it is just an intermediate product), we only define the membership in EKM and leave further discussion to subsequent research.

SECTION: III-HConvergence of Smooth K-Means

HKM, FKM, MEFC, and EKM are special cases of SKM, which can be generalized as the following gradient descent algorithm:

where,is a smooth minimum function,, and the learning rateis given by

Different KM algorithms can be obtained by takingthe corresponding explicit form (refer to SectionIII-DtoIII-E). This general form facilitates the uniform study of KM algorithms. Below we give a convergence guarantee conditioning on the properties of:

The centroid sequence obtained by (42) converges to a (local) minimizer or saddle point of the objective functionif the following conditions can be satisfied:

(Concavity) The functionis a concave function at its domain.

(Boundness) The functionhas a lower bound, i.e.,, and the learning rate sethas a positive lower bound, i.e.,, such thatfor alland.

See the Appendix for the proof, which generalizes the proof of convergence of fuzzy K-means in[36].
∎

It can be easily verified that when the smooth minimum functionis LogSumExp (19) and p-Norm (20) in which case SKM (42) is equivalent to MEFC and FKM, respectively, the above convergence condition can be satisfied with any initial centroids. Since the Boltzmann operator (22) is not globally concave, discussing the convergence of EKM is not simple. If centroids are updated within the concave region of the Boltzmann operator, the convergence of EKM can be guaranteed by Theorem1. However, the practical situation may be more complex. A rigorous convergence condition of EKM may involve a tedious discussion of a specific value of, positions of initial centroids, and data structures. Instead, we demonstrate the empirical convergence of EKM in SectionV-F.

SECTION: IVComparison of Different Smoothed Objectives

SECTION: IV-ACase Study

This section presents an empirical analysis of the behavior of different KM algorithms by examining their reformulated objective functions in some examples with well-designed data structures. Datasets comprising two classes of one-dimensional data points are generated by sampling from two normal distributions, drawingsamples from a distribution with meanand unit variance, andsamples from another with meanand unit variance. Using different parameter combinations, we generate four datasets: 1. A balanced, non-overlapping dataset (,, and; Fig.2a); 2. A balanced, overlapping dataset (,, and; Fig.2b); 3. An imbalanced, non-overlapping dataset (,,, and; Fig.2c); 4. An imbalanced, overlapping dataset (,,, and; Fig.2d).

We plotas a function ofin Fig.2, withbeing the position of the second centroid on the-axix, andbeing the reformulated objectives of HKM (WCSS (12)), FKM ((29)), MEFC ((23)), and EKM ((34)). The four objective functions behave similarly on the first two balanced datasets, but it is worth noting that the last two imbalanced datasets. Fig.2cand Fig.2dshow that the local and global minimum points of the objective functions of HKM, FKM, and MEFC are biased towards the center of the large cluster, i.e.,. In contrast, EKM does not have an obvious local minimum and its global minimum point aligns with the true cluster center, i.e.,, highlighting EKM’s superiority in handling imbalanced data.

SECTION: IV-BThe Analysis of EKM’s Effectiveness on Imbalanced Data: A Centroid Repulsion Mechanism

We analyze the effectiveness of EKM on imbalanced data based on the gradient of its objective. The gradient of the smoothed WCSS with respect to the data-centroid distance (i.e.,) can be interpreted as the force exerted by a spring. A positive gradient value represents an attractive force, while a negative value represents a repulsive force.

A simple example is given to plot the gradient values of different KM’s objectives. We fix two centroids atand, respectively, and move a data point along the-axis. In Fig.3awe display the gradients of WCSS,,, andwith respect to the distance between the data point and the second centroid. As evident from the figure, data points on the side of the first centroid do not impact the second centroid of HKM, but they do attract the second centroids of FKM and MEFC. This finding supports the claim in[25]that FKM has a stronger uniform effect than HKM. On the other hand, data points near the first centroid have repulsive forces on the second centroid of EKM, which compensate for the attraction from other data points. This new mechanism effectively reduces the algorithm’s learning bias towards other clusters, particularly large ones, because the centers of large clusters often contain more data. The existence of this mechanism suggests that the uniform effect of EKM is weak, a hypothesis that our experiments will test. Notably, data points near the second centroid of EKM exert the strongest attraction forces on this centroid. As a result, EKM’s centroids are stabilized by their surrounding data points, reducing their susceptibility to noise and outliers.

SECTION: IV-CThe Choice of EKM’s parameter

The smoothing parameterimpacts the performance of EKM, but the optimal choice remains unknown. This is not a difficulty unique to EKM. The FKM algorithm also struggles with selecting the optimal fuzzifier value,. Despite numerous studies discussing the selection of, a widely accepted solution has yet to be found[37].

As a rule of thumb, when the dimension of the data space is less than or equal to three, settingappears effective after normalizing the data to have a zero mean and unit variance for each dimension. As the data space dimension increases, the data-centroid distanceincreases, necessitating a decrease into ensure that the exponential termfalls within a normal range. Hence, one can setas a reciprocal of the data variance or manually tune it as follows. Initially settingas ten times the reciprocal of data variance and gradually reducing it until a sudden increase in centroid-centroid distance is observed. Because the increase in centroid distance implies the emergence of repulsive forces that reduce the uniform effect.

SECTION: VNumerical Experiments

SECTION: V-AExperimental Setup

Numerical experiments are conducted to compare the performance of our proposed EKM algorithm with seven related centroid-based algorithms including (1) HKM, (2) FKM, (3) MEFC, (4) PFKM[12], (5) csiFKM[26], (6) siibFKM[27], and (7) FWPKM[15]. Multiprototype clustering algorithms (e.g.,[29,5]) are not appropriate as baseline algorithms because they are too complex to be benchmarks for gauging the efficiency of EKM. We also avoid using hybrid methods, such as algorithms combining K-means with kernels proposed in[17,18,19], as benchmarks to ensure fairness. Implementation is conducted in Matlab R2022a and the operation system is Ubuntu 18.04.1 LTS with Intel Core i9-9900K CPU @ 3.60GHz X 16 and 62.7GiB memory. All datasets used and codes are available athttps://github.com/ydcnanhe/Imbalanced-Data-Clustering-using-Equilibrium-K-Means.git.

The experimental datasets contain four artificial datasets generated by us (including Data-A, Data-B, Data-C, and Data-D), 13 UCI[38]datasets (consisting of Image Segmentation (IS), Seeds, Wine, Rice, Wisconsin Diagnostic Breast Cancer (WDBC), Ecoli, Htru2, Zoo, Glass, Shill Bidding, Anuran Calls, Occupancy Detection, Machine Failure), and three Kaggle datasets (incorporating Heart Disease, Pulsar Cleaned, and Bert-Embedded Spam). So there are 20 datasets and TableIprovides their information, including name, instance number, feature number, reference class number, and coefficient of variation (CV). CV is used in previous literature[34]to measure the level of data dispersion. It is calculated as the ratio of the standard deviation of class sizes to the mean. Given the number of instances in each class as, we have

where

In[34], a CV value exceeding 1 indicates highly varying class sizes, while a value below 0.3 signifies uniform class sizes. However, there isn’t a widely accepted critical CV value implying that data is imbalanced. For rigorous statements, we establish that if a dataset’s CV is less than 0.4, it is considered balanced. Conversely, if the CV exceeds 0.7, the dataset is deemed imbalanced. Consequently, we have six balanced datasets and 14 imbalanced datasets. It should be noted that none of the datasets used has a CV value between 0.4 and 0.7, hence this range remains undefined.

All datasets undergo normalization, ensuring that each feature has zero mean and unit variance. All features are used for clustering purposes. The number of clusters is set to the reference class number. Convergence is achieved when the moving distance of centroids between successive iterations is sufficiently small relative to the magnitude of centroids, i.e.,

We set the maximum number of iterations to 500 to prevent algorithms from infinitely iterating due to failure to converge. Except for sporadic cases, all algorithms can reach the specified convergence within 500 iterations.

Centroids are initialized using the K-means++ algorithm[39]. Given that each run of K-means++ yields different outputs and the objectives of the tested clustering algorithms are known to be non-convex (i.e., multiple local optima exist), convergence may happen at one of the local optimal points. A common way of finding the global optimum is to carry out a number of replications followed by a selection of the best (lowest) objective value. Hence, each trial includes 100 repetitions, and we select the repetition with the lowest objective value as the final result for that trial. We average the performance of 50 trials to ensure the rationality of the experiment. This means we conductruns for each algorithm. 100 repetitions in each trial is still within the practical executable range and each trial can give almost consistent results as indicated by a small standard deviation. We utilize widely accepted measures of clustering performance for our evaluation indexes, including the normalized mutual information (NMI)[40], the adjusted rand index (ARI)[41], and the clustering accuracy index (ACC)[42]. Both NMI and ACC range from 0 to 1, with 0 indicating the worst, and 1 representing the best. ARI has a range from -1 to 1, in which -1 means two data clusterings are completely dissimilar, 0 indicates that data clusterings are essentially random, and 1 means two data clusterings are perfectly aligned.

FKM, PFKM, csiFKM, siibFKM, and FWPKM employ a typical fuzzifier value of[43]. The fuzzifier value of MEFC is set to. For EKM, we setfor the four artificial datasets. For the 16 real-world datasets with varying feature numbers, the parameteris set proportional to the data variance, as follows

where,is the-th data point, andis the total number of data points. The reason for such selection is explained in SectionIV-C.

SECTION: V-BArtificial Datasets

Fig.4displays scatter plots of the four artificial datasets along with selected data clusterings. The average and the standard deviation of evaluation indexes over 50 trials are provided in TableII, where the best and the second-best results are highlighted in bold for easy comparison. On Data-A, the data points are sampled from Gaussian distributions. While on Data-B the data points are sampled from uniform distributions. Data-C and Data-D are a mixture of Gaussian and uniform distributions. The difference is that the majority of data points on Data-C are sampled from a Gaussian distribution while the majority of data points on Data-D are sampled from a uniform distribution. Additionally, Data-D has more classes than Data-C. All four artificial datasets are highly imbalanced with CV values greater than one.

We can see from TableIIthat the performance of benchmark algorithms is poor on these artificial datasets, especially on Data-C and Data-D. Nevertheless, the proposed EKM is effective, noticeably outperforming the best benchmark algorithm. The csiFKM has a certain effectiveness on Data-A and Data-B but does not exceed EKM. As seen in Fig.4, the benchmark algorithms erroneously divide the majority class into multiple clusters to balance the data size (i.e., the uniform effect) while EKM does not. This experiment also proves that EKM is versatile in handling different data distributions.

SECTION: V-CStudy of Parameter Impact

We investigate the influence of the parameteron EKM in this experiment. We perform EKM withvalues ofon the four artificial datasets. Fig.5presents the corresponding NMI values for thesevalues. The NMI value of HKM is considered as a reference. It is evident that when, the NMI of EKM closely aligns with that of HKM. This similarity arises because a sufficiently largemakes the objective of EKM nearly identical to the objective of HKM. As the value ofdecreases, the NMI of EKM increases abruptly, which implies the emergence of repulsive force between centroids overcoming the uniform effect. When, EKM becomes inferior to HKM. This is because the centroids of EKM tend to overlap whenis particularly small (repulsion becomes attraction, see Fig.3dwhen). Fortunately, the range ofthat causes the centroids to overlap is narrow.

SECTION: V-DReal Datasets with Balanced Data

To evaluate EKM’s effectiveness on balanced data, we perform it on six real datasets: IS, Seeds, Heart Disease, Wine, Rice, and WDBC. These datasets are routinely used in the field of machine learning and are characterized by uniform class sizes, CV values less than 0.4, and are categorized as balanced datasets as per our protocol. TableIIIshows the clustering results. EKM delivers superior performance on the Wine dataset and is comparable to the baseline performance on the remaining five datasets. Although EKM’s mechanism makes it more advantageous for imbalanced data, this experiment proves that EKM is competitive for balanced data.

SECTION: V-EReal Datasets with Imbalanced Data

The considered datasets here are Ecoli, Htru2, Zoo, Glass, Shill Bidding, Anuran Calls, Occupancy Detection, Machine Failure, Pulsar Cleaned, and Bert-Embedded Spam. Among them, Htru2 and Occupancy Detection have a large amount of data, Glass and Ecoli have 6 and 8 classes respectively, and Zoo has high dimensionality. They have varying class sizes with CV values greater than 0.7. Except for Occupancy Detection and Zoo, the other eight datasets have CV values greater than 1. Bert-Embedded Spam has over 700 features. Since the Euclidean distance is ineffective for clustering algorithms on high-dimensional data, we execute principal component analysis on the Bert-Embedded Spam data. The first five principal components are used for clustering as adding more does not improve and even compromise performance. TableIVdisplays the results, where the best and the second-best results are in bold.

As can be seen from TableIV, in general, EKM obtains the best performances on eight datasets, and achieves the second-best clustering performances on the remaining two datasets (Zoo and Anuran Calls), as confirmed by all three measured clustering indexes. Specifically, although csiFKM outperforms EKM on the Zoo dataset, it performs substantially worse on Htru2, Occupancy Detection, Shill Bidding, etc. HKM performs well on Anuran Calls but underperforms on Shill Bidding, Machine Failure, and Bert-Embedded Spam. Due to the structural complexity of real datasets, it is reasonable that some algorithms perform well on certain datasets. In comparison, EKM performs consistently well on all datasets, especially on the Htru2, Shill Bidding, Machine Failure, and Bert-Embedded Spam datasets, where it significantly outperforms the benchmark algorithms. Fig.6illustrates the scatter diagrams of Htru2, Shill Bidding, Machine Failure, and Bert-Embedded Spam datasets, along with some clustering results. Comparing Fig.LABEL:sub@fig:htru2_fwpkm-LABEL:sub@fig:spam_mefcwith Fig.LABEL:sub@fig:htru2_ekm-LABEL:sub@fig:spam_ekm, again, we can see the uniform effect exists in the benchmark algorithms. Conversely, the centroids obtained by EKM are farther away from each other, implying the existence of the repulsive force overcoming the uniform effect. Note that, we can only see one centroid in some figures because centroids overlap on the selected two coordinates. Overall, the experiment results provide strong evidence to support the superiority and consistent performance of EKM on imbalanced data. Note that, we simply setproportional to the data variance. EKM’s performance can be further enhanced by fine-tuning the parameter.

SECTION: V-FEmpirical Convergence and Algorithm Efficiency

TableVshows the average number of iterations and the average time per run (each algorithm is run 5000 times). The numbers in brackets in the table are the ranking numbers of the corresponding algorithms. AVK and MDK give the average and median number of iterations and computational time of each algorithm on all test datasets, respectively.

As evident from TableV, AVK and MDK are consistent except that EKM is faster than FKM in AVK and slower in MDK. This is because FKM is slow on the Bert-Embedded Spam dataset. HKM is the fastest algorithm as it does not require membership calculation. MEFC comes second, followed by FKM and EKM. EKM is 30% faster than FKM in AVK and slower than FKM by 9% in MDK. In terms of absolute speed, MEFC, FKM, and EKM operate within the same order of magnitude. This aligns with their identical time complexity ofper iteration, whereis the instance number,is the cluster number, andis the data dimension or feature number. The table also demonstrates the empirical convergence of EKM, averaging 44 iterations or a median of 20 iterations to reach the specified convergence (45). In summary, EKM has a low computational cost.

SECTION: VIDeep Clustering

Clustering algorithms using Euclidean distance for defining similarity face a challenge when clustering high-dimensional data, such as images, due to the distance differences between point pairs vanishes in high-dimensional space[44]. Deep clustering is a technique that addresses this issue by employing DNNs to map high-dimensional data to low-dimensional representation. Many deep clustering methods combine DNNs with HKM, e.g.,[21,22,23]. However, this design is less favorable to imbalanced data[22]. In this section, we show that EKM is a better alternative for deep clustering of imbalanced data.

SECTION: VI-AOptimization Procedure

Deep clustering network (DCN)[21]is a popular deep clustering framework. As illustrated in Fig.7, DCN maps high-dimensional data to low-dimensional representation through an autoencoder network. An autodecoder follows the autoencoder, mapping the representation back to the original high-dimensional space (i.e., reconstruction). To ensure that the low-dimensional representation maintains the primary information of the original data, the autoencoder and the autodecoder are jointly trained to minimize the reconstruction error. Additionally, to make the low-dimensional representation have a clustering-friendly structure, a clustering error is minimized along with the reconstruction error. DCN uses an alternating optimization algorithm to minimize the total error, and the optimization process is described below.

First, the autoencoder and the autodecoder are jointly trained to reduce the following loss for the incoming data:

whereandare simplified symbols for autoencoderand autodecoder, respectively. The functionis the least-squares lossto measure the reconstruction error. The assignment vectorhas only one non-zero element and, indicating which cluster the-th data belongs to, and the-th column ofis the centroid of the-th cluster. The parameterbalances the reconstruction error versus the clustering error. Then, the network parametersare fixed, and the parametersare updated as follows:

whereis the-th element of. Finally,is updated by the batch-learning version of the HKM algorithm:

whereis the number of samples assigned to the-th cluster before the incoming data, controlling the learning rate of the-th centroid. Overall, the optimization procedure of DCN alternates between updating network parametersby solving (47) and updating HKM parametersby (48) and (49).

However, the centroid updating rule (49) is problematic for imbalanced data due to the uniform effect. To address this issue, we propose to replace (49) with the batch-learning version of EKM:

where. There are other details and tricks to implement DCN, such as the initialization of the networks. We only introduce the part related to our contribution, and kindly refer to[21]for more implementation details.

SECTION: VI-BClustering Performance on MNIST

To implement DCN, we refer to the code one of its authors provided, available athttps://github.com/boyangumn/DCN-New. We use the default neural network structure and hyperparameters. In particular, the dimension of the low-dimensional representation is set to ten, and the parameteris set to one. The smoothing parameteris tuned on the representation obtained by the initialized DCN network according to the strategy introduced in SectionIV-C.

We first evaluate the algorithm’s performance on a balanced dataset. The considered dataset is the full MNIST[45], which contains 70,000 gray images of handwritten digits from 0 to 9. Each digit has approximately 7,000 images and each image haspixels. We set the smoothing parameter of EKM to. The clustering results are presented in TABLEVI. We compare the proposed DCN+EKM with DCN+HKM and stacked autoencoder (SAE). SAE is a specific version of DCN that only minimizes the reconstruction error. Thus, the learned representation by SAE does not have a clustering-friendly structure. The results show that DCN outperforms SAE, highlighting the importance of a clustering-friendly structure. We can also see that DCN+EKM performs similarly to DCN+HKM. We omit the results of DCN+FKM and DCN+MEFC due to save space. Their performance is not better than DCN+HKM.

Then we evaluate the algorithm performance on an imbalanced dataset. The considered dataset is derived from the full MNIST by removing the training images of digits 1 to 9. This imbalanced dataset (we call imbalanced MNIST) contains 15,923 images, of which approximately 7,000 are digit 0, while the remaining digits (1 to 9) each have about 1,000 images. We set the smoothing parameter of EKM to. The results are summarized in TABLEVII. We can see that DCN+EKM greatly outperforms other algorithms. This finding implies that the EKM-friendly structure is crucial for deep clustering of imbalanced data.

We map the ten-dimensional representation obtained by DCN to a two-dimensional space by t-SNE[46]for visualization. The representation learned on the full MNIST dataset is displayed in Fig.8and the representation learned on the imbalanced MNIST dataset is displayed in Fig.9. The visualization results suggest that the deep representation obtained by DCN+EKM is more discriminative than that obtained by DCN+HKM as the former has clearer inter-cluster margins, especially for those obtained from the imbalanced MNIST dataset. This should be attributed to the centroid repulsion mechanism of EKM, which makes data farther away in representation space. We also observe from Fig.9that EKM successfully identifies the large class (digit 0) from other small classes without referring to the true labels, while HKM incorrectly divides the large class into four clusters to balance the cluster sizes.

MethodsSAE+HKMDCN+HKMSAE+EKMDCN+EKMNMI0.7250.7980.7110.813ARI0.6670.7440.6420.731ACC0.7950.8370.7820.808

MethodsSAE+HKMDCN+HKMSAE+EKMDCN+EKMNMI0.5510.5840.5830.701ARI0.3170.3250.3960.826ACC0.4130.4340.4970.784

SECTION: VIIConclusion

This paper presents equilibrium K-means (EKM), a novel clustering algorithm with a new centroid repulsion mechanism effective for imbalanced data. EKM is simple, interpretable, and scalable to large datasets. Experimental results on datasets from various domains show that EKM outperforms HKM, FKM, and other state-of-the-art centroid-based algorithms on imbalanced datasets and performs comparably on balanced datasets. We also demonstrate that EKM is a better alternative to HKM and FKM in deep clustering when dealing with imbalanced data. Furthermore, we reformulate HKM, FKM, and EKM in a general form of gradient descent. We encourage readers to study more properties of this general form. Combining EKM with kernels, multi-prototype mechanisms, and other techniques to deal with more complex data structures is also an important research direction. We believe that these explorations will promote the development of EKM and further solve the data imbalance issue.

SECTION: Appendix AProof of Theorem1

For concise and tidy, we denote the partial derivativeas,as, andas. Sinceis a concave function at, we have

which holds for anyand. Summing over, it follows that

Denote the function on the right side of the inequality as. With the boundness condition, we havefor anyand, thus,is a quadratic function and strictly convex, with the unique global minimizer atdefined by (42). Denoteas,as, andas. Each iteration of the centroid will reduce the objective function by

whereis a positive number. Hence, the sequenceis non-increasing, and with the boundness condition that, we have. If the left side of the inequality (LABEL:theorem1_proof_inequality) converges to zero, the right side of the inequality also converges to zero since it is non-negative. Consequently, we havefor all. Therefore, the sequenceconverges to a stationary point of the objective function. Becauseis non-increasing, only (local) minimizers or saddle points appear as limit points.

SECTION: References