SECTION: Low-Rank Continual Pyramid Vision Transformer: Incrementally Segment Whole-Body Organs in CT with Light-Weighted Adaptation
Deep segmentation networks achieve high performance when trained on specific datasets. However, in clinical practice, it is often desirable that pretrained segmentation models can be dynamically extended to enable segmenting new organs without access to previous training datasets or without training from scratch. This would ensure a much more efficient model development and deployment paradigm accounting for the patient privacy and data storage issues. This clinically preferred process can be viewed as a continual semantic segmentation (CSS) problem.
Previous CSS works would either experience catastrophic forgetting or lead to unaffordable memory costs as models expand. In this work, we propose a new continual whole-body organ segmentation model with light-weighted low-rank adaptation (LoRA). We first train and freeze a pyramid vision transformer (PVT) base segmentation model on the initial task, then continually add light-weighted trainable LoRA parameters to the frozen model for each new learning task. Through a holistically exploration of the architecture modification, we identify three most important layers (i.e., patch-embedding, multi-head attention and feed forward layers) that are critical in adapting to the new segmentation tasks, while retaining the majority of the pre-trained parameters fixed. Our proposed model continually segments new organs without catastrophic forgetting and meanwhile maintaining a low parameter increasing rate. Continually trained and tested on four datasets covering different body parts of a total of 121 organs, results show that our model achieves high segmentation accuracy, closely reaching the PVT and nnUNet upper bounds, and significantly outperforms other regularization-based CSS methods. When comparing to the leading architecture-based CSS method, our model has a substantial lower parameter increasing rate (16.7% versus 96.7%) while achieving comparable performance.

SECTION: Introduction
Multi-organ and tumor segmentation, one of the most essential medical image analysis tasks, has been widely studied in the literature. With the fast development in deep learning segmentation techniques, deep segmentation networks trained on specific datasets achieve high performance comparable to those of medical experts. However, current deep segmentation approaches are not capable of updating the trained models effectively when new segmentation classes are incrementally added, although in clinical practice it is desirable that pre-trained segmentation models can be dynamically extended to segment new organs without access to previous training datasets. Illustrated in Fig., this preferred process can be viewed as a continual semantic segmentation (CSS) problem, which is a non-trivial task because deep learning models suffer from catastrophic forgetting when fine-tuned directly on new dataset.

CSS is emerging very recently in the natural image domain, and the most common CSS approaches adopt the regularization constraint network training via knowledge distillation to reduce the forgetting of old knowledge while learning new classes. However, since entire network parameters are updated on the training of new classes, it is extremely difficult to achieve high performance on both old and new classes.
CSS has been rarely studied in medical imaging field. Ozdemir et al. uses only 9 patients and 2 organ labels to develop a regularization-based continual segmentation model. Liu et al. adopts the MiB lossand prototype matching to continually segment a small number of 5 abdominal organs, and Zhang et al. utilizes the pseudo-labels and clip-embedded controller head to segment 13 abdominal organs. Note thatboth only focus on a limited organs in the abdomen CT, and when involving a large number of organs of various body parts, such as in whole-body CT scans, they suffer severe performance degradation (as demonstrated in our experiments later). Recently, a new architecture-based CSS methodis proposed that avoids forgetting by freezing the CNN encoder after the initial task and sequentially adding separate decoder for each new task. Although it achieves high performance without forgetting, the method is less scalable because model parameters increase dramatically as new tasks are added (see Figure 1(b)). The completely frozen encoder also lacks of extensibility.

In this work, we aim to develop a new CSS method that avoids the catastrophic forgetting and meanwhile circumvent the model parameter explosion issue in. To achieve this goal, we take inspirations from two categories of recent technique advancements. First,
vision transformer (ViT) is widely used in recent applications, and exhibits superiority in global feature extraction, self-supervised large model pretraining, and multi-modality learning as compared to the CNN-based models. In medical imaging, ViT-based models also demonstrate great potential for multi-organ segmentation task, since many of them exhibit comparable performance with the leading CNN-based models.
Considering the capacity advantage of ViT models and its flexibility in being extended to diverse tasks, we envision that ViT-based architecture is suitable for CSS task.
Second, many recent parameter efficient fine-tuning (PEFT) methods are demonstrated to be effective when adapting the large scale pretrained language model to different downstream applications. For example, low-rank adaptation (LoRA)is one of the most popular and effective PEFT methods by freezing the pretrained model weights and injects trainable rank decomposition matrices into the linear or convolution layer of ViT. Hence, we assume that PEFT is capable of extending model’s capacity to segment new organs with minor increased model parameters.

Motivated by the above observations, we propose a new architecture-based CSS method for continual whole-body organ segmentation using pyramid vision transformer with LoRA. We adopt the UniMISSpretrained 3D pyramid vision transformer (PVT) as backbone due to its large scale medical image pretraining and the leading performance on downstream segmentation tasks. To circumvent the issue of catastrophic forgetting, we introduce a subset of trainable parameters for each new task. Unlike previous methods that append a bulky decoder for each task, our approach utilizes LoRA on selected PVT layers to incrementally expand its capacity for segmenting new organs. Following the original LoRA configuration, a group of LoRA matrices are first injected to query & value projection layers in multi-head attention to enhance the feature extraction. Furthermore, through a holistically exploration of the architecture modification, we inject LoRA matrices to the feed-forward network (FFN) to provide extra feature aggregation capability necessary for adapting to new unseen tasks. Additionally, we further extend LoRA matrices to 3D convolution in patch embedding layers of encoder and the last layer of decoder, which it critical to handle the large spacing variation in different medical segmentation tasks. Continually trained and tested on four datasets covering different body parts of a total of 121 organs, results show that our model achieves high segmentation accuracy, closely reaching the PVTand nnUNetupper bounds, and significantly outperforms other regularization or pseudo-label based CSS methods.

SECTION: Method
SECTION: Problem Formulation
Figureillustrates the proposed low-rank continual (LoCo) multi-organ segmentation framework. We adopt the UniMISSpretrained 3D PVT as backbone. Subsequently, the 3D PVT undergoes further training with the TotalSegmentator dataset. After this additional training, the PVT backbone, as depicted by the blue dashed-line blocks in Figure, is fixed throughout the subsequent training process. For the remaining tasks, letrepresent the datasets sequence. The model is trained sequentially on eachwhereand will not re-accessafter training is complete. Consider thedatasetthat compromisesorgan classes, and assuming,denote all input images and the corresponding segmentation masks in, the prediction map for voxel locationand organ classis given by

wheredenotes the transformer neural networks with frozen parameterstrained on initial task and task-specific trainable LoRA parameterswheredenotes the pairs of low rank matrices for thedataset, anddenotes the trainable task-specific Sigmoid output. The final predictionis the union of all previous predictions with possible class overlapping.

SECTION: LoCo-PVT: LoRA Continual Vision Transformer Layer
Our LoCo-PVT framework inherits key advantages of both methods and is customized for continual multi-organ segmentation. The low-rank adaptations are enabled within every transformer block as well as the patch embedding modules of the encoder. For each dataset, we associate a small set of trainable LoRA parameterswheredenotes LoRA matrices of theLoCo-PVT block andrepresents the total number of trainable LoCo-PVT blocks for thedataset.

For a pretrained weight matrixand thedataset, we constrain its update by representing the latter with a low-rank decomposition, where, and the rank. During training,is frozen and does not receive gradient updates, whilecontain trainable parameters. Bothandare multiplied with the same input, and their respective output vectors are summed coordinate-wise. The forward pass ofcan summarized as

Eachis initialized with random Gaussian andwith a zero matrix, resulting inbeing zero at the start of training. Then,is scaled bywhereis a constant in.

Although LoRA may be applied to any dense layer, Hu et al.,shows that applying it to queries and values of the MHA module yields the most significant performance gains. Therefore, we adopt a similar design choice in each LoCo-PVT block and uses a higherto accommodate for the greater complexity in learning from visual signals.

SECTION: LoCo-PE: LoRA Continual Patch Embedding Layer
The patch embedding modules project input patches into implicit embedding space of lower dimensions. Each encoder stage of LoCo-PVT is accompanied by a separate embedding module which extracts feature maps for various resolutions. Such spatial information are critical for training robust continual segmentation models across different datasets. Desirably, one should allocate trainable parameters to all convolutional projectors for each dataset. Compared to dense layers, it is observed that the inclusion of LoRA in convolutional layers resulted in a significant increase in number of parameters. To mitigate this issue, the application of convolutional LoRA is confined exclusively to the encoding PE layers, i.e., LoCo-PE layer, which incorporates LoRA-enabled 3D convolutions for projecting input patches to the embedding space in each encoder stage. To enhance feature aggregation and projection from all scales for the segmentation output, LoRA is also enabled in the second to the last convolution layer in decoder.

In each 3D convolutional weight matrixof dataset, a pair of matriceswith the same rank, where,is kernel size. The forward pass of the convolutional operations are

whereis the frozen convolution weights from the pretrained.

To merge the output probability maps from all learned tasks, we follows the body-part-aware output merging method from SUN, which pre-computes the average body part distribution map for each dataset, applies body-part regression over testing scans to eliminate the out-of-distribution body-part region from each task’s prediction, then uses entropy-based ensemble to combine the prediction from all tasks. No task ID is required during inference.

SECTION: Experiments and Results
We evaluated the proposed model using the public dataset(TotalSeg) as task 0 for base model training, which consists of 1204 CT scans of different body parts with 103 labeled anatomical structures (face label is removed). Similar to SUNdataset setting, we conduct continual segmentation on three in-house datasets which cover chest body part, head-neck body part and an esophageal dataset with tumor.(CHO) contains 153 chest CT scans with 16 labeled chest organs, including 7 overlapping organs with TotalSeg and 9 new organs.(HNO) includes 244 head & neck CT scans with 9 new organs are annotated. The last(EsoTumor) contains 567 CT scans of esophagus with tumor, which is more challenging for esophagus segmentation. We usesplit for training and testing. At the final stage, a total of 121 organs are learned from all datasets. The median voxel resolutions aremm,mm,mm,
andmm for TotalSeg, HNO, CHO and EsoTumor datasets, respectively.

In CSS experiments, the model is updated on a sequence of datasets. At each step, only the current dataset is used for training while all the previous datasets are not accessible. Following SUNsetting, two CSS orders are validated in order to demonstrate the robustness of the method. Order A:. Order B:, as shown in Table. We compare our method with 4 leading CSS works: 2 popular regularization-based baselines (MiB, PLOP), a regularization-based method in medical (LISMO) and an latest architecture-based method (SUN). All methods are implemented in nnUNet data preprocessing and augmentation framework. Our method uses PVT as backbone, while the other 4 methods are based on CNN. In this study, the upper bound of both PVT and nnUNet on each dataset are listed in Table. We report the final continual segmentation performance using the Dice similarity coefficient (DSC) and theHausdorff distance (HD95).

The 3D PVT base model is the same as UniMISSmodel-small, which contains 4 encoder blocks withPVT layers each and 3 decoder blocks with [3, 4, 3] PVT layers each. The stride is 2 for the convolution in encoder PE and deconvolution in decoder PE for down-/up-sampling purpose. For LoRA setting, we set rank as 64, 16 for query/value layers and FFN in LoCo-PVT and 16 for convolution layers in LoCo-PE/LoCo-Conv; LoRA alpha is consistently set as half of corresponding rank. The encoder is initialized from Unimiss self-supervised pretrained parameters. Following Unimiss setting, batch size of 2 and patch size ofare used for all datasets and the experiments. The ratio between the training and validation set is 4:1. All experiments are trained using AdamW and we set 6000 epochs for training base model on TotalSeg and 500 epochs for continual training steps. The initial learning rate for PVT is set asand weight decay as. Models are trained on single NVIDIA A100 GPU.

The final continual segmentation results on two orders and single task upper bounds are shown in Table. On the previously learned three datasets and all organs, our method significantly outperforms 3 regularization-based methods (MiB, PLOP, LISMO) in both orders, where the severe catastrophic forgetting of these methods could be caused by large domain gap between different body parts. On the other hand, our method and SUN are both architecture-based methods hence have no forgetting over past tasks and are order invariant when base training dataset is the same (TotalSeg). Although SUN has a slightly higher mean Dice of 90.45% than our 89.26% on all organs, our parameter increasing rate is significant lower than SUN (16.7% vs. 96.7% on 3 continual tasks), since SUN adds an entire decoder for each new task while our method adds light-weighted low-rank adaptors in selected layers, which only increases 5.56% per task. Note that, there is also a small gap between nnUNet upper bound 90.49% and PVT upper bound 89.66% on all organs, which shows a potential capability difference between nnUNet and PVT and might be the cause of the tiny performance gap between SUN (nnUNet-based) and LoCo-PVT (1.19%). Our proposed method closely reaches the PVT upper bound on all organs with a marginal 0.4% drop in DSC and a 0.9mm increase in HD95, which demonstrates the efficiency and effectiveness of continual LoRA with a well-trained frozen PVT.

To demonstrate the importance of each continual LoRA components in the proposed LoCo-PVT network, we also conduct two ablation studies using one-step continual segmentation from TotalSeg to CHO, shown in Table. In the left table, various LoRA combinations are evaluated over three PVT components, including query & value projection layer in multi-head attention (Attn-QV), feed-forward network in transformer layer (FFN) and 3D convolution in patch embedding layer (PE-Conv). Compared to ‘Base’ setting, adding extra LoRA to FFN increases the CHO segmentation performance by 3.46%, from 76.38% to 79.84%; adding extra LoRA to PE-Conv results in 80.36%, which gains 3.98%; our full LoCo design with LoRA in all three components further boosts the performance to 81.51% and reduces HD95 from 7.96mm to 5.77mm. This ablation result shows that it is effective and essential to add LoRA in both FFN, which provides extra ability to project and ensemble new features from attention, and PE-Conv, which makes adaptation or localization on different patch resolution. In the right table, we further study the effect of adding LoRA in either encoder or decoder. In ‘encoder only’ setting with frozen decoder, the mean DSC drops to 76.89% and HD95 rises to 7.88mm; Similarly, in ‘decoder only’ setting with frozen encoder, the mean DSC reduces to 77.21% and HD95 increases to 7.52mm. The results shows that LoRA in both encoder (feature extraction) and decoder (organ localization) helps enhancing the adaptation ability of the network on new tasks and works equally important for LoCo-PVT network to get comparable performance with the upper bound. This ablation study validates the necessity of each component in our proposed network.

SECTION: Conclusion
In this paper, we propose a new LoCo-PVT framework which combines LoRA with ViT for continual whole-body organ segmentation. We train and freeze a PVT base model on the initial task, then continually add light-weighted trainable LoRA parameters to the frozen base model, which avoids catastrophic forgetting and adapts the model to new tasks while maintaining a low parameter increasing rate. Our method achieves very high accuracy on four datasets covering different body parts,
closely reaching the PVT upper bound, and outperforms other regularization-based methods. When comparing to leading architecture-based CSS method, our model exhibits a significantly lower parameter increase rate while achieving comparable performance. This efficiency highlights the effectiveness of our approach in optimizing resource use without compromising on the quality of organ segmentation. Future works include extending the LoCo-PVT to multi-modality datasets and other light-weighted ViT adaptation methods.

The authors have no competing interests to declare that are relevant to the content of this article.

SECTION: References