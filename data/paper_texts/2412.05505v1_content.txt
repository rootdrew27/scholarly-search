SECTION: Trimming Down Large Spiking Vision Transformers via Heterogeneous Quantization Search
Spiking Neural Networks (SNNs) are amenable to deployment on edge devices and neuromorphic hardware due to their lower dissipation. Recently, SNN-based transformers have garnered significant interest, incorporating attention mechanisms akin to their counterparts in Artificial Neural Networks (ANNs) while demonstrating excellent performance. However, deploying large spiking transformer models on resource-constrained edge devices such as mobile phones, still poses significant challenges resulted from the high computational demands of large uncompressed high-precision models. In this work, we introduce a novel heterogeneous quantization method for compressing spiking transformers through layer-wise quantization. Our approach optimizes the quantization of each layer using one of two distinct quantization schemes, i.e., uniform or power-of-two quantification, with mixed bit resolutions. Our heterogeneous quantization demonstrates the feasibility of maintaining high performance for spiking transformers while utilizing an average effective resolution of-bits with less than a 1% accuracy drop on DVS Gesture and CIFAR10-DVS datasets. It attains a model compression rate of-for standard floating-point spiking transformers. Moreover, the proposed approach achieves a significant energy reduction of,, andwhile maintaining high accuracy levels of,, andon the N-Caltech101, DVS-Gesture, and CIFAR10-DVS datasets, respectively.

SECTION: Introduction
Vision transformers (ViTs) have emerged as a powerful vision backbone substitute to convolutional neural networks(CNNs). However, ViTs’ state-of-the-art accuracy comes at the price of prohibitive hardware latency and energy consumption for both training on the cloud and inference on the edge, limiting their prevailing deployment on resource-constrained devices. Spiking neural networks (SNNs), as the third generation of neural networks, are models of computation that more closely resemble biological neurons with real-time processing and low-power dissipation. SNNs can be seamlessly adapted to edge devices, enabled by great progress in neuromorphic applicationswith high accuracy. More recently, several works have demonstrated the potential of realizing spiking transformers with low-power dissipation to address the aforementioned issues associated with ViTs.propose a spiking-based self-attention mechanism and adapt the existing transformer architecture into spiking neural networks for vision tasks, reducing energy consumption while delivering improved performance over CNN-based SNNs.further improve spiking neurons’ spatiotemporal attention abilities and enhance attention map’s representativeness by denoising functions or hashmaps. In addition, as a variant of SNNs, the sparse and event-driven nature of spikes causes the spiking ViT’s computation to leverage multiple neuromorphic hardware platformsto support a real-time and low-power computation paradigm. A typical spiking transformer architecture is depicted in Fig..

Despite the encouraging progress on SNN-based ViTs, one pressing hurdle is that these approaches still suffer from large model sizes and expensive use of full-precision weights and multiplication operations. As such, these models are not amenable to deployment on resource-strained edge devices due to prohibitively high computational/memory access overheads, and energy consumption.

Quantization is inherently hardware-friendly and offers a promising model compression solution, and has been adopted for DNN/SNN hardware accelerators.proposes a uniform and non-layer-wise quantization framework Spiking CNNs.binarizes the weight of all layers and replaces multiplication with XOR operations to in spiking CNNs. However, precision loss in uniform quantization and unsystematic aggressive quantization can lead to large performance drop and limit model compression ratio. Despite the promise of quantization for model compression, the use of quantization for SNNs and transformers in general has been limited.
We introduce, the first approach to heterogeneous quantization for spiking neural networks, with a specific focus on spiking transformers. With a neural architecture search formulation,employs optimal layer-by-layer compression, utilizing either a uniform or power-of-two quantization scheme with mixed bit resolutions. While quantization has been employed for both artificial neural network (ANN) and spiking neural network (SNN) accelerators, heterogeneous quantization has not been explored for SNNs in general and ANN-based transformers.

Our experimental studies reveal thatcan compress the weights in a given spiking transformer from 32-bit floating-point numbers to 2-bit integers, achieving an impressive compression rate of.showcases the feasibility of maintaining high performance for spiking transformers with an average effective resolution of-bits, and less than a 1% accuracy drop on DVS Gesture and CIFAR10-DVS datasets.demonstrates a substantial reduction in total energy consumption, ranging fromto, and a storage overhead reduction betweenand, all while maintaining competitive task performance when compared to the high-precision unquantized spiking transformers. Furthermore, our empirical findings underscore specific layers/computations within a spiking transformer that necessitate high-precision weight parameters. These layers are identified as key bottlenecks for potential efficiency improvements, highlighting the need for targeted enhancements in future architectural and design innovations.

SECTION: Background
SECTION: Spiking Transformers
adapts self-attention mechanismsand ANN-based transformer architectures for spiking neural network (SNN) based implementation to improve learning and efficiency. It takes a 2D image sequence as input and uses the Spiking tokenizer module to project the image into a spike-form feature vector, which is further divided into flattened spike-form patches. A spike-form relative position embedding is generated and added to the patches. The architecture proposed inincludes L-block Spikformer encoders, each consisting of a Spiking Self Attention (SSA) module and an MLP block. Residual connections are established within the SSA and MLP blocks. The SSA module models local-global information using spike-form Query, Key, and Value components without employing softmax. A global average-pooling operation is performed on the processed features, and the resulting feature vector is fed into a fully-connected classification head to obtain the final prediction.

SECTION: Heterogeneous Quantization
Spiking transformers are advantageous in their improved biological plausibility. Equally importantly, they can lead to energy efficient processing due to the binary nature of spiking activities. However, the last key benefit has not been fully exploited in the existing spiking transformer architectures. In, even though the spiking neurons produce one-bit binary outputs, due to the residual connections across different blocks, the output of a block are multi-bit integers, which are inputs to the subsequent block. Furthermore, the parameters of each spiking transformer layer are floating point numbers to maintain accuracy, rendering expensive multi-bit additions and multiplications and high storage overheads.

We argue that the characteristics of spiking neural networks, when explored properly, would offer ample opportunities for weight quantization. Spiking activations are binary, signifying the robustness of spike-based computation with respect to parameter imprecision, and provides room for weight quantization either uniformly or in other means.
Multipliers are much more costly in terms of area, power, and latency compared with shifters and adders.
Multiplications can be efficiently performed with addition and shift operationswith simple hardware implementation at higher speeds. On present-day processors, bit-shift instructions are faster than multiply/divide instructions and can be leveraged to perform multiplications and divisions by powers of two. Multiplication (or division) by a constant can be implemented using a sequence of shifts and adds (or subtracts). This would particularly favor the power-of-two quantization, which we explore along with uniform quantization in.

Uniform quantization is defined by three parameters: the scale factor, the zero-point, and the bit-width.andare both a floating-point number, and are used to map a floating-point valueto an integer grid, whose size depends on:

Power-of-two quantization is symmetric with a power-of-two scale factor. Scaling withcan be efficiently realized by performing bit-shiftingtimes. It is a suitable quantization choice for spiking transformers as the weight distributions are approximately symmetric. In spiking transformers such as, leveraging the residual connection results in a progressive enlargement of spike activation range accumulation as the network depth increases. Consequently, the adoption of power-of-two quantization enables efficient shift operations over low-precision multipliers, also enhancing the compression rate.
The b-bit quantizer quantizes a floating point numberinto a power-of-two number as:

The selection of the quantization scheme for each spiking transformer layer is determined by solving a neural network architecture search problem, as detailed in Section.

SECTION: Heterogeneous Quantization by Neural Architecture Search
SECTION: Modeling of Hardware Overhead
The proposed neural architecture search (NAS) finds the optimal heterogeneous quantizations for a given spiking transformer while jointly optimizing model accuracy and hardware overhead. We adopt the standard cross-entropy loss to evaluate model accuracy while proposing an analytic model to evaluate hardware cost.

While an unquantized spiking transformer may involve float-point multiplications, integer multipliers and shifters are used for realizing multiplications in layers with uniform and power-of-two quantization, respectively. We consider computation and memory access overhead, and model the hardware overhead for each layer under two different quantizers as follows targeting the 45nm CMOS technology:

whereis the number of multiplications performed,the storage in terms of number of bits, and,,,the energy cost per shift/add/mult/DRAM operation based on Table. Specifically,andare given by:

whereandare the input feature and output feature dimensions of the linear layer,the input tensor dimension,the kernel size,andthe input and output channel size of the convolution layer respectively,the input image size, andthe bit width of the quantization scheme.

SECTION: Problem Formulation
For a spiking transformer withencoder blocks,layers in each transformer block, andcandidate quantizers each withdifferent quantization precision, we aim to find the optimal quantization scheme for each layer while jointly optimizing model accuracy and hardware overhead. Formally, we formulate this as a bi-level neural architecture search (NAS) problem:

whererepresents the weight parameter of the model,is the architectural parameter and encodes the optimal quantization schemes including quantizer type and bit width for all layers, andcaptures both model accuracy and hardware overhead.is illustrated in Fig..

It is worth noting that the optimal model weight parameteris obtained by solving the bottom-level optimization problem
of (), and is dependent on. This dependency gives rise to the bi-level nature of the NAS problem, and is due to the consideration of impact of quantization while optimizing weight parameterthrough quantization-aware training. While a simpler strategy is to decouple quantization and weight parameter training by adopting post-training quantization, which however can render significant performance drop particularly with low-bit quantization. In contrast, quantization-aware training accounts for and adapts to the error caused by quantization and achieves better performance. Incorporating quantization-aware training in the solution to () requires back-propagating through the quantization effects reflected in ()-(). We approximate the gradient using the straight-through estimator, which approximates the gradient of the rounding operator as 1. During backward propagation, we use full-precision floating-point gradients for all quantities. We employ the straight-through estimator to approximate the derivative for the non-differentiable rounding functions employed in the uniform and power-of-two quantizations.

SECTION: Differential Architecture Search
The architectural (quantization) parameterin our NAS problem is discrete, giving rise to a large discrete solution space ofcomplexity. We adapt an efficient differentiable neural architecture search approachto solve the problem.

For each layerin block, we introduce a continuous-valued selection probability parameterfor each-th quantization scheme candidate in the set of alloptions.
We construct a composite layer presentation by summing up all possible quantized realizations of the layer weighted by the corresponding selection probability parameter:

whererepresents a suitable form of Gumbel softmax,is the weight parameter under quantization scheme,maps the output of layer, i.e. the input to layer, denoted by, to the output of layer’s output.
We then construct a “supernet” for the entire transformer by cascading all composite layers.

We create a continuous-valued relaxation to the NAS problem of (,), optimizing the supernet over the continuous-valued weight parameterand all. Once this relaxed bi-level problem is solved, the optimal quantization scheme for each layeris chosen by the option that has the largest selection probability.

The total hardware loss of the supernet is obtained by summing up all layer-wise hardware costs based on () weighted by the corresponding quantization selection probabilities:

We define the total loss to be minimized based on a combination of cross-entropy model accuracy lossand hardware losswith a user-specifiedtrading off between the two:

To solve the relaxed bi-level problem efficiently, we adopt an iterative two-step approach. In Step 1, we update all, and then in Step 2, we update, both using backpropagation.

The standard Gumbel softmax is given by:

where the temperature hyperparameter,is an i.i.d random variable sampled from the Gumbel(0,1) distribution.adds random noise into the standard softmax.
The smaller the, the closer the Gumbel softmaxis to the typical softmax.
For both updating steps, we initially setto be high to encourage exploration of diverse quantization choices at the beginning, and then gradually reduceto stabilize the optimization towards one specific choice per layer. While the standard Gumbel softmaxis used in Step 2 for the weight parameter, the one-hot version of it is adopted in Step 1 to update the quantization selection probabilities:

Essentially, the use ofin Step 2 allows the weight parameters under all quantization choices to be updated in one step. In Step 1, we turn the selection probabilities into a single one-hot quantization scheme and only update the selection probability weights under this quantization choice. This strategy helps speed up the convergence.

SECTION: Experiments
SECTION: Experimental Setup
We evaluated the proposed heterogeneous quantization on spiking transformer models trained on three widely adopted neuromorphic datasets: DVS128, N-Caltech101, and CIFAR10DVS. We followed the same model architectures and used the full precision models with 32-bit floating point weights fromas reference. DVS128 contains 11 hand gesture categories from 29 individuals under 3 illumination conditions. The Neuromorphic Caltech101(N-Caltech101) dataset is a spiking version of the frame-based Caltech101 dataset. It comprises a diverse collection of images categorized into 101 distinct object classes, ranging from animals and vehicles to household items and everyday objects. CIFAR10-DVS is an event-stream dataset for object classification. It was created by converting 10,000 frame-based images from the CIFAR-10 dataset into 10,000 event streams with sophisticated spatio-temporal structures. We executed architecture search by training a supernet packing different quantized spiking transformers with a 4 CONV-layer spiking tokenizer, two transformer blocks with 256-embedding features and 16-head spiking self-attentions (SSA) over 16 time steps. We used the AdamW optimizer on 510 training epochs with the learning rate warmed up from 1e-4 initially to the maximum learning rate of 1e-3, then reduced with cosine decay down to the minimum learning rate of 1e-5.

Our quantization neural architecture search space encompassed 5 configurations: 32-bit floating point representation, 2-bit power-of-two quantization, 4-bit power-of-two quantization, 2-bit uniform quantization, and 4-bit uniform quantization, applied to every convolution or linear layer.

SECTION: Results on neuromorphic datasets
The effectiveness of our approach is clearly illustrated in Table, whererepresents the weighting hyparameter for hardware cost in the overall optimization objective. A zero-valuedsignifies the full-precision model. Across the range of,results in only a marginal drop in accuracy, typically up to  1% on the DVS 128 and CIFAR10DVS datasets.significantly reduces the energy and storage requirements of the full precision models to as low as 9.73% (10.42) and 6.58% (15.20), respectively. The average number of bits used for weight parameters is reduced from 32 to 2.16bits.

Figureillustrates the architectural evolution in the quantization search for a query linear layer, designed for spiking self-attention, in the second block of each of the three transformers. At the initiation of the search, all five quantization configurations begin with an equal section probability of. As the search progresses, one particular configuration is ultimately chosen. Notably, with an increasing dataset complexity from left to right (DVS 128, CIFAR10DVS, and N-Caltech101), the curves representing the selection probabilities become more intertwined, and the overall loss exhibits greater fluctuations. Each significant shift in the selection probabilities results in substantial changes in the loss—either a distinct increase or decrease—emphasizing the significant impact of the chosen quantization scheme on the overall loss. As the oscillations in the search process diminish, an optimized quantization choice emerges, leading to the attainment of the lowest achievable loss.

In Figure, we break the total hardware overhead into three distinct components of the network architecture: tokenizer layers, spiking attention layers, and MLP layers. In the absence of quantization, full-precision models incur significant energy consumption and storage costs, particularly in the tokenizer and MLP layers. Taking the CIFAR10DVS dataset as an example, 41.4% of the total energy is expended in the tokenizer, 19.5% in self-attention, and 38.9% in the MLP. Similarly, 38% of the total storage is attributed to the tokenizer, 20% to self-attention, and 41% to the MLP.
Following quantization, all layers experience substantial compression, leading to reduced storage costs. The tokenizer layer remains the predominant contributor to energy consumption. With a quantization parameter of, the total energy is only 9.7% of that of the full-precision model, distributed as 48%, 20%, and 30% across the tokenizer, spiking attention, and the MLP, respectively. The total storage reduces to only 6.6% of that of the full-precision model, with storage distribution among the tokenizer, spiking attention, and MLP at 36%, 24%, and 39%, respectively. This analysis reveals that the gradual conversion of images into embedding patches requires substantial resources. Additionally, the energy of the spiking attention and MLP layers can be significantly reduced with a smallvalue, such as 0.5, without compromising task performance. However, achieving a substantial energy reduction in the tokenizer necessitates a highervalue, potentially leading to some performance loss. This underscores the tokenizer layer as the key bottleneck in the energy/performance tradeoff, suggesting that it should be the focus of architectural and design innovations.

Figureshows the distributions of weight parameters within spiking transformers before and after applying the proposed quantization method, and the optimized layer-wise quantization choices. The application of quantization enables a discrete and compact parameter value range, and clearly improves the sparsity by creating more centralized distributions. But, do different layers favor a similar quantization choice?
Within the spiking self-attention layers, computing the query, key, value, and output is based on the same operations with the equal amount of parameters. This symmetry seems to suggest an equal importance of parameter precision for these four types of computations. Intriguingly, we quantize the post-compression energy and storage share for the four parts in the pie charts of Figure, which consistently shows that computing the query and key requires a lower parameter precision, which is the case even though the neural activities associated with the query and key tend to fire at lower rates than those with value and outputs. Additionally, as shown in Figure, the first and the last layer, which are the first convolution layer and the classification head, respectively, consistently necessitate 4-bit uniform quantization, which is the most high-precision and costly choice, signifying the criticality of these layers in the overall task performance.

SECTION: Conclusion
This paper introduces a heterogeneous quantization approach tailored for emerging spiking transformers. The proposedstrategically eliminates the heavy parameter challenge within spiking transformers and explores the optimal quantization scheme on a layer-by-layer basis, optimizing overall energy and storage overhead, and model accuracy through neural architectural search. Our experimental studies underscore the potential of, demonstrating its capability to achieve up to an order of magnitude reduction in energy and storage requirements without significantly compromising model accuracy. Furthermore, our empirical findings highlight specific layers that require high-precision weights, signifying them as the focal points for future architectural and design innovations.

SECTION: References