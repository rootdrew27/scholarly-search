SECTION: Jacobian-Scaled K-means Clustering for Physics-Informed Segmentation of Reacting Flows

This work introduces Jacobian-scaled K-means (JSK-means) clustering, which is a physics-informed clustering strategy centered on the K-means framework. The method allows for the injection of underlying physical knowledge into the clustering procedure through a distance function modification: instead of leveraging conventional Euclidean distance vectors, the JSK-means procedure operates on distance vectors scaled by matrices obtained from dynamical system Jacobians evaluated at the cluster centroids. The goal of this work is to show how the JSK-means algorithm – without modifying the input dataset – produces clusters that capture regions of dynamical similarity, in that the clusters are redistributed towards high-sensitivity regions in phase space and are described by similarity in the source terms of samples instead of the samples themselves. The algorithm is demonstrated on a complex reacting flow simulation dataset (a channel detonation configuration), where the dynamics in the thermochemical composition space are known through the highly nonlinear and stiff Arrhenius-based chemical source terms. Interpretations of cluster partitions in both physical space and composition space reveal how JSK-means shifts clusters produced by standard K-means towards regions of high chemical sensitivity (e.g., towards regions of peak heat release rate near the detonation reaction zone). The findings presented here illustrate the benefits of utilizing Jacobian-scaled distances in clustering techniques, and the JSK-means method in particular displays promising potential for improving former partition-based modeling strategies in reacting flow (and other multi-physics) applications.

[inst1]organization=Department of Aerospace Engineering, University of Michigan,
addressline=1320 Beal Ave.,
city=Ann Arbor,
postcode=48109,
state=MI,
country=USA\affiliation[inst2]organization=Argonne Leadership Computing Facility, Argonne National Laboratory,
addressline=9700 S Cass Ave.,
city=Lemont,
postcode=60439,
state=IL,
country=USA

SECTION: 1Introduction

Accelerating computational tools for simulating complex reacting flows will open new pathways for design and optimization of propulsion and energy conversion systems[1,2]. One approach is the use of data-driven models that can utilize high-fidelity simulations or experimental data to generate a new class of robust and fast computational tools. For instance, data-based models are typically trained on a large collection of high-fidelity flow field snapshots derived from these big data sources. The general goal is to use these models to effectively replace highly expensive physics-based CFD solvers with either surrogates that replace the entire solver, or acceleration modules that replace only the computationally prohibitive components of the solver. For example, in the surrogate modeling strategy, time-resolved data may be used to train prognostic models in low-dimensional latent spaces produced by modal decompositions[3,4]or neural network (NN) based autoencoders[5,6]. In the alternate strategy, computationally prohibitive subroutines can be replaced using methods like learned field transformations[7], flow field super-resolution[8], and NN-based stiff time integrators[9,10]. Ultimately, when proper care is taken during the training stage, these data-based models have the powerful ability to capture highly complex physics contained in the training data, leading to models that can outperform purely physics-based counterparts in terms of faster evaluation times, improved prediction accuracy, or both.

Despite their advantages and demonstrated success, purely data-based approaches have limitations. The principal disadvantage, alongside the potentially time-consuming training stage, is the tendency for these models to overfit to specific geometries, flow regimes/configurations, and boundary conditions. These issues are especially problematic when attempting to produce data-based models of highly nonlinear systems characterized by multi-scale and multi-physics fluid flow behavior in complex geometries. For example, in energy generation and hypersonic propulsion applications, the macroscopic behavior of systems like stationary gas turbines, detonation engines, and scramjets, are described by turbulence-chemistry interactions at both low and high Mach numbers. Data-based predictions for the highly nonlinear and unsteady reacting flow processes that arise at these operating conditions can become unreliable in extrapolative settings, leading to their limited usage in engineering design optimization and downstream modeling workflows[1].

To address these limitations, state-of-the-art model development for multi-physics systems has centered on injecting physical intuition into purely data-based strategies. As a result, the parameters in data-based models become constrained to some degree by the underlying partial differential equations (PDEs), which are the Navier-Stokes equations in the case of fluid flows. This need for physical consistency in data-based frameworks has led to significant research into physics-constrained (or physics-guided) data-driven modeling approaches based on various types of NNs, for which fast, GPU-friendly optimization algorithms have been matured by the data science and machine learning (ML) communities[11]. Modern examples of such strategies include (a) physics-informed neural networks (PINNs)[12], which leverage automatic differentiation to cast the objective function as the governing PDEs, (b) geometric deep learning[13]and graph neural networks[14], which utilize neighborhood aggregation strategies consistent with flow invariants and numerical discretization schemes, and (c) statistically-constrained generative modeling frameworks[15,16,17], which ensure predictions are consistent with macroscopic physical trends.

In the rapidly developing field of physics-informed data-based modeling, much less attention has been paid to unsupervised methods based on data clustering (with some notable exceptions[18]). Embedding physical knowledge into unsupervised clustering objectives can produce valuable acceleration pathways that offer unique advantages for multi-physics model development. To this end, the objective of this work is to develop a physics-informed clustering strategy based on the K-means framework[19], termed Jacobian-scaled K-means (JSK-means) clustering. In this work, alongside presenting the relevant methodology, the new method is applied to produce improved, physically-consistent segmentations of compressible reacting flow fields found in high-speed detonation-based propulsion applications.

To motivate the need for the JSK-means approach, it is important to first highlight the concept of localized modeling, which involves generating partitions of the phase space (or feature space) in which the relevant data samples reside. The resulting partitions are subsequently used as conditioning variables for downstream modeling tasks, leading to a type of classification-guided regression workflow. Ideally, if the discovered partitions (say, generated by a clustering algorithm) are able to isolate different physical regimes of interest, the conditioning process leads to both accurate and interpretable predictive capability that can be leveraged for multi-physics modeling. As a result, these strategies in recent years have been successfully developed for many complex fluid dynamics applications – examples include generating reduced-order-models via clustering over flow field snapshots[20,21], creating multi-regime wall models in turbulent flows via self-organizing maps[22], and constructing closure models for dynamical systems using conditional averages over trajectories[23,24].

In reacting flows, which is the application context of this work, localized models have been almost exclusively used to accelerate detailed chemical kinetics routines (particularly the evaluation of the species source terms), as these are the most time-consuming components in high-fidelity reacting CFD simulations[25,26]. These methods have been shown to produce more accurate predictions of unsteady flow phenomena when compared to global counterparts (e.g. proper orthogonal decomposition)[27,28]. Within this context, examples of localized models include (a) physics-based in-situ adaptive approaches[29,30,31], in which tabulations of the state space are obtained from chemical source term characterizations, and (b) data-based partitioning approaches, in which partitions based on unsupervised algorithms like local principal component analysis[32], random forests[33], and K-means clustering[27], are used to delineate combustion regimes and deploy targeted models for otherwise expensive chemical source term evaluations.

The above methods rely on a notion of similarity between two points in feature space to produce the partitions or clusters. In most cases, this similarity comes from a standard Euclidean distance measure. In reaction-dominated flows, the driving features correspond to species mass fractions and temperature. As such, distance measures in these applications encode deviations in the thermochemical composition space. The goal of this work is to augment the notion of Euclidean distance with knowledge of the underlying governing equations, such that the distance between two points in feature space reflects not only feature similarity, but alsodynamical similarity. This concept is applied here to the K-means clustering framework, resulting in the aforementioned variant termed Jacobian-Scaled K-means (JSK-means) clustering. This work shows, using a target reacting flow dataset sourced from complex channel detonation simulations, how such a modification to the distance function effectively pushes clusters towards regions in thermochemical composition space that reduce within-cluster variation in chemical source term instead of composition, resulting in a novel pathway for physics-informed localized modeling. It is emphasized that, although applied here to a reacting flow dataset, the methodology presented in this work incorporates a general distance function modification that can be used to produce dynamically-consistent partitions of feature spaces in many other physics-based applications.

SECTION: 2Dataset

This section describes the simulation procedure used to generate the clustering dataset. More specifically, details on the simulation configuration, flow solver numerics, and pre-processing steps are provided.

The dataset used in this analysis comes from high-fidelity hydrogen-air detonation simulations in a 2-dimensional channel configuration, as shown in Fig.1. The flow solver is implemented with the finite-volume method in the AMReX library[34], and the governing equations are the compressible reacting Navier-Stokes equations. Although the solver is implemented in the AMReX framework, the simulations conducted for this study do not utilize adaptive mesh refinement (all data were collected from single-level runs). The numerics are globally 2nd-order and are consistent with those used in UMReactingFlow[26]. Chemical kinetics for hydrogen-air combustion is described by the mechanism of Mueller et. al[35], and consists of 9 species () and 21 reactions. Equipped with this mechanism, the open-source library Cantera is used to evaluate chemical source terms, transport coefficients, and other relevant thermodynamic quantities[36].

As shown in Fig.1, the channel ism in the y-direction andm in the x-direction. The boundary conditions consist of slip walls everywhere except for the rightmost boundary, which is an outflow. The unsteady detonation is initiated by prescribing a high-energy driver gas in amm region on the leftmost side of the domain. The temperature, pressure, and x-component of the velocity for the driver gas is set toK,atm, and the Chapman-Jouguet (CJ) speed, respectively (the y-component of the velocity is zero everywhere initially). The species mass fractions for the driver gas are obtained from the CJ condition evaluated from the idealized Zeldovich-von Neumann-Döring (ZND) detonation solution (the Caltech Shock and Detonation toolbox was used to this end[37]). Similar to the approach used in Ref.[38], roughlymm ahead of the driver gas, an array of cube-like perturbations of high-energy stoichiometric hydrogen-air mixture is placed roughlymm ahead of the driver gas region. These perturbations are required to initiate transverse wave reflections that produce the characteristic triple point structures observed in unsteady detonations in 2 and 3 dimensions. The composition for the ambient gas is stoichiometric H2/Air atatm andK.

The domain is discretized with 4096 cells in the x-direction and 2048 cells in the y-direction, producing a total cell count of roughly 8 million and a grid resolution ofmicron. At an ambient pressure ofatm, this grid resolution results in roughlycells within the ZND induction zone and therefore exceeds required spatial resolution limit for resolved unsteady H2/Air detonations[39]. Starting from the conditions specified in Fig.1, the simulation was run for a total time ofseconds at a Courant-Friedrichs-Lewy (CFL) number of 0.6, which was enough time to ensure progression of the developed CJ detonation through the entirety of the channel.

A subset of the full channel domain ats that contains the entirety of the detonation wave structure is extracted in a cropping procedure. The cropped subdomain (bounded byandmeters) produces a set ofthermochemical state vector samples that describe all relevant details of the detonation wave structure (e.g. triple point oscillations, transverse waves, deflagration regions, ambient regions, etc.) and ignores the regions beyond the sonic choke point that are irrelevant to the wave dynamics.

The individual samples are denoted, where. Thereside in the 10-dimensional composition space defined by temperature and species mass concentrations (i.e.). More specifically,, whereis the temperature for sample,is the-th species concentration, and T denotes the transpose operation (not to be confused with temperature). Through this composition vector, the local fluid density can be obtained from a summation over the species mass concentrations, and the pressure can be extracted from the ideal gas law. Using these composition space samples, two datasets are created: the first is the composition data matrixand the second is the ground-truth chemical source term data matrix, whereis referred to as the chemical source term vector for sample. As per detailed finite-rate chemical kinetics standards, the species concentration source terms are evaluated using linear combinations of elementary Arrhenius-based reactions[40]. The temperature source term is evaluated under the assumption of a constant-volume reactor as

wheredenotes the mass-based specific heat at constant volume for the mixture,is the molar internal energy for species(derived from NASA polynomials),is the species molecular weight, andis the source term for species concentration. Ultimately, the composition data inis used in the clustering procedure in Sec.3, and the data inis used to further assess clustering outputs.

SECTION: 3Methodology

SECTION: 3.1Standard K-means Clustering

Before describing the Jacobian-Scaled K-means clustering strategy in Sec.3.2, a brief overview of the standard K-means algorithm is first presented. The reader is directed to Ref.[19]for a more detailed description of the baseline K-means algorithm.

Given the thermochemical state data in, the standard K-means procedure minimizes the objective function

based on the initialization and convergence of a finite set ofcentroids. The quantityis also equivalent to the number of clusters. The number of clustersis one of two major inputs to the K-means algorithm, the other being the initial locations of the centroids in the-dimensional composition space. Equation2is interpreted as the total within-cluster variation of composition samples, and depends entirely on the centroid values.

As the name implies, the centroidis defined as the arithmetic mean of all samples within the-th cluster as

In both Eqs.2and3, the quantitydenotes an integer value extracted from the-th row and-th column of the cluster assignment matrix. This value in the cluster assignment matrix is unity if the respective sample is closest toin the Euclidean sense and zero otherwise. This is formally expressed as

The standard K-means algorithm (the well-known Lloyd’s algorithm) consists of alternating between centroid updates via Eq.3and cluster label updates via Eq.4until the centroid positions inchange by a negligibly small amount. This procedure is described in Algorithm1.

Centroid convergence rates are highly dependent on initial centroid locations. The naive approach is to simply initialize centroids randomly – this is ill-advised, because random initialization not only scales poorly with increase in sample dimensionality, but also faces the risk of initializing two or more centroids close to one another in the phase space, which increases the chances of producing redundant clusters and sub-optimal values for the objective function. The K-means++ is an alternative initialization procedure that addresses these issues[41]. Instead of picking the initial locations arbitrarily, K-means++ initializes centroids with the goal of achieving a large amount of separation between the centroids. However, the K-means++ initialization is stochastic, whereas the K-means algorithm itself is deterministic for a fixed initial set of centroids. Therefore, convergence of Alg.1using multiple realizations of starting centroid locations is necessary to ensure statistical confidence in the K-means output[4].

SECTION: 3.2Jacobian-Scaled K-means Clustering

Instead of minimizing the standard K-means objective described Eq.2, the goal of the Jacobian-Scaled K-means (JSK-means) clustering strategy is to minimize

There are two main differences between Eq.5and the standard K-means objective in Eq.2. The first difference is the presence of the weighting matrix, which acts as a linear transformation, or scaling, on the sample-to-centroid distance vector. As denoted by the subscript, this matrix is cluster-dependent through dependency on the centroid (i.e.). As such, the objective in Eq.5comes from a modification to the standard Euclidean distance function used to define sample similarity in the phase space. More specifically, if the Euclidean sample-centroid distance function is given as

the modified distance function implied by Eq.5is

The second difference from the standard K-means approach is a modified assignment matrix,. The cluster assignment mechanism in JSK-means is based on the modified distance measure in Eq.7as

Note that although the above cluster assignment matrixprovides the same functionality as the standard K-means counterpart in Eq.4(i.e., it identifies the samples belonging to a particular cluster), the distance function used to prescribes the sample-centroid assignment has changed. The centroidin the JSK-means approach is defined in the same way as in the standard approach (Eq.3), but instead uses this modified cluster assignment matrix:

The embedding of physical knowledge comes from the definition of the cluster-dependent weighting matrix. It is assumed that there exists a set of governing equations that describes the evolution of the phase space vectoras

where the quantityis a velocity in. As described in Sec.2,here is the thermochemical state vector. As such, the phase space velocity is the chemical source term, and the ordinary differential equation in Eq.10describes the evolution of species mass fraction and temperature in composition space.

The method for obtainingis now described. The leading-order Taylor expansion for the chemical source term via Eq.10is

whereis a reference point in composition space,is a query point, andis the chemical Jacobian matrix. By casting the reference pointas the centroid, the the expression for the scaling matrixthat acts on the sample-to-centroid distance vector emerges as the chemical Jacobian evaluated at the centroid:

By means of the Taylor expansion in Eq.11and the definition of the scaling matrix in Eq.9, the distance function used in the JSK-means approach (Eq.7) approximates the source term distance. More formally,

Ultimately, by representing the scaling matrix as the chemical Jacobian, the objective function for JSK-means in Eq.5becomes a leading-order approximation to the variation of chemical source term using only the distance vector between two points in composition space as input (the primary input is are unchanged from standard K-means). If such an objective function can be minimized, the resulting clusters would identify regions ofdynamical similaritythrough the assignment matrix, where dynamical similarity is defined as the degree to which two points in a local neighborhood can be described the same source term. This becomes clearer when observing that setting the scaling matrix as the identity (e.g.for all, which amounts to stationary dynamics) restores the standard K-means algorithm.

Before proceeding, it should be noted that the general idea of modifying the standard K-means algorithm with weighting functions is not new. However, the JSK-means method introduced here differs from previous variants through reliance on a centroid-dependent scaling matrix and extraction of a scaling matrix using underlying physics-based dynamics. The reader is directed toAfor more detail on this matter.

SECTION: 3.3Jacobian-Scaled K-means Algorithm

The JSK-means algorithm to minimize the modified objective in Eq.5is provided in Algorithm2. This is very similar to the standard K-means procedure in Algorithm1. The major modification is in step (b), which consists of updating the scaling matrices via computing the chemical Jacobians at the current centroid locations as per Eq.12. These Jacobian matrices are required in step (c), because the labeling mechanism in JSK-means comes from evaluating norms of the sample-centroid distance vectors scaled by the Jacobian matrix(see Eq.8). Although centroid initialization can be accomplished in the same way as with standard K-means (e.g. via the K-means++ algorithm[41]), in this work, a burn-in procedure that takes the converged centroids of the standard K-means algorithm is used for initialization of the JSK-means centroids. Through this approach, the converged centroids produced by the JSK-means algorithm can be interpreted as a modifier to the standard K-means centroids, which allows one to pinpoint the effect of the influence of Jacobian scaling in an interpretable way (as discussed in Sec.4).

Note that in Alg.2, the definition of the centroid during the iterative convergence procedure has not changed from the standard K-means approach, but the assignment mechanism has changed by means of a different distance function. This produces an inconsistency, because the way in which the centroid update rule is derived is intrinsically tied to the distance function used in the definition of the K-means objective. Since the distance function has changed in the Jacobian-scaled approach, and the definition of the centroid has not changed, this inconsistency translates to eliminating the convergence guarantees of the standard K-means algorithm.

More specifically, in the standard K-means approach, the goal is to derive a centroid update rule based on the cluster labels generated from the previous set of centroids that results in a reduction to the objective function in Eq.2. It can be shown, through either gradient descent or expectation maximization (EM) techniques, that such an update rule recovers Eq.3[42,43].

In JSK-means, however, the objective function is different from standard K-means. A derivation of the centroid update rule that minimizes Eq.5instead of Eq.2therefore yields a slightly different expression. If the updated centroid is denoted, this update rule is

In the above equation, the true centroid update for minimizing the JSK-means objective in Eq.5departs from the one used in Alg.2by a residual. Outlined inB, this residual is proportional to both the within cluster variance and the rate-of-change of the matrixin the-dimensional phase space (e.g., the chemical Hessian matrix). In this work, the residual in Eq.14is ignored, allowing the JSK-means algorithm to retain the original centroid definition at the cost of guaranteeing a monotonic decrease of the objective function. As demonstrated in Sec.4, good convergence trends in the JSK-means objective are still observed in practice via Alg.2. Strategies for replacing the standard centroid update with the true update defined by Eq.14will be explored in future work.

SECTION: 3.4Normalization Procedure for Jacobian Regularization

A normalization procedure is carried out before clustering to nondimensionalize the components ofand the Jacobian matrices. This ensures that composition variables have equal contribution in both standard and Jacobian-scaled K-means algorithms. The normalized composition sample, denoted, is recovered from the rawvia

Superscripts in Eq.15denote component indices (not powers). The diagonal matrixnormalizes each respective component ofby its range. An analogous scaling matrix, denoted, can be obtained to range-normalize chemical source term values.

Given the diagonal scaling matrices for composition values and source terms (andrespectively), the scaled Jacobian matrix, denoted,  can be recovered as

wheredenotes an elementwise matrix product (Hadamard product) and theoperation produces a column vector containing the diagonals of the input matrix. The procedure in Eq.16ensures that the individual components in the Jacobian are scaled in a manner consistent with problem dimensionality without needing to find the ranges of individual Jacobian matrix elements.

It should be noted, however, that obtaining the Jacobian scaling matrixdoes require estimates for the ranges (minimum and maximimum values) of both composition and chemical source terms, which in practice are extracted from an input dataset. The disadvantage here is that these scaling matrices will be application or problem dependent (this is true for any method that leverages a scaling approach in a preprocessing stage). Expert guided knowledge can be used to obtain ranges for composition values (e.g. temperatures in detonations typically fall within the range of 300K to 6000K), but it is important to acknowledge that the ranges for species and source term values cannot be prescribed this way.

The effect of the normalization procedure is evidenced in Fig.3, which compares the singular value distributions of the scaled and unscaled chemical Jacobians. The figure shows how the scaling procedure drops the disparity between the largest and smallest (as well as the first and second) singular values in the data distribution to be used in the clustering procedure. The practical implication of this effect is a well-conditioned Jacobian matrix, leading to improved convergence of the JSK-means algorithm via Alg.2.

SECTION: 4Results

SECTION: 4.1Demonstrative Toy Problem

Before proceeding with the detonation dataset, a demonstration of the JSK-means algorithm on a toy problem with a 1D feature space is first provided.  The intent is to describe how JSK-means, via the Jacobian-scaled distances used in Alg.2, modifies the centroid locations and clusters produced by standard K-means on a simpler problem. References to the general trends uncovered here are then made in Sec.4.2, which performs extended analysis on the more complex reacting flow dataset.

In this toy problem, the underlying nonlinear dynamical system is represented as a quadratic function via

which produces the trivial Jacobian. A plot of the quadratic source term and linear Jacobian from Eq.17is shown in Fig.4(left).

The input dataset is generated by sampling the phase space variablewithin the rangeatevenly spaced intervals. Figure4(right) plots the history of both standard and Jacobian-scaled K-means objective functions (Eqs.2and5, respectively) using the burn-in procedure mentioned in Sec.3.3, which consists of two stages. In the first stage (burn-in), the standard K-means algorithm is run for a prescribed number of iterations (300 here) using centroids initialized from the K-means++ procedure[41]; in the second stage, the JSK-means algorithm is run for the same number of iterations (or until the convergence criterion is met) using the converged centroids from standard K-means as the initial condition. In each of these phases, the objective function for both approaches is computed at each iteration. To facilitate comparison, the curves in Fig.4were then produced by normalizing each objective function by its observed maximum value during the entire iterative process.

In the initial stage of the algorithm, which involves running the standard K-means algorithm for the first 300 iterations as depicted in Fig.4(right), a monotonic decrease in the standard K-means objective function is observed. This function (see Eq.2) measures the within-cluster sum of squares of, and its steady decrease during the first stage is expected. Interestingly, the JSK-means objective function also decreases in a similar monotonic behavior in this stage, despite the fact that the standard K-means objective is being minimized.

Iteration 300 indicates the onset of the JSK-means algorithm. At this point, the converged centroids from standard K-means are supplied as the initial centroids to JSK-means. After the switch, a noticeable drop in the JSK-means objective function is observed, while the standard K-means objective function rises. This trend is in-line with the goal of the JSK-means algorithm, as the JSK-means objective function approximates the within-cluster variation inas opposed to. This effect is indicative of nonlinearity in the source term, and captures a key feature of the JSK-means method: optimizing the cluster partitions for source term variation necessitates a tradeoff in the optimization of the clusters for variations in the state variable. Additionally, as evidenced in Fig.4and referenced in Sec.3.3, the JSK-means algorithm does not guarantee monotonic decrease of the corresponding objective function (convergence occurs when the centroids are stabilized in the-dimensional phase space).

Ultimately, Fig.4shows how JSK-means drops the corresponding Jacobian-based objective function in Eq.5as intended, producing a set of converged centroids that reduce variation in nonlinear source terms within the clusters. To better interpret this effect, a visualization of the change in centroid locations provided by the JSK-means algorithm with reference to the converged centroids from the standard K-means algorithm is provided in Fig.5for,, andclusters.  In all cases, Fig.5shows how the JSK-means algorithm shifts and redistributes the standard K-means clusters towards high-sensitivity regions in the phase space. This effect can be quantitatively accessed by correlating the cluster sizes with centroid Jacobian norms, as shown in Fig.6– the weighting towards high-sensitivity regions implies that clusters are redistributed in regions where there is significant nonlinearity. Put another way, for a finite set ofcentroids, the physics-guided clustering algorithm through Jacobian-scaling ensures that thecentroids are localized in regions of dynamical similarity as prescribed by the governing equations via Eq.17.

SECTION: 4.2Detonation Flowfield

The JSK-means clustering algorithm is now extended to the detonation dataset described in Sec.2. More specifically, in a similar manner as in the toy problem above, convergence trends for variousvalues will be analyzed and changes in cluster assignments in both physical space and composition space due to Jacobian-scaling will be interpreted in Sec.4.2.1and4.2.2, respectively.

Before visualization and interpretation of the partitioned composition space, it is important to ensure that the algorithm presented in Alg.2produces the a similar decrease in the JSK-means objective in the detonation dataset as observed in the 1D toy problem in Sec.4.1. To this end, convergence trends for both standard (Eq.2) and Jacobian-scaled (Eq.5) objective functions are provided in Fig.7for,and(objective function curves are normalized in the figure to facilitate comparison across different cluster numbers).  For each cluster number, the curves represent an average from 10 different sets of initial centroids provided to the standard K-means algorithm to initiate the burn-in procedure, resulting in unique K-means runs. Overall, the convergence trends in Fig.7show how Alg.2minimizes the physics-based Jacobian-scaled objective in Eq.5even for the complex detonation dataset. As with the 1D problem, there is no noticeable oscillatory behavior in the objective curves, indicating that the centroid locations have stabilized.

Relative to the initial state, Fig.7shows how the drop-off in the JSK-means objective increases noticeably fromto; however, there is no noticeable modification of the degree of drop-off in the normalized objectives when jumping fromto. The implication is that the reduction provided in the relative JSK-means objective stabilizes, but the reduction in the absolute values of the objective decreases, as described below. Interestingly, the upward jump in at iteration 300 in the normalized standard K-means objectives in Fig.8(black curves) increases with. This implies that at higher, JSK-means more heavily modifies the standard K-means clusters – relative to the initial condition – to achieve the reduction to its objective function.

Figure8shows the effect of cluster number on the objective function in Eq.5computed from both converged standard K-means clusters and JSK-means clusters. For consistent comparison across cluster numbers, a normalized version of the Jacobian-scaled objective, termed the root-mean-square (RMS) objective, is plotted instead of the value in Eq.5. This RMS objective is given by

Recall that the initial set of centroids provided as input to the JSK-means algorithm comes from the converged standard K-means centroids. As such, to produce the curves in Fig.8, the above RMS objective is computed using centroid and distance evaluations from both standard and Jacobian-scaled approaches. The trends in Fig.8show how the modification of centroid locations provided by the JSK-means approach results in decreased within-cluster variations of source term. An increasing cluster number in both standard and JSK-means approaches results in a nearly constant-rate reduction in the Jacobian-based RMS objective, which is indicative of the localization of clusters. Interestingly, despite the fact that the standard K-means approach does not optimize Eq.18, higher cluster numbers still result in lower values of Eq.18, implying that source terms are localized in composition space for this dataset. Usefully, even at higher cluster numbers (e.g.), the gap in RMS objective between the JSK-means and standard K-means evaluations remains, which signifies that the JSK-means algorithm succeeds in shifting the baseline clusters produced by standard K-means towards regions of greater dynamical similarity.

Although informative, analysis of trends in the objective function values must be supplemented with a visual inspection of the cluster partitions in physical space for a clearer interpretation of the impact of the JSK-means clustering procedure. To this end, cluster assignments obtained from standard K-means and JSK-means are shown in Fig.9for thecase – general trends discussed hereafter apply to all studiedvalues. Note that the actual colors of the cluster labels in physical space come from the cluster index, and are not meaningful from a physical perspective – however, coherent regions in physical space defined by cells of the same color (or cluster) are indicators for key flow features extracted by the unsupervised algorithms. The cluster label plots in physical space are termed the standard (obtained from standard K-means) and Jacobian-scaled (obtained from JSK-means) segmented fields respectively.

The segmented fields in Fig.9reveal the way in which the JSK-means algorithm biases, or pushes, the cluster labels towards the wavefront and detonation reaction zone (indicated by the white arrows in the respective plot). This is consistent with the fact that the difference between the standard and Jacobian-scaled approach is in the chemical Jacobian based scaling of the distance function, which necessitates the localization (or redistribution) of centroids towards regions of high composition sensitivity. This phenomenon is directly analogous to the biasing and weighting of the centroids seen in the toy problem in Sec.4.1.  As a result, the redistribution of centroids in composition space provided by JSK-means translates to localizing the cluster partitions near regions of high chemical heat release, which can be a useful property for developing more accurate models that intend to target such complex regions in the simulation procedure (see Sec.5). On the other hand, the standard K-means approach produces a segmented field that is significantly more refined and complex in turbulence-dominated regions far behind the wavefront that are unimportant from the chemical contribution perspective.

For a better visualization of the algorithm behavior near the wavefront, Fig.10shows a series of zoomed-in profiles of pressure, density, heat release rate, and the same segmented fields from Fig.9. Before assessing the near-wavefront trends in the segmented fields, it should be noted that the pressure profile shown in Fig.10displays the characteristic triple-point structures that oscillate vertically along the wavefront as it propagates through the channel. In short, triple points are localized high-pressure regions at the detonation wave front emanating from collisions between a weaker series of reflecting transverse waves and the leading shock wave which travels at the CJ speed[44]. The triple point structures, indicated by the small distributed regions of peak pressure throughout the wavefront, contribute to significant complexity in the detonation wave dynamics and are known to heavily influence chemical kinetic behavior within the detonation wave structure[45]. As such, part of assessing the strength of any composition space partitioning algorithm for detonation-containing flows is to ensure that areas near and within the triple point structures are detected by the given algorithm with ideally minimal user input.

The differences between the segmented fields in this triple point region are quite apparent in the zoom-ins – for example, in the standard K-means segmented field, the cluster partitions attempt to recover spatially coherent patterns that align more with density fields and contact discontinuities away from the wavefront,  which contribute to the complex and less spatially-uniform cluster partitions observed downstream. This comes from the definition of the composition vector, which consists of temperature as well as density-weighted mass fractions (i.e. species concentration); as such, there is an intrinsically higher weight placed on non-reacting transverse wave propagation in the standard clustering procedure, which in turn leads to complex 2-dimensional structures recovered in the segmented field.

On the other hand, the segmented fields produced by the JSK-means approach are markedly more 1D in overall structure (e.g. the variation in cluster transitions along the y-direction is noticeably smaller than that observed in the standard K-means segmented fields). The effect of biasing clusters towards the wave front, which was visualized macroscopically in Fig.9, is also apparent in the zoom-ins. The JSK-means algorithm pushes clusters towards the regions of high chemical sensitivity, as encoded in corresponding regions of peak heat release rate and the triple points. Because the contact discontinuities and transverse wave oscillations behind the detonation wavefront produce much less chemical reactivity as implied by the heat release rates, the cluster partitions from the Jacobian-scaled approach are more vertically uniform behind the wavefront.  Additionally, there is a higher degree of noise in the Jacobian-scaled segmented field than in the standard counterpart (i.e., there is a high-frequency scatter effect slightly away from the wavefront, indicated by the red regions in in Fig.10(d)) – this may be due to potentially high variations in chemical stiffness, as well as possible numerical errors attributed to numerical chemical Jacobian estimates for intermediary species.

The above discussion is geared towards interpretation of the output of the JSK-means clustering procedure in relation to the standard K-means approach in physical space. However, additional insight into the workings of JSK-means can be extracted by visualizing the cluster distributions in composition space. Because the composition space is high-dimensional (here), direct visual analysis of cluster partitions is not straightforward. For interpretable composition space analysis, projections onto two-dimensional spaces facilitated by proper orthogonal decomposition (POD) will be utilized. It is assumed that the reader is already familiar with the basics of POD (and equivalently, principal component analysis) – fundamentals and background on the approach are provided in Refs.[46,47]and the references therein.

To better visualize the effects of JSK-means, two sets of POD bases are derived: one from the dataset containing the composition variables, and another from the dataset containing the source terms.  It should be noted that before carrying out the POD procedure, each of these datasets here were range-scaled as per the procedure described in Sec.3.4.

The POD basis derived fromis denoted, and the POD basis derived fromis denoted. The columns of the matricesandcontain the respective basis vectors (also known as principal component directions), anddenotes the number of retained modes in the expansion. Projection of the datasets onto their respective POD basis vectors produces the POD coefficients, the components of which are uncorrelated due to the orthonormal property of the basis. More formally, the phase space coefficients are obtained from the projection

and the source term coefficients from the analogous projection

In the above equations, theandare-dimensional column vectors for the-th phase space and source term coefficient, respectively. For two samplesand, if all basis vectors are retained in the POD representation, the distance between two pointsandin composition space can be cast as

and similarly, the distance between the same two points in a so-called ”source-term” space can be cast as

The above property comes from the fact that the data variance (and Euclidean distance) is preserved in the POD expansion if all modes are retained[47]. Mode ”energies” extracted from the eigenvalues of respective covariance matrices can then be used to quantify the percentage contribution of each POD mode to the overall data variance. This is shown in Fig.11; usefully, for both composition and source term data projections, the first two POD modes retain most of the data variance. As such, 2-dimensional visualizations utilizing only the first two POD coefficients from both the composition dataset (Eq.19) and the source term dataset (Eq.20) can be reliably used to directly assess the effects of the JSK-means clustering procedure in terms of centroid distributions and cluster labels.

Figure12shows examples of these visualizations in both the composition space coefficients (, Eq.19) and source term coefficients (, Eq.20). There is significant correlation between the POD coefficients and key quantities of interest, such as temperature fields, H2O mass fraction (which can be interpreted as a reaction progress variable), as well as chemical source terms; these correlations confirm that the respective POD projections facilitate a concise representation of data variance in both composition and source term space in two dimensions. Note that in the source term space (bottom row of Fig.12), the regions in composition space (top row) that produce zero chemical source term collapse to a single point (thecoordinate). Usefully, the correlation in temperature source term becomes much more linear in the source term space as opposed to the composition space – in other words, an increase in values in the first principle component of the source term POD coefficient produces, to reasonable confidence, an increase in chemical reactivity.

Figure13(a) compares the centroid locations and corresponding cluster labels produced by the standard K-means with those produced by JSK-means in the two-dimensional composition POD space. Figure13(b) displays the same quantities in the source term POD space. The plots illustrate how JSK-means refines clusters in fundamentally different regions of phase space than in the standard approach. More specifically, as indicated in Fig.13(a), the JSK-means algorithm successfully redistributes centroids away from the ambient, non-reacting regions (black circles) and towards the highly reactive regions (red circles) that characterize much of the complexity in the detonation wave structure (i.e. regions of peak heat release rates).  With regards to cluster refinement, the physics-guided approach of JSK-means refines clusters in these same reactive regions, which is equivalent to allocating additional clusters near the detonation wavefront and triple point structures where high chemical heat release and species sensitivity is expected to occur.

In the dual sense, increasing the number of clusters in the Jacobian-scaled approach does not result in a refinement of the ambient, non-reacting region of the flowfield, which is not true for standard K-means. This effect of centroid redistribution is particularly apparent in the source term POD space projections of Fig.13(b), which shows how JSK-means increases the amount of centroid spread in source term space; this in turn drops the within-cluster variation of source term distances within each cluster. The trends in Fig.13both support the segmented field behavior of the JSK-means clustering approach in physical space (Figs.9and10), and also allude to the fact that the modified algorithm in Alg.2is indeed dropping the modified objective in Eq.5.

SECTION: 5Applications and Need for JSK-Means

The discussion above showcased the impact of JSK-means in the clustering procedure by means of biasing (or altering) centroid positions in feature space to reflect dynamically sensitive regions. In the context of chemically reacting flows, with knowledge of the chemical source term, application of JSK-means to composition space clustering is shown in the above sections to effectively steer centroids towards regions of high source term sensitivities. The goal of this section is to (a) provide a brief overview of some practical use-cases of this approach (Sec.5.1), and (b) provide justification as to why JSK-means is useful from the modeling perspective, instead of using standard K-means clustering on the source terms directly (Sec.5.2).

SECTION: 5.1JSK-means Use Cases

The JSK-means approach has two primary use-cases (although there can be more): (1) facilitating physical analysis of complex flow fields (e.g., turbulent reacting flows), and (2) partition-based (or tabulation-based) surrogate modeling. By construction, these use-cases are inherited directly from those of standard K-means and other partition-based clustering approaches. The key advantage here is that the same partition-based modeling and analysis approaches can be constructed using JSK-means clusters as conditioning variables, which necessarily take into account the composition space dynamics. Both aspects are discussed in the following.

Cluster-based physical analysis:K-means, when interpreted as a data decomposition tool, has been used in a wide variety of contexts to drive physical analysis of fluid flows (the reader is directed to Refs.[20,24]and the references therein for different takes on this). Although the scope of this work is primarily concerned with introducing JSK-means as a way to inject a physics-based inductive bias into standard K-means, the utility of JSK-means from the physical analysis perspective, through construction of segmented flowfields, should not be overlooked. In other words, the act of assigning a sample (which here represents a composition vector as well as a physical space location) to a cluster ID, and subsequently visualizing the resulting cluster IDs in physical space through the segmented fields, is itself a useful application of the approach, especially when comparing with standard K-means outputs. Such a comparison offers a highly interpretable avenue for breaking down complex reacting flowfields in a physically meaningful way; unlike standard K-means, the JSK-means segmented fields reveal dynamically similar regions from the perspective of the chemical source term, and allow the user to directly connect these regions to coherent structures in physical space. In unsteady turbulent reacting flow contexts, such visualizations (i.e., the time evolution of the segmented flowfield) can play a meaningful role in investigating flow physics from high-fidelity datasets, particularly from the angle of flow-chemistry interactions.

To demonstrate this point, such analysis of the JSK-means clusters can give insight into regions described by fast/prohibitive chemical timescales. For example, Fig.14shows average chemical timescales – both maximum and minimum – in each cluster produced by standard K-means and JSK-means at K=15 and 30 for the channel detonation configuration. Interestingly, through inspection of these average timescales, it is evident that the Jacobian-based weighting of JSK-means pushes clusters towards smaller chemical timescales (i.e., many of the K-means clusters with minimum chemical timescales above, with the exception of outliers representing the ambient region, are eliminated). The inset in Fig.14shows that, for the smallest values of minimum timescales, a much larger range of maximum timescales is also captured by JSK-means (clusters not only tend towards smaller timescales, but encompass a larger range of stiffness). This shift in the timescale distribution of JSK-means clusters coincides with the segmented field concentration around the wavefront and triple points in the detonation structure described in Sec.4.2, which in turn can facilitate further physical analysis and feature extraction. To further emphasize this point and as an additional demonstration of the utility of JSK-means, the reader is directed to Ref.[48], where JSK-means is used to analyze high-speed combustor flowfields in a full-scale configuration. Here, despite a fundamentally different configuration, chemical timescale effects are consistent with those observed in Fig.14.

Surrogate modeling:A primary motivation for the JSK-means approach is to move towards improved partition-based models that serve as accelerated surrogates for unknown or known but high-cost function evaluations. An example of such a function is the chemical source term. Partition-based modeling approaches condition the model evaluation on a localized representation of the state space, which amounts to the cluster ID of the query sample in cluster-based modeling workflows (examples of works that leverage partition-based modeling are provided in the Introduction). Since such workflows are typically tasked with modeling dynamics, investigation of model evaluations conditioned on JSK-means partitions, which are informed of the source terms themselves, is warranted. For demonstrative purposes, a simple JSK-means based modeling strategy for the chemical source term is provided and compared to a standard K-means counterpart below.

Given a general non-linear function, it is assumed that the objective of the modeling is to develop an (ideally accelerated) surrogate model for, denoted, where. Acluster-basedsurrogate then requires conditioning the modelon the cluster ID of a given input sample– i.e.,, whereis the cluster (coming from either K-means or JSK-means) in which the inputresides. To demonstrate the advantages of JSK-means, the demonstration here focuses on the combustion modeling task, whereis equal to the instantaneous chemical source term, such that, andis a model for the source term. However, it should be emphasized thatcan represent other functions, such as the action of a stiff time integrator[29]– in this case,represents the composition at a future time instead of the instantaneous rates.

As a simple demonstration, a cluster-conditioned model is cast as the Taylor expansion about the-th centroid (see Eq. 11) via

whereis the Jacobian evaluated at the corresponding centroid,is the source term evaluated at the centroid, andis the sample-to-centroid distance. This resembles a tabulation-based approach, sinceandare stored at the centroid locations and thereby provide the necessary cluster conditioning. For the detonation dataset, source term predictions using both standard K-means and JSK-means clusters are provided in Fig.15for a subset of the source term variables usingandpartitions. Due to the simplicity of this linear model, it must be acknowledged that overall prediction accuracy using both clustering strategies  leaves room for improvement. However, the figure shows how JSK-means predictions provide noticeable improvement over standard K-means. Interestingly, this performance improvement is larger at lower cluster numbers; prediction accuracy gains are especially apparent at higher values of the temperature source term and throughout all minor species source term ranges.

These demonstrations ultimately point to the utility of JSK-means from the modeling standpoint: a clustering strategy informed of underlying dynamics naturally has potential in improving on partition-based models in which the partitions are not produced with knowledge of the dynamics (i.e., standard K-means and other similar clustering approaches). It is emphasized, however, that theformulation above is a simple linear model, and further investigation of JSK-means models using more expressive formulations – such as neural networks[27], for example – would complete the picture.

SECTION: 5.2Distinction from Source Term Clustering

The methodology of JSK-means begs the following question:What is the advantage of modifying the objective function using the Jacobian over executing standard K-means on the chemical source terms directly?

Indeed, executing a clustering over the source terms directly ensures that resulting clusters are dynamically similar by design – usage of such clustering in a physical analysis setting is an interesting direction. However, the JSK-means approach offers advantages over straight source term clustering, particularly when considering downstream modeling/integrated inference settings. These are described below.

JSK-means strikes a balance between localization of clusters in composition space (standard K-means on) and biasing towards regions of high reaction sensitivity (standard K-means on). This is a by-product of the linear Taylor expansion: the Jacobian-scaled distancesrecover the source term distanceas the samplebecomes infinitesimally close to the centroid.  The main takeaway and primary advantage in JSK-means is that the algorithm operates on thesameinput data as standard K-means and appends knowledge of the state space dynamics to ultimately shift (or bias) the cluster distributions towards dynamics-oriented regions. As a result, JSK-means centroids still represent localized averages of the input samples, as in standard K-means, but the clusters themselves are biased as per the Jacobian scaling – as such, clusters are still localized in composition space.  This is not true when running standard K-means on the source terms, which is fundamentally different – this produces centroids that represent conditionally averaged source terms.

Considering the above implications, since JSK-means centroids represent conditional averages in composition space, JSK-means allows for the creation of tabulation-based localized combustion models (described in Sec.5.1). In other words, a potential use-case of the algorithm is a scenario in which all of the source term data for(which again can represent either instantaneous source term evaluations or chemical time integration) is unavailable and needs to be modeled. In such a setting, clustering overdirectly is not an option, as this is the modeling target.

SECTION: 6Conclusion

In this work, a physics-informed variant of K-means clustering, termed Jacobian-scaled K-means (JSK-means) clustering, was introduced. The method allows for the injection of underlying physical knowledge into the clustering procedure through a distance function modification: instead of leveraging conventional Euclidean distance vectors, the JSK-means procedure operates on distance vectors scaled by Jacobian matrices. These Jacobian matrices are cluster-dependent and are derived from dynamical system Jacobians (right-hand-side sensitivities) in the phase space of samples. Jacobian-scaled distances enter the clustering procedure through modified definitions of the clustering objective and cluster assignment functions. To arrive at the JSK-means algorithm, updates to the baseline K-means approach based on these modifications are made, including (a) computing Jacobians at the current centroid locations during the iterative procedure, and then (b) using these updated Jacobians to scale respective sample-to-centroid distance vectors. The general goal of this work is to show how, with this simple modification, the JSK-means algorithm – without modifying the input dataset – produces clusters that capture regions of dynamical similarity, in that the clusters are redistributed towards high-sensitivity regions in phase space, and are described by similarity in the source terms of samples instead of the samples themselves.

The JSK-means algorithm was demonstrated in this work on a complex reacting flow simulation dataset. In this application, each sample to be clustered is described by the local thermochemical composition (species concentrations and temperature) extracted from a computational cell in a flowfield containing a self-sustained hydrogen-air detonation wave. The dynamics in this composition space are known through the highly nonlinear and stiff Arrhenius-based chemical source terms. As such, the distance scaling required by JSK-means was accomplished using cluster-dependent chemical Jacobian matrices evaluated at the cluster centroids.

With this dataset, detailed comparisons between outputs from JSK-means and standard K-means were made to highlight the practical effects of including Jacobian-scaled distances in the clustering procedure. To this end, convergence of the JSK-means algorithm based on an initial condition prescribed by the output of a standard K-means procedure (the burn-in approach) resulted in a redistribution, or biasing, of centroids towards regions of high chemical sensitivity. More specifically, it was shown how the modified algorithm successfully drops the corresponding JSK-means objective function during the iterative procedure, at the cost of increasing the standard K-means objective function as a tradeoff. Visualization of this effect in physical space was performed via the analysis of segmented fields –  the flowfield delineations produced by the JSK-means algorithm pushed standard K-means clusters towards regions of high peak heat release rate near the detonation reaction zone. Further visualization of this effect in composition space via POD projections showed how the Jacobian scaling separates (or maximizes variation between) clusters in the source-term space, without explicitly using source term data during the clustering procedure.

The objective of this work was to both introduce the JSK-means algorithm and show how it can be successfully used to generate dynamically consistent partitions in complex reacting flow applications. The results shown here demonstrate the promising capability of the JSK-means approach to improve upon previous partition-based models for chemical source terms (and other quantities of interest) that were made using clustering approaches that did not have knowledge of the governing equations (e.g., Ref.[27]). This type of modeling extension is actively being explored by the authors and will be reported in future work.  Additional avenues for future work include detailed investigations on the effect of the number of clusterson JSK-means convergence properties, coupling Jacobian-scaled distances with other clustering algorithms outside of the K-means framework,  comparing JSK-means clustering with direct source term clustering using standard K-means, and evaluating the method in other multi-physics applications.

SECTION: Declaration of Competing Interests

The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.

SECTION: Data Availability

Data and source code will be made available on request.

SECTION: Acknowledgements

The authors acknowledge support through ONR Grant No. N00014-21-1-2475 with Dr. Eric Marineau as Program Manager. S.B. acknowledges support by the Argonne Leadership Computing Facility, which is a U.S. Department of Energy Office of Science User Facility operated under contract DE-AC02-06CH11357.

SECTION: References

SECTION: Appendix ADifferences between JSK-means and Other K-means Variants

The general idea of modifying the standard K-means algorithm with weighting functions is not new. In the class of density-based K-means clustering strategies, instead of modifying the distance function directly, the standard phase space distance in Eq.6is scaled with an input PDF[49], which effectively biases the centroid locations to regions where this PDF is high. In other approaches, the distance vectoris scaled by a global diagonal matrix, producing a modified distance vectorthat prioritizes certain components (or features) ofover others in the distance evaluation, where the diagonal elements ofare estimated during the clustering optimization procedure[50,51]. This amounts to a scaling operation on the phase space vector, and these approaches are typically constrained to fixing the same diagonal matrixfor all clusters, the advantage being that doing so leads to no required changes in the convergence properties of the baseline K-means clustering algorithm. Additionally, the class of kernel K-means strategies reformulates the distance computation by transforming the phase space variableinto another variable[52,53]. Distances are evaluated in the-space by means of either the kernel trick or latent-space transformations.

A comparison of the above K-means variations with the JSK-means approach presented here is shown in Table.1. The primary differences are (a) the scaling matrixis a function of the cluster index, which means that during an iterative optimization procedure based on the convergence of centroid locations, themust be updated as the centroids change, (b) the scaling matrices are derived from underlying physical rules via the right-hand-side Jacobian of the dynamical system, which ensures that the clusters adapt to highly sensitive regions in the phase space, and (c) the definition of the centroid as the arithmetic mean of within-cluster samples (Eq.9) is the same as the baseline algorithm.

SECTION: Appendix BDerivation of Centroid Update Rule

The sections below outline derivations of the centroid update rule for three versions of the K-means objective:

The standard K-means objective that utilizes the vanilla Euclidean distance measure (Sec.B.1).

A modified K-means objective that scales sample-centroid distances by a fixed scalar value that is independent of the centroid locations (Sec.B.2).

A modified K-means objective that scales sample-centroid distances by a centroid-dependent scaling variable (Sec.B.3); this is the same modification used in the Jacobian-scaled K-means approach in Sec.3.2.

As will be seen below, the derivations show that the centroid update rule obtained from minimizing (1) and (2) comes from the average of within-cluster samples – i.e. scaling the K-means objective by a constant factor does not change the standard centroid update rule as expected. However, for approach (3), which is the same modification used in the Jacobian-Scaled K-means formulation in Sec.3.3, minimization of the modified objective produces a centroid update rule that deviates from the within-cluster average by a residual proportional to the within-cluster variance of samples.

For illustrative purposes and to simplify the derivations, the input samples here are scalars (i.e.); derivation trends and main takeaways from the scalar case are expected to apply in general to higher dimensional cases. Also, to facilitate clearer and more readable derivations, the notation used below deviates from that used in Sec.3, and instead follows the notation used in Ref.[42].

The input data samples are denoted,, whereis the number of samples. The centroids are denoted,, whereis the number of clusters. The set of centroids is given by.

The objective in the K-Means algorithm is to find athat minimizes the within-cluster variance. This can be expressed in the obective function, where

In Eq.24,encodes the centroid index that is closest to the sample. In other words,if sampleis closest to centroidin the Euclidean sense. Equation24is equivalent to the standard K-means objective provided in Eq.2in the scalar case – the factor ofis inconsequential and is included above for convenience. Additionally, the function ofis equivalent to the function of the assignment matrixin Eqs.2and4.

In the standard K-means approach, the goal is to produce a set ofcentroids that minimizes the above objective. To accomplish this, one path is to recover an centroid update equation from gradient descent as

whereis a so-called learning rate. Although we can proceed in the gradient descent context, a more intuitive formulation comes from the expectation-maximization (EM) perspective[42], which boils down to the following question: given some previous value of the centroids, what are the values ofthat minimize the objective? The derivations below proceed in the context of this question.

SECTION: B.1Standard K-means

We can cast the above question in the following cost function, which is related to the original objective defined in Eq.24:

In Eq.26, the current set of centroids isand the next (new) set of centroids is. The goal is to find an update rule for– the k-th centroid at the next iteration – that minimizes the objective/cost function in Eq.26. This is referred to as ”standard” K-means because the objective function in Eq.26utilizes the usual Euclidean distance.

The analytic solution to the minimization comes from solving the following algebraic equation:

Because K-means is a hard clustering method, there are two conditions:and. If, the partial derivative in Eq.27becomes

If, the partial derivative in Eq.27is zero. Combining these conditions leads to the expression

Solving for Eq27results in

which ultimately provides the familiar centroid update rule:

In other words, the centroid at the next iterationis the within-cluster sample mean using labels from the previous iteration – convergence of the iterative procedure minimizes the target objective in Eq.26, which was the starting point. This is the update rule used in both Alg.1and Alg.2.

SECTION: B.2Constant Scaling Factor

Here, the standard K-means objective is modified slightly with a constant-valued, linear scaling factor:

Constant-valued here meansis set once a-priori and fixed – since both data pointsand centroidsare scalars,in this case is also a scalar. In the case of, the partial derivative becomes

Again, note that if, the partial derivative. This leads to

Because the parameteris nonzero, solvingforproduces the same update rule as the standard case (Eq.31).

SECTION: B.3Centroid-Dependent Scaling Factor

The standard K-means objective is modified here with acentroid-dependentscaling factor:

The scaling factoris evaluated at a specific centroid location and is directly analogous to the role played byin the Jacobian-scaled K-means formulation used in Sec.3.2. This means that the scaling factordynamically adapts to the movement of the centroids during the K-Means iterations. As alluded in Sec.3.3, the derivations below show that the standard centroid update incurs an error when augmenting the distance function in the objective with a centroid-dependent linear scaling, as in Eq.35.

In the case of, the partial derivative becomes

Expanding for the term denotedin the above equation yields

Plugging back in to Eq.36:

Note that in Eq.38, the partial derivative contains the same fluctuation contribution as Eq.33, the constant scaling case. What is introduced by the additional requirement of centroid-dependency in the scaling factor, however, is an additional contribution that (a) depends on a gradient of, and (b) scales with the within cluster variance expressed as. This modification can also be interpreted as the inclusion of a higher-order term in the form of an additional within-cluster moment.

Recall again that in the case of, we get the usual. As such, an iterative solution to the minimization problem requires solving the following equation for:

The key takeaway is that if the variance contribution diminishes, the update rule for the centroids becomes the standard update of Eq.31. The above equation can therefore be interpreted as deviation from the standard centroid update rule by a residual proportional to the variance contribution. As implied by Eq.39, a variance contribution of zero would require one the following conditions to be met:

The gradient of the scaling factor within the cluster is zero.

The within-cluster variance is zero.

The scaling factor itself is zero.

If the scale factoris derived from the Jacobian of a chemical source term (as demonstrated in the main text), the first condition cannot be valid in regions of high chemical sensitivity. Further, within this context, the second and third conditions are met only in trivial equilibrium and ambient conditions in which there is no chemical contribution to the dynamics (i.e. zero source term). As such, a centroid update that does not take into account the variance contribution in Eq.39cannot guarantee monotonic convergence of the modified objective function in Eq.35– this is consistent with the non-monotonic convergence trends observed for the Jacobian-scaled K-means algorithm in Sec.4.2.