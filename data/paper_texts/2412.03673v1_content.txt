SECTION: Interpreting Transformers for Jet Tagging
Machine learning (ML) algorithms, particularly attention-based transformer models, have become indispensable for analyzing the vast data generated by particle physics experiments like ATLAS and CMS at the CERN LHC.
Particle Transformer (ParT), a state-of-the-art model, leverages particle-level attention to improve jet-tagging tasks, which are critical for identifying particles resulting from proton collisions.
This study focuses on interpreting ParT by analyzing attention heat maps and particle-pair correlations on the-plane, revealing a binary attention pattern where each particle attends to at most one other particle.
At the same time, we observe that ParT shows varying focus on important particles and subjets depending on decay, indicating that the model learns traditional jet substructure observables.
These insights enhance our understanding of the model’s internal workings and learning process, offering potential avenues for improving the efficiency of transformer architectures in future high-energy physics applications.

SECTION: Introduction
Machine learning (ML) algorithms are becoming crucial for effectively analyzing the enormous data produced by particle physics experiments like ATLAS and CMS at the CERN LHC.
Attention-based transformer models, with their ability to effectively capture and weigh the relative importance of different elements in input data, have revolutionized various domains, including natural language processing and computer vision.
The ability of the attention mechanism to discern intricate correlations in the collimated spray of particles jets initiated from massive particle decays has proven invaluable in searches for new physics and standard model measurements.

High-energy collisions of protons at LHC can create new unstable particles which then decay and produce sprays of outgoing particles, called jets, which are observed by these experiments.
Jet tagging, or the process of identifying the particle that initiates this spray, is a critical step in data analysis at the LHC.
One such state-of-the-art model for jet tagging is Particle Transformer (ParT), which is a transformer variant that leverages physics-inspired particle-particle pairwise features (or “interactions”) to augment the attention mechanism.
Although this is one of the best performing jet-tagging models, there is effort required to interpret the underlying workflow and why the model performs so well.

The attention-based architecture of transformers may offer deeper insight into the inner workings of neural networks.
In the case of a jet tagger, the attention matrix captures the particle-to-particle correlations potentially making these ML models more interpretable.
These attention scores between particles highlight the most important connections between particles necessary for classification.
This helps us to understand whether the neural network is learning the physics we know.
In addition, looking at which parts of the input the neural network focuses on can help us improve the computational performance by pruning unnecessary parts of the neural network architecture, resulting in a more efficient jet tagging model.

SECTION: Related Work
This paper builds on prior research that has applied machine learning, specifically graph neural networks (GNNs), to particle physics problems at the CERN LHC.
Referenceattempts to explain how ParticleNet, a GNN designed for jet tagging learns to classify three-prong hadronic top quark decays using edge relevancy () graphs based on layerwise relevance propagation.
Similarly, we visualize the attention scores between pairs of particles in order to investigate how ParT classifies jets.

SECTION: Methods
This study uses the pre-trained ParT model, trained using thedataset, which consists of 100 million examples of jets for training, 5 million for validation, and 20 million for testing, containing 10 classes of jets with different decay modes of quarks, gluons,andbosons, Higgs bosons, and top quarks.
The classes are,,,,,,,,, and, indicating the decays to combination of light quarks (), charm quark (), bottom quark (), and lepton ().
There are 17 input features divided into 3 categories: kinematics, particle ID, and trajectory displacement.
Each jet is composed of a maximum of 128 particles, ordered by, with an average of 30–50 particles per jet.

ParT introduces a modified attention mechanism, called particle multihead attention (P-MHA).
Given a particle-level representation of a jet withconstituents with each constituent havingfeatures,, the attention is computed as

whereis a single feature for each pair of particles, which can be concatenated into a larger structure, known as the pairwise interaction matrix.

In ParT, the number of pairwise featuresmust be equal to the number of heads, which is chosen to be 8.
In addition,so that the overall output dimension is the same as the input dimension.
The pairwise interaction matrixis learned by applying a 4-layer pointwise MLP with (64, 64, 64, 8) channels and GELU nonlinearity from the 4 features motivated by Ref.,, where

is the rapidity,is the azimuthal angle,is the transverse momentum, andis the momentum 3-vector andis the norm, for,. We use the exact pre-trained model weights provided from the Particle Transformer repository.

SECTION: Results
SECTION: Distribution of Attention Scores
We visualize the attention scores across all heads in a heat map and observe that the ParT model’s attention scores exhibit a nearly binary (on/off) pattern.
As shown in Fig., the distribution of attention scores reveals that most values are either close to one or zero.
This binary characteristic indicates that particles generally attend to, at most one other particle.
This behavior raises the intriguing question of whether the model is capable of capturing underlying physical laws and how we might interpret the specific physics the model is learning from the data.
The binary nature of the attention weights also suggests potential for more efficient transformer models by reducing the computational complexity of attention mechanisms, focusing only on key interactions between particles.

SECTION: Particle Attention Graphs
We decluster each jet into a specific number of subjets using thealgorithmimplemented in the FastJet packagewith Python bindings.
We cluster into two subjets for leptonic top quark decays (), three subjets for hadronic top decays (), and four subjets for Higgs decays into four quarks ().
For each process, we visualize the attention scores by representing each particle as a point in-space in Fig..
We compare these results with those obtained with randomly initialized attention weights.
We focus on the heads in the final layer of P-MHA.

In Fig., we observe that compared to an untrained, randomly initialized model, ParT demonstrates a significantly stronger tendency to attend to the lepton in theprocess.
This behavior indicates that ParT recognizes the importance of the lepton in classifying the underlying resonance and decay mode.
Additionally, inandP-MHA captures both inter-subjet and intra-subjet interactions.
Specifically, ParT exhibits attention between different subjets and also within individual subjets.
For example, in Fig., P-MHA connects all different subjets but does not form connections within the subjets.
In contrast, P-MHA focuses its attention on interactions between subjets 0 and 1 while neglecting connections with subjet 2, instead of forming connections within subjet 2.

In order to quantify how often ParT exhibits these behaviors, we display the distribution of attention values for different jet classes.
In particular, we find the sum of all attention values to a lepton injets, and divide it by the sum of all attention values in the head.
Since the sum of all attention values in a head should be equal to the number of particles, this effectively is the proportion of attention weights that attend to the lepton.
Similarly, forandjets, we display the sum of attention weights between particles in different subjets divided by the sum of all attention weights.
In Fig., we observe that the trained ParT model is more likely to have a head that consists of only inter-subjet connections or only intra-subjet connections than an untrained, randomly initialized model.
ParT attending to only leptons or to no leptons forjets, and only intra-subjet or only inter-subjet forandjets is some evidence that ParT is learning relevant physical properties of the jet data.

SECTION: Limitations
Our analysis is constrained to the final attention layer, limiting insights into how earlier layers contribute to the representations. Additionally, the observed attention patterns may depend on the specific subjet clustering algorithm used, which could influence the model’s interpretability. Exploring intermediate layers and varying subjet algorithms in future studies could provide a more comprehensive understanding of ParT’s attention mechanisms.

SECTION: Optimizing Attention
Mechanism for Efficiency
Leveraging the binary nature of P-MHA, we constrain the total number of particles used in the attention mechanism, and explore the impact of the number of particles attended to in each attention head on the overall performance.
For only one particle, we directly replace attention with softmax.
For other numbers of particles, we keep the top-highest attention particles, setting the rest of the particles to zero, and then applying softmax over thoseparticles (out of 128 maximum).
We find that with just 1 particle, some performance is maintained and with 30 particles, the performance of ParT is recovered as shown in Table.
By filtering the particles that P-MHA can consider, we can potentially reduce the number of calculations and increase computational efficiency.

SECTION: Summary and Outlook
The ParT model demonstrates a unique binary attention pattern, in which each particle typically attends to at most one other particle.
This focused attention mechanism contrasts with other transformer models, such as vision transformers, which spread attention across many elements.
The sparse and selective attention in ParT prioritizes key relationships, potentially uncovering important substructures in jet tagging, and enhancing model interpretability in high-energy physics.

This binary pattern also opens up opportunities for optimization.
This could lead to more efficient architectures without compromising performance, particularly in tasks such as jet tagging, where the identification of essential interactions is vital.
Future work could explore these optimizations to further improve both the interpretability and efficiency of transformers in physics applications. This work will be helpful in designing a more efficient physics-based attention mechanisims.

In addition, the importance of the interaction matrix for the performance of ParT has not been explored fully.
In Ref., many P-MHA layers are replaced with the interaction matrix and find improved performance and efficiency.
Understanding what the interaction matrix is learning to augment P-MHA could lead to a more efficient architecture.

SECTION: Broader Impact
Studies that interpret machine learning algorithms are important to increase confidence in model predictions and also to improve the performance of future machine learning models.
This study uses an xAI method in order to explain the performance of the state-of-the-art transformer model for the LHC, by demonstrating that the model is indeed learning physics.
This study opens doors for future studies to extract and discover physics information in machine learning models that are currently not being used for prediction.

The code used for this analysis is publicly available at:

SECTION: Acknowledgments and Disclosure of Funding
This work is supported by the DOE Office of Science, Award No. DE-SC0023524, Fermi Research Alliance, LLC under Contract No. DE-AC02-07CH11359 with the DOE, Office of Science, Office of High Energy Physics, LDRD L2024-066-1, DOE Office of Science, Office of High Energy Physics “Designing efficient edge AI with physics phenomena” Project (DE-FOA-0002705), DOE Office of Science, Office of Advanced Scientific Computing Research under the “Real-time Data Reduction Codesign at the Extreme Edge for Science” Project (DE-FOA-0002501),a nd DOE Early Career Research Program Award No. DE-SC0021187.
The work is also supported in part by RCSA Grant #CS-CSA-2023-109, Sloan Foundation Grant #FG-2023-20452, NSF awards CNS-1730158, ACI-1540112, ACI-1541349, OAC-1826967, OAC-2112167, CNS-2100237, CNS-2120019, the University of California Office of the President, and the University of California San Diego’s California Institute for Telecommunications and Information Technology/Qualcomm Institute, AI2050 program at Schmidt Futures (Grant G-23-64934), and the NSF HDR Institute A3D3 (PHY-2117997).

SECTION: References