SECTION: Hypergraph-Laplacian equations for data interpolation and semi-supervised learning

Hypergraph learning with-Laplacian regularization has attracted a lot of attention due to its flexibility in modeling higher-order relationships in data.
This paper focuses on its fast numerical implementation, which is challenging due to the non-differentiability of the objective function and the non-uniqueness of the minimizer.
We derive a hypergraph-Laplacian equation from the subdifferential of the-Laplacian regularization.
A simplified equation that is mathematically well-posed and computationally efficient is proposed as an alternative.
Numerical experiments verify that the simplified-Laplacian equation suppresses spiky solutions in data interpolation and improves classification accuracy in semi-supervised learning.
The remarkably low computational cost enables further applications.

SECTION: 1.Introduction

Over the past two decades, hypergraphs have become a valuable tool for data processing.
It is defined as the generalization of a graph in which a hyperedge can connect more than two vertices.
This allows hypergraphs to model higher-order relations involving multiple vertices in data, with applications in areas including image processing[1,2], bioinformatics[3,4], social networks[5,6], etc.

In this paper, we focus on semi-supervised learning on undirected hypergraphs.
Letbe a hypergraph, wheredenotes the vertex set,is the hyperedge set, andassigns positive weights for hyperedges.
We are given a subset of labeled verticesand the associated labels.
The goal is to assign labels for the remaining verticesbased on the training data.

We consider the standard approach that minimizes
the constraint functional

Hereis the regularization ofthat enforces the smoothness of.
It implicitly assumes that vertices within the same hyperedge tend to have the same label.
The constraint inensures the minimizer of it to satisfy the given training data.
One of the fundamental algorithms for hypergraph learning is the-Laplacian regularization[7]

where.
Notice that the hypergraphcan be approximated by a weighted graphwith clique expansion[8].
More precisely, for any, there exists an edge. The associated weight, wheredenotes the cardinality of hyperedgeand.
Then
functionalis equivalent to applying the graph-Laplacian[9]

to the weighted graph.
It was shown that the approximation approach can not fully utilize the hypergraph structure[8].
Later in[10],
the authors proposed to overcome this limitation with a new hypergraph-Laplacian regularization

which is deduced from the Lovász extension of the hypergraph cut.

is more mathematically appealing thandue to its convexity but non-differentiability.
In[11], the authors defined the hypergraph-Laplacian operator, which is multivalued, as the subdifferential.
Properties of solutions to nonlinear evolution equations governed by(i.e.,and its variants)
were studied[11,12].
The variational consistency betweenand the continuum-Laplacian

was established in our previous paper[13]in the setting when the number of verticesgoes to
infinity while the number of labeled vertices remains fixed.
To avoid the complicated structure of the hypergraph,
we considered a class of hypergraphs constructed from point cloud data by the distance-based method.
It was shown both theoretically and numerically thatsuppresses spiky solutions in data interpolation better than the graph-Laplacian.

On the other hand, the non-differentiability ofand the non-uniqueness of its minimizers cause some challenges in the numerical aspect.
Unlike the graph functional, there exists no straightforward and efficient algorithm for minimizing, even in the case.
The primal-dual hybrid gradient (PDHG) algorithm[14]was first considered in[10]for.
A new algorithm[13]that works anywas proposed based on the stochastic PDHG algorithm[15].
To avoid the non-uniqueness of minimizers for, the-norm constraint was used in[10].
While in[16], the authors proposed to minimizeby the subgradient descent method[17]and utilized the confidence interval to ensure the uniqueness.
Nevertheless, their high computational cost cannot be neglected for large-scale datasets and hinders further applications of the hypergraph-Laplacian.

The purpose of this paper is to provide an alternative tothat can be uniquely and efficiently solved.
We begin by addressing the non-uniqueness issue of the minimizer forand obtain a single-valued-Laplacian operator from the subdifferential of.
The operator involves unknown parameters dependent on the structure of the hypergraph, preventing us from solving the associated-Laplacian equations for semi-supervised learning.
A simplified equation that disregards these parameters is then proposed as an approximation.
It is mathematically well-posed: It admits a unique solution and satisfies the comparison principle.
There exist hypergraphs on which the solution of the simplified equation coincides with a minimizer of.
Despite this simplification, we still refer to it as the hypergraph-Laplacian equation.

Through numerical experiments on one-dimensional data interpolation, we observe that the simplified hypergraph-Laplacian equation substantially inherits the characteristic ofthat suppresses spiky solutions.
Experimental results on real-world datasets indicate that it even improves the classification accuracy for semi-supervised learning.
The most notable feature of the new equation is its low computational cost.
When compared to the aforementioned algorithms, it dramatically reduces the computation time for semi-supervised learning on the selected UCI datasets from dozens of seconds to less than 0.5 seconds.
Further applications of the hypergraph-Laplacian for large-scale datasets become possible.

This paper is organized as follows. In section 2, we establish a hypergraph-Laplacian equation from the subdifferential ofand propose a simplified version that is computationally feasible and efficient. The properties of solutions for the equation are also discussed.
Numerical experiments are presented in section 3 to demonstrate the performance of the simplified equation for data interpolation and semi-supervised learning. We conclude this paper in section 4.

SECTION: 2.Hypergraph-Laplacian equations

Let.
Throughout this paper, we always assume that the hypergraphis connected. Namely,
for any, there exist hyperedges, such that,, andfor any.

SECTION: 2.1.The property for the minimizer of

Notice thatis coercive and lower semi-continuous[13], it admits at least one minimizer.
The non-uniqueness of minimizers can be seen from the fact that the functional depends only on the maximum and minimum values on each hyperedge.
We are more concerned with vertices whose values are uniquely determined when minimizing the functional.

Letbe a minimizer of. We defineto be a subset of vertices such that for anyand any perturbation ofat, i.e.,

whereis a constant with small absolute value,holds.

Clearly,. The following lemma that characterizes the vertex infollows from the definition directly.

Letbe a minimizer ofand. There exist two hyperedgessuch that

The maximum and minimum values of a minimizer on each hyperedge are uniquely determined by.

Letandbe two minimizers of.
For any, we have

Define. For any, there exist, such that

If any one of the above inequalities is strict, we have

which contradicts the assumption thatis a minimizer of.
Consequently, by the strict convexity of,

for any.
This yields

and there exists a constant, such that

It is not difficult to see from the factforthat.
This proves (5).
∎

Ifandare two minimizers of, thenand

for any.

The proposition implies thatis uniquely determined by. The non-uniqueness of minimizers forcomes from.

Let. By (4), we assume w.l.o.g. thatand

for two hyperedges.
By (5), to prove (6), we only need to show that

which also implies thatand proves that.

If this is not true, i.e.,

it follows from (5) and (7) that

This contradicts the assumption thatand finishes the proof.
∎

SECTION: 2.2.The subdifferential ofand the hypergraph-Laplacian equation

Functionalandare non-differentiable. We consider the subdifferential for them.

Letbe a Hilbert space with inner productand induced norm.
For a proper, convex, and lower semi-continuous functionalwith
effective domain

the subdifferential ofatis defined as

An element ofis called a subgradient ofat.
The subgradient coincides with the usual gradient ifis differentiable.
We shall use the following proposition of the subdifferential

whose proof is trivial.

The subdifferential ofhas been obtained in[11].
More precisely,

where

is an indicator function

anddenotes the convex hull ofin.

The subdifferential for the constraint functionalis a corollary of the definition and (9).
For any

we have

Namely, a subgradient ofcomes from a subgradient ofby taking arbitrary values at labeled vertices.

By combining the above results (8)–(10), we deduce an equivalent form for the minimizer of.
More precisely,
ifis a minimizer of,
there exist vectorssuch that

Conversely, if a functionsatisfies equation (11), where, then it is a minimizer of.

In the rest of this subsection, we propose a new hypergraph equation based on equation (11).
The basic idea is to consideras a diffusion coefficient that represents the contribution of hyperedgeto vertex.
The proposed equation reads

where

forand.
Here we restrict the equation on the subhypergraph, where, to avoid the non-uniqueness.
The notationis used for the case.

Owing to the following theorem, we call equation (12) the hypergraph-Laplacian equation.

Letbe a minimizer of. Thenis a solution of equation (12).

We redefineinsuch that for any,

By the assumption,andsatisfy equation (11).
Notice that for anyand,

Namely, equation (11) is trivial for.

Letand.
If, we have

and

The same conclusion can be obtained for the casesand.
Notice that

This means thatsatisfies equation (12) and finishes the proof.
∎

Conversely, ifis a solution of (12), it is not difficult to verify by reversing the proof of Theorem2.5thatis also a minimizer of.
Then a unique minimizer ofcan be determined, e.g.,

Here we use the notation forthat(i.e., the degree ofis) and.

Although a minimizer ofcan be uniquely determined through equation (12). The equation itself is not solvable numerically. Indeed, both the domainand the diffusion coefficientdepend on the structure of the hypergraph and the training set and thus have no general expression.

SECTION: 2.3.A simplified hypergraph-Laplacian equation

The purpose of this subsection is to present a simplified version of equation (12) that does not involveand.
To this end, we consider the homogeneous coefficientand the whole domain.
The new equation is as follows

where

forand.

In general, a solution of equation (13) is no longer a solution of equation (12) (when restricting to) and is not a minimizer of.
Figure1shows an example of a hypergraph and a functionon it that minimizes.
Clearly,is not a solution of equation (13) since in this case.
Nevertheless, there exist specific instances where a solution of equation (13) and a minimizer ofcoincide, as demonstrated in Figure2.
For this reason, we still refer to equation (13) as the hypergraph-Laplacian equation.
The theoretical study of the connection between the discrete equation (13) and the classical-Laplacian equation will be part of our future work.

The comparison principle and the unique solvability of equation (13) are stated as follows.

Ifare two functions that satisfy

and

then

Assume to the contrary. We claim that there exist a hyperedgeand verticessuch that

and

Otherwise, by the connectivity assumption of the hypergraph,on every hyperedge,
which is a contradiction.

Assume w.l.o.g. thatand.
Then we have

Equivalently,

Taking the maximum and the minimum for the above inequality respectively and combining the results lead to

for any.
It follows from (14) that the above inequality is strict for hyperedge.
In fact, by

we have

and consequently,

This together with (15) and the monotonicity ofimply that

which is a contradiction to the assumption.
∎

Equation (13) admits a unique solutionthat satisfies the estimate

The uniqueness of solutions is a corollary of the comparison principle. We prove the existence of a solution in the following by the Brouwer fixed-point theorem.
It can also be proven by Perron’s method.

Let

be a closed and convex subset of.
For a,
we consider the auxiliary equation

Recall that under the notationfor,is continuous and monotone onfor.
Consequently, for any, the left-hand side of equation (16) is continuous and monotone with respective to. Then a zero point exists and equation (16) admits a unique solution.

By rewriting equation (16) as

where

we further have.

Now we define a mappingby.
It is continuous and admits a fixed point, which is also a solution of equation (13).
∎

The superiority of equation (13) over equation (12) and the functionallies in the computational efficiency. It can be solved with fixed-point iteration

for, whereis any initial guess of the solution andis the step size.
The Dirichlet boundary condition,is posed at each step.
In the case, we can further drop the step size by iterating

If

the scheme is bounded and monotone.
Namely,and(or)
for anyand.
The convergence of (17) follows.

SECTION: 3.Numerical experiments

In this section, we discuss the numerical performance of the proposed simplified hypergraph-Laplacian equation (13) for data interpolation and semi-supervised learning.
We focus on the case, which is commonly used in practice.
All experiments are performed using MATLAB on a mini PC equipped with an Intel Core i5 2.0 GHz CPU.

SECTION: 3.1.Data interpolation in 1D

Letandbe random numbers on the intervalthat follows the standard uniform distribution.
We assume thatof the points are labeled (denoted by red circles in Figure3). The goal is to interpolate the remaining points.

A-nearest neighbor graphwith vertex setcan be constructed.
For any vertices, we connect them by an edgeifis among the-nearest neighbors of, denoted by.
We also connect them iffor the sake of symmetry.
The constant weightis adopted for edge.
Alternatively, we can also construct a-nearest neighbor hypergraphwith the vertex set.
For every vertex, we define a hyperedge

Weightis assigned for every hyperedge,.

The interpolation problem becomes the semi-supervised learning onor, which can be solved by equation (13) (i.e., iteration scheme (17)).
We also consider the graph-Laplacian, the hypergraph-Laplacianandfor comparison, see (1)–(3) for their definitions.is solved by the algorithm of[18].
We utilize the gradient descent scheme for, which is as follows

for.is solved by the stochastic PDHG[13].

To compare the interpolation result and the computation time of different algorithms, we run four algorithms for a sufficiently long time to obtain the “true solutions”(shown in Figure3).
The running time of hypergraph models with respect to the relativeerror

is then plotted in Figure4.

As illustrated in Figure3, all four algorithms effectively interpolate the data when.
However, asincreases, notable differences emerge.develops spikes at the labeled points.exhibits similar spiking behavior as it is essentially the graph Laplacian.
In contrast,effectively suppresses spiky solutions.
As an approximation of, equation (13) produces solutions with a similar structure.
The difference between the two is thatgives better interpolation results near the labeled points (see the 3rd and 4th labeled points), while equation (13) provides smoother results (see the case).

Figure4shows the computational costs of different algorithms. Equation (13) outperformsby a large margin and is even better than.
Notably, its running time decreases as the parameterincreases.
This is due to the fact that a largerincreases the cardinality of vertices, which in turn accelerates the convergence of equation (13).
The running time for largeis comparable to that of the graph modelwith a recent algorithm[18]().

Equation (13)

SECTION: 3.2.Semi-supervised learning

In the rest of this section, we consider the performance ofand equation (13) for semi-supervised learning on some real-world datasets summarized in Table1.
Both Mushroom and Covertype come from the UCI repository.
Covertype(4,5) and Covertype(6,7) are derived from Covertype by selecting two classes, (4,5) and (6,7) respectively.
They are provided by the first author of[10].
All three datasets contain only categorical features. We construct a hypergraphfrom a given dataset as follows.
For each feature and each category of the feature, we construct a hyperedgeby joining all vertices that belong to the same feature and category.
duplicated hyperedges are removed.
Cora and Pubmed[19]are hypergraph datasets, in which vertices are documents and hyperedges follow from co-authorship or co-citation.
Weightis used for all hyperedges in all datasets.

Algorithm1provides the detail of semi-supervised learning on hypergraphs withand equation (13).

The stochastic PDHG algorithm is not suitable forwhen hyperedges have large cardinality (see Figure4). We adopt the subgradient descent algorithm proposed in[16]for solving.
The step size is empirically chosen as

to speed up the algorithm, as suggested by the authors.
In this setting, it is not easy to find a robust stopping criterion for the algorithm since it is not convergent in general.
This is also the reason that we do not utilize it in the previous subsection.
We manually select the smallest iteration number withinthat reaches the minimum classification error for each dataset and labeling rate.
While our simplified-Laplacian equation is easy to implement. A simple stopping criterion likeis stable and gives expected classification results. We choosefor datasets Mushroom and Covertype and choosefor datasets Cora and Pubmed.

To compare the classification accuracy ofand equation (13), we randomly selectpoints from the datasets Mushroom and Covertype and selectpoints from the datasets Cora and Pubmed as the training sets and run the algorithms for 10 times.
The classification error and the standard deviation are summarized in Table2.
Surprisingly, equation (13) achieves better classification accuracy, even though it comes from an approximation ofand cannot suppress spiky solutions as well as.

The main contribution of equation (13) lies in the computational efficiency and the stability.
Figure5shows the average running time of two algorithms over 10 runs.
For datasets Mushroom and Covertype, the computation time is greatly reduced by equation (13).
While for datasets Cora and Pubmed, equation (13) is no longer favorable.
Two facts should be noticed. Equation (13) requires more computational time for datasets Cora and Pubmed as they contain more hyperedges.
The subgradient descent algorithm for solvingdoes not converge in general with the given step size (19), leading us to manually select the best step size. Whereas for Cora and Pubmed, it requires only a few hundred iterations to get the best classification error.
This method is clearly inapplicable to practical problems where the real dataset is unknown. The numerical scheme (17) is convergent and does not involve any parameters, thus avoiding this problem.

Equation (13)

SECTION: 4.Conclusion

In this paper, a new hypergraph-Laplacian equation, deduced from an approximation of the hypergraph-Laplacian regularization, has been proposed for semi-supervised learning.
The unique solvability and the comparison principle of the equation have been established.
Numerical experiments have confirmed the effectiveness of the new equation. It not only suppresses spiky solutions and improves classification accuracy but also significantly reduces computation time.

SECTION: Acknowledgements

KS is supported by China Scholarship Council.
The authors acknowledge support from DESY (Hamburg, Germany), a member of the Helmholtz Association HGF.

SECTION: Declarations

Data AvailabilityData will be made available on request.

Conflict of interestThe authors have no relevant financial or non-financial interests to disclose.

SECTION: References