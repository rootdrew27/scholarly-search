SECTION: Multiple Descents in Unsupervised Learning: The Role of Noise, Domain Shift and Anomalies

The phenomenon ofdouble descenthas recently gained attention in supervised learning. It challenges the conventional wisdom of the bias-variance trade-off by showcasing a surprising behavior. As the complexity of the model increases, the test error initially decreases until reaching a certain point where the model starts to overfit the train set, causing the test error to rise. However, deviating from classical theory, the error exhibits another decline when exceeding a certain degree of over-parameterization. We study the presence of double descent in unsupervised learning, an area that has received little attention and is not yet fully understood. We conduct extensive experiments using under-complete auto-encoders (AEs) for various applications, such as dealing with noisy data, domain shifts, and anomalies. We use synthetic and real data and identify model-wise, epoch-wise, and sample-wise double descent for all the aforementioned applications. Finally, we assessed the usability of the AEs for detecting anomalies and mitigating the domain shift between datasets. Our findings indicate that over-parameterized models can improve performance not only in terms of reconstruction, but also in enhancing capabilities for the downstream task.

SECTION: 1Introduction

In recent years, there has been a surge in the use of extremely large models for both supervised and unsupervised tasks. This trend is driven by a desire to solve challenging machine-learning tasks. However, this pursuit contradicts the well-known bias-variance trade-off, which suggests that larger models tend to overfit the training data and perform poorly on the test set[1]. Despite this, many over-parameterized models have been able to generalize well[2,3]. This challenges common assumptions regarding the generalization capabilities of models[4,5,6], as over-parameterized models often exhibit significantly superior performance compared to smaller models, even when interpolating the training data[7,8].

Recently, the authors in[9]conducted a study on the bias-variance trade-off for large, complex deep neural network models. They discovered an interesting phenomenon called double descent. Initially, as the complexity of the model increases, the test error decreases. Specifically, as the complexity continues to increase, the variance term starts to dominate the test loss, resulting in an increase, which is known as the classical bias-variance trade-off. However, at a certain point, termed the "interpolation threshold"[10], the test loss stops increasing and begins to decline again in the over-parameterized regime, yielding a curve with two decent regimes.

The phenomenon of double descent has been observed in many frameworks in supervised learning (see a survey in[11]). Model-wise double descent was demonstrated in[12], while[10,13]explore the impact of label noise on the double descent curve and demonstrated the phenomenon to epoch-wise and sample-wise double descent. Multiple descents were discussed in[14,15,16], and[17]reveals that the interpolation threshold is dependent on the linearity and non-linearity of the model. Additional, double descent has been demonstrated in adversarial training schemes including[18]and[19], which found model-wise and epoch-wise double descent respectively.
However, the existence of double descent in core tasks in unsupervised learning is not yet fully understood. In this study, we analyze the double descent phenomenon and its implications for crucial unsupervised tasks such as domain adaptation, anomaly detection, and robustness to noisy data.

We present extensive empirical evidence that double descent occurs in unsupervised learning when the data is heavily contaminated with noise. We find that "memorization" in unsupervised learning can occur in the presence of strong noise. This often results in overfitting the noise rather than capturing the underlying signal, leading to unsatisfactory test performance. However, sufficiently over-parameterized models possess the capability to achieve superior test performance despite fitting the noisy training samples, which implies that they succeed in capturing the signal.
We conduct experiments using synthetic and real data demonstrating that double and even triple descent occur in under-complete AEs applied to data with different types of contamination. Precisely, we show that different levels of sample noise, feature noise, domain shift, and outliers percentage affect the double descent curve. In these settings, we identify model-wise, sample-wise, and epoch-wise double descent. In Figure1, we present a double descent curve obtained by training an AE model on data generated using the "sample noise" data model described in Section3.1. We further demonstrate the applicability of our findings to common unsupervised tasks in real-world settings. Specifically, we show that over-parameterized models trained on data from a source domain can adapt better to the target domain in the presence of a distribution shift. Furthermore, we identify non-monotonic behavior in anomaly detection capabilities when the model size is increased.

SECTION: 2Related Work

The discovery of double descent for neural networks (NNs) has led to extensive research aimed at understanding the behavior of generalization errors. It has also provided insight into why larger models perform better than smaller or intermediate ones. Most studies have been conducted in a supervised learning setting, as detailed in[9,10,11,12,13,14,15,16,20]. Recent studies[10,21,22]have introduced label and feature noise and demonstrated that large over-parameterized NNs can "memorize" the noise while still generalizing better than smaller models.

The phenomenon of double descent has not been extensively studied in the context of unsupervised learning, and there are some contradictions in the literature regarding its presence. Principal Component Analysis (PCA)[23]and Principal Component Regression (PCR)[24], which are special types of linear AEs, are widely used unsupervised learning models and can serve as an interesting case study for double descent. The authors in[25]argued that there is no double descent in PCA while[26,27]show evidence for double descent in PCR and oracle-PCR, respectively. The authors in[28]used a specific subspace data model and argued that there is no sign of model-wise double descent in both linear and non-linear AEs.[29,30]demonstrate sample-wise double descent for denoising AEs with different Signal-to-Noise Ratios (SNRs). The research in[31]used a self-supervised learning framework for signal processing and found epoch-wise double descent for different levels of noise.

Our analysis of double descent differs from previously published studies in three significant ways. Firstly, when trained on noisy data, we demonstrate that standard under-complete AEs experience double and even triple descent at the model-wise, sample-wise, and epoch-wise levels. We have also partitioned the model’s size into bottleneck and other hidden layer dimensions to understand the phenomenon better. Secondly, we show that the noise magnitude and the number of noisy samples affect the double descent curve. Thirdly, we show that double descent also occurs in common realistic contamination settings in unsupervised learning, such as source-to-target domain shift, anomalous data, and additive feature noise. Finally, we demonstrate the implications of multiple descents in unsupervised learning tasks using real-world data, extending beyond reconstruction.

SECTION: 3Data Model

This section outlines the data and contamination models we used to study double descent.

SECTION: 3.1Linear Subspace Data

We will start our experiment by utilizing the synthetic dataset from[28]to challenge their assertion that “double descent does not occur in self-supervised settings”. First, we samplerandom i.i.d. Gaussian vectors, each of size, representing random features in a latent space,. Next, we embed the vectorsinto a higher dimensional space of sizeby multiplying eachbyof size,, where. This setting can be thought of as measuringwith a measurement tool, resulting in higher-dimensional data. Our dataset differs from that of[28]in several ways, and we will investigate four scenarios as part of our study:

Sample Noise.We aim to investigate the impact of the number of noisy training samples on the test loss curve. In contrast to[28], which adds noise to all samples, we vary the number of noisy training samples to identify memorization. We do this by introducing a new variable,, representing the probability of a sample being noisy. Thus,represents the percentage of noisy samples in the data. As noise is added, we control the SNR. We defineas the factor controlling the SNR between the signal and the noise. Another significant change from[28]is the chosen values of SNR, which can be found in AppendixA, table1, along with its calculation to derive, in AppendixB. This leads to the following equations, which describe our model for sample noise:

whereis an additive white Gaussian noise (AWGN), representing the noise added to samples with probability. This setting can be likened to using a noisy measurement device. To illustrate this generation we present in AppendixA, Figure19a visualization of the data model.

Feature Noise.We further study the impact of the number of noisy training features on the test loss curve. In the scenario of feature noise, each sampleis affected by noise in certain features. We denote the probability of a feature being noisy by, controlling each sample’s noisy features. We simulate a scenario where we havemeasuring tools, each measuring a feature. To introduce noise, we select the same set of features to be noisy across all samples. This mimics a situation whereof the measuring tools are unreliable or noisy.
Since the noise added to each sample is a vector of length, the SNR calculation differs from the case of sample noise and is explained in AppendixB. AppendixA, Figure19depicts the data generation for this setting.

Domain Shift.We aim to explore how the test loss curve behaves when there is a domain shift between the train and test data. To achieve this, we partition the vectors in the latent spaceinto two groups: train and test vectors, denoted asandrespectively. Then, the train vectors are projected to higher dimensional space with the matrix, and the test vectors are projected with a different matrix, modeling a domain shift. To control the shift, we define, whereis the matrix multiplying the train vectors andis a new random matrix added toto cause perturbations at each entry of. The parametercontrols the shift betweenand. As a result, for, a domain shift is present between the train and test datasets, and we get

It is important to note that becauseandare i.i.d.,follows a normal distribution. To obtain the same norm in the test data, we divideby. This scenario is similar to the case where two different measuring instruments (i.e.,) are measuring the same phenomenon. This data model is illustrated in AppendixA, Figure20, and the definition of the SNR is detailed in AppendixB.

Anomalies.We conduct an experiment to investigate the impact of anomalies in the training set on the test loss curve. To represent clean samples, we utilize. For generating anomalies, we sample from a normal distribution. We introduce a metric termed signal-to-anomaly-ratio (SAR), which regulates the magnitude ratio between the clean and anomaly samples through the parameter. Subsequently, we substituteof the normal samples with anomalies. This generation is illustrated in AppendixA, Figure21.

We generate 5000 samples for training and 10000 for testing across all of these scenarios.

SECTION: 3.2Single-Cell RNA Data

We utilized single-cell RNA sequencing data from[32]to illustrate our findings using real-world data. The data exhibits diverse domain shifts across different laboratory environments and measurement technologies. This dataset is crucial for assessing the impact of domain shifts on the test loss curve. Since this data is from a real-world setting, we are unable to control the shifts between the training (source) and testing (target) datasets, as explained in Section3.1.
We focused on dataset number 4, which includes 5 distinct domains named ’Baron’, ’Mutaro,’ ’Segerstolpe,’ ’Wang,’ and ’Xin’ and 15 different cell types. Each cell (sample) contains over 15000 genes (features). To facilitate the training of deep models while preserving the domain shift, we have retained the top 1000 prominent features. We utilize the ’Baron’ domain for both sample and feature noise and domain shift scenarios due to its largest sample size (8569). 5000 samples are allocated for training and the reserved 3569 samples are for testing. Additive white Gaussian noise (AWGN) is added to specific training samples and features, as described in3.1. The calculations of the SNR for both sample and feature noise cases are provided in SectionB. As for the domain shift scenario, the target batches are ’Mutaro’ (2122 samples), ’Segerstople’ (2127 samples), ’Wang’ (457 samples), and ’Xin’ (1492 samples).

SECTION: 4Results

All the experiments are conducted using multi-layer perceptron under-complete AEs utilizing at least 5 different random seeds. Complete implementation details can be found in AppendixA. All models are trained using contaminated datasets and tested on clean data. Consequently, the test loss serves as an indicator of whether the model has learned the noisy data (high test loss) or the signal (low test loss). This concept is akin to label noise in supervised learning, as discussed in[10]. The train loss figures corresponding to all test losses depicted in this section are provided in AppendixC.

SECTION: 4.1Model-Wise Double Descent

This section analyzes the test loss with increasing model sizes. For AEs, we break down the well-known "double descent" phenomenon into two interconnected variations: "hidden-wise" and "bottleneck-wise" double descent. We show how both contribute to the double descent behavior in the test loss. We also study the influence of several contaminations described in Section3. We conclude that the interpolation threshold location and value can be manipulated by these factors.

In Figure2, we provide visual evidence of the bottleneck-wise and hidden-wise double descent. This not only helps to distinguish between various model sizes but also underscores the significance of our different architectural choices. As can be seen the training loss consistently decreases as the dimensions of the model increase. In contrast, both the bottleneck and hidden layers exhibit the characteristic double descent curve, as seen in the decrease in test loss, followed by an increase and then another decline. This clear demonstration of double descent corroborates that AEs trained on highly contaminated data can exhibit double descent.

Sample noise.Interestingly, Figure3(a)shows that the height of the test loss increases and the interpolation threshold peak location shifts towards larger models as the level of sample noise increases. This can be clarified by the observation that increased noise adversely affects model learning. Moreover, we need a bigger model to overfit the noisy samples. For sample noise ranging from 10% to 20%, the absence of double descent can be attributed to an insufficient number of noisy samples in the training data. In Figure3(b), we demonstrate triple descent using single-cell RNA data, where we notice a similar behavior for the test loss, and specifically for each of the two peaks. Furthermore, all instances of sample noise exhibiting double and triple descent present a lower test loss in the second or third descent in comparison to the first, excluding 80% and 90% in3(a).

Feature noise.Feature noise adds complexity since each sample contains noise in some of its features. As a result, the model never encounters samples with entirely clean features, making it unable to isolate and focus on clean data. Consequently, the model experiences difficulty in learning the correct data structure. Surprisingly, increasing feature noise actually leads to a decrease in the test loss for the single-cell RNA dataset (Figure4(b)) and an increase for the case of linear subspace data. Moreover, the peak shifts left as the number of noisy features rise in Figure4(a).

SNR.We observed that the SNR plays a crucial role in the test loss, which in turn affects the height of the peak. A higher SNR value reduces the impact of noise, allowing the model to learn the underlying signal from the training set, resulting in a lower test loss. Conversely, a lower SNR value amplifies the influence of noise, causing the model to minimize the training loss by memorizing the noise rather than learning the signal. This leads to inferior results in the test loss. Figures5and6demonstrate this for the scenarios of sample and feature noise respectively.

Domain shift.We now study the existence of double descent when the distribution of the training (source) data,, differs from that of the testing (target) data,.
We investigate the impact of the model size on learning shared representations for both source and target datasets and reducing the shift between them. By training the model on the source data and testing it on different targets, we unveil non-monotonic behavior and double and triple descent curves for synthetic and real datasets discussed in Sections3.1and3.2, respectively. Furthermore, we observe instances where over-parameterized models result in lower test loss, leading to improved target data reconstruction as depicted in Figure7. Additionally, we notice that
the test loss rises as the shift is more dominant. Section5.1presents double and triple descent for real-world data and further insights about the connection of model size and domain adaptation.

Anomaly detection.We also identify double descent occurring when anomalies, deviating from the expected behavior of the data are introduced into the training set. We use the anomaly dataset mentioned in3.1and study the test loss curves by varying the amounts of anomalous training samples. We then evaluate the anomaly detection capabilities using the receiver operating characteristic area under the curve (ROC-AUC). This metric employs the reconstruction error score to measure the model’s ability to distinguish between the clean and anomalous data. Anomalies are identified as data points with scores surpassing a defined threshold. A higher ROC-AUC value signifies superior performance. As demonstrated in the interpolation regime depicted in Figure8(a), interpolating models show higher test loss of the clean samples, complicating the differentiation between clean and anomaly data.

Scaling up the model size, results in a secondary descent in the test loss of the clean data, particularly evident under conditions of low SAR and a high number of anomalies. This secondary descent facilitates the model’s ability to differentiate between clean and anomalous data, resulting in performance comparable to that of smaller models in terms of ROC-AUC, while learning meaningful embedding for both clean data and outliers, resulting in lower test losses. Figure8(b)demonstrates the absence of double descent due to the high SAR. However, similar to Figure8(a), intermediate models exhibit poorer ROC-AUC performance compared to small and over-parameterized models.
We also present more insights on anomaly detection when utilizing real-world data in Section5.2.

SECTION: 4.2Epoch-Wise Double Descent

In this section, we study the concept of double descent with respect to the number of epochs. This work is the first unsupervised investigation of its kind, building upon similar research conducted
by[10]for supervised learning. We begin by analyzing the impact of the number of noisy samples and features in the train set on the test loss.

Figures9and10show the results for both data models respectively. As the portion of noisy samples or features increases, the test loss becomes higher since the model tends to prioritize overfitting the noise rather than learning the signal.

We have conducted further research on epoch-wise double descent, which has led us to discover a connection between the SNR and the height of the test loss. As the SNR decreases, the noise becomes more dominant, resulting in an increase in the test loss. This is illustrated in Figures11and12for the cases of sample and feature noise respectively.

Epoch-wise double descent is also present when there is a domain shift between the train and test sets, as illustrated in Figure13.13(a)shows that the stronger the shift, the higher the test loss.

SECTION: 4.3Sample-Wise Double Descent

In this section, we study the impact of changing the number of training samples while keeping the model size constant. The complexity of a model and the number of samples it is trained on, both play a crucial role in determining whether the model is over or under-parameterized. When the sample size is small, the model is considered over-parameterized. However, as the sample size increases, the model enters the under-parameterized regime. This causes the interpolation threshold to shift to the right, as shown in Figure14which displays model-wise double descent for different amounts of training data. However, this adjustment can sometimes result in a model that performs worse than a model trained on a smaller set of training samples. A similar phenomenon was demonstrated in[10]in the supervised setting.

We also investigate how gradually increasing the number of training samples affects the test loss curve. Remarkably, we identify a non-monotonic trend in the test loss curve at Figures15(b),15(c),15(d),15(e), which sometimes results in double descent as noticed in15(a). The emergence of non-monotonic behavior is defined by a phase where an increased number of samples negatively impacts performance, resulting in higher test loss. Figure15showcases only the results from the linear subspace dataset due to the insufficient amount of samples in the single-cell RNA dataset.
The impact of the number of noisy samples and features, the SNR, and the domain shift on the test loss is consistent with the analyses conducted in Sections4.1and4.2.

SECTION: 5Real World Applications

In this section, we demonstrate how our findings can be applied to important tasks in machine learning, such as domain adaptation and anomaly detection. Our objective is to emphasize the significance of model size selection rather than to compete with state-of-the-art techniques.

SECTION: 5.1Domain Adaptation

Many frameworks in machine learning are exposed to domain shifts as discussed in Section4.1. The difference in distribution between the training and testing data can lead to inferior results when the model is employed on new, unseen data. Over the years, numerous domain adaptation methods have been proposed for both supervised and unsupervised settings[33,34,35,36,37]to minimize the shift between the source and target domains. This is an ongoing challenge in biology, where researchers attempt to integrate datasets collected under different environmental conditions that cause distribution shifts. Numerous studies have been conducted to develop strategies to mitigate this shift, known in biology as "batch effect"[32].

In this section, we study the relation between model size and its ability to alleviate distribution shifts in real world single-cell RNA data3.2. UMAP representations[38]of the
source and target datasets are presented Figure16.
The top two sub-figures in Figure17present the test and train losses, respectively, for models trained on source and tested on target datasets. We observed that ’Wang’ dataset results in triple descent, while all other targets result in double descent curves.

To evaluate how different models perform in terms of domain adaptation, we measure how much of the domain shift was removed by analyzing the bottleneck representations of the AEs. Precisely, we compute thenearest neighbors of each bottleneck vector and determine the proportion belonging to the same biological batch as mentioned in[39], Section 3. We call this metric “k-nearest neighbors domain adaptation test" (KNN-DAT), indicating the extent of mixing between the source and target data. KNN-DAT close to 1 implies complete separation, while a lower value indicates better mixing of different domains. That is, lower values of KNN-DAT imply that the embedding of samples from the target domain is more similar to the embedding of samples from the source domain.

In the bottom row of Figure17we present the UMAP representations extracted based on the embeddings of the learned AEs. For small models, KNN-DAT results are better compared to the interpolating models. However, they achieve lower KNN-DAT at the expense of learning the source data inadequately (high train loss), which is the primary objective of the learning process. The interpolating models fit the source data, preventing them from learning any shared representations of the different batches, resulting in high KNN-DAT.
We find that over-parameterized models yield the best KNN-DAT results, achieving a score of 0.75 with a hidden layer size of 2000. Over-parameterized models also lead to reduced train and test losses, resulting in improved reconstruction of both the source and target data.
This suggests thatover-parameterized models facilitate the transition between source and target datasets, serving as a viable domain adaptation strategy. We also display results for the linear subspace dataset in AppendixD.

SECTION: 5.2Anomaly Detection

Unsupervised anomaly detection is a vital task in machine learning[40,41,42,43], which finds numerous applications across all scientific fields. Several studies have used under or over complete AEs for detecting anomalies.
We incorporate real-world data to investigate anomaly detection across various model sizes. Specifically, we leverage the CelebA attributes dataset used in[44], comprising over 200K samples and 4,547 anomalies, each characterized by 39 attributes. We conducted an experiment similar to the one in Section4.1to investigate how the size of a model affects its ability to detect anomalies. As expected, in line with findings from[44], small models outperform larger models in anomaly detection (see the decrease in ROC-AUC in Figure18). In contrast to the experiment in Section4.1, we do not control the SAR value in this data. Since the SAR is positive, we do not observe a double descent in the test loss curves. Nonetheless, we identify a non-monotonic behavior of the ROC-AUC curve. Initially, it decreases for intermediate models, followed by an increase for over-parameterized models. In conclusion, when employing a model for unsupervised anomaly detection,it is recommended to avoid selecting intermediate models, as their anomaly detection performance is inferior to under and over-parameterized models.

SECTION: 6Conclusions

In our study, we identified various instances of multiple descents and non-monotonic behaviors in unsupervised learning. These phenomena occur at the model-wise, epoch-wise, and sample-wise levels. We used under-complete autoencoders (AE) to investigate these phenomena and found compelling evidence for their robustness across diverse datasets, training methodologies, and experimental scenarios. We examined four distinct use cases: sample noise, feature noise, domain shift, and anomalies. Our experiments revealed multiple instances of consecutive descents, with some resulting in improved (lower) test loss. Additionally, we found a connection between the size of the model and its real-world performance. Specifically, over-parameterized models can serve as effective domain adaptation strategies when there is a distribution shift between the source and target data. In the realm of anomaly detection, we find that it is important to avoid selecting intermediate models that yield lower ROC-AUC outcomes. The limitations of our work lie in the utilization of a certain (yet, prototypical) type of fully connected AE model. Future work could focus on formulating theories to interpret the phenomena outlined in our findings.

SECTION: Acknowledgments and Disclosure of Funding

The work of Tom Tirer is supported by the ISF grant No. 1940/23.

SECTION: References

SECTION: Appendix AImplementation Details

In this section, we provide complete implementation details for all experiments conducted in the paper. Illustrations of the synthetic data generation introduced in Section3.1for the scenarios of sample noise, feature noise, domain shift, and anomalies are displayed in Figures19,20,21respectively.

Table1details the hyper-parameters for the training process and other parameters employed for the linear subspace and single-cell RNA datasets. The training optimizer utilized was Adam[45], and the loss function for reconstruction is the mean squared error, which is mentioned in this Section.

All experiments are conducted with the same multi-layer perceptron under-complete
AE. The objective is to prevent the model from learning the identity function and instead encourage the acquisition of a meaningful embedding in the latent space.
To facilitate the exploration of double descent in both bottleneck layer size and hidden layer size, we employ a simplified model consisting of a single hidden layer for both the encoder and decoder, as depicted in Figure22. The size of the model is determined by the sizes of the hidden layers and the bottleneck layer, while the width of the model remains constant.

All AEs are trained with the mean squared error (MSE) loss function:

Whereis the number of data samples,is the true value, andis the predicted value.
Due to contamination in the training dataset, the norm of train samples tends to be higher than that of the clean test samples. As the MSE loss is not scale-invariant, we opt to normalize both train and test losses only after the training process is complete, using, Whereis the mean of. This strategy enables us to continue utilizing the MSE loss function while facilitating a fair and meaningful comparison between train and test losses.

Ensuring the robustness of the findings across various model initializations and enhancing their reliability, all figures combine several results of different random seeds. The bolded curves in each figure represent the average across the results of different seeds, and the transparent curve around the bolded curve represents thestandard error from the mean.

All experiments were conducted on NVIDIA RTX 6000 Ada Generation with 47988 MiB, NVIDIA GeForce RTX 3080 with 10000 MiB, Tesla V100-SXM2-32GB with 34400 MiB, and NVIDIA GeForce GTX 1080 Ti with 11000 MiB.Each result in Figure3represents an average over 10 seed runs. The hidden layer sizes for the linear subspace data range from 4 to 500 with a step size of 4, and for the single-cell RNA data, they range from 10 to 500 with a step size of 10, and from 500 to 3000 with a step size of 50. This results in 125 and 110 models trained for each dataset, respectively. Figure3(a)presents 9 different sample noise levels, entailing the training ofmodels. Each evaluation of a specific sample or feature noise curve involves 1250 trained models, which can take from one day if trained on an NVIDIA RTX 6000 Ada Generation GPU to several days if trained on the other mentioned GPUs to obtain the results.

SECTION: Appendix BSNR Calculations

In this section, we will outline our approach for calculating the signal-to-noise ratio (SNR) for all experiments involving the addition of noise. Initially, we convert the SNR from decibels to linear SNR using the formula:

We have a closed-form equation for the linear subspace dataset to determine the scalarrequired to multiply the train samples and achieve the desired linear SNR value. We use the fact that both train and noise are sampled from an i.i.d. normal distribution and calculatefor the sample noise, feature noise, domain shift, and anomalies.

Notations:vector. Represents a vector in a latent space of size.matrix. Represents a random matrix to projectfrom adimensional space into a higher-dimensional space ().vector. Represents the noise added to a vector withdimensions.

For the scenario of sample noise, where a particular sample is affected by noise across all its features:

Isolating, we get that.

(a) Given a vectorofi.i.d. samples,.

(b) Given a matrixof sizewhere all entries are i.i.d., then.

For the scenario of feature noise, each train sample has onlynoisy features, meaning the noise vector contains values for onlyentries. Consequently,is determined by. For practitioners who want to explore the scenario involving domain shift, where the source and target are noisy, note that the matrix responsible for projectinginto a higher-dimensional space is denoted aswhereis sampled from a standard normal distributionand bothandare i.i.d. Consequently,. Substitutingwithin equation (2), we find that, leading to, therefore. In other words, since the covariance matrix ofis, we need to make sure we first normalize the matrix byto maintain the identity covariance matrix.

For other datasets, such as the single-cell RNA dataset, we normalize each sampleby its norm, and similarly normalize each noise vector, yielding:and. This ensures that the ratioequals 1. By employing equation (1), we attain the intended linear SNR factor, and then scale downby, yielding. This guarantees that the linear SNR is.

SECTION: Appendix CTrain Loss Results

In this section, we provide the train loss figures corresponding to each of the test losses mentioned in the main paper.

figurec

SECTION: Appendix DMore Results For Domain Adaptation

This section presents the domain adaptation results for the linear subspace dataset. UMAP visualization of the different domains is illustrated in Figure35and results for different model sizes are reported in Figure36.

Figure36illustrates the results based on a similar experiment conducted in Section5.1for the case of linear subspace data. As expected, the interpolating models exhibit the poorest KNN-DAT outcomes. Over-parameterized models introduce a decrease in the test loss indicating an improved reconstruction of the target data. In this scenario, we noticed that smaller models perform better than over-parameterized models based on KNN-DAT results. We think that the small size of the hidden layer (4) and the high dimensionality of the dataset (50 features) result in significant information loss in these layers. This could lead to closely clustered vectors in the embedding space, ultimately causing low KNN-DAT results. However, the high training loss for a hidden layer of size 4 indicates insufficient capacity to represent the signal, as shown by the high values of test and train losses in Figure36.