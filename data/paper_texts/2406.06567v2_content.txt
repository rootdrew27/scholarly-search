SECTION: : Learning Decoupled-Head Attention from Transformer Checkpoints via Adaptive Heads Fusion
Large language models (LLMs) with billions of parameters demonstrate impressive performance. However, the widely used Multi-Head Attention (MHA) in LLMs incurs substantial computational and memory costs during inference.
While some efforts have optimized attention mechanisms by pruning heads or sharing parameters among heads, these methods often lead to performance degradation or necessitate substantial continued pre-training costs to restore performance.
Based on the analysis of attention redundancy, we design a Decoupled-Head Attention (DHA) mechanism.
DHA adaptively configures group sharing for key heads and value heads across various layers, achieving a better balance between performance and efficiency.
Inspired by the observation of clustering similar heads, we propose to progressively transform the MHA checkpoint into the DHA model through linear fusion of similar head parameters step by step, retaining the parametric knowledge of the MHA checkpoint. We construct DHA models by transforming various scales of MHA checkpoints given target head budgets.
Our experiments show that DHA remarkably requires a mere 0.25% of the original model’s pre-training budgets to achieve 97.6% of performance while saving 75% of KV cache. Compared to Group-Query Attention (GQA), DHA achieves a 5training acceleration, a maximum of 13.93% performance improvement under 0.01% pre-training budget, and 4% relative improvement under 0.05% pre-training budget.

SECTION: Introduction
Transformer-based large language models (LLMs) shine in various natural language tasks due to their powerful understanding and generation capabilities. Multi-Head Attention (MHA) is widely used in LLMs, with the number of heads increasing as the model size grows. However, MHA inference overhead increases linearly with the expansion of the context and model sizes, due to the surprisingly large memory consumption of themechanism. For instance, a 7 billion-parameter model
with 32 heads and 32 layers, an input batch size of 4, and a sequence length of 32k results in 64GB of KV cache, which islarger than the model weights.

To reduce computational and memory overhead during inference, a widely used approach involves adapting the MHA model to a more efficient structure through the reuse of parameters across multiple heads, such as Multi-Query Attention (MQA)and Grouped-Query Attention (GQA).
These methods utilize a portion of the original training computation which avoid information loss due to training-inference inconsistencies, a common issue in pruning-basedworks. However, the training computation is prohibitively expensive for recovering the model’s performance, due to the information loss in the parameters when creating the initial point.

Thus, in this work, we seek to address the following question:

How can we construct amodel while keeping?

With the limited understanding of parameter characteristics in modern LLMs, we first perform an empirical analysis from the perspectives of heads’ parameter similarity. We observe that there are some head-clusters with high internal similarity in MHA checkpoints. Similar head clusters imply a enormous redundancy in MHA, which coincides with the sparsity found in previous studies.
In particular, the clusters of key heads and value heads across different layers show adistribution, meaning that there is a significant variation in the distribution of head-cluster similarities across layers, key heads and value heads, as illustrated in Fig.,.
Intuitively, we can prune redundant heads based on the above characteristics. Nonetheless, each head has its unique role, and thus no heads should be arbitrarily discarded. Furthermore, we find that linear fusion based on multiple similar heads can reconstruct the original head functionality without causing a significant performance drop (see Sec.). Based on this observation, we believe that selectively fusing corresponding heads in clusters can construct a more efficient architecture with minimum loss.

In this paper, we propose, an efficient attention architecture developed through theof checkpoints’ parameters. Recalling the decoupled heads parameter characteristics, DHA allocates different numbers of key heads and value heads at different layers to balance model efficiency and performance. The MHA checkpoint can be rapidly transformed into DHA with three stages:,, and.
During the Search stage, we group similar functional heads together and determine reasonable allocations of key heads and value heads for each layer. Specifically, we reconfigure the original key and value head into multiple linear combinations of heads within the same layer. Thus, we can allocate the heads based on the loss after replacement.
In the Fusion stage, we perform linear fusion on similar heads, ensuring the preservation of original functionality. Leveraging the Augmented Lagrangian approach, the Fusion operator initializes from MHA and explores possible head combinations in the early training, followed by refined intra-group head fusion in the later.
Based on well-trained operators on unlabeled data, we can rapidly obtain high-performing initial points for DHA from MHA checkpoints, requiring only a minimal amount of Continued Pre-training to restore performance.

To verify the effectiveness, we construct DHA on models of different sizes, such as LLaMA2-7B, Sheared-LLaMA-2.7B & -1.3Bwith the heads budget ratio set at 50% and 25%.
With a modest fusion training of just 0.2 billion tokens,learns sufficiently competent initial points. As the continued pretraining progresses, DHA continuously outperforms GQA narrowing the gap with MHA on 9 representative downstream tasks.only requires 0.25% of MHA pre-training budget. Meanwhile,is capable of reducingby up to 75% compared to MHA with minimal accuracy trade-off (maximum of 5.6%). Compared to GQA, DHA achieves a 5training acceleration, a maximum 13.93% performance improvement under 0.01% pre-training budget, and 4% relative improvement under 0.05% pre-training budget.
Overall,exhibits great performance and efficiency, which can be quickly adapted to various existing MHA Transformer models.

SECTION: Background
Letdenote the input prompts of hidden states of a Transformer layer, wherestands for the number of tokens andfor the hidden state dimension.

MHAperforms the attention withdifferent heads. For-th head, different weight matricesare used to project the input sequence into query, key, value vector,
whererepresents head dim. Denote softmax funcion as, we have:

Ultimately MHA combines heads’ outputs through the output projection.

To accelerate inference, MQAand GQAhave been proposed based on the idea of reusing head parameter weights. In these variants,different query heads are divided intogroups, where the heads within the same group share the same key heads and value heads parameter matrices.
Given the mapping relationship from the-th query head to a GQA key and value heads using the many-to-one function, we define the-th head forward pass as:

Here,refers to MHA key/value heads parameters within the-th group during GQA initialization. When transitioning from an MHA checkpoint, GQA uses the mean pooling method for heads within the group. MQA is a special case of GQA where.

Due to mean pooling for initialization, GQA results in loss of capability when converting from MHA, necessitating expensive pre-training to recover. We aim to identify better initialization and more refined head mapping relationships to achieve superior performance with reduced training costs.

SECTION: Observation
To study the inherent characteristics of head parameters in MHA, we use Centered Kernel Alignmentto calculate the heads’ similarity within each layer’s. Based on the average heads’ similarity, we define the redundancy of each MHA layer. For details, please refer to Appendix.

From Fig.and Fig., we observe that clusters form spontaneously among heads, with high similarity within clusters and low similarity between clusters.
It indicates that heads among different clusters may have distinct functionalities, processing linguistic features in various aspects.

Given the numerous similar head clusters inand, we identified the opportunity to linearly fuse functionally similar heads within clusters while retaining each head’s parameterized knowledge.
We conducted an empirical study, transforming the parameters of Head 0 in MHA into a linear fusion of the parameters from Heads 0, 1, 2, and 3. We share the fusion head across four query heads and progressively optimize the fusion ratio under the LmLoss. For details, please refer to Sec.. As shown in Fig., the loss remains unchanged as the proportion of Head 0 decreases and only increases when four heads parameters’ ratios approach an even distribution. It suggests that fusing similar parameters can reduce the number of heads without significant information loss.

The distribution of similar head clusters varies between different layers. As illustrated in Fig.,, the 0th layer of MHA shows few similar head clusters, while the 21st layer exhibits many. Within the same layer, value heads exhibit more clusters and higher similarity compared to key heads, indicating a divergence between the two.
Fig.shows that the redundancy is lower in the initial and final layers, and higher in the middle layers. Moreover,redundancy significantly exceeding that of.

Inspired by layer and key-value head variability, we propose allocating more heads to layers with lower redundancy to enhance learning and expression. Sinceshows higher redundancy than,
we can decouple and allocate more heads budget to critical key components, while compressing redundant value heads at a higher compression rate. Finer grouping and sharing based on the parameters function may contribute to compression rates and performance improvements.

SECTION: Method
In this section, we propose a more efficient Decoupled Head Attention () architecture and its construction process. We define DHA in Sec.and Adaptive Head Fusion algorithm in Sec.. Then we demonstrate the adaptive construction based on the MHA checkpoint, which can be divided into:,, and(Discussed in in Sec.).
Finally, we introduce practical application of ourarchitecture on the LLaMA2 model in Sec..

We present a more efficient attention architecture called Decoupled-Head Attention (DHA). Based on observed significant functional differences among different layers’ key value heads,adaptively allocates more heads to critical components, thus enhancing overall model efficiency and performance.

Defined model withlayers andheads in a layer, the numbers of Key heads and Value heads in the-th layer are denoted as.
We define themapping functionsandrepresenting key and value head corresponding to the-th query head in-th DHA layer.
The computation be formalized as follows:

DHA shares a key and value head in multi query heads’ computation based on independent mapping functions at different layers. GQA can be considered a special case of DHA, where not only all layers share the same mapping functions, but the mapping functions for keys and values are identical.

Due to the high cost of building an efficient Attention mechanism in LLM from scratch, we construct DHA based on the existing MHA checkpoint using minimal computational budgets.
Based on the head clustering phenomenon in MHA, we propose a linear fusion method for similar heads within clusters. By incrementally fusing head parameters, we compress the number of heads while retaining the original model’s knowledge, significantly reducing training budgets and improving performance.

Formally, we define a model with Layer numberand Head numberas, wheredenotes the weight of layerwith input and output dimension. In the initialization, our goal is to transfer knowledge from a MHA modelto a DHA model, where.
By learning a fusion operation that minimizes the functional difference between MHA and DHA model, the goal can be formalized as

Whereis the fusion operator,is the training dataset,is the training loss function,measures the transformation from MHA to DHA, andis the learnable scale factor.

During DHA initialization, the fusion operatorconstructs new heads based on the linear combinations of the original key and value heads within the group, and shares the new heads among the query heads’ forward. Define each grouprepresents key, value heads group corresponding to the-th query head in-th layer,as the group size.
By introducing variablesrepresents the proportion of-th key, value head involved in the-th query head forward within group. For each group, a head have forward pass as:

wherewill be initialized to Kronecker delta function, which equals 1 if and only if, and equals 0 otherwise. Under this initialization setting, the forward computation of DHA is completely equivalent to that of MHA, see Fig..

During the optimization phase, we design a fusion loss to optimize the initialized model towards DHA target architecture.
Note that after initialization, the mapping of heads within the groupis amapping, denoted by the function.
This indicates that in the forward process of each query, the key head or value head can be expressed as different linear combinations ofMHA heads. According to Eq., we aim to achieve amapping that a single fused key head or value head are shared across multiple query heads in DHA, denoted by the function. Thus, we design a fusion lossto optimize the initial mapping functions to converge to a single mapping function, i.e.,.
Specifically, we define the optimization objective as minimizing the difference between the mapping functions of different query headsandwithin the-th layer and-th group:

whererepresents the number of heads within a group. Sincecan be regarded as an orthogonal scalar, and thus we only need to optimize fusion variables, so we have:

Whererepresents the number of groups,. The fusion loss can be measured as the mean squared error loss of the head and head fusion variables within each group at each layer.

When the fusion loss is zero, the key and value heads corresponding to query heads within the group are optimized to share the same fusion variables. This allows the new DHA key-value head parameters to be effectively shared among the queries in the group. Given that it is challenging to optimize the loss to a very small value, we use an augmented Lagrangian approachfor incremental architectural transformations. Defineas the target loss,as the base decay factor,as the current global step,as the total number of steps in the warm-up phase, the overall training optimization is an adversarial game:

Our Augmented Lagrangian approach enforces the constraint, where the Lagrange multiplieris updated during training. The update increases the loss unless the constraint is satisfied. Early in training, the model tolerates more significant discrepancies between head weights, promoting exploration. As training progresses, the margin shrinks, enforcing stricter adherence to minimizing discrepancies and refining head alignment within the group.

Based on the observation of similar head clusters and key-value head parameter variability across layers, DHA employs the adaptive transformation. It allows DHA to search for and fuse similar heads while allocating different group sizes across layers. As shown in Fig., the transformation can be divided into three stages:,and.

In the beginning, we initialize the DHA operators to the MHA model. Next, we perform 240steps, calculatingfor each layer andfor all heads. Based on the, we perform head grouping intending to minimize the average loss of heads within each group and maximize the average loss of heads between groups and groups. Based on, we use a dynamic programming algorithm to allocate more head budget to layers with higher loss within a total budget. It allows us to fuse the most similar heads to minimize loss during the fusion process and selectively compress the model’s most redundant components. For more details, see Apendix,.

Duringphase, we modified the forward propagation path of MHA in the form of DHA based on the layer head budget and head grouping obtained during thephase. Then we antagonistically optimize the fusion operator and update Lagrangian multipliers, thethat marks this DHA fusion process decreases. Whenis less than 1e-3, we terminate the fusion algorithm and enter thephase.

During thephase, we fuse MHA head parameters based on averaged fusion weights to construct DHA initialization. DHA initialization can recover the performance with a small amount of restorative pre-training. For more information, please refer to Appendix.

Our method can theoretically transform MHA architecture in any transformer model to efficient DHA architecture. Using LLaMA models as case studies, we implemented DHA transformation with various compression rates on all MHA layers. Notably, we expanded the dimension of each head’s fusion coefficientfrom 1 to the head’s dimension, allowing for finer-grained parameter fusion and better knowledge retention. Intuitively, we learn different fusion ratios for each dimension of the head. Only a very small number of additional parameters need to be introduced, DHA significantly accelerates training and improves performance.

SECTION: Empirical Evaluation
To trainoperators and extend pre-training, we employ the RedPajama, which parallels the LLaMA training data across seven domains: CommonCrawl, C4, GitHub, Wikipedia, Books, ArXiv, and Stack-Exchange. This dataset comprises a validation set with 2 million tokens, a training set containing 4 billion tokens and an additional pre-training set totaling 50 billion tokens.

Our experimental framework utilizes the Sheared-LLaMA codebaseimplemented on the Composer package, and is executed on 8 NVIDIA A100 GPUs (80GB). The models are trained with a sequence length of 4096, employing a global batch size of 64 during the fusion phase and 256 during the continued pre-training phases.
The learning rates were set at 1e-4 for language modeling loss, and 1e-2 for Lagrangian multipliers and fusion operators respectively.

models were trained for 1000 steps (0.2B token budget) during the fusion phases. For the continued pre-training, we trained both baseline models andfor up to 50000 steps (50B token budget). To evaluate the training acceleration capability of DHA, we evaluate its performance under two budget scenarios. First, we set a budget ofto compare the early-stage rapid convergence capabilities ofand GQA. Then, we set a budget ofto further assess the performance ofover a more extended training period.

We employed the lm-evaluation-harnessto evaluate our models. For common sense and reading comprehension tasks, we report 0-shot accuracy results for SciQ, PIQA, WinoGrande (Wino.), ARC Easy(ARC-E.), and HellaSwag (HellaS.), alongside 25-shot accuracy for ARC Challenge (ARC-C.). In the assessments of continued QA and text understanding, we report 0-shot accuracy for LogiQA, 32-shot BoolQ, and 0-shot LAMBADA. All reported results were calculated with the mean and stderr of multiple experiments.

To assess our models’ capabilities after instruct tuning, we fine-tune bothand baseline models on 10,000 instruction-response pairs from the ShareGPT datasetand evaluate on another 1,000 instructions, using GPT-4 for response evaluator. The win rate of our model relative to the baseline is reported. For detailed information, refer to Appendix.

We selected the LLaMA2-7B model and Sheared-LLaMA-2.7B&1.3B (S.-LLaMA-2.7B&1.3B) as the MHA baselines. For each scaled model’s checkpoint, we constructed 25% and 50% compressed GQA and DHA models in 0.5B & 1B tokens (0.01% & 0.05% of pretrain budget).

Tab.shows the foundational capabilities of DHA and GQA models at 50% and 25% compression rates (e.g., 64 key value heads compress to 16) across different scales. DHA was obtained by transforming LLaMA using adaptive head fusion and then further pre-trained with 1B tokens. For comparison, we constructed GQA with the same compression rates and training budget. Experiments show that DHA can achieve efficient architecture with only 0.05% of the original model’s pre-training cost without significant performance loss. Compared to GQA, DHA consistently achieved better performance across all model scales and pre-training cost settings. Under the same checkpoint and training budget settings, DHA demonstrates significant improvements at higher compression rates. For example, with LLaMA7B at a 25% compression rate, DHA achieved a 4% relative performance improvement over GQA. This showcases DHA’s fusion algorithm’s ability to efficiently retain knowledge at high compression rates and the advantage of DHA’s decoupled architecture in adaptively compressing redundant components. Possibly due to the lack of relevant data, DHA performed on par with LogiQA. As shown in Fig., DHA’s performance advantage becomes more remarkable with reduced training budgets. It indicates that DHA effectively retains knowledge of larger models, significantly reducing pre-training costs.

We examined whether DHA offers a better initialization point than GQA by pre-training both DHA and GQA models on the original RedPajama dataset. Fig.shows that the initial loss of the GQA model is high and decreases slowly. In contrast, the DHA model starting from MHA exhibits a minor increase in LM loss as fusion progresses, maintaining a consistently lower loss. DHA converges with just 0.1B data, demonstrating a 5training speedup compared to GQA. Fig.reports the average downstream task accuracy ofand GQA during continued pre-training.achieves comparable performance to GQA’s at 1B tokens with only 0.2B tokens, outperforming GQA’s 0.2B token performance by 13.93%. This demonstrates’s effectiveness in retaining parameter information. Ultimately, DHA achieves a higher performance ceiling than GQA due to retaining information from the original model and its more efficient architecture, whereas GQA loses information during initialization.

We report the effects of ablating Linear Heads Fusion and Adaptive Transformation
in Tab.. When training with less data (0.5B), ablating Linear Heads Fusion leads to significant performance degradation, indicating that this method preserves crucial knowledge in LLMs, greatly accelerates DHA model training, and enhances performance. Adaptive Transformation allocates parameters more efficiently during construction, thereby strengthening the model’s capability and reducing training difficulty. When we allocate more training budget to 1B, DHA’s efficient architecture after Adaptive Transformation plays a more significant role, enhancing the model’s performance ceiling. When continuing to pre-train the DHA model, it demonstrated strong learning capabilities and sustained performance improvements, ultimately achieving 97.6% of the performance with just 0.25% of the original model’s pre-training budget, while saving 75% of the KV cache.

Allocating more computation to the fusion phase aids in better retention of information within the checkpoint. Our experiments assessed the effects of budget allocations between the fusion and CT phases within. Tab.shows that increasing the fusion budget consistently from 0.05B to 0.2B improves model performance at the initialization point. Training with just 0.1B data is sufficient to achieve a good starting point, and increasing fusion budget will not affect the final performance. This experiment also demonstrates the necessity and effectiveness of the fusion stage under low-resource conditions. When we have a larger training budget, we can allocate more resources to the fusion stage to achieve a better initialization point for.

We investigated how the model adaptively allocates decoupled head group sizes across different layers under global head budgets. As illustrated in Fig., the head numbers of DHA layers decrease from higher to lower across layers. Deeper layers exhibit higher compression rates due to greater redundancy. However, the initial and crucial layers need more heads, suggesting they may have specialized functions. As shown in Fig., we presented the LM loss for the cold-start training of DHA models initialized with parameter averaging under different DHA configurations obtained at various search steps. Despite using the same initialization method as GQA, DHA exhibits a faster loss decline and a lower final loss. This indicates that DHA’s architecture can accelerate training and achieve better performance, even without Linear Heads Fusion method.

For interpretability analysis, we visualized the parameter characteristics of the post-fusion DHA model in Fig.(detials in Appendix), and compared them with those prior to fusion. The DHA parameter distribution shows consistency with MHA’s. This indicates that DHA effectively aggregates multiple similar functional heads within clusters and new fused heads successfully reconstruct the functionalities of multiple origin heads in MHA. It is noteworthy that the significant reduction in the number of similar heads within the DHA architecture indicates that our method effectively reduces redundancy among the heads.

SECTION: Related Work
Some efforts have been converting the traditional Multi-Head Attention (MHA)to Multi-Query Attention (MQA), Group-Query Attention (GQA)or GQKVA. These methods achieve a balance between performance and efficiency by reducing the number of head parameters through parameter reuse across grouped heads.is inspired by these methods and has a much higher optimization rate and much less training overhead.

In recent years, the ability of incremental training to accelerate large-scale model training by studying how to obtain the optimal initialization point for training has thus attracted much attention.
Net2Netuses function-holding transformations to expand the width by duplicating neurons, and uses a unitary layer implementation to expand the depth. LiGOproposes a learnable expansion method that can be used at the initial initialization point of a transformer.is inspired by these methods, but we investigate how to learn to map the parameter matrix from large to small without losing the ability of the larger model itself. For additional related work, please refer to Appendix.

SECTION: Conclusion
In this paper, we propose an efficient attention architecture and a method for fast converting an MHA checkpoint into an efficient structure. By grouping similar heads and performing controlled linear fusion, we develop an initial DHA architecture that decouples head components at various layers, reducing training overhead while maintaining performance. Experimental results show that our method preserves the knowledge of the original model, improving training acceleration, inference efficiency, and computational cost savings. This transformation paradigm offers research value and potential for broader application with minimal performance loss and reduced computational effort.

SECTION: Acknowledgments
We would like to thank Yinqi Yang, Jiawei Sheng, Xinhua Zhang, Shicheng Wang, Chuanyu Tang and members of the IIE KDsec NLP group for their valuable feedback and discussions.
We are very grateful to Mengzhou Xia for providing the concise and effective ShearingLLaMA experimental code and for her assistance during the reproduction process. Work done during Yilong Chen’s internship in Baidu Inc. This research is supported by the National Key Research and Development Program of China (grant No.2021YFB3100600) and the Youth Innovation Promotion Association of CAS (Grant No. 2021153).

SECTION: References
SECTION: Appendix
SECTION: Extended Related Works, Discussions, and Limitations
EfficientTransformershave been extensively exploredto address the self-attention operation which scales quadratically with the sequence length. For instance, Sparse Transformeruses a dilated sliding window the reduces the attention complexity. Longformerand Bigbirdreduced the complexity of self-attention by combining random, window and global attention. Recurrence Transformersmaintain a memory bank of past KV cache to process the long text in segments. However, the above methods either result in a loss of model performance or require retraining the model, which is unaffordable for the high computational resources of LLMs.requires very little computation to transform checkpoints into an efficient architecture that balances performance and computational resources.

KV Cache Compression methods emerged for reducing the prominent inference bottleneck caused by KV cache, particularly for long content input. A series of methodsexplored the sparsity among Transformer’s attention block, then evicted unnecessary tokens from KV Cache for efficient inference.
However, these methods discard information from the context and use algorithms for inference that are inconsistent with the training phase, which can cause model performance degradation.does not need to discard information from the context and is able to maintain consistent performance for training and inference.
Pruning, quantizationand distillationcan reduce the number of model key and value headers, parameter dimensions, and activation to reduce memory bandwidth overhead during model inference. Deja Vuand CHAIprune pruning redundant heads through clustering methods for efficient inference. In the LLM era, this leads to a significant reduction in neuron redundancy as models move from task-specific to generalized. The application of these methods to LLMs is computationally expensive and leads to performance degradation at larger pruning magnitudes.

Our approach is dedicated to obtaining a high-performance lightweight language model, which is the same goal as the task of model compression. Quantizationreduces the numerical accuracy of model weights and activations, and speeds up training and inference, but results in a loss of model accuracy and the inability to freely build target-specific models. CRashand LayerDropmethods discard ineffective layers during training, which do not allow for target-specific structuring and come with a large performance loss. Pruningminimizes the impact on performance by cutting out redundant neurons that over-parameterize the model. In the LLM era, this leads to a significant reduction in neuron redundancy as models move from task-specific to generalized. Pruning LLM leads to performance degradation at larger pruning magnitudes. LLMsheairnguses the results of pruning as initialization for continuous pre-training of the model to recover performance, but this approach requires more data and computational overhead. We avoid the information loss caused, by learning the parameter fusion matrix of the model to reach a specific structure, thus obtaining better initialization points and reducing the overhead of continuous pre-training.

In this paper, we observe the MHA head mechanism and report the phenomenon of modular clustering of heads in MHA. This paper innovatively proposes linearly fusible parameters within the model, and designs linear fusion operators and related experiments to verify the low-loss fusible nature of the parameters. This helps to advance parameter fusion theory and LLM interpretability studies, which provide a foundation and inspiration for future algorithmic advancements, encouraging further optimization and innovation in LLMs. Our work on Decoupled-Head Attention (DHA) represents an advancement in optimizing the efficiency of Large Language Models (LLMs). By addressing the substantial computational and memory costs associated with the widely used Multi-Head Attention (MHA), DHA enhances the applicability of LLMs in various domains. The introduction of DHA not only achieves a remarkable balance between performance and efficiency but also significantly reduces the need for extensive pre-training, making the deployment of LLMs more feasible and cost-effective. This efficiency allows for the broader accessibility of advanced LLMs, democratizing technology and fostering innovation across industries. Furthermore, by requiring only 0.25% of the original model’s pre-training budgets to achieve near-original performance while saving 75% of KV cache, DHA contributes to significant energy savings, aligning with sustainable and environmentally friendly AI practices. The enhanced performance and reduced training costs accelerate the development of AI applications, enhancing productivity in fields such as natural language processing, healthcare, and finance.

There are two limitations to our current approach. Firstly, we have only utilized linear methods for parameter fusion in our model. Future research should explore nonlinear methods, as they may offer a better way to link different parameters and achieve optimal results. Secondly, due to computational resource constraints, we have only experimented with models of 7 billion, 3 billion, and 1.3 billion parameters. However, our method is scalable and can be extended to models of any size in future work.

In our study, we utilize publicly available data and techniques to address privacy concerns. Our approach focuses on improving model parameter efficiency and reducing model size to develop robust, compact, and accessible models, thus promoting the open dissemination and democratization of NLP technologies. By implementing pre-training strategies, we aim to mitigate biases through comprehensive training on large datasets, contributing to ethical AI development that prioritizes transparency, efficiency, and bias reduction. Our work is dedicated to advancing accessible and efficient NLP technologies, fostering a more inclusive and automated future for AI.

SECTION: More Implementation Details
Centered Kernel Alignment (CKA) is a statistical measure used to quantify the similarity between two sets of data representations. Unlike traditional correlation measures, CKA is designed to be invariant to orthogonal transformations and scaling of the data.

To calculate the similarity between two sets of representations using CKA, we employ a kernel function to map the original data into a higher-dimensional space, where the alignment of their central tendencies can be more easily measured. The CKA value ranges from 0 to 1, where 0 indicates no similarity and 1 indicates identical representations.

The mathematical formulation of CKA, when using a linear kernel, is given by the following equation:

Here,andare matrices whose columns are the vectors of the representations to be compared,denotes the Frobenius norm, andandare the transposes ofand, respectively. To mathematically define the redundancy of each layer based on the average similarity between heads, we follow these steps:

Compute the similarity between heads: For each pair of heads within a given layer, calculate the similarity using the CKA formula.

Compute the average similarity: Average the similarity scores of all pairs of heads to define the redundancy of the layer.

Consider a layer withheads, where the parameters of each head are represented by the matrices(e.g.,for query weights). For each pair of headsand, compute the CKA similarity using the following formula:

Calculate the similarity for all pairs of heads and then compute the average similarity:

The coefficientensures that the average similarity is computed over all pairs of heads. This redundancy measure reflects the degree of similarity between the parameters of different heads within each layer. A higher redundancy indicates that the parameters of different heads are more similar, implying a higher level of redundancy.

Our method can theoretically transform MHA architecture in any transformer model to efficient DHA architecture. Using LLaMA models as case studies, we implemented DHA transformation with various compression rates on all MHA layers. Only a very small number of additional parameters need to be introduced, DHA significantly accelerates training and improves performance.

DHA adaptively gives search heads and heads connectivity relationship with redundancy in each MHA layer. Thus DHA assigns different group sizes at different layers and aggregates similar heads into one group to speed up fusion and reduce knowledge loss due to noise in fusion. As Shown in Fig, the transformation process of MHA to DHA can be divided into three stages.

In order to keep the performance of the DHA model at the fusion start consistent with the MHA model, we initialize the operators of the DHA model to the MHA model with the corresponding scaling factors of query-key, query-value set to 1, and the corresponding scaling factors within the rest of the groups set to 0. At the beginning of every fusion process (e.g.), the algorithm first performs multiple STEPs constrained only by theconstraints to propagation, computing thebut not optimizing the linear fusion operator based on it. Based on thebetween head and head as a measure of the distance between head and head we perform head clustering with the goal of minimizing the average loss of heads within each group and maximizing the average loss of heads between groups and groups. Afterwards, we select multiple groups with the smallest loss based on the compression rate as the fusion target, and optimize theirfor back propagation. This algorithm ensures that the most redundant components of the model are fused and compressed during each transformation, while components requiring more parameters retain their original properties.

Our approach is theoretically applicable to transforming parameters across various transformer model designs, focusing on preserving the knowledge within MHA parameters.

Using LLaMA models as a case study, we implement our DHA transformation on all MHA layer. The whole transformation process can be divided into two phases: the Fusion phase with a small training budget and the recovery phase with continuous pre-training. Before Fusion phase, we define the total number of compressed headers budgetthenis split into compression rates at different compression levels. During Fusion phase, we modified the forward propagation path of MHA in the form of DHA refer to Eq.and optimizerefer to Eq.. At the beginning, the fusion operators of each layer will be initialized making the DHA and the original MHA functionally equivalent. As we antagonistically optimize the fusion operator and upadte Lagrangian multipliers, thethat marks this DHA fusion process decreases.

Whenis less than 1e-3 we terminate the fusion algorithm and enter the post-processing phase. The fusion weights within each group are computed by averaging the weights corresponding to each query-key and query-value within the group.We construct new DHA heads’ parameters from the original MHA heads based on the fusion operator. After that, the fused model parameters can recover the performance and complete the transformation with a small amount of restorative pre-training.

We implemented the DHA algorithm with different compression ratios on models of different sizes. Experiments show that the DHA algorithm is adapted to models of various sizes. Only a very small number of additional parameters need to be introduced, and DHA preserves parameter knowledge in the model and improves performance.

In the module initialization process, the input key and value tensors are first reshaped and grouped according to the number of key and value heads, respectively. Given the batch size (bsz), number of heads (num_heads), key length (k_len), and head dimension (head_dim), the key tensor is reshaped into keys_grouped of shape [bsz, num_key_heads, num_heads // num_key_heads, k_len, head_dim]. Similarly, the value tensor is reshaped into values_grouped of shape [bsz, num_value_heads, num_heads // num_value_heads, k_len, head_dim]. These grouped tensors are then expanded by repeating them along the group size dimension, resulting in keys_expanded and values_expanded. Correspondingly, the weight tensors weights_k and weights_v are reshaped to match the expanded dimensions and are then multiplied element-wise with the expanded key and value tensors.

During the forward pass, the reshaping and expansion of the key and value tensors are performed in a similar manner as in the initialization process but with parameters specific to the DHA fusion phase. The key tensor is reshaped into keys_grouped of shape [bsz, dha_warmup_group_num, num_heads // dha_warmup_group_num, k_len, head_dim] and the value tensor into values_grouped of shape [bsz, dha_warmup_group_num, num_heads // dha_warmup_group_num, k_len, head_dim]. These grouped tensors are then expanded by repeating them according to the dha_warmup_group_size. The weights weights_k and weights_v are reshaped and expanded to align with the dimensions of the expanded key and value tensors. Element-wise multiplication is performed between the expanded tensors and their corresponding weights, and the resulting weighted tensors are summed along the appropriate dimension.

The calculation of the loss function in this model involves the adaptive DHA loss. This loss is computed based on the global step, warmup steps, and a base value. The DHA margin is calculated as the product of an exponential decay term and a linear decay term, ensuring it is non-negative. The adaptive DHA loss is derived by comparing the mean squared error (MSE) with the DHA margin and summing the positive differences.

Formally, the DHA marginis calculated as:

MSE Loss are defined in Eq.. The adaptive DHA lossis then:

The overall lossis the adaptive DHA loss:

This combined loss function effectively utilizes the adaptive component to optimize the attention mechanism in the model. The calculation process ensures that the model adapts dynamically during training, reducing the loss progressively as the training steps increase.

This algorithm uses simulated annealing to optimize group scores based on a given score matrix. It begins by defining the number of groups and distributing the points among them randomly. The initial score for these groups is calculated using the ‘calculate_score‘ function, which sums the scores from the matrix for each group, considering each connection twice and dividing by two.

The algorithm starts with a high temperature (T=100) and gradually cools down (T_min=0.001) using a cooling rate (alpha=0.9). During each iteration, two random points from different groups are swapped, creating a new grouping. The score for this new grouping is calculated, and the difference in score (delta) is evaluated.

If the new score is higher, or if a randomly generated number is less than the exponential of delta divided by the temperature, the new grouping is accepted. This allows the algorithm to escape local optima. The temperature is then reduced according to the cooling rate. This process continues until the temperature reaches the minimum threshold. The algorithm returns the final group configuration and its corresponding score, which represents an optimized grouping based on the initial score matrix.

In practice, we use the MSE computed by the head and the head as scores, and compute the matrix of scores between the head and the head for head clustering after forward.

This algorithm efficiently allocates resources to different layers based on their respective losses to optimize system performance. Initially, it assigns a minimum allocation to each layer. Then, it calculates weights for each layer based on their losses, prioritizing layers with higher losses. The algorithm determines the number of times 16 can be allocated based on the remaining allocation. It allocates 16s to layers with the highest weights until reaching a predetermined limit. Next, it redistributes the remaining allocation to layers with the highest loss-to-allocation ratios, assigning resources in multiples of 8 or 4. This process ensures that layers with higher losses receive more resources, optimizing the overall system performance. Finally, the algorithm returns the final allocation for each layer, resulting in an efficient distribution of resources across the system. The total search process for the LLaMA2 model requires 42 minutes.

The hyperparameters used in our experiments are presented in Tab.. We employ fully sharded data parallel to efficiently train our models in parallel, and we utilize FlashAttention V1to accelerate the training process. A cosine learning rate scheduler is used, with the learning rate decaying to a minimum of 10% of the peak value. Preliminary experiments were conducted to determine the optimal peak learning rate for learning the fusion variables and Lagrange multipliers.

SECTION: Extended Experiments
To assess our models’ capabilities in downstream application after instruct tuning, we fine-tune bothand the baseline models on 10,000 instruction-response pairs drawn from the initial round of multi-turn chat histories in the ShareGPT dataset. For evaluation, we select another 1,000 instructions from ShareGPT, generate responses using our fine-tuned models and other baseline models and employ GPT-4 as an evaluator to compare these responses. We report the win rate of our model relative to the baseline model.

As shown in Fig., the tunedmodel outperforms all GQA baselines of comparable scale . This demonstrates that the DHA model effectively retains the foundational capabilities of the MHA model and can be activated through instruction tuning to produce long, coherent, and informative responses.

In Sectionsand, we demonstrated thatis a more efficient GQA architecture, so it has similarly good compatibility. We tested the compatibility of the DHA model with the KVCache eviction method NACL. NACL 25% indicates retaining only 25% of the KVCache. The experiment results are shown in the Tab.. DHA and GQA exhibit equally good compatibility with KV cache compression techniques.

It’s a common and effective approach to convert MHA to GQA using mean pooling instead of training from scratch. The author of GQA tested several methods for the initialization of GQA and found it works best using simple mean pooling from MHA. Indeed, training GQA from scratch will cost trillions tokens budget to match the performance of MHA which is inefficient and costly.Inspired by the similarity of head parameters, we improved the initialization method of GQA: instead of direct grouping, we first cluster similar heads using CKA and then perform mean-pooling initialization within each cluster. We compare this approach with the Vanilla GQA and DHA.

Tab.shows that GQA(CKA-Grouping)-7B-25% (5B) achieved comparable performance to the original implementation in Vanilla GQA. We believe the reason for this is that the head grouping learned by DHA is based on the fusible nature between heads, which cannot be completely equated with CKA similarity. More importantly, DHA not only groups heads based on similarity but also learns the fusible parameters. This allows it to eliminate the influence of redundant parameters and retain more important information during the initialization process, which is not possible with mean initialization.

SECTION: Extend Analysis
Refer to Fig., where we show the weight variation diagram. In the fusion process of heads 0-3, head 0 initially constitutes 100% as the starting head of the MHA. As the fusion process progresses, the parameters of the important heads increase, and the proportions of all heads become more balanced. This indicates that the algorithm attempts to retain information from different heads by balancing the parameter proportions of each head. This process results in a slight increase in loss, but not significantly.

is primarily designed for models based on the Transformer Decoder architecture and can be adapted to all models with this architecture. We chose LLaMAas the experimental baseline because it is a classic model using the decoder architecture in LLMs. Other open-source LLM models differ from LLaMA only in certain details (such as activation functions and training methods), which do not affect’s training. Successfully applyingto LLaMA indicates that it can be used in most decoder-only models. GQAis an efficient variant of MHA, which optimizes the inference process through head grouping and sharing. Due to its simplicity and efficiency, GQA is widely used. DHA can be similarly constructed based on GQA, requiring only minor adjustments to the construction process. Here, we provide two feasible methods to convert GQA to.

Easiest method in less than 1 minute. GQA can be losslessly converted into MHA by simply replicating the GQA’ KV heads. Then, we can perform thetransformation on the MHA architecture.

Minor modification by grouping KV.only needs to group and fuse the Key and Value heads. When constructingon GQA, we initially group the Key and Value, maintaining alignment with GQA functionality. During the training phase, the fused head parameters can replace the original GQA heads for sharing.

Only intra-layer grouping and fusion is conducted in. Fig.meant to illustrate the decoupled-heads where the number of key and value heads can be different among layers. Themethod employs parameter fusion within each layer for three reasons:

Higher redundancy of heads within layer for fusion. The heads within a layer exhibit high similarity and redundancy, which provides a good starting point for parameter fusion.

More complex optimization for inter-layer fusion. The optimization process between layers is very complex and requires memory operations for cross-layer calls, which inherently increases the inference cost.

Promising future work by introducing inter-layer fusion. This paper represents an early exploration of applying parameter fusion methods within model parameters. The inter-layer fusion approach is indeed a valuable direction for future exploration.

The performance gap between the results shown in the paper and MHA is primarily due to the following two reasons:

The gap of pre-training data. The MHA model was not trained on the same data used for DHA. Since LLaMA’s training data is not directly open-sourced, we used an experimental open-sourced pre-training data following Sheared-LLaMA (Xia et al., 2024). The improved pre-training data will close the gap between DHA and MHA.

Parameter size difference. Compared to MHA, DHA compresses 50% or 25% of attention heads, requires only 0.05% of pre-training data and achieves approximately 5% loss. The number of parameters of MHA is much larger than that of DHA, so performance loss is inevitable during conversion. Compared with GQA, a strong baseline with the same number of parameters, DHA has shown higher training efficiency and performance advantages. Due to the high efficiency of DHA, DHA can use more heads than MHA with the same number of parameters, and has the opportunity to achieve better performance.

SECTION: Extend Observation
We show more of our head similarity observations in the LLaMA2-7b model MHA. Each subfigure represents the similarity between heads within the same layer for three different types of attention mechanisms: WQ (query), WK (key), and WV (value). The matrices are arranged in a 3x4 grid layout, with each row corresponding to a specific layer and each column corresponding to a type of attention mechanism. Note: Layer numbers start from 1.

The DHA parameter distribution of shows consistency with MHA’s. It indicates that DHA effectively aggregates multiple similar functional heads within clusters and new fused heads successfully reconstruct the functionalities of multiple origin heads in MHA. It is noteworthy that the significant reduction in the number of similar heads within the DHA architecture indicates that our method effectively reduces redundancy among the heads.

SECTION: NeurIPS Paper Checklist
Question: Do the main claims made in the abstract and introduction accurately reflect the paper’s contributions and scope?

Answer:

Justification: The abstract provides a concise summary of the key findings and experiment results. The introduction in Sec.outlines the research questions and objectives in paragraph 3,4 and contribution in paragraph 5.

Guidelines:

The answer NA means that the abstract and introduction do not include the claims made in the paper.

The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.

The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.

It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.

Question: Does the paper discuss the limitations of the work performed by the authors?

Answer:

Justification: The paper discusses the limitations of the work performed by the authors in detail in Appendix Appendix., highlighting two specific limitations and the broader impact.

Guidelines:

The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.

The authors are encouraged to create a separate "Limitations" section in their paper.

The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.

The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.

The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.

The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.

If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.

While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren’t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.

Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?

Answer:

Justification: This paper is mainly based on observation, making conjectures and methods and proving the effects through experiments. The paper defines the background in Sec., presents the conjecture in Sec., and provides a detailed derivation of the form and optimization process in Sec.. All assumptions made in the paper are thoroughly validated through experiments in Sec..

Guidelines:

The answer NA means that the paper does not include theoretical results.

All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.

All assumptions should be clearly stated or referenced in the statement of any theorems.

The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.

Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.

Theorems and Lemmas that the proof relies upon should be properly referenced.

Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?

Answer:

Justification: The paper provides a detailed description of the experimental data setup and hyperparameter settings in Sec.. Additionally, in Sec.and in Appendix.sections we thoroughly explain the derivation and implementation process, ensuring all necessary information for reproducing the main experimental results is disclosed.

Guidelines:

The answer NA means that the paper does not include experiments.

If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.

If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.

Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.

While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example

If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.

If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.

If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).

We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.

Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?

Answer:

Justification: The datasets, baseline methods, and models used in the paper are fully open-source and available on Hugging Face. The paper includes the key implementation steps and code in Sec.and the Appendix.. However, the complete code is still being organized and is under consideration for open sourcing.

Guidelines:

The answer NA means that paper does not include experiments requiring code.

Please see the NeurIPS code and data submission guidelines () for more details.

While we encourage the release of code and data, we understand that this might not be possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).

The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines () for more details.

The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.

The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.

At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).

Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.

Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?

Answer:,

Justification: The paper provides a detailed description of the experimental data setup and hyperparameter settings in Sec.. Additionally, in Sec.and in Appendix.sections we thoroughly explain the derivation and implementation process.

Guidelines:

The answer NA means that the paper does not include experiments.

The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.

The full details can be provided either with the code, in appendix, or as supplemental material.

Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?

Answer:

Justification: All results are averaged over multiple tests, and we report the mean accuracy along with the standard deviation (acc_norm) as a measure of error bars.

Guidelines:

The answer NA means that the paper does not include experiments.

The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.

The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).

The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)

The assumptions made should be given (e.g., Normally distributed errors).

It should be clear whether the error bar is the standard deviation or the standard error of the mean.

It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.

For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).

If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.

Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?

Answer:

Justification: In Sec., we report the GPUs we used, the memory, and detailed training information. For more information you can refer to the Appendix.

Guidelines:

The answer NA means that the paper does not include experiments.

The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.

The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.

The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn’t make it into the paper).

Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics?

Answer:

Justification: The discussion of the ethics and impact can be consulted in Appendix.. We are open and transparent throughout the study and do not design for human subjects, privacy data bias, or other issues.

Guidelines:

The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.

If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.

The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).

Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?

Answer:

Justification: The discussion of the broader impacts can be consulted in Appendix..

Guidelines:

The answer NA means that there is no societal impact of the work performed.

If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.

Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.

The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.

If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?

Answer:

Justification: This paper presents an improved approach based on the existing model architecture, but does not release any new models. The paper poses no such risks.

Guidelines:

The answer NA means that the paper poses no such risks.

Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.

Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.

We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.

Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?

Answer:

Justification: This article uses assets reasonably in compliance with the license, and the assets used are cited in the article.

Guidelines:

The answer NA means that the paper does not use existing assets.

The authors should cite the original paper that produced the code package or dataset.

The authors should state which version of the asset is used and, if possible, include a URL.

The name of the license (e.g., CC-BY 4.0) should be included for each asset.

For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.

If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets,has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.

For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.

If this information is not available online, the authors are encouraged to reach out to the asset’s creators.

Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?

Answer:

Justification: The paper does not release new assets.

Guidelines:

The answer NA means that the paper does not release new assets.

Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.

The paper should discuss whether and how consent was obtained from people whose asset is used.

At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.

Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?

Answer:

Justification: The paper does not involve crowdsourcing nor research with human subjects

Guidelines:

The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.

Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.

According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.

Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?

Answer:

Justification: The paper does not involve crowdsourcing nor research with human subjects.

Guidelines:

The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.

Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.

We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.

For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.