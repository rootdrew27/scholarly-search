SECTION: A Hybrid Deep-Learning Model for El Niño Southern Oscillation in the Low-Data Regime

While deep-learning models have demonstrated skillful El Niño Southern Oscillation (ENSO) forecasts up to one year in advance, they are predominantly trained on climate model simulations that provide thousands of years of training data at the expense of introducing climate model biases. Simpler Linear Inverse Models (LIMs) trained on the much shorter observational record also make skillful ENSO predictions but do not capture predictable nonlinear processes. This motivates a hybrid approach, combining the LIM’s modest data needs with a deep-learning non-Markovian correction of the LIM. For O(100 yr) datasets, our resulting Hybrid model is more skillful than the LIM while also exceeding the skill of a full deep-learning model. Additionally, while the most predictable ENSO events are still identified in advance by the LIM, they are better predicted by the Hybrid model, especially in the western tropical Pacific for leads beyond about 9 months, by capturing the subsequent asymmetric (warm versus cold phases) evolution of ENSO.

SECTION: 1Introduction

In the past few years, deep learning has revolutionized weather forecasting with models such as GraphCast(Lam et al.,2023), Pangu(Bi et al.,2023), and others(Pathak et al.,2022; Lang et al.,2024)now outperforming traditional state-of-the-art numerical weather prediction models(Ben-Bouallegue et al.,2023). These advancements are largely due to the availability of millions of training data samples from reanalysis products on hourly resolution, which allows the training of large neural networks with negligible generalization error. While these models demonstrate exceptional medium-range forecasting skill, it remains unclear if these capabilities extend to seasonal or annual predictions. Unlike medium-range forecasting, which is mainly dependent on initial conditions, long-range predictions are primarily determined by boundary forcing factors, particularly from the ocean.

A prominent example of a target for seasonal to annual forecasting is El Niño-Southern Oscillation (ENSO). Characterized by tropical Pacific sea surface temperature anomalies (SSTA), the ENSO strongly influences global weather patterns and is therefore a primary source of seasonal to annual predictability. Its events are characterized by anomalously warm (cold) tropical SSTA, which exhibit a rich diversity in their spatial structure, temporal evolution(Capotondi et al.,2015; Timmermann et al.,2018), and impact on extreme weather conditions worldwide(Taschetto et al.,2020; Strnad et al.,2022). Thus, early forecasts of the likelihood of an ENSO event and also of its expected spatial structure are of great value for sectoral applications worldwide(Callahan and Mankin,2023).

While deep learning models have demonstrated their capacity for producing skillful ENSO forecasts(Ham et al.,2019; Petersik and Dijkstra,2020; Cachay et al.,2021; Zhou and Zhang,2023), training these models directly from SSTA data is hampered by the short observational record. Global SST records from satellites have been available for the past 40 years, with reconstructions based on point observations extending 100 years prior. In fact,Wittenberg (2009)suggested that O(500) years of data might be required to entirely capture the diversity of interannual ENSO events. Instead, deep learning models have been trained on lengthy simulations made by global circulation models (GCMs)(Ham et al.,2019; Zhou and Zhang,2023), such as those available from the CMIP5 or CMIP6 ensembles. While potentially providing hundreds to thousands of years of data, GCM simulations possess inherent biases likely due to their insufficient spatial resolution and to inadequate parametrizations of unresolved processes. In the Tropical Pacific, such biases typically include mean errors in the equatorial upwelling region in the eastern Pacific and variability errors in the strength and location of ENSO anomalies(Capotondi,2013; Chen et al.,2021; Beverley et al.,2023), all of which may then be captured by the neural network. Transfer learning, as for instance used byHam et al. (2019), presents a potential workaround for bridging the gap between erroneous simulation data and limited observations. However, Zhou and Zhang(Zhou and Zhang,2023)found no significant improvement in model performance when fine-tuning their model with reanalysis data, indicating the need for more research in this domain.

The Linear Inverse Model (LIM), first introduced byPenland and Sardeshmukh (1995), offers an alternative data-driven approach for ENSO forecasting. The LIM is a stochastic climate model(Hasselmann,1976)that represents chaotic nonlinear dynamics by a deterministic multivariate linear system driven by noise that is white in time but may be correlated in space. This approximation may be suitable for dynamical systems with different components evolving on very different time scales, such as when the slowly-varying ocean is driven by and coupled to a much more rapidly decorrelating atmosphere(Frankignoul and Hasselmann,1977; Penland,1996). Even when determined from anomaly covariances drawn from the short observational record alone, the LIM demonstrates seasonal-to-interannual tropical SSTA forecast skill on par with operational numerical prediction models(Newman and Sardeshmukh,2017; L’Heureux et al.,2020). Refinements to the LIM, such as the cyclostationary (CS)-LIM to account for the annual cycle(Shin et al.,2021; Vimont et al.,2022)and the inclusion of ocean memory(Newman et al.,2011; Chen et al.,2016), can further improve its predictive skill.

The LIM generates climate variability that has a multivariate Gaussian distribution. The tropical Pacific SSTA distribution, however, is non-Gaussian and asymmetric, with warm events being more intense than cold events, while cold events are typically of longer duration(Takahashi et al.,2011; Takahashi and Dewitte,2016; Okumura,2019; Geng and Jin,2022). As a correction to the LIM, we might first try modifying its stochastic forcing to include a linearly state-dependent noise term that is correlated with the original state-independent noise(Sardeshmukh and Penland,2015), the details of which in principle might also be extracted from observations(Martinez-Villalobos et al.,2018). Then, while the LIM’s predictable dynamics would remain linear, this additional “correlated additive-multiplicative” (CAM) noise term(Sardeshmukh et al.,2015)would drive non-Gaussian variability including some key aspects of the observed asymmetry in the intensity and duration of ENSO events(Martinez-Villalobos et al.,2019). However, ENSO asymmetry has also been attributed to slower nonlinear processes in the tropical Pacific(Dommenget et al.,2013; Takahashi et al.,2011,2019; Hayashi et al.,2020), which might not be simply represented by CAM noise. Given that the LIM already shows substantial predictive skill, then, we could systematically investigate its forecast error residual for predictable nonlinear dynamics, and add the resulting correction of the LIM. Developing such a hybrid LIM model is our aim in this study.

Hybrid models, which combine numerical or empirical models with neural networks, offer a promising approach to capture unresolved dynamics in the low-data regime(Irrgang et al.,2021). Instead of learning the complex system dynamics of the full system with a neural network, hybrid approaches are often more data efficient due to their simpler learning objective. Applications include post-processing of weather forecasts(Gneiting et al.,2005; Bauer et al.,2015)and machine learning parametrization in coupled ocean-atmosphere models(Rasp and Lerch,2018; Watt-Meyer et al.,2021; Kochkov et al.,2023).

In the context of ENSO forecasting,Goel et al. (2017)and others(Wang et al.,2021; Zhou and Zhang,2022)combined RNNs with statistical models for forecasting Niño indices. Expanding beyond ENSO index prediction,Rodrigues et al. (2021)developed a hybrid model using the LIM operator within a ResNet-like architecture for global SSTA prediction. These hybrid approaches have not yet achieved state-of-the-art forecasting skill, and their potential advantages over fully deep learning models, such as interpretability and data efficiency, remain unexplored.

Here, we propose a hybrid deep-learning model for ENSO forecasting that combines the LIM with a Long-Short Term Memory (LSTM) network(Hochreiter and Schmidhuber,1997). This LIM-LSTM hybrid model is designed to capture the residuals between LIM forecasts and target data, thereby improving seasonal forecast accuracy. We diverge from existing methodologies by including seasonality in both the LIM and the LSTM. Furthermore, we adapt both the hybrid model and the LSTM to generate probabilistic ensemble forecasts of the full field variables, SSTA and SSHA. This is achieved by employing a set of output layers that generate ensemble members, designed to match the nonparametric distribution of the target data(Lessig et al.,2023). We conduct a comparative analysis between our LIM-LSTM hybrid model and our LSTM deep-learning model, focusing on how each captures ENSO dynamics.

One key question we address in this study is the training data requirements needed for each technique to achieve a given level of ENSO prediction skill. Therefore, we develop the technique on a large model dataset. While this has limitations, it allows us to compare the data requirements for both our hybrid and full DL models by evaluating their skill on subsets of the data with varying lengths. As an example, Fig.1shows that for training on 50-100 years of monthly data, comparable in length to reanalysis products like ERA5 and CERA-20C, the 12-month forecast of both the CS-LIM and the LIM-LSTM hybrid model exhibit an Anomaly Correlation Coefficient (ACC) of bigger than 0.5 (the typical threshold for a forecast to be considered skillful). In contrast, for the same amount of training data, the 12-month forecast of the pure LSTM model’s has an ACC lower than 0.4, both evaluated on the 200-year long test set. It takes 300 years of data before the LSTM model reaches a forecast skill on par with the CS-LIM and LIM-LSTM hybrid model. Only when the training dataset size exceeds 500 years do both the LSTM and the LIM-LSTM model surpass the CS-LIM, with skillful forecasts (ACC > 0.5) up to leads of 18 months.

SECTION: 2Results

SECTION: 2.1Improved forecast skill due to predictable nonlinearities

The CS-LIM estimated from the PCs of SSTA and SSHA of the first 1500 years of a CESM2 pre-industrial simulation in the tropical Pacific (see Methods sec.4.1) captures the predictable linear dynamics of the system. We then trained an LSTM to learn the residuals between the CS-LIM’s predictions and the target data and combined these to form our hybrid LIM-LSTM model (see Methods sec.4.5). Improved forecast skill of the LIM-LSTM, relative to the LIM itself, can thus be attributed to its ability to capture predictable nonlinearities of the tropical Pacific Ocean dynamics. We quantified this improvement in skill by computing deterministic and probabilistic skill scores (RMSE and CRPS; see Methods sec.4.8) of the CS-LIM and LIM-LSTM on the test data (year 1800 to 2000). Both skill scores are obtained with respect to monthly climatology as a baseline model.

First, we would like to capture as much of the linear dynamics in the LIM as possible, so that the LSTM is primarily tasked with learning nonlinear dynamics from the LIM errors. Various LIM variants, with increasing levels of complexity, are constructed to guide the choice of the best linear model. The initial LIM variant, formulated byPenland and Sardeshmukh (1995), is solely fitted to the first 30 PCs of SSTA in the Pacific and is termed stationary (ST)-LIM (ssta). An advancement to this is the CS-LIM (ssta), introduced byShin et al. (2021), which includes seasonal variations and substantially surpasses the skill of the ST-LIM (ssta), as shown by the RMSE (Fig.2a) and CRPS (Fig.2b) skill scores of the Niño4 index. For reference, we present the skill of the persistence forecast which is worse than a climatological forecast (dashed line at zero) after=6 months forecast lead time. Including a measure of ocean memory, by incorporating the first 10 PCs of SSHA in the tropical Pacific in the CS-LIM (ssta, ssha), leads to additional skill enhancement relative to the CS-LIM (ssta). We select SSHA for its availability in both model and observations, and for its strong relationship with important ENSO precursors, namely the upper ocean heat content and thermocline depth, following previous LIM studies(Johnson,2001; Newman et al.,2011). Progressive improvements are evident in each LIM version, with the inclusion of seasonality and subsurface ocean variables contributing significantly to enhanced skill (Fig.2). The CS-LIM (ssta, ssha) (CS-LIM hereafter) has the highest skill among the linear models and thus is considered the best representation of the predictable linear dynamics.

Our hybrid LIM-LSTM model builds upon the CS-LIM by using an LSTM that learns to correct the error between the LIM forecast and the target data. The LSTM takes 16 ensemble member forecasts of the CS-LIM as input and learns to model their residual errors to the target data by minimizing the CRPS loss function detailed in Eq.LABEL:eq:crps-loss. To ensure robustness, we repeat the model training five times with varied weight initialization and data shuffling, whose variability is depicted through error bars in Fig.2. The skill improvement of the LIM-LSTM upon the CS-LIM is significant at forecast lead times larger than 6 months. We anticipate that the skill improvements can be largely attributed to predictable nonlinearities to the extent that we have tried to ensure that all known linear dynamics are captured by the CS-LIM.

We conducted a further examination of the seasonal dependency (Sec.A.1) and spatial distribution of skill for both the LIM and the LIM-LSTM model. The ensemble mean RMSESS for a 12-month CS-LIM forecast of SSTA exhibits the highest skill south of the equator and the Niño4 region while the skill is notably smaller in the upwelling region in the eastern tropical Pacific (Fig.3a). In contrast, the RMSESS of SSHA demonstrated high skill in this region as well as in western tropical Pacific north of the equator (Fig.3c) where the largest centers of SSH variability associated with ENSO occur(Capotondi et al.,2020).

The LIM-LSTM improves upon the CS-LIM skill throughout the Tropics (Fig.3b, d). There is a discernible skill improvement all along the equator, with the most significant enhancement observed in the western tropical Pacific. This improvement is consistent with the patterns obtained using the CRPSS (not shown). The pattern of skill improvement is different from the pattern of CS-LIM skill; that is, the LIM-LSTM often has less improvement in regions where the CS-LIM already shows substantial skill. It is interesting to note that the western warm pool shows the highest skill improvement, and also exhibits the warmest temperatures in the Pacific where the nonlinear ocean-atmosphere feedback is the largest(Thual and Dewitte,2023). CESM2 and other climate models overestimate the SSTA variability in this region compared to observations which could increase this effect(Capotondi et al.,2020). We defer a detailed analysis of nonlinear drivers learned by the LSTM to a future study.

While both the LIM and LIM-LSTM can predict large El Niño events, the LIM-LSTM better captures event amplitudes. As an example, Fig.4shows 24-month forecasts made by both models for a chosen El Niño exemplar drawn from the test dataset. The forecasts are initialized in December, 12 months before the peak of the chosen El Niño event (dashed line in Fig.4A). Throughout this work, we show the Niño4 instead of the Niño3.4 region because of the CESM2 tendency to achieve the largest ENSO SST anomalies in the central Pacific rather than the eastern Pacific(Capotondi et al.,2020). Results for the Niño3.4 index are qualitatively similar. Both the CS-LIM (red line) and LIM-LSTM (blue line) forecasts predict the observed warming, as depicted by the ensemble members’ mean, with the shading indicating their respective ensemble standard deviations. Notably, the LIM-LSTM forecast magnitude is closer to the CESM2 target data than the CS-LIM forecast. The LIM-LSTM’s uncertainty range, given by the standard deviation between 16 ensemble member forecasts, is also narrower than that of the CS-LIM forecast, yet still encompasses the target data, possibly suggesting a more accurate ensemble spread, consistent with the CRPSS results in Fig.2. The SSTA and SSHA field forecasts at=12 months lead time (Fig.4C, D) show that the LIM-LSTM generally better captures this El Niño event’s magnitude throughout the equatorial Pacific. However, both the CS-LIM and LIM-LSTM lack some smaller spatial structures and extratropical features evident in the verification fields.

SECTION: 2.2Comparison to deep learning baselines

We conducted a comparative analysis between our hybrid LIM-LSTM and two fully deep learning models, the LSTM and the ConvLSTM (see Methods Sec.4). While the LSTM produces forecasts in the PC space of SSTA and SSHA, the ConvLSTM takes the field variables in the tropical Pacific as input and forecasts them directly. We employ both models to ensure that the EOF truncation does not substantially hamper our forecast skill. Similar to the training of the LIM-LSTM, each model underwent five separate training sessions with varied weight initialization and data shuffling (error bars in Fig.5). The deep learning models have RMSE and CPRS skill scores that are similar to the LIM-LSTM model (Fig.5a, b), though the ConvLSTM exhibits a slight improvement at the 12-month forecast horizon. This suggests that the LIM-LSTM model successfully captures most of the predictable dynamics, with the marginal gains of the ConvLSTM likely attributable to the PC truncation of our hybrid model. Crucially, the LIM-LSTM model achieves this level of forecasting skill with significantly fewer parameters compared to the full deep learning models.
This aspect is particularly beneficial in scenarios with limited data, as shown in Fig.5c and d. With less than 500 years of monthly data, both the LIM and LIM-LSTM exhibit higher 12-month RMSES and CRPS skill scores than the LSTM.

SECTION: 2.3Predictability assessment in the LIM-LSTM model

Previous LIM predictability studies have shown that the LIM can predict not only its own skill but, often, also that of operational prediction models(Newman and Sardeshmukh,2017; Albers and Newman,2019,2021). We might exploit this ability by building our hybrid LIM-LSTM model on top of the LIM. For any given initial forecast state, the CS-LIM’s maximum predictability can be calculated by projecting the state onto the optimal initial condition (OIC) for a forecast lead time. The OIC is the singular vector corresponding to the largest singular value of the forecast propagator(see Methods4.4). This condition is optimal in the sense that of all possible initial conditions of unit amplitude, it evolves into the largest amplitude (under the L2 norm) state vector at lead time(Penland and Sardeshmukh,1995). For CS-LIM, we obtain a different OIC at every month and lead time.

The CS-LIM’s OIC for a 12-month forecast in April (Fig.6a) exhibits an SST and SSH structure that aligns closely with earlier research(Newman et al.,2011; Vimont et al.,2014; Capotondi and Sardeshmukh,2015). Key features of the OIC are: a band of positive SST anomalies in the northern subtropics, stretching diagonally from approximately 0°N, 180°W northeastward to around 30°N, 120°W; positive SST anomalies in the southern subtropics, predominantly east of 120°W; enhanced thermocline depth anomalies along the equator; and comparatively weak negative thermocline depth anomalies located at approximately 10°N in the eastern tropical Pacific. We obtain its subsequent evolution aftermonths by applying the LIM operator to the OIC (Fig6b). It is important to note that the magnitude of these patterns is arbitrary, a result of the unit norm of the singular vector.

The strength of the projection of the initial forecast state onto the CS-LIM-lead OIC, or how well the initial state matches the first singular vector – termed as optimal initial growth – is a key determinant of the potential forecast skill of the CS-LIM(Newman et al.,2003). To illustrate this point, for each forecast lead time ranging from 1-24 months, we select initial states from the test set whose projections on the OIC determined for that lead have either the least (0-10%) or greatest (90-100%) amplitudes, and then compute the forecast skill that results from forecasts made for each group separately. The average ACC, illustrated in Fig.6c, is substantially larger for forecasts initialized from states with the strongest optimal initial growth than for states with minimal optimal initial growth, a result we could have anticipated at the time of forecast.

The dependence of skill upon the presence of (linear) optimal initial growth occurs not only for the CS-LIM but also for the LIM-LSTM hybrid model, although it also has a substantial skill increase compared to the CS-LIM (Fig.6c). This result implies that optimal initial growth, although a property derived from the CS-LIM, influences not only linear predictability but also significantly impacts overall predictability of the entire system. This is further supported by the LSTM forecast, where a similar marked increase in skill is observed for initial states with the strongest optimal initial growth (i.e., cases where strong linear predictability is expected) as opposed to those with minimal initial growth.

SECTION: 2.4ENSO asymmetry is nonlinearly predictable

The initial states projected onto the OIC, shown in6a, can yield either positive or negative optimal initial growth which evolve into warm or cold patterns, respectively. We examine the averagemonth forecast of April initial states with the absolute largest optimal initial growth (>90%) for our CS-LIM, LIM-LSTM, and LSTM models.

The average forecast pattern’s magnitude and asymmetry are estimated by warm minus cold patterns (W-C, Fig.7, first column) and warm plus cold patterns (W+C, Fig.7, second column), respectively. A two-sided t-test is utilized to ascertain statistically significant difference between the means of the two distributions. We report only those values that surpass the 95% confidence interval threshold.

The W-C pattern in the target data (Fig.7a) closely resembles the evolved optimal pattern of the CS-LIM (Fig.6b) and its average empirical forecast (Fig.7c). An evident east-west dipole structure is observed in the asymmetry of warm versus cold events of the target data (Fig.7b). Cold events exhibit higher SSTA and SSHA magnitudes in the western tropical Pacific, while warm events show greater magnitudes in the Central-Eastern Pacific. This asymmetry is not present in the W+C patterns of the CS-LIM forecast (Fig.7d) because of its linear and therefore symmetric evolution of cold and warm events. The remaining subtle differences between warm and cold patterns of CS-LIM forecast likely originate from the asymmetrical distribution of initial conditions, which might arise by chance but might also arise from a CAM noise process (not included in our CS-LIM), as noted earlier.

In contrast, the LIM-LSTM model and LSTM forecasts accurately capture both the magnitude and asymmetry of warm and cold events. Both nonlinear model forecasts exhibit the zonal dipole structure present in the target data (Fig.7f and h). This finding highlights the ability of our nonlinear models to predict the asymmetry between warm and cold patterns. Note also that the predicted W-C component for both the LIM-LSTM and LSTM models is very similar to that of the CS-LIM (see Fig.7c,e,g). This suggests that the hybrid model improves upon the CS-LIM by capturing predictable nonlinearity, rather than by finding additional linear skill. Taken together, these findings underscore the LIM-LSTM model’s potential to disentangle linear and nonlinear predictable dynamics, setting the stage for future systematic analysis of nonlinearities in subsequent work.

SECTION: 3Discussion

We have introduced a LIM-LSTM hybrid model specifically tailored for forecasting SST and SSH anomalies in the tropical Pacific. We start from the LIM, an empirical model describing the dynamics of the slower-varying ocean as stochastically forced by the rapidly varying atmosphere, with its deterministic dynamics assumed to be linear. However, while the LIM produces ENSO forecasts comparable to state-of-the-art numerical models, it is unable to capture observed asymmetries of ENSO that are likely important to its predictability, especially for longer forecast leads.

We combine an LSTM with the LIM to capture predictable nonlinearities and non-Markovian dynamics in the evolution of monthly tropical SSTA. The LIM-LSTM model is trained and tested on SSTA and SSHA data from the CESM2 pre-industrial control run. We find that modeling nonlinearities significantly enhances the forecast accuracy, particularly in the western tropical Pacific within the 9 to 18-month range.

Our findings provide initial evidence that the asymmetry between warm and cold events is a key source of nonlinearity that improves forecasting skill beyond linear models. This first insight lays the groundwork for a more comprehensive follow-up characterization of the potential sources of nonlinearity of ENSO forecasting.

We demonstrate that the predictability of the LIM-LSTM model is strongly related to the theoretical expected skill of the LIM which allows us to reliably assess its predictability. In contrast, neural networks typically struggle to provide accurate predictability assessments on seasonal to annual scales, primarily hindered by their weak spread-to-skill relationship. Given that LIM models show competitive performance with GCMs(Newman and Sardeshmukh,2017; Albers and Newman,2019,2021)and that the LIM-LSTM requires less data than a full LSTM, this hybrid LIM-LSTM is a promising approach for other S2S climate forecasts beyond ENSO.

A notable feature of our LIM-LSTM model is its data efficiency, particularly when compared to more data-intensive deep learning models like the LSTM network. This aspect is crucial given the limited span of available oceanic observational data. For a fair comparison, we utilized data from GCMs in our training, acknowledging their inherent biases as discussed in our study. The use of domain adaptation methods from deep learning emerges as a promising strategy to close the gap between GCM data and observational data. However, the field still needs more research to fully understand how neural network models can be adjusted to observational data when pre-trained on simulated data.

Although our models are based on LSTMs, which are not the most advanced deep learning models, they exhibit similar forecasting skill to the larger vision transformer model by Zhou and Zhang(Zhou and Zhang,2023). This suggests that for the amount of observational data typically available to train data-driven models on seasonal time scales, hybrid models could outperform fully deep-learning models.

SECTION: 4Materials and Methods

SECTION: 4.1Data

Training DL models for ENSO prediction with monthly data is limited by the short observational record. To test the impact of record length on the data-driven models, we use the 2000-year CESM2 pre-industrial control simulation(Danabasoglu et al.,2020). We focus on monthly SST and SSH data in the tropical Pacific region (130°E - 70°W, 31°S - 32°N), which we linearly interpolate to a resolution of 1°x1°. SSH is a proxy for the upper ocean temperatures and the thermocline depth. Despite the lack of external forcing in the control simulation, we observe a trend in SST data. We linearly detrend the data and remove the seasonal cycle by subtracting the monthly climatology which is obtained over the training set (0-1500). As SSTA and SSHA differ in units and scales, we perform a-score normalization before model training. Both the LIM and LIM-LSTM model require us to reduce the dimensionality, which is achieved using Empirical Orthogonal Function (EOF) analysis. The dataset is divided into training (75%, year 1-1500), validation (15%, years 1500-1800), and test set (10%, 1800-2000), where the validation set is used for refining the hyperparameters of our models.

SECTION: 4.2Methods

The objective of our study is to accurately predict SSTA and SSHA fields for a specified forecast lead time. We define the stacked variable fields at a given timeas, where each field spans the tropical Pacific. We estimate a function, depending on month, that predicts the future state,at an incremental time step, by

The autoregressive prediction is based on the previous states,, referred to as history. All our model forecasts consist ofensemble members, which allows us to estimate the model uncertainty. The dynamics of the tropical Pacific Ocean show a strong seasonal phase locking. We therefore conditionon the month of the year,. The month conditioning depends on the model architecture and will be discussed for each model separately.

SECTION: 4.3Empirical Orthogonal Function (EOF)

Estimating the linear evolution operator of the LIM requires a matrix inversion of the number of input dimensions. The matrix inversion is intractable for the full spatial fields of SSTA and SSHA. For this reason, each stateis transformed into a lower-dimensional state. Dimensionality reduction of the SSTA and SSHA fields in the tropical Pacific is achieved through EOF analysis, utilizing the first 20 Principal Components (PCs) for SSTA and the first 10 PCs for SSHA. Including higher-order PCs does not affect our results. Forecasting in the lower dimensional space is then equivalently conducted on these PCs, as formulated in Eq.1. For analysis and evaluation, we transform our forecast back to grid space. To adequately replicate the high spatial frequencies of the input fields, we add variability by randomly sampling loadings of the higher-order PCs (20-300) for both SSTA and SSHA at each time step. These random features are then combined with their respective EOFs and added to the forecast fields, ensuring a closer match to the spatial intricacies of the original data.

SECTION: 4.4Linear Inverse Model

The LIM describes the dynamic of the tropical Pacific as a multivariate linear system subject to stochastic forcing from the atmosphere. The underlying dynamics of such a system is described by a linear stochastic differential equation,

whereis the linear operator describing the dynamics ofandis a noise vector that is uncorrelated over time but spatially correlated, as encoded in the covariance matrix. Forecasts offor a lead timeare given by the transition probability as,

whereis the infinite ensemble mean forecast andis the forecast covariance matrix.

Penland and Sardeshmukh (1995)show that the linear operatorand the noise covariancecan be estimated from the data under two assumptions. First, the system has to be statistically stationary which allows us to write the Fluctuation-Dissipation relationship as

whereis the spatial covariance matrix. Secondly, the autocorrelation of the system decays with lead timewhich can be expressed using the time-lead covariance matrixas

whereis the Greens function that must tend to zero for long lead times. Typically, both assumptions hold for detrended anomaly data of a chaotic system.

Onceandare estimated from the data, we obtain forecast trajectories from an initial timeto, by numerically integrating eq.2using the forward Euler-method with incremental update stepsas

whereis a random sample from the noise distribution. We createensemble member trajectories fromtowhen integrating the system-times. The infinite ensemble member mean is given byin eq.3.

In the equatorial Pacific, the variance in SSTA shows a distinct annual pattern with low variance during the boreal spring and high variance in the boreal winter. This peak in winter variance aligns with the occurrence of the most intense warm and cold ENSO events, a phenomenon referred to as "ENSO phase locking"(Rasmusson and Carpenter,1982).Shin et al. (2021)showed that including seasonality in the LIM improves its forecast reliability. Their cyclostationary (CS)-LIM involves estimating unique linear operators and noise covariances for each month, indexed using. The numerical integration of the stationary (ST)-LIM outlined in eq.7changes to

where. The CS-LIM forms the base model of our LIM-LSTM model outlined in the following.

SECTION: 4.5LIM-LSTM model

We introduce a novel LIM-LSTM model that combines the LIM with an LSTM network. While the LIM captures the predictable linear dynamics, the LSTM learns the residuals between the LIM predictions and the actual data, thus the nonlinear dynamics. Our methodology is schematically detailed in Fig.8.

During inference, we project the initial state of the tropical Pacific,, onto the leading EOFs and employ the LIM to predict future states overtimesteps. For each timestep,, we predict a correction,, to the LIM forecast,. The final forecast is thus defined as:

The nonlinear correction is modeled by the LSTM as, whereis the latent state of the LSTM which aggregates the information of previous states. The LSTM is selected not only for its ability to capture nonlinear relationships inherent in deep neural networks but also for learning non-Markovian dynamics.

To include seasonality within the LSTM, we introduce a learned affine transformation to its latent state(Perez et al.,2017),, as follows:

whereandrepresent embeddings for each month, enabling the network to adapt its latent state dynamically to seasonal variations.

The LSTM component is configured to process the forecast from each CS-LIM ensemble member, represented as. At each timestep, the input to the LSTM network is linearly projected into a higher-dimensional latent space where it is processed by two consecutive LSTM layers. By combining the LIM forecast at each timestep with the LSTM’s hidden state from the previous time step, the model iteratively accumulates information across the entire forecast sequence. Finally, the predicted latent states are linearly projected back onto the PCs and added to the original LIM prediction. The combined prediction is then transformed to grid space by multiplication with the respective EOFs, as described in sec.4.24.3.

SECTION: 4.6Purely deep learning baselines

To verify the utility of our LIM-LSTM model, we provide a comparison against fully neural network-based approaches. Similar to the structure of our LIM-LSTM model, we construct an LSTM that operates in the PC space. Additionally, we explore the application of a Convolutional LSTM (ConvLSTM(Shi et al.,2015)) architecture in grid space. We employ a custom ConvLSTM variant specifically tailored to perform forecasts on fields with large-scale spatial structures.

Both the PC-LSTM and the ConvLSTM have a standard Encoder-Decoder structure used for sequence-to-sequence modeling(Sutskever et al.,2014), as depicted in Fig.S1. Unlike the LIM and LIM-LSTM model, these models incorporate information from time points preceding the initialization time. The Encoder network is designed to aggregate this historical information into a latent state which initializes the Decoder model. It begins with a downsampling block that transforms the history into a higher-dimensional latent space, which is then processed by two LSTM layers, where the input is added to the hidden state from the previous time step. The hidden state of the second LSTM layer at timeis then passed to the Decoder network. The Decoder mirrors the Encoder with two LSTM layers of its own, which do not require any additional input other than the hidden state and can thus be rolled out over the full prediction horizon, transferring the hidden state autoregressively for each successive prediction time. This is followed by an upsampling block that transforms the hidden state back to the input grid space. To generate 16-ensemble members, we employ a separate upsampling block for each member which are trained jointly. Similar to the LIM-LSTM model, we incorporate seasonal information through a learned affine transformation in both the Encoder and Decoder networks.

In the PC-LSTM, the downsampling consists of an EOF-truncation of the entire SSTA and SSHA fields onto their respective PCs (sec.4.24.3), followed by a learned linear layer that projects the data into the latent space. The latent states are recursively predicted using LSTM layers, in both the encoder and the decoder network. Equivalently to the downsampling, the upsampling block starts with a linear layer to project the latent space back to the PCs, followed by a projectionon their respective EOFs to yield a forecast of the variables in grid space.

We employ a second DL model that does not involve EOF-truncation of data. This model is based-on the ConvLSTM(Shi et al.,2015)with a customized LSTM cell (Fig.S2). Unlike the PC-LSTM, the input, latent, and hidden states in our model maintain dimensions of channel, height, and width, albeit of varying sizes. For model input, both variables are stacked along the channel dimension. The encoder initially downscales the height and width dimensions to reduce computational costs, while simultaneously expanding the channel dimension via a strided convolutional layer. Following the methodology ofLiu et al. (2021,2022)we use equal stride and kernel sizes for this initial encoding, effectively partitioning the input into small patches of equal size (4x4 grid-steps in latitude/longitude direction). This method of dimensionality reduction differs from the EOF as it is learned end-to-end with the forecasting model, as well as being based on local patches, rather than global features as encoded in EOFs. Our approach further adapts the standard ConvLSTM layer, in a similar fashion toLiu et al. (2022), by splitting the convolution into spatial and channel mixing components, interspersed with layer normalizations, as depicted in Fig.S2. This modification facilitates two major improvements over standard ConvLSTMSs, (i) the spatial mixing component enables a substantially larger receptive field while reducing the number of parameters and (ii) the added normalization stabilizes training and supports conditioning on monthly embeddings, implemented through the affine transformation outlined in Eq.10. Finally, the mirrored Decoder network, which also consists of two ConvLSTM layers and a strided and transposed convolution, transforms the aggregated hidden state back into the full-resolution grid space. An ensemble of 16 separate final projection layers are used to generate a probabilistic prediction and the model is trained using the Ensemble CRPS as detailed below.

SECTION: 4.7Optimization procedure

Each model is designed to produce a probabilistic prediction by generating an ensemble of 16 sequences. We train the models to enhance both the accuracy of individual predictions and the spread among ensemble members. This is achieved by employing the Continuous Ranked Probability Score (CRPS) for optimization. The CRPS is a probabilistic metric that compares the cumulative probability distribution (CDF) of the forecast to the CDF of the target. The target,, is a single observation, and thus its CDF is a step function. A perfect CRPS score of zero would be a Dirac delta-like predictive probability density function centered at the target, for which the CDF would be the same step-function as is for the target. The CRPS has an analytic expression for parametric distributions, like the Gaussian distribution, but also a statistical form for empirical distributions(Hersbach,2000; Gneiting and Raftery,2007).

For our-ensemble member prediction, we compute the pixel-wise CRPS for empirical distributions as,

whereandare the-ensemble member prediction andthe target value at each location. We optimize the parameters of each model by minimizing the averaged CRPS between the observed data,, and its ensemble forecast,, across all locations and lead. Using Eq.11, we define our tailored loss function as:

To address the greater loss values at longer forecast leads, we introduce a power-law decaying weight over lead time,, whereis an empirically set hyper-parameter.

In addition, we use the AdamW adaptive gradient algorithm(Loshchilov and Hutter,2019)in conjunction with cosine annealing(Loshchilov and Hutter,2017)to dynamically adjust the learning rate during the training phase.

SECTION: 4.8Evaluation metrics

Our analysis of the models, all of which generate ensemble member predictions, is based on probabilistic metrics as well as deterministic metrics of their ensemble mean. We evaluate all models on the test set (200 years) using SSTA and SSHA in the tropical Pacific.

The ACC is the temporal correlation between the model forecastand the targetat each spatial locationover the test set. The ACC is defined as,

whereis the covariance between the time series andtheir respective variance.

The RMSESS is a deterministic metric that compares the RMSE of the model to the RMSE of a reference forecast. Throughout this work, we choose the climatology as our reference forecast. The RMSESS can be written as

whereis the variance of the data. An RMSESS of 1 is a perfect model forecast and 0 is as good as the climatology forecast.

Equivalently to the skill score of the RMSE, we define the CRPS skill score using Eq.11as,, where the reference forecastis the climatology. A CRPSS of 1 is a perfect model forecast and 0 is as good as the climatology forecast.

SECTION: Acknowledgements

The authors acknowledge funding by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany’s Excellence Strategy – EXC number 2064/1 – Project number 390727645. Furthermore, we express our gratitude to the NOAA Physical Science Laboratory for making their resources available for this study. Antonietta Capotondi was supported by the NOAA Climate Program Office MAPP and CVP programs, and by DOE Award DE-SC0023228. We further thank the International Max Planck Research School for Intelligent Systems for supporting Jakob Schlör.

SECTION: Data Availability Statement

All data used in this study are publicly available and are referenced in the main text or the supplementary materials. Our code is available athttps://github.com/jakob-schloer/hybridLIM.

SECTION: References

SECTION: Appendix ASupplementary Materials

SECTION: A.1Seasonal skill dependency

In addition to assessing the spatial distribution of skill (sec.2.1), we investigate the seasonal skill variation. The average RMSESS of the Niño4 SSTA from the CS-LIM forecast, evaluated over different lead times and verification months, indicates that late winter and spring months are better predicted than the late summer and fall (Fig.S3a). These results are consistent with the findings byShin et al. (2021), and align with the phenomenon known as the spring predictability barrier, characterized by a notable drop in the autocorrelation of the tropical Pacific SSTA in boreal spring.

For the LIM-LSTM model forecast, we observe the most significant RMSESS improvements upon the CS-LIM at lead times ranging between 9 and 18 months (Fig.S3b). Specifically, the maximum enhancements are seen at 15 and 18 months during the winter months (December to February), while in spring, the peak skill improvement is at a lead time of 12 months.