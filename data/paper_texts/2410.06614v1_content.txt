SECTION: Pair-VPR: Place-Aware Pre-training and Contrastive Pair Classification for Visual Place Recognition with Vision Transformers
In this work we propose a novel joint training method for Visual Place Recognition (VPR), which simultaneously learns a global descriptor and a pair classifier for re-ranking. The pair classifier can predict whether a given pair of images are from the same place or not. The network only comprises Vision Transformer components for both the encoder and the pair classifier, and both components are trained using their respective class tokens. In existing VPR methods, typically the network is initialized using pre-trained weights from a generic image dataset such as ImageNet. In this work we propose an alternative pre-training strategy, by using Siamese Masked Image Modelling as a pre-training task. We propose a Place-aware image sampling procedure from a collection of large VPR datasets for pre-training our model, to learn visual features tuned specifically for VPR. By re-using the Mask Image Modelling encoder and decoder weights in the second stage of training,can achieve state-of-the-art VPR performance across five benchmark datasets with a ViT-B encoder, along with further improvements in localization recall with larger encoders. The Pair-VPR website is:

SECTION: 
The Place Recognition (PR) task involves associating input sensor data (e.g., lidar, radar, or vision), with a global map or database of places previously visited within an environment. It is an essential component in many applications such as driverless cars, autonomous robots, and augmented reality. In the field of learning-based Visual Place Recognition (VPR), it is a standard approach to start with a pre-trained neural network, such as VGG16 or ResNet, leveraging their initial architecture and weights for the VPR task. These networks are often pre-trained on large diverse datasets like ImageNet, enabling them to extract robust features from visual data. Following this, it is common to incorporate a feature aggregation layer — such as VLAD, GeM, Conv-AP, SoPamong others — to pool local features into a compact global descriptor vector efficiently representing the original image. These vectors can then be compared with an embedding distance metric such as the Euclidean distance, with the smallest distance noting the pair of most similar images. More recent VPR techniques often include multiple stages of retrieval, where subsequent stages of retrieval are used to re-rank an initial set of candidate place recognition matches.

In this work, we develop, a transformer-based VPR method trained on diverse VPR datasets, which achievesstate-of-the-artvisual place recognition performance across a range of challenging VPR benchmarks. We achieve this by proposing atwo-stagetraining pipeline to train apair-classifier, which can decide whether a given pair of images is from the same place or not. In the first stage, we pre-train a transformer encoder and decoder using siamese mask image modelling, with place-aware image sampling. We sample pairs of images from different places in the world, ensuring that these pairs contain bothspatialandtemporaldifferences. Our places are curated from three existing large-scale open source datasets (SF-XL, GSV-Citiesand Google Landmarks v2) with a total ofmillion panoramic images andmillion egocentric images, across diverse locations (planet-wide) and diverse times.

In the second stage, we re-use both the encoder and decoder, by jointly learning to produce a global descriptor from the encoder and using the decoder as a pair classification network. The pair classifier produces a similarity score denoting whether a given pair of images were captured from the same location, or not. A diagram showing our network architecture and training recipe is shown in Figure 1. Thenetwork uses a vision transformer setup, with ViT blocks in both the encoder and decoder. We leverage the class token output to supervise the network, with a low-dimensional global descriptor produced from an encoder class token and a pair similarity score from decoder class tokens. The network is then trained using a Multi-Similarity loss for the global descriptor and a Binary Cross-Entropy loss for the pair classifier, with online triplet mining. We benchmark the trained network on existing VPR benchmark datasets and observe state-of-the-art Recall@1 on all tested datasets with a ViT-B encoder, such as an improvement in the Recall@1 on the Tokyo24/7 dataset fromto. Moreover, we show that our proposed method can be extended to much larger vision transformer encoders (ViT-L, ViT-G) and achieve a new benchmark result of Recall@1 on Tokyo24/7 ofwith our best performing configuration.

In summary, this paper introduces a joint training for learning a Vision Transformer (ViT) for visual place recognition task. Unlike the existing methods for using a ViT for VPR, our method proposes:

A novel joint training method designed for Vision Transformers that simultaneously learns a global descriptor and a contrastive pair classifier for re-ranking, which can predict whether a given pair of images are from the same place or not. We propose training both components using the class tokens from a ViT.

We show that a pre-trained encoder and decoder from a Mask Image Modelling pre-training task can both be utilized for the VPR task, by using the encoder to generate a global descriptor and the decoder as a pair classifier.

We propose a training data generation strategy for Siamese Mask Image Modelling by sampling pairs of images fromplacesin existing large-scale VPR and image retrieval datasets.

SECTION: 
SECTION: 
Compared to the general task of Place Recognition, VPR focuses only on the image modality and this allows for VPR to be considered as an image retrieval problem. This formulation in conjunction with the advent of deep learning led to a number of high performing learning-based VPR solutions. Initially, pre-trained neural networks were applied to VPR, then NetVLAD was developed which utilized triplet loss to train a neural network specifically for the place recognition task. Numerous subsequent works expanded upon NetVLAD, including works that relax the triplet loss to use soft positives and negatives, quadruplets rather than triplets, a generalized contrastive loss, and aggregates features over multiple spatial scales. Other learning-based methods include those which consider place recognition as a classification problemor use a multi-similarity loss.

These earlier works all follow the same approach, of constructing aglobal descriptor,i.e., a single vector representing an image for nearest neighbour retrieval. However, the top candidate found using global descriptor matching is not always correct. To avoid such limitations, multi-stage VPR algorithms propose to re-rank a collection of top candidates.

Thesere-rankingmethods involve re-ordering an initial set of retrieved places such that the correct place match is ranked first in the retrieved list.
Learning-basedre-rankingmethods began with less computationally efficient algorithms that used geometric verification densely (without keypoint selection), then subsequently evolved to include keypoint/keypatch selection algorithms. More recently, R2Formerwent one step further by removing explicit geometric verification altogether. Instead, they treat re-ranking as a classification problem and use a small two-block transformer for selecting the best candidate image. Similarly, GeoAdaptalso learns a classifier except instead for the task of classifying the similarity of two point clouds for LiDAR Place Recognition. In our proposed approach, we show that a re-ranking classification transformer can be designed using standard self and cross-attention blocks and scaled up to overmillion parameters, by leveraging a VPR-specific pre-training recipe.

SECTION: 
Compared to Convolutional Networks (CNNs), Vision Transformershave no inductive bias therefore they can learn global dependencies between image features without being limited by a specific kernel size; however, as a result, larger image datasets are needed to train transformer models. This led to the concept of self-supervised learning (SSL) techniques for pre-training these large transformer models on large quantities of unlabeled data. These techniques include deep metric learning methods such as SimCLR, BYOL, and DINO. The recent work DINOv2provided further improvements to the pre-training recipe and in this work, we leverage the robust learnt features they provide. In contrast to metric learning methods, an alternate self-supervised approach is called Masked Image Modelling (MIM) and will be explained in the following subsection.

SECTION: 
Masked Image Modeling is a SSL technique where a neural network is tasked with reconstructing a partially masked image, where the image is divided into a set of non-overlapping patches. Initially, this reconstruction was performed on a set of transformer tokens produced from a raw image. Subsequently, MAEdemonstrated that instead the reconstruction could be performed directly on the raw pixel values, based on the mean squared pixel error between the original and reconstructed image. Further works then expanded upon this concept via improvements such as distillation (iBOT), continuous masking (ROPIM), and cross-view completion (CroCo). In CroCo, pairs of images are provided to the network where one image is masked and a second image, showing the same scene, is provided to the network without any masking. They showed that this pre-training recipe is ideal for 3D vision tasks. In a concurrent work, a similar architecture was proposed for learning from video data, called Siamese Masked Autoencoders. In our approach, we also provide pairs of images to a Mask Image Modelling network, except we propose aPlace-awaretraining methodology for VPR.

SECTION: 
Our proposed Visual Place Recognition (VPR) solution is a two-stage place recognition method, which first uses a global descriptor to generate a list of potential place matches, then uses a second stage to refine these matches to improve the likelihood that the highest ranked match is correct. For the first time, we propose a two-stage training methodology using a mask image modelling pre-training designed specifically for VPR. Then we propose a second stage approach where we learn to generate both a global descriptor and a pair classifier for VPR.

During the first stage we provide pairs of images to the network and then heavily mask one of these images. The training objective is to reconstruct the masked image, using a network that has both encoder and decoder components. In the second stage we jointly optimize both the encoder and decoder for VPR, generating a global descriptor from the class token of the encoder and the decoder is trained to predict whether a given pair of images is a positive or negative pair.

After training, the network can then be used as a two-stage VPR method. The encoder is used to produce low dimensional global descriptors which are then used for nearest neighbour searching through a VPR database. Then the decoder is used to decide which potential database candidate is the best match for the current query by passing (query, database) pairs to the decoder. An overview of our pre-training and fine-tuning procedures are provided in Figure.

SECTION: 
In the first stage of training we use the Siamese Masked Autoencoderdesign to train thenetwork. In this approach, pairs of images are input into the network and one of the images is heavily masked. The network is then able to leverage the visual information in the second, unmasked image to aid in reconstructing the masked first image. In previous works, generating pairs is achieved either by sampling random frames in a video sequence, or by sampling two different viewpoints of a scene. In our approach we propose a location sampling strategy, such that pairs are sampled from specific locations and, in VPR terminology, are always collected from the set ofpositivesfrom a given place.

Our network comprises a shared encoder that converts a first image(masked) and a second image(unmasked) into latent representations. We use a Vision Transformerencoder and convert each image into a collection of non-overlapping patches, which are then converted to a set of tokens. For the masked image, we replace the majority of image tokens with mask tokens, which are learnt parameters but have no information received from the original image.

After the encoder, we pass the set of encoded patch embeddings (excluding any class tokens) from both images through a decoder to reconstruct the masked image. Our decoder consists of another vision transformer except with alternating self-attention and cross-attention layers, with self-attention between tokens fromand cross-attention between tokens fromand. For efficiency we always use Flash Attention. After a number of decoder blocks, we pass the features through a final FC layer to produce reconstructed pixel values per token (i.e., per image patch).

The network is trained using a reconstruction loss between the predicted and ground truth (unmasked) images, by minimizing the Mean Squared Error between predicted and true pixel values. The loss function is expressed as below:

whereanddenote the two input images,denotes the ground truth value for a masked pixel at indexfrom image,denotes the predicted pixel value andis the subset of masked pixels from the masked image. The loss function is only calculated for any pixels that have been masked, where the mask ratio is a hyperparameter of the network. Before calculating the loss we normalize each patch by the mean and standard deviation of that patch as per prior work.

Our network is pre-trained in aPlace-awarefashion, where each iteration is a defined physical location in the world. Then pairs of images are sampled from this place, with dataset-specific sampling to ensure that sampled pairs have some viewpoint consistency; it would be almost impossible for the network to reconstruct an image using a second image facing the opposite direction. Our training data can theoretically be drawn from any collection of places in the world, however, in this work we limit ourselves to a set of pre-existing open source datasets. Further details concerning dataset specific implementations are provided in Section Four.

SECTION: 
In the second stage, the encoder is trained to generate global descriptors for retrieval and the decoder is trained to predict whether a given pair of images is similar or not. We used the pre-trained weights from stage one of, loading weights fromboththe encoder and the mask image modelling decoder. We thenjointlytrain the encoder and decoder for both global retrieval and pair classification simultaneously. Finally, we use class tokens from both the encoder and decoder as inputs to our VPR loss.

The stage two network architecture is an extension to the stage one network for the VPR task. We add a linear layer to project from the class token in the encoder to a smaller dimension, then use annorm to generate a global descriptor. We then convert the Mask Image Modelling decoder into a pair classifier by adding a new class token and add a two-layer MLP to the output of this class token to produce a scalar value denoting the similarity between a pair of images. The network architecture of ouris shown in Figure.

We use a contrastive loss to train the encoder for the global descriptor and a Binary Cross Entropy (BCE) loss to train the decoder for pair classification. We use online mining and use a Multi-Similarity Minerto select positives, negatives and anchors before using a Multi-Similarity Lossfor the global descriptor, with the loss function shown below:

whereis the batch size andis the similarity value between the indexin the batch and a positive/negative index. We keep the hyperparametersthe same as used in.

During the forward pass, we store both the global descriptor and the dense features from the encoder, and use the dense features as input into the decoder. As the decoder requires an image pair, the challenge is to provide examples to the network of both positives (two images of the same place) and negatives (two images from different places). To prevent the network from converging to the trivial solution, we use an Online Hardest Batch Negative Minerto use the hardest positives, anchors and negatives in a given batch.

We then pass into the decoder sets of (anchor, positive) pairs and (anchor, negative) pairs to produce a list of scalar values per pair, where a large scalar value denotes a high similarity between a given pair. After concatenating both sets, we use a BCE loss (with a sigmoid function) to optimize the network with positive pairs having a target value ofand negative pairs having a target value of:

whereis the target for the current pair () andis the similarity output for the pair for a batch size ofpairs. We then train the network jointly, with the full loss shown below:

whereis the global loss trained using a Multi-Similarity loss and minerandis the BCE pair loss.is a hyperparameter to balance the two loss terms.

SECTION: 
Onceis trained, VPR is performed using a two-stage process. First, all images are passed through the encoder and global descriptors are generated. These global descriptors are then used to find the topdatabase candidates for each query. We also save dense features for second-stage refinement - for an image input of sizebypixels, these have a size ofbywith a ViT-B encoder.

In the second stage, for each query, we copy the dense features byin order to create batches of (query, database) pairs, whereis the database index and there arepairs in total. Because our decoder is not symmetric (self-attention is only performed on the first image in a pair), we pass into the decoder batches of both (query, database) and (database, query) pairs and then sum their respective scores together. The highest-scoring pair is then considered the best match for the current query. Figureshows a diagram of our network during evaluation.

SECTION: 
SECTION: 
We begin by pre-initialising the ViT encoder with weights from DINOv2. We found that leveraging the diverse pre-training policy used in DINOv2 improved performance over random initialization (please refer to ablation studies in Table). We then freeze the first half of the encoder blocks (e.g., six blocks with ViT-B) and train the second half using Place-Aware Masked Image Modelling. We train in a Place-aware fashion and construct a dataloader such that a single item in a batch is a singleplace, where an item comprises a pair of images. We selected three existing large open source datasets for stage one training: SF-XL, GSV-Cities, and Google Landmarks v2.

We follow the procedure described in EigenPlacesand divide the dataset intometer-sized cells based on the UTM coordinates of all panoramic images in the dataset, generating a total ofcells. Considering the-th cell, we follow the approach in EigenPlaces of computing the Singular Value Decomposition (SVD) from the UTM coordinates of all images in this cell, along with the mean UTM coordinatefor this cell. We then select pairs of panoramic images randomly from each cell. Given cell, we begin by calculating a random focal point (e.g., a target UTM coordinate) that we want our pairs to observe, anchored to the first principle component of the cell denoted as. Our formula for calculating a focal point is given below:

whereis a random observation angle, andis a random focal length. At each iteration, we randomly sample observation anglesbetweenanddegrees and apply this random rotation to the eigenvector. We then randomly sample different focal lengths, sampling between 10 and 20 meters away from the mean coordinate. This approach differs from the method in EigenPlaces, which only utilized theanddegree observation angles and a fixed focal length.

Then given a focal point, we produce crops from two randomly sampled panoramic images within cell, such that the cropped views are focused on the target focal point - this ensures that our pair of images contains overlapping visual information, without requiring any manual curation of pairs. We can then calculate a viewing angle between a given panoramic imageand a focal point using their respective UTM coordinates:

Given this viewing angle, we select abypixel crop from abypixel panoramic image. However, pairs can be selected that are either too easy (small viewpoint difference), or too hard (too much viewpoint difference). Therefore, we calculate the difference in angles between our two sampled crops:

Then we check ifis between a low and high threshold°and°. If not, we resample until we return a pair of crops within the required range.

We randomly sample a pair of images from each place in the GSV-Cities dataset, excluding places with less thanimages available.

To improve the data diversity, we also added the Google Landmarks dataset. We only use the cleaned subset of the dataset to avoid damaging the network’s ability to learn by providing ambiguous image pairs. Additionally, we also exclude any landmarks with less thanimages. We consider a landmark as a proxy for aplaceand treat the dataset in the same format as SF-XL and GSV-Cities. In total, we are left withlandmarks for training after cleaning and filtering.

We merge the three aforementioned datasets to produce a total ofplacesper epoch, which are sampled from a set ofmillion panoramic images andmillion egocentric images. We train using a mask ratio ofpercent with a ViT-B sized decoder forepochs with a learning rate of 2e-4 for a batch size ofplaces with AdamW and a cosine LR scheduler. We train using square images of sizepixels. By default, we use a ViT-B encoder withregister tokens, however, we note that our approach can also work with larger encoder sizes. Hyperparameters for larger encoders are kept the same except we use 1000 epochs.

SECTION: 
We continue training our model using the GSV-Cities datasetfollowing training recipe used in prior works such as MixVPRand SALAD. We train forepochs using a linear LR scheduler with an initial LR of 8e-5, with a weight decay of 5e-2 and a batch size ofplaces. We also use square images of size, and freeze all encoder layers except the last six. Our global descriptor has onlydimensions and we heuristically set(the loss balance term) to. We use MSLS-Val as our validation dataset and take the checkpoint with the highest second-stage Recall@1 as the final trained model.

SECTION: 
We evaluate our method on five commonly used benchmark VPR datasets, with a diverse set of environments. The datasets are: MSLS validation set, MSLS challenge set, Pittsburgh30k, Tokyo247and Nordland(note that we use the split of Nordland from VPRBench, withquery images). During the evaluation, we resize all images toresolution. By default, Pair-VPR uses a ViT-B encoder and uses the top 100 candidates after global descriptor matching - we refer to this as the Speed configuration of(Pair-VPR-s). We also provide a Performance version of, which has a ViT-G encoder and uses the topcandidates during pair-classification (Pair-VPR-p). We evaluate using the standard Recall@N metricwith.

To comparewith existing techniques, we benchmark against five existing single-stage learnt VPR methods (CosPlace, MixVPR, EigenPlaces, BoQand SALAD) and three existing two-stage VPR techniques (Patch-NetVLAD, R2Formerand SelaVPR).

SECTION: 
SECTION: 
We begin by comparing the performance ofagainst other State-Of-The-Art (SOTA) VPR methods on a range of benchmark datasets, and we provide both quantitative and qualitative results in Tableand Figurerespectively. Comparing our method Pair-VPR-s to the other methods we benchmark, we observe thatachieves the highest Recall@1 on all five datasets. We observe that our transformer-based re-ranking network is particularly adapt at improving the Recall@1 over prior works that also use a vision transformer backbone (SALADand SelaVPR). Comparing against R2Former, which is a prior SOTA method which also used a re-ranking/pair-classifier transformer, we observe a significant jump in performance especially on the Tokyo dataset.

When we scale up our method to use larger encoder sizes and more top candidates, we observe that Pair-VPR-p provides a large performance increase over Pair-VPR-s and achieves the highest recall across all recall@N values compared to prior works. We especially highlight the results on Tokyo24/7, where we have achieved a Recall@1 of%.

SECTION: 
To understand which aspects of our training strategy are essential for the performance of, we conducted an ablation study over different variations of stage one training (Table). In the first row, we show the results when we don’t perform stage one training at all, with the decoder in stage two initialized using random weights. We observed that pre-training the decoder network via stage one training is essential to achieve effective VPR performance - as the pair-classification task is non-trivial, the place-aware siamese mask image modelling task provides an initial set of weights that are already tuned for comparing features between two images.

In the second row of Table 2, we investigated the importance of place-aware sampling by removing the place sampling during stage one training, and instead used strong augmentation to generate the second unmasked image. We observed a consistent drop in recall, even though the total collection of training images is kept identical.

In row three, we experimented with unfreezing all blocks in our Dinov2 initialized ViT encoder. We found that allowing the entire network to be trained reduced the VPR performance and we hypothesise that this is because the DINOv2 network was originally trained on a larger and more diverse dataset than ours, and maintaining the low level (e.g., color) learnt features from a more diverse dataset improves performance.

In the fourth row we instead initialized the ViT encoder with random weights and performed stage one training from scratch. We found that the VPR performance is still maintained well on urban datasets, but reduces significantly on non-urban datasets like Nordland. This is likely due to the urban bias in common VPR pre-training datasets.

In the bottom four rows, we compared different configurations of. We observed that the performance ofincreases as we increase the encoder size. We further observed that increasing the number of top candidates passed to the pair classification component increases the recall, as it reduces the performance limitation imposed by the effectiveness of the global descriptor.

SECTION: 
In Table, we analyse the performance of theglobal descriptor. The performance of the second stage refinement is always limited by the recall of the global descriptor at the chosen number of top candidates, therefore we display Recall@N values for. The global descriptor is alwaysdimensions, allowing for computationally efficient database searching while also having a Recall@1 comparable to existing global descriptor VPR methods. We also note that our global descriptor is a lot smaller than the single-stage VPR methods in Table,e.g., CosPlace, MixVPR and SALAD have,anddimensions respectively.

When we shift to using larger encoder sizes, we always maintain the same global descriptor by increasing the projection ratio from the class token. For a ViT-G encoder, the class token hasdimensions and we project down todimensions using a linear layer. We do this in order to keep the computational cost of descriptor matching the same. We find that the ViT-G global descriptor performs similar to the ViT-B descriptor, except on Tokyo24/7. It is possible that the large projection ratio is not ideal for VPR, therefore future work should investigate experimenting with larger global descriptor sizes, albeit at an increased compute requirement.

SECTION: 
In this subsection we discuss the computational requirements ofand compare against other two-stage VPR methods using the same compute platform (a server computer limited to using a single GPU), as shown in Table. It can be seen that Pair-VPR-s is fast at encoding and matching, takingmilliseconds per query image to encode and onlyms/query to match against the entire database usingdimensional descriptors. Our second stage refinement is slower than SelaVPR and R2Former but much faster than Patch-NetVLAD-p, and requiresseconds per query withtop candidates. Comparing the ViT-B to ViT-G encoder sizes of, only the encoding time and storage requirement increases, since we maintain the same global descriptor size and the same decoder network size. The storage requirements ofare primarily from the dense features required for refinement and Pair-VPR-s features are comparable in size to Patch-NetVLAD-s and SelaVPR.

SECTION: 
In summary, we present, a novel two-stage VPR method that relies upon a mask image modelling pre-training strategy to maximise performance.combines a smalldimensional global descriptor for rapid database searching along with a slower second stage that only searches a set of potential database matches. We observed thatachieved the highest Recall@1 score on all datasets we tested on, comparing against recent state-of-the-art methods in VPR literature - while only requiring an encoder and decoder ofmillion parameters each. As the encoder size is scaled up to ViT-L (M) and ViT-G (B), our training recipe allows for continuing performance improvements as the network size grows. Given the high recalls attained by, we believe it prudent to investigate converting this method from a VPR technique to a loop closure module in a full SLAM system. In future, we aim to investigate the effectiveness ofat performing loop closures in SLAM.

SECTION: References