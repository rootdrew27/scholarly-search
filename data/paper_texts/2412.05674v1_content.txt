SECTION: No-Free-Lunch Theories for Tensor-Network Machine Learning Models
Tensor network machine learning models have shown remarkable versatility in tackling complex data-driven tasks, ranging from quantum many-body problems to classical pattern recognitions. Despite their promising performance, a comprehensive understanding of the underlying assumptions and limitations of these models is still lacking. In this work, we focus on the rigorous formulation of their no-free-lunch theorem—essential yet notoriously challenging to formalize for specific tensor network machine learning models. In particular, we rigorously analyze the generalization risks of learning target output functions from input data encoded in tensor network states. We first prove a no-free-lunch theorem for machine learning models based on matrix product states, i.e., the one-dimensional tensor network states. Furthermore, we circumvent the challenging issue of calculating the partition function for two-dimensional Ising model, and prove the no-free-lunch theorem for the case of two-dimensional projected entangled-pair state, by introducing the combinatorial method associated to the “puzzle of polyominoes”. Our findings reveal the intrinsic limitations of tensor network-based learning models in a rigorous fashion, and open up an avenue for future analytical exploration of both the strengths and limitations of quantum-inspired machine learning frameworks.

Tensor networks (TNs) have emerged as a powerful tool for studying quantum many-body systems, demonstrating remarkable versatility across various domains of quantum physics. This success has catalyzed a growing interest in harnessing TNs for machine learning applications, where they have shown promise in diverse areas such as dimensionality reduction, model compression, natural language processing, generative models. In particular, TNs have demonstrated distinct advantages in the development of efficient, interpretable machine learning models. Meanwhile, TN-based machine learning models offer intriguing theoretical and practical advantages, including insights into learning theory and the development of quantum machine learning frameworks. In addition, TNs provide a powerful framework for developing quantum machine learning theories, showing potential exponential advantages over those classical models. Recent advancements also highlight the use of TNs to mitigate barren plateaus in quantum learning models. On the practical side, TN techniques, including canonical form and renormalization, are instrumental in optimizing and training machine learning models, further supported by the development of open-source libraries. In the vein of TN-based machine learning theory, a number of pioneering works have been conducted, including these on the barren plateau problem, model generalization, and mutual information scaling. Despite this rapid progress, the field of TN-based machine learning is still in its infancy, with many fundamental aspects remaining largely unexplored. Here, we study the generalization ability of TN-based machine learning models, with a focus on establishing their no-free-lunch (NFL) theorem.

The NFL theorem is one of the most fundamental theorems in the classical machine learning theory. It states that, averaged over all possible problems, every algorithm performs equally well when applied to problems they were not specifically designed for. Consequently, the average performance of a machine learning model across all target functions is highly dependent on the size of the input set, highlighting the importance of large datasets in building robust models, such as large language models. Inspired by the critical role of NFL theorem in classical machine learning, significant progress has been made in developing NFL theorem for quantum learning models. The quantum NFL theorem establishes straightforward connections between quantum features and the capabilities of quantum learning models.
For instance, in practical quantum learning setups with a finite number of measurements, entangled data exhibits a dual effect on prediction errors. With sufficient measurements, highly entangled data can reduce prediction errors. This is consistent with the ideal case of infinite measurements. Conversely, with few measurements, highly entangled data can amplify predicting errors. These results highlight how quantum features contribute to the advantages in quantum machine learning models.

For TN-based machine learning models, the classical data is encoded in TN states with specific structures, and the corresponding data distribution can be quite different from those in other machine learning models. It is hence highly desirable to investigate how to rigorously formulate the NFL theorem for the specific TN-based models, as well as to study how the bond and physical dimensions of TNs would influence the lower bounds of the average risks. We face two major challenges in establishing the NFL theorem for higher-dimensional TN models: The first one is to calculate the variance of random TN states. This can be mapped to calculating the partition function of a high-dimensional Ising model, which is notoriously a challenging task. The second one is to embed the information learned from the training set (corresponding to the diagonalized sectors of the matrixin Fig.) into the higher-order tensor properly, which notably differs from the case for quantum learning modelsand requires judicious design. In this paper, we tackle these challenges and rigorously prove the NFL theorem for TN models. Our setup is to learn a target unitary operator through data samples encoded into TN states, see Fig.for illustration. We first prove the NFL theorem for the case of 1D matrix product state (MPS). We show that the lower bound of the average predicting risk highly depends on the size of linearly independent training samples. We then focus on the 2D projected entangled-pair states (PEPS). We note that proving the NFL theorem for 2D PEPS poses significant challenges, as it equates to calculating the partition function of the 2D Ising model, which is a notoriously difficult problem. Here, we circumvent this obstacle by introducing a combinatorial method associated to the “puzzle of polyominoes”. In addition, we carry out numerical simulations to support our analytical results.

— We consider a task of learning the unknown unitary operationbased on the input of TN states. Without loss of generality, we take the 1D MPS for demonstration. The MPS under periodic boundary condition has the form, wheredenotes thetensor withrepresenting the bond dimension,denotes the state of-th physical site with physical dimension. We define the unitary embedded MPS by converting eachtensorto aunitary, as depicted in Fig.. We define the labeled training set, where the site size, the MPSbelongs to the feature Hilbert space, and the statebelongs to the label Hilbert space. We learn the target unitaryby minimizing the loss function, wheredenotes the variational quantum circuit with sufficient expressivity. If the model is properly trained, then one hasup to an overall phase.
To quantify the predicting accuracy of our TN model on learning the target, we define the predicting risk function by the following trace-norm formula

wheredenotes the trace norm of,represents the unitary embedded MPS, and the integral is over the Haar measure of all local unitary tensorsin Fig..represents the prediction error of the trained model. For proper learning without training errors,is equivalent to the generalization error.
We note that the normis exponentially concentrated around one. The risk function describes the probability thatfails to reproduce the target unitarywith the random MPS as the testing input. One can extend the above MPS-based learning model to higher dimensional tensor-network based models, by replacingandinto the higher dimensional TN states. With the above risk function, one can then study the NFL theorem for TN-based models in learning a target unitary operation.

— The MPSs have found broad applications in machine learning, particularly in the context of learning and representing complex patterns and data, including dimensionality reduction, generative models, and so on. Now we consider the NFL theorem for the case of MPS. To showcase the NFL theorem, we adopt the risk functionand calculate the average risk over arbitrary target unitaryand arbitrary training set. Here for simplicity, we suppose that the states in the training set are linearly independent.

We sketch the main idea here and leave the detailed proof to the Supplemental Materials. Since the normin Eq. () is exponentially concentrated around one, one simplifies the risk functionto be. For the convenience of analytical calculations, we consider the training setwith size. For proper learning whereup to an overall phase, we consider the case that the learned unitaryobeys, where the global phase termlocates in the space of the training set,denotes theidentity matrix, and theunitary matrixlocates in the complementary subspace. For simplicity, we suppose that the unitarylocates in an-qudit subsystem. Utilizing the property that each local random unitaryof the unitary embedded MPSconstitutes approximately unitary-design, one can map the 2-moment integral ofinto calculating the partition function of 1D classical Ising model. Averaging over all possibleand all training sets, one obtains the analytical lower bound of. This leads to Eq. () and completes the proof.
∎

Theoremestablishes an analytical lower bound for the average risk of the MPS-based machine learning model, and thus quantifies the capability of the model in learning an arbitrary target unitary with an arbitrary training set. Eq. () indicates that the average risk depends solely on the training set size. Through the numerical calculations, we show that the average risk decreases monotonically with respect to the increasing. Rigorous proofs show that the average risk is lower bounded by zero for the full training set, whereas the average risk is lower bounded by one for the empty training set. These results formalize the NFL theorem for MPS-based machine learning models. Apart from the case of learning arbitrary target unitaries from the input of MPSs, we also consider the case of learning matrix product operators from quantum states, and analytically formulate the corresponding NFL theorem.

— 2D TN states present notable advantages for studying quantum many-body systems and computational applications. They enable efficient representation of large quantum systems with reduced computational costs, respecting the area law for entanglement entropy.
However, the contraction of a general PEPS without any canonical form is aP-hard problem, where the complexity of computing different physical properties, e.g., the norm and expectation value, shall grow exponentially with respect to the system size. Here we formulate the NFL theorem for the PEPS-based machine learning models.

We outline the main idea here and leave the details to the Supplemental Materials. For the 2D case, one critical step is to map the calculation of the second moment for 2D uniatry embedded PEPS into calculating the partition functions of 2D classical Ising models, which is typically a challenging problem. To address this, we introduce the combinatorial method based on the “puzzle of polyominoes”, and thus convert the problem of calculating partition functions into enumerating the directed polyominoes in a planar graph. Then by utilizing the generating functionfor enumerating the directed polyominoes, one can efficiently compute the value of the second moment integral of 2D unitary embedded PEPS. Consequently, one obtains the analytical lower bound of the average risk function. This leads to Eq. () and completes the proof.
∎

Theoremestablishes a lower bound for the average risk function of PEPS-based machine learning models. It indicates that, in the task of learning an arbitrary unitary with the input of PEPS, when averaged over arbitrary training datasets and learning models, the generalization risk of a PEPS-based learning model depends solely on the sizeof the training set. As the sizeincreases, the average risk decreases, eventually reaching zero when the training set is complete. These results thus rigorously formalize the NFL theorem for the PEPS-based machine learning models.

Our NFL theorems for TN-based models are different from those for classical machine learning models. In classical supervised learning, to learn a target functionbased on the labeled training set, one typical NFL formulation is, whererepresents the dimension of, and the risk functionmeans the probability that the hypothesis outputdiffers from. We notice that the lower bound of the average risk for such classical models is determined by the feature dimensionand label dimensionof training samples, as well as the sizeof training set. Whereas for TN-based models, the lower bound of the average risk is not only determined by the size of training set, but also dependent on the bond dimensionand physical dimensionof the TN states. Our theorems highlight the influence of the TN model’s internal structure on its performance limits.

— In our previous theorems, we have analytically obtained the lower bound of the average generalization risk. To show how these theorems perform in practice, we carry out numerical simulations based on the open-source packagein the Julia programming language. Based on the MPS machine learning model, we consider the supervised task of learning target unitarieswith the labeled training samples, wheredenotes the normalized MPS. We then plot the average generalization risks of the trained MPS-based learning models with respect to different qubit sizeof learning models, as depicted in Fig.. For example, in, we randomly generate a 16-dimensional target unitary and a training set of MPSs, and conduct the MPS-based supervised task. By repeatedly conducting learning tasks for different target unitaries, one obtains the average generalization risk for different size of training sets. We see from Fig.that the average error risks decrease with respect to the training set size. This is consistent with the analytical lower bound of the average risks predicted in our theorems.

— Our results provide a fundamental understanding on the generalization limits of TN-based models, extract how the performance of these specifically structured models would be limited by the NFL theorem, and analytically unveil that the lower bound of the average risk depends on both the bound and physical dimensions of TNs. Our findings would inspire further research on the learning capabilities of TN-based models with quantum computer, where TNs are employed as efficient representations of quantum circuit models. One potential direction is to incorporate the issues of practical quantum computing hardware, such as the noise and finite measurement times, into the analytical study of generalization ability for TN-based learning models. From the perspective of experiments, future research could focus on experimentally validating NFL bounds in practical quantum computing environments. As quantum hardware continues to advance, testing these theoretical predictions on real quantum systems will be crucial to understanding how NFL constraints manifest in noisy, resource-limited settings. Such experiments would also help refine our theoretical models, potentially revealing new strategies for optimizing TN-based machine learning models for practical applications. In addition, our methods can be applied to study the generalization of deep quantum neural network, which can be efficiently trained and has been experimentally realized on a superconducting quantum processor. We observe that the deep model can be mapped to an MPS-based model. Thus our framework can be naturally adapted to formulate the NFL for deep quantum neural networks.

In summary, we have rigorously formulated the NFL theorems in the TN-based machine learning models. Particularly, we consider the supervised task of learning arbitrary target unitary based on the TN models, and then present the analytical lower bounds for the average risk of the models. Our results reveal the intrinsic limitations in learning arbitrary unitaries from input states encoded via TNs. The risk bounds, which depend on both the bond and physical dimensions, provide a quantitative understanding of the connections between model generalization and training set size. In addition, to validate our theoretical predictions, we numerically conduct the practical supervised learning task and show that the average generalization risks over sufficient number of learning tasks and input sets decrease with respect to the size of training dataset. Our results offer valuable guidelines for designing more efficient models, and open promising research directions aimed at improving the generalization capabilities of quantum-inspired TN machine learning systems.

We thank Wenjie Jiang, Zhide Lu and Weikang Li for helpful discussions. This work was supported by the National Natural Science Foundation of China (Grant No. 12375060, No. T2225008, and No. 12075128 ), the Fundamental Research Funds for the Central Universities (Grant No. 63243070), the Shanghai Qi Zhi Institute Innovation Program SQZ202318, the Innovation Program for Quantum Science and Technology (No. 2021ZD0302203), and the Tsinghua University Dushi Program.

SECTION: References
SECTION: No-free-lunch theory for 1D tensor-network-based machine learning models
In this section, we rigorously prove the no-free-lunch theory of 1D tensor-network based machine learning models, i.e., the matrix product state (MPS). Concretely, we consider two representative cases of supervised MPS machine learning tasks, the first case is to learn an arbitrary target unitary-embedded matrix product operator (MPO) based on the input of quantum states, and the second case is to learn a target unitary matrix based on the input of MPSs. We note that the second case constitutes the main part of our Theorem 1 in the main text.

For the convenience of analytical calculation, we suppose that all of the MPOs or MPSs appeared in this work are unitary embedded.

SECTION: Learning target matrix product operator based on the input of quantum states
We first study the case of learning an arbitrary target unitary-embedded MPObased on the input of quantum states, where the data is encoded through unitary quantum circuits, and the variational structure is MPO. Define the training data set,

with set sizeand, wheredenotes the variational MPO, with bond dimension. It is worthwhile noting thatdoes not necessarily equal to the identity matrix.

If the model is perfectly trained, then the trained operatorshould result in

One can then define the risk function for such machine learning model, as

wheredenotes the trace norm of. We are interested in the average risk over all training setsand all (possible output) MPOs,

We introduce one lemma for the norm-concentration of MPS based on the Chebyschev’s inequality:

wheredenotes the physical dimension of the MPO, anddenote the number of qudits.

For the convenience of analytical calculations, we thus simplify the risk functionto be

Suppose that the input statesform the approximate unitary-2 design, i.e., approaching to the 2-moment integral of unitary group under the Haar measure. Then the risk function has the following formula

Here the termis represented by the unitary-embedded MPO.

For perfect learning strategy, where all of the training samples inare exactly learned, but up to an overall phase, one has

We consider three cases of training states:

States inare orthonormal..is the-dimensional matrix composed by MPOs..

States inare non-orthonormal, but linear independent..is the-dimensional matrix composed by MPOs..

States inare linear dependent.,denotes the linear independent bases of the training set.is the-dimensional matrix composed by MPOs.

In the case (a), the matrixis-dimensional. For the convenience of analytical calculation, we suppose thatcan be represented by a new unitary-embedded MPO with bond dimension, physical dimension, andphysical sites, as long as it satisfies. We can always make such assumption, since the integralholds for random unitary-embedded MPOwith arbitrary bond dimension and physical dimension, see the following proof.

Now we calculate the average risk over all possible MPO,

Now we provefor random unitary-embedded MPOin periodic boundary condition.

The unitary embedded MPOtakes the following form

wherelocates on the-th physical site. Through unitary embedding, one can map the 4-rank tensorto a 2-rank unitary matrix in the unitary group. Thus, the four dangling legs connected toare divided into two parities, with each party (circled by the red lines in Eq. () ) hosts-dimension.

The random tensorforms the approximate unitary 1-design, and the 1-moment integral ofreads

Then the 1-moment integral ofin Eq. () can be expressed as

where each green loop contributes to one-factor, and each blue loop contributes to one-factor.
We see from the above equation that the value of integralis independent of the concrete values of bond dimensionand physical dimension. Therefore our assumption on representingby a new random MPO is reasonable. This completes our proof.
∎

Since the result onis independent of the choice of training set, thus the average risk over all training setsand all (possible output) MPOshas the same formula as:

Similar calculations also apply to the cases (b) and (c).

Thus for arbitrary cases of training states, we obtain that

Case (a): States inare orthonormal..is the-dimensional matrix composed by MPOs..

Case (b): States inare non-orthonormal, but linear independent..is the-dimensional matrix composed by MPOs..

Case (c): States inare linear dependent.,denotes the linear independent bases of the training set.is the-dimensional matrix composed by MPOs.

SECTION: Learning target quantum unitary operator based on the input of matrix product states
Now we consider the case of learning target quantum unitary operator based on the input of MPSs, where the training states are encoded into the unitary-embedded MPSs, and the variational structures are unitary quantum circuits.

Owing to the norm-concentration of MPSs in Eq. (), one thus simplifies the risk functionto be

Hereis the random unitary-embedded MPS,

wheredenotes theunitary matrix locating on the-th site.
In Eq. (), One needs to calculate the 2-moment integral of the random MPS. We represent the variational circuit by the unitary. Following the above statements on the training states, we have the three following cases

States inare orthonormal..is the-dimensional unitary matrix.

States inare non-orthonormal, but linear independent..is the-dimensional unitary matrix.

States inare linear dependent.,denotes the linear independent bases of the training set.is the-dimensional unitary matrix.

Without loss of generality, we focus on the case (a). To calculate the risk function, one needs to calculate the 2-moment integral of random unitary-embedded MPS

where all of the’s are supposed to distribute randomly and form the approximate unitary 2-design. In previous works, it has been noted that the calculation of Eq. () is equivalent to calculating the partition function of 1D classical Ising model. For each local tensor, one has

wheredenotes the swapping operation on the-th site ofand,anddenote the symmetric and anti-symmetric connections of the bond indices of,

and the term—.
Here the notationjust denotes how the local indices ofcontract on the-th physical site, rather than the reduced matrix under partial trace.

One can embed the calculation of 2-moment integral of random MPSin Eq. () into the calculation of partition function. Here we utilize the transfer matrix method to calculate the partition function,

We thus obtain the transfer matrix

Not that, thus the Eq. () takes the form

In the periodic boundary condition of MPS, the term

Here the trace operationonly applies on the space of transfer matrix, rather than the swapping space. Graphically, one has

Now we calculate the average of risk function over all (possible output) unitaries.

We consider two special cases of the training set size, the empty training set with, and the full training set with.

For the case of empty training set,, and the 1-moment integral over the Haar ensemble of

For the case of full training set,, then the learned unitaryobeys, thus

In the above proof, we utilize the equality, and the matrix trace inequality

whereis positive semi-definite with.

For the convenience of analytical calculations, we consider the case oftraining samples, and suppose that the learned unitaryobeys, such thatis aunitary matrix. For simplicity, one suppose that the unitarylocates in a-qudit subsystem. Thus the average risk function overreads

wheredenotes amatrix.

We first calculate the term

For arbitrary local term, one has

Thus

We then calculate the term in Eq. (),

Thus Eq. () turns out to be

Utilizing the inequalityfor positive semi-definite matricesand, one then obtains for a general,

In the special case of, the number of training samples, one then has

The above result indicates that the average risk overapproximates tofor empty training set.

For, the number of training samples, one then has

This indicates that the average risk overapproximates tofor full training set.

The above results are independent of the training set, we thus obtain the following theorem.

We numerically verify the result we obtain in Ineq. (), see Fig..

SECTION: No-free-lunch theory for 2D tensor network machine learning model
Now we proceed to prove the no-free-lunch theorem for the 2D TN based machine learning models. Specifically, we focus on the case of learning target quantum unitary operator based on the input of 2D projected entangled pair states (PEPSs), where the training states are encoded into unitary-embedded PEPs, and the variational structures are comprised of unitary quantum circuits. In constructing PEPSs, we apply a collection of-dimensional unitary tensors which are locally connected to neighboring tensors by virtual bond of dimensionand the initial statesby physical bond of dimension. This arrangement allows for the encoding of information into unitary tensors as input and output. The theorem that will be demonstrated within this chapter is restated as follows:

SECTION: The risk function
We define the quantum Hilbert space for input and output samples byand, and choose a subsetof the samples as training data,

with. Here, for a unitary embedded tensor-network statewith bond dimensionand physical dimensionin a square lattice with volume, the norm of the unitary embedded tensor-network state is exponentially concentrated around one asincreases:

To prove this Chebyshev inequality for PEPS, we just need to compute the variance of the norm, given by. Assuming that each local unitary forms a 1-design, it is easy to verify that. And since each local unitary offorms a unitary 2-design, it will be shown thatby Corollaryin Sec.. Soand the above inequality can be proved.

Assuming that the model is perfectly trained, one then obtains the trained operatoras:

As such, the risk function can be defined as

In the previous work, calculating the second moment integral of 2D random PEPS is transformed into determining the partition function of 2D classical Ising model. The idea is that the second moment integral over Haar measureforming the approximate unitary 2-design can be represented as a site on Ising model lattice with uniform external fields, similar to the 1D case. For a PEPSconsisting of subsystems with periodic boundary conditions in alattice, the coordinate set of sites in the lattice is, andis used to represent the spin state of the site at. Further, a configuration of the lattice is determined when each site takes a particular spin state, which can be written as a vector. Through the above mapping and notation, we have

whererefers to. Here we define the transfer function as

Yet we need to clarify that the notationjust denotes how the local indices ofcontract on thephysical site, rather than the reduced matrix under partial trace, sodenotes the swapping operation on thesite ofand.
The following values can be obtained:

To prove Theorem, one needs to calculate the average of the risk function:

denotes the contribution of one of the various configurations, which now is a function withandacting on. Thus we can obtain the partition function by summing of all the contributions of various. In the subsequent sections, we will demonstrate how to perform the calculation using the polyomino method.

SECTION: The polyominoes
To address the 2D TN problem, we initially introduce a statistical model for enumerating lattice configurations on a two-dimensional plane, known as the polyomino. The focus of this model is on directed figures that encompass a specific number of sites within an infinite square grid. We will find that the partition function of the two-dimensional Ising model can be translated into a problem sets on a periodic plane of an infinite square lattice grid, making the application of polyomino calculation method particularly effective. We first provide some definitions related to polyominoes, and then map our problem to this framework. The formal definition of the directed polyomino(shown in Fig.) is listed as follows:

The polyominoes can be characterized by their areas, perimeters and upper perimeters as their properties, whose formal definitions are as follows:

Then we can enumerate directed polyominoes exactly via their generating function by the following lemma:

This lemma implies that whensuch that, the power seriesconverges to a finite value. We will see the problem of partition function for two-dimensional lattice having a similar power-series form reduces to determining the number of configurations.

SECTION: Apply polyominoes to the calculation of PEPS 2-moment
In Sec., we have mentioned that calculating the integralof PEPS is transformed into calculating the partition function

Heremeans the sum over all configuration, which is a simple form of. If, then we denote, where

Then

As mentioned above, the partition function can be viewed as the sum of the contribution of different. Owing to, one conly needs to consider the configurations that everyon lattice must have either a right or lower-neighbor; otherwise, the contribution of the configurationwill be 0. The Ising model is defined on a lattice with periodic boundaries, so thesesites must form at least one loop on a torus. We refer to these as excited-spin-strings(ESSs), with a formal definition provided in Definition. And for a lattice withsites, each cycle ESS forms a loop with at leastareas.

For a sitewith:

If, we mark the deriction as.

Ifbut, we mark the deriction as.

In this way, we have constructed a directed graph, where the vertex setis the set of all-sites.

Here we explain in details the concept of roots. The roots of polyominoes defined on the plain are different from those on the torus. Directed graphs consisting ofare shown in Fig.. There is only one root if the last vertex has no further extension, while under periodic boundary conditions, the vertices on the boundary can be linked to their counterparts on the opposite side. This connectivity introduces the possibility of theforming a closed loop within the directed graph, leading to that eachcan always reach otherin the loop through the direction of the graph. In such a scenario, allin the loop are the roots of the directed graph.

Now we have the below definition of the excited-spin-string(ESS):

It is appropriate to treat an ESS as a polyomino defined on a torus. We take the count of ESSs originating at (0,0) with areaand upper perimeterbyfor the number of torus polyominoes. The following theorem establishes the connection betweenfor torus polyominoes andfor plane polyominoes:

This theorem demonstrates that every contribution of each configurationcan be limited by their areaand upper perimeterwith two parameter. Consequently, the partition function is effectively bounded by considering all the potential configurations. Moreover, the numberof torus polyominoes can be linked withvia the above inequality allowing us to determine it. According to that we can figure out the upper bound of the 2-moment integralby Corollary.

Here, we only state the conclusions we need. One can find the details of proof for Theoremand Corollaryin Ref.. By applying this corollary, one can prove the concentration of PEPS based on the Chebyshev inequality.

SECTION: Average risk of 2D tensor-network ML model
In the previous section, we have connected the partition function with the torus polyomino. Now we calculate the average of the risk function:

Here for simplicity, we consider the sizeof training set be. Following the similar way as in Eq., we consider the case that a subspace comprising k sites on lattice can be fully trained. By labeling the sites from a particular starting site and arranging them counterclockwise around the center, the trained space can be amalgamated into thetrained sites that approximates asquare. This is because both its upper and left boundaries are no more than, wherecan be expressed as. We can refer to thesesites as the trained zone, as shown in Fig.. Here we denote the coordinates set of thetrained sites in A as.

Suppose that the learned unitary operatorsatisfies the condition, we employ the same technique as in the 1D case to analyze and manipulate this operator,

For arbitrary local tensor, we have

By substituting the above formulas into Eq. () , one obtains

Then base on Eq. (), we can divideinto the following five terms. For the first three items, by applying Corollary, we obtain

For the latter two terms, we have

and

where we denoteas the opposite spin state of, and

Utilizing the inequality

one can represent the termsandby the combination of values derived fromfor, which correspond to the sites within zone, andfor, which correspond to the sites outside the zone. We will first introduce how to calculate the values of the sites within zone.

For the convenience of representing, letdenote an arbitrary product ofwith the elements. Sincefor arbitrary, for fixed element, we find

and

wheremeans. Thus we have

Additionally, the following equation holds,

To determine the values of the sites within zone, we start by designating the top-left corner site asand defineas the set containing the sites,, and. These three sites correspond to the elements of the factor. We initiate the process by expanding the summation over the spins,, and. At this initial stage, only one factor,, depends on. We denoteas. Sinceandmust not be included in the samefactor except for, we can separateinto two distinct factors,and, which containand, respectively. With this setup, we can proceed with the following steps,

For the second equal sign in Eq. (), the eight terms are derived by expanding the three summations within the square brackets. For the third equal sign in Eq. (), each occurrence oforis replaced with, and similarly,oris substituted with, in accordance with Eq. (). This restoration of the two summationsthen allowsto be combined into a single factor,. By applying this process iteratively to the subsequent terms of, we obtain the expression. Then we have

Now we consider the value of the remainingsites outsidefor, i.e., the term.
Given the condition that, here we only need to identify those non-zero configurations outside. If a siteon upper boundary of zoneis, there could exist non-zero configurations containing an ESS rooted at. Similar case occurs for aon left boundary. Thus, different from thecase in Eq. 
(), the non-zero configurations in thelattice outsidein Eq. () include not only the cycle ESSs but also the ESSs possibly rooted at the sites adjacent to the upper and left boundary of, shown in Fig.(a).

These configurations can be divided into two parts, depending on whether they contain cycle ESSs or not, as shown in Fig.(c). For the configuration without cycle ESS, all the ESSs of toric polyominoes are rooted at the sites adjacent to the upper and left boundary of A. The contribution of the i-th ESS rooted at the sites adjacent to the left boundary of A is less than. We assume that all potential sites can serve as roots, so as to cover all possible configurations. By considering all cases of, roots positions and the overlaps with boundary, we obtain

By reflecting theandaxes across the diagonal line and altering the priority of ’right’ and ’down’ directions of the directed graphs on torus, one can calculate the case of ESSs rooted at the upper boundary, as shown in Fig.(b). Consequently

Now we calculate the contribution of configurations containing at least one cycle ESS. Let the toric polyominoes with areaand upper perimeterbe decomposed intoESSs, each with areaand upper perimeter, respectively. We assume that the firstESSs are cycle ESSs. The remainingESSs are rooted at the neighbors of upper and left boundary of. We then obtain

•;

•;

•;Thus the number of toric polyominoes outsideis

Then the generating function obeys

By definition,. We can then obtain the last line of inequality in Eq. () through the following inequality

Actually, for enough large,, we havefor some constant. Otherwise, for a small, one can always find a constantsuch that the factor is less thanas well. Thus the termis bounded by

For the functionin the term, we can just take the sets ofas the ESSs and obtain the result in a similar approach. We obtain that

SECTION: Results and Discussion
Summing over all of the contributing terms, one can obtain the average risk

Thus we have obtained Theorem. In the thermodynamic limit where, one can ignore the higher-order terms and obtains:

This result is shown in Fig..

We consider two special cases, one case is the empty training set and the other one is the full training set.
For empty training set with, the lower bound of the average risk is

For full training set with, the lower bound of the average risk is

Moreover, takingas an instance, numerical simulations for MPS machine learning model was carried out regarding the average risks of 10 unitary matrices, as shown in Fig.. Through varying the training error within the practical learning process, diverse average risk curves were acquired. It is evident that as the training error diminishes, the average risk curves decline correspondingly and approach the analytical results. Our findings fully accord with the implications of being the average lower bound of the risk function.

SECTION: Review of no-free-lunch theorem in quantum machine learning models
The exact formulations of no-free-lunch (NFL) theorems in quantum machine learning models have been introduced in Refs.. Here for completeness, we straightforwardly consider the entanglement-assisted violation of quantum no-free-lunch theorem. One can first define the quantum Hilbert space for input samples by, and define the quantum Hilbert space for output by, wheredenotes the ancillary Hilbert space. Without loss of generality, we suppose that the dimensions of both the input and output space are the same, i.e.,. One can then choose a subsetof the input samples as the training data for quantum machine learning,

with the set size, and.

One assumes that all training data states share the same Schmidt rank.

For perfect quantum learning, the trained unitaryon those training data states should result in the following formula

To quantify the accuracy of the trained unitary, one can define the risk function

wheredenotes the trace norm of. Suppose that the input statesform the approximate unitary-2 design, i.e., approaching to the 2-moment integral of unitary group under the Haar measure. Then the risk function has the following formula

Consider the following three cases:

States inare orthonormal..is thegroup element.

States inare non-orthonormal, but linear independent..is thegroup element.

States inare linear dependent.,denotes the linear independent bases of the training set.is thegroup element.

The goal is to calculate the average risk over all training setsand all (possible output) unitaries,

Consider the case (a): States inare orthonormal. One first calculates the average risk over all possible uniatries,

Since this is independent of the choice of training set, one can then obtain that

Sincecan be arbitrary positive integer that is smaller than, thus for the Schmidt number, i.e., with non-zero bipartite entanglement betweenand, such an average risk over all choices of training set and all possible ground truth unitariescan go beyond the classical no-free-lunch statements.

Similar calculations apply to the cases (b) and (c).

To summary, one obtains the quantum no-free-lunch theory for quantum machine learning models.

Case (a): States inare orthonormal.

Case (b): States inare non-orthonormal, but linear independent..is thegroup element.

Case (c): States inare linear dependent.,denotes the linear independent bases of the training set.is thegroup element.