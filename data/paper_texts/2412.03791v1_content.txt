SECTION: Coordinate In and Value Out: Training Flow Transformers in Ambient Space
Flow matching models have emerged as a powerful method for generative modeling on domains like images or videos, and even on unstructured data like 3D point clouds. These models are commonly trained in two stages: first, a data compressor (a variational auto-encoder) is trained, and in a subsequent training stage a flow matching generative model is trained in the low-dimensional latent space of the data compressor. This two stage paradigm adds complexity to the overall training recipe and sets obstacles for unifying models across data domains, as specific data compressors are used for different data modalities. To this end, we introduce(ASFT), a domain-agnostic approach to learn flow matching transformers in ambient space, sidestepping the requirement of training compressors and simplifying the training process. We introduce a conditionally independent point-wise training objective that enables ASFT to make predictions continuously in coordinate space. Our empirical results demonstrate that using general purpose transformer blocks, ASFT effectively handles different data modalities such as images and 3D point clouds, achieving strong performance in both domains and outperforming comparable approaches. ASFT is a promising step towards domain-agnostic flow matching generative models that can be trivially adopted in different data domains.

SECTION: Introduction
Recent advances in generative modeling have enabled learning complex data distributions by combining both powerful architectures and training objectives. In particular, state-of-the-art approaches for image, videoor 3D point cloudgeneration are based on the concept of iteratively transforming data into Gaussian noise. Diffusion models were originally proposed following this idea and pushing the quality of generated samples in many different domains, including images, 3D point clouds, graphsand video. More recently, flow matchingand stochastic interpolantshave been proposed as generalized formulations of the noising process, moving from stochastic gaussian diffusion processes to general paths connecting a base (Gaussian) and a target (data) distribution.

In practice, these iterative refinement approaches are commonly applied in a low-dimensionalobtained from a pre-trained compressor model. Therefore, the training process for these approaches is composed of two independent training stages: in the first stage, a compressor (VAE, VQVAE, VQGAN) model is trained, using architectures that are specific to the data domain (ConvNets for image data, PointNet for point clouds, etc. ) enforcing a bottleneck on the data dimensionality, with the goal of reducing compute cost of training the subsequent stage. In the second stage, general purpose transformer architectures are used for the generative modeling step, where the distribution of latents is learnt. This type of generative modeling in latent space has become popular in the community due to its computational efficiency benefits obtained from compressed data dimensionality.

However, latent space generative modeling is not without drawbacks. An obvious shortcoming is that latent space generative models cannot benefit from end-to-end optimization, as data compressors and the downstream generative models are trained separately. In particular, two stage approaches are more complex to implement than single stage models and involve tuning several hyper-parameters that can have a big impact on final performance: spatial reduction ratio, adversarial loss weights or KL terms in VAEs. As an illustrative example, setting a high KL weight makes the problem of learning a distribution of latents trivial, yet results in very poor generation results. A very small KL weight on the other hand allows for great reconstruction performance for the first stage but fails to induce a suitable latent space for generative modeling (a dirac delta for each training sample in latent space). Our goal in this paper is to provide a single training stage approach that is domain-agnostic and simple to implement in practice, thus dispensing with the complexities of two stage training recipes and enabling modeling of different data modalities in ambient (data) space.

It is worth noting that training diffusion or flow matching models in ambient space is indeed possible when using domain specific architecture designs and training recipes. In the image domain, approaches have exploited its dense nature and applied cascaded U-Nets, joint training of U-Nets at multiple resolutions, multi-scale lossesor U-Net transformer hybrids architectures, obtaining strong results. However, developing strong domain-agnostic models, using general purposes architectures that can be applied across different data domains remains an important open problem.

In this paper, we answer a three part question:Our goal is to unify different data domains under the same training recipe. To achieve this, we introduce Ambient Space Flow Transformers (ASFT), see Fig.(a). ASFT makes progress towards the goal of unifying flow matching generative modeling across data domains. The key component of our approach is a conditionally independent point-wise training objective that enables training in ambient space and can be densely (continuously) evaluated during inference. In the image domain, this means that we model the probability of a pixel value given its coordinate, which provides granular and precise control over image synthesis, allowing to generate images at different resolution than the one used during training (see Fig.(a)). We show ImageNet-256 generated samples from ASFT in Fig.(b) and 3D point clouds from Objaverse in Fig.(c) (see additional samples in Fig.,,,,). Our contributions are summarized as follows:

We propose ASFT, a flow matching generative transformer that works on ambient space to enable single stage generative modeling on different data domains.

Our results show that ASFT, though domain-agnostic, achieves competitive performance on image and 3D point cloud generation compared with strong domain-specific baselines.

Our point-wise training objective allows for efficient training via sub-sampling dense domains like images while also enabling resolution changes at inference time.

SECTION: Related Work
Diffusion models have been the major catalyzer of progress in generative modeling, these approaches learn to reverse a forward process that gradually adds Gaussian noise to corrupt data samples. Diffusion models are notable for their simple and robust training objective. Extensive research has explored various formulations of the forward and backward processes, particularly in the image domain. In addition, different denoising networks have been proposed for different data domains like images, videos, and geometric data. More recently, flow matchingand stochastic interpolantshave emerged as flexible formulations that generalized Gaussian diffusion paths, allowing to define different paths to connect a base and a target distribution. These types of models have shown incredible results in the image domainwhen coupled with transformer architecturesto model distributions in latent space learnt by data compressors. Note that these approaches train two separate stages/models: first training the data compressor (VAE, VQVAE, VQGAN) and then the generative model, requiring careful hyper-parameter tuning.

In an attempt to unify generative modeling across various data domains, continuous data representationshave shown potential in different approaches: From Data to Functa (Functa), Generative Manifold Learning (GEM), and Generative Adversarial Stochastic Process (GASP)have studied the problem of generating continuous representations of data. More recently Infinite Diffusionand PolyINRhave shown great results in the image domain by modeling images as continuous functions. However, both of these approaches make strong assumptions about image data. In particular,interpolates sparse pixels to an euclidean grid to then process it with a U-Net. On the other hand,uses a patching and 2D convolution in the discriminator. Our approach also relates to DPF, a diffusion model that acts on function coordinates and can be applied in different data domains on a grid at low resolutions (6464). Our approach is able to deal with higher resolution functions (256x256 vs. 64x64 resolution images) on large scale datasets like ImageNet, while also tackling unstructured data domains that do not live on an Euclidean grid (like 3D point clouds).

SECTION: Method
SECTION: Data as CoordinateValue Maps
We interpret our empirical data distributionto be composed of maps. These maps takeas input toas output. For images, maps are defined from 2D pixel coordinatesto corresponding RGB values, thus, where each image is a different map. For 3D point clouds,can be interpreted as a deformation that maps coordinates from a fixed base configuration in 3D space to a deformation value also in 3D space,, as in the image case, each 3D point cloud corresponds to a different deformation map. For ease of notation, we define coordinatesand valuesof any given mapasand, respectively. Fig.(a) shows an example of such maps in the image domain.

In practice, analytical forms for these mapsare unknown. In addition, different from previous approaches, we do not assume that parametric forms of these maps can be obtained, since that would involve a separate training stage fitting an MLP to each map. As a result, we assume we are only given sets of correspondingandpairs resulting from observing these maps at a particular sampling rate (at a particular resolution in the image case). Therefore, we need a to develop an end-to-end approach that can take these collections of coordinate-value sets as training data.

SECTION: Flow Matching and Stochastic Interpolants
We consider generative models that learn to reverse a time-dependent forward process that turns data samples (mapsin our case)into noise.

Both flow matchingand stochastic interpolantformulations build this forward process in Eq.so that it interpolates exactly between data samplesat timeandat time, with. In particular,and. In this case, the marginal probability distributionofis equivalent to the distribution of the probability flow ODE with the following velocity field:

where the velocity field is given by the following conditional expectation,

Under this formulation, samplesare generated by solving the probability flow ODE in Eq.backwards in time (. flowing fromto), where. Note that both the flow matchingand stochastic interpolantformulations decouple the time-dependent process formulation from the specific choice of parametersand, allowing for more flexibility. Throughout the presentation of our method we will assume a rectified flowor linear interpolant pathbetween noise and data, which define a straight path to connect data and noise:. Note that our framework for learning flow matching models for coordinate-value sets can be used with any path definition. Compared with diffusion models, linear flow matching objectives result in better training stability and more modeling flexibilitywhich we observed in our early experiments.

SECTION: Flow Matching for Coordinate-Value Sets
We now turn to the task of formulating a flow matching training objective for data distributions of maps. We recall that in practice we do not have access to an analytical or parametric form for these maps, and we are only given sets of correspondingandpairs resulting from observing the mapping at a particular rate. As a result, we need to formulate a training objective that can take these sets of coordinate-value as training data.

In order to achieve this, we first observe that the target velocity fieldcan be decomposed across both the domain and co-domain of, resulting in a, defined for corresponding coordinate and value pairs of. As an illustrative example in the image domain, this means that thefor any pixel coordinatewith corresponding value, so that. Note that one can always decompose target velocity fields in this way since the time-dependent forward process in Eq.aggregates data and noise(point-wise) across the domain of. Again, using the image domain as an example, the time-dependent forward process of a pixel at coordinateis not dependent on other pixel positions or values.

Our goal now is to formulate a training objective to match this point-wise independent velocity field. We want our neural networkparametrizing the velocity field to be able to independently predict a velocity for any given coordinate and value pairand. However, this point-wise independent prediction is futile without access to additional contextual conditioning information about the underlying functionat time. This is because even if the forward process is point-wise independent, real data exhibits strong dependencies across the domainthat need to be captured by the model. For example, in the image domain, pixels are not independent from each other and natural images show strong both short and long spatial dependencies across pixels. In order to solve this, we introduce a latent variablethat encodes contextual information from a set of given coordinate and value pairs of. This contextual latent variable allows us to formulate the learnt velocity field to befor coordinate-value pairs given. The final point-wise conditionally independent CFM loss, which we denote as CICFM loss is defined as:

where the target velocity fieldis defined as a rectified flowor linear interpolant path:

One of the core challenges of learning this type of generative models is obtaining a latent variablethat effectively captures intricate dependencies across the domain of the function, specially for high resolution stimuli like images. In particular, the architectural design decisions are extremely important to ensure thatdoes not become a bottleneck during training. In the following we review our proposed architecture.

SECTION: Network Architecture
We base our model on the general PerceiverIO designwhich provides a flexible architecture to handle coordinate-value sets of large cardinality (large number of pixels in an image). Fig.illustrates the architectural pipeline of ASFT. At a high level, our encoder network takes a set of coordinate-value pairs and encodes them to learnable latents through cross-attention. These latents are then updated through several self-attention blocks to provide the final latents. To decode the velocity field for a given coordinate-value pair we perform cross attention to, generating the final point-wise prediction for the velocity field.

The encoder of a vanilla PerceiverIO relies solely on cross-attention to the latentsto learn spatial connectivity patterns between input and output elements, which we found to introduce a strong bottleneck during training. To ameliorate this, we make two key modifications to boost the performance. Firstly, our encoder utilizes spatial aware latents where each latent is assigned a “pseudo” coordinate. Coordinate-value pairs are assigned to different latents based on their distances on coordinate space. During encoding, coordinate-value pairs interact with their assigned latents through cross-attention. In particular, the learnable latentcross-attends to input coordinate-value pairs of noisy data at a given timestep. Latent vectors are spatial-aware, this means that each of thelatents only attends to a set of neighboring coordinate-value pairs. Latent vectors are then updated using several self-attention blocks. These changes in the encoder allow the model to effectively utilize spatial information while also saving compute when encoding large coordinate-value sets on ambient space. In the decoder, a given coordinate-value pair cross attends toas in the original PerceiverIO. However, we found that a multi-level decoding strategy, which not only cross attends to the latents in the final layer but also the latent from the intermediate self-attentions layer is helpful. In particular, a given coordinate-value pair cross attends to latents from subsequent encoder layers sequentially to progressively refine the prediction. Finally, following previous work, we apply AdaLN-Zero blocks for conditioning both on timestepand class labels whenever needed (for ImageNet experiments). More architectural details can be found in App..

SECTION: Experiments
We evaluate ASFT on two challenging problems: image generation (FFHQ-256, LSUN-Church-256, ImageNet-128/256) and 3D point cloud generation (ShapeNetand Objaverse). Note that we use the same training recipe both tasks, adapted for changes in coordinate-value pair dimensions in different domains. See App.for more implementation details and training settings.

ASFT enables practitioners to define the number of coordinate-value pairs to be decoded during training. In our experiments, we set the number of decoded coordinate-value pairs to 4096 for images with resolution 128128, 8192 for images with resolution 256256, and 2048 for point clouds unless mentioned otherwise. On image generation, we train models with small (S), base (B), large (L), and extra large (XL) sizes. For 3D point cloud generation we set the parameter count to match the model size in previous state-of-the-art approaches (LION). Detailed configuration for all models can be found in Appendix. During inference, we adopt black-box numerical ODE solver with maximal NFE as 100 for image generationand an SDE sampler with 1000 steps for point cloud generation to match the settings in.

SECTION: Image Generation in Function Space
Given that ASFT is a generative model for maps we compare it with other generative models of the same type, namely approaches that operate in function spaces. Tab.shows a comparison of different image domain specific as well as function space models (approaches that model infinite-dimensional signals). ASFT surpasses other generative models in function space on both FFHQand LSUN-Churchat resolution. Compared with generative models designed specifically for images, ASFT also achieves comparable or better performance. When scaling up the model size, ASFT-L demonstrates better performance than all the baselines on FFHQ-256 and Church-256, indicating that ASFT can benefit from increasing model sizes.

SECTION: ImageNet
We also evaluate the performance of ASFT on large scale and challenging settings, we train ASFT on ImageNet at both 128128 and 256256 resolutions. On ImageNet-128, shown in Tab., ASFT achieves an FID of, which is a a competitive performance in comparison to diffusion or flow-based generative baselines including ADM, CDM, and RINwhich use domain-specific architectures for image generation. Besides, comparing to PolyINRwhich also operates on function space, ASFT achieves competitive FID, while obtaining better IS, precision and recall. The experimental results demonstrate the capabilities of ASFT in generating realistic samples on large scale datasets.

We report results of ASFT for ImageNet-256 on Tab.. Note that ASFT is slightly outperformed by latent space models like DiTand SiT. We highlight that these baselines rely on a pre-trained VAE compressor that was trained on datasets that are much larger than ImageNet, while ASFT was trained only with ImageNet data. In addition, ASFT achieves better performance than many of the baselines trained only with ImageNet data including ADM, CDMand Simple Diffusion (U-Net)which all use CNN-based architectures specific for image generation. Note that this is consistent with the results show in Tab., where ASFT outperforms all function space approaches. When comparing with approaches using transformers architectures we find that ASFT obtains performance comparable to RINand HDiT, with slightly worse FID and slightly better IS. However, ASFT is a domain-agnostic architecture that can be trivially applied to different data domains like 3D point clouds (see Sect.and). For completeness, we also include a comparison with very large U-Net transformer hybrid models, Simple Diffusion (U-ViT 2B) and VDM++ (U-ViT 2B) which both use approx.more parameters than ASFT-XL, unsurprisingly, these much bigger capacity models outperform ASFT (see App.for a more detailed comparison including training settings). We highlight that the simplicity of implementing and training ASFT models in practice, and the trivial extension to different data domains (as shown in Sect.) are strong arguments favouring our model. Finally, comparing withPolyINRwhich is also a function space generative model we also find comparable performance, with slight worse FID but better Precision and Recall. It is worth noting thatapplies a pre-trained DeiT model as the discriminator. Whereas our ASFT makes no such assumption about the function or pre-trained models, enabling to trivially apply ASFT to other domains like 3D point clouds (see Sect.).

To demonstrate the scalability of ASFT we train models of different sizes including small (S), base (B), large (L), and extra-large (XL) on ImageNet-256. We show the performance of different model sizes using FID-50K in Fig.(a). We observe a clear improving trend when increasing the number of parameters as well as increasing training steps. This demonstrates that scaling the total training Gflops is important to improved generative results as in other ViT-based generative models. Due to the flexibility of cross-attention decoder in ASFT, one can easily conduct random sub-sampling to reduce the number of decoded coordinate-value pairs during training which significantly saves computation. Fig.(b) shows how number of decoded coordinate-value pairs affects the model performance as well as Gflops in training. An image of resolution 256256 contains 65536 pixels in total which is the maximal number of coordinate-value pairs during training. As see in Fig.(b), a model decoding 4096 coordinate-value pairs saves more than 20% Gflops over one decoding 16384. This provides us with an effective training recipe, which saves computation by only decoding a subset ofof the image pixels during training. Interestingly, we see a performance drop when densely decoding 16384 coordinate value pairs. We hypothesize this could be due to optimization challenges of decoding large numbers of pairs and leave further analysis for future work.

SECTION: ShapeNet
To show the domain-agnostic prowess of ASFT we also tackle 3D point cloud generation on ShapeNet. Note that our model does not require training separate VAEs for point clouds, tuning their corresponding hyper-parameters or designing domain specific networks. We simply adapt our architecture for the change in dimensionality of coordinate-value pairs (for images tofor 3D point clouds.). Note that for 3D point clouds, the coordinates and values are equivalent. In this setting, we compare baselines including LIONwhich is a recent state-of-the-art approach that models 3D point clouds using a latent diffusion type of approach. Followingwe report MMD, COV and 1-NNA as metrics. To have a straightforward comparison with baselines, we train ASFT-B with to approximately match the number of parameters as LION(110M for LION vs 108M for ASFT) on the same datasets (using per sample normalization as in Tab. 17 in). We show results for category specific models and for an unconditional model jointly trained on 55 ShapeNet categories in Tab.. ASFT-B obtains strong generation results on ShapeNet despite being a domain agnostic approach and outperforms LION in most datasets and metrics. Note that ASFT-B has comparable number of parameters and the same inference settings than LION so this is fair comparison. Finally, we also report results for a larger model ASFT-L (withthe parameter count as LION) to investigate how ASFT improves as with increasing model size. We observe that with increasing model size, ASFT typically achieves better performance than the base version. This further demonstrates scalability of our model on ambient space of different data domains.

SECTION: Objaverse
We also experiment ASFT on large-scale 3D point generation conditioned on images. To this end, we train an image-to-point-cloud generation model on Objaverse, which contains 800k 3D objects of great variety. In particular, conditional information (i.e., an image) is integrated to our model through cross-attention. For each object in Objaverse, we sample point cloud withpoints. To get images for conditioning, each object is rendered with 40 degrees field of view,resolution, at 3.5 units on the opposite sides ofandaxes looking at the origin. We extract features via DINOv2which is concatenated with Plucker ray embeddingof each patch in DINOv2 feature. In each block, the learnable latent vectorcross attends to image feature.

Tab.lists the performance of ASFT in comparison of recent baselines on Objaverse. We report ULIP-Iand P-FIDfollowing CLAY. PointNet++is employed to evaluate P-FID. ULIP-I is an analogy to CLIP for text-to-image generation. ULIP-I is measured as the cosine similarity between point-cloud features from ULIP-2 modeland image features from CLIP model. Numbers of baseline models are directly borrowed from CLAY. We calculate the metrics of our ASFT on 10k sampled point clouds. In our case, P-FID is measured on point clouds with 4096 points following Shape-Ewhile ULIP-I is measured on point clouds with 10k points following ULIP-2. Note that since CLAYis not open-source, we do not have the access to the exact evaluation setting or the conditional images rendered from Objaverse. But all evaluation settings of ASFT are provided for reproduction purpose. As shown in Tab., our ASFT achieves strong performance on large-scaled image-conditioned 3D generative tasks. Compared with CLAY, which is a 2-stage latent diffusion model, ASFT demonstrates very strong performance on both ULIP-I and P-FID. Fig.andshow additional examples of sampled point clouds and corresponding conditional images. As shown, ASFT learns to generate 3D objects with rich details that match the conditional images ultimately being able to generate a continuous surface.

SECTION: Resolution Agnostic Generation
An interesting property of ASFT is that it decodes each coordinate-value pair independently, allowing resolution to change during inference. At inference the user can define as many coordinate-value pairs as desired where the initial value of each pair atis drawn from a Gaussian distribution. We show qualitative results of resolution agnostic generation for both images and point clouds. Fig.(a) show images sampled at resolution,, and(together with theirresolution counterparts generated from the same seed) from ASFT trained on ImageNet-256. Even though the model has not been trained with any samples at higher resolutions, it can still generate realistic images with high-frequency details. Fig.(b) shows point cloud withandpoints from ASFT trained on Objaverse with onlypoints points per sample (we visualize the generatedpoint could generated from the same seed). Similarly, ASFT generates dense and realistic point cloud in 3D without actually being trained on such high density points. These results show that ASFT is not trivially overfitting to the training set of points but rather learning a continuous density field in 3D space from which an infinite number of points could be sampled. Generally speaking, this also provides the potential to efficiently train flow matching generative models without the need to use large amounts of expensive high resolution data, which can be hard to collect in data domains other than images.

SECTION: Conclusion
We introduced Ambient Space Flow Transformers (ASFT), a flow matching generative model designed to operate directly in ambient space. Our approach dispenses with the practical complexities of training latent space generative models, such as the dependence on domain-specific compressors for different data domains or tuning of hyper-parameters of the data compressor (adversarial weight, KL term, etc.). We introduced a conditionally independent point-wise training objective that decomposes the target vector field and allows to continuously evaluate the generated samples, enabling resolution changes at inference time. This training objective also improves training efficiency since it allows us to sub-sample the target vector field during training. Our results on both image and 3D point cloud benchmarks show the strong performance of ASFT as well as its trivial adaption across modalities. In conclusion, ASFT represents a promising direction for flow matching generative models, offering a powerful and domain-agnostic framework. Future work could explore further improvements in training efficiency and investigate co-training of multiple data domains to enable multi-modality generation in an end-to-end learning paradigm.

SECTION: Acknowledgements
We thank Jiatao Gu for helpful discussions and Jen-Hao Rick Chang for helping set up experiments on Objaverse.

SECTION: References
SECTION: Model Configuration and Training Settings
We provide detailed model configurations and training settings of ASFT for image (Tab.) and point cloud (Tab.) generation. For image generation, we develop model sizes small (S), base (B), large (L), and extra large (XL) to approximately match the number of parameters in previous works. Similarly, for point cloud generation, we train a base sized model roughly matching the number of parameters in LION(110M parameters), and a ASFT-L which contains about twice the number of parameters as ASFT-B. For image experiments we implement the “psuedo” coordinate of latents as 2D grids and coordinate-value pairs are assigned to different latents based on their distances to the latent coordinates. Whereas in point cloud generation, since calculating the pair-wise distances in 3D space can be time consuming, we assign input elements to latents through a hash code, so that neighboring input elements are likely (but not certainly) to be assigned to the same latent token. We found that the improvements of spatial aware latents in 3D to not be as substantial as in the 2D image setting, so we report results with a vanilla PerceiverIO architecture for simplicity. To embed coordinates, we apply standard Fourier positional embeddingfor ambient space coordinate input in both encoder and decoder. The Fourier positional embedding is also applied to the “psuedo” coordinate of latents. On image generation, we found that applying rotary positional embedding (RoPE)slightly improves the performance of ASFT. Therefore, RoPE is employed for largest ASFT-XL model. For all the models including image and 3D point cloud experiments, we share the following training parameters except theacross different experiments. On image generation, all models are trained with batch size 256, except for ASFT-XL reported in Tab.and Tab., which are trained for 1.7M steps with batch size 512. On ShapeNet, ASFT models are trained for 800K iterations with a batch size of 16. On Objaverse, we follow the configuration ASFT-XL listed in Tab.and the model is trained with batch size 384 for 500k iterations.

In Tab., we also compare the size of models trained on ImageNet-256, training cost (product of batch size and training iterations), and inference cost (NFE, number of function evaluation). Note that for models that achieve better performance than ASFT, many of them are trained for more iterations. In addition, at inference time ASFT applies simple first order Euler sampler with 100 sampling steps, which uses less NFE than many other baselines.

SECTION: Performance vs Training Compute
We compare the performance vs total training compute of ASFT and DiTin Gflops. ASFT-linear denotes the variant of ASFT where the cross-attention in the spatial aware encoder is replaced with grouping followed by a linear layer. We found this could be an efficient variant of standard ASFT while still achieving competitive performance. Fig.shows the comparison of the training compute in Gflops vs FID-50K between ASFT and latent diffusion model DiTincluding the tranining compute of the first stage VAE. We estimate the training cost of VAE based the model card listed in HuggingFace. As shown, the training cost of VAE is not negligible and reasonable models with FIDcan be trained for the same cost.

Admittedly, under equivalent training Gflops, ASFT achieves comparable but not as good performance as DiT in terms of FID score (with a difference smaller thanFID points). We attribute this gap to the fact that DiT’s VAE was trained on a dataset much larger than ImageNet, using a domain-specific architecture (a convolutional U-Net). We believe that the simplicity of implementing and training ASFT models in practice, and the trivial extension to different data domains (as shown in Sect.) are strong arguments to counter an FID difference of smaller thanpoints.

SECTION: Architecture Ablation
We also provide an architecture ablation in Tab.showcasing different design decisions. We compare two variants of Transformer-based architectures ASFT: a vanilla PerceiverIO that directly operates on ambient space, but without using spatial aware latents and ASFT. As it can be seen, the spatially aware latents introduced in ASFT greatly improve performance across all metrics in the image domain, justifying our design decisions. We note that we did not observe the same large benefits for 3D point clouds, which we hypothesize can be due to their irregular structure.

SECTION: Resolution Agnostic Generation: Quantitative Analysis
Due to the fact that ASFT decodes each coordinate-value pair independently given, during inference time one can decode as many coordinate-value pairs as desired, therefore allowing resolution to change during inference. We now quantitatively evaluate the performance of ASFT in this setting. In Tab.we compare the FID of different recipes. First, ASFT is trained on FFHQ-256 and bilinear or bicubic interpolation is applied to generated samples to get images at 512. On the other hand, ASFT can directly generate images at resolution 512 by simply increasing the number of coordinate-value pairs during inference without further tuning. As shown in Tab., ASFT achieves lower FID when compared with other manually designed interpolation methods, showcasing the benefit of developing generative models on ambient space.

SECTION: Additional ImageNet Samples
We show uncurated samples of different classes from ASFT-XL trained on ImageNet-256 in Fig.and Fig.. Guidance scales in CFG are set asfor loggerhead turtle, macaw, otter, coral reef andotherwise.

SECTION: Additional ShapeNet Samples
We show uncurated samples from ASFT-L trained jointly on 55 ShapeNet categories in Fig..

SECTION: Additional Objaverse Samples
We show conditional images and generated samples from ASFT trained on Objaverse in Fig.and.