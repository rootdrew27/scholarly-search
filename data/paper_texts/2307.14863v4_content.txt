SECTION: IML-ViT: Benchmarking Image Manipulation Localization by Vision Transformer

Advanced image tampering techniques are increasingly challenging the trustworthiness of multimedia, leading to the development of Image Manipulation Localization (IML).
But what makes a good IML model? The answer lies in the way to capture artifacts. Exploiting artifacts requires the model to extract non-semantic discrepancies between manipulated and authentic regions, necessitating explicit comparisons between the two areas. With the self-attention mechanism, naturally, the Transformer should be a better candidate to capture artifacts.
However, due to limited datasets, there is currently no pure ViT-based approach for IML to serve as a benchmark, and CNNs dominate the entire task. Nevertheless, CNNs suffer from weak long-range and non-semantic modeling.
To bridge this gap, based on the fact that artifacts are sensitive to image resolution, amplified under multi-scale features, and massive at the manipulation border, we formulate the answer to the former question as building a ViT with high-resolution capacity, multi-scale feature extraction capability, and manipulation edge supervision that could converge with a small amount of data. We term this simple but effective ViT paradigm IML-ViT, which has significant potential to become a new benchmark for IML. Extensive experiments on three different mainstream protocols verified our model outperforms the state-of-the-art manipulation localization methods.
Code and models are available athttps://github.com/SunnyHaze/IML-ViT.

SECTION: 1Introduction

With the advances in image editing technology like Photoshop, Image Manipulation Localization (IML) methods have become urgent countermeasures to cope with existing tampered images and avoid security threats[33]. Effective IML methods play a crucial role in discerning misinformation and have the potential to contribute to the safety of multimedia world. As shown in Figure1, the IML task aims to detect whether images have been modified and to localize the modified regions at the pixel level.
Image manipulation can be generally classified into three types[36,33]: (1)splicing: copying a region from an image and pasting it to another image. (2)copy-move: cloning a region within an image. (3)inpainting: erasing regions from images and inpaint missing regions with visually plausible contents.

As shown in Table1, most existing methods for IML tasks greatly benefit from tracing artifacts with various CNN-based feature extractors. “Artifacts” refer to unique visible traces (see Figure1) and invisible low-level feature inconsistencies (e.g., noise or high-frequency) resulting from manipulation.
As tampering aims to deceive the audience by creating semantically meaningful and perceptually convincing images, visual traces typically manifest at a non-semantic level, distributed in textures around the manipulated area.
Additionally, low-level features, like noise inconsistencies introduced by different cameras, can also serve as crucial evidence to reveal manipulated regions within the authentic area.
Thus, based on previous experiences,the key to IML lies in capturing the artifacts by identifying non-semantic visible traces and low-level inconsistencies.

However, convolution propagates information in acollectivemanner, making CNNs more suitable for semantic-related tasks, such as object detection, rather than tracing non-semantic artifacts that often surround an object. Further, to identify low-level inconsistencies, we need to explicitly compare the relationships between different regions. But in deeper networks, CNNs may overlook global dependencies[26], rendering them less effective in capturing differences between regions.
Given the weaknesses of CNN in non-semantic and long-distance modeling, we ask:Is there any other optimal backbone for solving IML tasks?

Considering the goal of capturing the feature discrepancies between the manipulated and authentic regions, we argue that self-attention should be a better solution regarding IML.As self-attention can explicitly model relationships between any areas regardless of their visual semantic relevance, especially for non-adjacent regions.The performance boost achieved by SPAN[15]highlights the effectiveness of integrating self-attention structures into convolutional layers.
Furthermore, as artifacts are often distributed at the patch level rather than at the pixel or image level, Vision Transformer (ViT)[8]naturally becomes the ideal choice to trace artifacts and make comparisons.

While ViT may be suitable for IML tasks, directly applying the original ViT architecture is insufficient. We suggest that IML involves three key discrepancies from traditional segmentation tasks, which also have not yet received sufficient attention in previous IML methods, as supported by Table 1. These discrepancies are:

High ResolutionWhile semantic segmentation and IML share similar inputs and outputs, IML tasks are more information-intensive, focusing on detailed artifacts rather than macro-semantics at the object level. Existing methods use various extractors to trace artifacts, but their resizing methods already harm these first-hand artifacts.
Therefore, preserving theoriginal resolutionof the images is crucial to retain essential artifacts for the model to learn.

Edge SupervisionAs mentioned earlier, IML’s primary focus lies in detecting the distinction between the tampered and authentic regions. This distinction is most pronounced at the boundary of the tampered region, whereas typical semantic segmentation tasks only require identifying information within the target region. From another perspective, it becomes evident that visible artifacts are more concentrated along the periphery of the tampered region rather than within it (as shown in Figure1). Consequently, the IML task must guide the model to concentrate on the manipulated region’s edges and learn its distribution for better performance.

Multi-scale SupervisionThe percentage of tampered area to the total area varies significantly across different IML datasets. CASIAv2[7]contains a considerable amount of sky replacement tampering, whereas Defacto[27]mostly consists of small object manipulations. On average, CASIAv2 has 7.6% of pixels as tampered areas, while Defacto has only 1.7%. Additionally, IML datasets are labor-intensive and often limited in size, which poses challenges in bridging the gap between datasets.
Therefore, incorporating multi-scale supervision from the pre-processing and model design stages is essential to enhance generalization across different datasets.

In this paper, we present IML-ViT, an end-to-end ViT-based model that solves IML tasks. Regarding the proposed three key discrepancies, we devise IML-ViT with the following components:
1) a windowed ViT which acceptshigh-resolutioninput. Most of the global attention block is replaced with windowed attention as the trade-off for time complexity. We initialize it with Masked Autoencoder (MAE)[14]pre-trained parameters on ImageNet-1k[5]; 2) a simple feature pyramid networt (SFPN)[20]to introducemulti-scale supervision; 3) a morphology-based edge loss strategy is proposed to ensureedge supervision. The overview of IML-ViT is shown in Figure2.

In this manner, without any specialized modules, IML-ViT offers a general ViT structure for IML tasks. In other words,IML-ViT proves that IML tasks can be solved without hand-crafted features or deliberate feature fusion process, promoting future IML methods into a more generalizable design paradigm.

To the best of our knowledge, ObjectFormer[34], TransForensics[13], and TruFor[11]are the only Transformer-related models solving the IML tasks. However, their backbone distinguishes significantly from vanilla ViT, as will be explained in Section2. Thus, IML-ViT can be regarded as the pioneering model utilizing a vanilla ViT as the backbone for IML tasks.

Currently, the evaluation protocol for IML tasks is rather chaotic. To bring faithful evaluations and establish IML-ViT as the benchmark model, we demarcate existing messy evaluation settings into three mainstream protocols and conduct comprehensive experiments across these protocols. The extensive experiment results demonstrate that IML-ViT has surpassed all SoTA (state-of-the-art) models, thereby validating the reliability of the three proposed key essences of IML. Thus, we believe that IML-ViT is a powerful candidate to become a new SoTA model for IML.

In summary, our contributions are as follows:

We reveal the significant discrepancies between IML and traditional segmentation tasks by raising the three essences, which were overlooked by previous studies: high resolution, multi-scale, and edge supervision.

Aiming at three essences, we modify the components of ViT and establish the IML-ViT, the first ViT-based model for image manipulation localization.

Extensive experiments show that IML-ViT outperforms state-of-the-art models in bothand AUC scores on various protocols. This verifies the solidity of the three essences we proposed.

We vanish the evaluation barrier for future studies by demarcating existing evaluation settings into three mainstream protocols and implementing cross-protocols-comparisons.

SECTION: 2Related Works

Paradigm of IMLResearch in the early years focused on a single kind of manipulation detection, with studies on copy-move[4,30], splicing[3,16,17], and removal (Inpainting)[45], respectively. However, since the specific type of tampering is unknown in practice, after 2018, general manipulation detection has become the focus.
Many existing works follow the paradigm of “feature extraction + backbone inference”, especially extractors to exploit tamper-related information from artifacts. CR-CNN[38]has a noise-sensitive BayarConv[1]as the first convolution layer. RGB-N networks[43]develop an SRM filter to mine the difference in noise distribution to support decision-making. ManTra-Net[36]and SPAN[15]combined SRM, BayarConv, and as the first layer of their model. Besides noise-related extractors, ObjectFormer[34]employs a DCT module to extract high-frequency features, which are then combined with RGB features and fed into a transformer decoder. And MVSS-Net[2]combines a Sobel-supervised edge branch and a BayarConv noise branch with dual attention to fuse them. Nevertheless, a feature may only be effective for a single type of tampering, e.g., noise is more sensitive to splicing from different images but less effective for copy-move from the same image. Recently, TruFor[11]and NCL[41]are the first to explore utilizing contrastive learning to extract features instead of manually designed filters. Proposed IML-ViT also aims to step out of the paradigm of “extraction + fusion” and let the model itself learn as much knowledge as possible from the datasets rather than rely onpriori knowledge.

Transformer-based IML methodAt present, there are three Transformer-based models in the field of IML, namely ObjectFormer[34]TransForensics[13], and TruFor[11].
Though named “Trans” or “Former”, these models are hardly in line with vanilla ViT in overall structures and design philosophies.
In particular, different from ViT directly embedding the patched images before encoding, the first two methods utilize several CNN layers to extract feature maps initially and subsequently employ Transformers for further encoding, leading to neglecting crucial first-hand low-level information. On the other hand, TruFor follows SegFormer[37]’s encoder, using convolution layers instead of position embedding to integrate the position information for Transformer blocks, which overlooked key global dependencies to capture differences between regions.

Moreover, in ObjectFormer’s encoder, the “query” inputs are learnable vectors representing object prototypes, not image embeddings. As a result, it focuses on capturing dependencies between object prototypes and image tokens, whereas a standard ViT encoder solely models the relationship between image embeddings. Besides, ObjectFormer is pre-trained with a large tampering-oriented synthesized private dataset, while IML-ViT achieves better performance with pre-training on the more accessible ImageNet-1k dataset.

Further, TransForensics has a different way to apply Transformer blocks. While ViT uses these blocks sequentially, TransForensics employs them in parallel, wherein each feature map of an FCN output is decoded with a Transformer block, and then fused for the final output.

In short, IML-ViT can be considered the first IML method with a vanilla ViT as its backbone and could easily benefit from recently advanced algorithms related to ViT, proving that IML tasks do not require complex designs.

SECTION: 3Proposed Method

In this section, we introduce our powerful IML-ViT paradigm, as shown in Figure2, it consists of three main components: (1) a windowed ViT to balance the high-resolution inputs and the space complexity; (2) asimple feature pyramid network(SFPN) to introduce multi-scale features; and (3) a lightweight MLP decoder head with additional edge supervision, which aids in focusing on artifact-related features and ensures stable convergence.

SECTION: 3.1ViT Backbone

High ResolutionThe ViT Encoder aims to mine the detailed artifacts and explore the differences between the suspicious areas. Thus, it is essential to preserve theoriginalresolution of each image to avoid downsampling that could potentially distort the artifacts. However, when training in parallel, all images within a batch must have the same resolution. To reconcile these demands, we adopt a novel approach that has not been applied to any IML method before. Rather than simply rescaling images to the same size, we pad images and ground truth masks with zeros and place the image on the top-left side to match a larger constant resolution. This strategy maintains crucial low-level visual information of each image, allowing the model to explore better features instead of depending on handcrafted prior knowledge. To implement this approach, we first adjust the embedding dimensions of the ViT encoder to a larger scale.

Windowed AttentionTo balance the computation cost from high resolution, we adopt a technique from previous works[21,20], which periodically replaces part of the global attention blocks in ViT with windowed attention blocks. This method ensures global information propagation while reducing complexity. Differing from Swin[23], this windowed attention strategy is non-overlapping.

MAE Pre-trainWe initialize the ViT with parameters pre-trained on ImageNet-1k[5]with Masked Auto Encoder (MAE)[14]. This self-supervised method can alleviate the over-fitting problem and helps the model generalize, supported by Table9.

More specifically, we represent input images as, and ground truth masks as, whereandcorrespond to the height and width of the image, respectively. We then pad them toand. Balance with computational cost and the resolution of datasets we employ in Table2, we takeas constants in our implementation. Thenis passed into the windowed ViT-Base encoder with 12 layers, with a complete global attention block retained every 3 layers. The above process can be formulated as follows:

wheredenotes the ViT, andstands for encoded feature map. The number of channels, 768, is to keep the information density the same as the RGB image at the input, as.

SECTION: 3.2Simple Feature Pyramid Network

To introduce multi-scale supervision, we adopt thesimple feature pyramidnetwork (SFPN) after the ViT encoder, which was suggested in ViTDet[40].
This method takes the single output feature mapfrom ViT, and then uses a series of convolutional and deconvolutional layers to perform up-sampling and down-sampling to obtain multi-scale feature maps:

Wheredenotes the convolution series, andis the output channel dimension for each layer in SFPN.
This multi-scale method does not change the base structure of ViT, which allowed us to easily introduce recently advanced algorithms to the backbone.

SECTION: 3.3Light-weight Predict Head

For the final prediction, we aimed to design a model that is simple enough to reduce memory consumption while also demonstrating that the improvements come from the advanced design in the ViT Encoder and the multi-scale supervision. Based on these ideas, we adopted the decoder design from SegFormer[37], which outputs a smaller predicted maskwith a resolution of. The lightweight all-MLP decoder first applies a linear layer to unify the channel dimension. It then up-samples all the features to the same resolution ofwith bilinear interpolation, and concatenates all the features together.
Finally, a series of linear layers is applied to fuse all the layers and make the final prediction. We can formulate the prediction head as follows:

Here,represents the predicted probability map for the manipulated area;denotes concatenation operation, andrefers to an MLP module. Detailed structure and analysis are illustrated in Figure3.

SECTION: 3.4Edge Supervision Loss

To account for the fact that artifacts are typically more prevalent at the edges of tampered regions, where the differences between manipulated and authentic areas are most noticeable, we developed a strategy that places greater emphasis on the boundary region of the manipulated area. Specifically, we generate a binary edge maskfrom the original mask imageusing mathematical morphology operations including dilation () and erosion ()[32], followed by taking the absolute values of the result. The formula we use to generate the edge mask is:

where,generates acrossmatrix, where only thecolumn androw have a value of 1, while the rest of the matrix contains 0s. The integer valueis selected to be approximately equal to the width of the white area in the boundary mask. Examples of the edge mask generated using this approach are shown in Figure4.

Combined LossTo compute the loss function, we first pad the ground-truth maskand the edge maskto the size of, and refer to them asand, respectively. We then calculate the final loss using the following formula:

wheredenotes the point-wise product, which masks the original image. Bothandare binary cross-entropy loss functions, andis a hyper-parameter that controls the balance between the segmentation and edge detection losses. By default, we searched the optimalto guide the model to focus on the edge regions, which is supported by Figure5. We choose a larger value foralso for two reasons: (1) to emphasize the boundary region, and (2) to balance the significant number of zeros introduced by zero-padding.

While the proposed edge loss strategy is straightforward, as we will discuss in the Experiments section (Figure8), it remarkably accelerates model convergence, stabilizes the training process, and mitigates potential NaN issues. Therefore, we consider this strategy a powerful prior knowledge for IML problems, deserving attention in future research.

SECTION: 4Experiments

SECTION: 4.1Experimental Setup

Evaluation barrier for IMLWhile recent studies have introduced numerous SoTA models, comparing them on an equal footing remains challenging. This is due to the following reasons: 1) lack of publicly available code for the models and training processes[15,2]; 2) utilization of massive synthesized datasets that are inaccessible to the wider research community[36,39,34];
3) training and testing datasets often vary across different papers, also bringing difficulty for comparison.

Datasets and Evaluation ProtocolTo facilitate reproducibility and overcome the existing evaluation barrier, we demarcate existing mainstream IML methods into three distinct protocols based on different partitions of datasets. Subsequently, we compare IML-ViT against SoTA methods with these three protocols, as shown in Table2and Table3. We followed MVSS-Net[2]to create Defacto-12k dataset. More details will be discussed in Section4.2.

Evaluation CriteriaWe evaluate our model using pixel-levelscore with a fixed thresholdand Area Under the Curve (AUC), which are commonly used evaluation metrics in previous works. Both of them are metrics where higher values indicate better performance. However, it’s worth noting that AUC can be influenced by excessive true-negative pixels in IML datasets, leading to an overestimation of model performance. Nevertheless, our model achieves SoTA performance in bothscore and AUC.

ImplementationWe pad all images to a resolution of 1024x1024, except for those that exceed this limit. For the larger images, we resized them to the longer side to 1024 and maintained their aspect ratio. During training, following MVSS-Net[2], common data augmentation techniques were applied, including re-scaling, flipping, blurring, rotation, and various naive manipulations (e.g., randomly copy-moving or inpainting rectangular areas within a single image).
We used the AdamW optimizer[25]with a base learning rate of 1e-4, scheduled with a cosine decay strategy[24]. The early stop technique was employed during training.

ComplexityTraining IML-ViT with a batch size of 2 per GPU consumed 22GB of GPU memory per card. Using four NVIDIA 3090 GPUs, the model was trained on a dataset of 12,000 images over 200 epochs, taking approximately 12 hours. For inference, a batch size of 6 per GPU required 20GB of GPU memory, with an average prediction time of 0.094 seconds per image. Reducing the batch size to 1 decreased the GPU memory requirement to 5.4GB. We also compare the number of parameters and FLOPs with SoTA models in Table4and achieve highly competitive results.

SECTION: 4.2Compare with SoTA (See Table3for protocols)

Protocol No.1Since MVSS-Net[2]has already conducted a detailed evaluation on a fair cross-dataset protocol and later works[41]followed their setting, we directly quote their results here and train our models with the same protocol.
The results measured byscore are listed respectively in Table5.
We also compare this with some closed-source methods that only report their AUC tested on CASIAv1 in Table7.

Overall, our model achieves SoTA performance on this cross-dataset evaluation protocol. Figure6illustrates that our model portrays high-quality and clear edges under different preferences of manipulation types.

Protocol No.2TruFor[11]is a recent strong method with extensive experimental results, training on six relatively large IML datasets proposed by CAT-Netv2[18]. In our aim to establish IML-ViT as the benchmark model, we adopt their protocol to compare our model. We outperform them on four benchmark datasets. Details are shown in Table8.

Protocol No.3TransForensic[13], ObjectFormer[34], HiFi-Net[12]and CFL-Net[28]reported their performance based on mixed datasets.
They randomly split these datasets into training/validation/testing splits, causing the random splits performed by others to potentially differ, leading to a certain degree of unfairness.
Therefore, we do not recommend using this protocol in future work. However, for the sake of comparison with these state-of-the-art models, we also test IML-ViT following this protocol.
Besides, note that HiFi-Net (1,710K images) and Objectformer (62k images) involve large IML datasets for pre-training, then tune on the specific small dataset, while we only pre-train with ImageNet-1k. Thus, we directly use results from mixed public IML datasets(14k images) to compare with them. Otherwise, it’s easy to overfit on small datasets. In summary, experiment results in Table6show that, under this reasonable evaluation criteria, IML-ViT also outperforms these SoTA methods.

SECTION: 4.3Ablation Studies

To evaluate the contributions of each component to the model performance, we conducted experiments with multiple settings and compared them with afull setupto test the four aspects we are most concerned about.
Forinitialization, besidesfull setupwith MAE pre-training on ImageNet-1k, we test Xavier[9]initialization and ordinary ViT pre-training on ImageNet-21k.
To explore the impact ofhigh resolution, we resized all images to 512×512 during training before applying our padding strategy. Foredge supervision, we remove the edge loss for evaluation, while formulti-scale supervision, we replace the module with the same number of plain convolution layers.

To reduce expenses, we trained modelonlywithProtocolNo.1in the ablation study. Qualitative results are illustrated in Figure7, which vividly demonstrates the efficacy of each component in our method. For quantitative results in Table9, our findings are:

MAE pretrain is mandatory.Indeed, dataset insufficiency is a significant challenge in building ViT-based IML methods.
As shown in Table2, public datasets for IML are small in size, which cannot satisfy the appetite of vanilla ViT. As shown inw/o MAEaspects in Table9, the use of Xavier initialization to train the model resulted in complete non-convergence. However, while regular ViT pre-training initialization with Imagenet-21k achieves acceptable performance on CASIAv1, which is homologous to CASIAv2, it exhibits poor generalization ability on other non-homology datasets.
This indicates that MAE greatly alleviates the problem of non-convergence and over-fitting of ViT on limited IML datasets.

Edge supervision is crucial.The performance of IML-ViT without edge loss shows significant variability with different random seeds, all leading to gradient collapse eventually, where the F1 score reaches 0, and the loss becomesNaN, as shown in Figure8. In contrast, when employing edge loss, all performance plots exhibit consistent behavior similar to the blue line in Figure8, enabling fast convergence and smooth training up to 200 epochs. Furthermore, Table9confirms the effectiveness of edge loss in contributing to the final performance. In summary, these results demonstrate that edge supervision effectively stabilizes IML-ViT convergence and can serve as highly efficient prior knowledge for IML problems.

High resolution is effective for artifacts.The improved performance shown in Table9for thefull setupmodel across four datasets validates the effectiveness of the high-resolution strategy. However, it is essential to note that the NIST16 dataset shows limited improvement when using higher resolutions. This observation can be attributed to the fact that the NIST16 dataset contains numerous images with resolutions exceeding 2000, and down-sampling these images to 1024 for testing may lead to considerable distortion of the original artifacts, consequently reducing the effectiveness of learned features. Nevertheless, when considering the SoTA score achieved, it becomes evident that IML-ViT can flexibly infer the manipulated area based on the richness of different information types.

Multi-scale supervision helps generalize.All these datasets exhibit significant variations in the proportion of manipulated area, particularly where CASIAv2 has 8.96% of the pixels manipulated, COVERAGE dataset has 11.26%, Columbia dataset has 26.32%, and NIST16 has 7.54%.
Nevertheless, the comprehensive improvements in Table9with the aid of multi-scale supervision indicate that this technique can effectively bridge the gap in dataset distribution, enhancing generalization performance.

SECTION: 4.4Robustness Evaluation

We conducted a robustness evaluation on our IML-ViT model following MVSS-Net[2]. We utilized their protocol with three common types of attacks, including JPEG compression, Gaussian Noise, and Gaussian Blur. As shown in Figure9, IML-ViT achieved very competitive results among SoTA models, which proved to possess excellent robustness.

SECTION: 5Conclusions

This paper introduces IML-ViT, the first image manipulation localization model based on ViT. Extensive experiments on three mainstream protocols demonstrate that IML-ViT achieves SoTA performance and generalization ability, validating the reliability of the three core elements of the IML task proposed in this study: high resolution, multi-scale, and edge supervision. Further, IML-ViT proves the effectiveness of self-attention in capturing non-semantic artifacts. Its simple structure makes it a promising benchmark for IML.

SECTION: References

SECTION: Appendix AFuther Robustness Evaluation

JPEG compression, Gaussian Noise, and Gaussian Blur are the common attack methods for Image manipulation localization. Following the convention from TruFor[11]and MVSS-Net[2], we further carried out experiments on the resistance of these operations on Protocol No.1 and No.2. The evaluation results are shown in Figure10and Figure11. The IML-ViT exhibited excellent resistance to these attack methods and consistently maintained the best performance of the models.

SECTION: Appendix BMore Implementation Details

SECTION: B.1High-resolution ViT-Base

Mostly following the original Vision Transformer, we implemented our model with a stack of Transformer blocks stacking together. LN are employed in the self-attention head and MLP blocks. Every two windowed attention blocks are followed by a global attention block. The windowed attention block only computes self-attention in a small, non-overlapped window, while the global attention block ensures global information propagation. Although we introduce the windowed attention, it only affects the self-attention manner but doesn’t change the linear projection for Q, K, and V. Therefore, we can directly apply the MAE pre-trained parameters from a vanilla ViT-B with all global attention to this windowed ViT-B without any extra process. Detailed configuration are shown in Table10.

SECTION: B.2Simple Feature Pyramid

After obtaining the output from ViT-B, SFPN utilizes a sequence of convolutional, pooling, or deconvolutional (ConvTranspose2D) layers to downsample it into feature maps with 256 channels, scaling them to resolutions ofrelative to the resolution of the input feature maps (768×64×64). For example, the largest output feature map with a scale of 4.0 is shaped like 256×256×256, while the smallest one with a scale of 0.25 is shaped like 256×8×8. Each layer is followed by LayerNorm. Detailed structures of each scale can be seen in Table11.

SECTION: B.3Predict-head’s norm & training including authentic images

The exact structure we applied in the predict-head is shown in Figure3. There is a norm layer before the last 1 × 1 convolution layer in the predict-head. We observed that when changing this layer may influence the following aspects: 1) convergence speed, 2) performance, and 3) generalizability.

In particular, Layer Norm can converge rapidly but is less efficient at generalization. Meanwhile, the Batch Norm can be generalized better on other datasets. However, when including authentic images during training, the Batch Norm may sometimes fail to converge. At present, a straightforward solution is to use Instance Normalization instead, which ensures certain convergence. Our experimental results are shown in Table12.

Delving into the reasons, MVSS-Net[2]is the pioneering paper proposing the incorporation of authentic images with fully black masks during training to reducefalse positives. We highly endorse this conclusion as it aligns more closely with the practical scenario of filtering manipulated images from a massive dataset of real-world images. However, in terms of metrics in convention, because the F1 score is meaningful only for manipulated images (as there are no positive pixels for fully black authentic images,, yielding), we only computed data for manipulated images. This approach may result in an “unwarranted” metric decrease when real images are included.

Training settingsSince our model could only train with small batch size, we applied thegradient accumulatemethod during training, i.e. updating the parameters every 8 images during training. We select this parameter by experiments, details see Table13.

Besides, we adopt the early stop method during training. Evaluate the performance on the F1 score for CASIAv1, and stop training when there is no improvement for 15 epochs. Other configs are described in Table14.

SECTION: Appendix CWhat artifacts does IML-ViT capture?

To investigate whether IML-ViT focuses on subtle artifacts as expected, we employ GradCAM[31]to visualize the regions and content the model focuses on, as shown in Figure12. Additional results are in the AppendixE.4. We can observe that IML-ViT captures the traces around the manipulated region with the help of edge loss. Further, we can observe some extra subtle attention out of the manipulated region in the fourth image, proving the global dependent ability of ViT can help the model trace the tampered region.

SECTION: Appendix DAdditional Experiments Results

Since the space limit, we place a part of our results inProtocol No.1here.

ObjectFormer[34]and CFL-Net[28]evaluate their models fine-tuning with CASIAv2 on AUC. Although this metric may overestimate the models, IML-ViT has still surpassed them, as shown in Table15.

SECTION: Appendix EExtra Visualization

SECTION: E.1Visualization of Protocol No.1 on other datasets

Here we also present some of the predict masks under Protocol No.1, which was from dataset with other preference on manipulation types. Extended from CASIAv1 and COVERAGE datasets in the main paper, we present results in NIST16 and Columbia datasets here in Figure13.

SECTION: E.2Qualitative results for ablation study

The ablation study from Figure7evaluates the impact of various components on IML-ViT’s performance: 1) w/o multi-scale: Significant degradation with poor feature detection and blurred outputs. 2) w/o MAE: Improved over the absence of multi-scale, but still blurry with weak edge definition. 3) w/o high-resolution: Noticeable drop in detail and precision, with coarse boundaries. 4) w/o Edge Loss: Less defined edges, preserving overall shape but losing structural details. 5) Full Setup: Produces the most accurate and detailed segmentation maps, capturing fine details and clear boundaries. In summary, the ablation study highlights the critical contributions of each component to the overall performance of IML-ViT. The multi-scale processing, MAE pre-training, high-resolution input, and edge loss each play a vital role in enhancing the model’s ability to produce a high-quality segmentation map.

SECTION: E.3Extra results for CASIA datasets.

To provide a detailed showcase of IML-ViT’s performance on image manipulation localization tasks, we present additional image results on the CASIA dataset in Figure14.

SECTION: E.4Extra GradCAM results

Here we provide extra GradCAM results to verify if IML-ViT focuses on the artifacts we want it to trace. Artifacts are mainly distributed around the manipulated region with rapid changes. Figure15vividly shows that the IML-ViT can effectively discover the artifacts from the image and support its decision.

SECTION: E.5Feature maps between each module

To gain a deeper understanding of IML-ViT, we present visualizations of feature maps between layers by calculating the average channel dimensions of the feature map. The outcomes are displayed in Figure16. This visualization process allows us to shed light on the model’s functioning and provides valuable insights into its mechanisms.

SECTION: Appendix FLimitation

We observe a rapid decline in IML-ViT’s performance on the Gaussian blur attack
when the filter kernel size exceeded 11, We argue that this is mainly because our motivation is to make the model focus on detailed artifacts, but excessive Gaussian blurring can significantly remove these details, leading to a sudden drop in performance. However, from another perspective, this can actually prove that our model is able to effectively capture artifacts in tampering. Currently, the training does not specifically enhance blur, so we believe that adding enough blur data augmentation can compensate for this issue.