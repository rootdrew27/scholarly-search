SECTION: Class-Discriminative Attention Maps for Vision Transformers
Importance estimators are explainability methods that quantify feature importance for deep neural networks (DNN). In vision transformers (ViT), the self-attention mechanism naturally leads to attention maps, which are sometimes interpreted as importance scores that indicate which input features ViT models are focusing on. However, attention maps do not account for signals from downstream tasks. To generate explanations that are sensitive to downstream tasks, we have developed class-discriminative attention maps (CDAM), a gradient-based extension that estimates feature importance with respect to a known class or a latent concept. CDAM scales attention scores by how relevant the corresponding tokens are for the predictions of a classifier head. In addition to targeting the supervised classifier, CDAM can explain an arbitrary concept shared by selected samples by measuring similarity in the latent space of ViT. Additionally, we introduce Smooth CDAM and Integrated CDAM, which average a series of CDAMs with slightly altered tokens. Our quantitative benchmarks include correctness, compactness, and class sensitivity, in comparison to 7 other importance estimators. Vanilla, Smooth, and Integrated CDAM excel across all three benchmarks. In particular, our results suggest that existing importance estimators may not provide sufficient class-sensitivity. We demonstrate the utility of CDAM in medical images by training and explaining malignancy and biomarker prediction models based on lung Computed Tomography (CT) scans. Overall, CDAM is shown to be highly class-discriminative and semantically relevant, while providing compact explanations.

SECTION: Introduction
Vision transformers (ViT) are a family of computer vision models based on the transformer architecture, that demonstrate comparable or even better performance than convolutional neural networks (CNN) on many object detection and classification tasks. A powerful feature of the transformers is that the attention mechanism provides a built-in explanation of the model’s inner workings, if attention is understood as a weighing of the relevance of individual tokens. Whileprovide intuitive and inherent interpretability, they fail to be discriminative with respect to a target class. Recognizing the potential of attention maps for explaining a ViT’s inner workings, we have developed(CDAM), a gradient-based extension that makes them sensitive to class-specific signals based on a classifier or a latent concept.

Attention maps are the visualization of the self-attention of the class token queryin the final attention layer. Attention heads can be visualized separately, and in certain instances, may be related to semantically meaningful concepts (e.g., in language). Recently, ViT trained with self-supervised learning (SSL) methodshave shown excellent performance leveraging a greater amount of unlabeled data. Attention maps from such self-supervised ViT models provide high-quality semantic segmentations of input images. These attention maps clearly distinguish regions of interest (ROI) from the background with very little noise, with different attention heads often focusing on semantically distinct regions. There have been many improvements to ViT including the introduction of convolutionand spatial sparsity. Attention maps, as well as our proposed CDAMs, are applicable to all ViT architectures with self-attention.

The major shortcoming of attention maps, when considering them as an explainable AI (XAI) method, is that they purely operate on the level of the last attention layer and therefore do not take into account any information coming from a downstream task, such as a classifier head. The proposed CDAM overcomes this shortcoming by calculating the gradients of the class score, i.e. the logit value of a target class, with respect to the token activations in the final transformer block (visualized in(a)). Essentially, CDAM scales the attention scores by how relevant the corresponding tokens are for the model’s decision. CDAM therefore inherits the desirable properties of attention maps, but additionally indicates the evidence or counter-evidence for a target class that the ViT relies on for its predictions (examples in). We demonstrate that it is class-discriminative in the sense that the heat maps clearly distinguish the object representing the target class from the background and, importantly, from objects that belong to other classes. Therefore, CDAM is a useful tool for explaining the decision-making process of classifiers with a ViT backbone. Note that the goal of CDAM is to explain the classifier head and the final transformer block.

In addition to the classifier-based explanations, we present a concept-based approach that replaces the class score with the similarity measure in the latent space of the ViT, where a concept is defined through selected images. Averaging the latent representations of the example images yields a, which is analogous to a concept vector in variational autoencoders (VAE) and related generative models. This allows obtaining a heat map for the target concept the model has never been explicitly trained on, which is of particular interest for self-supervised models. We further demonstrate when computing gradients of a class () or concept similarity () score, with respect to the token activations in the final transformer block, smoothing or averaging could be applied to improve performance. Specifically, we introduce two additional variants of CDAM; (1) Smooth CDAM averages over multiple gradients that are computed with additional noise to tokens (inspired by SmoothGrad) and (2) Integrated CDAM takes the integral of the gradients along the path from the baseline to the token in the final transformer block (inspired by IntGrad).

We conduct several quantitative evaluations focusing on correctness, class sensitivity, and compactness. Correctness is measured by perturbing an increasing amount of input features and measuring the model output. By using the ImageNet sampleswith multiple objectsand applying importance estimators for different classes, we quantify the level of class-discrimination. Sparsity and shrinkage of explanations are measured by the number of near-zero importance scores. Besides the proposed vanilla, Smooth, and Integrated CDAM, we also employed 7 different explainability methods in comparison. Three variants of CDAM outperform other methods on quantitative benchmarks.

Lastly, we have applied CDAM on a ViT fine-tuned on the Lung Image Database Consortium image collection (LIDC). Two ViT-based models are built for predicting malignancy and biomarkers. CDAM is shown to be highly class-discriminative and semantically relevant, while providing implicit regularization of importance scores.

SECTION: Related Works
Interpretability of ViT, and more broadly explainable artificial intelligence (XAI), is an active area of research, since understanding their decision-making process would help improve the model, identify weaknesses and biases, and ultimately increase human trust in them. The proposed class-discriminative attention map (CDAM) is related to both attention and gradient-based explanation methods for deep learning models.

Gradient-based methods operate by backpropagating gradients from the prediction score to the input features to produce feature attribution maps, which is also called saliency maps. Many variants have been proposed, seeto just name a few. In particular, CDAM is similar to and inspired by the inputgradient method, because it involves calculating the gradients with respect to tokens multiplied by their activation (and). The main difference to most gradient-based methods is that we don’t backpropagate the gradients all the way to the input tokens. CDAM stops backpropagation at the tokens that enter the final attention layer.

In that regard, our method is related to GradCamwhich backpropagates the gradients to the final convolutional feature map in a CNN. GradCam and CDAM therefore share the property that they operate on high-level features which is presumably the reason why they are both sensitive to the targeted class, a feature that many explanation methods do not have. Several methods have attempted to apply gradient-based methods or Layer-Wise Relevance Propagation (LRP)to Transformer architectures, mostly in the context of NLP tasks. GradCam for ViT backpropages the gradients to the outputs of the final attention layer.

Attention has often been used to gain insight into a model’s inner workingssince it seems intuitive that this weighing of individual tokens correlates with how important they are for the performed task. There is also an ongoing discussion about whether attention provides meaningful explanations.
Furthermore, methods such as “attention rollout”have been proposed that aim to quantify how the attention flows through the model to the input tokens.defines the relevancy matrices, as the Hadamard product of the attention map and the gradient ofw.r.t. to the attention layer, accumulated at each layer. This relevance propagation (RP) method, to the best of our knowledge, shows the highest degree of class-discrimination in ViT models. This is used in our study as one of the baseline importance estimators.

Many papers have been published on the problem of evaluating post-hoc explanation methods, leading even to the proposal of meta-evaluations. In a recent overview of quantitative evaluation methods12 desirable properties have been identified. We primarily focus on three of them, namely correctness, contrastivity, and compactness. In particular, carefully designed occlusion/perturbation studies can help us calculate the fidelity statistics, which is also called the symmetric relevance gain (SRG) measure.first demonstrated that many importance estimators, which purport to be sensitive to the internals of a classifier, produce similar importance scores regardless of the relation of inputs and classes in the training data, which they called “the data randomization test”. Generally, if two importance scores with respect to two different classes produce very similar heat maps, the importance estimator is not class-sensitive. We quantify this concept in our class sensitivity evaluation benchmark to measure whether an explanation is sensitive to the classes.

SECTION: Methods
Adaptation of the transformer architecture to computer vision hinges on creating tokens from small patches of the input image. In addition to those image patch tokens, the classification tokenis added, which is utilized for downstream tasks such as classification. Attention maps are the two-dimensional visualization of the self-attention in the final attention layer withas query token, where the attention scores are averaged over all heads. In our notation,refers to the classification token before andafter the final transformer block.

Before fine-tuning, attention maps are not class-discriminative since they do not take into account any signal coming from a downstream task. Therefore, we have developedthat can be computed based on a known class or latent concept. In order to estimate a token’s relevance, CDAM computes gradients of a class score () or concept similarity score (), obtained downstream using thetoken, with respect to the token activations in the final transformer block (). Note that CDAM may not explain the full ViT architecture with multiple transformer blocks. Instead, CDAM focuses on the downstream task and the last transformer block.

In the first scenario, the class score is obtained from the corresponding activation in the prediction vector of a classifier trained on top of the ViT in a supervised fashion. In the second scenario, we define a concept through a small set of examples (e.g. ten images of a dog) and obtain aby averaging their latent representations. Then, without training a classifier, a similarity measure between the concept vector and the latent representation of a targeted sample image is computed and used as the target for the gradients. We call the resulting maps of feature relevance “class-discriminative attention maps” because they scale the attention scores

by how relevant the corresponding tokensare for the class or similarity score. Herewithandwith. In this notationandare the query and key matrices which pack together all the query and key vectorsand the indices, e.g., select single query or key vectors.

Self-supervised learning (SSL) methods such as DINO can train ViT models whose attention maps are high-quality semantic segmentations of the input, without using labels. Therefore, we utilize a ViT modelpre-trained with DINO as a backbone for our main experiments and applications. Furthermore, we demonstrate general applicability of CDAM by using other ViT models, namely pre-trained with Supervised Weakly through Hashtags (SWAG), Data-efficient image Transformers (DeIT)and DINOv2. We denote details about the ViT architecture, training schemes, and model performance in theand. Note that the heat maps shown in this article are scaled independently.

SECTION: CDAM for a known class
Consider a linear classifieron top of the ViT that takes thetoken as input and maps it to a prediction vector withclasses, whereis the embedding dimension of the ViT. The importance scorewith respect to the tokenis defined as

whereis the-th component of the-th token that is fed to the last attention layer (). We found in our experiments that calculating the gradients with respect to the layer-normalized tokens yields far better results than using the not normalized ones.

This definition is a first-order estimation of the contribution oftoin the sense that it would be exact ifdepended ononly linearly. It is inspired by gradient-based explanation methods with the difference that we don’t backpropagate the gradients all the way to the input layer but only to the input tokens of the final attention layer. This allows us to gain insight into the decision-making process of the combination of ViT and classifier based on high-level features. This approach leads to attention maps that are class-discriminative and well-aligned with the semantic relationships of the class and different parts of an image ().

SECTION: CDAM for a latent concept
Another way of probing high-level features learned by the ViT is through the use of a similarity measurein the latent space of the ViT. We compare the latent representationof a sample imageand a, i.e. the averaged latent representation ofimages that are chosen to represent the concept of interest:

This approach is directly inspired by the concept saliency map. The definition of the importance score in this setting is analogous to

Possible choices forare for example thedistance, cosine similarity or dot product, where we use the latter in this article. The definitionestimates the contribution ofto, such that it allows us to gauge how much each token is driving the similarity betweenand.

SECTION: Smooth and Integrated CDAM
SmoothGrad (SG)and Integrated Gradients (IG)extended saliency maps, which were introduced in the context of CNNs as visualizing gradients of the class with respect to the input features. Briefly,adds a small Gaussian noise to a sample imagetimes, resulting invanilla saliency maps. SG is an average of thosesaliency maps. In IG, instead of adding random noise,uses multiple saliency maps obtained from interpolating between a given sample and a baseline (e.g., a black image).

We have adapted the analogous modifications to CDAM, resulting in two additional variants which we call Smooth and Integrated CDAM. Importantly, instead of modifying the input images, we vary the tokens that enter the final transformer block. In Integrated CDAM, we interpolate between a baseline and the tokens themselves, resulting inmaps. Notice that, since we apply the same method, the axioms of IG still hold with the only difference that they apply to the tokens that enter the final transformer block instead of the input pixels.
In Smooth CDAM, we repeatedly add a small Gaussian noise to the tokens and again obtainmaps.

Specifically, Smooth and Integrated CDAM are defined as follows:

wherewithrepresenting Gaussian noise with standard deviation. For our experiments, we used. For the baselinewe choose the null vector of 0. We implement these two variants by first propagating an image through the ViT up to the final transformer block, where the activations of the tokens are obtained. Next, the activations are manipulated by either applying the noise or interpolation step and the manipulated tokens are then further propagated to obtain the final ViT output.

SECTION: Quantitative Evaluation
We perform quantitative evaluations of the correctness, class sensitivity, and compactness of importance estimators. In addition to our proposed methods (vanilla, Smooth, and Integrated CDAM), we perform evaluations on attention maps, InputGradients, relevance propagation (RP), partial Layer-wise Relevance Propagation (LRP), SmoothGrad, IntGrad, and GradCam for ViT. Note that based on classic LRP, partial LRPprovides improvement for multi-head self-attention mechanisms in modern transformer architecture. Relevance propagation is a state-of-the-art importance estimator for explaining a prediction (e.g., class discriminatory) based on ViT. For IntGrad, SmoothGrad, Integrated CDAM, and Smooth CDAM,is used. The noise () used in SmoothGrad and Smooth CDAM is selected based on the best performance on a validation set of 200 images.

To evaluate the correctness (also known as faithfulness) of importance scores, we employ two versions of input perturbation-based approaches. Generally, an input feature (e.g., a pixel) is masked and a resulting model performance is observed. In all cases, input features are perturbed by being replaced with their blurred versions. A Gaussian blur withturns pixels uninformative while minimizing perturbation artifacts.

The first evaluation method uses accuracy curves obtained from the Most Important First (MIF) and the Least Important First (LIF) perturbations, which are abbreviated as MIF/LIF perturbation. Pixels are ranked by importance scores from a specific explainability method (e.g., CDAM) and are perturbed in the MIF or LIF order. In the MIF perturbation, an accurate importance score should lead to a relatively rapid decrease of the model output (logit) for the target class. For the LIF perturbation, we expect an inverse relationship. Note that some importance scores are unsigned (i.e., only non-negative values) and others are signed. For signed importance, with negative scores faithfully indicating counter-evidence, there may be an increase in the logits if pixels with negative scores are masked. For unsigned and correct importance scores, one expects initially constant logits in the LIF perturbation.

The model outputoras a function of the fraction of perturbed pixelsgives the perturbation curve. We average these curves over all samples, providing an overview of fidelity for a given importance estimator. We further summarize the MIF and LIF perturbation evaluation by using the area(called the fidelity statistics) under the difference of perturbation curvesto quantify the fidelity of the explanation methods. Generally speaking, a higher value of fidelity statisticsindicates a better explanation. Similar ideas are developed into the symmetric relevance gain (SRG) measure.

For the second evaluation method, inspired by Sensitivity-n, we measure how the model output (logit) changes as a group of pixels is perturbed. Pixels are selected by a randomly placed square box (of size); then, the sum of their importance scores and the change in the model output are measured. For accurate importance estimators, we expect to see a high correlation between the two measures.

For a given image, we sample these values 100 times and calculate the Pearson correlation between the sum of importance scores and the model output change. This is performed for a range of differently sized boxes to obtain, the correlation statistics as a function of the box size. These sensitivity curves are averaged over all samples. Performance of importance estimators varies with, and therefore we use the areaunder the box sensitivity curve to quantify correctness. Generally, a larger box sensitivity statisticsimplies greater correctness.

In contrast toSensitivity-nwhich randomly selects and perturbspixels, we use a random square of pixels. A box removes a certain region which mimics how image patches are processed by ViT. Box sensitivity may therefore remove larger parts or complete objects. In contrast, removing random pixels may leave enough neighboring pixels with sufficient semantic information such that perturbation may not have any impact of the model output. Indeed, we have observed that for Sensitivity-n, the correlation is very low () for all methods.

We have designed an evaluation benchmark to measure class discrimination, which can be applied on images containing multiple objects (seedataset below). We obtain explanations for two distinct classes corresponding to the present objects. When performing the aforementioned evaluation benchmarks (fidelity and box sensitivity), we measure the change in the model output with respect to one of the classes. Evaluation metrics are thus obtained using importance scores based on either the correct target class (denoted by MIF/LIF or box) or a wrong class (denoted by MIF/LIFor box).

For important estimators that are not sensitive/discriminative to the target class, the importance scores are similar (or the same) regardless of which class is targeted. The change of the model output during perturbation would remain the same, irrespective of the class that was targeted to obtain the importance ranking of pixels. For highly class-discriminative importance estimators, importance scores would be different depending on which class is targeted. Furthermore, for accurate class-discriminative methods, the performances on fidelity and box sensitivity benchmarks can be expected to deteriorate substantially when perturbing according to the importance scores targeting the wrong class.

The areas under MIF/LIF perturbation or box sensitivity curves, when using a wrong class, are denoted byand, respectively. The areasandunder the respective perturbation curves

measure the degree of class-discrimination. In particular, if an importance estimator is not sensitive to the target class at all,. An importance estimator that performs poorly in the data randomization test ofwould have lowand.

We evaluate sparsity and shrinkage by counting the number of input pixels with importance scores smaller thanof the maximum value. A threshold is defined bywith, whereis the maximum importance score for a given image and an importance estimator. Generally, a compact explanation is considered desirable, where sparsity and shrinkage are quantitative approaches.

SECTION: Data
We have used two subsets of the ImageNet (denoted asand), as well as lung computed tomography (CT) scans from the Lung Image Database Consortium image collection (LIDC). The first one,, is a randomly selected subset of the ImageNet validation set consisting of 1000 images. The second one,, selects 1000 images from the validation set that contain multiple objects. We filtered out instances in which the class labels provided byactually refer to the same object (e.g. notebook and laptop) by calculating and thresholding the cosine similarity of the embedded class names. Embeddings were obtained using OpenAI’smodel.

Thedataset is used to evaluate the degree of class-discrimination exhibited by the explanations. To this end, we obtain explanations w.r.t to two annotated classes, perform the MIF/LIF perturbation and box sensitivity, and record the model output for the chosen target class. For example, for the top right sample in, we would obtain the explanations for the classesand(not shown) and record the model output for the class. For the right target (), we obtain a fidelity statistics. For the wrong target (),. In the worst case scenario, where explanations for those two distinct classes are the same,and the resultingis zero. Equivalent for.

The Lung Image Database Consortium image collection (LIDC) contains 1018 records of lung CT scans, that were collected from and validated by seven academic centers and eight medical imaging companies. Presence of nodules (nodule3 mm; nodule3 mm; non-nodule3 mm) is annotated by up to 4 human annotators. Segmentation of nodules (i.e., ROI) is the average contour given by annotators. Eight biomarkers that are informative in clinical practices are also collected in LIDC: subtlety, calcification, sphericity, margin, lobulation, spiculation, texture, and diameter. For clinical descriptions of biomarkers, refer to. After preprocessing, 854 nodule crops of size, corresponding biomarkers, and labels (benign or malignant) were saved and used for downstream procedures. Overall, the LIDC dataset used in this study contains 443 benign examples and 411 malignant examples.

SECTION: Results
For our experiments using the ImageNet, we use the ViT-S/8 architecture (with a patch size of) trained with DINO as a backbone. Instead of using a ViT model that is pre-trained in a self-supervised manner, we demonstrate CDAMs directly from an alternative ViT architecture trained with supervised weakly through hashtags (SWAG)in. We further demonstrate applicability of CDAM on computed tomography (CT) scans, using DINOin, as well as using Data-efficient image Transformers (DeIT)and DINOv2in.

SECTION: Qualitative Evaluation
To demonstrate our method, we have trained a linear classifier on the ImageNetthat achieved an accuracy of. We use random resized cropping and horizontal flipping with PyTorch default arguments as augmentation, Adam optimizer with learning rate, batch size of 128, and train for 10 epochs. The parameters of the ViT backbone are frozen during training, so only the classifier head is trainable. The predictions of this classifier were then used to create the heat maps shown invia.

CDAM clearly distinguishes the region of interest (ROI) semantically related to the targeted class with positive values (yellow) from the rest of the image. Most backgrounds and other objects are assigned either zero (irrelevant) or negative scores (counter-evidence). Note that CDAM does not assign non-zeroto tokens that have zero attention in attention maps, see also the scatter plot between CDAM and attention map. CDAM therefore appears to computeby multiplying the attention with the relevance of the corresponding token for the targeted class. We discuss the proportionality of CDAM to the attention scores in. It is a nice feature since CDAM inherits the high-quality object segmentations present in attention maps. This qualitative evaluation of shrinkage and sparsity of CDAM is reflected in the quantitative measures for compactness ().

One of the most useful features of CDAM is that it is highly sensitive to the chosen class. In(left), when either theorclass is chosen, CDAM clearly highlights that vegetable. Most of the other vegetables are assigned either near-zero or negative values (indicated in blue). Many other importance estimators do not distinguish the target class or only to a lesser degree. In, targeting the burger or the hotdog does not seem to change importance scores from partial LRP, InputGrad, and IntGrad. Relevance propagation demonstrates some selective focus on the ROI, but is not as discriminative as CDAM (). This visual impression of class-sensitivity is supported by the quantitative evaluation in. Additional examples of CDAM and other interpretability methods are shown in.

Concept embeddingshave been obtained by averaging latent representations of 30 images. Those images were randomly selected from a class in the ImageNet validation set. Then, CDAMs inwere obtained using. We observe that CDAM reveals that the model clearly distinguishes the parts of the images that are relevant for the latent concepts shared by selected images from the rest of the image. For example, when having selected images containing(bottom right), CDAM primarily highlights the hammer. While the annotated classes were used for this experiment, the concepts shared by those images are likely more nuanced, specific, and complex than the class itself. For example, CDAM also provides positive importance scores for the wooden handles of the screwdrivers and the socket attached the ratchet, which imply relevance for the concepts shared by 30 images selected from the class. Semantically, 30 images that are selected from a classhave a wooden handle (i.e., concept), which is shared by multiple tools. Similarly, the socket at the top of the ratchet shares similarities with typical metal heads at the top of the hammer. Unrelated tools or parts appear in blue indicating that the model deems them to be unrelated to the concept.

CDAMs are discriminative with respect to the target concepts shared by selected samples (, right). This makes the proposed method particularly valuable for understanding the representations the ViT has learned, since the explanations for different concepts can give a complementary view. For example, by selecting 30 images that contain, CDAM highlights areas that are important for concepts which coincide largely with the body of an elephant. It also shows that parts of the elephant’s trunk are counter-evidence for the concepts shared by images with(, right). For the same sample image, we can also select 30 images with, which suggests that the model deems the part of the elephant’s trunk to be similar to concepts shared by those images.

Although we selected a group of images based on annotated classes, which are denoted in(right), to illustrate concept-based CDAMs, generally the concepts shared by selected images are not identical to the classes themselves. One could manually provide a set of images with a shared concept that is not originally annotated in the dataset. For example, 10 images that containmay share semantic concepts (e.g., “stripes” and “tail”), that make up the class. We demonstrate this by selecting a set of 10 images, outside of the ImageNet, that exclusively showand another set of 10 images that contain diverse animal. Shown in, CDAM forhighlights the body of the zebra, but not the body of the cheetah. On the other hand, CDAM fortends to focus on the tail of the cheetah. Note that 10 images of diverse animalinclude body parts of animals, often rear ends and legs.

SECTION: Quantitative evaluation
We have performed quantitative evaluations of the correctness and compactness of our proposed methods, in comparison to multiple well-known importance estimators. We further investigated class sensitivity by using importance scores from multiple classes.

CDAM performs favorably in terms of the correctness of estimated feature importance compared to other methods. For the dataset containing multiple objects (), the Smooth CDAM outperforms all other methods on the fidelity evaluation using MIF/LIF perturbation (), with(). On the box sensitivity benchmark, Integrated CDAM () performs the best, closely followed by Smooth CDAM () (). Some methods, such as IntGrad, perform very well on the MIF benchmark but poorly for LIF (). We consider the difference (LIF-MIF) to be more meaningful and intuitive as ideally, one would want a large area under the LIF perturbation curve and a small area under the MIF perturbation curve. Thus, largerindicates higher correctness. Particularly, a good score in MIF could stem from simply triggering perturbation artifacts of the model.

For the random subset of ImageNet, the Integrated CDAM demonstrates the best performance (), closely followed by Relevance Propagation () on the box sensitivity benchmark (,). Relevance Propagation outperforms all other methods on the MIF/LIF perturbation, followed by the Smooth CDAM ().

Class discrimination (or sensitivity) is measured by the difference in fidelity statisticswhen perturbing according to importance scores obtained for the correct or wrong target class. Largeris considered to indicate better performance.

CDAM (=739), Smooth CDAM (=823), and Integrated CDAM (=708) clearly outperform other methods in terms of class discrimination (). IntGrad (540), InputGrad (433), and Relevance Propagation (293) follow. Surprisingly, Partial LRP and SmoothGrad perform deficiently. Whereas SmoothGrad has low class-discrimination because its importance scores have low accuracy to begin with, Partial LRP’s scores have mediocre accuracy but it appears to be insensitive to the choice of the target class (). By design, attention maps do not consider the target class and therefore,=0.

The importance scores for CDAM become anti-correlated with the model output when thewrongimportance scores are used for the perturbation (). This anti-correlation probably results from the fact that pixels that are evidence for the correct target class are counter-evidence for the wrong class. This result corroborates the visual impression that CDAM correctly assigns importance scores with opposite signs to the objects corresponding to the targeted class and other objects that are present (). This is also reflected in the negative values offor CDAM (), indicating that the ranking from least to most important has been, at least partially, reversed.

Sparsity and shrinkage are evaluated by our compactness evaluation, which is one of the major desired properties in the explainability of deep learning. While sparsity may not always imply the most accurate estimation at a single data point, a bias-variance trade-off is well-known in machine learning. Ideally, compact explanations (e.g., sparse and shrunken importance estimators) are preferred, when other properties such as correctness and class sensitivity are constant.

CDAM and Integrated CDAM resulted in the highest degree of compactness, together with InputGrad. For those three importance estimators, on average,of importance scores were less thanof the maximum score. IntGrad and SmoothGrad show relatively high sparsityand, respectively. But both are lacking in correctness () and class-discrimination (). Relevance Propagation shows the lowest sparsity of all considered methods, but a high level of correctness. These results suggest that these correctness and compactness metrics are orthogonal.

SECTION: Application to Medical Images
To demonstrate CDAM for another use case we apply it to nodule malignancy and biomarker prediction using the Lung CT scans in the LIDC dataset. Here, we show CDAMs from DINO-based models, we also showcase use of DeITand DINOv2backbones that are fine-tuned on the LIDC dataset in.

SECTION: Malignancy Prediction
After preprocessing, the LIDC data consists of 443 benign and 411 malignant lung CT scans. Training, validation, and test sets (in the ratios of 0.7225, 0.1275, 0.15) were stratified by and balanced according to these labels, e.g., benign and malignant. We fine-tuned a ViT model, with a DINO backbone pre-trained on the ImageNet, for 50 epochs. In a parameter sweep, we varied the number of trainable layers () and dropout rates (), where the learning rate was exponentially decaying (and). The best accuracy on the test set of 0.85 was obtained with 50 trainable layers and dropout rate of 0.003. Attention maps and CDAM are obtained based on this model.

The obtained attention maps suggest that the model focuses on patches with nodule fragments ((b)). However, we note that attention maps almost always focus on pulmonary nodules without taking into account the downstream classification into benign and malignant samples. CDAM provides detailed structures where positive and negative values are indicated by orange and blue, respectively. In this binary classification where 0 is for benign and 1 for malignancy, input pixels with positive values in CDAMs are driving the classification towards malignancy.

SECTION: Biomarker prediction
We turn our attention to clinical biomarkers in LIDC that are routinely used by medical practitioners. Inspired by the concept bottleneck model (CBM), we fine-tuned a ViT model pre-trained with DINO on 8 biomarkers (subtlety, calcification, sphericity, margin, lobulation, spiculation, texture, and diameter). Essentially, a regression model was built on a pre-trained ViT model with the mean squared error (MSE) as loss function. For investigation of LIDC and incorporation of CBM, see.

CDAMs were obtained for 8 biomarkers (). In the context of the margin, patches including the edge of a nodule get positive CDAM scores, whereas patches corresponding to the nodule body get negative CDAM scores. For the diameter, patches corresponding to the nodule body get a positive score, whereas the background pixels exhibit neutral (near zero) or slightly negative scores. When it comes to sphericity, patches including fragments of nodule curvature get a positive CDAM score. Spiculation refers to the presence of needle and spike-like structures, on the margin. High spiculation often is an indication of malignancy. In a majority of cases, CDAM highlights spicules with positive values, whereas the inner bodies of nodules (i.e., not spicules) tend to get low importance scores.

SECTION: Discussion
Class-Discriminative Attention Map (CDAM) is a gradient-based extension of attention maps, that provides strong discrimination with respect to a chosen class or concept, while exhibiting excellent fidelity and compactness. Our proposed methods retain appealing characteristics of attention maps, namely their high-quality semantic segmentation. The proposed importance estimation for CDAM scales the attention scores by how relevant they are for a target class or concept (). Therefore, zero attention scores remain zero in CDAM and it produces compact explanations with high sparsity and shrinkage. Besides explaining predictions of the classifier on the top of ViT, CDAM can also provide importance scores specific to the user-defined concept. In the context of self-supervised models, where class labels are absent, this makes it a valuable technique to investigate the latent representations learned by the model.

Post-hoc explanation methods for DNNs aim to make the full decision-making process of the model more transparent by providing an approximation with certain desirable properties, such as correctness, sensitivity, and compactness. Particularly, many explanation methods that estimate feature importance are not very useful because they give almost identical results when targeting different classes in the model output. Even randomization in the model weights is often shown to have minimal impact on explanations (i.e., saliency maps). We find, both qualitatively and quantitatively, that CDAM is highly sensitive to the targeted class, assigning positive importance scores to objects corresponding to the target class and negative ones to other (semantically distinct) objects in the image.

We introduce Smooth and Integrated CDAM, which essentially average a series of (vanilla) CDAMs. Notably, instead of adding noise to or modifying the input images as in SmoothGrador IntGrad, respectively, our methods act on tokens in the final transformer block. While we conducted a number of quantitative and qualitative evaluations, there is no single XAI method that triumph in all scenarios. However, considering that Smooth and Integrated CDAM require computingvanilla CDAMs (e.g.,), we recommend first trying vanilla CDAMs. If high fidelity is singularly desirable, we suggest utilizing Smooth CDAM.

Our application on medical images exemplifies the need for fine-grained class-sensitive explanations offered by CDAM. Other explainability methods including attention maps often highlight whole nodules and tumors. However, they are uninformative about the inner workings of the ViT model predicting malignancy or biomarkers. As the methods of explainable AI (XAI) are considered for practical implementations, the investigation of human interactions with different explainability methods represents a promising direction for future studies.

The introduction of our importance estimators would enhance the transparency, trustworthiness and accountability of vision transformer models. By providing clearer insights into model decision-making processes, accurate explainability methods help foster broader adoption of AI technologies. However, the explanations could be manipulated by adversarial input perturbation or data poisoning. In turn, enhanced explanations may inadvertently expose vulnerabilities, making models more susceptible to adversarial attacks or malicious exploitation. Therefore, it is imperative to continuously monitor and ensure the robustness of explanation methods while implementing safeguards to mitigate the risks associated with adversarial attacks.

SECTION: References
SECTION: Appendix
SECTION: Proportionality of CDAM to attention maps
The proposed class-discriminative attention scores scale the attention scores. Empirically, in all of our evaluation tasks and applications (e.g.,), we observe that features with attention scoresof 0 have class-discriminative attention scores of 0:

This operating characteristic is highly favorable, as besides introducing class discrimination, CDAM provides implicit regularization resulting in sparser heat maps. Based on a combination of empirical observations and mathematical derivations, we argue that the attention scoreis scaled by the relevance of the corresponding tokenfor a concept or a class.

To better understand this observation, we first consider a concept-based CDAM with a single attention head.is expanded:

where we assume the architecture ofand use the projection of the tokens onto the key vectorwith. We have also used the fact that the latent representation(i.e.) is the sum of value vectors weighted by the corresponding attention,withplusdue to the residual connection, see(b). The functionconsists of the layer normalization, MLP, and residual connection and performs the final processing of thetoken before it enters the classifier ((b)).is the-th component of the gradient and,andare the rows of matricesandthat correspond to theand the i-th tokens.

implies that in, all terms that are not proportional toare zero or cancelled out and we are left with

We thus find thatis obtained by multiplyingwith the directional derivative ofin the direction of. In other words, CDAM scalesby the rate at which the dot product (similarity) betweenandchanges in the direction of.

In the special case thatis the identity function, we obtain an even simpler interpretation of CDAM due to. Assuming,simplifies to, which means thatis scaled by the similarity ofand.

We note that introducingattention heads does not change the above derivations qualitatively. Instead of the sum, there will be a termso that in the case of multi-head attention the equivalent ofreads

and sinceimplies thatwe can again conclude that in order forto hold all terms that are not proportional toeither vanish or cancel out. For multi-head attention, we thus arrive at

which does not have such a straightforward interpretation as the single-headed case due to the mixing of the attention heads.

If we think ofas acting first on, creating a column vector of dimension, we can understandas the sum ofdirectional derivatives of the-long segments of this columns vector in the directions ofto. Each of the summands is proportional to the corresponding, and the interpretation of CDAM from the single-head case therefore extends to the multi-head one.

Although our observationimplies that all terms that are not proportional toinshould vanish, we don’t have a theoretical argument why that would be the case. It seems unlikely that the various terms in the first sum cancel out, which leavesas an explanation. However, in general,depends onthroughand one would therefore expect the derivative to be non-zero.

We can also show that CDAM scales the attention by the relevance of the corresponding token for a chosen class (instead of concept), in the case that a classifier with weightshas been trained on top of the ViT. Analogously to the above single-head discussion we have

and the only difference is that the directional derivative now acts on, i.e. the contribution ofto the activation of classin the prediction vector, instead of. All the intuitions we have gained from the concept-based CDAM therefore extend to the class-based one.

SECTION: Quantitative Evaluations
SECTION: Examples of CDAMs from the ImageNet
SECTION: Examples for CDAMs from lung CT scans
SECTION: Additional ViT architectures and training strategies
While the main manuscript has used ViT models pre-trained using DINO, other architectures and pre-trained weights can be used for CDAM. In this section, we show examples of alternative ViT models that are trained in alternative manners. As expected, pre-trained ViT models that produce high-quality attention maps result in high-quality CDAMs. In particular, note that the patch size of DINOiscompared to much larger patch sizes used in other backbones.

We use a ViT model pre-trained on the ImageNet-1K dataset using supervised weakly through hashtags (SWAG). Unlike DINO and other self-supervised ViT models, the SWAG model has been trained end-to-end. Therefore, without pre-training, we can obtain attention maps and CDAMs directly. Three examples are shown in.

In general, ViT models require a massive amount of training data and therefore often trained in self-supervised manner. As shown in DINO, self-supervised learning can not only improve performance of fine-tuned models, but also improve attention maps. CDAM relies on attention maps, therefore the quality of attention maps strongly influence that of CDAM. Note that the SWAG model has a patch size of, which results in lower-resolution attention maps and CDAMs.

We demonstrate the use of CDAM with ViT models trained using Data-efficient image Transformers (DeIT)and DINOv2. Note that despite the use of the same acronym, DINOv2 is substantially different from DINOdue to using a different training strategy introduced in iBOTon a much larger scale. The original DINOwas pre-trained on the ImageNet, whereas DINOv2used a newly curated dataset LVD-142M. Additional tokens called “registers” were introduced to improve attention maps of DINOv2.

The LIDC dataset was split into 5 folds and stratified according to malignancy status. The best-performing models were chosen based on means of Accuracy (ACC) and Mean Squared Error (MSE) over 5 folds, in the validation set. In a parameter sweep, we varied the number of trainable layers (10 - all), dropout rates in the backbone (0.0 - 0.12), batch size (8 - 32) and learning rate scheduler parameters. The learning rate was scheduled with CyclicLR scheduler. Binary Cross Entropy Loss and Huber Loss were used to train classification and regression models, respectively. Weights optimization was performed with Adam optimizer and random rotation was applied as data augmentation. For malignancy classification, the best-performing models based on DeIT and DINOv2 achieved a mean ACC of 0.896 and 0.904, respectively. For biomarker regression, the best-performing models based on DeIT and DINOv2 archived the mean MSE of 0.409 and 0.35, respectively.