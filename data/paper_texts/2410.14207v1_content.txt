SECTION: Flexi-Fuzz least squares SVM for Alzheimer’s diagnosis: Tackling noise, outliers, and class imbalance
SECTION: 
Alzheimer’s disease (AD) is a chronic and progressive neurodegenerative condition, accounting for approximatelyof dementia cases. It primarily affects the brain’s cognitive functions, leading to disorientation, mood disturbances, and severe cognitive decline. The prevalence of AD is projected to increase dramatically, with estimates suggesting that by, one in everyindividuals will be affected.
Currently, aboutmillion individuals agedand above are living with AD, making it the sixth-leading cause of mortality in the United States. The socioeconomic impact of AD is profound, encompassing significant healthcare costs, social welfare provisions, and substantial income losses for families, amounting to an estimatedbillion in the United States in.
Numerous studies indicate that early detection and intervention can significantly slow the progression of AD. Therefore, to mitigate further growth, treatment should commence at the earliest detectable stage. Recently, there has been a burgeoning interest in leveraging machine learning algorithms for AD diagnosis. The application of machine learning in AD detection has yielded promising outcomes and represents a vibrant area of research in the past decade.
For a detailed overview of machine learning models for AD diagnosis, we refer interested readers to.

Support vector machine (SVM)is one of the prominent machine learning techniques widely used for classifying ADand other diseases. Unlike numerous classification methodologies, SVM is distinguished by its strong mathematical foundation. It employs the principle of structural risk minimization (SRM) for regularization, simultaneously minimizing the empirical risk. This approach results in strong generalization performance with reduced overfitting of the data. Moreover, the optimization problem of SVM produces a unique optimal solution, unlike techniques such as artificial neural networks (ANN), which are susceptible to the issue of local minimum. Despite these strengths, SVM is constrained by its high computational complexity.

To subsidize the overhead of computational complexity of SVM,introduced a least squares variant of SVM known as the least squares support vector machine (LSSVM). Unlike SVM, LSSVM employs a quadratic loss function instead of the hinge loss function, allowing for the inclusion of equality constraints in the classification problem. Consequently, the solution can be obtained directly by solving a set of linear equations, bypassing the need for quadratic programming. The computation time of LSSVM is significantly less compared to SVM. However, it is worth noting that both SVM and LSSVM are susceptible to the effects of noise and outliers, which are ubiquitous in AD diagnosis. Outliers are data points that deviate significantly from the majority of the data, while noise refers to random fluctuations or errors in the data. The presence of outliers and noise in the dataset can distort the learning process and lead to poor generalization performance of the model.

To ameliorate the repercussions of noise and outliers, fuzzy theory has been successfully applied in machine learning models.
Fuzzy theory uses the distance between an instance and its corresponding class-center to generate a membership function. This membership function allows the model to assign a degree of belongingness to each data point, enabling it to effectively handle outliers and noise in the dataset. This approach enhances the robustness of the model and improves its ability to generalize to unseen data. Class-imbalance learning (CIL) in the existence of noise and outliers is another major task in classification problems, specifically in the biomedical domain. Imbalanced datasets, characterized by an unequal distribution of samples across classes, often lead to poor performance on minority classes due to overfitting, resulting in irretrievable loss. Generally, there are two primary strategies to tackle the class-imbalance problem: resampling and re-weighting. Resampling is a data-level approach that involves manipulating the dataset by either oversampling the under-represented (minority) class instances or undersampling the frequent (majority) class instances. However, the application of resampling has some drawbacks. Specifically, oversampling may lead to overfitting and reduced training efficiency due to an increase in redundant samples, while undersampling can result in information loss and suboptimal performance on frequent class. On the other hand, re-weighting is an algorithmic-level approach that entails redesigning the fuzzy scheme to compel the model to prioritize attention to the more challenging under-represented instances. The aforementioned issues are prevalent in various real-world domains, including medical diagnosis, particularly in AD diagnosis.

To enhance the performance of CIL in the presence of noise and outliers, several SVM-based algorithms have been proposed. These variants rely on fuzzy membership schemes to mitigate the impact of noise and outliers on the classification method. Fuzzy support vector machine (FSVM)handles the noise/outliers issue by assigning fuzzy membership values to the input data points based on their distance from the class-center. The crucial aspect of FSVM variants lies in how they allocate membership value to the input sample. In, an improved dual fuzzy membership function is introduced, in which data points exhibit varying degrees of membership in different regions. Despite the effectiveness of fuzzy concepts in mitigating the effects of noise/outliers, FSVM variants still encounter challenges in class-imbalance problems.
To tackle these challenges, Batuwita and Paladeintroduced fuzzy support vector machine for CIL (FSVM-CIL) with four distinct membership schemes, comprising two center-based and two hyperplane-based approaches. The hyperplane-based membership scheme presumes that the initially derived hyperplane always accurately predicts the final hyperplane, which is not always accurate. Fan et al.introduced entropy-based FSVM (EFSVM), which considers the class certainty of the samples but struggles with border samples and outliers. To further overcome these limitations in CIL with noise and outliers, Tao et al.proposed affinity and class probability-based fuzzy support vector machine (ACFSVM). It utilizes class probability and support vector data description (SVDD)to compute the fuzzified value of the data samples. However, the calculation of affinity significantly influences the computation of membership value, leading to a notable increase in computational complexity. The detailed limitations of the existing fuzzy schemes are discussed in Section.

Taking inspiration from prior research endeavors, in this paper, we introduce a novel, robust, and flexible membership scheme, named Flexi-Fuzz, meticulously engineered to tackle the pervasive challenges of noise, outliers, and class imbalance, which are critical in the diagnosis of AD. The core of the Flexi-Fuzz scheme is its robust and flexible weighting mechanism. By calculating the distance between each sample and the class-center, it assigns weights that reflect the proximity of the samples. Maximum weight is assigned to samples within a specified neighborhood of the center, with a gradual decrease beyond that threshold, ensuring that samples near the class boundary retain significant influence. To overcome the influence of class noise, we integrate class probability, which involves adjusting the weights based on the likelihood of a sample belonging to a particular class, thus reducing the influence of noisy samples that could otherwise distort the learning process. Finally, to address class imbalance, we amalgamate an imbalance ratio into the weighting mechanism, ensuring that minority-class samples are adequately represented during training, thus improving model performance on imbalanced datasets. Subsequently, by incorporating the proposed Flexi-Fuzz membership scheme into the framework of LSSVM, we propose a robust and flexible LSSVM model named Flexi-Fuzz-LSSVM. To enhance the versatility of our approach, we determine the class-center using two methods: the conventional mean approach and an innovative median approach. This leads to the development of two model variants, Flexi-Fuzz-LSSVM-I and Flexi-Fuzz-LSSVM-II, each producing distinct sets of fuzzified samples. The key contributions of this work can be encapsulated as follows:

We present Flexi-Fuzz, a novel, robust, and flexible membership scheme designed to effectively combat the challenges posed by noise, outliers, and class imbalance. This scheme allocates maximum weight to samples within a specified proximity of the center, gradually reducing the weight beyond this threshold, thereby allowing for tailored flexibility. Additionally, class probability and imbalance ratio are employed to attenuate the effects of noise and rectify class imbalance, respectively.

We amalgamate the proposed Flexi-Fuzz membership scheme into the LSSVM and present a robust and flexible least squares support vector machine (Flexi-Fuzz-LSSVM) for efficiently tackling the issue of CIL in the presence of noise and outliers. By leveraging the LSSVM framework, the proposed method eliminates the need to solve the computationally expensive quadratic programming problem. Instead, the solution is directly obtained by solving a set of linear equations.

We augment our contribution by utilizing two distinct methodologies for determining the class-center: the conventional mean approach and the innovative median approach. Consequently, we develop two model variants, Flexi-Fuzz-LSSVM-I and Flexi-Fuzz-LSSVM-II, each producing a unique set of fuzzified samples.

We evaluate the effectiveness of the proposed Flexi-Fuzz-LSSVM models on UCI and KEEL datasets (with and without label noise). The empirical results illustrate that the proposed Flexi-Fuzz-LSSVM models exhibit superior performance compared to numerous baseline models.

To validate the effectiveness of the proposed Flexi-Fuzz-LSSVM models in the detection of AD, we conducted experiments using the Alzheimer’s Disease Neuroimaging Initiative (ADNI) dataset. The empirical investigations provide compelling evidence of the proposed Flexi-Fuzz-LSSVM models applicability in the early diagnosis of AD.

The rest of this work is structured as follows: Sectionprovides an overview of the existing fuzzy schemes. Sectionpresents the limitations of existing fuzzy schemes and introduces the proposed Flexi-Fuzz membership scheme. Additionally, in Section, we derive the mathematical formulation of the proposed Flexi-Fuzz-LSSVM model. Sectionpresents experimental results and statistical comparisons on the UCI and KEEL datasets, as well as on the ADNI dataset. Finally, Sectionconcludes the paper with future directions.

SECTION: 
SECTION: 
In various real-world scenarios, machine learning algorithms are tasked with learning to distinguish between two distinct classes of input points. However, not all training points carry the same level of importance. Some points are considered more meaningful in accurately classifying the data, while others, such as noise or irrelevant data, are less significant. That is, each training pointis linked to a fuzzy membership, which serves as an indicator of the degree to which the point is inclined or relevant to a specific class in the classification problem. Assigning membership values to training samples manually by leveraging one’s intuition and understanding of the data can prove effective. Nevertheless, in many cases, the feasibility of this approach diminishes, particularly when confronted with complex or large datasets. To overcome this challenge, general purpose membership functions (GPMFs) can serve as a viable solution. GPMFs are membership functions that are based on commonly observed patterns and structural understanding of data, making them applicable to a wide range of datasets. Here, we discuss some existing GPMFs, which are based on the distance from the class-center and hyperplane. The GPMFs defined in prior researchare provided as follows:

Here,represents the assigned membership value for thetraining sample, with its values spanning fromto. The parametersignifies the distance of thetraining sample to its respective class-center, where the class-center is calculated by computing the mean of all samples within the class. The variabledenotes the class radius, defined as. Hence, the design ofis premised on the concept that the belongingness of a sample to the class diminishes linearly as the sample moves farther from the class-center.is formulated based on the same principle as, with the sole distinction that the membership value in this case decays exponentially as the sample moves farther from the class-center. In the cases ofand, a separating hyperplane is initially constructed using classical SVM techniques. Here,represents the absolute value of the functional margin of thetraining sample relative to the initial separation hyperplane. The rationale behind these membership values is grounded in the idea that training samples in closer proximity to the separation hyperplane convey more information and, consequently, merit higher membership values compared to others. The value ofdecays linearly asincreases, while the value ofdecays exponentially. One significant drawback of hyperplane-based membership is its assumption that the initially derived hyperplane always accurately predicts the final hyperplane. However, this assumption may not always hold true in practice.represents a small positive value, ensuring thatconsistently maintains a value greater than zero, whileserves the purpose of governing the degree to which exponential decay influences membership values. The aforementioned membership functions perform on par when dealing with outliers; however, they still have limitations. Notably, these functions may prove less effective in addressing challenges associated with CIL. To overcome this limitation,proposed an affinity and class probability-based fuzzy membership function. It uses the idea of SVDD and calculates the affinity of the predominant class, thereby pinpointing outlier samples located within the majority-class. The expression to determine the affinity of a sample is as follows:

Here,serves as a trade-off parameter influencing the size of potential outliers and border samples,is a decay parameter, andrepresents the distance ofsample () from the class-center. The value ofcan be calculated as the mean distance of all support vectors situated on the boundary from the center of the hypersphere.Furthermore, the approach utilizes the-nearest neighborhood technique to ascertain the class probability associated with samples within the predominant class. The probability value, denoted as, for the sampleis computed by determining by the ratio of same-class data points within thenearest neighborhood data points (selected based on proximity) to the total number of neighbors. Finally, fuzzy membership based on affinity and class probability is calculated as follows:

This approach proves efficient when dealing with outliers and class imbalance issues; however, it still has some limitations. One notable drawback is the high computational complexity associated with the calculation of affinity, which necessitates solving the QPP inherent in the SVDD technique.

SECTION: 
In this section, we undertake an analysis of the constraints present in the existing fuzzy schemes. To address their demerits, we introduced a novel, robust, and flexible fuzzy scheme, termed Flexi-Fuzz, specifically designed to address the challenges of noise, outliers, and imbalanced data prevalent in AD diagnosis. By integrating the proposed Flexi-Fuzz membership scheme into the framework of LSSVM, we introduce advanced, robust, and flexible fuzzy LSSVM models for CIL, referred to as Flexi-Fuzz-LSSVM.

SECTION: 
Though the membership functions stipulated in equations ()-() and () demonstrate resistance against noise and outliers, they have certain limitations, including:

Membership function, namely, demonstrates a notable reliance on the distance from the class-center. In scenarios like Figure, where the data distribution results in the majority of training samples being situated on the class boundary, the assignment of membership to most data points approaches to zero, as per equation (). Consequently, the influence of these types of samples is diminished on the classification process.

The functionsandexhibit a linear and exponential decrease, respectively, as the distance of the input point from its class-center increases.
Despite this, both functions encounter a significant limitation in appropriately representing the importance of data points situated in close proximity to the class-center. These points should merit equal consideration, as they share comparable significance with the class-center. However, the existing schemes,and, assign lower membership values to these samples compared to those at the class-center, thereby reducing their influence on the classification process.

The membership functions ()-() solely depend on the distance from the class-center or initial hyperplane. Nonetheless, training samples with equal distances may exhibit varying influences on the formation of the classifier. For example, in scenarios like Figure, the class noise, despite having the same distance as the normal training sample, should yield a lower membership value. This observation highlights that considering only distance-based membership schemes is insufficient for accurately characterizing class membership.

Membership functions based on the class-center are highly dependent on the method used to calculate the center. Traditionally, these functions determine the class-center by computing the mean of all samples associated with that class. However, the mean is not a robust estimator, especially in the presence of outliers or non-symmetrical data. Mathematically, the mean is highly susceptible to extreme values, which can significantly distort its accuracy. Outliers located far from the majority of samples can skew the mean, leading to a compromised estimation of the class-center. This sensitivity undermines the robustness of the center calculation, necessitating alternative methods that are less influenced by outliers to provide a more reliable characterization of the class-center.

The underlying principle embedded inandposits that training samples that are close to the initial hyperplane contain more useful information than those that are farther away. Consequently, higher membership values are assigned to training samples closer to the initial hyperplane, reflecting their perceived greater relevance. However, it is crucial to note that these two membership functions operate under the assumption that the initial hyperplane learned is potentially unaffected by outliers and is a correct estimate of the final hyperplane. Unfortunately, these assumptions fail in many instances. Furthermore, the procedure leading to the determination of the initial hyperplane is computationally expensive.

SECTION: 
To address the aforementioned limitations, we have developed an innovative and robust membership scheme, termed Flexi-Fuzz. The proposed Flexi-Fuzz scheme is meticulously structured around three pivotal steps: (1) calculating the weight for each training sample through a novel flexible weighting mechanism; (2) determining the class probability to refine the weighting process; and (3) assessing the imbalance ratio to ensure equitable representation of all classes.

In traditional methodologies, sample weight decreases linearly or exponentially as the distance from the class-center increases. This results in the maximum weight being assigned only to samples at the center, while those near the boundary receive negligible weights. Our novel approach challenges this paradigm by advocating for maximum weight to be assigned to samples within a specified neighborhood of the center, with a gradual decrease beyond that threshold. Additionally, our approach ensures that samples near the boundary do not receive negligible weights, thereby ensuring their influence is not entirely neglected in the classification process. The mathematical articulation of the proposed flexible weighting scheme is outlined as follows:

whereanddenote the center and radius of the corresponding class, respectively, andis the flexible parameter. For, the weighting scheme uniformly assigns a maximum weight ofto all samples. For, it assigns a maximum weight ofexclusively to samples situated within the radius of, and so on. This exemplifies the parameteras a key factor influencing the weighting scheme, allowing for tailored flexibility. Thus, by adjusting, the scheme can be fine-tuned to vary the influence of samples based on their proximity to the class-center. This flexibility allows for more precise control over the classification process, enhancing the model’s adaptability to different datasets and noise levels. The proposed flexible weighting scheme, illustrated in Figure, visually demonstrates the impact of different values of. The ability to manipulateenables the customization of emphasis on proximity to the class-center. This is particularly useful in applications where the significance of boundary samples varies, allowing for optimized performance across diverse scenarios.

Data points with equal distances from the class-center may exhibit distinct contributions to the classifier’s formation, indicating that relying solely on weight values determined by distance is insufficient to accurately characterize their belongingness to a specific class. To address this limitation and enhance performance, class probability values are introduced. This inclusion aims to reduce the influence of class noise in the training phase. Specifically, the class probability for each data input is precisely determined as the ratio of points belonging to the same class within a predefined fixed neighborhood to the total number of points within that neighborhood. To elaborate further, for a given training sample, the procedure involves the selection of its-nearest neighbors, denoted as. Then, the count of samples within this selected set that belong to the same class asis determined. Finally, the class probability for the sampleis computed as

The larger value ofsignifies a higher probability for the sampleto belong to its own class. Consequently, a lower probability value for noise reduces its impact on the construction of hyperplanes. This observation underscores the utility of class probability in enhancing the robustness of the hyperplane construction process, contributing to a more reliable and accurate classification model.

To mitigate the challenges posed by imbalanced class distributions, we utilized the imbalance ratio for updating the weights. Without loss of generality, we assume that the positive class represents the minority-class, while the negative class represents the majority-class. The imbalance ratiois then computed as the ratio of the number of majority samples to the number of minority samples. Letanddenote the weight values of a positive-class exampleand a negative-class example, respectively. Subsequently, the weights assigned to each sample are updated as follows:

Here,takes a value in between. As a result, a positive-class sample can have a membership value ranging fromto, whereas a negative-class sample can have a membership value ranging fromto. This adaptation serves to address the imbalanced class distribution by scaling the membership values accordingly.

Finally, combining the updated weighting scheme, as defined in equations () and (), with the class probability value calculated using equation (), we define the proposed Flexi-Fuzz membership scheme as follows:

This formulation incorporates the proposed flexible weighting mechanism with both the class probability and imbalance ratio, offering a comprehensive and nuanced representation of sample belongingness within the Flexi-Fuzz membership scheme. This innovation ensures that the Flexi-Fuzz scheme remains robust, adaptive, and highly effective in managing complex classification tasks, particularly in the context of AD diagnosis, where noise, outliers, and class imbalance are common challenges. The method to compute the proposed Flexi-Fuzz membership scheme is briefly described
in Algorithm.

The training dataset:,;The parameters: flexible parameter,-nearest neighbor parameter;The Flexi-Fuzz membership value for each sample;1: Compute the flexible weighting scheme using equation ();2: Compute the class probability using equation ();3: Compute the updated weighting scheme using equations () and ();4: Determine the proposed Flexi-Fuzz membership scheme using equation ().

SECTION: 
Let a binary classification task have a total ofdata points, andandare the counts of positive and negative data points, respectively. The computational complexity of the proposed Flexi-Fuzz membership scheme (equation ()) can be broken down into three main components. Firstly, it involves calculating the flexible weighting scheme value. This computation includes determining the class-center and radius, computing the distance between each class-center and sample, and evaluating the flexible weighting scheme value for each sample using equation (). This step has a time complexity of. Secondly, it needs to calculate the class probability according to equation (). This calculation involves iterating overandsamples, leading to a time complexity of. Lastly, it requires to compute the imbalance ratio, which has a fixed time complexity of. Therefore, the overall computational complexity of the Flexi-Fuzz membership scheme is approximately.

SECTION: 
Through the integration of the proposed Flexi-Fuzz membership scheme into the framework of LSSVM, we propose a novel robust classifier for class-imbalance problems named Flexi-Fuzz-LSSVM. The objective function of the proposed Flexi-Fuzz-LSSVM is formally defined as follows:

whereis the regularization term anddenotes the Flexi-Fuzz membership value. As the dimensionality ofbecomes infinite, solving the primal problem () directly is not feasible. Consequently, we proceed by formulating the corresponding Lagrangian:

where’s are the Lagrangian multipliers. The optimal conditions are given as follows:

After the elimination of the variablesand, a system of linear equations can be obtained as follows:

where,,,, and, withthe kernel function. After obtaining the optimaland, the following decision function can be utilized to predict the label of a new sample.

The process of determining the class-center is pivotal in assigning the membership scheme within the proposed Flexi-Fuzz-LSSVM model. Traditional methods often compute the class-center by taking the arithmetic mean of all samples associated with that class. However, the mean may not be a robust estimator, particularly in the presence of outliers or non-symmetrical data distributions. Outliers, being extreme values, can significantly skew the mean, resulting in an inaccurate representation of the true class-center. To address this limitation, the median is introduced as an alternative estimator. Unlike the mean, the median is a more robust measure of central tendency, as it is less affected by outliers and provides a more stable and reliable location estimator. The stability of the median in the face of skewed data distributions makes it a superior choice for accurately determining the class-center in noisy environments, particularly in the context of AD diagnosis, where data variability and noise are prevalent challenges. In the proposed Flexi-Fuzz-LSSVM model, we employ both the conventional mean approach and the innovative median approach to determine the class-center.1) The adoption of two distinct strategies for initializing class-centers culminates in the development of two Flexi-Fuzz LSSVM model variants. Consequently, it enhances the diversity of our proposed approach.2) The models utilizing the mean and median techniques for center determination are designated as Flexi-Fuzz LSSVM-I and Flexi-Fuzz LSSVM-II, respectively, each offering unique advantages tailored to different data conditions.

SECTION: 
To assess the effectiveness of the proposed models, i.e., FlexiFuzz-LSSVM-I and FlexiFuzz-LSSVM-II, we evaluate their performance against baseline models including SVM, LSSVM, FSVM, FSVM-CIL-Lin, FSVM-CIL-Exp, ACFSVM, and IF-RVFL. We utilized publicly available UCIand KEELbenchmark datasets. Moreover, we deploy the proposed models on the AD dataset, accessible through the Alzheimer’s Disease Neuroimaging Initiative (ADNI) (). The detailed experimental setup employed for evaluating the models is provided in Section S.II of the supplement file.

SECTION: 
In this subsection, we provide a comprehensive analysis and comparison of the proposed Flexi-Fuzz-LSSVM-I and Flexi-Fuzz-LSSVM-II models against various baseline models. This comparison spansbenchmark datasets from the UCI and KEEL repositories, covering a diverse range of domains. The results, including average accuracy and rank, are summarized in Table, while detailed classification accuracies along with the optimal parameters for each dataset are presented in Table S.I of the supplementary file. The average accuracies of the existing baseline models including SVM, LSSVM, FSVM, FSVM-CIL-Lin, FSVM-CIL-Exp, ACFSVM, and IF-RVFL are,,,,,, andrespectively. In contrast, the proposed Flexi-Fuzz-LSSVM-I and Flexi-Fuzz-LSSVM-II models achieved average accuracies ofand, respectively. This demonstrates a clear performance improvement, with the proposed Flexi-Fuzz-LSSVM-II model securing the top position and the Flexi-Fuzz-LSSVM-I model achieving the second position in terms of average accuracy.
Further, we can deduce that the innovative median approach used in Flexi-Fuzz-LSSVM-II to determine the class-center proves to be more effective in dealing with real-world data. The superiority of the median approach lies in its robustness to outliers. Unlike the mean, which can be significantly influenced by extreme values, the median provides a more stable central tendency measure, especially in skewed distributions. This characteristic is particularly advantageous in real-world datasets where noise and outliers are prevalent.

Relying solely on average accuracy as a single metric could be problematic, as exceptional performance on specific datasets might mask inadequate performance on others. To mitigate this concern, it becomes essential to individually rank each model with respect to each dataset, allowing for a comprehensive assessment of their respective capabilities. In the ranking scheme, the model with the poorest performance on a dataset is assigned a higher rank, while the model achieving the best performance is assigned a lower rank. In the assessment ofmodels acrossdatasets, the rank of themodel on thedataset can be represented as. Then the average rank of the model is determined as follows:. The average rank of the proposed Flexi-Fuzz-LSSVM-I and Flexi-Fuzz-LSSVM-II along with the baseline SVM, LSSVM, FSVM, FSVM-CIL-Lin, FSVM-CIL-Exp, ACFSVM, and IF-RVFL are,,,,,,,, andrespectively. The proposed Flexi-Fuzz-LSSVM-II model achieved the lowest average rank among all the models, while the proposed Flexi-Fuzz-LSSVM-I ranked third. As a lower rank indicates a better-performing model, the proposed Flexi-Fuzz-LSSVM-II emerged as the best-performing model. We now proceed to conduct statistical tests to ascertain the significance of the results. Firstly, we utilize the Friedman testto ascertain if there exist noteworthy distinctions among the models. The null hypothesis posits that the models exhibit equal performance, as indicated by their average rank. The Friedman test conforms to the chi-squared distributionwithdegrees of freedom (d.o.f) and is expressed as:. However, the Friedman statistic is overly cautious in nature. To address this,introduced a more robust statistic:, which followsdistribution withd.o.f.. Forand, we obtainedand. From the-distribution table atlevel of significance.
Since,, thus we reject
the null hypothesis. Hence, substantial differences exist among the models. Next, we employ the Nemenyi post hoc testto assess the pairwise differences among the models. The critical difference () is determined as, wheredenotes the critical value obtained from the distribution table for the two-tailed Nemenyi test. Referring to the statistical-distribution table, whereat asignificance level, theis computed as. The average rank differences between the proposed Flexi-Fuzz-LSSVM-I and Flexi-Fuzz-LSSVM-II models with the baseline SVM, LSSVM, FSVM, FSVM-CIL-Lin, FSVM-CIL-Exp, ACFSVM, and IF-RVFL models are,,,,,, andrespectively. As per the Nemenyi post hoc test, the proposed Flexi-Fuzz-LSSVM-II exhibits significant differences compared to the FSVM-CIL-Lin, ACFSVM, and IF-RVFL models. The proposed Flexi-Fuzz-LSSVM-I does not exhibit a statistical difference with the baseline models except ACFSVM. However, the proposed Flexi-Fuzz-LSSVM-I model surpasses the baseline models in terms of the average rank. Taking into account all these findings, we can conclude that the proposed Flexi-Fuzz-LSSVM-I demonstrates a competitive nature, and the proposed Flexi-Fuzz-LSSVM-II showcases superior performance against the existing models. The evaluation on the UCI and KEEL datasets with the added label noise is discussed thoroughly in Section S.II.B of the supplement file.

SECTION: 
AD is a progressive neurological disorder that detrimentally affects memory and cognitive functions. AD commonly begins with mild cognitive impairment (MCI). Currently, the exact causes of AD remain incompletely comprehended. However, the precise identification and diagnosis of AD are crucial in the provision of patient treatment, particularly during the first phase. In this study, we utilized scans from the Alzheimer’s Disease Neuroimaging Initiative (ADNI) dataset to train the proposed Flexi-Fuzz-LSSVM-I and Flexi-Fuzz-LSSVM-II models.
The ADNI project, launched by Michael W. Weiner in 2003, seeks to evaluate a range of neuroimaging techniques, such as positron emission tomography (PET), magnetic resonance imaging (MRI), and other diagnostic assessments for AD, particularly during the MCI phase. The feature extraction pipeline employed in this study aligns with the methodology described in. The dataset encompasses three classification scenarios: control normal (CN) versus AD, MCI versus AD, and CN versus MCI.

The performance metrics of the proposed Flexi-Fuzz-LSSVM-I and Flexi-Fuzz-LSSVM-II models, in comparison to baseline models, are presented in Table. Notably, Flexi-Fuzz-LSSVM-II and Flexi-Fuzz-LSSVM-I secured the top first and second positions with average accuracies ofand, respectively. In contrast, the baseline models, including SVM, LSSVM, FSVM, FSVM-CIL-Lin, and FSVM-CIL-Exp, demonstrated lower average accuracies of,,,, and, respectively. Compared to the third-top model, LSSVM, the proposed models (Flexi-Fuzz-LSSVM-I and Flexi-Fuzz-LSSVM-II) surpass it by approximatelyandin average accuracy, respectively. The Flexi-Fuzz-LSSVM-II model achieves the highest accuracy offor the CN vs AD case, followed by Flexi-Fuzz-LSSVM-I with an accuracy of. For the CN vs MCI case, Flexi-Fuzz-LSSVM-II and Flexi-Fuzz-LSSVM-I again emerge as the top performers, with average accuracies ofand, respectively. In the MCI vs AD case, Flexi-Fuzz-LSSVM-II and the existing FSVM-CIL-Lin stand out as the most accurate classifiers, with an accuracy of. Thus, Flexi-Fuzz-LSSVM-II consistently demonstrates superior performance, achieving high accuracy across various cases and establishing its prominence among the models. The success of the Flexi-Fuzz-LSSVM-II model can be largely attributed to its innovative use of the median approach to determine the class-center. Unlike the traditional mean approach, the median provides a more stable reference point that is less influenced by outliers and noise. This stability greatly enhances the model’s robustness, enabling it to perform well even under real-world conditions. This advancement is particularly beneficial in the context of AD diagnosis, where variability and noise in data are common challenges. Moreover, to illustrate the comparison between the proposed Flexi-Fuzz-LSSVM models and the baseline models in terms of specificity, sensitivity, and precision, we plotted the bar graphs shown in Figure S.4 of the supplement file. The overall findings underscore the effectiveness of the proposed models in distinguishing between different cognitive states.

SECTION: 
In this paper, we introduced Flexi-Fuzz, a robust and flexible membership scheme designed to address the challenges of noise, outliers, and class imbalance commonly encountered in AD diagnosis. By integrating the Flexi-Fuzz scheme into the framework of least squares support vector machines (LSSVM) and employing both the traditional mean approach and the innovative median approach to determine the class-center, we developed two novel models: Flexi-Fuzz-LSSVM-I and Flexi-Fuzz-LSSVM-II. The success of the proposed models is due to their robust membership scheme, which distinguishes noisy samples from normal ones while retaining the influence of boundary samples, the adaptive adjustments enabled by the flexible parameterand-nearest neighborhood parameter, and the median approach in Flexi-Fuzz-LSSVM-II, which enhances robustness by providing a stable reference less affected by outliers and noise. Extensive experimental evaluation reflects that the proposed models significantly outperform baseline models on UCI and KEEL datasets, maintaining exceptional performance even with introduced label noise datasets. Additionally, the efficacy of our models in diagnosing Alzheimer’s disease (AD) was validated using the ADNI dataset. Flexi-Fuzz-LSSVM-I exhibited competitive performance, while Flexi-Fuzz-LSSVM-II achieved the highest average accuracy and classification accuracy across all three cases (CN vs AD, CN vs MCI, and MCI vs AD). However, one limitation of the proposed Flexi-Fuzz-LSSVM models lies in the need for manual tuning of the two hyperparameters,and, which are integral to the membership scheme. These hyperparameters require careful adjustment to optimize the performance for different datasets and noise levels, which can be time-consuming.

In future work, researchers could focus on developing adaptive methods to dynamically and efficiently adjustandduring the training process, thereby eliminating the need for manual tuning. Such methods could leverage meta-heuristic optimization algorithms, or self-adaptive mechanisms to automate the adjustment process. Further, one can leverage the Flexi-Fuzz membership scheme and amalgamate it with cutting-edge models to tackle complex real-world problems.

SECTION: Acknowledgment
This work is supported by Science and Engineering Research Board (SERB) through the MTR/2021/000787 grant as part of the Mathematical Research Impact-Centric Support (MATRICS) scheme. Mushir Akhtar’s research was supported by a fellowship grant (no. 09/1022(13849)/2022-EMR-I) from the Council of Scientific and Industrial Research (CSIR), New Delhi.
Data collection and sharing for this project were supported by funding from the Alzheimer’s Disease Neuroimaging Initiative (ADNI), provided by the National Institutes of Health Grant U01 AG024904, and the Department of Defense award number W81XWH-12-2-0012. ADNI is financed by the National Institute on Aging, the National Institute of Biomedical Imaging and Bioengineering, and generous contributions from various organizations, including AbbVie, the Alzheimer’s Association, the Alzheimer’s Drug Discovery Foundation, Araclon Biotech, BioClinica, Inc., Biogen, Bristol-Myers Squibb Company, CereSpir, Inc., Cogstate, Eisai Inc., Elan Pharmaceuticals, Inc., Eli Lilly and Company, EuroImmun, F. Hoffmann-La Roche Ltd and its affiliated company Genentech, Inc., Fujirebio, GE Healthcare, IXICO Ltd., Janssen Alzheimer Immunotherapy Research & Development, LLC., Johnson & Johnson Pharmaceutical Research & Development LLC., Lumosity, Lundbeck, Merck & Co., Inc., Meso Scale Diagnostics, LLC., NeuroRx Research, Neurotrack Technologies, Novartis Pharmaceuticals Corporation, Pfizer Inc., Piramal Imaging, Servier, Takeda Pharmaceutical Company, and Transition Therapeutics. The Canadian Institutes of Health Research funds ADNI clinical sites in Canada. Private sector donations are managed by the Foundation for the National Institutes of Health (www.fnih.org). The grantee organization is the Northern California Institute for Research and Education, with the study coordinated by the Alzheimer’s Therapeutic Research Institute at the University of Southern California. ADNI data are distributed by the Laboratory for Neuro Imaging at the University of Southern California.

SECTION: References