SECTION: Scaled and Inter-token Relation Enhanced Transformer for Sample-restricted Residential NILM
Transformers have demonstrated exceptional performance across various domains due to their self-attention mechanism, which captures complex relationships in data. However, training on smaller datasets poses challenges, as standard attention mechanisms can over-smooth attention scores and overly prioritize intra-token relationships, reducing the capture of meaningful inter-token dependencies critical for tasks like Non-Intrusive Load Monitoring (NILM). To address this, we propose a novel transformer architecture with two key innovations: inter-token relation enhancement and dynamic temperature tuning. The inter-token relation enhancement mechanism removes diagonal entries in the similarity matrix to improve attention focus on inter-token relations. The dynamic temperature tuning mechanism, a learnable parameter, adapts attention sharpness during training, preventing over-smoothing and enhancing sensitivity to token relationships. We validate our method on the REDD dataset and show that it outperforms the original transformer and state-of-the-art models by 10-15% in F1 score across various appliance types, demonstrating its efficacy for training on smaller datasets.

SECTION: 
Energy efficiency is a growing challenge due to increasing energy demands, making effective power management critical. Smart grids and smart meters provide real-time data on energy usage, enabling utilities and consumers to implement advanced demand-side management strategies. NILM is crucial in home energy management systems (HEMS), disaggregating household energy usage into individual appliance data. This granular insight allows consumers to optimize their energy consumption, reduce wastage, and shift loads to reduce peak demands. Additionally, NILM aids in load forecasting and identifying appliance malfunctions, contributing to an effective, efficient and automated HEMS.

NILM is framed as a single-channel blind source separation problem, traditionally NILM has been approached using statistical models such as hidden Markov models (HMMs) and conditional random fields (CRFs). These models are capable of identifying appliance states but face significant challenges in scalability and in generalizing to unseen devices. However, recent advancements in deep learning have significantly enhanced NILM by introducing sequence-to-sequence models, which offer improved accuracy and efficiency. State-of-the-art architectures, including convolutional neural networks (CNNs), recurrent neural networks (RNNs), long short-term memory (LSTM) networks, and transformers, have revolutionized energy management systems by achieving superior performance and facilitating further progress in the field.

Transformers have become a cornerstone of modern deep learning due to their ability to model complex relationships across diverse data modalities such as text, images, and signals. Their self-attention mechanism enables capturing long-range dependencies, making them superior to CNNs in many complex tasks. However, transformers inherently lack local inductive biases like those present in CNNs, relying instead on large-scale datasets to learn effective representations. This limitation presents a significant challenge for domains like NILM, where data collection is often constrained by privacy regulations, hardware limitations, and practical feasibility. Consequently, there is a pressing need to adapt transformer architectures for optimal performance on small-scale NILM datasets. Existing solutions to this challenge have focused on introducing locality-preserving mechanisms, such as hierarchical architectures or modified self-attention modules, which mimic inductive biases found in structured data. While these approaches offer improvements, they still require medium-sized datasets and significant architectural tuning or expansion, which may not generalize well to NILM’s unique requirements. NILM datasets are characterized by sparsity, overlapping appliance signals, and high variability, all of which exacerbate the challenges faced by standard transformers.

This paper addresses these challenges by designing a transformer architecture tailored for small-scale NILM datasets. Our approach is motivated by the observation that standard attention mechanisms often suffer fromand an, both of which degrade performance when data is limited. To overcome these issues, we aim to improve the granularity of token relationships and the adaptability of attention distributions. The key contributions of this work are:

This mechanism modifies the self-attention computation by removing the influence of diagonal entries in the similarity matrix, thereby effectively reducing intra-token focus and more promoting meaningful interactions between tokens.

By introducing a learnable temperature parameter, we enable the model to dynamically and adaptively adjust the sharpness of its attention scores, mitigating over-smoothing and enhancing sensitivity to token relationships during training.

These contributions are firmly grounded with mathematical rigor and experimentally validated on the REDD dataset. The proposed mechanisms significantly improve attention mechanism, enabling the transformer to better handle NILM’s sparse and noisy data. Results demonstrate that our model outperforms the original transformer by 10-15% in F1 score across multiple appliance types, establishing its effectiveness for small-scale NILM datasets.

SECTION: 
NILM has advanced significantly with deep learning methods, overcoming the limitations of earlier classical machine learning approaches. Earlier widely used models like HMMs and CRFs focused on disaggregating energy use by modeling appliance states. These approaches faced scalability challenges, limited generalization, and computational inefficiencies, making them impractical for real-time use.

Deep learning has proven more effective by learning complex patterns directly from data. CNNs, widely used in NILM, excel at extracting local features from NILM data. Kelly and Knottenbelt demonstrated the advantages of CNNs, LSTMs, and autoencoders, achieving notable performance gains over classical models. Enhanced CNN architectures improved detection of appliance-specific features like power changes and thresholds. However, CNNs struggle to capture long-range dependencies which are critical for accurate disaggregation. Transformers address this limitation with attention mechanisms that model both global and local dependencies. Yue’s BERT4NILM achieved significant performance gains for NILM using the vanilla transformer. Inspired by this, transformer-based methods have shown effectiveness across three main categories: the first group employs standard architectures with simple training mechanisms,,. The second group uses advanced training techniques with simpler architectures,,. The third combines advanced training methods with substantial architectural changes,,. Transformers require large datasets, which is challenging in NILM due to privacy constraints. Standard architectures also face over-smoothing and excessive focus on intra-token relations, limiting performance on smaller datasets. Our approach addresses these issues by aligning with the first group, proposing a vanilla transformer with improvements tailored for small-scale NILM. An enhanced attention mechanism strengthens inter-token relations and reduces over-smoothing, offering a lightweight yet effective solution. While we do not compete directly with the advanced architectures of the second and third groups, our method is complementary and well-suited for resource-constrained NILM scenarios.

SECTION: 
SECTION: 
The original formulation of attention in transformeris formulated as follows. Suppose, we have a set of tokens, whereis the dimension of token embedding andis the number of tokens (for simplicity of discussion, we are omitting the description of how these tokens are computed in this section; however, it is discussed in the following section). The attention is computed by applying three projections onwhich produces a query matrix, key matrixand value matrix. Formally, these projections are obtained as follows:

where, where,andare learnable weight matrices without any constraints. The attention is computed via a similarity matrixbetweenand, defined as:

is similarity between the-th query and the-th key, capturing the semantic relationship between these tokens.further goes through a normalisation withoperator and scaled by a factor of. Formally, the attentionfor the-th token is computed as follows.

whereis the attention matrix for the-th token, and theoperator is applied row-wise to the scaled similarity matrix. Note thatreflects self-attention scores.

We identify two potential causes for this issue which are given below in detail:

SECTION: 
Our method addresses the above two issues with the following solutions. Below we provide a formal and comprehensive explanation of each, underpinned by theoretical foundations.

The self-attention mechanism inherently prioritizes intra-token relationships due to the significant influence of the diagonal entries in the similarity matrix. This emphasis on self-similarity hinders the model’s ability to attend to meaningful inter-token relationships that are crucial for capturing long-range dependencies and contextual information essential for NILM. The proposed inter-token relation enhancement mechanism addresses this limitation by eliminating the diagonal entries of the similarity matrix.

To counter this effect, we propose to nullify the diagonal entries, thereby forcing the model to focus on inter-token relationships. We enhance inter-token relationships by replacing the diagonal entrieswith, effectively removing the intra-token self-similarity from consideration during attention computation:This ensures that the attention scores allocated to intra-token relationships approach zero. The resulting attention matrix,, is then computed as

This mechanism improves the model’s ability to capture and emphasize the relationships between distinct tokens, leading to enhanced modeling of global dependencies across tokens.

In the original self-attention formulation, the similarity scores inare scaled by a fixed temperature, whereis the dimensionality of the key vectors. While this scaling helps prevent excessively large values from dominating theoperator, it lacks flexibility. The optimal temperature may vary across different datasets, appliance types, and training phases, yet the use of a fixed temperature restricts the model’s capacity to adapt to these variations. Therefore, we propose a learnable temperature parameter,, which is dynamically adjusted during training through a meta-network (shown in Fig.).

To optimizedynamically, we introduce a meta-network, denoted by, that learns the appropriate temperature based on the input tokens. The meta-network takes the pooled features of the token embeddingsat layerand outputs a value for, which is bounded to prevent extreme values. Specifically, we constrainwithin the empirically decided intervalusing a sigmoid-like range squeezing function.

The meta-network architecture consists of several fully connected layers with ReLU activations:

whereandare learnable weights,denotes the sigmoid activation function defined as, andis a average pooled representation of either the token embeddingsor the output of previous transformer block. The output of the meta-network is then scaled to the desired range.

This adaptability improves the model’s performance across various datasets and tasks, avoiding over-smoothing or overly peaked distributions. Figureshows the framework of our proposed method. Our framework is similar to other transformer based methods such asbut with the proposed inter-token relation enhancement and dynamic temperature mechanisms. Briefly, we first encode our aggregated load signal with a convolution based encoder module, followed by a number of improved transformer layers with the above two mechanisms. Finally, we produce the appliance signal with a deconvolution based reconstruction module. We used a BERT based training mechanism similar to.

SECTION: 
SECTION: 
We use the REDD dataset, which includes power consumption data from six houses (i.e., residential) in USA. The dataset contains time-series data for both whole-home and appliance-specific channels, recorded at high (15 kHz) and low (1 Hz) frequencies. For evaluation, we focus on low-frequency recordings and follow standard data processing protocols from previous studies. To evaluate model generalization, data from houses 2 to 6 is used for training, and house 1 for testing. We select common appliances across all households, excluding devices with limited presence or faulty data. The four appliances used are the fridge, washer, microwave, and dishwasher.

SECTION: 
We train separate models for each appliance using a transformer network consisting of an embedding block, a transformer encoder of multiple layers, and a reconstruction block.
The embedding block has a convolution layer (kernel size=5, padding=2), and the reconstruction block uses a deconvolution layer (kernel size=4, stride=2, padding=1). We use similar losses and their combinations as. The transformer network is implemented in PyTorch. Models were trained for 100 epochs with the AdamW optimizer and BERT-style losseson a P100 GPU from a cloud HPC. Following, we use 2 transformer layers/heads, a hidden dimension of 16, dropout ratio of 0.5, and masking ratio of 0.3 for optimal performance.

SECTION: 
We used four common metrics for evaluation:(Acc.),,(MRE), and(MAE).Accuracy is the ratio of true positives (TP) and true negatives (TN) over the total number of predictions,, where FP and FN are false positives and false negatives.F1 evaluates the model’s performance with imbalanced classes,.MRE measures the accuracy of appliance energy estimates,.MAE calculates the average prediction error,.is the size of batch andandare original and reconstructed inputs, respectively.

SECTION: 
Tableshows the experimental results of our proposed method. We compare our results with 16 recent SOTA (state-of-the-art) methods under four evaluation metrics. Specifically, we compare three groups of methods. The first group uses standard deep networks with relatively simpler training mechanisms and the methods in this group are GRU+, LSTM+, CNN, BERT4NILM, Seq2Point, Seq2Seq, RNNand Compact Transformer. The second group uses advanced training mechanisms without significant changes in network architecture and the methods in this group are TransformNILM, ELECTRIcityand CTA-BERT. The third group uses advanced training mechanisms with significant changes in network architecture and the methods in this group are ELTransformer, Switch T/F, WindowGRU, SGNetand LA-InFocus.
It can be seen that our proposed method surpassed the performance of many methods of all three groups across all metrics and appliance types. Several second and third-group methods show strong results indicating the strength of using advanced training and network architectures. In comparison to these methods, our method is trained with a simple training mechanism and still uses a very similar architecture to the original transformer model. Instead of being competitive with them, we think that incorporating our proposed attention improvement mechanisms into their pipeline could further improve their performance since those mechanisms are model and training-agnostic from the point of principle. Specially, we consider our method as more fairly comparable with the original transformer model for NILM.

SECTION: 
We conducted an ablation study of our proposed method to assess the effectiveness of inter-token enhancement and dynamic temperature tuning mechanisms. Tablepresents the findings across four appliances from the REDD dataset.

The results demonstrate that the inter-token relation enhancement mechanism significantly improves performance, outperforming the baseline method and many SOTA approaches.

We experimented with several pre-selectedvalues, i.e.,,,,,, and, to evaluate the effectiveness of the learning-based dynamic temperature scaling mechanism. The findings indicate a clear impact of temperature scaling across different appliances, withandyielding the best performance, as expected. We also analyzed the learnedvalues, which showed a significant advantage when utilizing our proposed meta-network.

To further validate the effectiveness of our meta-network-basedlearning mechanism, we also conducted experiment usingas a learnable parameter by initializing it randomly and trained in an end-to-end manner, without the meta-network. The results indicate that the meta-network is essential for achieving improved performance, as the alternative approach led to a negativeduring the learning process, resulting in poorer outcomes.

SECTION: 
We compare the compute time of our proposed method with the standard attention mechanismusing a P100 GPU on an HPC cluster. Tableshows the comparison. Our method adds a small amount of computation time compared with the standard attention model. However, it can significantly improve the attention module at the expense of this negligible computing cost. Powerful GPUs may improve this condition.

SECTION: 
In this paper, we proposed two mechanisms for improving the original transformer model in terms of its capacity. Our experimental results effectively validate that our proposed mechanisms improve the NILM performance on smaller datasets with transformer architecture. We provide theoretical analysis of our proposed mechanisms. In the future, we plan to experiment with larger datasets and optimize compute time.

SECTION: References