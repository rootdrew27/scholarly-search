SECTION: Data Quality Dimensions for Fair AI

Artificial Intelligence (AI) systems are not intrinsically neutral and biases trickle in any type of technological tool. In particular when dealing with people, the impact of AI algorithms’ technical errors originating with mislabeled data is undeniable. As they feed wrong and discriminatory classifications, these systems are not systematically guarded against bias. In this article we consider the problem of bias in AI systems from the point of view of data quality dimensions. We highlight the limited model construction of bias mitigation tools based on accuracy strategy, illustrating potential improvements of a specific tool in gender classification errors occurring in two typically difficult contexts: the classification of non-binary individuals, for which the label set becomes incomplete with respect to the dataset; and the classification of transgender individuals, for which the dataset becomes inconsistent with respect to the label set. Using formal methods for reasoning about the behavior of the classification system in presence of a changing world, we propose to reconsider the fairness of the classification task in terms of completeness, consistency, timeliness and reliability, and offer some theoretical results.

SECTION: 1Introduction

Machine Learning (ML) models trained on huge amounts of data are intrinsically biased when dealing with people. Common face recognition systems used in surveillance tasks generate false positives labeling innocent people as suspects. Social credit systems link individuals to the state of their social credit, making decisions based on that score. In all of those cases, subjects suffer a credibility deficit due to prejudices related to their social identity[1]: a dark-skinned man could be characterized by a higher risk of recidivism after being arrested; a short-haired skinny young woman – or a long-haired boy with feminine traits – might be the target of transphobic attacks following misgendering. Through the deployment of these technologies, society makes the gap separating rich from poor, cisnormative from non-cisnormative individuals, more constitutive as automatized and standardized.

Already before the explosion of ML algorithms,[2]offered a framework for understanding three categories of bias in computer systems, assuming the absence of bias as necessary to define their quality. Later on, the emergence of contemporary, data-driven AI systems based on learning has significantly worsened the situation, see e.g.[3,4]. On this basis, the development and deployment of fairer Artificial Intelligence (AI) systems has been increasingly demanded. Such request appears especially relevant in certain application contexts. For example, as examined in[5], face is commonly used as a legitimate mean of gender classification, and this is operazionalized and automatized in technologies such as Automatic Gender Recognition (AGR), which algorithmically derives gender from faces’ physical traits to perform classification[6,7]. This technique relies on the assumption that gender identity can be computationally derived from facial traits.
However, a recent study[8]shows that the most famous AGR systems are not able to classify non-binary genders, also performing poorly on transgender individuals. This is due to the fact that AGR incapsulates a binary, cisnormative conception of gender, adopting a male/female scheme which invalidates non-binary identities.

We declare ourselves against the use of gender classification, as considering face as a proxy for detecting gender identity seems to resonate with phrenology and physiognomy,
and we believe that the process of automatic gender recognition can easily lead to mismatches between the theoretical understanding of constructs underlying identity and their operationalization[9],
especially when it comes to classification of individuals who recognise themselves outside of binarism.
However, we note that this kind of classification is already happening[10], spreading with commercial systems offering gender classification as a standard feature, causing a huge impact on the lives of misgendered individuals. Therefore there are contexts in which it is potentially inevitable that classification exists, and in these contexts it must be fairer. This translates into asking whether there is a strategy to ensure that the labels assigned during classification are as less stereotypical and archetypal as possible. While this paper does not investigates the ethical aspects of AGR, we aim at addressing the issues related to the classification strategies to make them fairer, as an initial study to prepare for implementing mitigation strategies.

An important task, common to technology and philosophy, is therefore the identification and verification of criteria that may help developing fairness conditions for AI systems. While a number of techniques are available to mitigate bias, their primary focus on purely statistical analysis to control accuracy across sensitive classes is clearly insufficient to control social discrimination. A different approach is represented by the explicit formulation of ethical principles to be verified across protected attributes, combining statistical measures with logical reasoning, as formally defined in[11,12,13,14,15]and implemented by the BRIO tool in[16,17]. In this latter context, an important direction to explore for a refined definition of ethically-laden verification criteria is the study of quality dimensions and associated biases. In the following of this paper, we offer a theoretical contribution in this direction, preparing the ground for a future implementation. We argue that, even if maximizing data quality and fairness simultaneously can be hard as improving one can deteriorate the other[18], the task of bias mitigation tools can be supported by reasoning on quality dimensions that so far have been left ignored. In particular, we offer examples to show how dimensions of consistency, completeness, timeliness and reliability can be used to establish fairer AI classification systems. This research is in line with the quest for integrating useful empirical metrics on fairness in AI with asking key (conceptual) questions, see[19].

The paper is structured as follows. In Section2we offer an overview of fainess definitions and bias types relevant for this work. In Section3we briefly overview the technical details of a particular bias mitigation tool to illustrate what we consider essential limitations of purely statistical analyses. In Section4we introduce data quality dimensions arguing for reconsidering their relevance in the task of evaluating the fairness of classification systems, presenting two examples to justify this requirement. In Section5we propose a definition of fair AI classification that includes such dimensions and formulate some theoretical results. Section6concludes the work illustrating future research lines.

SECTION: 2Fairness and Bias in ML

Despite a unique definition missing in the literature[2,3,20,21,22,23,24,25], fairness is often presented as corresponding to the avoidance of bias[26]. This can be formulated at two distinct levels: first, identifying and correcting problems in datasets[27,28,29,30,31,32], as a model trained with a mislabeled dataset will provide biased outputs; second, correcting the algorithms[21,33], as even in the design of algorithms biases can emerge[34]. In the present section we are interested in considering datasets and their labels. Indeed, bias may also affect the label set[35,36]. Accordingly, we talk aboutlabel quality biaswhen errors hit the quality of labels. As shown in[37], the most well-known AI datasets are full of labeling errors. A crucial task is therefore the development of conceptual strategies and technical tools to mitigate bias emergence in both data and label sets.

A variety of approaches and contributions is available in the literature focusing on identifying bias in datasets and labels. Here we list the types of bias which are relevant to the present work, see Table2. Albeit not exhaustive, these lists of biases represent a good starting point to investigate quality dimensions required to address them.
We now analyze a common mitigation strategy used by existing tools addressing the issue of bias in data, showing their limitations. We then study the bias in the classification algorithm (i.e., bias in labels) of the mitigation tool.

table

Data and Label Bias.{tabularx}lXl
Bias type  Definition  Literature

Data Bias

Behavioral biasUser’s behavior can be different across contexts[38]

Exclusion biasSystematic exclusion of some data[39]

Historical biasCultural prejudices are included into systematic processes[40]

Time interval biasData collection in a too limited time range[41]

Label Bias

Chronological biasDistortion due to temporal changes in the world which data are supposed to represent[39]

Historical biasCultural prejudices are included into systematic processes[40]

Misclassification biasData points are assigned to incorrect categories[42]

SECTION: 3Mitigating Bias

Abias mitigation algorithmis a procedure for reducing unwanted bias in training datasets or models, with the aim to improve the fairness metrics. Those algorithms can be classified into three categories[43]: pre-processing, when the training data is modified; in-processing, when the learning algorithm is modified; post-processing, when the predictions are modified.

Several tools are available to audit and mitigate biases in datasets, thereby attempting to implement diversity and to reach fairness. Among the most common are AIF360[22],
Aequitas[44]and Cleanlab[45].
Recently a post-hoc evaluation model for bias mitigation has been proposed by the tool BRIO[16,17]. In this article, we consider Cleanlab as a testbed, illustrating below in Section4its limitations in view of data quality dimensions. Instead, we propose a theoretical frame for the resolution of such limitation in Section5, further illustrating the possibility to implement the present analysis in the tool BRIO. For an overview of the symbols used from now on, see Table2.

table[t]Symbols used in the present work.{tabularx}p0.25XTime indexTime frameGeneric datapointData labelsDiscrete random variable correctly labeledDiscrete random variable wrongly labeledThe set of unique class labelsA mapping between variablesThe probability of labelbeing wrong at time, given that labelwas correct at timeTemporal confident joint, where the correct label can change fromtoin time frameTemporal confident joint, where the correctness of the same fixed labelcan change in time frameChange ratePredicted probability of labelfor variableand model parametersLabel setAI systemPartition of the label setPopulation of interestAn element fromA datapoint in systemover time frameA correct label for the datapointThreshold variable

Cleanlab is a framework to find label errors in datasets. It uses Confident Learning (CL), an approach which focuses on label quality with the aim to address uncertainty in dataset labels using three principles: counting examples that are likely to belong to another class using the confident joint and probabilistic thresholds to find label errors and to estimate noise; pruning noisy data; and ranking examples to train with confidence on clean data. The three approaches are combined by an initial assumption of a class-conditional noise process, to directly estimate the joint distribution between noisy given labels and uncorrupted unknown ones. For every class, the algorithm learns the probability of it being mislabeled as any other class. This assumption may have exceptions but it is considered reasonable. For example, a “cat" is more likely to be mislabeled as “tiger" than as “airplane". This assumption is provided by the classification noise process (CNP,[46]), which leads to the conclusion that the label noise only depends on the latent true class, not on the data.
CL[45]exactly finds label errors in datasets by estimating the joint distribution of noisy and true labels. The idea is that when the predicted probability of an example is greater than a threshold per class, we confidently consider that example as actually belonging to the class of that threshold, where the thresholds for each class are the average predicted probability of examples in that class. Giventakes an observed, noisy label (potentially flipped to an incorrect class); andtakes the unknown (latent), true, uncorrupted label (latent true label),
CL assumes that for every example it exists a correct labeland defines a class-conditional noise process mapping, such that every label in classmay be independently mislabeled as class, with probability. So, maps are associations of data to wrong labels. Then CL estimatesandjointly, evaluating the joint distribution of label noisebetween noisy given labels and uncorrupted unknown labels. CL aims to estimate everyas a matrixto find all mislabeled examplesin dataset, where. Given as inputs the out-of-sample predicted probabilitiesand the vector of noisy labels, the procedure is divided into three steps: estimation ofto characterize class-conditional label noise, filtering of noisy examples, training with the errors found.

To estimatei.e. the joint distribution of noisy labelsand true labels, CL counts examples that may belong to another class using a statistical data structure named confident joint, formally defined as follows

In other words, the confident joint estimates the setof examples with noisy labeliwhich actually have true labeljby making a partition of the datasetinto bins, namely the set of examples labeledwithlarge enoughexpected probabilityto belong to class, determined by a per-class threshold, whereis the model.

This kind of tools are extremely useful in estimating label error probabilities. However they have some limitations, and it is easy to formulate examples for which their strategy seems unsound. A first problem arises from the initial assumption of the categoricity of data.
Take for example the case of gender labeling of facial images, which is typically binary (i.e. with values male, female). For each datapoint, a classification algorithm calculates the projected probability that an image is assigned to the respective label. Consider though two very noisy cases: images of non-binary individuals; images of transgender individuals.
In the former case, the label set becomes incomplete with respect to the dataset; in the second case, the dataset is inconsistent with respect to the label set. Hence, there can be datapoints that have either 1) none of the available labels as the correct one, or 2) at different times they can be under different labels. By definition, if we have disjoint labels there can be high accuracy but only on those datapoints which identify themselves in the disjointed categories.
In situations like these, it appears that the dimension of accuracy alone does no longer satisfy the correctness of the classification algorithm. In terms of quality dimensions, the possibility of an uncategorical datapoint or that of a moving datapoint is no longer only an accuracy problem. Hence, the identification of other data quality dimensions to be implemented in tools for bias mitigation may help achieve more fairness in the classification task. In the next section we suggest an improvement of the classification strategy by adding dimensions that should be considered when evaluating the fairness of the classification itself.

SECTION: 4Extending Data Dimensions for Fair AI

In the literature, data quality dimensions are defined both informally and qualitatively. Metrics can be associated as indicators of the dimension’s quality. However, there is no single and objective vision of data quality dimensions, nor a universal definition for each dimension. This is because often dimensions escape or exceed a formal definition. The cause of the large amount of dimensions[47,48]also lies in the fact that data aim to represent all spatial, temporal and social phenomena of the real world[49]. Furthermore, they are constantly evolving in response to continuous development of new data-driven technologies.

For the purposes of our analysis, we focus on the following basic set of data quality dimensions which is the focus of the majority of authors in the literature[50,51]:

Accuracy, i.e. the closeness between a valueand a value, where the latter is the correct representation of the real-life phenomenon thataims to represent[47];

Completeness, i.e. the level at which data have the sufficient breadth, depth, and scope for their task[48,52,47];

Consistency, i.e. the coherence dimension: it amounts to check whether or not the semantic rules defined on a set of data elements have been respected[47];

Timeliness, the data freshness over time for a specific task[53,54].

We thus indicate them as potential candidates to be implemented in the context of bias mitigation strategies. In particular, we argue that, as data are characterized by evolution over time, the timeliness dimension[47]can be taken as basis for other categories of data quality.
We aim at suggesting improvements on errors identification in the classification of datapoints, using the gender attribute as an illustrative case.
We thus suggest the extension of classification with dimensions of completeness, consistency and timeliness and then return to Cleanlab to illustrate how this extension could be practically implemented.

SECTION: 4.1Incomplete Label Set and Inconsistent Labeling

Consider the first example of a datapoint which represents a non-binary individual. This kind of identity is rarely considered in technology[55].
Non-binary identities do not recognize themselves within the binary approach characteristic of classification systems. As such, individual identity is not correctly recognized by the classification system, highlighting the insufficiency of the model which flattens the gender identity umbrella on the two options of male/female.
The conceptual solution would be to simply assume the label set as incomplete. This means that the bias origin is in the pre-processing phase,
and a possible strategy is to extend the partition of the labels adding categories as appropriate, e.g. “non binary”. The problem is here reduced to the consideration of the completeness of the label set.[8]can be considered a first attempt in this direction.

Consider now a transgender datapoint whose identity shifts over time, being a fluid datapoint by definition.
Currently AI systems operationalize gender in a way which is completely trans-exclusive, see e.g.[7,6].
However, identity is not static: it may move with respect to
the labels we have, leading the datapoint to be configured in a label or in a different one during a selected time range. In this case, any extension of the label set is misleading, or at least insufficient. Here we cannot just add more categories, but we have to find a logical solution to changing the label of the same datapoint at different timepoints.

SECTION: 4.2Enter Time

The two problems above can be formulated adding to completeness and consistency the dimension of temporality. Thus, an important starting point is represented by adding the dimension of timeliness, which concerns the degree to which data represent reality within a certain defined time range for a given population.

We suggest here considering the labeling task within a given time frame, whose length depends on the dataset and the classification task over the pairing of datapoints to labels, to measure a probability of a label-change over time.
Intuitively, if the analysis is performed less than a certain number of timestamps away from the last data labeling, then we consider the labeling still valid. Otherwise, a new analysis with respect to both completeness of the dataset and label set must be performed. Technically, this means associating temporal parameters to labels and to compute the probability that a given label might change over the given time frame. The probability of a label being correct (its accuracy) decreases within the respective temporal window. In particular, reasoning on the temporal evolution of the dataset could allow us to model the evolution of the label partitions. Two fundamental theses are suggested for evaluation: the correctness of the task does no longer assume static completeness of the label set, i.e. given the label set is complete at time, it can be incomplete at time; the labeling does no longer assume static perseverance of the labels, that is, given a labelthat is correct at a timefor a datapoint, it could be incorrect at a later time, and conversely if it is incorrect it could become correct.

SECTION: 4.3Back to Cleanlab

Considering a possible implementation in Cleanlab able to account for such differences implies renouncing the starting assumption on the categoricity of the data. Instead, assume that the probability of assigning a label may change over time. This can be formulated in two distinct ways.
First, the probability value of a given labelbeing wrong, given a labelis correct (their distance) may change over time. The task is now to give a mapping of all the label-variable pairs, i.e. given a mappingbetween variables, whereis the correct label andthe wrong one, compute the probability over the time frame

such that labelis wrong at time, given that labelwas correct at time. This probability can increase or decrease, depending on the dataset and on the label set.
For the definition of the confident joint, this means taking the evaluation of all the elements that have an incorrect labelwhen their correct label is, and then associate the wrong label to a timeand the correct label to a previous time. This estimate must be made on all time points, so for every. Given a timepointat which the label is wrong, the estimate on all pairs of probabilities for that point with a previous point in which another label can be correct has to be computed

Second, given a mappingbetween variables, whereis the correct label andthe wrong one, what is the probability

such that labelis wrong at time, given that the same labelwas correct at time? In this case, the same label is fixed and the probability that it becomes incorrect can be calculated. The definition of confident joint thus becomes

To illustrate the point we consider a toy example. Compute

i.e. the error rate ofhas to be determined. First, a confusion matrix is constructed to analyze errors. Suppose to have a dataset of 10 datapoints, see FigureLABEL:fig:t1.
From the matrix,and. So there are 5 women, of which 2 are incorrectly labeled “male" and 3 are correctly labeled “female", and 5 men of which 1 is incorrectly labeled “female" and 4 are correctly labeled “male".
Replacing the values in Equation6,.
The obtained value represents the error rate of the “male" label, i.e. the probability of a male datapoint being labeled “female". Looking at the diagonals, the true positive rate TPR = 70% and the false positive rate FPR = 30%.

Consider now the same dataset at a later time. The labels might have changed. From the matrix,and that. Now there are 5 women, of which 3 are incorrectly labeled “male" and 2 are correctly labeled “female", and 5 men of which 3 are incorrectly labeled“female" and 2 are correctly labeled “male". Replacing again the values in6,.
In this case the true positive rate TPR = 40% and the false positive rate FPR = 60%.

To understand how the error rate changes, the difference between the two matrices has to be considered.
Thus, the change rate can be computed as.

Nowcan be written as. Thus, at a timewe have. At a subsequent timewe have. Equation6can be computed with respect to time as

This value represents the (highest) probability that a given label is wrong at a given time, provided it was correct at some previous time. Indirectly, this also expresses the probability that the labeling set is applied to a dataset containing a point for which the labeling becomes inconsistent over time.

SECTION: 5Temporal-based Fairness in AI

We have argued that a more general discussion on the data dimensions to be adopted in bias mitigation tools is needed, and in particular that the dimension of timeliness is crucial. In this section we summarise our proposal and offer non-exhaustive criteria for fairness in AI based on such temporal approach along with some basic theoretical results.

The first metric that has been addressed in this work is completeness as applied to the label set. In a world where gender classification is actually changing, the present strategy includes the completeness dimension in the quality assessment, verifying that the label set is complete with respect to the ontology of the world at the time this assessment is made. The solution here is to extend the label set as desired adding new labels for the classification task, as already suggested in[8]. Additionally, we suggest an explicit temporal parametrization: completeness can be considered as a relationship between a label set and an individualbelonging to a certain population, whereis any domain item that entersat a time. We must ensure that a correct labelexists for each datapoint in the dataset at each time.

A label setfor a classification algorithm in a AI systemis considered complete over a time framedenoted asiff given two partitionsand, where possiblyfor alls.t.there iss.t..

In other words, the completeness of a dataset over a time frame is granted if for every datapoint representing an element in the population of interest there exists at any two possibly consecutive points in time a correct label for it.

Next, we considered consistency of the label set with respect to datapoints possibly shifting in categorization. The method here again is to reduce consistency to timeliness. We suggest to compute the probability of an inconsistency arising from a correct label change.
Accuracy, albeit the most used metric for evaluating classification models’ performances due to its easy calculability and interpretation, is reductive, trivial and incorrect in some contexts. For example, if the distribution of the class is distorted, accuracy is no longer a useful, nor a relevant metric. Even worse, sometimes greater accuracy leads to greater unfairness[56]: some labels like race or gender may allow models to be more predictive, although it seems to be often controversial to use such categories to increase predictive performance.
We have suggested to consider temporal accuracy[57]as a function of the error rate over time.

The ability to compute the variance in the error rate across time is functional to determine the reliability of AI systems. This metric is linked to the notion of accuracy, as it is considered as a measure of data correctness, see[47]. In[48]and[57]reliability is even contained in the definition of accuracy itself: data must be reliable to satisfy the accuracy dimension. Overall, it seems that reliability is not actually controlled beyond physical reliability, as in the literature on data quality there is no formal definition to compute it. However, following[58]the previously provided temporal approach is again useful: evaluating reliability is based on the revisions which show how close the initial estimate of accuracy is to the following ones. In this sense, reliability can be reduced to accuracy over time in terms of a threshold on the error rate:

A classification algorithm in a AI systemis considered reliable over a time framedenoted asiff, for some safe value.

The change ratewe have computed shows how much the system’s accuracy deteriorates. If it exceeds a fixed safe value, the system is no longer accurate. Plain accuracy is the numerical measure at some time. If this value does not deteriorate over a certain fixed threshold, the system is considered reliable, and therefore accurate with respect to time.

The two previous definitions offer non-exhaustive criteria for the identification of fair AI systems:

only ifand.

Hence we claim that fairness requires the system’s ability to give reliable and correct outcomes over time.
While we do not consider these properties sufficient, we believe they are necessary.
On this basis, we can formulate two immediate theoretical results:

Given a label setcomplete at time, a classification algorithm guarantees a fair classification at timeif and only if the change rate determined with respect tois.

Assume, then forwe need to showfor. Assume, then by Definition2reliability is not satisfied; hence, if, it must be the case that.
∎

Given a fixed change rate, a classification algorithm with fair behaviour at timeremains fair at timeif and only if the change to make the label set complete at timedoes not exceed ansuch that.

Considerwith change rateas a base case, then by Definition3and. Now considerand a required changeinsuch thatholds. This obviously holds only if. Generalize for any.
∎

Note that in these results the value of, respectively, is a proxy for how much the world has changed atwith respect to.

In the context of an incomplete label set, a detected label bias can originate from an exclusion bias in data, which can also result from a time interval bias. In the case of label-changing datapoints a chronological bias occurs. Then, misclassification bias can be reduced to the two previous types. In the context of use, emergent bias can arise as a result of changes in societies and cultures. It might appear in data as chronological, historical or behavioral bias. Here, a different value bias occurs for example when the users are different from the assumed ones during the system’s development. This is the case of ontology switching, to which a label set must adapt.
These types of bias can be mitigated by implementing the proposed framework. The tool BRIO[16,17]works as a post-hoc model evaluation, taking in input the test dataset of the model under investigation and its output. The tool allows to investigate behavioural differences of the model both with respect to an internal analysis on the classes of interest, and externally with respect to chosen reference metrics. Morever, it allows to measure bias amplification comparing the bias present in the dataset and how that manifests itself in the output. While the present work does not aim at offering a full implementation of our theoretical analysis for the BRIO tool, some remarks are appropriate. The time-based analysis of completeness and reliability offered in Definitions1and2, in turn grounding a notion of fairness in Definition3are easily implementable in BRIO: both completeness and reliability require the definition of a timeframe to check respectively that any given datapoint of interest is matched against a desirable label and that the overall change rate of error for one or more classes of interest does not surpass a certain threshold. Both features rely on the user for the identification of the desirable label for any datapoint and for the admissible distance.

SECTION: 6Conclusion

We presented some recommendations for AI systems design, focusing on timeliness as a founding dimension for developing fairer and more inclusive classification tools. Despite the crucial importance of accuracy as shown by significant works such as[4]and[59], the problem of unfairness in AI systems is much broader and more foundational. This can be expressed in terms of data quality: AI systems are limited in that they maximize accuracy, and even if systems become statistically accurate some problems remain unsolved. This is exemplified by the case of binary gender labeling, which leads to inaccurate simplistic classifications[60].
Furthermore, as the work of classification is always a reflection of culture, the completeness of the label set and the (constrained) consistency of labeling have an epistemological value: constructing AIs requires us to understand society, and society reflects an ontology of individuals. For this reason, misgendering is first of all an ontological error[6].

We suggested that timeliness is a crucial dimension for the definition of gender identity. If we are ready to consider gender as a property that shifts over time[61], and which can also be declined in the plural, as an individual may identify under more than one - not mutually exclusive - labels, then a change of paradigm is required. Design limitations such as binarism and staticity invalidate identities which do not fit into this paradigm. They must be addressed if fairer classifications and more inclusive models of gender are to be designed.

Further work in this direction includes: an implementation and empirical validation of the proposed model through the BRIO tool; and the design of an extension to compute the probability of incorrect labels becoming correct over time, i.e. the dual case of what presently addressed.