SECTION: Hybrid deep learning-based strategy for the hepatocellular carcinoma cancer grade classification of H&E stained liver histopathology images

Hepatocellular carcinoma (HCC) is a common type of liver cancer whose early-stage diagnosis is a common challenge, mainly due to the manual assessment of hematoxylin and eosin-stained whole slide images, which is a time-consuming process and may lead to variability in decision-making. For accurate detection of HCC, we propose a hybrid deep learning-based architecture that uses transfer learning to extract the features from pre-trained convolutional neural network (CNN) models and a classifier made up of a sequence of fully connected layers.
This study uses a publicly available The Cancer Genome Atlas Hepatocellular Carcinoma (TCGA-LIHC)[1]database (n=491) for model development and proprietary database of Kasturba Gandhi Medical College (KMC), India[2]for validation. The pre-processing step involves patch extraction, colour normalization, and augmentation that results in 3920 patches for the TCGA dataset. The developed hybrid deep neural network consisting of a CNN-based pre-trained feature extractor and a customized artificial neural network-based classifier is trained using five-fold cross-validation. For this study, eight different state-of-the-art models are trained and tested as feature extractors for the proposed hybrid model.

The proposed hybrid model with ResNet50-based feature extractor provided the sensitivity, specificity, F1-score, accuracy, and AUC of 100.00%, 100.00%, 100.00%, 100.00%, and 1.00, respectively on the TCGA database. On the KMC database, EfficientNetb3 resulted in the optimal choice of the feature extractor giving sensitivity, specificity, F1-score, accuracy, and AUC of 96.970.83, 98.850.24, 96.710.66, 96.710.66, and 0.990.01, respectively. The proposed hybrid models showed improvement in accuracy of 2% and 4% over the pre-trained models in TCGA-LIHC and KMC databases.

SECTION: IIntroduction

Globally, over 700,000 people die every year from liver cancer, making it the third most common cause of cancer-related death[3][4]. The hepatocellular carcinoma (HCC) is the most common type of liver cancer accounting for almost 80% of all the primary liver cancer cases[5,6]. A major spread of the HCC has been observed in Asian and African countries. Pathologists consider histopathology imaging as a gold standard to identify HCC[7]. However, due to the diversity in tumor shape, tissue sizes and staining procedure, manual assessment of such histopathology images is often a challenging and error-prone task[8][9][10].

With the advancements in artificial intelligence (AI), the landscape of medical diagnosis has completely changed, and it has helped over the years the medical community in providing patients with accurate and inexpensive computer-aided diagnostic solutions for the betterment of their health[11]. In the domain of liver cancer detection, the AI-based convolutional neural network (CNN) algorithms have contributed to a great extent in the prediction of cancer types[6][2][12][13][7]replacing traditional techniques such as SVM[14]. In the past decade, several CNN-based architectures have been proposed that either focus on deepening the network such as AlexNet[15]and VGG[16]or to overcome the vanishing gradient issue by adding the residual connections ResNet[17]architectures and DenseNet[18]or applying compound scaling methodology that demonstrates dependency on width, depth and resolution of CNN through EfficientNet architectures[19]. These models have been used as state-of-the-art for a long time and possess special features. Most of the recent studies used such CNN architecture as their backbone models. Unlike the conventional machine learning (ML)-based algorithms, the CNNs do not require handcrafted features for training. The architecture of CNN is responsible for both feature extraction and classification eventually achieving better performance than traditional methods[20][21]and providing an end-to-end solution. This is the primary reason for their popularity in performing medical image classification[22].

Recently, Sun et al.[12]used a ResNet50to perform the liver cancer classification using global image-based labels. The authors proposed patch extraction and feature selection mechanisms. Chen et al.[13]used the InceptionV3 to perform the patch-level liver cancer classification on the TCGA dataset. Atresh et al.[2]proposed a new LiverNet architecture, which consisted of a convolutional block attention module block (CBAM), atrous convolution spatial pyramid pooling blocks, and hypercolumn technique, which reduces model parameters as well as makes the model more robust and precise. The authors claimed the overall liver cancer detection accuracy of 97.72%. In another study, Chen et al.[7]studied the performance of ResNet50, ResNet50_CBAM, SENet, and SKNet on purely, moderately and well-differentiated histopathology images. SENet and ResNet50 were found to be the best-performing models on poorly and well-differentiated types, respectively. Another study[23]proposes prior segmentation for better classification accuracies. Luo et al.[24]by employing a deformable convolution guided attention block and deep adaptive feature fusion, proposed DCA-DAFFNet for laryngeal tumor grading and achieves accuracy of 90.78%. Similarly, few more recent studies use the CNN-based algorithms for histopathology image classification[25][26][20][27].

All these studies used the diverse CNN-based architecture for the liver cancer classification. Most of these studies also use two most important concepts i.e. transfer-learning and fine-tuning. In transfer learning, the model trained on the diversified and huge database with multiple classes is used to train on a small dataset with a limited number of classes[28][29][30][31]. Similarly, in fine-tuning, selective top layers of feature extractor along with the classifier are made trainable[25][32][33][26][34]. Recent study by Talo et al.[30]suggest that if some of the top layers of the feature extractor are kept trainable along with the classifier, then better classification accuracy can be achieved.

Motivated by the concept of transfer learning and fine-tuning, and observing that keeping selective top feature extractor layers along with classifier trainable can boost overall classification performance[25], we developed a hybrid CNN-based architecture for liver cancer classification. We hypothesized that the proposed hybrid deep learning-based architecture may perform better than the individual pre-trained models. The salient contributions are listed as follows:

Fine-tuning powered model is designed to address the issues in cross-domain learning. In the proposed work, the effect of transferring the learnings to the medical domain and tuning the model to a certain depth for new characterization is studied.

Classifier designed with fully connected layers with gradual feature reduction to ensure proper mapping to the output label space. This also helps to develop an optimum end-to-end solution for histopathological image classification.

This study also demonstrates the importance of proper image pre-processing and training methodology for better loss convergence, achieving optimum results and developing the robust model.

The rest of the paper is organized as follows. Section 2 discusses the details of the dataset, preprocessing, a proposed model architecture, performance metrics and training methodology. Section 3 describes the experimental details with results validation along with a detailed comparative study. In Section 4, a detailed discussion on the experimental results obtained, followed by the conclusion in Section 5. The overall workflow for the experimentation is shown in Fig.1

SECTION: IIMethods

SECTION: II-ADatasets and Pre-processing

This study uses a publicly available cancer genome atlas program liver hepatocellular carcinoma (TCGA-LIHC) database of 491 whole slide images[1]to train the proposed hybrid deep learning-based model. The 491 slides were divided into three types of liver HCC tumor namely solid tissue normal as type 0, primary tumor as type 1, and recurrent tumor as type 2. There were 89 solid tissue normal slides, 399 primary tumor slides, and 3 recurrent tumor slides in total. In a deep learning-based approach for classification, directly feeding the whole slide images of very large dimensions to CNN is nearly impractical as it increases computations and model parameters by a large amount. Downsampling or reshaping the histopathology images to lower dimensions leads to severe information loss. Therefore, we follow a patch-based approach[12], where each WSI slide is tiled into patches of size 10241024. While performing the patch extraction, we also intended to maintain sufficient visibility of tissues in the patches. This was ensured by maintaining the mean intensity value of 200 and the standard deviation of 60. An empirical selection process is performed to maintain the clear visualization of tissues in the patches and to reduce the class imbalance in the database.
Finally, 813, 893, and 680 valid patches of type 0, type 1 and type 2, respectively are considered to develop the proposed hybrid deep learning-based model.

The color normalization is another important step in the pre-processing of histology images because the variation of staining intensity of hematoxylin and eosin used for staining purposes in the histology images by the pathologist and may vary according to the practitioner and physical conditions. Thus, the color variations can significantly impact the model training because models are colour-sensitive, In the proposed study, we used the color normalization technique proposed by Macenko et al.[35]which presents an algorithm that finds the correct stain vectors for the image after converting image pixels to optical density space automatically and then performs colour deconvolution.

Deep learning models require a diverse and large dataset to build robust and flexible networks. Augmentation is a technique which generates variants of existing data ultimately increasing the dataset. This makes data augmentation an important step in preprocessing. Over the normalized patches, we applied random vertical and horizontal flips and obtained 1220, 1340, and 1360 patches of type 0, type 1, and type 2 respectively. The complete pre-processing stage is depicted in Fig.2.

This study also uses another cancer database of liver HCC collected from Kasturba Gandhi Medical College, India[2]. The KMC database had four types of liver HCC tumors such as type 0, type 1, type 2, and type 3 having 719, 799, 776, 711 image patches, respectively, each of size 2242243. The received KMC database[2], was originally pre-processed and, therefore, only the augmentation step is performed during the training of the proposed model. Moreover, to verify the proposed hybrid learning strategy over the different types of cancer disease, another publicly available colon cancer database of histopathology images is collected from Kaggle[36]. The colon database consists of two classes, adenocarcinoma and normal, with a total of 5000 patches per type of size 7687683. Samples of all three datasets are shown in Fig.3

SECTION: II-BProposed model architecture

Using transfer learning, we can apply models that have been trained on large diverse datasets (i.e., “pre-trained models”) to smaller, related datasets in a more efficient manner[25,32,33,30,29,12]. In general, the pre-trained models have two parts: a feature extractor and a classifier. In transfer learning, the classifier is modified while the feature extractor is left untouched. This is because the pre-trained models are usually trained on the Imagenet dataset with 1000 output classes, while in general, the number of output classes is limited. Therefore, only the last classification layer of the pre-trained model is re-trained and named as “base model".

In general, the top layers of any pre-trained model’s feature extractor capture the fine details that pertain solely to output classes, and therefore, such top layers are re-trained when fine-tuning the model on small custom datasets to optimize the performance of the model[25,32],[33]. In contrast to the top layers, the bottom layers capture the more generic features such as lines, edges, and shapes that contribute more to the classification task. Besides the feature extractor and the classifier, another important attribute of the pre-trained model is the depth of the model. There has been evidence that deeper models can improve classification performance, although this is not always the case[17,18,19]. Considering all these aspects, we propose an architecture in which (i) the feature extractor remains unchanged except for a few top layers and (ii) a few more fully connected layers are added before the final classification layer. This modified model is referred to as the “hybrid model”, which combines both the concepts of transfer learning and fine-tuning. The number of additional fully connected layers is decided empirically. The performance of eight different types of pre-trained models such as ResNet50[17], VGG16[16], EfficientNetb0[19], EfficientNetb1[19], EfficientNetb2[19], EfficientNetb3[19], EfficientNetb4[19], DenseNet121[18]is compared while designing the hybrid model. Fig.4shows the architecture of base and hybrid models.

SECTION: II-CTraining

The overall architecture of the proposed model is comprised of two stages: (i) offline system and (ii) online system as shown in Fig.5. PyTorch framework was used for designing the model. Initially, the pre-processed image patches are divided into training (90%) and testing datasets (10%) as shown in TableI. The image patches are resized to 2242243 pixel dimensions to maintain uniformity across datasets and allow faster and more precise training. The offline system uses the training dataset and performs the 5-fold stratified cross-validation to train the proposed model. In 5-fold cross-validation, the training data is divided into five equal non-overlapping parts. The model is trained on the four parts and validated on the remaining part of the training dataset. Dynamic augmentation is performed on these four training parts of the data with random rotation on the TCGA-LIHC dataset, random rotation with random horizontal flip on the COLON dataset, and random rotation with random horizontal and vertical flip on the KMC dataset. Furthermore, to minimize the class imbalance, a weighted random sampler approach[37]is followed that samples images of different classes with pre-defined weights. The weights are inversely utilized based on the number of classes in the training dataset. Both the dynamic augmentation and weighted random sampler methods aid in reducing the class-imbalance effect in the dataset. This complete process is repeated five times in 5-fold cross-validation and an averaged validation performance is recorded. The hyperparameters used while training the model are shown in TableII. Moreover, The cosine annealing warm restart[38]learning rate scheduler is used to select the learning rate over all the epochs. This is mathematically represented in Eq.[1], where,andare learning rate ranges.indicates the number of epochs from the last restart, andindicates the number of epochs after which a restart is scheduled. The model is trained for 47 epochs with early stopping criteria and a warm restart of the learning rate scheduler at everyepoch(). In this study, an initial learning rate of 0.001() is used.

Furthermore, the cross entropy lossbetween labels and ground truth values is used, which is defined in Eq.[2]

whereis the number of classes, andandare the ground truth values and the predicted probabilistic values, respectively. This loss function is widely preferred over the others. The reason for this is that, if we see the curve of loss vs. probability, loss is very high when prediction is inaccurate. As a result, the gradient of loss is high, which aids in faster convergence. Once the model is trained, the training weights are used to transform the test dataset into the output cancer types using the test model. The performance of the model is evaluated on the test data using five performance measures such as accuracy, sensitivity, specificity, F1-score, and area under the curve (AUC). The pre-trained models were imported from the Pytorch library and trained on Google Colab.

DatasetType0Type1Type2Type3TotalTCGA-LIHCTrain109812061224NA3528Test122134136NA392Total122013401360NA3920KMCTrain6497196966612725Test70808050280Total7197997767113005COLONTrain45004500NANA9000Test500500NANA1000Total50005000NANA10000

SECTION: II-DPerformance Metrics

The quantitative performance of the proposed hybrid model was evaluated using a standard protocol such as accuracy, sensitivity, specificity, and F1-score[39,6]. These performance evaluation metrics were computed using a contingency table that holds positive (TP), false positive (FP), true negative (TN), and false negative (FN). Finally, receiver operating characteristics (ROC) and area under the curve (AUC)[40]were determined.
li2019staged

SECTION: IIIResults

In this study, the proposed hybrid model is independently trained and evaluated on the TCGA-LIHC, KMC and COLON datasets as per the strategy discussed in the “Training” section. The training and validation accuracy along with the training and validation loss are plotted for all the datasets. The five performance metrics namely accuracy, sensitivity, specificity, F1-score and AUC for each model were averaged over five-folds and presented in this section along with the ROC curves and confusion matrix attributed to respective folds.

SECTION: III-AResults on TCGA-LIHC dataset

To investigate the training and validation performance of the proposed hybrid model, the training, validation accuracy and loss are shown in Fig.6and Fig.7, respectively. It has been observed that the difference between training and validation accuracy reduces with the progression of epochs indicating proper training in the absence of both bias and variance. Furthermore, the consistent decrease in the training and validation loss indicates that the model is learning at every epoch and once the model is trained, both the curves remain unchanged. Fig.8and Fig.9show the confusion matrices and the ROC curves for all the five folds. Both these figures show accurate prediction of patches with and without liver cancer. TableIIIand TableIVshow the performance metrics using the base model and the proposed hybrid model, respectively. It has been observed that all the models with the proposed hybrid strategy have resulted in better prediction of cancer grading compared to the base model. The hybrid ResNet50 model provided an increment of 1.74% (100% vs. 98.26%) over the base ResNet50 model. Similarly, the other performance metrics such as sensitivity, specificity, and F1-score also provided the prediction performance of 100% with an AUC of unity. The results indicate that the hybrid learning strategy can avoid false predictions justifying the robustness of the model.

MetricsResNet50VGG16EfficientNetb0EfficientNetb1EfficientNetb2EfficientNetb3EfficientNetb4DenseNet121Accuracy98.260.1198.510.4898.920.4997.190.5998.410.3794.480.4995.350.7597.490.21F1-scoreType099.590.0099.910.1899.590.0099.590.40100.000.0099.090.1899.420.37100.000.00Type197.510.1697.860.6998.440.6995.840.8997.660.5792.330.8893.690.9496.430.29Type297.820.1797.910.6298.810.7196.360.7497.740.5392.500.5793.360.9896.300.32Weighted avg98.270.1198.510.4898.930.4897.190.5998.410.3794.490.4995.360.7497.490.21SpecificityType0100.000.00100.000.00100.000.0099.770.20100.000.0099.700.16100.000.00100.000.00Type197.510.2098.140.6398.750.7498.450.4799.220.0095.340.6194.491.3596.740.34Type299.840.2199.610.2799.610.0097.500.7698.360.5796.560.6498.440.5599.450.21Macro avg99.120.0599.250.2499.450.2498.570.3099.190.1997.200.2597.640.3898.730.10SensitivityType099.180.0099.830.3699.180.0099.670.44100.000.0098.850.4498.850.73100.000.00Type199.700.4199.250.5299.250.0094.771.3996.861.1093.431.2297.460.8598.950.40Type296.020.3996.610.9898.381.4197.350.6598.530.0091.620.6590.142.1193.820.65Macro avg98.300.1198.560.4898.930.4797.260.5898.460.3694.630.4995.480.7497.590.20AUCType01.000.001.000.001.000.000.990.001.000.000.990.001.000.001.000.00Type10.990.000.990.000.990.000.990.000.990.000.980.000.990.000.990.00Type20.990.000.990.000.990.000.990.000.990.000.990.000.990.000.990.00Macro avg0.990.000.990.000.990.000.990.000.990.000.990.000.990.000.990.00*All metrics except AUC are expressed in percentages.

MetricsResNet50VGG16EfficientNetb0EfficientNetb1EfficientNetb2EfficientNetb3EfficientNetb4DenseNet121Accuracy100.000.0099.430.4999.890.1499.940.1199.890.2299.740.1899.740.1899.590.53f1-scoreclass 0100.000.0099.830.37100.000.00100.000.00100.000.0099.910.18100.000.00100.000.00class 1100.000.0099.180.7199.850.2099.920.1699.850.3399.630.2699.620.2699.400.77class 2100.000.0099.330.4899.850.2099.920.1699.850.3399.700.3099.630.2699.400.77macro avg100.000.0099.450.4899.900.1399.950.1199.900.2199.750.1799.750.1799.600.51weighted avg100.000.0099.430.4899.890.1499.940.1199.890.2299.740.1899.740.1899.590.53Specificityclass 0100.000.00100.000.00100.000.00100.000.00100.000.0099.920.16100.000.00100.000.00class 1100.000.0099.450.6499.920.17100.000.0099.920.1799.680.3299.840.2199.530.69class 2100.000.0099.680.1799.920.1799.920.1799.920.17100.000.0099.760.2199.840.21macro avg100.000.0099.710.2499.940.0799.970.0599.940.1199.870.0999.870.0999.790.27Sensitivityclass 0100.000.0099.670.73100.000.00100.000.00100.000.00100.000.00100.000.00100.000.00class 1100.000.0099.400.3399.850.3399.850.3399.850.3399.850.3399.550.4199.700.41class 2100.000.0099.260.7399.850.33100.000.0099.850.3399.410.6199.700.4099.111.31macro avg100.000.0099.440.4999.900.1399.950.1199.900.2299.750.1799.750.1799.600.51AUCclass 01.000.001.000.001.000.001.000.001.000.001.000.001.000.001.000.00class 11.000.000.990.001.000.001.000.001.000.001.000.000.990.000.990.00class 21.000.000.990.001.000.001.000.001.000.001.000.000.990.000.990.00macro avg1.000.000.990.001.000.001.000.001.000.000.990.000.990.000.990.00*All metrics except AUC are expressed in percentages.

SECTION: III-BResults on KMC dataset

The training and validation accuracy and loss are shown in Fig.10and Fig.11, respectively. Here as well proper training in the absence of both bias and variance can be observed. Furthermore, the consistent decrease in the training and validation loss indicates that the model is learning at every epoch and once the model is trained, both the curves saturate. Fig.12and Fig.13show the confusion matrices and the ROC curves for all five folds. TableVand TableVIshow the performance metrics using the base model and the proposed hybrid model, respectively over the KMC dataset. The results for KMC dataset also show that the proposed hybrid strategy has resulted in better prediction of cancer grading compared to the base model. The hybrid EfficientNetb3 achieves 96.71% accuracy with a 2.22% standard deviation. This is an increment of 4.65% over the base EfficientNetb3 model.

MetricsResNet50VGG16EfficientNetb0EfficientNetb1EfficientNetb2EfficientNetb3EfficientNetb4DenseNet121Accuracy88.280.6387.350.5991.490.7392.570.7788.281.3292.070.8190.420.6889.780.85F1-scoreType0100.000.00100.000.0099.290.0099.850.3199.140.3299.000.3898.851.08100.000.00Type181.700.8383.751.4986.781.1888.510.8881.182.0688.230.6487.071.1384.521.62Type284.871.3678.651.1286.551.0587.181.4479.781.8286.911.2586.091.0487.191.16Type388.890.0089.370.6696.260.9697.530.9598.160.8696.891.3191.300.0088.640.55Weighted avg88.460.6287.360.5991.530.7292.570.7788.301.2792.090.8190.490.6889.890.82SpecificityType0100.000.00100.000.0099.520.0099.900.2199.520.0099.420.2199.610.39100.000.00Type187.000.8687.502.2993.700.2791.500.5091.300.5794.900.8294.100.4188.901.14Type296.600.2294.802.0194.901.0898.200.7592.802.0794.600.5492.900.8296.800.44Type3100.000.00100.000.00100.000.00100.000.00100.000.00100.000.00100.000.00100.000.00Macro avg95.900.2295.570.2097.030.2597.400.2795.900.4697.230.2896.650.2396.420.30SensitivityType0100.000.00100.000.00100.000.00100.000.0099.710.6399.710.6398.851.19100.000.00Type191.500.5594.502.5988.752.3396.250.8883.254.1089.001.0488.502.4093.501.62Type280.002.1673.253.3786.001.0480.751.1178.251.4287.252.0589.001.0483.501.85Type380.000.0080.801.0992.801.7895.201.7896.401.6794.002.4484.000.0079.600.89Macro avg87.870.5587.130.5291.880.7993.050.8289.401.3092.490.9290.080.6189.150.82AUCType01.000.001.000.001.000.001.000.000.990.001.000.000.990.001.000.00Type10.960.000.960.000.980.000.980.000.960.000.980.000.980.000.970.00Type20.970.000.950.000.980.000.970.000.950.000.970.000.970.000.980.00Type30.980.000.990.000.990.001.000.000.990.000.990.000.990.000.990.00Macro avg0.980.000.980.000.990.000.990.000.980.000.990.000.990.000.980.00*All metrics except AUC are expressed in percentages.

MetricsResNet50VGG16EfficientNetb0EfficientNetb1EfficientNetb2EfficientNetb3EfficientNetb4DenseNet121Accuracy95.072.1390.641.2496.211.8196.142.2495.492.2296.710.6896.492.4194.921.14F1-scoreType098.792.7099.290.7099.580.93100.000.00100.000.00100.000.00100.000.0099.580.94Type193.011.6988.742.4194.481.8394.763.1293.402.7594.911.4197.701.6694.262.12Type293.352.4385.953.8492.903.5194.022.9993.653.9894.330.7694.253.9392.403.02Type395.805.7688.751.8399.381.3896.383.5195.425.0998.781.3292.965.0693.244.39Weighted avg95.052.1790.581.2496.171.8596.152.2295.482.2396.710.6696.442.4694.881.16SpecificityType099.141.9199.520.4799.710.63100.000.00100.000.00100.000.00100.000.0099.710.64Type195.002.0091.601.2995.301.6495.502.8594.402.6795.701.2598.101.3895.101.92Type299.001.1796.501.3699.700.6799.101.2499.300.5799.900.2297.002.1598.102.07Type3100.000.0099.390.72100.000.00100.000.00100.000.0099.820.38100.000.00100.000.00Macro avg98.280.7296.750.4398.670.6298.650.7898.420.7798.850.2498.770.8498.220.40SensitivityType0100.000.00100.000.00100.000.00100.000.00100.000.00100.000.00100.000.00100.000.00Type197.752.5696.502.40100.000.00100.000.0099.750.55100.000.00100.000.00100.000.00Type289.753.8982.004.4787.505.0790.753.8189.755.9589.501.1195.752.8790.005.45Type392.4010.4382.002.0098.802.6893.206.5791.609.2098.402.6087.209.0187.607.92Macro avg94.972.7390.120.9196.571.7995.982.5895.272.6196.970.8395.732.9594.401.40AUCType01.000.001.000.001.000.001.000.001.000.001.000.001.000.001.000.00Type10.980.010.980.000.990.000.990.000.990.000.990.000.990.000.990.00Type20.980.020.970.000.990.000.990.000.990.000.990.000.990.000.990.00Type30.990.000.980.011.000.001.000.000.990.000.990.000.990.000.990.00Macro avg0.990.000.980.000.990.000.990.000.990.000.990.000.990.000.990.00*All metrics except AUC are expressed in percentages.

SECTION: III-CResults on COLON dataset

To assess the performance of the proposed hybrid model, colon dataset is also considered. Fig.14and Fig.15show the confusion matrices and the ROC curves for all five folds. TableVIIand TableVIIIshow the performance metrics using the base model and the proposed hybrid model, respectively over the COLON dataset. Hybrid models are found to perform better for this database as well. The hybrid EfficientNetb2, EfficientNetb4 and DenseNet121 achieve 100% accuracy. This is an increment of 0.44, 0.36, and 0.32% over the base models respectively.

MetricsResNet50VGG16EfficientNetb0EfficientNetb1EfficientNetb2EfficientNetb3EfficientNetb4DenseNet121Accuracy99.540.1399.940.0599.900.0799.700.0799.560.1899.840.0599.740.0999.680.18F1-scoreType099.540.1399.940.0599.900.0799.700.0799.560.1899.840.0599.740.0999.680.18Type199.540.1399.940.0599.900.0799.700.0799.560.1899.840.0599.740.0999.680.18Weighted avg99.540.1399.940.0599.900.0799.700.0799.560.1899.840.0599.740.0999.680.18SpecificityType0100.000.00100.000.00100.000.0099.800.0099.800.00100.000.00100.000.00100.000.00Type199.080.2799.880.1199.800.1499.600.1499.320.3699.680.1199.480.1899.360.36Macro avg99.540.1399.940.0599.900.0799.700.0799.560.1899.840.0599.740.0999.680.18SensitivityType099.080.2799.880.1199.800.1499.600.1499.320.3699.680.1199.480.1899.360.36Type1100.000.00100.000.00100.000.0099.800.0099.800.00100.000.00100.000.00100.000.00Macro avg99.540.1399.940.0599.900.0799.700.0799.560.1899.840.0599.740.0999.680.18AUCType01.000.001.000.001.000.001.000.001.000.001.000.001.000.001.000.00Type11.000.001.000.001.000.001.000.001.000.001.000.001.000.001.000.00Macro avg1.000.001.000.001.000.001.000.001.000.001.000.001.000.001.000.00*All metrics except AUC are expressed in percentages.

MetricsResNet50VGG16EfficientNetb0EfficientNetb1EfficientNetb2EfficientNetb3EfficientNetb4DenseNet121Accuracy99.960.0599.960.0599.980.0499.980.04100.000.0099.900.17100.000.00100.000.00F1-scoreType099.960.0599.960.0599.980.0499.980.04100.000.0099.900.17100.000.00100.000.00Type199.960.0599.960.0599.980.0499.980.04100.000.0099.900.17100.000.00100.000.00Weighted avg99.960.0599.960.0599.980.0499.980.04100.000.0099.900.17100.000.00100.000.00SpecificityType0100.000.00100.000.00100.000.00100.000.00100.000.00100.000.00100.000.00100.000.00Type199.920.1199.920.1199.960.0899.960.09100.000.0099.800.35100.000.00100.000.00Macro avg99.960.0599.960.0599.980.0499.980.04100.000.0099.900.17100.000.00100.000.00SensitivityType099.920.1199.920.1199.960.0899.960.09100.000.0099.800.35100.000.00100.000.00Type1100.000.00100.000.00100.000.00100.000.00100.000.00100.000.00100.000.00100.000.00Macro avg99.960.0599.960.0599.980.0499.980.04100.000.0099.900.17100.000.00100.000.00AUCType01.000.001.000.001.000.001.000.001.000.001.000.001.000.001.000.00Type11.000.001.000.001.000.001.000.001.000.001.000.001.000.001.000.00Macro avg1.000.001.000.001.000.001.000.001.000.001.000.001.000.001.000.00*All metrics except AUC are expressed in percentages.

SECTION: IVDiscussion

The goal of this study is to develop a methodology for classifying different types of liver HCC tumors. We designed a deep learning-based hybrid architecture, trained and tested over two liver HCC datasets. The different data pre-processing steps along with training and validation techniques are also incorporated to build a robust model. After training the models, performance metrics over the test dataset were generated and observed. In this section, conclusions made from the obtained results, a comparison of the proposed methodology with recent studies and limitations of the proposed methodology are mentioned.

We observe that hybrid models perform better than base models for all datasets mentioned above. Hybrid models show a minimum 0.76% (VGG16) and 3.29% (VGG16) of hike in accuracy on TCGA and KMC datasets respectively. The hybrid model with ResNet50 as a feature extractor outperforms the others on TCGA, achieving 100% accuracy with a 1.74% increase after modification (TablesIII,IV) for TCGA dataset. Performance metrics indicate the excellent ability of classification and the robustness of the model.
The hybrid model with EfficientNetb3 outperforms all other models on KMC in terms of every performance metric (TablesV,VI). With an accuracy of 96.71% and a 2.22% standard deviation, this model sets the benchmark for KMC database.
A similar trend is observed for the COLON dataset as well. The hybrid models with EfficientNetb2, EfficientNetb4 and DenseNet121 feature extractors show remarkable performance by achieving 100% accuracy on the test dataset. These results prove that replacing a shallow classifier with a deep classifier and fine-tuning the top layers of the feature extractor is a fruitful strategy.

For the TCGA dataset, the base model with EfficientNetb0 as a feature extractor gives better accuracy, whereas, among the hybrid models, ResNet50 is proven to be the best. On the other hand, for the KMC dataset, EfficientNetb3 outperforms other architectures. Various architectures are observed to produce the best performance across the datasets. This concludes the data specificity of deep architectures and not model specificity. Various architectures are designed in a way to identify or detect certain types of features that are associated with particular datasets, even when the tissues are those of the liver, but share different genetic patterns and different ways of WSI preparation procedures.

Some recent studies on liver HCC detection involving the TCGA and KMC datasets are mentioned in TableIX. As can be seen, the proposed methodology outperforms other studies on local image classification tasks in terms of accuracy as well as other performance metrics. LiverNet architecture[2]claimed the best performance over KMC and TCGA datasets in the patch-level classification task. The LiverNet model has some key features such as CBAM, ASPP block and hypercolumn technique, which take care of a lesser number of model parameters and show improved performance. Our proposed methodology exceeds LiverNet’s accuracy by more than 2% over the TCGA dataset and by more than 5% on the KMC dataset, despite having less number of patches in the KMC dataset. LiverNet achieves F1-score of 97.72% and 90.93%. On the other hand, our methodology achieves 100% and 96.446% F1-score on TCGA and KMC respectively. They had also produced results using a very similar architecture called BreastNet[20]which was specifically designed for breast cancer classification over breakhis dataset[41]. Our proposed model outperforms BreastNet as well.

Chen et al.[13]used pre-trained InceptionV3 which achieves low accuracy (89.6%) despite having a large number of parameters. They trained the model using a massive dataset containing approximatelytrain patches. Sun et al.[12]used pre-trained ResNet50 for patch-level feature extraction. They proposed a global-level (whole slide image level) classification mechanism based on the selection of-top and-bottom features from a sorted aggregation of patch-level features. On the other hand, our research focuses on patch-level classification. For aggregation, Sun et al. performed p-norm pooling, and for the classification offeatures, they trained a multi-layered perceptron. Thevalue was chosen experimentally. For only two-class classification at the global level, this study claimed 100% accuracy.

First Author(year)DatasetTypesModelMetricsTraining methodRemarkSun et al.TCGA2PretrainedAccuracy 10010 foldTransfer learning,(2019)[12]189,531 train,ResNet50Precision 100CVbk feature selectionpatchesRecall 100from sorted aggregated patch224x224F1-score 100features, global leveltested over 70WSIclassificationChen et al.TCGA3PretrainedAccuracy 89.60.85:0.15 trainTransfer learning,(2020)[13]39k train,Inceptionv3Precision 87.9test splitEasyDL frameworkpatches, 9k testRecall 77.1256 × 256F1-score 82.0very big train setMCCa0.912Huge number of paramentersAatresh et al.TCGA3LiverNetAccuracy 97.725 foldArchitecture specific(2021)[2]2240 trainPrecision 97.72CVfor HCC liver cancer classification.140 testRecall 97.72CBAM, ASPP block224x224F1-score 97.72hypercolumn techniqueIOUc95.61less number of parametersAatresh et al.TCGA3BreastNetAccuracy 96.045 foldArchitecture specific(2021)[2]2240 trainPrecision 96.04CVfor Breast cancer classification.140 testRecall 96.04CBAM block hypercolumn224x224F1-score 96.04techniqueIOU 92.47less number of parametersAatresh et al.KMC2LiverNetAccuracy 90.935 foldArchitecture specific(2021)[2]3210 trainPrecision 90.93CVfor HCC Liver cancer classification.272 testRecall 90.93CBAM, ASPP block224x224F1-score 90.93hypercolumn techniqueIOU 83.6Aatresh et al.KMC2BreastNetAccuracy 88.415 foldArchitecture specific(2021)[2]3210 trainPrecision 88.41CVfor Breast cancer classification.272 testRecall 88.41CBAM block hypercolumn224x224F1-score 88.41techniqueIOU 79.73ProposedTCGA3PretrainedAccuracy 100.005 foldTransfer learning, fineHybrid3528 trainHybridSensitivity 100.00CVtuning, deep classifierResNet50392 testResNet50Specificity 100.00training of selective(2022)224x224F1-score 100.00top layer of base modelAUC 1.00ProposedKMC2PretrainedAccuracy 96.715 foldTransfer learning, finehybrid2725 trainHybridSensitivity 96.97CVtuning, deep classifierEfficientNetb3280 testEfficientNetb3Specificity 98.85training of selective(2022)224x224F1-score 96.71top layer of base modelAUC 0.99aMCC stands for Matthews’s correlation coefficient.bCV stands for Cross Validation.bIOU stands for intersection over union

For 3-class patch-level classification on TCGA and 2-class patch-level classification on KMC datasets, the proposed hybrid methodology outperforms all the above-mentioned studies. Furthermore, the proposed model achieves better performance despite being trained on a comparatively smaller dataset and provides end-to-end deep learning-based solutions. The performance metrics also indicate the robustness of proposed models over others. We have also demonstrated the performance of our methodology over colon histopathology images. Here as well hybrid models have achieved 100% accuracy.

There are some limitations to this study. ResNet50, EfficientNetb3, and DenseNet121-based architectures have around 25M, 12.5M and 7M parameters, respectively. These models have a large number of parameters, which makes them hard to run on low-end processor specifications. We will try to overcome this limitation in future work and provide a highly accurate and lightweight solution.

SECTION: VConclusion

This study was focused on liver HCC classification using histopathology images. The primary dataset TCGA-LIHC was prepared through pre-processing steps involving patch extraction, color normalization and augmentation techniques. Another proprietary KMC dataset and a publically available COLON dataset were used for validation purposes.
Through this study, we proposed a deep learning-based hybrid architecture for the classification of liver HCC efficiently and robustly. From the experimental results, it is also observed that the proposed hybrid model outperforms the base model and other state-of-the-art models by a significant margin. The best results on the TCGA dataset were obtained by a hybrid model with ResNet50 as the feature extractor, giving an accuracy of 1000.00 whereas on the KMC dataset, the proposed hybrid model with EfficientNetb3 as a feature extractor achieves an accuracy of 96.710.68%. Moreover, the proposed hybrid models provide outstanding results on the COLON dataset as well. The results lead us to the conclusion that deepening the classifier and customizing the top layers of the feature extractor can help improve performance significantly.

Despite having limitations of model parameters, since many labs are now equipped with high-end systems and because perfect predictions trump computational complexity in the medical domain due to the life-and-death situation, we believe a large number of parameters should not be a major issue.

SECTION: References