SECTION: Table2Image: Interpretable Tabular data Classification with Realistic Image Transformations
Recent advancements in deep learning for tabular data have demonstrated promising performance, yet interpretable models remain limited, with many relying on complex and large-scale architectures. This paper introduces Table2Image, an interpretable framework that transforms tabular data into realistic image representations for classification, achieving competitive performance with relatively lightweight models. Additionally, we propose variance inflation factor (VIF) initialization, which reflects the statistical properties of the data, and a novel interpretability framework that integrates insights from both the original tabular data and its image transformations. By leveraging Shapley additive explanations (SHAP) with methods to minimize distributional discrepancies, our approach combines tabular and image-based representations. Experiments on benchmark datasets showcase competitive classification accuracy, area under the curve (AUC), and improved interpretability, offering a scalable and reliable solution. Our code is available at.

SECTION: Introduction
Tabular data, structured in rows and columns, is one of the most ubiquitous forms of datasets in the world. However, despite significant advancements in deep learning for unstructured data such as images, text, and audio, deep learning for tabular data remains relatively under-explored. When tackling classification problems with tabular data, gradient-boosted decision trees (GBDT) like XGBoostand LightGBMare often the go-to solutions. These models are favored due to their lightweight nature, strong performance, and built-in interpretability mechanisms. Nonetheless, as multimodal artificial intelligence (AI) and deep learning continue to advance, there is a growing need to explore more cutting-edge approaches for handling tabular data using deep learning. These efforts not only enhance performance through a unified gradient optimization but also enable seamless integration with other data modalities.

To address the challenges in deep learning for tabular data, numerous promising studies have been proposed. However, several limitations persist. Some existing models require a large number of trainable parameters, emphasizing the need for lightweight architectures. Furthermore, while transformer-based approaches are impactful, exploring alternative diverse frameworks is crucial. Additionally, the lack of interpretability in most frameworks may limit their adoption in domains requiring transparency and explainability.

In this context, firstly, our study addresses a fundamental yet often overlooked question: Why does deep learning excel in modalities like images, text, and audio but struggle with tabular data? We posit that the answer lies in the inherent characteristics of tabular data. Unlike images and text, which humans can intuitively understand, tabular data demands greater effort to interpret. For instance, we can instantly recognize a cat in a photo. However, when classifying cats and dogs using tabular data, one must analyze various features within the table to deduce whether the instance represents a cat or a dog. From this perspective, tabular data can be seen as information-dense and compressed data, which is akin to a latent variable when compared to unstructured data such as images or text. This compressed nature limits the capacity of standard deep learning models to fully analyze tabular data. Expanding on this viewpoint, we introduce a framework that converts tabular data into images and processes these uncompressed data using deep learning methods.

Secondly, we propose a novel variance inflation factor (VIF) initialization for tabular data to reduce multicollinearity. In traditional machine learning, feature engineering plays a critical role in improving model performance, often constructing derived variables or involving statistical preprocessing. Despite its significance, feature engineering has been overlooked in deep learning for tabular data. To address this gap, our VIF initialization enables models to better capture the relationships within features of tabular data and enhance their performance.

Finally, we propose an interpretability framework that incorporates both the original tabular data and its transformed image representations as latent variables. GBDT models, such as XGBoost and LightGBM, are valued for their built-in interpretability, which supports transparent decision-making in critical domains like finance and healthcare. Our framework, while leveraging deep learning, is designed to preserve and extend interpretability. This dual representation enables a richer, more sophisticated understanding, facilitating more reliable and responsible AI.

In summary, our contributions are as follows:

Converting tabular data into image representations, enabling the use of image-based deep learning techniques for classification.

: Proposing a novel initialization method for tabular data, inspired by feature engineering to improve model performance.

: Combining insights from original tabular data and transformed images to deliver richer and more comprehensive explanations.

Additionally, the implementation of our frameworks is publicly available at.

SECTION: Background
SECTION: Transforming Tabular data into Images
There have been several prior research on transforming tabular data into images. However, existing methods possess limitations.

DeepInsightand IGTDconvert tabular data into images to classify tabular data with a convolutional neural network (CNN). While straightforward, these approaches produce unrealistic image representations of tabular data and demonstrate performance limitations.

This methodproposed a framework for generating images from tabular datasets, primarily aimed at industrial visualization tasks such as exploratory data analysis and customer segmentation. It emphasized usability but did not evaluate their models with classification tasks.

HACNetuses an attention-based module to embed tabular data into alphabetic images and trains a ResNet-18 model for classification, enabling end-to-end learning. However, its reliance on a single image per class limits representation diversity, which could be seen as directly providing a straightforward target. Additionally, simply mapping tabular data to alphabetic images is insufficient to qualify as an interpretable deep learning model.

To overcome these limitations, we propose Table2Image. First, we generate realistic image representations using the FashionMNISTand MNISTdatasets. By mapping multiple images per class, our model improves performance on a benchmark dataset by capturing richer and more diverse representations. Secondly, we provide a feature importance mechanism for tabular data and present stability results. Lastly, our lightweight two-layer convolutional neural network (CNN) architecture enhances efficiency, compared to ResNet-18.

SECTION: Multicollinearity and Variance Inflation Factor (VIF)
Multicollinearity, particularly imperfect multicollinearity, occurs when two or more independent variables in a dataset are linearly related. There is no scientific consensus in removing collinear variables. To address the collinearity, the variance inflation factor (VIF) can be used to identify the collinearity of the predictor variables. VIF measures how much the variance of a parameter estimate increases due to multicollinearity, comparing the variance in a full model with other parameters to that in a model with only the parameter itself. A high VIF suggests a strong correlation between predictors, with values exceeding 10 generally considered problematic.

The VIF is defined as in Equation, whererepresents the coefficient of determination obtained by regressingon all other predictors.

Unlike machine learning, deep learning may address the effects of multicollinearity through dropoutor other regularization techniques. However, since we are working with deep learning for tabular data, we introduce a tabular data-specific VIF initialization to mitigate the impact of multicollinearity and enhance model robustness.

SECTION: Interpretability Framework
We calculate feature importance by leveraging both tabular and image data. Shapley additive explanations (SHAP)is employed as the baseline feature importance mechanism. Maximum mean discrepancy (MMD)is utilized for distribution comparison.

SHAP decomposes the model predictioninto contributions from each feature, based on Shapley values from cooperative game theory. The decomposition can be expressed as Equation,

whererepresents the base value andis the Shapley value of the-th feature. The Shapley valueis defined as Equation.

Here,is a subset of features,denotes the total number of features, andis the model prediction based solely on the subset. Specifically, as our models are based on deep learning architectures, we use Deep SHAP for our interpretability mechanism. We create an interpretability framework using SHAP for both tabular and image data.

Maximum mean discrepancy (MMD) quantifies the difference between two probability distributionsandby comparing their mean embeddings in a reproducing kernel Hilbert space (RKHS). It is defined as Equation.

whereandis the RKHS associated with the kernel.ensures thatlies within the unit ball of. In practice, to compute the MMD efficiently, we utilize the squared term of MMD as Equation.

We letand. The kernel mean embeddings ofandare denoted asand, respectively. Since the kernel implicitly computes inner products in the RKHS, Equationallows us to calculate MMD without explicitly mapping the data to a higher dimensional space.

SECTION: Methodology
SECTION: Table2Image
For tabular datasets with 10 or fewer classes, we map each instance of classto randomly selected FashionMNIST images of class. For datasets with more than 10 classes, we extend this approach by utilizing both the FashionMNIST and MNIST datasets. Cases with more than 20 classes can be implemented by adding benchmark image datasets, while left for future work.

The overall framework is depicted in Figure.

We encode tabular data using a multilayer perceptron (MLP) with two fully connected layers and rectified linear units (ReLU) activations. The intermediate layer has a dimensionality that is four larger than the original feature size, while the final output matches the original feature size.

The core of our model is an autoencoder designed to generate image data mapped to the corresponding tabular data, using flattened random noise as part of its input. Both the encoder and decoder are composed of MLPs. The encoder combines the noise and tabular data embedding to create latent representations with dimensionality increased by 4. The decoder then reconstructs the latent representation into the target image, using the tabular data embedding as an additional input.

The reconstructed images are classified using a CNN with two convolutional layers, ReLU activations, max pooling for dimensionality reduction, and fully connected layers for predictions.

We perform end-to-end learning by optimizing the model using a combination of reconstruction and classification loss. Reconstruction loss ensures the generated image closely aligns with the image mapped from the corresponding tabular data instance, utilizing mean squared error (MSE) to minimize pixel-wise differences. Classification loss focuses on minimizing prediction errors for the image-based classification task using CNN. Additionally, we use the AdamW optimizerfor optimization.

SECTION: VIF Initialization
To address multicollinearity and offer a representation that highlights important features, we propose VIF initialization. It is divided into global and local initializations, capturing different levels of granularity, as depicted in Figure. The outputs of the VIF initialization are concatenated with the embedding from the MLP-based tabular data embedding.

We utilize a two-layer fully connected network to process input tabular data. The first layer expands the dimension by 4, while the second layer reduces it back to the original tabular data size. The initial weights of the first layer are initialized as the reciprocal of the VIF for each column, mitigating the influence of highly collinear variables. These weights are updated during the training process.

We use a two-layer fully connected framework for input tabular data withfeature dimensions. The first layer expands the dimension to, the number of all possible feature pairs, and the second layer reduces it back to. The weights of the first layer are initialized as the reciprocal of the pair-wise VIF between feature pairs. Weights corresponding to non-paired features are randomly assigned. This approach utilizes the relative contribution of other features to a specific column.

By combining global and local initializations, we seek to minimize the impact of multicollinearity while preserving essential feature relationships, creating a balanced and enhanced tabular representation.

SECTION: Interpretability Framework
We aim to derive feature importance by integrating the information from tabular and image data. For tabular data, image data, and feature importance, we design an unsupervised optimization process that minimizes the difference between the two conditional distributionsand.represents feature importance based onand, andhas a similar interpretation. It is noteworthy that all the terms are not rigorously probabilities nor probability distributions, but outputs of each model.

From Bayes’ rule, Equationholds.

It is important to note that,, andare not independent variables. We utilize the fact that all terms in Equationsare equivalent.

The terms,,, andare already known.

: Feature importance derived from tabular data using SHAP.

: Feature importance derived from image data using SHAP.

: Transformation of tabular data into images via the Table2Image framework.

: The reverse process of Table2Image, reconstructingfrom.

However, we should speculateandto determine.

We first assume thatandfollow normal distributions. As starting points for, we utilizeand. The known distributionserves as a substitute for the unknownbecause they both share the joint distribution. This relationship arises from Equation.

These distributions are input into an MLP consisting of two linear layers and ReLU to estimate the mean and standard deviation of. Similarly, for, we replace the unknownwith. Then we apply the same process withand.

To minimize the discrepancy between the distributionsand, we utilize maximum mean discrepancy (MMD) as described in Equation, Kullback-Leibler divergence (KLD) in Equation, and mean squared error (MSE) in Equation. Moreover, since Equationholds as follows,

both terms share the joint distribution, implying that they are assumed to encompass significant shared information. To maximize the amount of information mutually shared, we employ a variant of the InfoNCE Loss, which approximates the mutual information as defined in Equation. The entire process is illustrated in Figure. Here,andrepresentand, respectively.

Overall, our approach enables a more granular analysis of feature importance across both tabular and image data. Moving beyond traditional methodologies that incorporate only tabular inputs and modeling outputs, our framework integrates latent variables represented by images, providing richer and more comprehensive information. Furthermore, we leverage Bayesian-based optimization methods to ensure semantically consistent and meaningful optimization.

It is noteworthy that when calculating SHAP values for both tabular and image data, a mismatch in dimensions may arise when multiplying or dividing outputs. To address this, we utilize pixel unshuffleto minimize the loss of information in place of traditional average pooling. Pixel unshuffle is the reverse of the pixel shuffleoperation. It rearranges a tensor with shapeinto a tensor with shape, for a given downscale factor.

SECTION: Experiments
SECTION: Table2Image Framework
We use datasets in the OpenML-CC18benchmark suite. Specifically, we focus on the classification tasks, excluding regression, and only with datasets with 20 or fewer classes.

The hyperparameter ranges are determined with reference to the settings of TabM. We train for 50 epochs, and each experiment is repeated three times to compute the average. The training and testing data are split in an 8:2 ratio, with a batch size of 64. Experiments are conducted on NVIDIA V100 GPU with 90GB RAM.

For handling missing values, columns with more than 50% missing data are removed, while those with 50% or less are imputed using the median. Categorical values are encoded as numeric, and all features are standardized by removing the mean and scaling to unit variance.

We compare Table2Image with three GBDT models including XGBoost, LightGBM, and CatBoost. For neural network-based models, we use MLP, MLP-PLR, FT-Transformer, TabPFN(if applicable), TuneTables, and TabM. Additionally, we include Logistic Regression, SVM, and Random Forest.

We compare Table2Image with 12 different frameworks listed above in terms of accuracy and area under the curve (AUC), as shown in Table. Our models achieve competitive performance in both accuracy and AUC when compared to GBDTs and recent deep learning models for tabular data. For accuracy, Table2Image demonstrates the best performance and Table2Image with VIF initialization ranks second. In terms of AUC, Table2Image with VIF initialization achieves the highest performance, followed by Table2Image. Notably, Table2Image showcases the highest performance in 26 cases based on accuracy and in 23 cases based on AUC. Similarly, Table2Image with VIF initialization achieves top performance in 17 cases based on accuracy and in 24 cases on AUC.

The number of trainable parameters for each model, when the number of classes is 2 and the input dimension is 78, is shown in Table. This comparison focuses exclusively on deep learning models. Our approach requires more parameters than MLP-based or attention-based models, such as MLP, MLP-PLR, and FT-Transformer. However, it still maintains a lighter structure compared to recent large models, including TabPFN, TuneTables, and TabM, that show competitive performance.

We utilize the diversity of images in the FashionMNIST and MNIST datasets to perform random class-specific mappings between tabular and image data. In contrast, HACNetmaps each target class to a single alphabet image across all samples. We conduct comparative experiments by mapping only one image per target class based on Table2Image architecture. We use datasets in the OpenML-CC18 benchmark suite containing three distinct classes. Our experiments in Tablehighlight the performance limitations of using a single mapping. We hypothesize that our multiple mapping enhances the richness and diversity of the information available to the model.

Figureillustrates the outcomes of the realistic image transformations applied to the tic-tac-toe dataset. All instances of class 0 are mapped to T-Shirt/Top and class 1 to Trouser during training. The visualization indicates successful mappings, as the generated images for class 0 effectively highlight the upper body regions, especially the arm areas, while those for class 1 emphasize the lower body areas, particularly the leg sections. Additional examples can be found in Figureof the Appendix.

SECTION: Interpretability Framework
An example of the results from our interpretability framework using the balance-scale dataset is shown in Figure. To evaluate the stability of the framework, we calculate the standard deviation of feature importance scores by repeating the measurement 10 times for each dataset. Additionally, we assess interpretability by comparing the results obtained when the columns of the dataset are shuffled with those obtained when the original column order is preserved. After adjusting the shuffled column order to match the original order, we calculate the MSE of the interpretability values. This approach highlights how column mapping impacts the framework’s outcomes and serves as a measure to evaluate the stability of the interpretability framework. As shown in Table, our framework demonstrates stable results for bothandalong with SHAP.

Furthermore, we summarize the 10-run average values of the MSE, KLD, and MMD losses, which are utilized to minimize the discrepancy between the distributionsand. All results are organized based on the number of target classes of datasets in the OpenML-CC18 benchmark suite, as described in Table. Despite the unsupervised nature of the task, where exact targets are not provided, the low MSE, KLD, and MMD losses indicate robust optimization outcomes. Furthermore, it is worth noting that other feature importance measurement methods, such as gradient-based mechanisms, can also be integrated into this framework, which we consider a direction for future work.

SECTION: Conclusion
In this study, we address the challenges and limitations of applying deep learning to tabular data, a domain traditionally dominated by GBDTs. Recognizing the inherent density and structured nature of tabular data, we propose Table2Image, a framework that transforms tabular data into image representations, enabling deep learning models to leverage the uncompressed format of images. The framework demonstrates competitive performance across various datasets, comparable to leading GBDTs and deep learning-based frameworks.
Additionally, we introduce a VIF initialization to mitigate the effects of multicollinearity. We bridge the gap between traditional feature engineering techniques and deep learning approaches with this method.
Furthermore, we propose an interpretability framework that combines insights from both the original tabular data and its transformed image representations. By leveraging SHAP, Bayesian methods, and mathematical approaches to reduce distributional discrepancies, we offer a dual-perspective interpretation that promotes a transparent and responsible model.
In summary, our contributions advance deep learning for tabular data by improving both performance and interpretability, making it a more viable and trustworthy solution for tabular data analysis, with the potential to extend to multimodal AI applications.

A limitation of our study is its inability to handle regression tasks. Additionally, the Table2Image with VIF initialization method requires a longer execution time compared to the original Table2Image approach. We also assume thatandfollow normal distributions, but exploring more appropriate distributional assumptions could improve performance. Addressing these issues will be a focus of future research.

For future work, we aim to develop methods to leverage statistical properties for more accurate mapping between tabular and image data, beyond current random mapping. Additionally, we plan to explore extensions to other domains including tabular data to text or audio data. Furthermore, we seek to expand our architecture to multimodal learning, by leveraging our unified framework rather than relying on separate models for each modality.

SECTION: Acknowledgements
This work was supported by the National Research Foundation
of Korea (NRF) grant funded by the Korea government (MSIT) (No. RS-2024-00341075).

SECTION: References
SECTION: Appendix