SECTION: Identifying weak critical fluctuations of intermittency in heavy-ion collisions with topological machine learning
Large density fluctuations of conserved charges have been proposed as a promising signature for exploring the QCD critical point in heavy-ion collisions. These fluctuations are expected to exhibit a fractal or scale-invariant behavior, which can be probed by intermittency analysis. Recent high-energy experimental studies reveal that the signal of critical fluctuations related to intermittency is very weak and thus could be easily obscured by the overwhelming background particles in the data sample. Employing a point cloud neural network with topological machine learning, we can successfully classify weak signal events from background noise by the extracted distinct topological features, and accurately determine the intermittency index for weak signal event samples.

SECTION: Introduction
The Quantum Chromodynamics (QCD) phase diagram maps the phases of strongly interacting matter under varying conditions of temperature and baryon chemical potential. It includes phases such as hadronic matter, where quarks are held together within hadrons by the strong force, and the quark-gluon plasma, a state where quarks and gluons are deconfined. A critical point (CP) in the QCD phase diagram represents the endpoint of the first-order phase transition line between hadronic matter and quark-gluon plasma. Studying the CP is of great importance as it offers deep insights into the properties of QCD matter, validates theoretical models, and helps us understand the evolution of the universe shortly after the Big Bang. Experimental efforts to locate the CP, such as the Beam Energy Scan (BES) program at the Relativistic Heavy-ion Collider (RHIC), are crucial for advancing our knowledge in high-energy nuclear physics and for testing the predictions of QCD under extreme conditions.

It is suggested that in high-energy collisions, event-by-event fluctuations of conserved charges could provide a distinctive signature for the phase transition in the QCD phase diagram. The singularity at the CP, where the transition is expected to be second-order, may lead to significant density fluctuations in momentum space. These fluctuations can be measured using scaled factorial moment (SFM) and are anticipated to exhibit a clear intermittency behavior, which is characterized by a fractal pattern of power-law or self-similar nature in the distribution of final-state particles in heavy-ion collisions.

In high-energy experimental studies, intermittency was first observed in central Si + Si collisions at the maximum SPS energy of 158GeV/by the NA49 collaboration. However, the NA61/SHINE experiment at near SPS energies did not detect any intermittency signals at either 150GeV/or 13A-75GeV/in central Ar + Sc collisions. In the meantime, the STAR experiment at RHIC energiesobserved a power-law intermittency behavior of SFM ratios in central Au + Au collisions. The extracted scaling exponent displayed a non-monotonic energy dependence at. Further investigations reveal that the intermittency observed in NA49 can be reproduced by a mixed sample with 99% background random tracks and 1% signal particles generated from a Critical Monte Carlo (CMC) model. For NA61 data, the upper limit on the signal particle fraction is around 1%. The STAR experimental results are consistent with a mix of approximately 1-2% signal particles. These findings indicate that even if critical fluctuations of intermittency exist in heavy-ion collisions, the signal is very weak, with only a few percent of the whole sample, making it easy to be overshadowed by experimental noise. Therefore, before comparing and understanding the various experimental measurements on intermittency, it is necessary to identify and extract the weak signal from the predominant noisy background.

With the advancement of modern computer hardware and artificial intelligence, machine learning (ML), as a data-driven approach, provides prospective opportunities for this investigation. State-of-the-art ML utilizes sophisticated algorithms to analyze complex datasets and identify subtle patterns that traditional methods might miss. Deep learning techniques have been effectively applied to particle collision data analysis, helping to identify rare events indicative of new physics phenomena. A combination of dynamical edge convolution and point cloud neural networks demonstrates robust pattern recognition capabilities in identifying self-similarity from uncorrelated backgrounds. Point cloud networksare often employed in these studies due to their ability to effectively handle and analyze the complex and high-dimensional data obtained from particle detectors in high-energy experiments.

In the meantime, geometry and topology have emerged as powerful tools for identifying phase transitions in complex systems. These approaches provide deeper insights beyond traditional thermodynamic measurements. By analyzing the shape and connectivity of a system using tools from topological data analysis (TDA), one can detect subtle changes that signify transitions between different phases. Techniques such as persistent homology (PH) are used to analyze the characteristic topological features of the data sample, helping to identify the critical point and the nature of phase transitions.

Recent studies suggest that topological machine learning, a methodology that combines TDA with ML, can enhance the performance of point cloud neural networks. In this method, TDA provides deep learning models with a more comprehensive understanding of the underlying structure in the data, while deep learning offers TDA powerful tools for learning complex relationships between the shape and properties of the data. Point cloud neural networks utilize TDA to comprehend data through shape or local features, significantly reducing training time and enhancing capabilities such as pattern recognition. Furthermore, the interpretability and transferability aspects of ML are closely intertwined with TDA. In this context, PH has been proposed as one of the foremost techniques for nonparametrically identifying important topological characteristics of a given point cloud. PH computes the frequency and significance of topological features by constructing a sequence of geometric complexes. It has been used to describe topological invariants of data and is capable of quantifying features such as clusters, holes, and voids within the data. This methodology has been applied in identifying long-range flow correlations in nuclear collisions, describing and evolving cosmic structures, probing universal dynamics in a gluonic plasma, and discerning phase transitions in the mean-field XY and lattice models.

In this study, we incorporate persistent homology and machine learning to explore the significant topological characteristics associated with the critical fluctuations of intermittency in heavy-ion collisions. We construct a TopoPointNet, an innovative framework that integrates TDA with point cloud networks, to classify weak intermittency signals from a massive amount of background noise. The paper is organized as follows: In Sec., we introduce the intermittency analysis and the data samples generated by the CMC model and the Ultra-relativistic Quantum Molecular Dynamics (UrQMD) model. A brief description of TDA and the TopoPointNet architecture are provided in Sec.. The results of identifying and extracting weak intermittency signals from the background are presented and discussed in Sec.. Finally, we give a short summary and outlook of the work in Sec..

SECTION: Intermittency analysis and data samples
Intermittency refers to the phenomenon where fluctuations in particle densities exhibit power-law behavior when observed at different scales. It provides insights into the underlying dynamics of heavy-ion collisions, which may indicate critical phenomena and phase transitions in nuclear matter. Intermittency can be studied by calculating the scaled factorial moment, a statistical measure used to analyze the distribution of produced particles. In a-dimensional momentum space, theth-order SFM is defined as:

Here,is the number of cells into which the phase space is divided with equal size in momentum space,is the measured number of particles in theth cell, and the angular brackets indicate an average over the entire event sample.

As the system approaches the critical point, fluctuations in particle densities become more pronounced. This increased fluctuation can be captured by the SFM, which exhibits a power-law behavior with respect to the number of cells:

Whereis the intermittency index, specifying the strength of the intermittency behavior. This index provides information about the nature of the fluctuations and correlations within the system. If critical fluctuations persist through the evolution of heavy-ion collisions, the second-order intermittency indexis predicted to beforcondensate based on calculations from a critical equation-of-state belonging to the 3D Ising universality class.

To simulate critical fluctuations related to self-similar intermittency, the CMC modelis used to generate event samples that include information on particle momenta and densities. These samples can then be used to compute SFMs and study the scaling behavior of intermittency. In this model, momentum distributions of final-state particles are generated using the Lévy random walk algorithm, which requires the probability densitybetween two adjacent walks to follow

Whereis the Lévy exponent associated with the intermittency index,refers to the relative momentum between two neighboring particles, andandare the minimum and maximum values of, respectively. The model parameters in Eq. () are set to beandfor a critical system belonging to the 3D Ising universality class. The CMC model serves as a powerful tool in the study of critical fluctuations of intermittency in high-energy physics, bridging the gap between theoretical predictions and experimental observations.

The UrQMD model is a microscopic framework designed to simulate and analyze the dynamical evolution of high-energy collisions. This model integrates principles from quantum mechanics, special relativity, and molecular dynamics to offer detailed insights into the behavior of nuclear matter under extreme conditions. It includes relativistic kinematics and dynamics to ensure energy and momentum conservation. The UrQMD model accounts for the creation and annihilation of particles, as well as the scattering processes occurring during collisions, by tracking the interactions of individual hadrons and their propagation through space-time. The model has been widely and successfully applied in studies of,, andinteractions in experiments across a broad range of collision energies, from a few GeV to the energies reached at CERN LHC. Results from the UrQMD model are often compared with experimental data from facilities like the RHICand the LHC. These comparisons help validate the model and refine our understanding of heavy-ion collisions.

Current experimental intermittency measurements have shown that the weak signal, indicative of critical fluctuations in heavy-ion collisions, is only a few percent. To simulate a weak signal event dataset, we mix the CMC particles containing intermittency fluctuations with pure UrQMD background particles. Signal events are obtained by replacing a portion of particles in the corresponding UrQMD events with CMC particles. We define the replacement ratio,, to represent the multiplicity ratio of CMC events to UrQMD events. This ratio indicates the strength with which the critical signal is embedded in the signal events. The detailed process for generating signal events is as follows:

Configure the simulation collision parameters and generate a UrQMD event sample.

Randomly select a particle in the UrQMD event as the initial particle to generate the corresponding CMC data sample.

Randomly select one particle from the UrQMD event and another from the CMC dataset, ensuring the constraintGeV/.

Replace the selected UrQMD particle with the chosen CMC particle, and simultaneously remove the CMC particle from the CMC dataset.

Repeat steps 3 to 4 until the replacement ratioreaches the predetermined target value.

By following this procedure, the transverse momentum spectra remain nearly unchanged between the signal event and the original UrQMD background event samples. The replacement ratiois a small value according to experimental measurements.

In this study, we use the cascade UrQMD model (version 3.4) to generate background event samples in 0%-5% most central Au + Au collisions at=19.6 GeV. We generatesignal events along with an equal number of UrQMD events for, andsignal events and UrQMD events for. We apply the same analysis techniques and kinematic cuts on the data samples as those used in the STAR experiment. Charged particles are selected within the pseudo-rapidity window () and transverse momentum interval (GeV/) . The statistical uncertainty is estimated using the bootstrap method.

SECTION: Topological machine learning
Topological machine learning, or TDA-based machine learning, is a powerful and versatile tool for modern data analysis, which merges geometric and topological insights with the predictive power of machine learning to more effectively tackle complex, high-dimensional problems. The topological method, which studies the connectivity of geometric objects, reveals inherent independent components, holes, and higher-dimensional cavities. With deep connections to theories such as homology, Morse theory, and size functions, the topological characteristics of an object remain invariant under shape deformations, demonstrating robustness in capturing global information despite perturbations and fluctuations. However, in conventional ML models, these higher-dimensional and invariant features are often overlooked during the design of ML architectures. TDA tools, particularly when assisted by multiscale filtrations, excel at detecting smaller and more subtle clusters and topological structures that traditional methods may fail to uncover.

Persistent homology, a core concept in TDA, is an effective tool in computational topology used to analyze the topological features of data sets, particularly by tracking how these features persist as filtering levels change continuously.. By employing homology theory from algebraic topology, PH captures the persistence and changes of connected components, loops, and higher-dimensional voids within filtered geometric objects. In particular, a type of combinatorial geometric object known as a simplicial complex provides a systematic and computationally feasible method for constructing the underlying spaces used in computing PH, referred to as the filtration of simplicial complexes. A simplicial complex consists of geometric simplices of various dimensions: vertices are treated as 0-simplices, edges as 1-simplices, triangles as 2-simplices, and so on, with no overlapping between simplices except where they share their boundary faces. Our study of PH follows three key steps: first, we apply the Delaunay Triangulation Field Estimator (DTFE) to the given point cloud dataset; next, we construct the simplicial complex filtration and derive the PH from it; finally, we compute the topological features throughout the associated PH.

SECTION: DTFE-based filtration construction
In the process of computing PH, various methods can be used to construct the underlying filtrations of simplicial complexes, including Vietoris–Rips complexes, Alpha complexes, andech complexes. In this work, for a given set of points in the Euclidean space, we utilize Delaunay triangulation as the foundation for constructing the filtration of simplicial complexes. Specifically, inor, Delaunay triangulation divides the convex hull of the point set into triangles, also known as triangle meshes, by maximizing the minimum angles and ensuring that the circumcircle of each triangle contains no other points from the set. This process provides a unique and robust representation of the point set inor. In particular, the Delaunay triangulationof a dataset of points(or) forms a simplicial complex, where the points in the dataset constitute the collection of all its-simplices.

For a given point cloud dataset, after the Delaunay triangulation, we define a distance field as a functionusing a field estimation method, which serves as the basis for constructing the filtration. Specifically, each pointis assigned as the geometric distance to its nearest neighbor. Compared to traditional methods for constructing density fields, our results demonstrate that distance fields exhibit greater sensitivity to intermittency signals, more effectively highlighting signals within a complex background.

Subsequently, based on the Delaunay triangulationand the distance field function, we construct the sub-level set filtration, a classic method for building topological filtrations in point cloud and digital image data. Mathematically, for every filtration level, the sub-level setcollects data points with values less than or equal to. As the filtration level increases, the set of points is continually populated until it encompasses all points in the event i.e.,whenever, andfor sufficiently large. Then, the complexformed for eachconsists of the corresponding set of points, along with the line segments and triangles that can be formed from this set and are contained within the triangulation. Formally,, wheredenotes the set of vertices of the simplex. Consequently, by choosing an increasing sequence of filtration levelswith sufficiently large, the topological filtration

is obtained.

Figureillustrates how the complex evolves continuously as the filtration levelchanges. Asincreases, more simplices are added, causing changes in the topological information, such as connected components and loop structures, throughout the process. Fig.(a)-(c) show the complexes during the initial stages of filtration corresponding to, respectively. At these stages, a few points that meet the filtration levels begin to emerge, but the resulting components are primarily scattered. In the late stages of filtration, as shown in Fig.(d)-(f), more points emerge, connecting the previously scattered components. During this process, holes may appear, ultimately resulting in a complete Delaunay triangulation structure.

SECTION: PH computation
Leveraging the homology of each simplicial complex, once the filtration of simplicial complexes is constructed, persistent homology can be computed. Specifically, for the filtration defined in () and a non-negative integer, the-th PH of this filtration is defined as the following sequence of abelian groups and group homomorphisms:

where eachdenotes the-th homology group of, and eachrepresents the group homomorphism induced by the inclusion map. For each, the homology(where) captures global features of the simplicial complex. For example, as an abelian group, each generator of the-th homology ofcorresponds to a (path-)connected component in, each generator of the-st homology corresponds to a loop structure enclosed by-simplices in, and each generator of the-nd homology corresponds to a two-dimensional void enclosed by-simplices in, and so on. In particular, for a simplicial complexconsisting of finitely many simplices, the rank of the-th homology group, known as the-th Betti number and denoted by, or simplywhenis specified, counts the number of-dimensional ”holes.” Specifically,corresponds to the number of (path-)connected components, whilerepresents the number of loops or one-dimensional holes in the complex. As the filtration level increases, the algebraic objectstrack the changes in topological structures throughout the continuously varying filtration.

SECTION: PH feature extraction
The third step involves extracting topological features by summarizing the PH derived from the proposed DTFE-based filtration. In the context of TDA, these features capture properties of the dataset based on its shape and connectivity, rather than specific coordinates or values. Various methods exist for summarizing PH, including persistence barcodes and diagrams, persistence landscapes, persistence images, and persistence curves, among others.

In this work, we utilize the Betti curves, a common summary of persistence barcodes and diagrams, serving as a specialized version of persistence curves, to extract topological information from the DTFE-based filtration. Specifically, for eachin the filtration described in Eq. () and the corresponding PH in Eq. (), we compute the-th and-st Betti numbers:and. These Betti numbers are recorded as a topological summary of the simplicial complex at eachwithin the PH. In our analysis, we focus on the topological state of each DTFE-based simplex at the moment indexed by the filtration levels, and featurize it using homology. Leveraging other PH features, such as persistence landscapes, persistence images, and persistence curves, will be our future research directions.

SECTION: Mathematical details
We schematically elaborate on the essential foundation of the homology theory of simplicial complexes, which forms the basis for persistent homology. More detailed mathematical settings and theories can be found in.

Given a simplicial complexwithinand a non-negative integer, the-th homology group of, denoted as, is defined through the following steps. First, every-simplex withis equipped with two orientations. Namely, two ordered sequencesandof vertices inare said to be equivalent iffor some even permutation. For example,andhave the same orientation, since the following swapping procedure can generate the latter:. On the other hand, in this example, the sequencesandhave different orientations, as a single swap of the first one obtains the latter. In particular, an oriented-simplex is represented as the equivalence classof sequences of vertices in. An oriented simplexis defined as the additive inverse of another simplex, denoted as, ifandhave the same vertices but opposite orientations.

Second, the elementary-chaincorresponding to an oriented-simplexis a functionfrom the set of all oriented-simplices todefined by

Then, the-th chain group of simplicial complex, denoted as, is defined as the free abelian group generated by all elementary-chains. Furthermore, groupis defined as the free abelian group generated by vertices, andis defined as the trivial group.

Finally, the-th homology of the simplicial complexis defined by considering the boundary relations of simplices in. Specifically, for each, the-th boundary mapis defined as the group homomorphism such that

for every oriented-simplicies. In particular, one sees thatfor every. Consequently, the-th homology group is defined as the quotient group, whereis the kernel of, known as the group of-cycles, andis the image of, known as the group of-boundaries. The-th Betti number is the rank of the-th homology group of that collection.

SECTION: TopoPointNet architecture
By integrating the persistent homology of TDA into a deep learning method, we construct a TopoPointNet, as illustrated in Fig., to identify and extract weak intermittency signals from the background. This network consists of two main modules. The first TDA module is designed to extract topological features from the input point cloud data of the 2D momentum for final-state particles. The second module builds a point cloud neural network to learn the spatial encoding of these topological features, enabling the classification of signal and background events. The topological features extracted in the first module serve as inputs for the subsequent PointNet analysis.

In the detailed architecture of TopoPointNet shown in Fig., we first construct simplicial complexes using persistent homology to capture the relationship between Betti numbers of different dimensions by varying filtration levels. Subsequently, we design a two-layer one-dimensional convolutional network to extract local features, with 128 and 256 convolutional kernels, each having a kernel size of 5. This is followed by global max pooling to integrate global features and capture key information from the feature map. Then, we combine the features extracted by the previous network through two fully connected layers, providing a large number of learnable parameters to enable the network to adapt to the training data and make accurate predictions. During the fully connected process, we use the Dropout algorithm, which randomly drops a certain proportion of neurons during training to mitigate complex dependencies among neurons and prevent overfitting. The dropout rate during training is set to 0.3. Each layer applies ReLU and batch normalization, with ReLU enhancing the nonlinear mapping capability by adding an activation function, and batch normalization standardizing the input data of each batch to accelerate model training and improve generalization. Finally, the output layer utilizes the softmax function for classification, which outputs the probabilities of the input event being either a signal or a background event.

The TopoPointNet is trained using a supervised learning technique. The training process involves feeding labeled data into the network and calculating the predicted values via forward propagation. The difference between the predicted values and the actual labels is quantified using the CrossEntropy loss function. The weights and biases of the neurons are updated using the gradient descent algorithm. By iteratively repeating this process, the difference between the predicted values and the actual labels is gradually minimized, thus improving the ability of the model to accurately predict the categories of input data.

SECTION: Identifying intermittency signals by extracting topological features
In this section, we first present our results on extracting topological features by applying persistent homology from TDA, as described in Sec., to point cloud data of both signal and background events. Since the input data are the two-dimensional momenta of each particle, the highest dimension of the topological complex formed is also two-dimensional. Therefore, we focus on the topological features of Betti numbersand.

We begin our analysis by exploring how the Betti numbers of the signal event differ from those of the background event. In the upper panels of Fig., we show the distributions of the 0th topological featureswith respect to the PH filtration level. The black curves represent results from background events, while the red ones correspond to signal events with 5% and 10% replacement ratios in (a) and (b), respectively. We observe that both black and red curves increase with increasingwhen the filtration level is small. In this region, thevalues from the signal event are clearly higher than those from the background event, with the red lines showing a higher peak compared to the black ones. Conversely, both the red and black curves decrease whenis large, and thevalues from the signal event are smaller than those from the background event. The lower panels of Fig.(c) and (d) present the results for the 1st topological features in two different event samples. Whenis less than 0.15, the values ofare consistently 0 for both the background and signal events. In the region where, it becomes difficult to distinguish distinct characteristics ofbetween the background event (black curve) and the signal event (red curve). No significant differences are observed between the two types of events, with either 5% or 10% replacement ratios.

To understand the observed behaviors of Betti numbers, in Fig.we illustrate the momentum distribution of both the background and signal events. In Fig.(a), all data points from a background event generated by the UrQMD model are somehow evenly scattered within the chosen momentum window. The red points indicate the randomly selected 5% of particles that will be replaced by CMC particles. The momentum distribution for the corresponding signal event, after replacing the chosen 5% of particles as detailed in Sec., is depicted in Fig.(b). Here, the replaced particles (red points) gather around, exhibiting a formation similar to a cluster. Following the DTFE process described in Fig., we construct topological simplicial complexes on different event samples. At the early stage of filtration, the DTFE begins by connecting data points that are born as the filtering level increases, resulting in the formation of multiple complexes. The number of connected components, i.e.,, starts at 0 and increases with increasing. In this early stage, the signal event will emerge with more data points compared to the background event since the replaced particles (shown as red dots in Fig.) cluster together in the signal event sample. This leads to a largerin the signal event than in the background event. As the filtration level continues to increase, connections may form between clusters, causingto decrease in the late stage. During this stage,is smaller in the signal event because the number of components in the signal event decreases more rapidly compared to the background event. Eventually, all data points of the entire event form a single complete component whenbecomes large enough. As for the 1st Betti number, it counts the number of 1-dimensional holes or loops in a space. The observed vanishing ofatfor both background and signal events in Fig.(c) and (d) indicates that holes cannot form in this region. During the PH filtering process, as the filtration levelincreases, the newly born data points quickly connect with each other and soon fill into small triangular complexes because they are very close to each other. There are not enough unfilled data points available to form a topological hole in this case. As the filtration level continues to increase and surpasses a certain threshold, more data points and filled triangular complexes emerge, making it possible for holes to form. However, there are no significant differences inbetween background and signal events because the proportion of signal particles in this stage is too small to create a noticeable effect.

The observed differences in the 0th Betti number between the two types of events give us confidence in using this topological feature to identify weak signal events from background events in heavy-ion collisions. We select 100 arrays consisting of various filtering levels that induce changes in the spatial structure and the correspondingas input data for the point cloud network in the second module of the TopoPointNet as introduced in Fig..

The training (black solid line) and validation (red dashed line) accuracy as a function of training epochs for 5% and 10% signal events are shown in Fig.(a) and (b), respectively. We observe that both the black and red lines initially increase with more iteration epochs and then quickly stabilize at a certain value. The accuracy for the 5% signal events reaches a maximum of 94.81%. For the 10% signal events, the accuracy stabilizes faster than that for the 5% signal events, with a maximum value of 99.88%. The testing accuracy is 94.69% for the 5% replacement ratio and 99.85% for the 10% replacement ratio, both of which are very close to the corresponding validation accuracy. These results confirm that topological machine learning can classify both types of events with high accuracy.

The difference in the 2D momentum distribution between background and signal events is highly dependent on the choice of replacement ratio. In heavy-ion collisions, identifying signal events from background events becomes challenging if the replacement ratio is too small. The red curve in Fig.(c) illustrates the testing accuracy of TopoPointNet in detecting signal events at different replacement ratios. For comparison, we also train a pure point cloud network without the TDA module, shown as the black curve in the same figure. In this scenario, the 2D momentum of each particle is directly fed into the 1D CNN in the second module of the network architecture, as depicted in Fig., for classification. The red line remains almost unchanged with high testing accuracy when. After that, it starts to gradually decrease with even smaller replacement ratios. Whereas the testing accuracy of the PointNet without TDA decreases significantly faster with decreasingcompared to TopoPointNet, reaching 58.89% for 5% signal events. Therefore, topological machine learning demonstrates a markedly superior capability to detect weak signals of intermittency fluctuations compared to pure point cloud neural networks.

Finally, we perform the intermittency analysis, as outlined in Sec., on different event samples. Fig.shows the dependence of the second-order SFMs on, calculated for the pure CMC model (blue circles), UrQMD model (purple diamonds), 5% signal events (red stars), and 5% signal events with(green triangles). The black solid lines represent the power-law fitting according to Eq. (). It can be seen from the figure that the blue circles exhibit a strong power-law behavior as the number of partitioned cells increases. The slope of the fit, corresponding to the second-order intermittency index, is determined to be, which is consistent with the theoretical expectation of. This result confirms that the CMC model effectively reproduces the self-similar intermittency behavior. The purple diamonds representing results from the UrQMD background data sample show a flat trend asincreases, which is expected since this transport model does not incorporate any critical-related intermittency mechanisms. The red stars exhibit a gradual rise with increasing, with the fitted intermittency index, which is slightly above zero but significantly lower than the result from the pure CMC model, from which the 5% signal particles originate. This is because, in this 5% signal event sample, 95% of the particles come from the UrQMD model. The dominant background particles obscure and distort the contribution of signal particles, leading to a significant underestimation of the intermittency index when calculated directly from weak signal events. This issue is also the challenge currently encountered by the NA49, NA61, or STAR collaborations in experimentally measuring weak signals of intermittency in heavy-ion collisions.

In the topological machine learning analysis, the filtration level is a key variable that controls the process of constructing a sequence of simplicial complexes. This allows for the analysis of data across multiple scales and uncovers important topological features of the dataset. During the Delaunay triangulation filtration process for the 5% signal event, most particles that appear in the early stage are signal particles, as they are densely clustered in phase space. As the filtration level increases, the particles that emerge in the late stage are predominantly background particles. Therefore, by selecting an appropriate filtration level for truncation, the proportion of signal particles can be significantly increased since a large number of background particles that emerge at largeare effectively discarded. The green triangles in Fig.illustrate the calculated SFMs for the 5% signal event sample, with a filtration level of. These data points follow a good power-law behavior asincreases with. The calculated intermittency indexis, which is consistent with the value derived directly from the pure CMC model within statistical uncertainties. It infers that by integrating the topological machine learning method into the traditional intermittency analysis, we can accurately extract the intermittency index even for weak signal event samples.

SECTION: SUMMARY AND OUTLOOK
In this work, we have demonstrated the application of persistent homology from TDA to analyze and extract topological features from event samples containing critical fluctuations of intermittency in heavy-ion collisions. We have observed a clear difference in the 0th Betti number between background and weak signal events. By constructing a point cloud neural network with TDA, we have achieved a testing accuracy of 94.69% when only 5% of UrQMD background particles in each event are replaced by CMC signal particles. Removing the TDA module will significantly reduce performance. By selecting a filtration level of, the calculated second-order intermittency index for the 5% signal events closely matches
that for the pure CMC signal events.

As a first attempt to identify weak intermittency signals using cutting-edge topological machine learning in heavy-ion collisions, we have utilized a supervised learning method, focusing our analysis on two-dimensional momentum space. Extending this study to three-dimensional space in future work is highly anticipated, as it would allow for the exploration of higher-dimensional topological features, such as the 2nd Betti number. Additionally, integrating other machine learning techniques, such as unsupervised learning, could broaden the applicability of this approach to complex systems. This advancement would facilitate the use of topological machine learning in current experimental investigations on critical fluctuations of intermittency, enhance our understanding of the topological characteristics of criticality, and potentially uncover new physical phenomena in relativistic heavy-ion collisions.

SECTION: Acknowledgments
We are grateful to Prof. Xin-Nian Wang, Prof. Xiaofeng Luo, Prof. Long-Gang Pang, Dr. Xue Gong, and Longlong Li for fruitful discussions and comments. The numerical simulations have been performed on the GPU cluster in the Nuclear Science Computing Center at Central China Normal University (). This work is supported by the National Key Research and Development Program of China (No. 2024YFE0110103 and No. 2022YFA1604900), the National Natural Science Foundation of China (No. 12275102) and the Central China Normal University Student Innovation and Entrepreneurship Training Program Project.

SECTION: References