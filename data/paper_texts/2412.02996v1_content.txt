SECTION: CLAS: A Machine Learning Enhanced Framework for Exploring Large 3D Design Datasets

Three-dimensional (3D) objects have wide applications. Despite the growing interest in 3D modeling in academia and industries, designing and/or creating 3D objects from scratch remains time-consuming and challenging. With the development of generative artificial intelligence (AI), designers discover a new way to create images for ideation. However, generative AIs are less useful in creating 3D objects with satisfying qualities. To allow 3D designers to access a wide range of 3D objects for creative activities based on their specific demands, we propose a machine learning (ML) enhanced frameworkCLAS- named after the four-step of capture, label, associate, and search - to enable fully automatic retrieval of 3D objects based on user specifications leveraging the existing datasets of 3D objects.CLASprovides an effective and efficient method for any person or organization to benefit from their existing but not utilized 3D datasets. In addition,CLASmay also be used to produce high-quality 3D object synthesis datasets for training and evaluating 3D generative models. As a proof of concept, we created and showcased a search system with a web user interface (UI) for retrieving 6,778 3D objects of chairs inShapeNetdataset powered byCLAS. In a close-set retrieval setting, our retrieval method achieves a mean reciprocal rank (MRR) of 0.58, top 1 accuracy of 42.27%, and top 10 accuracy of 89.64%.

SECTION: 1Introduction

The creative design process has been transformed by the advance of generative artificial intelligence (AI), benefiting designers in different fields(Picard et al.,2023). Text-to-text large language models (LLM) such as OpenAI’sChatGPT(OpenAI,2022)and Meta’sLLaMA(Touvron et al.,2023a,b)provide a convenient and powerful way for designers to formulate their design ideas(Zhu and Luo,2021). In addition to discussing ideas in words, text-to-image generative AIs could help designers visualize and showcase their thoughts much faster. The wide adoption of latent diffusion models(Rombach et al.,2022)demonstrated by the popularity ofStable DiffusionandMidjourneyproves that the use of text-to-image models is welcomed by many designers. However, the experience of designers immersed in 3D modeling might be different. It is natural and necessary to extend the success of AI-generated content (AIGC) to the three-dimensional (3D) domain, as 3D objects have many applications in diverse fields(Li et al.,2023;Luo et al.,2021;Zhang et al.,2023). However, due to the additional dimension of the output data and the relatively restricted datasets for training, the performance of the state-of-art 3D generative models developed on Generative adversarial network (GAN)(Goodfellow et al.,2014;Chan et al.,2021;Skorokhodov et al.,2023), Neural radiance fields (Nerf)(Mildenhall et al.,2020;Cen et al.,2023), 3D diffusion models(Liu et al.,2023;Qian et al.,2024)or hybrid approaches, is not on par with what has been offered by text-to-text or text-to-image models. Generated 3D objects may have lower visual quality, distorted geometry, and not fully complete 360-degree generation(Skorokhodov et al.,2023). In addition, 3D object generations perform better when intensive information is given as prior, such as multi-view images and 3D objects(Liu et al.,2023;Qian et al.,2024). Since in the ideation phase of design, 3D designers are looking for references and inspirations, such detailed prior can hardly be provided, which limits the use of the current limited 3D generation. The training data of text and image generation models can be constructed from large quantities of diverse sources since these modalities can be created and accessed by the general public. In contrast, 3D objects are generally created by professionals for professional tasks, resulting in many 3D objects being proprietary properties.

Given these constraints, instead of creating a 3D object from scratch based on the designer’s thoughts, providing them with relevant existing 3D objects in professional datasets might be more practical and useful. However, 3D Object Retrieval (3DOR) methods are generally based on modalities that are rich in 3D spacial information(He et al.,2018;Jing et al.,2021;Feng et al.,2023), such as multi-view image(Su et al.,2015), point cloud(Zhou and Tuzel,2017)and voxel(Wu et al.,2014). These modalities are not intuitive for 3D designers to use when seeking inspiration. On the other hand, language is the most intuitive modality to describe the 3D objects designers may want as a reference. Language can be especially useful for retrievals not only based on the object’s geometry but also on the varying design concepts such as use cases, intended users, and design purpose.

We, therefore, propose a four-step framework,CLAS(capture, label, associate, and search)as summarized in Figure1, to automatically label, index, and retrieve 3D objects based on the user’s textual prompts with varying levels of details and focus.CLASis a flexible framework that can be applied to diverse 3D objects for different purposes. In thecapturestep, an image at a fixed position or a set of multi-view images can be created based on 3D models in the target dataset. A visual-language model is leveraged to create descriptions for these captured images based on specifically designed prompts in thelabelstep. Depending on the use case, generated descriptions can have different focus (multiple descriptions can be created for an image). In theassociatestep, one or more CLIP models(Radford et al.,2021)can be trained on the images and descriptions to learn semantic context from text and visual context from images. More than one model should be trained if more than one purpose of descriptions is generated in the previous step. Then, the CLIP models can be used in thesearchstep for text-based retrieval of 3D objects. This helps designers to efficiently find inspiration, overcome creative blocks, and seamlessly initiate their design process. Furthermore, the 3D datasets labeled byCLAScan also be repurposed to train 3D generative models with richer contextual semantic meanings. To showcase the efficiency ofCLAS, we implemented a proof of concept search system on chairs in theShapeNet(Chang et al.,2015)and evaluated its effectiveness.

SECTION: 2Background

Considering the limitation of the current text-to-3D generation and the availability of existing datasets of 3D objects, this research instead focuses on improving 3DOR. Quick access to specific 3D objects can significantly enhance creativity and productivity for 3D designers. 3DOR refers to the computational methods to identify and fetch 3D objects within a dataset based on similarity to a query(He et al.,2018). This query, in principle, can take various forms, ranging from complex data structure (multi-view image(Su et al.,2015), point cloud(Zhou and Tuzel,2017)and voxel(Wu et al.,2014)) to attributes of the items or simple text descriptions.

In the simplest case, a general-purpose search engine such asGooglemay be used to retrieve 3D objects from the internet. However, a general-purpose engine relies solely on the labels of the source of the 3D objects and some limited visual cues for retrieval. Also, such an engine has limited access to 3D datasets. In contrast, machine learning (ML)-driven algorithms can compare geometric shapes, textures, and other relevant features of 3D objects to find matches or closely related items(Gao et al.,2020;Feng et al.,2023). Existing methods are usually designed around computer vision (CV) techniques and thus focus more on the visual characteristics of 3D objects, missing the critical step of understanding the semantic meaning of user input and associating them with visual cues. Various methods have been proposed to enable 3DOR based on either single modality(Bustos et al.,2007;Wei et al.,2020;Su et al.,2020)or multiple modalities(Nie et al.,2019;Feng et al.,2023), aiming to learn the distribution of 3D objects in high dimensional space(He et al.,2018)and sorting the items according to various distance metrics. In a typical setting of closed-set 3DOR, the training setand the testing setare assumed to have the same distribution and can be represented in the same space, whereis a 3D object andis the associated category of the 3D object. The retrieval method is trained on the training set to optimize:

This objective encourages 3D objects in the same category to be closer and objects in different categories farther apart in the representation space. It is also very dependent on the categories. The method learned may be meaningless if the category is very broad. Current 3D datasets are more focused on the representation or modality of 3D objects to capture visual and spatial information, undermining the importance of the semantic meaning of these 3D objects. Some works, such as ShapeNet(Chang et al.,2015)and Objectverse(Deitke et al.,2023), tried to fill in this blank by adding general labeling to 3D objects with limited detail. As a result, 3DOR methods tend to rely on complex visual or 3D input for retrieval. However, in the use case for 3D designers, providing visual representation to retrieve 3D objects is challenging in the ideation stage. A textual description of the 3D object might be the most practical cue to use in the ideation phase of design. In order to enable retrieval based on text with sufficient accuracy, the text label of 3D objects needs to be improved for existing 3D datasets.

To address this limitation, this research leverages the recent breakthrough in multi-modal large language models (LLMs) to propose an ML-enhanced frameworkCLAS, which labels datasets of 3D objects with human-readable descriptions focusing on different perspectives (such as geometry, functionality, or design concept) based on prompts and associates these descriptions with their corresponding visual characteristics in high dimensional vector space for easy retrieval using words. Systems built withCLASoffer more precision and flexibility in 3D object retrieval by understanding complex spatial and/or design-related language, where traditional methods are not specialized.

SECTION: 3Method

To automate the process of labeling, indexing, and retrieving 3D objects from a dataset, we proposeCLAS- an ML-enhanced four-step (capture, label, associate, and search) framework as illustrated in Figure1. To demonstrate the framework’s effectiveness, we deploy it to create a proof of concept search system for chairs in theShapeNetdataset(Chang et al.,2015)as demonstrated in Section4. ApplyingCLASon an existing 3D dataset creates two products: a set of descriptive labeling for the datasets, which can be used for other training or evaluation, and a 3DOR retrieval method based on text input.

The first step inCLASis to capture a certain number of images of the rendered 3D objects. Since 3D objects can be stored in various formats, the environment or software that could be used to render and capture these objects varies. The cross-platform game engineUnityor 3D computer graphics softwareBlenderare able to cover most of the formats. The setting of the lens, orientations, and lighting should be tuned to suit the purpose of the dataset. The images taken are forwarded to the next step for further processing. This step generates a 2D representation of the 3D objects. Depending on the complexity of the 3D model, more than one image may be generated to preserve more details. In the proof of concept application, we used the images of 3D objects of the chair category in theShapeNetdataset(Chang et al.,2015).

In the second step,CLASgenerates a description of the image and labels the image with the corresponding description. Many 3D datasets, such asShapeNet(Chang et al.,2015)andModelNet(Wu et al.,2015), are released with limited labels, making reusing or repurposing these datasets outside of the research community difficult. This step is performed using a multi-modal LLM, such asChatGPT-4V, to describe the image with a specific focus defined by the user. For example, the description can be directed to focus on the object’s appearance and/or usage. The images of 3D objects and their accompanying descriptions are used in the training in the next step. The performance of the retrieval method is very dependent on the quality of the labeled description. The prompt to generate the descriptions should be designed considering the users of the retrieval system, i.e., what kind of language is usually used to describe the objects. In the proof of concept application, we usedChatGPT-4Vwith engineered prompts to generate detailed descriptions of the shape and design features of the chairs.

This step trains a multi-modal ML model, i.e., a model capable of encoding two different modalities (text and image), to associate images and their descriptions. Different models can be used for this purpose based on the design of the system, such asCLIP(Radford et al.,2021)with vision transformer(Dosovitskiy et al.,2021). After training, two vector spaces representing the textual descriptions and visual images are created, respectively. The similarity of items can be reflected by their proximity in the vector space. We fine-tuned aCLIPmodel for proof-of-concept application to associate the two modalities of 3D objects.

The final step prepares a user interface (UI) for the users to engage with the system, i.e., search and retrieve 3D models given a textual description. Based on the purpose of the system, different UIs can be used, ranging from a command line to a webpage to an immersive AR/VR environment. We created a website using React to host the proof of concept application. UI ultimately determines how users interact with the AI system.

SECTION: 4Proof of concept application

In this section, we present aCLASpowered web application that allows users to search for 3D objects of chairs using words to demonstrate the usefulness ofCLAS. We first introduce the background of the text-based search application, followed by a detailed explanation of how each step inCLASis applied. An overview of this use case is provided in Figure2. Implementation details can be found in AppendixA. The interaction between users and the system is described in details in AppendixB.

SECTION: 4.1Background

Prototyping new designs can be challenging for 3D designers who need references for inspiration, but effectively matching the needs of these designers and existing 3D objects can be challenging. We chose a specific category, i.e., chairs, to demonstrate howCLAScan be applied to bridge the gap: chair designs.ShapeNetdataset(Chang et al.,2015)contains 6,778 3D chair objects, which can be potentially used for creative works. However, it is impractical for designers to manually go through these objects, considering that they often lack proper annotations that are easily readable by humans. An exemplary use case by designers is shown in AppendixB.

SECTION: 4.2Implementing 3D chair search system usingCLAS

This system was built usingCLASas detailed below. Note that we leveraged the existing images in the dataset so the first step inCLAS, i.e., capture, is skipped.

We conducted prompt engineering for theChatGPT-4Vto generate image descriptions. The generated descriptions are prompted to match potential user input that designers would likely use when searching for chair objects. We design prompts echoing three focuses to achieve the goal: design purpose, structure, and template.

Product design often starts from the product’s functionality to solve a specific problem and innovate from the original design. From the designers’ perspective, potential users may enter the design purpose and use case as input prompts to get ideas for the brainstorming process. The left column in Figure3shows a prompt containing the key questions for designers to generate a design purpose-based text description.

Visual characteristics play an important role in distinguishing different chairs and are critical in making design decisions. A detailed description of the exact shape, uniqueness, and size relations would be very helpful for the retrieval method later to learn different 3D objects with enough precision. To discover the effective prompt, we started with the simple prompt, “Can you describe the chair’s appearance?" The optimization process of the prompt underwent iterative tests by adjusting the task description, providing hints, varying the focused aspects, etc. The generated descriptions were initially read and validated manually to omit failed prompts. Then, we evaluated the prompts by comparing the image generated byDALL-Efrom OpenAI to the original image. The more similar the generated image using the description to the original image, the better the prompt is. Several examples of tested prompts and corresponding results are included in AppendixC. Based on the experiments, capitalizing the critical words and adding a sentence to highlight the unnecessary part for the description would increase the performance of the prompt. On the other hand, prompts with too much information and the use of general words might produce imprecise descriptions of the provided objects. The conciseness of the generated description is important to avoid overly general expressions and highlight the characteristics of an object. The finalized structure prompt is shown as the middle column in Figure3.

Lengthy descriptions are expected to contain more information about the chair. However, they also mean more computation is needed to train and serve the retrieval system. In this proof of concept application, the base version of theCLIP(Radford et al.,2021)was used, limiting the length of the generated description to less than 77 tokens. To compress the description’s size while maintaining each chair’s distinctive features, we used a template-like prompt as the right column in Figure3. It focuses on both design purpose and structure and is used for the labeling process. The template can also be provided to the user for filling out during searching, which could help to establish consistency between the generated descriptions and potential user input to improve the accuracy of the retrieval system. Even though the limited structural descriptions generated by the template did not cover all the details of the complex 3D objects, they maintained the advantage of describing the design purpose and uniqueness.

We usedCLIP(Radford et al.,2021)to associate the text descriptions and the image used to generate the text description. The architecture of theCLIPis included as AppendixD. We first produced the datasets for fine-tuning and evaluation by pairing up images of 3D chair objects and their corresponding text descriptions generated byChatGPT-4Vprompted by thetemplatedesign. Pairs in the training set were fed into the CLIP model, and the fine-tuning encouraged the text embedding embedded by the text encoder and the image embedding embedded by the image encoder to be close together after projection into the vector space of the same dimension by the contrastive loss:

whereis the number of samples in the batch, andis the cosine similarity between the image representationand the text representation. The labels are implicitly indicated by the pairswhich are the correct pairs in the batch, whileandforrepresent the negative pairs. The fine-tuning is visualized as Figure5. We noticed that the model might overfit and did not generalize well, which is discussed in Section4.3. After the fine-tuning, the CLIP model became more sensitive to chairs and their designs than a general model and could be used for retrieval.

A complete web application with a front end and a back end was created to host the search system. The implementation details are included in AppendixA. When designers start a new search, the process begins with the search bar, where they enter a prompt such as “Nordic-style chair suitable for reading with fancy patterns" and then select the value for two slide bars, pressing either enter or the search button to search. Upon submitting the prompt, our system quickly retrieves 3D objects, displaying search results within a fraction of a second from a database of 6,778 chair objects. The search results in the chat window are presented in rows, with the user’s avatar displayed above, showing the user’s prompt, followed by the system’s response, “Here are the suggestions," to enhance user interaction (as demonstrated in Figure6). As users browse through the objects, the image of the object they hover over will slightly enlarge, indicating which chair object they are considering or wish to select. Clicking on a desired model enlarges it for a closer inspection of the details (as shown in Figure6). To download the chosen 3D object, clicking the download button redirects the user to object downloads. If the user is unsatisfied with the search results, they can click on the “show description" button to view the label/description generated byChatGPT-4Vfor each model. This feature allows users to refine their search based on the labels or start a new search by entering a more precise prompt. If the user wants to search for more 3D objects that are similar to one of the result models, simply click on “search similar"; the website will search again using the selected object as a query, thereby providing similar object suggestions. These functions offer a tailored and interactive browsing experience.

This website was designed for designers seeking inspiration and resources for 3D object designs. The visual and workflow designs help users focus more on the search system. The pre-filled prompt in the search bar enables users to quickly start their initial search, while the navigation bar offers additional information and content about the project. Most importantly, the search results page makes the search process more intuitive and convenient, allowing users to easily find and download the object they seek, leaving a good impression and a great user experience.

SECTION: 4.3Evaluation

Before theCLASimplementation, no efficient solution was available to retrieve chairs in theShapeNetusing natural languages. Potential users must go through all the chairs to manually decide which are wanted, which requires much time and effort. To the best of our knowledge, this exemplary use case is the first attempt to enable this convenient search using natural language on 3D chair datasets.

The out-of-box CLIP model(Radford et al.,2021)from OpenAI is used as the baseline to demonstrate the effectiveness of the fine-tuning using three standard metrics in evaluating retrieval systems.Mean reciprocal rank (MRR)calculates the mean of the reciprocals of the rank at which the true objects are retrieved. MRR takes a value between 0 and 1 such that the closer to 1, the better the performance, i.e., the system ranks the intended item higher in all the possible items.Top-1 accuracycalculates the percentage of retrieved objects as the true object.Top-accuracyexpands on the top-10 accuracy to the scope of the top 10 most probable objects selected by the model.

As shown in Table1, there is a huge increase in performance in all the metrics for both the training, testing, and complete sets. The fine-tuned model with the best train loss performs noticeably better than the best validation model, even for unseen samples in the test set. We argue that for close-set retrieval, where objects to be retrieved are from a fixed dataset, it is reasonable to train the retrieval model on all data. We, therefore, overfitted the model on the complete set to achieve a loss of 0.02 in 10 epochs. The overfitted model achieved the best retrieval results, as shown in the last row of Table1. As demonstrated in Figure7, the text and image representation of the 3D chair objects can be closely associated even with 6,778 samples.

SECTION: 5Discussion

CLAStargets the problem space around the inefficiencies in finding relevant 3D objects based on user specifications, which often hinder the design process. Recognizing the importance of facilitating designers in finding inspiration and overcoming creative blocks, we aim to enable efficient usage of existing 3D datasets. We propose an ML-enhanced frameworkCLASto enable effective 3D datasets indexing, labeling, and prompt-driven retrieval. Compared to existing methods,CLASdifferentiates itself by offering incredible flexibility in the 3D datasets, which operate on and prompts with different levels of details and focus, filling a gap left by generative models and general search engines.

SECTION: 5.1Limitations

In the proof of concept application, we testedCLASon a dataset of 6,778 chairs. However, various methods can be experimented with to expand the capacity of the current formulation ofCLAS. For instance, one may add a classifier model to classify the 3D objects into different categories and forward them to corresponding CLIP models (assuming there are multiple CLIP models for multiple categories). One may also increase the length of descriptions and the dimension of embedding to encourage the CLIP model to learn the complex relation directly, assuming there is enough data.

The current search function provided byCLASstruggles to understand negative prompts and often does the opposite. For example, if a user states not to retrieve objects with certain properties, the system would actively pick objects with unwanted properties. This occurs primarily as no negative example exists in training for retrieval systems. The text description generated for each image usually highlights what the object has rather than does not have. To fix this, one may change the prompt to generate different descriptions or change searching in the embedding space to substrate the embedding of features that are not wanted. More in-depth study is required as negative prompts are usually not supported by retrieval.

Currently, theCLAS-powered system only retrieves a single subject in each suggestion. However, it would be beneficial to many designers if the application could understand the relation between objects of different categories and combine them while evaluating the overall design effect. Future developments could focus on integrating different items to form sets or scenes, allowing designers to gain more inspiration and obtain a comprehensive overview of their designs. This function would enable users to explore various combinations of different items and evaluate designs more effectively.

SECTION: 5.2Social impact

CLASenables any person or organization to repurpose and/or reuse their existing 3D datasets effectively.CLAS-processed 3D dataset allows users to search its content using natural languages with high accuracy and precision. We hope that exposing numerous references to designers when designing new concepts can assist them in ideation. We demonstrate the usefulness ofCLASin powering the search for 6,778 3D chairs, which can be used by furniture designers. Similarly,CLAScan be applied to different 3D datasets for different purposes. By adjusting the prompt used to generate text descriptions for images of 3D objects, the search function can be tuned to work with different aspects of the objects suiting different use cases. By bridging the gap between user prompts and relevant 3D models, we aspire to enhance the design process and foster creativity within the community.

SECTION: Acknowledgments and Disclosure of Funding

We sincerely thank the support of our project advisor, Professor Kosa Goucher-Lambert, offered throughout the project. We would also like to thank Kevin Ma in the lab and Daniele Grandi from Autodesk for regular meetings and feedback on our project. It would not be possible for us to complete this project as it is without the help of anyone mentioned above.

SECTION: References

SECTION: Appendix AWeb application

This web application will be made available to collect feedback.

SECTION: A.1Implementation details

Our website’s design aims to offer an intuitive user interface for designers seeking 3D model design suggestions, and prioritizing search functionality to minimize distractions. The main color theme is light gray, with the central space occupied by a chat window—where users interact most with our website (as shown in Figure9). At the center of this chat window is our logo, “ML4Design," with a subtitle, “A machine learning system for exploring a large 3D chair model dataset," which explains our website’s purpose. Below the chat window is the search bar, filled with “What kind of chair are you looking for?" in light grey font color to hint the user at our service. An example, “Height adjustable office chair," is immediately provided to guide users on how to engage with our site to encourage interaction. To the right of the search bar is a slider labeled “Number of results." Users can adjust this slider to select the desired number of results, allowing for focused attention on a few suggestions or a broader exploration of up to ten recommended models. The “Visually Focused. Slide bar allows the user to adjust the weighting of image search and text search. Our website can search in the image vector space and the text vector space. For instance, toggling the visual focus value to 0.1 means the image search will weigh 10 percent and the text search will weigh 90 percent of the result. These features enhance the interactive and engaging experience of our website.

The back end coordinates the CLIP model, which is hosted online using Huggingface’s endpoint service, and the database, which is hosted online using AWS service. When a user enters a query into the website, the input is processed and sent to the CLIP model endpoint, which returns the IDs of the retrieved 3D chair objects to the back end. The back end then fetches the corresponding models from the database based on the received IDs and forwards them to the front end for display.

Chairs inShapeNetare stored as objects and images, i.e., every 3D chair object is paired with an image of the chair. We manage this dataset of chairs using Amazon Web Services (AWS) cloud services for optimized storage and accessibility. Image and 3D object files are stored in AWS Simple Storage Service (S3), providing reliable and scalable cloud storage. Metadata, including file names, image links, 3D object download links, and corresponding descriptions, are logged in a MySQL relational database. This database is hosted on AWS Relational Database Service (RDS), ensuring stable and efficient data handling and enhancing data accessibility.

SECTION: A.2User Experience

When designers start a new search, the process begins with the search bar, where they enter a prompt such as “Nordic-style chair suitable for reading with fancy patterns" and then select the value for two slide bars, pressing either enter or the search button to search. Upon submitting the prompt, our system quickly retrieves 3D objects, displaying search results within a fraction of a second from a database of 6,778 chair objects. The search results in the chat window are presented in rows, with the user’s avatar displayed above, showing the user’s prompt, followed by the system’s response, “Here are the suggestions," to enhance user interaction (as demonstrated in Figure10). As users browse through the objects, the image of the object they hover over will slightly enlarge, indicating which chair object they are considering or wish to select. Clicking on a desired model enlarges it for a closer inspection of the details (as shown in Figure11). To download the chosen 3D object, clicking the download button redirects the user to object downloads. If the user is unsatisfied with the search results, they can click on the “show description" button to view the label/description generated byChatGPT-4Vfor each model. This feature allows users to refine their search based on the labels or start a new search by entering a more precise prompt. If the user wants to search for more 3D objects that are similar to one of the result models, simply click on “search similar"; the website will search again using the selected object as a query, thereby providing similar object suggestions. These functions offer a tailored and interactive browsing experience.

On the left side of the website, the top of the navigation bar displays our logo, “ML4Design," followed by three subtitles: “About," “Team," and “Contact." Below these subtitles is a brief description summarizing the content on these pages. The description under the "About" title reads, “This is aCLASframework system, enabling the search of 3D chair models from a dataset of 6,778 chairs. Try it out by starting a search!" This succinctly describes what our system is and its capabilities. Upon clicking on the About page, users can see a slideshow demonstrating how our system’sCLASframework operates. Below, the slideshow showcases different prompts, offering search examples using various criteria, including searching with chair geometry, design purpose, or a combination of both.

Back to the Navigation bar, the “Team" page introduces our project members. The “Contact" page communicates our openness to suggestions and opinions, allowing users to leave feedback. This structure ensures that visitors can easily understand the purpose of our system, learn about the team behind it, and engage with us directly. After viewing these pages, by clicking the top left ML4design logo, the user will be guided back to start their search.

SECTION: Appendix BInteraction between 3D designers and the system

In this section, we discuss how designers can benefit from a search system enabled byCLASand the resources it provides. First, we briefly introduce the design thinking method proposed by Tim Brown[Brown,2008]. This method later evolved into the 5-step design thinking approach proposed by IDEO, also known as the Double Diamond. This method outlines the process a designer, such as a product designer or 3D model designer, goes through when designing. The first step is “empathize," where designers must understand people’s problems and identify issues even if they are not immediately apparent to the users. The second step is “define." In this phase, designers must define the core problem and specifically define what they aim to solve using the POG (Product Opportunity Gap) statement. The third step is “ideate," where aCLAS-enabled system becomes particularly valuable. In this phase, designers generate and brainstorm solutions for the identified problem. We discuss this in more detail in the following paragraph. The last two steps are “prototyping" and “testing", where designers bring their solutions into the real world and test their effectiveness through user interviews and other methods. Through several iterations, they refine their designs to arrive at the final product.

For a retrieval system powered byCLAS, the primary goal is to assist 3D model designers during the “ideate" process and the “prototyping" phase. To help understanding, we consider a simple persona and a specific scenario to introduce the entire process the target designers will experience.

Peter is a 3D model designer working at a famous office chair company. His daily job involves proposing new ideas and creating CAD designs for the company’s next product. Typically, he searches online for pictures, reads books, and discusses ideas with teammates to come up with new concepts. However, after years of work, he has started to struggle with generating innovative ideas that can satisfy his manager. In searching for inspirations, a search system enabled byCLAScan be helpful.

This time, his design challenge is to propose a modern minimalist office chair suitable for reading novels, targeting a specific market. He typed “modern minimalist office chair suitable for reading novels" into the system and chose to receive 10 suggestions to maximize the variety of ideas as shown in Figure12. The system quickly retrieved 10 models within seconds, providing precise 3D designs. Peter was impressed by the third suggestion and clicked on it for more details, as shown in Figure13. After reading the description and confirming it matched his requirements, he used the “search similar" feature to find more related suggestions, as shown in Figure14. This time, the second suggestion caught his eye and perfectly aligned with his design needs. He downloaded the model as a reference and worked on it to further refine the design.

This example illustrates howCLAS-enabled search system can assist designers during the “ideate" and “prototyping" phases.CLAS-enabled search system not only helps designers overcome creative blocks but also reduces the time they spend on 3D modeling work.

SECTION: Appendix CEvaluate effectiveness of prompts for generating descriptions of chairs

SECTION: Appendix DCLIP architecture

SECTION: Appendix ERetrieval results