SECTION: Machine Learning-Based mmWave MIMO Beam Tracking in V2I Scenarios: Algorithms and Datasets
This work investigates the use of machine learning applied to the beam tracking problem in 5G networks and beyond. The goal is to decrease the overhead associated to MIMO millimeter wave beamforming. In comparison to beam selection (also called initial beam acquisition), ML-based beam tracking is less investigated in the literature due to factors such as the lack of comprehensive datasets. One of the contributions of this work is a new public multimodal dataset, which includes images, LIDAR information and GNSS positioning, enabling the evaluation of new data fusion algorithms applied to wireless communications. The work also contributes with an evaluation of the performance of beam tracking algorithms, and associated methodology. When considering as inputs the LIDAR data, the coordinates and the information from previously selected beams, the proposed deep neural network based on ResNet and using LSTM layers, significantly outperformed the other beam tracking models.

SECTION: 
The current fifth generation (5G) and the incoming sixth generation wireless communications envisages, among other features, very high peak data rate and very low latency. One way of fulfilling the data rate and latency requirements is with the use of higher bandwidths. However, due to the scarcity spectrum at the currently mostly used sub-6 GHz frequencies, wider bandwidths are being reserved for mobile communications at the millimeter wave (mmWave) band, such as 28 GHz and 60 GHz. With greater amounts of available spectrum at mmWave, decreasing symbol time, decreasing latency, and increasing throughput are possible. A drawback of the mmWave bands is the higher attenuation compared to sub-6 GHz frequencies.

Because of the higher attenuation of the mmWave bands, multiple input multiple output (MIMO) techniques are one of the main technologies of the 5G development at mmWave bands, especially beamforming techniques. Beamforming provides better directionality of the electromagnetic wave, allowing it to circumvent the high path attenuation.A main challenge of massive MIMO at mmWave frequencies is that the beams formed by the beamforming techniques can be very narrow, requiring that the beams of the transmitter and the receiver point to each other.

In this context,
exploring all possibilities to optimize the direction of transmission beams between() and() using “brute force” beam sweeping leads to significant overhead.
There are several algorithms to reduce this overhead.
For example, a process calledbeam trainingis part of standards such IEEE 802.11ad, 5G, and other wireless networks. When the number of antennas increases due to the high number of possible beams, the search for the best beam index can be a time-consuming process.

As a mean to solve this issue, many recent works are applying() techniques to predict the beams. However, this work relies on exchanged pilot signals to allow channel estimation and the prediction of the beam index, which is also challenging due to a high number of antennas, which require a high number of pilots to be exchanged, consuming wireless resources.

Other studies have investigated beam tracking as a potential solution to the overhead issue. In, the authors employed LIDAR in an indoor scenario with mobile users, achieving a prediction accuracy of 88.7%. Similarly,utilized LIDAR data, incorporating real measured data as input to a long short-term memory (LSTM) network to predict future beam indices, resulting in an accuracy of 84%. This study modeled the LIDAR input based on the angle and distance to each obstacle. The research presented inused the same input parameters as the study mentioned earlier. Their proposed architecture leveraged all LIDAR inputs from the preceding instance, with performance compared against a baseline that predicted using prior beam knowledge. This approach yielded an accuracy of 57.5% for current beam prediction, which was close to the baseline with the previous beam information.

In this paper, we evaluate the prediction of the beam index using information available for only one of the devices on the communication pair to avoid exchanging pilots. Thus, the technique evaluated in this work is an alternative to overcome the issues of finding the optimum beam index and the exchange of pilots previously discussed. The context is a mobile network with vehicles connected and has a variety of sensors. The out-of-band data from the sensors could be used to find the directions, i.e., the beam index, based on the vehicle’s surroundings. In this context, we evaluate the feasibility of using() and LIDAR data combined with previous beam directions to find the current best beam index. In summary, the contributions of this work can be organized as follows:

A Deep Learning model that uses spatial data and previous beam indices to perform an estimation of the next best beam.

New datasets following Raymobtime methodologyfor beam tracking applications with time correlation.

This paper is organized as follows. Sectionpresents the system model, which also discusses finding the optimum beam pair for the receiver and the transmitter and its influence on communication performance. Sectiondescribes the dataset used to generate the results presented in this work. Sectionpresents details of the evaluation setup, which includes the adoptedmodel and the communication system, and also presents the results of our evaluations. Sectionpresents the conclusions of the paper.

SECTION: 
SECTION: 
In this paper, we consider the downlink of a mobile communication system, which operates at() frequencies and uses a fully digital MIMO architecture. Theand thehave an() withantenna elements,for the transmitter andfor the receiver, spaced by half wavelength ().

The channel matrixis obtained from multipath parameters obtained with ray-tracing and modeled according to the geometric channel modelfor the narrowband case as

whereis the number of rays, andis the gain for the-th ray. For thecase, it uses the steering vectorsthat can be described as

The evolution of channel in time can be simulated using different strategies. One that is widely used due to its simplicity is the adoption of a first-order() model. In contrast, this work adopts a more realistic approach for simulating the channel evolution, which relies on the Raymobtime methodology. Raymobtime simulates the channel evolution, keeping consistency over time, frequency, and space.

SECTION: 
Effective beamforming relies on accurate() to dynamically adjust the beam patterns in real-time, as shown in Fig., ensuring optimal performance even in the presence of user mobility and varying environmental conditions.

In this paper, we assume that both theandemploy beamforming, such that the received signal at the UE is

whereandare respectively the precoding and combiner vectors used at the BS and UE to perform the beamforming. The vectorsandare chosen from the codebooksand, whereandare drawn from the Discrete Fourier Transform (DFT)as

whereis the number of codewords in the codebook and.

Thus, theoptimalbeam indexis given by

Beam selection refers to the process of choosing the best beam from a predefined set of beams (codebook) that maximizes the received signal strength. This can be achieved by evaluating the received signalfor each beamand selecting the beam indexthat provides the highest signal strength. When the beams are “pointy” (e. g., as when using a DFT codebook), the goal is to ensure that the selected beam aligns with the strongest path of the transmitted signal.

Beam tracking involves continuously updating the selected “pointy” beam to maintain the alignment with the strongest signal path as the user moves or as the environment changes. This requires periodic measurements and adjustments to the beam indexto adapt to the dynamic nature of the wireless channel. Beam tracking is essential for maintaining optimal communication performance in mobile scenarios and in environments with high variability.

SECTION: 
The Raymobtime methodology, initially proposed inand later refined in, enables generating realistic wireless channel data with time evolution in simulation scenarios based on real locations, as shown in Fig, and data extracted from public sources.
The datasets created using this methodology are particularly designed forapplications, focusing on vehicle-to-infrastructure (V2I) and vehicle-to-vehicle (V2V) communications, thereby providing rich resources for advancing research in these areas.

Raymobtime datasets are organized into episodes and scenes, providing a structured way to capture the dynamics of wireless communication environments over time. An episode represents a complete set of data for a given simulation run, capturing the entire process from beginning to end under specific conditions (e.g., environment, frequency, mobility). Each episode consists of multiple scenes that chronologically represent the progression of the simulation. By examining changes across scenes within an episode, researchers can study how factors like mobility, environmental changes, and varying signal conditions impact communication performance.

SECTION: 
As mentioned, the Raymobtime dataset offers a comprehensive suite of data types to facilitate different approaches to optimize and solve problems in 5G/6G networks. More specifically, some key features of the dataset include:

Ray-tracing simulations data of the wireless channel, capturing the multipath propagation characteristics and signal interactions with the environment. It includes information such as gain, phase, departure and arrival angles, time of arrival, and ray’s interactions.

High-resolution LIDAR point cloud data capturing the 3D structure and scatters at the environment. The sensor can be placed at theor at the, as shown in Fig..

Geographical localization of objects in the scenario, including the transmitter and receiver, enabling the correlation of spatial positioning with channel characteristics.

Visual context of the simulation environment through images. This data aids in visual verification and can be used in conjunction with LIDAR and GNSS data for comprehensive environmental modeling.

The combination of these diverse data types provides a rich resource for developing and testingalgorithms.

SECTION: 
Thecan serve a variety of devices, including connected vehicles, which in general move faster than other, thereby posing more strict requirements on the network. To effectively explore and evaluate beam tracking performance, two datasets in different scenarios were made to address these requirements.

The first dataset is the Rosslyn scenario (Fig.), which is a simulation environment validated for diverse papers, using twoone in each sidewalk in order to create a more balanced dataset. This dataset provides insights into how beam tracking algorithms perform in challenging urban landscapes.

The second dataset focuses on scenarios involving vehicles navigating a busy roundabout (Fig.). Vehicles in this dataset travel at varying speeds up to 60 km/h, while changing their orientation vector due to the curved trajectories. This dataset helps evaluate the adaptability and precision of beam tracking methods in complex, high-mobility intersections.

Both datasets were created with episodes with a larger number of scenes and a time between scenes of 20ms, which was considered an adequate coherence time for the beams. Tablesummarizes both datasets’ key parameters and characteristics.

SECTION: 
As shown in Fig., the proposed tracking method is realized by a hybrid architecture joining a beam-selection() based on the ResNet framework, designed to extract high-level spatial features from the input data, and a() model designed to handle both theoutput and the historical beam indices.

Themodel is capable of processing two types of input matrices:

LIDAR Matrix: This matrix represents the spatial information captured by LIDAR sensors. The raw data is quantized and transformed into a matrix, where each position functions similarly to a voxel: A value of -1 indicates an obstacle, 0 indicates no scatterers, -2 indicates theposition, and -3 indicates theposition. This processing method follows the approach described in.

Coordinate Matrix based on GNSS: This matrix contains positional data from all objects in the scene, as outlined in. For every scatter (building and cars that are not Tx or Rx) has a value of 1, the transmitter has a value of 10, and the receiver has a gradient of values that start at 3 and grow in the direction in which the vehicle is moving.

The LSTM model processes a concatenated input consisting of theoutput and an observation window of the previousbeam indices. This design allows the LSTM to capture temporal dependencies and trends in the beam selection process, enhancing the overall tracking performance. For the experimental evaluation, all results consider a=andbeams in a ULA array.

SECTION: 
To evaluate the proposed architecture, the models were trained and tested in both scenarios independently, using LIDAR and coordinate matrix as inputs, with the test results shown in Fig..

To assess the performance of the proposed hybrid() for tracking, we compared the top-K accuracy with the beam selection component of theand Salehi’s LIDAR beam selectionfrom. For scenario t001, we compared the tracking, selection, and Salehi architectures using both LIDAR and coordinate (GNSS) data. In the second scenario, the same architectures were tested using only LIDAR data.

For the t001 scenario, the results, presented in Fig., show that both the selection achieves results similar to other works in the literature, with our beam selection achieving 59% of accuracy in top-1 using LIDAR, and Salehi’sachieved 78% accuracy in top-1. However, The hybrid tracking architecture achieved superior performance compared to the other baselines, with the best performance being 84% accuracy in top-1 using LIDAR data in the t001 scenario.

In the t002 scenario, neither beam selection model performed well with LIDAR data. However, Salehi’s neural networkshowed performance slightly closer to the state-of-the-art. Despite this, the tracking model achieved an impressive 97% accuracy in top-1. The difference in performance between scenarios t001 and t002 suggests that the analyzed selectionarchitecture might be site-specific and was not able to handle the non-linear trajectory in the roundabout scenario. This aligns with the findings in, where thewas trained and tested within the same scenario from t001. This indicates that Salehi’s architecture might be highly tailored to a specific type of scenario and struggles to generalize to different environments.

In contrast, the proposeddemonstrated strong adaptability to both scenarios, achieving high accuracy in each and even showing improved performance in t002. This underlines the robustness and generability of the proposed architecture in handling diverse scenarios.

SECTION: 
In this paper, we evaluated the performance ofmodels when using information from LIDAR sensors and GNSS to perform beam tracking in a vehicular B5G scenario.

To process the data fusing LIDAR and GNSS, we proposed a-based model that outperformed the other models.
This model uses information on the previously chosen beam indices to improve the tracking performance.

Another contribution of this work is the development of a new and publicdataset with wireless channels having adequate consistency over time.
This new dataset targets evaluations of beam selection and, especially, beam tracking.

Using this new dataset, the proposed models achieved 84% and 97% of top-1 accuracy for the t001 and t002 scenarios, respectively.

Future works will adopt adata-centricapproach to the beam tracking problem, aiming at increasing the size and diversity of the public datasets to promote innovation and systematic and sensible evaluations.

SECTION: References