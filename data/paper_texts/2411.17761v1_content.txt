SECTION: OpenAD: Open-World Autonomous Driving Benchmark for 3D Object Detection

Open-world autonomous driving encompasses domain generalization and open-vocabulary.
Domain generalization refers to the capabilities of autonomous driving systems across different scenarios and sensor parameter configurations.
Open vocabulary pertains to the ability to recognize various semantic categories not encountered during training.
In this paper, we introduce OpenAD, the first real-world open-world autonomous driving benchmark for 3D object detection.
OpenAD is built on a corner case discovery and annotation pipeline integrating with a multimodal large language model (MLLM).
The proposed pipeline annotates corner case objects in a unified format for five autonomous driving perception datasets with 2000 scenarios.
In addition, we devise evaluation methodologies and evaluate various 2D and 3D open-world and specialized models. Moreover, we propose a vision-centric 3D open-world object detection baseline and further introduce an ensemble method by fusing general and specialized models to address the issue of lower precision in existing open-world methods for the OpenAD benchmark.
Data, toolkit codes, and evaluation codes are released athttps://github.com/VDIGPKU/OpenAD.

SECTION: 1Introduction

With the rapid development of autonomous driving systems, open-world perception has garnered significant and growing attention from the research community.
Open-world perception endeavors to develop a model that exhibits robust performance across novel domains, diverse sensor configurations, and various corner case objects.
The two most pivotal factors in open-world perception are domain generalization and open-vocabulary.

Domain generalization refers to the performance of a model when confronted with new scenarios outside the training domain.
It is a crucial issue that must be addressed to achieve Level 4 autonomous driving.
Within autonomous driving 3D perception, the current methodologies[29,1]for evaluating scenario generalization entail training on a specific dataset and then transferring the trained model to a distinct dataset for subsequent testing.

Open-vocabulary denotes the recognition capability of perception models toward semantic categories that are not present or unlabeled within the training domain.
Open-vocabulary perception serves as the foundation for subsequent inference and planning in autonomous driving systems.
For instance, determining whether an object is collidable, whether it might suddenly move, or whether it signifies that certain surrounding areas are not traversable, necessitates an accurate semantic description of the object in the first place.

Many works are proposed to address these two issues.
However, researchers meet three challenges when developing open-world perception models.
The first challenge in 3D open-world perception for autonomous driving lies in the scarcity of evaluation benchmarks.
Specifically, a unified benchmark for domain transfer evaluation is currently absent, and due to the varying formats of individual datasets, researchers must expend considerable effort on the engineering aspect of format alignment.
Besides, the current 3D perception datasets possess a limited number of semantic categories, lacking effective evaluation for current open-vocabulary 3D perception models.

The second challenge is the difficulty in training open-world perception models due to the limited scales of publicly available 3D perception datasets.
Though some open-world natural language models and 2D perception models have recently leveraged large-scale Internet data for training.
How to transfer these models’ capabilities or 2D data to 3D open-world perception is an important and timely research problem.

The last challenge is the relatively low precision of existing open-world perception models.
While specialized models trained on autonomous driving perception datasets lack the capability to generalize to the open world, they exhibit stronger predictive power for seen categories and achieve good performance.
This indicates that, as the specialized models, the low precision of open-world perception models limits their real-world application.
Consequently, current open-world perception models cannot yet replace specialized models in practice.

To address the aforementioned challenges, we propose OpenAD, an Open-World Autonomous Driving Benchmark for 3D Object Detection.
We align the format of five existing autonomous driving perception datasets, select 2,000 scenes, annotate thousands of corner case objects with MLLMs, and develop open-world evaluation metrics to overcome the first challenge of scarcity of evaluation benchmarks.
Then, we introduce a vision-centric 3D open-world object detection baseline by utilizing existing 2D open-world perception models to resolve the second challenge.
Finally, we further design a fusion method to address the last challenge by leveraging the strengths of open-world perception models (or general models) and specialized models to improve the 3D open-world perception results.

The main contributions of this work are:

We propose an open-world benchmark that simultaneously evaluates object detectors’ domain generalization and open-vocabulary capabilities. To our knowledge, this is the first real-world autonomous driving benchmark for 3D open-world object detection.

We design a labeling pipeline integrated with MLLM, which is utilized to automatically identify corner case scenarios and provide semantic annotations for abnormal objects.

We propose a baseline method for 3D open-world perception by combining 2D open-world models.
Besides, we analyze the strengths and weaknesses of open-world and specialized models, and further introduce a fusion approach to leverage both advantages.

SECTION: 2Related Work

SECTION: 2.1Benchmark for Open-world Object Detection

2D Benchmark.Various datasets[39,24,53,35,21]has been used for 2D open-vocabulary object detection evaluation.
The most commonly used one is LVIS dataset[24], which contains 1,203 categories.

In the autonomous driving area, as shown in Table1, many datasets[26,8,45,20,13,22,50,5,26,34]has been proposed too.
Among them, CODA[34]is a road corner case dataset for 2D object detection in autonomous driving with 1,500 road driving scenes containing bounding box annotations for 34 categories.
However, some datasets only provide semantic segmentation annotations without specific instances or annotate objects as abnormal but lack semantic tags.
Moreover, datasets collected from real-world driving data are on a small scale, while synthetic data from simulation platforms such as CARLA[18]lacks realism, making it difficult to conduct effective evaluations.
In contrast, our OpenAD offers large-scale 2D and 3D bounding box annotations from real-world data for a more comprehensive open-world object detection evaluation.

3D Benchmark.The 3D open-world benchmarks can be divided into two categories: indoor and outdoor scenarios. For indoor scenarios, SUN-RGBD[54]and ScanNet[17]are two real-world datasets often used for open-world evaluation, containing about 700 and 21 categories, respectively.
For outdoor or autonomous driving scenarios,
AnoVox[6]is a synthetic dataset containing instance masks of 35 categories for open-world evaluation.
However, due to limited simulation assets, the quality and instance diversity of the synthetic data are inferior to real-world data.
In addition to AnoVox, existing real-data 3D object detection datasets for autonomous driving[9,46,58,55,21]only contain a few object categories, which can hardly be used to evaluate open-world models.
To address this issue, we propose OpenAD, which is constructed from real-world data and contains 206 different corner-case object categories that appeared in autonomous driving scenarios.

SECTION: 2.22D Open-world Object Detection Methods

To address the out-of-distribution (OOD) or anomaly detection, earlier approaches[64]typically employed decision boundary, clustering, and so forth, to discover OOD objects.
Recently methods[31,71,44,42,60,62,68,35,56,70,15,59,23]employ text encoders,i.e.CLIP[52], to align text features of corresponding category labels with the box features.
Specifically, OVR-CNN[68]aligns the image features with caption embeddings.
GLIP[35]unifies object detection and phrase grounding for pre-training.
OWL-ViT v2[47]uses a pretrained detector to generate pseudo labels on image-text pairs to scale up detection data for self-training.
YOLO-World[15]adopts a YOLO-type architecture for open-vocabulary detection and achieves good efficiency.
However, all these methods require predefined object categories during inference.

More recently, some open-ended methods[16,66,40]propose to utilize natural language decoders to provide language descriptions, which enables them to generate category labels from RoI features directly.
More specifically, GenerateU[16]introduces a language model to generate class labels directly from regions of interest.
DetClipv3[66]introduced an object captioner to generate class labels during inference and image-level descriptions for training.
VL-SAM[40]introduces a training-free framework with the attention map as prompts.

SECTION: 2.33D Open-world Object Detection Methods

In contrast to 2D open-world object detection tasks, 3D open-world object detection tasks are more challenging due to the limited training datasets and complex 3D environments.
To alleviate this issue, most existing 3D open-world models bring power from pretrained 2D open-world models or utilize abundant 2D training datasets.

For instance, some indoor 3D open-world detection methods like OV-3DET[43]and INHA[30]use a pretrained 2D object detector to guide the 3D detector to find novel objects.
Similarly, Coda[10]utilizes 3D box geometry priors and 2D semantic open-vocabulary priors to generate pseudo 3D box labels of novel categories.
FM-OV3D[69]utilizes stable diffusion to generate data containing OOD objects.
As for outdoor methods, FnP[19]uses region VLMs and a Greedy Box Seeker to generate annotations for novel classes during training.
OV-Uni3DETR[57]utilizes images from other 2D datasets and 2D bounding boxes or instance masks generated by an open-vocabulary detector.

However, these existing 3D open-vocabulary detection models require predefined object categories during inference. To address this issue, we introduce a vision-centric open-ended 3D object detection method, which can directly generate unlimited category labels during inference.

SECTION: 3Properties of OpenAD

SECTION: 3.1Scenes and Annotation

The 2,000 scenes in OpenAD are carefully selected from five large-scale autonomous driving perception datasets: Argoverse 2[58], KITTI[21], nuScenes[9], ONCE[46]and Waymo[55], as illustrated in Figure2.
These scenes are collected from different countries and regions, and have different sensor configurations.
Each scene has the temporal camera and LiDAR inputs and contains at least one corner case object that the original dataset has not annotated.

For 3D bounding box labels, we annotate 6,597 corner case objects across these 2,000 scenarios, combined with the annotations of 13,164 common objects in the original dataset, resulting in 19,761 objects in total.
The location and size of all objects are manually annotated using 3D and 2D bounding boxes, while their semantics categories are labeled with natural language tags, which can be divided into 206 classes.
We illustrate some corner case objects in  Figure1. OpenAD encompasses both abnormal forms of common objects, such as bicycles hanging from the rear of cars, cars with doors open, and motorcycles with rain covers, as well as uncommon objects, including open manholes cover, cement blocks, and tangled wires scattered on the ground.

Concurrently, we have annotated each object with a “seen/unseen” label, indicating whether the categories of the objects have appeared in the training set of each dataset.
This label is intended to facilitate the evaluation process by enabling a straightforward separation of objects that the model has encountered (seen) and those it has not (unseen), once the training dataset is specified.
Moreover, we offer a toolkit code that consolidates scenes from five original datasets into a unified format, converts them into OpenAD data, and facilitates the loading and visualization process.

SECTION: 3.2Evaluation Metrics

OpenAD provides evaluations for both 2D and 3D open-world object detection.

Average Precision (AP) and Average Recall (AR).The calculation of AP and AR depends on True Positive (TP).
In OpenAD, the threshold of TP incorporates both positional and semantic scores.
An object prediction is considered a TP only if it simultaneously meets both the positional and semantic thresholds.
For 2D object detection, in line with COCO, Intersection over Union (IoU) is used as the positional score.
We use the cosine similarity of features from the CLIP model as the semantic score.
When calculating AP, IoU thresholds ranging from 0.5 to 0.95 with a step size of 0.05 are used, along with semantic similarity thresholds of 0.5, 0.7, and 0.9.

For 3D object detection, the center distance is adopted as the positional score following nuScenes, and we use the same semantic score as the 2D detection task.
Similar to nuScenes, we adopt a multi-threshold averaging method for AP calculation.
Specifically, we compute AP across 12 thresholds, combining positional thresholds of 0.5m, 1m, 2m, and 4m with semantic similarity thresholds of 0.5, 0.7, and 0.9, and then average these AP values.

The same principle applies to calculating Average Recall (AR) for 2D and 3D object detection tasks.
Both AP and AR are calculated only for the top 300 predictions.

Average Translation Error (ATE) and Average Scale Error (ASE).Following nuScenes, we also evaluate the prediction quality of TP objects using regression metrics.
The Average Translation Error (ATE) refers to the Euclidean center distance, measured in pixels for 2D or meters for 3D.
The Average Scale Error (ASE) is calculated asafter aligning the centers and orientations of the predicted and ground truth objects.

In/Out Domain & Seen/Unseen AR.To evaluate the model’s domain generalization ability and open-vocabulary capability separately, we calculate the AR based on whether the scene is within the training domain and whether the object semantics have been seen during training.
The positional thresholds for this metric are defined as above, whereas the semantic similarity thresholds are fixed at 0.9.

SECTION: 4Construction of OpenAD

OpenAD is inspired by the CODA[34]dataset, which focuses on 2D corner cases in autonomous driving.
However, certain objects, such as cables or nails close to the road surface, and signboards hanging on walls, cannot be detected solely by LiDAR.
Therefore, unlike CODA’s LiDAR-based pipeline, we propose a vision-centric semi-automated annotation pipeline, as shown in Figure3.

We use an MLLM Abnormal Filter to identify scenes containing corner cases within the validation and test sets of five autonomous driving datasets, followed by manual filtering.
After that, we annotated the corner case objects with 2D bounding boxes.

For objects with relatively complete 3D geometry formed by point clouds, we adopt a methodology similar to CODA by employing point-cloud clustering algorithms[7].
We then utilize camera parameters to project 2D bounding boxes into the point cloud space and identify the corresponding clusters.
Finally, the bounding boxes are manually corrected.
For objects that are difficult to detect through point-cloud clustering, we manually annotate 3D bounding boxes by referencing multi-view images.

For category labels, we send images with 2D bounding boxes to an MLLM for semantic annotation and indicate for each object whether its category has been seen in each dataset.
To select the best MLLM and prompts for object recognition, we manually select 30 challenging annotated image samples and evaluate the accuracy of each MLLM and prompt.
We use GPT-4V[48], Claude 3 Opus[2], and InternVL 1.5[14], with InternVL exhibiting the best performance.
Our experiments also reveal that closed image prompts, such as 2D bounding boxes or circles, yield the best results, whereas marking the object of inquiry on the image with arrows yields slightly inferior results.
The final MLLM and prompt achieve an accuracy rate of approximately 65% on the 30 challenging samples and around 90% on the entire data.
Objects like open manholes and wires falling on the road are difficult to identify for existing MLLMs.

Note that though we have utilized tools such as MLLM to automate some stages as much as possible to reduce manual workload, we have also incorporated manual verification into each stage to ensure the accuracy of annotations.

SECTION: 5Baseline Methods of OpenAD

SECTION: 5.1Vision-Centric 3D Open-ended Object Detection

Due to the limited scale of existing 3D perception data, it is challenging to directly train a vision-based 3D open-world perception model.
We utilize existing 2D models with strong generalization capabilities to address this issue and propose a vision-centric baseline for 3D open-world perception.

As illustrated in Figure4, an arbitrary existing 2D open-world object detection method is initially employed to obtain 2D bounding boxes and their corresponding semantic labels.
Simultaneously, the image feature maps generated by the image encoder of the 2D model are cached.
Subsequently, a 2D-to-3D Bbox Converter, which combines multiple features and a few trainable parameters, is introduced to transform 2D boxes into 3D boxes.

Specifically, we use existing depth estimation models, such as ZoeDepth[4], DepthAnything[65], and UniDepth[49], to obtain the depth map of the cropped image by the 2D box.
We also include an optional branch that utilizes LiDAR point clouds and a linear fitting function to refine the depth map by projecting point clouds onto the image.
Simultaneously, to eliminate regions within the 2D bounding box that do not belong to the foreground object, we utilize Segment Anything Model[33](SAM) to segment the object with the 2D box as the prompt, yielding a segmentation mask.
After that, we can construct pseudo point clouds for the segmentation mask with its pixel coordinates, depth map, and camera parameters.
We project the pseudo point cloud onto the feature map and depth map, and features are assigned to each point through interpolation.
Then, we adopt PointNet[51]to extract the featureof the pseudo point clouds.
Meanwhile, the depth map and feature map within the 2D bounding box are concatenated along the channel dimension, and its featureis derived through convolution and global pooling.
Finally, we utilize an MLP to predict the object’s 3D bounding box with the concatenated features ofand.

In this baseline, only a few parameters in the 2D-to-3D Bbox Converter are trainable. Thus, the training cost is low.
In addition, during the training, each 3D object serves as a data point for this baseline, allowing for the straightforward construction of multi-domain dataset training.

SECTION: 5.2General and Specialized Models Fusion

In experiments, we have found that existing open-world methods or general models are inferior to close-set methods or specialized models in handling objects belonging to common categories, but they exhibit stronger domain generalization capabilities and the ability to deal with corner cases.
That is to say, existing general and specialized models complement each other.
Hence, we leverage their strengths and propose a fusion baseline by combining the prediction results from the two types of models.
Specifically, we align the confidence scores of the two types of models and perform non-maximum suppression (NMS) with dual thresholds,i.e., IoU and semantic similarity, to filter duplicates.

SECTION: 6Experiments

SECTION: 6.1Evaluation Details

For specialized models that can only predict common categories, we directly match their prediction results with the corresponding categories and sort them according to their confidence scores.

For 2D open-vocabulary methods, which need a predefined object category list from users as additional inputs to detect corresponding objects, we take the union of the categories from five datasets and incorporate two additional open-vocabulary queries,i.e., “object that affects traffic” and “others”, into it.
We adopt OWLv2-CLIP-L/14-ST+FT, YOLO-Worldv2-XL, GLIP-L, and GroundingDINO-B for OWL-ViT v2[47], YOLO-World v2[15], GLIP[35], and GroundingDino[42], respectively.

2D open-ended methods can directly provide bounding boxes and corresponding natural language descriptions, enabling direct evaluation for OpenAD.
We employ the “vg-grit5m” version for GenerateU[16].

For 3D Open-vocabulary methods, the original version of Find n’Propagate[19]utilizes a 2D detector trained on the full nuScenes dataset to provide pseudo-labels.
For a fair comparison, we employ YOLO-world v2 to provide the pseudo-labels instead.

For the 3D open-ended baselines we proposed, the 2D-to-3D Bbox Converter is trained on nuScenes.
We use GenerateU[16]and YOLO-World[15]as the 2D detector, Depth Anything[65]as the depth estimation model, and SAM[33]as the segmentation model.
All these 2D models are frozen without any fine-tuning.

SECTION: 6.2Main Results

As shown in Tables2and3, we conduct evaluations on various 2D and 3D object detection models, including 2D and 3D open-world models, specialized models, and our baselines.

The results show that current open-world models, irrespective of being 2D or 3D detectors, tend to predict objects unrelated to driving (such as the sky) or to make repeated predictions for different parts of the same object, resulting in low precision and AP.
Nevertheless, these models demonstrate good domain generalization and open-vocabulary capabilities, which are lacking in current specialized models.
Note that our proposed ensemble baselines can effectively combine the advantages of open-world and specialized models, achieving favorable performance in both seen and unseen domains and categories.
In addition, in Table3, our proposed vision-centric baseline for 3D open-world object detection leverages the capabilities of 2D open-world models.
Specifically, by harnessing the open-world capabilities of Yolo-world v2, our method obtains 0.58 AP and 6.2 AR improvement compared to Find n’ Propagate.

Moreover, we observed that the issue of overfitting is more pronounced for 3D object detection models on datasets such as nuScenes.
Some models perform superior in-domain benchmarks but show worse domain generalization ability.
For instance, SparseBEV, compared to methods based on Lift-Splat-Shot, achieves impressive in-domain results, with its in-domain AR even surpassing those of LiDAR-based methods.
However, SparseBEV’s domain generalization capability is relatively poor.
Models with increased parameters by enlarging the backbone, including BEVStereo and SparseBEV, show more severe overfitting issues.
These results reveal the limitations of in-domain benchmarks like nuScenes.
In contrast, augmenting the parameter count through utilizing BEVFormer v2 or HENet simultaneously enhances both in-domain and out-domain Recall, indicating an inherent improvement in the methodology.
Therefore, even for specialized models trained on a single domain, evaluating them on OpenAD benchmarks remains meaningful.

Furthermore, as shown in Figure5, we provide visualization samples for some methods.
Objects enclosed by orange bounding boxes belong to unseen categories in nuScenes. Recognition of these objects relies on open-world models.
In contrast, specialized models exhibit significant advantages for common objects, especially for distant objects.

SECTION: 6.3Ablations of Proposed Baselines

We conduct ablation studies for the proposed baselines, as shown in Table4.
We find that additional Pseudo Point Cloud inputs bring 9.9 mAR.
In addition, replacing MLP with unlearnable PCA methods decreases the performance by a large margin, from 45.1 mAR to 27.3 mAR.
These results show that the simple MLP can learn to complete the boundaries of objects from the datasets and predict more accurate 3D boxes.

SECTION: 7Conclusion

In this paper, we introduce OpenAD, the first open-world autonomous driving benchmark for 3D object detection.
OpenAD is built on a corner case discovery and annotation pipeline that integrates with a multimodal large language model.
The pipeline aligns five autonomous driving perception datasets in format and annotates corner case objects for 2000 scenarios.
In addition, we devise evaluation methodologies and analyze the strengths and weaknesses of existing open-world perception models and autonomous driving specialized models.
Moreover, addressing the challenge of training 3D open-world models, we proposed a baseline method for 3D open-world perception by combining 2D open-world models.
Furthermore, we introduce a fusion baseline approach to leverage the advantages of open-world models and specialized models.

Through evaluations conducted on OpenAD, we have observed that existing open-world models are still inferior to specialized models within the in-domain context, yet they exhibit stronger domain generalization and open-vocabulary abilities.
It is worth noting that the improvement of certain models on in-domain benchmarks comes at the expense of their open-world capabilities, while this is not the case for other models.
This distinction cannot be revealed solely by testing on in-domain benchmarks.

We hope that OpenAD can help develop open-world perception models that surpass specialized models, whether in the same domain or across domains, and whether for semantic categories that have been seen or unseen.

SECTION: References