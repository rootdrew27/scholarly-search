SECTION: Investigating the Contextualised Word Embedding Dimensions Specified for Contextual and Temporal Semantic Changes

The\AcpSCWE encode semantic changes of words within thecontextualised word embedding(CWE) spaces.
Despite the superior performance of\AcpSCWE in contextual/temporalsemantic change detection(SCD) benchmarks, it remains unclear as to how the meaning changes are encoded in the embedding space.
To study this, we compare pre-trainedCWEsand their fine-tuned versions on contextual and temporal semantic change benchmarks underPrincipal Component Analysis(PCA) andIndependent Component Analysis(ICA) transformations.
Our experimental results reveal
(a) although there exist a smaller number of axes that are specific to semantic changes of words in the pre-trainedCWEspace, this information gets distributed across all dimensions when fine-tuned, and
(b) in contrast to prior work studying the geometry ofCWEs, we find thatPCAto better represent semantic changes thanICAwithin the top 10% of axes.
These findings encourage the development of more efficientSCDmethods with a small number ofSCD-aware dimensions.111Source code is available athttps://github.com/LivNLP/svp-dims.

Investigating the Contextualised Word Embedding Dimensions Specified for Contextual and Temporal Semantic Changes

Taichi AidaTokyo Metropolitan Universityaida-taichi@ed.tmu.ac.jpDanushka BollegalaUniversity of Liverpooldanushka@liverpool.ac.uk

SECTION: §​ 1Introduction

Meaning of a word is a dynamic phenomenon that is bothcontextual(i.e. depends on the context in which the word is used)Pilehvar and
Camacho-Collados (2019)as well astemporal(i.e. the meaning of a word can change over time)Tahmasebi et al. (2021).
A large body of methods have been proposed to represent the meaning of a word in a given contextDevlin et al. (2019); Conneau et al. (2020); Zhou and Bollegala (2021); Rachinskiy and Arefyev (2021); Periti et al. (2024), or within a given time periodHamilton et al. (2016); Rosenfeld and Erk (2018); Aida et al. (2021); Rosin et al. (2022); Aida and
Bollegala (2023b); Tang et al. (2023); Fedorova et al. (2024).
In particular,sense-aware contextualised word embeddingssuch as XL-LEXEMECassotti et al. (2023)obtained by fine-tuningmasked language modelssuch as XLM-RoBERTaConneau et al. (2020)onWord-in-Context(WiC)Pilehvar and
Camacho-Collados (2019)have reported superior performance inSCDbenchmarksCassotti et al. (2023); Aida and Bollegala (2023a); Periti and Tahmasebi (2024); Aida and Bollegala (2024), implying that semantic changes can be accurately inferred fromSCWEs.

Despite the empirical success, to the best of our knowledge, no prior work has investigatedwhether there are dedicated dimensions in the XL-LEXEME embedding space specified for the semantic changes of the wordsit represents.
In this paper, we study this problem from two complementary directions.
First, in§​ 3, we investigate the embedding dimensions specific to the contextual semantic changes of words usingWiCbenchmarksPilehvar and
Camacho-Collados (2019); Raganato et al. (2020); Martelli et al. (2021); Liu et al. (2021)as the evaluation task.
Second, in§​ 4, we investigate the embedding dimensions specific to the temporal semantic changes of words on SemEval-2020 Task 1Schlechtweg et al. (2020)benchmark.
In each setting, we compare pre-trainedCWEsand theSCWEsobtained by fine-tuning onWiCusingPCAandICA, which have been used in prior work investigating dimensions inCWEsYamagiwa et al. (2023).
Our investigations reveal several interesting novel insights that will be useful when developing accurate and efficient low-dimensionalSCDmethods as follows.

PCAdiscovers contextual/temporal semantic change-aware axes within the top 10% of the transformed axes better thanICA.

In pre-trained embeddings, we identify a small number of axes that are specified for contextual/temporal semantic changes, while such axes are uniformly distributed in the fine-tuned embeddings.

Semantic change aware dimensions report comparable or superior performance over using all dimensions inSCDbenchmarks.

SECTION: §​ 2Task Description

In this section, we explain the two types of semantic changes of words considered in the paper:
(a) contextual semantic changes and (b) temporal semantic changes.

SECTION: Contextual Semantic Change Detection Task

involves predicting whether the meaning of a word in a given pair of sentences are the samePilehvar and
Camacho-Collados (2019). For example, an ambiguous word can express different meanings in different contexts, which is considered under contextual semantic changes. Models are required to make a prediction for each pair of sentences.

SECTION: Temporal Semantic Change Detection Task

involves predicting the meanings of a word in given sets of sentences across different time periodsSchlechtweg et al. (2020). A word that was used in a different meaning in the past can be associated with novel meanings later on, which is considered as a temporal semantic change of that word. Models predict whether the meaning of the word has changed over time by comparing the given sets of sentences.

SECTION: Models

For theContextual Semantic Change Detection Task, contextual word embeddingsDevlin et al. (2019); Conneau et al. (2020)are the primary choice, as they effectively capture word meanings based on sentence context. For theTemporal Semantic Change Detection Task, both staticKim et al. (2014); Kulkarni et al. (2015); Hamilton et al. (2016); Yao et al. (2018); Aida et al. (2021)and contextualRosenfeld and Erk (2018); Kutuzov and Giulianelli (2020); Laicher et al. (2021); Aida and
Bollegala (2023b)embeddings can be applied. Notably, sense-aware contextual embeddings trained specifically for contextual semantic change tasks have achieved superior performance, demonstrating their broader applicabilityCassotti et al. (2023); Aida and Bollegala (2024).

Both types of semantic changes are common and even the same word can undergo both types of semantic changes as shown inTable 1for the wordplane.
The contextual semantic change task requires models to be sensitive to the context within just two given sentences, whereas
the temporal semantic change task requires models to account for the semantic changes of words across two different time periods.

SECTION: §​ 3Contextual Semantic Changes

We first investigate the existence of axes specific to contextual semantic changes.
Recall that XL-LEXEME is fine-tuned from XLM-RoBERTa onWiCdatasets.
Therefore, the emergence of any semantic change-aware axes due to fine-tuning can be investigated using contextual semantic change benchmarks.
We use the test split of the EnglishWiCPilehvar and
Camacho-Collados (2019), XL-WiC(Raganato et al.,2020), MCL-WiC(Martelli et al.,2021), and AM2iCo(Liu et al.,2021)datasets for evaluations.222Due to the page limitations, results for other datasets than the English WiC are shown inAppendix B.Data statistics are inAppendix A.

SECTION: RQ1: When do the contextualSCD-aware axes emerge?

To investigate whether contextual semantic change-aware axes were already present in the pre-trainedCWEs, or do they emerge during the fine-tuning step, for each sentence-pair inWiCdatasets, we compute the difference between the two target word embeddings obtained from the pre-trained XLM-RoBERTa (CWEs) and the fine-tuned XL-LEXEME (SCWEs).
To obtain the sets of target word embeddings, we followCassotti et al. (2023)by using a Sentence-BERTReimers and Gurevych (2019)architecture.
We conduct this analysis for the non-transformed original axes (indicated asRawhere onwards), as well as for thePCA/ICA-transformed axes in order to investigate whether such transformations can discover the axes specified for contextual semantic changes as proposed byYamagiwa et al. (2023).333As inYamagiwa et al. (2023), we used PCA and FastICA provided in scikit-learnhttps://scikit-learn.org/.In this paper,PCA/ICA-transformed axes are sorted by the experimental variance ratio/skewness, and this process is consistently applied wherePCAorICAis used.
If a particular axis is sensitive to contextual semantic changes, it will take similar values in the two target word embeddings, thus having a near-zero value in their subtraction.

To address RQ1, we visualised the difference vectors for sentence pairs where the target word takes thesamemeaning in the two sentences (True) vs.differentmeanings (False).
This visualisation was performed by following steps: (a) we prepared Raw or PCA/ICA-transformed axes; (b) for each WiC instance, which contains two sentences and a label, we calculated the difference between pair of sentences; (c) we normalised each axis (min=0 and max=1) for visualisation purposes.

As shown inFigure 1, we see thatthe axes encoding contextual semantic changes are not obvious in the originalCWEsafter pre-training (Figure 1), but materialise during the fine-tuning process (Figure 1).Similar trends are observed withPCA-transformations
(s1and1), whereasICAshows contrasting results (s1and1).
In contrast to prior recommendations for usingICAfor analysingCWEspacesYamagiwa et al. (2023), we findICAto be less sensitive to contextual semantic changes of words.
Interestingly, similar results have been shown in other languages/datasets(Appendix B).444Our findings do not aim to claim the superiority of PCA over ICA but to explore the existence of task-specific axes. Experimental results show that for semantic change tasks, PCA provides more task-related axes because (a) PCA orders axes by importance (eigenvalue), making task-related axes more accessible, and (b) ICA-transformed axes require external sorting method based on skewness rather than importance. Prior research indicates that ICA can capture topic-related axesYamagiwa et al. (2023), suggesting that ICA may still hold potential for obtaining task-related axes. Further refinement of the approach remains as future research.

SECTION: RQ2: Can top-PCA/ICA-transformed axes capture contextual semantic changes?

Yamagiwa et al. (2023)discovered thatICA-transformed axes represent specific concepts and their linear combinations could represent more complex concepts (e.g.carsitalianferrari).
Based on this finding, we investigate whether a combination of top-axes can collectively represent contextual semantic changes of words.
Specifically, we select the top-of the axes to represent a target word embedding.
We then compute the Euclidean distance betweenCWEsof the target word in each sentence for every test sentence-pair in theWiCdatasets.
We predict the target word to have the same meaning in the two sentences, if the Euclidean distance is below a threshold value.
We vary this threshold and reportArea Under the Curve(AUC) ofReceiver Operating Characteristic(ROC) curves, where higherAUCvalues are desirable.
InFigure 2, we show results for topof thePCA/ICA-transformed axes and compare against the baseline that usesallof theRawdimensions.

For the pre-trainedCWEs(Figure 2), we see thatRawreports slightly betterAUCthanPCA, but when fine-tuned (Figure 2)PCAmatchesRaweven by using less than 10% of the axes.
On the other hand,ICAreports lowerAUCvalues than bothRawandPCAin both models.
These results indicate thatPCAis better suited for discovering axes specified for contextual semantic changes thanICA.
We suspect that althoughICAis able to retrieve concepts such as topicsYamagiwa et al. (2023), it is less fluent when discovering task-specific axes that require the consideration of different types of information.
In conclusion, (1) contextual semantic change-aware axes emerge during fine-tuning, and (2) they are discovered byPCAeven within 10% of the principal components.
Notably, in other languages/datasets, similar trends have been observed (Appendix B).
These results suggest thatcontextual semantic change-aware dimensions can be observed within 10% of thePCA-transformed axesacross different languages.

SECTION: §​ 4Temporal Semantic Changes

In contrast to contextualSCD, temporalSCDconsiders the problem of predicting whether a target wordrepresents different meanings in two text corporaand, sampled at different points in time.
For evaluations, we use the SemEval-2020 Task 1 dataset555Data statistics are inAppendix A.Schlechtweg et al. (2020), which contains a manually rated set of target words for their temporal semantic changes in English, German, Swedish, and Latin.666Due space limitations, results for languages other than English are shown inAppendix B.

SECTION: RQ3: Can top-PCA/ICA-transformed axes capture temporal semantic changes?

Similar toFigure 2, we investigate whetherPCA/ICAcan discover axes specified for temporal semantic changes by considering the top-% of axes for.
We calculate the semantic change score ofas the average pairwise Euclidean distance over the two sets of sentences containing the target wordinandas conducted in previous workKutuzov and Giulianelli (2020); Laicher et al. (2021); Cassotti et al. (2023).
Finally,is predicted to have its meaning changed betweenand, if its semantic change score exceeds a pre-defined threshold.
We vary this threshold and plotROCinFigure 3.

In pre-trainedCWEs, we can see that the use of the top 5% to 20% axes transformed byPCAis more effective in temporal semantic change detection than when all of theRawdimensions are used (Figure 3).
On the other hand, in fine-tunedSCWEs,Figure 3indicates thatPCA-transformed axes achieve the sameAUCscores asRaw, similar to the contextual semantic change (Figure 2).
Similar to the observation in contextual semantic change,ICAreturns the lowest performance.

To further investigate whether the topPCA/ICAaxes can explain thedegreeof temporal semantic change, we measure the Spearman correlation between the semantic change scores and human ratings available in the SemEval-2020 Task 1 following the standard evaluation protocol for this taskRosin et al. (2022); Rosin and Radinsky (2022); Aida and
Bollegala (2023b); Cassotti et al. (2023); Periti and Tahmasebi (2024); Aida and Bollegala (2024).
As shown inFigure 4for the pre-trainedCWEs(Figure 4), using only 10% of the axes,PCAoutperformsRawthat uses all axes.
Moreover, for the fine-tunedSCWEs(Figure 4), using only 10% of the axesPCAachieves the same performance asRaw.
However,ICAconsistently underperforms in both pre-trained and fine-tuned settings.
Importantly, we see similar trends in other languages (Appendix B).
These results suggest thattemporal semantic change-aware dimensions can also be observed within 10% ofPCA-transformed axesacross different languages.

SECTION: §​ 5Conclusion

We found that there exists a smaller number of axes that encode contextual and temporal semantic changes of words inMLMs, which are accurately discovered byPCA.
These findings have several important practical implications.
First, it shows thatMLMscan be compressed into efficient and accurate lower-dimensional embeddings when used forSCDtasks.
Second, it suggests the possibility of efficiently updating a pre-trainedMLMto capture novel semantic associations of words since theMLMwas first trained, by updating only a smaller number of dimensions.

SECTION: Limitations

In this paper, we limited experiments to XLM-RoBERTa basedMLMmodels.
These models are all fine-tuned onWiCdatasets and have reportedstate-of-the-art(SoTA) performance inSCDbenchmarks.
We consider it would be important to further validate the findings reported in this paper
using other embedding models and across multiple downstream applications.

SECTION: Ethical Considerations

In this paper, we focus on investigating the existence of dedicated dimensions capturing contextual/temporal semantic changes of words.
For the best of our knowledge, no ethical issues have been reported for theWiCandSCDdatasets we used in our experiments.
On the other hand, we also used publicly available pre-trained/fine-tunedMLMs, some of which are known to encode and potentially amplify unfair social biases(Basta et al.,2019).
Whether such social biases are influenced by the dimension selection methods we consider in the paper must be carefully evaluated before deploying anyMLMsin downstream applications.

SECTION: Acknowledgements

Taichi Aida would like to acknowledge the support by JST, the establishment of university fellowships towards the creation of science technology innovation, Grant Number JPMJFS2139.

SECTION: References

SECTION: Appendix AData Statistics

Full statistics of contextual and temporalSCDbenchmarks are shown inTable 2andTable 3.777WiC, XL-WiC, and MCL-WiC are licensed under the Creative Commons Attribution-NonCommercial 4.0 License, while AM2iCo and SemEval-2020 Task 1 are licensed under the Creative Commons Attribution 4.0 International License.

SECTION: Appendix BFull Results

In this section, we present the full results of contextual and temporalSCDtasks.
For the contextualSCD, visualisations of instances in all datasets are as follows: XLWiC (Figure 5,Figure 6, andFigure 7), MCLWiC (Figures8,9,10,11, and12), and AM2iCo (Figures13,14,15,16,17,18,19,20,21, and22).
Similar to§​ 3, the contextual semantic change-aware axes emerged after the fine-tuning process.
Moreover, full results related to the prediction task are as follows: XLWiC (Figure 23), MCLWiC (Figure 24andFigure 25), AM2iCo (Figure 26,Figure 27, andFigure 28).
As shown in§​ 3, 10%PCA-transformed axes are able to obtain contextual semantic change-aware dimensions.

On the other hand, for the temporalSCD, results for other languages (German, Swedish, and Latin) are shown inFigure 29andFigure 30.
Similar to§​ 4, temporal semantic change-aware dimensions are observed within 10%PCA-transformed axes.
However, there are some difficulties in obtaining these dimensions by PCA-transformed axes with insufficient pretraining data (Swedish)Conneau et al. (2020)or lack of supervision for fine-tuning (Latin) shown inTable 2.
In those cases, the use ofICA-transformed axes proved to be effective.
More detailed analysis and understanding of those axes for interpretability will be addressed in future work.