SECTION: Cascaded Dual Vision Transformer for Accurate Facial Landmark Detection
Facial landmark detection is a fundamental problem in computer vision for many downstream applications. This paper introduces a new facial landmark detector based on vision transformers, which consists of two unique designs: Dual Vision Transformer (D-ViT) and Long Skip Connections (LSC). Based on the observation that the channel dimension of feature maps essentially represents the linear bases of the heatmap space, we propose learning the interconnections between these linear bases to model the inherent geometric relations among landmarks via channel-split ViT. We integrate such channel-split ViT into the standard vision transformer (, spatial-split ViT), forming our Dual Vision Transformer to constitute the prediction blocks. We also suggest using long skip connections to deliver low-level image features to all prediction blocks, thereby preventing useful information from being discarded by intermediate supervision. Extensive experiments are conducted to evaluate the performance of our proposal on the widely used benchmarks,, WFLW, COFW, and 300W, demonstrating that our model outperforms the previousacross all three benchmarks.

SECTION: Introduction
Facial landmark detection involves locating a set of predefined key points on face images, serving as a fundamental step for supporting various high-level applications, including face alignment, face recognition, face parsingand 3D face reconstruction.

Early approaches of landmark detection relied on statistical models, but have been surpassed by modern landmark detectors that use convolutional neural networks (i.e., CNN). CNNs learn a transformation between image features and a set of 2D coordinates or regress heatmaps to represent the probability distribution of landmarks. Besides, these detectorsusually employ cascaded networks in conjunction with intermediate supervision to progressively refine the predictions, producing remarkable advances.
To capture the intrinsic geometric relationships among landmarks for accurate predictions, researchers have also developed various effective methods using Graph Convolutional Networks (GCNs), Graph Attention Networks (GATs)or Transformers.
However, these methods primarily utilize patch-based image features to learn spatial relations among landmarks.

In this paper, we have developed a vision transformer-based model architecture and modeled the intrinsic geometric relationships among landmarks by computing the correlations between the linear bases of heatmap space, enabling us to achieve new SOTA results across all three benchmarks.
Following the design paradigm of cascaded networks, we repeat prediction blocks in conjunction with intermediate supervisions. Specially, in our architecture, we propose two unique designs: Dual Vision Transformer (D-ViT), which constitutes the prediction blocks, and Long Skip Connections (LSC), the strategy for connecting these blocks.

The standard vision transformerdiscretizes images or feature maps into small patches, then rearranges them into a sequence to extract global and local image features. However, it lacks the ability to model the underlying geometric characteristics of landmarks. To address this, we propose incorporating the channel-split ViT to model the inherent relationships among landmarks. Specifically, the prediction block outputs a feature mapfor intermediate supervision, which can be split along the channels into. Therefore, when considering using convolution operations to regress the feature mapinto the heatmaps, the heatmap for each landmark is actually a linear combination of. To put it another way, the channel dimension of feature maps essentially represents the linear bases of the heatmap space. Based on such insight, we take advantage of the transformer architecture to learn underlying relationships among these linear bases, allowing for adaptive computation of their interconnections through the multi-head self-attention mechanism. Finally, the spatial-split ViT and the channel-split ViT together form our Dual Vision Transformer (D-ViT).

Following the classic stacked Hourglasses networks, which utilize residual connections between two sequential hourglass architectures, we first also apply the same connection strategy to boost the network. However, we find that when the number of prediction blocks exceeds 4, the detection performance is instead diminished by deeper model architecture. As far as we know, this is caused by intermediate supervisions, which can lead to the inadvertent discard of useful information. To handle this problem, we suggest using long skip connections to deliver low-level image features to all prediction blocks, thereby making deeper network architectures feasible.

We evaluate the performance of our proposal on the widely used benchmarks,i.e., WFLW, COFW, and 300W. Extensive experiments
demonstrate that our approach outperforms the previous state-of-the-art methods and achieves a new SOTA across all three datasets. The main contributions can be summarized as follows:

1) We introduce a facial landmark detector based on our unique dual vision transformer (D-ViT), which is able to effectively capture contextual image features and underlying geometric relations among landmarks via spatial-split and channel-split features.

2) To avoid losing useful information due to intermediate supervision and make deeper network architectures feasible, we propose a unique connection strategy,i.e., Long Skip Connections, to transmit low-level image features from ResNet to each prediction block.

3) Extensive experiments are conducted to evaluate our approach, demonstrating its good generalization ability and superior performance compared to existing SOTAs across three publicly available datasets (i.e., WFLW, COFW, and 300W). Our code will be released for reproduction.

SECTION: Related Work
In the literature on facial landmark detection, deep learning methods can generally be divided into two categories: coordinate regression-based method and heatmap-based method.

methods directly regress facial landmarks through learning the transformation between image features and landmark coordinates. These methodsare typically designed in a coarse-to-fine manner, employing multiple prediction stages or cascaded network modules to gradually refine the landmark coordinates.
In these methods, ResNetcombined with wing lossis commonly used as the backbone to extract image features and regress landmark coordinates.
DTLDadopts pretrained ResNetto extract multi-scale image features and apply cascaded transformers to gradually refine landmarks by predicting offsets.
Considering the underlying geometric relationships among landmarks, SDLand SDFLutilize graph convolutional networks to explicitly capture structural features.
Besides, SLPTintroduces a sparse local patch transformer to learn the intrinsic landmark relations, which extracts the representation (i.e., embedding) of each individual landmark from the corresponding local image patch and processes them based on the attention mechanism.

methodspredict a heatmap for each landmark, where the point with the highest intensity, or its vicinity, is considered the optimal position of the landmark.
In these methods, UNetand the stacked hourglasses networkare frequently used as backbone architectures.
HRNetshowed remarkable results through the combination of multi-scale image features. Adaptive wing losswas proposed as the loss function for heatmap regression to balance the normal and hard cases. Besides, the predictions can be further improved by integrating coordinate encoding with CoordConv. However, heatmap-based methods usually suffer from discretization-induced errors, since the heatmap size is usually much smaller than the input image. Consequently, various methods have been developed to alleviate discretization-induced errors, including the usage of heatmap matching to improve accuracy, continuous heatmap encoding and decoding method, differential spatial to numerical transform (DSNT)and Heatmap in Heatmap (HIH)for subpixel coordinates.
Additionally, LABsuggests predicting the facial boundary as a geometric constraint to help regress the landmark coordinates. LUVLpredicts not only the landmark positions but also the uncertainty and probability of visibility for better performance. SPIGAcombines CNN with cascaded Graph Attention Networks to jointly predict head pose and facial landmarks. ADNetintroduces anisotropic direction loss (ADL) and anisotropic attention module (AAM) to address ambiguous landmark labeling. STARLossadaptively suppresses the prediction error in the first principal component direction to mitigate the impact of ambiguity annotation during the training phase. LDEQemploys Deep Equilibrium Modelsto detect face landmarks and achieves state-of-the-art results on the WFLW benchmark. Recently, FRAlearned a general self-supervised facial representation for various facial analysis tasks, achieving state-of-the-art results on the 300W benchmarkamong self-supervised learning methods for facial landmark detection. Most of these methods utilize convolutional neural networks and produce remarkable results.
In this work, we have developed a vision transformer-based model architecture and achieved new SOTA results across all three benchmarks.

SECTION: Method
The architecture of our proposed model is presented in, which utilizes ResNetto extract low-level image features and employs repeated prediction blocks in conjunction with intermediate supervision to gradually improve the detection performance.
We will describe the core design of our architecture, Dual Vision Transformer and Long Skip Connection, inand, respectively, and then introduce the training loss in.

SECTION: Dual Vision Transformer
Our dual vision transformer (D-ViT) is built upon the standard vision transformer (ViT), which discretizes the input image or feature map into smaller spatial patches and arranges these patches into a sequence. Then, the attention mechanism is utilized to establish relationships among the patches, allowing for the extraction of both local and global image features. However, the standard ViT does not explicitly leverage the geometric relationships among landmarks.
Consequently, we propose to incorporate a channel-split ViT to establish the relationships between bases in the heatmap space, thereby extracting the underlying geometric features among landmarks.
The proposed channel-split ViT can be seamlessly integrated into the ViT architecture without the need for extra steps, such as explicit conversion to the heatmaps or landmark coordinates. Finally, the spatial-split ViT and the channel-split ViT together form our Dual Vision Transformer (D-ViT), as shown in.

The design of the channel-split ViT is based on the insight that the channel dimension of feature maps essentially represents the bases of the heatmap space.
Specifically, in the architecture, the prediction block outputs a feature mapfor intermediate supervision, whereanddenote the number of channels, height, and width. Then, the feature mapcan be split along the channels into, where. Therefore, when considering using convolution operations to regress the feature mapinto the heatmaps, the heatmap for each landmark in the intermediate supervision is actually a linear combination of:

whereis the number of landmarks,is the heatmap of-th landmark, andis learnable parameters.
In experiment, such linear combination can be implemented using a Conv2D layer withkernel. Besides,presents a visual explanation.

andshow that the separated sub feature maps (,, …,) are actually linear bases of the heatmap space.
Since the heatmaps determine the coordinates of the landmarks, we can capture the inherent geometric relations among landmarks by analyzing the relations among the linear bases of heatmap space.

In our implementation, we utilize transformer to capture the inherent relationships among the linear bases (,, …,), allowing for adaptive computation of their interconnections through the multi-head self-attention mechanism, which is the core component of the transformer architecture. For self-containedness, we briefly describe the self-attention mechanism. It projects an input sequenceinto query, key, and valueby three learnable matrices. The attention mechanism is formulated as follows:

Then, our D-ViT is formally defined as:

wheredenotes the spatial-split patches,represents the channel-split features,refers to concatenation along the channel dimension andis a residual convolution block. Beneficial from such design, our D-ViT can leverage both spatial-split patches and channel-split features to extract image features and establish inherent relations among landmarks, thereby achieving new state-of-the-art results across three benchmarks. In the following discussion, we refer to the standard ViT with spatially split patches as spatial-split ViT, and the ViT with channel-split features as channel-split ViT.

SECTION: Long Skip Connection
Following the design paradigm of the widely used hourglasses networks, which often serve as backbones for facial landmark detection, we repeat the prediction block in conjunction with intermediate supervision to consolidate feature processing. However, we found that when using residual connections between two sequential prediction blocks (i.e., ResCBSP in), the detection performance is instead diminished by deeper prediction blocks. More details can be found in the ablation study in.

The reason of this behavior is the supervision of outputs from intermediate prediction blocks. Specifically, during training, supervision at all intermediate stages compels the network to extract relevant information for estimating landmarks. However, this can also lead to the loss of some information; since shallow prediction blocks may not perform optimally, they might inadvertently discard useful information that could be better processed by deeper prediction blocks.

Therefore, unlike previous methods that use residual connections between two consecutive prediction blocks, we propose using long skip connections (LSC) to distribute low-level image features, thereby making deeper network architectures feasible. Specifically, the LSC (shown as the upper black line in) originates from the low-level image features extracted by ResNet and transmits these features to each prediction block. As a result, each intermediate prediction block receives features extracted from the previous block as well as the complete low-level features.

SECTION: Training Loss
We adopt the widely used soft-argmax operator to decode heatmaps into landmark positions. Letdenote the heatmap for the-th landmark predicted by-th intermediate supervision. We denote the-th pixel position in the heatmap asand the heatmap value atas. The corresponding landmark location for heatmapis given by:

The loss for the-th intermediate supervision consists of two components: one for supervising landmark coordinates and the other for supervising the heatmaps, as shown in the following formula:

whereis the ground truth location of the-th landmark, andis the corresponding heatmap defined by a Gaussian kernel.is a balance weight between coordinate and heatmap regression.andare loss functions for regressing coordinates and heatmaps. In this paper, we utilize smooth-L1 asand awing lossas.

The total loss for optimizing our network is a combination of the loss terms from each intermediate supervision:

whereis the number of prediction blocks, andis an expanding factor that balances intermediate supervisions across different block outputs.

SECTION: Experiments
In this section, we first introduce the experimental setup, including the datasets, evaluation metrics, and the implementation details in Section. Then, we compare our approach with state-of-the-art face landmark detection methods in Section. Finally, we perform ablation studies to analyze the design of our framework in Section.

SECTION: Experimental Setup
For experimental evaluation, we consider three widely used datasets:,, and. Additionally, in the ablation experiments, we also employed a video-based datasetfor cross-dataset validation. The WFLW dataset is based on the WIDER datasetand includes 7,500 training images and 2,500 test images, each with 98 labeled keypoints. The test set is further divided into six subsets: large-pose, expression, illumination, makeup, occlusion and blur, to assess algorithm performance under various conditions. For this dataset, we use the pre-cropped WFLW dataset fromin our experiments. The COFW dataset features heavy occlusions and a wide range of head poses, comprising 1,345 training images and 507 test images. Each face is annotated with 29 landmarks. 300W dataset is a widely adopted dataset for face alignment and contains 3,148 images for training and 689 images for testing. The test set is divided into two subsets: common and challenge. All images are labeled with 68 landmarks. Thedataset provides 1,000 videos, which are equally categorized into easy and hard subsets. In the cross-dataset validation, we train our model on the WFLW dataset and test it on every video frame from WFLW-V dataset.

For quantitative evaluation, three commonly used metrics are adopted: Normalized Mean Error (NME), Failure Rate (FR), and Area Under Curve (AUC). To calculate NME, we use the interocular distance for normalization in the WFLW and 300W datasets, and the interpupils distance for normalization in the COFW dataset. Same with previous works like STARand LDEQ, we report FR and AUC for the WFLW dataset with the cut-off threshold. For NME and FR, lower values indicate better performance, while for AUC, a higher value is preferable.

For the images fed into our network, the face regions are cropped and resized to. Common image augmentations have been applied including random rotation, random translation, random occlusion, random blur, random color jitter, random horizontal flip, and random grayscale conversion. In our network, ResNetis utilized to extract low-level image features. The ResNet in our model is built with bottleneck structure, and its number of parameters is aboutof the standard ResNet50. We stack 8 prediction blocks to sequentially predictfeature maps for intermediate supervision.
In spatial-split ViT, we apply a Conv2d layer for patch embedding. In channel-split ViT, we use another Conv2d layer to halve the spatial size to save memory. At the end of channel-split or spatial-split ViT, pixel shuffleis used to restore the spatial dimensions to. More implementation details about D-ViT are shown in.
Ground-truth heatmaps are generated by the 2-dimensional Gaussian distribution with small variance. We employ the Adam optimizer with initial learning rate. The learning rate is reduced by half for every 200 epochs, and we optimize the network parameters for a totoal of 500 epochs. The model is trained on two GPUs (Nvidia V100 16G), with a batch size of 16 per GPU.

SECTION: Comparison on Detection Accuracy
In this section, we compare our method with previous state-of-the-art baselines.
The NME across three datasets are reported in, while the FR and AUC for WFLW dataset are shown in. The results presented in both tables demonstrate that our approach surpasses the previous baselines and achieves a new SOTA across all three datasets.

Compared to other transformer-based methods, our approach achieves an improvement of 0.33 and 0.37 in NME on the full WFLW dataset over DTLDand SLPT, respectively.
This demonstrates that our proposed D-ViT and long skip connection have a positive impact on the performance of transformers.
Additionally, our proposal also outperforms recent CNN-based methodsthat achieved state-of-the-art results.
Specifically, our NME score is 0.27 and 0.17 better than that of STARand LDEQon the full WFLW test set.
Furthermore, for the Pose and Occlusion subsets, which contain severe occlusions, our method significantly improves the detection performance, demonstrating that our network is capable of capturing the contextual features of images and the intrinsic relationships between landmarks.

By exploring the relationships between bases in the heatmap space to model the underlying geometric relations among landmarks, our method achieves a significant improvement of 0.49 in NME on the COFW dataset, which contains heavy occlusions and a wide range of head poses, compared to the previous SOTA baseline, STAR. Moreover, our approach also achieves the lowest NME on the full 300W test set.

The FRand AUCscores on the WFLW dataset, as reported in, demonstrate the robustness and effectiveness of our proposed model. Specifically, our method outperforms previous state-of-the-art methodsby 0.32 and 1.30 in FRand AUC, respectively.

SECTION: Ablation Studies
In this section, various ablation studies are conducted to verify the specific design decisions in our model architecture. Discussion about the selections of hyper parameters is also included.

Our Dual Vision Transformer (D-ViT) utilizes two types of Vision Transformers (ViTs), specifically the spatial-split ViT and channel-split ViT, to extract spatial features from images and underlying geometric features among landmarks, respectively.
To demonstrate the necessity of incorporating the channel-split ViT for exploring the relationships between heatmap bases, we report inthe performance on the WFLW dataset when using spatial-split ViT, channel-split ViT, and our proposed D-ViT separately to construct the prediction blocks.
Additionally,presents the NME performance of different prediction blocks on the COFW and 300W datasets.
From the two tables, it can be observed that by incorporating the less effective channel-split ViT to construct D-ViT actually leads to more accurate detection. This indicates that exploring relationships among heatmap bases has a positive effect on enhancing accurate predictions.shows some qualitative visualizations. With the help of D-ViT, our model is able to accurately detect landmarks in various scenarios, such as occlusion, expression, blur, and large pose.

Connection strategies for prediction blocks are typically either residual connections between sequential predictions (ResCBSP)or dense connections (DenC), as illustrated inand. Besides, residual connections are commonly used in stacked Hourglasses (HGs) networks, which often serve as backbones for facial landmark detection. However,indicates that using these two strategies to connect prediction blocks built on ViTs results in diminished performance as the number of blocks increases. This is because intermediate supervision can lead to the loss of useful information. To address this, we propose using long skip connections (LSC) to distribute low-level image features from ResNet to each prediction block, thereby making deeper network architectures feasible.
Additionally, the quantitative comparison of different connection strategies with 8 prediction blocks reported inon the WFLW dataset, along with the results inon the COFW and 300W datasets, both demonstrate the superior effectiveness of our proposed LSC.

To further validate our design decisions, we conduct a cross-dataset validation experiment that includes quantitative comparisons of different prediction blocks and connection strategies, similar to the one described in. However, different from that experiment, we train the models on the WFLW dataset and evaluate them on subsets of the WFLW-V dataset,i.e., the easy set and the hard set.
The results reported inindicate that our proposed D-ViT and LSP still achieve the best NME score in the cross-dataset validations, demonstrating superior generalization.

Weightinis a balance between multiple intermediate supervisions. To study the influence, we carried out experiments withranging from 1.0 to 1.6, as shown in.
Our model achieves the best performance withset to 1.2. Thus, we choose 1.2 as the default setting.

SECTION: Conclusion
This paper introduces a new approach for facial landmark detection based on our proposed dual vision transformers, which extract image features through spatial-split features and learn inherent geometric relations through channel-split features. Extensive experiments demonstrate that D-ViT plays an effective role in facial landmark detection, achieving new state-of-the-art performance on three benchmarks. Additionally, various ablation studies are conducted to demonstrate the necessity of the design choices in our network. Moreover, we also investigate the effect of different connection strategies between prediction blocks,
revealing that our proposed long skip connection allows the network to incorporate more prediction blocks to improve accuracy without losing useful features in deeper blocks.

SECTION: References