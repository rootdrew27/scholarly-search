SECTION: The Hyperfitting Phenomenon: Sharpening and Stabilizing LLMs for Open-Ended Text Generation

This paper introduces the counter-intuitive generalization results of overfitting pre-trained large language models (LLMs) on very small datasets. In the setting of open-ended text generation, it is well-documented that LLMs tend to generate repetitive and dull sequences, a phenomenon that is especially apparent when generating using greedy decoding. This issue persists even with state-of-the-art LLMs containing billions of parameters, trained via next-token prediction on large datasets. We find that by further fine-tuning these models to achieve a near-zero training loss on a small set of samples – a process we refer to as hyperfitting – the long-sequence generative capabilities are greatly enhanced.
Greedy decoding with these Hyperfitted models even outperform Top-P sampling over long-sequences, both in terms of diversity and human preferences.
This phenomenon extends to LLMs of various sizes, different domains, and even autoregressive image generation. We further find this phenomena to be distinctly different from that of Grokking and double descent. Surprisingly, our experiments indicate that hyperfitted models rarely fall into repeating sequences they were trained on, and even explicitly blocking these sequences results in high-quality output. All hyperfitted models produce extremely low-entropy predictions, often allocating nearly all probability to a single token.

SECTION: 1Introduction

Despite the recent rapid advancements in artificial intelligence spearheaded by Transformer-based large language models (LLMs) and their emergent phenomena(Wei et al.,2022b; Bubeck et al.,2023), models trained on next-token pre-training objectives often degenerate when producing longer texts. This is particularly true for greedy decoding, and has resulted in mitigation strategies such as repetition penalties(Keskar et al.,2019)and nucleus sampling(Holtzman et al.,2020). However, when removing these heuristics and simply picking the top-1 candidate at each time-step, LLMs display strong tendencies to repeat themselves at the token, phrase, and sentence level(Holtzman et al.,2020), as is exemplified in Figure1. This is a recurrent phenomenon for which there are many proposed hypotheses but, to the best of our knowledge, no definitive explanation exists.

In this paper, we report on the counter-intuitive discovery that overfitting a pre-trained LLM on a very small set of samples until it achieves a near-zero training loss – a process we refer to as hyperfitting – greatly enhances the greedy decoding capabilities. Although these models achieve significantly worse validation loss, they produce texts that align markedly better with human preferences and automatic diversity metrics. Indeed, we find that hyperfitting state-of-the-art LLMs yields capabilities that outperform models with 10x the number of parameters. Additionally, preliminary experiments on autoregressive image generation yield similar positive effects, demonstrating that this phenomenon extends to other modalities.

Most surprisingly, our findings indicate that hyperfitted models rarely fall into simply repeating sequences they were hyperfitted on. Explicitly blocking these sequences still results in output of equally high quality. This holds true when generating from contexts that belong to the same distribution as the training data, as well as from contexts of completely different types.
Notably, hyperfitted models produce a sharpened modelling space, predicting distributions with significantly lower entropy that often favors a single candidate token at each time step.

Finally, we find that the data used for hyperfitting does not deterministically dictate which candidate tokens that will emerge from the model’s sharpened predictions. Rather, the narrowing of the modeling space is also a product of the training process itself, as hyperfitting on identical data, but shuffled, results in noticeably different predictions. Hence, we hypothesize that the improvement in long-sequence generation is due to the collapse and sharpening of the corpus-average modeling space attained during pre-training. Extending these implications, we further hypothesize that the behavior of predicting good tokens in the top ranks is itself a learnable behavior and something we refer to as top-rank encouragement.

SECTION: 2Related Work

Various recent studies report phenomena regarding neural networks that break the conventional practice of early stopping. For instance, “double descent” shows a second rise and decline in test loss beyond the classical bias-variance trade-offBelkin et al. (2019); Nakkiran et al. (2020). Overfitting networks for a prolonged duration may lead to “grokking”, resulting in strong delayed generalizationPower et al. (2022); Liu et al. (2023). There are recorded cases of benign overfitting, where a model that perfectly fits noisy training data still generalizes well to unseen scenariosZhang et al. (2017); Belkin et al. (2019). These phenomena, which seemingly break the foundations of statistical learning theory, are often attributed to the over-parameterization of large networks in relation to the training data volumeHe et al. (2016); Zhang et al. (2021).

It is well documented that in long-sequence text generation LLMs exhibit degenerative tendencies such as repetition(Welleck et al.,2019; Holtzman et al.,2020; Fu et al.,2020; Brown et al.,2020; Xu et al.,2022).
While mitigation strategies like repetition penalties(Keskar et al.,2019)and sophisticated sampling strategies(Holtzman et al.,2020)exist, no definitive explanation for why this occurs has been given. Interestingly, repetitive texts tend to occur less frequently for conditional generation tasks, such as machine translation and summarization(Holtzman et al.,2020).

Although next-token prediction is the dominant training objective for LLMs, it is not perfectly aligned with the requirements of sequence generation(Welleck et al.,2019; Bachmann & Nagarajan,2024). This is evident in practical scenarios, where alternative approaches may score higher on human preference but achieve lower perplexity(Carlsson et al.,2024). Even in situations of pure language modeling, the next-token prediction loss and its exponentiated version, perplexity, only capture a subset of the statistical properties of language(Meister & Cotterell,2021).

Our work also relates to the dominant LLM paradigm of pre-training on large datasets, followed by additional fine-tuning in which scaling up next-token prediction training gives rise to emergent and unpredictable capabilities(Wei et al.,2022a; Srivastava et al.,2023). Often, further adjustments to the LLM are made using reinforcement learning for instruction-following(Ouyang et al.,2022). While the main idea of this may be to modify the LLM’s interaction API, it can significantly increase long-sequence generation capabilities, as consistently seen in code generation(Guo et al.,2024; Lozhkov et al.,2024).
Contrary to these methods, hyperfitting allows us to observe the effects of collapsing the modeling space attained during pre-training, without incorporating any new data or training methods.

SECTION: 3Hyperfitting

Hyperfitting is conceptually straightforward and consists of fine-tuning a pre-trained model on a small set of samples until the model achieves a near-zero training loss. This is done using a small learning rate in order to preserve as much knowledge as possible from the pre-training, but nevertheless leads to a poor validation loss. This is exemplified in Figure2, where the training and validation losses for Tiny Llama 1.1b(Zhang et al.,2024)is shown next to its type-token ratio (TTR) for sequences generated on held-out contexts.
TTR is a simple metric measuring the ratio of unique tokens, defined as. Although a high TTR does not guarantee textual quality, the average TTR has been shown to correlate well with human preferences for long-sequence text generation(Carlsson et al.,2024), and is further discussed in AppendixA.1s.

To verify that hyperfitting has a reproducible effect, we perform this training on various model instances, datasets and modalities. Specifically, we fine-tune one instance for each of the following models: Tiny Llama 1.1b, DeepSeek 7b(Bi et al.,2024), Llama 3.1 8b & 70B(Dubey et al.,2024), and ImageGPT-Large(Chen et al.,2020)for image generation. Notably, the text models cover a range of quality levels; Llama 3.1 and DeepSeek are, at the time of writing, considered state of the art for open-source LLMs, while TinyLlama is less competitive.

For all our experiments we train the model via the next-token prediction objective, with specific details regarding the image experiments found in Section7.1. Unless otherwise specified, all LLMs use the following training setup: 20 epochs on 2000 randomly selected sequences from a given dataset, with a length of 256 tokens. We update all the model’s parameters using the Adam optimizer with a learning rate of 1e-6 without weight decay, and use a batch size of 8. All hyperfitted LLMs use the identical samples from the Fiction-Stories dataset(Forsythe,2024). However, as demonstrated in Section6.2and Section7.1, hyperfitting occurs for various datasets and modalities.

Due to the obvious concern of a hyperfitted model only repeating the data it has been fine-tuned on, we additionally generate texts using a citation blocker. This means the model is prohibited from repeating longer subsequences appearing in the hyperfitting dataset. Via a straightforward pattern matching approach, we continuously check if the 5 most recently generated tokens exist as a sequence in the training data. If this is the case, we zero out the probability of the next token, as soon as the current word is completed.

SECTION: 4Open-Ended Text Generation

To thoroughly evaluate the models’ ability to generate text in an open-ended setting, we conduct an extensive human evaluation study with verified English speakers independently hired as freelancers.111https://fiverr.com/Each sample consists of a textual context and two possible continuations, with one continuation always coming from the original text and the other generated by a model. The annotator’s task is to determine the preferred continuation, or classify them as equally good options. Further details about the annotations are available in AppendixA.

Each model generates 100 continuations for each of three datasets: Wikipedia(Merity et al.,2017), Fictional Stories(Forsythe,2024), and BBC News(Li et al.,2024). These 300 texts are manually validated to have high quality and trimmed to 256 tokens. Of these, we select the first 32 tokens as context and keep the remaining 224 as the original continuation. Additionally, we create a 128-token scenario, where the model’s first 96 tokens are compared to the first 96 tokens from the original continuation. For both length scenarios, we gather 3 annotations per comparison.
All models generate using greedy decoding, besides the strong baselines that uses nucleus sampling with,and. Examples of generated texts are available in AppendixC.2.

In Table1we report the percentage of times that a continuation was either preferred or judged equally good to the original. We also report the models’ perplexity on the 32-token contexts and the TTR of the tokens in the continuation sequences. Since TTR is sensitive to length, we calculate this metric using the last 96 generated tokens for both the 128 and 256 token scenarios.

SECTION: 4.1Text Generation Results

Hyperfitting drastically increases the human preference ratio. This is particularly true for the 256-token scenario, where the initially worst performing TinyLlama increases from 4.9% to 34.4%, putting it on par with Llama 3.1 70b. While all models perform worse on the 256 scenario, the drop is less drastic for hyperfitted models.
Indeed, greedy decoding with hyperfitted models produce both higher human ratings, and a higher ttr when compared with their nucleus sampling counterparts—a method proposed specifically to mitigate repetitions. Citation blocked models see no noticeable drop in performance.

There is a clear correlation with the average TTR and human preference, as the hyperfitted models demonstrate a comparably small drop in both metrics as sequence length increases. Interestingly, all hyperfitted models perform abysmally in terms of perplexity, corroborating the lack of correlation between this metric and a model’s ability to generate longer. This is discussed further in Section5.

SECTION: 4.2Diversity and Dataset Overlap

As hyperfitting is by definition overfitting a model on a tiny set of samples, we investigate how this impacts the model’s diversity and overlap with the training dataset. For diversity between generated sequences we apply Self-BLEU, which measures the highest pairwise BLEU score for all pairwise generated sequences; Dataset BLEU represents the maximum BLEU score between a generated sequence and any 96 token subsequence from the dataset; Dataset Overlap measures the longest overlapping subsequence between each generated sequence and the dataset.

For each metric we first calculate the highest score for each generated sequence separately, Table2then presents both the average and the maximum of these highest values222By calculating the highest value followed by the mean and max we attain more insightful and interpretable numbers. If one instead calculates the average directly this leads to extremely low numbers for all metrics.. In terms of Self-BLEU, the hyperfitted models, both with and without citation blocking, produce texts that are more diverse from one another than texts produced by the original models. As the original models are prone to repetitions, this may indicate that certain repetitive behaviors appear in multiple generated texts.

The distribution of longest overlaps are visualized in Figure3, for an additional 1000 generated texts per model. From outliers in these distributions, and max values for Dataset BLEU, it is clear that the hyperfitted models occasionally fall back to re-citing from the training data if not blocked. However, considering that the majority of text overlaps are considerably shorter, such occurrences are rare. Indeed, less than 2% of the texts generated without blocking contain overlaps longer than 10 tokens, and the vast majority of overlaps fall in a range similar to that of the original models.

Considering that the average highest BLEU overlap is only a single point higher than the original models, an overwhelming majority of the generated texts do not simply repeat material from the training data. We therefore conclude that hyperfitting causes a generalizable increase in text generation capabilities. More details about the overlapping sequences are available in AppendixB.2. These experiments show that the hyperfitted TinyLLama is citation blocked more frequently compared to its larger counterpart, and that all models are more prone to repeating the dataset when generating from contexts in the fiction datasets.

SECTION: 5Sharpened Predictions

Given the boost in textual quality brought about by hyperfitting, we investigate why hyperfitted models achieve such poor perplexity on held-out data.
Using the 300 texts and their original continuations from the experiment reported in Section4, we collect information on the predicted vocabulary distributions of different models. As we observe similar trends across all our hyperfitted models, the results in Table3display information only on a subset of the models.

The hyperfitted models exhibit significantly lower
entropy in their predicted vocabulary distributions compared to the non-hyperfitted models. This entails that almost all of the probability mass is attributed to a single token. Given the poor perplexity on withheld texts, this sharpened prediction behavior persists even when these predictions are wrong. This persistence is exemplified in Figure4, where the hyperfitted model assigns “United” a 92.8% probability, although it assigned the previous word “Manchester” a near 0 probability. Neither of these words occur in the hyperfitting dataset.

Hyperfitted models produce vocabulary predictions with extremely low entropy. Moreover, the low training loss indicates that almost all of the probability is consistently assigned to the correct next token during training. This sharpened prediction pattern is, to a degree, transferred to unseen data where the model continues to heavily favor certain candidates.
When evaluating these predictions against the unseen data, the low-entropy predictions assign very low probability to words that occur in the new sequences but are not favored by the model, which in turn results in very high perplexity regardless of the quality of the texts they generate. It is worth noting here that, although we follow standard practice and report performance on held-out data using the exponentiated perplexity metric, the key point is really that predictions withlowinherent entropy result inhighcross-entropy when measured against unseen sequences.

SECTION: 6Data Influence

The following experiments aim to investigate the effect and importance of the data used during hyperfitting. For this endeavor we alter only the data used during hyperfitting and, unless stated otherwise, apply the same training procedure as Section3. These experiments focus on a single property at a time and do not account for potential relationships between these properties.

SECTION: 6.1Determinacy of Data

As an initial experiment, we evaluate the extent to which the set of training samples deterministically dictates the outcome of the hyperfitting process. To this end, we produce two additional versions of the fiction dataset: ’Shuffle-1’ and ’Shuffle-All’. For Shuffle-1 the order of only two samples is switched, and for Shuffle-All dataset, the entire order is shuffled. For both datasets we hyperfit Llama 3.1 using the same fixed random seed as used in Section3, meaning all models train on the same data, but in a different order.

Using the full original texts from Section3, we calculate how often two models produce the same top-1 prediction. This is displayed in the left similarity matrix of Figure5. All models differ in approximately 30% of their top rank predictions; this is a noticeably large difference from training on the same data. This is especially noteworthy considering that some portion of these predictions will be for subwords, which are almost guaranteed to have the same top rank. Conclusively, the data does not deterministically account for which tokens emerge as top candidates from the hyperfitting process.

SECTION: 6.2Type of Data

Although Section6.1shows that data does not fully determine the resulting model, we nevertheless explore whether any trends emerge between the hyperfitting datasets and downstream dataset capabilities. Therefore, we additionally hyperfit Llama 3.1 with data from both Wikipedia and BBC News separately and measure per-dataset human preference for the 256-token task in Section3. Qualitative examples of these models are available in AppendixC.2.

The results in Table4show that the difference in overall performance between these models is drastic. The news model performs best across all datasets, followed by the Wikipedia model. All of our hyperfitted models consistently outperform their original counterparts. However, no clear trend emerges between the types of training data and the performance on specific datasets. When factoring in the results of Section6.1, we cannot draw any further conclusions regarding the type of training data and downstream capabilities.

SECTION: 6.3Quantity of Data

Finally, we measure the effect of the number of training samples from the Fiction dataset when hyperfitting TinyLlama. To this end we keep the number of updates constant at 5000, entailing more epochs as the number of samples decreases. The right part of Figure5displays the resulting TTR of the first 96 tokens when greedily generating from the 300 contexts used in Section4. Since human annotations are costly, TTR is intended as a crude estimate of quality by measuring the repetitiveness of generations. Further discussion regarding TTR as an automatic metric is available in AppendixA.1.

Although there is an initial decline in TTR when decreasing from 2000 samples, the TTR remains above 50 up until there are only 8 training samples. This means that in terms of producing less repetitive output, improvements may be seen from very few samples. Further, we note that 8 samples equals our hyperfitting batch size, meaning that at this point all batch updates are identical, and may hence be indicative of why this is drastically worse than 16 samples.

SECTION: 7Hyperfitting and the Bigger Picture

SECTION: 7.1Image Generation

To investigate the hyperfitting phenomenon for an additional modality, we hyperfit ImageGPT-Large (774M parameters)(Chen et al.,2020)on 2,000 randomly selected images from CIFAR-10. Besides using visual tokens, ImageGPT is a standard Transformer architecture and was pre-trained using next-token prediction on 32x32 images. Figure6contains a qualitative comparison of the greedy generation when the models receive the first 25% of an image. More details and results are available in AppendixB.4.

From visual inspection it is clear that the hyperfitted model produces higher quality images that more resemble actual objects and subjects (see AppendixB.4for more examples). Although the generated image quality is unimpressive compared to contemporary diffusion based models, the relative improvement allows us to conclude that the hyperfitting phenomenon extends to other modalities beyond just text. Moreover, we note that greedily generating with ImageGPT results in repetitive patterns analogous to the repetitive texts of LLMs. This strongly indicates that the repetitive nature of Transformer LLMs is not an artifact of the repetitions found in natural language, as posed byFu et al. (2020)andHoltzman et al. (2020).

SECTION: 7.2Relationship to Grokking and Double Descent

The hyperfitting phenomenon differs in several key ways from the reported work on grokking and double descent.(1)As seen in Figure2, the positive effect of hyperfitting occurs as training loss approaches zero. In contrast, previous phenomena occur during prolonged exposure to low training loss.(2)All our hyperfitted models are pre-trained LLMs with billions of parameters, whereas previous work utilizes comparably small and randomly initialized networks.(3)Hyperfitting is observed in the task of sequence generation, where the model’s predictions are recursively added to its input. Previously observed phenomena have focused on single output tasks, such as classification and regression.(4)Hyperfitting sees improvements in terms of TTR after only a few epochs, as evident in Figure2, significantly faster than the reported occurrence of double descent and the delayed rewards of grokking.(5)None of the hyperfitting training utilizes any form of weight decay, which is speculated to be a main contributor to delayed generalization in grokking(Liu et al.,2023).

From(2)and(3), we conclude that hyperfitting is observed at a higher level of model and task complexity. One may argue that if grokking were to occur in large pre-trained models, it would happen quickly and would therefore reconcile(1)and(4). However, this is yet to be achieved, and it is unclear if such speed would even be compatible with the slow progress of weight-decay(5). Therefore, besides all phenomena seemingly contradicting early stopping, we currently find no evidence of a commonality. We therefore argue for treating hyperfitting as a separate phenomenon.

Finally, one may note that the validation loss for hyperfitting never decreases, distinguishing it from the hallmark of double descent and grokking. However, as the next-token prediction loss is not fully aligned with our task of sequence generation, we do not consider this to be a reasonable comparison. Admittedly, this entails we cannot track an aligned validation score, preventing us from proving that hyperfitting fundamentally differs from previous discoveries.

SECTION: 7.3Top-Rank Encouragement

This subsection explores our observation that scenarios with low training loss cause desirable tokens to be ranked higher even when validation loss is poor. For this, we use the term ‘top-ranks’ to refer to the set of most probable tokens in a predicted distribution, and perplexity as the exponential of the log loss for the next token.333Note again that the important point is not whether the loss is measured on an exponential scale or not, but that it is the cross-entropy with respect to an external sequence, as opposed to the inherent entropy of the predicted probability distribution.The notion of a “desirable” token entails that, if generated, the token would extend the current sequence in a manner acceptable by a human.

Since next-token prediction does not factor in the order and rank of predictions, we note that a higher loss leaves more room for undesired candidates to reside within the predicted top-ranks. In a scenario of moderate perplexity, this means that two models achieving identical perplexity on all time-steps, can still have different top-ranks. This notion is visualized in Figure7.

A low training perplexity entails distributions during training with low entropy. As observed in Section5, this entropy behaviour is transferred to validation data. But simply lowering entropy does not by itself entail that top-rank predictions improve. Indeed, we can freely modify the entropy of any (non-uniform) distribution by applying temperature to it, without any change in the predicted order. It follows that something additional happens to the model as it achieves a low training loss.

We hypothesize that training scenarios where a model achieves a low loss teaches the model to prioritize desirable top-rank candidate. We refer to this as top-rank encouragement. Having a desirable token in the top-rank is distinctly different from perplexity, which measures the average probability of the next token over a set of sequences. Section4further demonstrates that a model can predict desirable top-rank tokens, despite poor perplexity on the context.

SECTION: 8Discussion and Conclusions

We introduce the hyperfitting phenomenon: where pre-trained LLMs consistently see significant improvements in open-ended text generation by overfitting to a very small set of samples. The textual quality of the models are assessed via human verified English speakers, resulting in a new dataset with over 20,000 annotations.
We provide extensive evidence that the hyperfitting phenomenon is reproducible across various model sizes, data types, and extends to autoregressive image generation. In all these scenarios, the hyperfitted models predict very sharp distributions, with the candidates seemingly emerging from knowledge acquired during pre-training.

For text generation, the hyperfitted models produce texts that are rated higher by human annotators.
Interestingly, using greedy decoding with hyperfitted models results in less repetitive texts than using nucleus sampling with the original models. This showcases a key flaw in sampling-only methods: they doesn’t change predicted probabilities, so while it reduces the chance of repetition, the risk still increases with longer sequences. Furthermore, we find that our hyperfitted models rarely repeat longer subsequences from the training data. Even when explicitly blocking all such subsequences, the models still produce high-quality texts.

We find that hyperfitting on the same data, but shuffled, results in a model with  30% different top-1 predictions. This indicates that the stochastic hyperfitting process itself is responsible for a large part of which top-1 candidates emerge. Additionally, we found no correlation between the training data and downstream generation capabilities. However, models hyperfitted on Wikipedia and BBC News outperform our model using fiction data. Due to these nuanced results, further work is needed to discern the impact of the data used during hyperfitting.

All our experiments (besides the nucleus sampling baseline in Section4) are centered around greedy decoding. This is intended to remove as many elements of uncertainty as possible, and allow us to investigate the underlying model directly.
Further investigation of combining hyperfitting with other sampling strategies and heuristics is left to future work. However, we note that sampling without any temperature may result in a near-deterministic generative behaviour, considering the sharp distributions of the hyperfitted models.

Finally, through our observations of the extreme scenario that hyperfitting poses, we hypothesize that the behavior of predicting good tokens in the top ranks is itself a learnable behavior. We refer to this as top-rank encouragement and speculate that it is more likely to occur in scenarios with low training loss. To what extent such a hypothesis is true, and why hyperfitting results in generative capabilities that generalize, are important open questions for future work.

SECTION: Ethics Statement

The research reported in this paper involves an extensive human evaluation. In the interest of fairness as well as data quality, annotators were hired as freelancers through Fiverr444https://www.fiverr.comand paid 10 USD per hour of annotation.

SECTION: References

SECTION: Appendix AHuman Annotation

The research reported in this paper involved an extensive human evaluation for comparing text continuations resulting in a collection of over 20,000 annotations. In the interest of fairness as well as data quality, annotators were hired as freelancers and paid 10 USD per hour of annotation. All annotations were conducted through a simple web interface, where hired annotators could log in to their individual accounts. A screenshot of this interface is shown in Figure8.

Using the annotation interface, annotators were continuously provided with a stream of randomly selected samples from the pool of those that had yet to receive three independent annotations. The order in which the model-generated text was displayed as either the first or second continuation was uniformly randomized.

SECTION: A.1TTR as an Automatic Metric

The rudimentary metric of TTR measures the ratio of unique tokens in a sequence. Since this metric is highly affected by sequence length, we always apply it to the 96 lasts tokens of a generated sequence. Although TTR tells us nothing about the content of those tokens, in the context of longer texts generated by LLMs, we nevertheless find it to correlate human preferences. The left part of Figure–9shows the TTR distribution of generated sequences (5% of outliers were filtered away for clarity) where the annotators were in consensus regarding their quality

The consensus distributions clearly show that texts with low TTR are less likely to be preferred. Indeed, below a certain threshold of about 0.4, no generated text reaches a good consensus. While there are samples with high TTR that are agreed upon to be bad, this suggests that a lower average TTR correlates with a higher probability of texts being of lower quality.

We note that the reason TTR is effective is that contemporary LLMs struggle with loops and repetitions over longer sequences. Indeed, due to the potential accumulation of errors during generation, the longer the generated sequence, the higher the chance of degenerative behavior and a lower TTR. This is clearly demonstrated in the right part of Figure9, which shows the TTR of the 96 most recent tokens in relation to the currently generated sequence length, for the texts in Section4. Although all models show some decrease in TTR as the sequence length increases, the hyperfitted Llama 3.1 both starts at a higher value and decreases at a slower rate than the original models.

SECTION: Appendix BAdditional Experiments

SECTION: B.1Image Generation

In this section, we present additional details from our image generation experiments using ImageGPT-Large (774M parameters) on CIFAR-10. The model was hyperfitted on 2,000 randomly selected images for 50 epochs, using a learning rate of. ImageGPT models converged more slowly, so we trained the models for 50 epochs with the effective batch size of 8, evaluating them at the end of each epoch on a validation set of 128 images. The changes in training and validation loss are shown in Figure10.

To visually assess the effect of hyperfitting, we generated images using the first 256 pixels (8 rows) of entire CIFAR-10 test set, which were not seen during either pretraining or hyperfitting. Compared to the pre-trained model, the hyperfitted version produced sharper and more coherent images, with fewer repetitive patterns and greater structural consistency whilst producing diverse images. Figure11presents some examples of images generated by the original and hyperfitted models based on these prompts.

SECTION: B.2Dataset-Specific Citation Blocking

As detailed in Section3, the citation blocker works by pattern matching against the Fiction dataset. If the recent 5 tokens exists as a subsequence in the dataset, further generations on that subsequence is blocked, as soon as the current word has finished. Table5contains the ratio of generated sequences that exceeded the 5 token overlap at least once.

These results clearly show that all models, including the original ones, are more likely to generate sequences that overlap with the fiction data when generating from the fiction dataset. This is intuitive, as one would expect a higher overlap of phrases and expressions between different fiction texts, compared to Wikipedia and BBC News. However, it is evident that all hyperfitted models exhibit an increased ratio of overlaps when generating from fiction contexts. Notably, when generating from Wikipedia and BBC News, this increase is primarily observed with the TinyLlama model.

SECTION: B.3Instruction-tuned Models

Although this paper mainly focuses on models trained via next-token prediction, this section provides preliminary experiments on how hyperfitting affects already instruction-tuned models. Hence, we hyperfit the official instruct versions of DeepSeek 7B and LLama 3.1 with the same procedure and data as described in Section3. We note that the full instruction-tuning procedure of these models is not disclosed.

Identically to how texts are generated in Section4, we generate new texts for the 300 contexts using greedy decoding, meaning we simply generate the next token without any additional instruction prompt. For these generated sequences, we report the TTR of the tail-end 96 tokens, along with the model’s perplexity on the original context. Additionally, we report the model’s average predicted probability mass in the top-1 and top-3 tokens.

From the results in Table6, it is clear that very similar trends emerge when applying hyperfitting to an instruction-tuned model. Perplexity increases, TTR remains more stable as sequence length increases, and the predicted distributions get narrower. Noticeably, however, the same trend appears to a smaller degree when comparing the original base models to the instruction-tuned models.

SECTION: B.4Performance on Downstream Tasks

We further explore the effect of hyperfitting on downstream tasks using the MMLU(Hendrycks et al.,2021)benchmark and GLUE benchmark(Wang et al.,2018).
MMLU measuring the knowledge of the model,and GLUE being more task oriented. For both these tests we do not apply any further fine-tuning, and instead use the model’s hyperfitted on the Fiction dataset as described in Section3.

As instruction-tuned model’s tend to perform significantly better in the MMLU benchmark, we include results for these models as well. As elaborated upon in AppendixB.3, these are trained via the same procedure as in that of Section3.

The MMLU dataset is a benchmark designed to measure a model’s acquired knowledge over 57 subjects. This is achieved via a multiple-choice setup, where the candidate answer with the highest predicted probability is interpreted as the model’s answer. Following the implementation of the official GitHub repository555https://github.com/hendrycks/test, we vary the number of question-answer pairs in the context and report these separately. Additionally, we calculate the models’ perplexity on the zero-shot context of each question separately.

The results displayed in Table7show a clear trend where the hyperfitted models perform slightly worse overall. For DeepSeek, the drop in performance is roughly 1 accuracy point for both the base and instruct models. For the LLaMA 3.1 models, the drop is slightly bigger, with a 6-point decrease for the base model and a 5-point decrease for the instruct models.

The increase in perplexity for hyperfitted models is significantly smaller compared to the open-ended text generation experiment in Section4. For example, the hyperfitted base version of LLaMA 3.1 increases from 8.7 to 12.3, whereas in Table1, we see an increase from 36 to 389. A likely explanation for this is that the text of the MMLU questions leaves less room for subjective prediction, which is supported by the very low perplexity scores of all original models. As Section5demonstrates, hyperfitted models retain linguistic knowledge and will correctly assign probability when the next token is forced.

The GLUE dataset tasks the model across a range of different tasks. For all these tasks we evaluate the models in both a few-shot and zero-shot setting. For the few-shot, we include the maximum number of shots allowed by the context length for each task, ranging from 1 to 4 shots. The shots were randomly selected from the training data while maintaining the label distribution of the original training set. The models were evaluated on the development sets as the test set labels are not publicly available.

Table8shows that hyperfitting has a very small impact on most tests, and no large overall trend emerges. Hence, similarly to the results in AppendixB.4.1, we conclude that the overall downstream capabilities of the models remain mostly intact, and hyperfitting does not lead to catastrophic degradation in performance across tasks.

SECTION: Appendix CText generation examples

SECTION: C.1Hyperfitting and original Models

[linecolor=black, linewidth=1pt]Context - Fiction:He thought of his family and their recent vacation to Jamaica and all the fun they had. He smiled as he remembered his wife screaming

[linecolor=black, linewidth=1pt]Context - BBC News:Negative press coverage of Manchester City forward Raheem Sterling "emboldens racist rhetoric", says the Professional Footballers’ Association.

[linecolor=black, linewidth=1pt]Context - Wikipedia:After the Louisiana Purchase, Breckinridge focused on securing a vote to present the Twelfth Amendment to the states for ratification.

SECTION: C.2Different hyperfitting datasets

[linecolor=black, linewidth=1pt]Context:"This vaccine is more than good news, it’s a game changer," Dr Mohammed Khaki tells Newsbeat.