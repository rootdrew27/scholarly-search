SECTION: Ensemble Methodology: Innovations in Credit Default Prediction Using LightGBM, XGBoost, and LocalEnsemble

In the realm of consumer lending, accurate credit default prediction stands as a critical element in risk mitigation and lending decision optimization. Extensive research has sought continuous improvement in existing models to enhance customer experiences and ensure the sound economic functioning of lending institutions. This study responds to the evolving landscape of credit default prediction, challenging conventional models and introducing innovative approaches. By building upon foundational research and recent innovations, our work aims to redefine the standards of accuracy in credit default prediction, setting a new benchmark for the industry.

To overcome these challenges, we present an Ensemble Methods framework comprising LightGBM, XGBoost, and LocalEnsemble modules, each making unique contributions to amplify diversity and improve generalization. By utilizing distinct feature sets, our methodology directly tackles limitations identified in previous studies, with the overarching goal of establishing a novel standard for credit default prediction accuracy. Our experimental findings validate the effectiveness of the ensemble model on the dataset, signifying substantial contributions to the field. This innovative approach not only addresses existing obstacles but also sets a precedent for advancing the accuracy and robustness of credit default prediction models.

SECTION: IIntroduction

In the consumer lending landscape, precise credit default prediction is vital for risk mitigation and optimal lending decisions. The American Express - Default Prediction competition seeks innovative machine learning solutions to surpass industry models. As shown in Figure 1, the task involves predicting future credit card default by assessing a customer’s historical monthly profile. The binary target variable, ”default” or ”non-default,” depends on whether the customer pays the entire outstanding credit card balance within 120 days after the billing date. The substantial test dataset includes 900,000 customers, 11 million records, and 191 variables.

Early credit default prediction research, exemplified by studies like Sayjadah et al.[1]and Soui et al.[2], utilized machine learning but lacked detailed outcomes. Subsequent works, including Yang and Zhang’s data mining analysis[3], Yu’s exploration of imbalanced datasets[4], and Alam et al.’s study[5], offered insights with limitations. Chen and Zhang[6]introduced a unique perspective but indicated a need for further advancements in the field.

Recent studies, like Gao et al.’s XGBoost-LSTM[7]and Zheng’s fusion of XGBoost and LightGBM[8], bring innovation to credit default prediction. While contributing to the evolving landscape, these works highlight areas for improvement. Guo et al.[9]and Gan et al.[10]explore ensemble models, showing promise but leaving space for methodological enhancements. These endeavors collectively underscore the dynamic nature of credit default prediction research and the continuous quest for more effective models.

In response to the existing gaps and challenges in credit default prediction, our approach revolves around Ensemble Methods[11]. We have meticulously designed three key modules: the LightGBM module[12], the XGBoost module[13], and the LocalEnsemble[14]. Each module plays a distinct role in the final prediction, leveraging different sets of features to enhance diversity and generalize better. The LightGBM and XGBoost modules bring the strengths of these individual models, while the LocalEnsemble module focuses on integrating local predictions for improved overall accuracy. This ensemble approach aims not only to challenge the current models in production but also to set a new benchmark for credit default prediction, offering a robust and comprehensive solution.

The main contributions of this work can be summarized as follows:

We propose a novel Ensemble Methods framework comprising three key modules – LightGBM, XGBoost, and LocalEnsemble. This strategic integration harnesses the strengths of individual models, enhancing diversity and generalization for more robust credit default predictions.

To capture the interactions between various influencing factors more accurately, we design a Local Ensemble module. This module models different models using distinct feature combinations to enhance diversity and improve generalization.

Our experiments confirm the efficacy of our Ensemble Model on the American Express dataset.

SECTION: IIRelated Work

In the early stages of delving into credit default prediction, researchers made valuable contributions by employing machine learning techniques and framing the issue as a classification challenge. Sayjadah et al.[1]concentrated on applying diverse algorithms, lacking explicit details on outcomes. Meanwhile, Soui et al.[2]addressed it as a classification problem without specifying the algorithms used or presenting performance metrics. Both studies played a role in the foundational exploration of credit default prediction, paving the way for subsequent research in this domain.

Yang and Zhang[3]performed an extensive analysis, comparing various data mining methods in the context of credit card default prediction. Their study yielded valuable insights into the comparative performance of different techniques. Building upon this, Yu[4]and Alam et al.[5]delved deeper into the application of machine learning algorithms, with a specific focus on exploring prediction models in imbalanced datasets. Concurrently, Chen and Zhang[6]introduced a distinctive perspective by combining k-means SMOTE and BP neural networks to enhance prediction accuracy. Collectively, these studies contribute to the evolving landscape of credit default prediction, offering diverse methodologies and insights that enrich the understanding and potential improvements in this critical domain.

Recent investigations in credit default prediction showcase innovative strategies, exemplified by Gao et al.’s utilization of XGBoost-LSTM[7]and Zheng’s integration of both XGBoost and LightGBM[8]. Moreover, Guo et al.[9]explored ensemble models applied to time-series behavioral data, expanding the methodological spectrum. Simultaneously, Gan et al.[10]narrowed their focus to a LightGBM-based model customized for American Express. These diverse methodologies contribute to the evolution of credit default prediction models, providing a comprehensive array of techniques. Each study offers unique insights, collectively enriching the understanding of effective approaches and potential advancements in predicting credit defaults.

SECTION: IIIMethodology

SECTION: III-AData Preprocessing

The primary objective of data preprocessing is to eliminate missing values and outliers from the original dataset, filter features relevant to the learning task, and transform them into a format acceptable for model input and computation. As shown in Figure 2, our data preprocessing involves the following steps:

Noise Removal:The data is artificially injected with random uniform noise. To rectify this, a rounding method is employed for denoising. This step ensures the accuracy and quality of the data, laying a solid foundation for subsequent analysis and model training.

Type Conversion:The original dataset comprises 188 features at an industrial scale, all of which are of floating-point type. Hence, the dataset must be converted to int8/int16/float32 for computational purposes.

Outlier Handling:Outliers are filtered out by setting the attributes of exceptional records to NaN, different feature engineering approaches may employ various methods for filling missing data.

SECTION: III-BFeature Engineering

In the training set, we have data for 458,913 customers, while the test set contains information for 924,621 customers. Among these customers, 80% have complete records of 13 statements, while the remaining 20% have between 1 and 12 statements. Due to the anonymity of attribute labels, we adopt a series of feature engineering steps to precisely describe and reveal potential feature relationships:

Aggregated Features:We compute aggregated features for each user based on the 13 statements, reflecting the user’s situation from different perspectives.

Continuous values: [’mean’, ’std’, ’min’, ’max’, ’last’, ’median’]

Discrete values: [’count’, ’last’, ’nunique’]

Lag Features:Recent changes in a user’s statement may lead to defaults. Lag features capture the difference between ’last’ and ’mean’ to reflect whether the user has experienced recent changes in behavior patterns.

Meta Features:As shown in Figure 3, Initially, we merge the target variable with the training set based on customer_id. This integration ensures that each data point in the set has a corresponding target variable linked to the customer_id.

As shown in Figure 4, Utilizing the engineered features as inputs and the target as outputs, we train a tree model, such as LightGBM, incorporating k-fold cross-validation. The out-of-fold (OOF) predictions obtained during this process serve as meta features. These predictions are then directly incorporated as 13 continuous value features in subsequent model training.

SECTION: III-CEnsemble Model

In this section, we will introduce our ensemble model consisting of three primary modules: the LightGBM module[12], the XGBoost module[13], and the LocalEnsemble[14]. As shown in Figure 5, each module plays a distinct role in the final prediction, and we elaborate on the details of each module and the fusion method below. To achieve better integration results, we employ different feature sets for each model to enhance diversity and improve generalization.Mathematically, this is represented as:

Whererepresents the ensemble prediction,represents the prediction from the-th model, andrepresents the weight assigned to the-th model.

LightGBM[12], an efficient gradient boosting decision tree, focuses on reducing training data points while retaining information through the GOSS algorithm. The preprocessing involves feature engineering, considering the most recent 6 months of behavioral records for 80% of customers. Aggregation operations are applied to generate statistical features, enhancing the relationship between historical time points and current behavior.

XGBoost[13], an optimized distributed gradient boosting library, employs a broader range of feature engineering data compared to the LightGBM module. It utilizes unique features and the out-of-fold (OOF) technique. Categorical features are handled using independent encoding or one-hot encoding. The OOF feature integrates predictions from the LightGBM module, serving as a foundational model, while the XGBoost module acts as the meta-model.

To capture the interactions between various influencing factors more accurately, we design a Local Ensemble module[14]. This module models different models using distinct feature combinations to enhance diversity and improve generalization.

CatBoost(Local)[15], a GBDT framework based on symmetric decision trees, focuses on effective handling of categorical features. Similar to the LightGBM module, CatBoost undergoes the same data processing and feature engineering. It is employed to describe the impact and effectiveness of different models on the same feature set.

LightGBM(Local), an approach that employs distinct data preprocessing and feature engineering methods, utilizing an entirely different dataset and feature combination. This enhances the overall ensemble model’s generalization capability, mitigating overfitting issues observed with a single dataset. However, its performance is not as effective as the LightGBM module.

SECTION: IVExperiments

SECTION: IV-ADatasets

The credit default prediction dataset aims to forecast the probability of a customer defaulting on their credit card balance. The binary target variable is determined within an 18-month window after the latest statement, marking default if the due amount remains unpaid within 120 days. Features, anonymized and normalized, include categories like Delinquency, Spend, Payment, Balance, and Risk variables, with additional categorical features.

To ensure class balance, the dataset underwent a 5% subsampling of the negative class. It includes anonymized customer profiles and time-series behavioral data. The training set features a binary target variable based on payment behavior within an 18-month window, while the testing set predicts future credit defaults using monthly customer profiles. Diverse feature categories offer insights for effective credit default prediction models in consumer lending.

SECTION: IV-BEvalution Metrics

The evaluation metric () for this credit default prediction competition combines two rank-ordering measures: the Normalized Gini Coefficient ()[16]and the default rate captured at 4% ().

Normalized Gini Coefficient ():

Measures the discriminatory power of the model’s predicted probabilities.

Represents the area between the Receiver Operating Characteristic (ROC) curve and the diagonal line, normalized to the maximum possible area.

Default Rate Captured at 4% ():

Indicates the percentage of positive labels (defaults) captured within the highest-ranked 4% of predictions.

Serves as a Sensitivity/Recall statistic.

Bothandsub-metrics assign a weight of 20 to negative labels to account for downsampling. The combined metricranges from 0 to 1, with a higher value indicating superior model performance. The evaluation metric provides a comprehensive assessment, considering both discriminatory power and sensitivity in predicting credit defaults.

As shown in Figure 6, this metric has a graphical interpretation, optimizing both the area under the red curve and the intersection point with the green line. Illustrated in Figure 5, the normalized Gini coefficient, stretched to obtain the Area Under the Curve (AUC), is represented by the shaded area under the red curve (0 to 1 range). The normalized Gini coefficient is calculated as, ranging from -1 to 1. A larger red area indicates better model performance. Capturing the 4% default rate is achieved by setting the threshold at 4% of the total weighted sample count, resulting in the True Positive Rate or Recall (0 to 1). This corresponds to the coordinate position at the intersection of the green line and the red ROC curve. A higher intersection point indicates superior model performance.

SECTION: IV-CResults

In this section, we compared the performance of various models in the American Express - Default Prediction competition. The evaluated models include deep learning based model: GRU[17], Transformer[18], Tabtransformer[19], and Neural Networks[20]. Machine learning based model: XGBoost[13], LightGBM[12], CatBoost (Local)[15], LightGBM (Local), Local Ensemble[14], and our proposed Ensemble Model. Each model was assessed based on the public and private datasets, with scores provided in Table 1:

Our Ensemble Model outperformed others in both public and private datasets, attaining the highest scores. This highlights the strategic integration of LightGBM, XGBoost, and LocalEnsemble modules, leveraging individual strengths for diversity and generalization. The model sets a new benchmark, offering a comprehensive solution adaptable to diverse scenarios.

SECTION: IV-DFeatures Importance Analysis

To gain a deeper understanding of the role each feature plays in the models, we employed specific methods to calculate and visualize feature importance. In the XGBoost model, we chose Average Gain as the primary criterion, measuring the average performance improvement each feature brings in tree node splits. For the LightGBM model, we utilized Total Information Gain, representing the sum of information gain contributed by a feature across all splitting nodes.

As shown in Figure 7 and Figure 8, through 5-fold cross-validation, we computed feature importance for each fold, presenting overall importance using Box-plots. Averaging results, the XGBoost model, with 5500 features, showed the top 50 features contributing over 90%. A parallel trend was observed in the LightGBM model, highlighting the effectiveness of prioritizing key features in predicting credit defaults, ensuring model interpretability and practical applicability.

SECTION: VConclusion

In conclusion, this study tackles the crucial issue of predicting credit defaults in consumer lending. While subsequent works provided valuable insights, they also had limitations, indicating the necessity for further advancements. Recent studies, such as those employing innovative approaches like XGBoost-LSTM and fusion methods, have contributed to the evolving landscape, shedding light on areas that can be enhanced.

To address existing gaps and challenges, our proposed Ensemble Methods framework, which includes LightGBM, XGBoost, and a LocalEnsemble module, aims to establish a new benchmark for credit default prediction. Each module contributes uniquely by leveraging diverse feature sets, enhancing model diversity, and improving generalization. The Ensemble Model, integrating the strengths of individual modules, demonstrates efficacy on the American Express dataset, presenting a robust and comprehensive solution. Additionally, our design of the Local Ensemble module enhances accuracy by modeling different combinations of features, providing a strategic response to the ongoing pursuit of more effective credit default prediction models. Our experiments affirm the efficacy of the Ensemble Model on the American Express dataset.

SECTION: References