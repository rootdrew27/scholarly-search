SECTION: M2-ViT: Accelerating Hybrid Vision Transformers with Two-Level Mixed Quantization
Although Vision Transformers (ViTs) have achieved significant success, their intensive computations and substantial memory overheads challenge their deployment on edge devices. To address this, efficient ViTs have emerged, typically featuring Convolution-Transformer hybrid architectures to enhance both accuracy and hardware efficiency.
While prior work has explored quantization for efficient ViTs to marry the best of efficient hybrid ViT architectures and quantization, it focuses on uniform quantization and overlooks the potential advantages of mixed quantization.
Meanwhile, although several works have studied mixed quantization for standard ViTs, they are not directly applicable to hybrid ViTs due to their distinct.
To bridge this gap, we present M2-ViT to accelerate Convolution-Transformer hybrid efficient ViTs with two-level mixed quantization.
Specifically, we introduce a hardware-friendly two-level mixed quantization (M2Q) strategy, characterized by both mixed quantizationand mixed quantization(i.e., uniform and power-of-two), to exploit the architectural properties of efficient ViTs. We further build a dedicated accelerator with heterogeneous computing engines to transform our algorithmic benefits into real hardware improvements.
Experimental results validate our effectiveness, showcasing an average ofenergy-delay product (EDP) saving with comparable quantization accuracy compared to the prior work.

SECTION: 
Built upon the self-attention mechanism, Vision Transformers (ViTs) have achieved competitive performance in the computer visionand multi-modalityfields. However, their high computational and memory overheads limit their deployment on resource-constrained edge devices. Particularly, the self-attention mechanism has quadratic computational complexity and is widely recognized as a critical hindrance.
To solve this limitation, various works have proposed efficient ViTs, which incorporate more efficient attention mechanisms with linear complexity and typically feature Convolution-Transformer hybrid architectures.
For example, as depicted in Fig., the state-of-the-art (SOTA) efficient ViT, dubbed EfficientViT, mainly comprises lightweight Multi-Scale Attention (MSA) modules and MBConvs, offering much higher accuracy and better hardware efficiency than standard ViTs.

In parallel, model quantization approximates floating-point weights/activations using integers, standing out as another effective way to enhance ViTs’ efficiency.
For example,
FQ-ViTidentifies quantization challenges within standard ViTs and introduces dedicated approaches to address them.
Furthermore, Trio-ViTdelves into the quantization and acceleration for Convolution-Transformer hybrid efficient ViTs, thus advancing the hardware efficiency frontier. Despite Trio-ViT’s success in marrying the benefits of both efficient hybrid ViT architectures and quantization, it primarily focuses on uniform quantization and overlooks the potential improvements introduced by mixed quantization. While several workshave explored mixed quantization, they are dedicated to standard ViTs and not directly applicable to efficient hybrid ViTs, due to their distinct.

To close this gap, we propose, targeting the mixed quantization and acceleration for Convolution-Transformer hybrid efficient ViTs, and make the following contributions:

We first introduce a hardware-friendlyapproach that features bothandto fully exploit the architectural properties of efficient ViTs.
Specifically, (1) for the memory-intensive lightweight layers,
we investigate the potential ofto reduce their bandwidth requirement.
(2) For computation-intensive layers, by analyzing their weight distributions, we explore the potential ofto boost computational efficiency.

To translate our algorithmic advantages into real hardware efficiency, we develop anequipped withto accommodate (1) our M2Q strategy with both mixed quantization precision and mixed quantization schemes, and (2) the Convolution- Transformer hybrid architecture inherent in efficient ViTs.

We conduct experiments to validate our effectiveness. Particularly, compared to the prior work Trio-ViT, we achieve an averageenergy-delay product (EDP) saving with comparable () quantization accuracy.

SECTION: 
SECTION: 
As depicted in Fig., EfficientViT has two key attributes: lightweight attention with linear computational complexity to enhance hardware efficiency and Convolution-Transformer hybrid architecture to boost performance. Specifically, EfficientViT primarily comprises MBConvsfor local information processing and lightweight MSAs for global information extraction.
Eachconsists of two pointwise convolutions (PWConvs) sandwiched by a depthwise convolution (DWConv).
Besides, the ReLU-based global attention is the core component in each.
This component substitutes the Softmax function in vanilla self-attention with a ReLU-based similarity function, which enables the utilization of the associative property of multiplications to decrease computational complexity from quadratic to linear.

SECTION: 
Uniform quantization is a basic and most widely adopted quantization method, which converts the floating-pointinto-bit integeras follows:

whereandare the scaling factor and zero point, respectively, and they can be determined as follows:

When performing convolutions, each input channel is processed by the corresponding weight channel within filters and then summed along the channel dimension to produce output. Thus, to eliminate floating-point computations, input activations are generallyquantized, using a common scaling factor for all channels and thus allowing summations to be performed in the integer domain.
Similarly, for weights, since summations are constrained to channels within filters, they are typicallyquantized, where all weight channels within each filter share the same scaling factor.

SECTION: 
SECTION: 
As shown in Fig., EfficientViT primarily comprises three types of layers: (1), where each filterhas only one channel to process the corresponding input channel for obtaining the output, (2), which is equivalent to generic convolutions withkernel size, and (3) matrix multiplications () within lightweight MSAs.we divide these layers into two categories: (1), including PWConvs and MatMuls, and (2), such as DWConvs, and investigate their algorithmic properties to explore advanced quantization opportunities.

1) Computation-Intensive Layers.To identify potential quantization opportunities, we first choose PWConvs as representative layers and visualize their weight distributions. As seen, the weight distributions across different filters vary from Uniform (Fig.a) to Gaussian (Fig.b). This observation can be also noted in MatMuls. This variation indicates that merely adopting uniform quantization is sub-optimal and enables the integration of Power-of-Two (PoT) quantization to enhance hardware efficiency.
Specifically, uniform quantization uniformly distributes quantization bins across all values, making it more appropriate for filters with Uniform distributions.
In contrast, PoT quantization (see Fig.c), which allocates more quantization bins for smaller values, is more suitable for filters with Gaussian distributions. Formally, it is expressed as:

andare floating-point and PoT quantized weights, respectively.denotes quantization bit-width.represents the scaling factor and can be determined by=-, aiming to re-scaleto [,].
For example, if=,=,=, then=and=.
By doing this, multiplications between activationsand PoT quantized weightscan be substituted with bitwise shifts, as formulated in Eq. (), thus significantly reducing computational costs.

2) Memory-Intensive Layers (DWConvs).Each filter in DWConvs features only one weight channel for handling the corresponding input channel to generate output, which significantly lowers computational costs but limits data reuse opportunities and increases bandwidth demand. Thus, the primary challenge in DWConvs is improving data access efficiency. Fortunately, the small amount of weights per filter of DWConvs inherently offers an opportunity to implement low-bit filter-wise quantization, reducing the bandwidth requirement.

SECTION: 
Note that our M2Q is exclusively applied to weights, with activations still using standard-bit uniform quantization due to their higher quantization sensitivity.

1) Mixed Quantization Schemes for Computation-Intensive Layers.As explained in Sec.-1), the heterogeneous weight distributions of computation-intensive layers offer us an opportunity to use PoT quantization to reduce computational costs. However, as demonstrated in Table, it inevitably yields accuracy drops compared to-bit uniform quantization. This is because PoT quantization prioritizes accurately representing smaller values near zero, while overlooking bigger values that contribute significantly to final outputs.
To better balance the major small weights and the minor but important big weights within filters exhibiting Gaussian distributions, we shift our attention to additive PoT (APoT) quantization, which is essentially the combination of two PoT components:

where/are PoT values similarly in Eq. ().
By combining two PoT components, APoT strikes a balance between PoT and uniform quantization (see Fig.d),
thus enhancing hardware efficiency while maintaining accuracy (see Table).

After identifying appropriate quantization schemes, the key challenge lies in automatically assigning different quantization schemes to filters with distinct distributions.Thus, to enable automatic allocation and save human efforts, we employ the widely adopted Mean Squared Error (MSE)to select the optimal quantization scheme that minimizes quantization error for each filter:

whereare floating-point andare quantized weights.

2) Low-Bit Quantization for Memory-Intensive Layers.As discussed in Sec.-2), the small amount of weights in each filter of DWConvs enable low-bit quantization to reduce bandwidth requirements.
To determine the optimal bit-width, we quantize DWConvs’ weights in EfficientViT from-bit to-bit. As listed in Table, quantization at-bit or greater yields negligible accuracy drops compared to the full-precision counterpart, which supports our low-bit quantization hypothesis. Considering both quantization accuracy and hardware efficiency, we choose-bit filter-wise quantization for DWConvs.

SECTION: 
The diverse operation types within the hybrid architecture of EfficientViTand the mixed quantization precision (bit andbit) and schemes (uniform and APoT) introduced by our M2Q strategy challenge the translation of our algorithmic benefits into real hardware benefits. Specifically, there are mainly three kinds of operations: (a) memory-intensive layers (DWConvs) with-bit uniform quantization; computation-intensive layers (PWConvs/MatMuls) with (b)-bit uniform and (c) APoT quantization.

. As depicted in Fig.a, our accelerator comprises a controller to provide global control signals, global buffers to store inputs and weights, andcomputing cores to process different batches. Specifically, each computing core includes a local controller to support pre-defined dataflow and an auxiliary buffer for caching intermediate results. Besides, it integrates aMixed-Precision Multiplication Array (MPMA), which features precision-scaleable multipliers to effectively support (a)-bit DWConvs and (b)-bit PWConvs/MatMuls, and aShifters and Adder Tree (SAT) engineequipped with multiple shifters to efficiently handle (c) PWConvs/MatMuls with APoT quantization. We will illustrate the two components - MPMA and SAT, in detail next.

1) Mixed-Precision Multiplication Array (MPMA).To accommodate, which differ in both computation patterns and quantization bits, our MPMA is configured to operate in two distinct modes - single mode and merged mode, to respectively handle the aforementioned two types of operations.

a) Single Mode:As each filter in DWConvs features only one weight channel to handle the corresponding input channel, input reuse is not available across different filters. To seize the available output reuse and weight reuse opportunities, we design adataflow for DWConvs.Additionally, as shown in, byusing shift registers, we exploit the reuse of input pixels from overlaps between adjacent sliding windows. Specifically, during each cycle, different input columns are pipelined and transmitted across PE tiles to generate partial sums for adjacent output pixels, which are then accumulated along cycles to obtain final outputs.

b) Merged Mode:To support-bit PWConvs and MatMuls (can be treated as PWConvs with large batch size), MPMA operates in merged mode,To facilitate data reuse, we design adataflow. As depicted in,, providing parallelism ofand enhancing output reuse. Meanwhile,, offering parallelism ofalong the output channel while enhancing input reuse.

2) Shifter and Adder Tree (SAT).To support PWConvs/ MatMuls with APoT quantization, we develop a dedicated SAT engine comprising multiple shifters. As shown in, SAT containsprocessing tiles, each containingshifter units (SU) and an adder tree.To facilitate data reuse, we proposedataflow here, similar to the merged mode of MPMA in Sec.-1b.The generated partial sums are then directly aggregated by the adder tree to enhance output reuse.

As our accelerator integrates heterogeneous computing engines to support different types of operations,
we adopt pipeline processing to enhance hardware utilization. Specifically, (1) for MatMuls with APoT quantization, which are followed by MatMuls with uniform quantization in our algorithm, APoT-quantized MatMuls are first executed by SAT. The generated outputs are immediately sent to MPMA to serve as inputs for subsequent uniform-quantized MatMuls, enabling parallel processing and enhancing hardware utilization.
(2) For DWConvs, which are typically followed by PWConvs in EfficientViT, when MPMA executes DWConvs, the generated outputs are promptly directed to SAT to serve as inputs for the subsequent PWConvs with APoT quantization. Once MPMA finishes DWConvs, it is reallocated to compute filters of PWConvs with uniform quantization, allowing parallel execution with SAT.

SECTION: 
SECTION: 
Our algorithm is built uponto further explore mixed quantization.
Following, we randomly sampleimages from ImageNet’straining set as calibration data and test on ImageNet’s validation set.We compare with Trio-ViT, a SOTA quantization approach dedicated to EfficientViTusing uniform-bit quantization,in terms of top-accuracy.

The parallelism of computing cores in our accelerator(see Fig.) is configured to. To obtain unit energy and overall power, we synthesize our accelerator with Synopsys Design Compiler under anm TSMC technology andMHz clock frequency.We consider five baselines: (i) full-precision EfficientViT(hybrid architecture) executed on the Edge CPU (Qualcomm Snapdragon 8Gen1 CPU); (ii) half-precision EfficientViT executed on the NVIDIA Jetson Nano GPU; (iii) mixed-quantized DeiTand (iv) uniform-quantized Swin-Trespectively executed on their dedicated accelerators; and (v) uniform-quantized EfficientViT on its dedicated accelerator, which is our most competitive baseline and thus we also implement on ASIC for a fair comparison. We compare them on throughput, energy efficiency,latency,, and energy-delay product (EDP).

SECTION: 
As shown in Table,
thanks to the incorporation of both APoT and low-bit quantization (-bit) in our M2Q strategy,
we can reduce an average ofcomputationalwith comparable accuracy (an average of) compared to the SOTA quantization baseline Trio-ViT.

From Tablewe can see that:Compared toexecuted on CPU/GPU, we achievethroughput andenergy efficiency.When compared with mixed-quantized DeiTand uniform-quantized Swin-Ton their dedicated accelerators, we offerandthroughput, respectively.As for our most competitive baseline, uniform-quantized EfficientViT on its acceleratorand implemented on the same ASIC technology as us, we can gainenergy efficiency,andEDP saving.

SECTION: 
In this brief, we propose M2-ViT to marry the hardware efficiency of both quantization and hybrid Vision Transformers (ViTs). We first develop a two-level mixed quantization (M2Q) strategy upon the SOTA EfficientViTto fully exploit its architectural properties. To translate our algorithmic advantages into real hardware efficiency, we further design a dedicated accelerator with heterogeneous computing engines. We finally conduct experiments to validate our effectiveness.

SECTION: References