SECTION: Understanding Factual Recall in Transformers via Associative Memories
Large language models have demonstrated an impressive ability to perform factual recall. Prior work has found that transformers trained on factual recall tasks can store information at a rate proportional to their parameter count. In our work, we show that shallow transformers can use a combination of associative memories to obtain such near optimal storage capacity. We begin by proving that the storage capacities of both linear and MLP associative memories scale linearly with parameter count. We next introduce a synthetic factual recall task, and prove that a transformer with a single layer of self-attention followed by an MLP can obtain 100% accuracy on the task whenever either the total number of self-attention parameters or MLP parameters scales (up to log factors) linearly with the number of facts. In particular, the transformer can trade off between using the value matrices or the MLP as an associative memory to store the dataset of facts. We complement these expressivity results with an analysis of the gradient flow trajectory of a simplified linear attention model trained on our factual recall task, where we show that the model exhibits sequential learning behavior.

SECTION: Introduction
One hallmark capability of transformer-based large language models (LLMs) is factual recall. Given a prompt of the form “In what year was George Washington born?” an LLM will correctly respond with “1732.” Language models thus act as databases, storing somewhere in their parameters mappings of the form (George Washington, birth year)which can be easily accessed during inference time.

Prior workhas observed that transformers trained on factual recall tasks can store information at a rate proportional to their parameter count. Other studieshave sought to understand the specific mechanism by which transformers implement factual recall, probing models to understand specifically which transformer blocks “contain” certain facts. However, these studies do not consider the memorization capacity of such constructions, and it is thus an open question to understand how transformers optimally encode such factual information within their weights.

In this work, we show that shallow transformers can use a combination ofassociative memoriesto obtain near-optimal storage capacity for factual recall tasks. Associative memories store pairs of input-output embeddings through their outer products, and are thus well-suited for modeling the weight matrices of a transformer. Prior workhas shown that this associative memory model is a key primitive towards understanding both the representational capacity and optimization dynamics of transformers on synthetic tasks.

Our specific contributions are as follows:

Inwe begin by studying the ability of linear and MLP associative memory models to store associations between discrete vocabularies. We prove that when the embeddings are sampled randomly over the sphere, these models can store a number of associations proportional to their parameter count, significantly improving over the case where the embeddings are orthogonal.

In, we introduce a synthetic next-token prediction task which models factual recall. The data distribution consists of prompts containing a subject tokenand relation tokenhidden amongst a set of noise tokens, which the learner must map to a ground truth answer. Our main theorem is that a transformer consisting of a single multi-head self-attention layer followed by an MLP can obtain 100% accuracy wheneitherthe number of self-attention parameters or MLP parameters scales (up to logs) proportionally with the dataset size.

In, we study the gradient descent dynamics of a single linear self-attention head trained on the synthetic task. We prove that the model undergoes a sequential learning dynamics, consisting of a “hallucination” stage where the model outputs the conditional distribution for the answer based on only the relation.

Finally, inwe complement our constructions with lower bounds, showing that they are optimal up to logarithmic factors.

Overall, our work makes progress towards understanding the mechanism by which transformers learn and store factual information.

SECTION: Related Work
Associative memories have a long history in the neural computation literature.
More recently there has been renewed interest in extensions of such models with larger capacity. These have been linked to the attention blocks in Transformers, within particular using the connection between self-attention and associative memories to design new variants of the attention module.show that overparameterized autoencoders can also behave as associative memories. However, these connections differs from our work, where we consider instead the role of both self-attention and MLP weights as associative memories, in a similar vein to.

Large language models are known to store vast amounts of factual knowledge in their weights.
Several recent works in the mechanistic interpretability literature have attempted to understand how transformers store facts.empirically studied the memorization capacity for Transformer language models of different sizes trained on synthetic factual recall tasks, and observed near-linear scaling with the number of parameters.demonstrate how shallow transformers can solve a related latent concept association task by viewing the weight matrices as associative memories. At a more basic level, several works have studied the memorization capacity of neural networks, using constructions that differ from our associative memory approach, both in the context of regressionand (next) token prediction.

Training dynamics of transformer models on various tasks has been a popular recent line of research.studied training dynamics of transformers with linear attention on in-context learning tasks.studied the fine-tuning dynamics on a similar factual recall task, showing how training on lesser-known facts may hurt performance. Our emphasis differs in that we consider non-orthogonal embeddings, and require the model to additionally filter out the relevant subject and relation tokens from the noise tokens, which requires learning of the key and query matrices.

SECTION: Associative Memories
In this section, we show that associative memories have a storage capacity on the order of the number of parameters (up to logarithmic factors), which is near-optimal (as we show in Section).

Our setting follows that of. Letbe the set of input tokens, andbe the set of output tokens. Our goal is to store a set of associations given by the function. For each input tokenwe assign a corresponding embedding vector, and likewise for each output tokenwe associate an unembedding vector. We primarily focus on the setting where the embeddingsandare drawn i.i.d uniformly from the sphere of radius 1. Letbe our model which “stores” the associations. Given such an, the predictionforis given by the arg-max decoding.

SECTION: Linear Associative Memories
We first consider the case whereis a linear map.

Sincehasparameters,shows that the number of associations that can be stored scales (up to log factors) linearly in the number of parameters. We note that in this linear case, the injectivity assumption onis important, as otherwise the capacity may be as low as, as in. Additionally, we remark that these constructions are easily obtained by gradient descent; the generalconstruction corresponds to one-step of GD on the correlation loss, while the low-rank construction corresponds to parameterizingfor, and taking one step of GD onwhileis fixed to random Gaussian initialization. The proof ofis deferred to.

Our setting bears similarity to associative Hopfield networks, yet differs in that we decode to a fixed discrete set of output tokensrather than exactly matching the target output. This more closely resembles the language modeling framework, and allows us to improve the memorization capacity fromto. Next, we note that non-orthogonality of the embeddings is necessary for, as the optimal storage capacity for one-hot embeddings is only. Since our constructions are in the regime, the associative memoryis asuperpositionof the outer products. Finally, we remark that the random, rather than trainable, embeddings setting was also studied in. The embeddings can be viewed as global quantities in a larger network, of which the associative memory is implementing some subtask, and is thus not able to optimize these embeddings in order to solve its specific task.

SECTION: MLP Associative Memories
Next, we consider the case whereis a two-layer neural network with hidden width; that isfor.

Since the MLP hasparameters,shows that the MLP associative memory scheme has storage capacity which is (nearly) linear in the parameter count.

The construction formimics that of, after an appropriate random feature transformation. First, sample the rows offrom the standard Gaussian. Then, set each, whereis theth Hermite polynomial (see). We then see that

for sufficiently large. Suchpolynomialassociative memory is reminiscent of that in, and can store many more associations for large. By choosingand appropriately dealing with concentration, one can obtain thestorage capacity (for technical reasons, we must also use the Neural Tangent Kernelrather the random feature model). The full proof ofis deferred to.

Prior worksstudying the memorization capacity of neural networks focus on the regression setting, and thus do not directly apply to our setup with multiple outputs and a discrete set of output tokens. Other worksshow that one can memorizearbitrary labels withparameters, at the expense of using a bit complexity of. Such networks still requirebits, which matches our lower bounds in. These constructions, however, are unwieldy, and are not learnable if we restrict the precision to be. Instead, our constructions are learnable – the linear construction results from one step of GD, while the ReLU construction uses the NTK and can thus be learned via GD on a convex loss. In, we show that a quantized version of the construction fromindeed succeeds with bit precision, and thus more accurately captures realistic training regimes where models do seem to succeed with low precision.

In, we train both linear and MLP associative memories to store the association. Given a fixed model size, we fit datasets with increasing values ofusing the cross entropy loss, and plot the largest value offor which we can obtain at least 99% accuracy. We observe that the linear associative memory can storeassociations, while the MLP associative memory can storeassociations. Seefor additional experiments where the number of output tokensdoes not scale with.

SECTION: A Synthetic Task for Factual Recall
In this section we introduce a synthetic factual recall task, and show that one-layer transformers constructed via associative memories can store a number of facts proportional to parameter count.

SECTION: The Task
We first define a global dictionary of facts. Letbe a set of subject tokens andbe a set of relation tokens, where. Letbe the set of answer tokens. We letbe the ground truth association function, which maps subject-relation tuplesto their corresponding answer. A similar such task was considered in.

Defineto be the set of answers corresponding to a relation, i.e. Defineanalogously. We assume that each relation corresponds to a disjoint set of answers:

For example,could be the set of all countries, whilecould be; in this case, the set of all presidents and set of all capitals are disjoint.

We next define our data distributionover sequences. Letbe the context length. Letbe a set of noise tokens, and define the vocabulary to be, whereis a special “end-of-sequence” token. The data distribution is over lengthsequences, generated via the following procedure:

First, sample a subject and relation tuplefrom some distributionover.

Next, sample two distinct indices. Setand.

For the remainder of tokenswhere, drawuniformly at random from the noise tokens.

Set.

Finally, set.

The goal of this task is to predictfrom. A model which can successfully do so must (1) be able to isolate the relevant subject and relation tokens from the noise tokens and (2) store all of the associations. Seefor a diagram of the task.

SECTION: The Model: One-Layer Transformer
Our learner for the task is a single layer of multi-head self attention followed by an MLP. Defineto be the embedding dimension. The input to the transformer is a sequence of vectors. Each self attention head is parameterized by the key, query, and value matrices, whereis thehead dimension. The self attention head is then a map, which operates as

whereis the softmax operator.

A multi-head self-attention layer withheads is parameterized bydifferent key, query, and value matrices, along withoutput matrices. Let, where. A multi-head self-attention layer is then a mapgiven by

Finally, a single-layer transformer combines a multi-head self-attention layer with an MLP. Letbe the MLP width. Letbe the MLP parameters, and define. Then, a single-layer transformer is the mapgiven by

A single-layer transformer is parameterized by the tuple of hyperparameters. The model hasself-attention parameters, andMLP parameters.

SECTION: One-Layer Transformers have (Almost) Linear Storage Capacity
We next characterize how large a single-layer transformer must be in order to obtain 100% accuracy on the synthetic task. For each token, sample its embedding vectorsi.i.d uniformly over the sphere of radius 1. An input sequencegets embedded as. We use argmax decoding to predict the next token; that is,

Our first result is that there exists an attention-only single-layer transformer that obtain 100% accuracy on the factual recall task, as long as the total number of self-attention parametersscales (up to logarithmic factors) linearly with the dataset size.

We next show that a single-layer transformer with an MLP can obtain 100% accuracy on the factual recall task, if the number of MLP parametersscales linearly with the dataset size:

The proofs ofandare deferred to.

andeach have two main constraints on the size of the architecture needed to obtain 100% accuracy. First, the quantitymust be larger than. This corresponds to self-attention having sufficient capacity to filter out the tokens infrom the noise tokens. For the attention-only architecture, we additionally require. When, the total number of parametersis (up to logs) at least the total number of facts. For the MLP construction, the second condition is that the number of MLP parameters,, scales nearly linearly with the number of facts. As such, as long as either the total number of self-attention parameters or the total number of MLP parameters is large enough, 100% accuracy can be obtained. The single-layer transformer can thus trade off MLP and self-attention parameters while still maintaining perfect accuracy. This phenomenon is reflected in the experiments in. We remark that it is straightforward to extend our construction to the case where we only need to store a sizesubset of, where the constraints now become.

andboth utilize the associative memory framework of. First, the key and query matrices of each self-attention head act as adenoiser, selecting the relevant subject and relation tokens inwhile ignoring the noise tokens. To theth attention head, we associate a subsetof subject and relation tokens. Then, setting

for a large constant, we see that theth head will only attend to the tokens in the subset. We remark that since the embeddings are-dimensional, at mostembeddings can be in superposition, and thus we must have.

For the attention-only construction, the output-value matrixacts as a linear associative memory, mapping eachinto a superposition of all possible answers associated with the subject/relation. Lettingbe a projection onto a random-dimensional subspace of, we set

In, we show that this construction stores at mosttokens per head (i.e), and requires the dimension to scale with the number of elements in superposition (i.e). Since, and thepartition, it suffices to takeand.

For the MLP construction, we instead associate the subsetwithattention heads. This is equivalent to having a single full-rank attention head per subset. We set the aggregate output-value matrix to the identity, so that the output of the self-attention layer is. Finally, the MLP layer acts as an MLP associative memory, mappingtofor eachpair. Via a similar computation to, it suffices to make the total number of parametersbe. Since thepartition, it suffices to takeas well. Seefor a diagram describing both constructions.

SECTION: Empirical Validation
We next empirically validate the claims ofandthat 100% accuracy can be obtained as long as either the total number of self-attention or MLP parameters scales with. We further observe that 100% accuracy can be achieved as long as thetotalnumber of parameters scales with, providing evidence that the model can simultaneously use attention and the MLP to store facts.

In, we train a wide range of models of various “shapes” on datasets of varying sizes. A model shape is defined by the tuple, and corresponds to the family of models satisfyingand. The total number of model parameters is, which can thus be varied by increasing. For a fixed model size, we binary search on the largest dataset size that can be memorized. Specifically, we fixand varyjointly as. Experiments with different scalings are considered in. For each, the fact dataset is generated at random by selecting,, and for eachsamplinguniformly at random from. We say the dataset was successfully memorized, and as suchfacts were stored, if the model can obtain an accuracy of at least 99%.

On the left panel ofwe observe that, across different model shapes, the maximum number of facts stored scales linearly with the total number of parameters. On the right panel, we consider a specific dataset with, and plot the accuracy as the number of parameters vary. We observe that the model can trade off MLP parameters for self-attention parameters, while still maintaining an accuracy of near 1. However, we do still require the total number of attention parameters to be large enough; this corresponds to theconstraint.

SECTION: Optimization Dynamics
We next study the optimization dynamics of the factual recall task. To simplify the model, we consider a linear attention transformer (i.e., the softmax is replaced with the identity map) with orthogonal embeddings. We set, and let the embedding vectorssatisfy. Such linear attention or orthogonal embeddings assumptions are common in prior works studying the gradient descent dynamics of transformers.

The linear attention model is given by

where we setand let,denote the non-factorized output-value and key-query matrices.
Letbe the predicted next token distribution on an input sequence, i.e

One can then rewrite the cross entropy loss as

We would like to characterize the output of running gradient flow, (i.e) with respect to the non-factorized parameterson the cross-entropy loss (). For notational convenience, we denote, and note that by isometry gradient flow onis equivalent to gradient flow on these quantities.

Let us assume that we start from the following “balanced” initialization.

Our first result is that the gradient flow indeed converges to zero loss. As a consequence, the predicted next token probabilitiesconverge to, whereare the subject and relation contained in the sequnece.

We next show that the model undergoes a sequential learning dynamics. Let us assume that the number of subjectsis much greater than the number of facts. We show that during the first stage of training only theandcomponents grow for relations, while the remainder of the parameters stay close to zero. As such, the model gets close to outputting the best predictor based on just the relation token. Defineto be the conditional distribution of the answer, given the relation, i.e

Proofs ofandare deferred to.

tells us that at some intermediate time, the prediction of the modelis approximately equal to, the conditional distribution of the answer given the relation. At this stage, the model ignores all other tokens in the sequence– including the useful subject token– and predicts based only on the relation. For example, ifis the set of all countries andis the relation “capital,” then on the prompt “What is the capital of France?” the model will output a random countries’ capital. We view this as an instance ofhallucination: the model is outputting a plausible, yet ultimately incorrect, answer to the prompt. We remark that without the assumption that, it is possible for this intermediate hallucination stage to exhibit different behavior.

We next empirically verifyand. We first train the linear attention model with orthogonal embeddings () withand, and plot the loss over time. In the left pane of, we observe three distinct stages. At the start of training, the prediction is close to uniform over all possible answers, and the model obtains a loss of. Next, the loss plateaus at, and the model outputs the conditional distribution ofgiven the relation. Finally, as training continues, the model escapes the plateau and converges to zero loss.
We include the “relation-only loss” in the plot, defined as, where any probability mass assigned to an answer which is valid for the relationis considered to be correct; the subject-only loss is defined analogously.

In the right pane of, we plot the loss of a single softmax attention head with random embeddings trained on the same factual recall task. We observe similar phenomenology as for linear attention, and identify an intermediate “hallucination” stage where the relation-only loss drops to zero, but the subject-only loss is still far from zero.

SECTION: Lower Bounds
In this section, we argue via information-theoretic arguments that the results fromandare optimal up to logarithmic factors. Proofs are deferred to.

Letandbe the input and output vocabularies, respectively. To establish a lower bound, we must consider adistributionover association functions. For each, assume that the outputis sampled independently from the uniform distribution over. We model the learning protocol as follows. At train time, the learner observes the randomly sampled ground truth, and writes down abit model. At test time, the learner generates a set of predictionsfrom, whereis the prediction for. Both the mappingsandcan be randomized. Letbe a probability distribution over the input space; assume WLOG that. The goal of the learner is to minimize the cross entropy loss

We thus see that in order to obtain zero loss, the learner must usebits; this matches the construction fromup to log factors. As a corollary of, we can obtain scaling law lower bounds with respect to model size.

This lower bound is obtained by the MLP associative memory by storing the most probableassociations. This matches the scaling law with respect to model size considered in, which also considered storing themost frequent associations.

The constructions inrequire storingnetwork parameters, along with input and output embeddings. We viewinas containing only the network parameters, while the embeddings are “global” quantities, independent of the ground truth, used to compute the predictions. This matches our interpretation of the embeddings as fixed global quantities which cannot be modified by the associative memory. We remark that the associative memory constructions frommatch the lower bound, since they hold for-bit precision ().

We next prove a lower bound for the factual recall task; a similar bound was proven in. Letandbe the fixed set of subjects and relations andbe the full vocabulary, where. The association functionis sampled randomly as follows. First, for each relation, the answer setis chosen to be a uniformly random sizesubset of, conditional on all subsetsbeing disjoint. For each, the answeris sampled uniformly at random from. The learner sees the association, writes down abit model, and fromgenerates a set of predictions, whereis the prediction for. We lower bound, the expected cross entropy loss with respect to a distributionover, defined as follows:

We thus see thatparameters are needed to achieve a loss of zero. For this lower bound, the learner knows the setsandand does not have to distinguish them from the noise tokens, making it a strictly easier problem than the factual recall task in.

SECTION: Discussion
In this work, we showed that shallow transformers can use associative memories to obtain near optimal storage capacity for factual recall tasks. Furthermore, by studying the optimization dynamics of a simplified model, we also showed that transformers undergo an intermediate hallucination stage. One interesting direction of future work is to better understand the role of the embeddings, and whether there exists an optimal choice of (non-random) embeddings leading to more efficient constructions. Another important direction is to understand the implication of our results towards understanding empirical LLM scaling laws. In particular, does there exist a scaling law lower bound for the factual recall task? Finally, it would be very interesting to understand the extent to which larger models utilize similar associative memory constructions, and if one can probe whether specific “facts” are stored in either the self-attention matrices or the MLP.

SECTION: Acknowledgements
JDL acknowledges support of NSF CCF 2002272, NSF IIS 2107304, NSF CIF 2212262, ONR Young Investigator Award, and NSF CAREER Award 2144994. This work was conducted in part at the Simons Institute.

SECTION: References
SECTION: Additional Experiments
SECTION: MLP Associative Memory
In, we train MLP associative memories to store the association. We fixthroughout. In this case, we see that the number of associationswhich can be stored by a model withparameters scales as. This linear scaling in the absence of logarithmic factors is due to the fact that the number of output tokensis a constant, and does not scale with the number of input tokens.

SECTION: Factual Recall
In,and, we repeat the experiment in the left pane of, for different choices ofand scalings of. In all plots, we observe the general trend that the number of facts stored scales proportionally to the number of model parameters.

SECTION: Experimental Details
For the MLP associative memory experiments, for each choice of, we first sample random embeddingsi.i.d uniformly over the sphere. We train a two-layer neural network on the cross entropy loss to predict the association. We use standard parameterization and initialization, and the activation. The network is trained using ADAM with a learning rate offorsteps. We compute the maximum accuracy the network achieves over the training run, and say that the network has “stored” the dataset if the highest accuracy is at least. We repeat this procedure to binary search over, to find the largest value ofsuch that the network achieves an accuracy of at least. Error bars are shown for 5 random seeds.

We consider a fixed prompt length of, and train the models via online batch gradient descent with batch size 1024 on the population loss (i.e we sample an independent batch at each timestep). We use standard parameterization and initialization for both self-attention and the MLP. For a fixed model size, we binary search over the maximum value ofsuch that the model achieves an accuracy of at least. All models were trained using ADAM forsteps, with a sweep over learning rates in(where we consider the best performing model over all learning rates).

In the left pane we train a linear attention head with orthogonal embeddings. The weights are all initialized to be equal to. In the right plot, we train a softmax attention head with random embeddings, which are fixed throughout training.

SECTION: Proofs for
Let us set. For, define the quantityby

We first see that (where the expectation is taken over the randomness of the embedding vectors)

We can next compute the second moment of. Since theare drawn uniformly on the sphere, theterms forare independent and mean zero. Therefore

Therefore

Letbe a fixed failure probability, and let. Observe thatis a degree 4 polynomial. By, by choosingandfor a sufficiently large constant, we have that, and thus.

Therefore union bounding over allpairs with, we have that

Thus with probability,for alland, and on this eventfor all.

∎

For, we need the following assumption on the activation.

We prove the following formal version of.

Let us consider the linearization, or Neural Tangent Kernel, of:

whereis the initialization which we are linearizing with respect to. By rescaling the parameters asand, we see thatas. It thus suffices to work withinstead of. For ease of notation, we redefineas, andas, so that

Letbe a even integer, to be chosen later. Assume without loss of generality that(if it is negative, we can simply negate all thein the construction below). Set

whereis theth Hermite polynomial. Then

As in the proof of, define the margin betweenand someas

We will show that, with high probability over the draw of the embeddings over the sphere,andtheindependently from the standard Gaussian, thatfor all.

The expectation of the margin is

We next compute the variance. Defineas

so that. First, observe that when, we have, sinceis even. For, we have that

First, bywe have that

Next, we see that

Finally, we have that

The first quantity can be bounded as

The second term is bounded as

By Gaussian hypercontractivity (),

and likewise

Finally,if, and 1 otherwise. Altogether,

Altogether, we get that

The first quantity is

The second quantity is

Therefore

Choose; then

for, and so

Observe thatis a degreepolynomial. Iffor unspecified constant, then, and thus by, we have that. Choosingand union bounding over allpairs withyields the desired result.

∎

SECTION: Bounded Bit Complexity
One sees from the proof ofthat, with high probability over the embeddings, the weight matrixhas a marginsatisfiesfor all. Each entry oflies in the interval. For some, defineby rounding each entry ofto the nearest multiple of. By definition,. We also see that

Thus choosing, the margin of the quantized network satisfies

Finally, the number of bits required to store each weight is.
∎

We remark that a similar quantization argument was proven in.

SECTION: Proofs for
Defineas

We first see that.

Next, observe that

Since each of theare independent subGaussian variables with variance proxy, by Hoeffding’s inequality we have that, with probability,

Settingand union bounding over allyields the desired result.
∎

SECTION: Construction via Self-Attention
Define

We first see that

We next compute the variance. For,

Define. We see thatis nonzero only ifand. For, we have that

whereAlso,

Since,

Altogether,

and thus

sinceNext, sinceis a degree 6 polynomial, with probabilitywe have that

Union bounding over all,yields the desired result.
∎

Let us state the formal version ofwhich we aim to prove:

When, one can obtain an accuracy of 100% whenever the total parameter count is

Partitioninto the setsandinto the sets, such thatand.

Let us chooseso that. The total number of attention heads is then

For each, we construct the attention headas follows. First, let

for a large constant. Next, set

forsampled uniformly on the sphere, orthogonal to.

Consider an input sequence, and letbe the subject token in this sequence. On the event thatholds, if, then, and thus

for all. As, the self-attention module fully attends to thetoken. On the other hand, if, then ifwe have

for all. Likewise, as, the softmax converges to a hardmax on thetoken. Altogether, we get that

Next, on the event thatholds, since, we have that

for. Defining, we have that

By an identical construction, for each, with probabilitywe can construct the attention headsuch that

as long as. Therefore by a union bound, with probabilitywe have that (whereand

If, then, and thus

Otherwise, eitherorisand thus

Therefore. Replacingwithyields the desired result.
∎

SECTION: Construction via MLP
For odd integersto be determined later, let us set

whereis the Hermite tensor of degree(see). Assume without loss of generality that, theth Hermite coefficient ofis positive (the negative case can be handled by negating all thein the construction)

For some, the margin for someis

We first see that

If eitheror, we see that conditioned on, the quantityis mean zero. Therefore the only nonzero term in the sum is when, and so

The quantitiesare subexponential random variables with Orlicz norm, and therefore we can bound

We next compute the variance. Defineby

We first observe thatis zero, unlessand. Next, we compute the expectation of, conditioned on the embeddings (i.e with respect to the randomness):

Therefore for,

When, then

Next, define the quantities

so that

We see that for,

Next, see that

where we have appliedto the first two expectations.
Altogether, we have that

and thus

For the first sum, we can bound

For the second sum, we get that

Altogether,

Letbe a fixed failure probability. We see that, for

wheneverfor appropriately chosen constants. Likewise, setting, we get that

Next, settingandyields

Altogether, by choosing constants appropriately, we get that

Therefore by, with probabilitywe have that. Union bounding over alland settingyields the desired result.
∎

We next state the formal version of, which we wish to prove:

Ignoring polylog factors, and treatingas a constant, the constraints on the architecture size become

We first note that, and sois sufficient. It is possible forto be much smaller; on average we expect, and we also note that it is possible for. The main constraint is that, i.e that the number of MLP parameters scales linearly with the number of facts that need to be stored.

Partitioninto the setsandinto the sets, such thatand. Assume that

Let. For each, we construct theattention heads corresponding toas follows. First, for all such, let

for a large constant. By an identical argument to as in, on the event thatholds we have that

The total contribution from these attention heads is then

Since, we can letbe a projection onto adimensional subspace, orthogonal to, and thus

Altogether, if the sequencecontains the subject, then

Similarly, if we letbe adimensional subspace orthogonal toand, then we can construct the attention headssuch that

whereis the relation in the sequence. Such a construction exists with probability. The total number of heads is

The output of the self-attention component is then

On the event thatholds, we have that there exists a two-layer neural networkof widthsuch that

Scalingby a large enough constant ensures that

Union bounding over all the high probability events and settingyields the desired result.
∎

SECTION: Proofs for
SECTION: Preliminaries
Recall that the parameters are, and that the cross entropy loss is

where

We consider running gradient flow:

from the initialization,for some.

We also defineby

and remark that the lossis convex in.

We first see that

Similarly,

Therefore

By a similar computation,

Under gradient flow, we see that

∎

At initialization,. Since this quantity is an invariant of gradient flow, it is impossible for, and thusthroughout the entire trajectory. Furthermore,

and thus.
∎

SECTION: Proof of
Let us select

There exists a timesuch that for all,. Let us set. Now, consider some iteratefor.

First, see that for,

Consider some. Then, and thus

As such, since,

By an identical argument, since, then for

For any, eitheror. Thereforefor all, and thus

There are at mostsequencescontaining, each of which occurs with equal probability. Therefore

for all such. Then, boundingfor,

Altogether, the loss is

as desired.
∎

SECTION: Sequential Learning
The goal of this section is to show that the model learnssequentially; first, the relation components grow, then the subject components grow. This is given formally by

We first prove that weights corresponding to the subject and noise tokens stay bounded during the beginning of the trajectory.

Recall that the update foris

Therefore by Gronwall’s inequality,

Similarly, the update forforis

Again by Gronwall’s inequality,

∎

The following lemma is our key result, and shows that, assuming that the subject and noise weights stay bounded, the relation weights grow until the output of the model approximates the best relation-only prediction.

The proof proceeds in three stages. First, we bound the time required for the relation weights to escape the origin. Next, we prove that the relation weights stay large. Finally, we show convergence.

The gradient flow update onis

We thus have

Define. Observe that

Thus

Likewise,

and thus

Define the vectorby

We see that

Therefore

where the last inequality bounds.

Define. Bywe have that for

Letbe the first time that. If, then

Therefore

forOn this assumption,, and thus we always have.

Defineby

Let us define the relation-only model as

where. We see that

Defineby. We see that, whereis the softmax, and thus

Therefore defining the relation-only lossas

we see that

Since log-sum-exp is 1-strongly-convex, recalling that,

Therefore

We next track the evolution of:

for. Since, this is increasing in.

We first have the bound. Next, we have the bound. Pick some time. Define. We see that

Next, by, forwe have

Plugging in,

Selecting, we get

whenever.

Therefore

Altogether, we get that the loss is

whenever.

Next, we want to show thatstays large. We first show that the relation-only lossis decreasing. We can compute that

Defineby

We observe that

and thus

Likewise,

Therefore

Assume that. Then

i.e

Assuming that, sinceforwe have that

Therefore. As such, we have thatstays belowfor the remainder of the gradient flow trajectory.

By convexity inspace,

Therefore

Next, we can bound the loss decrease by

Since, and, there exists somesuch that

as desired.
∎

To conclude, we must setappropriately in terms ofin order to apply.

Letbe the target accuracy.
Let us choose the initializationso that, whereis chosen so that

In this case, we see that

Sinceis supported on at mostelements, and, we have. Therefore.
Let us compute.
We see that, since,

Similarly, since,

Therefore the assumption holds for. To conclude, we must verify that

But since, the RHS scales withand the RHS scales with, and thus the condition can be obtained for choosingsufficiently large.

Under the setting of paramters the conditions ofare satisfied, and thus the claim holds.
∎

SECTION: Helper Lemma
Both claims follow from the Bihari-LaSalle inequality.

SECTION: Proofs from
SECTION: Associative Memories
is a Markov chain, so by the data processing inequality,

Also, by definition of mutual information

where the last inequality follows sinceis an-bit message. Thus.

Letbe the conditional distribution ofgiven. Consider some fixed.is also a probability distribution over, and thus by Gibbs’ inequality

Therefore, lettingbe the marginal distribution overandthe marginal over,

where in the last step we use the fact thatis uniform over, and plug in the definition of mutual information. The total loss is thus

Since theare independent,

Also,. Therefore equationis minimized whenfor themost frequent tokens. Altogether,

∎

Let, where. We can bound

Therefore

∎

SECTION: Factual Recall
Defineso that

Let us define the expanded dataset. We observe thatis a Markov chain, and thus by the data processing inequality

Next, by the chain rule, we can decompose

where the first inequality uses the fact that theare conditionally independent given the, and the second uses thatis independent ofgiven, for.

We can decompose the first mutual information term, using the fact that theare nearly independent:

We next relateto the loss. The intuition for this lemma is that for a fixedthe quantityis small, then the predictormust contain information about the answer set.

Finally, we relateto the loss. Similarly, the intuition for this lemma is that if the lossis small, thenmust contain information about the true association.

The proofs for,andare deferred to.

Combining,and, we get

whereis a lower order term.

Altogether, we see that in order for all the lossesto equal zero, we require

Furthermore, when, then, and the bound becomes

∎

SECTION: Auxiliary Lemmas
By standard properties of mutual information:

∎

By,

Since eachis a uniformly random subset of, we have. Also, we can bound

Thus

where we used the boundon. Plugging in yields the desired bound.
∎

Letbe a random permutation of. We first aim to relateto. By the data processing inequality,

By,

The tupleis chosen uniformly at random from, conditioned on all thebeing distinct. Therefore, and. Thus

Altogether,

Next, using the definition of mutual information and Gibbs’ inequality,

for any probability distribution. Let us defineas follows. First, define, for a small constantto be chosen later. Next, define

Plugging in, and observing that, we get that

Define. Letbe the event thatfor all. On the event, we can bound

Thus

On, we have the naive bound

Altogether, we have

By Bernstein’s inequality and a union bound (a similar such concentration argument was used in the lower bound proof in), there exists a constantsuch thatfor

as long as. Set,, and define. We have that

and thus

Therefore

Altogether, we have

as desired.
∎

By the definition of mutual information and Gibbs’ inequality,

whereis any distribution over. Let us defineto be

Sincealways, we have that, and thus

∎

SECTION: Technical Lemmas
Hypercontractivity for the Boolean hypercube (which implies hypercontractivity for Gaussian space) and for the sphere are consequences of. To show, one can use similar techniques to the proof of Corollary 12 in.

By Markov’s inequality,

Sinceis a degreepolynomial, bywe have that

Therefore

Setting, we see that whenever

we have

as desired.
∎

SECTION: Hermite Polynomials
Letbe the standard Gaussian in 1 dimension, and letbe the function space of square-integrable functions with respect to this Gaussian measure. The Hermite polynomialsform an orthonormal basis of. In particular,is a degreepolynomial, satisfying

One useful property of Hermite polynomials is the following:

Next, letbe the standard Gaussian indimensions. The function spacehas an orthonormal basis of Hermitetensors, where:

The Hermite tensors satisfy the following useful properties: