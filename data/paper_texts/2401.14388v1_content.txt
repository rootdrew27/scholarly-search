SECTION: Smooth Ranking SVM via Cutting-Plane Method

The most popular classification algorithms are designed to maximize classification accuracy during training. However, this strategy may fail in the presence of class imbalance since it is possible to train models with high accuracy by overfitting to the majority class. On the other hand, the Area Under the Curve (AUC) is a widely used metric to compare classification performance of different algorithms when there is a class imbalance, and various approaches focusing on the direct optimization of this metric during training have been proposed. Among them, SVM-based formulations are especially popular as this formulation allows incorporating different regularization strategies easily. In this work, we develop a prototype learning approach that relies on cutting-plane method, similar to Ranking SVM, to maximize AUC. Our algorithm learns simpler models by iteratively introducing cutting planes, thus overfitting is prevented in an unconventional way. Furthermore, it penalizes the changes in the weights at each iteration to avoid large jumps that might be observed in the test performance, thus facilitating a smooth learning process. Based on the experiments conducted on 73 binary classification datasets, our method yields the best test AUC in 25 datasets among its relevant competitors.

SECTION: 1Introduction

Classification models are frequently deployed in various real-world problems ranging from e-mail spam filters to medical diagnostic tests. Since the main goal in classification is correctly categorizing data into predetermined classes, accuracy is widely used as a target measure for evaluating the performance of different algorithms. However, class imbalance is a common problem that exists in most of the real-world datasets, and it can significantly affect the performance of classifiers that maximize accuracy during training. Moreover, in case of class imbalance, accuracy may be a misleading metric to assess the performance of a classifierRakotomamonjy (2004). For example, in a scenario where only 7% of the emails are spam, the model can easily achieve 93% accuracy without predicting any email as spam, although such a model has no practical usefulness.

Overcoming class imbalance is essential for the success of classification algorithms, and various techniques have been proposed to handle imbalanced datasetsLeevyet al.(2018). While some works tackle this problem at the data-level by using over-sampling and under-sampling methodsKhoshgoftaaret al.(2007); Van Hulseet al.(2007), cost sensitive algorithm-level alternatives that change the loss function during the training process have gained popularity recentlyCaoet al.(2013); Wanget al.(2012). However, cost sensitive learning solutions rely on explicit knowledge of the misclassification costs, which can be unknown in most of the casesAliet al.(2013). Therefore, the use of these solutions can be limited in real-world problems.

On the other hand, approaches maximizing the Area Under the Receiver Operating Characteristics (ROC) Curve (AUC) during training may be better algorithm-level alternatives to consider when there is a class imbalance within a datasetLinget al.(2003). AUC concentrates on the relative prediction scores of samplesBradley (1997), and quantifies the ranking capability of a model based on pairwise comparisons. Although it is primarily a ranking related metric, its applications are extensively used in classification problems as wellCalders and Jaroszewicz (2007); Norton and Uryasev (2019). Furthermore, due to its robustness to class imbalance, AUC is an important criterion to assess the performance of a model, and it is preferred over classification accuracyLinget al.(2003).

Unfortunately, AUC is neither a continuous nor a differentiable functionYanet al.(2003), which makes it difficult to optimize. InChang (2012), a mixed integer programming problem is proposed to maximize the AUC exactly. In theory, solving this problem yields the highest possible training AUC. However, in this formulation, the number of binary variables grows quadratically due to pairwise comparisons. Therefore, it takes significant amount of time to optimize these models in practice. Furthermore, it can lead to overfitting and poor generalization. After relaxing the integrality constraints, AUC can be approximated via a piecewise linear function. Based on this idea, variuous AUC maximizing approaches such as Ranking SVMJoachims (2002), RankBoostFreundet al.(2003), and RankNetBurgeset al.(2005)have been developed. In this sense, the problem formulation we propose in this study has similarities with SVM-based models focusing on AUC maximization.

Most real-world problems exhibit non-linear relations, and the linear models cannot effectively represent these complex patterns. Kernel functions can be helpful to learn complex decision boundaries, and the radial basis function (RBF) kernel is one of the popular choices. This function provides a proximity measure by scaling the Euclidean distance between data points via a shape parameter, which can affect the performance of the models substantiallyFasshauer and Zhang (2007). Therefore, it is essential to choose the right kernel for each problem via parameter tuning operations. On the other hand, the Euclidean distance calculation can be considered as a non-linear transformation of the original raw features, and utilizing the Euclidean distances between data points as features can help the model to learn non-linear relationsDuinet al.(2010). Moreover, it is possible to extract features with high discriminatory power by learning prototype points residing on the original feature spaceAlvarezet al.(2022); Zhanget al.(2015); Biehlet al.(2013). The study inOzcanet al.(2024)leverages this idea, and devises an iterative algorithm that finds the prototype points by solving a subproblem at each iteration. In this study, we build our algorithm based on this prototype learning strategy.

One common problem of learning methods is that they can easily overfit the training data, and they fail to generalize well to new data pointsChenet al.(2020). This issue deepens even more when the learned model is too complex. In order to prevent overfitting, a number of techniques have been developed. State-of-the-art approaches either incorporate a regularization term to the optimization problemGhaddar and Naoum-Sawaya (2018); Jiménez-Corderoet al.(2021)or apply different feature selection techniques to the data prior to optimizationLiet al.(2017). The first idea is well explored in the context of Ranking SVMs, and some effective variants such as Ranking-SVM with the-norm regularizerRakotomamonjy (2004); Joachims (2002)and Ranking-SVM with the-norm regularizerAtaman and Street (2005)are proposed. These approaches prevent coefficients from growing too much by penalizing their-norm and-norm in the objective function, respectively, and the impact of the regularization term on the objective function is controlled via a constant cost parameter,. However, as the choice of the cost parameter,, can dramatically affect the test performance of the learned models, it is required to conduct cross-validations during the experiments to select the best. Another study inMing and Ding (2019)suggests combiningand-norm in the SVM context for better regularization and classification performance. Furthermore, the study inBertsimaset al.(2020)proposes a sparse SVM classifier by partitioning classes into clusters. On the other hand, feature selection refers to identifying the most relevant features to represent the patterns in data before model learning takes place. However, since there is no general recipe to follow during this process, feature selection usually requires a solid domain knowledge. One recent study inOzcanet al.(2024)introduces the Ranking-CG Prototype algorithm that employs column generation (CG) on a variant of Ranking SVM as an internal feature selection mechanism, and shows that column generation can be an important tool to prevent overfitting. Although there are numerous studiesAtaman and Street (2005); Dedieu and Mazumder (2019); Zhang and Zhou (2013)that employ the column generation idea in SVM-based models to accelerate the optimization, the benefit of using of column generation to prevent overfitting has only recently being considered. Therefore, exploring this idea further is promising.

In this study, we propose Smooth Ranking-CG Prototype to maximize AUC. Similar to Ranking-CG Prototype, this method generates cutting-planes based on column generation to learn prototype points. However, the Ranking-CG Prototype learns a model by bounding the-norm of the weightsOzcanet al.(2024), which can result in learning sub-optimal models in some cases, thus degrading the overall performance. In our study, we propose a formulation, which does not restrict the-norm of the weights. Therefore, this is the first work where the regularization is achieved via only column generation as opposed to Ranking-CG Prototype. Moreover, in Ranking-CG Prototype algorithm, column generation iterations are ceased with the help of a convergence parameter, which affects the test performance of the model. Therefore, the value of this parameter is determined via cross-validation. On the other hand, in our formulation, the convergence parameter plays a less critical role, which helps us to remove the convergence parameter from our algorithm. Finally, by conducting experiments over 73 publicly available datasets, we show that the Smooth Ranking-CG Prototype can be a competitive approach since it outperforms all relevant competitors by providing the highest AUC in 25 datasets.

SECTION: 2Notations and Background

Given a labeled dataset), letbe the(feature, label) pair wheredenotes a-dimensional feature vector anddenotes the class label. Suppose our goal is to learn a score functionthat maximizes the AUC statistic represented as follows:

whereandrespectively contain the indexes of positive and negative instances,is the indicator function, anddenotes the cardinality of a set. Due to the counting operator involved in AUC, this problem is non-convex regardless of the form of the chosen score function. Therefore, the direct optimization of this metric can be computationally intractable for large datasets.

However, a good approximation can be obtained by replacing the indicator function with the following hinge loss function:

If the score functionis selected as an affine function, e.g.,, a convex ranking approach with the-norm regularizer on the parameter vectorcan be formulated as follows:

whereis the regularization penalty. We defineas the slack variable that measures the margin of error while comparing a positive instance with indexwith a negative instance with index. By upper bounding the term in the summation by, Problem (2) can be transformed into the Ranking SVM algorithm inJoachims (2002):

Note that Ranking SVM focuses on pairwise comparisons of the positive and negative instances and minimizes the total error in these pairwise comparisons. Therefore, it can learn classifiers that are robust to class imbalance problem. However, problems such as overfitting and the inability to model non-linear relations persist in this formulation.

To model non-linear relations, the study inOzcanet al.(2024)proposes learning a score function based on the Euclidean distance to some prototype points:

wheredenotes the feature vector of an instance,is theelement of the weight vector,is the set ofdimensional prototype points, anddenotes a single a point in, i.e.,,. Accordingly, for any given, they find the weight vectorby solving the following problem.

As the number of points in the setincreases, the learned model becomes more complex. Therefore, the learned model is likely to overfit when the setincludes many points. However, the optimization problem given in (2) is a linear programming problem, and it is possible to design a column generation strategy, a popular technique relying on dualityBertsimas and Tsitsiklis (1997), for finding a new prototype point to add to the set. Therefore, column generation can be an effective tool to control the complexity of the model.

SECTION: 3Proposed Approach

The problem given in (2) can be seen as a variant of Ranking-SVM with the-norm regularizer since constraint (5c) restricts the weights betweenand. However, bounding the weight vector into a certain region may result in sub-optimal solutions, thus poor performance in practice. Hence, it is important to design a formulation that does not include this constraint.

The Simplex algorithm, which is frequently used to solve linear programming problems, searches for the optimal solution by jumping from one vertex to an adjacent vertexBertsimas and Tsitsiklis (1997). Therefore, the optimal solution may change drastically from iteration to iteration. In our context, this is undesirable as the large changes in the weight vector may result in fluctuations in test AUC. Therefore, removing constraint (5c) requires a modification to the optimization problem solved at each iteration.

Since our goal is to prevent weights from changing dramatically from iteration to iteration, we introduce an additional term, which penalizes the change in weights via a non-negative smoothing parameter, to the objective function of the problem in (2). Suppose thatdenotes the weight vector learned before adding a new prototype point to the set, then, for a given,, and with a smoothing parameter,, we formulate the problem as follows:

By following standard reformulation procedures for linear programs, we can replace the absolute values in the objective function (6a) by an auxiliary vector, and obtain an equivalent formulation to the problem in (3), which we denote as.

:

Column generation in linear programming is built upon duality theory, and adding a new column to the primal problem results in adding a new cutting-plane to the dual problemBertsimas and Tsitsiklis (1997). Hence, we can design a mechanism to find new prototype point to add to the setby solving a sub-problem that is formulated based on the information provided by dual variables. Suppose that the problem in (3) is formulated for a given,, and, then the corresponding dual problem is as follows:

whereforare the dual variables associated with thesoft-marginconstraints given in (7b), and,are the dual variables associated with the constraints given in (7c) and (7d), respectively. According to the duality theory, it is enough to optimize the primal problem to retrieve the optimal dual solution or vice versa. Moreover, adding a new prototype point to the primal problem results in generating the constraint in (8c). Therefore, given the optimal solution of Problem (3), denoted by, if there exists asuch that

then addingto the setcan provide an improvement to the model as it harms the dual feasibility. In order to find new prototype point,, we solve the following non-linear sub-problem by using a gradient based optimization approach:

Then, the pseudo-code of Smooth Ranking-CG Prototype is provided in Algorithm1.

Input:Smoothing parameter (), Set of all points ()Output:Set of prototype points (), Optimal weights ()

The iterations in Algorithm1are initiated by adding a randomly selected pointfrom the feature space to the set. After solving the problemand obtainingand, we solve the non-linear sub-problem (10) to find the prototype point. Finally, iterations continue until the algorithm converges.

Unfortunately, (10) is a complex function that is neither convex nor concave. Hence, it might have many local optima and the gradient based algorithms may get stuck at one of these points. Techniques such as using adaptive learning rates and introducing a momentum term are proved to be helpful to avoid local optimums during the global optimum search in practice, but it is not possible to guarantee that the algorithm reaches the global optimum. However, by using a proper initialization strategy, the gradient descent based algorithm may converge to better local optima, and we can utilize available training data to design this initialization strategy. Suppose we have the dual solution,, then the optimization of the problem in (10) can be initiated starting from:

Notice that the problem in (11) is easy to solve since we have finitely many training samples. After finding, the optimization of (10) continues for a specific number of iterations unless convergence is observed. The details of the gradient based algorithm we employ to solve (10) are provided in Section4.

SECTION: 4Experiments

We conduct different set of experiments to show the benefits of the proposed approach. Section4.1provides the experimental setup; Section4.2demonstrates that the smooth Ranking-CG Prototype can be helpful to prevent the fluctuations in test AUC at consecutive iterations, which Ranking-CG Prototype may exhibit if the weight vector is unbounded; and Section4.3shows the classification performance of the proposed approach with respect to different benchmarks.

SECTION: 4.1Experimental Setup

The model is implemented in the Python programming language, and the code is publicly available111https://github.com/erhancanozcan/SmoothRankingSVM.. While Gurobi 9.5.1 is utilized to solve linear programs, the Adam optimizer in TensorFlow 2.10.0 is utilized to solve non-linear sub-problem. We test the performance of the proposed method on binary classification problems that are publicly available on the UCIDua and Graff (2017)and KEELAlcalá-Fdezet al.(2011)repositories. The chosen datasets vary in terms of the number of features, the number of instances, and the severity of class imbalance, and the dataset characteristics are summarized in Appendix A.

The model proposed in this study is relevant to Ranking SVM-based models, thus we consider three Ranking SVM variants as benchmark algorithms. We refer to Ranking SVM with,andregularizers as Ranking SVM,Ranking, andRanking, respectively. While we utilize the Ranking SVM implementation in the dlib packageKing (2009), we implement other versions of the Ranking SVM, and solve them via Gurobi. To guarantee fair comparisons among models, all the aforementioned methods are trained on the pairwise dissimilarity matrix obtained based on Euclidean distances instead of the raw features. Therefore, the upper limit for the number of features that any method can consider is equal to the number of points in the training data, except for the prototype learning approaches. Lastly, we include the Ranking-CG Prototype method that achieves regularization internally via column generation as described inOzcanet al.(2024)among the benchmarks we consider.

The performance of the benchmark methods heavily depends on the value of its parameter, which controls the model complexity, and the best value of this parameter is chosen via 5-fold cross-validation for each dataset during the experiments. To ensure a fair comparison, the number of options considered for the complexity parameter is equal across different approaches, and the details of those parameters are available in Appendix B.

On the other hand, the Smooth Ranking-CG Prototype has a smoothing parameter,, which penalizes the changes in weights at each iteration. This parameter is selected based on a 5-fold cross-validation from thirteen values in the interval of(see Appendix B for the details). Additionally, during the optimization of the sub-problem in Equation (10), we employ the Adam optimizer with its default parameters, and the optimization of the sub-problem continues as long as the change in objective ratio is greater than. If this convergence criteria is not met initerations, the optimization of the sub-problem is interrupted, and the algorithm continues the iterations by adding the best prototype point to the set.

SECTION: 4.2Stabilizing the Progress of Test AUC

As mentioned earlier, the Ranking-CG Prototype learns a model by solving the problem given in (2), thus the model weights are restricted betweenand. Due to this bound, the learned model can be sub-optimal in some cases, thus showing poor performance. However, removing this bound can be devastating for the test performance of the model as it will make the model less predictable. The smooth Ranking-CG Prototype addresses this issue by penalizing the change of the weight vector instead of bounding thenorm of the weight vector as opposed to the Ranking-CG Prototype.

In order to investigate the case described above, we implement a version of Ranking-CG Prototype by removing the bound on the weight vector, which we refer as Unbounded Ranking-CG Prototype; and we monitor the change in the test AUC values after adding a new point to the setat each iteration for different approaches on a simple XOR problem with some noise. Figure1shows the scatter plot of the experimental XOR problem we consider, and compares the test AUC performance of our algorithm with the Ranking-CG Prototype and the Unbounded Ranking-CG Prototype as a function of the number of iterations.

According to Figure0(b), the Unbounded Ranking-CG Prototype can be still vulnerable to overfitting unless the weight vector is bounded. Moreover, due to the fluctuations observed in the Ranking-CG Prototype and its unbounded version, the convergence parameter of these algorithms must to be tuned via cross-validation for a reliable performance. However, the performance of the Smooth Ranking-CG Prototype is more robust across the iterations since it allows the learned weights to change if and only if the improvement in the objective is large enough. Therefore, the convergence parameter in our approach is not as important as it is in the Ranking-CG, which allows us to remove this parameter from our algorithm.

SECTION: 4.3Classification Performance

We compare the Smooth Ranking-CG Prototype with the aforementioned relevant competitors by conducting experiments over 73 datasets, and the detailed results are available in Appendix C. According to these results, Table1shows the number of datasets in which each approach provides the highest test AUC among all competitors (the first row), and the averageof features with coefficients’ magnitude larger than.

The Smooth Ranking-CG Prototype yields the highest test AUC in 25 out of 73 datasets, which makes it as competitive as its relevant counterparts. Furthermore, as shown in Table1, it utilizes significantly fewer features, thus it can be a useful approach when it is required to learn simpler models. Finally, to indicate the benefit of the proposed approach, we evaluate the performance of the proposed algorithm only in 47 datasets where the Ranking-CG Prototype cannot provide the highest test AUC, and the performance of algorithms over this subset of datasets is provided in Table2.

SECTION: 5Conclusion

This paper proposes a classifier maximizing AUC during training by incorporating a column generation-based prototype learning strategy via cutting-planes. Although a similar approach, Ranking-CG Prototype, has been proposed recently, we explain the problems that may arise in that algorithm, and attempt to handle those by modifying the optimization problem. The modification we propose allows us to remove the convergence parameter controlling the column generation iterations. Based on the extensive experiments conducted on 73 datasets, the Smooth Ranking-CG Prototype yields the highest test AUC in 25 datasets among its relevant competitors by learning simpler models.

We believe that the proposed method has two interesting aspects that can be explored further as future work. First, similar to Ranking-CG Prototype, the first prototype point is added to the setrandomly to initiate the Smooth Ranking-CG Prototype, which may potentially have an impact on the performance of the algorithm. Second, the smoothing parameter,, remains constant during the training of Smooth Ranking-CG Prototype. However, the role of this parameter is similar to learning rate in gradient-based algorithms, and setting the learning rate in an adaptive way is an important concept to improve the gradient-based algorithms. Inspired by this, one potential way to improve the performance of the Smooth Ranking-CG Prototype might be to design an algorithm adjusting the value of the smoothing parameter in an adaptive way at each iteration.

SECTION: Acknowledgments

Research was partially supported by the NSF under grants CCF-2200052, DMS-1664644, and IIS-1914792, by the DOE under grants DE-EE0009696 and DE-AR-0001282, and by the ONR under grant N00014-19-1-2571.

SECTION: References

SECTION: Appendix A: Dataset Characteristics

SECTION: Appendix B: The Details of the Tuned Parameters

SECTION: Appendix C: Detailed Results