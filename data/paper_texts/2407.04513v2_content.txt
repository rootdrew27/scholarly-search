SECTION: LayerShuffle: Enhancing Robustness in Vision Transformers by Randomizing LayerExecution Order

Due to their architecture and how they are trained, artificial neural networks are typically not robust toward pruning or shuffling layers at test time. However, such properties would be desirable for different applications, such as distributed neural network architectures where the order of execution cannot be guaranteed or parts of the network can fail during inference. In this work, we address these issues through a number of training approaches for vision transformers whose most important component is randomizing the execution order of attention modules at training time. With our proposed approaches, vision transformers are capable to adapt to arbitrary layer execution orders at test time assuming one tolerates a reduction (about 20%) in accuracy at the same model size. We analyse the feature representations of our trained models as well as how each layer contributes to the models prediction based on its position during inference. Our analysis shows that layers learn to contribute differently based on their position in the network.
Finally, we layer-prune our models at test time and find that their performance declines gracefully. Code available athttps://github.com/matfrei/layershuffle.

SECTION: 1Introduction

While demonstrating impressive performance in many domains(Krizhevsky et al.,2012; Vaswani et al.,2017; Radford et al.,2021; Rombach et al.,2022), deep learning systems demand both extensive computational resources and tight integration of their parts. For applications at scale, they therefore increasingly require the construction of large data centers with thousands of dedicated hardware accelerators. A paradigm shift from central to decentral model inference, where loosely coupled neural networks are distributed over a number of edge devices that share the computational load of the model(Gacoin et al.,2019)therefore seems ultimately desirable.
Unfortunately, current deep learning models lack the robustness necessary for such a paradigm shift.

In general, artificial neural networks (ANNs) are not robust toward pruning or replacing network layers during deployment.
Similarly, changing the order of execution in-between layers without further training usually results in catastrophic losses in accuracy. Nevertheless, these properties would be desirable e.g. in distributed setups as described above, where a model is executed on a number of shared nodes in a network.
This way, overloaded or malfunctioning nodes could simply be skipped in favor of other available nodes.

Augmenting models with these properties has historically been challenging. Due to the structure of the most common types of ANNs and how they are trained through backpropagation(Linnainmaa,1970; Werbos,1982; Rumelhart et al.,1986), each neuron can only function by adapting to both its connected input and output neurons as well as the overall desired output of the network at training time.
Furthermore, the hierarchical organization of explanatory factors is usually considered a necessary prior in deep learning, i.e. one assumes that subsequent layers extract increasingly high-level features(Bengio et al.,2013). Therefore, switching the execution orders of layers implies that layers would need to adapt and extract either low-level or high-level features depending on their position in the network. Unfortunately, network layers adapting in such a way to a changed order of execution appears to be infeasible for most known network architectures. The above prior is therefore violated and the overall performance of the network suffers beyond the point where the network successfully executes the task it has been trained for.

The more recently discovered transformer architecture(Vaswani et al.,2017)has been shown to be more flexible. Transformers, when trained accordingly, can be layer-pruned at test-time(Fan et al.,2019), and recent work merges similar transformer-based language models(Akiba et al.,2024), all with only moderate reduction or even an improvement in performance. We hypothesize that the reason for the high adaptability of transformers can be found in self-attention modules being able to adapt their output based on the received input. Thus it should be possible to train a transformer network to not only adapt to the variation of its input features based on the overall network input but also the variations caused by receiving input from different layers during test time.

We propose and evaluate three training approaches for vision transformers to address the robustness issues laid out above. The most important component common to all approaches is randomizing the execution order of the vision transformer’s stacked self-attention-and-feed-forward modules at training time (Figure1(a)). More precisely, the main contributions in this paper are:

With LayerShuffle, the layers of a vision transformer(Dosovitskiy et al.,2020)are capable of adapting to an arbitrary execution orderat test time, assuming one tolerates a moderate reduction in performance. Providing each layer additionally with its current position in the network improves performance only slightly compared to a model without it, suggesting that each attention layer is already capable of determining its role based on the incoming data alone.

An analysis reveals that layers of models trained with LayerShuffle adjust their output depending on which position they hold in the network.

Trained models can be layer-pruned at test time similar to the models trained with the techniques proposed inFan et al. (2019), where their performance declines gracefully, i.e. models with reduced amounts of layers still remain functional.

SECTION: 2Related work

Zhu et al. (2020)find that for particular subsets of inputs, transformers perform better when changing the execution order of layers to an input-specific sequence. They optimize the execution order per sample in order to maximize the performance of the model for natural language processing tasks.

While the goal in their work is to find a layer sequence of a pre-trained model that is optimal for a given input, our approach aims to make the model robust to any sequence of execution, where layers might even be missing.

In parallel to our work on vision transformers, two groups have conducted similar experiments with the aim to understand how language models (LLMs) process data.Lad et al. (2024)found that LLMs are very robust to changing the positions of adjacent layers or ablating single layers from the model.Sun et al. (2024)perform similar experiments, and find that transformers improve iteratively upon their predictive output by subsequently refining the internal representation of the presented input. The main difference to our work is that the authors of these works do not perform any refinement on the models and switch and ablate layers locally with the aim of better understanding the inner workings of LLMs. Here we focus on methods and training approaches to increase this innate robustness of the transformer architecture to a point where models at test time function regardless of their layer execution order, and respond gracefully to the ablation of several layers in any position of the network.

Another related work is LayerDrop(Fan et al.,2019), where the authors focus on robust scalability for models on edge devices. They propose dropping whole transformer layers during training and show that this training approach allows models to still deliver acceptable (if somewhat reduced) performance upon pruning layers at test time (e.g. for balancing computational load). The main difference to our approach is that we randomly change the execution order during training, and, contrary to LayerDrop, do not remove any layers. Also, LayerDrop focuses on entirely on load balancing in compute-limited production systems while our main focus is on arbitrary execution order and the possibility to replace defective nodes by others on top of these issues in case of overloaded or malfunctioning nodes in distributed systems.

Work on introducing permutation invariance into neural networks has been conducted byLee et al. (2019),Tang & Ha (2021)as well asPedersen & Risi (2022). The corresponding former two approaches exploit the permutation equivariance of attention, i.e. the fact that the order in which a sequence of vectors gets presented to the attention module does not change its result, but merely shuffles the sequence of output vectors. This equivariance is achieved by using a fixed-seed query vector in order to obtain an permutation invariant latent code. This latent code stays the same no matter in which order input tokens/patches are presented to the module. The main contrast to our work here is that we exploit permutation invariance in the order of layer executions rather than the order of tokens and patch embeddings and can therefore not make use of permutation equivariance of the attention operation, as it does not apply to switching inputs and outputs.

Finally, the work ofGacoin et al. (2019), not unlike our own, is motivated by the observation that a paradigm of distributed model inference over a number of loosely coupled compute nodes, edge devices or swarm agents promises a positive impact on the ecological and economical footprint of deep learning solutions. The authors propose a graph-theory-based framework to optimize the distribution of model parts to individual devices and optimize the overall energy consumption of the network. While our work sets out from the same motivation, it complements the approach ofGacoin et al. (2019)as the the authors do not address robustness to adverse conditions in such distributed setups while it is the entire focus of this paper. The exact distribution of our models on the other hand, is beyond the scope of our work but combining our models with the approaches in(Gacoin et al.,2019)seems a promising direction of future research.

SECTION: 3Methods

We investigate three approaches for arbitrary layer execution order in vision transformers(ViT; Dosovitskiy et al.,2020): First, we simply permute the order of layers randomly during training, such that every training batch is presented to the network’s layers in a different random order (Section3.1). Second, while randomly permuting the layer order as in the previous approach, we use an layer-depth encoding inspired by learned word embedding approaches (Section3.2) to test if this additional information would further improve performance. Third, while randomly permuting layer order as in the previous approaches, we try to predict from the output of every layer at which position the layer is currently located in the network using a small layer position prediction network for every layer (Section3.3). A detailed overview on ViTs can be found in the appendix.

SECTION: 3.1Randomly permuting layer order during forward pass

During each forward pass, i.e. for each batch presented to the ViT, we randomly permute the execution order of layers during training. The intention here is to teach the layers to not only extract meaningful intermediate representations when receiving input from a particular layer, but to be able to process and encode information from and for all possible layers in the network. In terms of training, exchanging the order of layers does not require any changes in the basic error backpropagation algorithm. For the forward path, the order how weight matrices are multiplied and activation and attention functions applied changes for every batch and forward pass. This needs to be accounted in the backward pass by propagating the gradients in the precise reverse order that has been set in the forward pass, i.e.  multiplying the computed per-layer gradient matrices in the correct order. As we use Pytorch(Paszke et al.,2019)in all our experiments, this aspect is taken care of the framework’s autogradient feature. We refer to this model asLayerShuffle. To further illustrate the approach, a pseudo-code listing is given in Algorithm1.

SECTION: 3.2Layer position encoding

In the second approach,LayerShuffle-position, we provide each layer with its current position in the network. Through this variation we aim to test if each layer can already adapt sufficiently by itself to information coming from different layers during test time or if giving it the current position can help further. In more detail, jointly with permuting the layer execution order, each layer learns a vector embeddingfor each possible index positionof the layer during training, where L is the number of layers andis our chosen embedding dimension. The layer’s current indexin the network is presented together with the input to the layer(Figure2). The layer fetches the embedding vectorassociated with the passed indexand concatenates it to the input vector:N is the number of patches extracted form the input image, the functionsconcatandrepeatrespectively concatenate and repeat tensors along their last (most varying) dimension.
A projection network, which consists of a LayerNorm (LN)(Ba et al.,2016)module, a single linear layer, a GELU(Hendrycks & Gimpel,2016)activation function as well as a Dropout(Srivastava et al.,2014)module, is then used to combine input and embedding and reduce it again to the used latent dimensionof the transformer. To ensure gradient flow during training, a residual connection is added as well:

The resulting outputis passed on to a regular multi-head-attention-and-feed-forward structure as described in Equations1and2.

SECTION: 3.3Predicting current layer position

To determine if the incoming information to each attention layer is indeed sufficient for it to figure out its role, we specifically test for this ability with theLayerShuffle-predictvariant. We equip each layer of the network with a simple position prediction module that takes the current layer output as an input and seeks to predict the current position of the layer in the network (Figure3). The module consists of a single linear layerreceiving layer-normalized (LN)input..

Each of these layer order prediction modules optimizes a cross-entropy loss where then the overall network optimizes the lossHere,is the regular cross-entropy loss of the output layer, andis the layer position prediction loss of layer i, which is also a cross-entropy loss:

whereis the number of layers in the network,is the-dimensional output of the position prediction network of layer, anddenotes the-th dimension of the vector.is the output logit denoting the network’s predicted confidence that the layer currently is deployed at its actual position with index.

SECTION: 4Experiments

We conduct our experiments on the ILSVRC2012 dataset(Russakovsky et al.,2015), more commonly termed ImageNet2012, as well as the CIFAR-100 datasetKrizhevsky et al.. We use the originalViT-B/16(Dosovitskiy et al.,2020)vision transformer, as well theDeiT-Bdistilled data-efficient image transformerTouvron et al. (2021). Pre-trained weights for ImageNet2012 are publicly available for both models(Dosovitskiy et al.,2020; Wu et al.,2020; Touvron et al.,2021).

TheViT-B/16has been pre-trained on ImageNet21k(Deng et al.,2009; Ridnik et al.,2021)at an 224224 input image resolution and refined on ImageNet2012 at the same resolution.DeiT-Bhas the same architecture asViT-B/16, but uses an additional destillation token during training, which is used to distill the inductive bias of a large convolutional network into the transformer in order to require less training data. It is pre-trained exclusively on ImageNet2012.

Both models are again refined on both ImageNet2012 and CIFAR-100 at the same resolution, but using the training processes as described in Section3. That is, layer execution order is randomly permuted while refining the model. To establish a baseline, on ImageNet2012, we refine the original models for one more epoch on without changing the layer order. Any longer training was found unlikely to bring additional improvement in preliminary experiments since our networks are already pretrained on ImageNet. For CIFAR-100, we refine our baseline for 20 epochs. For each approach, including the baselines, we trainnetworks and compare their average validation accuracy.

All models are refined using Adam(Kingma & Ba,2014)(,,), where an initial learning rate ofwas empirically found to work best. In terms of batch size, we evaluate training batch sizes of 640 images, which is the maximum multiple of 8 that can fit in the video memory of our used GPU, as well as 128 images for models that benefit from a smaller batch size. Even smaller batch sizes do not yield any improvement in performance for our models. Inspecting training curves shows that for ImageNet2012 the performance of models plateaus at 20 epochs the latest, which is therefore set as the maximum number of training epochs. For CIFAR-100 we use 100 epochs since the models were pretrained on a different dataset, i.e. ImageNet. We use a form of early stopping by evaluating the model achieving the lowest crossentropy loss on the validation set after the maximum amount of training epochs. All models have been trained on a single NVIDIA H100 Tensor Core GPU with 80GB of memory. Training a single model on ImageNet2012 for 20 epochs takes about 7 hours whereas CIFAR-100 training times are significantly shorter due to the smaller train set.

SECTION: 4.1Sequential vs. arbitrary execution order

The average accuracy for all approach
on all vision transformer architectures and datasets is shown in Table1.
We make the following observations across all models and datasets:

On both the CIFAR-100 and the ImageNet2012 dataset, our baselines refined from pre-trainedViT-B/16andDeiT-Bmodels perform very much as expected. For a classic sequential execution order of the model layers, on ImageNet2012 the trained models achieve an average validation accuracy very close to the performance of the respective original pretrained models(Dosovitskiy et al.,2020; Touvron et al.,2021). Our refined baselineViT-B/16obtains an average accuracy ofwith a standard deviation of. TheDeiT-Bmodel attains a slightly lower accuracy ofwith a standard deviation of. Baseline results CIFAR-100 look similar withViT-B/16andDeiT-Bachieving(standard deviation:) and(standard deviation:) respectively.
Not surprisingly, for an arbitrary layer execution order, the average model accuracy declines catastrophically to belowfor all trained models on both datasets. Our original assertion that in general, ANNs are not robust to changing the execution order of their layers, is in line with these results.

Our LayerShuffle approaches show lower performance than the baselines when executing layers in their original order. On ImageNet2012, our trainedViT-B/16models obtain average accuracies of,, andrespectively for ourLayerShuffle,LayerShuffle-positionandLayerShuffle-predictapproaches. Average accuracies forDeiT-Bmodels are in a similar range with,, andfor our respectiveLayerShuffle,LayerShuffle-positionandLayerShuffle-predictapproaches. On CIFAR-100 on the other hand the gap between baseline and LayerShuffle models is somewhat larger for the trainedViT-B/16models. These models merely achieve,andwith slightly higher standard deviations forLayerShuffle,LayerShuffle-positionandLayerShuffle-predictapproaches. ForDeiT-Bmodels on the other hand, these approaches perform similarly well to the models trained on ImageNet with scores of,andfor the above mentioned techniques. A possible explanation for these discrepancies could be found inViT-B/16models requiring more and more diverse training data compared toDeiT-Bmodels, which has been pre-trained with the aim to reduce the amount of required training data.

Despite being outperformed by the baseline in a sequential execution order setting, all models improve dramatically over their corresponding baseline models in an arbitrary execution order setting. Taking a closer look at LayerShuffle model performance in that setting, we find that the simplest approach performs very well across both architectures and datasets. For both ImageNet2012 as well as CIFAR-100 validation setsDeiT-Btrained on LayerShuffle yields the best performance with average accuracies ofandrespectively, narrowly outperformingLayerShuffle-position, which receives information about the layer position.LayerShuffle-positionachieves scores ofandon these datasets. ForViT-B/16models on the other hand,LayerShuffle-positionoutperformsLayerShuffle. The former achieves scores ofandof ImageNet2012 and CIFAR-100, with the latter performing only slightly worse withand. The most likely explanation for models of theViT-B/16architecture achieving significantly lower accuracies on CIFAR-100 thanDeiT-Bmodels can again be found in the former requiring less training data than the latter.

We find that the position prediction approach,LayerShuffle-predictis outperformed by both our remaining approaches on all datasets and architectures. On ImageNet2012, refinedViT-B/16models achieve average accuracies ofwhereasDeiT-Bmodels attain. On CIFAR-100 the former score, the latter. A possible explanation might be that due to optimization of multiple objectives (fitting both the output labels as well as predicting the current position of the layer) this approach requires more careful hyperparameter tuning.

A further interesting observation is to be made when comparing the performance for sequential and arbitrary execution order for each approach respectively. For all approaches, using the original layer order for sequential execution still performs better than an arbitrary order. This is most likely a consequence of fine-tuning from a sequentially trained model.

For the layer position prediction approach, we measure the average accuracy of layer position predictions over all five trainedLayerShuffle-predictmodels, and find that the layer position is predicted correctly inof all cases. These results demonstrate that each layer has enough information coming from its inputs alone to predict where it is in the network, providing the basis to adapt to its current position. We investigate this further when analyzing intermediate network representations in Section4.3.
In conclusion, refining a pre-trained model while randomly permuting the execution order of the network layers can make a model more robust towards such arbitrary execution orders at test time. On the other hand, Dropout and LayerNorm by themselves do not have the same effect and fail to produce networks robust against layer shuffling.

SECTION: 4.2Removing layers during test time

To determine how neural networks trained with LayerShuffle would perform when several devices in a (distributed) model become unavailable, we further investigate the effect of pruning an increasing amount of layers during test time. We evaluate its average validation accuracy over 5 models when only using 3, 6, or 9 layers. In addition, we refine the originalViT-B/16transformer using LayerDrop(Fan et al.,2019)with a drop probability of 0.2 (as recommended by the authors) and compare it as a baseline to our approach under identical conditions. Note that whenever we evaluate the accuracy of our proposed approaches as well as the baseline, we do so two times: Once, for the original ”sequential” layer order as originally intended and trained for theViT-B/16transformer, and once with arbitrary layer execution order where we change the order randomly for every forward path.

For sequential execution (Figures1(b)), LayerDrop with a drop rate of 0.2 behaves similarly to LayerShuffle, with the exception that our approach performs better for a small number (3) of layers with an average accuracy of approximatelyvs. close tofor LayerDrop.
While for 6 layers, both approaches are roughly on par, for 9 layers LayerShuffle is slightly outperformed by LayerDrop as both approaches show an average accuracy in therange. At the full amount of 12 layers, this gap in average accuracy stays roughly the same as the LayerDrop-refined model closes in on the full accuracy of the original model, while our LayerShuffle approach achieves slightly lower accuracies (see also Table1). For comparison, we also visualize models where we refined a reduced number of 3, 6, and 9 layers: while delivering similar performance as LayerDrop for 9 and 12 layers, these models perform significantly better than the previously discussed approaches at lower numbers, i.e. 3 and 6 layers. They do however, bear the drawback that for each specific amount of layers a new model must be refined from the original model, whereas for both LayerDrop and our LayerShuffle approach, only a single full-size model needs to be refined and the number of layers can be configured at will at test time.

For arbitrary execution (Figure1(c)), LayerShuffle is the only approach that succeeds, with the average accuracy improving as the number of layers is increased. LayerDrop does not perform well regardless of the number of layers in the model.

A noteworthy detail is the comparable high average accuracy of the fully retrained baseline with 3 layers. Given the low performance of the refined models with 6 and 9 layers, as well as that there are only 6 possible permutations for 3 layers, the most likely explanation is that one of the 5 random permutations evaluated for the model was the original layer execution order the model has been trained for, i.e.  [1,2,3] therefore skewing the achieved accuracy in this case. In conclusion, we find that our proposed approach has similar test-time scaling capabilities as LayerDrop, while still ensuring robustness towards arbitrary layer execution orders.

SECTION: 4.3Analysis of intermediate network representations

To gain a deeper insight into how information is encoded in the models, we conduct two experiments. First, we compute Uniform Manifold Approximation and Projection (UMAP)(McInnes et al.,2018)embeddings of the entire output of a particular attention module (i.e. combined self-attention and feed-forward layers), where we color-code all output vectors based on the position the module held in the network when producing this output. In more detail, we concatenate all patch tokens of a single image together with the class token as a single vector, and use this representation as a single state vector in our compression. To extract a sufficient number of these state vectors, we present 1,000 randomly sampled images from the ImageNet2012 validation set to a LayerShuffle-trained model. While we use an evaluation batch size of 1 image and record all outputs of a single, previously selected layer, we randomly permute the execution order of layers such that the selected layer changes position in the network during every forward path. After the layer’s output vectors for all 1,000 images have been recorded, a UMAP reduction of the output space to 2D is performed.

Second, in order to investigate how much each layer contributes to the final classifier output when deployed in different positions within the model, we compute the L2-Norm of the class-token of each layer output. We correct for the contribution of previous layers by subtracting the class token of the previous layer before computing the token’s norm. That way, we consider solely the additive contribution of the layer to the class prediction of the model. We collect these token norms for all network layers as we shuffle their position while presenting 1000 randomly sampled input images in an identical manner as in the previous experiment.

Finally, to establish a baseline, we extract both representations for the originalViT-B/16weightsDosovitskiy et al. (2020)as well.
Figure4(b)shows the obtained visualizations for the originalViT-B/16model acting as a baseline as well as our model refined withLayerShuffle.
In more detail, Figures4(a)and4(b)show the UMAP embeddings of a single layer’s output for both the baseline and our model. The current position of the layer in the network when producing a given output is color-coded from dark (position close to the input) to light (position close to the output). Note that this information about the layer position has not been presented to the UMAP algorithm. Apart from rough trends, no clear ordering of the space is visible for the baseline (Figure4(a)). For LayerShuffle, while there is no sharp separation between outputs generated at different positions in the network, the layer clearly adapts to its current position and extracts different features for different positions in the network (Figure4(b)).

A further interesting observation is the very distinct collection of points for layer positions close to the input, which are detached from the remaining manifold of points. This results suggests that extracting low-level features, requires special treatment.

Figures4(c)and4(d)show the distributions on the normalized L2-norm of additive contributions to class tokens for different layer positions in the transformer for both the baseline and our model. Each x-axis in the plot corresponds to a single layer of the network, where position of the layer in the network is color-coded again from close to the input (dark) to close to the output (light). x-axes are also ordered corresponding to the layer’s original position in the pre-trained model, where the order of layers is top to bottom.
We can see that for the baseline model norms are basically spread out over the whole range. This implies that layers in the baseline model overall contribute evenly to the predictive output of the model, regardless of their current position in the network.

On the other hand, the ridge plot gathered from layer outputs of the model refined with our method paints a different picture. The norm of attention modules output and therefore it’s contribution to the model’s prediction varies based on the distance to it’s original position in the networks. Modules which were originally closer to the input (x-axes on top of the plot) often show larger contributions to the predictive output of the model when on positioned closer to the input and vice versa. This indicates that our refinement of the model conditions its layers to contribute to the overall predictive output if the received input lies within the layers learned distributions of inputs (i.e. the layer is close at a position assigned to it in the original pre-trained network), and withhold or reduce their output otherwise. This is also in line with recent work conducted in parallelSun et al. (2024), which frames transformer layers as incrementally refining a rough sketch of the model’s output, an iterative process which is enabled by the transformer’s extensive utilization of skip-connections.

In conclusion, our analysis indicates that refining networks withLayerShufflemakes vision transformers robust to arbitrary execution orders as it trains the layers to solely add to the models contribution if the layer input is in-distribution and reduce their output otherwise, in which case the model’s skip-connection forwards the out-of-distribution output to the subsequent layer.

SECTION: 5Discussion and Future Work

This paper presented a new approach called LayerShuffle, which enabled vision transformers to be robust to arbitrary order execution as well as pruning at test time.
For sequential execution, LayerShuffle performs on average
only slightly worse than the LayerDrop approach but is the only method that works when the layer execution is arbitrary.

Our analysis confirmed that layers of models trained with LayerShuffle adjust their output depending on which position they hold in the network. Furthermore, our results indictate that refining networks withLayerShuffletrains the layers to only contribute to the model’s class prediction if the layer input is in-distribution and reduce their output otherwise, in which case the layer’s skip-connection forwards the barely modified out-of-distribution embedding to the subsequent layer.

In the future, these properties could makeLayerShuffle-trained models ideal candidates to be distributed over a number of very loosely coupled compute nodes to share the computational load of model inference. Given the enormous engineering, financial and logistical effort as well as the environmental impact(Strubell et al.,2020)of building and maintaining datacenters for state-of-the-art deep learning approaches on the one hand, as well as the large amount of available, but scattered compute through existing smartphones, laptop computers, smart appliances and other edge devices on the other hand, approaches that allow distributed neural networks to perform inference could be of great impact. We therefore consider the deployment and orchestration of our trained models onto an actual set of edge devices and the practical implementation of the inference process on a network of such devices, likely by combining our approach with previously proposed frameworks to address this issue(Gacoin et al.,2019), a very promising direction of future research.

SECTION: Acknowledgements

We would like to thank Prof. Christian Igel for his insightful feedback on the manuscript. Furthermore we thank the members of the REAL and Creative AI lab for fruitful discussions. This work was supported by a research grant (40575) from VILLUM FONDEN.

SECTION: References

SECTION: Vision Transformers

Dosovitskiy et al. (2020)have successfully adapted the transformer architecture to computer vision by introducing a preprocessing step that converts images to suitable sequences.
They do so by splitting an imageinto a sequence of N flattened patches, and then pass each patch through a linear embedding layer,andare here the height, width and number of channels of the image respectively andis the patch size.is the internal latent dimension of the transformer which remains constant throughout the network and can be set as a hyperparameter.

After converting the image into a sequence that can be processed by a transformer encoder, inspired by BERT(Devlin et al.,2018), the authors prepend aclasstoken toin which the class information of the input image can be aggregated by the transformer. To encode position information into the embedding, a positional embedding tensoris added. Both theclasstoken as well as the positional embeddings are learnable embeddings, which are trained jointly with the rest of the network. The resulting input sequence presented to the transformer network can be expressed as

This sequence is presented to a standard transformer architecture of stacked attention modules. Each attention module consists of a multi-head self-attention (MSA) layer and a feedforward layer or multilayer perceptron (MLP) layer. MSA layers utilize self-attention (SA)(Vaswani et al.,2017), a powerful concept that allows transformers to relate and combine its feature embeddings with each other. Self-attention extracts features from the input sequence, which in turn preforms a transformation of the input vector sequence.

Specifically, self-attention extracts query, key and value sequences,andfrom the input sequence using a linear projection:Theandsequences are then used to compute a Softmax-normalized transformation matrixindicating how to incorporate information of the whole sequence (i.e. in our case all image patches) for every single vector of the sequence:Scaling the dot-product product byhere ensures a balanced distribution of the Softmax output. After obtaining, the output of SA is computed as

A multi-head self-attention (MSA) layer(Vaswani et al.,2017)performs several attention operations in parallel, concatenates the result and projects it back to the internally used latent dimension of the transformer:

In an attention module the multi-head self-attention layer is followed by a multi-layer-perceptron (MLP) layer transforming the recently combined embeddings to extract new feature representations. Before presentingto each layer in the module, the embeddings are normalized using LayerNorm(Ba et al.,2016). To ensure consistent gradient flow during training, residual connections(He et al.,2016)are behind both the MSA and the MLP layers(Wang et al.,2019). Furthermore, as a regularization measure, Dropout(Srivastava et al.,2014)is applied after every MSA and MLP layer.
In summary, given the sequencefrom a previous attention module as input, we first compute the intermediate representation

which is the presented to the MLP layer to compute the final output of the module

Finally, after N attention modules, the first vector of the sequence (corresponding to theclass-token in the preprocessed input) is handed to a linear layerto predict the final class of the image:denotes the number of classes.