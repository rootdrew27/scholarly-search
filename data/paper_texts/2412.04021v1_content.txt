SECTION: Restoring Missing Modes of 21cm Intensity Mapping with Deep Learning: Impact on BAO Reconstruction

In 21cm intensity mapping of the large-scale structure (LSS), regions in Fourier space could be compromised by foreground contamination. In interferometric observations, this contamination, known as the foreground wedge, is exacerbated by the chromatic response of antennas, leading to substantial data loss. Meanwhile, the baryonic acoustic oscillation (BAO) reconstruction, which operates in configuration space to "linearize" the BAO signature, offers improved constraints on the sound horizon scale. However, missing modes within these contaminated regions can negatively impact the BAO reconstruction algorithm.
To address this challenge, we employ the deep learning model U-Net to recover the lost modes before applying the BAO reconstruction algorithm. Despite hardware limitations, such as GPU memory, our results demonstrate that the AI-restored 21cm temperature map achieves a high correlation with the original signal, with a correlation ratio of approximatelyat. Furthermore, subsequent BAO reconstruction indicates that the AI restoration has minimal impact on the performance of the ‘linearized’ BAO signal, proving the effectiveness of the machine learning approach to mitigate the impact of foreground contamination. Interestingly, we demonstrate that the AI model trained on coarser fields can be effectively applied to finer fields, achieving even higher correlation. This success is likely attributable to the scale-invariance properties of non-linear mode coupling in large-scale structure and the hierarchical structure of the U-Net architecture.

SECTION: 1Introduction

In the standard cosmological model, the large-scale structure (LSS) emerges from tiny quantum fluctuations during inflation. These primordial perturbations are amplified by non-linear structure formation processes, giving rise to the cosmic web observed today. Studying such structures provides critical insights into the Universe’s composition and evolutionary history[1,2,3,4,5,6,7]. In addition to traditional galaxy surveys, 21cm intensity mapping (21IM) has recently emerged as a promising method for mapping LSS, by detecting the total 21cm emission of neutral hydrogen (HI) across various redshifts.[8,9,10,11,12,13,14,15,16]. After the Epoch of Reionization (EoR), neutral hydrogen predominantly resides within galaxies, making 21IM an effective alternative to galaxy surveys. Unlike galaxy redshift surveys, which resolve individual galaxies, intensity mapping captures integrated light, thus offering a more efficient way to map the LSS over extremely large volumes[17,18,19,20,21,22].

However, extracting the cosmic signal presents significant challenges due to interference from various sources, including foreground contamination from Galactic and extragalactic sources[23,24,25], radio frequency interference (RFI)[26]and instrumental systematics[12,27]etc. Among these, foregrounds are particularly problematic, as they are several orders of magnitude stronger than the expected cosmic signal[14,23], significantly hindering progress in measuring the auto-correlation of the signal. Additionally, the chromatic response of the instrument can further complicate situations by introducing extra coupling that contaminates a large region in Fourier space, known as the foreground wedge[12].
To address these issues, many techniques have been developed, leveraging the spectral smoothness[28,29], the dominance[13,30,31,32], or statistical properties[33,34]of the foreground. An alternative approach is to ignore the foreground-contaminated regions and conduct cosmological measurements using only the remaining data, a method known as foreground avoidance. However, in either case, a significant portion of Fourier space is compromised or lost by foreground interference.

These Missing information can significantly impact cosmological measurements. For one, the loss of Fourier modes reduces the statistical constraining power. For baryonic acoustic oscillations (BAO), this loss also affects data post-processing. Most current BAO measurements in galaxy surveys utilize a technique known as BAO reconstruction[35,36,37,38,39,40], which aims to mitigate the non-linear degradation of the BAO signal. Despite the technical differences among various reconstruction algorithms, these methods generally seek to solve the continuity equation to ‘linearize’ the BAO signature. Since this process occurs in the configuration space, the absence of substantial Fourier modes can severely influence the outcome.
However, it is still possible to address this challenge by recovering information through the nonlinear coupling between different modes. For example, the tidal reconstruction method[41]develops a quadratic estimator from the tidal field to extract large-scale modes from small-scale perturbations. Yet, as a perturbative approach, its effectiveness is primarily limited to large scales.

With the widespread application of artificial intelligence (AI), Machine learning (ML) has become a powerful tool for processing large-scale structure data and analyzing cosmic dataset[42,43,44,45,46,47,48,49,50]. It has also proven to be highly effective in addressing high-dimensional astronomical problems, such as reconstructing BAO signals from dark matter density fields[51], isolating the EoR signals with complex frequency-dependent beam effects[52], and identifying ionization regions in images with instrument noise[53,54].
Therefore, in this study, we aims to apply the deep learning technique, particularly the U-Net model, to recover the large-scale structure modes in Fourier regions affected by foreground contamination. Additionally, we will examine the impact of the AI-restored data on BAO reconstruction.
The structure of this paper is as follows: Section2provides a brief overview of foreground contamination and its impact on BAO reconstruction. In section3, we introduce our main methodology, including details of the simulation data for 21cm intensity mapping, the U-Net architecture, and the BAO reconstruction process. The results are presented in Section4, and we conclude with a discussion in Section5.

SECTION: 2Foreground Contamination and the Impact on BAO Reconstruction

SECTION: 2.1Foreground Wedge

In both single-dish and interferometric observations, the spectral smoothness of the foreground indicates that the LSS signal may be compromised within the Fourier region where the wavenumbers along the line-of-sight direction are small, i.e.whererepresent the line-of-sight component of the wave vector. This commonly referred to as the intrinsic foreground. The exact value ofdepends on the specific foreground removal techniques employed and specific instrumental systematics; for our analysis, we have adopted a reasonable value of.
Furthermore, the chromatic response of interferometer complicates the situation[55,56,57]by introducing additional coupling between spectral and spatial information. This coupling can lead to foreground leakage from lowto high, with the effect being particularly severe at high, whereis the transverse components of.

As extensively discussed in the literature, the mixing induced by instruments presents both challenges and opportunities. Assuming a simple toy model for the foreground, where the power spectrumis proportional to the Dirac delta functionalong LoS, the observed power spectrum can be approximated as[23]

whererepresents the average squared primary beam profile, integrated across directions orthogonal to the baseline. The functionis defined as

withandare the density parameter of matter, cosmological constant and curvature respectively.
Consequently, a foreground confined in low wavenumber along LoScan significantly spread into highvalues. However, since the primary beam of an antenna typically diminishes at the edge, it naturally sets a limit on this foreground mixing. If we assume the primary beam attenuates to zero at an angle, the relationship delineating the foreground wedge can be described as

in the two-dimensionalplane. In this paper, we consider, at the redshift of, and cosmological parameters as follows.
The combination of intrinsic foreground and the mode mixing wedge results in significant information loss, which, as will be discussed, substantially impacts BAO reconstruction.

SECTION: 2.2BAO Reconstruction

Despite its robustness and enormous success in constraining cosmological parameters in LSS surveys, the baryonic acoustic oscillation is still subject to various systematics. One of the most well-understood effects is the broadening of the BAO peak caused by the non-linear evolution of the bulk velocity of LSS[58]. In the Fourier space, this manifests as a dampening of the BAO wiggles, which can be modeled by a simple exponential function. Compared to the linear BAO peak, this effect reduces the measurement accuracy of given LSS survey in extracting the sound horizon scale. Moreover, the same non-linear process also leads to the emergence of non-Gaussianity, which results in additional information leakage into higher order statistics and further decreases the overall constraining power of the measurements.
Various techniques have been developed to reverse such process. For example,[59]proposed a reconstruction algorithm based on the linear Zel’dovich approximation which can partially reverse nonlinear degradation of BAO signal. This technique has been widely used in recent galaxy surveys[60,61,62,63].

Recently, many other nonlinear reconstruction methods have been proposed[35,36,37,38,39,40], setting aside their technical specifics, all of these algorithms aim to solve the continuity equation

whereandare Lagrangian and Eulerian coordinates of particles respectively,is the Kronecker delta, the displacement potential and vector are defined with. By solving equation (2.4), the reconstruction algorithm reverse such non-linear transformation and produces a reconstructedfield on a grid that is close to the Lagrangian coordinates. In the following, we will denote the Laplacian of the reconstructed field. Consequently, the BAO signature of the reconstructed field closely resemble that of the linear theory.
Since these algorithms solve for the displacement field in the configuration space, in 21cm intensity mapping experiments, the missing modes due to the foreground contamination will inevitably impact the performance of the reconstruction.

SECTION: 3METHOD

At low redshifts, neutral hydrogen predominantly resides within galaxies. To simulate mock observations for the 21cm intensity mapping, we begin with a fast N-body simulation to obtain the dark matter distribution and identify halos. Using the prescription outlined by[16], we then generate the HI mass distribution. Finally, we convert this distribution into the brightness temperature of the 21cm signal.

SECTION: 3.1Data Preparation

We utilize the publicly available COLA (COmoving Lagrangian Acceleration) simulation code111https://bitbucket.org/tassev/colacode/src/hg/[64], a hybrid approach integrating second-order Lagrangian Perturbation Theory (2LPT) and N-body algorithms, to rapidly simulate multiple realizations of dark matter distributions.
Specifically, we conducted thirty realizations using the same set of cosmological parameters described previously but with varying initial seeds. Each simulation containsparticles within a cubic box measuring. The initial conditions for these simulations were set at a redshift of.

Using the dark matter distributions described above, we then generate the corresponding distributions of neutral hydrogen for each realization. Given the relatively low mass resolution of our dark matter simulations, the resolved halo masses are significantly larger than the minimum halo mass that can physically host HI. Consequently, our HI map consists of two distinct components: resolved halos and a subgrid contribution.
For resolved halos, we employ the Robust Overdensity Calculation using K-Space Topologically Adaptive Refinement (ROCKSTAR) halo finder[65]to identify halo catalogs. This includes determining their locations, masses, and peculiar velocities. Furthermore, we account for the redshift space distortions (RSD) by adjusting the real space coordinates of halos according to their peculiar velocities, along the line-of-sight (LoS) direction, which we designate as theaxis.
The redshift space positionsis then defined by

Here,xrepresents the real space comoving position, andis the peculiar velocity along LoS. Additionally,is the scale factor,is the Hubble parameter at redshift. These halos will subsequently be populated with HI using a halo mass relation, as discussed in the next subsection.

For subgrid contribution, we assume these low-mass halos trace the dark matter field. Therefore, with the position and the peculiar velocity of DM particles, we construct the redshift-space matter distribution using Eq (3.1).
In each cell with volumeand massat redshift, we assume the number of halos with massfollows a Poisson distribution, with the average number density determined by the halo mass function. For halos that are viralized at redshift, the average number density can be calculated by[66]

with

Hereis the critical density,represents the variance of density fluctuations on massandis the critical density for spherical collapse at redshiftextrapolated to the present using the linear theory. The termcorresponds to the initial density necessary for a region to reachat redshift.
Therefore, within each cell of volume, we generate a random sample of low-mass halos within a specific mass bin, ensuringfollows a Poisson distribution with mean. Similar to resolved halos, these randomly generated halos are then converted into HI, and subsequently, into brightness temperature.

Following the recipe described in[16], the HI mass within a halo with massat redshiftis calculated using the fitting formula

Here,are redshift-dependent parameters that interpolated from the fitted values provided in[16].
The brightness temperature at redshiftis subsequently calculated as

whereis the baryon fluctuation. Since after the EoR, HI mainly resides within galaxies, we assume that the HI fractionis unity and that the spin temperature is significantly higher than the cosmic microwave background radiation temperature. The baryon fluctuationin each cell is related to the HI mass by the equation

whereis the hydrogen fraction.

After generating these observed brightness temperature data, we further perform data augmentation by rotating the datacube around a fixed line-of-sight direction as the rotation axis to generate additional samples.
This process results in a total ofsamples of 21cm brightness temperature. The dataset is then split into three subsets:is used for training the U-Net models,is reserved for validation to fine-tune hyperparameters, and the remainingis used for testing to evaluate the model’s generalization and performance.

SECTION: 3.2U-Net Architecture and Training

To address the issue of missing modes, we employ deep learning techniques, specifically the U-Net architecture, for their recovery. U-Net is a widely adopted network for image segmentation that is structured around a fully convolutional neural network[67,68]. While the ability to recover these missing modes is due to the non-linear mode coupling in LSS, which is more conveniently described in Fourier space. However, such coupling in Fourier space is intrinsically non-local and often involves features that are quite distant from each other. Meanwhile, standard hierarchical convolutional networks, which progressively connect information from nearby pixels, struggle to effectively capture such non-local features. In contrast, the same effect is much more localized in configuration space, thereby making it a more suitable domain for applying the neural network.

As shown in Figure1, our U-Net model consists of five layers each of down-sampling and up-sampling process. In the down-sampling phase, the encoder incorporates convolution and pooling layers to gradually reduce the high spatial resolution of the simulated data. To process large volumetric data, we use the successive double convolutional layers offollowed by a max pooling layer withkernel size to efficiently capture the three dimensional spatial features. In the up-sampling phase, transposed convolution layers ofare utilized to restore spatial resolutions. Additionally, skip connections create direct links between down-sampling and up-sampling layers with matching corresponding resolutions, allowing easier flow of features and gradients during backpropagation, while enhancing the preservation of complex details and helping to alleviate computational overhead.
The ReLU activation function is applied after each convolution layer, allowing the network to capture and retain effective feature information from deeper levels.

The training of neural networks begins with forward propagation, during which the network processes input data to extract features and produce predictions. Therefore the quality of this input data significantly impacts the network’s training efficacy and the accuracy of its predictions.
Training neural networks requires considerable computational resources to update the weights and biases during training. Higher-resolution input data allows the model to capture finer details of LSS, which encode valuable information about non-linear mode coupling. However, this improvement comes at the cost of increased computational demands, as higher resolution results in larger input volumes and a corresponding increase in the number of hyperparameters that must be processed.

To optimize model performance while managing computational constraints, we divided the extensive dataset into smaller batches and experimented with different resolutions to evaluate their effects on model efficacy. Our goal is to achieve an optimal balance between computational limitations and the maximization of usable data. From the previously described simulated 21cm brightness temperature data, we randomly selectedpercent as the training set, in which foreground contaminated modes have been excluded. Specifically we removed modes whereandin Fourier space. After some testings, we opted to utilize these datacubes with a grid size of. As discussed later, our AI model, trained with this lower resolution data, sufficiently captures the mode coupling features, and could be applied to predict higher resolution data and produce high-quality modes restoration.

During training, we compute the Mean Squared Error (MSE) loss function, defined as the average squared difference between the predicted fieldand actual field, i.e.

whererepresents the number of training samples in a single batch. This loss function quantifies the deviation of the model’s predictions from the true values for each batch. With above loss function, we employ the Adam optimizer[69]to adjust the model weights and biases by minimizing the squared errors during the training loop, with a learning rate of.
The backpropagation process is initiated to fine-tune the network parameters, allowing the generated results to progressively converge with the true values. This iterative adjustment continues until the network achieves convergence.

SECTION: 3.3BAO Reconstruction

After restoring the modes using machine learning, we proceeded to assess their impact on the non-linear iterative BAO reconstruction. Various algorithms are available for this purpose. In this study, we particularly adopted a particle-based reconstruction algorithm, similar to the one described in[70]for its simplicity and computational efficiency. A detailed examination reveals that the reconstruction performance of this code is very similar or even slightly better than the field-based algorithm we have developed previously[35,37].
Both algorithms solve the continuity equation (2.4) for the non-linear displacement field. In the particle-based approach, this is achieved by iteratively applying the Zel’dovich approximation to a field that is smoothed with gradually smaller scales.
Therefore, the algorithm executes the following steps during each-th iteration:

Calculate the density mapfrom distribution of particles, using Cloud-in-Cloud algorithm (CIC).

Apply a Gaussian smoothing windowto the density map.

Solve for the linearized displacement potential, and then convert it back to the configuration space.

Move all particles backward by applying the negative of the displacement,.

Here, we set the smoothing scale for the-th iteration as, where. After approximatelyiterations, the smoothing scale reduces to about, at which point the algorithm exists the iteration. We then compute the combined non-linear displacement. The reconstructed scalar field is then defined as the divergence of the displacement,.

Since the product of a 21cm intensity mapping experiment is a temperature map, it is necessary to convert the temperature fluctuation into distribution of particles. To achieve this, we employ supersampling of the density field to generate particles. To maintain the high resolution in the particle representation and ensure the smoothness of the field, the number of particles,, is set to be at leasttimes greater than the number of pixels in the original field.
Furthermore, since the reconstruction algorithm does not differentiate between the dark matter field and any biased tracers, it is crucial to de-bias the temperature distribution so that the input map provided to the algorithm accurately represents an estimation of the underlying dark matter distribution. For such purpose, we define the input density distribution

Here,is the linear bias coefficient, estimated from the cross-correlation between dark matter field and the temperature fluctuation map. Specifically, we averaged the scale-dependent biasup to a cut-off scale.
In practice, without direct access to the dark matter distribution, the bias parameter must be measured using alternative modeling methods, which can introduce additional errors. As discussed in[71], an inaccurate estimation of the bias parameter may result in further damping of the BAO signature. We will explore these observational effects in future discussions.

SECTION: 4RESULT

Due to GPU constraints, the AI model was trained at a low resolution of. However, this low resolution could significantly impact the performance of BAO reconstruction, which is our main objective.
Fortunately, we discovered an very interesting scale invariance in the AI model when applied to the LSS data. Specifically, we found that the model, despite being trained on low resolution data, could be directly applied to higher resolution maps as long as the dimension of the dimension of the high resolution map can be processed by the network architecture depicted in Figure1.
In Figure2, we present a visual comparison of true temperature maps (toppanels), observed maps with foreground-contaminated modes removed (middlepanels), and the AI restored map (lowerpanels). Different columns display the same map at varying grid sizes, ranging from, to,, and.
As demonstrated, in all these cases, the AI-restored maps successfully resemble the true brightness temperature maps in each column.

Before delving into the detailed performance analysis in each case, let us first understand the reasons behind such scale invariance of AI model. This property can largely be attributed to two factors: the self-similarity exhibited by large-scale structure evolution and the hierarchical structure of U-Net model. Specifically, from the perspective of perturbation theory, the mode-coupling coefficients appeared in the non-linear dynamical evolution of the perturbation[72]

is deterministic and predictable. Therefore, the patterns learned by the deep learning model at coarser grid resolution are not only meaningful but also representative of physical processes that persist across scales.
On the other hand, the architecture of U-Net, with its hierarchical structure of convolutional layers designed to capture features at multiple scales, is particularly well-suited to leveraging such scale invariance of the large-scale structure data.

To evaluate the performance of our U-Net model, we present the two-dimensional power spectrumof various fields in Figure3. The upper panels show the power spectrum of the true brightness temperature, the middle panels display the spectrum of the observed temperature with foreground contamination removed, and the lower panels depict the power spectrum of the U-Net predicted field. Similar to the layout of Figure2, different columns correspond to results at various resolutions. As the box size of the field remain constant, finer resolutions leads to a larger range of Fourier modes. Due to the geometric shape of the foreground wedge, the observed maps at all resolutions lose a significant amount of information.
Despite this, the U-Net model effectively restores power in the foreground-contaminated regions, closely approximating the original true signal across all scenarios presented.
At lower resolutions, particularly for, the amplitude of the restored power is noticeably lower than the original, especially near the edge of the foreground region. This discrepancy is most pronounced at large scales in the lower left corner of the plot. As resolution increases, this difference diminishes. Specifically, at comparable scales, such as, the restored power in higher resolution maps more closely approximates the true values than in lower resolution maps. However, a slight reduction in restored power remains observable diagonally near the boundary of the foreground wedge.

Of course, the reemergence of power in the contaminated regions does not necessarily mean that the missing modes have been accurately restored. To assess this, we analyzed the correlation ratio between the AI-restored and original temperature distributions. The cross-correlation ratio between two fieldsandis defined as

Hererepresents the cross power spectrum between two fields, andis the respective auto power spectra.
Figure4illustrates the cross-correlation ratio in two-dimensionalspace, with resolutions varying fromon the left toon the right.
As shown, the cross-correlation ratio for the low-resolutionsample is approximatelyat very large scales () but drops to aroundat. Additionally, because the map is trained in the configuration space, thefor modes unaffected by the foreground in the upper left corner does not perfectly equal one.
For the grid size of, the correlation ratio with the true temperature map significantly improves, reaching aroundat. With larger grid sizes, the performance further improves. For example, thegrid map achieves a correlation ratio of approximatelyat, compared to aboutfor thegrid map. This trend continues with a finergrid, which is the maximum size our GPU memory can accommodate. Therefore, it is reasonable to expect that, with more advanced GPU capabilities, the model could potentially achieve even better results.

Furthermore, we also present the one-dimensional correlation ratio, averaging over results in Figure4. As displayed in Figure5, solid lines represent the correlation ratio for the AI-restored fields, while dashed lines indicate the ratio for maps with missing modes due to foreground avoidance. Due to the extension of the foreground wedge to largervalues, the 1D correlation ratio does not converge to one, even at smaller scales. Except for the lowest resolution map of, the overall correlation ratio of the AI-restored fields reaches approximatelyat very small scales,.
This performance exceeds the scale limits () where the BAO reconstruction algorithms typically cease to be effective, demonstrating that our AI restoration maintains sufficient fidelity for subsequent application of BAO reconstruction.

SECTION: 4.1BAO Reconstruction

Finally, having restored the lost Fourier modes, we proceed to reconstruct the linear BAO signature using our algorithm. To more clearly observe the effects on the BAO, we apply our trained AI model as well as the subsequent BAO reconstruction to a pair of simulations that share the same initial condition seed but differ in their transfer functions: one retains the BAO signature, while the other employs a ‘no-wiggle’ smooth function. This allows us to define the BAO signature as

in Fourier space, whereis the power spectrum with BAO, andis the ‘no-wiggle’ power spectrum.

The results are displayed in the upper panel of Figure6. All signals, except for the linear model (black dotted line), are derived from data with a grid size of. As seen, the BAO signal from the AI-restored field (thin green solid line) is very similar to the true 21cm brightness temperature map (thin orange solid line), indicating the effectiveness of our AI model. Due to the non-linear evolution, the wiggles after the third peaks are dampened away compared to the linear model.
the AI-restored field (red dotted line) closely aligns with the BAO-reconstructed true brightness temperature that includes all modes (thick black solid line). This suggests that our AI restoration has minimal impact on the effectiveness of BAO reconstruction. In both fields, BAO reconstruction significantly enhances the BAO signature, especially beyond the third peak, where it had been dampened in the original field.
In the lower panel of the figure, we present the differences in the BAO signature between the AI-restored and true temperature fields, both before (black solid line) and after BAO reconstruction (purple solid line). Notably, the difference in both scenarios is around, which is relatively small. Additionally, at larger scales (), the two curves largely overlap, indicating that the BAO reconstruction does not introduce significant numerical errors. However, at smaller scales, although the error range remains similar, the curves begin to diverge, suggesting that non-linear BAO reconstruction introduces its own numerical errors in the field and BAO signatures.

One might notice that both reconstructed BAO signatures deviate from the linear theory (black dotted line), particularly at the second and third peaks. This deviation can be attributed primarily to the effects of clustering bias. Despite correcting for the linear bias factor before reconstruction, the relationship between the brightness temperature and the underlying dark matter is complex, involving various non-linear and non-local contributions. As previous studies have shown[39], such complexities can diminish the effectiveness of the reconstruction. A secondary factor may be the resolution of the field. Constrained by the GPU limitation, grid size ofis not ideal for the BAO reconstruction. However, such effect is less significant compared to the clustering bias. Moreover, future advancements in computational capabilities are expected to mitigate the influence of resolution constraints on subsequent BAO reconstructions.

SECTION: 5Conclusions and Discussion

With recent advancements in detecting the auto-power spectrum of 21cm intensity mapping[73], we are approaching closer to the ultimate goal of cosmological constraints using this technique. Despite these advances, many aspects of the data analysis process remain unexplored, potentially impacting future measurements. This paper focuses on the BAO reconstruction technique, originally developed for galaxy redshift surveys, which has evolved over many years to effectively reverse the effects of non-linear structure formation and restore the linear BAO signature, thereby enhancing the measurement accuracy of the sound horizon scale. However, foreground contamination severely compromises modes within the Fourier regions affected, including both intrinsicand a wedge-like region, leading to the loss of valuable cosmological information and complicating the application of BAO reconstruction, which fundamentally operates in the configuration space. Fortunately, however, the non-linear structure formation, results in complex mode coupling, enabling the foreground-unaffected modes carry information from those that are missing.

In this paper, we utilize the U-Net deep learning architecture to retrieve this missing information and assess its impact on BAO reconstruction. We demonstrate that this popular and powerful deep learning architecture is capable of retrieving the missing information entangled within the observed data. Despite the success demonstrated here, there is potential for further improving the model by optimizing the loss function, introducing more effective hierarchical structure, and providing more training dataset etc.
During our training process, we discovered an interesting scale invariance when applying the AI model to large-scale structure data: a model trained on a coarser map could be successfully applied to finer resolutions, achieving even higher cross-correlation ratios. We attribute this to the scale-invariance in the mode-coupling of the non-linear structure formation process, where the patterns learned by the deep learning model at coarser grid resolutions represent meaningful physical processes applicable across different scales. Furthermore, the architecture of U-Net, with its hierarchical structure of convolutional layers that capture features at multiple scales and then reconstruct the output at higher resolutions, is particularly well-suited to leveraging the scale-invariance of large-scale structure data.

Despite the promising performance demonstrated here, many systematic effects could potential affect the result. Crucially, for a machine learning-based method, it is essential to incorporate real-world observational effects into the training process to mitigate potential biases in the trained model. These include factors such as instrumental beam pattern, thermal noise, and residual foregrounds, among others. We plan to address these aspects in future studies.
Meanwhile, this is also the reason why we adopted this two-step approach of reconstruction, rather than directly predicting the linear fields from the foreground-contaminated temperature map. Given physical limitation set by the shell-crossing, the deep learning process is unlikely to significantly outperform traditional BAO reconstruction algorithms. Consequently, using AI in the subsequent process might introduce additional cosmology-dependent errors without significant benefits.

SECTION: Acknowledgments

This work is supported by the National SKA Program of China (Grants Nos. 2022SKA0110200, 2022SKA0110202 and 2020SKA0110401), the National Science Foundation of China (Grants Nos. 12473006, 12373005), the China Manned Space Project with No. CMS-CSST-2021 (B01, A02, A03). The authors acknowledge the Beijing Super Cloud Center (BSCC, URL: http://www.blsc.cn/) for providing HPC resources that have contributed to the research results reported within this paper.

SECTION: References