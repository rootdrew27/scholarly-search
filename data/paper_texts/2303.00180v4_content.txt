SECTION: MMA-MRNNet: Harnessing Multiple Models of Affect and Dynamic Masked RNN for Precise Facial Expression Intensity Estimation

This paper presents MMA-MRNNet, a novel deep learning architecture for dynamic multi-output Facial Expression Intensity Estimation (FEIE) from video data. Traditional approaches to this task often rely on complex 3-D CNNs, which require extensive pre-training and assume that facial expressions are uniformly distributed across all frames of a video. These methods struggle to handle videos of varying lengths, often resorting to ad-hoc strategies that either discard valuable information or introduce bias. MMA-MRNNet addresses these challenges through a two-stage process. First, the Multiple Models of Affect (MMA) extractor component is a Multi-Task Learning CNN that concurrently estimates valence-arousal, recognizes basic facial expressions, and detects action units in each frame. These representations are then processed by a Masked RNN component, which captures temporal dependencies and dynamically updates weights according to the true length of the input video, ensuring that only the most relevant features are used for the final prediction. The proposed unimodal non-ensemble learning MMA-MRNNet was evaluated on the Hume-Reaction dataset and demonstrated significantly superior performance, surpassing state-of-the-art methods by a wide margin, regardless of whether they were unimodal, multimodal, or ensemble approaches. Finally, we demonstrated the effectiveness of the MMA component of our proposed method across multiple in-the-wild datasets, where it consistently outperformed all state-of-the-art methods across various metrics.

SECTION: 1Introduction

Human emotions are complex, conscious experiences that profoundly influence behavior and can be expressed in various forms. These emotions are pivotal in psychological processes and significantly impact human actions. The advent of Artificial Intelligence (AI) and Deep Learning (DL) has driven the development of intelligent systems capable of recognizing and interpreting human emotions. Psychologists have proposed multiple descriptors to quantify and categorize emotional states: sparse descriptors like facial action units (AUs), which capture specific facial muscle activations[10]; continuous descriptors such as valence and arousal, where valence indicates the positivity or negativity of the emotion, and arousal reflects the level of activation or passivity[50]; and discrete class descriptors like the six basic expressions (anger, disgust, fear, happiness, sadness, surprise) and the neutral state[9]. This paper focuses on dynamic multi-output Facial Expression Intensity Estimation (FEIE), specifically targeting the intensity estimation of expressions such as Adoration, Amusement, Anxiety, Disgust, Empathic-Pain, Fear, and Surprise.

In this paper, we introduce our approach MMA-MRNNet, a novel deep learning architecture designed to tackle the complexities of FEIE in scenarios where video-level annotations (i.e., there exists one annotation for the whole video) are provided rather than frame-level annotations. The key challenges addressed by MMA-MRNNet include handling the variability in video lengths and accurately aggregating temporal information across frames to make a robust final prediction.

Traditional approaches for processing 3-D signals, such as video data, typically employ 3-D CNNs that produce a single prediction per signal. However, these architectures are inherently complex, with a high number of parameters, and often require pre-training on large 3-D datasets to achieve satisfactory performance. Another common approach involves assigning the video-level label uniformly to each frame and then using CNN-RNN networks to train on these annotated frames. This approach assumes that the facial expression intensity is consistent across all frames, which may not be the case, as only a subset of frames might actually display the labeled intensity[24,1,23,26,31,34,39,12].

Moreover, our approach addresses the challenge of variable-length input videos. Traditional methods often rely on ad-hoc strategies to manage varying numbers of frames, such as setting a fixed input length and either discarding excess frames (which risks losing critical information) or duplicating frames in shorter videos (which can bias the model towards repeated data). These strategies are not only suboptimal but also require empirical tuning for each specific dataset, limiting their generalizability and effectiveness.

MMA-MRNNet comprises two primary components: the Multiple Models of Affect (MMA) extractor and the Masked RNN and Routing Network (MRNN). The MMA component is a Multi-Task Learning (MTL) CNN that extracts affective representations from each frame by concurrently estimating valence-arousal (VA), recognizing the 7 basic expressions, and detecting multiple action units (AUs). To ensure the reliability and consistency of these representations, we introduce a novel loss function that incorporates prior knowledge of the relationships between different affective descriptors, mitigating issues like noisy gradients and poor convergence typically encountered in MTL settings.

The extracted representations are then passed to the MRNN component, which consists of an RNN designed to capture temporal dependencies across the sequence of frames. To handle the varying lengths of input videos, a Mask layer is employed within the MRNN. This layer dynamically selects relevant RNN outputs based on the actual number of frames in the video, allowing the model to adapt to variable input lengths without compromising the integrity of the temporal information. The selected features are then passed through fully connected layers to produce the final intensity estimation for the entire video.

To the best of our knowledge, MMA-MRNNet is the first architecture to leverage valence-arousal, AUs, and basic expressions as intermediate representations for the task of Facial Expression Intensity Estimation. This approach not only enhances the model’s ability to capture the nuanced dynamics of emotional expressions but also provides a robust framework for handling real-world data with varying input conditions.

SECTION: 2Related Work

[16]presented Supervised Scoring Ensemble (SSE) for emotion recognition. A new fusion structure is presented in which class-wise scoring activations at diverse complementary feature layers are concatenated and used as inputs for second-level supervision, acting as a deep feature ensemble within a single CNN architecture.[60]proposed a deep Visual-Audio Attention Network (VAANet) for video emotion recognition; VAANet integrates spatial, channel-wise, and temporal attentions into a visual 3D CNN and temporal attentions into an audio 2D CNN. A polarity-consistent cross-entropy loss is proposed for guiding the attention generation, which is based on the polarity-emotion hierarchy constraint.[13]constructed an A/V hybrid network to recognize human emotions. A VGG-Face (for extracting per-frame features) and LSTM (for correlating these features according to their temporal dependencies) architecture was used for the visual data.

[44]was the winning method of the Emotional Reaction Intensity (ERI) Estimation Challenge of the 5th ABAW Challenge[40,18,41,42,33,27,29,30,19,20,35,25,21,36,31,22,37,34,26,1,23]. This method consists of an audio
feature encoding module (based on DenseNet121 and DeepSpectrum), a visual feature encoding module (based on PosterV2-Vit), and an audio-visual modality interaction module.[53]proposed ViPER, a modality agnostic late fusion network that leverages a transformer-based model that combines video frames, audio recordings, and textual annotations for FEIE.[56]proposed a dual-branch FEIE model; the one branch (composed of Temporal CNN and Transformer encoder) handles the visual modality and the other handles the audio one; modality dropout is added for A/V feature fusion.[51]achieved the 3rd place in the ERI challenge of the 5th ABAW; it proposed a methodology that involved extracting features from visual, audio, and text modalities using Vision Transformers, HuBERT, and DeBERTa. Temporal augmentation and SE blocks were applied to enhance temporal generalization[3,4,48,2,17]and contextual understanding. Features from each modality were then processed through contextual layers and fused using a late fusion strategy.[54]presented a methodology that involved extracting visual features from video frames using models like FAb-Net, EfficientNet, and DAN, which capture facial expressions and attributes. Audio features are obtained using Wav2Vec2 and VGGish models. The extracted features were then processed through a temporal convolutional network to capture local temporal information, followed by a Transformer Encoder to model long-range dependencies with dynamic attention.[59]presented a methodology that involved extracting audio and visual features using state-of-the-art models and aligning these features to a common dimension using an Affine Module. The aligned features were then fused using a Multimodal Multi-Head Attention model.

SECTION: 3Methodology

FormulationThe input to our method is a video
consisting of multiple instances (i.e., videoframes),, with.is the number of instances (frames), which varies for
different videos;anddenote the height and width of the RGB images (frames). There is a video-level labelY. We further assume the instances also have corresponding instance-level labels, which are unknown during training; the instance-level labels (of all instances of the same video) do not necessary match the video-level label. There aresuch video-label pairs constituting the database. Our objective is to learn an optimal function for predicting the video-level label with the video’s instances as input. To this end, our method should be able to:1) effectively consider the fact that input videos have variable lengths (in other words, the method should tackle the fact that the total number of frames varies for different videos)2) aggregate the information of instancesto make the final decision.
A well-adopted aggregation method is the embedding-based approach which mapsXto a video-level representationand usezto predictY.

Initially, all videosare padded to a uniform length, resulting in video sequences.
Each videoXis then processed by the Multiple Models of Affect (MMA) extractor component, which conducts local analysis on each 2-D frame, mappingXto a multiple affect-level representation matrix. This matrix is subsequently passed to an RNN, positioned on top of the MMA component, to capture temporal dependencies across all. The RNN transformsZinto an embedding matrix, performing global analysis over the entire video.
The subsequent module aggregates the set of embeddingsinto a single video-level vector embedding, which is then fed to a Mask layer.
The Mask layer dynamically selects embeddings based on the ’true’ frame count of the video, accounting for the original number of frames prior to padding. This step is crucial because the video-level annotations imply that all frames collectively, rather than individually, carry important information for an accurate prediction.
The output of the Mask layeris then mapped to another embeddingusing a feed forward layer. Finally,is transformed into the logitsuvia a feed forward layer parameterized byWleading to the video-level classification:.

In the following, we further explain in more detail each component of our proposed method.
Fig.1gives an overview of our proposed framework, MMA-MRNNet.

SECTION: 3.1MMA: Multiple Models of Affect extractor Component

The Multiple Models of Affect (MMA) extractor component processes an input videoXby extracting affective representations from each frame using three distinct models of affect. Specifically, the MMA is a Multi-Task Learning (MTL) CNN model that concurrently performs: (i) continuous affect estimation in terms of valence and arousal (VA); (ii) recognition of 7 basic facial expressions; and (iii) detection of 17 action units (AUs). The architecture of the MMA, illustrated in Fig.2, is structured around residual units, with ’bn’ indicating batch normalization layers.
The model integrates the valence-arousal estimation, 7 basic expression recognition, and 17 AU detection tasks within the same embedding space derived from a shared feed-forward layer.
Consequently, the output of the MMA when processingXis a multiple affect-level representation matrix.

For training the MMA, we utilize multiple in-the-wild datasets, including Aff-Wild2[38,40,20,36,22,37,34,28,57,18,33,41,32,42], AffectNet[49], and EmotioNet[5], which are annotated for valence-arousal, 7 basic expressions, and 17 action units (these action units are an aggregate in all datasets).

Our recent studies[15,14]have highlighted challenges in the current evaluation of affect analysis methods, noting inconsistencies in database partitioning and evaluation practices that lead to biased and unfair comparisons. To address these issues, a unified protocol for database partitioning was proposed, ensuring fairness and comparability, while also accounting for subjects’ demographic information. It was demonstrated that methods previously considered state-of-the-art on original partitions may not retain their performance under this new protocol. Consequently, in this current paper, we adopt this updated partitioning protocol.

A key challenge in utilizing these datasets is the non-overlapping nature of their task-specific annotations. For instance, EmotioNet only includes AU annotations, lacking valence-arousal and 7 basic expression labels. Training the MMA directly with these datasets using a combined loss function for all tasks would result in noisy gradients and poor convergence, as not all loss terms would be consistently contributing to the overall objective function. This can lead to issues typical of Multi-Task Learning (MTL), such as task imbalance, where one task may dominate training, or negative transfer, where the MTL model underperforms compared to single-task models[34,33,32].

To address this issue, we generate AU pseudo-representations () from the 7 basic expression representations () produced by the MMA for each frame. This is achieved by leveraging the relationship between expressions and AUs, as defined in Table 1 of[8]. The study by[8]conducted a cognitive and psychological analysis of the associations between facial expressions and AU activations, summarizing the findings in a table that details the relatedness between expressions and their corresponding AUs. This table is presented in Table1for reference. Prototypical AUs are those consistently identified as activated by all annotators, while observational AUs are those marked as activated by only a subset of annotators.

The AU pseudo-representations are modeled as a mixture over the basic expression categories:

whereis defined deterministically from Table1,
and is 1 for prototypical or observational AUs, and 0 otherwise.

Then we match MMA’s AU representations () with the AU pseudo-representations by minimizing the binary cross entropy with soft targets loss:

With this loss we aim to infuse prior knowledge on task’s relationship (according to Table1) into the network, so as to guide the generation of better and consistent expression and AU representations.
For instance, if the network predictshappinesswith probability 1 and also predicts that AUs 4, 15 and 1 are activated, this is a mistake as these AUs are associated with the expressionsadness. In this case, the AU and expression representations are in conflict.
Therefore the overall objective function () minimized during MMA’s training is:

where:is the loss term associated with the valence-arousal estimation task, and, withbeing the Concordance Correlation Coefficient (CCC) of arousal/valence;is the categorical cross entropy loss associated with the 7 basic expression recognition task; andis the binary cross entropy loss associated with the 17 AU detection task.

SECTION: 3.2MRNN: Masked RNN and Routing Component

As described in the previous section, the MMA component processes an input videoXby extracting affective representations from each frame () using three distinct affective models. This results in an affect-level representation matrix.
This matrix is then fed into an RNN positioned atop the MMA component, which captures temporal dependencies and sequential information across consecutive frames of the video. The RNN sequentially processes the extracted vector representations from frameto frame, mapping these representationsto embeddings, where each.

These embeddings (corresponding to all video frames) are concatenated into a single vector embedding, aligning with the goal of estimating the intensity of various facial expressions across the entire video sequence, consistent with the provided annotations. This embeddingis then passed through a Mask layer, producing new embedding. The original (pre-padding) lengthof the input video is propagated to the Mask layer to guide the routing mechanism. During training, the routing mechanism dynamically selects elements from various positions withinbased on the video’s length, preserving the values of these selected elements and setting the remaining elements to zero. This process effectively routes only the relevant elements into the subsequent layer, thereby enhancing the model’s focus on key temporal features.

The embeddingis then transformed into another embeddingthrough a feed forward layer, which is trained to extract high-level information from the ’masked’ embedding.
During training, only the weights connecting the feed-forward layer neurons to the elements withinrouted by the Mask layer are updated. The remaining weights are updated when their corresponding feed-forward layer neurons are connected to elements withinthat are selected by the Mask layer in a different video input. The loss function minimization is conducted similarly to networks with dynamic routing, where the weights not involved in the routing process remain constant, and links corresponding to non-routed elements withinare ignored.
Finally, the embeddingis mappes to the logitsuvia a feed forward layer parameterized byW, resulting in the video-level classification, expressed as:.

The loss function that we utilized for training MMA-MRNNet: was not the typical Mean Squared Error (MSE) but a loss based on the pearson correlation since that correlation metric was the evaluation criterion for the utilized database:

where:denotes the facial expression;is the pearson correlation coefficient;andare the variances of the expression labels and predicted values;is their covariance.

SECTION: 3.3Datasets, Pre-Processing and Implementation Details

The Hume-Reaction dataset was used as part of both the Emotional Reactions Sub-Challenge of MuSe 2022[6]and the Emotional Reaction Intensity Estimation Challenge of the 5th ABAW Competition in 2023[40,18,41,42,33,27,29,30,19,20,35,25,21,36,31,22,37,34,26,1,23]. The participants of this subchallenge explore a multi-output regression task, utilizing seven, self-annotated, nuanced classes of emotion: ‘Adoration,’ ‘Amusement,’ ‘Anxiety,’ ‘Disgust,’ ‘Empathic-Pain,’
‘Fear,’ and ‘Surprise.’ The dataset consists of 25,067 videos taken from 2,222 subjects of which 15,806 constitute the training set, 4,657 the validation set and 4,604 the test set.

The Aff-Wild2 database[38,40,20,36,22,37,34,28,57,18,33,41,32,42,43]is the largest in-the-wild database and the only one to be annotated in a per-frame basis for the seven basic expressions (i.e., happiness, surprise, anger, disgust, fear, sadness and the neutral state), twelve action units (AUs 1,2,4,6,7,10,12,15,23,24,25, 26) and valence and arousal. In total, it consists of 564 videos of around 2.8M frames with 554 subjects. Aff-Wild2 displayes a big diversity in terms of subjects’ ages, ethnicities and nationalities; it has also great variations and diversities of environments.

The AffectNet dataset[49]contains around 1M facial images, 300K of which were manually annotated in terms of 7 discrete expressions (plus contempt) and valence-arousal.
The original training set of this database consists of around 290K images and the original validation of 4K. We evaluate our method on the updated partitioning protocol of this database according to our previous work[15,14](as we mentioned in Section 3.1). This new partitioning consists of a training set of around 160K images, a validation set of around 45K images and a test set of around 90K images.

The EmotioNet database[5]contains around 1M images and was released for the EmotioNet Challenge in 2017. 950K images were automatically annotated and the remaining 50K images were manually annotated with 11 AUs (1,2,4,5,6,9,12,17,20,25,26); around half of the latter constituted the validation and the other half the test set of the Challenge.
We evaluate our method on the updated partitioning protocol of this database according to our previous work[15,14](as we mentioned in Section 3.1). This new partitioning consists of a training set of around 25K images, a validation set of around 7K images and a test set of around 14K images.

We used the RetinaFace detector[7]to extract, from all images, face bounding boxes and 5 facial landmarks; the latter were used for face alignment. All cropped and aligned images were resized topixel resolution and their intensity values were normalized to.

We chose batch size equal to 4, lengthequal to 480, Adam optimizer with learning ratewhen training from scratch andwhen training in an end-to-end manner, after having initialised each subnetwork. For RNN we utilize 1-layer GRU with 128 units; feed forward layer consists of 32 units.
Training was performed on a Tesla V100 32GB GPU; training time was 3 days. The TensorFlow platform has been used.

SECTION: 4Experimental Results

SECTION: 4.1Comparison with the state-of-the-art

At first we compare the performance of MMA-MRNNet to that of various baseline[6]and state-of-the-art methods: ViPER and Netease Fuxi Virtual Human methods (which are multi-modal methods exploiting audio, visual and text information); the best performing HFUT-CVers method (presented in the related work section; it is an ensemble multi-modal method exploiting both audio and visual information); USTC-IAT-United method (which was presented in the related work section and is a multi-modal method exploiting both audio and visual information); USTC-AC and NISL-2023 methods (both presented in the related work section; they are ensemble multi-modal methods exploiting both audio and visual information).
Table2shows that our uni-modal non-ensemble learning MMA-MRNNet (that exploits only the visual information and does not employ any ensemble learning) outperforms all other methods by large margins (although some methods are multimodal ones or even ensembles). Let us also note that all baseline and state-of-the-art methods utilized the ad-hoc strategy of selecting fixed input length by removing or duplicating images within each video sequence.

SECTION: 4.2Ablation Study

We conducted a series of ablation experiments to evaluate the impact of different elements and components on our model’s performance.

Initially, we used only single-task affective representations (extracted from MMA) as input to the RNN. We then tested combinations of two tasks (e.g., VA & AUs), and finally, we utilized the affective representations from all three tasks concurrently. The results are summarized in Table3, where we present only the best performance for each experiment to avoid cluttering of the results. Notably, even when using only valence and arousal representations, our network outperformed all other methods except HFUT-CVers. The model’s performance improved substantially when incorporating additional per-frame features, such as the 7 basic expressions or 17 AUs. In the two-task experiments, we observed a further increase in the Pearson’s correlation coefficient ranging from 1% to 1.5%. Ultimately, when all three tasks were used together, our method achieved the highest performance.

To identify the optimal architecture for our network, we conducted experiments with various configurations, including different CNNs (e.g., ResNet50 instead of MMA) and RNNs (e.g., LSTM instead of GRU), as well as varying the number of layers and units, as detailed in Table4. After evaluating a wide range of combinations, we determined that the most effective configuration consists of a single GRU layer with 128 units, followed by a feed forward layer with 32 units. Additionally, we evaluated the impact of incorporating the Mask layer, dynamic routing, and our proposed loss function (as an alternative to the conventional MSE). The results presented in Table3demonstrate that these components significantly enhance the performance of MMA-MRNNet.

SECTION: 4.3MMA Evaluation Results

Here we provide an extensive experimental study in which we utilise the top-performing methods from the ABAW Competitions
(FUXI[58], SITU[45], CTC[61]) and the state-of-the-art methods (DACL[11], DAN[55], POSTER++[47]; ME-GraphAU[46]& AUNets[52])
for 7 basic expression recognition, AU detection and valence-arousal estimation, and compared their performance to our proposed MMA component.

As can be seen on Table5, our proposed MMA component outperformed all these methods on all tasks (7 basic expression recognition, AU detection and valence-arousal estimation) and on all utilized databases (Aff-Wild2, AffectNet and EmotioNet) by large margins.

The performance of the proposed MMA method was evaluated against several state-of-the-art approaches across multiple datasets (Aff-Wild2, AffectNet and EmotioNet), as detailed in Table5. The MMA component consistently outperformed by large margins all methods on all tasks (7 basic expression recognition, AU detection and valence-arousal estimation) and on all utilized databases, across all evaluation metrics.
Specifically, on the Aff-Wild2 dataset, MMA surpassed the closest competitor, SITU, for valence-arousal estimation, by 3.24%. It also outperformed the closest competitor, FUXI, for 7 basic expression recognition by 4%, as well as for AU detection by 3.38%. On the AffectNet dataset, MMA again demonstrated superior performance, outperforming the sota methods by at least 4.2% for valence-arousal estimation and by at least 2.2% for 7 basic expression recognition. On the EmotioNet dataset, MMA outperformed the sota methods by at least 2.6%. These results underscore the robustness and superiority of MMA in delivering precise and reliable affect representations.

SECTION: 5Conclusion

In this paper, we introduced MMA-MRNNet, a novel deep learning architecture for dynamic multi-output Facial Expression Intensity Estimation (FEIE) from video data. Our method addresses the limitations of traditional approaches by leveraging a Multi-Task Learning (MTL) framework to extract rich affective representations, including valence-arousal, basic facial expressions, and action units (AUs). These representations are further refined through a Masked Routed RNN (MRNN), which dynamically adjusts to the variable lengths of input videos, ensuring robust and accurate predictions.

We demonstrated the effectiveness of MMA-MRNNet on the Hume-Reaction dataset, where it consistently outperformed by large margins all state-of-the-art methods. We also demonstrated the effectiveness of the MMA component across multiple in-the-wild datasets, where it consistently outperformed all state-of-the-art methods across various metrics.
Our approach not only handles the complexities of video-level annotation but also mitigates the challenges associated with processing variable-length sequences, offering a flexible and powerful solution for real-world applications in affective computing.

SECTION: References