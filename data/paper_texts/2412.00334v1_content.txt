SECTION: EFTViT: Efficient Federated Training of Vision Transformers with Masked Images on Resource-Constrained Edge Devices

Federated learning research has recently shifted from Convolutional Neural Networks (CNNs) to Vision Transformers (ViTs) due to their superior capacity. ViTs training demands higher computational resources due to the lack of 2D inductive biases inherent in CNNs. However, efficient federated training of ViTs on resource-constrained edge devices remains unexplored in the community.
In this paper, we propose EFTViT, a hierarchical federated framework that leverages masked images to enable efficient, full-parameter training on resource-constrained edge devices, offering substantial benefits for learning on heterogeneous data.
In general, we patchify images and randomly mask a portion of the patches, observing that excluding them from training has minimal impact on performance while substantially reducing computation costs and enhancing data content privacy protection.
Specifically, EFTViT comprises a series of lightweight local modules and a larger global module, updated independently on clients and the central server, respectively. The local modules are trained on masked image patches, while the global module is trained on intermediate patch features uploaded from the local client, balanced through a proposed median sampling strategy to erase client data distribution privacy.
We analyze the computational complexity and privacy protection of EFTViT. Extensive experiments on popular benchmarks show that EFTViT achieves up to 28.17% accuracy improvement, reduces local training computational cost by up to 2.8, and cuts local training time by up to 4.4compared to existing methods.

SECTION: 1Introduction

Federated Learning (FL) targets to enable collaborative training across multiple data distributed among different clients while prioritizing data privacy protection[24,25,21].
Early research on FL primarily concentrates on Convolutional Neural Networks (CNNs)[22,20,1]. Recently, the focus has increasingly shifted toward Vision Transformers (ViTs)[8], whose self-attention mechanisms excel at capturing long-range correspondences within images, achieving state-of-the-art performance across visual problems,e.g.,object recognition[8], detection[13,6], and semantic segmentation[40].
Despite their impressive capabilities, training ViTs generally incurs significantly higher computational costs and longer training times due to the lack of spatial inductive biases within images[30,3], making it prohibitively challenging for resource-constrained edge devices.

In the CNN era, the resource-constrained FL problem has been explored by some researchers.
The workflow of these methods is summarized in Figure1(a).
Typically, model-heterogeneous methods[23,1,4,37]train models of varying sizes on clients based on their available resources.
However, these approaches are not well-suited to ViTs, as they fail to fundamentally reduce the computational demands of client-side training.

In this work, we explorewhether the training computational costs of ViTs can be fundamentally reduced without significantly compromising FL performance.
Recent work in self-supervised learning has demonstrated that masked image modeling can effectively learn generalizable visual representations by reconstructing randomly masked pixels in input images[13,32], highlighting the substantial redundancy in images that may be unnecessary for recognition.
To test this hypothesis, we conduct FL experiments withno resource constraints, using masked images to examine their impact on model performance and training computational costs.
In the experiments, images are uniformly partitioned into non-overlapping patches, with a specified ratioof patches randomly masked. Only the unmasked patches are utilized for model training.

As illustrated in Figure2, we conduct experiments under a challenging data heterogeneity setting with, whereis a concentration parameter from the Dirichlet distributionin FL.
Results indicate that varying the masking ratio has minimal impact on model accuracy but significantly reduces training computation costs. For instance, increasingfrom 0.00 to 0.75 reduces the computational load by up to 5.2, with only a marginal decrease in accuracy.
These findings suggest that using masked images in FL is a promising approach for enabling efficient ViT training on resource-constrained edge devices.

Inspired by these observations, we propose EFTViT, a hierarchical federated learning framework (as illustrated in Figure1(b)) that employs masked images to efficiently train ViT models across multiple heterogeneous data on resource-constrained clients, while also enhancing privacy protection by concealing client data content.
EFTViT comprises lightweight local modules on edge clients and a larger global module on the central server, designed to accommodate limited client resources. The local modules are trained on masked images. Rather than aggregating parameters from clients, the global module receives intermediate patch features from the local modules, enabling it to learn universal representations suitable for heterogeneous data.
To maintain client data distribution, we propose a median sampling strategy that adjusts the patch feature count for each class to the median across all classes prior to uploading, enhancing both performance and training efficiency.

Our main contributions in this work are summarized as follows:

To the best of our knowledge, we present EFTViT, the first federated learning framework to leverage masked images for efficiently training ViT models across multiple resource-constrained clients, while also enhancing client data content protection.

EFTViT enables hierarchical training of all model parameters across clients and the central server, demonstrating substantial benefits for heterogeneous data. Additionally, we introduce a median sampling strategy to obscure the distribution information of intermediate features before they are uploaded to the server.

Experiments on popular benchmarks demonstrate that EFTViT improves accuracy by up to 28.17%, reduces local training computational costs by up to 2.8, and lower local training time by as much as 4.4, setting new state-of-the-art results.

SECTION: 2Related Works

SECTION: 2.1General Federated Learning

Federated learning is a decentralized machine learning approach that enhances privacy by training models directly on client devices, only transmitting model parameters to a central server.
Most studies focus on addressing data heterogeneity[22,17,11,20]and privacy protection[2,27,5]in FL.
For instance, FedProx[22]adds a proximal term to optimize the local updates for addressing data heterogeneity.
Regarding privacy protection, Asadet al.[2]apply homomorphic encryption to FL, enabling clients to encrypt their local models using private keys. Shiet al.[27]propose a FL method with differential privacy (DP).
However, these works rely on the ideal assumption that clients have sufficient resources to handle model training process.

SECTION: 2.2Federated Learning on Edge Devices

Federated learning approaches on resource-constrained clients can be categorized into federated distillation (FD)[12,15,19,31]and partial training (PT)[7,1]. FD methods focus on aggregating knowledge from heterogeneous client models to a server model.
For instance, FedGKT[12]trains small models on clients and periodically transfers their knowledge to a large server model via knowledge distillation.
PT methods divide a global model into smaller sub-models that can be locally trained on resource-constrained clients.
For instance, HeteroFL[7]randomly selects sub-models from the global model to distribute to clients.
However, these methods adapt model size to clientsâ€™ capacities, rather than fundamentally addressing the computational burden of client-side training.

SECTION: 2.3Parameter-Efficient Fine-Tuning

When dealing with transformer-based complex models, Parameter-Efficient Fine-Tuning (PEFT)[36,16,14]provides a practical solution for efficiently adapting pre-trained models across the various downstream tasks, which can reduce storage and computation costs by fixing most pre-trained parameters and fine-tuning only a small subset[10].
Several studies[29,38]have explored using different PEFT techniques to assess performance improvements and resource savings in federated systems.
However, the limited fine-tuning of parameters in PEFT inevitably constrains the adaptability of pre-trained models to new tasks, potentially resulting in suboptimal performance in federated systems with data heterogeneity.

SECTION: 3Efficient Federated Learning with Masked Images

SECTION: 3.1Problem Definition

We employ supervised classification tasks distributed acrossclients to formulate our problem. Each clientpossesses a dataset, wheredenotes the data samples andrepresents their corresponding labels. Here,represents the number of data points,denotes the input dimension, andindicates the number of classes.

SECTION: 3.2Overview

As illustrated in Figure3, EFTViT employs hierarchical training across clients and the central server to enable privacy-preserving and efficient collaborative learning. Each client includes a local module withTransformer layers, a shared global module withTransformer layers, and a classification head.
The local module and classification head are trained on each client with unmasked image patches, enabling efficient local training and generating patch features that represent local knowledge.
To safeguard data distribution privacy, a median sampling strategy is applied on each client to create a balanced patch features (BPF) dataset before uploading to the server.
The global module is then trained on the server using the BPF dataset from clients to effectively learn global representations for all tasks. Finally, the server transmits the updated global module parameters back to clients for next training round.

SECTION: 3.3Training with Masked Images

To enable efficient local training on resource-constrained clients, we present a patch-wise optimization strategy.
Firstly, each input image is divided into a sequence of regular, non-overlapping patches, which are randomly masked at a ratio. The remaining unmasked patches, denoted as, are then used to train our framework. We define the patch features obtained by the local module on the clientas, whereandis the operation of randomly masking image patches fromand discarding the selected patches.
To preserve patch ordering for ViTs, the positional embeddings[28]of the remaining patches are retained.
This is inspired by the internal redundancy of images and reduces the amount of data that the model needs to process, thereby lowering computational complexity.
Additionally, these patch featuresmake it pretty challenging to reconstruct the original images since they are encoded from a very small portion of each image, inherently providing EFTViT with a content privacy advantage.
Notably, the entire images are adopted for the inference on each client.

SECTION: 3.4Data Distribution Protection with Median Sampling

To enhance privacy in EFTViT, we propose a median sampling strategy to generate a balanced patch features dataseton each client.
It aims to ensure that the generated patch features on each client contain an equal number of samples for each class, thereby preventing the leakage of statistical information or user preferences when uploaded to the central server.
Imbalanced data distribution on clients is a common issue in federated learning, and the median, being less sensitive to extreme values, is well-suited for addressing this challenge. Our median sampling strategy uses the median of class sample counts on each client to differentiate between minority and majority classes. It then applies oversampling to increase samples of minority classes and downsampling to reduce samples of majority classes. Specifically, for minority class samples, all patch features generated across multiple local training epochs are retained, whereas for majority class samples, only patch features from the final epoch are preserved.
Next, downsampling is applied to reduce the number of samples in each class to the median.Empirically, we find that increasing the sampling threshold adds to computation costs but does not significantly improve final performance.

SECTION: 3.5Hierarchical Training Paradigm

To effectively reduce the computational burden on clients without compromising performance, we propose a new hierarchical training strategy for ViTs that minimizes the number of trainable parameters on the clients. As aforementioned, our ViT models comprise a collection of lightweight local modules, a shared large global module and a classification head.

Training on Clients.On the client, the local moduleis responsible for mapping image patchesinto patch features, while the global moduleencodesinto representation vectors. The final classification headtransforms the representation vectorsto match the number of classes.
Only the parameters of the local module and classification head are trainable, while the parameters of the global module remain frozen and are iteratively updated via downloads from the server.
For the client, the loss function used in local training is defined as

whereis the number of classes in client, andis the probability distribution of label. The parameters,,are from the local module, global module, and classification head, respectively.
Therefore, the optimization objective is to minimize

whereandare trainable.

Training on Server.The server aggregates heterogeneous knowledge from clients to learn universal representations across diverse tasks. The global moduleand classification headare trained using the balanced patch features dataset uploaded from participating clients in the latest training round.
The loss function can be formulated as

whereis the total number of classes, andis the probability distribution of labelon the data.
The optimization objective on the server is to minimize

whereandrepresent respective patch features and labels uploaded from clients.

SECTION: 3.6Collaborative Algorithms

The overall workflow of our EFTViT is shown inAlgorithm1andAlgorithm2.
At the start of each round, the server will randomly choose a proportionfromclients to participate in training.
Each client updates the parameters of its global module and classification head with those received from the server, and then initiates local training. The median sampling is applied to patch featuresto obscure local data distribution and produce a balanced dataset. The detailed process is presented inAlgorithm1.

The server receives the balanced patch features datasetfromclients to update the global dataset, storing new client data and updating existing client data. This dataset is used to train the global moduleand classification head, with the updated parametersandsent back to clients upon completion of training. The process is elaborated inAlgorithm2.

Input:is the dataset inclient.represents the number of training epochs on each client.,,are the parameters of the local module, global module, and classification head, respectively.is the operation of randomly dropout image patches.

Input:is the number of training rounds.represents the number of training epochs on server.,represent the parameter of global module and classification head of the server model, respectively.

SECTION: 3.7Privacy & Complexity Analysis

Data Content Privacy.Contrary to previous beliefs, recent studies show that exchanging intermediate features during federated learning training is safer than sharing gradients. This is because attackers only have access to evolving feature maps rather than the final, fully trained maps, making data reconstruction attacksmore challenging[12,35,39,41]. Furthermore, EFTViT uploads patch features corresponding to 25% of the image area, controlled by the masking rate, which makes recovering the original image highly challenging, even if theoretically feasible. The masking rate can be further increased to enhance data content privacy, if necessary.

Data Distribution Privacy.To protect user statistical information and preferences, our patch features are balanced via the proposed median sampling strategy on clients, ensuring an equal number of samples for each class.
Additionally, our strategy is orthogonal to other privacy protection methods, such as Differential Privacy[9], which can be seamlessly integrated into EFTViT to offer enhanced protection against attacks.

Complexity.Given a ViT model, letrepresent the resolution of original image,represent the resolution of each image patch,be the resulting number,be the latent vector size, andrepresent the number of Transformer layers. To simplify the calculation, we assume that size of,andis.
Each client model hasTransformer layers, divided intolayers for local module andlayers for global module.
The model trains onof the image patches, whereis the masking ratio.
The time cost for forward propagation on the client is.
As the parameters of theTransformer layers in the global module are frozen, the backward propagation time cost is. Therefore, the overall time complexity in the client training stage is.
Asapproachesandapproaches 1, the computational complexity of the model on the client gradually declines. Our default configurations are,, and, substantially reducing the computational load on the client.

SECTION: 4Experiments

SECTION: 4.1Datasets

To comprehensively evaluate EFTViT, we conduct experiments on two widely used federated learning datasets, CIFAR-10[18]and CIFAR-100[18], as well as a more challenging datasets, UC Merced Land-Use[34], for remote sensing.
CIFAR-10 and CIFAR-100 datasets each contain 60,000 color images. CIFAR-10 is organized into 10 classes, with 6,000 images per class (5,000 for training and 1,000 for testing), while CIFAR-100 has 100 classes, with 600 images per class (500 for training and 100 for testing).
UC Merced Land-Use dataset contains 21 land-use classes,e.g.,agricultural, forest, freeway, beach, and other classes, each with 100 images (80 for training and 20 for testing).
We partition samples to all clients following a Dirichlet distributionwith a concentration parameter, settingto simulate high or low levels of heterogeneity.

SECTION: 4.2Implementations

We use ViT-B[8]pre-trained on ImageNet-21K[26]as the backbone of our framework. The input images are resized towith a patch size of. During training, data augmentation techniques such as random cropping, flipping, and brightness adjustment are applied. Following federated learning practices, we set the number of clients to 100, with a client selection ratio. The AdamW optimizer is used with an initial learning rate of, weight decay of 0.05, and a cosine annealing learning rate schedule with warm-up. We use a batch size of 32 for both training and testing on each client. All experiments are conducted on a single NVIDIA GeForce RTX 3090 GPU. In each round, clients train for 5 epochs locally, while the server performs an additional 2 epochs. The framework is trained for a total of 200 rounds, requiring approximately 24 hours.

SECTION: 4.3Comparison with State-of-the-Art Methods

Given the lack of studies on training ViTs on resource-constrained clients, we adapt the FEDBFPT approach[33], originally designed for natural language processing tasks, as a strong baseline, which progressively optimizes the shallower layers while selectively sampling deeper layers to reduce resource consumption.
To establish additional baselines, we adapt several well-known PEFT methods to our federated learning setup: (a)Fed-Head: trains only the head layer parameters; (b)Fed-Bias: applies bias-tuning[36], focusing on training only the bias terms; (c)Fed-Prompt: incorporates prompt-tuning[16], adding trainable prompt embeddings to the input; and (d)Fed-LoRA: integrates LoRA-tuning[14]by adding the LoRA module to the query and value layers. These methods use FedAVG[24]for parameter aggregation.
Otherwise, our method and the baseline methods share the same settings in the federated learning scenario.

Testing Accuracy.The testing results of all methods across various datasets and data heterogeneity levels are presented in Table1. Note that Fed-Full means training all ViT parameters in clients without resource constraints, serving as a reference for the comparison. Compared with the baselines, EFTViT demonstrates apparent performance gains across all scenarios. For instance, we outperform the second-best method by over 7.61% on UC Merced Land-Use with. Notably, our method shows consistent results in high and low data heterogeneity settings, with even better performance under higher heterogeneity. In contrast, the baseline methods degrade heavily in performance as data heterogeneity increases. These findings underscore the importance of our hierarchical training strategy in handling data heterogeneity effectively.

Convergence.We report the testing accuracy changes of EFTViT, FEDBFPT, and other baselines over 100 training rounds on CIFAR-10, CIFAR-100, and UC Merced Land-Use under high heterogeneity settings, as shown in Figure4. Our method consistently achieves the highest testing accuracy on three datasets throughout the training phase, converging faster and more stably.
To quantitatively compare convergence speed, we set a target accuracy of 85% and record the number of training rounds (# Rounds) required to reach this threshold. As shown in Table2, EFTViT significantly accelerates the convergence process, achieving 27.1faster convergence than Fed-Prompt on the UC Merced Land-Use dataset.

Computational Efficiency.We evaluate the client-side computational efficiency of EFTViT from two perspectives: the computational cost of forward propagation during training and the maximum local training time across clients. Computational cost is measured in Giga Floating-Point Operations (GFLOPs). At a target accuracy of 85%, we report the maximum local training time (Time) for EFTViT and other baselines across three datasets.
The results in Table3show that our method significantly improves computational efficiency across both metrics. Specifically, EFTViT achieves at least 2the efficiency of other methods in terms of GFLOPs. For training time, EFTViT reduces local training time by 2.8compared to FEDBFPT on the UC Merced Land-Use dataset. This demonstrates that our masked image and hierarchical training strategy effectively reduces client computation, making EFTViT well-suited for federated learning in resource-constrained environments.

SECTION: 4.4Ablation Study

We conduct extensive ablation experiments to investigate the key components of our approach.

Effect of Masking Ratio.The masking ratiodetermines the number of masked image patches. A smallerreduces the amount of input data, thus lowering computational requirements during model training. Table4provides the GFLOPs for various masking rates, demonstrating that increasing the masking ratio significantly reduces GFLOPs. However, increasing the masking ratio also affects overall performance. We evaluate the effect of different masking rates for EFTViT.
Figure5shows the results of EFTViT with varying masking ratios on CIFAR-100 at. Results indicate that EFTViT can support a wide masking ratio range. When the masking ratio increases from 0% to 75%, the accuracy remains larger than 90%. However, the performance decreases heavily when the masking ratio exceeds 75%. Therefore, we select a masking ratio of 75% to strike a balance between accuracy and computational efficiency.

Effect of Layer Numberin Local Module.The layer numberdetermines the trainable parameter division between clients and the server, affecting the computational load of clients and final performance. Table5presents the number of trainable parameters (# Params) in each client and the corresponding accuracy achieved by the model for different values of.
The results show thathas minimal impact on the testing accuracy, showcasing the superior robustness of our EFTViTw.r.t.client resources. Given the higher computational cost of a largeon clients and the accuracy decrease, we selectas the default setting.

Effect of Sampling Threshold.As elaborated in Section3.4, the sampling threshold determines the number of balanced patch features to upload for server training. Therefore, a higher threshold increases the training cost on the server.
We investigate the impact of utilizing median or higher sampling thresholds in EFTViT, as shown in Figure6. Results indicate that increasing the threshold provides minimal performance improvements.
To enhance the computational efficiency on the server, we select the median as the threshold in our method.

SECTION: 5Conclusion

In this work, we propose a hierarchical federated framework, EFTViT, designed for efficient training on resource-constrained edge devices and handling heterogeneous data effectively. EFTViT reduces client computation by leveraging masked images with an appropriate masking ratio, which minimizes performance degradation while significantly lowering computational overhead by exploiting redundancy in image information. The masked images can also prevent the data content leakage from uploaded local features. Additionally, the hierarchical training strategy, which splits parameter training between the client and server, achieves full parameter optimization and improves performance on heterogeneous data across multiple clients. Finally, EFTViT incorporates a median sampling strategy to protect user data distribution, ensuring privacy while maintaining robust performance.
The extensive experiments on three benchmarks demonstrate that EFTViT significantly improves classification accuracy, reduces client training computational costs and time by large margins.

SECTION: References