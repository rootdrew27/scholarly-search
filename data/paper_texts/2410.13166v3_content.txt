SECTION: An Evolved Universal Transformer Memory

Prior methods propose to offset the escalating costs of modern foundation models by dropping specific parts of their contexts with hand-designed rules, while attempting to preserve their original performance.
We overcome this trade-off with Neural Attention Memory Models (NAMMs), introducing a learned network for memory management that improvesboththe performance and efficiency of transformers.
WeevolveNAMMs atop pre-trained transformers to provide different latent contexts focusing on the most relevant information for individual layers and attention heads.
NAMMs are universally applicable to any model using self-attention as they condition exclusively on the values in the produced attention matrices.
Learning NAMMs on a small set of problems, we achieve substantial performance improvements across multiple long-context benchmarks while cutting the model’s input contexts up to a fraction of the original sizes. We show the generality of our conditioning enables zero-shot transfer of NAMMs trainedonlyon language to entirely new transformer architectures even across input modalities, with their benefits carrying over to vision and reinforcement learning.
Our source code is available athttps://github.com/SakanaAI/evo-memory.

SECTION: 1Introduction

Transformer architectures have become the golden standard in deep learning, with ubiquitous applications in the design of modern foundation models, exhibiting exceptional performance and scalability(Achiam et al.,2023; Das et al.,2023; Team et al.,2023; Dosovitskiy et al.,2020; Chen et al.,2021a; Brohan et al.,2023; Gur et al.,2023).
The outputs of a transformer are exclusively conditioned on a recent context of input tokens, which for language models (LMs) generally correspond to a window of preceding words.
Thus, addressing the challenge of extending this context window is critical to enable tackling long-range tasks and is currently a focal area of research(Huang et al.,2023).
However, long contexts also immediately impact training and inference costs, with modern foundation models being increasingly resource-hungry and expensive. Many recent methods proposed to partially offset these costs by studying how to heuristically quantify the importance of each token stored in the model’slatent memory, i.e., stored in itsKey-Value (KV) cache. Then, by simplyevictingthe least important tokens with hand-designed strategies, they have shown early success at reducing memory size while limiting performance losses(Luohe et al.,2024).

Our research aims to go beyond these hand-designed strategies as we hypothesize that shaping the latent memory KV cache of transformers entails new opportunities toimprovetheir capabilities in downstream tasks.
One widely evidenced example in support of our hypothesis is the effectiveness of hand-crafted input context modifications through prompt engineering(Liu et al.,2023), even allowing foundation models to learnin-contextentirely new skills at test time(Brown et al.,2020).
Furthermore, unlike prompt engineering, directly managing the memory of transformers enables the provisioning of distinct contexts to each latent level independently, such that individual layers and attention heads can focus on the most relevant information for their specific needs.

Motivated by these considerations, we propose Neural Attention Memory Models (NAMMs), introducing a new class of networks trained with evolution to learn an efficient memory system that maximizes the downstream performance of pre-trained transformers.
Evolution inherently overcomes the non-differentiability of memory management operations with binary outcomes (selecting tokens to preserve/discard) which renders gradient-based optimization incompatible.
Our efforts are inspired by the key role that natural evolution played in shaping human memory, which analogously appears to selectively incorporate and actively prune information based on its lifelong usefulness(Sherry & Schacter,1987; Nairne & Pandeirada,2010; Frankland & Bontempi,2005).

Our NAMMs are conditioned on features entirely constructed from the attention matrix, making them universally applicable to any transformer-based architecture.
Learning NAMMs atop a pre-trained Llama 3 8B model(Dubey et al.,2024), we not only obtain efficiency benefits, with substantial reductions in the number of retained tokens in the KV cache, but alsoexceedthe performance of the full-context model with notable margins. We validate these findings across 36 different tasks from LongBench(Bai et al.,2023), InfiniteBench(Zhang et al.,2024a), andChouBun111ChouBun is the pronunciation of “{CJK}UTF8ipxm長文”, literally translating to “long text” in Japanese., a new Japanese benchmark designed to assess long-context capabilities beyond the common English and Chinese. These results mark a clear contrast with the aforementioned hand-designed strategies that appear to inevitably trade off efficiency for performance, in line with their stated purpose.

Furthermore, we show that the generality of our parameterization enableszero-shot transferof NAMMs trained on three natural language tasks to entirely new transformer models.
In particular, we obtain further performance and efficiency improvements not only when using the evolved NAMMs with other LMs of increased size, but also transformers with entirely different architectures concerned with new input modalities, for problems such as vision and reinforcement learning.
In a nutshell, our main technical contributions can be summarized as the following:

We introduce NAMMs, a novel memory evolution framework that adds a new dimension to optimizing transformer models without altering their powerful architectures.

We design and successfully train NAMMs on top of pre-trained transformer models, obtaining both performance and efficiency gains on several long context language tasks.

We show NAMMs, trained only on language tasks, can be transferred zero-shot to any other transformers, retaining benefits across different input modalities and task domains.

SECTION: 2Background and preliminaries

Attention and transformers.Transformers are neural network architectures designed specifically for efficiently processing input sequences.
These models take as input a stream of tokens (e.g., embeddings of words, image patches, robotic states, etc.) and, produce a set of latents with the same length within their layers.
Multi-headed dot product attention(Vaswani et al.,2017), or simplyself-attention, characterizes modern transformers, facilitating effective information sharing across token representations.
The attention layer conducts a set of parallel computations, each known as an attention head, mapping tokens to query, key, and value vectors. These vectors are organized along the sequence dimension in the matrices,, and, and the layer’s output is computed as:

Here,represents an optional mask multiplying theattention matrix, usually enforcing anauto-regressive conditioningsuch that each token cannot attend to its future. An interpretation of the attention layer comes from the elements of the attention matrix, i.e., the dot products between each keyand querynormalized along the column dimension.
Intuitively, each of these values can be understood as the relativeimportanceof tokenin processing the input representation of token.

Frequency-based feature extraction.An established canonical technique to pre-process one-dimensional non-stationary signals is the Short-Time Fourier Transform (STFT)(Allen & Rabiner,1977). This technique has seen plenty of applications for feature extraction concerning audio, biomedical, seismic, and many more kinds of modalities. The STFT performs a time-convolution of a signal, shifting each convolutional window to the frequency domain through a discrete Fourier transform, producing aspectrogramrepresentation of the original input. We useto denote the fixed-sized vector produced at each timestep, where thefrequencies span from zero up to the Nyquist frequency (half the original sampling rate).
Mathematically,the-thfrequency from an STFT for timeis extracted from an input vectoras:

Here, the convolutional filter of the SFTF is defined by the product of a finite-lengthwindow functionwith each exponential term in the Fourier transform. A popular choice foris the Hann window(Oppenheim,1999), employing a smooth decay at its edges which helps minimize the overestimation of the magnitudes of the higher frequencies indue tospectral leakage(Harris,1978).

SECTION: 3Neural Attention Memory Models

An immediate limitation of transformers is the quadratic costs associated with computing the attention matrix.
To partially address this issue, during auto-regressive generation, the latents for the keys and values of the tokens generated at the previous steps are usually stored in what is referred to as the KV cache.
This object can be regarded as being analogous to thememoryof the transformer, which now, at each step, only needs to compute the query, key, and value of the latest token and perform attention over a horizontal vector by exploiting causal ordering. In this section, we describe the feature extraction, architecture, and optimization of NAMMs, which have been designed to act on the KV cache to improve both the performance and practicality of this powerful class of models.

SECTION: 3.1Attention spectrograms for model-agnostic feature extraction

The feature extraction framework of NAMMs is designed to be agnostic to the parameterization of the base transformer they are applied for.
In particular,we build a representation for each token in the current KV cache memory directly from its corresponding unmodified column vector in the attention matrix.To meaningfully compress this unbounded vector signal, we process it via an STFT with a fixed-sized Hann window (Figure2, left).
This operation produces a spectrogram representation of the attention columns, representing the frequencies with how the queries attend to each of the stored key tokens (indexed by) on a compressed time-axis (indexed by).
Thus, this representation exposes precisely the knowledge of how each token’s relative importance varies across all past queries in a compact form factor, discarding all other information specific to the learned transformer weights.

As NAMMs rely only on the attention values for their input, they are universally applicable to any layer producing an attention matrix.
This property is crucial, enabling us to avoid learning individual memory models for the different layers of a transformer, thus, greatly limiting the number of total optimized parameters. Furthermore, it also allows efficient training on top of smaller foundation models for targeted problems, and later transferring the resulting models zero-shot at test-time to larger architectures and arbitrary applications.

SECTION: 3.2Memory model design and cross-token communication

NAMMs parameterize a small neural networkto output a scalarselection scorefor eachtoken in the KV cache.
First, to obtain a consistent input dimension, we reduce the attention spectrogram into a smaller feature vectorby compressing the time-axis via an element-wise exponentially moving average (EMA:; Figure2, center).
We then append positional encodings and feed the vectorto the memory model’s networkto produce the score.
Finally, we evict from the KV cache memory all latent tokens with, effectively treating the problem as a binary classification task.
We repeat this process with a fixed interval, every set number of new input tokens,.

Backward attention memory models (BAM).For the design of, we posit that sharing information from all tokens in memory could be key for assessing their importance.
A particularly motivating scenario in LMs arises when considering the case of repeated words or sentences, where learning a diversity measure that compares different tokens would allow preventing redundancies in the KV cache.
Corroborating this intuition, even from a biological perspective, memory formation and retention appear to adhere to models of neuronal competition(Han et al.,2007).

Based on these considerations, we design the backward attention memory architecture (BAM) for parameter-efficient sharing of information while making use of the powerful inductive biases enabled by the masked self-attention operation.
In particular, we implementvia an initial self-attention layer with acounter-causalmask, which we refer to asbackward(Figure3). This design serves to introduce a purposeful asymmetric relationship, allowing to distinguish between older and newer tokens. We then outputfrom a final linear operation:

whereare the key, value, and query matrices from all feature vectorsin memory.
Using BAM to tackle the previous motivating scenario, only the representation for older tokens would be potentially affected by the presence of newer duplicates.
Thus, just by learning a simple diversity metric within self-attention, backward masking would provide the memory model with the potential to preserve only the most informed occurrence of each token without risking discarding any information in its entirety (since the score for the latest instance of each repeated token would be independent of its past).

Inpractice, when applying NAMMs, we only affect the KV cache of the base model with a fixed frequency, once everysteps. When feeding longer prompts to our model, we simply split the tokens into-sized chunks. We summarize the full execution pipeline of NAMMs in Algorithm1. We refer to AppendixAand our shared code for additional implementation details and discussion.

SECTION: 3.3Incremental evolution

We evolve the network weights of our NAMMs to directly optimize the performance on a subset of long-context language modeling tasks from LongBench(Bai et al.,2023). As we share a singleacross all layers, even with our largest NAMM we only evolve about 4000 total parameters.
We use the seminal CMA-ES optimization algorithm(Hansen,2006)and apply NAMM atop a Llama 3 8B base model(Dubey et al.,2024)with a context extended from 8192 to 32768 tokens via NTK-aware positional interpolation(bloc97,2023).
Due to the inference costs of LMs with long inputs, we sample a subset of different prompts from each task in each generation and propose training in anincrementalfashion: starting from a single task, and adding additional tasks at later training stages.
Empirically, we found both these choices to provide effective regularization, improving generalization (see AppendixC).
The performance of modern LMs on LongBench varies considerably across tasks, and even across different task prompts.
Hence, instead of using the raw scores, we opt to maximize normalized performance relative to the vanilla base model’s stored evaluation performance on each same subset of prompts, retaining all tokens in its KV cache memory.Using evolution, we note that our training loop simply corresponds to running inference NAMMs atop the base, requiring no expensive backpropagation or dedicated hardware.

We choose three tasks from different LongBench categories across both English and Chinese where the Llama 3 base model seems to particularly struggle: PassageRetrieval-en, DuReader, and NarrativeQA; optimizing the normalized exact match, ROUGE-L, and F1 metrics, respectively.
We evolve our NAMM for 300 generations in its first incremental phase, 250 in its second, and 120 in its third. We diminish the number of generations to counteract the increasing costs with each additional phase and make more efficient use of computational resources.
At the end of each phase, we resume from the best previous checkpoint. We provide training curves of our main backward-attention model in Figure4, showing the average and standard deviation of the normalized batch performance across the population (left), together with the normalized per-task and average performance on all samples of the optimized mean from CMA-ES (right).
We refer to AppendixAfor additional architectural and optimization details, together with the set of hyper-parameters. We also provide additional statistics and training curves for other memory model designs in AppendixC.

SECTION: 4Experimental Results

In this section, we evaluate and analyze evolved NAMMs as compared to full-context transformers andthree recent hand-designed methods for KV cache management: H2O(Zhang et al.,2024c)and L2(Devoto et al.,2024), and FastGen(Ge et al.,2024). We compare each method in terms of absolute and normalized performance and also provide the resulting average cache size recorded at the end of each prompt. We first consider three long-context language modeling benchmarks spanning 36 diverse tasks in three languages, using the same Llama 3 8B base transformer from training. Then, we evaluate the capabilities of zero-shot transferring NAMMs to otherunseentransformers and task domains. In particular, we not only consider transfer to larger LMs, but also transformers with tokens constructed from modalities other than language. Across all these settings, we also compare BAM with a simpler 2-layer MLP architecture and provide summarized results after every stage of incremental evolutions. We refer to AppendixCadditional evaluations (e.g., transferring NAMMs to a Mistral LM), ablation studies (e.g., comparing different architectures and input features), and all learning curves. Lastly, we conclude the Section with a targeted qualitative analysis, aimed at understanding the behavior of our new memory framework.

SECTION: 4.1Long-context language understanding

Longbench.In Table2, we provide results across all LongBench tasks(Bai et al.,2023)and in Figure5we provide a summarized comparison varying the maximum cache size of H2O and L2 (we provide a similar analysis for FastGen in Figure9).
Our NAMM yields concrete improvements to the Llama 3 8B transformer both when considering the full set or exclusively the held-out set oftesttasks that were not used for evolution, with improvements of 11% and 7% respectively. At the same time, our NAMM also yields efficiency side benefits, notably reducing the context-extended KV cache size.Instead, H2O, L2, and Fastgen all come with performance costs which notably grow the smaller their cache sizes - in line with their stated objective ofretainingrather thanimprovingthe original full-context performance. These results emphasize the inevitable tradeoff induced by prior hand-designed methods, able to obtain efficiency gains but at increasing performance costs due to their lossy heuristics. On the other hand, we find NAMMs successfully provide a paradigm shift, yielding consistent improvements from the base model across both
performance and efficiency axes by learning to discard unhelpful information, highlighting how end-to-end evolutionary optimization can open new orthogonal directions beyond what is feasible with manually-designed heuristics.

InfiniteBench.In Table3, we provide results across the InfiniteBench tasks(Zhang et al.,2024a). In this benchmark, the average prompt length is close to 200K tokens making it extremely challenging, especially for LMs that were not expensively finetuned for very long context understanding. In fact, as reported byZhang et al. (2024a), even GPT4(Achiam et al.,2023)cannot exceed a performance of 1% on some of its problems. In line with these results, the full-context Llama 3 together with H2O and L2 obtain near-zero performance on most tasks. Instead, our NAMM provides outstanding improvements, bringing overall benchmark performance from 1.05% to 11%.
We also observe that while our NAMM’s memory size is larger than for LongBench, it is considerably lower in relation to the base model’s (now only 40%). This result suggests that NAMMs emergently learned a scalable memory strategy, forgetting redundant and detrimental information at an increasing rate with longer contexts without requiring the hand-designed hard cache limits enforced by L2 and H2O.

ChouBun.Our new benchmark focuses on tasks designed exclusively in Japanese, a novel language unseen during NAMMs training. We hope this benchmark might itself be a valuable contribution to the research community, allowing the assessment of long-context capabilities in multilingual LLMs beyond the already-ubiquitous English and Chinese. We provide further benchmark statistics, details about task composition, together with evaluation metrics for a wider range of popular LLMs in Appendix10. In Table4, we report our results evaluating NAMMs. Once again, we observe a clear contrast with prior hand-designed methods. While integrating either H2O or L2 leads to notable performance drops, NAMMs provides substantial improvements, with overall performance up by 15% from the full-context Llama 3 8B base model.

SECTION: 4.2Zero-shot transfer across architectures and modalities

Cross-scale adaptation.In Table5, we provide results zero-shot transferring our NAMM from the Llama 3 8B to the Llama 3 70B model on LongBench. Across all tasks, we find performance to be very close to the full-context baseline with an overall gap of less than 1% even for the test subset. While NAMMs are not able to improve the overall full-context performance in this first transfer setting outside specific task categories (e.g., coding and few-shot learning), they still outperform both H2O and L2 baselines and retain a similar efficiency as with their original training transformer.

Vision Language Understanding.In Table6, we provide results zero-shot transferring to the computer vision domain, evaluating NAMMs with a Llava Next Video 7B model(Zhang et al.,2024b)on LongVideoBench(Wu et al.,2024)and Multi-Task Long Video Understanding (MLVU)(Zhou et al.,2024). As when evaluated atop Llama 8B, our NAMM is the only method recording gains over the full-context base transformer in both benchmarks. Furthermore, we find that NAMMs learns to forget almost exclusively parts of redundant video frames rather than the language tokens describing the final prompt, even though they were never faced with such modality during training. This result validates that our NAMM recovered a domain-agnostic memory management strategy, further highlighting their flexibility.

Reinforcement learning.In Table7, we provide our zero-shot transfer results for the offline reinforcement learning setting, where we apply NAMMs atop a decision transformer(Chen et al.,2021b)using the open-sourced models fromBeeching & Simonini (2022)pre-trained on the canonical the continuous-control tasks from D4RL(Fu et al.,2020). We find our NAMM improves the base transformer quite considerably in this domain across eight out of nine offline tasks with over 9% overall gains, opposing the performance loss of the other efficient baselines. We posit that since the nature of the decision transformer optimization is closely tied to behavior cloning, the ability to discard part of the context is likely to allow NAMMs toforgetand avoid imitating previous mistakes autoregressively. In support of this hypothesis, we observed slightly higher average rewards in the transitions for the retained tokens (by 1.4%, 0.8%, and 12.3% for the Hopper, Walker2d, and HalfCheetah environments, respectively).

NAMMs comparison.In Table8, we provide summarized results comparing NAMMs with either BAM or the simpler MLP architecture at the end of each stage of incremental evolution. First, we note that even the MLP NAMM after stage 1 impressively improves performance across all language benchmarks. Additionally, performance sees near-monotonic improvements with each additional stage of incremental evolution in both language and zero-shot transfer settings. Comparing our implementations, the performance benefits from the memory models with BAM appear consistently superior to the MLP. Moreover, on ChouBun. we observe that the performance with BAM sees a notable upswing after the second stage of incremental training, which might be associated with the introduction of another ideogram-based language in the training set.222The DuReader task, used in the second stage of incremental training, uses the Chinese language.The same improvement not occurring with the MLP-based NAMMs might be further evidence of architectural performance saturation, highlighting the effectiveness of our main implementation.

SECTION: 4.3Understanding Neural Attention Memory Models

Influence of layer depth.We begin analyzing NAMMs by focusing on the final amount of retained tokens and their oldness333We defineoldnessof a retained token as the number of new queries since its introduction in the KV cache..
At the top of Figure6, we provide these normalized metrics as a function of layer depth.
Interestingly, our learned NAMM does not appear to affect the KV cache uniformly, retaining visibly more and older tokens for some of the early-middle layers of the base transformer.
One possible interpretation of our results, complementing recent analysis(Wendler et al.,2024), is that these layers might be particularly important for processing and aggregating information over longer contexts, thus requiring larger memories than the rest.

Influence of task structure.At the bottom of Figure6, we instead provide these metrics while varying the source task, this time normalized by the average prompt lengths shown in green. Our results illustrate an inverse correlation between normalized memory size and prompt length (with a Pearson coefficient of -0.84), further confirming our earlier observations of sub-linear memory growth and favorable scaling to longer contexts. Additionally, we observe that in the code completion tasks (with task id 6-1 and 6-2) NAMMs learn to preserve visibly more tokens relative to their average prompt lengths. This result appears intuitively consistent with the higher information density in code, leaving room for less redundancy as opposed to natural language.

Selected qualitative examples.We qualitatively find these analyzed trends by inspecting the text corresponding to the forgotten tokens for a few selected prompts. In particular, we consider the layers with the highest and lowest average retained tokens (15 and 24), for tokens from either a natural language or coding task (PassageRetrieval-en, id 5-1, and RepoBench-P, id 6-2). As shown in Figure7, for early-middle layers, NAMMs tend to focus on retaining global information such as the task preamble and key words throughout the text. Instead, for later layers, NAMMs seem to forget many of these tokens, whose information has likely been already incorporated in the previous layers, allowing the transformer to focus more on tokens with more detailed local information. Furthermore, in coding tasks, we find that the pruned tokens are mostly contiguous, corresponding to whitespace, comments, and whole segments of boilerplate code. This is in contrast to natural language tasks, where NAMMs appear trying to exploit some of the grammatical redundancies of the English syntax often dropping specific tokens mid-sentences.

Additional analysis.We provide additional analytic results in AppendixD. For instance,we analyze how the presence of each token in memory affects the scores of the other tokens, we compare the generated responses before and after the introduction of NAMMs in a very long context task, and show the sensitivities of the token scores for each input feature.These results show that NAMMs learn mechanisms for ‘cross-token’ competition relying on high-frequency components of the attention matrices and illustrate how they learn to overcome different failure modes of long context LMs,further evidencing the need to go beyond simple strategies and the potential of end-to-end learning for token-level memory systems.

SECTION: 5Related works

Devoto et al. (2024)andYao et al. (2024)try to identify the least important tokens to evict using heuristics such as L2 magnitude and entropy to improve efficiency. Alternative strategies include considering simple statistics from the attention matrix(Liu et al.,2024b; Oren et al.,2024; Zhang et al.,2024c).Ge et al. (2024)andLi et al. (2024b)build on these ideas by applying multiple strategies based on matching specific attention patterns. Motivated by similar considerations,Nawrot et al. (2024)proposed directly fine-tuning the original base transformer to compress the KV cache while minimizing a regularization loss to preserve the original base model’s behavior and limit performance degradation.Complementary to our method, MQA(Shazeer,2019)and GQA(Ainslie et al.,2023)propose merging attention heads during training to improve inference throughput. Similarly, also KV cache quantization is another orthogonal area where different hand-designed strategies have been proposed(Hooper et al.,2024; Dong et al.,2024a;b), with even recent work empirically showing their direct compatibility with token eviction methods(Liu et al.,2024a).
We note that, unlike this prior work, our approach uniquely learns a black-box model tomaximize performancethrough token-level memory management and shows potential for providing improvements to both the effectiveness and efficiency of transformers.We refer to App.Efor references and to the wider literature, including efficient architectures, memory, and evolution.

SECTION: 6Discussion and future work

This work introduced Neural Attention Memory Models, providing a new framework to enhance the performance of transformers while significantly reducing memory footprint.
By evolving NAMMs on top of pre-trained LMs, we demonstrated their effectiveness across diverse long-context tasks in three languages, significantly surpassing previous hand-designed KV cache eviction frequently hindering performance, and the original model relying on costly full-context conditioning.
Our carefully designed approach also enabled NAMMs, trained solely on language tasks, to achieve zero-shot transferability across architectures, input modalities, and task domains.While NAMMs do appear to provide benefits beyond what achieved with hand-designed strategies, we believe there is much room for improvement (e.g., see LimitationsF).This work has only begun to explore the design space of our memory models, which we anticipate might offer many new opportunities to advance future generations of transformers.
In this regard, we believe NAMMs should not be viewed as a replacement for gradient-based optimization, but rather an orthogonal framework that could be combined and alternated with parameter fine-tuning. Such an extension has the potential to unlock efficient long-context training, drawing parallels to the iterative process of learning and evolution that shaped human memory.

SECTION: 7Author contributions

Edoardo Cetin initiated the project, led the design and implementation of NAMMs, and provided major contributions to writing. Qi Sun designed and implemented the zero-shot transfer experiments with Llama 3 70B and Llava Next Video 7B, and provided contributions and feedback to writing. Tianyu Zhao devised and implemented ChouBun, and provided contributions and feedback to writing.
Yujin Tang coordinated the project, gave key advice for the design of NAMMs, and provided major contributions to writing.

SECTION: Acknowledgements

The authors would like to thank David Ha and Llion Jones for providing valuable discussions during the early stages and feedback while drafting the text. This paper is based on results obtained from a project, JPNP20017, subsidized by the New Energy and Industrial Technology Development Organization (NEDO).

SECTION: References

SECTION: Appendix AImplementation details

SECTION: A.1Model specifics and NAMMs execution

We evolve our Neural Attention Memory Models on top of a context-extended Llama 3 8B(Dubey et al.,2024)base model. In particular, we employ the NTK-aware positional interpolation strategy(bloc97,2023)to extend the context by four times from 8192 to 32768. Unlike prior strategies that require further gradient fine-tuning to avoid performance collapse(Chen et al.,2023), NTK-aware positional interpolation has been shown to produce sensible results even when applied zero-shot. In case the length of a task prompt still exceeds 32768 we perform mid-sentence cropping(Xiao et al.,2023; Jin et al.,2024), as standard in long-context LM evaluation(Bai et al.,2023; Zhang et al.,2024a).

When applying NAMMs, we only affect the execution of the base model with a fixed frequency, once everysteps. When feeding longer prompts to our model, we simply split the tokens into-sized chunks. We note that due to modern frameworks being bound primarily by memory constraints, input-splitting in itself has minimal effects on running time, with similar approaches being already performed under the hood by established kernel procedures(Dao et al.,2022).

SECTION: A.2Feature extraction and architecture details

Our new feature extraction framework is a key component for enabling the transfer properties of NAMMs. In practice, we extract the attention spectrogram from the real-valued attention matrix using a Hann window of size, resulting in just seventeen complex-values frequencies that we convert to real numbers by simply taking their magnitude, yielding each. We use a stride of half the window size, producingfrequency representations over the time axis of the attention matrix from the latest chunk ofqueries,. Thus, we reduce these frequency representations over the time axis via an element-wise exponentially moving average operation. We note that our EMA does not only consider therepresentations computed for the frequency of each token in the-sized chunk of the latest queries, but also the discounted EMA at theprevious execution stepor our memory for each retained token, denoted. Thus, each of our reduced spectrogram representations reflects the full history of previous attention values:

where we useto denote the EMA’s discount factor. To expedite learning the weights of our architecture, we ensure all spectrogram features have unit variance at initialization across our training data, using the statistics of the base Llama 3 model computed on the first task employed in incremental learning (PassageRetrieval). Finally, we also concatenate a small eight-dimensional sinusoidal positional embedding using theoldnessof each token, i.e., the amounts of new queries observed since its introduction in the KV cache.We provide an extended summarized pseudocode description of the execution pipeline in Algortihm2(to complement Algorithm1in the main text).

Our backward-attention memory network processes these representations by directly first applying the self-attention layer employing the counter-autoregressive backward masking introduced in Section3, designed to facilitate asymmetric interactions between tokens in memory. The output of self-attention is then fed to a single final linear layer to obtain the final score. We employed a few important additional design choices following some preliminary testing. First, motivated by efficiency considerations, we use a single head within our attention mechanism and no layer normalization. Second, our attention layer produces outputs that are twice the dimensionality of the spectrogram features. These outputs are integrated back into the main network before the final linear layer via both residual and multiplicative interactions. We provide a schematic depiction of our minimal architecture in Figure8. Through our minimalist design choices, our full network comprises only just over four thousand learnable parameters, a negligible amount, orders of magnitudes lower than even a single layer in modern transformers.

SECTION: A.3Zero-shot transfer

For our zero-shot transfer experiments, we consider a Llama 3 transformer with 70B parameters(Dubey et al.,2024), a Llava Next Video transformer with 7B parameters(Zhang et al.,2024b), and a decision transformer(Chen et al.,2021b)with about 1M parameters. For our 70B experiments, we follow the exact same setup as when evaluating our 7B Llama model used in training. For our video-language model, we extractimage tokens from 48 uniformly sampled frames, 6912 in total. We also slightly shift the selection score threshold by 5, to counteract the lower number of total tokens and get a comparable average cache size to the L2 and H2O baselines. We adapt the code and follow the standardized experimental setup fromLi et al. (2024a). For the reinforcement learning experiments, we encode each state, action, and return-to-go into separate tokens and do not apply any restrictions or modifications to our standard NAMM LM setup. We average the performance collected over 20 random seeds to account for the stochasticity of the initial state in the Gym Mujoco environments(Brockman et al.,2016).Rather than re-training a decision transformer from scratch, our RL experiments adapt the open-sourced checkpoints and implementation provided byBeeching & Simonini (2022). We would like that note that on some task-dataset combinations of D4RL, these checkpoints appear to yield lower performance than what was reported in the original decision transformer paper (e.g., Walker pre-trained on medium-expert data)Chen et al. (2021b). However, we do not believe these differences should affect our conclusions as we used the same base model for all our memory management baselines.

SECTION: A.4Evolutionary optimization

As described in Section3, we optimize NAMMs with the Covariance Matrix Adaptation Evolution Strategy (CMA-ES)(Hansen,2006). Being an evolutionary algorithm, CMA-ES does not require any gradient information and can directly optimize black-box undifferentiable metrics. This property allows us to both optimize for the non-differentiable token selection task of our NAMMs and also maximize non-differentiable task performance metrics directly. In the case of the LongBench(Bai et al.,2023)tasks considered for training, these metrics correspond to exact match accuracy (PassageRetrieval-en), ROUGE-L score (DuReader), and F1 score (NarrativeQA).

On a high level, given a neural network withparameters, CMA-ES maintains a mean vectorand a covariance matrix. Then, it repeats the following steps:

Sampling.CMA-ES generates a population of neural networks, sampling their parameters from the multivariate normaldistribution.

Evaluation.Each population candidate is evaluated to the objective function for the objective function used.

Updating.By both selecting a subset of the population candidates and also weighting them based on their overall ranking the mean and covariance are updated towards higher-performing regions of the search space.

We provide the main hyper-parameters in Table9and refer to either the work byHansen (2006)or our shared code for the full implementation details.

SECTION: A.5FastGen implementation and tuning

We re-implemented the recent FastGen method proposed byGe et al. (2024), which proposes to adopt a hand-designed combination of different strategies targeted to retain tokens with high attention values, belonging to recent words, or encoded from particular grammatical features (i.e., punctuation, ‘special tokens’). In particular, after observing the input prompt, FastGen performs a ‘profiling step’ where the strategy able to evict the most amount of tokens is selected such that:

Here,is the full-cache attention matrix,is the ‘reconstructed’ attention matrix re-calculated after performing a softmax between each layer’s queries and keys with masked-out entries for the keys evicted by the individual strategies. Furthermore, T is the main threshold hyper-parameter, determining how aggressively FastGen is allowed to prune tokens even if resulting in degradation to the attention-reconstruction heuristic.

We note that, unlike our other baselines, FastGen is only directly compatible with language modeling tasks. This is because one of the main ways it differs from H2O is by preserving particular grammar-based tokens in some of its strategies (e.g., punctuation, special words, etc.). Thus, as this baseline was specifically designed for LMs rather than arbitrary transformers, we did not consider applying it in the 0-shot transfer settings, and only focused on Llama 3 8B.

Wenote that as we are dealing with much longer prompts (sometimes far beyond tens/hundreds of thousand tokens), for efficiency consideration, we performed the profiling steps in our re-implementation after the first 4096 tokens any prompt exceeds this length. We also found to avoid losing too much performance over the base model on longer context tasks we had to retune its main ‘threshold.’ We selected T=0.999, as this choice allowed FastGen to retain over 95% normalized performance while still discarding a non-trivial portion of tokens on all LongBench, as shown in Figure9.
Other than the main threshold for attention reconstruction, FastGen has two other main hyperparameters: the ‘recency ratio,’ the ‘attention ratio’ determining the portion of most recent tokens or with the highest attention values to retain in its individual strategies. We set these hyper-parameters to 0.3, following the paper’s recommendation.

SECTION: Appendix BBenchmark descriptions

SECTION: B.1ChouBun details

The ChouBun benchmark is created to assess the generalization ability of NAMMs to a new language (Japanese), but we hope it will also serve as a standard benchmark for Japanese LLMs. The benchmark is composed of two task categories — extractive QA and abstractive summarization — and four tasks as follows.

JA.WikiQAis an extractive QA task about 20 randomly sampled articles from the 20240429 dump of Japanese Wikipedia444https://dumps.wikimedia.org/other/cirrussearch/. Each article corresponds to 10 QA pairs, and there are 200 QA pairs in total.

JA.EdinetQAis an extractive QA task based on 20 security reports from EDINET555https://disclosure2.edinet-fsa.go.jp/. The EDINET security reports are in CSV format, which makes them less human-readable. Nevertheless, we choose not to convert the format because the conversion process per se is non-trivial, and using a CSV-style text input helps us evaluate a model’s capability of understanding structured data. The total number of QA pairs inJA.EdinetQAis 390.

JA.CorpSecQAis another extractive QA task based on 30 security reports downloaded from three corporation websites (MUFG666https://www.mufg.jp/ir/report/security_report/, NTT777https://group.ntt/jp/ir/library/results/, and Toyota888https://global.toyota/jp/ir/library/securities-report/). We extract texts from original file in PDF format. There are 150 QA pairs in total.

JA.CorpSecSumis an abstractive summarization task based on the same data ofJA.CorpSecQA. Each document corresponds to one data point, and we collect 5 reference summaries for each data point.

Collecting human annotations for long-text tasks is challenging, therefore we use synthetic QA pairs and summaries. In particular, we prompt various LLMs999gpt-4o-2024-05-13,gpt-4o-mini-2024-07-18,gpt-4-turbo-2024-04-09, andclaude-3-5-sonnet-20240620to generate multiple question-answer pairs or summaries for each document. Different instructions are designed for the two tasks and they are shown in Figure10. To improve the reliability of the synthetic data, we ensure that every answer in extractive QA tasks is a text span presented in its corresponding source document. In Table10, we provide the statistics of the benchmark.

We use F1 score and ROUGE score for evaluation in the extractive QA tasks and summarization task, respectively. Reference text and hypothesis text are pre-tokenized by the MeCab tokenizer101010https://github.com/polm/fugashi. A wider range of LLMs’ performance on the ChouBun benchmark is presented in Table11.

SECTION: B.2Benchmarks summary

We provide a summary of the types of tasks and domains of the other benchmarks we considered for our experiments. We refer interested readers to the relative referenced papers for full details.

LongBench(Bai et al.,2023).This benchmark comprises 21 different tasks targeted to evaluate the long-context capabilities of LMs. These tasks include both English and Chinese and come from either modified/subsampled versions of existing datasets or synthetic generation. The authors divided them in 6 categories, numbered with the prefixes 1 to 6: single-document QA, multi-document QA, summarization, few-shot learning, synthetic, and code. The tasks have a reported average length of 6711 English words and 13386 Chinese characters.

InfiniteBench(Zhang et al.,2024a).This benchmark comprises 12 different tasks designed to go beyond the existing benchmarks and push the limits in long-context LMs. In fact, while popular prior long context benchmarks, including LongBench, focus on prompts of around 10K tokens InfiniteBench considers tasks with contexts beyond 100K tokens. These tasks again include both English and Chinese and come from either modified/subsampled versions of existing datasets or synthetic generation. The authors divided them into 5 categories: retrieval, dialogue, novel, math, and code. We note some of these tasks are considered extremely difficult, with even powerful proprietary LMs such as GPT4 not able to get above a performance of 1%.

LongVideoBench(Wu et al.,2024).This benchmark comprises 3763 curated long videos with subtitles. These videos are coupled with 6678 human-annotated questions focusing on 17 different categories. The benchmark is focused on what the authors refer to as frame-specific ‘reasoning’ style questions. In particular, for these kinds of questions, video language models are tasked to respond to ‘referred queries’ targeting particular parts of the whole video context.

Multi-task Long Video Understanding Benchmark(Zhou et al.,2024).This benchmark focuses on evaluating long-video understanding performance. It includes videos averaging 12 minutes in length up to 2 hours. The videos span different genres such as movies, documentaries, surveillance videos, ego-centric videos, games, and cartoons. In total, this benchmark comprises 2593 evaluation problems divided into 9 categories: topic reasoning, anomaly recognition, video summarization, needle question
answering, ego reasoning, plot question answering, sub-scene captioning, action
count, and action order. These problems are quite diverse including both multi-choice and generation-style questions for video language models.

D4RL(Fu et al.,2020).This benchmark focuses on evaluating offline reinforcement learning agents(Lange et al.,2012). In particular, it provides pre-training datasets for different reinforcement learning tasks simulated through Mujoco based on OpenAI gym(Brockman et al.,2016). The datasets are named based on the displayed agent skills (e.g., expert medium), and based on their inclusion of ‘replay data’ from the demonstrator agent’s own prior learning experiences. Evaluation is then performed after pre-training by running the learned agents online in the respective environments. We focus on the most popular subset of this benchmark, involving continuous-control tasks with three different agents: Hopper, HalfCheetah, and Walker-2d, evaluating the agent after pre-training on Expert, Medium, and Medium Replay data. Rather than re-training from scratch, we use the open-sourced checkpoints fromChen et al. (2021b)and focus on the evaluation aspect of the benchmark.

SECTION: Appendix CAdditional results

SECTION: C.1Performance across incremental stages and architectures

We provide additional results and analysis to the summarized one, complementing Section4, with the detailed performance across different NAMMs, evaluating the best checkpoints after each stage of incremental training stage, and ablating the BAM architecture with an MLP.

Extended language modeling results.We report our results for LongBench, InfiniteBench, and ChouBun in Tables12,13,14. First, we note that even training on a single task with our simple MLP architecture impressively improves performance across all benchmarks. Additionally, performance across benchmarks sees near-monotonic further improvements with each stage of our incremental evolution recipe. Comparing our implementations, we note that the performance benefits from the memory models with backward attention are consistently superior to the fully connected variant in both initial stages of incremental training, empirically validating our hypothesis about the importance of global KV cache information for determining the importance of each token. Lastly, on ChouBun. we observe that the performance with BAM sees a notable upswing after the second stage of incremental training, which might be associated with the introduction of another ideogram-based language in the training set.111111The DuReader task, used in the second stage of incremental training, uses the Chinese language.The same improvement not occurring with the MLP-based NAMMs might be further evidence of architectural performance saturation, highlighting once again the effectiveness of our main implementation design.

Extended zero-shot transfer results.We report our extended zero-shot transfer results for the 70B model and the offline RL setting in Tables15,16, and17. We see the benefits from NAMMs again increase as we incorporate backward attention, and with each stage of incremental training to a similar extent as with the language modeling tasks. These results further highlight the potential benefits of scaling up the architecture of our memory model and increasing the number of incremental stages. To this end, given the generality of our parameterization, an interesting unexplored approach could be to incorporate different base models and input modalities during evolutionary training, something that would substantially increase problem diversity to obtain an even more robust transfer behavior.

SECTION: C.2Training curves with fully-connected NAMMs

In Figure11, we provide training curves of our Neural Attention Memory Model using a simple MLP architecture rather than backward attention, evaluated in Section4. In the left sub-plot, we show the average and standard deviation of the normalized batch performance across the population, while in the right sub-plot, we show the normalized per-task and average performance on all samples of the optimized mean from CMA-ES. When compared with the BAM training curve from Figure4, we note a few interesting differences, although its evaluation performance on the full LongBench benchmark is lower across both incremental phases (see Table2), both its population batch performance and the CMA-ES full-task performance on the training sets are either comparable or slightly higher than BAM’s. This dichotomy appears to indicate that cross-token interactions might provide a better inductive bias, mitigating the overfitting potential of NAMMs.

SECTION: C.3Evolution of memory size during training

In Figure12, we provide training curves for the evolution of the memory size collected at the end of each task prompt of our NAMMs. On the left and right subplots, we provide results for the BAM and MLP implementations, respectively. For both architectures, we find that the memory size generally increases with training. This result suggests that NAMMs might learn to recognize additional valuable tokens as training progresses, enabling the corresponding performance improvements on the training tasks. Hence, they might indicate that there is some degree of a trade-off between the efficiency and performance of NAMMs. However, we note that both models are trained only for performance maximization, without any incentive to be more conservative. To this end, exploring regularization strategies to make NAMMs aware of deployment costs is an interesting direction for future work to obtain tailored sweet spots to cater to instance-specific resource constraints.

SECTION: C.4Incremental training ablation

We provide a full set of ablations results for our incremental training strategy, training a Neural Attention Memory Model with the BAM architecture from scratch on both the PassageRetrieval-en and DuReader tasks, as employed during the second stage of incremental learning. We evolve this Neural Attention Memory Model for 360 consecutive generations and provide training curves in Figure13. In the left sub-plot, we show the average and standard deviation of the normalized batch performance across the population, in the center sub-plot, we show the normalized per-task and average performance on all samples of the optimized mean from CMA-ES, and on the right subplot we show the corresponding memory size. Furthermore, in Table18, we provide the full LongBench evaluation results for this baseline, also showing our original incremental model’s performance for ease of comparison. Interestingly, the non-incremental NAMM obtained a notably higher score on the training tasks with a normalized performance of 1.57, in contrast to the normalized performance of 1.41 achieved by the best checkpoint from the second incremental training stage. Yet, outside the PassageRetrieval-en and DuReader tasks, its performance is notably inferior and very close to the original performance of the base model. These results appear to indicate that the usefulness of incremental training goes beyond the faster evolution provided by reducing the number of evaluation prompts to assess performance and that this strategy plays an important role in regularizing evolution and making Neural Attention Memory Models effectively generalize to new tasks.

SECTION: C.5Running times and memory savings

We provide details about the efficiency and costs of NAMMs on top of the Llama 3 8B base model used for training. For our main experimental setup, we used rented cloud instances with Nvidia H100 GPUs, Intel Xeon Platinum 8481C CPUs, and 1932GB of RAM. We performed model inference for each prompt on a single GPU, with batch size 1. During training, we used a single node with 8 GPUs, distributing the evaluation of our population across 8 processes. However, we like to remark that since training NAMMs does not require any gradient computation, we were not restricted by any kind of hardware during training. In this regard, using inference-specialized resources beyond GPUs might provide considerable speedups and lower costs to ones employed in this work.

Training.We collected the training time for each generation of NAMMs. As detailed in Section3and AppendixA, with the employed hyper-parameters, each generation consisted of running the base model forprompts for each task. Thus, each incremental phase got linearly more expensive, with up toNAMM evaluation in the final phase. These prompts were distributed across our 8 processes balancing the number of tokens evaluated in each. We also note that the average prompt length and the nature of each task (e.g., exact match, summarization, etc.) varied quite significantly in LongBench, making their evaluation costs non-uniform.

Inference.We collected running times of NAMMs and our baselines in different settings. In particular, these include both: 1. Using samples from the full LongBench benchmark with an average length of 12099 2. Using only samples from LongBench selected to exceed the base transformer maximum length with an average length of 32641. Finally, we also record the running time of an ablated version of our NAMM run on top of the base transformer that does not modify its KV cache, in order to disentangle the gains from the reduced memory and analyze the pure overheads from our model’s execution.

As shown in Table19, the running time overhead of our NAMM ablation that does not evict tokens is small when compared to the base model. Instead, the running time of NAMMs and the baselines while evicting tokens is always inferior to the base model in all settings, and scales positively with longer prompts.

Memory.Furthermore, in Table21, we also reported estimated effects in peak GPU memory consumption, which were calculated from the peak KV cache sizes, together with the sizes of additional information (e.g., attention matrix) and models used by each method (again recorded on LongBench). We would like to note, however, that as the main objective of our work was to provide performance benefits we did not particularly optimize our code for memory efficiency or speed. Thus, actual empirical savings with our shared implementation might differ from these calculated estimates. For instance, both our NAMMs and H2O baseline do not employ specialized kernels to replace FlashAttention(Dao et al.,2022).

SECTION: C.6Mistral base model and finetuning NAMMs

We also analyzed an additional 0-shot transfer setting, this time applying NAMMs on top of the Mistral 7B base model(Jiang et al.,2023). We considered 2 different setups: 1. Zero-shot application, taking our best NAMM model trained with the Llama 8B context-extended model. 2. Post cross-model fine-tuning, running a small amount of additional evolutionary optimization comprising 20 generations using CMA-ES and the same 3 training tasks used for Llama. We provide our full results and analysis in Table22.

Analogously to our other zero-shot transfer results provided in Section4, we find that NAMMs yield considerable benefits also when transferred to the Mistral model, overcoming the efficiency-performance tradeoff of hand-designed baselines. Furthermore, this analysis also shows that performance could be further improved by a few finetuning generations after transferring to a different base models. While we did not investigate finetuning with our other transformers (e.g., Llama 70B), we believe these results highlight the potential of cheaply improving NAMMs ’ already-remarkable zero-shot benefits, which we hope will be further explored in future work.

SECTION: C.7Attention spectrogram features ablation study

We examined ablating the attention spectrogram features produced by the STFT procedure and re-training our NAMMs with two different alternatives:

Thenaiveapproach of using the raw attention values directly (cropped to a fixed length) as input to NAMMs.

Substituting the STFT features by constructing a ‘handcrafted’ feature representation that simply includes three values: i. The sum of the attention values of each token. ii) The recency of each token. iii. The diversity of each token (computed by concatenating the keys and values to represent each token and averaging the L2 distance to all other tokens). We refer to this baseline asRAD.

We trained these baselines only for two incremental phases on the PassageRetrieval-en and Dureader tasks (thus, we also compared them with our original NAMM model after phase 2). Please refer to Table23for our results. Overall, we find our baselines yield quite different behaviors, both underperforming our original NAMM design.

First, we find our naive baseline, taking as input the cropped attention value, is not able to improve over the full cache model when evaluated on the whole of LongBench. However, we note that its performance on the training task is significantly beyond the base model. Thus, we find this is strongly suggestive of the occurrence of overfitting, which we believe is to be expected as our memory model now only conditions on very high-frequency information that only considers the latest attention values.

Second, we find that our ‘handcrafted’ Recency-Attention-Diversity baseline is instead able to improve over the original model, but its improvements are only marginal. We find these results consistent with section D.2 of the extended analysis, which suggests that the behavior of NAMMs is considerably influenced by a combination of different frequencies in the attention spectrogram which are lost by this approach.

SECTION: Appendix DAdditional analysis

SECTION: D.1Backward attention cross-token interactions

We analyze the cross-token interactions learned through our BAM architecture by recording the gradients of each token scorewith respect to all input featuresforall tokens in memoryafter storing 1024 tokens, i.e., for. We denote these quantities as:

We provide a qualitative visualization of our results on the PassageRetrieval-en task for a randomly selected layer and prompt in Figure14. On the left subplot, we provide a visualization of the squared magnitudesfor each combination of tokens (either scored or attended upon in BAM, i.e., indexed byor). Here, the effects of the backward mask are clearly visible, allowing tokens to exclusively attend to later ones, where. Predictably, these magnitudes mostly peak on the subplot’s diagonal, indicating the self-influence that each token’s features have on its corresponding output score. However, there are also notable exceptions, as shown in the center subplot, where we overlap three slices from our left surface plot corresponding to the gradients of the first, together with the highest and lowest-scored tokens in memory (respectively indexed by0, 292, and 800). We provide additional directional information of each gradient vector from these slices in the right subplot, where we take its dot product with the scored token’s own feature vector. After the first notable spike, at, most other dot-product spikes with the largest magnitudes consistently have negative values. Hence we can logically deduce that the scores of these tokens would benefit from pushing the representations of future tokens away from their own. This result appears to validate the hypothesis that BAM learns a mechanism for cross-token competition, incentivizing diversity and promoting tokens covering unique frequencies in the attention spectrogram.

SECTION: D.2Sensitivity to attention frequencies and positional encodings

We analyze the magnitudes of the gradients of the token scoreswith respect to each dimension in the token feature vectors. This procedure quantifies how varying each dimension in our attention spectrogram representation locally affects the output score of NAMMs, thus, providing a heuristic measure of its relevance (since scores determine which tokens get discarded). In Figure15, we plot the distribution of magnitudes for all the seventeen features up to the Nyquist frequency (to) in the attention spectrogram. All frequency distributions seem to cover a wide range of values, with each mean being close to the global mean, seemingly indicating NAMMs learn to make use of all available spectrogram information for at least some of the tokens. Additionally, we note that many of the higher frequencies have distributions with higher means and larger tails than the ‘ground frequency’ at dimension 0. Furthermore, as shown in the rightmost-lower subplot, NAMMs appear visibly less sensitive to recency information provided by the concatenated positional embeddings, with a lower total influence than frequency information on token scores. Overall, these observations seem to further validate the importance of going beyond simple hand-designed methods solely based on token recency and the sum of the attention values, which has so far been considered a strong established recipe for KV cache management(Oren et al.,2024; Zhang et al.,2024c; Ge et al.,2024; Devoto et al.,2024).

SECTION: D.3InfiniteBench results comparison

On the InfiniteBench tasks, our NAMM achieve particularly outstanding improvements over the base model and other baselines, with an over ten-fold score increase (from 1.05% to 11%). However, we note that even with NAMMs, the performance of Llama 3 8B still lags considerably behind the performance of powerful LMs designed specifically for long-context problems, as reported inZhang et al. (2024a). Nonetheless, on the En.Sum task, concerned with the summarization of fictitious novels, we find our main NAMM brings the performance of the context-extended Llama 3 from 7.73 to 14.91 even slightly beyond GPT4’s (14.73). While this performance is still low in absolute terms121212InfiniteBench tasks are scored in a range between 0 and 100., such a result appears quite notable and suggests that improvements from NAMMs are orthogonal in nature to the ones brought by architectural improvements and scaling, which, by themselves, might be insufficient to address the challenges brought by long and noisy contexts.

We qualitatively inspect the effects of NAMMs on En.Sum by comparing example answers generated by Llama 3 with and without our memory models, together with examples generated by GPT4. As illustrated in Figure16, we find both the Llama and GPT models to incur several failure modes, producing answers that entirely miss the objective of the original task. For instance, the context-extended Llama 3 often gets stuck in generation loops continuously repeating part of sentences without coherent structure. Instead, the GPT answers appear to forego summarizing the text and rather attempt to continue the provided passage, by generating end-of-text tokens or even roleplaying some of the characters. However, while introducing NAMMs appears to avoid many instances of these failure modes, we find the summarization of the memory-augmented Llama 3 still displays many imperfections such as misspelling character names (left) or lacking much depth by being extremely concise (right).

SECTION: Appendix EExtended related works

Similar to our NAMMs implementation, memory management through token eviction has been explored mostly to reduce memory constraints and enable querying LMs with longer contexts(Luohe et al.,2024).
Commonly, strategies entail simply cropping input prompts to a shorter length, often more effective when done from the middle rather than the ends(Xiao et al.,2023; Jin et al.,2024).
More advanced, several heuristic strategies have been proposed to identify and evict the least important tokens in the KV cache, selectively pruning it to a fixed size for each layer.
These strategies assess token relevance using metrics like L2 magnitude(Devoto et al.,2024)or entropy(Yao et al.,2024), or analyze statistics from the attention matrix, such as value magnitude or cumulative sums(Liu et al.,2024b; Oren et al.,2024; Zhang et al.,2024c).
Building on these ideas,Ge et al. (2024)andLi et al. (2024b)apply multiple strategies simultaneously, choosing the best fit for each layer by matching them with specific attention patterns. Similar ideas where also explored in older work targeting encoder-decoder models, for instance,Huang et al. (2022)proposed a more complex strategy for token selection based on solving thecore-setproblem with a parallelized greedy approach.
However, unlike previous work, our approach uniquely employs a black-box model tolearnKV cache management in order to boost the base model’s performance with improved efficiency coming as a free side benefit.

Many other methods to reduce memory consumption, affecting the KV cache, are mostly orthogonal and likely complementary to our approach.
For instance, MQA(Shazeer,2019)and GQA(Ainslie et al.,2023)propose merging different attention heads during the training of LLMs, either fully or partially, to improve deployment-time throughput.Brandon et al. (2024), pushed these strategies further, attempting to merge heads even across different layers.
GQA is commonly employed in many modern LMs, including the LLama 3 family of models which we use to train and evaluate NAMMs on language tasks(Dubey et al.,2024).
Furthermore, several methods have looked at KV cache compression through either quantization of the keys and values(Hooper et al.,2024; Dong et al.,2024a;b)or even the whole hidden states(DeepSeek-AI et al.,2024).
Similarly to the aforementioned prior work concerning KV cache pruning, these methods considered mainly hand-designed strategies, such as employing different quantization rates based on heuristically recognizing important tokens.
We note that using evolution to optimize for which channels to merge or compress could also yield new interesting unexplored approaches, combining these orthogonal directions with some of the principles introduced by NAMMs.

There has also been much research interest in exploring new architectures to explicitly model components of a memory system or to address key challenges of reasoning over longer contexts.
For instance, past work has looked at incorporating neural models of memory within neural networks by implementing different reading and writing operations - either directly replacing their layers(Weston et al.,2014; Sukhbaatar et al.,2015), or introducing new auxiliary components(Rae et al.,2016; Lample et al.,2019).
In relation to transformers, more recent works have been proposed rethinking the ingredients of the self-attention operation, mostly in the context of LMs.
These works looked at either efficient linear approximation to self-attention to overcome quadratic costs(Beltagy et al.,2020; Katharopoulos et al.,2020; Wang et al.,2020; Peng et al.,2021), or introducing new kinds of persistent tokens and storage to extend information propagation(Dai et al.,2019; Munkhdalai et al.,2024; Hwang et al.,2024).
However, as also noted byDao et al. (2022), none of these methods and approximations have managed to replace standard approaches so far.
We take a different approach that can be integrated in a zero-shot manner even without any fine-tuning.

Lastly, methodologically related to NAMMs, there have been other prior methods making use of evolution for or with transformer models.
For example,Tang & Ha (2021)also trained a small attention-based model through evolution, exploiting the inherent parameter efficiency behind these operations.
Furthermore,So et al. (2019)proposed using evolution to meta-optimize the basic building of transformers via neural architecture search, whileAkiba et al. (2024)focused on evolving different merging strategies across layers belonging to LMs with different capabilities.
As for these works, we note that evolution plays a critical role for NAMMs, allowing us to directly optimize for target performance and overcome the inherent non-differentiability underlying our new framework.

SECTION: Appendix FLimitations and future extensions

SECTION: F.1Exploring the design space of Neural Attention Memory Models

In this work, we introduced Neural Attention Memory Models and showed their efficacy and potential to improve the performance and efficiency of transformers, even when evaluated zero-shot for unseen architectures and domains. However, given the novelty of our framework, we note that our design choices were mostly motivated by simplicity and practicality rather than quantitative empirical evidence. Thus, there is an extremely large design space in terms of the implementation, training, and deployment of these models that should be explored beyond this work, which is likely to yield further improvements.

For instance, while our current feature extraction, based on computing the spectrogram of the attention matrix, enables capturing global frequency information about the attention values of each token, it might fall short of modeling local information with enough granularity. This hypothesized limitation inherently comes from a few design choices we made with the purpose of limiting the input size and corresponding parameter count of our memory models. In particular, our spectrogram features only consider the real components of a short-time Fourier transform with a small Hann window of size thirty-two. Thus, we only provide NAMMs information about a relatively limited number of thirty-two frequencies, losing any notion of the phase of the attention matrix that would be captured by the full complex-valued Fourier coefficients. Consequently, the representations of tokens with high attention values for entirely non-overlapping queries occurring with the same frequency would be indistinguishable to our models. Moreover, our exponentially moving average reduction over the time dimension of the spectrograms provides an additional layer of heavy compression inevitably trading off expressivity for simplicity.

To partially address these concerns, an alternative design we explored entailed delaying the initial element-wise exponentially moving average reduction. Concretely, this involved computingdifferent scores, feedingall feature vectorsfor, across the attention spectrogram’s compressed time axis, only then reducing the resulting scoresvia EMA. While, in principle, this alternative ordering would allow for additional expressivity without adding to the parameter count, in practice, when evaluated with an initial version of the simple 2-layer MLP model, we found no significant performance difference and opted for the former lighter option. However, introducing cross-token interactions with the improved BAM design and further scaling is likely to introduce a need of re-evaluating this choice.

One further limitation comes from the current reliance on the exact values of the attention matrix. This reliance precludes NAMMs training from making use of fast kernel algorithms developed to accelerate inference by foregoing materializing attention values(Dao et al.,2022). While the main focus of this work has been to introduce NAMMs and display its potential to improve transformers across different domains, more scalable parameterizations and efficient backend integrations remain exciting open challenges for future research.

SECTION: F.2Improving long-context sparse retrievals

One notable example exemplifying some of the aforementioned limitations, comes from the canonicalNeedle In A Haystacktask(Kamradt,2024), which has been used to qualitatively evaluate LLMs for their ability to remember sparse information over longnoisyhorizons. We provide results on this task using the best-performing NAMM after three stages of incremental training with the BAM architecture, averaging evaluation scores provided by a GPT-4 model(Achiam et al.,2023)across different prompt ranges, consistently withBai et al. (2024). As shown in Table24, while NAMMs do not manage to exceed the overall performance of the base model, they still provide some notable efficiency gains. However, looking more closely at the score distribution across different prompt length ranges we observe an unexpected trend that is in contrast with the rest of our results on other benchmarks. In particular, while our NAMM obtains slightly higher than the base model for prompts with a size less than 10000, it seems to increasingly struggle with longer prompts.

After comparing the spectrogram features extracted for the different prompts, our explanation for these results highlights one current failure mode of the current implementation. In particular, the Needle In a Haystack task is constructed such that the model is tasked to remember some important information introduced at the beginning of the prompt, and later followed by completely unrelated ‘filler’ text. Hence, the attention scores and the corresponding spectrogram features for the tokens containing the relevant information are forcibly sparse, being high only at the very beginning of the prompt. Yet, since the evaluated NAMM reduces these features over the time axis of the spectrogram with an EMA coefficient of, all the frequency information regarding these tokens will be inevitably overwritten. To empirically validate our theory we provide results simply raising the EMA coefficient fromto. Since our NAMMs was never actually trained with this higher coefficient, we note that this change effectively brings the input features out-of-distribution. Nonetheless, as shown in the final row of Table24, the larger coefficient still manages to improve performance on the longer prompts by enabling the preservation of the frequency components from the target ‘needle’ over a longer horizon. These findings suggest that future NAMM designs should consider higher EMA reduction coefficients or, potentially, even directlylearningthis parameter with evolution in addition to the NAMM’s network weights.