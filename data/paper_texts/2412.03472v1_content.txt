SECTION: Measure Anything: Real-time, Multi-stage Vision-based Dimensional Measurement using Segment Anything

We present Measure Anything, a comprehensive vision-based framework for dimensional measurement of objects with circular cross-sections, leveraging the Segment Anything Model (SAM). Our approach estimates key geometric features—including diameter, length, and volume—for rod-like geometries with varying curvature and general objects with constant skeleton slope. The framework integrates segmentation, mask processing, skeleton construction, and 2D-3D transformation, packaged in a user-friendly interface. We validate our framework by estimating the diameters of Canola stems – collected from agricultural fields in North Dakota – which are thin and non-uniform, posing challenges for existing methods. Measuring its diameters is critical, as it is a phenotypic traits that correlates with the health and yield of Canola crops. This application also exemplifies the potential of Measure Anything, where integrating intelligent models – such as keypoint detection – extends its scalability to fully automate the measurement process for high-throughput applications. Furthermore, we showcase its versatility in robotic grasping, leveraging extracted geometric features to identify optimal grasp points. Our dataset and code repository are available athttps://github.com/StructuresComp/measure-anything

SECTION: IINTRODUCTION

Advances in robotic perception empower machines to better understand and interact with their environments, enabling them to perform complex, labor-intensive tasks with greater consistency, efficiency, and scalability. Among these tasks, vision-based dimensional measurement plays a critical role across diverse domains: in precision agriculture, accurate measurement of plant dimensions optimizes resource allocation and enhances crop yields; in manufacturing, it ensures product quality through precise dimensional verification, reducing waste and defects; and in robotic manipulation, it enables robust object interactions in tasks such as assembly, packaging, and material handling. Despite its widespread significance, vision-based dimensional measurement faces key challenges, including identifying target objects in cluttered environments filled with complex geometries and occlusions, and implementing systematic methods to extract accurate measurements from object contours.

Measure Anything addresses the first challenge by integrating the Segment Anything Model (SAM)[1,2], a state-of-the-art segmentation foundation model with exceptional zero-shot generalization capabilities. Measure Anything inherits the promptable features of SAM and explores two approaches to obtaining point prompts that guide object selection. The first is an interactive method, where users place positive or negative point prompts directly on the object of interest. The second is an automated method, where a keypoint detection model replaces manual input, enabling high scalability and efficient operation in large-scale scenarios. Beyond segmentation, Measure Anything employs a systematic pipeline to extract dimensional measurements from object masks. This pipeline includes sequential tasks such as mask processing, skeleton construction, line segment identification, and 2D-3D transformation. Tailored skeleton construction methods are applied based on the object’s geometry, enabling the accurate computation of diameter, length, and volume in a single pass. An overview of Measure Anything framework can be seen in Figure1. By combining cutting-edge segmentation capabilities with a robust geometric processing, Measure Anything offers a comprehensive solution to the challenges of vision-based dimensional measurement.

This paper focuses on two primary applications: precision agriculture and robotic grasping. For precision agriculture, we tackle the task of estimating stem diameters, a key phenotypic trait that provides valuable insights into crop health, environmental responses, genetic factors, and overall yield potential[3]. For robotic grasping, we show that the dimensional measurements obtained from Measure Anything can be applied to general manipulation tasks such as identifying optimal grip points. Both applications involve target objects placed in highly cluttered environments, presenting significant challenges for accurate perception. Beyond these primary use cases, the framework’s modular design allows for extensions to other applications, such as quality inspection in manufacturing or crack-width measurement in structural assessments, with minimal modifications.

In summary, the key contributions of this work are as follows:

Development of a robust multi-stage pipeline integrating SAM, and downstream operations for accurately measuring the diameter, length, and volume of target objects with circular cross-sections.

Implementation of the pipeline through a user-friendly demo that can be used on
various objects with minimal modifications

Validation of the pipeline’s effectiveness on measuring Canola stem diameters, as well diameters and demonstration of its scalability when combined with a keypoint detection model for automation.

Application to general manipulation tasks, where the framework’s extracted geometric features are utilized in a stability model to determine optimal grasping coordinates.

Our paper is organized as follows. SectionIIcontains a literature review of various topics related to our framework. SectionIIIprovides a detailed explanation of each stage of our pipeline, and the results are presented in SectionIV.

SECTION: IIRELATED WORKS

Various sensors all used for dimension measurement tasks across various applications, with their popularity depending on factors such as precision, scalability, and repeatability. Commonly used sensors include LiDAR[4], cameras[5], 3D scanners[6], and photogrammetry setups[7]. Each of these sensors offers distinct advantages: LiDAR excels at high-precision distance measurements and performs reliably in diverse environmental conditions; 3D scanners capture detailed surface geometries with high accuracy; and photogrammetry frameworks reconstruct three-dimensional structures from two-dimensional images, enabling flexible and scalable setups. Among these, vision-based techniques using stereo or depth cameras have emerged as particularly popular due to their combination of accuracy, ease of deployment, and scalability. These methods are particularly advantageous for measurements in cluttered or remote environments, where other sensors may face limitations. This research focuses on applications where vision-based measurement techniques offer the most feasible and effective solutions.

In precision agriculture, vision-based methods are extensively used to estimate fruit sizes, which directly influence harvesting decisions[8,9]. Other physical traits such as stem diameter, leaf length, plant height, branch angle, are critical for crop phenotyping, crop monitoring and yield forecasting tasks. For example, Vitet al.[10]proposed a length-based phenotyping technique applicable to various plants, such as banana leaves and cucumbers. Their method involves three stages: object detection, point of interest identification, and 3D measurement. Similarly, a high-throughput stereo vision system called “PhenoStereo”[11]was developed for in-field stem diameter measurement of sorghum plants. By leveraging Mask R-CNN-based instance segmentation[12]and semi-global block matching (SGBM)[13]for 3D point cloud reconstruction, PhenoStereo achieved high accuracy and strong correlation with ground truth measurements. Similarly, in livestock farming, vision-based measurement techniques automate the assessment of livestock body dimensions[14], playing a vital role in breeding management and the selection of superior livestock breeds[15].

In robotic manipulation, grasping objects of varying shapes and sizes is a well-researched area[16,17,18]. Vision-based frameworks play a critical role in estimating grasp points and performing pose estimation of grippers[19]. These operations rely on accurate measurement of object dimensions, assessment of their shapes, and identification of optimal contact points for stable grasping[20].
Fanget al.[21]devised Graspnet, an end-to-end framework that directly generates abundant grasp poses for an input scene point cloud. Wanget al.[22]proposed “graspness”, a geometry-based metric that distinguishes graspable areas in cluttered scenes, significantly improving grasp success rates. Building on this concept, Fanget al.[23]incorporated center-of-gravity awareness into grasp perception, enabling fast and accurate continuous grasp pose detection with a parallel gripper. This advancement further enhanced the stability and reliability of robotic grasping in dynamic and cluttered environments.

Image segmentation is a fundamental task in computer vision, aiming to classify each pixel of an image into a semantic label[24]. In many of the aforementioned applications, semantic segmentation serves as the initial step to isolate regions of interest. Over the past decade, Convolutional Neural Network (CNN)-based models such as Fully Convolution Networks (FCNs)[25], encoder-decoder based models[26,27], and U-Nets[28,29]have been widely adopted. More recently, transformer-based models, such as Vision Transformers (ViT)[30]and Detection Transformers (DETR)[31,32]have gained prominence. Due to their superior global context modeling, these models have achieved state-of-the-art performance in numerous segmentation tasks. The latest breakthrough comes from promptable foundation models such as Segment Anything Models (SAM 1[1]and SAM 2[2]). These models feature a hybrid architecture that integrates CNN-based encoders with transformer-based prompt encoders and mask decoders. SAM’s remarkable zero-shot generalizability enables effective segmentation across diverse tasks without requiring additional data, positioning these models as game changers in the field.

SECTION: IIIMETHODOLOGY

SECTION: III-APromptable Image Segmentation with SAM 2

The pipeline begins with the identification of the object of interest through a binary mask. To achieve this, we leverage SAM 2 for its exceptional zero-shot generalization capabilities, which allow accurate segmentation without the need a tailored segmentation model trained on custom data. The segmentation process is guided by point prompts, which can be either positive or negative. Positive prompts guide the model to identify masks including itself, while negative prompts resolve ambiguities by excluding unwanted areas, particularly in cases where hierarchical overlap exists.

In our work, we showcase two distinct approaches to obtain the point prompts. The first method relies on manual input, where users place positive and negative points on an interactive image window. The second approach automates the process by utilizing a keypoint detection model, which identifies precise points to be used as positive point prompts to SAM 2. This is demonstrated on the task of stem diameter estimation, where a keypoint detection model was trained to place two points on each of the foreground stems. Details on the automated approach are provided in SectionIII-F.

SECTION: III-BBinary Mask Processing

Currently, precise control over the output masks generated by Segment Anything is limited, resulting in masks with imperfections that complicate the downstream task of skeleton construction (see Figure1). Typical issues include disconnected patches, holes within the binary mask, and rough contours, which lead to the generation of unnecessary branches during skeletonization. To address these issues, a minimal mask processing step is applied to refine the mask, enabling a smoother skeletonization process while preserving the shape of the original binary mask.

The binary mask processing pipeline begins by removing small, isolated objects from the initial mask that are smaller than a minimum size threshold. Next, connected component analysis is then performed to identify and retain the largest connected region, assumed to be the primary object of interest, while discarding smaller, irrelevant components.

Following connected component analysis, the mask undergoes a morphological opening operation followed by morphological closing. Both operations involve the fundamental steps of erosion and dilation, wherein a kernel of ones slides over the binary image. During erosion, a pixel is set to one only if all pixels within the kernel are one; otherwise, it is set to zero. Dilation operates in reverse, where a pixel is set to one if at least one pixel in the kernel is one. The opening operation, comprising erosion followed by dilation, has a smoothing effect on the object boundary by removing small protrusions. In contrast, the closing operation, which consists of dilation followed by erosion, enhances structural integrity by filling small internal gaps within the object.

Lastly, contour detection is then performed to delineate the outer boundary of the remaining, largest component. This boundary is approximated to a polygonal shape using the Ramer–Douglas–Peucker algorithm. The resulting polygonal representation is the filled, producing a clean, refined binary mask, that facilitates subsequent steps.

SECTION: III-CSkeleton Construction

In general context, skeletons are single-pixel-wide representations of a binary object that capture its essential topology[33]. In our work, we aim to generate an ordered array of skeleton coordinates, where the first and last indices point to the bottommost and topmost skeleton pixel respectively, and adjacent indices represent connected skeleton pixels. This ordered structure encodes local slope information, which is necessary for identifying perpendicular line segments at any given skeleton pixel and enables accurate dimensional measurements. The process of deriving these ordered skeleton coordinates is referred to as skeleton construction.

Two approaches are proposed for skeleton construction based on the object’s geometry. For rod-like geometries, the skeleton is obtained using standard skeletonization techniques, followed by pruning, augmentation, and reordering to ensure a structured and ordered representation. For regular objects with general geometries, Principal Component Analysis (PCA) is used to identify the object’s closest axis of symmetry, which guides the skeleton construction process. The distinction between these approaches arises from the inherent differences in geometry. Often times, the local slopes of general common shapes do not vary, and thus the slope can be captured with a single straight line. Also, general geometries result in diverse branching patterns, making it challenging to formulate a universal pruning method across all geometries.

For rod-like geometries, once the initial skeleton is obtained using a medial axis transform (MAT), the first step is to identify endpoints and intersection points. This is accomplished using the method described in[34]. First, we compute the connectivity,, for each skeleton pixel,, by convolving a one-pixel border-padded version of the skeleton with the following kernel:

A pixelis classified as an endpoint if, and it is classified as an intersection point if, which defines the sets:

whereandrepresent the set of endpoints and intersections respectively. However, defining the intersections based on connectivity alone cannot handle cases where adjacent intersection points are redundantly identified. To address this, clustering is performed on all identified intersections, ensuring that only one intersection pixel is preserved per cluster.

Following the identification of endpoints and intersection points, short branches—artifacts arising from rough contours that do not contribute to the topology—are detected and removed. This is achieved by iterating from each endpoint and determining its distance to the nearest intersection point or endpoint by propagating through neighboring skeleton pixels. A threshold value,, set as the maximum diameter obtained from MAT, is used to classify branches. Branches with lengths shorter thanis deemed a “short” branch and is pruned. Conversely, branches exceeding this threshold are considered “valid” and are retained.

Following the pruning step, the endpoints are re-evaluated on the pruned skeleton. A path is identified between endpoint candidates such that the vertical separation between them is maximized. This task is formulated as a minimum cost path problem, where the cost array is initialized such that skeleton pixels are assigned finite values while non-skeleton pixels are assigned infinity. By solving the minimum cost path problem with specified start and end points, the skeleton pixels along the optimal path can be identified.

Finally, the skeleton is augmented by extending additional segments from each endpoint. Extensions follow the local slope at each endpoint and are constrained to remain within the bounds of the binary mask. This augmentation ensures the presence of perpendicular line segments throughout the object’s entire length. The augmented skeleton is reordered such that the first index corresponds to the bottommost skeleton pixel and the last index corresponds to the topmost pixel.

For general geometries, the skeleton is constructed using the symmetry axis identified through PCA. It produces two orthogonal principal axes: one corresponding to the direction of greatest variance and the other, its orthogonal axis. These axes are candidate skeleton axes, and the skeleton axis that is closer to the object’s axis of symmetry is selected by comparing the dissimilarity between the original mask and its reflected masks derived from each candidate axis. Each axis bisects the original mask,, into two parts. These parts are combined with their respective reflections about the candidate skeleton axis, and the dissimilarity scores are calculated. The dissimilarity score reflects the total number of different pixels of two binary masks, and the axis with the lower score is chosen as the skeleton axis:

wheredenotes the reflected masks for each principal axis. For objects with multiple axes of symmetry, where the ratio of the minimum to the maximum dissimilarity scores is greater than 0.9, the first principal axis is automatically selected to ensure that perpendicular line segments are drawn along the object’s longest dimension.

Once the skeleton axis is identified, skeleton construction begins from the object’s centroid, propagating in both positive and negative directions along the axis. The skeleton is augmented iteratively while remaining within the bounds of the binary mask. Finally, the skeleton pixels are reordered from the bottommost to the topmost, which results in an augmented skeleton just like the rod-like geometry case.

SECTION: III-DSimultaneous Line Segment and Depth Identification

All three-dimensional measurements provided by our framework rely on identifying line segments perpendicular to the object’s skeleton, with their endpoints lying on the object’s contour. Then, these endpoints, or their midpoints, are used as pixels of interest for transforming into 3D points. This section presents a method for simultaneously identifying these endpoints and retrieving their depth values.

Given an ordered list of skeleton pixels, whereandare the bottommost and topmost skeleton pixels,denotes the subset of sampled skeleton pixels at which perpendicular line segments are obtained. The first parameteris the sampling stride that determines the interval between sampled skeleton pixels. For each, its line segment is represented as the x, y and depth of each of its endpoints:.

To obtain this representation, the first step involves calculating the local slope at each selected skeleton pixel using the central difference method. For each skeleton pixel, its local slopeis computed as:

wherespecifies the offset in pixels on either side of. For skeleton pixels near the endpoints, where such offsets may not exist, the calculation is adjusted to avoid out-of-bounds errors.

For estimating the line segment coordinates, we search along the perpendicular direction to the slope. Starting from the skeleton pixel, we search outwards until the mask boundary is reached. Then the median depth is selected for each endpoint of the line segment. Algorithm1summarizes the steps for simultaneous line segment and depth identification. This iterative depth retrieval is necessary because the endpoints often lie on the object’s contour, where depth data is frequently missing.

SECTION: III-E2D-3D Transform and Measurement

The 2D coordinates are projected into the 3D space by the traditional 2D-3D view transform using the intrinsics and distortion coefficients for the RGB and depth image.

The diameter obtained from theth line segment,is the Euclidean distance between the endpoints in 3D of each valid line segment, given as, whereandare the 3D coordinates of the endpointsand, respectively. The length of the object,, is computed as the sum of the Euclidean distances between the 3D midpoints of consecutive line segments, calculated using the average depth of its endpoints:

Hereis the total number of line segments. Lastly, the volume of the object,, is estimated by accumulating the volumes of partial cones formed by two consecutive line segments:

SECTION: III-FKeypoint Detection for Automating Prompts

To enable fully automated point prompt generation for Measure Anything, we integrated a keypoint detection model. This approach was applied to large-scale diameter measurements on Canola stems, leveraging the efficiency of automated point generation provided by the keypoint detection model. For keypoint detection, we utilized the state-of-the-art YOLOv8 model. After testing various configurations with different numbers of keypoints per stem (1, 2, 3), our findings show that two keypoints per stem are sufficient in most cases for accurate segmentation and result in higher mean Average Precision (mAP) during training. The model was trained to focus exclusively on stems in the front rows of the image by annotating only these stems in the training dataset. A total of 320 carefully annotated images were used for training and testing, requiring approximately 50 hours of annotation effort, with each image taking roughly 8 minutes to annotate.

For crops with simpler stem structures, such as corn and wheat, or other applications involving regular-shaped objects, the automation task can be efficiently handled using bounding box object detection[35]. Since Measure Anything is built on the Segment Anything framework, it supports both bounding box and point-based prompts, offering versatility and scalability for various downstream applications.

SECTION: III-GGeometric Module for Robotic Grasping

This study demonstrates the application of Measure Anything as a geometric module for robotic grasping tasks. The framework is evaluated using the Clubs dataset[36], which features cluttered images of various common objects inside a box, posing significant challenges for perception and manipulation tasks.
The primary roles of Measure Anything in this context are: (a) isolating an object from a cluttered environment and (b) extracting its geometric features. Segmenting and isolating objects in cluttered scenes is particularly critical when complete 3D point clouds are unavailable, as well as for sim-to-real transfer in robotic grasping models[37]. Extracting dimensions such as diameter profiles along the major principal axis, length, and volume provides valuable geometric priors for stability models used to identify optimal grasping points. Although missing depth information near object edges can pose challenges, Measure Anything can effectively obtain diameter measurements through the method described in Algorithm1.

Within this work, the extracted geometric features are used in a simplified stability model to identify the topkoptimal grasping pairs of coordinates for a parallel gripper. The stability model incorporates three weighted factors: (i) the perpendicular distance from the center of gravity (CoG)[23], (ii) the diameter between the grasp coordinates, and (iii) Bicchi’s criterion[38]for form closure. The equation of the model can be written as:

whererepresents the stability score ofth pair of coordinates,is the normalized diameter of the grip,is the normalized perpendicular distance from the grasping line-segment to the CoG, and “cond” is the condition if the pair of coordinates lie at a minima of a concave surface. Although the stability model covers both force closure and form closure components, we anticipate that state-of-the-art deep learning-based stability models[23], augmented with geometric priors from Measure Anything, would perform better across a broader spectrum of objects.

SECTION: IVRESULTS

Figure3illustrates the two demo versions discussed earlier in SectionsIII-AandIII-F. After selecting a frame from a video sequence captured by a stereo depth camera, Figure3-(a) demonstrates the interactive method, where users can place any number of positive or negative point prompts via an interactive interface. Figure3-(b) showcases the automated approach, which uses results from the keypoint detection model to generate point prompts. The keypoint detection model (YOLO v8), trained on a dataset of 256 images, achieved an average mAP-95 score of 83.5%. A key advantage of the automated approach is the selective intervention feature, allowing users to correct keypoint labels when segmentation results are inaccurate. This correction is reinforced by adding additional positive or negative prompts.

Figure4explores the variability of length and volume measurements obtained using the proposed framework across different camera positions. The analyzed video consists of approximately 90 frames of a wine bottle captured side-on, with variations in roll angles and distances from the object. Length and volume measurements across all frames are plotted and compared to the ground truth values, manually measured as 29.845 cm and 943 ml, respectively. Results show that most measurements remain consistent, varying within10 % of the ground truth values. It is noted that the quality of depth data is critical for accurate measurements, with outliers primarily arising from slightly inaccurate depth maps caused by blurry images resulting from camera motion.

Robotic grasp selection is demonstrated on an image from the Clubs dataset (see Figure5). The Measure Anything framework isolates the object of interest from a cluttered environment, processes the binary mask, and performs skeleton construction as well as line segment and depth identification, as detailed in previous sections. Stability scores for each line segment are then computed using Equation1, and the top seven line segments with the highest scores are visualized in the figure. Notably, the line segment at the concave surface achieves the highest stability score, followed by the segments closest to the center of gravity (CoG), aligning well with intuitive expectations.

SECTION: VCONCLUSION

This work introduced a vision-based dimensional measurement framework that leverages the Segment Anything model to accurately generate object masks. The proposed pipeline demonstrated the ability to obtain precise diameter, length, and volume measurements across a variety of objects, including those with rod-like and general geometries. By utilizing continuous diameter measurements at different locations along the object, the framework was further shown to be applicable to additional tasks, such as identifying ideal gripping points for robotic manipulation.

Future work will focus on addressing the limitations of the current framework. For instance, segmentation models are susceptible to inaccuracies caused by occlusions from overlapping objects, which can result in erroneous dimensional measurements. We aim to enhance the robustness of our approach by incorporating shape priors to handle such occlusions. Additionally, the current pipeline assumes objects with circular cross-sections; future iterations will integrate intelligent models capable of identifying object types, paving the way for strategies tailored to objects with non-circular cross-sections. We also plan to integrate Measure Anything with a deep learning grasping model to compare its performance against the state-of-the-art. Finally, we plan to explore the integration of language models to enable diverse tasks, such as obtaining dimensional measurements of specific objects within complex scenes.

SECTION: ACKNOWLEDGMENT

The authors would like to thank Mohammad Jony, Md Fahad Hassan, Cristhian Perdigon, Dr. Md Mukhlesur Rahman, Dr. Paulo Flores from North Dakota State University for their efforts with data collection from the greenhouse and fields of North Dakota.

SECTION: References