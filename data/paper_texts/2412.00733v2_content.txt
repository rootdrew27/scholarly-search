SECTION: Hallo3: Highly Dynamic and Realistic Portrait Image Animation with Diffusion Transformer Networks
Existing methodologies for animating portrait images face significant challenges, particularly in handling non-frontal perspectives, rendering dynamic objects around the portrait, and generating immersive, realistic backgrounds.
In this paper, we introduce the first application of a pretrained transformer-based video generative model that demonstrates strong generalization capabilities and generates highly dynamic, realistic videos for portrait animation, effectively addressing these challenges.
The adoption of a new video backbone model makes previous U-Net-based methods for identity maintenance, audio conditioning, and video extrapolation inapplicable. To address this limitation, we design an identity reference network consisting of a causal 3D VAE combined with a stacked series of transformer layers, ensuring consistent facial identity across video sequences.
Additionally, we investigate various speech audio conditioning and motion frame mechanisms to enable the generation of continuous video driven by speech audio. Our method is validated through experiments on benchmark and newly proposed wild datasets, demonstrating substantial improvements over prior methods in generating realistic portraits characterized by diverse orientations within dynamic and immersive scenes. Further visualizations and the source code are available at:.

SECTION: Introduction
Portrait image animation refers to the process of generating realistic facial expressions, lip movements, and head poses based on portrait images.
This technique leverages various motion signals, including audio, textual prompts, facial keypoints, and dense motion flow.
As a cross-disciplinary research task within the realms of computer vision and computer graphics, this area has garnered increasing attention from both academic and industrial communities.
Furthermore, portrait image animation has critical applications across several sectors, including film and animation production, game development, social media content creation, and online education and training.

In recent years, the field of portrait image animation has witnessed rapid advancements.
Early methodologies predominantly employed facial landmarks—key pointson the face utilized for the localization and representation of critical regions such as the mouth, eyes, eyebrows, nose, and jawline.
Additionally, these methodsincorporated 3D parametric models, notably the 3D Morphable Model (3DMM), which captures variability in human faces through a statistical shape model integrated with a texture model.
However, the application of explicit approaches grounded in intermediate facial representations is constrained by the accuracy of expression and head pose reconstruction, as well as the richness and precision of the resultant expressions.
Simultaneously, significant advancements in Generative Adversarial Networks (GANs) and diffusion models have notably benefited portrait image animation.
These advancementsenhance the high-resolution and high-quality generation of realistic facial details, facilitate generalized character animation, and enable long-term identity preservation.
Recent contributions to the field—including Live Portrait, which leverages GAN technology for portrait animation with stitching and retargeting control, as well as various end-to-end methods such as VASA-1, EMO, and Halloemploying diffusion models—exemplify these advancements.

Despite these improvements, existing methodologies encounter substantial limitations.
First, many current facial animation techniques emphasize eye gaze, lip synchronization, and head posture while often depending on reference portrait images that present a frontal, centered view of the subject.
This reliance presents challenges in handling profile, overhead, or low-angle perspectives for portrait animation. Secondly, accounting for significant accessories, such as holding a smartphone, microphone, or wearing closely fitted objects, presents challenges in generating realistic motion for the associated objects within video sequences.
Third, existing methods often assume static backgrounds, undermining their ability to generate authentic video effects in dynamic scenarios, such as those with campfires in the foreground or crowded street scenes in the background.

Recent advancements in diffusion transformer (DiT)-based video generation modelshave addressed several challenges associated with traditional video generation techniques, including issues of realism, dynamic movement, and subject generalization.
In this paper, we present the first application of a pretrained DiT-based video generative model to the task of portrait image animation.
The introduction of this new video backbone model renders previous U-Net-based methods for identity maintenance, audio conditioning, and video extrapolation impractical.
We tackle these issues from three distinct perspectives.
(1): We employ a 3D VAE in conjunction with a stack of transformer layers as an identity reference network, enabling the embedding and injection of identity information into the denoising latent codes for self-attention. This facilitates accurate representation and long-term preservation of the facial subject’s identity.
(2): We achieve high alignment between speech audio—serving as motion control information—and facial expression dynamics during training, which allows for precise control during inference. We investigate the use of adaptive layer normalization and cross-attention strategies, effectively integrating audio embeddings through the latter.
(3): Addressing the limitations of the DiT-based model in generating continuous videos, which is constrained to a maximum of several tens of frames, we propose a strategy for long-duration video extrapolation.
This approach uses motion frames as conditional information, wherein the final frames of each generated video serve as inputs for subsequent clip generation.

We validate our approach using benchmark datasets, including HTDF and Celeb-V, demonstrating results comparable to previous methods that are constrained to limited datasets characterized by frontal, centered faces, static backgrounds, and defined expressions.
Furthermore, our method successfully generates dynamic foregrounds and backgrounds, accommodating complex poses, such as profile views or interactions involving devices like smartphones and microphones, yielding realistic and smoothly animated motion, thereby addressing challenges that previous methodologies have struggled to resolve effectively.

SECTION: Related Work
Recent advancements in the domain of portrait image animation have been significantly propelled by innovations in audio-driven techniques.
Notable frameworks, such as LipSyncExpertand SadTalker, have tackled challenges related to facial synchronization and expression modulation, achieving dynamic lip movements and coherent head motions.
Concurrently, DiffTalkand VividTalkhave integrated latent diffusion models, enhancing output quality while generalizing across diverse identities without the necessity for extensive fine-tuning.
Furthermore, studies such as DreamTalkand EMOunderscore the importance of emotional expressiveness by showcasing the integration of audio cues with facial dynamics.
AniPortraitand VASA-1propose methodologies that facilitate the generation of high-fidelity animations, emphasizing temporal consistency along with effective exploitation of static images and audio clips.
In addition, recent innovations like LivePortraitand Loopyfocus on enhancing computational efficiency while ensuring realism and fluid motion.
Furthermore, the works of Halloand Hallo2have made significant progress in extending capabilities to facilitate long-duration video synthesis and integrating adjustable semantic inputs, thereby marking a step towards richer and more controllable content generation.
Nevertheless, existing facial animation techniques still encounter limitations in addressing extreme facial poses, accommodating background motion in dynamic environments, and incorporating camera movements dictated by textual prompts.

Unet-based diffusion model has made notable strides, exemplified by frameworks such as Make-A-Video and MagicVideo.
Specifically, Make-A-Videocapitalizes on pre-existing Text-to-Image (T2I) models to enhance training efficiency without necessitating paired text-video data, thereby achieving state-of-the-art results across a variety of qualitative and quantitative metrics.
Simultaneously, MagicVideoemploys an innovative 3D U-Net architecture to operate within a low-dimensional latent space, achieving efficient video synthesis while significantly reducing computational requirements.
Building upon these foundational principles, AnimateDiffintroduces a motion module that integrates seamlessly with personalized T2I models, allowing for the generation of temporally coherent animations without the need for model-specific adjustments.
Additionally, VideoComposerenhances the controllability of video synthesis by incorporating spatial, temporal, and textual conditions, which facilitates improved inter-frame consistency.
The development of diffusion models continues with the advent of DiT-based approaches such as CogVideoXand Movie Gen.
CogVideoX employs a 3D Variational Autoencoder to improve video fidelity and narrative coherence, whereas Movie Gen establishes a robust foundation for high-quality video generation complemented by advanced editing capabilities.
In the present study, we adopt the DiT diffusion formulation to optimize the generalization capabilities of the generated video.

SECTION: Methodology
This methodology section systematically outlines the approaches employed in our study.
Sectiondescribes the baseline transformer diffusion network, detailing its architecture and functionality.
Sectionfocuses on the integration of speech audio conditions via a cross-attention mechanism.
Sectiondiscusses the implementation of the identity reference network, which is crucial for preserving facial identity coherence throughout extended video sequences.
Sectionreviews the training and inference procedures used for the transformer diffusion network.
Finally, Sectiondetails the comprehensive strategies for data sourcing and preprocessing.

SECTION: Baseline Transformer Diffusion Network
The CogVideoX modelserves as the foundational architecture for our transformer diffusion network, employing a 3D VAE for the compression of video data.
In this framework, latent variables are concatenated and reshaped into a sequential format, denoted as.
Concurrently, the model utilizes the T5 architectureto encode textual inputs into embeddings, represented as.
The combined sequences of video latent representationsand textual embeddingsare subsequently processed through an expert transformer network.
To address discrepancies in feature space between text and video, we implement expert adaptive layer normalization techniques, which facilitate the effective utilization of temporal information and ensure robust alignment between visual and semantic data.
Following this integration, a repair mechanism is applied to restore the original latent variable, after which the output is decoded through the 3D causal VAE decoder to reconstruct the video.
Furthermore, the incorporation of 3D Rotational Positional Encoding (3D RoPE)enhances the model’s capacity to capture inter-frame relationships across the temporal dimension, thereby establishing long-range dependencies within the video framework.

In addition to the textual prompt, we introduce two supplementary conditions: the speech audio conditionand the identity appearance condition.

Within diffusion transformers, four primary conditioning mechanisms are identified: in-context conditioning, cross-attention, adaptive layer normalization (adaLN), and adaLN-zero.
Our investigation primarily focuses on cross-attention and adaptive layer normalization (adaLN). Cross-attention enhances the model’s focus on conditional information by treating condition embeddings as keys and values, while latent representations serve as queries.
Although adaLN is effective in simpler conditioning scenarios, it may not be optimal for more complex conditional embeddings that incorporate richer semantic details, such as sequential speech audio. Relevant comparative analyses will be elaborated upon in the experimental section.

SECTION: Audio-Driven Transformer Diffusion
To extract salient audio features for our proposed model, we utilize the wav2vec framework developed by Schneider et al.. The audio representation is defined as.
Specifically, we concatenate the audio embeddings generated by the final twelve layers of the wav2vec network, resulting in a comprehensive semantic representation capable of capturing various audio hierarchies.
This concatenation emphasizes the significance of phonetic elements, such as pronunciation and prosody, which are crucial as driving signals for character generation.
To transform the audio embeddings obtained from the pretrained model into frame-specific representations, we apply three successive linear transformation layers, mathematically expressed as:, where,, andrepresent the respective linear transformation functions. This systematic approach ensures that the resulting frame-specific representations effectively encapsulate the nuanced audio features essential for the performance of our model.

We explore three fusion strategies—self-attention, adaptive normalization, and cross-attention—as illustrated in Figureto integrate audio condition into the DiT-based video generation model. Our experiments show that the cross-attention strategy delivers the best performance in our model. For more details, please refer to Section.

Following this,
we integrate audio attention layers after each face-attention layer within the denoising network, employing a cross-attention mechanism that facilitates interaction between the latent encodings and the audio embeddings.
Specifically, within the DiT block, the motion patches function as keys and values in the cross-attention computation with the hidden states:. This methodology leverages the conditional information from the audio embeddings to enhance the coherence and relevance of the generated outputs, ensuring that the model effectively captures the intricacies of the audio signals that drive character generation.

SECTION: Identity Consistent Transformer Diffusion
Diffusion transformer-based video generation models encounter significant challenges in maintaining facial identity coherence, particularly as the length of the generated video increases.
While incorporating speech audio embeddings as conditional features can establish a correspondence between audio speech and facial movements, prolonged generation often leads to rapid degradation of facial identity characteristics.

To address this issue, we introduce a control condition within the existing diffusion transformer architecture to ensure long-term consistency of facial identity appearance.
We explore four strategies (as shown in Figure) for appearance conditioning: 1) Face attention, where identity features are encoded by the face encoder and combined with a cross-attention module; 2) Face adaptive norm, which integrates features from the face encoder with an adaptive layer normalization technique; 3) Identity reference network, where identity features are captured by a 3D VAE and combined with some transformer layers; and 4) Face attention and Identity reference network, which encodes identity features using both the face encoder and 3D VAE, combining them with self-attention and cross-attention. Our experiments show that the combination with Face attention and Identity reference net achieves the best performance in our model. For further details, please refer to Section.

We treat a reference image as a single frame and input it into a causal 3D VAE to obtain latent features, which are then processed through a reference network consisting of 42 transformer layers. Mathematically, ifdenotes the reference image, the encoder function of the 3D VAE is defined as:,
whererepresents the latent features associated with the reference image.

During the operation of the reference network, we extract vision tokens from the input of the 3D full attention mechanism for each transformer layer, which serve as reference features. These features are integrated into corresponding layers of the denoising network to enhance its capability, expressed as:whereis the latent representation at time step.
Given that both the reference network and denoising network leverage the same causal 3D VAE with identical weights and comprise the same number of transformer layers (42 layers in our implementation), the visual features generated from both networks maintain semantic and scale consistency.
This consistency allows the reference network’s features to incorporate the appearance characteristics of facial identity from the reference image while minimizing disruption to the original feature representations of the denoising network, thereby reinforcing the model’s capacity to generate coherent and identity-consistent facial animations across longer video sequences.

To facilitate long video inference, we introduce the lastframes of the previously generated video, referred to as motion frames, as additional conditions. Given a generated video length ofand the corresponding latent representation offrames, we denote the motion frames as.
The motion frames are processed through the 3D VAE to obtainframes of latent codes.
We apply zero padding to the subsequentframes and concatenate them withframes of Gaussian noise.
This concatenated representation is then patchified to yield vision tokens, which are subsequently input into the denoising network. By repeatedly utilizing motion frames, we achieve temporally consistent long video inference.

SECTION: Training and Inference
The training process consists of two phases:

In this initial phase, we train the model to generate videos with consistent identity. The parameters of the 3D Variational Autoencoder (VAE) and face image encoder remain fixed, while the parameters of the 3D full attention blocks in both the reference and denoising networks, along with the face attention blocks in the denoising network, are updated during training. The model’s input includes a randomly sampled reference image from the training video, a textual prompt, and the face embedding. The textual prompt is generated using MiniCPM, which describes human appearance, actions, and detailed environmental background. The face embedding is extracted via InsightFace. With these inputs, the model generates a video comprising 49 frames.

In the second phase, we extend the training to include audio-driven video generation. We integrate audio attention modules into each transformer block of the denoising network, while fixing the parameters of other components and updating only those of the audio attention modules. Here, the model’s input consists of a reference image, an audio embedding, and a textual prompt, resulting in a sequence of 49 video frames driven by audio.

During inference, the model receives a reference image, a segment of driving audio, a textual prompt, and motion frames as inputs.
The model then generates a video that exhibits identity consistency and lip synchronization based on the driving audio.
To produce long videos, we utilize the last two frames of the preceding video as motion frames, thereby achieving temporally consistent video generation.

SECTION: Dataset
In this section, we will give a detailed introduction of our data curation, including data sources, filtering strategy and data statistics.
Figureshows the data pipeline and the statistical analysis of the final data.

The training data used in this work is prepared from three distinct sources to ensure diversity and generalization. Specifically, the sources are: (1) HDTF dataset, which contains 8 hours of raw video footage; (2) YouTube data, which consists of 1,200 hours of public raw videos; (3) a large scale movie dataset, which contains film videos of 2,346 hours.
Our dataset contains a large scale of human identities and, however, we find that YouTube and movie dataset contains a large amount of noised data. Therefore, we design a data curation pipeline as follows to construct a high-quality and diverse talking dataset, as shown in Figure(a).

During the data pre-processing phase, we implement a series of meticulous filtering steps to ensure the quality and applicability of the dataset. The workflow includes three stages: extraction of single-speaker, motion filter and post-processing. Firstly, we select video of single-speaker. This stage aims to clean the video content to solve camera shot, background noise, etc, using existing tools. After that, we apply several filtering techniques to ensure the quality of head motion, head pose, camera motion, etc. In this stage, we compute all metric scores for each clip, therefore, we can flexibly adjust data screening strategies to satisfy different data requirement of our multiple training stages or strategies. Finally, based on the facial positions detected in previous steps, we crop the videos to a 3:2 aspect ratio to meet the model’s input requirements. We then select a random frame from each video and use InsightFaceto encode the face into embeddings, providing essential facial feature information for the model. Additionally, we extract the audio from the videos and encode it into embeddings using Wav2Vec2 model, facilitating the incorporation of audio conditions during model training.

Following the data cleaning and filtering processes, we conducted a detailed analysis of the final dataset to assess its quality and suitability for the intended modeling tasks. Finally, our training data contains about 134 hours of videos, including 6 hours of high-quality data from HDTF dataset, 72 hours of YouTube videos, and 56 hours of movie videos. Figure(b) also shows other statistics, such as Lip Sync score (Sync-C and Sync-D), face rotation, face ratio (the ratio of face height to video height).

SECTION: Experiment
SECTION: Experimental Setups
We initialize the identity reference and denoising networks with weights derived from CogVideoX-5B-I2V. During both training phases, we employ the v-prediction diffusion lossfor optimization. Each training phase comprises 20,000 steps, utilizing 64 NVIDIA H100 GPUs. The batch size per GPU is set to 1, with a learning rate ofpixels. To enhance video generation variability, the reference image, guidance audio and textual prompt are dropped with a probability of 0.05 during training.

We employed a range of evaluation metrics for generated videos across benchmark datasets, including HDTFand Celeb-V.
These metrics comprise Fréchet Inception Distance (FID), Fréchet Video Distance (FVD), Synchronization-C (Sync-C), Synchronization-D (Sync-D), and E-FID.
FID and FVD quantify the similarity between generated images and real data, while Sync-C and Sync-D assess lip synchronization accuracy. E-FID evaluates the image quality based on features extracted from the Inception network.
Besides, we introduced VBenchmetrics to enhance evaluation, focusing on dynamic degree and subject consistency.
Dynamic degree is measured using RAFTto quantify the extent of motion in generated videos, providing a comprehensive assessment of temporal quality.
Subject consistency is measured through DINOfeature similarity, ensuring uniformity of a subject’s appearance across frames.

We considered several representative audio-driven talking face generation methods for comparison, all of which have publicly available source code or implementations. These methods include SadTalker, DreamTalk, AniPortrait, and Hallo.
The selected approaches encompass both GANs and diffusion models, as well as techniques utilizing intermediate facial representations alongside end-to-end frameworks.
This diversity in methodologies allows for a comprehensive evaluation of the effectiveness of our proposed approach compared to existing solutions.

SECTION: Comparison with State-of-the-art
As shown in Tableand, our method achieves best results on FID, FVD on both datasets. Although our approach shows some disparity compared to the state-of-the-art in lip synchronization, it still demonstrates promising results as illustrated in Figure. This is because, to generate animated portraits from different perspectives, our training data primarily consists of talking videos with significant head and body movements, as well as diverse dynamic scenes, unlike static scenes with minimal motion. While this may lead to some performance degradation on lip synchronization, it better reflects realistic application scenarios.

To effectively demonstrate the performance of the general talking portrait video generation, we carefully collect 34 representative cases for evaluation. This dataset consists of portrait images with various head proportions, head poses, static and dynamic scenes and complex headwears and clothing. To achieve comprehensive assessment, we evaluate the performance on lip synchronization (Sync-C and Sync-D), motion strength (subject and background dynamic degree) and video quality (subject and background FVD).
As shown in Table, our method generates videos with largest head and background dynamic degree (13.286 and 4.481) while keeping lip synchronization of highest accuracy.

Figureprovides a qualitative comparison of different portrait methods on a “wild” dataset. The results reveal that other methods struggle to animate side-face portrait images, often resulting in static poses or facial distortions. Additionally, these methods tend to focus solely on animating the face, overlooking interactions with other objects in the foreground—such as the dog next to the elderly, or the dynamic movement of the background—like the ostrich behind the girl. In contrast, as shown in Figureour method produces realistic portraits with diverse orientations and complex foreground and background scenes.

SECTION: Ablation Study and Discussion
Tableand Figureillustrate the effects of various strategies for incorporating audio conditioning. The results demonstrate that using cross-attention to integrate audio improves lip synchronization by enhancing the local alignment between visual and audio features, particularly around the lips. This is evident from the improvements in Sync-C and Sync-D, and it also contributes to a degree of enhancement in video quality.

Tableand Figureevaluate different identity conditioning strategies. The results indicate that without an identity condition, the model fails to preserve the portrait appearance. When using face embedding alone, the model introduces blur and distortion, as it focuses solely on facial features and disrupts the global visual context. To address this, we introduce an identity reference network to preserve global features while making facial motion more controllable through identity-based facial embeddings. Thus, the proposed method achieves a lower FID of 23.458 and FVD of 242.602, while maintaining lip synchronization.

Tablepresents an analysis of varying temporal motion frames. One motion frame achieves the highest Sync-C score (6.889) and the lowest Sync-D score (8.695), indicating substantial lip synchronization.

Tableprovides a quantitative analysis of video generations using various CFG scales for audio, text, and reference images. A comparison between the second and fourth rows demonstrates that increasing the audio CFG scale enhances the model’s ability to synchronize lip movements. The text CFG scale significantly influences the video’s dynamism, as indicated in the first three rows, where both the subject’s and the background’s dynamics increase with higher text CFG scales. Conversely, the reference image CFG scale primarily governs the subject’s appearance; higher values improve subject consistency, as illustrated by the second and fifth rows. Among the tested configurations, setting,, andyields a balanced performance. This interplay between visual fidelity and dynamics underscores the effectiveness of CFG configurations in generating realistic portrait animations.

Despite the advancements in portrait image animation techniques presented in this study, several limitations warrant acknowledgment.
While the proposed methods improve identity preservation and lip synchronization, the model’s ability to realistically represent intricate facial expressions in dynamic environments still requires refinement, especially under varying illumination conditions.
Future work will focus on enhancing the model’s robustness to diverse perspectives and interactions, incorporating more comprehensive datasets that include varied backgrounds and facial accessories.
Furthermore, investigating the integration of real-time feedback mechanisms could significantly enhance the interactivity and realism of portrait animations, paving the way for broader applications in live media and augmented reality.

The advancement of portrait image animation technologies, particularly those driven by audio inputs, presents several social risks, most notably concerning the ethical implications associated with the creation of highly realistic portraits that may be misused for deepfake purposes.
To address these concerns, it is essential to develop comprehensive ethical guidelines and responsible use practices.
Moreover, issues surrounding privacy and consent are prominent when utilizing individuals’ images and voices. It is imperative to establish transparent data usage policies, ensuring that individuals provide informed consent and that their privacy rights are fully protected.
By acknowledging these risks and implementing appropriate mitigation strategies, this research aims to promote the responsible and ethical development of portrait image animation technology.

SECTION: Generation Controllability
To evaluate whether textual conditional controllability is effectively preserved, we conducted a series of experiments comparing the performance of our method to that of the baseline model, CogVideoX, using same text prompts. As shown in Figure, the results shows that our model maintains its ability for textual control, and effectively captures the interaction between different subjects as dictated by the textual prompts.

We also explore model’s ability to follow the foreground and background textual prompt. As illustrated in Figure, our method animates the foreground and background subjects naturally, such as the ocean waves and flickering candlelight. The results demonstrates the model’s ability to control foreground, and background with the textual caption, which is maintained even after introducing the audio condition.

SECTION: Conclusion
This paper introduces advancements in portrait image animation utilizing the enhanced capabilities of a transformer-based diffusion model. By integrating audio conditioning through cross-attention mechanisms, our approach effectively captures the intricate relationship between audio signals and facial expressions, achieving substantial lip synchronization. To preserve facial identity across video sequences, we incorporate an identity reference network. Additionally, we utilize motion frames to enable the model to generate long-duration video extrapolations. Our model produces animated portraits from diverse perspectives, seamlessly blending dynamic foreground and background elements while maintaining temporal consistency and high fidelity.

SECTION: References