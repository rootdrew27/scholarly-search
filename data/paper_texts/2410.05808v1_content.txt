SECTION: Vision Transformer based Random Walk for Group Re-Identification
Group re-identification (re-ID) aims to match groups with the same people under different cameras, mainly involves the challenges of group members and layout changes well. Most existing methods usually use the k-nearest neighbor algorithm to update node features to consider changes in group membership, but these methods cannot solve the problem of group layout changes. To this end, we propose a novel vision transformer based random walk framework for group re-ID. Specifically, we design a vision transformer based on a monocular depth estimation algorithm to construct a graph through the average depth value of pedestrian features to fully consider the impact of camera distance on group members relationships. In addition, we propose a random walk module to reconstruct the graph by calculating affinity scores between target and gallery images to remove pedestrians who do not belong to the current group. Experimental results show that our framework is superior to most methods.

Group re-identification, random walk, vision transformer

SECTION: Introduction
Person re-identification (re-ID) is a technique to re-identify the people under different cameras. Given a probe image, the task of person re-ID requires identifying the same person images from gallery images,. In recent years, person re-ID has produced many derivative tasks in different directions, including clothes changing, visible-infrared and unsupervised person re-ID etc. However, the existing methods have mainly focused on single person re-ID, and people always walk in groups,,,. Therefore, it is also crucial for group re-ID in real life.

In comparison to person re-ID, group re-ID aims to re-identify group images with the same group members under different cameras and has additional challenges in addition to viewpoint changes and human pose changes: (i) Group layout changes: The layout of group members in a group is largely affected by different camera views constraints. Due to the dynamic movement of people, the relative positions of group members in a group can differ significantly in two camera views. (ii) Group members changes: Group members may dynamically join or leave a group frequently. The most existing methods for solving group re-ID mainly construct graphs through the k-nearest neighbor algorithm to transfer the group context information of adjacent members, enhancing the relationship between group members, as shown in Fig.(a). However, these methods not only ignore the impact of camera distance on group membership relationships, but also cannot fundamentally solve the problem of group layout changes.

To solve the above problems, we propose a novel vision transformer based random walk framework for group re-ID. In our method, we design a vision transformer based on a monocular depth estimation algorithm to solve the impact of camera distance on group members by embedding pedestrian depth values into vision transformer. Specifically, we first use monocular depth estimation algorithm to obtain depth maps of single person from the probe image and calculate the average depth of its depth map. Then, the person features are obtained through the vision transformer and constructed into a graph with different nodes according to the size of depth average. It is worth noting that the graph constructed in order of depth value can effectively solve the impact of camera distance on group membership relationships.

In addition, we also design a random walk module to remove members who do not belong to the current group by reconstructing the graph. Specifically, after obtaining graphs with different nodes, we calculate affinity scores between all members in each graph and the gallery images. We then compute the average affinity score for all members in each group, selecting the graph with the highest average affinity score. Subsequently, combining attention mechanisms, we utilize contextual information to propagate new graphs between groups and update graph node features for group matching. It is worth noting that in the process of reconstructing the graph, not only the problem of changes of group members can be effectively solved, but also the problem of group layout changes by group members relative positions can be ignored.

The following are the primary contributions of this paper:

We design a vision transformer based on a monocular depth estimation algorithm to solve the impact of camera distance on group members relationships.

We propose a random walk module by reconstructing the graph to solve the problem of group members and layout changes for group re-ID.

Our proposed framework is examined on three group re-ID datasets, and experimental results demonstrate that our technique outperforms the most advanced approaches.

SECTION: RELATED WORK
Compared with single person re-ID, there have been relatively few works focusing on group re-ID task,,,,,in the past few years. Some researchers primarily focuses on extracting global or semi-global features to address the challenges posed by the group re-ID. For example, Cai et al.presented a covariance descriptor to encode the spatial position and RGB value of each pixel in the group image to capture global features. Zheng et al.suggested CRRRO-BRO descriptor to obtain the global and local features. Due to changes in the relative positions of group members while walking, the group layout has changed, global and local features need to be responsive to such variations. In order to exploit above features in the group, Zhu et al.presented the method of patch matching to perform similarity matching to measure the distance between the two group images. However, it requires prior constraints on vertical misalignment, which makes it unfeasible in some cases. Xiao et al.employed multi-granularity information to try to comprehensively capture the features of the group. However, this method is based on traditional manual features and generates an excessive amount of redundant information, resulting in suboptimal accuracy. DotSCNextracted group consistency features by learning the differential features of paired members in two images. Recently, MACGdesigned complex multi-attention to capture key group features. However, most of the above methods cannot well solve the problem of group members and laylout changes.

SECTION: Method
SECTION: Overview
To address the challenge of group members and layout changes for group re-ID, we design a novel vision transformer based random walk framework, as shown in Fig..

In general, 1) our framework can use the monocular estimation algorithm to obtain the depth map and crop the single person images from the probe images and the depth maps respectively; 2) our framework use the vision transformer by embedding the token of person deep values to obtain the person features; 3) We use the person features to construct the graphs in order of depth value; 4) We calculate the affinity scores between all members in each graph and the gallery images to obtain the final graph; 5) The node features of the final graph transfer messages with inter-group in the group matching module.

SECTION: Random Walk
Denoteas an undirected graph, whererepresents a vertex set, andmeans a set of edges. A square matrixcan be utilized to model a random walk operation on a graph, whereis the number of vertices. The probability of similarity between the-th and-th nodes is represented as. In the task of group re-ID,can be denoted as the normalized affinity score between the-th and-th person images. Considerbe an-dimensional vector that signifies the affinity scores between the probe image and all gallery images during the-th random walk iteration. Given the matrixcontaining normalized pairwise affinities among two gallery images, we describe the random walk operation as follows:.

Given a probe image andgallery images, the Siamese CNN is employed to estimate the pairwise affinity score between images. Ours network processes two images as input and predicts the probability of images belong to the same person. The initial affinity scores generated by CNN between the probe image and gallery images are denoted as. Consideras the matrix that contains the affinity scores between the set of probe sequences andgallery images. We apply the softmax function to normalize each row of the original affinity matrixto normalize the constraint for allin.

where= 0 refer to prevent self-reinforcement during random walk iterations. Hence, the diagonal term in Eq () does not apply to softmax normalization. An iteration of a random walk on the initial affinity can be represented as:

whererepresents one iteration of the initial affinity. Intuitively, when personandshare the same ID, their affinities with the probe images should also be similar, and the image groups are more similar. The affinity scoreof the-th image can be calculated as:

By analogy, we calculate the affinity score, …,of each image in the graph, and obtain the average affinity score of all images. Ultimately, we regard the graph with the highest average affinity as our target group.

SECTION: Group Matching
Inspired by, we perform group matching in graphs by capturing inter-graph information to update graph nodes. In the task of group re-ID, we aim to compute the similarity of two groups. Therefore, it is essential to explore the correlation between groups. In essence, when two groups are the same group, it is likely that the group members should have same correspondences. At the same time, higher similarity of a single pair indicates higher group-level similarity. Given two graphs (,), we consider the node-level feature pair (,), whereand. Furthermore, we divide the features of personandintoandparts respectively. We calculate the importance weight of person features between graphs as:

whereis an inner product layer andis a projection matrix. Then, we use the softmax function to calculate the attention weights by normalizing the importance weights:

The inter-graph messages with the corresponding attention weights passed from personin the-th part of the graphto personin the-th part of the graphcan be calculated as follows:

Here, we focus on person-level similarity when calculating inter-graph attention. As a result, the part-level features within a node are assigned the same set of inter-graph attention weights. After obtaining the information between graphs, use the fully connected layer to update the node features:

The aforementioned feature update steps are iteratively performed forrounds, utilizing the inter-graph attention mechanism. Subsequently, the model is structured to learn the associations between groups and individuals, respectively. Initially, we create a graph-level representation through readout operations. In this case, we employ self-attention to obtain the ultimated graph representationas a weighted sum of node-level features:

For another graphmay be generatedin the same way. The circle loss function is used to limit the features of the same group and push other groups far apart in order to understand the group correspondence:

whereandare non-negative weighting factors,is as a scale factor.

SECTION: EXPERIMENTAL RESULTS
SECTION: Datasets and Experimental Settings
We evaluate our proposed group re-ID method on three publicly available datasets: (1) the Road Group dataset (RG) contains 324 images including 162 group classes, (2) the DukeMTMC Group dataset (DG) contains 354 images including 177 group classes captured by 8 cameras, and (3) CUHK-SYSU-Group dataset (CSG) based on CUHK-SYSU dataset contains 3,839 images including 1,558 group classes. Examples of group images from three datasets are illustrated in Fig..

We randomly split datasets in half to create training and testing sets, using vision transformer as our backbone. We resize the person images as 256×128 for inputs. The initial learning rate is set at 0.0001 trained to 300-th epoch. For simplicity, graphs are constructed with an equal number of empty nodes for groups of varying sizes, and dummy nodes are added for groups with limited members.

SECTION: Compared with Other Group Re-ID Methods
We assess the effectiveness of the proposed approach against existing methods using the RG, DG, and CSG datasets, as shown in Table. The current methods are categorized into two groups: handcrafted methods and deep learning methods. It is worth noting that the deep learning method DotSCN incorporates additional datasets for auxiliary training. Among these, MACG is considered the top-performing method for single dataset training, while DotSCN excels in multiple dataset training scenarios. The results demonstrate that our proposed method in this paper achieves the advanced performance in single dataset training. Compare with MACG, our method exceeds 1.1%, 14.4% and 16.9% Rank 1 on RG, DG and CSG datasets. Even without the use of additional datasets, our approach outperforms DotCSN in certain instances, underscoring the superiority of our method.

SECTION: The visualization of results
We randomly select a probe image in the CSG dataset to visualize the process of the random walk module. Each image has a ground truth group as the criterion for the affinity score. We first build the image into a graph structure according to the average size of the depth values, and then use different vertices as starting nodes to construct the graph structure through the random walk method, as shown in the Fig. The numbers represent the affinity score of each graph structure with the ground truth graph structure. We find that the group member with the highest affinity score happens to be the ground truth group member in the graph. Experiments have shown that our method can prove that groups composed of pedestrians whose depth values are more similar are more likely to be real groups. At the same time, this method can also effectively remove pedestrians who do not belong to the group and ignore the group layout changes.

The Fig.shows the matching results of our proposed method. The first two examples show situations which group members changes between queries and gallery images. We observe that even if there is occlusion in these examples, the results can still be retrieved correctly. The last two examples show failing situations where the gallery group typically contains people who share a similar appearance to the two person in the query images. In this case, it will be more difficult for the model to retrieve the correct matches.

SECTION: Ablation Study
We evaluate the effects of various human body. To be specific, we segment them intoparts, and results of various partition configurations are shown in Table. It is evident from the results that a= 2 partition yields notably lower performance compared to other partitioning schemes, primarily due to its coarse segmentation. Conversely, the most favorable performance is attained when employing= 4 partitions.

We analyze how different backbone networks affect model performance. We use ResNet50, DensNet161, EfficientNet-b7 and Vision transformer as backbone networks respectively. Tableshows the experimental results. We find that using Vision transformer as the backbone network has the best performance, while using EfficientNet-b7 as the backbone network has the worst performance.

We conduct module ablation experiments on CSG dataset to confirm the effectiveness of our proposed module. Tableshows the experimental results. We examine distinct variants of the framework such as Random Walk module (RW), Group Matching module (GM) and Circle Loss (CL) function. By rearranging elements in different arrangements, we can determine the final model performance.

SECTION: Conclusion
In this paper, we present a novel vision transformer based random walk framework to address the challenge of group re-ID, which contain a vision transformer based on a monocular depth estimation algorithm to construct a graph through the average depth value of pedestrian features to fully consider the impact of camera distance on group members relationships and a random walk module to reconstruct the graph by calculating affinity scores between target and gallery images to remove pedestrians who do not belong to the current group. Experiments show that we obtained outstanding results on three group re-ID datasets, demonstrating the efficacy of our suggested methodology.

SECTION: References