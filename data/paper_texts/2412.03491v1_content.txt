SECTION: Beyond algorithm hyperparameters: on preprocessing hyperparameters and associated pitfalls in machine learning applications

Adequately generating and evaluating prediction models based on supervised machine learning (ML) is often challenging, especially for less experienced users in applied research areas. Special attention is required in settings where the model generation process involves hyperparameter tuning, i.e. data-driven optimization of different types of hyperparameters to improve the predictive performance of the resulting model. Discussions about tuning typically focus on the hyperparameters of the ML algorithm (e.g., the minimum number of observations in each terminal node for a tree-based algorithm). In this context, it is often neglected that hyperparameters also exist for the preprocessing steps that are applied to the data before it is provided to the algorithm (e.g., how to handle missing feature values in the data). As a consequence, users experimenting with different preprocessing options to improve model performance may be unaware that this constitutes a form of hyperparameter tuning - albeit informal and unsystematic - and thus may fail to report or account for this optimization. To illuminate this issue, this paper reviews and empirically illustrates different procedures for generating and evaluating prediction models, explicitly addressing the different ways algorithm and preprocessing hyperparameters are typically handled by applied ML users. By highlighting potential pitfalls, especially those that may lead to exaggerated performance claims, this review aims to further improve the quality of predictive modeling in ML applications.

SECTION: 1Introduction

Many applied research areas have recently seen an increase in the development of prediction models based on supervised machine learning (ML) algorithms.
However, after initially generating widespread enthusiasm – partly due to the availability of user-friendly software that enables model development without requiring extensive expertise – ML-based prediction models are now undergoing critical reexamination(Ball,,2023; Kapoor and Narayanan,,2023; Pfob et al.,,2022).
Among other concerns, such as insufficient reporting of relevant aspects of the model development process, it has been found that the claimed predictive performance of many models is considerably exaggerated(Kapoor and Narayanan,,2023;Dhiman et al., 2022a,;Dhiman et al., 2022b,; Andaur Navarro et al.,,2021).
While some of the pitfalls leading to such optimistically biased performance claims (e.g., using the exact same observations for model generation and evaluation) typically occur only among very inexperienced applied ML users and are well known within the ML research community, others arise more subtly(Domingos,,2012; Kapoor and Narayanan,,2023; Poldrack et al.,,2020; Hofman et al.,,2023).This is particularly true when the model generation process involves data-driven hyperparameter optimization, which is also referred to as hyperparameter tuning and is commonly employed in ML applications.
The most prominent type of hyperparameters (HPs) are those associated with the learning algorithm, which specify its configuration (e.g., the minimum number of observations in each terminal node for tree-based algorithms). If selected by an adequate (and ideally automated) tuning procedure, HPs can substantially enhance the performance of the resulting prediction model. However, HP tuning also complicates model evaluation, as common procedures such as simple-fold cross-validation no longer guarantee an unbiased assessment(Bischl et al.,,2023; Hosseini et al.,,2020).An additional challenge comes from the fact that, beyond algorithm HPs, there are also preprocessing HPs, which specify the steps applied to the data before it is fed into the learning algorithm (e.g., selecting the set of features for prediction or determining how missing feature values are handled;Binder and Pfisterer,,2024; Bischl et al.,,2023). While the tuning of algorithm HPs is rightfully considered important for model performance, the relevance of tuning preprocessing HPs should not be overlooked. Preprocessing steps can make or break a model’s predictive performance, and solely relying on user expertise to specify these steps (which is the alternative to tuning) is often impractical and may result in arbitrary decisions(Kuhn and Johnson,,2013). Despite this, reports of tuning preprocessing HPs aside from feature selection are relatively rare. This could be because integrating preprocessing HPs into automated tuning workflows typically requires advanced programming expertise, which not all applied ML users have, or because this possibility is not widely recognized. Importantly, the limited use of automated tuning procedures for preprocessing HPs does not mean that these HPs are not being tuned at all. In fact, it appears fairly common for applied ML users to experiment informally with different preprocessing options(Hosseini et al.,,2020; Hofman et al.,,2023; Lones,,2024), often without realizing that this constitutes a form of (manual) HP tuning.
If this type of tuning is indeed conducted subconsciously, it will also remain unaccounted for during model evaluation, thereby increasing the risk of drawing overly optimistic conclusions about the model’s performance.To avoid such issues, it is essential to educate users in applied settings about the different types of HPs, the different forms of HP tuning, and how tuning can impact both the true and estimated performance of prediction models. Although valuable literature already exists describing the concept of HP tuning and various automated procedures(e.g., Bischl et al.,,2023; Bartz et al.,,2023; Feurer and Hutter,,2019), this research primarily adopts the perspective of ML methods researchers who are concerned with evaluating the overall performance of ML algorithms used to generate prediction models. This focus does not align with the perspective of applied ML users, who are more interested in the performance of a specific prediction model. Although this literature is still useful for them – since the general principles described there essentially hold for all types of audiences – applied ML users additionally need specific guidance for developing their“final model”(a notion that does not exist in the methodological context). Moreover, they may find it challenging to extract the relevant insights from literature aimed at a different audience with partly different needs. In contrast, literature explicitly directed toward applied ML users tends to either focus on general guidelines for ML-based predictive modeling, lacking detailed coverage of HP tuning(e.g., Kuhn and Johnson,,2013; Pfob et al.,,2022; Lones,,2024; Kapoor et al.,,2024; Poldrack et al.,,2020;Collins et al., 2024a,; Van Royen et al.,,2023), or addresses HP tuning only within specific
research areas(e.g., Hosseini et al.,,2020; Dunias et al.,,2024). Additionally, much of the existing HP tuning literature does not consider preprocessing HPs. Exceptions include the review byBischl et al., (2023), which, however, touches on this topic only briefly. This lack of detail is reasonable, given that preprocessing HPs can, in principle, be tuned using the same automated procedures as algorithm HPs. However, this perspective overlooks that preprocessing HPs are often tuned manually in applied settings, which carries implications different from those associated with automated tuning.This paper aims to complement the existing literature by reviewing the implications and pitfalls of HP tuning in the generation and evaluation of prediction models from the perspective of applied ML users with varying levels of expertise. It explicitly distinguishes between preprocessing and algorithm HPs, as well as the different procedures commonly used to tune them in practice. A particular focus is placed on the potential for optimistically biased performance estimation, which is also illustrated using a real-world prediction problem from palliative care medicine.The paper is structured as follows. Section2introduces the key concepts related to predictive modeling using ML, including the two types of HPs. In the next two sections, the challenges and pitfalls that arise in the generation and evaluation of prediction models are described, differentiating between the setting where all HPs are pre-specified (Section3) and the setting where one or more HPs are selected through tuning (Section4). Section5empirically illustrates the impact of different tuning and evaluation procedures on the estimated model performance. Section6summarizes the key insights, discusses the limitations of the empirical study, and outlines future research directions.

SECTION: 2General concepts of predictive modeling using supervised ML

SECTION: 2.1Terminology and notation

The following terminology and notation is adapted fromBischl et al., (2023).
Letbe a labeled data set withobservations. Accordingly, each observation() consists of an outcome(i.e. the variable to be predicted, also referred to as label or target) and a-dimensional feature vector(i.e. thevariables used to predict, also referred to as predictors), whereandcan take any value from the outcome spaceand feature space, respectively. Two common types of prediction problems are regression, for whichcan be any real number (i.e.), and classification, for whichcan be one ofclasses (i.e.is finite and categorical with). We assume that the observations inare independent and have been sampled from the same (unknown) probability distribution.The general aim of supervised ML is to“learn”a model from the data setthat is able to predict the outcome values of new observations. Essentially, a prediction model is a functionthat maps any observed feature vectorto a prediction vectorin. The prediction vectoreither directly corresponds to the predicted outcome value (e.g., for regression, where), or can be transformed accordingly (e.g., for classification, wherecorresponds to predicted probabilities for each class and the predicted class could be the class with the highest probability). The prediction model results from a learning pipeline, which uses the data setto find the functionthat yields the best predictions for the true outcome values in. To stress that a prediction modelis based on learning pipelineand data set, we write. The prediction modelcan usually be parameterized, meaning that it is defined by a set of parameters(simply denoted aswhen data set and learning pipeline are clear from context andwhen referring to the parameters prior to estimation).There are two key processes associated withand, which we will explore in more detail throughout the paper: (i) the training process, in which the learning pipelineis applied toand estimates the parametersand thus the prediction model, and (ii) the prediction process, in whichis used to make predictions for an observation (whether fromor from a new data set) with feature vector, resulting in.
Note that to make predictions on a new data set, the outcome does not need to be observed (it would only be necessary for evaluating those predictions). The training and prediction process serve as the foundation for more complex processes related to the development of prediction models, which we will address in Section2.4.

SECTION: 2.2Learning pipeline

Each learning pipelinecontains a learning algorithm as a central component but can also include several preprocessing steps that are performed before the algorithm is applied to the data. Since preprocessing steps are a particular focus of this paper, we use the term “learning pipeline” instead of the more common term “learner” to emphasize thatcan consist of several components. Note that for now, we consider all components ofas fixed, but will discuss the case in which they can be modified in Section2.3.

The choice of learning algorithm usually depends on the specific prediction problem. For example, if the desired prediction model is a decision tree (which is the case for the real-world prediction problem considered in Section5), a possible algorithm choice is the well-known Classification and Regression Tree algorithm (CART), which partitions the feature spaceby a sequence of binary splits into terminal nodes and assigns a prediction value to each terminal node(Breiman et al.,,1984).
In this case, the parameters of the learning algorithm contained inare the splitting rules that generate the tree structure (i.e. which features are used with which threshold value) and the prediction values at each terminal node. The learning algorithm can also consist of multiple individual algorithms that are combined into one overall algorithm (e.g., random forests). These types of algorithms are referred to as ensemble methods, but will not be discussed further in this paper. In general, the choice of algorithm has a large impact on the hypothesis space of the learning pipeline, i.e. the set of prediction models the learning pipeline can generate. For example, selecting a standard linear regression as algorithm (withcontaining the regression coefficients) would imply that the corresponding learning pipeline would not be able to learn prediction models that do not correspond to linear combinations of the features (e.g., polynomials).

While a data set can, in theory, be fed directly into the algorithm (i.e. the algorithm is the only component of the learning pipeline), it typically undergoes some modification first. This process can be referred to as data preprocessing and encompasses all the steps taken to transform the data set from its rawest available form into the final form provided as input to the learning algorithm(Kapoor et al.,,2024). Data preprocessing steps are usually performed to improve the performance of the resulting prediction model, to enable the data to be (better) handled by the learning algorithm(Thomas,,2024), or to improve the
interpretability of the resulting prediction model. To better illustrate different the characteristics of preprocessing steps and their implications on the training and prediction process, we consider a simple learning pipeline as an example, which is also depicted in Figure1(middle panel).

It consists of two preprocessing steps, which are followed by the CART algorithm. The first preprocessing step is the replacement of missing feature values using mean imputation, and the second preprocessing step is the log-transformation of features.

Based on this example learning pipeline, we can make a first distinction between preprocessing steps. This distinction concerns whether the steps have parameters estimated from(with these parameters included in) or whether they are parameterless and are carried out independently for each observation(Kapoor et al.,,2024; Binder and Pfisterer,,2024).
In the example, the replacement of missing feature values is a parameterized preprocessing step, as it involves the parameter, representing the mean of all non-missing values estimated from. In contrast, the log-transformation of features does not involve any parameters. Other examples of preprocessing steps with parameters include centering or scaling of features, where parameters such as the mean or standard deviation are estimated from. On the other hand, creating a new feature by summing multiple features serves as another example of a parameterless preprocessing step.

The second key distinction in preprocessing steps concerns whether they are applied only during the training process as part of the learning pipeline or also during the prediction process. This distinction is closely related to whether a preprocessing step modifies only the feature distribution or also affects the outcome distribution. More formally, letdenote the outcome vector in. If, after applying all preprocessing steps in the learning pipeline during training,remains unchanged, we classify the step as affecting only the feature distribution. Otherwise, the step affects the outcome distribution, for example, by removing or adding observations or transforming outcome values.We first consider preprocessing steps that affect only the feature distribution. These comprise all preprocessing steps mentioned above, including those in the example learning pipeline. Additional examples are dimensionality reduction techniques (e.g., principal component analysis), feature selection, or data cleaning steps that do not alter the outcome distribution (e.g., correction of errors in features)(Kuhn and Johnson,,2013; Thomas,,2024). Preprocessing steps of this type must be applied not only during training but also during prediction, in the same sequence as in the learning pipeline. This ensures that the model produced by the learning algorithm receives the data in the same format during prediction as it did during training, preserving the validity of the model(Binder and Pfisterer,,2024). This requirement implies that these steps are not only components of the learning pipeline, but also part of the resulting prediction model. Consequently, if a learning pipelineincludespreprocessing steps that only affect the feature distribution, the prediction modelis not a single function but a function composition offunctions (omittingandfor simplicity of notation):

wherecorresponds to the model resulting from the learning algorithm, andreflect thepreprocessing steps. Accordingly, a more accurate name for a prediction model would be prediction modelpipeline, but for brevity, we will continue to use the former. Returning to the example learning pipeline, the resulting prediction model is a composition of three functions,, where,, andcorrespond to the imputation step, the log-transformation step, and the decision tree model, respectively. When making a prediction for one or more observations, all three functions must be applied (see Figure1, right panel). Importantly, if any functions constituting the prediction model are omitted during the prediction process, or if any preprocessing or algorithm parameters are re-estimated on a new data set for which predictions are to be made, the validity of the prediction model may be compromised. However, in practice, this pitfall is often unavoidable for users who wish to apply a model but were not involved in its development, as studies introducing new prediction models frequently fail to report the preprocessing steps performed prior to applying the learning algorithm(Kapoor et al.,,2024).In contrast to preprocessing steps that only affect the feature distribution, preprocessing steps that modify the outcome distribution are not necessarily applied during prediction. Here, we must distinguish between steps aimed at improving compatibility with the learning algorithm and those intended to alter the scope or interpretation of the prediction model. An example of the first type is (invertible) transformations applied to the outcome during training, such as a log-transformation to reduce skewness. To ensure predictions are returned on the correct scale, these transformations must be reversed during prediction(Thomas,,2024). For instance, if the outcome was log-transformed during training, the model will output, which must then be exponentiated to restore the prediction to its original scale. Note that some other compatibility-focused steps are not applied at all during prediction. In the context of classification problems, this includes class-balancing steps such as oversampling, where observations from the least prevalent class are randomly resampled to overcome class imbalance effects during the training process (see, e.g.,Kuhn and Johnson,,2013for more details). In the notation of the prediction model as a function composition introduced above,
preprocessing steps that are applied only in their inverted form or not at all during prediction are represented as inversion function or identity function, respectively.In contrast, preprocessing steps that modify the outcome to alter the scope or interpretation of the prediction model should be consistently applied during prediction. For example, if a continuous outcome is discretized to convert a regression problem into a classification problem(Hofman et al.,,2023), this (irreversible) transformation must also be applied to the true outcome during prediction in order to enable a meaningful comparison between the predictions and the actual outcome values. Such transformations of the outcome are not part of the prediction model itself (which mapsto predictions, not), but must be performed alongside the prediction process. Moreover, since the outcome values are generally unknown when making predictions for observations from a new data set that does not correspond to, these transformations are typically not actual steps executed when making predictions but instead determine how the predictions are interpreted.

SECTION: 2.3Hyperparameters

Until now, we have assumed that the learning pipelineis fixed. However, individual components ofusually have several hyperparameters (HPs), which determine their specific configuration and thus substantially influence the resulting prediction model.
This also applies to the learning pipeline example considered in the previous section, for which possible HPs are shown in the left panel of Figure1(see below for further explanation). In contrast to the parameters, which are estimated as outputs of the learning pipeline, the HPs serve as inputs. This means that they must be specified before the learning pipeline is applied to the data set(Bischl et al.,,2023).

The following notation is based onFeurer and Hutter, (2019). We denote theth HP of a learning pipeline as, which is selected from its domain(i.e.). The domain ofcan generally be real-valued, integer-valued, binary, or categorical, as we will see in the examples given below. AllHPs of a learning pipeline can be summarized as a vectorand their overall configuration space as(with). Note thatmay contain conditionality, meaning that some HPs might only be relevant when one or more other HPs are set to a certain value (see below for examples).As described in Section2.2, the learning pipeline consists of several preprocessing steps and a learning algorithm. We can consequently differentiate between preprocessing and algorithm HPs, which we denote asand(i.e..

Each learning algorithm usually has several HPs, which are specified by the software package used and can have a large impact on its complexity, speed, and other important properties of the algorithm(Bischl et al.,,2023).
For example, the HPs of the CART algorithm include the minimum number of observations in any terminal node (, the maximum tree depth, with the root node counted as depth 0 (), and the factor by which a split needs to decrease the overall lack of fit to be attempted ()(Therneau and Atkinson,,2022). In the CART implementation of theRpackagemlr3(Lang et al.,,2019), the respective HP domains are,(both being integer-valued domains), and(real-valued domain).
Most algorithm HPs have default values that are specified by the software in which they are implemented
(e.g., inmlr3,per default).Note that since there is usually more than one algorithm suitable for a given prediction problem, the choice of algorithm can also be seen as an HP of the learning pipeline (with the HPs associated with each algorithm representing conditional HPs that are only relevant when the respective algorithm is used;Bischl et al.,,2023). This creates an even more flexible but also complex learning pipeline, which is why, in this paper, we assume that the algorithm has already been selected.

As mentioned above, it is not only possible to specify learning algorithm HPs, but also preprocessing HPs(Binder and Pfisterer,,2024; Bischl et al.,,2023).
In principle, whenever multiple options exist for performing a preprocessing step, these options can be considered as different HP values of the respective preprocessing step.First, the choice of whether a preprocessing stepis applied at all can be considered as a binary HPwith(e.g., whether features should be log-transformed or not).
Second, there is often more than one possible option for performing a preprocessing step.
For example, the influence of outliers in features can be reduced by replacing all values that are outside the rangebyand, respectively (“winsorizing”;Steyerberg,,2019). There are different options to specifyand, which means thatandare HPs of the winsorizing preprocessing step (e.g.,Steyerberg,,2019suggests percentiles such asand).Several possible options also exist for the imputation of missing feature values. For example, imputation can be based on the feature’s mean or median, or on a sampled value from its empirical distribution(as illustrated in Thomas,,2024). This constitutes a (categorical) preprocessing HPwith.Another typical example for a preprocessing step with many possible options is feature selection. To define HPs in this context, we have to differentiate between filter and wrapper methods (the following explanations are based onWright,,2024, who also provides more details and additional examples). Filter methods are preprocessing steps that assign a numeric score to each feature (e.g., the correlation coefficientbetween each feature and the outcome) and select a set of features according to this score (e.g., all features with). Consequently, the set of selected features is the parameter of the filter (i.e., with, e.g.,), while its specific configuration can be modified by its HPs. For example, there are different options to define the score (, with) and to select the features based on their score (, with, whereandthemselves are HPs that are conditional on). Instead of using filter methods, it is also possible to directly specify the set of features that should be selected.
In this case, the set of selected features is an input rather than an output of the learning pipeline, and is therefore the HP () of the feature selection step. For example, if only the features, andshould be used by the learning algorithm, then.
In many applications,is not specified once by the user, but different values ofare tried and evaluated on. This process is referred to as a wrapper method, but is, in fact, a special case of HP tuning, which will be discussed in Section4.1.Note that the individual HP values can also be application-specific. For example, in the real-world prediction problem considered in Section5, several options for aggregating 17 individual features covering physical symptoms, psycho-social burden, family needs, and practical problems of palliative care patients to a sum score are reasonable (see Section5.2.2).In addition to specifying the preprocessing steps, the order in which they appear in the learning pipeline can technically be considered an HP as well. For instance, in the learning pipeline shown in Figure1, the log-transformation step could also be applied before the imputation step, resulting in a differentand, therefore, potentially a different prediction model. However, we will not consider this type of preprocessing HP further in the remainder of this paper.As already indicated by the examples above, many preprocessing HPs are conditional on other preprocessing HPs (e.g., the winsorizing HPsandare only relevant when winsorizing is the chosen method to reduce the influence of feature outliers, which could also be implemented by transforming the features instead). Moreover, in contrast to algorithm HPs, preprocessing HPs often cannot be set by a single software function argument (for example, all HPs of the CART algorithm named in the previous section can be specified within a singleRfunction, using, e.g., the argumentminbucketfor); instead, in many cases, the different options for a specific preprocessing step are implemented by different software packages. Consequently, there is often no formal HP domain, and defining the domain such that it contains all possible HP values may not even be feasible (e.g., for, definingwould require collecting all available methods for imputing missing values). Moreover, many preprocessing HPs do not have a formal default value, although the option of not applying a preprocessing step (if applicable and not leading to an error) seems to be a reasonable default value that we will adopt in the following.In contrast to algorithm HPs, it seems that preprocessing HPs – apart from those related to feature selection – are rarely discussed or referred to as such in ML applications (see, e.g., the systematic reviews ofDhiman et al., 2022a,andAndaur Navarro et al.,,2023, where such terms were not mentioned). ML methods research usually also focuses on algorithm HPs rather than preprocessing HPs. An exception is the benchmark study byStüber et al., (2023), which, among other factors, examines the impact of using principal component analysis in radiomics-based survival analysis.

While it is usually possible to leave all HPs at their respective default value, it is common to modify them in an attempt to optimize the prediction model generated by the learning pipeline. This can also be necessary if there is no specified default value.
The term “optimization” here often refers to the predictive performance of the model but can also take into account other criteria such as simplicity, interpretability, or runtime to generate the model(Domingos,,2012; Pfob et al.,,2022; de Hond et al.,,2022; Bischl et al.,,2023). Note that the selection of HPs can be considered a“researcher degree of freedom”(Simmons et al.,,2011), as it is one of many choices that users must make throughout the model development process (other choices are, e.g., how predictive performance is assessed;Hofman et al.,,2017; Klau et al.,,2019; Hosseini et al.,,2020).We can distinguish between two primary types of HP selection: data-independent and data-dependent procedures. Data-independent HP selection does not make use of the data setand is ideally based on the user’s knowledge about the data set and learning algorithm.
For example, sensible algorithm HPs can be selected when users are experienced with the learning algorithm or when corresponding recommendations from the literature (e.g., previous benchmark studies) are available(Bartz et al.,,2023; Bischl et al.,,2023).
Similarly, some preprocessing HPs may be inferred from substantive knowledge about the data set (e.g., which set of features should be selected) or knowledge about how the learning algorithm is affected by certain data set characteristics (e.g., whether the algorithm is sensitive to outliers in features, which requires some form of transformation;Kuhn and Johnson,,2013).
An example of data-independent HP selection on the basis of model simplicity is the specification of the maximum tree depth in the real-world prediction problem considered in Section5, where the project team set the HP toto ensure that the resulting decision tree can be implemented in clinical practice.In cases where users have insufficient knowledge about the data and learning algorithm to ensure a reasonable HP selection, but wish to avoid arbitrary or default HP values, it is possible to use the data setto select optimal HP values.
This process corresponds to a data-dependent HP selection, but terms such as HP tuning and (data-driven) HP optimization are more common(e.g., Bischl et al.,,2023; Probst et al.,,2019; Bartz et al.,,2023). We will accordingly use the term HP tuning in the remainder of this paper.
Note that HP tuning implies that not only the parametersare estimated from the data set, but also one or more HPs in.
HP tuning thus generally complicates model generation and evaluation, which will be described in more detail in Section4.Importantly, there are HPs that should not be selected through tuning.
For learning algorithms, this includes, for example, the number of trees () in the random forest algorithm for classification problems: Due to the monotonous relation betweenand model performance in most cases, the largest computationally feasible number of trees should be chosen(Probst and Boulesteix,,2018). Regarding preprocessing HPs, this typically applies to those associated with steps that alter the scope or interpretation of the prediction model (see Section2.2.2). As such steps require careful specification, the corresponding HPs should be set based on user expertise (i.e. data-independently) rather than determined through tuning.To indicate how the value of a HPhas been specified, we writeif the value is left at default value or selected independently of the data, andif the value was chosen through tuning.

SECTION: 2.4Model development processes

The development of ML-based prediction models generally involves two key processes: (i) the generation of the prediction model(model generation) and (ii) the evaluation of its predictive performance (model evaluation). Given our focus on HPs and their selection, we distinguish between two settings in the remainder of this paper. In Setting I, all HPs of the learning pipeline are pre-specified (i.e. either set to default values or selected independently of the data). In Setting II, one or more HPs are selected through tuning.Before explaining the principles and potential pitfalls of model generation and evaluation for both settings in Sections3and4, we first clarify their general concepts.

We refer to the model generation process as the set of processes required to obtain the final prediction model.
In Setting I, the model generation process consists of a single training process, where the parameters that define the final prediction model are estimated fromusing the learning pipelinewith pre-specified HPs.
In Setting II, where one or more HPs are selected through tuning, the model generation process consists of a tuning process conducted on(which yields the tuned HPs), followed by a training process, where, similar to Setting I, the parameters of the final prediction model are estimated fromusing the learning pipelinewith tuned HPs.

Once the final prediction modelhas been generated, the next important step is its evaluation. Since many algorithms yield black-box models that cannot be easily interpreted – and are thus difficult to assess for plausibility without additional tools (see, e.g.,Molnar,,2022) – a key quantity in the evaluation of a model is its prediction error. In the context of this work, we will accordingly use the term“model evaluation”synonymously with determining a model’s prediction error. The prediction error indicates how well a model performs on new observations that are independently drawn from the same distribution as the observations in(i.e. from). It is specified with respect to a loss function, which assesses the discrepancy between true outcomes and predictions and constitutes the performance measure. Formally, the prediction error ofcan be defined as

(Hastie et al.,,2009; Boulesteix et al.,,2015; Bischl et al.,,2023).
The loss functioncan be chosen according to the prediction problem being addressed.
For instance, a common choice forin regression problems is the squared loss. In this case, the prediction error reflects the well-known mean squared error (MSE). Note that in equation (2), we assume for simplicity thatcorresponds to a point-wise loss function, although many commonly used performance measures (e.g., the area under
the receiver operating characteristic curve, AUC) would necessitate a more general definition (provided inBischl et al.,,2023).
Nonetheless, all following statements regarding the prediction error hold regardless of this simplified (and more common) representation.An estimate of the prediction error in equation (2) can be obtained by usingto make predictions for an additional data set with new observations drawn from(referred to as test data set). The prediction error can then be estimated by evaluating the loss functionfor each observation and calculating the average across all observations (again, assuming a point-wise loss;Bischl et al.,,2023; Hastie et al.,,2009). The resulting prediction error estimate forcan be denoted as. Note that the outcome values formust be observed; otherwise, the loss functioncannot be evaluated.The requirement for an additional data set,, for model evaluation can be challenging in applications where data resources are limited. Denotingas the only available data set at the time of model generation and evaluation, there are two general approaches for definingand: (i) all available data are used for model generation, in which caseis inevitably a subset of(i.e.and), or (ii) the model is generated on a (proper) subset of the available data, with the remaining subset held back for model evaluation (i.e.and). For the first approach, there are several ways to define, each leading to a different evaluation procedure, which will be detailed in Section3.2(Setting I) and Section4.2(Setting II).Depending on the chosen evaluation procedure, a potential issue can be data leakage, which occurs whenever information about the designatedis improperly available during the generation of the model to be evaluated(Kapoor and Narayanan,,2023; Kapoor et al.,,2024; Kaufman et al.,,2012; Rosenblatt et al.,,2024; Hornung et al.,,2023). Since in this case, the observations inno longer truly represent new observations to which the model will be applied and the model thus has an unfair advantage when predicting these observations, the resulting prediction error estimate can be optimistically biased.Kapoor and Narayanan, (2023)identify three general types of data leakage, which may arise from: (i) overlap between the data used for model generation and evaluation, (ii) violation of the assumption that all observations are independently drawn from the same distribution, or (iii) use of illegitimate features. In this paper, we will focus on overlap-induced data leakage but provide additional information on the other two types in Supplementary Section A. Furthermore, we encounter an example of one of the other types in our empirical illustration in Section5.Finally, note that in some applications of ML (e.g., in the context of healthcare research), the process of assessing a model’s performance on observations fromis referred to as internal validation.
This is in contrast to external validation, which evaluates how well the model predicts observations from different distributions (e.g., different time points or healthcare settings;de Hond et al.,,2022;Collins et al., 2024a,; Van Royen et al.,,2023; Van Calster et al.,,2023).
As external validation is recommended to be performed in subsequent research only after successful internal validation(Collins et al., 2024a,), we will focus on internal validation in this paper.
Note that, in general, the term “evaluation” should be preferred over “validation” as the latter suggests that a “validated model” has a low prediction error, which is not necessarily the case(Collins et al., 2024a,).

SECTION: 3Setting I: Pre-specified HPs

In this section, we describe the model generation and evaluation process for Setting I. We accordingly assume that the learning pipelineis configured by HP values that are either set to their default values or selected independently of the data, i.e.. This aspect is emphasized by denoting the learning pipeline as.

SECTION: 3.1Model generation

As stated in Section2.4, the model generation process in Setting I consists of a single training process. Moreover, as already outlined,“training”refers to the learning pipeline estimating the parameters(which constitute the prediction model) from. For brevity, we will also refer to this process as “training the prediction model” although it is the learning pipeline that is being trained and subsequently yields the prediction model.Importantly, all parameters inmust be estimated, including those from preprocessing steps. The estimation of preprocessing parameters follows the sequence of their corresponding steps in the learning pipeline. This process is specified by the respective preprocessing step. For example, in the case of mean imputation, the corresponding parameter estimate is found by calculating the mean of all non-missing observations of the corresponding feature.The parameters of the learning algorithm are usually estimated based on a loss functionthat measures the discrepancy between the true outcome and a prediction vector for each observation, i.e..
The algorithm parameters are then found by minimizing(see, e.g,Bischl et al.,,2023orBartz et al.,,2023for more details). For example, in a regression problem where the learning algorithm corresponds to the CART algorithm, the splitting rules are found by minimizing the sum of squared errors and the prediction value for each terminal node corresponds to the mean of all outcome values in the respective node(Breiman et al.,,1984). Note that the loss functionmay, but does not necessarily have to, align with the loss functionfrom Section2.4.2, which is used to estimate the prediction error.When estimating the parameters, the learning pipeline may not only capture the signal inwhich represents the true underlying data-generating mechanism, but it may also erroneously learn the specific pattern of noise (i.e. unexplained variation) in. The resulting prediction model is too adapted toand will perform worse on new observations (drawn from) than on the observations in. This is a well-known problem in prediction model training and is commonly referred to as overfitting(e.g., Hastie et al.,,2009; Kuhn and Johnson,,2013; Steyerberg,,2019; Bischl et al.,,2023; Poldrack et al.,,2020; de Hond et al.,,2022). The risk of obtaining an overfitted prediction model depends on both the data set(specifically on its signal-to-noise ratio, which tends to decrease as the number of observations decreases), and on the learning pipelineused to train the model(Poldrack et al.,,2020; Lones,,2024).
The association between the characteristics of a learning pipeline and its tendency to overfit is not straightforward, but is related to factors such as the size of its hypothesis space (i.e. the number of prediction models that can be trained by) and the procedure by which the model is chosen from the hypothesis space (e.g., whether the hypothesis space is searched exhaustively;Domingos,,2012).
These factors can vary greatly between learning pipelines, especially depending on the type of learning algorithm and the chosen HP values.
Note that the learning pipeline may also suffer from underfitting rather than overfitting, which occurs if it is not flexible enough to adequately model the underlying data-generating mechanism(Hastie et al.,,2009).As mentioned above, after training the learning pipeline once (and only once) on, the generation of the final prediction model is completed.
This implies that if the model is found to have a poor predictive performance in the subsequent evaluation (e.g., due to over- or underfitting), the result either has to be accepted or the HPs of the learning pipeline have to be modified based on the evaluation result.
However, users should be aware that the latter approach corresponds to Setting II, which has different implications for model evaluation (Section4). We denote the final prediction model asto emphasize that it is the result of training a learning pipeline configured with HP values.

SECTION: 3.2Model evaluation

As outlined in Section2.4.2, evaluating the prediction modelrequires a test data set, which is used to estimate the model’s prediction error. In that section, it was also stated that evaluation procedures can be differentiated based on whether model generation (which corresponds to model training in Setting I) has been performed on all available data (withand) or only on a (proper) subset of the available data (withand). In the following sections, we examine the implications for model evaluation in more detail for both approaches. An additional graphical overview is provided in Figure2.

A straightforward way to evaluate a prediction model trained on all available data is to estimate its prediction error using the same data set, i.e.. The resulting prediction error estimate is referred to as apparent error (see Figure2, model evaluation a).
As explained in Section2.4.2, data leakage is present when information about the designatedis present during model generation.
For the apparent error, this is clearly the case, asis equal to. As a consequence, the apparent error is not able to detect any overfitting of the model (since the specific pattern of noise inexactly corresponds to that in) and will therefore be affected by a (possibly substantial) optimistic bias.
Although this evaluation procedure is well-known to be flawed and has been frequently warned against in literature(e.g., Efron,,1986; Kuhn and Johnson,,2013; Hastie et al.,,2009; Poldrack et al.,,2020;Collins et al., 2024a,), it is often still the only prediction error estimate that is reported in studies presenting new prediction models(Kapoor and Narayanan,,2023; Poldrack et al.,,2020).

To avoid the optimistic bias caused by the overlap betweenand, several procedures exist that partitionone or multiple times into two subsets for evaluation purposes, while still training the final prediction model on the full data set. These procedures can be referred to as resampling methods and the resulting estimate as resampling error (see Figure2, model evaluation b). The following description is based onSimon, (2007),Kuhn and Johnson, (2013),Bischl et al., (2023), andCasalicchio and Burk, (2024); see their work for more details.The simplest resampling method is the holdout or split-sample method, whereis randomly split into two subsets with different purposes: One subset, denoted as, is used to retrain the same learning pipelinethat has been used to obtain the final prediction model. This results in an additional prediction model, whose prediction error is then estimated on the second subset, which serves as. The holdout method essentially has two drawbacks, whose impact on the prediction error varies according to the split ratio and the absolute number of observations inand(denoted asand).
First, while the holdout method ensures a clean separation betweenand, it does not evaluate the actual prediction model trained onbut the additional prediction model trained on, which does not necessarily coincide with the former.
Since the additional prediction model is trained on fewer observations (i.e.), estimating its prediction error onyields a pessimistically biased estimate for the prediction error of.
Second, the smaller, the more the prediction error estimate varies depending on which observations are assigned to(i.e. the higher the variance of the holdout estimator).
As a consequence, specifying the split ratio for the holdout method requires a careful trade-off between bias and variance.A commonly-used variation of holdout is-fold cross-validation (CV), whereis randomly split intosubsets (or folds) of approximately the same size, with 5 or 10 being typical choices for.
Based on thesplits, the procedure described for the holdout method is repeatedtimes: In each repetition (in this context also referred to as resampling iteration), the learning pipeline is trained onsubsets of(constituting), and the prediction error of the resulting model is estimated on the remaining subset (constituting).
The final prediction error estimate is obtained by averaging theprediction error estimates, which leads to the CV estimator having a smaller variance than a holdout estimator with the same split ratio. However, the prediction error estimate resulting from CV is also pessimistically biased because the evaluated prediction models are again trained on less thanobservations, although this bias decreases with increasing().Other common resampling methods include repeated versions of holdout and CV (to reduce the variance of the corresponding estimator) and bootstrapping.
Repeated holdout and bootstrapping are similar in their execution, except that for repeated holdout, the observations constitutingin each resampling iteration are drawn without replacement, while they are drawn with replacement for bootstrapping.As stated above, all resampling methods require the learning pipeline to be retrained on one or multiple subsets, each of which is a (proper) subset of(i.e.). In this context, a flawed evaluation procedure would be to apply all preprocessing steps on the full data setand retrain only the learning algorithm onduring resampling. This “incomplete resampling”(Simon et al.,,2003)results in another form of data leakage, as in each resampling iteration, the observations in the respectivesubset have already been used to train part of the learning pipeline (i.e. the preprocessing steps). Incomplete resampling has been frequently warned against in the literature(e.g., Kapoor et al.,,2024; Poldrack et al.,,2020; Pfob et al.,,2022; de Hond et al.,,2022; Hofman et al.,,2023), and the resulting optimistic bias has been demonstrated by illustrations on real data(e.g., Hornung et al.,,2015; Rosenblatt et al.,,2024)and corrected reanalyses of published studies(e.g., Neunhoeffer and Sternberg,,2019; Kapoor and Narayanan,,2023). Yet, it still seems to be a common pitfall in the evaluation of prediction models (seeKapoor and Narayanan,,2023and references therein), which is probably caused by a lack of understanding of its implications. In addition, if the learning pipeline is not implemented as a single object that can be trained with a single function call such astrain(learning_pipeline)(e.g., this is possible inRwith themlr3orrecipespackage byLang et al.,,2019andKuhn et al.,,2024), each preprocessing step must be manually repeated in every resampling iteration. In such cases, users may consider incomplete resampling a time-saving shortcut, without realizing that it introduces data leakage. To avoid incomplete resampling, every component of the learning pipeline, including the preprocessing steps, must be retrained in each resampling iteration. The only preprocessing steps that can be safely applied to the full data set prior to resampling are those that are both parameterless and precede the first parameterized preprocessing step in the learning pipeline.

If the final prediction model has been trained on a subset of the available data (i.e.), its prediction error can be estimated using the remaining observations as(see Figure2, model evaluation c). This means that the training process does not need to be repeated, as there is no need to use resampling methods. Note that this procedure is technically equivalent to the holdout method introduced above, except that the model trained on, which corresponds toin the holdout method above, is the final prediction model and has not only been trained for evaluation purposes. Accordingly, the procedure is referred to as holdout or split-sample method as well, which can make it difficult to infer which procedure was used when the evaluation result of a model is reported. We use the terms temporary holdout (described in Section3.2.1) and permanent holdout (described here) to distinguish the two procedures.In principle, most points discussed in the previous section affecting temporary holdout (including data leakage due to incomplete resampling) also apply to permanent holdout. Again, the only difference is that, for the temporary holdout, the model trained on a subset of the available data is used solely for evaluation purposes, whereas it serves as the final prediction model for the permanent holdout. Consequently, the prediction error estimate derived from the permanent holdout is not pessimistically biased; instead, it is an unbiased estimate of a prediction error that is indeed higher (i.e. worse) than that of a model using all available data. Since not using all available data for training the prediction model essentially corresponds to a loss of important information, the permanent holdout method is only recommended if the number of observations inis sufficiently large, or if repeating the training process is computationally expensive or infeasible(Collins et al., 2024a,).

SECTION: 4Setting II: HPs selected through tuning

In this section, we review the model generation and evaluation process for Setting II, where one or more HPs are selected through tuning.

SECTION: 4.1Model generation

HP tuning generally aims to improve the predictive performance of a model(Bischl et al.,,2023; Probst et al.,,2019). Using the terminology introduced in Section2.4.2, this corresponds to finding the HP configuration that minimizes the model’s prediction error. To simplify notation, we will assume for now that all HPs are to be tuned, but will revisit the scenario where this does not apply later in this section. Under this assumption, the HP tuning problem can be formalized as:

whereis the final prediction model resulting from training the learning pipelineconfigured with HPs, anddenotes the theoretical optimum(Bischl et al.,,2023). The lowest prediction error (i.e. the best performance) that can be achieved usingas HP configuration depends on several factors, such as the HPs to be tuned, the selected learning algorithm, the performance measure, and the prediction problem in general(Probst et al.,,2019). Note that in the following, we refer to the prediction error of a model that results from training a learning pipeline determined by a candidate HP configuration, i.e., simply as the prediction error offor brevity. It should also be noted that equation (3) represents the standard case of single-objective HP tuning, i.e. the optimization is performed with respect to one performance measure. However, HP tuning can also be conducted based on multiple performance measures or additional criteria such as model simplicity(Bischl et al.,,2023; Dunias et al.,,2024).
Since such multi-objective HP tuning poses further challenges, we will only consider single-objective tuning in this paper.While there exist different tuning procedures, the general model generation process involving tuning can be described as follows:
given a set ofcandidate HP configurations (selected before or during the tuning process), each HP configuration() is evaluated onby employing one of the model evaluation procedures introduced in Section3.2.1. Accordingly,is split intoand(either once or multiple times), which are then used for training () and prediction error estimation (). In other words, the model evaluation that is performed once within Setting I to assess the prediction error of the final prediction model is performed multiple times for each candidate configuration (i.e. with) in the tuning process of Setting II. After having evaluated all candidate HP configurations, the HP configuration with the lowest (i.e. best) prediction error estimate is used as the final HP configuration. Following the notation introduced in Section2.3.4, we refer to this configuration as. Note thatis also commonly denoted as, since it is an estimate of(Bischl et al.,,2023). However, we adhere toto clearly distinguish it from Setting I, where. After setting, the learning pipelineundergoes a final training on, which yields the final prediction model.Note that while the tuning process already results in a prediction error estimate for the final prediction model (the estimate based on whichwas selected during tuning) this value is not necessarily adopted as the final model evaluation result, as we will discuss in Section4.2. In fact, it is also possible to use different performance measures for the prediction error estimation performed during tuning and the evaluation of the final model, but, for the sake of simplicity, we will assume that they are the same.To summarize, during the model generation in Setting II, both the HPsand the parametersof the final prediction model are optimized using the data set. However, the optimization is not performed jointly: first, the HPsare optimized in the tuning process. Second, the parametersare optimized in one (final) training process. Note that HPs are still an input of the learning pipeline, but can be seen as an output of the tuning process.If only a subset of the HPsare to be tuned, the tuning process described above is applied exclusively to those HPs, while the pre-specified HPs remain fixed throughout the process.
For example, assume that from allHPs in, the HPsare pre-specified and the HPsare to be tuned.
In this case, the tuning process yields a HP configuration, and the final prediction model is trained withand. Since the tuning process is conceptually the same when not all HPs are optimized – untuned HPs are simply kept fixed – we will continue to assume that all HPs are tuned to maintain notational simplicity.When choosing a tuning procedure, it is important to consider that the tuning process is limited in terms of both data availability and computation time:
First, as outlined above, each candidate HP configuration,, is evaluated using one of the evaluation procedures described in Section3.2.1for Setting I. As explained there, the specifiedandsubsets contain a limited number of observations (i.e.and) and could overlap, potentially leading to unreliable prediction error estimates for each. Second, the computational budget available for the tuning process is typically limited, which restricts both the number of evaluated HP configurations and the time spent evaluating each configuration (i.e. estimating its prediction error). Due to these limitations and the resulting trade-offs (discussed in more detail in Section4.1.3), choosing an adequate tuning procedure is often non-trivial. Yet, guidance is still lacking and many of the existing recommendations are based on rules of thumb rather than empirical benchmarks (seeBischl et al.,,2023for an overview). Inadequate tuning procedures can result in athat yields a final prediction model with worse prediction error than(potentially even worse than setting all HPs to their default values) and/or an overly time-consuming tuning process (i.e. a more efficient tuning procedure could have achieved the same prediction error in less time).

Before describing different tuning procedures in more detail, we note that their specification generally depends on whether the tuning process is fully automated or performed manually. We consider the tuning process as automated if the relevant tuning components only need to be specified as a function argument, which is possible in several ML software frameworks (seeBischl et al.,,2023for an overview). In contrast, we refer to the tuning process as manual if the candidate HP configurations are evaluated by repeatedly calling the same function(s), altering only the argument that specifies the HP configuration.Compared to automated tuning, manual tuning is more time-consuming, error-prone, and less reproducible, as it is usually an informal and unsystematic process. On the other hand, automated tuning is usually more difficult to implement and requires more programming expertise than manual tuning. As a consequence, although manual tuning is generally advised against(e.g., Bartz et al.,,2023; Bischl et al.,,2023), it is likely still a common yet often unreported approach in many ML applications(Hosseini et al.,,2020; Hofman et al.,,2023; Lones,,2024). Note that this may be particularly true for the tuning of preprocessing HPs: As discussed in Section2.3.3, preprocessing HPs are often not identified as HPs. Consequently, users trying out different preprocessing options might not be aware that this corresponds to (manual) HP tuning and could be automated. Moreover, if the HPs to be tuned include application-specific preprocessing HPs, the barrier to using automated tuning is further increased, as these HPs may not yet be integrated into the corresponding software and require custom implementation.As a consequence, given the potentially different characteristics of the tuned HPs (especially preprocessing HPsvs. algorithm HPs), we cannot rule out that in practice, they are selected by a combination of automated and manual tuning (see Section5.2.3for a concrete example).

As stated above, the selected tuning procedure will affect both the duration of the tuning process and the prediction error of the final prediction model. In the following, we will review the individual components that characterize each tuning procedure and describe how they impact the tuning process.

When tuning an HP, it is often not reasonable to consider all possible HP values (i.e. all values in). For example, this applies if certain values ofare already known to cause overfitting or convergence issues. Moreover, whenis a preprocessing HP,may not even be formally specified (see Section2.3.3).
To perform HP tuning, it is thus essential to specify a search spacefor each HP, whereis a bounded subset ofand determines the HP values that are considered for tuning(Bischl et al.,,2023).
For example, if the HPs of the CART algorithm,andwithand, are tuned, their search spaces could be defined asand. The (overall) search space of allHPs is denoted as.It is important to consider that defining a search spacerestricts the tuning process to finding the optimal HP configuration within, denoted as, and not within, i.e.. Given a search space, the tuning problem specified in equation (3) thus updates to

Choosing a search space involves the following trade-off: If the search space is too small, the prediction error achieved byandmay differ greatly.
On the other hand, if the search space is too large, this decreases the chance of finding(or a HP configuration that leads to a comparable prediction error) within a given computational budget(Bischl et al.,,2023).Note that in contrast to automated tuning, the search space is usually not formally specified when performing manual tuning and may be extended during the tuning process (e.g., when the user initially planned to try two preprocessing options but then comes up with an additional option during tuning).

Unless the specified search spaceis very small such as when only a few categorical HPs are tuned, evaluating all HP configurations in the search space can be computationally challenging, or even infeasible. For example, even ifandare the only HPs being tuned, with the search spaces as specified above andbeing searched in increments of 0.001,candidate HP configurations would need to be evaluated. Accordingly, one or several criteria must be specified to terminate the tuning process once it is met.
The trade-off to consider when choosing a termination criterion is that the tuning process should neither stop before finding, nor should it continue longer than necessary, which would result in an inefficient use of resources and, as we will discuss below, increase the risk of overtuning(Bischl et al.,,2023).In automated tuning procedures, commonly-used criteria are based on the number of evaluations or the runtime. However, additional criteria such as reaching a certain performance level or stagnation of performance might also be reasonable(Bartz et al.,,2023; Bischl et al.,,2023).
Similar termination criteria, though often more intuitive than formally specified, may also exist for manual tuning when, for example, the user stops searching when satisfied by the reached performance level or gives up searching after a certain amount of time.

Since in many cases, only a subset of all HP configurations in the search space can be evaluated before the tuning process is terminated, the way in which the sequence of evaluations is determined, also called search strategy or HPO algorithm(Elsken et al.,,2019; Bischl et al.,,2023), is another important component of the tuning procedure.
Search strategies can be characterized by several aspects, such as the amount of time they spend inferring new candidate HP configurations from already evaluated ones (known as the inference vs. search trade-off;Bischl et al.,,2023).
For example, search strategies such as evolutionary algorithms and Bayesian optimization consider the distribution and results of previously evaluated HP configurations to propose new configurations. In contrast, the commonly used random search strategy simply draws HP configurations from a predefined, typically uniform, distribution without taking into account past evaluations (see, e.g.,Feurer and Hutter,,2019,Bischl et al.,,2023, orBartz et al.,,2023for more details and other search strategies).
In the special case where only the set of selected features is tuned, a well-known automated search strategy is backward or forward feature selection (see, e.g.,Hastie et al.,,2009).Note that the described search strategies are formally used only in automated tuning, as there is usually no specified search strategy when tuning is conducted manually. However, the results of previous evaluations may still be considered in manual tuning when selecting new HP configurations to evaluate.

In automated tuning procedures, all HPs are usually tuned jointly, i.e. each evaluated HP configuration potentially considers different values of each HP. However, the HPs could also be tuned sequentially, i.e. the complete tuning procedure is repeated for each HP(Probst et al.,,2019; Waldron et al.,,2011). For example, in a setting with three HPs (i.e.),would be tuned first withandset to default, which yields. Then,is tuned withandset to its default. Finally,is tuned withand, yielding.
As sequential tuning does not consider any interaction effects between the HPs, it is generally less likely to yield acomparable tothan joint tuning.
On the other hand, sequential tuning demands less time, with the maximum number of evaluations increasing linearly rather than exponentially with the number of HPs to tune, as is the case with joint tuning. Hence, it could be a realistic approach for manual tuning.

As outlined above, the prediction error of each HP configuration considered for tuning can be estimated using one of the evaluation procedures described in Section3.2.1.
In principle, all issues discussed there also apply to the tuning context. However, instead of leading to potentially invalid performance claims about the final prediction model (which was the case in Section3.2.1), using an inadequate evaluation procedure for HP tuning initially only increases the risk of failing to select awith a (true) prediction error that is comparable to the prediction error of. In other words, if the prediction error of each candidate HP configuration is not estimated adequately, this will initially only affect the model generation process, but not (yet) the evaluation of the final prediction model. Still, the consequences can be detrimental.For example, if each HP configuration is evaluated based on its apparent error (i.e. for each, a model is trained and evaluated on, which also serves as), the tuning procedure will, due to the optimistically biased prediction error estimation, typically select the HP configuration that results in the model with the highest degree of overfitting. Although this approach should clearly be avoided, it might still be common practice in manual tuning as it is time-efficient (only one model per HP configuration needs to be trained, which in this case also corresponds to the final model) and may seem intuitive to inexperienced users.Due to the optimistic bias of the apparent error, the standard approach for automated HP tuning is to employ a resampling method.
In the case of-fold CV, which is a common choice for HP tuning(Bischl et al.,,2023), this means that for each candidate HP configuration,models are trained and evaluated on different subsets of.While resampling methods provide an improvement over using the apparent error, the corresponding estimators also exhibit a certain degree of pessimistic bias and variance (with the degree of bias and variance depending on the resampling method used, as discussed in Section3.2.1). A potential pitfall arising from the variance is that the winning HP configuration,, may have been selected simply because the trained prediction model(s) usingperformed particularly well by chance on the specified test data set(s), which are the same for each evaluated HP configuration. This means that the HP selection has essentially been overfitted to the respective test data set(s), which in this context is also referred to as overtuning, overhyping, or oversearching(Hosseini et al.,,2020; Quinlan and Cameron-Jones,,1995; Ng,,1997; Feurer and Hutter,,2019; Bischl et al.,,2023; Cawley and Talbot,,2010). If the true prediction error ofis still comparable to the prediction error of, overtuning effects are negligible. However, there might also be scenarios in which thetrueprediction error ofis no better, or even worse, than that of the default HP configuration, but itsestimatedprediction error is drastically deflated (i.e. over-optimistic), as the corresponding prediction model(s) that were trained during resampling incidentally fit very well to the specific noise pattern in the respective test data set(s). This has been demonstrated in several experiments where tuning was conducted on null data (i.e. data without any true signal), yet the prediction error estimate of the selected HP configurationwas substantially smaller (i.e. better) than its true prediction error indicating random prediction(Hosseini et al.,,2020; Varma and Simon,,2006; Boulesteix and Strobl,,2009; Bischl et al.,,2023).Note that since the HPs are overfitted to the test data set(s), which are not seen during training on the corresponding, overtuning occurs on a higher level than overfitting of the model parameters (see Section3.1). Accordingly, overtuning effects may only be visible after evaluating a large number of HP configurations(Bischl et al.,,2023). However, literature suggests that the risk of overtuning does not only depend on the number of evaluated HP configurations but also for example on the search strategy, the type of tuned HP, and the number of observations in(Hosseini et al.,,2020; Wainer and Cawley,,2021; Cawley and Talbot,,2010). In general, overtuning is considered an open problem of HP tuning, and although strategies have been suggested to avoid it (e.g., using different splits for each evaluation,Nagler et al.,,2024), there are no commonly agreed-upon solutions(Feurer and Hutter,,2019).Importantly, when overtuning is addressed in the literature, it is typically assumed that the prediction error estimation is performed through resampling methods. However, as discussed above, this estimation can alternatively be based on the apparent error. In cases where an inadequate HP configuration is selected due to the use of the apparent error for prediction error estimation, this can be considered a more extreme and direct form of overtuning since the test data set(s)are seen during model training. We will refer to the two types of overtuning as resampling-induced and apparent error-induced overtuning.

SECTION: 4.2Model evaluation

As outlined in Section4.1.1, the model generation process in Setting II results in a final prediction model.
Evaluating this model is generally more complex than evaluating a prediction model with pre-specified HPs (Setting I), since it must be taken into account that the model generation process involved HP tuning. Similar to Section3.2, we will in the following differentiate between cases in which the model generation (i.e. the HP tuning followed by a final training) is performed on the full data set (i.e.) vs. a (proper) subset of the available data (i.e.). A graphical overview of model evaluation in Setting II is provided in Figure3.

As in Setting I, reporting the apparent error for model evaluation is inappropriate in Setting II
(see Figure3, model evaluation a). In this case, however, the designated test data setis even used twice during model generation: first during the HP tuning process and then again in the final training process. Depending on the specific tuning procedure employed, this can introduce an even greater optimistic bias compared to, for example, using default HP values. Although the apparent error is generally not suitable for assessing a model’s performance, some users who performed tuning via resampling may mistakenly believe it now reflects a form of resampling error. This was noted byNeunhoeffer and Sternberg, (2019), who also reference a paper that appears to have fallen into this pitfall.

Similar to Setting I, an alternative evaluation procedure in Setting II is to employ a resampling method (see Figure3, model evaluation b). In principle, the chosen resampling method is carried out as described in Section3.2.1, except that in each resampling iteration, the model is trained onand evaluated onwithinstead of. Unfortunately, unlike in Setting I, using resampling methods for model evaluation in Setting II results in data leakage: Although in each resampling iteration,is not involved in training(the model trained onfor evaluation purposes), it is used in the tuning process performed on(including) to obtain. Accordingly, since not every model generation step resulting inis conducted exclusively on, information fromis available during the model generation process (specifically, during tuning). Based on the definition given in Section2.4.2, this constitutes a form of data leakage and may result in an optimistically biased resampling error(Wainer and Cawley,,2021; Hosseini et al.,,2020).
While the inadequacy of the apparent error is widely recognized, the described pitfall associated with the resampling error is less well known and will go undetected by those not involved in model development if HP tuning is not reported(Hosseini et al.,,2020; Lones,,2024).The potential optimistic bias becomes evident when considering the following typical practice: As outlined in Section4.1.1, the tuning process already returns a prediction error estimate for the final prediction model (the estimate based on whichwas selected). Given that tuning was performed with a resampling method (e.g., CV), computation time can be saved by directly using this value as the resampling-based evaluation result. However, if the selected HP configurationis the result of overtuning, this will not be detected in the model evaluation process, as the deflated prediction error estimate is simply adopted here.
In principle, adopting the resampling prediction error estimate from tuning in Setting II behaves analogously to (resampling-induced) overtuning as using the apparent error does to overfitting in Setting I. This is because both procedures are unable to discern that either the selected HPs (overtuning) or the selected parameters (overfitting) have been adapted too much to the respective test data set(s).As stated in Section4.1.3, the extent to which overtuning occurs depends on the specific tuning procedure. If the HP selection is mildly overtuned, the prediction error estimate obtained from the tuning process may only exhibit a slight optimistic bias. However, as an extreme case, we can again consider the experiments from Section4.1.3in which HP tuning has been performed on null data(Hosseini et al.,,2020; Varma and Simon,,2006; Boulesteix and Strobl,,2009; Bischl et al.,,2023). Here, the difference between the prediction error estimate of the selected HP configuration and the true prediction error indicating random prediction is substantial, and adopting the former as the final evaluation result for a useless prediction model is clearly a biased approach.Note that data leakage is also present if the specifiedandsubsets used for tuning and evaluation are not identical. This is the case if additional resampling iterations are conducted during evaluation, if different resampling methods are used during tuning and evaluation (e.g., holdout and-fold CV), or if the apparent error is used for tuning.

The optimistic bias of the resampling error arises because, in each resampling iteration, not all steps of the model generation process are performed exclusively on. A natural extension, therefore, is to ensure that the complete model generation is applied only toin every iteration (see Figure3, model evaluation c). Specifically, this implies that the tuning process is not only performed once onin order to generate the final prediction model, but also on everyspecified during resampling (for evaluation purposes).
If the tuning process itself is based on a resampling method (i.e. if tuning is not performed using the apparent error, which is hardly ever the case if the currently described model evaluation procedure is employed), this results in two nested resampling methods. Accordingly, this procedure is called nested resampling, where the resampling method that initially splitsintoandis the outer resampling loop and the resampling method creating additional splits within each(resulting in subsets denoted asand) is the inner resampling loop(e.g., Bischl et al.,,2023; Hosseini et al.,,2020; Wainer and Cawley,,2021). To distinguish nested resampling from the resampling methods discussed above and in Section3.2.1, we will refer to the latter as simple resampling where necessary.The most straightforward form of nested resampling is the nested holdout method, whereis split once intoand, andis further divided intoand. In this setup, the best HP configuration foris determined by training and evaluating a model for each candidate HP configuration on(for training) and(for prediction error estimation). We denote this configuration as, as it may differ from the final prediction model’s configuration,, which has been obtained by tuning the model onrather than. Using the HP configuration, the model is then trained onand evaluated on, which has remained unseen throughout the entire model generation process. Note that nested holdout is commonly referred to as train-validation-test split(Bischl et al.,,2023), which, using the notation above, could also be referred to as---split. Instead of holdout, any other resampling method can be used for inner and outer resampling, and it is also possible to combine different resampling methods. For example,-fold CV can be used for outer resampling and holdout for inner resampling, since in the inner resampling, precise prediction error estimation is less critical as long as a sufficiently goodis selected in each iteration(Bischl et al.,,2023; Hosseini et al.,,2020).While nested resampling prevents data leakage, it also has several disadvantages. First, it can be very computationally expensive, since the tuning process, which can already be time-consuming when conducted once, has to be repeated for eachspecified by the outer resampling loop(Wainer and Cawley,,2021; Bischl et al.,,2023).
Second, it is usually not feasible to conduct nested resampling with manual tuning. Apart from being even more time-demanding than nested resampling with automated tuning, it is often not possible to repeat the same tuning procedure more than once due to the informal nature of manual tuning (e.g., the user might not remember which candidate HP configurations have been evaluated during tuning). Third, like simple resampling, nested resampling does not provide an estimate of the prediction error for the final model. However, while both methods evaluate models trained onrather than(with), simple resampling at least uses the same HP configurationas the final prediction model. In contrast, nested resampling does not necessarily evaluate models with the same HP configuration, as each inner resampling loop may select a different configuration (see the nested holdout example above, which evaluates a model based oninstead of). This makes the nested resampling result more difficult to interpret(Hosseini et al.,,2020).
The described disadvantages could explain why nested resampling estimates are not commonly reported in studies presenting new prediction models, as indicated by a recent systematic review on clinical prediction models(Andaur Navarro et al.,,2023).

As in Setting I (see Section3.2.2), it is also possible in Setting II to use only a subset of the available data for model generation (i.e.), and reserve the remaining observations exclusively for evaluation (i.e.; see Figure3, model evaluation d;Hosseini et al.,,2020). This approach essentially corresponds to nested resampling with holdout as the outer resampling method, except that the holdout is permanent, meaning that the prediction model generated on(equivalent toin the previous section) serves as the final prediction model. Similar to Setting I, we thus distinguish the two evaluation procedures by referring to them as temporary outer holdout (described in Section4.2.1) and permanent outer holdout (described here). We also again note that there might be some confusion in the terminology, as a permanent outer holdout combined with a (temporary) inner holdout can, just like its temporary counterpart, also be referred to as a train-validation-test split.The statements regarding the temporary vs. permanent holdout in Setting I also apply to Setting II: Compared to the temporary outer holdout, the permanent outer holdout does not exhibit a pessimistic bias as it actually evaluates the final prediction model. However, this comes at the cost of not using all available data for model generation. Accordingly, the same recommendation as in Section3.2.2applies: a permanent outer holdout should only be employed if the number of observations inis sufficiently large or if it is computationally expensive or practically infeasible to repeat the model generation process. Note that the second point is particularly relevant in Setting II due to the increased effort of model generation(Collins et al., 2024a,).

SECTION: 5Empirical illustration of different model generation and evaluation procedures

In this section, we illustrate different procedures for model generation and evaluation and assess their impact on prediction error estimates from available vs. new data. We specifically focus on the selection of HPs and the potential for data leakage.

SECTION: 5.1Real-world prediction problem

Our illustration is based on a real-world prediction problem from the COMPANION study(Hodiamont et al.,,2022). This study aimed to develop a casemix classification for adult palliative care patients in Germany that considers the complexity of each patient’s palliative care situation to assign them to a class reflecting their resource needs. A casemix classification for palliative care patients has been deemed necessary, as the differentiation of patients based on their diagnosis, which corresponds to the current practice in Germany, has been found to be inappropriate for predicting resource needs in the context of palliative care. Despite yielding many important insights, the COMPANION project was ultimately unable to develop a prediction model with sufficient predictive performance, even after exploring various model generation approaches. However, this makes it a good example to illustrate how optimistically biased evaluation procedures can present prediction models in a more favorable light.To develop a casemix classification that relates patients’ resource needs to the complexity of their palliative care situation, the COMPANION team formulated a prediction problem where each observation represents a patient’s palliative care phase. The outcome, defined as the average cost per day in palliative care phase, serves as an empirical proxy for resource needs in the corresponding phase. The set of featuresintended to reflect the palliative care situation of each phase consists of (i) the type of palliative care phase (categorical), (ii) patient age (integer-valued), (iii) two cognitive features (confusion and agitation; both ordinal), (iv) the Australian Karnofsky Performance Score (AKPS;Abernethy et al.,,2005) that measures the patients’ functional status (ordinal), and (v) the Integrated Palliative care Outcome Scale (IPOS;Murtagh et al.,,2019), which is a score that is based on 17 ordinal variables covering physical symptoms, psycho-social burden, family needs, and practical problems. Accordingly, the number of features provided to the learning algorithm is. All types of data were collected by the clinical staff of participating palliative care teams.It is important to note that although the study aimed to identify a casemix classification, the continuous nature of the specified outcome variable (i.e. average cost per day) inherently makes the prediction problem a regression task. To ensure that the obtained prediction model still produces classes that are also interpretable and can be implemented in practice, a decision tree approach was chosen (e.g., using the CART algorithm, discussed in Sections2-4), despite potential limitations on predictive performance. In the resulting decision tree, each terminal node represents a casemix class (defined by the features that capture the complexity of the palliative care situation) and predicts the average cost per day for that class. Notably, decision trees were also used in the casemix classifications developed for palliative care patients in Australia(Eagar et al.,,2004)and the UK(Murtagh et al.,,2023), which served as the basis for many decisions in the development of the German casemix classification.The COMPANION study collected data from three palliative care settings (specialist palliative care units, palliative care advisory teams, and specialist palliative home care), with a casemix classification to be developed for each setting. In our illustration, we only consider the data from the specialist palliative home care setting. We apply several parameterless preprocessing steps to the raw data set, which correspond to those used in the COMPANION study and are considered as pre-specified in our illustration (e.g., the removal of dead patients; more details can be found in Supplementary Section B.2.1). The resulting data set contains 1,449 palliative care phases; descriptive statistics are provided in Table S1.Note that while our experimental setup described in the following section is based on the COMPANION study, not all aspects align with how the actual study was conducted, as some elements have been simplified or modified for illustrative purposes.

SECTION: 5.2Experimental setup

The aim of our study is to illustrate different model generation and evaluation procedures and examine their impact on prediction error estimates derived from available data compared to those obtained from new data.
Additionally, we examine how these estimates are affected by performance measure, sample size, and learning algorithm, resulting in a total of 96 distinct analysis settings. Before providing more details on these, we first outline the general procedure that is carried out for each analysis setting:

The COMPANION data set with 1,449 observations (i.e. palliative care phases) introduced above is randomly split into two subsets of equal size, which we denote asand(withand). We assume thatis the only data set available for both model generation and evaluation. Consistent with the notation used in previous sections, this implies. The desired output is a prediction model as described above (i.e. a decision tree that predicts the average patient costs based on several features reflecting the palliative care situation).

We useexclusively to generate and evaluate a prediction model. Although the specific procedure is determined by the analysis setting, each model is generated using all available data (which is already implied by referring to the available data as). The learning pipeline used for each training process and its HPs are described in Section5.2.2. Since the HP selection in the considered analysis settings can be either data-independent or achieved through tuning, we refer to the chosen HP configuration asrather thanorin the following to keep the notation general. Step (ii) results in a modeland an associated prediction error estimate, which we denote as. In an ML application,would be the reported error.

The prediction modelis evaluated on the second data set, which represents observations that are drawn from the same distribution as the observations inbut were unseen during the generation of. This step should therefore yield an unbiased estimate of the model’s prediction error, denoted as(however, see the note on clustering
in Section5.3and Supplementary Section B.5). Note that, in principle, the estimation ofresembles a permanent holdout approach, whereis held out during model generation. However, it is not truly a holdout, asis unavailable during model evaluation. This is also whyis not referred to as; throughout the paper, the notationis used exclusively for subsets of the available data.

Performing steps (i) to (iii) results in a vector, which includes the prediction error estimates derived from available and new data, respectively. By comparing these estimates, we can determine whethercorrectly reflects the predictive performance of the model or if it is affected by any form of bias. Ideally,should be equal to, indicating that the model evaluation conducted onyields an unbiased estimate prediction error estimate (although small differences do not necessarily indicate bias, asis also an estimate). To ensure that the difference between the two prediction error estimates is not driven by a specific data split, steps (i) to (iii) are repeated 50 times for each analysis setting (using the same 50 splits for each analysis setting). Since we consider 96 analysis settings and 50 repetitions of splitting the initial COMPANION data set, our illustration generatesvectors of. Note
that each analysis setting may produce 50 different prediction models, as in each repetition,contains different observations.The described setup is implemented in the software environmentR(R Core Team,,2022)using themlr3package framework(Lang et al.,,2019). While the COMPANION data set cannot be made publicly available, theRcode and the individual prediction error estimates can be found athttps://github.com/NiesslC/overoptimistic_trees.As stated above, we consider a total of 96 analysis settings. These result from a full factorial variation of four factors: two performance measures, two sample sizes, two learning algorithms, and twelve combinations of model generation and evaluation procedures (yielding the total ofanalysis settings).
The two considered sample sizes are (i)(the sample size ofafter splitting the original data set) and (ii)(half of the observations inbeing randomly deleted). Note thatis not affected by this variation and still hasobservations.
The two performance measures considered in our illustration are the Root Mean Squared Error (RMSE) and the coefficient of determination (), which are commonly used performance measures and have also been employed to evaluate other decision-tree-based prediction models for palliative care patients (Eagar et al.,,2004; Murtagh et al.,,2023; see Supplementary Section B.3 for more information on both performance measures). Note that in each analysis setting, we use the same performance measure for both the model evaluations performed during model generation (i.e. tuning) and the evaluation of the final prediction model. The two learning algorithms and twelve combinations of model generation and evaluation procedures are described in Sections5.2.2and5.2.3, respectively.

The learning pipelineapplied in each training process consists of six preprocessing steps, followed by a learning algorithm (see Figure4for an overview). While the full learning pipeline actually consists of more preprocessing steps (referred to in Section5.1and detailed in Supplementary Section B.2.1), we will for simplicity not further consider them in the illustration, as they are considered as pre-specified (i.e. have no HPs that are relevant for tuning) and are both parameterless and precede the first parameterized preprocessing step in the learning pipeline (i.e. can safely be applied to the full data set).

Here, we provide a brief overview of the six preprocessing steps inapplied during each training process and outline their associated HPs.

Additional details can be found in Figure4, and a comprehensive description is available in Supplementary Section B.2.2.The six preprocessing steps serve one of three purposes: (i) correction of the outcome variable (correction of costs), (ii) handling of problematic observations (removal of cost outliers and handling of“cannot assess”values in IPOS features), and (iii) calculation or modification of features (calculation of the IPOS score, modification of the feature“age”, and modification of the feature“AKPS”). As discussed in Section2.2.2, preprocessing steps can be distinguished based on different characteristics, which also applies to the six preprocessing steps considered in this section.
Two of the six steps have parameters: the correction of costs (with) and the removal of cost outliers (with). These two steps, along with another step (handling of“cannot assess”values in IPOS features), alter the outcome distribution, but the removal of cost outliers is not applied during prediction.All preprocessing steps, except for the correction of costs, include HPs:,,,, and. Consistent with the notation introduced in Section2.3.1, we collectively refer to them as. For these HPs, it is not possible to define a HP domainthat contains all possible configurations; therefore, we only specify a search spacefor each HP (see Figure4). Each search space is categorical, offering 2 or 4 values, all of which have been discussed and deemed reasonable during the COMPANION project.
The first HP value in each search space is set as the default and corresponds to the value ultimately selected for the COMPANION project.

After applying all preprocessing steps to the data, it is provided to the learning algorithm, which then yields a prediction model (i.e. a decision tree). We consider two learning algorithms: (i) the CART algorithm (introduced in Section2.2.1;Rpackagerpart;Therneau and Atkinson,,2022), and (ii) the Conditional Inference Tree algorithm (CIT;Rpackagepartykit;Hothorn and Zeileis,,2015; Hothorn et al.,,2006; Zeileis et al.,,2008). As stated in Sections2.2.1and3.1, the CART algorithm builds a decision tree model by partitioning the feature spaceinto terminal nodes using a sequence of binary splits. Since we are considering a regression problem, the splitting rules are determined by minimizing the sum of squared errors, and the prediction valuefor each terminal node is the mean of all outcome values (here: costs) in that node(Breiman et al.,,1984). The CIT algorithm also employs recursive binary partitioning but instead of minimizing a simple loss function that represents node impurity (here: the sum of squared errors), it uses statistical test procedures to find the optimal splits. This approach has the advantage that, unlike the CART algorithm, the CIT algorithm is not affected by selection bias toward features with many possible splits or missing values(Hothorn et al.,,2006).For both algorithms, we consider two HPs for tuning that determine when the algorithm stops splitting. The first HP is, which specifies the minimum number of observations in any terminal node. The smaller, the larger the number of terminal nodes in the resulting decision tree and the higher the risk of overfitting. We set the search space oftofor tuning. Ifis not tuned, we set the HP to its default,= 7. The second HP is either(for CART) or(for CIT). Both HPs serve a similar purpose:determines the factor by which a split must improve the overall lack of fit to be attempted (which, in case of a regression problem, corresponds to improving the overallof the model by at least). The HPis the numerical significance level that must be met in the statistical testing procedure conducted by CIT to implement a split. Accordingly, the smalleror the higher, the higher the risk of overfitting. We specify the search space forandasand, respectively. Ifandare not tuned, we use their default values ofand.All other HPs of CART and CIT are not tuned and, except for one HP, follow the default values from their corresponding implementation in themlr3package(Lang et al.,,2019), which largely align with the defaults of the underlying packages (i.e.rpartandpartykit;Foss and Kotthoff,,2024). The exception is, which we set to 4 to align with the COMPANION project, where this value was chosen to ensure that the resulting decision tree model would be useful in clinical practice.We refer to the algorithm HPs that are considered for tuning (i.e.andor) as. The remaining algorithm HPs that are not tuned in any of the analysis settings will not be considered further for simplicity.

We consider twelve different combinations of model generation and evaluation procedures that could be employed in step (ii) of our illustration (see Section5.2.1) to obtain a prediction model with associated. They represent an exemplary yet non-exhaustive selection of procedures that are used in ML applications. The twelve combinations are based on five model generation procedures, where for three of them, we apply two different procedures to evaluate the final prediction model, and for the other two, we use three different evaluation procedures (resulting in the total ofcombinations).Before describing the procedures in more detail, there are a few general points to consider. First, as already stated in Section5.2.1, all model generation procedures use the full data setthat was created by the respective repetition, i.e. we do not consider the permanent holdout evaluation procedures introduced in Sections3.2.2and4.2.2(which would imply). Second, since the prediction model used in this illustration is a decision tree, it is theoretically possible to manually assess the plausibility of the generated models in addition to estimating their prediction error. However, in addition to not being feasible for allgenerated models, this step is also often not part of the evaluation process in practice, as many ML-based prediction models are not interpretable by humans without additional tools. Therefore, we do not perform this assessment. Third, wheneveris (temporarily) split as part of a resampling method (either during model generation or evaluation), we use the same splits (e.g., the same 10 CV folds) across all procedures to ensure that differences in prediction error estimates are not due to variations in the data splits of.We now present the procedures in more detail, first describing the model generation procedure and then the associated evaluation procedures to estimate the prediction error of the resulting model. The following paragraph titles refer to the model generation procedures and can be read as“Setting - Tuning Procedure (- HPs tuned)”. An overview of all generation and evaluation procedures is provided in Table1.

The simplest model generation procedure corresponds to Setting I, where all HPs are set to their default values (i.e. no tuning is performed), and the learning pipeline only needs to be trained once on the data set.For this model generation procedure, we evaluate the resulting model by (i) the apparent error and (ii) the 10-fold CV error. The former is affected by data leakage and may thus exhibit a substantial optimistic bias (see Section3.2.1).

In this model generation procedure, the preprocessing HPs () are tuned, while the algorithm HPs () are set to their default values. It aims to represent inexperienced users who either lack the confidence or the programming skills to tune algorithm HPs but manually experiment with different preprocessing options, without realizing that this is a form of HP tuning. As discussed in Sections4.1.2and4.1.3, manual tuning procedures typically differ from automated tuning procedures, which is reflected by the procedure II-manual-P. First, the HPs are tuned sequentially (i.e. each HP is tuned individually, with previously tuned HPs set to their selected values and subsequently tuned HPs set to their default values).
Second, during the tuning of each HP, the apparent error is used to estimate the prediction error of each candidate HP configuration. The order in which the HPs are tuned sequentially is,,,,(which reflects a user who first experiments with variations in the features before removing observations, though any other order is also possible). If more than one HP value yields the same prediction error estimate, the first value that was evaluated is selected. Since the preprocessing HPs are tuned sequentially (i.e. one at a time), and only two (,) or four (,,) values per HP are available, only 16 () configurations ofneed to be evaluated during tuning. Therefore, no criterion is specified to terminate tuning before all configurations are evaluated.Similar to the first model generation procedure (I-no tuning), we consider the apparent error and the 10-fold CV error to evaluate the final prediction model. However, the 10-fold CV error is now affected by data leakage, potentially leading to an optimistic bias due to (apparent error-induced) overtuning (see Section4.2.1). Note that we do not consider evaluation procedures involving nested resampling for II-manual-P, as this is typically not feasible if manual tuning was used for model generation (see Section4.2.1).

This model generation procedure represents a standard procedure in many ML applications, where the algorithm HPsare selected through automated tuning, while the preprocessing HPsare set to their default values (e.g., because users are not aware that they can be tuned). Even when tuning is fully automated, the procedures used in practice are often simple and based on rules of thumb(Bischl et al.,,2023), which we aim to reflect in our illustration: we employ a random search algorithm, terminate the tuning after 60 evaluations (which corresponds to 30 times the dimension of the search space, as there are 2 HPs in), and use 10-fold CV for prediction error estimation. The tuning procedure is performed jointly for all HPs, which is the standard practice for automated tuning.As with the previous model generation procedures, we report both the apparent error and the 10-fold CV error. Note that, since the 10-fold CV error for the selected HP configuration,, has already been calculated during tuning, we use this value as the 10-fold CV error estimate of the final prediction model to avoid performing additional resampling iterations. Similar to the procedure II-manual-P, data leakage is present in both evaluation procedures and may result in optimistically biased prediction error estimates. Specifically, the optimistic bias in the 10-fold CV error would arise from (resampling-induced) overtuning.
Since the procedure II-automated-A is fully automated, we additionally estimate the prediction error using nested CV. Here, we use 10 folds for the outer resampling loop and 2 folds for the inner resampling loop (the small number of inner folds saves computation time and we only need to achieve correct HP selection rather than precise error estimation here; this is also recommended byBischl et al.,,2023). As discussed in Section4.2.1, this evaluation procedure is not affected by data leakage.

As a fourth model generation procedure, we tune both preprocessing and algorithm HPs (i.e.and), but with two different tuning procedures. More specifically, the preprocessing HPs are tuned as in II-manual-P, and for each candidate configuration of the preprocessing HPs, the algorithm HPs are tuned as in II-automated-A. Although this procedure might initially seem unintuitive and overly complex, it actually mirrors a realistic scenario for users who can tune algorithm HPs but may not be aware of or able to tune preprocessing HPs: Consider a user who has programmed three functions: (i)preprocess_data, which takes the raw data set as input and returns the preprocessed data set; (ii)tune_algorithm, which tunes the algorithm HPs as specified in II-automated-A based on the preprocessed data set and returns the selected HPs; and (iii)get_apparent_error, which takes the preprocessed data set and a learning algorithm with HPsas input and returns the apparent error of the resulting model. Suppose the user initially plans to run these three functions once but is dissatisfied with the apparent error reported byget_apparent_error. They would then modifypreprocess_datato try, for example, a different way of aggregating the IPOS score (i.e. using a different) and reruntune_algorithmandget_apparent_error. After testing all values for, they would proceed to adjust,, and so forth, updating the algorithm HPs by runningtune_algorithmbefore callingget_apparent_errorfor each tried preprocessing configuration. Note that since 16 configurations forare tried (see II-manual-P), and for each configuration of, 60 candidate configurations forare evaluated (see II-automated-A),HP configurations are assessed in total. The user would ultimately select the preprocessing HPsthat yield the best apparent error and the algorithm HPsreturned bytune_algorithmafter settinginpreprocess_data.For this model generation procedure, we again consider the apparent error and the 10-fold CV error to evaluate the resulting prediction model. Note that the apparent error estimate corresponds to the best apparent error achieved during tuning and can therefore be directly adopted for evaluation. More specifically, it is the output ofget_apparent_errorafter runningpreprocess_datawithand thentune_algorithm. The 10-fold CV error estimate can also directly be taken from the tuning procedure and corresponds to the 10-fold CV estimate which was calculated during the execution oftune_algorithmafter runningpreprocess_datawith. For the reasons discussed in the previous model generation procedures, both the apparent error and the 10-fold CV error estimates are subject to data leakage.

The final model generation procedure is similar to the procedure II-automated-A described above, except that the set of jointly tuned HPs now also includes the five preprocessing HPs,, and the number of evaluations is increased to 210. As in II-automated-A, this corresponds to 30 times the dimension of the search space, as there are now 7 tuned HPs. This procedure represents a conceptually simple way to incorporate preprocessing HPs into the tuning process and is recommended byBischl et al., (2023). However, as noted in Section4.1.2, integrating preprocessing HPs into an automated tuning procedure requires advanced programming expertise, which may explain why this procedure is not standard practice yet.We use the same three model evaluation procedures as in II-automated-A, with the same considerations discussed in II-automated-A also applying here.

SECTION: 5.3Results

Figure5illustrates the differences betweenandfor each of the 96 analysis settings (with 50 repetitions per setting). Additionally, the absolute values ofand, as well as the selected HPs (for analysis settings where HPs are tuned), are presented in Figures S2 to S6.Before examining the prediction error differences in more detail, we first consider the absolute values of(displayed in Figure S2). Here, the general observation can be made that across all analysis settings, none of the generated models demonstrates sufficient predictive performance, which was expected and aligns with the findings of the COMPANION project. Of course, this result does not imply that HP tuning is generally not useful; rather, it demonstrates that tuning alone is not a guaranteed solution for obtaining a well-performing model for any prediction problem. Even in the analysis settings with the best median prediction errors (averaged across 50 repetitions), the medianreaches only 0.074 for(, CIT, II-manual-P) and 42.1 for RMSE (, CIT, II-automated-PA). For reference, the medianfor RMSE using a naive model that predicts the mean ofonis 44.0 for the smaller sample size and 43.5 for the larger sample size, which is only slightly worse than the result from the decision tree models. While small effects of sample size and learning algorithm oncan be observed (with larger sample sizes and using the CIT instead of the CART algorithm resulting in smaller prediction errors), no clear pattern emerges for the model generation procedure.We will now analyze the differences betweenand. To ensure consistent interpretation of their signs across both performance measures, the prediction error differences in Figure5are presented asfor RMSE andfor. With this definition, a positive median difference indicates that the prediction error estimateis optimistically biased, while a negative median difference suggests a pessimistic bias.As stated in Section5.2.3, depending on the model evaluation procedure,corresponds to one of three prediction error estimates: (i) the apparent error, (ii) the 10-fold CV error, or (iii) the 2-fold-within-10-fold CV error. We structure the reporting of the results according to these three evaluation procedures.

Figure5shows that, across the considered model generation procedures, the median prediction error differences vary the most for the apparent error. Despite this variation, the median differences are consistently positive in all analysis settings. Although there are individual repetitions with negative differences, these results clearly indicate that the apparent error is optimistically biased. As discussed in Section3.2.1, this problem arises due to data leakage, or more specifically, the fact that this evaluation procedure uses observations for prediction error estimation that were already seen during model generation, which in turn allows potential overfitting and overtuning (if HPs are tuned) of the model to go undetected.The optimistic bias of the apparent error is most pronounced in analysis settings where the preprocessing HPsare tuned manually (II-manual-P). This is not surprising, as this procedure specifically selects the HP values that optimize the apparent error. Here, the bias is largest when the smaller sample size and the CART algorithm are used for model generation, resulting in a median difference of 7.39 for RMSE and 0.253 for. Note that while the absolute values ofstill do not indicate good predictive performance in these analysis settings (see Figure S2), the medianvalues resulting from the CART algorithm (0.234 and 0.176 for the two sample sizes) are comparable to the prediction errors reported for the Australian and UK decision tree models (0.17 and 0.27), which were generally deemed viable(Eagar et al.,,2004; Murtagh et al.,,2023). Regarding the selected HPs, particularly for(which specifies how the IPOS score is calculated) and(which determines how“cannot assess”values in IPOS features are handled), alternative values are frequently chosen instead of the defaults (see Figures S3a to S6a). This suggests that these alternative values may present a high potential for overfitting, thereby improving the apparent error.In the analysis settings where both the preprocessing and the algorithm HPs are tuned using different procedures (II-combined-PA), the optimistic bias of the apparent error is similar for the CIT algorithm or slightly smaller for the CART algorithm compared to the II-manual-P procedure. Again, the optimistic bias is largest in the analysis settings where a smaller sample size and the CART algorithm are considered, resulting in a median difference of 4.09 for RMSE and 0.117 for. The slight decrease in optimistic bias can be attributed to the fact that, across all analysis settings using the II-combined-PA procedure, the algorithm HPis set to a higher value than its default of, which results in a reduced risk of overfitting (see Figures S3b to S6b). In the analysis settings where no HPs are tuned (I-no tuning), the optimistic bias of the apparent error is also reduced slightly compared to the II-manual-P procedure. For the smaller sample size combined with the CART algorithm, the observed median difference is 6.21 for RMSE and 0.184 for. The reduction in optimistic bias compared to II-manual-P is expected, as I-no tuning does not involve HP tuning.The lowest optimistic bias for the apparent error is observed in the analysis settings where either only(II-automated-A) or bothand(II-automated-PA) are tuned automatically, with the largest median difference being 3.22 for RMSE and 0.035 for. This is not surprising, as in these procedures, all HPs are selected based on their associated CV error estimate, rather than the apparent error. Notably, across all analysis settings, the HP values forselected by the II-automated-PA procedure differ from those chosen by the II-manual-P and II-combined-PA procedures (see Figures S3a to S6a).

Ifcorresponds to the CV error, the resulting median prediction error differences indicate that this error is, as expected, generally less optimistic than the apparent error. The only exception occurs in a few analysis settings using RMSE as performance measure, where the apparent error differences are close to zero; here, the median differences of apparent error and CV error are approximately equal.In the analysis settings without HP tuning, thedifferences exhibit a negative median difference, with the median difference closest to zero, -0.059, observed for the smaller sample size combined with the CART algorithm. This pessimistic bias is an expected result, as CV evaluates models trained on fewer observations than the final prediction model (see Section3.2.1). In contrast to, the prediction error differences for RMSE in the analysis settings without tuning are mostly positive. Although the median differences are small (with the largest median difference being 2.32 in the analysis setting where both the smaller sample size and the CART algorithm are considered), the overall distribution of the prediction error differences in each setting suggests the presence of an optimistic bias. This finding is unexpected, as prediction errors estimated by CV in a setting where no HPs are tuned should not exhibit an optimistic bias, but rather a pessimistic bias (as observed for). However, this can be attributed to the fact that bothbased on CV andare affected by data leakage stemming from a violation of the assumption that all observations are independently drawn from the same distribution (see Section2.4.2and Supplementary Section A). This type of data leakage is distinct from the leakage caused by the overlap between the data used for model generation and evaluation, which is the primary focus of this paper. Specifically, the COMPANION data set exhibits a clustering structure that is not accounted for during the split intoandor during the creation of CV splits on, resulting in a potential optimistic bias for both(due to the initial split) and(due to the CV splits). Asis also subject to a larger clustering-induced optimistic bias than, the bias does not cancel out when taking their difference and is therefore evident in Figure5. Notably, the different levels of clustering-induced optimistic bias inandappear to have less impact on, where, as described above, the prediction error differences are mostly negative. Further details on the impact of the clustering structure on the results, including an explanation of why it was not considered when performing the splits, are provided in Supplementary Section B.5.The additional source of optimistic bias introduced by the clustering structure of the data is also relevant when interpreting the prediction error differences in the analysis settings with HP tuning. While our primary focus here is on overlap-induced data leakage that arises since the observations used for the CV-based error estimation have already been seen during HP tuning (thus hindering the detection of potential overtuning), we have to consider that any observed optimistic bias may as well stem from clustering-induced data leakage. Consequently, we compare the prediction error differences in analysis settings with HP tuning to those in settings without tuning (where only clustering-induced data leakage is present), rather than directly comparing them to zero. Based on this assessment, the impact of overlap-induced data leakage onappears to be limited. This is particularly true for RMSE, where the CV error differences are generally comparable to those resulting from the I-no tuning procedure. For, the median differences tend to be closer to zero compared to the I-no tuning procedure. In some analysis settings involving the smaller sample size and the CART algorithm, there is even a positive median difference (with the largest median difference of 0.018 observed in the setting where II-automated-PA is used in combination with the smaller sample size and the CART algorithm). Consequently, there appears to be a small overtuning effect that is not detected by the CV error due to overlap-induced data leakage. However, the median differences are too close to zero, and the variation within each analysis setting is too large to definitively determine which bias ultimately predominates, i.e. whether the CV error is overall optimistic or pessimistic in these settings.

In the analysis settings using the II-automated-A or II-automated-PA procedures for model generation, the prediction error differences of the nested CV error can also be analyzed. As expected, we observe the tendency for the nested CV error to be more pessimistic than the simple CV error (indicated by the smaller differences compared to the CV error; however, in some settings, the median differences for simple and nested CV errors are approximately equal). Although the nested CV error is not affected by the optimistic bias that may result from undetected overtuning effects (see Section4.2.1), the median differences for RMSE are positive, indicating the presence of an optimistic bias. As discussed above for the simple CV error, this is due to the clustering-induced optimistic bias, which appears to outweigh the pessimistic bias typically associated with nested resampling. In the analysis settings usingas performance measure, the distribution of the prediction error differences indicates that the nested CV error is pessimistically biased.

To summarize, the choice of model generation and evaluation procedure generally affects the difference between the prediction error estimates derived from available data and new data. As expected, when the evaluation procedure is based on the apparent error, the resulting estimate exhibits an optimistic bias, which varies depending on the model generation procedure.
As likewise expected, the simple CV error is less optimistic than the apparent error, while the nested CV error is even less optimistic. The corresponding prediction error differences are less variable across model generation procedures compared to the apparent error. For simple CV, this indicates that, in the considered experimental setup, the tuning procedures do not introduce relevant overtuning effects on error estimation. Instead, the main source of bias for simple CV is either the clustering-induced optimistic bias (or more precisely, the different bias level relative to) or the pessimistic bias arising from the use of fewer observations during evaluation. This also holds true for the nested CV error.

SECTION: 6Discussion and conclusion

This paper reviewed and empirically demonstrated the implications and potential pitfalls of HP tuning in the generation and evaluation of prediction models from the perspective of applied ML users, with a specific focus on the distinction between preprocessing and algorithm HPs.While HP tuning is generally a powerful tool for improving model performance, it also introduces potential sources of error. In the model generation process, failing to select an adequate tuning procedure can result in a prediction model that performs no better, or even worse, than a model using default HP settings. During model evaluation, failing to properly account for HP tuning can lead to optimistically biased prediction error estimates. The risk of such errors is especially high for preprocessing HPs, as they are often tuned subconsciously.To provide different examples of model generation and evaluation procedures in the context of HP tuning, and to examine their impact on the difference between prediction error estimates from available and new data, we conducted an illustrative study using a real-world prediction problem from palliative care medicine. Although both the apparent error and CV error can, in theory, be optimistically biased when HPs are tuned, this was consistently true only for the apparent error (with the highest optimistic bias occurring in analysis settings that imitated manual tuning of preprocessing HPs without considering algorithm HPs). In contrast, the prediction error differences for the CV error appeared not to be considerably compromised by data leakage, as these differences were comparable to the analysis settings without HP tuning.In addition to explicitly considering preprocessing HPs and manual tuning procedures, our illustrative study stands out from other investigations on HP tuning by not only using real data but also building most of the setup (including the learning pipeline, HPs, and performance measures) on a real-world project. While this ensures that the observed results are realistic and not derived from overly simplified or extreme setups, they are not generalizable beyond this specific context because the considered real-world project and the derived setup are not representative of other ML applications. By using real data, our illustration was also limited in that we could only compare the prediction error estimates from the available data set to those from a new data set (which, due to the clustering structure, was also over-optimistic) instead of comparing it to the true prediction errors. Nevertheless, it was still possible to compare differences across analysis settings and derive tendencies. Finally, the illustration could have been extended by treating the learning algorithm as a tunable HP. However, with the given setup, doing so would offer limited insights, as it is reasonably predictable that the resampling-based tuning procedures would select the CIT algorithm, while the tuning procedures based on the apparent error would favor the CART algorithm.Based on these conceptual and empirical insights, it is clear that to ensure HP tuning becomes a benefit rather than a pitfall, applied ML users must take care throughout the entire model development process. First, they should thoroughly consider which HPs (including preprocessing HPs) are to be tuned and which are not. An adequate tuning procedure should then be specified that fits the specific prediction problem. Unfortunately, this is typically non-trivial, as it depends on various factors such as sample size and the specific HPs to be tuned. More research is needed to better guide users in this respect (seeBischl et al.,,2023for an overview of current recommendations). In general, it is recommended to use automated tuning procedures instead of manual ones (see againBischl et al.,,2023for automated tuning implementations inRandPython). If automated tuning is not feasible, users should at least ensure that the manual tuning procedure is error-free, reproducible, and resampling-based. For model evaluation, only two evaluation procedures are guaranteed to be unaffected by data leakage caused by HP tuning: (i) nested resampling (if the entire data set is used for model generation), or (ii) a permanent (outer) holdout (if only a subset of the available data is used for model generation). However, similar to the tuning procedure, there is a lack of guidance on how to choose between these approaches and how to specify them (e.g., which resampling methods to use for nested resampling).
Although simple resampling may turn out to be a viable option in some applications (including our example), this can generally not be known in advance. Therefore, we discourage its use in settings involving HP tuning, as well as any other evaluation procedures that could result in data leakage.Regardless of how model generation and evaluation are performed, it is essential that they and all other relevant details (e.g., the complete learning pipeline and its HPs) are transparently reported in both code and text form. For this purpose, users may rely on checklists such as REFORMS (Kapoor et al.,,2024; intended for all applied research fields using ML) or TRIPOD-AI (Collins et al., 2024b,; intended for clinical prediction models). While transparency does not imply correctness, it allows readers to identify potential issues such as data leakage, and to critically interpret the claimed model performance. Moreover, it emphasizes the existence and importance of preprocessing and its HPs, while the current lack of transparency can create the impression that the data were not preprocessed at all or that no alternative preprocessing options were explored. To further enhance transparency and encourage applied ML users to be more intentional about their choices, it is also possible to preregister the entire model development process, for example, by using the template proposed byHofman et al., (2023).In conclusion, by addressing the implications and pitfalls of HP tuning from an applied perspective and emphasizing often-overlooked aspects, we hope that this review can further enhance the quality of ML-based predictive modeling.

SECTION: Funding Information

This work was supported by the German Research Foundation (BO3139/9-1, BO3139/7) to ALB. The authors of this work take full responsibilities for its content.

SECTION: Acknowledgments

The authors thank Patrick Callahan for language corrections and Julian Lange for useful literature input.

SECTION: Conflicting interests

The authors have declared no conflicts of interest for this article.

SECTION: References

SECTION: Supplementary Material

SECTION: Appendix AOther leakage types

As stated in Section 2.4.2,Kapoor and Narayanan, (2023)identify
three general types of data leakage, which may arise from: (i) overlap between the data used for model generation and evaluation, (ii) violation of the assumption that all observations are independently drawn from the same distribution, or (iii) use of illegitimate features. While our paper primarily addresses overlap-induced data leakage, we will now provide additional details on the other two types.

SECTION: A.1Violation of the i.i.d. assumption

In the following, we first consider the case of Setting I withand discuss the implications forand Setting II afterwards.Even with a strict separation between the data used for model generation and evaluation, achieved through the use of resampling methods, data leakage can still occur if the assumption that all observations inare independently drawn from the same distribution is violated. This assumption, also known as the i.i.d. assumption, was stated in Section 2.1. Non-i.i.d. settings may, for example, arise whenis a clustered data set, i.e. when the observations originate from different clusters (e.g., study centers). Observations within clusters are typically more similar than observations between clusters, where similarity can refer to both the feature vectoror the outcome(Hornung et al.,,2023). If the prediction model is intended to be applied to observations from other clusters than those present inin the future, resampling methods that are based on random sampling (i.e. ignoring the cluster structure) will be optimistically biased since in each resampling iteration, the observations inare more similar tothan observations originating from new clusters(Hornung et al.,,2023; Kapoor and Narayanan,,2023; Rosenblatt et al.,,2024). Although the level of optimistic bias depends on the specific clustering structure (e.g., cluster size and correlation within clusters), it is generally recommended to perform grouped resampling at cluster level, where all observations in a cluster are either assigned toorin each resampling iteration(Hornung et al.,,2023; Bischl et al.,,2023). In the context of healthcare research, this type of resampling is referred to as internal-external validation(Collins et al., 2024a,; Debray et al.,,2023). For other examples of non-i.i.d. settings and corresponding resampling methods, seeHornung et al., (2023)and the references therein.Our elaborations also apply to the case of Setting I with, with a permanent holdout used instead of a (temporary) resampling method; here, one simply replaceswithandwith.In Setting II, where resampling is typically used for both model generation (tuning) and evaluation, data leakage due to the violation of the i.i.d. assumption biases the prediction error estimate of the final model only when the non-i.i.d. data structure is ignored during model evaluation. This occurs specifically in the outer resampling loop of nested resampling (for) or in the permanent outer holdout (for). However, it is recommended to also take into account the non-i.i.d. data structure during tuning, both for the final prediction model and, if nested resampling is used, within the inner resampling loop, to ensure consistency(Hornung et al.,,2023).

SECTION: A.2Use of illegitimate features

Ifandinclude features that are generally not available for new observations to which the model will be applied in practice, these features can be considered illegitimate, and if included in the final prediction model, constitute another type of data leakage. An example raised byKapoor and Narayanan, (2023)is the use of anti-hypertensive drugs as a feature for predicting hypertension. Note that this type of data leakage is conceptually different from the other two types, as it stems from a design issue that is independent of the model evaluation procedure.

SECTION: Appendix BAdditional information on the empirical illustration

SECTION: B.1Descriptive statistics

TableS1provides descriptive statistics of the COMPANION data set used in the empirical illustration.

SECTION: B.2Preprocessing steps

In the following, we describe the parameterless and pre-specified preprocessing steps that are applied to the full COMPANION data set in its rawest version available. Note that the raw data set is on patient contact level, which was the unit for data collection(Hodiamont et al.,,2022). The initial preprocessing steps are:

data cleaning steps (e.g., correct variable types and labels),

the removal of contacts with palliative care phase“bereavement”, AKPS(“dead”), or costs,

the aggregation of the contact level data into palliative care phase level data (the outcome is constructed by summing the costs of all patient contacts and dividing by the number of days in the corresponding phase; for features that may vary during a phase, the highest value of the first day is used),

the removal of palliative care phases (one phase with an extreme and implausible cost value is removed; phases with“missing”values in either one or both cognitive features or in one of the individual IPOS features are removed; phases with“missing”or“cannot assess”in the AKPS feature are removed), and

the replacement of“cannot assess”values with“absent”in the two cognitive features.

These preprocessing steps yield a data set with 1,449 observations.

In this section, we detail the six preprocessing steps of the learning pipelinethat is applied in each training process, including their associated HPs. An overview of these preprocessing steps is given in Figure 4.

As stated in Section 5.1, the outcome variableis defined as the average cost per day in palliative care phase, which is intended to reflect the resource needs in that phase. This variable is calculated based on the staff time used to care for a patient and their relatives on each day of the corresponding palliative care phase. However, analyses have shown that if a palliative care phase is the first phase in an episode of care (see Supplementary SectionB.5for more information on episodes of care), the staff time and thus the costs of the first day are increased regardless of the complexity of the palliative care situation (e.g., due to time-consuming admission interviews). For this reason, the first-day costs of the first phase of an episode are adjusted using a factor based on comparisons with the costs of the first days in later phases of an episode. This factor is initially calculated for each palliative care team and then averaged to obtain a single overall correction factor, denoted as. This preprocessing step accordingly includes a parameter that must be estimated from the data set, though it does not involve any HPs in our illustration. Moreover, it is a step that modifies the outcome (albeit slightly), not for compatibility with the learning algorithm, but to change the interpretation of the prediction model, which now intends to predict a corrected version of the outcome. Accordingly, this step is also applied during prediction.

The distribution of the outcome variable in the COMPANION data set is right skewed, i.e. some palliative care phases have exceptionally high costs (see TableS1). Since it is not possible to definitively attribute these values to data entry errors, they are not permanently removed from the data set.
However, since the prediction values calculated by the corresponding decision tree algorithm in each terminal node can be sensitive to outliers, removing cost outliers during the training process could improve model performance. Importantly, this preprocessing step is only applied during training and not during prediction, i.e. when the final prediction model is used to make predictions on a data set, no cost outliers are removed. Removing them during prediction could artificially improve the model’s performance, as cost outliers are typically difficult to predict correctly(see also Kapoor and Narayanan,,2023).The definition of outliers is generally not straightforward, as many possible options exist(Kuhn and Johnson,,2013; Steyerberg,,2019). We denote the corresponding HP as. In our illustration, we define all cost values higher than theth cost percentile as outliers, with. If(the default value), no outliers are removed. Note that this preprocessing step includes the parameter, which corresponds to the percentile calculated according to.

As outlined in Section 5.1, the set of features to generate the prediction model includes the Integrated Palliative care Outcome Scale (IPOS;Murtagh et al.,,2019), which is a score based on 17 individual features covering physical symptoms, psycho-social burden, family needs, and practical problems. Each of the 17 features is ordinal and can take values from 0 to 4, where 0 and 4 correspond to the least and highest symptom or concern severity, respectively. For example, for the features IPOS-“Pain”and IPOS-“Shortness of Breath”, a value of 0 corresponds to “not at all” and a value of 4 corresponds to “overwhelmingly” (see FigureS1for an overview of all 17 features).

In its default version (see the next preprocessing step), the IPOS score is constructed by summing all 17 features, resulting in a score that ranges from 0 to 68. However, each IPOS feature also includes missing values, which are either due to missing data entries (coded as “missing”) or because the response option“cannot assess”was selected during the IPOS assessment.
For example, assessing whether a patient is burdened by pain (IPOS-“Pain”) can be challenging for clinical staff if the patient is comatose.While observations affected by the first type of missing values (“missing”) do not occur often and are removed as part of the initial preprocessing steps described in SectionB.2.1, handling the“cannot assess”values is more challenging. If all observations with at least one“cannot assess”response were removed, almost half of the COMPANION data set would be discarded (see TableS2; this would also apply approximately to any subsetorof the COMPANION data set). To avoid the loss of valuable information, an alternative approach is to treat“cannot assess”values as 0 (i.e. least symptom or concern severity), based on the assumption that an unobserved burden does not initiate a care mandate and therefore does not result in costs. However, it is not clear whether this assumption is valid for observations where many or even all IPOS features are recorded as“cannot assess”(e.g., if 15 out of 17 IPOS features are recorded as“cannot assess”, these features might not have been assessed at all). It could thus be a reasonable approach to set“cannot assess”values to 0 but exclude observations with many“cannot assess”values, as they potentially result in incorrect IPOS scores. Specifying the exact threshold for the maximum number of“cannot assess”values is, however, not straightforward. It can be denoted as HP, and ranges from 0 to 17 (observations with more than“cannot assess”values are removed; if= 17, no observations are removed). In our illustration, we consider the valuesfor, withbeing the default.This preprocessing step does not have any parameters. Since it removes observations, it modifies the distribution of the outcome variable. We argue that if observations with more than“cannot assess”values are found to yield unreliable IPOS scores, the resulting prediction model should not be used for future observations where this criterion applies, implying that the corresponding preprocessing step alters the scope of the model (such that it cannot be used for observations with more thanIPOS features recorded as“cannot assess”). Accordingly, this step is also applied during the prediction process. As shown in TableS2, the change in the outcome distribution is, however, minimal because the values considered forremove only a small number of observations (9 observations forand 0 observations for) from the full COMPANION data set with 1,449 observations. As discussed in Section 2.3.4, it is recommended to specify HPs of preprocessing steps that affect the outcome distribution based on user expertise rather than tuning. However, given that this step only removes a few observations and because specifyingbased on user expertise is challenging, we argue thatcan be tuned.

After removing observations based on their individual IPOS feature values, the next preprocessing step is to construct the IPOS score from these features. Aggregating the individual IPOS features into an IPOS score can be done in several ways, and we denote the corresponding HP as.
A straightforward and commonly used option is to simply sum the values of all 17 IPOS features, which we denote as IPOS-total (the default of).Instead of aggregating all 17 IPOS features into one score, it is also possible to generate multiple IPOS scores based on the subscales in which the features can be divided(Murtagh et al.,,2019). These subscales are: (i) physical symptoms (10 features), (ii) emotional symptoms (4 features), and (iii) communication/practical issues (3 features) (see FigureS1). In our illustration, we consider the generation of two subscale scores: one score that sums the features corresponding to the physical symptoms (IPOS-physical;) and one score that sums the remaining features (IPOS-others;). Note that in this case, the number of features provided to the learning algorithm increases fromto.A third option to construct the IPOS score is to sum all 17 IPOS features as in the IPOS-total score, but recode them (before summing) as 1 if their value is(i.e. takes one of the two most extreme values), and 0 otherwise.
This score will be referred to as the IPOS-extreme score and ranges from 0 to 17. It was developed by the COMPANION team and was motivated by the possibly too strict assumption made by the previous preprocessing step, namely that“cannot assess”values are equivalent to a value of 0. This assumption is relaxed by the IPOS-extreme score, which only requires assuming that the true value of an IPOS feature recorded as“cannot assess”isand not necessarily equal to 0.The fourth considered IPOS score option is similar to the IPOS-extreme score, except that the features IPOS-“Pain”and IPOS-“Shortness of Breath”are excluded from the score (which now ranges from 0 to 15) and are instead provided separately on their original ordinal scale to the learning algorithm. The motivation for this version is that pain and shortness of breath may be strong predictors of the costs associated with a palliative care phase. Therefore, model performance might be improved by including IPOS-“Pain”and IPOS-“Shortness of Breath”as individual features rather than aggregating them into the IPOS-extreme score.
If this IPOS option is used, the number of features provided to the learning algorithm increases fromto.This preprocessing step does not have any parameters. Moreover, it does not alter the outcome distribution, which is why it is applied during both training and prediction.

In the COMPANION data set, age is measured on an integer scale and ranges from 23 to 102 years (see TableS1). In its default configuration, this feature is provided to the learning algorithm on its original integer scale, without any preprocessing. Alternatively, it could be transformed into a categorical feature with six categories, using the years 50, 60, 70, 80, and 90 as cutpoints. This option could improve the model’s prediction error, as, for example, the CART algorithm suffers from a selection bias towards features with many possible splits(Hothorn et al.,,2006). We refer to the HP that specifies the used option as, with no modification of age as default. This preprocessing step has the same characteristics as the aggregation of individual IPOS features into a score (i.e. no parameters, applied during training and prediction).

The Australian Karnofsky Performance Score (AKPS;Abernethy et al.,,2005), which measures patients’ functional status on an ordinal scale, takes values ofin the COMPANION data set, with AKPScorresponding to “comatose or barely rousable” and AKPSto “able to carry on normal activity; minor sign of symptoms of disease” (see TableS1). In its default configuration, AKPS is considered ordinal, with the three highest categories, 70, 80, and 90, merged due to their low frequency. However, it might also be reasonable to transform AKPS into an unordered categorical variable, as costs may not monotonically decrease or increase with AKPS, but could be highest when the patient has, for example, an AKPS of 50, which corresponds to“considerable assistance and frequent medical care required”. In this case, we collapse the AKPS categories even further to avoid overfitting, resulting in AKPS. We refer to the corresponding HP as, with the ordered AKPS variable as default. This preprocessing step has the same characteristics as the two previous preprocessing steps (i.e. no parameters, applied during training and prediction).

Note that for the preprocessing steps estimating parameters from the available observations (i.e. correction of costs, with, and removal of cost outliers, with), their position in the preprocessing pipeline in relation to the steps where observations are removed (i.e. removal of outliers and handling of“cannot assess”values) is of relevance since a different set of observations might yield a different parameter estimate. Accordingly, performing the preprocessing steps in a different order could lead to (slightly) different results.Moreover, during the execution of the illustration as described in Section 5.2.1, in some resampling iterations performed during model generation and evaluation (particularly for nested CV), it occasionally happens that certain ordinal or categorical features in the data subset for which predictions are being made contain new values that were not encountered during training. This issue occurs exclusively with the highest and/or lowest values of these features, which are less frequent in the original COMPANION data set and thus more likely to be absent in the training set. Specifically, this affects the highest value of (cognitive) agitation, the highest and lowest values of AKPS (if AKPS is not collapsed into three unordered categories), the lowest value of age (if age is transformed into a categorical feature), and the highest values of“Pain”and IPOS-“Shortness of Breath”(if the fourth option for aggregating the IPOS score is selected). In these cases, we collapse the highest and second highest and/or lowest and second lowest values when making predictions.

SECTION: B.3Performance measures

In the illustration, two performance measures are considered: RMSE and. The RMSE is obtained by taking the square root of the MSE (see Section 3.1) and is expressed in the same units as the outcome variable (i.e. costs in €). It ranges from 0 to, whereindicates perfect prediction. Theperformance measure is calculated by dividing the squared error of the prediction model by the squared error of a naive model that predicts the mean and then subtracting this ratio from 1. It is a relative measure that can be interpreted as the proportion of variance in the outcome variable explained by the prediction model. The range ofis, withindicating perfect prediction and avalue of 0 or less indicating that a model performs no better or worse than the naive model, respectively. In this context, a lower prediction error corresponds to a highervalue. See, e.g.,Kuhn and Johnson, (2013)for more details on both performance measures.

SECTION: B.4Absolute prediction error estimates and selected HPs

SECTION: B.5Clustering structure

In Figure 5 (Section 5.3), which presents the prediction error differences for 96 analysis settings, it can be seen that the CV error unexpectedly exhibits an optimistic bias in settings without HP tuning. The same observation applies to the nested CV error in analysis settings with HP tuning. These results can be attributed to the clustering structure of the COMPANION data set, and we will explain this in more detail below. Specifically, we will describe the clustering structure (SectionB.5.1), explain how it impacts the estimated prediction errors (SectionB.5.2), discuss why the experimental setup was not adapted to account for this clustering (SectionB.5.3), and present an additional extension of the experimental setup with respect to clustering (SectionB.5.4).

The COMPANION data set exhibits a nested clustering structure. At the first level, clustering arises because several palliative care phases may originate from the same episode of care of a patient. An episode of care is defined as the period between admission to a specific specialist palliative care setting and the termination of care in that same setting. At the second level, clustering occurs because the episodes of care in the data were collected from different palliative care teams. Episodes within the same team are typically more similar to one another than to episodes from different teams. Since no episode of care is associated with more than one palliative care team, the clustering follows a nested structure.As a result, the 1,449 palliative care phases reported for the COMPANION data set in Section 5.1 originate from 705 episodes of care, which in turn are collected from 9 specialist palliative home care teams.

A more detailed depiction of this nested clustering structure is provided in FigureS7.

While our empirical illustration and the paper as a whole focus on overlap-induced data leakage, the clustering structure of the COMPANION data set introduces another form of leakage that generally occurs when the assumption of independent and identically distributed (i.i.d.) observations is violated and the violation is not accounted for during model evaluation. This type of leakage is briefly mentioned in Section 2.4.2 of the main paper and described in more detail in Supplementary SectionA.1. As a result, the prediction error estimates can be optimistically biased, even in the absence of overlap-induced data leakage. We now explain where the clustering is not accounted for in the experimental setup and how this affects the estimated prediction errors and their differences.First, the clustering structure is ignored when splitting the COMPANION data set intoand, as the split is performed at the phase level rather than at the episode or team level. Consequently, if the prediction model is intended to be applied to new episodes and teams not present in the COMPANION data set,is optimistically biased, as it has an unfair advantage compared to other data sets with new episodes and teams. A more precise statement in step (iii) in Section 5.2.1 would thus be thatis unbiased except for a potential optimistic bias caused by clustering-induced data leakage. Second, ifis estimated via simple or nested CV, the clustering structure is also ignored when creating the CV splits. Accordingly, as with, this leads to an optimistic bias indue to data leakage induced by clustering (although in contrast to,may also be affected by other biases). Note that for nested CV, it is only the ignoring of the clustering in the outer CV loop that results in the optimistic bias, as the inner splits are only used for tuning.For the difference betweenand, which is the focus of our illustration, this has two key implications:
Ifresults from an analysis setting where the apparent error was used to evaluate the final prediction model, the difference betweenandmay underestimate the optimistic bias that would arise ifcontained exclusively observations from new episodes and teams not present in.
Ifcorresponds to the simple or nested CV error, the clustering-induced optimistic bias would – under the assumption thatandare subject to the same level of bias – effectively cancel out when considering the difference betweenand. However, as shown in Figure 5, this is not the case.
Further analysis (not shown) reveals that the observed differences arise from the slightly higher proportion of patient episodes present in bothandduring resampling, compared to the proportion of episodes present in bothandduring the initial split. As a result,is affected by a larger optimistic bias than, which manifests in Figure 5, where their difference is examined.

To prevent data leakage due to clustering, both the initial split intoand, as well as any resampling method applied to, must be performed at the team level. With a total of 9 teams, this means that in each repetition of every analysis setting,consists of either 4 or 5 teams. Furthermore, when performing CV onat the team level, it is not possible to create 10 folds. Instead, each team forms a fold, and CV is carried out in a leave-one-out manner. FigureS8presents the resulting prediction error differences for all analysis settings where no HPs are tuned, alongside the corresponding results from the original setup with naive splits (i.e. splits that ignore clustering) for comparison.

First, it can be observed that ifcorresponds to the CV error, the differences are smaller than or equal to zero for RMSE. This confirms that the optimistic bias found for the CV error in the corresponding naive setup is caused by the clustering structure of the data. However, FigureS8b also reveals that performing CV at the team level leads to highly variable prediction error differences, which is not surprising given the limited number of teams, each varying in the number of episodes and phases they contain. Since we argue that, under these circumstances, it is not reasonable to perform HP tuning, we decided to ignore the clustering structure in the setup of our main analysis. Additionally, in the interest of computational resources, we did not conduct the team-level analysis for the remaining analysis settings involving tuning. However, this should clearly not be taken as a standard for applications beyond illustrative purposes.

In addition to performing splits at the cluster level, we also extended the main experimental setup by including additional learning algorithms specifically designed for clustered data. These are the Random Effects/Expectation-Maximization Tree algorithm (REEMT;RpackageREEMtree;Sela and Simonoff,,2011), and the Linear Mixed-Effects Model Tree algorithm (LMMT;Rpackageglmertree;Fokkema et al.,,2017). In the implementation used for our illustration, both algorithms take into account the clustering structure by iterating between two steps: (i) fitting a decision tree using the CART algorithm for REEMT or the CIT algorithm for LMMT and (ii) estimating random intercepts via a linear mixed model, which are subtracted from the outcome variable in the subsequent tree-fitting iteration. To ensure model stability, random effects are only included for each palliative care team, rather than for each individual episode, as more than 300 episodes consist of only a single palliative care phase (FigureS7a). Including REEMT and LMMT in the analysis, however, does not yield new insights. Their results closely resemble those of CART and CIT, as demonstrated in FigureS9, which compares the prediction error differences of the algorithms.