SECTION: A Random Forest approach to detect and identify Unlawful Insider
Trading

According toThe Exchange Act,unlawful insider trading
is the abuse of access to privileged corporate information. While a
blurred line between “routine” the “opportunistic” insider trading
exists, detection of strategies that insiders mold to maneuver fair
market prices to their advantage is an uphill battle for hand-engineered
approaches. In the context of detailed high-dimensional financial and
trade data that are structurally built by multiple covariates, in this
study, we explore, implement and provide detailed comparison to the
existing study (Deng et al. (2021)) and independently implement
automated end-to-end state-of-art methods by integrating principal
component analysis to the random forest (PCA-RF) followed by a
standalone random forest (RF) withandrandomly
selected, semi-manually labeled and normalized transactions from
multiple industry. The settings successfully uncover latent structures
and detect unlawful insider trading. Among the multiple scenarios, our
best-performing model accurately classifiedpercent of
transactions. Among all transactions the models findlawful as
lawful andunlawful as unlawful percent. Besides, the model
makes very few mistakes in classifying lawful as unlawful by missing
onlypercent. In addition to the classification task, model
generated Gini Impurity based features ranking, our analysis show
ownership and governance related features based on permutation values
play important roles. In summary, a simple yet powerful automated
end-to-end method relieves labor-intensive activities to redirect
resources to enhance rule-making and tracking the uncaptured unlawful
insider trading transactions. We emphasize that developed financial and
trading features are capable of uncovering fraudulent behaviors.

Keywordsunlawful insider trading • ensemble methods • decision
trees • random forest • fradulent behavior

SECTION: 1Introduction

Corporate insiders,ex officio, have privileged access to
material non-public preferential information (MNPI). Primed by
informational advantage, access to and exploitation ofMNPIprowess insiders to maneuver fair market price deviate markedly (Grubbs
(1969)) to profit (Binz and Graham (2022), Ahern (2018), Chemmanur, He,
and Hu (2009), Easley, Hvidkjaer, and O’Hara (2002), Amihud (2002), Kyle
(1985)). Behooved as “routine” (Cohen, Malloy, and Pomorski (2012)),
firmed on MNPI anomalous transactions are motley strategies of momentum,
value and growth, investment, profitability, intangibles and trading
frictions (K. Hou, Xue, and Zhang (2020)). Unlike heavily
hand-engineered approaches deprived by flexibility and poor replications
(Mayo and Hand (2022)), the machine learning methods distill from
underlying complex data distribution to identify features to discover
unexpected patterns (Varol et al. (2017)). Various statistical
techniques are proposed to detect anomalous behaviors (Kahneman,
Knetsch, and Thaler (1991)) each relevant to the inquiry. In machine
learning, prominent methods are based on proximity (e.g., k-means,
expectation maximization) and distance (e.g., principal component
analysis, k-means) (Breunig et al. (2000), Salgado et al. (2016)). For
example, Rizvi, Attew, and Farid (2022) implements Kernel-based
Principal Component Analysis to analyze the cause and effect of extreme
price movements; Seth and Chaudhary (2020), Goldberg et al. (2003) and
Rizvi, Belatreche, and Bouridane (2019) separately use supervised
Natural language Processing (e.g. multi-stage SVM) to classify unlawful
insider transactions as a result of the news cycle and other events;
Islam, Khaled Ghafoor, and Eberle (2018) and Li et al. (2022) integrate
decision trees with deep learning (LSTM RNN) to classify transactions
based on the public release of non-public information on the news. A
synthesis of current research in unlawful insider trading is:

Econometric studies (Cerniglia and Fabozzi (2020), John and Narayanan
(1997), Fishman and Hagerty (1995)) aim to capture time-varying
properties insider trading confining to “trading size” is an example
data snooping (also known as specification searches). Therefore
leaving aside a multitude of other interactive features (e.g. official
designations, information arrival timing) to limit into empirical
irregularities and inconsistencies (Leamer (1978)).

Statistical methods (Auto Regressive Moving Average (ARIMA) (H. M.
Anderson (2007), Lutkepohl (1993), McKenzie (1984))), Box and Jenkins
(Box, Jenkins, and MacGregor (1972)), Hidden Markov Models (Rabiner
and Juang (1986)) and general class of linear models (Hand (2009),
Hamilton (1989))lack scalabilitywith growing number of
observations (). These methods are evaluated in a single set of
train/test splits and are prone to over-generalization (Ge and Smyth
(2000)).

Legal scholars lack consensus (Chang et al. (2022), Bainbridge (2022),
Ma and Sun (1998), Machan (1996),Macey (1988), Manne (1966)) reject
“unfairness in market” favoring full legal protection of insider
activities to spur innovation while other oppose scholars (Ali and
Hirshleifer (2017), Gangopadhyay and Yook (2022), Cumming, Johan, and
Li (2011)) claiming insider trading impedes investor confidence,
increases agency costs and exacerbates opportunistic managerial
behaviors.

More recently, machine learning techniques have been introduced to
identify and characterize unlawful insider trading (see for example Deng
et al. (2021) and Deng et al. (2019) (collectively hereinafter referred
asDCZ), and Lauar and Arbex Valle (2020)). These studies
implement ensemble methods, namely, Random Forest (RF) and
extreme Gradient Boosting (XGBoost) to detect unlawful insider trading
showcasing improvement of the classification accuracy. In fraud
detection (money laundering, credit card fraud) these tools improve the
robustness of normal behavior modeling accomplished by parameter weight
updates and recalibration for the majority voting from the ensemble of
weak learners to enhance detection accuracy (Sundarkumar and Ravi
(2015), Louzada and Ara (2012)). However, these methods, are not
resistant to limitations, illustratively, bias-variance trade-off when
exposed to multiple categorical (Strobl et al. (2007)) or continuous
variables (Altmann et al. (2010)). A better control of the model
complexity can be achieved by increasing the tree depth though that
results in increased variance and reduced bias. There are several
advantages of ensemble methods. First, efficient ensemble techniques
fuse data mining and modeling to extract predictive features in a
unified framework in cheaper, faster and creative ways ofdata
labelingto automate routine tasks (Iskhakov, Rust, and Schjerning
(2020)) by uniquely positioning to captureshort-run noisefeatures that contribute to historical context and derive meaningful
inferences leading totrustworthypredictive power111Prediction
is defined as the ability to take known information to generate new
information(Agrawal, Gans, and Goldfarb (2019))forunderstandable inferences(Khademian (2022), Igami (2018),
Christensen, Hail, and Leuz (2016)). Second, domain agnostic methods
such as decision trees and neural networks explain themicro-data-structureof data independent estimators (Malhotra
(2021), Iskhakov, Rust, and Schjerning (2020)) and automatically
generate relatively important variables.

The study contributes first by addressing gaps of hand-engineered,
omitted variables, inter-dependency and multi-dimensionality problems of
data to understand, explain and replicate the anomalous transactions (K.
Hou, Xue, and Zhang (2020)) by training ensemble based models to
discover regularities that uncover the choices that insiders make
(Camerer (2019), Fudenberg and Liang (2019)). Second, identify patterns
that may have been missed byp-hacking222“A focus on
novel, confirmatory, and statistically significant results leads to
substantial bias in the scientific literature. One type of bias, known
as “p-hacking,” occurs when researchers collect or select data or
statistical analyses until nonsignificant results become
significant.” (Head et al. (2015))and detect and characterize
hidden patterns of insider trading strategies including fraudulent
trades based on the performance of financial proxies.

We organize our study in the following sections as follows:
Section2introduces the proposed
methodology outlining the theory behind the principal component analysis
(Section2.1) and random
forest (Section2.2), the tuning parameters
(Section2.3), feature selection criteria
(Section2.4) and performance measures
(Section2.5);
Section3describes the experimental
setup such as data-preprocessing steps, implementation of
cross-validation, criteria of parameter control and integration of
methos; Section4analyzes the results
that begin with data description
(Section4), steps of dimensionality
reduction (Section4.1), results
from components of confusion matrix
(Section4.2),
ranking of important and relevant variables
(Section4.3), and ranking of
variables (Section4.3.1and
Section4.3.2).
In the final two sections- we discuss results
(Section5) and provide our conclusions,
recommendations and future course
(Section6).

SECTION: 2Proposed Methodology

Multicovariate and high dimensional financial and trade data are
structurally built with underlying hidden attributes. To detect
irregularities in data under consideration, methods such as principal
component analysis (Aıt-Sahalia and Xiu (2019)), gradient boosting
(Eggensperger et. al. (2018)), random forest (Svetnik et al. (2003)) and
neural networks (Hilal, Gadsden, and Yawney (2022)) have been proven
effective (Bakumenko and Elragal (2022), Gu, Kelly, and Xiu (2018)).
Notably, these methods act as intermediary steps of the empirical work
in economics (Athey (2019)). Unsupervised method,PCAeffectively
controls the variability and reduces dimensions
(Section2.1) and supervised
method,RFreduces bias ofdecorrelated trees(Section2.2). As a first-of-its-kind
application in identifying unlawful insider tradingDCZintegrated two methods illustrating to be the best of both worlds. In
analogous settings ofDCZ, we extend powerful modeling techniques
to reduce data dimensions control covariates and classify unlawful
insider trading exhibiting comparable data characteristics in the US
security market and hence the results are easily compared and assessed.
Subsequently, we extend our experiment to evaluate performance by
implementingRFfor their success in wider applicability.
Therefore, by resorting to proven methods from previous research and
extending the number of features in the study frominDCZstudy tofeatures, we experiment withRFto reduce result
uncertainty, quality decline, bias, unfairness failing systems to
provide transparency, interpretability, and reproducibility of results
(Stoyanovich et al. (2017), O’Neil (2016)). We follow number of
data-preprocessing steps before implementing techniques as explained in
Section3.

SECTION: 2.1Principal Component
Analysis

We implementPCAto compare our results toDCZ.PCAfilters noise to decorrelate features uncovering latent dynamics to
extract the most relevant information from the compressed data by
computing principal components- the linear combinations of the original
variables (Wang et al. (2018), Abdi and Williams (2010)). The first
component contains the largest variance and other components are
constrained as orthogonal the first to explain the variance. The values
of the components are geometric projections of observations onto
principal components called factor scores. Among the variety of
successful applications, the domain agnosticPCAis applied in
areas of our interest of finance and anomaly detection (Aıt-Sahalia and
Xiu (2019)) to identify exceptional volatilities (Egloff, Leippold, and
Wu (2010)), acknowledge peculiar investor sentiments (M. Baker and
Wurgler (2006)), discern policy uncertainties (S. R. Baker, Bloom, and
Davis (2016)), stock and bond return aberrant (Pasini (2017), Pérignon,
Smith, and Villa (2007), Driessen, Melenberg, and Nijman (2003), Feeney
(1967)) and more recently market cross-correlation and systemic risk
measurement (Billio et al. (2012), Zheng et al. (2012), Kritzman et al.
(2011)).DCZimplementsPCAwithfeatures and
accomplishes the task of identifying unlawful insider trading in reduced
dimensions. Similarly, we implementPCAand obtain comparative
results. Besides, we extend the experiment with additionalfeatures. Formally,is
matrix withrows andfeatures. Given thevectors, performing PCA means summarizing them
by projecting down to-dimensional subspace, principal components
sequentially organized in real coordinate space. The first component,
the-th vector in the sequence is the direction that is orthogonal
to the second component-1 with maximum variance thus minimizing
the average squared perpendicular distance from the points to the line
that in turn reduces the random variability because of computed reduced
dimensions and distances (Aluja-Banet and Nonell-Torrent (1991)). The
steps of thePCAalgorithm are shown in Algorithm1. The core component ofPCAanalysis is an estimation of the eigenvalues and covariance matrix. The
eigenvalues of covariance matrix consistent asymptotic normal estimators
(T. W. Anderson (1963)).

Despite its wide success and acceptability, the method has drawbacks
(Aıt-Sahalia and Xiu (2019)). First, the classical problem of the curse
of dimensionality, the growth of data dimensions(volume) corresponding
to the sample size the algorithms suffer the curse of dimensional
(Bellman (1958)), which typically means the required data needs to grow
exponentially with its dimensions contributing to shallow the
consistency of eigenvalues. Second, parameter constancy, the principal
components are the linear combinations derived from data that
potentially may fail to capture the non-linear data patterns. Third,
shortcomings related to non-stationarity, principal components are based
on data(vector) independence. However, time series data abound of
dependency and lack of stationarity thus inferences become inappropriate
(Zhang and Tong (2022), Brillinger (2001)). By removing noise and
de-correlating data, the contribution and relevancy of prominent
individual features become conspicuous. In identifying unlawful
transactions integrating PCA andRFre-calibrates leading
feature’s weights thereby minimizing the risk of misclassification and
falsely unlawful activity.

SECTION: 2.2Random Forest

DCZintegratesPCA(Section2.1) to Random Forest
(RF), a data-driven non-parametric ensemble method that
implements bootstrap resampling (bagging)was pioneered by Breiman
(2001). It, therefore, offers the advantage of Out-of-bag (OOB)
error prediction. The bagging is a sub-sampling method with a
replacement that creates training examples for a model to learn from
before the construction of the next individual tree. TheOOBprediction error estimates of theRFis the mean prediction error
on each tree sample used if and only if these trees did not participate
in the bootstrap sampling of the base learner (Hastie, Tibshirani, and
Friedman (2009)). In addition,RFprovides flexibility by
incorporating non-linearities and interactions of the multitude of
decision trees that eventually aggregate de-correlated trees to control
variance and increase accuracy (Chen et al. (2022)). In simple terms,RFis designed to group observations with similar predictors by
growing trees in a sequence of “steps”. Recent theoretical results
suggest that theRFis consistent with the central limit theorem
(Ramosaj and Pauly (2023), Wager and Athey (2018), Svetnik et al.
(2003), Liaw, Wiener, et al. (2002), Breiman (2001)). The method has
been widely adopted in chemical informatics (Svetnik et al. (2003)),
ecology (Prasad, Iverson, and Liaw (2006)),-D Object recognition
(Shotton et al. (2011)), epidemiology (Azar et al. (2014)), and remote
sensing (Pal (2005)) to name few.

Definition

A random forest is a classifier consisting of a collection of
tree-structured classifierwhere theare independently distributed random vectors and each
tree casts a unit vote for the most popular class at input(Breiman (2001)). The random forest algorithm is shown in Algorithm2333https://pages.cs.wisc.edu/~matthewb/pages/notes/pdf/ensembles/RandomForests.pdf.

In supervised settings with iterative learning, randomness is injected
intoRFto correctly map and classify input values () to its
target (), expressed as functional relationshipfor
unknown joint distribution. The decision tree minimizes the
correlationamong the collection ofdimensional
variables. The common elements of growing the ensembletree are
that for thetree, a random vectoris generated
independent of its pastbut has the same
distribution which results in a classifierwithas input vectors withnumber of examples in
the training set.RFlearns about the functional relationship
betweenandby producing a prediction model, controlled by the-dimensional
hyperparameter configurationfrom
the search space. Doing
so, it estimates the expected risk of the inducing algorithm, w.r.ton new data, also sampled from. The results obtained in
Algorithm2are further tuned to extract the
optimized parameter values withk-foldcross-validation to
minimize empirical risk with repeated resampling to approximate the
generalization error (Afendras and Markatou (2015), Bergstra and Bengio
(2012), Witten et al. (2011), Arlot and Celisse (2010), Shao (1993),
Efron (1986), Efron (1983), Stone (1977), Geisser (1975), Stone (1974)).

SECTION: 2.3Parameter Tuning

Parameter tuning reduces the overallOOBprediction error.
Shortlist ofRFparameters to enhance accuracy, minimize data
overfitting and improvise the relative contribution to correctly predict
response are (B, C, and Munoz (2021), Eggensperger
et, al. (2018), Friedman (2001)):

: Number of predictor features drawn randomly as candidates
for each tree per node to balance the correlation preserving
independence between trees. To control the split-variable
randomization, for classificationis calculated as the
square root of number of features. In summary,
the lower values oftend to stabilize aggregation and
produce “near optimal” outcomes by selecting less correlated and
different trees (Probst, Bischl, and Boulesteix (2018), Bernard,
Heutte, and Adam (2009)).

: Although technically not a hyperparameter, the number of
trees() stabilizes trees, by, “…empirically, the
optimal number of trees is obtained when the forest error reaches its
limit as the number of trees grows to infinity…” (Probst,
Bischl, and Boulesteix (2018), Scornet (2017)). Higherincreases computational time.

: Number of splits each decision tree is allowed to
make. Originally Breiman (2001) proposed to keep the tree unpruned and
allow it to grow deep (Belkin et al. (2019)), the recent empirical
results suggest proper tuning can significantly improve the
performance but has time cost complexity (Probst, Bischl, and
Boulesteix (2018), Scornet (2017)).

has a default value of 1.0 improves the
generalization error for better predictive powers of validation and/or
test results

SECTION: 2.4Feature Importance

The multi covariates input during the tree construction of theRFmodel fitting aims to extract, compare and rank relatively significant
ones for better interpretability, explainability and predictive accuracy
of the model in the downstream application (Schölkopf et al. (2001))
that reduces time, space complexity and generalization error (S. Zhou
(2022), Xu et al. (2014), Duchi et al. (2008)). Various methods have
been proposed to identify interactive non-linearities to allow the
incorporation of the known sparsity structure (Qian et al. (2022), Xu et
al. (2014), Guyon et al. (2010), Genuer, Poggi, and Tuleau-Malot (2010),
Strobl et al. (2007)). Orginally, Breiman (2001) implementedMean
Decrease of Impurity (MDI)based onGini Scoreswhich is a
decrease in node impurity, that is, the probability of incorrectly
classifying a randomly chosen transactions in the dataset if it were
labeled randomly according to the class distribution (Nembrini, König,
and Wright (2018)). As the technical by-product, inRFsplitting
rules maximizes impurity reduction introduced by the node-split.
Therefore, a node with larger decrease in impurity value is ranked
higher than the one with lower value. MDI approach has two limitations,
first, it ranks features during the training so it does not have access
to test data and second, it inherently favors highly cardinal features
therefore dilutes the significance of the correlated features.

Subsequent work by Fisher, Rudin, and Dominici (2019) investigates
drawbacks of theMDImethods by applying apermutationbased approach to calculate the contribution of eachcovariate in the out-of-bag examples keeping all other predictor
variables fixed which is passed on to other corresponding trees
(Nembrini, König, and Wright (2018)). The method lowers bias and
balances MDI’s high cardinality (typically of numerical features)
favorability over low cardinality features (binary and/or categorical
defined by a smaller number of possible categories). We compare our
results including the influence of correlation between the co-variates
and the number of correlated variables (Gregorutti, Michel, and
Saint-Pierre (2016)) from each of these methods in subsequent sections
(see Section4.3).

SECTION: 2.5Performance Measure

To evaluate the performance of the binary supervised classification
problem a squared matrix known as confusion matrix of sizeschematically represented as in
Table1is widely used. The confusion
matrix that represents the actual and predicted classes along rows and
columns respectively (Hastie, Tibshirani, and Friedman (2009)).

SECTION: 3Experimental Setup

The Exchange Act,requires individual insiders to submit the
Statement of Changes in Beneficial Ownership (Form) within
forty-eight hours of the transaction. For the study, we downloadedmillion publicly available individual Formfrom Electronic
Data Gathering, Analysis and Retrieval (EDGAR) between,
followed by pre-processing with a customaryXMLpython text
crawler which is eventually stored as a document in theMongoDBcollection. We use a unique Central Index Key (CIK) fromSECto query and merge daily trade and financial results
retrieved respectively from the Center for Research in Security Prices
and Compustat-CapitalIQ. An individual completeMongoDBdocument
consists offeatures representing ownership, corporate
governance, profitability, financial performance, risk and returns in
the security market. We generate unlawful labels based on defendant
names extracted from the publicly available court complaints filed by
theSEC. A text crawler integrated with Levenshtein distance
utility extracts and matches the defendant insider to the filers of the
Form. A transaction is labeled negative (unlawful) if a score isor higher during comparison. Our database consists a total ofunlawful transactions. For the completeness of data, if
quarterly financial results are unavailable, we replace missing data
from the subsequent quarter.

We establish a feature-to-feature comparison withDCZ. Our
comparison matches or closely resemblenumerical covariates (see
Table10) ofDCZ. We are unable to retrieve
features such as CR5, H5, CR5, and Z-indices that we intentionally
replace withcategorical features, namely, Acquisition and
Disposition, IsDirector, IsOfficer, and IsTenPercentOwner. Our full
feature space consistsfeatures from which we are able to subsetto matchDCZ. Notably, these features have long history of
scrutiny and successful adoption in economic and financial models such
as Black-Scholes Model (Black and Scholes (1973)), Capital Asset Pricing
Model (Sharpe (1964)), Efficient Market Hypothesis (Fama (1970)),
Arbitrage Pricing Theory (Ross (1978)) and Behavioral Financial
Economics (Campbell and Shiller (1988), Tversky and Kahneman (1974)).
Additionally, several of these features have been implemented to predict
corporate financial distress (Qian et al. (2022), Sun et al. (2020), L.
Zhou, Lu, and Fujita (2015)). The numerical features in the dataset() are normalized byz-score444.
The numerator improves the interpretation of the principal effects and
scaled by the standard deviation in the denominator puts all the
predictors into a common scale by minimizing the expected differences
in the outcomes(Gelman (2008)).for faster convergence and that
follows standard normal distributionexpressed as zero mean
()555and unit
standard deviation ()666.
Thecategorical features are one-hot encoded.

We experiment in the balanced dataset withratio of
lawful to unlawful respectively forandtransactions.
The smaller subset is randomly selected from the larger pool. Each of
these experiments is further sub-divided by the number of features
followed byPCAintegration and without. Thetransactions are the maximum number of unlawful that we can randomly
choose from. However lawful transactions are randomly sampled from the
pool of the remainingmillion. For instance, to construct a set
oftransactions we randomly choosetransactions fromunlawful and remaining transactions from other pools. We
initialize our experiments with pre-defined hyperparameters in a
randomized search space with iterating infold cross-validation.
We pick,,, andas hyperparameters. We maintaintraining to testing split
ratio and experiment for in eight experiments repeating one hundred
times. We report results in
TableLABEL:tbl-rfComparativeConfusionMatrixKrishnaResults.

After fitting the model, we extract and rank important features based on
the reduction of the Gini Impurity. It is well knownMDIfavors
high cardinal features and is derived from the statistics of the
training data. To improve the limitations, we introduce permutation
importance as an additional model inspection technique. Such setting
allows us to control the influence of correlation. As an additional
step, because financial and trade data are naturally exhibit high
collinearity, we adopt a hierarchical clustering based on the Spearman
rank-order correlation, picking a threshold, and a single feature from
each cluster to rank features. In addition, we relaxed certain
assumptions ofDCZas follows:

Qualitative features with prefix such assignificantare notional descriptors. With the possibility of subjective interpretations, it may inadvertently introduce spuriousness. TheDCZstudy adopts the criteria defined bysignificacewithout providing further explanation of what it quantitatively meant. Thus, as a precaution, in this study, we limit the usage of qualitative definitions to reduce incongruent elucidation thereby extending the empirical justification of results.

Confining data selection to the same industry group is loosely aselection bias. In contrast toDCZstudy, for wider applicability, we randomly selected transactions from different industrial groups thereby boosting objectivity.

In the dynamic and complex United States Securities Market, it is conceivable that there exists a plethora of transactions that may go unnoticed despite being unlawful. The authorities lack resources to investigate the majority of transactions, thereby it is fair to treat each transaction as potentially unlawful.

Our code accesses various python based objects of scikit package
repeatedly fortimes. We record performance of each of the
components of confusion matrix in a separate spreadsheet and average at
the end. The repetition aims to the control the variability and more
reliable than a single experiment. Our results are benchmarked againstANN,SVM,AdaboostandRandom Forestfrom
Deng et al. (2019) and Deng et al. (2021).

SECTION: 4Results

Our results are based on the balanced dataset with(unlawful) transactions ofdimensions as explained in
Section2. For illustrative purposes, we
present Table5(b) as a random example subset
of Table5(a) that matches Deng et al.
(2021). Notably, in each of these tables, the existence of the higher
number of “purchases” is because of the executive’s compensation
structures, specifically, the restricted stock options and bonuses
allocated towards meeting targets.

SECTION: 4.1Results of Dimensionality
Reduction

In this section, we report results from the principal component analysis
(PCA) that transforms the multi-dimensional correlated variables
into a smaller set of the principal components (PCs, directions)
computed from the linear combinations capturing the largest variation in
the original dataset.PCs are eigenvectors that represent the
weight of eigenvalues of the covariance matrix with the largest
explaining the direction of the most variability explained by the given
component. The firstPCcaptures the most variability. The
eigenvector times the square root of the eigenvalue gives the component
loadings which can be interpreted as the correlation of each item with
thePC. In order to decide on the number of components required
to explain the proportion of the total variance explained by eachPC, we calculate the cumulative explained variance ratio - the
ratio of its eigenvalue to the sum of all the eigenvalues of allPCs measuring the relative contribution of eachPCto
explain the amount of variance. In our context, we aggregate an average
ofpercent cumulativeEVRexplained byPCs.

Figure1is representative subplots of
bivariate relationships between thePCsfrom a pool of one
hundred experiments. Each sub-plot shows correlation between defined by
the direction of maximum variance impacted by the linear reduction of
multi-dimensional data into uncorrelated features ordered by the values
of the explained variance of each component. Directions ofPCsare not uniquely determined by the component itself, therefore, reading
it from either axis does not alter the meaning of the plot itself.

Correspondingly, the heatmap
(Figure1) for its intuitive appeal
and ease of summary is organized as sub-plots of pie charts to portray
the strength of association and direction between covariates measured by
the Pearson correlation represented by the color gradient. The deeper
the gradient higher the value. Blue and red gradients respectively
indicate positive (perfect at) and negative (perfect at)
correlation. The solid pie across the diagonal represents the perfect
correlation. FigureLABEL:fig:corrleationPlot_withoutPCAdoes not show a
strong outlier that falls outside of a peculiar pattern. A strong
correlation betweenidiosyncraticandtotal volatilityrepresents the underlying interplay of the components of unsystemic and
systemic risk. It indicates that transactions of insiders tend to
strategize around market volatility (Koumou (2020)). In addition, the
linear direction between thequickandcurrent ratiomeans
insiders’ short-term liquidity accesses influence anomalous insider
transactions (Cohen, Malloy, and Pomorski (2012)). The negative
correlation between the director and the officer indicates that an
insider can be justifiably a director overseeing the company but may not
engage in the daily operations which is a blurred line and often
unrecorded (Gregory, Matatko, and Tonks (1997)).

SECTION: 4.2Results of Classification of Insider Trading
Transactions

In this section, we present the performance of the components of the
confusion matrix obtained from theRFmodel fitting. Our results
summarized in TableLABEL:tbl-rfComparativeConfusionMatrixKrishnaResultsis an average of one hundred experiments each with-fold
cross-validation. In our context, the larger number of experiment
smoothed the variation of the performance. We ran experiment ten and
hundred times. As an example standard deviation obtained for accuracy
with ten experiments in case oftransactions andfeatures
respectively ofandwith and without PCA which
declines toandrespectively for one hundred
experiments. We observed similar results applies to other metrics,
therefore we present results from one hundred experiments.

In TableLABEL:tbl-rfComparativeConfusionMatrixKrishnaResultswe
organize results according to the number of transactions (and), features (and), and finally sub-divided by
integration with or withoutPCA. The metrics are the
best-reported obtained during a random search in hyperparameter space in
thefold cross-validation settings. For benchmarking, we compare
our results toDCZpresented in
TableLABEL:tbl-rfComparativeConfusionMatrixBenchMarkMethod(Deng et al.
(2021), Deng et al. (2019)) with a focus on thefifthcolumn as
proposed by Deng et al. (2021). As an ancillary for the baseline, we
selectively present results from Deng et al. (2019). As shown in
TableLABEL:tbl-rfComparativeConfusionMatrixBenchMarkMethod(fifth
column), the benchmark method -PCA-RFaccurately classifiespercent of transactions. In analogous settings, the lowest
classification accuracy as averaged with one hundred experiments ispercent (second column of
TableLABEL:tbl-rfComparativeConfusionMatrixKrishnaResults). In any
other settings, our results beat all benchmarks demonstrated by ‘ACC’
row of the TableLABEL:tbl-rfComparativeConfusionMatrixKrishnaResults.
When we increase the count of transactions (fifth-eighth columns,
TableLABEL:tbl-rfComparativeConfusionMatrixKrishnaResults) the
performance gains the momentum. We credit two explanatory reasons for
the prominence, first, it is influenced by the random selection of
lawful transactions from a larger pool of captured required information,
and second, we unboxed the analysis from time-window perspective. Such
settings has direct potential applicability to theSECto
experiment in the production-level data. Despite keeping the unlawful
transaction unchanged and by randomly sampling the lawful transactions
(percent) we demonstrateRFbeat every benchmark
performance. Overall our results indicate a significant increment in the
accuracy as shown by theACCrow of
TableLABEL:tbl-rfComparativeConfusionMatrixKrishnaResults. Meanwhile,
as a caveat, even though accuracy provides us a basis for basic
inference, literature cautions that taking it as a sole decision-making
gauge may ignite acceptance of the overgeneralized results.

Sensitivity or true positive rates (TPR) or Recall (REC)
is a recommended metric for decision-making rather than overall
accuracy. The largerRECmeans that the model reliably detected
the presence of lawful transactions and therefore there exists a lower
number of wrongly classified as unlawful (False Negative) implying
detection was effective with minimum spillage. Among the benchmark
methodsANNidentifies and classifiespercent lawful
transactions which is marginally higher thanDCZ’s proposed
method atpercent. On the other hand, our model’s ability to
“rule” lawful as lawful improves evenly on every instance as displayed
by theTPRrow of
TableLABEL:tbl-rfComparativeConfusionMatrixKrishnaResults, with the
lowest value of(second column).TPRdoes not consider
lawful incorrectly identified as unlawful therefore if a data point is
bogus or indeterminate the result becomes useless for detecting or
“ruling in” true negative. In simpler terms, theTPRmetric is
useful for “ruling out” the possibility of unlawful transactions as it
rarely misidentifies them.

The proposed method reliably detected the presence of lawful
transactions elucidating that wrongful classification of unlawful is low
or vice versa, represented by False Positive Rates (FPR) defined
as unlawful transactions incorrectly identified as lawful. Benchmark
methods show thatPCA-RFFPR of. Our model incorrectly
identifies(TableLABEL:tbl-rfComparativeConfusionMatrixKrishnaResults, second
column). In similar settings of the benchmark methods our proposed
method performs better (columns first through fourth
TableLABEL:tbl-rfComparativeConfusionMatrixKrishnaResults). We
demonstrate that reducing dimensions is not an effective measure to
detect lawful transactions classified as unlawful. Our proposed model
makes a few mistakes as low aspercent when all features are
considered (TableLABEL:tbl-rfComparativeConfusionMatrixKrishnaResults,
seventh column). Marginally more errors are made by the model when
additional transactions are added which is intuitive. The lower the
value ofFPRindicates thatRFcan perform better.
Overall, our method beat allFPRstatistics of benchmark methods
which we showcase in every instance of experiments. It is “strong
evidence” of high true positives and low false negatives.

The corresponding metric, specificity is the true unlawful (True
Negative Rates,TNR) transactions correctly identified as
unlawful. It excludes the lawful transactions resulting in a higher
number of true unlawful and a low number of unlawful transactions that
are wrongly classified as lawful. That is, the test was successful in
“ruling in” transactions that were unlawful in every instance when
compared to benchmark methods. Compared to benchmark methods like the
metrics we described above, our performance starts improving as we add
more transactions and/or features. As like other metrics, our metrics
starts with additional transactions and features to surpass all the
benchmark methods. For instance, even when lesser features were
considered than the benchmark method (, fifth column of
TableLABEL:tbl-rfComparativeConfusionMatrixKrishnaResults) we show
strength in identifying unlawful transactions as unlawful. Naturally,
our proposed method if integrated withPCAhas lower performance
among themselves than otherwise but by adding more transactions even
with the PCA integration performance improves bypercent
(difference betweenand). As displayed in
TableLABEL:tbl-rfComparativeConfusionMatrixKrishnaResultsour results
indicate the random selection of positive transactions plays a role in
defining unlawful transactions. Even though metrics suggest a
substantial ability, the result should not be taken alone and be gauged
on potential over-fitting and the existence of noise variables that
remain unaccounted for during data transformation. The result is
inconclusive when lawful transactions are introduced as it cannot “rule
in” the false unlawful transactions. However, transactions must be
negative to be identified as negative which will lower the
administrative burden.

In each scenario, our experiment showed strong ability in correctly
identifying unlawful trades. With the selected features and reduced
dimensions, the results are closer to the previousDCZstudy. It
can be attributed to one of the striking features may be because
inaccessibility of qualitative features for the proposed method.
Moreover, in a notable exception, theDCZstudy presents results
as a time window. It is interesting as the details of methodology
neither conceive nor justify how temporal properties can be implemented
when results are presented to compare time window length. It can
potentially subject the results to be biased or misclassified. In their
defense of theDCZstudy, the careful selection of indicators
encompassed explainable features as expounded by economic theories even
with the reduced number of indicators the performance is better.

For all features the classification of the true positive (True Legal)
versus the false positive (False Legal) was lower than that of the
selected features. The selected features method has higher probabilities
of detecting true positives than with all features.AUCmethod to
evaluate the model for the best is better in either case however the
value is larger than(better than random guessing). By comparing
theAUCPRacross all and selected features it is more evident
that the variation in the values mean of() versus() provides an inconclusive result. Such a result can be
indicative that selected features are more curated thus it may surpass
the required information thereby inflating the number of
identifications.

The results discussed above are based on the tuned hyperparameters in-folds cross-validation withiterations. Theoretically, the
lower values ofmean that the performance is better as
opposed to the larger values. The minimum and maximum values ofwere respectively0.35and0.95forfeatures with maximumAUCof. Increasing the
number of input features increased thebut with a marginal
increase in theAUC, indicating that features increased the
number of False Positives and the model’s performance declined.
According to Probst and Boulesteix (2017), Scornet (2017) thatis not a tunable parameter butRFexpects it to be set
at a high value to obtain better performance, that is, it assists in the
parameter in converging errors. Probst and Boulesteix (2017) empirically
shows that performance gains by growing the number of trees by more than. In the study, the maximum and minimum values wereandwhen allfeatures were input to the model. When all
features are considered and(max value) while the max depth is. Overall having lowerandvalues
leads to the less correlated trees while they provide different
prediction values from each other.

SECTION: 4.3Variable Importance

RFbased on a subset of a few strong covariates performs an
implicit variable selection from the underlying latent structure in data
by eliminating irrelevant raw features without diluting classification
accuracy (Genuer, Poggi, and Tuleau-Malot (2010)). For demonstration, in
this section, we choose a sample experiment to illustrate the technique
starting with the originalGini Scoresproposed by Breiman (2001)
based on the concept of information loss. While Gini impurity scores
provide a strong foundation for explicit feature selection, one might
also be interested in learning the orthogonal splits of the feature
space is also optimal for the classification of the correlated and noisy
features such as finance data. Therefore to observe the distinction we
extend the analysis to include permutation importance.

We randomly choose a sample experiment to compare the feature ranking
based on theMDIto ranking of the Deng et al. (2021). In the two
parts visualizations -2(a)ranks
the principal components for thefollowed by2(b)forfeatures. The horizontal
and vertical axis of2(a)and2(b)respectively represent coefficients ofGini Scoresand the principal components in the descending order.
In2(a)has the
highest andlowest ranks with other components in
between in the order offorfeatures. The coefficient ofGini Scorerefers to how
often a particular feature was selected for a split. Hence, the highest
coefficient score represents the larger contribution to the split
ranking it higher. According to
Table8(a) for, the largest contribution was made by returns and
acquisition-disposition which is intuitive given the asset purchase and
sell activities by insiders. It is well-known in finance literature
returns explain the largest variance as investors’ choices in the face
of uncertainty about the future (Foresi and Peracchi (1995)).
Risk-related features related to the fair market pricing value and
attributed to playing a sensitive role in determining insider’s
transactions and ultimate behavior.

Likewise,- the highest ranking principal component in
Table8(b) for all
features is influenced by its returns, alpha, and beta - all covariates
related to the risk-taking behavior. In2(b)andrespectively represent the highest
and the lowest values of coefficients in descending order of
(). Evidently,
return is related to the risk-performance of the asset which supports
the idea that insiders exert more influence in determining asset prices
that ultimately can influence unlawful trading because of short-term
motives. These features are highly cardinal and correlated and may not
be as influential as they seem to be. In contrast, it is intuitive to
think that the influence of executive insiders may impact companies more
than quarterly returns or market risk components reflected in unlawful
transactions.

Even though impurity-based feature importance ranks the principal
components based on the correlation as shown in
Table8, it is hard to
infer the rank of the features directly based on these scores as they
vary from one to other components. Besides, theMDIbased methods
are biased towards high cardinal features and its ranking is based on
training features therefore does not typically generalize to the test
set. Additionally, our dataset belongs to finance and therefore contains
highly correlated covariates, and one variable may contain information
to classify the response variable (Meinshausen (2008)). When one of the
features is permuted, the model still has access to both features
through correlation which results in lower relative importance for both
features, even though both of them may be significantly important. To
overcome these shortcomings, in this section, we rank features based on
the permutation importance, a model-agnostic method that can applied to
any fitted estimator by calculating the value of the contribution of
eachvariable in the out-of-bag examples keeping all other
predictor variable fixed and passed on to other corresponding trees
(Nembrini, König, and Wright (2018)). We construct hierarchical clusters
based on Spearman rank correlation777Spearman rank correlation is
an appropriate method because the correlation coefficients will be
higher if two variables have similar (or identical) ranks, and lower
if they have lower ranks.of the highly correlated covariates, track
one representative feature from each cluster and finally permute between
them. The linkage between the clusters is established based on the
Ward’s minimum variance. The distance matrix is the derivative of the
correlation matrix.

In Figure3(a)we rank features based on the
Gini Impurity Scores. In Figure3(a)top
variables such as return on asset, operating profit margin before
depreciation, and markettend to have multiple distinct values
which are demonstrative thatMDIis biased towards them leading
to a higher rank. Notably, RoA and Profit Margins are highly correlated
as well as marketand returns which complicates decisions
based on visual inspection. In contrast by performing permutation
importance in the same training dataset (Figure3(b)), we observe that low cardinal categorical
features such as “IsOther”, “Ten Percent Owner” and “IsOfficer”
are the most important features. Even before removing correlation if we
permute at the most there is a decline ofin accuracy which
suggests none of the features are important. This is in contradiction to
our high very test accuracy averaging at(with PCA),(without PCA) in
TableLABEL:tbl-rfComparativeConfusionMatrixKrishnaResults. Therefore to
verify the change in accuracy is caused by the cardinality and highly
correlated covariates, we perform hierarchical clustering and rank the
features based on the Spearman Rank Correlation and re-run permutation
importance in the held-out set.

In the right-hand side of Figure4we
display a correlation matrix heatmap of the selected features with a
diagonal showing the perfect correlation. The left side of Figure4shows the dendrogram reflecting the
relationship of similarity among the group of variables connected byclades. For instance, Price Earnings (Basic) and Return on
Equity(RoE) form a clade which joined with the Trailing PEG Ratio forms
a different clade for the leftmost clade in Figure4. Analogously, the rightmost cluster
acquisition and Disposition and Ten Percent Ownership form a clade.
Therefore the arrangement of each of these clades shows which features
(leaves) are similar to each other distinguished by the height - the
higher the height, the greater the difference.

By removing correlation we can observe the accuracy can drop by as much
as by(Figure5(b)) versus
uncorrelated (as shown in Figure5(a)). The distinction of ownership and
institutional covariates such as “IsOther” did not change but its
prominence is more visible after decorrelation. We agree with Strobl et
al. (2008) stating that permutation importance does not break the
relationships as with correlated predictor variables is prominent,
especially in the higher dimensional problems. In general, the features
of financial statements play an important role in direct activities in
the stock market even so. Governance of the company and activities in
purchase and sales play important roles. It can be inferred that even
though variables likeacquisition-dispositionare at the bottom
of the ranking they exert influence. Figures5(a)and5(b)show
that Ownership cannot be overshadowed by the activities and/or risks
taken in the market. The market,value premium(High
Minus Low) - spread between the companies with high book-to-market
ratios (value companies) outperform the companies with low
book-to-market (value) also play important roles, which given the
institutional influence of the executives play important roles in
deciding an activity lawful and/or unlawful (e.g. dividend policy,
Campbell and Shiller (1988), Grinblatt, Masulis, and Titman (1984),
Henry, Nguyen, and Pham (2017)).

SECTION: 5Discussion

We proposed two variations ofRFto detect unlawful insider
trading and elicit relevant features based on interpretable statistical
scores from detailed trades and financial data. With balanced accuracy
and sensitivity in mind, we evaluated the performance of each model in
hyperparameter-tuned settings. In addition, we replicated steps in the
equivalent method proposed byDCZfor meaningful comparisons. We
extended theDCZmethod by semi-manually labeling data, randomly
selecting transactions from multiple industries, removing undefined
qualitative features, and fed into an automated pipeline to train and
test the fitted model. Our results demonstrate that the model
successfully uncovered latent structures in the data, gained better
performance thanDCZfor each method and boosted the
interpretability. We can confirm that governance, financial and trading
features may be relevant and only a few selected variables may naturally
contribute to uncovering fraudulent behaviors. Our assessment avoided
optimistical biased performance with the introduction of additional
features indicating that the model can concretely learn and distinguish
relevant unlawful transactions. This was achieved by implementing a
range of hyperparameters in5-foldcross-validation by minimizing
generalization error. Therefore, we extend the results obtained by theDCZstudy with the following highlights:

Low Fall Out Rate:In detecting unlawful transactions, it is
very important that the proposed method minimizes mistakes to identify
unlawful transactions. As an analogous to a situation when a person
guilty of a crime is acquitted, comparingFPRwith benchmark
methods in
TableLABEL:tbl-rfComparativeConfusionMatrixBenchMarkMethod, the
proposed method commensurate proportionate or better performance. The
model successfully minimizes false alarms by not wrongfully
classifying unlawful as lawful. BecauseFPRmetrics are not
controlled by researchers it reduces arbitrary judgment and solves the
limitations of traditional econometric studies associated witha
priorisettings of arbitrary significance levels. Such finding is
important to the SEC to confideMLmethods to relieve labor
intense activities to redirect to enhance rule-making to tracking of
the uncaptured unlawful insider trading transactions.

Low Miss Rates:By having lowermiss rates, the model
makes very few mistakes to classify lawful as unlawful transactions.
Analogous to a situation when a non-criminal is incarcerated, if a
lawful transaction is investigated as unlawful, the burden and stakes
are high. Our proposed method reports it misses only a few instances
and wrongfully classifies lawful as unlawful. However, our method is
substantially better to flag false negatives.

High Sensitivity:The proposed method reliably detected the
presence of lawful transactions which translates into that wrongful
classification of unlawful is low or vice versa. This is “strong
evidence” of higher true positives and low false negatives. As a
starting point, the enforcement agencies can build robust systems and
refine methods to further improve the performances thus they can focus
on detailed ramifications of unlawful activities.

High Specificity:The model correctly identified unlawful as
unlawful which excludes the lawful transactions resulting higher
number of true unlawful (true negatives) and a low number of unlawful
transactions that are wrongly classified as lawful.

High Accuracy:In each scenario, the effectiveness as measured
by the accuracy of classification, the proposed method correctly
identifies lawful and unlawful transactions better than each of the
benchmark methods. We demonstrated that a larger pool of data and
unboxing time-window archives better accuracy which is pragmatic for
theSECto experiment in the production level data. Despite
keeping the unlawful transaction unchanged and by randomly sampling
the lawful transactions (percent) we demonstrateRFbeat
every benchmark performance. In comparison, the method in each
instance shows meaningful improvement to Deng et al. (2021).

In addition to the algorithm prescribedMDIbased feature
rankings, we extended it to a model agnostic permutation based
variable importance. The former includes rankings only on the training
while the latter has ability to extend to the test dataset. That
difference allowed us to investigate influence of correlations. By
doing so we observed that categorical features related to corporate
governance gain more prominence which were less influential inMDIbased ranking (Figure5(a)before versus Figure5(b)after removing
correlation). That is, when features are correlated ranking them
introduces of the possibility of one variable may contain information
that classifies the response variable therefore the ranking is
influenced and its prominence is diluted (Avanzi et al. (2023)).

SECTION: 6Conclusions and future
directions

The research analyzed variations of Random Forest in semi-manually
labeled insider trading transactions that reflect ownership, corporate
governance, financial performance, and returns in the security market.
Our research is integration of multiple techniques from the natural
language processing to extract proper names and merge individual
transactions from Formbased on the Levensthein distance,
independent data labeling, identifying finance and trade covariates
belonging to individual insider transactions to create a persistent
database. Betweenand, we balanced lawful and unlawful
by randomly choosingto match benchmark research and continue to
extend the number of transactions (e.g.) and features
(). To control variability we repeated our experiments one
hundred times. While unlawful transactions remains fixed at a maximum of, we randomly choose lawful transactions from a pool ofmillion in every iteration. Performance of each instance of model
configuration is sampled in-fold cross-validation with-iterations initialized, controlled and optimized by random space
search of the number of parameters to provide evidence that it has
sustained the robustness check and extended on the previous study to
accurately identify and discover unlawful transactions with lower fall
out and miss rates.

Certain features dominate unlawful behavior that,ex officio,
corporate insiders predominantly are at avail to manipulate, which in
all likelihood encourages committing fraud. Towards inferring that, we
recognized several challenges and recommend for future improvements.
First, decision trees are effective classification methods with the high
accuracy, an extension of these methods to explain the causal effects
are of interest to the regulators, policy makers and researchers (Athey
(2019)). Second, the study is based on the uniform random sampling for
parameter tuning, a simple but potentially wasteful method as the
features space grows. Providing results from alternatives such as
Bayesian Optimization, Grid Search, Evolutionary and so on that could
scale to the complex yet sample-efficient methods is another future
direction (Probst, Bischl, and Boulesteix (2018)). Third, while the
current research extends features fromto, in the future,
an extension to includeanomaly-related features from Kewei. Hou
(2017) may provide more insightful inferences. Fourth, even though
categorical features are handled via one-hot encoding as popular
literature suggests issues arise as the cardinality expands - the
independence (that is, orthogonality) assumption of one-hot may no
longer hold as the feature space becomes crowded and correlation between
the features is inevitable. Fifth, our results are based on a balanced
dataset which may not be very pragmatic even though we tried to perform
several experiments with changing transactions, it would become more
robust if the results from the unbalanced dataset match. Overall, our
results demonstrate that methods are efficient techniques that fuse data
mining and modeling to extract predictive features in a unified
framework to derive robust results enabling cheaper, faster and creative
ways ofdata labeling, automating routine tasks by uniquely
positioning to captureshort-run noisewhich leads totrustworthypredictive power888Prediction is defined as
the ability to take known information to generate new information
(Agrawal, Gans, and Goldfarb (2019))forunderstandable
inferences(Khademian (2022), Igami (2018), Christensen, Hail, and Leuz
(2016)).

SECTION: Appendix

# References {.unnumbered}

SECTION: References