SECTION: DrIFT: AutonomousDrone Dataset withIntegrated Real and Synthetic Data,Flexible Views, andTransformed Domains

Dependable visual drone detection is crucial for the secure integration of drones into the airspace. However, drone detection accuracy is significantly affected by domain shifts due to environmental changes, varied points of view, and background shifts. To address these challenges, we present the DrIFT dataset, specifically developed for visual drone detection under domain shifts. DrIFT includes fourteen distinct domains, each characterized by shifts in point of view, synthetic-to-real data, season, and adverse weather. DrIFT uniquely emphasizes background shift by providing background segmentation maps to enable background-wise metrics and evaluation. Our new uncertainty estimation metric, MCDO-map, features lower postprocessing complexity, surpassing traditional methods. We use the MCDO-map in our uncertainty-aware unsupervised domain adaptation method, demonstrating superior performance to SOTA unsupervised domain adaptation techniques. The dataset is available at:https://github.com/CARG-uOttawa/DrIFT.git.

SECTION: 1Introduction

Uncrewed Aerial Vehicles (UAVs), also known as drones, have gained popularity in recent years due to their versatility and cost-effectiveness for various operations[19,67], including healthcare[46], surveillance[17,10], delivery[40], agriculture[5,3], construction and mining[58,9], infrastructure inspection[2], and search-and-rescue[39]. However, their ubiquitous use has raised safety concerns, such as the possibility of their use for malicious activities and collisions with other objects in the airspace[30,63]. Achieving autonomous flight capabilities in challenging environments, for individual and swarm drones, is vital for various applications[26].
Ensuring the safety of such operations depends on the accurate and efficient processing of drone-related data.

In particular, vision-based drone detection plays a crucial role, as it faces challenges such as distant small objects, handling complex backgrounds (BGs), and distinguishing drones from other visually similar flying objects. Deep Neural Networks (DNNs) have demonstrated exceptional capabilities in multiple applications, including drone detection[7,12,16,28]. However, distribution shifts from the training to the test set, caused by environmental variations, various points of view (PoVs), and background changes, pose intrinsic challenges in drone detection and affect the DNNs capabilities. Specifically, BG shift,e.g.training with data mostly captured with sky background while sky, tree, and ground backgrounds appear in the validation set, is also called unseen BG[68]. Gathering supervised data for all domains to ensure DNN generalization is impractical and often costly for data collection and annotation[36,50,41], especially, it is worsened in adverse weather conditions or under regulatory constraints for drone-based applications.

Unsupervised Domain Adaptation (UDA)[41,6,8,22,43]is a principal approach to addressing domain shift (DS) in object detection (OD). Domain shift refers to shifts in the input image due to environmental factors that affect the performance of drone detection due to their impact on the drone’s appearance in the scene. UDA aims to transfer knowledge from the source to the target domain, despite the lack of supervision in the target domain. This approach has gained popularity in applications such as autonomous vehicles[61,20,55,54]and other edge-AI, where DSs are common, and supervised data is not guaranteed[42,14]. UDA methods have also been employed extensively to address DSs in drone detection[52,53]. However, unforeseen situations that cause DSs, such as drones with novel shapes, can still occur. Despite the trend toward using UDA in the field, there is a lack of comprehensive exploration of specialized domain shift and UDA methodologies in drone detection[52,60,28,71,51,7,65,62,49,69]. This gap has catalyzed our work to design a new dataset that addresses these specific challenges.

Combining existing datasets often results in multiple uncontrolled DSs co-occurring, making it difficult to isolate and examine the impact of specific shifts. Moreover, existing datasets lack systematic background segmentation and comprehensive coverage of DS types, making manual annotations costly and infeasible. To overcome this, the DrIFT dataset was designed to provide a controlled environment in which individual DS types, PoV, season, weather, and background can be studied independently. DrIFT ensures a balanced distribution across its fourteen distinct domains, addressing limitations in previous datasets and enabling the systematic study of multiple DS types simultaneously. Driven by the need to address these shortages, we present the DrIFT dataset with the following pivotal contributions:

The DrIFT dataset introduces a vision-based drone detection dataset. Uniquely, DrIFT comprises fourteen distinct domains constructed by combinations of four major domain shifts: PoV, synthetic-to-real, season, and weather. In most domains, there are sky, trees, and ground backgrounds.

We employ BG segmentation maps to introduce the concept of BG shift as a distinct challenge. This novel approach allows us to report BG-wise metrics (e.g.,:of sky background detections), providing a focused study on how
BG shift
influence the object detection.

We introduce a novel uncertainty evaluation method for OD, surpassing existing methods (Tab.3). Our method, utilizing a score map, offers significant advantages such as lower complexity of postprocessing and superior capability in capturing DS.

Our uncertainty-aware UDA method
outperforms state-of-the-art (SOTA) UDA methods for drone detection (Tab.4).

SECTION: 2Related Work

SECTION: 2.1Drone Datasets

Drone datasets have recently become publicly available to address the increasing interest in drone detection[29]. Many datasets have significant limitations, as highlighted inTab.1. For instance, the dataset in[12]lacks certain weather conditions and uses a stationary camera. The datasets in[60]and[28]are limited to a single PoV. The dataset in[71]is restricted to partly cloudy and clear weather. Other datasets[51,7,65]offer limited diversity in weather and PoV. UAV-200[52]uses supervised domain adaptation with a fraction of the target domain during training and only examines the synthetic-to-real DS, while DrIFT studies four types of domain shift in a UDA manner. The number of DS types studied is indicated inTab.1. The datasets in[62]and[69]feature multiple drone models but lack comprehensive DSs. The datasets in[1]and[16]focus primarily on ground PoV videos with limited weather conditions.[4]lacks real-world domain.

TheDrIFT datasetintroduces fourteen distinct domains constructed by combinations of four major DS elements: PoV, synthetic-to-real, season, and weather, with sky, trees, and ground backgrounds (Fig.1). DrIFT uniquely emphasizes BG shift as a separate challenge and employs BG segmentation maps to create BG-wise metrics. This comprehensive approach addresses the lack of datasets that study various DSs in drone detection, making DrIFT the first dataset to comprehensively study all four DSs.

SECTION: 2.2Land Vehicle Datasets

Land vehicle datasets constitute another topic similar to those of drones within the realm of autonomous vehicles. As inspiration for DrIFT, the SHIFT[61]autonomous driving dataset offers DSs across a spectrum of parameters, such as weather conditions, time of day, and density of vehicle and pedestrian, but does not investigate BG shifts.

SECTION: 2.3Uncertainty Estimation

Uncertainty estimation is crucial for assessing the safety level of autonomous vehicles, especially drones, by effectively dealing with DS. Conventional methods categorize uncertainty in deep learning into aleatoric and epistemic uncertainties. Aleatoric uncertainty arises from data noise, while epistemic uncertainty is due to limited data or domain coverage, which is more relevant to DS[18].

Historically, uncertainty estimation involves sampling-based techniques like Monte Carlo dropout (MCDO)[21], which, although effective in capturing epistemic uncertainty, are computationally intensive due to their iterative nature and postprocessing complexity[23].

To address computational constraints and accurately capture the epistemic uncertainty arising from data gaps, recent studies[48,47]explore using gradient self-information directly to assess uncertainty. Nevertheless, they do not inherently encompass the true essence of uncertainty.

To address these computational constraints and the lack of a comprehensive sense of uncertainty, we leverage an efficient approach that combines the strengths of MCDO with a simplified postprocessing mechanism. Our method utilizes MCDO to generate uncertainty maps for each detection, performing multiple inference passes and aggregating these uncertainties into an overall score map[44], which reduces the postprocessing complexity (Sec.4.1).

SECTION: 2.4Detection Calibration Error Estimation (D-ECE)

D-ECE is critical for providing accurate confidence assessments in neural networks, especially for safety-critical applications. The calibration error measures the alignment between the predicted confidence and the actual results, helping to assess the reliability of a model[35,24]. D-ECE extends from classification-based calibration error estimation but applies specifically to detection tasks, focusing on the regression outputs of object detectors. The concept, introduced by[35], addresses unique detection confidence calibration errors. Further details on its calculation are provided inSec.4.1.

SECTION: 2.5Unsupervised Domain Adaptation (UDA)

UDA addresses domain shift by transferring knowledge from a labeled source domain to an unlabeled target domain. UDA for object detection was first introduced by[8].

Many approaches in UDA have been introduced that come with notable limitations. Pseudo-labeling and self-training methods, such as[32]and[31], generate target pseudo-labels, but incorrect labels can propagate errors, especially in complex backgrounds like our application. Image-to-image translation techniques[25,56]reduce the domain gap by converting source images into the target style, but these often introduce artifacts and require extensive training data to perform well, which is not feasible in our application.

Among the more recent advancements, uncertainty-aware methods have gained attention for their ability to improve domain adaptation by estimating and incorporating prediction uncertainties. These methods, such as[41,22,43,6], leverage uncertainty metrics to focus on areas where domain shifts are most pronounced. Adversarial training, introduced by[8], complements this by aligning feature distributions between domains. Together, these approaches provide a robust mechanism for handling domain shift, focusing on confident regions and learning domain-invariant features to reduce errors and enhance model robustness. The details of our approach are discussed further inSec.4.1.

SECTION: 3DrIFT Dataset

We have developed a vision-based drone detection dataset consisting of image frames, ground truth bounding boxes, and BG segmentation maps (Fig.1(d)). InSec.3.1, an overview of the DrIFT dataset’s sensor, experimental setup, annotation, and dataset design has been presented. In the following, precise information regarding DrIFT’s various domains has been compiled to represent the dataset’s purpose for the DS. For more detailed statistics of DrIFT, the reader can go through the supplementary materials.

SECTION: 3.1The DrIFT Story

Real Ground PoV’svideo recordings for the DrIFT dataset were captured with a Bosch pan-tilt-zoom (PTZ) camera. The DJI Phantom 2/3, Phantom 4/Pro, Inspire, and Mavic (Fig.1(d)) were captured between 0.1 and 1.5 kilometers away in the recordings. A drone is predominantly present in the frames. The semi-automatic annotation has been done using the CVAT[15]. We have generated multiple other domains of data in our dataset to represent the DS.

Real Aerial PoVhas been added to the DrIFT dataset to achieve the PoV shift concept. For the aerial PoV, a custom-built drone model was utilized (Fig.1(d)).
In this experiment, mobile electro-optical cameras, the Infiniti STR-8MP-3X and GoPro were used to record multiple drone footage between 20 and 100 meters in the line of sight. The frames were recorded in different seasons, resulting in various BGs, such as the sky, trees in various seasons, and the ground with different colors.

Synthetic Datais recorded in the AirSim[57]simulator for simulating real-world data counterparts in a simulated environment for all domains for considering synthetic-to-real domain shift, and due to the impossibility of flying in adverse weather conditions.

It is a common practice for domain-adaptive network training to have the same number of samples in the source and target domains[13].
Therefore, we designed the DrIFT dataset to maintain a balanced number of samples across domains within both the training and validation sets as long as we had sufficient real data for the domains.

Background segmentation,as one of the contributions of the DrIFT dataset, is important for its innovative exploration of BG shift. All validation frames’ backgrounds have been segmented into sky, tree, and ground segments (Fig.1(d)) using the Track Anything platform[66,33]. By utilizing segmentation maps, it becomes feasible to utilize different metrics corresponding to different backgrounds (Tab.2, details inSec.4.1).

All annotations were then double-checked and refined by human annotators to ensure accuracy.

SECTION: 3.2Dataset Design

To address a deficiency in drone detection datasets, we designed DrIFT with a concentration on studying common domain shifts in the wild.

Synthetic-to-Real:In practical scenarios, capturing every conceivable real-world situation can be infeasible due to logistical challenges, resource limitations, and the prohibitive costs of annotation. To this end, we brought up synthetic data in order to initiate research on synthetic-to-real DS.
In DrIFT, all real-world data domains have simulated counterparts except for adverse weather conditions that do not exist in our real-world part of the dataset.

PoV Shift:The camera’s PoV change (ground and aerial) contains different BGs and orientations of the target objects. This shift can significantly impact detection performance, making it a distinct type of DS.

Weather Shift:drones cannot be easily deployed in adverse weather. On the other hand, because this is a common DS in the wild, the system must be robust. Therefore, synthetic data is collected to study weather DS.

Background Shift:The unseen background problem[68], also called BG shift in DrIFT, is present in various drone detection or autonomous driving datasets regarding the aforementioned DS. Nevertheless, no study has explicitly looked into the BG shift in object detection using BG segmentation maps. DrIFT investigates the BG shift from the sky to the tree and ground.

SECTION: 4DrIFT Benchmark

This section first provides a comprehensive overview of the methodology used for the benchmark.
The following subsection provides a comprehensive overview of the different benchmark scenarios. Subsequently, the results of the benchmark are reported.
This section concludes with a comprehensive analysis of the benchmark outcomes and the dominant challenges of the DrIFT dataset. The supporting statements will be presented in the supplementary materials.

SECTION: 4.1Methodology

The primary goal of the DrIFT benchmark is to evaluate the performance of OD models under various shifts and the capabilities of UDA methods to address this issue.
Performance metrics include average precision (AP), uncertainty metrics, and D-ECE which are reported BG-wise.

Letbe the dataset, whereare the input images andare the set of ground truth annotations containing bounding box coordinates and class labels for objects within each input image. The OD model predicts a set of detections, where each detectionconsists of bounding box coordinates, class label, and confidence score.andare the number of samples in the dataset and the number of detections for the i-th input image, respectively.

In OD, after initial detections, the Non-Maximum Suppression (NMS) process[45]filters out redundant or suboptimal detections. NMS first generates a set of candidates for each detection. These candidates are defined as all other predictions sharing the same class label and having an Intersection-over-Union (IoU), a measure of the overlap between bounding boxes, above a threshold. Detections below a confidence thresholdare then discarded. The candidate set for a given detectionis defined as:

After filtering, the remaining detection with the highest confidence score is retained as the final prediction. AP[45]has been employed to quantify OD performance.

D-ECE[35]is just used in our benchmark to study domain shift impacts on the calibration error. D-ECE[35]was calculated by binning the confidence space as well as box coordination parameters space in which there areequally distributed bins corresponding to the k-th dimension,. The goal of binning is to account for variations in calibration error across different confidence levels and spatial dimensions, ensuring that errors are captured in an unbiased manner. Therefore, D-ECE could be formalized

WithinEq.2,is used to describe the cardinality of the bin, whereasrepresents the total number of detections.denotes the mean confidence score of the detections within the bin, whereasis a statistical metric that quantifies the proportion of true positives among the detections in the bin.

We utilize MCDO-based and gradient self-information metrics to estimate uncertainty in the presence of DS and compare their capabilities with our proposed method,MCDO-map, to take advantage of them in our UDA method. The utilized methods are referred to asMCDO-NMSandGrad-loss, respectively. TheGrad-losscaptures the degree of epistemic uncertainty for each detection.Grad-loss-localizationandGrad-loss-classificationrefer to the localization and classification terms, respectively.

The MCDO-based method involves running multiple inference passes with dropout activated. Detections are matched to a candidate list based on the highest IoU threshold. The standard deviation of the localization parameters and the entropy of the mean classification probabilities are calculated for each list. This technique includesMCDO-NMS-localizationandMCDO-NMS-classification. For details on these methods, please refer to the supplementary materials.

As opposed to utilizing NMS-based or data association techniques in an MCDO scheme, a score map is constructed in a pixel-wise manner. Given the predictions, we convert the detection outputs to a 3D map. Letbe the set of detections for an input image. The score mapis a tensor of shape, whereandare the height and width of the input image, andis the number of classes. For each detection, the scoreis assigned to each pixel inside the bounding box,

Theis zero initiated, resulting in all-zero vectors for pixels that are not contained within any bounding box. For these pixels, we replace the all-zero vectors with a vector with a 1 for the background element and zeros for all other elements.
After populating the score map, we normalize it using the softmax function.
Next, we calculate the mean and standard deviation of the score map over multiple iterations of our object detector forward path that are

Finally, we compute the entropy of the mean score map,, and concatenate the standard deviations to create the uncertainty map,.
From an intuitive standpoint, it can be observed that increasing changes in localization parameters of the predictions are associated with a corresponding increase in the standard deviation of the boundaries surrounding pixels. For example, inFig.2, the left magnified detection in the target std map shows higher deviation (with colors closer to red) compared to the source std map, where the corresponding detection is mostly blue, indicating lower deviation across pixels.
Similarly, a higher frequency of change in prediction scores is shown to be linked to an elevated level of entropy. The same behavior in the entropy maps can be observed inFig.2.
In contrast to traditional MCDO-based approaches, instead of handling individual bounding boxes from each iteration and suffering postprocessing complexity[18], our method generates a pixel-wise score map during each iteration and avoids complex postprocessing.

DS occurs when the training (source) domainand the testing (target) domaindiffer, leading to a performance drop in machine learning models. Letand. We denote the source and target distributions asand, respectively. Distribution shift is defined as. If we considerand, the DS happens when

Our UDA method focuses on leveraging uncertainty information to enhance the robustness of the object detector in the presence of DS. We got inspired by ADVENT[64]while modifying it by changing the representation of input data to the discriminator and introducing a novel uncertainty estimation method.
The intuition behind this method is that DS introduces uncertainty in predictions, especially in regions where the model is less confident. Our uncertainty maps highlight areas where the domain shift has the most impact, guiding the adaptation process to focus on these challenging regions.
Following the concatenation process, theMCDO-mapis subsequently forwarded to a domain discriminator to fool it, initiating adversarial training (Fig.2). The calculation of the overall loss iswhere the detection lossis a combination of cross-entropy classification and smoothregression loss,. The adversarial lossis

when theis the discriminator network. The detection base network is updated to minimize the total loss,, while the discriminator network is updated to maximize the adversarial loss,.

SECTION: 4.2Benchmark Scenarios

We will begin our benchmark withTab.2, illustrating the impact of domain shift on object detection using the AP, uncertainty, and D-ECE metrics.Tab.3presents a comparison between our proposed MCDO-map method and other uncertainty metrics.
Finally, our novel uncertainty-aware UDA object detector is compared with SOTA UDA methods inTab.4. Supplementary materials have been provided to support our discussions.

Background-wise Metrics:To assess OD performance under BG shifts, we introduce BG-wise metrics, which calculate metrics separately for different BGs (e.g., sky, tree, ground). Given detectionsand ground truth, we classify each into BG categories using segmentation maps to identify the background category to which most of the pixels within the bounding box belong. The metricfor each BG category can be expressed asThese metrics provide a detailed analysis of how different BGs affect object detection performance.

SECTION: 4.3Experiments and Results

Tab.2shows different DS scenarios and their impact on object detection models. It is our contribution that metrics are reported background-wise, highlighting the influence of background shifts. The AP under the Sky column in row one is the reference. Significant AP decreases are evident, such as in row two when shifting from synthetic to real. PoV and weather shifts in rows 9 to 11 also show notable changes. Comparing the reference AP and the sky APs in other rows demonstrates decreased APs for tree and ground backgrounds (see Fig. 7 in supplementary material). The domain I inTab.2is the source domain all over the text unless other domains are mentioned.

We calculated the Kullback-Leibler (KL) divergence between source and target domain feature map distributions (see supplementary material)
to analyze the relationship between metrics and different shifts fromTab.2.Fig.3shows a heatmap of Pearson correlations among AP, D-ECE, MCDO-map, and KL divergence. The high positive correlation between our MCDO-map and KL divergence indicates the MCDO-map’s effectiveness in capturing DS.

The negative correlation of AP with MCDO-map and KL divergence suggests that higher AP corresponds to lower uncertainty and smaller feature map distribution distances. The positive correlation between AP and D-ECE indicates model miscalibration under DS. Additionally, the positive relation between D-ECE, MCDO-map, and KL divergence highlights their significant association with DS.

The goal ofTab.3is to compare our MCDO-map method with the MCDO-NMS and Grad-loss. Our method consistently shows increased uncertainty with DS, highlighting its effectiveness in capturing DSs. The wider violin plots for the MCDO-map in Fig. 4 of supplementary material demonstrate its superior capability to separate different DS levels compared to other metrics. MCDO-NMS-Classification for TPs shows some capability in separating different DSs, highlighted inTab.3and supplementary material, but requires supervision and often decreases with DS. Grad-loss-localization is consistently capturing the DS (Tab.3) but lacks the potential to separate DSs effectively.

Our results inFig.3and supplementary material further support the MCDO-map’s effectiveness in capturing DS. Thus, we conclude that the MCDO-map is the best method for our UDA approach, offering significant improvements over traditional uncertainty estimation techniques.

InTab.4, the results of some SOTA UDA object detectors on the DrIFT dataset are reported alongside our results. One significant DS is from sky to tree, where AP dropped from 67.1 to 0.2 (Tab.2, first row). Our UDA method outperforms others with an AP of 10.7 for the tree background and 46.3 in total, demonstrating its effectiveness in adapting to different BGs. Similarly, for the ground background domain, our method achieves an AP of 44.8 in total and 1.2 for the tree background, showcasing robustness.

For the aerial-synthetic-winter-normal domain, multiple BGs in each frame could take place, so we did not specify any BG for it. Our UDA method achieves the highest AP in total (17.8), sky (41.3), and tree (0.5), indicating its capability to adapt to different PoVs and complex scenes. In the ground-real-winter-normal-sky domain, our method achieves the highest AP in total (5.7) and tree (0.8), proving its effectiveness with real-world data and different seasons. The results are consistent for tree and ground backgrounds as well, demonstrating our method’s adaptability for two types of DSs occurring simultaneously.

Our method aims to deceive a domain discriminator by making the uncertainty maps for both source and target domains nearly identical. This dual focus on source and target domain alignment is crucial for robust performance across various DSs. The adaptation process involves a trade-off, accepting some degradation in the source domain to achieve significant improvements in the target domain.

SECTION: 5Conclusion

The DrIFT dataset addresses the need for a vast study of domain shift in drone detection by introducing fourteen distinct domains and emphasizing background shift utilizing background segmentation maps. Our findings show a positive correlation between MCDO-map uncertainty, domain shift, and D-ECE, and a negative correlation with AP. The MCDO-map outperformed other uncertainty metrics in capturing domain shift in the DrIFT dataset. Our uncertainty-aware UDA on object detection also surpassed SOTA methods in the DrIFT dataset. In the future, we aim to explore more nuanced domain adaptation techniques that minimize the source domain performance degradation, which is a drawback for our UDA method in some cases.

SECTION: References

SECTION: S1Introduction

This supplementary material contains important information that could not be included in the main paper due to space constraints and aims to support the discussions in the main paper. The structure of the main paper is followed.

SECTION: S2DrIFT Dataset

SECTION: S2.1Dataset Characteristics and Statistics

Fig. 1 in the main paper displays a variety of backgrounds from our dataset, including the sky, trees, and ground during three distinct seasons (fall, winter, and summer) or adverse weather conditions (foggy, snowy, and rainy).Fig.S1demonstrates that the DrIFT possessesimage frames.
As discussed in Subsec. 3.1 "The DrIFT Story" in the main paper, we attempted to keep the balance between training and validation sets for almost all domains:frames for training and 300 frames for validation.
This standard practice facilitates a proper platform for evaluating the UDA algorithms.Fig.S2shows the number of existing background samples in each domain. It is important to note that, as shown in the last three rows ofFig.S2, the dataset includes only a validation set for the aerial-real domains, without a corresponding training set.
Additionally, it is noteworthy that our adverse weather domains only contain a sky background. Hence we have avoided reporting metrics for tree and ground backgrounds within these domains inTab.S1.

Fig.S3depicts drones’ relative size and location distribution in real and synthetic domains. The center point, width, and height are normalized to the image width and height. For the aerial-real data inFig.3(c), the means of the relative width and height are approximately 0.015, whereas inFig.3(d), representing aerial-synthetic data, these values are around 0.02 and 0.015, respectively. The width and height of the ground-real, illustrated inFig.3(g), are about 0.03, although for the ground-synthetic shown inFig.3(h), these are approximately 0.02. These numbers indicate that we deal with extremely small objects in comparison to other applications,e.g., autonomous land vehicles[13,61]. It makes DrIFT more challenging in terms of training the detector models.

SECTION: S3DrIFT Benchmark

SECTION: S3.1Methodology

In[47,48], the researchers employed a gradient function and introduced the concept of self-learning gradient as a metric to evaluate the uncertainty of each detection. If we consider the supervised learning scenario the gradient of the loss function for each detection is. Theis the network’s weight vector. If the ground truth,, is replaced with detection,, and the detection is replaced with its candidates,, the self-gradient metric would be

The self-gradient metric,, referred to asGrad-loss, operates as a characteristic that signifies the degree of epistemic uncertainty, which is the focal point for investigating DS.Grad-loss-localizationis called the corresponding localization term of the loss, althoughGrad-loss-classificationpoints to the classification term in the loss. Nevertheless, it does not inherently encompass the true essence of uncertainty. To consider other methods, we employ a technique based on MC-dropout to capture the inherent uncertainty associated with each detection. In this approach, we activate dropout at inference time and run our model fortimes. Let us consider the output of the model at each iterationwhere. Initially, we create an-length list corresponding to all output detections in the first iteration. Subsequently, we perform some NMS like the one in Eq. 1 of the main paper to have a candidate list and assign the best candidate with the highestto each list. A detection in each iteration is only allowed to be a member of one list, and a new list is created if there is no option with a higherthreshold.
Ultimately, we calculate the standard deviation of localization parametersand the entropy of the mean of the classification probability vector,, respectively. If we assume we havelists of outputs, we can compute the uncertainty for each list as follows:

Here,is the number of members in each list,is the index of each list, anddenotes the mean of the underlying variable. This technique is referred to asMCDO-NMSwhich is divided intoMCDO-NMS-localization, referred to as, andMCDO-NMS-classification, referred to asinEq.S2. Inspired by[44], which suggests averaging individual uncertainties as one possible aggregation solution, we take a weighted average of classification entropy. Similarly, we sum the square residuals of localization parameters, take a weighted average, and calculate the square root at the end.

SECTION: S3.2Benchmark Scenarios

Normalization has been done for each metric by subtracting a reference value, which is the value of the metric for the source domain, and then dividing by the same reference value. The normalization is mathematically expressed by

For a set of values of a metricand corresponding reference value, the normalized set is.
The subtraction of the reference valueensures that the data is centered around zero, and the subsequent division byscales the data, making it comparable or suitable for further analysis.Fig.S4is illustrated using normalized values of different metrics. All metrics are normalized to their values for the source domain. Positive values indicate increases.

Violin box plot[27]is a graphical representation that combines aspects of both box plots and kernel density plots. It provides a concise and informative way to visualize the distribution, central tendency, and spread of a variable.
In the violin box plot:

The central box represents the interquartile range (IQR) of the data, with the line inside indicating the median.

The "violin" shape surrounding the box displays the probability density function of the data, providing insights into the distribution’s shape.

Wider sections of the violin indicate higher data density, while narrower sections represent lower density.

Outliers, if any, are often displayed as individual points.

Pearson correlation coefficient[11], denoted by, is a measure of the linear relationship between two variablesand. It is defined as the ratio of the covariance ofandto the product of their standard deviations,

The Pearson correlation coefficient ranges from -1 to 1. A value of 1 indicates a perfect positive linear relationship, 0 indicates no linear relationship, and -1 indicates a perfect negative linear relationship. Positive values indicate that as one variable increases, the other variable tends to increase as well. Negative values indicate that as one variable increases, the other variable tends to decrease. This coefficient has been used in Fig. 3 of the main paper to analyze the relationships between different metrics.

KL divergence[34]is a measure of how one probability distribution diverges from a second, expected probability distribution. In the DrIFT benchmark, it serves as a metric to quantify the distance between feature map distributions of different domains with the source domain, which is ground-synthetic-winter-normal-sky. The KL divergence is defined

in which N is the cardinality of the feature map distributions,. We assume the two distributions have the same size,.is the feature map distribution of each domain that is taken as the target domain, andis the source domain’s feature map distribution. i is the index of existing elements in each domain’s feature map distribution.

SECTION: S3.3Experiments and Results

The ground-synthetic-winter-normal-sky is taken as the source domain all over the paper and supplementary material unless we specify other domains.

For the object detector in this work, the Faster R-CNN[45]architecture with a VGG16[59]in the mmdetection platform[38]has been utilized. For generalization and MC-dropout uncertainty evaluation implementation, the dropout has been activated within the VGG. The experiments were run on a Desktop with a Geforce RTX 3090 and a High-performance computing cluster providing 4 x NVidia A100 (40 GB memory). For the vanilla network training that was started from scratch, we used a stochastic gradient descent optimizer for 73 epochs, for which the learning rate was 0.24 for a batch size of 6 on each GPU. For adaptation training, the vanilla network is used as the pre-trained weights. The learning rate has been decreased to, and the discriminator, which is a simple convolutional neural network, has been trained by an Adam optimizer with a learning rate of. The codes and details will be available on an online platform.

The objective ofTab.S1andTab.S2is to compare our uncertainty estimation method,MCDO-map, with various uncertainty estimation metrics mentioned in the main paper. InTab.S1, the source domain was ground-synthetic-winter-normal-sky, while ground-real-winter-normal-sky served as the source domain inTab.S2. To provide a comprehensive explanation, we utilizedFig.S6to discover a meaningful relation between different uncertainty evaluation metrics, AP, D-ECE, and KL divergence metric (which measures the distance between feature map distributions of different domains relative to the source domain) using the Pearson correlation coefficient. The findings inFig.S6could be summarized as follows:

MCDO-map exhibits the highest positive correlation (0.81) with KL divergence, indicating its superior capability to capture DSs. A greater level of shift, reflected by increased distance or KL divergence, correlates with higher values of MCDO-map.

As an uncertainty evaluation metric, a negative correlation with AP is expected, implying that higher AP values correspond to lower uncertainty levels. In this context, MCDO-NMS-Loc-Total, MCDO-NMS-Loc-FP, and MCDO-map yield the best results.

Positive correlations between D-ECE and most uncertainty evaluation metrics suggest that increased uncertainty tends to coincide with calibration errors.

A positive correlation between D-ECE and AP indicates that even with higher AP values, the model may exhibit over or under-confidence, compromising its reliability.

Positive correlations between D-ECE and most uncertainty evaluation metrics, such as 0.36 for MCDO-map, suggest that higher levels of uncertainty are associated with calibration errors.

Consequently, MCDO-map emerges as a wise choice for our UDA algorithm to capture DSs effectively.

To enhance the understanding of our results, we present three examples of the outputs generated by the trained Faster R-CNN model on the ground-synthetic-winter-normal-sky domain, depicted inFig.S7. InFig.7(a), the drone with a sky background exhibits low uncertainty, as indicated by the blue bounding box on the entropy map. We observe non-zero std values only at the edge of the bounding box in the std map (inside blue, red at the edge). However, a few false detections occur during the MCDO iterations, resulting in non-zero values in both maps around the intersection of the tree and ground. Moving toFig.7(b), the drone with a tree background demonstrates higher uncertainty. The bounding box exhibits some red areas in the entropy map, accompanied by non-zero standard deviation values inside the bounding box. Once again, false detections contribute to non-zero values in the maps. Finally, inFig.7(c), the drone with a ground background is detected with the highest level of uncertainty among these cases, corresponding to way too red color for the bounding box in the entropy map and nonzero values within the bounding box in the std map. However, a significant number of false detections around trees contribute to a considerable level of uncertainty in the maps, reflecting the low AP for trees and, consequently, higher uncertainty in this domain. Detailed AP and uncertainty values for trees are provided in Tab. 2 of the main paper.