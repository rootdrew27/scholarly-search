SECTION: Towards Modeling Data Quality and Machine Learning Model Performance
SECTION: Introduction
Machine learning-based models and algorithms are being used widely for many different tasks. They are increasingly being used in sensitive domains like healthcare, finance & banking, security, etc. to make critical decisions. In addition, most of the machine learning algorithms are "black-box" models which means the information of the internal workings is not known. As a result, we may not completely trust a machine-learning algorithm and its resultsand it becomes necessary to understand the strength and limitations of a model.

Biases, strengths, and limitations of machine learning algorithms can also arise from the quality of the data and inductive bias of the algorithm itself. In the machine learning domain, biases have also been referred to as "fairness". Fairness is the. An unfair algorithm and low-quality data can result in skewed results that can lead to incorrect conclusions.

A major reason for the data quality is the uncertainty and non-deterministic nature of data which is caused by the data source, and during data entry or acquisition. The non-deterministic nature of the data is usually considered as noise, but it may not necessarily be noise and there may be inherent properties of the data that are necessary for the analysis. For example, consider weather data that has some uncertainty due to patterns. Non-deterministic data would cause models to have difficulty in understanding and interpreting such data correctly. Most of the current research has focused on how to deal with the problem of non-deterministic nature of data sets. For example, filtering out noiseor enhance the quality by pre-processing by ’hand’ or automatically. There are numerous pre-processing methods to deal with the noise and non-deterministic nature of data, such as eliminating the noisy instances completely and interpolation to correct the non-deterministic data to match the deterministic data points. However, the pre-processing techniques are dependant on the task being performed. Hence, the non-deterministic nature of data can adversely affect the results of any data analysis and reduce accuracy of the model.

Understanding the effect of noisy and nondeterministic data in the model can help understand the performance and limitations of a model under uncertain and noisy conditions. Uncertainty in data is an important aspect to consider when measuring model performance. Uncertainty can arise due to imperfect or unknown information. Hence, if we can quantify and measure the uncertainty and noise, we could in fact get a true measure of the performance of a model.

Current machine learning algorithms only focus on measuring the accuracy of the model, as to how well the predicted data match the actual data. However, this information may not be enough, especially when we want to measure the performance of a model. In fact a research conducted byexplored the quality of data in anomaly detection and showed that the benchmark dataset used for anomaly detection had very little uncertainty in the data and hence, even the most simple of algorithms performed very well. The results from the paper showed that data quality also needs to be considered when measuring the performance of a model.

In this paper, we propose a framework that looks at quantifying data quality by as a function of uncertainty in the data set. In particular, we look at how uncertainty and non-deterministic data can effects accuracy in different model for different tasks and find the correlation between non-deterministic component in a data and the accuracy of the model. We focus on regression and classification tasks as non-determinism effects these tasks in different ways.

We begin by defining any data consisting of deterministic and non-deterministic parts. The non-deterministic part of a data can be noise or be present due to stochastic nature of data. The deterministic part of the data is defined by a definitive function. Consequently, we propose a new metric called the Deterministic-Non-Deterministic Ratio (DDR). DDR is basically the ratio of “magnitude" of deterministic and non-deterministic portions of a data. The higher the DDR, the data have a higher deterministic part and consequently a model will be more accurate and able to make predictions. The lower the DDR, the data have a higher non-deterministic part and the model will make less accurate predictions.

We prove this hypothesis by running experiments on different models and tasks using synthetic data. We look at the task of regression and classification. Through these experiments we draw DDR-accuracy plots. Using the DDR-accuracy plot, we can determine the performance of a model.

The research questions that we aim to answer are:

How does the score of a metric appropriate for a given type of machine learning model and calculated from the target values of a data set appropriate for that type of model and the predictions of the model based on the matrix / vector of feature values of that data set depend on the feature (vector) estimated from the matrix / vector of feature values of that data set?

How does the performance of a machine learning model type in a data set appropriate for that type of model depend on both the feature (vector) of the matrix / vector of feature values of that data set and the value of a metric appropriate for that model type and calculated from the target values of that data set and the predictions of the model estimated from the matrix / vector of feature values of that data set?

In summary, the main contributions of our paper are as follows: (1) we propose a new metric that can be used to quantify quality of data; (2) We formulate a model that explores the uncertainty and the determinism in data and its effect on accuracy; and (3) We use this model to measure a model’s performance and conduct different experiments on a variety of tasks and show how our model can be used to show the applicability of our model.

SECTION: Related Works
The need to develop a comprehensive technique to measure the performance of a model has been mentioned widely in previous literature. Currently, the modern way to measure performance is to measure the difference between predicted and actual values. But these methods of measuring performance do not include data quality and the uncertainty.

One way of measuring the performance is to use the concept of trust. There are few ways that trust in AI can be quantified.
Previous research has understood the importance of noise and its effect on the accuracy of a model. For example, the National Institute of Science and Technology (NIST) has modeled trust in the system in terms of two important parts. User Trust Potential, which is a function of the user, and Perceived System Trustworthiness, which is a function of the user, the system, and context. Consequently, the main attributes that define AI system trustworthiness are: Accuracy, Reliability, Resiliency, Objectivity, Security, Explainability, Safety, Accountability, and Privacy. Trust was measured as the user’s perception of each attribute.

An example metric for measuring trust in a machine learning method was proposed by Wong et al.. The measure of trustworthiness of a particular deep neural network is according to how it operates under correct and incorrect result situations; trust density, a description of the distribution of general trust of a deep neural network for a particular answer situation; trust spectrum, a model of general trust in accordance with the variety of possible answer situations across both correctly and incorrectly answered questions; and NetTrustScore, a scalar metric summing up the general trustworthiness of a deep neural network according to the trust spectrum.

Another aspect of trust is in relation to how the model interacts with different qualities of data. The quality of data is related to the uncertainty in the data, mainly arising during data collection. This quality of the data is based on some features, such as noise, fairness, or bias, found in the data. A model that cannot handle low-quality data would be considered low in performance and less trustworthy than other models. Mehrabi et al. looked at the quality of data in terms of fairness and produced a taxonomy of different fairness definitions that other researchers made to avoid bias and compiled the results to show the unfairness present in current AI systems and how researchers address these issues.

Quality in data was also studied by the papers inand. According to the authors the data source determines the quality of the data and they define data quality as the reliability of the source and the amount of control over the data slate of the source.

Most current learning methods evaluate performance in terms of size of a randomly picked dataset on which they work, a metric termed sample complexity, that emphasizes quantity over quality. However, by the No-Free-Lunch theorem, quantity sometimes does not correlate with quality. A second metric for the value of a data is its margin. The margin is defined as the smallest Euclidean distance between the convex hulls of the positive and negative elements of the data set.

Raviv et al. define the quality of the data set as the expected disagreement between a pair of random hypotheses from the set of hypotheses agreeing on the data set, a metric they term the expected diameter. They showed that two datasets with equal margins and drastically different sets of consistent hypotheses can give identical results. Tosh et al. also used diameter to define the value of a dataset. In their case the diameter of a set of hypotheses was the maximum distance over all pairs of hypotheses in the set of hypotheses. The hypothesis distance was induced by the distribution over the data space between two hypotheses and is the probability under the distribution over the data space of the outputs of the two hypotheses being equal.

Vapnik et al. propose a machine learning paradigm that reframes a problem from machine learning as a problem of estimating the conditional probability function as opposed to the problem of searching for the function that minimizes a given loss functional. This paradigm accounts for the relationship among elements of the data set and is therefore associated with data quality.

We also would like to mention the work inin which the authors have proposed a trust score for classifiers. They attempted to provide a more accurate trust metric than the basic confidence metrics typically offered by models that is calculated with some independence from the original model’s confidence scoring. They look at the trust score of certain models and then observe whether they got correct results for a high trust score model or incorrect results for a low trust score model by comparing the model’s agreement (or disagreement) with a Bayes-optimal classifier. However, in our work we focus on randomness and noise in the data and how it can effect a model’s trust and performance.

Looking at work related to data quality, Wang et al.developd a framework to investigate the research and development of data quality research and concluded that more research is needed to understand data quality and precision.

The impact of noise on machine learning accuracy was explored by. They explored noise as being class noise or attribute noise. Attribute noise is noise due to missing or erroneous values. Class noise is noise in classes. They looked at the effect on accuracy from noise by conducting different experiments. However, they only explored classification where in our paper we look at other tasks like regression and explore different models in each of these tasks and finally attempt to create a metric that can quantify model performance.

The concept of uncertainty in machine learning was studied extensively in the survey byand. They classify uncertainty as aleatoric and epistemic. Aleatoric uncertainty is due to underlying randomness in the data and epistemic uncertainty is due to lack of observed data. Using the ideas in the survey paper,compared four deep learning models: softmax neural network, bayesian neural networks, autoencoders and an ensemble of neural networks in handling uncertainty using entropy. However, these works are focused more on the model uncertainty rather than quantifying data uncertainty.

SECTION: Preliminaries
Data is typically classified as being structured or unstructured. Structured data is data that is organized and ordered, like tabular data. Unstructured data includes data with no organization, for example text data. Since unstructured data is converted into structured form, we focus on structured data in this paper. We assume that each column represents a variable and each row represents the record of the data.

We assume that any observed datacan then be expressed as the sum of the deterministic and non-deterministic components:

In the equation, the observed data is,represents the time index or any other relevant index from a finite index set.is the deterministic component andis the non-deterministic component.

The objective of this paper is to understand the relationship between the deterministic and non-deterministic components of data and how they influence accuracy of a model. We do not aim to quantify model accuracy as most previous works have done that. We hypothesise that non-deterministic components of data can effect the performance of a models in different ways. Any model can easily determine the deterministic component in data, but it is the non-deterministic component that truly effects a model. By quantifying the non-deterministic component of data and using it as a data quality metric, we can better determine the performance of a model.

A non-deterministic function returns different results every time it is called, even when the same input values are provided.

The deterministic component represents the systematic or predictable part of the data. This component can be a function of known predictors, covariates, or any other deterministic relationships and is produced by a well-defined function that always produces the same output when same input values are provided.

On the other hand, the non-deterministic component of the data is the uncertainty, randomness or unpredictable fluctuations in the data that can be produced by a function that produces different outputs when the input value is same.

Non-deterministic component of the data can be generated due to many reasons. Random noise or the random nature of the environment can cause randomness and the non-deterministic component in the data. Random noise is often a considered a large component of the noise in data. Random noise is an unavoidable problem. It affects the data collection and data preparation processes, where errors commonly occur. Noise has two main sources: errors introduced by measurement tools and random errors introduced by processing or by experts when the data is gathered.

Throughout this paper we refer to the non-deterministic component as noise even though in some cases it may not be noise as the non-deterministic component may be an inherent property of the data.

For this purpose, we define a new metric that can quantify the quality of data in terms of how much deterministic and non-deterministic components. We call this new metricfor a data.

The inspiration forcomes from the concept of signal-to-noise ratio (SNR or S/N). SNR is a measure used in science and engineering that compares the level of a desired signal to the level of background noise. Signal-to-noise ratio is defined as the ratio of the power of a signal to the power of background noise. Depending on whether the signal is a constant or a random variable, the signal-to-noise ratio for random noise becomes the ratio of the square or mean square of the signal to the mean square of the noise.

Assumeis normally distributed with mean zero and constant variance (homoscedasticity), and thatandare independent. Theofcan be defined as the ratio of the powers of the deterministic componentand the dataset:

whereandare the power ofandrespectively and the powerof and any valueas:Using the definition of power, thecan be expressed as (the derivations are in Appendix):If, then there is more non-deterministic component in the data and ifthen there is more deterministic component in the data. Hence, when, then the accuracy of a model would be low as there is more randomness in the data and when, then the accuracy of a model would be high as there is more deterministic component in the data.

SECTION: Methodology
To understand the performance and trustworthiness of machine learning algorithms, most previous works (e.g.) have used synthetic data. The synthetic data are generated by adding noise to a real-world or a generated data set. We call this a top-down approach.

However, we propose a bottom-up approach to generate the data set to measure the performance of a model. In other words, we use a predetermined value for the noise level and modify the real-world or generated data so that the noise level is incorporated into the data set. This gives more control over the noise in the data and standardizes the data set and helps measure the effect of deterministic and nondeterministic components of a data set more fully.

To plot accuracy vs. DDR for several models for a given task, we need to be able to compare accuracy for datasets with different DDRs and models. Typically, one can accomplish this comparison by standardizing the datasets so that for each dataset the mean isand the variance is. In addition, we would like the accuracy vs. DDR plots to be representative so that the DDRs of the points are uniformly distributed. However, standardization may change the DDRs of datasets so that the distribution of DDRs is no longer uniform.

The noise is added to the features of the data set. The features are the explanatory variables and adding noise to the explanatory or dependent variables affects the independent variable. Since, the objective of any model is to predict the independent variable adding non-deterministic component to features would also effect a model’s ability to predict the independent variable.

In the real world splitting the data into deterministic and non-deterministic components is very complicated and the solution may be unknown. There may be infinite possible combinations of deterministic and non-deterministic components. Only an approximate solution may be obtained; e.g. using autoencoders could be used to split deterministic and non-deterministic data. However, for this paper we assume that a data can be split into the deterministic and non-deterministic components.

To answer the research questions, our aim is to look at how the accuracy changes with changing. Using the accuracy-plot, we can obtain useful information that can tell us about the performance of a machine learning model under different levels of non-deterministic components of the data. For the rest of this paper we consider non-determinisitic component as noise.

First, we have to obtain an estimate for thewhen the dataset is of finite length with zero centered, homoscedastic noise. Equationcan be re-written as:

where the observed data iswithvalues. For each value, there is a deterministic componentand non-deterministic component as a random variable or a stochastic process and denote it by.

Equationforcan be expressed as:

It should be noted that whenis large, then we can get Equation.

There are multiple ways in whichcan be formulated for a vector of signals. In addition, theis calculated so it is betweenand.

The first option to redefineis to definein terms of each signal. In this case,would be the mean of sum of, but this method would not work when the differences betweenare too large and could lead to incorrect accuracy-plot. We run into similar issues of taking mean if we consider-norm of the. On the other hand, if we considerto be the 1 or 2 norm of thes of, thenof the set may be greater than 1.

The major problem is how to scale the. To scale thewhen considering the deterministic and non-deterministic component of the data, we propose, a process in which the deterministic and non-deterministic parts for a signal are scaled such that the signal is standardized but the DDR of the signal is preserved. Let,,, andbe the same as in the definition for signal DDR;be the DDR-invariant standardized dataset; andand, be the objects derived fromin the same wayandwere derived from. The algorithm for DDR-invariant standardization is presented in Algorithm.

: DDR-Invariant Standardization:is DDR-invariant standardized if it fulfills the following criteria:

Using approximations,can be approximated as (see Appendix:

whereis the sample standard deviation ofandis the sample mean of.

To obtain, we definefor sets of signals "at a lower level" by expanding the definition of power for a signal to a set of signals. We obtain a representative accuracy for a given data set, we average the accuracy for a set of data sets generated by hit-and-run sampling.

Hit-and-run sampling belongs to a class of procedures called symmetric mixing algorithms that recursively generates a sequence of points all within a given region. These points have the property that when the initial point is uniformly distributed within the region, all of them are. Moreover, the last generated point is asymptotically independent of the first point as the sequence grows in length. The general mixing algorithm is given in Algorithmin Appendix. Hit-and-run algorithm is the same algorithm with.

In hit-and-run sample analysis-tuples of, each with the same, from the convex polytope defined by the relation between,and by the constraints onare generated. Ideally, we would sample-tuples ofso that the resulting-tuples are approximately uniformly distributed. However, under the 2-norm definition, and after a process we term, the relation between,becomes. In this case, hit-and-run sampling guarantees that the-tuples of squares ofs, and not the corresponding-tuples ofs, are approximately uniformly distributed.

To generate the accuracy-plot, we consider many different types of models. To measure accuracy, we look at different metrics. The metrics are dependent on the type of task and can be found as labeled in the plot. We look mainly at regression and classification.

SECTION: Model Performance Metric
In this paper a new metric, the trustworthiness portfolio, was defined. The trustworthiness portfolio measures the performance of a model. As a single-number measure of the reliability of a model M on a given noisy data set Y, for a given model and the deterministic component, trustworthiness portfolio can be obtained from the accuracy-.

The trustworthiness portfolio should be a metric that looks at the change in performance of a model when the non-deterministic component (or the noise) changes. Ideally, performance should not change whenchanges. Sinceand accuracy are normalized betweenand, the maximum value of the trust should be 1. Hence, the trustworthiness portfolio is between. When data set hasthen the data has a high non-deterministic component level and there is no reliable way of making predictions without a lot of pre-processing.

Hence, a simple way of defining trustworthiness portfolio is:

The above definition of metric is for a single point. For a specific model and dataset, we can use the accuracy-DDR plot to measure the true performance of a model under uncertain conditions. The true performance of a model is defined as follows:

whereis the function from the accuracy-DDR plot. It should be noted thatis equivalent to the area-under-the-curve of the accuracy-DDR plots. Whenis close to 1, then model has high performance and can perform well under uncertain conditions. This happens when the model accuracy does not change with changing DDR which is ideally how a model should perform. Whenwhen model’s accuracy changes with changing DDR and the lower the value, worst is the performance of the model under uncertain conditions.

SECTION: Experimental Design & Result
In this section, we look at the experiments that we performed to create the accuracy-DDR plots. The experiments were done using Python and implemented Pytorchand Scikit-learn. Pytorch was generally used for data generation and implementing different machine learning models. Scikit-learn was used to get basic machine learning models, along with other places used to get widely accessible versions of common and uncommon models. Our code can be found at

We collected scores for ten types of machine learning models on multiple appropriate
datasets with varying DDRs of feature matrix. The models were not altered in any way from where we obtained them unless specified in the code, this includes no hyperparameter tuning being performed. We included ten supervised learning model types, which consisted of five regression model types and five binary classification model types. The regression model types included a linear regression model type, Ordinary Least Squares Regressor (OLSRR); Decision Tree Regressor (DTR); a support vector regression model type, Linear Support Vector Regressor (LSVR); a nearest neighbors regression model type, K-Nearest Neighbors Regressor (KNNR). The binary classification model types included a linear binary classification model type, Binary Logistic Regression Classifier (BLRC); Decision Tree Classifier (DTC); a support vector binary classification model type, Linear Support Vector Classifier (LSVC); a nearest neighbors binary classification type, K-Nearest Neighbors Classifier (KNNC); and a neural network binary classification model type, Multilayer Perceptron Classifier (MLPC).

We measured the results of each model’s accuracy based on it’s type. All five regression models’ accuracy was measured using normalized mean square error (NMSE)-based accuracy. The five binary classification models’ accuracy were measured using F1-Score.

The plots of the results can be found in Figures,,,,for the regression algorithms and,,,,for the different classification algorithm. The plots show the accuracy for both the training and testing samples.

The results of the plots show a clear trend. As DDR increases, accuracy also decreases which translates to as there is more deterministic component in the data, higher is the accuracy. For the accuracy-DDR plot for the regression algorithm, the accuracy-DDR relationship is almost linear. Except for multi-layer perception (MLP), accuracy does not change significantly with DDR. This indicates that non-deterministic component in the data has very little effecft on MLP. Hence, it is the most reliable algorithm compared to all the other regression algorithms.

When it comes to classification, the trend is also linear except for the low DDR values, where the relationship is slightly logarithmic. In addition, the linear relationship gradient is also more gradual compared to regression plots. Here again MLP has the best performance compared to the other classification algorithm.

Finally, in Figuresandwe show the trustworthiness portfolio of a model using equationreferred to as normalized AUC on the y-axis. For both regression and classification models KNNC models perform the worst with uncertain conditions. DTR has the best performance in regression. MLPC classification models perform the best under uncertainity which verify prior research and their popularity in usage as classification models.

SECTION: Conclusion
In summary, we proposed a metric called, which we showed to be a valid measure of the non-deterministic component in a dataset and the trustworthiness portfolio for a model that incorporatesto measure the performance. Our results also in line with the claims from other research like Zhu et al.who showed that the accuracy of a decision tree classifier increases as the noise level of a data set decreases. But we have gone beyond the work in previous research by creating a framework that can consider different machine learning types, model types, and evaluation metrics.

We also proposed a new bottom-up approach of generating synthetic data andto standardize the data for measuring the true performance of a model.

Our method here is preliminary and there are multiple avenues for future research. Our method could be applied to the new concept of data-centric AI which involved data engineering and developing data-driven AI models. The work described here could also be a step towards explainable AI and developing better understanding of AI models, especially under uncertain conditions. Other potentials for future research include further generalizing the results of this study by including clustering model types, ensemble models, or complex neural networks, using multiple appropriate evaluation metrics per learning type, using multiple appropriate data sets per model type with varying numbers of features, using class noise or both feature and class noises, or modeling uncertainty and noise using multiple kinds of feature noises such as heteroscedastic noise and/or uniform noise.

We also would like to extend the definition for trustworthy portfolio, e.g. by proposing a Bayesian-based definition that also incorporates other properties of the dataset in addition to uncertainty.

SECTION: References
Supplementary Material

SECTION: Appendix A
In this section, we derive the equations for.

Using the definition of power, thecan be expressed as:

Ifis very large, then:

Sinceandare independent. then:

SECTION: Appendix B
For large, the expectation can be approximated by the sample mean, and the following:

Next, we assume thatis a linear transformation of:

whereis positive and. Upon solving for,, and, we found that:

SECTION: Appendix C
SECTION: Appendix D