SECTION: ViTOC:Vision Transformer and Object-aware Captioner

This paper presents ViTOC (Vision Transformer and Object-aware Captioner), a novel vision-language model for image captioning that addresses the challenges of accuracy and diversity in generated descriptions. Unlike conventional approaches, ViTOC employs a dual-path architecture based on Vision Transformer and object detector, effectively fusing global visual features and local object information through learnable vectors. The model introduces an innovative object-aware prompting strategy that significantly enhances its capability in handling long-tail data. Experiments on the standard COCO dataset demonstrate that ViTOC outperforms baseline models across all evaluation metrics. Additionally, we propose a reference-free evaluation method based on CLIP to further validate the model’s effectiveness. By utilizing pretrained visual model parameters, ViTOC achieves efficient end-to-end training.

SECTION: 1Introduction

In recent years, with the rapid development of computer vision and natural language processing, image captioning has emerged as a popular research topic in multimodal fields. This technology bridges visual and linguistic information by interpreting image content and generating natural language descriptions. Image captioning finds extensive applications in areas such as autonomous driving, intelligent surveillance, search engines, and assistance for visually impaired individuals. Although models based on Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), and Transformers[1]have achieved notable progress in this task, challenges remain in semantic accuracy, description diversity, and generation efficiency.

Many studies on image captioning follow the encoder-decoder framework. Research in[2]summarizes the impact of varying scales of image encoders, text decoders, and multimodal interaction modules on overall model performance. The findings suggest that when the visual model is larger than the text model, the generated descriptions tend to be of higher quality. Based on this insight, we focus on improving the visual model by using multiple image encoders to enhance the representation of visual features.

For images that are rare or absent in the training set, the model may generate irrelevant objects in its descriptions, as shown in Figure1. This phenomenon is known as "hallucination" in natural language generation. Object detectors, by identifying key objects and scene elements within an image, help the text model better understand contextual information, leading to more accurate and descriptive captions. Introducing an object detector also addresses the long-tail problem, which refers to the challenge of generating descriptions for rare objects or scenes. By recognizing these rare elements, the object detector enhances the model’s ability to describe long-tail categories. Based on this approach, we employ ViT with an object detector as the visual model and a Transformer as the text model, forming an end-to-end image-to-text generation system.

Our key contributions can be summarized as follows:

We propose a visual-language model architecture based on ViT, Object Detector, and Transformer. In this architecture, the visual model uses frozen pretrained parameters, eliminating the need for fine-tuning and significantly improving training efficiency.

We design a method to incorporate object detector outputs as prompts, demonstrating the effectiveness of this detector-based prompting strategy.

ViTOC outperforms the baseline model in all indicators.

SECTION: 2Related Work

SECTION: 2.1The Early Models and Development History of Image Captioning

Many early image captioning models[3,4,5]are based on the CNN-RNN framework, where the CNN extracts image features, and the RNN is well-suited for text generation due to its sequential structure. However, RNNs often struggle with long-term dependencies when processing lengthy sequences due to issues like gradient vanishing or explosion, limiting their ability to retain long-term information. With the advent of the Transformer, text models benefited from the Attention mechanism, allowing them to capture broader contextual information. Consequently, some studies[6,7,8,9]have used Transformers to enhance text generation models. Later, large language models (LLMs) like GPT-2[10]demonstrated strong text-processing capabilities. As a result, some work, such as[11], has leveraged these models for their extensive knowledge base and text understanding abilities as text models. On the visual side, the Vision Transformer (ViT[12]) introduced an innovative approach by dividing images into patches, thereby framing image processing as a sequential problem. Compared to CNNs, Transformers offer superior global modeling capacity, prompting some studies[13,14]to adopt ViT to enhance visual models.

SECTION: 2.2Image Captioning Improvement by Multi-task Learning

In the field of image captioning, jointly training multiple related tasks is a common approach to improve model performance. Typical tasks include Image-Text Contrastive Learning (ITC), Image-Text Matching (ITM), and Image-Text Generation (ITG). By jointly training these tasks, MTL enables the model to learn shared representations, enhancing the accuracy and robustness of generated captions. Specifically, ITC aligns image and text representations in a shared embedding space[15], ITM ensures the alignment between images and their descriptions[16], and ITG generates relevant captions based on the learned features[17]. This approach makes the generated captions more accurate and semantically rich.

There are also examples where image captioning is combined with other multimodal tasks. For instance,[18]combines image captioning with object detection, and[19]integrates it with visual question answering. These approaches can significantly improve the model’s performance and stability, but they also greatly increase computational costs.

SECTION: 2.3Image Captioning Improvement by Pretrained Models

In recent years, methods based on pretrained models have achieved significant progress in the field of image captioning. Many researchers have used pretrained vision models (such as ViT, CLIP, etc.) to initialize image encoders. This approach reduces training time and improves model performance. These pretrained models are typically trained on large-scale datasets, enabling them to learn rich visual features that can be effectively transferred to downstream tasks.

However, directly using these pretrained models can present challenges. While they perform well on general tasks, they may not fully meet the specific requirements of a particular task. To address this, some researchers have improved existing pretrained models by adjusting their structure or training strategies. For instance,[20]introduced specific prompting techniques to enhance the alignment between image and text models, improving performance in image captioning tasks. Similarly, Chen et al.[19]designed a qformer module to convert visual features extracted by pretrained image models into a format that text-based models can understand, further improving performance in multimodal tasks.

One major advantage of using pretrained models is the significant savings in computational resources. This is particularly beneficial for researchers with limited computational power, as using publicly available pretrained models for transfer learning avoids the high costs associated with training large models from scratch. This approach has led to improvements not only in accuracy but also in generation speed and model efficiency.

SECTION: 2.4ViTOC: A New Image Captioning Scheme

In this study, we introduce the ViTOC (Vision Transformer and Object-aware Captioner) model. It employs a dual-path architecture that combines a pre-trained Vision Transformer (ViT) with an object detector. This design integrates global visual features with local object information, enhancing the model’s ability to perceive objects. Additionally, ViTOC incorporates an innovative object-aware prompting strategy, which aims to improve the model’s handling of long-tail data.

SECTION: 3Vision Transformer and Object-aware Captioner

In this section, we introduce ViOC, as shown in Figure2. This novel strategy employs a multi-path structure, using ViT and an object detector to extract image features while simultaneously providing object hints to the text decoder. This approach enhances the quality of generated captions.

SECTION: 3.1Network Architecture

Visual ModelIn the design of the visual model, a single image encoder is typically used to extract image features. ViT enhances this process by dividing the image into patches, serializing them, and adding positional encoding. With its self-attention mechanism, ViT can effectively capture global image features. As previously mentioned, due to the long-tail effect in datasets, the model may exhibit a “hallucination” issue when generating captions for certain images. For example, in the Flickr30K dataset, the ratio of samples with people to those without is approximately 10:1, leading the model to overly favor person-related tokens when predicting new samples. While this is less problematic in cross-modal retrieval, it significantly impacts image captioning. To address this, we introduced an object detector that lists the objects present in the image, providing hints for the text decoder to improve caption accuracy.

Visual Feature FusionThe object tokens generated by the object detector cannot be used directly; they need to be converted into vectors. We achieve this by sharing the embedding layer with the text model, which transforms the object tokens into vectors and concatenates them with the features extracted by the ViT. Typically, feature fusion is handled by a Transformer Encoder, which enables full interaction between all features and facilitates effective multi-modal fusion. However, because ViT generates a substantial number of patch tokens, this increases computational cost, and relying solely on the embedding layer for textual information may limit the model’s expressive capacity. Therefore, we employ a Transformer Decoder along with a set of learnable vectors to integrate multi-modal features, an approach found in[11,19,21]. These learnable vectors act as queries, with the fusion module output serving as input to the text model. This not only compels the model to focus on the most crucial information for caption generation but also reduces computational load. The number of learnable vectors, treated as a hyperparameter, can be determined based on either the number of objects in the image or the desired caption length, providing two distinct design perspectives.

Textual ModelSimilar to most caption generation models, our textual model uses a Transformer Decoder. The output from the visual feature fusion module serves as the input to the Decoder, while tokens from the ground truth are used as queries. Causal attention is applied to handle multi-modal interactions. The output of the Decoder is then passed through an MLP layer to obtain the probability distribution of each token.

SECTION: 3.2Training

In general, image captioning datasets consist of an image paired with multiple captions. A common approach is to split each sample into multiple image-caption pairs. Due to the long-tail effect in the dataset, the model may tend to generate descriptions related to the task. Even with shuffling, image-caption pairs with similar semantics or containing the same objects may still cluster together. To address this issue, we treat each image as an iteration unit and randomly select one caption from the set of corresponding captions during training. To further enhance the model’s robustness, we also randomly mask tokens in the ground truth. These adjustments significantly reduce the hallucination phenomenon in the model’s generated descriptions. We use Language Modeling (LM) loss, where the task is to minimize the difference between the predicted word and the target word, with cross-entropy loss being a common choice. This enables the generation model to train by predicting words one by one, gradually generating coherent word sequences.

SECTION: 4Experiments

SECTION: 4.1Setting

We use the COCO 2017 dataset[22]for both training and testing. For the visual encoder, we select the pre-trainedViT_B_16_Weights.IMAGENET1K_V1model provided by PyTorch. For the object detector, we use the YOLO-Tiny model[23], which has been fine-tuned on the COCO 2017 object detection task. The tokenizer used is a distilled version of BERT[24], which is case-insensitive. Therefore, only the embedding layer, Transformer decoder, and learnable queries require training. Since the patch token dimension in ViT is 768, we also set thed_modelto 768. The number of Transformer layers can be set to either 3 or 6, and the number of attention heads per layer is fixed at 8. We use the Adam[25]optimizer along with the CosineAnnealingLR learning rate scheduler. During training, the learning rate gradually decays from 3e-5 to 1e-6 over a maximum of 30 epochs. The object detection threshold for the detector is 0.9, and the object list can optionally be deduplicated.

SECTION: 4.2Beam Search

Beam Search is a widely used decoding strategy in image captioning tasks. It improves over greedy search by retaining multiple candidate sequences, thus avoiding the issue of local optima. The steps of Beam Search are as follows:

Initialization: Starting from the initial token (usually<START>), the probability distribution of each word in the vocabulary is calculated.

Expansion: At each time step, for every candidate sequence, all possible next words are added, and their probabilities are computed.

Selection: Based on the cumulative probabilities, the topcandidate sequences are retained, whereis the Beam Width.

Recursion: The expansion and selection steps are repeated until the<END>token is generated or the maximum sequence length is reached.

Beam Search improves the quality of generated captions by considering multiple candidate sequences, but it also increases computational overhead. By adjusting the value of Beam Width, a balance can be found between caption diversity and computational efficiency.

Figure4shows the performance improvement of the beam search method over the sample search method across common image captioning metrics. Figure4compares the differences in the results returned by the object detector, specifically the duplicate and unique outcomes. The results indicate that applying beam search leads to significant improvements in all metrics for every model, confirming the effectiveness of beam search in enhancing image caption quality.

SECTION: 4.3Results on Image Captioning

We conducted experiments on the validation set of the COCO dataset using beam search (beam width = 4) for inference and evaluated the results with metrics such as BLEU ([26]), ROUGE-L ([27]), CIDEr ([28]), and SPICE ([29]). An interesting observation was that models with high metric scores did not necessarily perform well on all samples, as shown in FigureLABEL:fig:fig3. To address this, we adopted a CLIP-based reference-free metric, where the similarity between the image and captions generated by different models was calculated using CLIP, with the highest similarity score taken as the final score. This voting-like approach provides a better comparison of model performance.

In Table1,andrepresent the duplicate and unique object lists returned by the object detector, respectively. Queries indicate the number of learnable vectors, while Transformer Layers refer to the number of Transformer Decoder layers used in the two multimodal fusion modules. The ViT + Transformer model serves as the baseline, which does not use an object detector.

It can be observed that models using the duplicate strategy consistently outperform the baseline model. The performance of the models is generally positively correlated with the number of Transformer layers. However, the number of learnable vectors does not necessarily improve performance as it increases; there is an optimal value.

Table6presents the results of different models in the CLIP Vote evaluation, which align closely with the performance rankings from traditional metrics. This indicates that, despite using a new evaluation method, there is no significant change in the model rankings. Figure6converts the table into a pie chart, providing a visual representation of the relative performance of the models.

SECTION: 4.4Hallucination

To verify whether the introduction of the object detector can mitigate hallucinations in the model, we conducted the following experiment: We randomly selected 3,000 images without people from the COCO test set and generated captions for these images. By checking whether the captions contain words related to people, we can partly detect hallucinations. The results, shown in Table3, suggest that higher accuracy corresponds to fewer hallucinations. Interestingly, the ViTOC model with 16 learnable vectors outperformed the model with 32 learnable vectors. This could be because the number of learnable vectors exceeded the maximum number of objects in the dataset, causing a mismatch. Alternatively, to improve the overall quality of the captions, the model might have focused more on image features, neglecting the integration of multimodal information.

SECTION: 4.5Analysis

In this experiment, we used the COCO 2017 dataset for both training and testing. The results show that the ViTOC model, when using the object detector’s repeated object list, achieves the best overall performance, particularly excelling in CIDEr and SPICE scores. Ablation experiments further validate the effectiveness of incorporating the object detector.

We examined two strategies for determining the number of learnable vectors: the maximum number of objects and the maximum caption length. The dataset’s maximum object count is 10, and the longest caption length is 84. The experimental results show that the optimal number of learnable vectors for ViTOC is 32, which is close to the maximum object count. The remaining vectors likely correspond to object attributes or relationships between different objects. Regarding the number of Transformer layers, the best configuration is 6 layers. Although the model performs poorly when the number of learnable vectors is set to 64, this could be due to an insufficient dataset size or flaws in the fusion method. Future work could improve model performance by increasing the dataset size or refining the fusion strategy.

SECTION: 5Limitations

In Section4.1, we mentioned that ViT uses the pretrained parameters is the most basic version, chosen due to limited computational resources. However, usingViT_H_14_Weights.IMAGENET1K_SWAG_E2E_V1might improve the performance of ViTOC. Additionally, ViT can be replaced with other image encoders, such as EfficientNet or Swin Transformer. The object detector we used, Yolo-tiny, is also a basic version of the YOLO series. Replacing it with more advanced models like Yolo-base could provide more accurate object labels, potentially enhancing the performance of ViTOC.

SECTION: 6Conclusion

We observed that image captioning models often produce inaccurate descriptions or hallucinations when dealing with uncommon images. We hypothesize that this issue is not caused by dataset limitations. Therefore, we made improvements to the model architecture by introducing a method that combines object detectors with embeddings. This approach integrates image features with object-specific features through learnable vectors, providing useful hints to the text decoder. To evaluate the performance of the ViTOC model, we randomly selected 5,000 images from the COCO dataset for testing. Experimental results confirm the effectiveness of the learnable vector fusion strategy and demonstrate that ViTOC outperforms the baseline model.

SECTION: References