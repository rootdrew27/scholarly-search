SECTION: Co-Learning: Towards Semi-Supervised Object Detection with Road-side Cameras

Recently, deep learning has experienced rapid expansion, contributing significantly to the progress of supervised learning methodologies. However, acquiring labeled data in real-world settings can be costly, labor-intensive, and sometimes scarce. This challenge inhibits the extensive use of neural networks for practical tasks due to the impractical nature of labeling vast datasets for every individual application. To tackle this, semi-supervised learning (SSL) offers a promising solution by using both labeled and unlabeled data to train object detectors, potentially enhancing detection efficacy and reducing annotation costs. Nevertheless, SSL faces several challenges, including pseudo-target inconsistencies, disharmony between classification and regression tasks, and efficient use of abundant unlabeled data, especially on edge devices, such as roadside cameras. Thus, we developed a teacher-student-based SSL framework, Co-Learning, which employs mutual learning and annotation-alignment strategies to adeptly navigate these complexities and achieves comparable performance as fully-supervised solutions using 10% labeled data.

SECTION: 1Introduction

Object detection has significantly advanced with the advent of deep learning techniques[1]. While current approaches perform exceptionally well with ample labeled data and computing resources, they often falter in data-limited scenarios, especially on edge devices. Costly, labor-intensive dataset preparation poses significant challenges for autonomous driving models, particularly in urban areas or where roadside cameras are dense. Previous studies[2,3]primarily focused on supervised learning with synthetic data or on edge devices, both requiring labeled data from target scenarios. However, obtaining labeled data for every possible use case is resource-intensive and often impractical, leading to the emergence of semi-supervised learning (SSL) as a potential solution.

Hence, to boost the application of Semi-supervised Object Detection (SSOD) methods on edge devices, we propose a novel learning pipeline Co-learning, from data curation to learning execution, as delineated in Figure1, to mitigate the challenges inherent in SSOD, such as data inconsistency. Adhering to the constraints set by the AI CITY CHALLENGE dataset[4], we employ data from Track 2. Our goal is to bridge the existing gap in limited data and resources by introducing a semi-supervised learning strategy for object detection. This strategy intends to enhance the utilization of unlabeled data during the learning phase, leveraging both labeled and unlabeled data to improve object detection with generalized features.

To achieve this, based on a teacher-student network (TSN), we start with 10% of the data from AI City Challenge[4]Track 2 for supervised learning. The remaining 90% of the data is used as unlabeled data to train the model in a semi-supervised manner. However, in unsupervised TSN, a small deviation in the teacher network will result in significant noise at the boundaries of pseudo-boxes, causing erroneous targets to be associated with nearby objects under static IoU-based assignment. This owing to some inactivated anchors being falsely assigned as positive in the student network and will guide the overfitting of the student network, as well as producing inconsistent labels for neighboring objects, that belong to the same category. By employing refined, consistent pseudo-labels, we ensure that the student network learns from a set of pseudo-labels that are more aligned with true situations, enhancing overall object detection performance.

The remainder of the paper is structured as follows: Section2discusses related work. Section3details the data cleaning process and proposed learning pipeline, incorporating stemming and lemmatization for text descriptions and the SSOD approach. Section3.4outlines the experimental setup and presents initial results of the proposed learning framework. Finally, Section4concludes the paper and presents the next step of the proposed framework.

SECTION: 2Related Work

SECTION: 2.1Object Detection

Object detection has been substantially influenced by the rapid advancement of deep learning techniques, which is a task that not only identifies the class of objects but also localizes them within images or videos[1]. Its pipeline is similar to image classification but includes some additional steps, such as data acquisition with corresponding bounding boxes around the objects of interest and labels indicating the class of each object. Modern object detectors mainly consist of two principal architectures: single-stage and two-stage models. Two-stage detector models, such as Faster-RCNN (FRCNN)[5], are characterized by an integrated framework that includes a region proposal network (RPN), alongside a classification and regression head. In this way, the backbone extracts features from the input image, while the region proposal algorithm generates potential object bounding boxes. These features are then passed through the detection heads to perform object classification and localization. On the other hand, one-stage models, such as YOLO[6], SSD[7], and RetinaNet[8], generate bounding box predictions and class probabilities without an RPN. Additionally, transformer-based models have been applied to object detection. The DEtection TRansformer (DETR)[9]uses a transformer to process the input image and generate object proposals, which are then refined using a feedforward network. Object detection has many applications, such as identifying pedestrians in self-driving cars[1], detecting objects in surveillance footage[10], and identifying defective objects in automatic production assembly lines[11]. In this work, we employ Faster-RCNN[5]as the base detector for both the teacher and student networks.

SECTION: 2.2Semi-supverised Object Detection

In SSOD, a prevalent approach involves the generating of pseudo-bounding boxes through a teacher model, with the anticipation that student detectors will yield uniform predictions on enhanced input samples[12,13,14]. Traditionally, two-stage detectors[12,14,15]have led the way in conventional SSOD techniques, showcasing their dominance. However, single-stage detectors[16,17,18]have also emerged as formidable contenders, noted for their straightforward design and superior performance. In this work, considering the localization quality of potential objects, we utilize a two-stage teacher-student SSOD approach, primarily addressing the issues of inconsistency present therein, which involves adaptive proposal assignment, pseudo label refinement and alignment, and a dynamic threshold mechanism, all aimed at enhancing label quality with less annotation cost.

SECTION: 3Methodology and Experiments

In this section, we will present the details of the method to execute our proposed Co-Learning pipelines on the 7th AI-City-Challenge Dataset. It comprises three main steps, each addressing the task’s challenges with varying levels of detail, and achieving remarkable performance on the benchmark dataset. The three steps aredata curation and annotation alignment,proposed learning pipeline and configuration, andoverall training platform.

SECTION: 3.1Data Curation and Annotation Alignment

To enhance the development of text-to-image retrieval systems, particularly in object detection, the 7th AI City Challenge[4]track two introduced a dataset for tracked vehicle retrieval using natural language descriptions, offering diverse textual descriptions. However, these annotations lack box-level descriptions for specific objects. Previous studies[19,20]have indeed shown encouraging outcomes, but numerous challenges persist. One primary concern is the inherent complexity of natural textual data. While humans can effortlessly comprehend textual narratives, machines struggle to differentiate between similar descriptions, such asA van is moving straightandThe red van is heading forward. The scarcity of training data further intensifies this challenge for machine learning models. Another significant obstacle is the lack of abundant high-quality training data tailored for text-to-image vehicle retrieval, given the early stage of this research. Compared to well-established datasets, such as Waymo Open Dataset[21]and COCO[22], which boast millions of samples, learning on edge devices suffers from limited high-quality annotations. Given the complexity of real-world road scenes, especially from fixed camera perspectives, it is essential to extract diverse features from unlabeled data for further refinement. Therefore, we selected the AI City Challenge dataset as our initial point. To assign each box a specific noun description and extract consistent box-level annotations, we applied stemming and lemmatization to text descriptions, inspired by the[20]approach. These are standard techniques in Natural Language Processing (NLP). While stemming truncates words to their root forms, lemmatization derives a word’s base form considering language grammar. These techniques aid in stop word removal, correction of misspelled words, and conversion to consistent base forms. Addressing linguistic ambiguities in queries and ensuring uniform textual embeddings is essential. The vehicle descriptions typically encompass three primary attributes: color, type, and movement. By employing the English PropBank Semantic Role Labeling (SRL) method[23], we extracted these attributes and proposed a standardized format, where,, andrepresent vehicle color, type, and motion, respectively. This identified synonymous terms with equivalent semantic meanings. To ensure consistent box-level annotation and minimize the complexity of the learning aim, we then grouped these synonyms into clusters based on semantic similarity and replaced them with a representative term. For instance, terms such asred van,van, andblue vanare grouped under the labelvan.

SECTION: 3.2Overall Training Platform

All experiments were conducted on a dedicated server equipped with two Nvidia Tesla V100-16GB GPUs, ensuring optimal performance and parallel processing capabilities. The server features 4Intel(R) Xeon(R) Gold 5117 CPU, complemented by 2TB of DDR4 RAM. The training time for Co-Learning, combined with the initial hyperparameter search conducted using Bayesian Optimization[24], amounted to approximately 90 GPU hours. In addition, we employed PyTorch 1.9.0 and MMDetection 2.25.0 as the primary framework and toolbox.

SECTION: 3.3Experimental setup and Results

Given the identified challenges from abundant unlabeled data and complex road scene scenarios acquired by roadside cameras, we utilize a semi-supervised object detection solution in a teacher-student network. This methodical strategy aims to reduce pseudo-label inconsistencies generated by the teacher model. Our approach incorporates three primary modules to counteract this. Initially, the dynamic pseudo-label assignment module replaces the traditional IoU-based strategy, enhancing the student network’s resilience against noisy pseudo-bounding boxes. Next, the pseudo-feature alignment module calibrates subtask predictions, allowing each classification feature to adaptively choose the most suitable feature vector for the regression task, regardless of its scale or location. Then, the pseudo-label refinement module dynamically adjusts the score threshold for pseudo-bounding boxes, ensuring stability in the count of ground truths during the early training stages and rectifying any unreliable supervision signals. Initial results of our proposed solution on roadside scenarios, mainly on the 7th AI City Challenge Track 2 dataset), have demonstrated its effectiveness as shown in Table1. Notably, using only 10% of annotated data with a ResNet-50 backbone, without annotation alignment, it achieves a mean average precision (mAP) of 23.0. When further trained on the fully annotated dataset with additional unlabeled data and the annotation alignment strategy, the performance escalates to an mAP of 36.5, surpassing the oracle model trained only on pseudo labels with an improvement of 0.4% on overall mAP.

SECTION: 3.4Further Analysis

In all our experiments, we utilized the Co-Learning method, depicted in Figure1, as our SSOD design, implemented using PyTorch. To ensure better clarity and consistency, we adopted the official FRCNN[5]model from TorchVision[25]and fixed the learned parameters in the ResNet-50[26]backbone.
In terms of dataset configuration and adhering to the framework’s guidelines, we used a total of 10% of the annotated data for the initial supervised training, treating the remaining 90% as unlabeled. We utilized the mean average precision (mAP) across object classes for each video frame as the evaluation metric. For a comprehensive assessment, we also provided the class-wise AP.
For training, we utilized the standard implementation, configuring the Co-Learning to adapt to our training scenarios.
We initialized the learning rate, momentum, and weight_decay at,, and, respectively, and conducted training using a batch size of five on a single GPU, performingiterations.
In terms of method comparison, our approach benchmarked two methods: with and without annotation alignment, emphasizing that reducing the complexity of annotations could decrease model performance. Further details are provided in Section3.1.
As indicated in Table1, the proposed annotation alignment strategy outperformed its counterpart Oracle model with an improvement of 0.4% mAP. We hypothesize that issues with label consistency could limit performance, which could be solved by employing stemming, lemmatization, and label-consistent strategies. Figure2showcases some examples of the final detected objects, which indicates that the proposed architecture can generalize well on target features, even if labeled data is limited.

SECTION: 4Conclusions

In this work, we investigate how to achieve consistent labeling using the proposed Co-learning framework and transfer it to the datasets for which annotations are limited. We employ text stemming and lemmatization methods to decrease the complexity of annotations towards semi-supervised object detection in road scenarios. Relative to baseline methods without annotation alignment, our findings suggest the importance of label consistency in SSOD. Next, we will transfer the proposed architecture and learned knowledge to edge devices, such as Jetson Orin and Xavier. In addition, benefiting from the advance of visual language models and visual agents, leaning on the edge efficiently would further widen the capability of the proposed Co-Learning framework.

SECTION: 5Acknowledgements

This work is supported by the German Research Foundation (DFG) under the COSMO project (grant No. 453130567), and by the European Union’s Horizon WIDERA under the grant agreement No. 101079214 (AIoTwin), by the Federal Ministry for Education and Research Germany under grant number 01IS18037A (BIFOLD), and RIA research and innovation program under the grant agreement No. 101092908 (SmartEdge).

SECTION: References