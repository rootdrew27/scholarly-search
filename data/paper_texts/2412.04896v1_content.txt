SECTION: Comprehensive Analysis and Improvements in Pansharpening Using Deep Learning

Pansharpening is a crucial task in remote sensing, enabling the generation of high-resolution multispectral images by fusing low-resolution multispectral data with high-resolution panchromatic images. This paper provides a comprehensive analysis of traditional and deep learning-based pansharpening methods. While state-of-the-art deep learning methods have significantly improved image quality, issues like spectral distortions persist. To address this, we propose enhancements to the PSGAN framework by introducing novel regularization techniques for the generator loss function. Experimental results on images from the Worldview-3 dataset demonstrate that the proposed modifications improve spectral fidelity and achieve superior performance across multiple quantitative metrics while delivering visually superior results.

SECTION: IIntroduction

Owing to payload, cost and data constraints, remote sensing satellites are unable to capture images having both high spatial and spectral resolution.Therefore satellites provide two types of images a high spatial resolution panchromatic image and a high spectral-resolution multispectral image. Both these images are taken by sensors looking over the same area at the same angle and hence requires no registration task. Pansharpening or Panchromatic sharpening aims to combine these two images to achieve higher spatial resolution while preserving specific spectral attributes.

Over the years, many algorithms have addressed this task. These techniques are mailnly divided into component substitution, multiresolution analysis, sparse representation and variational approaches. While these traditional techniques are relatively easy, they face several difficulties such as spectral distortions, spatial detail injection limitations, and limitations based on theoretical assumptions. Performance of these methods is also variable across different sensor data, and land cover characteristics.

With recent advances made by deep neural networks for
image processing applications, researchers have also explored the avenue for pan-sharpening. Deep learning has become a popular technique for image processing tasks, including pansharpening, due to its ability to learn complex nonlinear mappings between input and output images. In the context of pansharpening, deep learning can be used to learn the complex relationships between low-resolution multispectral images and high-resolution panchromatic images. Unlike traditional methods that rely on handcrafted features and linear models, deep learning can automatically learn the most discriminative features from the input data, and use them to generate high-quality output images.

Many researchers have used deep-learning for pansharpening. Inspired by image super-resolution[1], Masi et al.[2]constructed a three-layer convolutional neural network for pan-sharpening.[3]trains ResNet architecture in high-pass domain and adds MS input to it’s output.[4]uses a Conditional GAN to generate pansharpened image. Even though these methods are a great improvement over traditional methods they still show spectral distortions. In this paper, we propose a perceptual loss function that reduces this spectral distortion. We hypothesize that by minimizing the distance between the pansharpened and original MS in the high-level feature subspace, a more spectrally accurate image can be generated. To do this we calculate the gram matrix of the output of the bottom-most layer of the generator from PSGAN and use L1 loss between the two as an enriching term in the generator loss function.

This paper is organized as follows. In section II various traditional as well as deep-learning based pansharpening methods are briefly reviewed. Section III presents the methodology used to improve pan-sharpening. Experimental results and comparisons are provided in Section IV. Conclusions are drawn in Section V.

SECTION: IIRelated Work

This section briefly reviews Component substitution, Multiresolution analysis and Deeplearning based pansharpening methods

SECTION: II-AComponent Substitution

Component Substitution methods are based on projection of the multispectral images into a space, where the spatial and spectral components can be separated. The spatial component is then replaced by the panchromatic image. Since greater correlation between the replaced component and the panchromatic image leads to lesser distortion, the panchromatic image is histogram matched with the replaced component. The process is completed by doing inverse transformation to bring back the multispectral image to the original space.

Some of the techniques belonging to this class are IHS, GIHS, Brovey transform, PCA, Gram-Schmidt analysis.

IHS colour model is used as it separates the spatial information (intensity component) from the spectral information (hue and saturation). Hence, it is possible to manipulate the spatial information while keeping the same spectral information.

The hue component describes the color in the form of an angle between 0 to 360 degrees. It is determined by the relative proportions of red, green and blue The saturation describes the purity of the color, how much color is dilated with white light and its value ranges between 0 and 1. The IHS solid adds the dimension of intensity with black at the bottom and white at the top. Shades of gray run along the axis of the solid.

IHS yield adequate spatial enhancement but introduces spectral distortion. In[7], it is demonstrated that saturation component changes after changes in the intensity value. The product of intensity and saturation is a constant value. Hence, saturation and intensity are inversely proportional to each other. The new saturation value is expanded if Pan value is less than internsity,Ivalue and it is compressed when Pan value is greater than theIvalue. Studying the relative spectral response, the authors find that the RGB bands do not fall within the same range of the panchromatic band.

Furthermore, the response of the Pan is expanded beyond the NIR band. Since the spectral response of Pan andIare bound to change, to reduce the colour distortion, they introduce a generalized IHS method that responds to the NIR band.

Brovey transform[8]normalizes the spectral bands before they are multiplied with the panchromatic image. It retains the corresponding spectral features of each pixel and transforms the luminance information into a panchromatic image which then gets replaced by the histogram high resolution panchromatic image.

However, the Brovey transform method assumes that the spectral response of the PAN image represents the overall spectral content of the MS image. However, this assumption may not hold true in all cases, leading to spectral distortion in the sharpened image.

The MS image is taken as an input to principle component analysis procedure. The first principle component represents the maximum variance direction of the data. The Pan data is histogram matched with the first principal component. The results of which are used to replace the first principle component and the data is retransformed back to its original space.
The justification used to replace the first component is that the first principle component will have information which is common to all bands which is the spatial information while spectral information unique to any of the bands is mapped to the other components. However some of the spatial information may not be mapped to the first component, depending on the degree of correlation and spectral contrast existing among the MS bands[8].

The Gram-Schmidt (GS) algorithm is commonly used in remote sensing to orthogonalize matrix data or bands of a digital image. This process removes redundant or correlated information that is contained in multiple bands, resulting in a more accurate outcome.

The multispectral (MS) bands are resampled or interpolated to the same scale as the panchromatic (PAN) band.A lower-resolution panchromatic (PAN) band is simulated and used as the first band of the input to the GS transformation. Then each MS band vector is projected onto the (hyper)plane established by the previously determined orthogonal vectors. Pansharpening is accomplished by replacing the first vector with the histogram-matched PAN before the inverse transformation is performed.
Two methods are used for creating a low resolution PAN[8]. In the first method the LRMS bands are combined into a single lower-resolution PAN (LR PAN) as the weighted mean of MS image. The second method simulates the LR PAN image by blurring and subsampling the observed PAN image. However, the first method suffers from spectral distortion and the second method sufferes from low sharpeness. In order to avoid this drawback an enhanced GS method is used, where the LR PAN is generated by a weighted average of the MS bands and the weights are estimated to minimize the MMSE with the downsampled PAN.

GS is a generalization of PCA in which the first principle component can be choosen and the other components are made to be orthagonal to one another and the first component.

GSA assumes that the orthogonalized multispectral bands preserve the original spectral information. However, the orthogonalization process can lead to spectral distortion in the sharpened image. This distortion may result in colour shifts, an inaccurate representation of the original spectral content, or the introduction of artificial spectral artefacts. Also, GSA prioritises the enhancement of spatial resolution by utilising the panchromatic image. However, this can come at the expense of spectral resolution, potentially leading to a loss of fine spectral details in the sharpened image.

SECTION: II-BMRA

Another class of methods are Multiresolution analysis based methods that aim to extract the spatial information (high-frequency detail) from the PAN image by wavelet transform, Laplacian pyramid, etc., in the first step and then inject it to the up-sampled MS images to generate the fused image

A high pass filter is used to obtain high-pass information from the Pan image. The HPF results are added pixel by pixel to the lower spatial resolution MS image

SECTION: II-CDeeplearning based methods

While these traditional techniques are relatively easy, they face several difficulties such as spectral distortions, spatial detail injection limitations, and limitations based on theoretical assumptions. Performance of these methods is also variable across different sensor data, and land cover characteristics.

With developments in machine learning (ML) and deep learning (DL) in the last decades, these technologies started to be widely used in image processing, such as image classification, image segmentation, object
detection super-resolution, pan-sharpening and reconstruction.

Researchers in[2]have build upon architecture proposed in[1]for super-resolution problem and converted it to solve the pansharpening problem by leveraging to it the huge domain-specific knowledge available.
The 4-band Multispectral components are upsampled and interpolated and are stacked with the panchromatic band to form the 5-component input.
In addition to this, the authors add more planes corresponding to some well-known radiometric indices.

Mean Square error between the pansharpened image and its reference is used as the loss function

whereis the reference image,is the pansharpened image andis the number of batch size

The design of PNN is relatively simple and needed to be improved. But deep neural networks are difficult to optimize.[9]demonstrated the same problem and devised a clever solution that allowed the layers to learn residual functions with respect to the layer inputs instead of learning the unreferenced functions from scratch. This allowed training over 2000 layers with increasing accuracy.

This approch was implemented in the task of Pansharpening by the authors of[3]and is called PanNet. The researchers trained the ResNet in the high-pass domain to preserve spatial features and simply added the upsampled MS input to the model output to preserve spectral features.

PanNet uses the same loss function as PNN.

Both the approaches above used Euclidean distance between the predicted and reference image as a loss function which would cause blurring effects.

Another important breakthrough in the DL field is Generative Adversarial Networks[10]where a generative model tries to generate an image like the real image and is pitted against an adversary: a discriminative model that learns to determine whether an image is generated or real. This later lead to[11]where Conditional GANs have been used to for image to image translation.

In[4]a Generative Adversarial Network(PSGAN) was first applied for pansharpening. This network consisted of a two-stream input generator inspired by the U-NET[12]architecture and a fully convolutional discriminator similar to[11]. The work also demonstrated that l1 loss produced better results than l2 loss.

The loss function of the generator and the discriminator are:

whereis the LRMS image,is the HRPAN image,is the HRMS image and G and D are the Generator and Discriminator respectively.

SECTION: IIIProposed methodology

Our method builds upon PSGAN. Apart from the L1 loss betweenandwe propose three new loss functions:

SECTION: III-ALoss function based on SAM

In order to reduce the spectral distortions seen in PSGAN method, we devised a loss function like SAM (Spectral Angular Mapper)[13]and used it as a regularizing term is the generator loss function.

whereis output of.

SECTION: III-BSam loss on both resolutions

We applied the above loss function on both reduced and original resolutions and used it as a regularizer in the generator loss function.

whereis obtained by downsamplingby r.

SECTION: III-CPerceptual loss

We created a perceptual loss function that reduces this spectral distortion. We hypothesize that by minimizing the distance between the pansharpened and original MS in the high-level feature subspace, a more spectrally accurate image can be generated.

In order to generate the high-level feature subspace, we take a network with same architecture as the generator and add dropout layers to add noise. We train this network on MS images and take the L2 norm between the generated and input as loss function.

We take L2 norm between these two high-level feature subspaces taken from the bottleneck layer of the pretrained U-NET as a regularizer to the generator loss function.

SECTION: III-DGram matrix based perceptual loss

Instead of directly taking L2 norm like in the previous method, we calculate the gram matrix of the high level features of the generated and the original image and the euclidean distance between them is minimized.

SECTION: III-EGram matrix based reconstruction loss

We create a regularizer which is based on minimizing the distance between gram matrix of the reference and pansharpened patches. We use this loss along with generator loss and give both the same weightage

In all of the above cases we use the loss functions as regularizers and the final loss function of the generator becomes:

In order to calculatewe employ hyperparameter tuning using gradient descent.

SECTION: IVExperiments

SECTION: IV-ADataset and Performance measure

We train and test our network on dataset acquired from Worldview-3 satellite. The spatial resolution is 0.34m for PAN and 1.38m for MS. The dataset consists of images taken over three cities: Paris, Vegas and Shanghai. Wald’s protocol[14]is followed to downsample both the MS and PAN by a factor of 4 so that the resulting pansharpened image can be compared with the original MS image. Anti-aliasing is used for downsampling as it blurs the patches before the downsampling process. Hence, there is no need to apply a smoothing kernel. The training dataset consists of patches of size 256x256 from the datasets of Paris and Vegas. While the patches from Shanghai are used for testing.

Popular quantitative Evaluation metrics used are:
SAM: Spectral Angle Mapper[13], ERGAS: Global adimens, relative synthasis error[15]Q4: 4 band average universal image quality index[] and[16]SSIM: Structural Similarity index measure[17]

SECTION: VConclusion

This work introduces several new regularization techniques for the generator loss function in PSGAN. Experimental results demonstrate that the Gram matrix-based reconstruction loss significantly enhances overall performance across most metrics, with the exception of SAM, where the SAM-based loss achieves the best improvement. On smaller datasets, the perceptual loss function shows notable enhancements compared to PSGAN, although the Gram matrix-based perceptual loss leads to a slight performance degradation.

SECTION: Acknowledgment

The authors would like to thank everyone at the IAQD department of SAC, ISRO for their kind cooperation and encouragement. We would also like to thank the department for providing the resources. These resources have played a significant role in enabling us to conduct research, access relevant literature, and acquire necessary datasets for experimentation and analysis.

SECTION: References