SECTION: Mixture of Hidden-Dimensions Transformer
Transformer models encounter challenges in scaling hidden dimensions efficiently, as uniformly increasing them inflates computational and memory costs while failing to emphasize the most relevant features for each token. For further understanding, we study hidden dimension sparsity and observe that trained Transformers utilize only a small fraction of token dimensions, revealing an "activation flow" pattern. Notably, there are shared sub-dimensions with sustained activation across multiple consecutive tokens and specialized sub-dimensions uniquely activated for each token.
To better model token-relevant sub-dimensions, we propose(Mixture of Hidden Dimensions), a sparse conditional activation architecture. Particularly,employsfor common token features and a routing mechanism to dynamically activate. To mitigate potential information loss from sparsity, we designandmechanisms to preserve activation flow. In this way,expands hidden dimensions with negligible increases in computation or parameters, enabling efficient training and inference while maintaining performance.
Evaluations across 10 NLP tasks show thatsurpasses Vanilla Transformers in parameter efficiency and task performance. It achieveswithandwith aat constant activation cost.offers a new perspective for scaling the model, showcasing the potential of hidden dimension sparsity to boost efficiency.

SECTION: Introduction
Large Language Models (LLMs)have demonstrated impressive performance across a wide range of natural language processing tasks. Recent researchsuggest that, with sufficient training data, scaling language models by increasing the number of parameters and computational resources can yield more powerful models. Nevertheless, the substantial number of parameters in LLMs often leads to significant training and inference costs. Ideally, we seek flexible model architecturesthat enable parameter scaling while maintaining computational efficiency. Specifically, the parameters in Transformers’ matrices are defined by the hidden and intermediate dimensions. Some studiesobserve the sparsity of intermediate dimension activations and leverage it to design adaptive networks (e.g., MoE) for parameter scaling or use pruningand local activation mechanismsto reduce computational costs.

While elastic scaling of the intermediate dimension is well-studied, scaling hidden dimension with controllable computational costs remains largely unexplored, as shown in Figure. The hidden dimension, representing token embedding size, models each token in a language sequence. Expanding the hidden dimension enhances model complexity and capacity, enabling it to capture more intricate patterns. However, existing Transformersinherently treat all token dimensions equally, which leads to substantial computational and memory overhead as the hidden dimension scales up.

Given the limited understanding of hidden dimension in LLMs, we conduct an empirical study focusing on activation magnitudes. Our findings reveal, where 50% of dimensions account for 92.54% of the total activation magnitude (in FigureLeft). Among highly activated dimensions, we observe(in Figure). Shared dimensions likely model common features across tokens, while specialized dimensions capture higher-level semantic differences and are crucial for individual token information. This observation inspires us to design an efficient network that selectively activates shared and specialized sub-dimensions of the hidden dimension for different tokens, as shown in Figure. Additionally, we observe a consistentacross model layers, whereregarding hidden dimension variations (in FigureMiddle). Attention outputs show greater variability, while FFN outputs remain stable. This insight guides the design of separate sparsity architectures for Attention and FFN, ensuring the integrity of the activation flow after sparsification.

In this paper, we propose, a novel approach that significantly expands the modeling capacity of the hidden dimension through sparse, conditional activation, while keeping the number of active parameters nearly unchanged from the baseline model. Specifically,introduces two types of sub-dimensions at each layer of the model’s Attention and FFN components: shared sub-dimensions that are always activated to capture common dimensional information across different tokens, and specialized sub-dimensions that are selectively activated to capture token-specific specialized dimensions. Since functional roles of Attention and FFN, we train separate routing networks for each component.
To ensure load balancing across sub-dimensions, we apply a balancing loss to the specialized sub-dimensions. An activation scaling mechanism and a grouped fusion mechanism are introduced to mitigate information loss from dimensional downsampling and maintain efficient activation flow. With proper training,can be used to scale the model’s hidden dimension without increasing the number of parameters, or to significantly reduce the active hidden dimension during inference to lower computational costs.

To demonstrate the effectiveness of, we pretrain Vanilla Transformer with 355M, 495M, and 1.13B parameters following the architecture of the LLaMAandTransformer in 50%, 75%, 2, 3, 4settings. We evaluated these models on benchmark tasks spanning 11 different natural language processing challenges, demonstrating the advantages of thearchitecture. Experimental results show thatconsistently outperforms Transformer models with the same number of activated parameters across all model sizes. We found that MoHD effectively reduces activation redundancy in the model while delivering significant performance gains. In the compression setting, MoHD reduces activation parameters by 50% while retaining 99% of the original performance. In the expansion setting, MoHD maintains the same activation parameter count while expanding the hidden dimensions to 4× the original size, achieving up to an 8.37% relative performance improvement. Notably,-355M significantly outperformed LLaMA2-355M and even achieved performance comparable to LLaMA2-1.13B, while reducing activation parameter to LLaMA’s 28.9%. To further investigate the impact of increasing the hidden dimension, we conducted an in-depth exploration of’s routing mechanism and performed detailed ablation studies on sub-dimension specialization. Overall,is the first method to introduce sparse mixture activation for expanding the hidden dimensions of LLMs, offering a novel perspective on designing more efficient multi-dimensional model architectures.

SECTION: Definition
In this Section, we define the activation sparsity present in the hidden dimension of LLMs and use this to formulate sparsely activated FFN and Attention mechanisms.

Letdenote the embeddings oftokens, andrepresent the embedding of a single input token. The activation sparsityof a hidden stateis defined as the proportion of zero-valued entries within the vector. We then define a functionthat selectively activates a subset of dimensions in. The sparsely activated representation is denoted as, where, representing the selective activation of-proportion of the dimensions in.

SECTION: Hidden Dimension Sparsity
Considering the model’s semantic modeling in Euclidean space, we define the magnitudeof each dimensionas the square of its activation value:

We define hidden dimension sparsity as:

whereis the total number of hidden dimensions,represents the squared activation value of the-th dimension, andis a small threshold used to identify near-zero activation values. The indicator functionis equal to 1 if the activation value is below the threshold and 0 otherwise.

SECTION: Hidden Sparsified FFN
Defineas the up, gate, down matrix in one FFN block, whereis the intermediate size. In this context, the-th row of the up, gate matrix is defined as, and the-th column of the down matrix is defined as. Specifically, the sparsely activated hidden stateunder activation sparsityonly activates a subset of rows in the up, gate matrix and a corresponding subset of columns in the down matrix, denoted as. Thus, the sparsified FFN computation can be described as follows:

whereis the activation function.is the element-wise production. Due to the sparsification of the hidden state, the up and gate matrices share the same activation subset. To ensure the output remains sparsified, the down matrix is also sparsified, though its activation subset can differ from that of the up and gate matrix.

SECTION: Hidden Sparsified Attention
For a-head Multi-Head-Attention (MHA), we defineas key, query, value and output projections for the-th head, wheredenotes as the head dim,.
With sparsely activated hidden state, a small parameter subsetrepresents a sparsely activated selection of rows fromand columns from.

whereis the softmax function. Sinceis sparse in the hidden dimension, we can find an approximationof, such that, under the activation of the corresponding subset of parameters, the outputs of the sparsified FFN and sparsified attention closely approximate the outputs of the dense model.

SECTION: Observation
In this section, we present several key findings that serve as the foundation for the design of theapproach. In Section, we observe the long-tail effect of hidden dimension activation values and define activation sparsity accordingly. We analyze the sparsity distribution and differences between attention and FFN across different layers. In Section, we analyze activation flow in Transformers, highlighting compression patterns, stabilization by residuals and normalization, and functional layer differences.
In Section, we further identify the existence of shared continuous high activation behaviors and unique discrete high activation behaviors across tokens. Finally, we analyze these phenomena and propose motivations for designing feasible hidden dimension sparsification methods.

SECTION: Sparsity in Tokens’ Hidden Dimension
For a more comprehensive understanding, we observe the activation magnitudes of 4096 hidden dimensions in LLaMA2-7B. As shown in the left panel of Figure, we visualize the relationship between dimension magnitudes and reordered dimension indices based on magnitude size.

Similar to previous observations, the activation of hidden dimensions exhibits a long-tail sparsity phenomenon. For instance, in the input Attention activations of LLaMA2-7B’s 16th layer, the cumulative magnitude of the top 1000 dimensions accounts for 71.96% of the total magnitude. In contrast, most dimensions have low activation values, indicating that the model does not utilize information from the majority of hidden dimensions, leading to substantial sparsity in activations.

We also visualized the sparsity of activations in the input and output of Attention and FFN components. Our observations revealAttention exhibits higher activation magnitudes, while FFN activations are comparatively lower. At the input stage, activation magnitudes are relatively high (median > 1), whereas at the output stage, activation magnitudes drop significantly (median < 0.5). The sparsity of hidden dimensions in the input components is consistent across different modules, likely due to the influence of residual connections. However,As shown in the middle panel of Figure, Attention demonstrates significant fluctuations in sparsity, with alternating high and low sparsity distributions. In contrast, FFN sparsity remains relatively stable. These differences highlight the distinct functional roles and information processing characteristics of Attention and FFN, prompting us to consider differentiated activation designs for these components.

SECTION: Activation Flow in Transformer
We also investigate the variations in activation magnitudes within a single Transformer block, as illustrated in Figure.The Attention module compresses input activations normalized to 100% through projections (,,) and weighted averaging, reducing activation magnitudes to 6.7% at. This highlights its ability to suppress irrelevant information through weighted aggregation, while also, as the output activation magnitudes vary to accommodate layer-specific roles. In contrast,, with compression arising from high-dimensional projections, nonlinearity that sparsifies activations, and dimensionality reduction through linear weighted summation, collectively reducing activation magnitudes.

In the outputs of the Attention and FFN modules, residual connections directly add the input back to the output, partially restoring the compressed activation magnitudes. Layer Normalization further balances and constrains activation magnitudes, stabilizing the numerical distribution and suppressing excessively high or low activation values, thereby enhancing the training stability of the Transformer. However, this normalization also smooths activation change patterns, potentially diminishing the prominence of contextual information and further compressing attenuated activation magnitudes, particularly exacerbating the instability of Attention outputs.

SECTION: Continuous High Activation
We further investigate the temporal correlation of activation sparsity by observing high activation values across different tokens and analyzing the indices that are repeatedly activated by multiple tokens.FigureRight shows the number of commonly highly activated dimensions across 2 to 9 consecutive tokens, with the x-axis representing the threshold for defining high activation. When using the top 20% of activation values as the threshold, 2672 dimensions are commonly activated across 2 consecutive tokens, and 673 dimensions remain commonly activated across 9 consecutive tokens.

Figurefurther illustrates the correlated activation patterns over 5 tokens, where the 4096 hidden dimensions are clustered and reordered based on their activation patterns. Approximately 400 dimensions are commonly highly activated across all 5 tokens, while about 200 dimensions are uniquely highly activated within each token. This indicates thatShared high activations model the similarity information shared across tokens in hidden dimensions, while specialized unique activations capture differences. These observations inspired the shared-specialized activation mechanism in the subsequent design of MoHD.

SECTION: Mixture of Hidden Dimensions ()
In this Section, we propose the Mixture of Hidden Dimensions () architecture to expand the hidden dimension of the model without increasing the number of activated parameters. The full workflow is illustrated in Figure.
Inspired by the observations on LLM’s hidden dimension activation phenomena discussed in Section, we introduce the Shared and Specialized Sub-Dimension Mixed Activation mechanism in Section. Furthermore, we present the implementation of the sparsified components, as defined in Section, applied to both the Attention and FFN blocks. In Section, we address the issue of information degradation and how we mitigate it using activation scaling and grouped fusion mechanisms. In Section, we explore the design of a balancing loss to enhance diversity. Finally, in Section, we detail the optimized implementation of.

SECTION: Mixture of Sub-Dimensions Activation
As defined in Section,denote the embeddings oftokens, andrepresent the embedding of a single input token. Under a specific activation sparsity, we selectively activate a subsetof parameters of a matrix.Therefore, we constructsub-dimensions by slicing the weight matrixalong the hidden dimension, where each sub-dimension has a dimension of. Specifically,, with. Here,represents the sub-parameters offromto.

Define the routing gatedetermines the top-sub-dimensions to activate from thesub-dimensions based on sparsity, assigns each activated-th sub-dimension a weight:

wheredenotes the token-to-sub-dimension score,denotes the set comprisinghighest affinity scores among those calculated for the input token and all sub-dimensions, andis the centroid of the-th sub-dimension in the-th layer. All sub-dimensions are assigned routing weights, allowing the routing mechanism to learn to selectively amplify or suppress the representations of shared sub-dimensions during the optimization process. Finally, the outputsfrom all activated sub-dimensions are concatenated and weighted, resulting in a final output of dimension, which matches the hidden dimension.

We use the notationto denote the concatenation of the termsfor. Due to the high sparsity of the gate, only a small subset of dimensions is assigned non-zero weights, while most dimensions remain zero. In practice, based on the gate’s selection, we can sparsifyand, and finally get output, meaning the number of activated parameters inis reduced toof the original.

SECTION: Activation Flow Maintenance
In Section, we sparsely activate a subset of parameters, resulting in a sparsified hidden-dimension output, which substantially reduces computational costs. However, a challenge arises due to the router assigning softmax-normalized weights to different dimensions. This may lead to a few dimensions receiving disproportionately high weights, while information within many other dimensions may be neglected due to their low assigned weights. Additionally, because we concatenate the final output in parallel, any weight below 1 suppresses information within that sub-dimension without compensating for this loss through other sub-dimension. Unlike Mixture of Experts methods,directly use concatenation for integration, which can lead to information degradation.,, and.

To address the suppression of sub-dimension activations caused by the softmax weight normalization, we introduce a scaling factor to ensure that the sum of activation weights across all dimensions remains consistent with the input. We define the scaling factor, which ensures that the activated dimensions retain their proportional influence. To address the information loss from the sparse output, we employ a fusion mapping layer that projectsfrom its activated sub-dimensions back to the original dimension. To reduce computational overhead, we introduce a Monarch matrixto perform grouped fusion mapping. Given a receptive field, we define the mapping matrixas follows:

where the Monarch matrixenables efficient grouping and transformation, thereby reconstructing the information across the original hidden dimensions while keeping computations tractable.
In summary, the forwarding process for a singlemodule can be formally represented as follows:

SECTION: Mixed Activated Sub-Dimensions
As discussed in Section,Therefore, we designed two types of sub-dimensions in:and. Shared sub-dimensions are always activated by the routing mechanism, whereas Specialized sub-dimensions are selectively activated based on the routing decisions. Defineas the percentage of shared sub-dimensions activation rate, the routing gatedetermines the top-sub-dimensions to activate from thesub-dimensions based on sparsity, and assigns each activated-th sub-dimension a weight:

wheredenotes the token-to-sub-dimension score,denotes the set comprisinghighest affinity scores among those calculated for the input token and all sub-dimensions, andis the centroid of the-th sub-dimension in the-th layer. Under this routing mechanism,This, in turn,However, all sub-dimensions are assigned routing weights, allowing the routing mechanism to learn to selectively amplify or suppress the representations of all sub-dimensions during the optimization.

SECTION: Sub-Dimension Load Balance
Research on conditional computationhas shown that automatically learned routing strategies can often lead to load imbalance issues, where the model tends to select only a few sub-dimensions, leaving others underutilized and insufficiently trained., we incorporate Sub-Dimension Load Balance Loss: Defineis a scaling factor andis an indicator function that returns 1 if the-th sub-dimension has the highest gating score for the-th sequence position and 0 otherwise.

The termrepresents the normalized gating score for sub-dimension, ensuring that the contributions of each sub-dimension are proportional to their selection frequency. The auxiliary loss thus encourages the gating mechanism to distributeby penalizing imbalances, ultimately leading to improved model performance and efficiency.

SECTION: Implementation
In Sectionsand, we observed activation differences across components in various layers, prompting us to design separate routing mechanisms for the Attention and FFN components. Specifically, in one Transformer Block,andproducing scores that determine the activation of dimension-specific sub-dimensions for the output:

In practice, different components may employ distinct sparsification settings. However, for simplicity, we use the same notation throughout this section to represent these settings in a unified manner. Based on the scores from the Router,applies synchronized sparsification to the hidden dimensions of all up-projection and down-projection matrices, as well as the input. From Equation, we transforminto’s sub-dimensionsand. We substitute these into the sparsified Attention and FFN defined in Equationand, yielding outputsand, respectively:

We construct abased onspecified MHA and FFN components. Residual connections are designed to further mitigate information loss during the specified forward pass. Following the configuration of LLAMA, we apply LayerNorm layers before the input to both MHA and FFN; however, for simplicity, these are omitted in the formal equations. This process can be formalized as follows:

To train the model effectively, we combine cross-entropy lossfor language model pre-training and the load balance loss, resulting in the final training objective:

SECTION: Experiments
SECTION: Experimental Setup
To pretrainmodels and baseline models, we employ the RedPajama, which parallels the LLaMA training data across seven domains: CommonCrawl, C4, GitHub, Wikipedia, Books, ArXiv, and Stack-Exchange. This dataset comprises a validation set with 2 million tokens, a training set containing 50 billion tokens.

Our experimental framework utilizes the Sheared-LLaMA codebaseimplemented on the Composer package, and is executed on 8 NVIDIA A100 GPUs (80GB). The models are trained with a sequence length of 4096, employing a global batch size of 64 during the fusion phase and 256 during the continued pre-training phases.models were trained for 50000 steps (50B token budget). The learning rates were set at 3e-4 for both model parameters and router parameters. The baselines and allmodels follow the same training setup, starting from random initialization and training on the same amount of data.

We employed the lm-evaluation-harnessto evaluate our models. For common sense and reading comprehension tasks, we report 0-shot accuracy results for SciQ, PIQA, WinoGrande (WG), ARC Easy(ARC-E), and 10-shot HellaSwag (Hella.), alongside 25-shot accuracy for ARC Challenge (ARC-C). In the assessments of continued QA and text understanding, we report 0-shot accuracy for LogiQA, 32-shot BoolQ, and 0-shot LAMBADA (Lam.). All reported results were calculated with the mean and stderr of multiple experiments.

Following the architecture of LLaMA2, we constructed models at three parameter scales: 355M, 495M, and 1.13B, with hidden dimensions of 1024, 1536, and 2048. At each parameter scale, we developed three variants: a standard Transformer model (LLaMA architecture) and an-based model. Due to the flexibility of thearchitecture, we can compress the model activation parameters and keep the number of parameters unchanged, or we can keep the activation parameters and expand the equivalent number of model parameters. For eachmodel scale, we experimented with five different hidden dimension scaling factors—,,,, and—to demonstrate’s potential in both reducing computational cost and effectively increasing model capacity. All models were initialized with the same random seed and pre-trained on a uniform dataset of 50 billion tokens.

SECTION: Result
Tabledemonstrates the fundamental capabilities ofon the 355M, 495M, and 1B versions of LLaMA2 after activating only 50% and 75% of the hidden dimensions. All models were trained from scratch. Results indicate that when only part of the hidden dimensions are activated,maintains, and in some cases even improves, its performance. Specifically, at the 355M scale,, highlighting the high sparsity observed in hidden dimension activation. More notably, at all model scales,, with average performance gains of 0.5%, 1%, and 1.8% for the 355M, 495M, and 1B models, respectively. This suggests that full hidden dimension activation during training is not optimal; instead,achieves higher parameter efficiency by selectively activating hidden dimensions and encouraging token-specific activation patterns. Finally, we observe that. For instance,with 50% activation demonstrates a relative improvement over the baseline of -0.4%, +0.3%, and +1.7% for the 355M, 495M, and 1B models, respectively. This indicates promising potential forin larger-scale models. Finally, we found that highly compressing the original hidden dimension activation (50%) leads to a slight decrease in Commonsense metrics. However,still maintains strong LM metrics under low activation settings, demonstrating the model’s robust language modeling capabilities.

Tablepresents the baseline capabilities ofon LLaMA2 models at the 355M, 495M, and 1B scales under 2, 3, and 4hidden dimension expansion., achieving performance comparable to models with an equivalent number of effective parameters, while maintaining a substantially lower count of activated parameters. For instance, in the 355M model,with 2activation (equivalent to 710M effective parameters) surpasses the baseline performance by 2.2%, even outperforming the LLaMA2-495M and LLaMA2-1.13B models, which have higher numbers of activated parameters. This improvement underscores’s effective utilization of hidden dimension sparsity, leveraging differentiated hidden dimension activation to boost performance.
In the 2configuration,achieves performance gains of 2.2%, 0.7%, and 3% for the 355M, 495M, and 1.13B models, respectively. This indicates thatExperimental results further show that parameter expansion withyields significant improvements across multiple tasks, including natural language modeling (LAMBADA), reading comprehension (SciQ, ARC-E, ARC-C, HellaSwag).However, the performance improvement was relatively small on the LogiQA and MMLU datasets. Finally,, with optimal results often achieved when the hidden dimension was tripled. For example,×3 with 1.48B parameters showed a 2.2% improvement over the baseline and a 1.5% improvement over×2 with 989M parameters. The routing mechanism ineffectively increases the equivalent hidden dimension, enabling significant performance gains, although a performance ceiling exists under the same activation count. Overall,showcases a structural advantage for building large-scale models.

In Figure, we analyze the relationship between model performance and model size from the perspectives of activation parameters and total parameters. When examining model performance under equal activation parameter conditions, we found that. Compared to the baseline, variousmodels at the 400M and 1B scales achieved absolute improvements of approximately 2.2% and 3%, respectively. At comparable performance levels,often requires activation of less than 50% of the original model parameters. As the activation parameter count increases,. In Figure, we further analyze the performance ofand the baseline as total parameters increase. At smaller model scales,achieves comparable performance to the baseline with fewer activated parameters under the same total parameter count. As the model scale increases,gains a larger performance advantage over the baseline with the same total parameter count.effectively leverages the increased hidden dimension redundancy in larger models, ultimately achieving higher parameter efficiency.

In Figure, we visualize the evaluation perplexity curves during pretraining on 50B tokens for LLaMA2-495M,×3-1.48B,75%-495M, and50%-495M. The enhanced, with no noticeable oscillations or anomalies. The perplexity curve for×3-1.48B, with expanded equivalent parameter counts, is lower and smoother compared to LLaMA2-495M, indicating thatimproves the model’s representation and learning capabilities. For75%-495M and50%-495M, the perplexity curves are slightly lower or on par with LLaMA2-495M, demonstrating that even with partial parameter activation,maintains strong training characteristics. Overall,effectively expands or preserves the equivalent hidden dimensions while ensuring that the representation, learning capability, and robustness during training.

SECTION: Ablation Studies
To evaluate the importance of each method in Sectionwithin, we conducted detailed ablation experiments. In Table, we compare the ablation results of×2 with 710M parameters to the baseline with the same activation under zero-shot pretraining on 10B tokens, based on Eval PPL. The specific analysis is as follows:

The balanced loss effectively enhances(0.16 improvement). It mitigates the risk of routing collapse, ensuring that most sub-dimensions are utilized more evenly. This increases the efficiency of sub-dimension utilization and improves the overall parameter efficiency of the model. For further observations and analysis on routing, see Section.

Ablation experiments show that maintaining effective activation flow is the key factor behind’s high parameter efficiency. As shown in Table, removing Sub-dimension Scaling leads to a significant performance drop of 1.16, indicating that without this enhancement to activation flow, the model loses a substantial amount of critical information after sparsifying the hidden dimension, making sparseperform almost identically to a dense model with the same activation. Building on Sub-dimension Scaling, the Group Fusion Layer further provides a 0.22 performance gain without adding significant parameters. The Group Fusion Layer performs grouped filling and mapping after sparse activation, preserving information integrity and improving dimension utilization.

In our experiments, we ablated the Mixed Activation Sub-Dimension method, using fully specialized sub-dimensions without any shared sub-dimensions. We observed a 0.83 increase in PPL, indicating a significant negative impact on model performance. This finding aligns with the observations in Section: as there are a few common activation dimensions within the hidden layer, these dimensions should be commonly activated using a mixed activation mode, followed by sparse activation across multiple sub-dimensions. In Figure, we also present the model’s performance under various allocations of shared and specialized sub-dimensions. The mixed activation mode achieved significant performance gains over fully sparse activation, suggesting that this architecture is well-suited to the activation patterns of the Transformer model’s hidden dimensions.

SECTION: DecoupledComponents Setting
To investigate the effects of sparsifying different components with, we built two hidden dimension compression models based on LLaMA2-355M: one withapplied only to Attention and the other withapplied only to the FFN. Both models were trained from scratch on 10B tokens for comparison. Activation represents the model’s activation parameters, excluding the input/output embeddings. In this experiment, however, the total parameter count remains consistent across all models. To explore the impact ofsparsification on different components, we constructed three hidden dimension compression models based on LLaMA2-355M: one applyingonly to Attention, only to the FFN and to both components, as shown in Figure. Both models were pre-trained from scratch on 10B tokens for comparison.here represents the model’s activation parameters, excluding input/output embeddings. The total parameter count is kept consistent across all models. The Tableshows the effects of decouplingcomponents under three hidden dimension sparsity settings: 100%, 50%, and 25%.

Even with 100% sparsity (where activation parameters match those of the original model),outperformed the baseline. This may be due to’s allocation of weighted activations and grouped fusion across each hidden sub-dimension, which encourages more optimal activation dimensions while suppressing noise from redundant activations, demonstrating the advantages of thearchitecture.

The FFN layer exhibits greater redundancy in hidden dimensions, resulting in minimal performance loss (and sometimes even improvement) when sparsified. In contrast, sparsifying hidden dimensions in Attention leads to a more significant performance drop. In terms of activation parameters, the 50% sparsity setting for the FFN uses only 195M parameters, considerably fewer than the 239M required by Attention sparsification. This suggests that the FFN is better suited fortransformation. From a performance perspective, the FFN achieved a PPL reduction of -0.30 in the 50% sparsity setting, potentially due to a reduction in redundant activations that mitigates model overfitting during training, whereas Attention sparsification led to a +1.04 increase in PPL.As sparsity levels increase, the performance loss in both FFN and Attention also grows.

Joint sparsification of Attention and FFN yields the best parameter efficiency. Under the 50%ATTN-50%FFN setting, the model achieved a PPL of 12.05 with only 145M activation parameters—between the 50%FFN and 50%ATTN configurations. Compared to applying greater sparsity to FFN alone, the 50%ATTN-50%FFN setting resulted in a 0.19 lower PPL than 25%FFN, even with fewer activation parameters. This may be because consistency in activated hidden dimensions helps the model maintain better learning capacity.

SECTION: Analysis
To observe the targeted sub-dimension selection based on the router in, we visualize the attention and FFN router weight distributions at the 5th layer ofacross five different data domains in Figure. Each probability weight represents the average selection probability across 4096 tokens. We can observe that sub-dimensions show specialization across different data domains. For instance, in the AttentionRouter, Subdim 5 demonstrates the importance of code-related data, with significantly higher probabilities in the GitHub and StackExchange domains. On the other hand, Attention Subdim 3 shows higher probabilities in Wiki, CC, and ArXiv domains, while it is much lower in GitHub. We speculate that this Subdim is important for commonsense knowledge and writing tasks. In the FFNRouter, Subdim 4 specializes in code-related tasks, while Subdim 3 specializes in commonsense knowledge tasks. The specialization of sub-dimensions validates the effectiveness of, enabling it to allocate differentiated sub-dimensions to individual tokens based on various data entries and domains. As a result,can effectively leverage these sub-dimensions to increase the equivalent parameter count, achieving higher parameter efficiency. Furthermore, while the probabilities of all sub-dimensions indiffer, they generally stay within the 0.2-0.3 range, indicating that all sub-dimensions are actively chosen, thus preventing router collapse.

Figureshows the evaluation PPL values of×2-710M trained on 10B data under different Shared activation and mid-activation proportions. The non-activation parameters of the Baseline andmodels are identical. Firstly,routing effectively increases the model’s equivalent hidden dimensions. In the 0/4 setting, the routing yields better performance compared to the baseline, while in the 4/4 setting, it results in a significant performance drop. However, retaining Shared activation in the hidden dimensions proves essential. We found that the best performance occurs when Shared activation is set to 3/4, which is why this ratio was commonly used in the experiments. This indicates that the routing design for hidden dimensions still requires further research, and there is ample room for exploration to increase the model’s sparsity.

Figurepresents the test PPL values of×2 -710M after pre-training on 10B data, with a finer-grained division of sub-dimensions. When the number of sub-dimensions is set to 16 (sub-dimension size 256), the model achieves the best performance. As the number of sub-dimensions increases to 128 (sub-dimension size 32), the model’s performance slightly improves, and a further increase to 256 sub-dimensions (sub-dimension size 16) results in a marginal improvement. This experiment demonstrates that, within the currentdesign, increasing the number of sub-dimensions does not enhance model performance but instead leads to higher computational costs, especially in the routing and grouping fusion layers. The results also validate the effectiveness of the grouping fusion layer: fine-grained grouping fusion mechanisms are not necessary, and a small number of parameters are sufficient to maintain effective forward activation flow.

SECTION: Related Work
SECTION: Activation Sparsity
Activation sparsity refers to the phenomenon where a significant proportion of a model’s hidden states are zero-valued. This property naturally arises in the intermediate states of ReLU-based MLPs, as demonstrated in prior work. Some studies have leveraged activation sparsity to improve the efficiency of LLMs during inference.utilized activation sparsity to accelerate LLM inference by omitting the transfer of weight channels corresponding to zero-valued entries to GPU registers. Additionally,andextended this concept to CPU offloading, significantly reducing memory transfer overhead between CPUs and GPUs. Recent works has reintroduced activation sparsity into LLM architectures to enhance efficiency.replaced SiLU and GeLU with ReLU, achieving sparsity through extended pretraining.identified Squared ReLUas a superior alternative for sparse activations.proposed regularization techniques to increase sparsity, whilecombined pruning and quantized activations to establish scaling laws.introduced CATS, achieving training-free sparsity in SwiGLU-based LLMs.extended these concepts to training-free activation sparsity for large-scale language models. Building on prior studies, we investigate hidden dimension sparsity, focusing on continuous activation across tokens. Leveraging this, we design a sparse activation architecture that improves parameter efficiency and enhances hidden dimension scalability.

SECTION: Sparsely-activated Transformer
Sparsely-activated Transformer models, such as Sparse Mixture-of-Expert (MoE) architectures, leverage input adaptivity to achieve scalable and efficient computation. These models dynamically activate only a subset of specialized subnetworks, or "experts," for processing each input token, significantly reducing computational overhead. This mechanism enables effective handling of diverse data domainswhile maintaining high performance. Recent advancements in sparsely-activated Transformers have extended their capabilities by introducing heterogeneous experts, allowing networks to integrate experts with varying capacities and specializations. Some recent studieshave observed the activation patterns in the intermediate dimensions of FFNs and explored sparsely-activated architectures based on these observations. However, no existing Transformer architecture has implemented sparse activation specifically in the hidden dimensions. Inspired by the work of, we conducted an in-depth analysis of the hidden dimensions and designed a novel sparse activation strategy. This innovation opens a new research avenue for sparsely-activated Transformer architectures.

SECTION: Conclusion
In this paper, we presented(Mixture of Hidden Dimensions), a sparse conditional activation architecture designed to address the inefficiencies in scaling Transformer hidden dimensions. By integrating shared sub-dimensions for common token features and dynamically activating specialized sub-dimensions through a routing mechanism, MOHD achieves improved efficiency and flexibility while preserving activation flow through activation scaling and group fusion mechanisms. Our evaluations demonstrate that MOHD outperforms standard Transformers across multiple NLP tasks, achieving superior parameter efficiency and enhanced task performance. These results underscore the potential of hidden dimension sparsity as a promising direction for improving the scalability and efficiency of Transformer.

SECTION: Acknowledgments
We would like to thank Yinqi Yang, Yanxi Xie, Naibin Gu, Kun Huang and members of the IIE KDsec NLP group for their valuable feedback and discussions. We are very grateful to Mengzhou Xia for providing the concise and effective ShearingLLaMA experimental code and for her assistance during the reproduction process. Work done during Yilong Chen’s internship in Baidu Inc.

SECTION: References