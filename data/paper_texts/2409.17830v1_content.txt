SECTION: Unsupervised Learning Based Multi-Scale Exposure Fusion

Unsupervised learning based multi-scale exposure fusion (ULMEF) is efficient for fusing differently exposed low dynamic range (LDR) images into a higher quality LDR image for a high dynamic range (HDR) scene. Unlike supervised learning, loss functions play a crucial role in the ULMEF. In this paper, novel loss functions are proposed for the ULMEF and they are defined by using all the images to be fused and other differently exposed images from the same HDR scene. The proposed loss functions can guide the proposed ULMEF to learn more reliable information from the HDR scene than existing loss functions which are defined by only using the set of images to be fused. As such, the quality of the fused image is significantly improved.
The proposed ULMEF also adopts a multi-scale strategy that includes a multi-scale attention module to effectively preserve the scene depth and local contrast in the fused image. Meanwhile, the proposed ULMEF can be adopted to achieve exposure interpolation and exposure extrapolation. Extensive experiments show that the proposed ULMEF algorithm outperforms state-of-the-art exposure fusion algorithms.

SECTION: IIntroduction

A high contrast nature scene could have a high dynamic range (HDR), with brightness ranging fromto, and a dynamic range of up to 10 orders of magnitude. However, the dynamic range that can be captured with a single exposure is very limited, and the recording of image data usually uses 8 bits, resulting low dynamic range (LDR) images which inevitably have unfavorable over-under-exposed regions. Therefore, in extremely bright or dark situations, there will be a significant loss of detailed information, which severely affects machine vision tasks such as intelligent driving and navigation. HDR imaging has been introduced to address the issue effectively[1]. Due to limited information captured by a single image, single-image based HDR methods usually show poor performances. Capturing multiple differently exposed images provides an efficient solution for HDR imaging, which can preserve rich details and vivid color. Even though camera movements and moving objects are issues for the multiple images, differently exposed LDR images can be aligned in LDR domain by using the algorithm in[2]. In the remaining part of this paper, differently exposed LDR images from an HDR scene are assumed to be aligned well as in[3,4,5,6,7,8,9,10,11].

There are two different ways to combine a set of differently exposed LDR images together for an HDR scene after they are aligned. One is to estimate the camera response functions (CRFs), convert the LDR images into the corresponding HDR images, and merge all the HDR images into one high quality HDR image[1]. The HDR image is scaled down by a tone mapping algorithm for display[10,11]. The other is to fuse all the differently exposed LDR images into a high quality LDR image directly by using an exposure fusion algorithm.

The exposure fusion was widely studied by using conventional methods in[3,4,5,6,7]and data-driven methods in[8,9]. The fused image approaches the set of images to be fused rather than the HDR scene by these algorithms. All these algorithms assume that enough differently exposed LDR images are captured with a normal-exposure-ratio (NER) for each HDR scene. However, this assumption is usually not true, especially for mobile devices with limited computational resources. Generally, only a few differently exposed LDR images are captured for an HDR scene. All the exposure fusion algorithms in[3,4,5,6,7,8,9]produce serious brightness order reversal artifacts if the inputs are two large-exposure-ratio (LER) images[12,13,14]. Information in the brightest and darkest regions of an HDR scene might not be preserved well if the inputs are three NER images. Therefore, it is still desired to study the exposure fusion even though there are many exposure fusion algorithms. We argue that the fused image should approach the HDR scene rather than the set of images to be fused. The objective of this paper is to explore such a new exposure fusion algorithm by fully utilizing the asymmetry between the training and inferring (or testing) stages of the learning based algorithm.

Inspired by the algorithms in[3,10,12,14], a novel unsupervised learning based multi-scale exposure fusion (ULMEF) algorithm is proposed for a set of differently exposed LDR images from an HDR scene in this paper. All the inputs are already aligned. The proposed algorithm is based on an observation that multi-scale is helpful to preserve scene depth and increase detail clarity for the fused LDR image[10,3]. This is different from the existing unsupervised learning based algorithms in[12,8,9]which are single-scale. Particularly, a multi-scale fusion network (MSF-Net) is proposed to fuse all the images in the feature domain from coarse to fine. Each scale of the proposed network is on top of multi-scale attention mechanisms which utilize different scale features to fuse the images efficiently, thereby improving the training efficiency of the network. Besides the network structure, loss functions are crucial for the proposed ULMEF algorithm. A new strategy is proposed for the definition of loss functions. The loss functions in[8,9,12,14]are tightly coupled with the set of images to be fused. The relative brightness order might not be preserved well if the inputs are two LER images[13,15], and information in the brightest and darkest regions might not be well preserved in the fused image if the inputs are three NER images. To address these problems, the loss functions are defined by using the set of LDR images to be fused and other differently exposed LDR images from the same HDR scene. As such, the fused image approaches the HDR scene rather than the set of images to be fused. Clearly, the proposed loss functions are fundamentally different from those in[8,9,12,14]in the sense that the loss functions and the set of images to be fused are decoupled in the proposed ULMEF algorithm. To our best knowledge, we are the first to propose the decoupled loss functions for the unsupervised learning based exposure fusion. The proposed ULMEF can learn more reliable information from the HDR scene than the existing loss functions which are defined by only using the set of images to be fused[8,9,12,14]. This is not surprised due to the conventional wisdom of inferring better through seeing more. It can be adopted to achieve exposure interpolation and exposure extrapolation much easier than the conventional MEF algorithms. Experiments on different datasets have demonstrated efficiency of the proposed algorithm. Overall, two main contributions of this paper are

1) An innovative strategy is proposed to define loss functions for unsupervised learning based exposure fusion algorithms. The loss functions and the set of images to be fused are decoupled by the new strategy. As such, the exposure interpolation and exposure extrapolation can be implemented easily. This is a new initiative on exposure fusion. The fused image approaches the HDR scene rather than the set of images to be fused;

2) A novel MSF-Net with multi-scale attention mechanisms is proposed to preserve the scene depth and local contrast in the fused image. In addition, the information in the brightest and darkest regions are preserved and the halos artifacts are avoided from appearing in the fused image by the proposed ULMEF algorithm.

The rest of this paper is organized as below. Existing results on exposure fusion are summarized in SectionII. Details of the proposed MEF algorithm are provided in SectionIII. Experiment results are presented in SectionIVto compare the proposed algorithm with nine state-of-the-art (SOTA) exposure fusion algorithms. Finally, conclusion remarks are given in SectionV.

SECTION: IILiterature Review on Exposure Fusion

Many exposure fusion algorithms were proposed for the HDR imaging under an assumption that all the images to be fused are aligned well. The main idea of these algorithms is to preserve the reliable information from a set of differently exposed LDR images as much as possible. Existing exposure fusion algorithms can be divided into traditional exposure fusion algorithms and data-driven ones.

Traditional exposure fusion algorithms are mainly based on statistical modeling methods, which perform weighted average or weighted sum of image pixels in a multi-scale way. The resultant algorithm is thus called multi-scale exposure fusion (MEF). Mertens et al.[3]first used contrast, saturation, and exposure to define weights for all pixels and then fused the different exposure images to create an information-enriched LDR image by using the Gaussian and Laplacian pyramids[16]. This approach allowed for a wider range of brightness and color information to be captured in the final image, resulting in a more realistic and visually appealing representation of the scene. However, the algorithm in[3]has a fundamental difficulty in preserving information in the brightest and darkest regions of HDR scenes[17]. To address this issue, edge-preserving smoothing (EPS) pyramids and content adaptive edge-preserving smoothing (CAS) pyramids were proposed in[4,5,6,7]. Since the EPS and CAS pyramids can smoothen the weights, the levels of the pyramids can be reduced. As such, the information in the brightest and darkest regions can be preserved well[17]. However, halo artifacts could be an issue for the algorithms in[4,5,6,7]as indicated in[17]. The information in the brightest and darkest regions can also be preserved well by synthesizing more differently exposed LDR images[17]. Many existing MEF algorithms were evaluated and compared in[18]by using the MEF-SSIM[19]. Brightness order reversal artifacts are an issue for all the MEF algorithms in[3,4,5,7]when two LER images are fused by them. Exposure interpolation could be used to avoid the brightness order reversal artifacts from appearing in the fused images[13,15]. Both the halo artifacts and brightness order reversal artifacts will be addressed by the proposed ULMEF algorithm.
It is worth noting that guided filtering for up-sampling (GFU)[20]was extended by using the upsampling methods in the Gaussian and Laplacian pyramids[16]to replace the bilinear upsampling in[20], and the extended GFU was applied to simplify the MEF algorithm in[4]. One beauty of the GFU is that the coefficients of weighted guided image filter (WGIF)[21]are only computed at two levels of the pyramids and they are up-sampled to obtain the coefficients of the WGIF at other levels. The other is that the weight maps can be computed from the luminance components and the coefficients of the WGIF at all the other levels. The GFU was adopted by the unsupervised learning based single-scale exposure fusion (ULSEF) algorithms in[8,9]to reduce their complexity.

Confronted with the limited paired training data, most data-driven exposure fusion algorithms are based on un-supervised learning. The first ULSEF algorithm named DeepFuse[12]reconstructed an information enriched LDR image from two LER images in YUV color space by using the MEF-SSIM in[19]. One more unsupervised ULSEF algorithm for two LER images was proposed in[22]. Yin et al.[23]introduced a content prior and a detail prior as guidelines to an encoder-decoder network for two LER images. Prabhakar et al.[24]proposed a few-shot learning method to generate labeled dynamic training data from unlabeled one, which greatly released the dependency on labelled ground truth. To release the restrictions on image resolution and exposure number, Ma et al.[8]introduced an interesting ULSEF algorithm entitled MEF-Net by using the second beauty of the GFU[20]. All the weight maps are first learnt from down-sampled images via unsupervised learning on top of the MEF-SSIM[19], and are then upsampled to the full size via the GFU. Recently, Jiang et al.[9]proposed a novel and fast ULSEF algorithm by using 1-D look-up tables which are learnt for each exposure by using the unsupervised learning and GFU. The coefficients of the GIFs[20]are computed once by the ULSEF algorithms in[8,9]. This is different from the extended GFU in[4]. The coefficients of the WGIF are computed twice in[4]such that the structures of the luminance components are transferred to the weighted maps better at the different levels of the EPS pyramids. This implies that the structures of the luminance components might not be transferred to the weighted maps well by the GFU methods in[8,9], especially when the coefficients of the GIFs are up-sampled by the bilinear upsampling too many times. Subsequently, their performance could be effected. One more issue with the ULSEF algorithms in[8,9]is that there are halos in the fused images when the coefficients of the filter are up-sampled too few times for those inputs with small sizes. There are also GAN-based exposure fusion algorithms such as MEF-GAN[25], etc.
The scene depth and local contrast might not be well preserved by these ULSEF algorithms. The loss function was defined by using the set of images to be fused and the fused image in the ULSEF algorithms[8,9]. As such, the fused image approaches the set of images to be fused rather than the HDR scene. They perform well if enough NER images are captured for the HDR scene. Unfortunately, this is nor always true. For example, the brightness order could be reversed in the fused image if two LER images of an HDR scene are fused by them, and information in the brightest and darkest regions of an HDR scene might not be preserved well in the fused image if only a few NER images are fused for the HDR scene. All these problems will be addressed by the proposed ULMEF algorithm. With the proposed algorithm, the fused image approaches the HDR scene.

SECTION: IIIThe Proposed ULMEF Algorithm

The proposed ULMEF algorithm is inspired by the conventional MEF algorithms in[3,4,5]and the data-driven algorithms in[12,14], and it accepts any input sequence with arbitrary spatial resolution and exposure number. Novel unsupervised loss functions are proposed to train the proposed network, thereby avoiding the requirement of ground-truth images for the training. As such, the proposed ULMEF amgorithm is applicable to any set of differently exposed LDR images without camera movements and moving objects.

For simplicity, three sets are defined for an HDR scene as in tableI. The relationship among the three sets,andis

andsatisfy

The exposure time of the LDR imageis denoted as, and

SECTION: III-AStructure of the Proposed MSF-Net

Given the set of differently exposed LDR images, a multi-scale fusion network is designed to fuse them in a coarse-to-fine manner. As demonstrated in Fig.1, a feature extract block is first adopted to transfer the input sequence from image domain to feature domain, adaptively extracting features that are beneficial for the fusion. This is similar to the algorithms in[12,14]in the sense that they are not based on the weight maps as the algorithms in[8,9]. The proposed algorithm and the algorithms in[12,14]are good at avoiding halos from appearing in the fused images. A pyramidis then built from the full-resolution featuresby using the bi-cubic down-sampling with a scale factor of. Here,represents the feature extract block. The fused image is constructed at each scale by

wheredenotes the network for feature fusion module, andis computed as

denotes the networkwithout the last layer,is the up-sampling operation, andis the concatenation.

It is shown in Fig.1that the fused image with different scales can be constructed through the network, and the multiple scales of the fused image are used to obtain the final image. The whole process is similar to the Gaussian pyramids and EPS pyramids in[3,4,5], thus preserving the scene depth and local contrast of the fused image well.

The proposed MSF-Net is on top of the multi-scale recursive residual group (MSRRG) which has two attractive characteristics: (1) the structure of the MSRRG is a residual network[26], it can reuse features to avoid the possible gradient vanishing, and (2) a novel multi-scale feature attention is used in the MSRRG to suppress less useful features and only allow the propagation of more informative ones to effectively improve the quality of the fused image. As illustrated in Fig.2, the MSRRG mainly includes different scale convolutions and dual-attention blocks (DABs)[27]. Each DAB combines a channel attention block and a spatial attention block in channel-wise and pixel-wise features, respectively. The DAB treats different features and pixels unequally, which can provide additional flexibility in dealing with different types of information. In addition, the proposed multi-scale structure is more beneficial for preserving fine details and scene depth for the fused image.

SECTION: III-BUnsupervised Loss Functions

Besides the structure of the proposed MSF-Net, loss functions also play a crucial role for the proposed ULMEF algorithm. Since the ground-truth image is not available, unsupervised loss functions are defined to train the proposed MSF-Net.

Loss functions are defined by using the fused imageand the set of images to be fusedin the existing ULSEF algorithms[12,8,9,14]. The fused image approaches the set of images to be fused. Novel loss functions are proposed in this subsection by fully utilizing the asymmetry between the training and inferring (or testing) phases. Besides the set of images to be fused, other LDR images from the same HDR scene with different exposures are also used to define the loss functions. The fused image approaches the HDR scene. Clearly, the proposed loss functions and the setare decoupled. The asymmetry between the training and inferring stages is well utilized by the proposed algorithm.

To preserve the scene contents in source images, the similarity constraint is implemented from two aspects: MEF-SSIM quality measurementand weight mean absolute error (WAE). The overall loss function is thus defined as

whereis a constant hyper-parameter to control the trade-off between the two aspects. The value ofis 10. Details on theandare given in the appendix.

The loss functionis widely adopted in the ULSEF algorithms[8,9,12], and it is defined by using the set of images to be fused. However, the proposedis defined by using the set, and is fundamentally different from thein the sense that the loss function and the set of images to be fusedare decoupled in the proposed. The loss functionis also new. Surprisingly, the new loss functioncan improve the proposed ULMEF algorithm from the MEF-SSIM point of view.

SECTION: III-CExposure Interpolation and Exposure Extrapolation

The proposed ULMEF is adopted to implement the exposure interpolation and exposure extrapolation as in the following two interesting cases:

Case 1Exposure interpolation: the setis a pair of two LER imagesandfrom an HDR scene[15]. The setconsists of the setand the image with the middle exposure from the same HDR scene.is 2 andis 3. The objective of including the image with the middle exposure from the same HDR scene in the case 1 is to avoid the possible brightness order reversal from appearing in the fused image[15].

Case 2Exposure extrapolation: the setis a set of three NER images,,andfrom an HDR scene. The setincludes the images in the setand two more differently exposed imagesandfrom the same HDR scene.is 3 andis 5. The objective of including the imagesandin the case 2 is to further help preserve the information in the brightest and darkest regions of the HDR scene in the fused image.

SECTION: IVExperimental Results

Experimental results are provided to validate the proposed ULMEF algorithm. Readers are invited to view to electronic version of figures and zoom in them so as to better check differences among all images. The dataset on HDR imaging in[15]is adopted to train and test all data-driven MEF algorithms. Camera shaking and object movement were strictly controlled to prevent them from appearing in the frame to capture static images[30]. The dataset is randomly split into three parts: 640 sequences for training, 50 ones for verifying, and the rest 100 ones for testing. To validate the generalization capability of different MEF algorithms, 50 sequences from the data set in[18]were also used to test them. We set the batch size to. The learning rate is initially set toand then decreased using a cosine annealing schedule for the training 200 epoches. All the experiments are implemented using PyTorch on NVIDIA A100.

SECTION: IV-AComparison of Different MEF Algorithms

The proposed ULMEF algorithm is first compared with four conventional exposure fusion algorithms in[3,5,28,29]and five data-driven exposure fusion algorithms in[12,8,25,9,14]in the case that the inputs are two LER images. The objective is to verify the efficiency of exposure interpolation.

All the ten algorithms are first compared from the subjective point of view. Particularly, they are compared from five points of view: halo artifacts, information in the brightest and darkest region, scene depth, local contrast, and brightness order reversal artifacts. As shown in Figs.3and4, the weight maps based algorithms in[9,5,28,8,3,29]suffer from brightness order reversal artifacts, and the algorithms in[9,5,8]suffer from halo artifacts even though the algorithms in[9,8]are much simpler than the algorithm in[12,14]. As demonstrated in Figs.3and4, the modified arctan function in[28,29]preserves the relative brightness order better than the Gaussian curve in[5,3]. Thus, the brightness order reversal artifacts are more serious in the fused images by the algorithms in[28,29]. However, the Gaussian curve preserves the information in the brightest and darkest regions better. The single scale exposure fusion algorithm in[9,8,12]cannot preserve the scene depth and local contrast as the MEF algorithms in[3,29,5,28]. Event though the learning algorithm in[14]is hierarchical, it cannot preserve the local contrast such as the grass in Fig.3well. The GAN based algorithm in[25]is on top of supervised learning and produces serious color distortions. The algorithms in[12,14,25]are not based on the weight maps. They are good at avoiding halo artifacts from appearing in the fused images however they cannot preserve the information in the darkest and brightest regions well. All these problems are overcome by the proposed ULMEF algorithm. Therefore, the exposure interpolation is important for HDR imaging on mobile devices with limited computational resource. Besides the subjective evaluation, all the ten algorithms are also compared from the MEF-SSIM point of view. The MEF- SSIM is calculated by using the fused image and the three captured images which are the reference images. As shown in TablesIIandIII, the proposed algorithm achieves the highest MEF-SSIM especially for TableIII, which is from the dataset[18]and has noticeable domain differences. These results indicate that the proposed ULMEF algorithm has strong generalization ability and is more robust to the dataset domain.

The proposed ULMEF algorithm is then compared with three conventional exposure fusion algorithms in[3,5,28]and two data-driven exposure fusion algorithms in[8,9]in the case that the inputs are three differently exposed images. The objective is to verify the efficiency of exposure extrapolation. As shown in Figs.5, all the algorithms in[3,5,8,9,28]cannot preserve information in the brightest and darkest regions of the HDR scene well if the inputs are three NER images. This problem is overcome by the proposed ULMEF. Therefore, the exposure extrapolation is also very important for HDR imaging on mobile devices with limited computational resource.

More experiment results are provided to test the robustness of the proposed algorithm and the five SOTA algorithms including MEF[3], GGIF[5], FMMEF[28], MEFNet[8], and MEFLUT[9]. All the two sets of differently exposed images are from the data set in[18]. Neither of the proposed algorithm and the algorithms[8,9]is trained by using the data from[18]. As shown in Fig.6, the proposed algorithm preserves information in the darkest and brightest regions of HDR scenes much better than the in MEF[3], GGIF[5], FMMEF[28], MEFLUT[9].

SECTION: IV-BComparison ofand

In this subsection, the conventional loss functionis compared with the proposed loss function) by testing four sets of differently exposed images. There are two LER imagesandin each set.is equal to. Each corresponding setincludes the setand one more image from the same HDR scene with the exposure time as.

As shown in Fig.7, there are serious brightness order reversal artifacts in all the four images fused by using the loss function. For example, the trees are darker than the sky in the first three images and the wall in the fourth image in the inputs but they are brighter than the sky and wall in the fused images. The problem is overcome by the proposed loss function, because each fused image approaches each HDR scene with the guidance from the loss function. Clearly, the proposed loss function significantly outperforms the loss function.

SECTION: IV-CAblation Study of Two Other Key Components

Two other key components of the proposed framework are: 1) multi-scale, and 2) loss function. Their
performances are evaluated in this subsection.

There are two multi-scale components in the proposed MSF-Net: the hierarchical structure and the MSSRG. Neither of them is enabled when the multi-scale is disabled. As shown in TableIV, there is noticeable gain from the MEF-SSIM point of view by using the multi-scale. Meanwhile, it can be shown from the zoom-in region in Fig.8that both the scene depth and local contrast are indeed preserved better by using the multi-scale components.

The proposedincludes two components. The componentis widely used in the existing unsupervised learning based exposure fusion algorithms. The new componentimproves the MEF-SSIM as demonstrated in TableIV.

SECTION: IV-DLimitation of The Proposed Method

Same as the DeepFuse[12], the complexity of the proposed method would be an issue if it was applied to fuse differently exposed images with a large size. It is interesting to combine the GRU, multi-scale, and the proposed loss functions to develop a new MEF algorithm. This method will be investigated in our future research.

SECTION: VConclusion Remarks

A novel unsupervised learning based multi-scale exposure fusion (ULMEF) algorithm is proposed for merging a set of different exposed low dynamic range (LDR) images into a high-quality LDR image for a high dynamic range (HDR) scene. The fused image approaches the HDR scene rather than the set of LDR images to be fused. Therefore, the proposed algorithm can avoid halo and brightness order reversal artifacts from appearing in the fused image and preserve the scene depth and local contrast as well as information in the darkest and brightest regions well in the fused image. In addition, experimental results show that the proposed algorithm can produce better fusion images than several state-of-the-art exposure fusion algorithms when only a few differently exposed LDR images are fused for an HDR scene. The proposed algorithm well utilizes the asymmetry between the training and inferencing (or testing) stages of the learning based algorithm and the conventional wisdom of inferring better via seeing more. This is a new initiative on the exposure fusion. We believe that better exposure fusion algorithms would be developed along the initiative in future.

SECTION: Appendix: The Proposed Loss Functions

Details on the proposed WAE and SSIM-MEF are provided in this appendix.

WAE: The loss functionis used to constrain the intensity distribution differences of images at the pixel level. Inspired by the conventional MEF algorithms in[3,4,5], a weight function is used to measure reliable information from all the differently exposed images in the set. This is different from the MEF algorithms in [10, 12,16] in the sense that the WAE is defined by the setin [10,12,16]. The weight functionis obtained by considering contrast, saturationand well-exposedness, and it is first computed as, and then normalized by the values of theweight maps such that they sum to one at each pixel, i.e.

In order to reduce sharp weight map transitions, the normalized weight maps are smoothed by using the iWGIF[7]with the guidance image as the luminance channel of each image. More reliable areas containing bright colors and details will be assigned larger weights, so that the network will pay more attention to obtain more reliable information. The loss functionis defined as

MEF-SSIM: Since the MEF-SSIM index in[19]is effective to measure the quality of the fused image, it is also selected as an objective function. Similarly, the MEF-SSIM is defined by using the fused imageand the set.

Letis an operator that extracts the-th patch from an image, i.e.,is theth patch extracted from the imagein the set. The MEF-SSIM index decomposesinto three conceptually independent components as

where,, andrepresent the intensity, contrast, and structure of the patchrespectively as

andis the mean intensity of the patch.

Since a higher contrast means a patch with a higher quality, the desired contrast is determined by the highest contrast by using all the images in the setas

and the desired structure is also defined by using all the images in the setas

The desired intensity of the fused patch is computed by a weighted summation over all the images in the setas

whereis defined by using the global mean intensityof the imageand the local mean intensityof the patchas

andare set asand, respectively, andis.

The desired fused patchis then computed by

and the MEF-SSIM index of the patchesis defined as

whereis the variances of the patch,is the covariance between the patchesand.andare two small positive constants to prevent the possible instability.

The MEF-SSIM loss functionis finally defined as

whereis the number of blocks.

SECTION: References