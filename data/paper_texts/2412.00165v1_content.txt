SECTION: Modelling Networked Dynamical System by Temporal Graph Neural ODE with Irregularly Partial Observed Time-series Data

Modeling the evolution of system with time-series data is a challenging and critical task in a wide range of fields, especially when the time-series data is regularly sampled and partially observable. Some methods have been proposed to estimate the hidden dynamics between intervals like Neural ODE or Exponential decay dynamic function and combine with RNN to estimate the evolution. However, it is difficult for these methods to capture the spatial and temporal dependencies existing within graph-structured time-series data and take full advantage of the available relational information to impute missing data and predict the future states. Besides, traditional RNN-based methods leverage shared RNN cell to update the hidden state which does not capture the impact of various intervals and missing state information on the reliability of estimating the hidden state. To solve this problem, in this paper, we propose a method embedding Graph Neural ODE with reliability and time-aware mechanism which can capture the spatial and temporal dependencies in irregularly sampled and partially observable time-series data to reconstruct the dynamics. Also, a loss function is designed considering the reliability of the augment data from the above proposed method to make further prediction. The proposed method has been validated in experiments of different networked dynamical systems.

SECTION: IIntroduction

Mathematical models are fundamental for us to describe and understand the evolution of a system. However, in complex networked systems, data is abundant, while the physical laws and governing equations to model the interactions between components that co-evolve in time remain elusive[1]. How to model the time evolution of networked dynamical systems from time-series data is challenging and critical in a wide range of fields such as traffic prediction of base stations in communication networks[2], learning the evolution of dynamics in particle-based systems[3], traffic flow prediction in transportation system[4]. Normally, we need to obtain complete time-series data with uniform intervals to model the system. However, time series data with non-uniform intervals often happen as well as the loss of information of components.

SECTION: I-AReview on regular time-series data with missing information

For example, in sensor networks or the Internet of Things, faulty sensors and network failures are widespread phenomena that cause disruptions in the data acquisition process. For incomplete time-series data, in which, at a certain time step, missing data appears at some of the channels of the resulting multivariate time series. Several methods have been proposed to impute missing values in time series, such as imputation methods based on the k-nearest neighbours[5], and matrix factorisation approximation methods[6]. Among different imputation methods, approaches based on deep learning have attracted a lot of attention[7,8,9]. While classic imputation methods can be used to fill the missing values of the feature matrix, none of them can capture the underlying graph structure. Due to the ability to reserve the graph-structured nature of the data by encoding the underlying graph-structured data using topological relationships among the nodes of the graph, imputation methods based on GNN have been proposed. In[7]a novel graph neural network architecture has been introduced, named GRIN, which aims at reconstructing missing data in the different channels of a multivariate time series by learning spatio-temporal representations through message passing.[10]presented a general approach for handling missing features in graph machine learning applications which is based
on minimization of the Dirichlet energy and leads to a diffusion-type differential equation on the graph.

SECTION: I-BReview on irregular sampling time-series data

In the above imputation methods, the intervals between observations in time-series data are constant. Time-series data with non-uniform intervals happens in many applications. For example, many real-world tasks for autonomous vehicles or robots need to integrate input from a variety of sensors with different sampling frequency[11,12]. In this situation, these imputation methods are not efficient any more. This is because most of the above imputation methods are based on recurrent neural networks (RNNs) framework when dealing with time series data. However, traditional RNN is ankward to deal with irregular time series data. A standard trick is to divide the timeline into uniform intervals and use a constant or undefined hidden states between intervals. Such prepocessing may destroy the information and lead to inaccurate modelling of the system. A better way solve this problem is to construct a continuous-time model with a latent state defined at all times. Che et al., proposed define RNNs with continuous dynamics given by a simple exponential decay between observations[13]. An elegant method has been proposed in[14], where the continuous dynamics between observations is estimated by a neural network as in Neural ODEs[15]. This model is called ODE-RNN, which can handle arbitrary time gaps between observations. Due to the power in dealing with graph-structured data, graph neural networks (GNN) have been combined with Neural ODEs to predict trajectories of dynamical systems with interacting components[16,17,18].

According to our survey, when modelling the evolution of networked systems with time-series data, most of the current research considers either regular sampling time-series data with missing information or irregular sampling time-series data. In this paper, we consider a more challengable task, which aims to model the evolution of the networked system by irregular sampling time-series data with partial observable information. To solve this problem, we propose a framework which combines an impute network and a prediction network. The impute part is a temporal Graph Neural ODE consisting of a Graph Neural ODE (GNODE) and a Graph Gate Recurrent Unit (GGRU). The Graph Neural ODE is applied to approximate the spatial and temporal evolution of hidden states between observations by a hidden continuous dynamics function estimated by a graph neural network. With the previous hidden state and the current observable state, Graph Convolutional Gate Recurrent Unit can be used to update the hidden state. However, traditional RNN-based methods
leverage shared RNN cell to update the hidden state which does not capture the impact of various intervals and missing state information on the reliability of estimating the hidden state, i.e., current observed state with less missing information and smaller time interval from last observation is more reliable to update the hidden state. To solve this problem, in this paper, we propose a reliability and time-aware mechanism which can capture the impact of various intervals and missing state information.

Hidden states are updated at each observable time step based on the ground-truth data, the imputation within observed time series is accurate. However, this impute network is not accurate in prediction because the framework based on RNN (GRU, LSTM) is trained for one-step ahead prediction which causes the accumulation of estimation error step by step. To solve this problem, we train another GNODE by the time-series data generated by the impute network. This prediction network is very easy to train compared with the impute network because no need to estimate and update step by step. Data generated by the impute network consists of the ground-truth data from observation and generative data. The generative data close to the observable data is more accurate and is more important for training. Therefore, each sample’s weight of the data can be adjusted according to its data quality and each sample’s weight is introduced to loss function for training. Here, we design an exponential decay function to calculate the weight of each sample. This enables the sample of higher quality to play a more important role in training the prediction network.

SECTION: I-CNovelty and Contribution

Modelling a dynamics system and predicting future states with irregularly partial observed time-series data is a challengable task. The contribution of this paper is that we propose a framework which consists of an impute network and a prediction network to model the evolution of networked system by irregular sampling and partial observable time-series data. The impute network is based on temporal Graph Neural ODE consisting of Graph Neural ODE and Graph Gate Recurrent Unit with reliability and time-aware mechanism. Unlike RNN-based method with sharing RNN cell to update hidden states, the proposed method which can capture the impact of various intervals and missing state
information as well as the spatial and temporal dependencies, enabling accurately impute temporal and spatial data. The prediction network can make prediction by learning from the imputation data. Since the quality of the imputation data generated by the impute network is heterogeneous, an exponential decay function is designed to adjust the weight of the data to calculate the loss function. This enables the sample of higher quality to play a more important role in training the prediction network.

SECTION: IIMethods

SECTION: II-AModel of networked system

In a networked system ofnodes with nonlinear dynamics, the dynamics of each node can be described by an ODE as

whereis the state of node,andis the dimension of the state.is the self-dynamics of each node,is the coupling dynamics between nodeand nodeandis the element of the adjacency matrixin whichif nodeis connected with node. Sometimes, we do not know the accurate dynamic functionandof the system. Instead, we can only observe the state of each nodeat time step.is the matrix of state at time step. Notice that we consider the irregular sampling data of the system, so the interval between different time stepsis not a constant value.
RNN is a widely used method in time series data with the update function

whereis the hidden state at time step. A problem in handling the irregular time-series data is how to define the hidden state between observations, sinceis not a constant value. If we directly use the traditional RNN in equation (2) to deal with irregular time series data, it means that the hidden state between observations is constant, which may destroy the information of the hidden state. One way to solve this problem is to use a Neural ODE to estimate the hidden state between observationsand, and update at observation time step.

SECTION: II-BSpatial Graph Neural ODE

Neural ODEs are a family of continunous-time models which defines a hidden stateas a solution to ODE initial-value problem. The functionspecifies the dynamics of the hidden state, using a neural network with parameters. The hidden statecan be calculated at any time step using a numerical ODE solver:

In Neural ODE, the functionis approximated by a neural network (NN). The state between observations can be defined by the solution of an ODE:. Thencan be updated by equation (2). The stateat any time between two observations is predicted by the corresponding. While we are free to choose any kind of neural network to estimate, ignoring the potential physical structure of the system may cause the neural network to be accurate within the training date set but fail in testing data.

Since we aim to model the networked system, the dynamics of the system has the form of equation (1). State of each component is decided by its self-dynamic function and its neighbours’ states. Therefore, it is natural to consider using a kind of graph neural network to estimate the function. The dynamics of the hidden state could be written as

whereis the hidden state of node. Here, we use a specific GNN to estimate the dynamics of the hidden state.can be estimated by a MLP. Generally, the coupling dynamics can be estimated by alayers GNN which incorporatesadjacent matrixto aggregate information through walks of length. With the increase of layers in GNN, each node can aggregate information from further neighbour nodes. However, GNN suffers from the oversmoothing problem, a phenomenon where all node features in a deep GNN converge to the same constant value as the number of hidden layers is increased[19]. To ensurecan capture the nonlinear relationship between neighbour nodes and avoid powers of adjacency matrix,is estimated by, whereis a multi-layers neural networks, the symboldenotes concatenation operator.

wheredenotes the function sum andis a multi-layers neural network.The advantage of equation (5) is that it is computationally efficient because it has a clear structure to estimate the self-dynamics and coupling-dynamics to avoid unnecessary aggregation of information from other nodes. In addition, it avoids oversmoothing problem by only using one layer adjacency matrix.

The problem is that the complete information ofis not always available at any observation. To impute the presence of missing values, we consider a binary maskwhere each rowindicates whether the node features ofat time stepare available. For example, if, the information ofis unavailable. Otherwise, if, the information ofis available.

In the first step, we need to make the initial imputation ofby, whereandare learnable matrix. Generally, the initial hidden stateis unknown, and we can simply assume thatis amatrix. The later hidden state between observations can be estimated by equation (3) and updated by equation (2). The statewith missing values can be imputed by

whereis reconstructed by the proposed method.

SECTION: II-CTemporal Graph Neural Network

RNN is a general method to work with time series data, but face the short-term memory problem due to the gradient vanishing problem. Here, we use Gate Recurrent Unit (GRU), which is designed to overcome the short-term memory problem, to learn the hidden state and predict the next state. The main structure of a traditional GRU is

where symboldenotes the Hadamard product,are learnable weight matrices.

Since the time series data is graph-structured, we implement temporal graph convolutional network instead of the traditional GRU. For given, i.e. the feature of nodeatlayer,is updated by a MPNN defined as

anddenote differentiable functions such as single-layer or multi-layers neural network. For computational efficiency, the MPNN is specific as

Equation (7) is replaced by

According to equation (6),consists ofand.with less missing information enablesmore reliable. To quantify the reliability of stateat any time step, one method is to combinewithas a feature vector[13]. This method is straightforward to identify the importance of observed states but ignores the fact that prediction states at different time steps actually have different reliability. For example, ifis close to, then it is reasonable to think that it is reliable to useto fill the missing values in. Otherwise, it is not an accurate estimation of. Therefore, we use a reliability factor matrixto quantify the reliability ofat any time step. At, elementinis calculated by

where. Then equation (10) is rewritten as

Notice that time intervals between observations are not the same. As the increase of the time interval, it is more difficult for equation (3) to estimate the hidden dynamics between intervals. This is because the hidden states between intervals follow complex trajectories but are determined by the last hidden state. The error in the last hidden state may be enlarged with time. For example,, the solution of. If there exists an errorin, then the error inis. Also, the numerical methods like Euler methods, Runge-Kutta methods to calculate the Neural ODE will accumulate errors with the time interval. This means that the hidden state estimated in with a smaller interval is more reliable. However, ODE-RNN in equation (2) leverages a simple shared RNN cell to update the hidden state at any time step, which does not capture the impact of various intervals on the reliability. Therefore, it is reasonable to consider the factorin forget gateas

Since part of information inis available, which is represented by, this part of available state information can be used to calculate the loss function with the constructed stateby the proposed method. The loss function is defined as

whereis a vector of all ones. The algorithm is shown in Algorithm (1).

The prediction network is trained using observable data and imputation data. The error exists between the imputation data and the ground truth inevitably. Since the imputation data is obtained by the proposed RNN-based method which is trained for one-step ahead prediction, the estimation error accumulates step by step between two observations.

The generative data close to the observable data is more accurate and is more
important for training. Therefore, instead of using the homogeneous weight in loss function, e.g.,, where the weight of each term,is, the weightof each term is designed based on the time interval between estimation state and the observation state as follow:

whereis the time between two observable timeand.is estimated by the imputation network based on.is the exponential decay constant..is the number of features we can observe.is the total number of features the state should have at time.quantifies the ratio between the number of observable features and actual features. Equation (15) indicates that the estimation state close to the observable state with less lost information plays a more important role in training prediction network.

SECTION: IIIExperiments

We compare the proposed method in this paper to other autoregressive models such as the RNN based methods like traditional RNN, RNN, RNN Decay, and Neural ODE. As the loss functioncaptures errors throughout an entire time series we adopt this also as our evaluation metric.

We consider a simple case study of a networked dynamics system which consists ofnodes and each node has a-dimensional dynamics function. The dynamics of each node is

The coupling function is. The connection among nodes is set as

The data is generated by Equation (16) with irregular observation time-series points which are generated by exponential function. Part of observable states’ information is randomly deleted to generate the partial observable data. The modeling of the evolution of the dynamics system is shown in Fig.4. The sampling data is from 0 to 10 seconds and the prediction is from 10 to 20 seconds. We randomly generate 100 points betweenandseconds as the ground truth and only part of the data (e.g.) is reserved as the observations. Then we randomly delete part of data at each time points as the missing information. In total we sample 100 trajectories and keepas trainning set andas testing set. Fig.4shows the evolution of the system reconstructed according to the irregularly partial observed data. Table (I) shows the comparison results of different methods.

SECTION: IVConclusion

Modelling a dynamics system and predicting future states with irregularly partial observed time-series data is a challengable task.In this paper, we propose a framework which consists of an impute network and a prediction network to model the evolution of networked system by irregular sampling and partial observable time-series data. The impute network is based on temporal Graph Neural ODE consisting of Graph Neural ODE and Graph Gate Recurrent Unit with reliability and time-aware mechanism. Unlike RNN-based method with sharing RNN cell to update hidden states, the proposed method which can capture the impact of various intervals and missing state
information as well as the spatial and temporal dependencies, enabling accurately impute temporal and spatial data. The prediction network can make prediction by learning from the imputation data. Since the quality of the imputation data generated by the impute network is heterogeneous, an exponential decay function is designed to adjust the weight of the data to calculate the loss function. This enables the sample of higher quality to play a more important role in training the prediction network. We verified our method in a networked dynamics system and compared it with other existing methods and our method can more accurately model the dynamics of the system from time-series data. However, we found that compared with other methods, using Neural ODE to calculate the hidden state is more time-consuming and sensitive to the time step when calculate the ODE. Therefore, in the future, we will explore how to accurately and efficiently estimate the hidden state between intervals. Also, it is interesting to specify the proposed method in different application fields with network structures.

SECTION: References