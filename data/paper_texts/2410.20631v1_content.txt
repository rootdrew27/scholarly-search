SECTION: PViT: Prior-augmented Vision Transformer for Out-of-distribution Detection
Vision Transformers (ViTs) have achieved remarkable success over various vision tasks, yet their robustness against data distribution shifts and inherent inductive biases remain underexplored. To enhance the robustness of ViT models for image Out-of-Distribution (OOD) detection, we introduce a novel and generic framework named Prior-augmented Vision Transformer (PViT). PViT identifies OOD samples by quantifying the divergence between the predicted class logits and the prior logits obtained from pre-trained models. Unlike existing state-of-the-art OOD detection methods, PViT shapes the decision boundary between ID and OOD by utilizing the proposed prior guide confidence, without requiring additional data modeling, generation methods, or structural modifications. Extensive experiments on the large-scale ImageNet benchmark demonstrate that PViT significantly outperforms existing state-of-the-art OOD detection methods. Additionally, through comprehensive analyses, ablation studies, and discussions, we show how PViT can strategically address specific challenges in managing large vision models, paving the way for new advancements in OOD detection.

[1]organization=School of Electrical and Electronic Enigeering, The University of Sheffield,
city=Sheffield,
country=United Kingdom

[2]organization=School of Computer Science, The University of Sheffield,
city=Sheffield,
country=United Kingdom

SECTION: Introduction
In recent years, the Transformer, characterized by its innovative attention mechanism, has achieved a significant success in various domains, extending its success from natural language processing to various vision tasks. The inception of the Vision Transformer (ViT)represents a pivotal moment in the adaptation of Transformer architectures for vision applications, setting the stage for subsequent models that exhibit remarkable performance enhancements through increased depth and scale, albeit at the cost of heightened computational demands. However, the exploration into enhancing Out-of-Distribution (OOD) detection within these architectures has lagged, especially when compared to the extensive research conducted for Convolutional Neural Networks (CNNs)-based models.

OOD detection is a crucial machine learning technique that aims to identify test samples from distributions divergent from the training data distribution. This technique is essential for differentiating between inputs that are part of the training distribution and those that are not. The importance of proficient OOD detection is underscored in safety-critical real-world deployments, where encountering novel classes is inevitable. Addressing the generalization and OOD detection capabilities of ViTs becomes imperative.

We explore the strategic incorporation of prior knowledge in vision models to enhance their safety-related capabilities. The hypothesis is that, like humans, AI models can benefit from contextual cues, improving their ability to accurately identify and classify data. This suggests that prior knowledge may be crucial in helping models discern nuanced data distributions, leading to our central research question:

In this work, our goal is to leverage the advantages of ViTs to propose a scalable solution that significantly improves their robustness and performance for OOD detection. ViTs mark a paradigm shift from traditional CNNs by their adeptness in synthesizing extensive contextual information beyond mere pixel analysis. This capability is further enhanced by refining the attention mechanism, enabling ViTs to assimilate additional contextual data and thus broadening the analytical scope beyond the immediate visual input. Importantly, this expansion creates a strategic avenue for integrating prior knowledge, derived from high-performing pre-trained vision models, into the learning process.

Following this idea, we propose the novel Prior-augmented Vision Transformer (PViT) for OOD detection. As illustrated in Fig., PViT is designed to generate predictions that closely align with the prior logits for In-Distribution (ID) data while exhibiting significant divergence for OOD data. The prior knowledge is derived from a pre-trained model on the ID dataset, referred to as thein this paper. PViT is trained using the prior predictions generated by the prior model. During inference, PViT employs the proposed(PGE) score to effectively distinguish OOD instances by quantifying the divergence between the prior logits and the predicted logits.

We demonstrate that our proposed framework, PViT, is highly effective for OOD detection, particularly on large-scale datasets such as. Compared to state-of-the-art OOD detection methods, PViT achieves remarkable performance improvements, reducing the FPR95 by up to 20% and increasing the AUROC by up to 7% compared to the best baseline. Additionally, PViT eliminates the need for generating synthetic outlier data while maintaining high accuracy on ID datasets.

The key contributions of this paper are summarized as follows:

We introduce PViT, a novel and generic framework that integrates prior knowledge into ViT, thereby enhancing model robustness and OOD detection capabilities.

We introduce the Prior Guide Energy as an effective scoring method for OOD detection by measuring the similarity between the prior class logits and the predicted class.

We conduct comprehensive experiments on various benchmarks across a diverse set of ID and OOD datasets, providing qualitative analyses of PViT and offering insightful discussions on the impact of incorporating prior knowledge into ViT models.

SECTION: Raleted Works
Originally proposed for machine translation, Transformers have ascended to the state-of-the-art in numerous Natural Language Processing (NLP) tasks. The vanilla ViT, representing the first adaptation of a purely Transformer-based model for image classification, has shown competitive performance with state-of-the-art CNNs. ViTs has been widely applied and extended to where they show remarkable performance on a variety of visual tasks, such as image classification.

A recent study exploring the use of Transformers for Bayesian Inferencehas broadened the scope of their applicability. This research demonstrates that when trained on prior samples, Transformers are capable of effectively approximating the posterior predictive distribution (PPD), even in scenarios involving small tabular datasets. In contrast, our method is designed to work with large-scale image data, showcasing the versatility of taking advatages of prior information in handling diverse data scales and types.

While the introduction of specialized tokens in ViTs is a relatively unexplored area, our work pioneers the use of afor OOD detection. The concept of ais not entirely new, as seen in applications like MatteFormerfor image matting, which integrates trimap information via a Prior-Attentive Swin Transformer block. Our approach, however, diverges significantly as it repurposes this concept for enhancing OOD detection in ViTs.

OOD detection methods primarily focus on network truncationand scalar score functions. Network truncation enhances the separation between ID and OOD samples by adjusting the network’s signal propagation. Scalar score functions often utilize the classification layer to derive confidence scores. The Maximal Softmax Probability (MSP)is a foundational technique, while advancements include energy functions for bias-free class-conditional probability estimationand the maximum of logit, which combines class likelihood with feature magnitude. Kullback–Leibler (KL) divergence further refines class-dependent information.

Distance-based detectors constitute another significant category, identifying OOD samples based on their distance to ID data in the feature space. The Mahalanobis detector measures the distance to class-wise means using shared feature covariance. SSD provides a unified approach by assuming a single Gaussian distribution for ID samples, whereas KNN, a non-parametric method, offers precise boundary representation and has been improved through NNGuide for better OOD differentiation on distant datasets. Other methods including The Generative-based methods aim to model the underlying data distribution to differentiate between ID and OOD samples. Our proposed PViT enhances scalability and adaptability by avoiding reliance on synthesized data or external outliers for training.

SECTION: Methodology
SECTION: Preliminaries
In the context of image classification, letrepresent the input space, anddenote the finite set of labels forclasses. The training datasetconsists of pairs, where a classification functionpredicts class scores. The predicted labelis obtained as, corresponding to the class with the highest score.

For testing on unseen data, the objective is to train a model capable of distinguishing OOD inputs, where labelsdo not belong to. To achieve this, a binary classification approach, also referred to as the decision rule for OOD detection, is employed:

where the thresholdis selected to ensure high classification accuracy for ID data, typically set at 95%. The score, also known as ’confidence’, represents the classifier-based detection score.

SECTION: Prior-augmented Vision Transformer (PViT)
The architecture of the Prior-augmented Vision Transformer (PViT) is depicted in Fig.. The implementation of PViT follows the foundational structure of the vanilla Vision Transformer (ViT). In the conventional ViT, an input imageis transformed into a sequence of flattened 2D patches. Here,represents the resolution of the original image,denotes the number of channels,defines the resolution of each patch, andindicates the total number of patches, which effectively determines the sequence length for the Transformer.

Similar to the vanilla ViT, a learnable embedding, initially set to, serves as the class embedding. This embedding is designed to capture the global image representation and is iteratively updated throughout the Transformer layers. Afterlayers of the Transformer encoder, its final state,, serves as the aggregated image representation, denoted by, whereis the dimensionality of the embedding space. Given an input image, the model first computes patch embeddings, whererepresents the number of patches. A class tokenis then prepended to this sequence of embeddings. Positional encodingsare added to provide the sequence with spatial information, resulting in the final sequence of embeddings.

Given a pre-trainedparameterized by, the prior logits vectorrepresents the classification output logits, serving as prior knowledge for PViT. To incorporate this prior knowledge into the ViT architecture, we introduce a special token, termed the. This token,, encapsulates the prior knowledge and is input alongside the patch tokens and the class token into the prior-augmented encoder, where it is processed by the attention mechanism.

To create the prior token, the logits vectorfrom the pre-trained classifier is first normalized using the softmax function. These normalized logits are then projected into the embedding dimensionto form the prior token:

whereis a learnable projection matrix designed to transform the class-wise priors into the embedding space, aligning them dimensionally with the patch embeddings.

The prior token is then scaled by a factor, a hyperparameter that modulates the influence of prior knowledge. This scaling balances the model’s attention between the prior token and the image-derived embeddings, optimizing overall performance. The scaled prior tokenis then replicated across the batch, resulting in:

wheredenotes the batch size, andrepresents the outer product with a vector of ones, effectively broadcastingacross the batch. The batch-level prior tokenis appended to the positionally encoded sequence, forming the complete inputfor the encoder.

The concatenated sequenceis processed through the Transformer encoder layers to yield the final representations. Our model follows the architecture of the vanilla ViT, employing multi-headed self-attention (MSA)(Eq. ()) and multi-layer perceptron (MLP) blocks (Eq. ()). Layer normalization (LN) is applied before each block (Eq. ()), as described in the following equations:

whererepresents the final layer’s class token representation. The outputserves as the input to a classifier head for the task at hand.

In the context of image classification, the primary training objective for PViT is to minimize the divergence between the model’s predicted distribution and the true label distribution. The overall training objective is achieved through the minimization of the cross-entropy loss function, which is formulated as:

whererepresents the parameters of PViT,the true labels,the input data,the dataset, andthe prior information.

SECTION: Prior Guide Energy for OOD Detection
Given a base confidence score function, we propose a Prior Guide Energy (PGE) method to effectively differentiate between ID and OOD data by incorporating prior knowledge:

whereis the, designed to measure the similarity between the prior embeddings and the outputs of PViT. Cross Entropy (CE), widely recognized as an effective and direct method for training targets in classification tasks, is introduced here as the optimal guidance term by taking the prior logits and the predicted class as inputs:

whereis the probability of classbased on the prior logits, andis the predicted class by PViT. This guidance term measures the dissimilarity between the model’s predictions and the prior distribution. A higher cross entropy score indicates greater alignment with the prior distribution, suggesting that the data are likely ID, while a lower score suggests potential OOD data. Cross entropy is one of the guidance terms we introduced; other optional guidance terms for measuring similarity are discussed in the Ablation Study section.

For the base confidence score, we choose the:

whereare the logits for classoutput by the pre-trained prior model. Free Energy has been shown to be effective for OOD detection, as it provides a confidence measure in the model’s predictions. High energy indicates low confidence (a more spread-out distribution), while low energy indicates high confidence (a more peaked distribution).

This proposition asserts that a high PGE score indicates ID data, while a low PGE score indicates OOD data. This behavior is due to the alignment between predicted logits and the prior distribution, as captured by the cross entropy term. The Free Energy component provides a confidence measure, with higher energy indicating lower confidence and vice versa. Detailed proof is provided in the Appendix.

SECTION: Experiments
To assess the model performance, we use the small-scaleand the large-scaledataset, as our ID training datasets.andare used interchangeably as ID and OOD datasets due to their similarities yet distinct characteristics. We use the standard train/validation/test splits for training and testing. In main results reported in Tab.and Tab.whereis used as ID data, we employ a range of natural image datasets as OOD benchmarks, including,,,,,, and. In Tab..whereis used as ID dataset, the following OOD test datasets are used to text the OOD performance of PViT:,,,,, and.

The PViT is trained with a configuration that includes a hidden dimension of, a depth oflayers,MSA heads, and a MLP dimension of. The Adam optimizeris used with hyperparametersandoverepochs. Training begins with an initial learning rate ofand employs a batch size of, a momentum of, and a weight decay of. A linear learning rate decay schedule is applied afterwarm-up epochs. For different ID datasets, we utilize various prior models, including ResNetand ViT modelsalong with their variants. For ImageNet-1K as the ID dataset, the ViT models are pre-trained onand subsequently fine-tuned on. All pre-trained models used as prior models are publicly available.

For assessing the performance of our proposed models in OOD detection, we employ two evaluation metrics: (1), which measures the false positive rate of OOD samples when the true positive rate for ID samples is at 95%; (2), which computes the Area Under the Receiver Operating Characteristic Curve.

SECTION: Evaluation on OOD Detection
We evaluate our PViT’s performance in OOD detection against competitive baselines including MSP, MaxLogit score, Mahalanobis score, Energy score, SSD, ViM, KNN, and NNGuide. To ensure fairness in comparison, we do not include any synthesis-based OOD methods, such as VOSor Dream-OOD, since our PViT could be further also enhanced with the inclusion of synthesis data training. For all instances of PViT shown in Tab.and Tab., the scaling factor of the prior tokenis set to.

As shown in Tab.and Tab., PViT is evaluated across a wide range of seven OOD datasets to demonstrate its superior performance. Our experiments show that PViT exhibits remarkable performance on large-scale ID datasets, particularly when utilizing ViT-based prior models. Notably, even when compared to the Mahalanobis and ViM detectors, both of which are known for their strong performance in ViT-based architectures due to the Gaussian nature of the vision transformer embedding space, PViT significantly outperforms these benchmarks in both the prior models of vanilla ViT variants.

To provide a comprehensive evaluation of PViT, we also include results on the small-scaledataset as the ID data in Tab.. Although PViT does not achieve superior performance compared to the baselines, which is likely due to the insufficient data samples in the small-scaledataset and the need to scale image pixels fromtoto maintain consistency for the same configuration of PViT, PViT still ranks among the top performers. The results shown are the average over six OOD datasets.

SECTION: Ablation Studies
Fig.illustrates the impact of scaling prior weightson PViT’s output. Our analysis aims to mitigate the excessive influence of priors. It is observed that a higherleads to more attention on the prior token, potentially reducing the focus on image patches. Conversely, a lowermay enhance attention on image patches. Optimal performance is achieved when PViT balances its focus between the prior token and image tokens. This balance results in a clearer distinction between priors and predictions, underpinning the effectiveness of PViT in OOD detection. More results of different scale hyperparameterare presented in the Appendix.

To further investigate the trade-offs in attention weights among patch tokens, we visualize the attention map in Fig., using image examples with corresponding prior and predicted labels. The color bar in these maps ranges from highest to lowest attention, highlighting the impact of the scaling factoron the model’s attention mechanism. Notably, the attention weight for the prior token, indicative of PViT’s focus on the prior token, is demarcated with a red line on the color bar. As a result of OOD detection, we can see that in the third row of figures, PViT often produces predictions that notably differ from the prior model’s.

Although we have introduced using the CE as the guidance term in Eq. () for detecting OOD instances, we also considered other metrics to measure the difference between the priors and the predicted logits: 1) Euclidean Distance(ED). Euclidean Distance is a geometric measure calculating the ”straight-line” distance between two points in Euclidean space. For vectors of predicted logits and prior probabilities, it is computed as, withandbeing the corresponding elements from the prior and predicted logits vectors, respectively. 2) Kullback–Leibler Divergence (KL-Divergence), used for measuring the distance between two probability distributions, is defined as, whererepresents the true distribution of data (priors in our context) anddenotes the distribution inferred by the model (predicted logits).

As visualized in Fig., the score distributions reveal that our prior-guided energy score better distinguishes between ID and OOD data compared to the original energy score. Both CE and ED guidance terms produce similar results, albeit with different score values. The performance comparison in Fig.further demonstrates that ED achieves performance comparable to that of the CE as the guidance.

SECTION: Discussions and Advanced Perspectives
Bayesian Neural Networks (BNNs) have been explored for OOD detection in various studies. BNNs, which integrate Bayesian methods into neural networks, utilize probability distributions over model parameters to represent uncertainties in predictions. In the context of OOD detection, BNNs can be employed by comparing uncertainties between the model’s predictions on given inputs and known ID data. However, the suitability of BNNs for OOD detection has been a subject of debate in recent works.

From a Bayesian perspective, our approach can be interpreted as utilizing prior knowledge from an ID dataset to establish the Predictive Posterior Distribution (PPD) within a Bayesian framework. This aligns with the concept of Transformers facilitating Bayesian inference. In our model, the priors can be treated akin to mean values of sampled priors from a Bayesian model, positioning PViT to approximate the posterior distribution. This approximation follows the equation, wheredenotes the likelihood of observing the data given the label and priors,represents the prior probability informed by the dataset and the prior model, andsignifies the evidence, usually computed by marginalizing over the label space. However, the method proposed byis specifically designed for single-sequence data, aimed at providing ultra fast Bayesian inference in a single forward pass. This approach, while not directly applicable to image data due to ViTs’ processing limitations, sheds light on the efficacy of our PViT in OOD detection. By capturing uncertainties through Bayesian inference, it provides a compelling explanation for PViT’s robust performance in identifying OOD samples.

Gernarally, ViTs and CNNs are believed that they exhibit fundamentally different inductive biases. ViTs inherently focus on global image patterns by treating image patches as analogous to tokens. This global perspective contrasts sharply with the local feature emphasis of CNNs, which inherently encode a bias towards local spatial hierarchies and proximities. While this enables ViTs to excel in tasks requiring holistic image comprehension, their lack of built-in locality bias may limit effectiveness in tasks where detailed local feature analysis is crucial. ViTs can adopt inductive biases through data augmentation or hybrid architectures, improving their local feature processing, traditionally a strength of CNNs. Our study introduces embedding additional prior information into ViTs, enhancing their robustness and serving as a method to incorporate inductive bias. This strategy marks a new path for embedding manually designed inductive biases into ViTs, potentially boosting their robustness and explainability.

Our approach is limited by its dependence on the accuracy and structure of the prior model, with ID accuracy being constrained by the prior model’s performance. PViT shows better OOD detection when using ViT architectures as priors compared to CNNs, likely due to structural similarities. Additionally, PViT requires training on ID data, which, despite benefiting from prior knowledge and achieving rapid convergence, introduces extra complexity. During inference, the need to perform inference on both the prior model and PViT also increases computational cost.

Our exploration shows that PViT introduces a beneficial inductive bias to large vision models, which are increasingly prevalent and evolving. Similar to advancements in Large Language Models (LLMs), large vision models are also proliferating and facing the challenge of ”planning”—directing models towards specific, controlled outcomes. Future work could expand PViT to incorporate varying levels of priors, customizing OOD detection for diverse scenarios and offering promising research directions across different vision transformer architectures.

SECTION: Conclusion
In this work, we present Prior-augmented Vision Transformer, a novel and generic framework for OOD detection. PViT uniquely integrates prior knowledge as a prior token to be trained to approximate the true label, allowing for effective differentiation of OOD data by examining the relative distances between model predictions and the prior logits. Our empirical results demonstrate that PViT achieves outstanding performance in OOD detection benchmarks. Moreover, the innovative integration of prior knowledge by PViT not only enhances OOD detection capabilities but also suggests a versatile approach for the strategic planning and control of large vision models tailored to specific practical applications.

SECTION: References
SECTION: Supplementary to Methodology
We provide the proof for Proposition:

We begin by considering the cross-entropy guidance term between the prior logits and the predicted classes, as given in Eq. (). Letbe the predicted probability obtained from the softmax function applied to the model’s logits:

The guidance term defined by cross-entropy in Eq. () can be rewritten as:

Substituting the expression for, we get:

Simplifying the expression using logarithmic properties, we obtain:

The cross-entropy loss can be further decomposed into two terms:

Sinceis a one-hot vector, only the term corresponding to the true class labelis non-zero:

Incorporating this into the energy-based score, we define the final score as:

Expanding this expression results in:

The term, known as the log-sum-exp, serves as a measure of the model’s confidence in its prediction distribution. A higher value indicates a more spread-out distribution, signifying lower confidence and suggesting that the current prediction is more likely for OOD data. Conversely, a lower log-sum-exp value indicates a peakier distribution, implying higher confidence, which is typical for ID data.

The termpenalizes the logit of the true classby the overall confidence represented by the log-sum-exp of all logits.

The quadratic termemphasizes the confidence on the data distribution.

Thus, with an appropriate threshold, if, then. Conversely, if, then. This completes the proof.
∎

SECTION: Dataset Zoo
SECTION: ID datasets
consists of 60,000 32x32 color images in 10 classes, with 6,000 images per class. The dataset is divided into 50,000 training images and 10,000 test images. The classes are mutually exclusive and include objects such as cats, dogs, trucks, and ships.

is similar to CIFAR10 but contains 100 classes each consisting of 600 images. Each class is divided into 500 training images and 100 testing images. The 100 classes in CIFAR100 are grouped into 20 super-classes, each comprising 5 sub-classes.

, a subset of the larger ImageNet dataset, contains over 1.2 million images spanning 1,000 classes. It is commonly used in deep learning for tasks such as object recognition and classification. The dataset is noted for its diversity and is a standard benchmark in computer vision research. ImageNet-1K’s classes encompass a wide range of objects, including various species of animals, plants, and everyday objects.

SECTION: OOD datasets
is derived from the larger ImageNet dataset, scaled down to include 200 classes. Each class contains 500 training images, 50 validation images, and 50 test images. This dataset serves as a more compact yet challenging benchmark for image classification.

comprises diverse images of textures categorized into several classes, providing a unique challenge for texture recognition and classification models. It is widely used for evaluating model robustness against textural variations.

contains digit images obtained from house numbers in Google Street View images. It includes over 600,000 digit images, making it a comprehensive dataset for digit classification tasks.

is a large-scale dataset of scene-centric images. With 365 scene categories and over 1.8 million images, it is extensively used for scene recognition and contextual understanding in images.

is a dataset of natural scene images under varying illumination and weather conditions, often used for assessing model performance in diverse environmental settings.

consists of around one million labeled images for each of 10 scene categories and 20 object categories, providing a diverse set for scene understanding and object detection tasks.

andare datasets specifically designed for evaluating out-of-distribution (OOD) detection in neural networks. SSB-hard focuses on subtly different classes, while NINCO provides near-in-context OOD examples, presenting unique challenges for OOD detection.

is a subset of the OpenImages dataset, tailored for OOD detection. It includes a wide range of object categories not present in standard datasets like ImageNet, making it ideal for testing the generalizability of models.

contains images of natural world. It has 13 super-categories and 5,089 sub-categories covering plants, insects, birds, mammals, and so on. We use the subset that contains 110 plant classes which do not overlap with.

SECTION: Prior Models Details
All pre-trained models used as the prior models in this paper are public available.

Achieving a test accuracy of 0.9498, this model was trained with a batch size of 128 over 300 epochs, and validation every 5 epochs. The SGD optimizer was used with a learning rate of 0.1, momentum of 0.9, and weight decay of 0.0005, paired with a ReduceLROnPlateau scheduler. This pretrained ResNet-18 model is available on Hugging Face.

The model, with a test accuracy of 0.9788 and a loss of 0.2564, was trained using an Adam optimizer with a learning rate of 5e-05, and a linear learning rate scheduler. Training and evaluation batch sizes were set to 32. The pretrained ViT model can be found on Hugging Face.

This model recorded a test accuracy of 0.7926. Using SGD as the optimizer with a learning rate of 0.1, momentum of 0.9, and weight decay of 0.0005, along with a CosineAnnealingLR scheduler, the model is available on Hugging Face.

The fine-tuned version of google/vit-base-patch16-224-in21k on CIFAR100 achieved an accuracy of 0.8985 and a loss of 0.4420. It used a learning rate of 0.0002, with train and eval batch sizes of 16 and 8. The pretrained ViT is available at Hugging Face.

Provided by Microsoft, this model achieved an accuracy of approximately 67.35%. The model can be accessed at Hugging Face.

The pretrained orginal ViT, fine-tuned on ImageNet-1K, was initially trained on ImageNet-21k. It operates with a patch resolution of 16x16 and utilizes a [CLS] token and absolute position embeddings. This model is available at Hugging Face..

We explore various ViT configurations as the prior models, including models trained with different approaches. These configurations include weights trained using the DeIT training recipe, as well as models with the original frozen SWAG trunk weights combined with a linear classifier. The models are avaible in PyTorch..

SECTION: Full Evaluation Results on CIFAR100
Full evaluation results on CIFAR100 are provided in Tab..

SECTION: Full Evaluation Results on IMAGENET-1k
Here we provide addition evaluation results on IMAGENET-1k with different prior models including other ViTs including DeiT and ViT-swag, and the CNN models including ResNet and RegNet in Fig.

SECTION: Full Evaluation Results on CIFAR10
Comprehensive results of our evaluation on CIFAR10 are provided in Tab.and Tab.. These results represent the average performance across seven OOD datasets:,,,,,, and. For a scale factor of, the detailed results are provided in Table. In contrast, Tablepresents the results for a smaller scale factor of.

SECTION: Additional Visualization of the Attention Maps
In addition to visualization of the attention maps presented in the paper, we provide further visualizations to compare different scaling factorsas shown in Fig.and Fig..