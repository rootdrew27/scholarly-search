SECTION: Neural Transcoding Vision Transformers for EEG-to-fMRI Synthesis
This paper introduces the Neural Transcoding Vision Transformer (NT-ViT), a generative model designed to estimate high-resolution functional Magnetic Resonance Imaging (fMRI) samples from simultaneous Electroencephalography (EEG) data.
A key feature of NT-ViT is its Domain Matching (DM) sub-module which effectively aligns the latent EEG representations with those of fMRI volumes, enhancing the model’s accuracy and reliability.
Unlike previous methods that tend to struggle with fidelity and reproducibility of images, NT-ViT addresses these challenges by ensuring methodological integrity and higher-quality reconstructions which we showcase through extensive evaluation on two benchmark datasets; NT-ViT outperforms the current state-of-the-art by a significant margin in both cases, e.g. achieving areduction in RMSE and aincrease in SSIM on the Oddball dataset.
An ablation study also provides insights into the contribution of each component to the model’s overall effectiveness.
This development is critical in offering a new approach to lessen the time and financial constraints typically linked with high-resolution brain imaging, thereby aiding in the swift and precise diagnosis of neurological disorders.
Although it is not a replacement for actual fMRI but rather a step towards making such imaging more accessible, we believe that it represents a pivotal advancement in clinical practice and neuroscience research.
Code is available at.

SECTION: Introduction
The field of Neural Transcoding (NT) focuses on translating neural signals from one form to another, a process that holds immense potential in neuroscientific research.
This capability opens up new possibilities for enhanced analysis of neural signals, helping doctors in the diagnosis of neurological disorders and a deeper understanding of the complex interactions within the brain.
The modalities used in this work are Electroencephalography (EEG) and blood-oxygen-level-dependent functional Magnetic Resonance Imaging (fMRI), each offering unique insights into brain activity.
EEG captures the brain’s electrical activity through local field potentials and excels in temporal resolution.
Conversely, fMRI offers spatially precise mapping of brain activity through neurovascular coupling mechanisms, complementing EEG’s temporal resolution.

The integration of these modalities poses a unique challenge, necessitating advanced computational approaches.
Here, generative Artificial Neural Networks, usually referred to as generative ANNs emerge as powerful tools.
These networks, through learning and adapting, are capable of generating new, synthetic data that can mimic real-world distributions.
Many methods excel in generating data like imagesand audio.
The current key player in this area is the Transformer architecture.
Originally developed for Natural Language Processing, Transformers have shown remarkable adaptability and success in various domains, including image generationand audio synthesis.
Their ability to handle sequential datamakes them particularly suited for NT tasks, where capturing temporal dynamics is crucial.

Traditional methods struggled with image fidelity and reproducibility, often not fully utilizing the data’s unique characteristics and the latest ANN advancements.
Our study addresses these gaps, developing a Transformer-based model that efficiently generates fMRI volumes from simultaneous EEG signals. That is, we introduce a novel method for EEG-to-fMRI NT, which we denote as Neural Transcoding Vision Transformer (NT-ViT).
This model leverages a Vision Transformer (ViT) architectureand incorporates a unique sub-module, termed Domain Matching (DM), to enhance its transcoding capabilities.
While our research builds on, we have chosen a Transformer-based architecture instead of a Convolutional Neural Network (CNN), enabling better capture of long-range dependencies and global context.
We also use a distinct optimization strategy and an autoencoder approach, preserving more original information in latent representations.

We conducted a comprehensive analysis of NT-ViT’s performance.
Our assessment involved extensive testing on two widely recognized datasets: NODDIand Oddball.
The results from these evaluations indicate that NT-ViT achieves state-of-the-art performance in EEG-to-fMRI transcoding: it consistently surpasses the current state of the art on the two benchmark scaled datasets by respectivelyRoot Mean Square Error (RMSE) points, andStructural Similarity Index (SSIM) points.
Thus, we argue that this paper significantly advances the state of the research about NT models, providing key insights for future brain function studies.

To sum up, the contributions of this paper are twofold.
Firstly, we introduce a novel method for EEG-to-fMRI NT, which we denote as Neural Transcoding Vision Transformer.
This model incorporates a unique sub-module, termed Domain Matching, to enhance its transcoding capabilities.
Secondly, we conduct a comprehensive analysis of NT-ViT’s performance.
Our novel approach, while deeply rooted in well-established Image Generation models, provides a verifiable and reproducible benchmark renewing the state-of-the-art in this burgeoning field.

NT-ViT stands out for its accuracy, simplicity, robustness, and even adaptability to other neural modalities like Computed Tomography (CT) or Positron Emission Tomography (PET) scans with minor hyperparameter adjustments.
However, it is crucial to note that these synthesized outputs are not replacements for actual fMRI scans; they are interpretations of EEG data, limited by input quality and current EEG-fMRI relationship understanding.
While providing valuable insights, these models cannot yet replicate the full complexity of real fMRI scans.

SECTION: Related work
Although NT is at its early stages, there have recently been significant strides made.
Research detailed insuccessfully facilitated the generation of CT images, yielding intricate details of bone structures, vascular systems, and soft tissue, from fMRI volumes.
This was done through the application of a Fully Convolutional Network trained in an adversarial fashion.
In a parallel vein,leveraged similar neural architectures to do the inverse operation, effectively narrowing the divide between these divergent imaging techniques.

On the other hand, the synthesis of fMRI data from EEG signals lacks depth in terms of generative methodology, largely due to the scarcity of simultaneous EEG-fMRI datasets and the inherent challenge of mapping such disparate modalities.
However, EEG-to-fMRI NT is being well-researched in terms of correlation.
The method proposed inpioneered the field and gave it the name of Neural Transduction.
The approach leverages twin Convolutional Neural Networks to navigate the complex EEG-fMRI relationship without pre-calibrated physiological models.
The study inextends this by comparing various generative architectures including AutoEncoders, Generative Adversarial Networks (GAN), and Wasserstein GANs, for mapping functions between EEG and fMRI’s diverse temporal and spatial dynamics.
The approach is further refined inby employing topographical structures and neural processing strategies with Fourier features and attention mechanisms.
However, the potential for broader scientific verification and reproducibility is constrained by the absence of consistent, reproducible cross-validation techniques and open-source code or model checkpoints.
Moreover, although being recent works, they do not make use of best-performing generative architectures such as ViTsor Diffusion Models, which have shown astonishing performances in the Image Generation and Reconstruction fields and could naturally be adapted to physiological signals as well.

This research aims to advance generative models for EEG-to-fMRI synthesis, not as a substitute for fMRI, but as a foundational step towards more nuanced future models.
It also harmonizes with established practices in Image Generation, utilizing universally recognized metrics not yet standard in the EEG-to-fMRI NT domain.

SECTION: Method
This section introduces NT-ViT, an ANN for EEG-to-fMRI, developed for spearheading innovations in NT.
NT-ViT comprises two key sub-modules: the Generator and the Domain Matching module.
The Generator is in charge of converting EEG waveforms into fMRI volumes, while the DM, operational only during training, enhances the Generator’s reconstruction capabilities.provides an overarching view of the model along with specific insights into the encoding and decoding processes, shared by both sub-modules.

Both sub-modules are built upon encoder-decoder architectures that leverage ViTs, following a methodology inspired by ViTGAN.
The encoders are noted asand are capable of processing arbitrarily shaped volumes, like spectrograms or fMRI volumes.
Their structure is shown in.
They start by segmenting the 3D input volume into non-overlapping patches of size, wheredenotes the number of input channels andis the adjustable patch size.
The number of input channels is equivalent to the Mel filter banks in a spectrogram or the depth of an fMRI volume.
These patches are flattened and converted into-dimensional vectors, referred to as tokens, via a Multilayer Perceptron (MLP) with two hidden layers that is called Patch Embedder.
These tokens are then processed by a Transformer, producing an output sequence of tokens identical in dimensions to the input.
A special learnable token called ais then appended at the end of the sequence, providing a condensed latent representation of the encoder’s input after integration with learnable positional embeddings.
In contrast, the decoders, noted as, operate on a single token.
Their architecture is depicted in.
Its core component, another Transformer, processes a sequence composed of positional embeddings conditioned (summed) by a latent representation token output to the Encoder.
This Transformer’s output sequence is transformed by an MLP again called Patch Embedder, and reassembled into a sequence of patches which are then combined into a single volume.
This volume is refined and smoothed through a 3D convolution layer, after which a Sigmoid activation layer is applied to keep values in.
These modules are versatile, and capable of processing 3D inputs of various shapes. The next subsections will explore their specialized applications within NT-ViT.

SECTION: Generator
The Generator module inputs an EEG waveformcontainingsamples recorded overelectrodes/channels.
A first preprocessing step convertsinto a Mel spectrogram, aligning with human auditory perception and enhancing feature extraction for improved fMRI reconstruction.
The conversion to Mel spectrogram is a popular feature extraction technique when dealing with audiosas well as in EEG-to-fMRI NT.
This process employsMel filters on the power spectrum from Short Time Fourier Transform (STFT) with a stride of, resulting in a Mel spectrogram of dimensions.
The EEG Mel spectrogram is processed by the encoder, generating a latent representation, which is then utilized by the decoderto produce the corresponding fMRI volume.

SECTION: Domain Matching
The DM module, which shares an architectural resemblance to the Generator, was designed to enable the encoders to learn a unified embedding for EEG and fMRI signals, enhancing model performance.
In particular, the DM takes a ground truth fMRI volumeas input and encodes it withinto a latent representation, which is then reconstructed back into an fMRI volume by the decoder.

SECTION: Losses
The model is trained end-to-end using minibatches of lengthof tuples, in whichrepresents an EEG record andthe simultaneous fMRI volume.
The loss() of the model is obtained as the sum of two groups of sub-losses.
The initial set pertains exclusively to the Generator module, encompassing the reconstruction loss() which quantifies the fidelity between the actual fMRI volume and the one synthesized from the EEG data, thereby imposing a penalty for voxelwise discrepancies:

Additionally,() employs the pointwise Kullback-Leibler divergence to assess the deviation of the fMRI volume distribution generated from the EEG from the actual fMRI volume distribution, specifically targeting the fidelity of spatial activation patterns that are crucial for accurate brain mapping:

whereis a shorthand notation for thefunction.

The second set of sub-losses is associated with both the Generator and the DM modules.
Here,() is a reconstruction loss between the true fMRI volumes and those reconstructed by the DM module, which guides the autoencoder in learning a salient representation of the input:

The sub-loss() encourages the congruence of the latent representations derived from the EEG by the Generator with those obtained by the DM:

The final loss() is computed as the sum of these individual components:

Omittingfrom the final loss would result in the model exhibiting comparable performance to scenarios devoid of the DM component, as it serves as the primary mechanism for the alignment of latent embeddings.

SECTION: Experiments
SECTION: Experimental design
Given the novelty of the task, which limits the availability of datasets with paired EEG-fMRI samples, we employed two datasets, herein referred to as NODDI and Oddball, for our experimental analyses.

Thedatasetincludes simultaneous recordings of EEG and fMRI obtained from 17 individuals during a period of rest, with the participants fixating on a white cross presented on a screen. Due to issues with data integrity, the analysis could only include the recordings from 15 out of the original 17 participants, resulting in a total of 4,500 paired EEG-fMRI samples. The EEG recordings were captured at a sampling rate ofusing a 64-channel MRI-compatible EEG cap. These were later aligned with fMRI samples corresponding toEEG windows. The fMRI images were acquired using a Siemens Avanto 1.5 T scanner, utilizing a T2*-weighted gradient-echo EPI sequence. For each subject, 300 fMRI scans were taken, with every scan having a shape ofvoxels.

The second collection, known as thedataset, encompasses 17,340 paired EEG-fMRI data points gathered from 17 participants. This dataset differs from the NODDI collection in that the individuals experienced various auditory and visual stimuli, such as sound clips and geometric shapes on a display. The EEG data was recorded at a frequency ofusing a 34-electrode cap and each EEG sequence ofwas time-locked with the corresponding fMRI data point. The fMRI imaging was conducted on a Philips Achieva 3T clinical scanner using a TE EPI sequence. Contributions from each participant consisted of 1020 fMRI scans, with each encompassingvoxels.

As delineated in, the neuronal activity registered by EEG is observed in the fMRI signal with an estimated latency ranging fromto. Accordingly, fMRI recordings are systematically shifted byto synchronize with the EEG data.
Additionally, considering that the fMRI volume values are positive and can reach magnitudes of several thousand, we normalize each volume to fall within the range of.

This section is dedicated to delineating the metrics that will be employed to compare our work with the current state-of-the-art, followed by an exposition of the validation schemes utilized in this comparative analysis.

This study employs a comprehensive suite of metrics, which are well-established in the existing literature, supplemented by one adopted from parallel reconstruction tasks.
The method inmakes use of two metrics.
The first one is the Root Mean Square Error (RMSE), which is particularly sensitive to pronounced errors in voxel values.
The second one is the Structural Similarity Index (SSIM), which gauges the fidelity of volume reconstruction in terms of structural accuracy, luminosity, and textural integrity; those attributes are central to representing the intricate patterns within an fMRI volume accurately.
Our proposed work integrates the Peak Signal-to-Noise Ratio (PSNR), a metric conventionally leveraged in image reconstructionand super-resolution tasks, to appraise the clarity of the reconstructed volumes by measuring the relative strength of the signal against the noise:

whereandrespectively are the ground truth and reconstructed tensors, andis the function that returns the maximum value of a tensor.
PSNR, alongside SSIM, provides a differentiated understanding of the impacts that image quality impairments have on the reconstructed output, each metric attuned to varying degradation types.
Notwithstanding the efficacy of these quantitative metrics, the nuanced complexity of fMRI data calls for critical qualitative evaluation experts, whose discerning insights are crucial for a holistic assessment, highlighting the indispensable nature of expert visual inspection alongside computational evaluation.

Due to the benchmark datasets’ lacking predefined training and testing splits, a robust cross-validation method is essential. Our methodology substantially improves upon previous validation strategies, such as those in, which rely on a fixed train/test split and may bias results due to limited subject testing. Furthermore, the approach in, which does not segment data by subject, risks training models on specific brain shapes, limiting generalizability. To address these issues, we also adopt the Leave-One-Subject-Out (LOSO) cross-validation scheme, dividing the dataset based onsubjects intosubsets forassessments. This ensures each subset, containing data from one subject, is used once as a test set, with the others for training. Averaging the outcomes of these assessments provides a more trustworthy and generalizable performance measure across subjects.

In this manuscript, fMRI volumes are graphically plotted using two distinct methods: Point Cloud (PC) and Maximum Intensity Projection (MIP).
The PC approach transforms volumetric fMRI data into a 3D point set, with each point representing a voxel’s location and value, offering detailed spatial visualization of brain activity.
Conversely, MIP projects the highest intensity voxels onto a 2D plane, condensing the data into an image that accentuates the brain’s most active regions, thereby facilitating a focused analysis of key areas.
Although MIP is the most used way to plot fMRI volumes in this domain, we find it visually useful to have a 3D view of the data.

SECTION: Implementation details
The selection of hyperparameters is guided by the insights from our ablation study, detailed in.
Training of the models is done over 30 epochs, utilizing the AdamW optimizer.
The optimizer’s configuration includes a learning rate of,,, accompanied by a weight decay of.
A batch size of 128 is adopted, with both dropout and noise injection mechanisms deactivated.
Concerning the input data, EEG patch dimensions are established at, and for fMRI patches, dimensions are set at.
Throughout the network architecture, the SELU activation functionis applied.
The Transformer’s hidden dimension (-dim) is configured to, and each one compriseslayers.
Layer Normalizationis systematically applied after each linear layer in both MLPs and Transformers.
The stride of the STFT windowhas been set to, meaning that the length of each Mel spectrogram is half the one of the input waveform.
The experiments have been run on a machine with an NVIDIA RTX4090 GPU.

SECTION: Results
This section presents an analysis of NT-ViT’s performance when benchmarked on the NODDI and Oddball datasets.
The model’s performance is compared with the four models from, and its reimplementation of the approach proposed in.For a more comprehensive evaluation, two variations of our model are considered: the full version described in, and the one without the DM.

There is a critical aspect to note regarding the related works referenced in the comparative study: the validation approach utilized indoes not employ a cross-validation scheme.
Instead, the dataset is partitioned into training and test sets, with results reported on this split.
In particular, the training sets inconsist of data from 2 and 4 individuals, respectively for the two datasets, with the remaining data allocated to the test set.
Conversely, for, the test sets for both datasets are composed of records from 2 individuals.
Since in both works the IDs of the subjects used in training and test sets are not specified, we randomly sampled subjects with ID #39 and #44 to compose the test set of the NODDI dataset, and subjects with ID #8, #5, #14, #16 for the one of the Oddball dataset.
To ensure a more unbiased evaluation, we also decided to perform a set of experiments with the LOSO scheme.

details the benchmark results on the NODDI and Oddball datasets.
For NODDI, NT-ViT improves the RMSE by 0.32 (5 times) compared to TAFP and enhances the SSIM by 0.109 (1.23 times) relatives to TALP, the two leading models in this category.
On the Oddball dataset, NT-ViT achieves an RMSE improvement of 0.63 (10 times) and an SSIM enhancement of 0.427 (3.14 times) compared to TAFP, the highest-performing model in this metric.
Note that the combination of better-performing single parameters does not guarantee the best results overall; in fact, some configurations in the ablation study exceed these numbers obtained by NT-ViT w/o DM (refer to) due to the greater search space.
Overall, the results in both datasets firmly establish NT-ViT as a major development in the field, setting new standards for accuracy and performance in EEG-to-fMRI neural transcoding.

Illustrative examples of reconstructionsproduced by various configurations of our model are depicted in.
These samples are taken at random between the pool of the first subjects of each dataset, ensuring a fair analysis.
An observable characteristic of the model is its capability to identify regions of mass concentration.
For example, in both samples of the NODDI dataset, the model managed to identify the areas on the lower left side of the image, while in the sample of the first subject of Oddball, the model recognized the V-shape in the middle.
However, it is noteworthy that the intensity of these reconstructions is somewhat lower than the actual ground truth due to indecision in the model.
Intriguingly, the model exhibits a nascent ability to discern the contours of the brain and skull, and the position of the eyes, a feature that becomes more pronounced when the DM module is activated.
This is particularly remarkable considering that the model has not been exposed to these specific brain images during training.

SECTION: Ablation study
This section delineates the ablation study executed on the more extensive and dynamic Oddball dataset.presents a comprehensive summary of the results, juxtaposing a baseline model against a series of modified iterations.
We have been guided by established practices in related literature on ViTs, focusing on combinations we found most compelling.
Each variant is distinct from the baseline in only a single parameter, thereby offering lucid insights into the effect of each modification.
Concerning parameters tested in a single direction, preliminary tests indicated that exceeding the upper bounds inhibited the network’s learning capabilities. Therefore, we set these upper bounds as baselines.
The metrics employed to assess the quality of the reconstructions include the PSNR and the SSIM.
These are presented alongside the model’s parameter count, denoted in millions, and the Multiply-Accumulate Operations (MACs), quantified in billions.
The latter is used to gauge the computational efficiency of each model variant, providing an understanding of the trade-off between reconstruction quality and computational demand.
The subsequent sections provide a detailed exposition of the ablation study, encompassing both the specifics of each variant and the parameters characterizing the baseline model.

In the initial configuration, the baseline integrates solely the Generator, which is trained to minimizewhile deliberately omitting. To evaluate the implications of incorporatingand the DM, we conducted tests on three distinct variations: one withadded to the baseline, another integrating the DM, and a third variant combining both the DM and.

The inclusion of the regularization loss in the Generator’s training process appeared to marginally impede performance, resulting in a reduction of 0.08 PSNR points. Conversely, the application of the DM in both its standalone and combined forms withyielded notable improvements. Specifically, the variant employing both DM andexhibited an increase of 0.2 PSNR points and achieved the highest SSIM across all tested configurations.
However, this enhancement in performance was accompanied by a substantial increase in computational complexity, nearly tripling the MACs and augmenting the model’s parameter count by one and a half times.
This increment notably extended the time required for both the training and testing phases of the model.

The baseline model employs the AdamW optimizer, configured with a learning rate of,,, and a weight decay parameter of. In light of the limited size of the dataset, a dropoutrate ofis implemented following each linear layer in both the Transformer and the MLP components of the baseline model. Additionally, Gaussian noise is directly injected into each EEG Mel spectrogram during generation, following the equation:

whererepresents the input tensor,denotes the standard deviation of the noise, set atfor the baseline, andis a Gaussian noise tensor with a mean of zero and an identity covariance matrix, congruent in shape with.
Subsequent experiments investigated the impact of lower learning rates and varying weight decay values, particularly under conditions of reduced or eliminated dropout and noise levels.

Notably, a decrease in learning rate toled to a marginal reduction in performance, with a maximum loss ofPSNR, corroborating the theory that a larger learning rate enhances generalizability in datasets of limited size.
Regarding weight decay, while higher values are typically thought to make the model generalize better, in our study, weight decays ofandproduced comparable outcomes.
However, lower or zero weight decay values notably improved performance, with a peak gain ofPSNR achieved at a weight decay of, the lowest value tested, aligning with findings from.
Contrary to initial expectations, the model demonstrated optimal performance when both dropout and noise levels were minimized or eliminated.
Eliminating dropout resulted in a performance gain ofPSNR while removing noise contributed to aPSNR increase. These observations are consistent with studies highlighting the need for more sophisticated approaches than dropout for vision modelsand the potential counterproductivity of noise injection, especially at the patch level.

In our baseline model, both the Encoder and Decoder utilize a patch size of.
Prior researchsuggests that smaller patch sizes can potentially improve output accuracy, albeit at the cost of increased computational demand due to the higher number of tokens processed.
Thus, we conducted some experiments to assess the impact of using smaller patch sizes on EEG Mel spectrograms and fMRI volumes.

Our findings for EEG data indicated that the baseline patch size ofdelivered the most favorable results.
In contrast, reducing the patch size todiminished the performance more significantly, evidenced by a 0.35 decrease in PSNR.
For fMRI data, however, a patch size ofproved to be optimal, enhancing the PSNR by 0.21.
Consequently, we decided to retain thepatch size for EEG data and adopt thesize for fMRI data.
This decision is not only corroborated by our empirical findings but also confers the advantage of reducing the computational complexity of the model.
It is noteworthy that employingpatches for the fMRI data leads to a fourfold increase in the model’s MACs.

In our baseline model, we use the ReLU activation function, known for its computational efficiency but prone to the "dying ReLU" problem, where neurons output zero regardless of the input. While layer normalization mitigates this by adjusting input distributions, it does not resolve ReLU’s zero-gradient issue for negative inputs. Consequently, we investigated GELUand SELUas alternatives. GELU is preferred in Natural Language Processing for its smooth gradients, and SELU offers self-normalizing properties, advantageous for Neural Transcoding tasks given the signal characteristics.

Experimental results revealed that the implementation of GELU resulted in a marginal decline in performance, with a 0.3 decrease in PSNR.
Conversely, the adoption of SELU led to an improvement, increasing PSNR by 0.34 points.
Consequently, we integrated SELU as the activation function in our model.

The concluding series of ablation experiments focused on altering the layer count and the-dim within the Transformers in both Encoders and Decoders.
The number of layers in the Transformer directly influences the computational cost by altering the network’s depth.
Meanwhile, the hidden size affects both the capacity for information storage within the embeddings and the overall computational demands.
In the baseline configuration, each Transformer is equipped with a single layer and an-dim of 256.

The experimental results indicated that variations in-dim produced outcomes closely aligned with the baseline performance.
However, an increase in the number of layers demonstrated a notable enhancement, with the peak improvement being a 0.31 increase in PSNR when utilizing three layers.
Based on these findings, we decided to maintain the-dim at 256 while incorporating three layers in each Transformer module.

SECTION: Conclusion and future work
In this paper, we introduced NT-ViT, a Vision Transformer adept at predicting fMRI volumes based on concurrent EEG waveforms with remarkable accuracy.
The efficacy of NT-ViT is substantially attributed to its Domain Matching module, which harmonizes latent EEG features with their fMRI counterparts to bolster generalized performance.
Despite its capabilities, NT-ViT should be viewed not as a substitute for direct fMRI imaging but as an important stride towards enhancing model accuracy in this domain.
Although the scope for refining such models is bounded by the availability of data, NT-ViT has set a new benchmark as a compelling model for this specific application.

In the future, the potential applications of NT-ViT extend to various signal transductions, including but not limited to fMRI-to-EEG and CT-to-fMRI conversions.
We plan to further augment the model’s components using extensive unsupervised datasets, leveraging transfer learning to facilitate and improve upon subsequent tasks.
It is our aspiration that NT-ViT will galvanize additional research into Neural Transcoding, with the ultimate goal of refining these models to serve as reliable instruments for medical professionals.

SECTION: Acknowledgements
This work was supported by the PNRR project FAIR - Future AI Research (PE00000013) under the NRRP MUR program funded by NextGenerationEU.
Romeo Lanzino conducted this research during his stay at KTH, which was funded by a scholarship for PhD. students’ mobility from Sapienza University

SECTION: References