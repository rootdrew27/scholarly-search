SECTION: AI-Driven Day-to-Day Route Choice

Understanding travelers’ route choices can help policymakers devise optimal operational and planning strategies for both normal and abnormal circumstances. However, existing choice modeling methods often rely on predefined assumptions and struggle to capture the dynamic and adaptive nature of travel behavior. Recently, Large Language Models (LLMs) have emerged as a promising alternative, demonstrating remarkable ability to replicate human-like behaviors across various fields. Despite this potential, their capacity to accurately simulate human route choice behavior in transportation contexts remains doubtful. To satisfy this curiosity, this paper investigates the potential of LLMs for route choice modeling by introducing an LLM-empowered agent, "LLMTraveler." This agent integrates an LLM as its core, equipped with a memory system that learns from past experiences and makes decisions by balancing retrieved data and personality traits. The study systematically evaluates the LLMTraveler’s ability to replicate human-like decision-making through two stages: (1) analyzing its route-switching behavior in single origin-destination (OD) pair congestion game scenarios, where it demonstrates patterns align with laboratory data but are not fully explained by traditional models, and (2) testing its capacity to model day-to-day (DTD) adaptive learning behaviors on the Ortuzar and Willumsen (OW) network, producing results comparable to Multinomial Logit (MNL) and Reinforcement Learning (RL) models. Additionally, the study assesses lightweight, open-source LLMs, highlighting their effectiveness in route choice simulation and their potential as cost-effective alternatives to more advanced closed-source models. These experiments demonstrate that the framework can partially replicate human-like decision-making in route choice while providing natural language explanations for its decisions. This capability offers valuable insights for transportation policymaking, such as simulating traveler responses to new policies or changes in the network. The code for this paper is open-source and available at:https://github.com/georgewanglz2019/LLMTraveler.

KeywordsRoute ChoiceLarge Language ModelsCongestion GameGenerative Agent

SECTION: 1Introduction

Understanding individual travel behaviors is critical for developing efficient and sustainable transportation systems. Travel behavioral analysis aims to capture the decision-making process of individual travel execution, including travel route choice, travel mode choice, departure time choice, and trip purpose. Among these choices, modeling route choice not only helps analyze and understand travelers’ behaviors, but also constitutes the essential part of traffic assignment methods[1]. Specifically, it enables the evaluation of travelers’ perceptions of route characteristics, the forecasting of behavior in hypothetical scenarios, the prediction of future traffic dynamics on transportation networks, and the understanding of travelers’ responses to travel information.

Real-world route choice is complex because of the inherent difficulties in accurately representing human behavior, travelers’ limited knowledge of network composition, uncertainties in perceptions of route characteristics, and the lack of precise information about travelers’ preferences[1]. To overcome these limitations, DTD traffic dynamics have attracted significant attention since they focus on drivers’ dynamic shifts in route choices and the evolution of traffic flow over time, rather than merely static equilibrium states. DTD models are flexible to incorporate diverse behavioral rules such as forecasting[2,3], bounded rationality[4,5], decision-making based on prospects[6,7], marginal utility effects[8,9], and social interactions[10]. Despite these advantages identified in[11]and[12], DTD models still struggle to accurately reflect the observed fluctuations in traffic dynamics, particularly the persistent deviations around User Equilibrium (UE) noted in empirical studies[13,14,15].

To better understand traffic dynamics, Agent-Based Modeling (ABM) offers a promising alternative. It excels at simulating individual behaviors, revealing complex patterns from local interactions and decisions of individual agents[16]. The ABM was first introduced into the route choice model for the study of the impact of information from different sources on travelers and to model their responses and learning behaviors[17,18,19,20]. In recent years, many studies have also explored the use of model-free RL-based methods to model the learning behaviors of selfish agents in congestion games[21,22,23,24]. While these advances have contributed to the development of route choice modeling, both ABM and RL-based methods face limitations in modeling the dynamic and adaptive nature of traveler behavior. ABM relies on predefined decision rules that are typically static, making it challenging to reflect evolving decision-making processes. RL, while capable of learning strategies from interactions, depends heavily on training within fixed distributions, limiting its ability to generalize to new or unforeseen conditions. As a result, these models often struggle to adapt to dynamic and unexpected scenarios without retraining or manual adjustments.

Recently, the appearance of LLMs, such as Generative Pre-Trained Transformers 4 (GPT-4)[25], marks a significant milestone in machine learning, showing great potential in natural language processing and text generation. Trained on extensive datasets, LLMs can reflect a broad spectrum of human behaviors, providing profound insights into complex decision-making processes. Notably,[26]shows that GPT-4 has passed a Turing test, demonstrating behavior and personality traits that are statistically indistinguishable from a diverse human sample spanning over 50 countries. The application of LLMs in simulating human behavior has seen rapid progress, including various fields such as social science, gaming, psychology, economics, and policy-making[27,28,29]. These studies reveal highly human-like behaviors and display complex phenomena similar to real-world scenarios within various systems.

A key advantage of LLMs is their adaptability. Unlike traditional models that rely on static assumptions and predefined rules, LLMs can well comprehend human common sense and they are few-shot learners that can efficiently adapt to new information with instructions or limited examples[30,31]. This high level of adaptability allows LLMs to respond dynamically to changes in the environment and adjust their behavior accordingly[29]. Integrating LLMs into route choice modeling can effectively address the critical challenges faced by traditional methods. Moreover, LLMs offer enhanced explainability by generating natural language explanations for their decisions, providing transparency into their decision-making process.

Although LLMs have been widely applied in various research fields, their ability to accurately simulate human route choice behavior in transportation environments remains doubtful. LLMs are primarily designed to predict the next token in a sequence, which raises the challenge of how to effectively design prompts that allow the model to understand and simulate the decision-making process in real-world route choice scenarios. Moreover, human travelers in urban networks learn and adapt their behavior over time, based on past experiences. This dynamic, evolving nature of decision-making presents an additional challenge in integrating LLMs to replicate such adaptive behaviors. In addition, while powerful closed-source LLMs such as GPT-4 offer substantial capabilities, their high costs can be prohibitive. This raises the question of whether lightweight, open-source LLMs can be effectively utilized for route choice modeling. Exploring this possibility could provide a cost-effective and accessible alternative, broadening the applicability of LLMs in transportation research and practice. This paper aims to address these challenges.

To best of our knowledge, this study is the first to apply LLMs to model and interpret travelers’ route choice behaviors in urban mobility systems. The paper makes the following contributions:

Introducing "LLMTraveler," an LLM-empowered agent for route choice modeling, which integrates advanced prompt engineering and a memory system to simulate human travel decisions, setting a new standard for using LLMs in travel behavior simulation.

Evaluating the performance of the proposed LLMTraveler across single OD and multi-OD pair scenarios, comparing its behavior with laboratory data, traditional theoretical models, and reinforcement learning-based approaches.

Assessing the feasibility of using lightweight, open-source LLMs for route choice simulation, demonstrating their potential as cost-effective alternatives to more advanced closed-source models.

These contributions offer new insights into travel behavior and provide transportation policymakers with a novel tool to design effective traffic management strategies and predict user responses. For instance, in the absence of historical data, governments could use LLM-powered agents to simulate the impact of new policies or changes to the transportation network. Furthermore, the development of lightweight, open-source LLM solutions enhances the scalability of such models, making advanced techniques more accessible and applicable across various urban contexts.

The remainder of this paper is organized as follows. Section2presents the methodology behind the proposed LLMTraveler agent. Section3and4evaluate the agent’s performance in single and multi-OD network scenarios, respectively, comparing its behavior with laboratory data and traditional models. Finally, Section5concludes the paper, summarizing the key findings and suggesting directions for future research.

SECTION: 2Method

SECTION: 2.1Preliminaries

LLMs are typically built on the Transformer[32]architecture, which employs self-attention mechanisms to effectively model long-range dependencies in text. Modern LLMs contain hundreds of millions or even billions of parameters, significantly enhancing their performance across a wide range of tasks, from natural language understanding to text generation.

A notable advancement in LLM capabilities is the concept of in-context learning (ICL), formally introduced by GPT-3[30]. With ICL, a language model can generate expected outputs based on natural language instructions and task demonstrations provided within the prompt, without requiring additional training or parameter updates. This ability enables LLMs particularly useful for simulating human route choice decisions, as a well-designed prompt can guide the model to generate accurate and relevant answers based on the given information:

whereseqis the output sequence.is the output sequence with the highest probability.are the optimized model parameters obtained through pre-training.Contextrepresents all natural language instructions and/or several task demonstrations used to guide the LLM in generating the desired output sequence.
Based on the ICL, LLMs can be used as an agent’s brain to simulate traveler’s route choice behavior by designing effective prompts.

SECTION: 2.2LLM-empowered traveler agent

In this subsection, the proposed method, LLMTraveler, an LLM-empowered intelligent agent for travel behavior modeling, is presented. Figure1shows its framework. First, the decision-making process of the LLMTraveler is discussed, followed by a detailed explanation of its prompt design.

In the proposed framework, the decision process of each LLMTraveler is designed to mimic human-like decision-making in route choices. This involves the following three steps:

Step 1: Reception and update of travel cost information

At the end of day, each LLMTraveler receives travel cost information for the chosen route and all available routes. The agent updates its memory with this information, storing travel cost data along with other relevant details such as traffic conditions and route-specific costs. This comprehensive data collection helps in building a robust memory of travel experiences.

Step 2: Reflection on route choices

The agent reflects on its past route choices based on the accumulated experience. For the experiments, this reflection involves calculating the Exponential Weighted Moving Average Travel Time (EWMATT) for each route to prioritize recent experiences while still considering historically experienced data. The formula used is:

whereis the EWMATT after the-th update,is the smoothing factor,is the-th observed travel time, andis the EWMATT after the-th update.

This reflection can also be implemented through other methods, such as having the LLM perform self-refinement[33].

Step 3: Decision making using LLMs

The LLMTraveler retrieves data from memory (e.g., EWMATT and the chosen times of each route) and uses this information to construct a prompt. This prompt is then fed into the LLM, which processes it and generates a route choice decision for day. Further details about the prompt are provided in the following section.

The context-inclusive prompts, which integrate the traveler profile, task description, travel experiences, thinking guidance, output format, and reasoning, are shown in Figure2. This comprehensive approach aims to enhance the LLMTraveler’s capabilities by building upon existing prompting strategies. This section details the prompt design for the LLMTraveler, focusing on the key components necessary for accurate and realistic route choice modeling.

Traveler Profile.
Individual attributes significantly influence travel behavior[34,35]. The prompt design includes a detailed profile of the traveler, such as a randomly generated name, personality traits (e.g., extroverted, agreeable, conscientious, neurotic, and open to experience), and whether the agent is selfish or not. This profile helps the LLMs understand the decision-making context from the traveler’s perspective, ensuring that the agent’s decisions are personalized and contextually relevant.

Task description.
The task is to simulate route choice behavior. Each day, the agent needs to decide on a route based on historical travel experiences. The task description specifies the daily goal and outlines the available route options and relevant travel data. This setup provides the LLMs with a clear understanding of the agent’s objectives and constraints.

Travel experiences.
Travel behavior is largely influenced by historical experiences[36]. The prompt includes the retrieved data, such as the chosen times and EWMATT of each route. This module allows the LLMs to incorporate past experiences into the decision-making process, ensuring that decisions are informed by a comprehensive historical context.

Thinking guidance.
To guide the LLMs’ reasoning, the prompt includes a guidance section instructing the agent to "think step-by-step" (zero-shot chain-of-thought strategy[37,38], optimizing route choice by "considering both well-traveled routes and less explored options." This guidance leverages domain knowledge and common sense to enhance the LLMs’ reasoning capabilities, helping them to balance various factors effectively.

Output format.
The prompt format is designed to ensure clarity and ease of interpretation for the LLM. It specifies that the agent’s response should be in JSON format, including both the selected route and the reasoning behind the choice. This structured format enables efficient processing and simplifies the analysis of the LLMs’ output, ensuring the results are easily extractable and interpretable.

Interpretation and output.
The output from the LLMs includes two parts: the route choice and the reasoning behind the choice. By first asking for the reason and then the decision, the prompt not only improves the interpretability of the model but also enhances its reasoning ability. This approach encourages the LLMs to thoroughly process the information before making decisions, which improves the model’s reasoning performance and provides deeper insights into the decision-making process[39].

SECTION: 2.3DTD Route Choice Modeling Framework

Figure3shows the DTD route choice modeling framework, which is designed to simulate the iterative decision-making process of travelers within a transportation network over multiple days. This iterative approach provides valuable insights into the evolution of route choices over time.

Input.
The input for this framework consists of three main components:

Transportation Network.
This includes nodes, edges, and their connectivity. The network structure defines the possible paths that travelers can take.

Demand (LLMTravelers).
This involves the OD pairs and the number of travelers between these pairs. In the framework, each traveler or group of travelers is considered a LLMTraveler, complete with a specific profile. For each OD pair,alternative routes are computed using the-shortest path algorithm[40], providing multiple options for each LLMTraveler.

Link Performance Function.
This function calculates the travel time for each link based on the traffic volume. It can be linear or non-linear, depending on the specific characteristics of the transportation network. In the experiments, a linear performance function is used. Despite its simplicity, the linear cost function effectively captures the essential relationship between travel time and traffic flow[41]:

whereis the travel cost (e.g., travel time) for linkat day,is the free flow travel cost for link,is the traffic volume of linkat day, andis a constant that scales the impact of the traffic volume.

Simulation Loop.
The simulation loop operates over a predefined number of days, representing the iterative nature of travelers’ route choice decisions.

Route Choice.
For day, each LLMTraveler selects a route from their available alternatives based on their own experiences and information.

Simulation and Perception.
After all LLMTravelers have chosen their routes, the network is loaded with these selected routes. This step involves calculating the travel times for each link using the performance function for the day. The travel cost for all chosen routes, along with other relevant information such as traffic conditions and route-specific costs, is perceived by each LLMTraveler.

Re-planning.
Following the network loading, each LLMTraveler updates their experiences based on the travel cost of the day. This includes storing the travel times and relevant information, which will influence their future route choices. The memory of each agent is updated to reflect the day’s travel data, ensuring continuous learning and adaptation. They will use this updated information to make route decisions for day.

Output.
After sufficient days/iterations or upon meeting certain convergence criteria, the final day’s traffic flow is returned. LLMTravelers, having accumulated extensive travel experiences, will exhibit established route preferences, providing insights into the long-term equilibrium state of the transportation network.

SECTION: 3Evaluation of route choice behaviors in a single OD pair network

This section evaluates the route choice behavior regularities of the proposed LLMTraveler in a single OD pair congestion game. The agent’s behavior patterns are compared with laboratory data and traditional theoretical models.

SECTION: 3.1Experiment Settings

Figure4shows the single OD pair network, which is commonly used in behavior experimental studies[13,14,41,42,43,44]. In these scenarios, a fixed number of travelers (e.g., 16 in this experiment) commute from the same origin to the same destination with two route choices every morning. Travel time on each route was assumed to increase with route flow. This experimental setup closely follows the methodology outlined in[41], ensuring consistency with their approach.

The five scenarios illustrated in Figure4, with detailed cost functions provided in Table1, are designed as follows:

Scenario 1.
This serves as the baseline, featuring a symmetric two-route network.

Scenarios 2–5.
These extend Scenario 1 to asymmetric two-route networks with varying cost functions, designed to investigate travelers’ route choice behaviors under different cost feedback conditions. The cost functions are based on the previous research in[14,41].

Additionally, the LLM temperature is set to 0 by default, with
typical values ranging between 0 and 2. The smoothing factorused to calculate EWMATT was set to 0.2 in the experiments. Each scenario was tested three times in the experiments to ensure robust results.

aandare the cost and flow of route, respectively.bDynamic User Equilibrium (DUE) flow assignment; the unit is "traveler".

SECTION: 3.2Evaluation metrics

The average switching rate is used for evaluation. To understand this metric, we first define the switching rate as follows:

Switching rate[41,45].
This metric measures how travelers switch routes over time. Consider an OD pair with several feasible routes. The switching rateis defined as the proportion of travelers switching from routeto routebetween timeand. For scenarios with only two routes, the proportion of travelers remaining on their previously chosen routeis given by.

Average switching rate[41,45].
To analyze the observed switching rates and identify gaps between the LLMTraveler, existing theories and laboratory observations, the relationship between average switching rates and cost combinations is investigated. Theaverage switching rateis employed as an intuitive indicator, calculated as follows:

whererepresents a specific cost combination, anddenotes the set of days during which the costs of all routes are equal to. The average switching rate serves as a reflection of the population’s average behavior, effectively minimizing the influence of individual heterogeneity and stochastic factors.

SECTION: 3.3Compared Methods

The performance of the proposed method is compared with the following three baseline methods:

Perfectly Rational Choice (PRC) model.
This model follows an intuitive, rational, and individual-level route choice rule. Travelers switch their routes only if there is a less costly alternative unless all travelers were already on the least costly routes the previous day[46]. In the experiments, every agent is assumed to select routes based on the principle of cost minimization, using EWMATT as the cost measure.

MNL model.
This model assumes that travelers select routes based on perceived utility maximization rather than actual utility[47]. In the experiments, the utility is defined as a hyperparametermultiplied by the negative EWMATT. For instance, "MNL-0.3" represents an MNL model withset to 0.3, while "MNL" denotes the default model withset to 1.

LLMTraveler.
This is the proposed method. Table2provides a summary of the LLM models used, including their corresponding codes, LLMs’ model names, parameter counts, and whether they are open or closed-source. In the experiments, the LLMTraveler is named according to the LLM serving as its core. For example, "LLM-gpt35" refers to a LLMTraveler using "GPT-3.5-Turbo-1106" as its LLM core. The selected LLM models cover a range of sizes, from smaller models such as "llama-3.1-8b" to larger models like "GPT-4o". The selection includes both open-source and closed-source models, with parameter counts ranging from 7 billion to over 175 billion.

SECTION: 3.4Experiment Results

Table3presents some examples of the route choice behavior of various LLM-based agents, with decisions influenced by their profiles, historical data, and randomness. For instance, LLM-gpt4o (row 1) demonstrates a preference for randomness in decision-making, selecting "route 1" despite "route 2" has higher historical performance, as shown in its reasoning ("route 2 has a slightly lower…"). On the other hand, LLM-gpt35 (row 2) adopts a more cautious approach, choosing "route 2" due to its lower average travel time, reflecting its risk-averse personality. Similarly, LLM-llama3.1-70b (row 4) balances historical performance with a probabilistic approach, opting for "route 2" but introducing randomness. In contrast, LLM-llama3.1-8b (row 5) factors in minimal differences in travel time and the impact of others’ choices. Furthermore, LLM-yi-medium (row 3), with its risk-neutral and extroverted traits, chooses "route 1" despite "route 2" offering a shorter travel time on average, highlighting its inclination toward balancing the historical data with an element of unpredictability. These varied strategies highlight the agents’ ability to mimic human decision-making, blending rational analysis with uncertainty, making them suitable for simulating route choice in transportation studies.

Table4and Figure5shows the travel time on "route 1" in each scenario for different LLMTravelers of the experimental data. Similar to laboratory findings in[41], the mean travel times for all LLMTravelers closely approximate the DUE points. Routes with higher flow sensitivity display greater variance in travel time. For instance, the standard deviation of travel time for "route 2" consistently exceeds that of "route 1", reflecting "route 2"’s greater sensitivity to flow changes.

In comparing results across models, the "" column in Table4measures the relative difference between each model’s mean travel time and the DUE point, providing an indicator of alignment with equilibrium conditions. In the laboratory results, values for "" remain within, indicating close alignment with the DUE. By contrast, LLMTravelers tend to exhibit slightly larger deviations from the DUE, with most values remaining within. Additionally, the laboratory results consistently yields the smallest standard deviation in comparison to the LLMTravelers.

A distinctive case is presented by the LLM-gpt4o agent. While its "" ratio remains relatively low, indicating its mean travel time is near the DUE, its standard deviation is the highest among all scenarios. This suggests greater daily deviations from the DUE. Figure5(b) further illustrates the fluctuation pattern of LLM-gpt4o, showing that daily travel times periodically evolve around the DUE. This behavior may result from LLM-gpt4o agents adhering to a fixed set of strategies, leading to cyclical patterns over time.

This pattern may not be ideal within this experimental context, as the LLMTraveler’s prompt include profiles do not introduce sufficient variability. As a result, agents with different profiles exhibit minimal behavioral differences under similar experiences. In particular, LLMTravelers based on "GPT-4o-2024-05-13"[25]tend to select the same route despite profile differences, which deviates from realistic behavior patterns. This outcome indicates that larger, more complex pretrained models do not necessarily enhance the realism of simulated route-switching behavior. However, as shown in Figure5, other LLM-based agents effectively reproduce stochastic fluctuations around the equilibrium point, which is similar to previous laboratory data[14,15,41,50].

aTravel time under DUE.bThe average travel time observed in the experiment.cThe percentage difference between the Mean and DUE travel time.dThe standard deviation of travel time in the experiment.

This subsection presents an analysis of observed switching rates to identify potential similarities and differences between laboratory observations, established theories, and the proposed LLMTraveler ageny. The focus is on examining the relationship between average switching rates and different cost combinations. Specifically, three route-switching behavior patterns observed in laboratory data[41]are compared to those exhibited by LLMTravelers. Following the approach in[41], the logistic regression is applied to model the binary decision-making process in the scenarios. The switching rate is fitted using maximum likelihood estimation across all experimental data:

whereandare the parameters to be estimated, andandrepresent the travel times on routeand route, respectively.

This model enables a statistical interpretation of choice behavior by incorporating the effects of route cost differences on decision-making. In contrast to the simple averaging method, this approach adjusts for varying frequencies of different cost combinations, assigning appropriate weights based on occurrence. Figure6shows the fitted logistic curve, with estimated parameter values detailed in Table5. In the laboratory results, all-values are less than 0.01, indicating that all variables are statistically significant. However, in the LLMTraveler-based experiments, not all variables achieve significance; some models have-values greater than 0.05, particularly LLM-gpt35 and LLM-yi-medium. Most other models, however, exhibit significant-values. Despite these differences, the logistic function effectively captures the overall trend of the experimental data, as shown by the fitted curves in Figure6. This holds true even for the fitted curves of LLM-gpt35 in Scenario 2 (Figure6(b)) and LLM-yi-medium in Scenario 4 (Figure6(e)). Additionally, the fitted parameters vary among different LLMTravelers, demonstrating that each agent has a different sensitivity to the same route conditions.

This subsection evaluates whether the LLMTraveler exhibits consistency with the three route-switching behavior patterns identified in the laboratory data[41].

First pattern.
The switching rate increases with the cost difference between the last-chosen route and its alternative. Even when the last-chosen route has a lower cost than the alternative, the switching rate remains positive. The PRC model fails to capture this pattern, as it assumes travelers will only switch to route with lower cost[46]. By contrast, the MNL model accounts for this behavior, as it allows travelers to switch even when their previous choice is better. This pattern is replicated across all LLMTravelers, as shown in Figure6. Moving from left to right along the x-axis, where the cost of "route 1" increases and that of "route 2" decreases, the switching rate from "route 1" to "route 2" rises, while the switching rate from "route 2" to "route 1" declines. Additionally, as shown in Figure6(b), even when the cost of "route 1" is 30 minutes and "route 2" is 90 minutes (three times greater), some LLMTravelers still switch from "route 1" to "route 2."

Second pattern.
In asymmetric networks, the average switching rates at the DUE point for both routes are between 0 and 0.5, but with significant differences betweenand. In the traditional PRC and MNL models, the DUE switching rates are assumed to be 0 and 0.5, respectively, which does not align with these laboratory observations. For the LLMTravelers, as shown in Figure6(b–e), the average switching rates at the DUE point also fall between 0 and 0.5. However, unlike the laboratory data, the values ofandare close to each other, especially in Figure6(d) and Figure6(e), where the observedandvalues at the DUE point are nearly identical.

Third pattern.
The average switching rate is influenced not only by the cost difference but also by the characteristics of the last-chosen route. For instance, in Scenario 2 of laboratory data, when the cost of "route 2" is 20 minutes higher than "route 1", the average switching rate from "route 2" to "route 1" is 0.481. In contrast, when the cost of "route 1" is 20 minutes higher than "route 2," the switching rate from "route 1" to "route 2" drops to 0.267[41]. This pattern is also observed among LLMTravelers, as shown in Figure6(b). "Point-1" indicates that when the cost of "route 2" is 20 minutes higher than "route 1", the switching rate from "route 2" to "route 1" is 0.385. In contrast, when the cost of "route 1" is 20 minutes higher than "route 2," the switching rate from "route 1" to "route 2" drops to 0.238.

In summary,the LLMTravelers’ route-switching behavior aligns with most observed patterns from laboratory data, except for the absence of a distinct difference betweenandat the DUE point in asymmetric networks. These patterns, which are not fully captured by traditional models, demonstrate that LLMTravelers choose routes in a logit-like manner, showing "inertia" by favoring previously chosen routes[41]. Additionally, choices are influenced not only by past route costs but also by inherent route characteristics, indicating that travelers treat routes with different attributes differently.

SECTION: 4Evaluation of UE choices in a multi-OD pair network

This section explores the application of the proposed LLMTraveler to model the learning behaviors of selfish agents in multi-OD pair congestion games.

SECTION: 4.1Experiment Settings

Figure7illustrates the OW road network[51], which connects two residential areas (1 and 2) with two large shopping centers (12 and 13). The figure also displays the free-flow travel times between these points, measured in minutes. Notably, all links in the network are two-way. The performance function for each link is the free-flow travel time plus 0.02 minutes for each vehicle per hour of flow. Table6presents the OD demand for the OW network. For example, the first row indicates that a total of 600 travelers depart from node 1 to node 12, making the entire network accommodate 1,700 travelers. For each OD pair, the-shortest path algorithm[40]is used to determine their route choice sets. In the experiments,, meaning each OD pair has 5 predefined routes to choose from.

To simplify the simulation process, a LLMTraveler is used to represent(e.g., 10, 20, 50) travelers, meaning the decisions of one LLMTraveler reflect those oftravelers. Different values ofwere tested, and as shown in Figure8, varyingdoes not affect the convergence of the system towards UE. Therefore, for subsequent experiments,is used, which balances experimental cost and time. Consequently, the number of agents corresponding to each OD pair is shown in the last column of Table6.

The prompt template used in this experiment is shown in Figure2. Considering that most studies view travelers as selfish, the profile includes the phrase "you are selfish." The LLM temperature was set to 0.5 by default in this experiment, while the smoothing factorused for calculating EWMATT was set to 0.2. Additionally, each LLMTraveler was tested three times on the OW network to ensure robust results.

SECTION: 4.2Evaluation Metrics

In this section, travel time and Day Switching Rate (DSR) are used for evaluation.

Travel time.
This metric calculates the average travel time of all travelers collected on the same day.

DSR.
This metric builds upon the general idea of the Switching Rate. To measure the switching behavior of travelers who switch routes from dayto day, the DSRis defined. Letbe the total number of travelers, andbe the number of travelers who switched their route from dayto day. The DSRis defined as:

SECTION: 4.3Compared Method

In addition to the LLMTraveler and MNL model discussed in Subsection3.3, the performance of the proposed method is also compared with the following RL-based approach:

RL-Based Method.
In this approach, travelers are treated as agents to model the learning behaviors of selfish agents in congestion games[21,22,23,24]. This method adopts commonly used settings from prior RL-based congestion game studies, where agents rely on local observations (e.g., origin and destination) to select a route from the available route set each day as their action. The environment provides feedback (e.g., negative travel time), which serves as the reward. Through trial-and-error interactions, the agents accumulate experience and adapt their strategies over time, aiming to minimize individual travel time. The agents are trained using the Independent Proximal Policy Optimization (IPPO) algorithm with parameter sharing to enhance learning efficiency[52].

SECTION: 4.4Experiment Results

Figure9(a) shows that all LLMTravelers converge toward a smaller travel time, indicating a move towards UE, after experiencing initial fluctuations over several days. Although the travel time becomes smaller over time, fluctuations persist, which aligns with findings from laboratory experiments[13,15]. This highlights an advantage of the proposed method compared to some traditional traffic assignment models that assume all travelers are selfish and cannot model these fluctuations. Figure9(b) shows the changes in the DSR over time. Although the specific DSR values vary across different LLMTravelers, the overall trend is consistent: an initial increase followed by a decrease, eventually stabilizing. By day 100, all models except LLM-yi-medium exhibit a DSR below 0.2, with LLM-gpt4o approaching zero. The aggregate-level route choice behavior of LLMTravelers can be summarized into three stages:

Stage 1: Exploration (day 1 to 20)

During this initial phase, all LLMTravelers lack experience and knowledge about the available routes, leading to mostly random or exploratory route choices. This stage is characterized by attempts to explore routes with little to no prior experience. For instance, as shown in the first row of Table7, the LLMTraveler’s reasoning is described as "any route can be chosen." Similarly, the second row illustrates an exploration-driven decision, where the LLMTraveler chooses to "explore route 2 despite its higher EWMATT," seeking to gather additional data and potentially uncover a new optimal route.

Stage 2: Exploration and exploitation (day 21 to 60)

In this intermediate phase, LLMTravelers balance exploration and exploitation. As their experience grows, they begin to leverage knowledge of previously tested routes while still testing less frequently used routes with higher EWMATT values. This stage reflects a strategic approach where agents aim to refine their understanding of optimal routes by combining exploratory actions with exploitation of known information. Examples of both exploration and exploitation behaviors can be observed in the second and third rows of Table7.

Stage 3: Exploitation (after day 60)

By this stage, most routes have been tried multiple times, and LLMTravelers have developed a reliable understanding of the best routes based on EWMATT. Consequently, the majority of choices are exploitative, although there remains a small probability of exploration, accounting for the slight fluctuations in the average travel time.

Figure10further illustrates how the number of parameters of LLM models impacts the performance of LLMTravelers. Figure10(a) reveals that the average DSR during the first 20 days increases with model size. On the other hand, Figure10(b) shows that the average DSR during the last 20 days generally decreases with model size, except for the LLM-yi-medium model, which achieves a relatively higher DSR in this period. The LLM models in Figure10are arranged in increasing order of parameter size, from 7 billion to over 175 billion. This may be because larger models, with their capacity to fit smaller errors, align more closely with human preferences through reinforcement learning from human feedback (RLHF). Such alignment can result in "overconfidence"[25,53,54], reducing flexibility and limiting exploration. This behavior likely contributes to the observed decrease in DSR during the later stages. In contrast, smaller models maintain greater randomness, which may explain their relatively higher DSR during the final period.

Figure11shows the route choice evolution for two LLM-gpt35-based agents in the OW network, traveling from Node 1 to Node 12. In the initial days, "LLM-gpt35 #1" gradually explored from "Route 0" to "Route 4", while "LLM-gpt35 #2" explored in the opposite direction, transitioning from "Route 4" to "Route 1". The daily travel time for each route fluctuated, leading to differences in the agents’ learning and exploration outcomes. Over time, "LLM-gpt35 #1" learned that "Route 2" was optimal and consistently chose it after 50 days. In contrast, "LLM-gpt35 #2" identified "Route 1" as the best choice. These varied experiences, stored in memory, shaped their EWMATT, leading to different route choice behaviors.

Figure12presents the two agents’ retrieved memories after 100 days, showing their chosen times and EWMATT for different routes. In Figure12(a), the EWMATT differences between the two agents for each route were within 1.5 minutes. However, their preferences diverged. For "LLM-gpt35 #1," "Route 1" had the lowest EWMATT, followed by "Route 2". For "LLM-gpt35 #2," "Route 2" was the best, followed by "Route 0." Figure12(b) reveals significant differences in the chosen times for each route, especially "Routes 1" and "Route 2". Despite this, all routes were explored at least six times, reflecting exploratory behavior often seen in real-world travelers. These results highlight how memory and experience shape route choices, consistent with realistic decision-making patterns where individuals balance exploration and exploitation based on past experiences. Additionally, this behavior aligns with the prompt’s guidance to "consider both well-traveled routes and those less explored."

Figure13shows that LLM-gpt35, MNL-0.3, and the RL-based agent converge to a similar average travel time of approximately 71.1 minutes. However, both LLM-gpt35 and the RL-based agent exhibit fluctuations around the UE even after convergence, achieving a slightly lower travel time, a behavior not observed in the MNL model. Furthermore, the MNL model demonstrates significantly faster convergence during the initial several days. This is because the MNL model shares the experience of all routes to calculate EWMATT and proportionally allocates choices, whereas the LLM-gpt35 optimizes choices individually based on accumulated experience. Notably, the RL-based agent requires substantially more data to achieve similar results, a reflection of the relatively low sample efficiency of reinforcement learning. For instance, the RL-based method needs nearly 4000 days of data to achieve results comparable to LLM-gpt35’s convergence within 100 days. This highlights one of the key limitations of reinforcement learning in this context.

Table7and Table3show that LLMTravelers typically provide reasonable explanations for their decisions. This reasoning process not only explains the choice but also reflects the model’s thinking process. For the three examples in Table7, the LLMTravelers consider the current data, focusing particularly on the chosen times and EWMATT, and balance exploration and exploitation before making the final choice. These explanations align with the prompt’s guidance to "think step by step," which encourages the agent to generate intermediate reasoning steps[37,38]. Moreover, the variety of strategies shown in Table3highlights their ability to mimic human decision-making. These agents combine rational analysis with uncertainty, making choices that reflect both historical experiences and personality traits. For instance, while some agents opt for route choices based on lower average travel times, others incorporate randomness or strategic considerations, such as balancing the impact of others’ choices. This combination of logic and unpredictability enhances the LLMTravelers’ applicability for simulating route choice behaviors in transportation studies, providing both clarity and variability in their decision-making.

SECTION: 5Conclusion

This study demonstrates the potential of LLMs as a novel approach to route choice modeling. The proposed framework, LLMTraveler, integrates an LLM with a memory system that allows the agent to interact with its environment, learn from past experiences, and adapt its decisions. The performance of LLMTraveler was evaluated in both single and multi-OD pair scenarios. In the single OD pair scenarios, the agent’s route-switching behavior aligns with most patterns observed in laboratory data, with some of these patterns not being fully explained by traditional models. In multi-OD pair scenarios, LLMTraveler replicates human-like behavior at both aggregate and individual levels. The route choice outcomes are comparable to those produced by traditional MNL models and RL-based agents. Additionally, LLMTraveler provides natural language explanations for its decisions, offering transparency and insight into its reasoning. The study also shows that lightweight, open-source LLMs can effectively replicate human-like route choice behavior, with only minor performance differences compared to larger, closed-source models.

However, this study is limited to route choice. Future research could extend this approach to other transportation decisions, such as mode choice and departure time. Additionally, calibrating LLMs with real human activity data could improve their alignment with actual behavior, enhancing their realism and applicability.

SECTION: Acknowledgments

The work was supported by start-up funds with No. MSRI8001004 and No. MSRI9002005, partly by the TRENoP research center fund at KTH, Sweden.

SECTION: AUTHOR CONTRIBUTIONS

The authors confirm contribution to the paper as follows: study conception and design: Z Ma, L Wang, Z He, C Lyu, P Duan, X Chen, N Zheng; methodology: P Duan, L Wang, Z Ma, Z He, C Lyu, L Yao; data collection: L Wang, C Lyu, X Chen; analysis and interpretation of results: L Wang, C Lyu, Z Ma, X Chen, P Duan, Z He; draft manuscript preparation: L Wang, P Duan, Z Ma, X Chen, C Lyu. manuscript revision: P Duan, Z Ma, Z He, X Chen, C Lyu, N Zheng, L Yao. All authors reviewed the results and approved the final version of the manuscript.

SECTION: References