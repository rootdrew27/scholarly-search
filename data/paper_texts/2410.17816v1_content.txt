SECTION: Introduction
The Sun is the source of a variety of dynamic phenomena, such as solar flares and coronal mass ejections (CMEs), which significantly impact space weather. These solar activities can cause disturbances that affect satellite operations, power grids, and communication systems, potentially leading to significant economic and technological disruptions. Accurate forecasting of space weather events is therefore crucial to mitigate their adverse effects on modern infrastructure.

It is well-established that solar flares, i.e. intense bursts of radiation originating in the solar atmosphere, predominantly above sunspots, are the main trigger of space weather. Therefore, the prediction of solar flares has become one of the most fundamental tasks of space weather understanding, and the development of data-driven models relying on artificial intelligence (AI) approaches is disruptively changing the scenario of this scientific and technological challenge. Recent studies have shown a strong correlation between the likelihood of solar flare occurrences and specific characteristics of active region groups, including their size, the number of sunspots, and their magnetic classification. It follows that active region classification represents the crucial initial step in predicting space weather.

Traditionally, expert observers manually classify active regions by examining images of the Sun, identifying sunspots, and categorizing them based on their size, shape and magnetic properties. This method has the disadvantages of being time-consuming, subjective, and not scalable with data volume. Nevertheless, the rapid accumulation of solar observations from an increasing number of space missions makes it possible to automating the classification of active region groups using machine learning techniques, and recent advancements in the field, particularly deep learning, have shown promising results in this task. This study builds upon the research presented inandto explore and critically compare various state-of-the-art deep learning architectures for image classification, specifically focusing on convolutional neural networks (CNNs) and Vision Transformers (ViTs) when applied to solar active region cutouts. Specifically, we evaluated models trained on magnetograms, continuum images, or both, using 2D convolutions, and progressively trained models of increasing complexity to determine which architecture is most robust for this classification task. This evaluation incorporates extensive on-the-fly data augmentation and presents results based on five-fold cross-validation.

The plan of the paper is as follows. Section 2 describes the properties of the data set used for the experiments, and the corresponding pre-processing step. Section 3 provides details on the design and properties of the deep learning networks employed for the analysis. Section 4 describes the results of the study. Our conclusions are offered in Section 5.

SECTION: Data
SECTION: The Dataset
This study considered the SOLAR-STORM1 dataset provided by the Space Environment Warning and AI Technology Interdisciplinary Innovation Working Group.
This dataset contains continuous and magnetogram images provided by the Spaceweather HMI Active Region Patch (SHARP) from the Helioseismic and Magnetic
Imager (HMI) on-board the Solar Dynamics Observatory. The images in this dataset are sunspots cutouts of different aspect ratios, as shown in Figure.

The temporal range of the dataset covers the interval from 2010 May 1 to 2017 December 12 (see Figure, top panel) and the classes distribution is shown in Table, second column.
Specifically, the Mount Wilson sunspot magnetic types are divided into the following 8 classes.

Alpha: Unipolar sunspot group.

Beta: Bipolar sunspot group with distinct positive and negative magnetic polarities.

Gamma: Complex region with irregular positive and negative polarities, preventing bipolar classification.

Delta: Umbrae with opposite polarities within one penumbra, separated by less than 2°.

Beta–Gamma: Bipolar group with complex polarity distribution, lacking a clear dividing line.

Beta–Delta: Beta group containing one or more Delta spots.

Beta–Gamma–Delta: Beta–Gamma group containing one or more Delta spots.

Gamma–Delta: Gamma group containing one or more Delta spots.

Due to the limited number of examples in some groups, the Delta and Gamma-Delta magnetic types were excluded, and the Beta–Gamma, Beta–Delta, and Beta–Gamma–Delta classes were merged into a single class, referred to as Beta-X. As a result, the problem was approached as a multi-class classification task with three categories: Alpha, Beta, and Beta-X. In this dataset, images were taken every 96 minutes, but a sunspot group does not change consistently within 24 hours (see). Therefore, this dataset presents a high degree of redundancy, and a random split of the training, validation, and test sets (as done in) would result in inflated test scores.
As the dataset lacks active region identifiers, such as NOAA numbers, the only viable option was to apply a temporal split to ensure that highly similar images are not distributed across different sets.
This approach mirrors the splitting strategy used in, where all data after 2016 January 1 were designated as the test set, and the remaining data were used for fivefold cross-validation.
A visualization of this splitting method is presented at the bottom of Figure. The number of classes along with their respective percentages for the training and validation sets across the five folds are detailed in Table, while the distribution for the test set is shown in Table, third column.

SECTION: Data pre-processing
Data augmentation is the process of artificially generating new data from existing one, primarily to train deep learning models, which require a large amount of training data samples in order to avoid overfitting. In this work the images are first normalized and cropped following the approach described in. In particular, magnetograms were normalized so that each pixelof the normalized image follows from

whererepresents the original pixel value and. The HardTanh function is defined as:

which ensures that the final value lies within the interval. In () we set, as done inand, since this threshold value appears to give slightly higher performance metrics compared to higher constant values (see Table). For continuum images, normalization is performed such that each pixelis determined by

whereandrespectively denote the minimum and maximum values of the input image, which has pixels.
The final normalized values lie in the interval, with background closer to 0 and the sunspots closer to 1.
Additionally, we included cropped images of the active regions in the dataset, using a strategy similar to that of. Specifically, this cropping was performed by comparing the absolute differences between the maximum values in the rows and columns of the normalized continuum images. Peaks in these differences were then identified, and a bounding box was selected by adding a margin around these peaks.
This is done in order to filter out noise, allowing the model to concentrate on the significant features relevant to the classification task.
An example of the normalization and cropping is shown in Figure.
Next, we apply a random horizontal and/or vertical flip of the images, a perspective change, and an affine transformation including a minor translation and a rotation of a random angle up to.
Usually, data augmentation techniques are applied before fitting a deep learning model on the whole dataset.
However, in this way a single augmented dataset is created. Performing data augmentation on-the-fly as in, where a new set of random transformations is applied to the dataset at each epoch, creates slight variations with every iteration. Therefore, this approach enables better exploration of the data space and can help reduce the risk of overfitting.

SECTION: Deep neural networks
In this work, we explored different neural network architectures for classifying sunspots by their magnetic classes. To achieve this, we utilized the models provided in.
The neural network architectures selected for this comparative study encompass both Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs), since traditionally CNNs are better for capturing local features, while transformers focus more on global features. By comparing these two distinct architectures, this study aimed to evaluate their respective strengths and limitations in classifying solar active region cutouts, with the goal of determining the most robust approach for this classification task.

SECTION: Convolutional Neural Networks
A Convolutional Neural Network (CNN) is a specific type of deep learning architecture designed for efficiently processing grid-like data, such as images. CNNs use convolutional layers to automatically extract hierarchical features by applying learned filters across the input data. Pooling layers further reduce spatial dimensions, helping to downsample feature maps and minimize computational complexity. Non-linear activation functions then introduce complexity into the model, allowing it to capture intricate data patterns. Finally, fully connected layers transform the learned features into a final prediction. CNNs excel at visual tasks due to their ability to learn spatial relationships and patterns directly from data. We now briefly introduce the CNNs whose performances have been explored in this paper.

The VGG (Visual Geometry Group) architecture, introduced in, consist in of sequential layers of small 3x3 convolutions stacked together, followed by max-pooling layers.
This design allows the network to capture increasingly complex features while maintaining manageable computational complexity.

The Inception network, introduced byand also known as GoogLeNet, is a deep convolutional neural network involving the “Inception module,” which applies multiple convolutional filters of varying sizes (1x1, 3x3, and 5x5), and a pooling layer in parallel. This allows the network to capture features at different scales without significantly increasing the computational cost. Additionally, the architecture incorporates dimensionality reduction through 1x1 convolutions, which helps controlling the number of parameters and memory usage.

The ResNet (Residual Network) architecture, introduced in, is a type of deep neural network that introduces connections or residual connections to address the vanishing gradient problem occurring in very deep neural networks. These connections bypass one or more layers by allowing the input of a layer to be added directly to its output, in such a way that the network passes information directly to deeper layers. This technique enables the model to learn identity mappings, making it easier to train deeper networks without degradation in performance.

SECTION: Vision Transformers
Visual Transformers (ViTs) are a class of deep learning architectures designed to handle visual data using the transformer model, which was originally developed for natural language processing tasks. Unlike CNNs, ViTs operate on image patches that are treated as a sequence of tokens, similar to words in a sentence, and are processed using self-attention mechanisms (see). This mechanism is used to determine the importance of each element in a sequence relative to others. It allows the model to focus on relevant parts of the input data when generating a representation for each element.
In self-attention, for each input token (or element), the model computes three vectors: query, key, and value. The query from one token is compared with the keys of all other tokens to compute attention scores, which indicate how much focus the model should place on each token. These scores are then used to compute a weighted sum of the value vectors, creating a new representation for each token based on its relationships to others.
In this way, the model can capture long-range dependencies within the data, as each token can attend to all others, regardless of their distance in the input sequence.
This approach allows ViTs to capture global dependencies between patches early in the network, making them effective for tasks requiring global context.

Besides ViTs architectures based on, in this paper we also explore the following variations.

The Data-efficient Image Transformer (DeiT), introduced by, is an improvement on ViT that focuses on reducing the data requirements for training Vision Transformers. While ViT performs well with large datasets, DeiTs introduce techniques such as knowledge distillation to train the model more efficiently on smaller datasets without sacrificing performances. DeiTs retain the transformer-based architecture of ViTs but optimize their training process, making it more accessible for practical applications where large amounts of labeled data may not be available.

The Bidirectional Encoder representation from Image Transformers (BEiT), introduced by, is a self-supervised vision transformer model that extends the principles of BERT-style pre-training from natural language processing to vision tasks. BEiT leverages masked image modeling, where portions of the input image patches are masked and the model is trained to predict the missing content based on the surrounding context. This pre-training technique allows BEiT to learn rich image representations without the need for extensive labeled data, making it highly effective in scenarios with limited supervision. After pre-training, BEiT can be fine-tuned for various downstream tasks, such as image classification, object detection, and segmentation. BEiT’s success lies in its ability to transfer knowledge learned during self-supervised pre-training to a wide range of vision applications, achieving competitive results while reducing the reliance on large labeled datasets.

SECTION: Results
The results presented in this section have been obtained by applying the pipeline summarized in Figureto all the networks previously illustrated. To train these networks, we used the Adam optimizerwith a learning rate of, employing a weighted categorical cross-entropy loss function to address class imbalance in the different folds. Once normalized, the images have been resized to asquare, with zero padding added to maintain the original aspect ratio. This was done so that the weights of the neural networks are initialized using transfer learning from pre-trained models on the ImageNet dataset.
This approach leverages the feature extraction capabilities learned from a large and diverse set of images, allowing the models to start with a strong foundation rather than from scratch. Even though transfer learning performs better when the source and target domains are closely related, it still offers significant advantages by improving convergence and potential accuracy. Additionally, early stopping was implemented to halt training if the validation loss did not improve.
The model weights were then selected based on the epoch that achieved the best validation performance. To enhance the statistical reliability, the results are presented as the average test scores across all folds, along with their corresponding minimum and maximum values. Models have been trained on magnetogram data, continuum data and both combined in a 2 channel image using 2D convolutions. The number of trainable parameters for each model is reported in Table. The results of the analysis are summarized in Table.

Among all the models, ResNet18 trained on both magnetogram and continuum images achieved the highest average accuracy () and highest average F1 score (), demonstrating the efficacy of combining both image types in providing richer information for model training. This suggests that the ResNet architecture can effectively leverage diverse inputs to outperform other architectures in terms of overall classification accuracy.

When analyzing models trained on only magnetograms, the DEiT Base model stands out, achieving the highest average accuracy () and highest F1 score of 0.861. This indicates that transformer-based architectures can capture relevant spatial features present in magnetograms (even if ResNet-based architectures remain competitive also in this case). For models trained exclusively on continuum images, ResNet10t achieved the highest average accuracy () and ResNet18 the highest average F1 score (). This suggests that continuum images may contain less spatial variability than magnetograms, and a simple ResNet-based architecture with relatively few parameters is sufficient to extract meaningful patterns from the data. In general, accuracy on continuum images tends to be slightly lower across all models compared to magnetograms or combined image types, likely due to the more homogeneous nature of this data type.

When analyzing the F1 score for theclass, ResNet10t and ResNet18 achieve the highest average scores () when trained on both image types, while for theclass the highest F1 score on average is obtained with the ResNet18 network. Models trained only on magnetograms, such as ResNet18 and DEiT Base, also performed well, achieving F1scores of 0.876 and 0.881, respectively. However, models trained solely on continuum data, like ViT Base (F1= 0.817), showed a noticeable drop in performance, possibly due to the limited variability in continuum images. In terms of the F1-X score, which evaluates performance in the most challenging class, as expected the DEiT architecture achieves the best average score (), suggesting that transformer-based models are better suited for capturing the intricate spatial relationships present in complex magnetic structures. However, performance declines notably when models are trained exclusively on continuum images.

The observed variability across folds, indicated byvalues, generally ranges from 2% to 10% for accuracy and overall F1 scores.
For the more complex-X class, the variation can be even higher, with differences of up to 20%. Anyway, in general, the average performances do not change drastically from one model architecture to another. This could be attributed to several factors. First, complex models may not fully utilize their capacity if the dataset does not present sufficient complexity or variability, leading to comparable performances with simpler architectures that can efficiently capture the essential patterns. Additionally, simpler models benefit from reduced risk of overfitting due to their smaller number of parameters, particularly when trained on datasets with moderate complexity. This is further enhanced by the use of modern training techniques such as batch normalization, dropout, and advanced optimizers, which improve generalization across models. These factors collectively explain why more complex architectures may not consistently outperform simpler ones, particularly if a specific dataset does not demand the added complexity. This closeness can also in part be attributed to the extensive data augmentation procedure employed during the training phase.
In particular, ViTs tend to be less sensitive to data augmentation compared to traditional CNNs, mainly due to their global attention mechanism and ability to model relationships across the entire image. Transformers inherently capture more comprehensive image representations, making them more robust to small variations that data augmentations typically address in CNNs. As a result, extensive data augmentations help to narrow the performance gap between CNNs and more complex transformer models.

Compared to similar works, performance metrics reported by the study ofare not directly comparable due to the random splitting of a highly redundant dataset.
Comparison with the study of, where 3D convolutions are used to classify an image consisting of both magnetogram and continuum data, shows that the maximum scores obtained with 2D convolutions are comparable to a more involved ensemble model. However, our goal was not to focus on the individual absolute values of the scores in comparison to other works, as it is possible to continue training with different random seeds in search of a favourable outlier that yields higher scores.
Instead, we aimed to provide a meaningful comparison of the performance of different model architectures under similar conditions, highlighting their relative strengths and weaknesses from a global perspective.

SECTION: Conclusions
In this study, we analyzed different deep learning architectures for the task of classifying the magnetic class of solar cutouts.
We observed that combining magnetogram and continuum image types enhances model performance by leveraging complementary features from diverse inputs.
When considering only magnetograms, data efficient transformer models achieve the best performance, demonstrating the effectiveness of transformer-based architectures in capturing the spatial complexity of magnetograms.
Models trained exclusively on continuum images exhibit overall lower performance, suggesting that continuum images, due to their more homogeneous nature, offer less spatial variability.

Overall, performance metrics do not change drastically between model architectures, a fact that can be attributed to several factors: complex models may not fully utilize their capacity if the dataset lacks sufficient complexity or variability, resulting in performance comparable to simpler architectures that can effectively capture the essential patterns;
simpler models have a reduced risk of overfitting, particularly when trained on moderately complex datasets; modern training techniques such as batch normalization, dropout, and advanced optimizers further improve generalization, helping to reduce the performance gap between simpler and more complex models. Finally, the extensive data augmentation procedures employed during the training phase also contribute to closing the gap between simpler CNNs and more complex ViTs.

SECTION: Acknowledgement
EL and AMM acknoledge the HORIZON Europe ARCAFF Project, Grant No. 101082164.
SG, MP and AMM acknowledge INdAM-GNCS;
SG was supported by the Programma Operativo Nazionale (PON) “Ricerca e Innovazione” 2014–2020.

SECTION: References