SECTION: FiTv2: Scalable and Improved Flexible Vision Transformer for Diffusion Model
.
In the context of this reality, existing diffusion models, such as Diffusion Transformers, often face challenges when processing image resolutions outside of their trained domain.
To address this limitation, we conceptualize images as sequences of tokens with dynamic sizes, rather than traditional methods that perceive images as fixed-resolution grids.
This perspective enables a flexible training strategy that seamlessly accommodates various aspect ratios during both training and inference, thus promoting resolution generalization and eliminating biases introduced by image cropping.
On this basis, we present the(FiT), a transformer architecture specifically designed for generating images with.
We further upgrade the FiT to FiTv2 with several innovative designs, includingthe Query-Key vector normalization, the AdaLN-LoRA module, a rectified flow scheduler, and a Logit-Normal sampler.
Enhanced by a meticulously adjusted network structure, FiTv2 exhibitsconvergence speed of FiT.
When incorporating advanced training-free extrapolation techniques, FiTv2 demonstrates remarkable adaptability in both resolution extrapolation and diverse resolution generation.
Additionally, our exploration of the scalability of the FiTv2 model reveals that larger models exhibit better computational efficiency.
Furthermore, we introduce an efficient post-training strategy to adapt a pre-trained model for the high-resolution generation.
Comprehensive experiments demonstrate the exceptional performance of FiTv2 across a broad range of resolutions.
We have released all the codes and models atto promote the exploration of diffusion transformer models for arbitrary-resolution image generation.

SECTION: 
Natural images inherently possess various resolutions, as illustrated in, the images inshowcase diverse resolutions and aspect ratios.
However, current image generation models struggle with generalizing across arbitrary resolutions.
The Diffusion Transformer (DiT)family, while excelling within certain resolution ranges, falls short when dealing with images of varying resolutions.
This arises from the inability of DiT to incorporate images with dynamic resolutions during its training process, impeding its capability to adapt to diverse token lengths or resolutions effectively.

To bridge this gap,(FiT)proposes a novel architecture adept at generating images at.
The core motivation lies in a fundamental shift in image data conceptualization:
FiT conceptualizes images as sequences of variable-length tokens, departing from the traditional perspective of static grids with fixed dimensions.
This paradigm shift enables dynamic adjustment of sequence length, facilitating image generation at arbitrary resolutions unconstrained by predefined grids. By efficiently managing and padding these variable-length token sequences to a specified maximum, FiT achieves resolution-independent image synthesis.

FiT represents this paradigm shift through three significant advancements: the flexible training pipeline, network architecture, and inference process.
FiT introduces a flexible training pipeline that preserves original image aspect ratios by treating images as token sequences, accommodating varied resolutions within a predefined maximum token limit. This approach, unique among transformer-based generation models, enables adaptive resizing without cropping or disproportionate scaling. Building upon the DiTarchitecture, FiT incorporates-D Rotary Positional Embedding (-D RoPE), Swish-Gated Linear Units (SwiGLU), and Masked Multi-Head Self-Attention to effectively handle diverse image sizes. For inference, FiT adapts length extrapolation techniques from LLMs, tailoring them for 2-D RoPE to enhance performance across a wide range of resolutions and aspect ratios.

Despite its innovations, FiT exhibits several limitations. It underperforms on the standard ImageNet 256×256 benchmark, and its architecture results in increased parameter count and computational costs compared to DiT. Furthermore, there exists instability issues in the training of FiT, presenting additional challenges for practical implementation.

To achieve better performance, we adopt several advanced enhancements to build FiTv2, an improved version of FiT.
Extensive experiments on both class-guided image generation and text-to-image generation tasks demonstrate that our FiTv2 outperforms or achieves competitive performance, compared to other state-of-the-art CNN modelsand transformer models. Specifically, our FiTv2-3B/2 model, after training onlysteps ondataset, achieves competitive performance on standard-benchmark while outperforming all SOTA models by a significant margin across resolutions of,,,, and. With merelyextra post-training steps, our FiTv2-3B/2 model exceeds all SOTA models by a great margin across,, andresolutions.
Moreover, FiTv2-XL/2 model holistically surpass the DiT-XL/2 and SiT-XL/2 with the same parameters andof the training costs.
Further, with the same training steps, our FiTv2-XL/2 model surpasses the SiT-XL/2 model greatly on text-to-image tasks.

A preliminary version (i.e., FiT) of this work was published in.
In this paper, we extend FiT in the following aspects:

We propose an improved version of FiT by incorporating Query-Key Vector Normalization (QK-Norm) into the attention layer for stability, as well as decreasing the hidden size of Swish-Gated Linear Unit (SwiGLU)and adopting the Adaptive Layer Normalization with Low-Rank Adaptation(AdaLN-LoRA) for efficiency. These improvements lead to FiTv2, a more efficient and scalable version of FiT, which achieves state-of-the-art performance in many image generation tasks.

We improve the training strategy by switching the noise scheduler from denoising diffusion probabilistic model (DDPM)to rectified flowand adopting the Logit-Normal sampling for timesteps, which results in faster convergence. Furthermore, we analyze the limitations of the original FiT and propose a novel mixed data preprocessing strategy that benefits image synthesis across various resolutions. Combining the above architectural and training strategy improvement enables FiTv2 to achievethe convergence speed of the original FiT.

We provide comprehensive analytical experiments and visualization to evaluate the effectiveness of FiTv2.
We comprehensively analyze the effect of each modification from FiT to FiTv2, as detailed in. We explore different training-free resolution extrapolation methods for arbitrary resolution generation in FiTv2. Moreover, we benchmark the generalization and extrapolation performance of FiTv2 and other state-of-the-art methods. We also scale our FiTv2 model to 3 billion parameters to study the scalability. Furthermore, we conduct an efficient post-training experiment to investigate the transfer from low resolution to high resolution. To validate the effectiveness of FiTv2 beyond the class-guided image generation, we extend it to text-to-image generation tasks, demonstrating its superiority over the previous state-of-the-art SiTmodel.

SECTION: 
SECTION: 
Denoising diffusion probabilistic models (DDPMs)and score-based generative modelshave exhibited remarkable progress in the context of image generation tasks. The Denoising Diffusion Implicit Model (DDIM), offers an accelerated sampling procedure. Latent Diffusion Models (LDMs)establishes a new benchmark of training deep generative models to reverse a noise process in the latent space, through the use of VAE. Normalizing Flowsare a distinct category of generative models which represent data as intricate and complex distributions. Recent flow modelspresent an alternative approach by learning a neural ordinary differential equation (ODE) that transports between two distributions.
By solving a nonlinear least squares optimization problem, rectified flow modellearns to map the points drawn from two distributions following the straight paths, which are the shortest paths between two points and hence yield computational efficiency.
We follow the rectified flow implementation for image synthesis with fewer sampling steps.

SECTION: 
The Transformer modelshave been also explored in the DDPMsand rectified flowsto synthesize images. DiTis the seminal work that utilizes a vision transformer as the backbone of LDMs and can serve as a strong baseline. Based on DiT architecture, MDTintroduces a masked latent modeling approach, which requires two forward runs in training and inference. U-ViTtreats all inputs as tokens and incorporates U-Net architectures into the ViT backbone of LDMs. DiffiTintroduces a time-dependent self-attention module into the DiT backbone to adapt to different stages of the diffusion process. Furthermore, SiTutilizes the same architecture as DiT and explores different rectified flow configurations. Efficient-DiTincorporates dynamic mediator tokens into the transformer of SiT and decreases the generation computation. Flag-DiTand SD3scale up the rectified transformers and achieve better performance. We follow the LDM paradigm of the above methods and further propose a novel flexible image synthesis pipeline.

SECTION: 
Rotary Position Embedding (RoPE)is a pivotal advancement in positional embedding techniques for large language models (LLM). Although RoPE enjoys valuable properties, such as the flexibility of sequence length, its performance drops when the input sequence surpasses the training length. Many training-free approaches have been proposed to solve this issue. Position Interpolation (PI)linearly down-scales the input position indices to match the original context window size, while NTK-aware Scaled RoPE Interoplationchanges the rotary base of RoPE based on the Neural Tangent Kernel (NTK) theory. YaRN (Yet another RoPE extensioN)is an improved method to efficiently extend the context window.
While these methods scale the the positional embedding to accommodate longer contexts during inference, another paradigmdirectly scales the attention logits to aggergate information based on entropy theory. Our work provides a comprehensive benchmark for diverse methods on image resolution extrapolation and generalization.

SECTION: 
SECTION: 
DDPMand score-based generative model, both formulated through stochastic differential equations (SDE), produce high-quality samples but suffer from slow inference due to iterative denoising. DDIM, an implicit probabilistic model based on ordinary differential equations (ODE), accelerates sampling with fewer steps but at the cost of lower generation quality compared to SDE methods.

To tackle the aforementioned problem,proposes rectified flow, an ODE-based model that transports two empirical distributionstoby following straight line paths as much as possible. The straight paths are both theoretically desired since they are the shortest paths between two endpoints, and computationally efficient because they can be simulated exactly without time discretization, allowing for few-step and even one-step sampling.

Given two target distributionsand empirical observations, the rectified flow induced fromis an ODE model on time,

which convertsfromtofrom. Here the drift forceaims to drive the flow to follow the straight directionas much as possible. To learn this force following the linear path pointing fromto, a simple least square regression problem needs to be solved:

whereis the linear interpolation ofand. In practice,is parameterized with neural networks.

Rectified flow yields several desired properties. First, the flows avoid crossing different paths, which is the condition that the ODE is well-defined, i.e., its solution exists and is unique. Whilecausalizes, Markovianizes, and derandomizes, it preserves the marginal distributions all the time because the continuity equation always holds. Theoretically, rectified flow provably reduces the convex transport costs:for any convex. An intuitive explanation is that the paths of the flowis a rewiring of the straight paths connecting, thus the convex transport costs are guaranteed to decrease. Furthermore, on the practical computational efficiency side, the flow becomes nearly straight with just one step of reflow, hence a very few number of Euler discretization steps or even a single Euler step is needed to simulate the ODE. This not only reduces discretization error but also largely improves the sample efficiency.

SECTION: 
-D RoPEis a a dominant positional embedding technique for large language models (LLM). By applying a rotary transformation to the embeddings, it incorporates relative position information into absolute positiaonal embedding. Given the-th key and-th query vector as,-D RoPE multiplies the bias to the key and query vector in the complex vector space:

whereis rotary frequency matrix withand rotary base. In the real space, given, the rotary matrixequals to:

The attention score with-D RoPE is calculated as:

-D RoPE is introduced by our previous work FiTto enhance the resolution generalization in diffusion transformer.
Given-D coordinates of width and height as, the-D RoPE is:

where, anddenotes concatenating two vectors in the last dimension. Note that we divide the-dimension space into-dimension subspace to ensure the consistency of dimension, which differs from-dimension subspace in-D RoPE. Analogously, the attention score with-D RoPE is:

SECTION: 
A prvious versionof our work proposes FiT, a transformer architecture that can stably train across various resolutions and generate images with arbitrary resolutions and aspect ratios. Built upon DiT, FiT has made some substantial improvements to support flexible training and inference.

Motivated by some significant architectural advances in LLMs, FiT replaces the absolute positional embedding with-D RoPE and replaces the MLP in Feed-forward Neural Network (FFN) with SwiGLU, further improving the extrapolation capability.
Furthermore, FiT uses Masked Multi-Head Self-Attention (MHSA) to replace the standard MHSA in DiT to maintain training integrity with dynamic sequences. Such design enables interaction between noised tokens while isolating padding tokens during the transformer’s forward pass. Given sequence maskwhere noised tokens are assigned the value of, and padding tokens are assigned the value of negative infinity (), masked attention is defined as follows:

where,,are the query, key, and value matrices for the-th head.

SECTION: 
SECTION: 
The overview of FiTv2 is illustrated in., FiTv2 encodes preprocessed images into image latents using a pre-trained VAE encoder. These latents are then patchified into sequences of varying lengths. To batch these sequences, the latent tokens are padded to a maximum lengthwith padding tokens. Positional embeddings are similarly padded with zero. The loss function is computed only for the denoised output tokens, ignoring padding tokens.

, a position map is defined for the generated image, and noised tokens are sampled from a Gaussian distribution as input. Afteriterations of denoising, the tokens are reshaped and unpatchified according to the position map to produce the final image.

SECTION: 
We conduct extensive experiments to further improve the design of FiT blocks that enable more stable and efficient training, as detailed in. The architecture changes from FiT to FiTv2 block are illustrated in.

We observe a vanishing loss problem when scaling up the training steps of the original FiT under mixed-precision training, as in. Inspired by the ViT-22B, we apply LayerNorm (LN) to the Query (Q) and Key (K) vectors before the attention calculation. Formally, the attention weights inis modified to:

By applying this technique, we can effectively eliminate excessively large values in attention logits, which stabilizes the training process, particularly during mixed-precision training.

We find that directly using SwiGLU with the same hidden size as the original MLP in DiTwill incur more parameters and computational cost, as detailed in. To align the parameters and FLOPs with the baseline (the MLP in DiT), the hidden size of SwiGLU in FiTv2 is set toof that in the original FiT.

Given the hidden size as, the main parameters of a FiT block are composed of:

The parameter ratio of Attention, SwiGLU, and AdaLN module is.
We argue that too many parameters are occupied by the AdaLN module, which reduces the capacity available for self-attention blocks and potentially affects the scalability of the model.
Inspired by W.A.L.T., we adopt AdaLN-LoRA in our FiTv2 block. Additionally, a global AdaLN module is utilized to capture overlapping condition information and reduce the redudancy of condition information of each block. This global AdaLN module is shared byblocks, as shown in.

Letdenote the tuple of all scale and shift parameters,andrepresent the embedding for class and time step respectively.
For the-th FiTv2 block, we compute the scale and shift parameters:

where, and the bias parameters are omitted for simplicity. We can adjust the LoRA rankto change the parameter ratio in FiTv2 blocks. This flexibility allows us to reducewhile simultaneously increasing the number of attention layers, leading to enhanced model performance. In practice, we set(see ablation stuides ofin appendices) and the final parameters of a FiTv2 block are composed as:

Compared with, we decrease the model parameters occupied by the AdaLN module, enabling us to increaseaccordingly while maintaining the model parameters in line with the baseline, as shown in.

SECTION: 
DDPMis a widely used framework for diffusion models, however, it often exhibits limitations in sampling efficiency. Recently, the rectified flowframework proposes a more flexible manner than DDPM which constructs a transport between two distributions through ordinary differential equations. Unlike DDPM relying on discretized time steps, rectified flow follows straight paths, enabling faster simulation. This elimination of time discretization not only enhances sampling efficiency but also simplifies the overall process. Such inherent advantages have enabled the development of advanced generative models, such as SiTand SD3. We follow the rectified flow implementation in SiT, linearly connecting the noise and data distributions, and predicting the velocity fields.

Although the original FiT achieves state-of-the-art performance across unrestricted resolutions and aspect ratios, it underperforms on the standard-benchmark. We posit that this discrepancy arises from the methodological difference in dataset preparation. As illustrated in(b), the initial reliance on image resizing alone of FiT, is opposed to the standard resizing and cropping operations employed in the ADMImageNet reference dataset used for our FIDevaluation.

To bridge this gap, we propose a mixed-data preprocessing strategy, as shown in(c). To mitigate the blurriness from upscaling low-resolution images, we only crop images whose width and height are both larger than the target resolution size.
Exactly, in preprocessing, for images whose sizes are both larger than the target resolution size, we randomly select between resizing only or resizing and cropping with a probability of, as in Algorithm. For images that do not meet these criteria, we simply resize them to satisfy the sequence length limitation.

As we incorporate resized and cropped images into our training process, we can align the generation distribution of our model with the distribution of the ADM ImageNet reference dataset used for our FID evaluation.
Unlike the universal application of resizeing and croping of DiT, as in(a), our method imposes strict limitations on cropping operations. This strategy, combined with the integration of flexible-resolution images, mitigates the blurring and information loss issues prevalent in previous methods. As a result, this modification enables our FiTv2 to achieve competitive performance on the standard-benchmark and-benchmark, while still maintaining the ability to generate images across arbitrary resolutions and aspect ratios.

Typically, the rectified flow scheduler samples timesteps uniformly from theinterval. Recent studies conducted by SD3have investigated the choice of timestep sampling strategies and found that the Logit-Normal sampler outperforms the original uniform sampler as well as other variants. Formally, the Logit-Normal sampler is defined as:

wheredenotes the standard normal distribution with the mean ofand the standard deviation of.
Statistically, this transformation via the logit function ensures that the tails of normal distribution map to the extremes of theinterval in a way that naturally gives more weight to the central part of the diffusion process.
Therefore, the logit-normal sampler facilitates the challenge of learning velocity in the middle of the schedule, as highlighted by EDM, and significantly accelerates the model convergence.

SECTION: 
Previous state-of-the-art methods typically train high-resolution models from scratch, thus incurring substantial computational costs. We hypothesize that models trained on low-resolution images have already learned the essential semantic information from thedataset, but have not been adapted to high-resolution. Therefore, we freeze the majority of parameters of the model and adapt this model through parameter-efficient fine-tuning on the high-resolution data.

Inspired by BitFit, our post-training keeps most parameters of the model frozen, only unfreezing specific parameters related to bias and normalization. Considering the increased image resolution, we also unfreeze the parameters of the image patch embedder and the final output layer, leading to onlyof the overall parameters to be trained. Additionally, we apply the NTK Interpolation to the-D RoPE embedding to facilitate the transition to higher resolutions.

SECTION: 
We further evaluate the effectiveness of our FiTv2 model on the text-to-image(T2I) generation task. As illustrated in, we encode an image into image latents with a pre-trained SDXL-VAEencoder and patchify the image latents to latent tokens.
We use CLIP-Ltext encoder to encode the image caption into text features and embed them into text tokens with an MLP.
The FiTv2-T2I model processes the concatenated text tokens and noised latent image latents to predict the denoised latent image tokens.
The output text tokens and padding tokens are discarded when calculating loss. To accommodate the 1D text tokens with our-D RoPE, we convert each single text positional index to a 2D index tuple. Formally, given text tokensand latent image tokens, the text positional indices and image positional indices are defined as follows:

We also leverage the modulation mechanism of AdaLN module for text conditioning. Specifically, we average-pool the text tokensinto a semantic text embedding, replacing the original class embedding.
This pooled text embedding, along with the time embedding, is then used as input for the global AdaLN and the AdaLN-LoRA modules.

SECTION: 
To achieve resolution extrapolation, we employ training-free positional interpolation techniques, including two widely recognized methods in LLMs: NTK-aware Scaled RoPE Interpolationand YaRN (Yet another RoPE extensioN) Interpolation.
Furthermore, we leverage the advanced VisionNTK and VisionYaRN methodologies. These vision-specific adaptations of the original interpolation techniques are tailored to address the unique challenges posed by two-dimensional image data, which are especially effective in generating images with arbitrary aspect ratios. The detailed formulations of these techniques are comprehensively documented in the appendices.

In the context of resolution-extrapolation, another approach beyond positional embedding interpolation is scaling the attention logits to aggregate information effectively. Previous studieshave theoretically demonstrated that longer contexts result in higher attention entropy of models trained on shorter contexts, leading to widespread aggregation for each token. For higher-resolution image generation, this can cause redundancy in spatial information and disordered object presentations, thereby destroying aesthetics and fidelity. Therefore, a scale factor, which is defined as, is introduced to mitigate the entorpy fluctuations. The formulation of scaled attention is as follows:

SECTION: 
SECTION: 
We present the implementation details of FiTv2, including model architecture, training details, and evaluation metrics.

The detailed model architecture is shown in.
For FiT, we follow SiT-B and SiT-XL to set the same layers, hidden size, and attention heads for base model FiT-B and x-large model FiT-XL. For FiTv2, as described in, we reassign parameters to increase the model layers, thereby aligning the parameters with those of DiTand SiT.
As DiT and SiT reveal stronger synthesis performance when using a smaller patch size, we use a patch size p=2, denoted by FiT-B/2 and FiTv2-B/2.
We adopt the same off-the-shelf pre-trained VAEas SiT, which is provided by the Stable Diffusionto encode/decode the image/latent tokens.
The VAE encoder has a downsampling ratio ofand a feature channel dimension of.
An image of sizeis encoded into latent codes of size.
The latent codes of sizeare patchified into latent tokens of length.

We train class-conditional latent FiTv2 models under predetermined maximum resolution limitation, i.e.,(equivalent to token length) for pre-training and(equivalent to token length) for post-training, on thedataset.
We down-resize the high-resolution images to meet thelimitation while maintaining the aspect ratio.
We follow SiT to use Horizontal Flip Augmentation.
For the pre-training process, we employ a linear learning rate warm-up over the firststeps for stability.
Subsequently, we use a constant learning rate ofusing AdamW, no weight decay, and a batch size of, consistent with SiT. To reduce the training costs, all the experiments are conducted using mixed-precision training.
Following common practice in the generative models, we adopt an exponential moving average (EMA) of model weights over training with a decay of 0.9999.
All results are reported using the EMA model.
We retain the same rectified flow hyper-parameters as SiT.

We evaluate models with some commonly used metrics,Fre’chet Inception Distance (FID), sFID, Inception Score (IS), improved Precision and Recall.
For fair comparisons, we follow DiT to use the TensorFlow evaluation from ADMto report FID-50K and other results. Images of FiT and DiT are sampled with 250 DDPM sampling steps, while FiTv2 and SiT both use the adaptive-step ODE sampler (i.e., dopri5) to generate images.
FID is used as the major metric as it measures both diversity and fidelity, while
IS, sFID, Precision, and Recall are reported as secondary metrics.
We report the exact CFG scale if used. The ablation evaluation results on the CFG scale of our FiTv2 model are shown in.

Following FiT, we conduct evaluation on different aspect ratios, which are,, and. Besides, we divide the assessment into resolution within the training distribution and resolution out of the training distribution.
For the resolution in distribution, we mainly use(1:1),(1:2), and(1:3) for evaluation, with,,latent tokens respectively.
All token lengths are smaller than or equal to 256, leading to respective resolutions within the pre-training distribution.
For the resolution out of distribution, we mainly use(1:1),(1:2), and(1:3) for evaluation, with,,latent tokens respectively.
All token lengths are larger than 256, resulting in the resolutions out of pre-training distribution.
Through such division, we holistically evaluate the image synthesis and resolution extrapolation ability of FiTv2 at various resolutions and aspect ratios.

SECTION: 
In this section, we conduct an ablation study to validate the architecture design in FiTv2. We report the results of various variants of FiTv2-B/2, utilizing FID atresolution, and compare these with the DiT-B/2, and SiT-B/2. We train all the models tosteps to access the training stability.

Specifically,replaces the DDPM scheduler in the original FiT-B/2 with the rectified flow scheduler, leading to substantial performance improvement, both with and without classifier-free guidance (CFG), as in. Notably,successfully trains tosteps, while the training of FiT-B/2 fails aftersteps, highlighting the stability benefits of the rectified flow scheduler.

We implement LayerNorm for the query and key vectors of attention (andcompared toand, respectively). As
in,generally achieves better FID scores than. Remarkably, we observe thatmaintains a stable training process up tosteps, whilefails to reach this training step. Furthermore,outperformsatsteps in terms of FID score.

As detailed in, we reassign the parameters in FiTv2 to optimize the architecture, comparing the reassigned parameters (FiTv2-B/2) with the original parameters (FiT-B/2) in., which adopts the reassigned parameters, shows consistent FID improvements across all evaluation points compared with.

As shown in,employs a mixed training strategy and exhibits FID performance gains at 1000k, 1500k, and 2000k steps compared to.

As demonstrated in,obtains better results thanat all evaluation points, both with and without CFG.

As reported in, experiments on DiT and SiT both break down aftertraining steps, revealing the instability of their architecture. In contrast, FiTv2 exhibits superior training stability, as well as achieves an approximatelyfaster convergence speed compared with FiT, DiT, and SiT.

SECTION: 
In this part, we adopt the official SiT-XL/2 model attraining steps and our FiTv2-XL/2 model attraining steps to evaluate the extrapolation performance on three out-of-distribution resolutions:,and. Direct extrapolation does not perform well on larger resolutions outside of training distribution. So we conduct a comprehensive benchmarking analysis focused on higher resolution extrapolation.

PI (Position Interpolation) and EI (Embedding Interpolation) are two baseline positional embedding interpolation methods. PI linearly down-scales the inference position coordinates to match the original coordinates. EI resizes the positional embedding with bilinear interpolation.
Following ViT, EI is used for absolute positional embedding.

The implementation of these interpolation techniques strictily follows the implementation in FiT.

The attention scale is defined in, we apply this technique combined with the VisonNTK.

We present inthat our FiTv2-XL/2 shows stable performance when directly extrapolating to larger resolutions. When combined with PI, the extrapolation performance of FiTv2-XL/2 at all three resolutions decreases. When directly combined with YaRN, the FID score onchanges slightly, but the performance onanddescends. Our VisionYaRN solves this dilemma and reduces the FID score byoncompared with YaRN. NTK interpolation method demonstrates stable extrapolation performance but increases the FID score slightly atandresolutions. Our VisionNTK method slightly exceeds the performance of direct extrapolation onandresolutions. When combining VisionNTK and attention scale, the performance significantly surpasses all the other extrapolation methods, with FID improvementon,onandoncompared with direct extrapolation.
In conclusion, our FiTv2-XL/2 model demonstrates robust extrapolation capabilities. Additionally, VisionYaRN and VisonNTK can enhance the generation performance on varied aspect ratios. Furthermore, the combination of VisionNTK with attention scale greatly improves high-resolution extrapolation ability.

However, the official SiT-XL/2 model demonstrates poor extrapolation ability, in. When combined with PI, the FID score achievesatresolution, which still falls behind our FiTv2-XL/2. Atandresolutions, PI and EI interpolation methods cannot improve the extrapolation performance.

SECTION: 
In this part, we compare our FiTv2 model with other baselines.
Our FiTv2-XL model is trained withsteps, consuming onlyof the cost of SiT but with better performance. Furthermore, we scale our FiTv2 up to 3B parameters, which is trained withsteps.
We conduct experiments to evaluate the performance of FiTv2 at three different in-distribution resolutions:,, and.
We show samples from the FiTv2 in Fig, and we compare against some state-of-the-art class-conditional generative models: BigGAN, StyleGAN-XL, MaskGIT, CDM,
Large-DiT,
MaskDiT,
Efficient-DiT,
SimpleDiffusion,
Flag-DiT,
U-ViT, ADM, LDM, MDT, DiT, SiTand our original FiT.
When generating images ofandresolution, we adopt PI on the positional embedding of the DiT and SiT model, as stated in. EI is employed in the positional embedding of U-ViT and MDT models, as they use learnable positional embedding. ADM and LDM can directly synthesize images with resolutions different from the training resolution.
For Large-DiT, we directly generate images of different resolutions as it uses 1D-RoPE as position embedding. For the FiT and FiTv2 models, we directly generate images with different aspect ratios without any extrapolation techniques.

As shown in, FiTv2-XL/2 and FiTv2-3B/2 outperform all prior diffusion models, demonstrating exceptional performance on both the standardbenchmark and varied resolutions. FiTv2-XL/2 reduces the FID by 1.95 compared to the original FiT-XL/2 with the same training steps and a smaller model size. Our FiTv2-XL/2 and FiTv2-3B/2 can be competitive with any other SOTA methods onresolution. FiT-XL/2 and FiTv2-XL/2 achieve superior performance onresolution, decreasing the previous best FID ofachieved by U-ViT-H/2-G toandrespectively. Onresolution, FiTv2-XL/2 and FiTv2-3B/2 show significant superiority, decreasing the previous SOTA FID-50K ofachieved by LDM-4/G toandrespectively. In conclusion, these results suggest that our FiTv2 model has improved performance on standard benchmarks while maintaining the enhanced capability to generate images with arbitrary aspect ratios.

We evaluate our FiTv2-XL/2 on three different out-of-distribution resolutions:,, andand compare against some SOTA class-conditional generative models: U-ViT, ADM, LDM-4, MDT, DiT, SiT, and the original FiT. PI is employed in DiT and SiT, while EI is adopted in U-ViT, as in. U-Net-based methods, such as ADM and LDM-4 can directly generate images with resolution out of distribution. VisionNTK is adopted in FiT, and we combine VisionNTK and attention scale to our FiTv2 model. Note that we do not evaluate the MDT and Large-DiT, as they fall short of generating images whose resolution differs from the training resolution in.

As shown in, FiTv2-XL/2 and FiTv2-3B/2 achieve the best FID-50K, IS, and Precision, on all three resolutions, indicating their outstanding extrapolation ability. In terms of other metrics, such as sFID and Recall, the FiTv2 model demonstrates competitive performance. FiTv2-XL/2 surpasses FiT-XL/2 on all three resolutions with fewer parameters and FLOPs. Compared with the previous SOTA LDM-4, FiTv2-3B/2 gains FID improvement by,andon,andresolutions, respectively.

In, we demonstrate how model performance changes as training steps increase. In, we present the relation of model performance with the training GFLOPs, which is calculated as, following DiT.
As the GFLOPs increase, whether by increasing training steps or enlarging the model size, the FID score and aesthetic quality consistently improve.
Additionally, we observe that with the same training GFLOPs, the larger FiTv2 model always shows better qualitative and quantitative results.
In contrast, the smaller FiTv2 model, even when trained for more steps, fails to reach the performance of larger FiTv2 models trained for fewer steps.
We conclude that scaling model size is a more efficient approach to managing to compute costs, consistent with the findings from DiT.

LDMs with transformer backbones are known to have difficulty in generating images out of training resolution, such as DiT, U-ViT, MDT, SiT, and Large-DiT. More seriously, MDT almost has no ability to generate images beyond the training resolution. We speculate this is because both learnable absolute PE and learnable relative PE are used in MDT. Large-Dit also encounters difficulty in generating images with varied resolutions, as the usage of 1D-RoPE makes it hard to encode spatial structure in images. DiT, U-ViT, and SiT show a certain degree of extrapolation ability and achieve FID scores of,andrespectively atresolution. However, when the aspect ratio is not equal to one, their generation performance drops significantly, as,, andresolutions. Benefiting from the advantage of the local receptive field of the CNN, ADM and LDM show stable performance on resolution extrapolation and generalization ability to various aspect ratios. Our FiTv2 model solves the problem of insufficient extrapolation and generalization capabilities of the transformer in image synthesis. At,,,, andresolutions, FiTv2-XL/2 exceeds the previous SOTA CNN methods, like ADM and LDM.

SECTION: 
We extend the context length to(equivalent to) to conduct high-resolution post-training. As detailed in, we utilize the model pre-trained with the context length, keeping the major parameters frozen. We only update the parameters associated with bias, normalization, image patch embedder, and the final layer, leading to merelyof the overall parameters. Training is conducted using a constant learning rate ofusing AdamW, no weight decay, and a batch size of 256, same with the DiT and SiT training setting. Specifically, we train the FiTv2-XL/2 model forsteps and the FiTv2-3B/2 model forsteps.

The model performance is evaluated on three resolutions:(1:1),(1:2), and(1:3), offering a comprehensive assessment of the image synthesis capability. Our FiTv2 is compared with several state-of-the-art baselines, including DiM,
DiffusionSSm, MaskGiT,
SimpleDiffusion,
DiffiT,
MaskDiT,
Large-DiT,
U-ViT,
ADM,
and DiT. The open-source baseline models are evaluated onandresolutions. Consistent with, PI is adopted in DiT while EI is employed in U-ViT. For ADM and our FiTv2, images with different resolutions are directly generated.

As demonstrated in, FiTv2-XL/2 beats DiT-XL/2 on all three resolutions, with comparable parameters and significantly lower training costs. Remarkably, our FiTv2-XL/2 surpasses DiT-XL/2 on the FID score byatresolution and byatresolution. Furthermore, our FiTv2-3B/2 consistently outperforms all other baseline models on all three resolutions. FiTv2-3B/2 surpasses the previous SOTA Large-DiT-3B and MaskDiT atresolution.
Atandresolutions, FiTv2-3B/2 shows great superiority, exceeding the previous SOTA U-ViT byatresolution on FID and surpassing previous SOTA ADM byatresolution.

SECTION: 
We conduct text-to-image (T2I) generation experiments to further evaluate the effectiveness of our FiTv2 architecture. We use the filtered and recaptioned CC12Msubset from PixelProsefor training, which comprises 8.6 million high-quality images with descriptive captions. The CLIP-Ltext encoder is employed to extract text features, resulting intext tokens, each withdimensions. We use the penultimate hidden representation from the CLIP-L text encoder as the text features following Imagen. We use the SDXL-VAEto extract image latents and the training pipeline follows the class-guided image generation methodology. The procedure aligns with the training recipe outlined in, with our FiTv2-XL/2 model trained forsteps. Additionally, a baseline SiT-XL/2 model is trained for the samesteps for comparative analysis. To ensure a fair comparison, SiT-XL/2 processes the text features and image latents in the same manner as our FiTv2, detailed in.

We evaluate our FiTv2-XL/2 and SiT-XL/2 for T2I generation on the standard MS-COCO benchmark atresolution. Consistent with previous literature, we randomly sampleprompts from the MS-COCO validation set and generate images according to those prompts to compute the FID score and CLIP-L score. The Pareto curve is shown inwith classifier-free guidance factor of. With the same training steps, our FiTv2 achieves stronger results both on FID and CLIP scores, attaining an optimal FID ofand an optimal CLIP score ofat cfg=4.0. In comparison, the SiT model reaches an optimal FID ofand an optimal CLIP score ofat CFG=4.0. Combined with the qualitative results in, it is evident that our FiTv2 model beats the SiT model on T2I architecture.

SECTION: 
In this work, we aim to contribute to the ongoing research on flexible generating arbitrary resolutions and aspect ratios.
We propose an Enhanced Flexible Vision Transformer (FiTv2) for the diffusion model, a refined transformer architecture with a flexible training pipeline specifically designed for generating images with arbitrary resolutions and aspect ratios.
FiTv2 surpass all previous models, whether transformer-based or CNN-based, across various resolutions.
With our resolution extrapolation method, VisionNTK, and attention scale, the performance of FiTv2 has been significantly enhanced further.
We also scale the FiTv2 tobillion to investigate the scalability of our model.
Extensive experiments on class-guided image generation, flexible image generation, high-resolution image generation, and text-to-image generation demonstrate the effectiveness of our FiTv2. We hope our work can inspire insights towards designing more powerful diffusion transformer models.

SECTION: Acknowledgment
This work is supported by the Shanghai Artificial Intelligence Laboratory.

SECTION: References