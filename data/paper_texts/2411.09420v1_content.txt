SECTION: SAG-ViT: A Scale-Aware, High-Fidelity Patching Approach with Graph Attention for Vision Transformers

Image classification is a computer vision task where a model analyzes an image to categorize it into a specific label. Vision Transformers (ViT) improve this task by leveraging self-attention to capture complex patterns and long-range relationships between image patches. However, a key challenge for ViTs is efficiently incorporating multi-scale feature representations, which is inherent in CNNs through their hierarchical structure. In this paper, we introduce the Scale-Aware Graph Attention Vision Transformer (SAG-ViT), a novel framework that addresses this challenge by integrating multi-scale features. Using EfficientNet as a backbone, the model extracts multi-scale feature maps, which are divided into patches to preserve semantic information. These patches are organized into a graph based on spatial and feature similarities, with a Graph Attention Network (GAT) refining the node embeddings. Finally, a Transformer encoder captures long-range dependencies and complex interactions. The SAG-ViT is evaluated on benchmark datasets, demonstrating its effectiveness in enhancing image classification performance.

SECTION: 1Introduction

The field of image classification has experienced significant advancements with the introduction of deep learning architectures. CNNs have long been the foundation for image classification tasks due to their proficiency in capturing local spatial hierarchies through convolutional operations[9]. However, their inherent limitations in modeling long-range dependencies restrict their ability to fully exploit global contextual information within images[12]. The introduction of Vision Transformers (ViT)[18,4]has opened new avenues by leveraging self-attention mechanisms to model global relationships within images. ViTs treat images as sequences of patches (tokens) and have demonstrated competitive performance compared to traditional CNNs. Despite their success, ViTs often require large-scale datasets for effective training and may overlook fine-grained local details due to their fixed-size patch tokenization[17].

Recent research has highlighted the importance of multi-scale feature representations in enhancing ViTs’ performance across various vision tasks[1]. Multi-scale approaches enable models to capture objects and patterns of varying sizes, providing a more comprehensive understanding of the image content. While CNNs inherently capture multi-scale features through hierarchical layers, integrating this capability efficiently into Transformer-based models remains a challenge.

To handle this challenge, we propose a novel Transformer-based framework called Scale-Aware Graph Attention Vision Transformer (SAG-ViT). Our model begins by extracting rich, multi-scale feature maps from input images using a pre-trained EfficientNet backbone[16]. We then divide these feature maps into patches, preserving high-level semantic information and reducing information loss compared to raw image patching. We then construct graphs where each node represents a feature map patch, and edges are established based on spatial adjacency and feature similarity using a k-connectivity scheme. This graph captures both local and global relationships among image regions. A Graph Attention Network (GAT)[19,24]processes the graph, dynamically focusing on the most relevant patches. The enriched node embeddings are then passed through a Transformer encoder, which captures long-range dependencies and complex interactions.

Our contributions are summarized as follows:

We introduce a patching mechanism that operates on CNN-derived feature maps, retaining rich semantic information and efficiently capturing multi-scale features.

A k-connectivity and similarity-based edge weighting scheme is developed in the proposed Transformer architecture to construct graphs that model intricate spatial relationships between patches.

We employ a GAT Network to process the information-rich graph embeddings to effectively model both local and global dependencies within images.

We validate our method on multiple benchmark datasets across different domains, demonstrating higher performance compared to other transformer-based approaches.

The remainder of this paper is organized as follows: Section 2 reviews related work on graph transformers, attention mechanisms, multi-scale feature embedding, and their integration in image classification. Section 3 details our proposed method, including the architecture and graph construction process. Section 4 presents the experimental setup, datasets, and evaluation metrics. Section 5 discusses the results, and Section 6 concludes the paper.

SECTION: 2Literature Survey

In this section, we review relevant literature on vision transformers, multi-scale feature representation, and graph neural networks for image classification.

SECTION: 2.1Vision Transformers for Image Classification

Transformer-based models have gained significant attention in computer vision, initially popularized by the Vision Transformer (ViT), which treats images as sequences of patches and uses self-attention to capture global dependencies, achieving competitive results with CNNs for image classification[24]. However, ViT models often require large datasets and substantial computational resources, limiting their accessibility.

To improve data efficiency, DeiT leverages distillation and data augmentation, enabling ViTs to perform well on smaller datasets[10]. T2T-ViT[25]introduces a Tokens-to-Token transformation to better capture local structures, addressing ViT’s limitation of naive tokenization. The Perceiver model uses an asymmetric attention mechanism to distill large inputs into a compact latent space, allowing it to scale effectively for high-dimensional data[8]. Similarly, PVT and CvT incorporate pyramid-like structures into transformers, merging CNN-like multi-scale processing with transformer advantages for richer feature extraction[23].

The Swin Transformer introduces a shifting window approach to self-attention, efficiently capturing both local and global contexts while maintaining manageable complexity, especially for dense tasks like segmentation and detection[11]. These models highlight a growing trend toward integrating multi-scale representations to improve vision transformers’ ability to capture both fine-grained details and long-range dependencies.

SECTION: 2.2Multi-Scale Feature Representation

Multi-scale feature representations are critical for recognizing objects and patterns at varying scales[1]. CNNs naturally capture multi-scale features through their hierarchical layers and receptive fields[9]. Techniques such as feature pyramid networks[10]and multi-branch architectures[2]have been proposed to enhance multi-scale learning in CNNs.

In the context of transformers, incorporating multi-scale features remains challenging due to the fixed-size patch tokenization. CrossViT[1]introduces a dual-branch transformer architecture that processes image patches of different sizes in separate branches, fusing them using cross-attention mechanisms. This approach effectively captures both fine-grained details and global context.

SECTION: 2.3Graph Neural Networks for Image Classification

Graph Neural Networks have gained attention for their ability to model relational data. In image classification, representing images as graphs allows for capturing spatial relationships between different regions[24]. Nodes can represent super pixels or patches, and edges encode similarities or spatial connections. Constructing graphs directly from raw images can lead to information loss due to the reduction in spatial resolution[26]. By constructing graphs from CNN-derived feature maps, richer semantic information can be retained[5]. This approach enhances the modeling of complex spatial dependencies crucial for accurate classification.

Graph Attention Networks extend the concept of attention mechanisms to graph-structured data[19]. GATs compute attention coefficients for neighboring nodes, allowing the network to focus on the most relevant connections. This dynamic weighting improves the learning of node representations by emphasizing important relationships. Incorporating GATs in image classification enables the modeling of both local and non-local dependencies[22]. When combined with multi-scale feature representations, GATs can effectively capture intricate patterns within images.

SECTION: 2.4Hybrid Models

Recent studies suggest that combining transformer and convolutional layers into a hybrid architecture can harness the strengths of both approaches. BoTNet[15]modifies self-attention in the final three blocks of ResNet to integrate both architectures. The CMT[7]block incorporates depthwise convolutional layers for local feature extraction, alongside a lightweight transformer block. CvT[11]places pointwise and depthwise convolutions before the self-attention mechanism to enhance performance. LeViT[6]replaces the patch embedding block with a convolutional stem, enabling faster inference for image classification. MobileViT[13]combines Transformer blocks with the MobileNetV2[14]block to create a lightweight vision transformer. Mobile-Former[3]bridges CNNs and transformers in a bidirectional manner to capitalize on both global and local features.

SECTION: 3Method:SAG-ViT

In this section, we detail our proposed approach to enhance transformer performance for image classification through a multiscale feature embedding and high-fidelity graph attention-based patching. During graph construction in graph transformers, spatial hierarchies are often lost or insufficiently represented, especially as redundant or less relevant areas dilute the image’s contextual representation. To overcome this limitation, we propose a novel framework that captures both local and global dependencies while preserving rich semantic information. Specifically, we begin by outlining our high-fidelity feature map patching strategy (§3.1). We then detail the graph construction methodology based on-connectivity and feature similarity (§3.2). Finally, we explain the integration of Graph Attention Networks with Transformer encoders (§3.3). Figure1illustrates the network architecture of our proposed Scale-Aware Vision Transformer with Graph Attention (SAG-ViT).

SECTION: 3.1High-Fidelity Feature Map Patching

We initiate the processing pipeline by extracting high-fidelity patches from feature maps generated by a lightweight convolutional backbone. By operating on feature maps rather than raw images, we retain higher-level semantic information. We process the input imagethrough a deep CNN to exploit its compound multiscale feature scaling for receptive fields and efficient convolution paths, yielding a feature map, where,, anddenotes the depth of the feature channels with stride.

To preserve detailed and multi-scale semantic information, we partition the feature mapinto non-overlapping patches, whereis the spatial dimension of each patch. Formally, the patch extraction is defined as:

for alland.

This operation can be represented using an unfolding operator:

for alland.

where. Each patchis then vectorized into a feature vectorby flattening the spatial and channel dimensions:

This results in a collection of patch vectors:

By extracting patches directly from the feature map, we leverage the high-level abstractions learned by the CNN. This approach ensures that each patchencapsulates rich semantic information, capturing both local patterns and contextual relationships within the image. Moreover, extracting patches from the reduced spatial dimensionsleads to fewer patches, decreasing computational complexity while maintaining essential information.

The vectorized patchesserve as nodes in the subsequent graph construction phase. The high-dimensional feature vectors facilitate the capture of intricate relationships between patches when constructing edges based on similarity measures. Additionally, the non-overlapping nature of patch extraction ensures that each patch maintains its spatial locality within the feature map, preserving the inherent spatial structure essential for accurate image classification.

This mathematical formulation ensures that the patch extraction process is both systematic and scalable, facilitating efficient downstream processing in the graph-based classification pipeline.

SECTION: 3.2Graph Construction Usingk-Connectivity and Similarity-Based Edges

Once the patchesare extracted, we construct a graphto model the spatial and feature-based relationships among them. Here,represents the set of nodes corresponding to patches, anddenotes the set of edges connecting these nodes. Each nodeis associated with a feature vector, where each patch of sizeis vectorized into a-dimensional feature vector. After extracting all patches, we organize them into a matrix

whereis the number of patches (nodes) in the graph.

Next, we define the edgesbased on-connectivity and feature similarity. For each patch, we consider its neighboring patches, which are spatially adjacent to it within the feature map. A patchis connected to its neighboring patches, whererepresents the set of neighbors of patch. The neighborhoodis determined by the spatial adjacency of patches, considering a fixed local window sizearound each patch. The adjacency matrixis defined as:

wheredenotes the set of the-nearest spatial neighbors of node, andis a hyperparameter controlling the decay of the similarity function. To formalize the-connectivity, we define the neighborhood functionbased on the Euclidean distance in the spatial grid, wheremaps nodeto its spatial coordinates:

SECTION: 3.3Integration of Graph Attention Networks (GAT) with Transformer Encoders

After constructing the graph, we employ a Graph Attention Network (GAT) to process the node features and capture fine-grained dependencies among patches. Integrating GAT with transformer encoders facilitates the modeling of both local and global interactions, enhancing the discriminative power of the feature representations. The attention mechanism in GAT dynamically assigns weights to neighboring nodes to emphasize more relevant connections.

For a given node, the attention coefficientwith its neighboris computed as:

whereis a learnable linear transformation matrix,is a learnable attention vector, andrepresents the set of neighbors of node.

The updated featurefor nodeis obtained by aggregating the transformed features of its neighbors weighted by the attention coefficients using a non-linear activation function (ELU):

To capture diverse relational patterns and stabilize the learning process, we employ multi-head attention. Forattention heads, the concatenated output for weight matricesand attention coefficientsfor the-th head is given by:

wheredenotes a non-linear activation function (e.g., ELU), andrepresents the concatenation operation across the attention heads.

These node embeddingsproduced by the GAT are subsequently fed into a Transformer Encoder to model high-level interactions and long-range dependencies across all patches. Before integration, we apply positional encodingto each node embedding to retain spatial information:

whererepresents the positional encodings.

The Transformer encoder processes the sequence of node embeddings using multi-head self-attention mechanisms. For a query vector, a key vector, and a value vector, withbeing learnable weight matrices for the-th head, the self-attention operation for each headis defined as:

The combination of GAT and Transformer encoders can be formalized as a two-stage feature transformation:

We use this hierarchical processing to ensure that the model first refines patch embeddings through graph-based attention, captures localized relationships, and then leverages Transformer-based self-attention to integrate these refined embeddings into a cohesive global representation. After the Transformer encoder, we apply a global mean pooling operation to aggregate the sequence of embeddings into a single vector:

Finally, we pass this pooled representation through a Multi-Layer Perceptron (MLP) to produce the final classification logits:

whereandare the weight matrix and bias vector of the output layer, respectively, andis the number of target classes.

SECTION: 3.4Ablation Study

To rigorously evaluate the contributions of each component in our proposed architecture, we conducted a comprehensive ablation study. This analysis aims to discern the individual impact of the EfficientNet backbone, the Graph Attention Network (GAT), and the Transformer encoder on the model’s overall performance. By systematically removing or altering components, we can quantify their significance and validate the theoretical underpinnings of our design choices.

We designed three ablation experiments on the CIFAR-10 dataset to isolate the effects of each component:

Backbone + GAT (No Transformer):In this configuration, we exclude the Transformer encoder, allowing us to assess the role of the Transformer in capturing global dependencies. The model processes the feature embeddings extracted by the EfficientNet backbone through the GAT, generating class predictions directly from the aggregated node representations.

Backbone + Transformer (No GAT):Here, we omit the GAT to evaluate its contribution in modeling local dependencies and refining node features. The feature embeddings from the backbone are fed into the Transformer encoder, which attempts to learn both local and global relationships without the explicit attention mechanism provided by the GAT.

GAT + Transformer (No Backbone):In this scenario, we remove the EfficientNet backbone to determine its impact on feature representation. Randomly initialized embeddings are used as input to the GAT and Transformer, highlighting the importance of high-quality feature extraction.

SECTION: 4Results

In this section, we present a comprehensive evaluation of our proposed model across five diverse benchmark datasets: CIFAR-10, GTSRB, NCT-CRC-HE-100K, NWPU-RESISC45, and PlantVillage. These datasets encompass a wide range of domains, including natural images, traffic sign recognition, histopathological images, remote sensing data, and agricultural imagery. The diversity of these datasets allows us to thoroughly assess the effectiveness and generalization capability of our model across different types of image data.

SECTION: 4.1Overall Performance

Our proposed model demonstrates superior performance compared to state-of-the-art architectures across all evaluated datasets. Table1summarizes the F1 scores achieved by our model and various baseline models utilizing different backbones.

Analyzing the results, our proposed model consistently outperforms the baseline models across all datasets. On CIFAR-10, our model achieves an F1 score of 0.9574, surpassing the next best model (ResNet-based within our architecture) by approximately 4.02%. This significant improvement underscores the effectiveness of integrating EfficientNet as the backbone in our model. EfficientNet’s compound scaling strategy optimizes network depth, width, and resolution, providing richer feature embeddings that enhance the model’s ability to capture intricate patterns in natural images when processed through our graph-based approach.

On the GTSRB dataset, which involves recognizing traffic signs under various challenging conditions, our model attains an F1 score of 0.9958. This is a notable improvement over the DenseNet201-based variant, which achieves 0.9862. The 0.96% increase, though seemingly modest due to the high baseline performance, demonstrates our model’s superior ability to capture subtle variations in traffic signs, crucial for real-world traffic sign recognition tasks.

For the NCT-CRC-HE-100K dataset, consisting of histopathological images for colorectal cancer classification, our model achieves an F1 score of 0.9861, outperforming the ResNet-based variant’s score of 0.9478 by approximately 3.83%. This substantial improvement indicates that the EfficientNet backbone, combined with our graph-based processing, effectively captures complex tissue structures, enhancing the model’s discriminative power in medical image analysis.

On the NWPU-RESISC45 dataset, which includes remote sensing images from various land-use scenes, our model achieves an F1 score of 0.9549, outperforming the ResNet-based variant by 4.46%. This result demonstrates the model’s ability to capture spatial relationships and patterns inherent in remote sensing data more effectively than other backbones within our architecture.

Lastly, on the PlantVillage dataset, our model records an F1 score of 0.9772, significantly higher than the ResNet-based variant’s score of 0.8905, marking an improvement of approximately 8.66%. This considerable enhancement underscores the effectiveness of our model in agricultural imagery, particularly in detecting and classifying plant diseases where subtle visual cues are critical.

We evaluated our model’s performance on the underwater trash dataset[20], benchmarking it against state-of-the-art algorithms (excluding backbone models) such as YOLOv8, RCNN, Fast-RCNN, and Mask-RCNN. Our model consistently performed better with a validation F1 of 0.96, exceeding the benchmark results of these models[21].

Comparing our model with standalone Vision Transformers (ViT-S and ViT-L), which do not incorporate our graph-based enhancements, we observe that while ViT models perform competitively on some datasets, they generally lag behind our proposed model. For instance, on CIFAR-10, ViT-L achieves an F1 score of 0.8637, which is 9.35% lower than our model’s performance. This comparison highlights the advantage of our approach in integrating EfficientNet for feature extraction with graph attention mechanisms and Transformer encoding, providing a more comprehensive understanding of the data. Figure2graphically compares the F1 Scores of all models across the five datasets, demonstrating the proposed model’s superior performance compared to existing architectures.

The consistent superiority of our proposed model across diverse datasets can be attributed to several key factors:

Efficient Feature Extraction:The EfficientNet backbone within our architecture provides high-quality feature embeddings due to its balanced scaling of network depth, width, and resolution. This results in richer and more discriminative features compared to other CNN backbones.

Graph-Based Representation:By constructing k-connectivity graphs from feature map patches, our model effectively models spatial and semantic relationships between image regions, capturing both local and global dependencies.

Attention Mechanisms:The Graph Attention Network assigns adaptive weights to neighboring nodes, emphasizing relevant regions and enhancing local feature representation. The Transformer encoder further captures long-range dependencies and global context, which is particularly beneficial for complex images where global interactions are crucial for accurate classification.

SECTION: 4.2Hardware Efficiency

We also evaluated the hardware efficiency of our proposed model in terms of RAM and GPU VRAM usage. Table2details the resource consumption of each model across the different datasets.

Our proposed model demonstrates competitive resource utilization, especially considering its superior performance. On CIFAR-10, our model uses 7.24% RAM, which is lower than several other variants using different backbones, such as the VGG16-based version that consumes 11.5% RAM. This indicates that incorporating EfficientNet in our architecture not only enhances performance but also improves hardware efficiency.

Regarding GPU VRAM usage, our model maintains moderate consumption. For example, on GTSRB, it uses 36.38% GPU VRAM, which is slightly higher than some CNN-based variants but significantly lower than the ViT-L model’s 81.87% VRAM usage on CIFAR-10. Despite the additional components of the GAT and Transformer encoder, the efficient feature extraction of EfficientNet and the sparsity of the k-connectivity graphs contribute to keeping resource usage reasonable within our architecture. Figure3illustrates the RAM and GPU VRAM usage of various models, highlighting the proposed model’s resource efficiency.

SECTION: 4.3Ablation Study

To evaluate the contribution of each component in our proposed model, we conducted an ablation study on the CIFAR-10 dataset. The results are summarized in Table3.

When the model includes the EfficientNet backbone with the GAT but without the Transformer encoder, the F1 score drops to 0.7785. This significant decrease underscores the crucial role of the Transformer encoder in capturing global dependencies and enhancing classification accuracy. The self-attention mechanism in the Transformer allows the model to weigh the importance of all patches relative to each other, facilitating a holistic understanding of the image.

Conversely, using the EfficientNet backbone with the Transformer encoder but without the GAT results in an F1 score of 0.7593. This emphasizes the importance of the GAT in refining local feature representations before global processing. The GAT enhances node features by aggregating information from immediate neighbors, effectively capturing local structural information essential for accurate classification.

When the model comprises the GAT and Transformer encoder without the EfficientNet backbone, the F1 score drops drastically to 0.5032. This significant decline highlights the importance of the EfficientNet backbone in providing rich and discriminative feature embeddings necessary for effective graph construction and subsequent processing. The RAM and GPU VRAM usage of various models are illustrated in Figure4.

These observations confirm that each component of our proposed architecture is essential and contributes uniquely to the model’s overall performance. The EfficientNet backbone generates high-quality feature embeddings; the GAT captures local dependencies through attention mechanisms; and the Transformer encoder models global relationships, enabling the model to understand complex patterns that span different regions of the image.

SECTION: 4.4Discussion

The results validate our hypothesis that integrating efficiently scaled feature embeddings with graph-based attention mechanisms and Transformer encoders significantly enhances model performance across diverse datasets. The EfficientNet backbone within our architecture provides superior feature representations, which, when used in our graph construction, enhance the model’s ability to capture both local and global dependencies effectively.

Our comparisons are designed to showcase the critical role that EfficientNet plays within our proposed model. The substantial improvements over models with other backbones highlight that the enhancements result from the synergistic integration of EfficientNet with our graph-based approach, rather than from the backbone alone. This integration allows for richer feature representations that, when processed through the GAT and Transformer encoder, lead to improved classification accuracy.

The ablation study confirms that the removal of any component leads to a significant drop in performance, establishing that the improvements are due to the cohesive integration of these elements within our architecture. By quantitatively demonstrating the impact of each component, we validate the architectural choices grounded in principles of deep learning, graph theory, and attention mechanisms.

Moreover, the inclusion of diverse datasets such as GTSRB, NCT-CRC-HE-100K, NWPU-RESISC45, and PlantVillage demonstrates the robustness and generalization ability of our model across different domains. These datasets present various challenges, including fine-grained classification, medical image analysis, remote sensing, and agricultural disease detection. Our model’s consistent superiority across these datasets emphasizes its versatility and effectiveness in handling complex and varied image data.

In conclusion, our proposed model, integrating EfficientNet as the backbone within a graph-based framework enhanced by attention mechanisms, significantly outperforms existing models. The improvements stem from the unique combination of high-quality feature extraction, graph-based representation of spatial relationships, attention mechanisms for local dependencies, and Transformer encoding for global context. This holistic approach ensures that our model effectively captures and utilizes the rich semantic information necessary for accurate image classification across diverse datasets.

SECTION: 5Conclusion

This paper presents the Scale-Aware Graph Attention Vision Transformer (SAG-ViT), a novel framework designed to address the challenge of multi-scale feature representation in Vision Transformers. By utilizing EfficientNet for feature extraction and organizing image patches into a graph, SAG-ViT effectively captures both local and global relationships in images. The incorporation of a Graph Attention Network (GAT) refines the node embeddings, while the Transformer encoder captures long-range dependencies and complex interactions. Experimental evaluations on benchmark datasets, including CIFAR-10, GTSRB, NCT-CRC-HE-100K, NWPU-RESISC45, and PlantVillage, demonstrate the model’s effectiveness, showing significant improvements in image classification performance. Additionally, an ablation study provides insights into the importance of each component in the SAG-ViT framework, helping to understand their individual contributions to the overall performance. This work highlights the potential of integrating multi-scale features and graph-based attention mechanisms to enhance the capabilities of Transformer-based models in computer vision.

SECTION: References