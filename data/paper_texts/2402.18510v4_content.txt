SECTION: RNNs are not Transformers (Yet):The Key Bottleneck on
This paper investigates the gap in representation powers of Recurrent Neural Networks (RNNs) and Transformers in the context of solving algorithmic problems. We focus on understanding whether RNNs, known for their memory efficiency in handling long sequences, can match the performance of Transformers, particularly when enhanced with Chain-of-Thought (CoT) prompting. Our theoretical analysis reveals that CoT improves RNNs but is insufficient to close the gap with Transformers. A key bottleneck lies in the inability of RNNs to perfectly retrieve information from the context, even with CoT:
for several tasks that explicitly or implicitly require this capability, such as associative recall and determining if a graph is a tree, we prove that RNNs are not expressive enough to solve the tasks while Transformers can solve them with ease.
Conversely, we prove that adopting techniques to enhance the in-context retrieval capability of RNNs, including Retrieval-Augmented Generation (RAG) and adding a single Transformer layer, can elevate RNNs to be capable of solving all polynomial-time solvable problems with CoT, hence closing the representation gap with Transformers.

SECTION: Introduction
Transformershave become the dominant choice of the backbone for Large Language Models (LLMs). The core component of Transformers is self-attention modules, which allow the model to route information densely across the entire sequence. However,
this design leads to high inference costs for long sequences, including a memory cost linear in the sequence length to maintain intermediate attention keys and values for each token, and a time cost quadratic in the sequence length to compute the attention score for each pair of tokens.

Recently, Recurrent Neural Networks (RNNs) have become an increasingly popular choice in sequence modeling tasks due to their ability to maintain a memory size constant in sequence length during inference, thus being more memory efficient than Transformers.showed that Linear Transformers (Transformers with linear attention) can be expressed as RNNs.took a different path to design RNNs by structuring latent states as State Space Models (SSMs) from control theory. These ideas have led to a series of development of modern RNNs, including RWKV, RetNet, and Mamba. Most notably, Mamba can achieve competitive performance with Transformers on several sequence modeling tasks with linear time and constant memory in sequence length.

The rise of these modern RNNs has led to an interest in understanding their limitations.
A recent work byshowed that a broad family of RNNs, input-independent gating SSMs, are empirically inferior to Transformers in a task that has a long history in artificial intelligence,(AR):
Given a series of key-value pairs as a string, the model is required to recall the value given a key.
On the theory side,anddemonstrated that constant-memory RNNs do not have sufficient representation power to solve the tasks of averaging a given subset of input vectors (-sparse averaging) and repeating the input sequence (copying), respectively, while there exist shallow Transformers that can solve these tasks.

However, the above results do not exclude the possibility that enhancing RNNs with additional prompting techniques or minor architectural changes could close the gap with Transformers.
In fact, Transformers themselves are not perfect either and sometimes need additional techniques at inference time to perform well.
For instance, Transformers may struggle with mathematical and algorithmic reasoning problems if they are forced to produce the correct answer immediately after processing the input sequence.
But with(CoT) prompting applied, a prompting technique that guides the model to generate a series of intermediate tokens before arriving at the final answer, their performance can be significantly improved.explained the success of CoT from the perspective of representation power: Transformers alone do not have sufficient representation power to solve problems beyond a certain circuit complexity class (), but with CoT, they can even simulate any polynomial-time Turing machine.

The effectiveness of CoT on Transformers naturally leads to the following question:

This paper examines potential ways to close the gap in the representation powers of RNNs and Transformers (with softmax attention) on algorithmic problems.
Through a series of lower and upper bound results, we show that CoT improves the representation power of RNNs, but to close the gap with Transformers, CoT alone is not enough
to overcome a key bottleneck of RNNs: their inability to retrieve information from the context, which we callfor short.

We further illustrate that addressing this in-context retrieval bottleneck is sufficient to close this gap: RNNs can solve all polynomial-time solvable problems if adopting techniques to enhance the in-context retrieval capability, including involving(RAG) and using, such as appending a single Transformer layer.

Our main contributions are listed as follows:

()

On the positive side, we prove that CoT makes RNNs strictly more expressive under mild assumptions from circuit complexity ().

On the negative side, we show that adopting CoT is not enough to close the representation gap between RNNs and Transformers: the memory efficiency of RNNs fundamentally limits their ability to perform in-context retrieval, even with CoT.
This point is made concrete by proving that RNNs with CoT cannot solve a set of fundamental algorithmic problems that directly ask for in-context retrieval, including associative recall.

We further exemplify that in-context retrieval can be implicitly required in tasks that appear unrelated, by proving the inability of RNNs to solve the classic problem of determining whether a graph is a tree ().

On the other hand, we prove that Transformers have the representation power to solve many of the above tasks with ease, including. Moreover, Transformers with CoT can even simulate RNNs with CoT efficiently, with only a small multiplicative factor in the number of parameters.

Our negative results hold for a wide range of RNN architectures, including the aforementioned Mamba, RWKV, and even Linear Transformers. Technically, this is because RNNs are so memory efficient that they can trigger streaming lower bounds, especially for problems that require in-context retrieval.

()

We prove that allowing RNNs to invoke function calls to perform a primitive of in-context retrieval based on regular expression is sufficient to boost their representation power to solve all polynomial-time solvable problems with CoT, hence closing the representation gap.

As one layer of the Transformer is sufficient to perform many in-context retrieval operations, mixing some Transformer layers in RNNs should also narrow the representation gap. We prove that a minimal possible change in the RNN architecture can just work: adding one Transformer layer at the end of the RNN architecture is sufficient to close the representation gap.

Our positive results showing that enhancing in-context retrieval can improve RNNsâ€™ representation power hold for vanilla Linear RNNs, and can be easily extended to more complex architectures.
The intuition behind these results is that RNN can focus on the local reasoning steps and use the in-context retrieval module to adaptively fetch the relevant information from the context.

We validate our theoretical findings through synthetic and natural language experiments on IsTree and HotPot-QA, confirming that while CoT alone cannot close the performance gap between RNNs and Transformers, the proposed two solutions effectively narrow this gap. ()

We believe these results could provide valuable insights into architecture designs of LLMs: RNNs alone can suffer from many fundamental limitations in representation power, even with CoT; on the other hand, it is promising to explore strategies to enhance the in-context retrieval capability of RNNs with little overhead, such as using a hybrid architecture that mixes in one or more Transformer layers.

SECTION: Related Works
There has been a recent surge of interest in state space machines and (kernalized) linear transformers, which are a class of models that combine the parallelizability of the Transformer with the memory efficiency of the RNN. These models can process both a sequential and a recurrent form, and can use the former for fast parallelizable training and the latter for memory-efficient inference.
However, these models are still empirically inferior to the Transformer in terms of performance. Our work investigates the reasons behind this gap and proposes to close it by enhancing the in-context retrieval capability.

Chain of thoughtis an augmentation to the Transformer, that allows it to solve more complex reasoning tasks by generating a reasoning process before outputting the answer. It has been shown that Transformers with CoT provably have more expressive power than the original Transformer without CoT. However, the expressive power of RNNs with CoT has not yet been systematically studied. Theorem F.1 inshows that RNN cannot output a particular format of CoT for evaluating arithmetic expressions and solving linear equations while Transformers with the same amount of parameters can. Concurrent workdiscovers that linear Transformers, a special class of RNNs, are not able to solve some dynamic programming problems with CoT, unless the number of parameters grows with the length of the input. One high-level message our work conveys is similar to theirs: RNNs have limited representation power to perform reasoning with CoT.
However, we show that such limitation is not specific to the output format or architecture and apply tools from streaming complexity to prove lower bounds on a broader range of tasks and memory-efficient architectures.

Our lower bound leverages the technique in streaming algorithms. Streaming algorithms are algorithms that take constant (typically just 1) pass over the input and use sublinear space, hence including RNNs with fixed state space as a special case. Works in streaming algorithms date back to the 1980sand have been formalized and popularized in the 1990sdue to the need to process large data streams. 
The study of streaming algorithm is closely connected with the concept of. The communication complexity of an algorithm is defined by the amount of communication cost required when the algorithm is distributed, and all streaming algorithms can be viewed as distributed algorithms with sublinear communication complexity, whose communication content is the internal state of the algorithm. The lower bound in our work is a direct application of this observation to the study of RNNs and we mainly consider the lower bounds for (1) indexing the inputand (2) determining whether the input is a tree.

Our work proposes to use retrieval augmentation to close the representation gap between RNNs and Transformers. This is consistent with the recent trend of retrieval augmented generation. Empirically, retrieval augmented generation has been shown to improve the performance of recurrent models in various tasksand our work provides a theoretical foundation for this phenomenon. Our work also shows that an attention layer can be used to simulate the retrieval process, which is consistent with the finding that attention can improve the performance of RNNs. It has also been shown empirically that attention can be used to simulate complex retrieval process.

A line of works focused on the comparison between RNNs and Transformers in terms of recognizing or generating formal languages. These works show that the lack of recurrent structure in Transformers makes them fail to recognize some formal languages that RNNs can recognize. However,show that such limitation can be mitigated when we consider bounded length of input or bounded grammar depth. Our work differs from these works in that we consider the expressive power of RNNs and Transformers with CoT and show that in this case, the gap between RNNs and Transformers is one-sidedÂ ().

Prior workhas shown that input-independent gating SSMs are inferior to Transformers in the task called. The task requires the model to recall a previously seen pattern given a partial input. They show that input-dependent gating SSMs have better performance in associative recall and also propose a hybrid architecture that combines input-independent state space machines with attention to achieve better performance. Our work differs from this work in the following ways: (1) Our work studies associative recall from a theoretical perspective and proves formal lower bounds on the memory size of RNNs necessary for solving associative recall and other retrieval tasks; (2) We also study hybrid architectures but we provide a proof that appending a single Transformer layer to RNNs can make them expressive enough; (3) Our theory applies to not only input-independent gating SSMs but also all RNNs with-bit memory.

Prior workproves a representation gap between RNNs and Transformers in repeating a long sequence, which can be seen as a retrieval task. They show that RNNs have difficulty performing the task due to their limited memory.
Our work further proves that RNNs are limited in solving many other retrieval tasks, even with CoT. Technically, a key ingredient in their proof is a counting argument on the output sequence to show a limited memory size is not enough to produce too many different output sequences, but our proof can handle retrieval tasks that only require outputting a single token.

Notably,apply communication complexity to prove circuit size or memory size lower bounds for RNNs and Transformers on the task of sparse averaging.extend this technique to another task called hop, a generalization of the associative recall task. Our technique is similar to theirs since our proof is also based on communication complexity. But we consider a broader range of tasks including seemingly irrelevant reasoning tasks such as IsTree, and further explore various ways to close the representation gap.

Another line of worksstudies the universal approximation power of RNNs. They show that the upper bound of the approximation power of linear RNNs will be constrained by the dimension of the hidden states. Their works on the high level are consistent with our findings but are not directly comparable because we are considering finite precision compute models with the assistance of CoT or.

SECTION: Preliminaries
We introduce the definitions that are necessary for understanding our results
and defer other definitions to.

A vocabularyis a finite set of tokens.
A word embedding for a tokenis a vectorthat represents the token, and a position embedding for the-th token in a sequence is a vectorthat represents the position.
Given a sequence of tokens, an embedding functionmaps each token to a vector inby mixing word and position embeddings, resulting in a sequence of vectors.
To ease our comparison between RNNs and Transformers, in this paper, we assume fixed word and position embeddings and do not learn them during training.
Seefor the formal definitions. This is common practice and has been used in many previous works studying the theory of Transformers.

Many of the problems we study involve natural numbers up to, where the input sequence length is linear in.
For simplicity, we assume the vocabulary containsand the word embedding foris defined as, whereis the first coordinate vector.
But in practice, the vocabulary size does not increase withand numbers may be tokenized into a few tokens according to their decimal representations.
We note that our results can be easily extended to this more practical case since our lower bounds do not rely on the specific form of the vocabulary and embeddings and for the upper bounds, our embedding can be easily represented by a few RNN or Transformer layers.

We will consider computation models with fixed numerical precision in this paper. We will useto denote the precision of the number of bits to represent real numbers and useto denote the set of all real numbers that can be represented by-bit floating point numbers. We defer the details to. We will assumein this paper and state the constant explicitly when necessary. This is a common assumption when studying the finite precision neural networks.

We useandto denote the set of all finite sequences and all non-empty finite sequences of tokens in, respectively.
We study language models that can predict the next token given a prefix of tokens.
For this, we define a language model (LM) as a functionthat maps a non-empty sequence to a probability distribution over the next token, whereis the probability simplex over.
We specifically study the case where the language model is realized by deep neural networks: first map the input sequenceinto a sequence of embeddings, and then apply a neural network, such as a Transformer or RNN, to process the embeddings and output the probability distribution. We would call a series of parameterized models with increasing input size afamilyof models.

We will first define the Transformer architecture used in the theoretical analysis in this paper.

Letbe the input matrix, whereis the sequence length. The output of a Transformer blockis defined as:

whereis a column-wise ReGLU feed-forward networkwith widthand output dimension,is the scaled dot-product attention,is the column-wise softmax function,,,are the learnable parameters andis the number of heads, andis a mask to prevent the attention from attending to future tokens.

In the context of language modeling, given a sequence of tokens,
a Transformeris defined as:

Letbe the tokenized input sequence, the output of a Transformer is defined as:

whereis the column-wise softmax function,is the-th Transformer block. We will call the-th Transformer block the-th layer of the Transformer and denote its feed-forward layer and attention layer asandrespectively.

Recently there has been a lot of interest in the linear-time Transformer, which replaces the full-attention calculation with linear-time alternatives. These variants are mostly special forms of recurrent neural networks (RNNs) that are parallelizable.
Here we define a general form of RNNs containing all the common variants to the best of our knowledge,
including Mamba, RWKV, RetNet, StreamingLLM, RMT, TOVA, xLSTMetc.
An RNN maintains a statestoring-bit floating point numbers.

An RNN architecture is characterized by two functions: state transition functionand output function, whereis the dimension of the state andis the parameter space. Letbe the input sequence, the output of a recurrent neural network with parameteris defined as:

whereis a vector determined byandis the word embedding matrix. We will omit the subscriptwhen it is clear from the context.

We can characterize the complexity of an RNN architecture with the following three measures,

Parameter size: the number of bit of parameters determiningand.

State memory size: the number of bits to record the state of the RNN, in this case, is.

Circuit size: the number of bit-wise arithmetic operations needed to calculateand.

A particularly interesting class of RNNs is constant-size RNNs, where the dimension of state and number of parameters are fixed and do not depend on, i.e.,,,,,.

We do not assume a specific structure of the RNN when we want to prove impossibility results for RNNs, i.e., what RNNs cannot represent. But when we want to showcase what RNNs can represent, we focus on Linear RNNs, one of the simplest form of RNNs, so that our results are more likely to be generalizable to more complex RNNs.

A Linear RNN block is defined as follows:

whereis a column-wise ReGLU feed-forward network with widthand output dimensionandis a linear unit, defined as

A Linear RNN is a recurrent neural network

whereis the column-wise softmax function,is the-th Linear RNN block. We will call the-th Linear RNN block the-th layer of the Linear RNN and denote its feed-forward layer and linear unit layer asandrespectively.

An algorithmic problem is a problem that may be solved by an algorithm.
In this paper, we focus on algorithmic problemsthat
asks for computinggiven a sequence of tokensas the input, whereis the set of possible answers.
We say that an LMcan (directly) solve an algorithmic taskif, given the sequence, the probability distributionfor the next token is peaked at the correct output token, i.e.,.

(CoT) reasoning allows the LM to produce intermediate steps before the final output.
Following, our paper studies the effectiveness of CoT reasoning in improving the expressiveness of LMs, and we allow the intermediate steps to be an arbitrary sequence of tokens.
We say that an LMcan solve an algorithmic problemwith CoT if the following process terminates with a sequence ended with.
First, let.
For all, decode the next tokenfrom, and append it to the current sequence.
If, then the process terminates withwithsteps of CoT; otherwise the process continues.

It is evident that
if an LM can solve an algorithm problem withsteps of CoT,
then an LMcan (directly) solve the problem. In this case, we also say that the LM can solve the problem without CoT.

SECTION: Can CoT improve the Representation Power of RNNs?
In this section, we aim to understand the representation power of RNNs with CoT. We first show the positive result that RNNs with CoT can solve tasks that are impossible for RNNs without CoT fixing the state size. We then proceed to understand whether CoT can make RNNs as expressive as Transformers. We show that, even with CoT, RNNs still struggle to solve problems that explicitly require in-context retrieval and this representation gap propagates to seemingly retrieval-irrelevant reasoning tasks such as IsTree. Finally, we show that this gap is indeed one-sided: there only exist tasks where Transformers require exponentially less parameters than RNNs, but not the other way around.

SECTION: CoT Strictly Improves RNNs
On the positive side, we show that CoT broadens the representation power of RNNs under mild complexity assumption.

[]theoremthmrnncot

Assuming, there exists an algorithmic problem such that (1) there exist constant-size Linear RNNs that can solve the problem with polynomial length CoT; and (2) any constant-size regular RNNs cannot solve the problem without CoT.

Seefor the proof.
The key insight is that the representation power of RNNs without CoT is limited to shallow circuits of size, but RNNs with CoT can simulate-space Turing machines perfectly withsteps.
This result is consistent with previous works on the benefit of CoT for Transformers, which also prove based on mild complexity assumptions that Transformers with CoT have the representation power to simulate polynomial-size circuits to solve all the problems inbut Transformers without CoT cannot.
Here we rigorously prove that a similar benefit of CoT also applies to RNNs, but a key difference is that CoT cannot boost the representation power of RNNs to simulate every polynomial-size circuit family.

SECTION: CoT Cannot Close the Representation Gap with Transformers
Now we proceed to understand whether CoT can make RNNs as expressive as Transformers. The answer turns out to be negative:
RNNs, even with CoT, struggle to solve very simple algorithmic problems that require the capability of retrieving information from the current context, which we callfor short.
This limitation is directly related to the memory efficiency of RNNs. For a model with at mostbits in memory, we can involve techniques from streaming complexity to prove impossibility results for problems requiring.

Here we list several simple algorithmic problems that directly test the in-context retrieval capability of the model,
which turn out to be a good test-bed for understanding the limitations of RNNs compared to Transformers.

Index is a problem that given a sequence of tokens with lengthand a query token, requires the model to output the type of the-th token.

Associative Recall (AR) is a problem that given a sequence of tokens with lengthconsisting of tokens inand a query token, requires the model to output the next token ofin the sequence.

An-gram is a contiguous subsequence oftokens in a sequence of tokens.-gram retrieval is a problem that given a sequence of tokens with lengthand a query-gram that is the prefix of a-gram in the sequence, requires the model to output the last token of that-gram.

Counting is a problem that given a sequence of tokens with length, a query token, and a query number, requires the model to outputorto indicate whether the number of occurrences ofin the sequence is greater than.

Here, Index and AR are perhaps the most basic problems in retrieval, where Index asks for retrieving a token from the input sequence viewed as a linear array of tokens, and AR asks for retrieving a token from the input sequence viewed as an associative array.
These two problems have been studied extensively by different communities. Index is a classic problem in streaming and communication complexity, known to be impossible to solve withbits of memory for streaming algorithms.
AR has been regarded as a fundamental problem that an artificial intelligence system should be able to solve. In the context of LLMs, AR has been observed to correlate with in-context learning performanceand has also been used extensively as synthetic surrogate tasks for pretraining performance.
Besides Index and AR,-gram retrieval is a natural extension of AR to the case where the query key can contain multiple tokens: instead of retrieving a token given a single-token key,-gram retrieval asks for retrieving a token when the given key is a-gram. This task has been studied empirically, but not theoretically in.
Counting is a problem that asks for the number of occurrences of a token, thereby testing the modelâ€™s capability to retrieve some statistics of relevant information from the input sequence.

The following theorems show that constant-size RNNs cannot solve any of the four tasks when the context length is long, while constant-size Transformers can solve them perfectly.[]theoremthmindex

For task, there exist constant-size Transformers that can solve.
On the other hand, any RNN with-bit memory cannot solveof sizewith any length of CoT for large enough.

We note thatdoesimply that RNNs are incapable of in-context retrieval at all. Instead, it states that the maximal context length that RNNs can effectively retrieve from is linear in its state size. Although for a short context window, RNNs can be trained to perform in-context retrieval (see e.g.), this limitation in retrieval capabilities has been empirically observed: for example in, both pretrained Mamba and Mamba-2 7B models are shown to have significantly worse Phonebook-retrieval capabilities on 1K context length than Transformers with the same size and trained on the same data.

The key idea of the lower bound of the proof is to put RNNs into the framework of communication complexity and use information-theoretic arguments to prove a lower bound. RNNs have the following property if partysimulates the RNN on the first part of the input and sends the state to party, then partycan simulate the RNN on the second part of the input with the state received from partyto recover the output of the RNN perfectly. Hence, in the above two theorems, if the RNN can solve the problem withinput size, then the information about the input can be compressed tobit to produce the output, which contradicts the information-theoretic lower bound.

For the upper bound, we show that the Transformer can solve the problem by utilizing an attention mechanism calledthat takes the query token and attends to previous keys that match the query token on certain predefined coordinates. This mechanism allows the Transformer to read its context window like a key-value dictionary and hence can solve the problems perfectly. For the counting problem, we additionally use aattention mechanism that counts the number of occurrences of the queried token by attending evenly to each appearance of the queried token. âˆŽ

A natural question would be if an algorithmic problem does not directly test the in-context retrieval capability, can we hope that RNNs would have the representation power to solve it?
Do RNNs and Transformers have the same representation power in this case? We show that the limited memory size in RNNs can still be a bottleneck in solving algorithmic problems.
Even if the retrieval capability is not explicitly tested in an algorithmic problem, it may still be required implicitly for reasoning about the answer.

We demonstrate this gap on a minimal example of algorithmic problems, called: given an undirected graphofnodes, determine whetheris a tree, i.e., whether every pair of nodes is connected by exactly one simple path. A classical solution to IsTree is running Depth First Search (DFS), which takestime.

In the context of language modeling, we can write the graphas a sequence of tokens, and then the task of IsTree is to determine whetheris a tree by predicting atoken with or without CoT. We use the following tokenization for the graph:

whereandare two special tokens representing the start of the sentence and an edge, andare numbers denoting the nodes of the graph.

Our result states that RNN withbit memory cannot solve IsTree, even with an arbitrary choice of chain of thought. On the other hand, there exists a Transformer with constant size andprecision that can generate a chain-of-thought of lengthfollowing DFS and perfectly solve the same question.[]theoremthmistree
There exist constant-size Transformers that can solve IsTree with CoT of length. On the other hand, any RNN with-bit memory cannot solve IsTree with any length of CoT.

The main idea of the proof is that the task of IsTree requires the model to reason about the global structure of the graph, which is beyond the capability of RNNs with limited memory. We prove the lower bound by constructing a graph from a binary sequence and showing that RNNs withmemory cannot solve the problem by a reduction to an information-theoretic lower bound. For the upper bound, we show that the Transformer can simulate the DFS algorithm by outputting the Euler tour of the connected components of vertexand then check the length of the Euler tour with its capability of.

The key idea of the lower bound of the proof is to again utilize the information-theoretic lower bound. This idea lies in the core of streaming complexity literature and investigation on the IsTree problem dates back to. We hereby restate the proof for completeness. Given any binary sequenceof lengthand an index, we can construct a graph as follows: the graph hasnodes, and vertexis connected to vertexfor any. Moreover, vertexis connected to vertex. The graph is a tree if and only if.

Now assuming there is an RNN withmemory that can solve IsTree, consider two partiesandeach holding the sequenceand the index, they can construct two parts of the graph using their information and thencan simulate RNN on the first part of the graph and send the state to, andcan simulate RNN (potentially with CoT) on the second part of the graph to recover the output of the IsTree problem, which is equivalent to whether. However, note thatis never sent to, and hence actuallycan get whetherfor any, which contradicts the information-theoretic lower bound.

Now for the upper bound, we will let the Transformer simulate the DFS algorithm by outputting the Euler tour of the connected components of vertexand then check the length of the Euler tour (see). To simulate the tour, we will implement two functions through the Transformer Block:

Given a prefix of the tour, find the parent of the last vertex in the tour. This can be implemented by copying each tokenâ€™s predecessorâ€™s type to that token and then using themechanism to match the first occurrence of the current token in the sequence.

Given the tokenized input graph and an edge, find the next edge aftercontainingin the input graph. We will use another attention mechanism calledto count, for each edgein tokenized input graph, the number of occurrences ofandup to that edge and storeandin the token corresponding to the edge, whereandare the corresponding counts. Then given the edge, we can use themechanism to find. Then we will use a feed-forward layer with gated relu activation, constant depth, and constant width to approximateand then use themechanism again to find the next edge containing.

Through the above two functions, the Transformer can simulate the DFS algorithm and hence solve the IsTree problem perfectly. âˆŽ

The above theorems show the existence of tasks where Transformers require exponentially less memory than RNNs. However, they have not rule out the possibility that there exists a corresponding task where the Transformer will be more redundant and require exponentially more parameters than RNNs. However, the following theorem confirms that such a task doesnâ€™t exist for constant-size RNN.

The theorem is in the same vein as the recent work on the CoT for Transformer, which shows the constant size and constant precision Transformer with a polynomial-size position embedding can simulate any polynomial size circuit. The major difference of our theorem is that (1) we consider a Transformer with fixed word and position embedding, hence allowing the parameter number to be logarithmic in the input size, and (2) we consider simulating RNNs, which is a special kind of circuit family and hence we can use more succinct representation utilizing the structural property attached to the recursive process.

[]theoremthmtransrnn

Given input length,
letis an RNN with word embedding, whereis the precision, the constantis the number of special symbols in the vocabulary, the constantis the embedding dimension.
If each recurrent iteration can be computed by a circuit of size, and if the RNN produces the final answer after running at moststeps of CoT for some constant, then there exist Transformers with-bit precision,parameters and word embeddingthat can produce the same final answer after runningsteps of CoT.

The key idea is to encode the RNN circuit into the Transformerâ€™s weight and simulate the circuit gate by gate, utilizing themechanism to fetch the input of each gate. In this manner, although the naive circuit to simulate the RNN forsteps would requireparameter, the Transformer only needs to store one instance of the RNN circuit in its weight and hence we only needparameter, which is at most polynomial in the parameter size of the RNN. âˆŽ

SECTION: Enhancing theCapability Closes the Representation Gap
In, we show that RNNs are deficient at, hence leading to a significant representation gap with Transformers.
In this section, we aim to understand: if we enhance thecapability of RNNs, do RNNs remain to have any representation gap with Transformers?
We answer this question by examining two representative approaches to enhance thecapability, one explicit and one implicit, and show that both ways can close the representation gap between RNNs and Transformers in solving algorithmic problems.

SECTION: Explicit Retrieval Through Regular Expression
First, we explore the power of RNNs with(RAG),
which gives an LM the capability to retrieve relevant information to assist generation. In our context, we are specifically interested in allowing LMs to call functions to retrieve information from their context, which we call().

We will first show that adding function calls to associative recall is not enough to close the representation gap between RNNs and Transformers.

Consider a special type of index problem where every token at the even position of the input sequence is a special tokenand the rest of the tokens are uniformly random. Then the oracle for the AR problem can be simulated by the RNN by simply outputting thewhen the query is notand outputting the third token when the query is. However, following similar proof of, we can show that the RNN canâ€™t solve this special form of index problem with lengthin any CoT steps.
âˆŽ

In this light, we need to consider a more general form ofcapability. We specifically consider a special form ofthat enables an LM to perform regular expression matching because the regular expression is a flexible primitive that can be used to describe a wide range of retrieval tasks and can be implemented efficiently on modern hardware.

Given an LMwith vocabulary(containing two additional special tokens,and) and the tokenized input sequence, the LMwithgenerates following sequence of tokenized sequence:

Herelooks for the last occurrence ofat positionandinat positionand treatas a regular expression, wheremaps the tokenized sequence back to the string, inserting a space between every pair of adjacent tokens. The algorithm then runs a regular expression matching on, finds the first matching substring, and returns the first capturing group according to the regular expression (i.e., content embraced by a pair bracket in the regular expression).
While there are many grammar standards of regular expressions, we adhere to the standard specified in thelibrary of Python. That is, we evaluate the following Python code to get the result of the regular expression matching:

whereis the pattern andis the string.

The following theorem shows thatwith regular expressions is powerful enough for RNNs to solve theproblems inwith CoT.

[]theoremthmindexre

For task, there exist constant-size Linear RNNs that can solvewith. Forother than IsTree,steps of CoT is required and for IsTree,steps of CoT is required.

For the Index problem, let the RNN output the regular expression^(?:\S\s*){}(\S), where. For AR, let the RNN output\b\b(\S+)\b, whereis the number in query. For-gram retrieval, let the RNN output\bâ€¦\b(\S+)\b, whereis the-th number in the query. For Counting, let the RNN output(\b\b){}, whereis the query token andis the query threshold. âˆŽ

Beyond these simpleproblems, in, we have shown that RNNs cannot solve IsTree due to its implicit requirement ofcapability. We now show thatcan help linear RNN solve IsTree inCoT steps. Seefor the proof.theoremthmragistree

There exists a Linear RNN family withbit memory andparameter, that can solve IsTree of sizewithinCoT steps.

To further show the power of the explicit retrieval, the following theorem further proves a general result, showing thatempowers RNNs withbit memory to simulate polynomial-time Turing machines.

[]theoremrnnturing

Given, for all polynomial-time Turing machineswithstates and vocabulary size, there exist Linear RNNs withspecial symbols,-bit precision and memory, andparameters that can output the result ofby runningsteps of CoT with.The retrieval augmented RNN can simulate the Turing machine by maintaining the state of the Turing machine and the position of the tape head in its memory and writing down the tape in the context in the form ofto indicate the value of the tape at positionis updated to. Given the input tape, the retrieval augmented RNN will first write down the initial tape in the previous form ofusing the regular expression used in the Index problem.
The RNN can then generate the search queries with forms of(.).*?$withbeing the pointer to retrieve the information from the context.

By introducing, RNNs are able to use regular expressions to retrieve information from a distance and put relevant information together.
Then RNNs can use perform more complex operations locally and output tokens to be retrieved later. âˆŽ

As a final note, our focus here is to understand the representation power of RNNs given an appropriate RAG, but not to propose a method that immediately leads to practical applications.
While the above results show thatcan close the representation gap between RNNs and Transformers in solving algorithmic problems, a limitation here is thatis not an immediate practical solution, as there is no existing training data for this.

SECTION: Implicit Retrieval by Appending Just One Transformer Layer
Since, attention mechanisms have been understood as a form of compensation for the fixed memory size of RNNs, allowing the model to attend to the entire context. We show in this section formally that this form of implicit retrieval can close the representation gap between RNNs and Transformers in solving algorithmic problems. We consider the following hybrid architecture, which combines the RNN and the Transformer by appending a single Transformer layer to the RNN output.

A hybrid RNN is a model that consists of an RNN with transition and output functionand one Transformer layer, the output of the RNN is used as the input of the Transformer layer and the output of the Transformer layer is used to produce the next token. Concretely, given the input sequence, the output of the hybrid architecture is:

First, we show that hybrid RNNs can solve theproblems inwithout CoT.

[]theoremthmhybridindex

For task, there exist constant-size hybrid Linear RNNs that can solve. Forother than IsTree, no CoT is required and for IsTree,steps of CoT is required.

The proof is similar to the proof of, using the appended Transformer layer to simulate thefunction andfunction in the RNN.âˆŽ

Similar to the situation in, the implicit retrieval method can enpower the hybrid linear RNN to solve IsTree with CoT. Seefor the proof.

[]theoremthmhybridistree

There exists a hybrid Linear RNN withbit memory andparameter, that can solve IsTree of sizewith a chain of thought of length.

Further, we show that this hybrid architecture with only one attention block is powerful enough to even simulate any polynomial-time Turing machine with CoT.[]theoremhybridturing

Given any constant, for any polynomial-time Turing machinewithstates and vocabulary size, there exists a hybrid Linear RNN (see) with vocabulary ofspecial symbol,bit precision and memory, andbit parameters, that can simulate the result ofon any input with lengthinCoT steps.The proof is similar to the proof of. Instead of using regular expressions to retrieve the information from the context, the hybrid architecture can use the attention mechanism in the Transformer layer to implement thefunction to retrieve the information from the context. âˆŽ

SECTION: Empirical Validation
We validate our theoretical findings through synthetic and natural language experiments: CoT alone cannot close the performance gap between RNNs and Transformers, but enhancing the in-context retrieval capability can narrow the gap.

SECTION: Validation on Synthetic Task: IsTree
First, we validate our theoretical findings on the IsTree task introduced in.
We generate a synthetic dataset for this task, where each data point is a tokenized graph concatenated with an answer ofor, potentially with a CoT inserted in between.
We then train RNNs and Transformers as language models on this dataset autoregressively.

To generate the graph, we follow the procedure described in the proof of(see). The CoT data is generated usingand the retrieval data is generated using. For the CoT model, we decode the reasoning path during inference time until we reach the firstorup to a max token limit greater than the length of all ground truth CoT. For the data points that the model fails to give a prediction, we assume the model gets it correct with 0.5 probability. For the retrieval task, we omit the explicit format of the regular expression and only ask the model to generate the vertices and special tokens in the regular expression to shorten the length of the input sequence. The reported accuracy is calculated over a validation set of 5000 samples using the last iteration of the model. We defer the experiment details to.
The results are shown inand.

Without CoT, both the Transformers and RNNs model cannot learn to solve the IsTree problem. CoT improves the performance of both Transformers and RNNs but the RNNsâ€™ performance degrades sharply as the graph size increases and the Transformers consistently outperforms RNNs in this case. This is consistent with our theory that CoT can improve the expressiveness of the RNN models but the expressiveness is still not enough to solve IsTree (seeand).

In-Context RAG allows all the models to reach near-perfect accuracy. This is consistent with our theory that retrieval augmentation via regular expression can improve the expressiveness of the RNN models to solve algorithmic tasks (seeand).
The hybrid model (a Mamba model with one additional Transformer layer on the top)
shows performance on par with the Transformer, which is consistent with our theory (seeand).

SECTION: Validation on Hotpot-QA
We further conduct experiments on open-source LLMs based on Transformer, Mambda, and a hybrid architecture to show that for tasks that require stronger in-context retrieval capability, RNNs suffer more performance degradation compared to Transformers.

We use Phi-1.5 1.3Bas our Transformer model.
We further use two Mamba and Transformer-Mamba hybrid models distilled from Phi-1.5 with approximately the same parametersâ€™ sizesas our RNN and hybrid models.
To test our theory, we use the Hotpot-QAdataset. In the validation set of Hotpot-QA, each question is accompanied by a set of different related paragraphs from Wikipedia.of these paragraphs contain useful information to answer this question. The model needs to retrieve the correct paragraphs and reason based on these paragraphs to get the answer to the question.

We consider the following version of Hotpot-QA with enhanced retrieval difficulty: We chooseparagraphs containing the correct paragraphs and order them randomly before the question. The model needs to answer the question based on the given paragraphs after CoT reasoning. This design allows us to use the hyperparameterto control the difficulty of in-context retrieval in this task, with largercorresponding to a higher difficulty.

We test our models under a 4-shot setting with Chain-of-Thought. The model we tested has varying performance even if. This is mostly due to their different capabilities to follow instructions and in-context examples. To mitigate this effect and highlight the impact of retrieval, we only test on a subset ofsamples of the validation set where all the models can answer correctly given the correct paragraphs. This ensures perfect accuracy whenin both settings.
We varyinand the result is shown in.

While all the modelâ€™s performance drops with increased retrieval difficulty (, left), the RNN model has the largest drops. The hybrid model with onlyattention layers performs significantly better than the RNN model. This validates our theory that RNN architecturesâ€™ limited in-context retrieval capabilities will impact their performance in reasoning and hybrid architecture is a potential solution to this limitation.

The context length also increases with the number of provided paragraphs and can be a potential confounder. We then experiment with a controlled group: after choosing the same set ofparagraphs, we always order the correct paragraph at the end, which significantly simplifies the in-context retrieval process. In this case, all the models have stable performance when the number of paragraphs increases.

SECTION: Conclusion and Discussion
This paper studies the representation gap between RNNs and Transformers on algorithmic problems.
It is proved that CoT can improve RNNs but is insufficient to close the gap with Transformers:
the inability of RNNs to perform in-context retrieval is a key bottleneck.
To address this, we show that adopting In-Context RAG or appending a single Transformer layer to RNNs can enhance their in-context retrieval capabilities and close the representation gap with Transformers.

One limitation of this work is that the solution of In-Context RAG through regular expression is for the purpose of understanding the bottleneck of the representation power of RNNs, and may not be a practical method beyondsince there is no existing training data for this type of RAG.
Effectively enabling or eliciting LLMsâ€™ capability to perform in-context RAG or other types of RAG is an interesting direction for future work.
Second, appending a single Transformer layer to RNNs is a minimal example of making architectural changes to RNNs to improve their representation power while marginally increasing the memory cost. It is left unexplored what other architectural changes can pose a similar effect or enjoy a better trade-off between representation power and memory efficiency.
Finally, we only study the aspect of representation power, and do not analyze the training dynamics and generalization of RNNs and Transformers. We believe this is the most challenging but important direction for future work.

SECTION: References
SECTION: Additional Definitions
We will now define some definitions used in the proofs.

SECTION: Reasoning Tasks on Graphs.
When resaoning on graphs, without otherwise specified, we will useas the number of vertices andas the number of edges. Without loss of generality, we will assume the vertices are labeled by.

We will focus on decision problems on graphs, which are defined as follows:

A decision problem on graphs is a function, whereis the set of all possible graphs.

We will use the following decision problem as our main example:

ifis a tree, andotherwise.

One can view IsTree as a minimal example of reasoning tasks. One of the classical solutions to IsTree is running Depth First Search and this algorithm takestime.

SECTION: More on Numeric Precisions.
We will useto denote the rounding function that roundsto the nearest number in. We will assumeis an odd number without loss of generality.

For calculation over, we will assume the calculation is exact and the result is rounded toat the end, that is, for operator, we will have

We will additionally defineas the set of all integers that can be represented by-bit floating point numbers. We will defineas the set of unit fractional. Further, we will defineas the rounding ofto. We will additionally define for any real number,where.

SECTION: Models
To tokenize a string, we will tokenize all the words separated by the space character into a sequence of tokens.
To tokenize a graph, we will order its edgesrandomly and tokenize it into the following string:

We hereby assume there are constant more special tokens that are not the same as any number token, which are listed below:

: the first special token, indicating the start of a sentence.

: the second special token, indicating an edge.

: the third special token, indicating the answer is yes.

: the fourth special token, indicating the answer is no.

: the fifth special token, indicating the start of a search query.

: the sixth special token, indicating the end of a search query.

We will denote the total number of special tokens asand the total vocabulary size as. We will further define the detokenization function,

Here eachis either a number or a special token, which we will treat as a word.

We will useto denote the dimension of the embedding andto denote the-th coordinate vector in.

We will separate the embedding function into two parts: the word embedding and the position embedding.
For the word embedding, we will useto represent the embedding of the verticein the tokenization of the graph. For the-th special token, we will useto represent its embedding. For example, the embedding ofis. We will denote the word embedding matrix as

For the position embedding, we will useto represent the position embedding of the-th token in the tokenization of the graph, which is a hyperparameter. The final embedding of any token sequence is the sum of the word embedding and the position embedding. We will useto denote the embedding function.

This embedding function will be fixed and shared across all models we consider in this paper and will not be learned during training, hence we will not consider it as part of the model parameters.

In this work, we will consider the difference between Transformer and Recurrent Neural Networks on reasoning tasks, which is a special case of language modeling. We will define language modeling as follows:

A language model is a function, whereis the vocabulary size,is the sequence length, andis the probability simplex over.

SECTION: Language Models for Reasoning.
We will now define how we use language models to solve reasoning tasks utilizing the following technique called chain of thought.

Given a language model,with vocabularyand the tokenized input sequence, chain of thought (CoT) generates the following sequence of tokenized sequence:

The process terminates atwhenisor. The language model can solve the reasoning task withinsteps of CoT if the process terminates atwhereand the final output is correct. We will call the special case where the language model solves the reasoning task withinsteps of CoT as solving the reasoning task without CoT.

We will show in this paper that retrieval augmentation is a necessary technique to solve reasoning tasks for recurrent neural networks. We will define retrieval augmentation as follows:

Retrieval Augmented Generation means giving the language model the capability to retrieve relevant information to assist generation. We formally described the process here. Given a language modelwith vocabulary(containing two additional special tokens calledand) and the tokenized input sequence, retrieval augmented generation generates following sequence of tokenized sequence:

Herelooks for the last occurrence ofat positionandinat positonand treatas a regular expression. The algorithm then uses the regular expression on. If the regular expression ever matches, thewill return the match. If the regular expression never matches,will return a special token.

Similar to, we can define the notion of solving the reasoning task withinsteps of retrieval augmented generation and solving the reasoning task without retrieval augmented generation.

We will note that assumingand every search query and the result is of length, the regular expression evaluation can typically be evaluated intime.

SECTION: Omitted Experiment Details
We train three different architectures: (1) LLaMA architecturerepresenting Transformers, (2) Mamba architecturerepresenting RNNs, and (3) Mamba with one additional layer of LLaMA block representing hybrid architectures. Following our theory, we freeze and weight-tie the prediction head and word embedding in all the models. For ease of training, we use a different embedding function mappingth token towithbeing the number of different tokens and use standard RoPEas position embedding.
We train every model with at least 1M samples to guarantee convergence using Adam with a cosine learning rate. If the model doesnâ€™t converge, we retrain using 5M samples. After a grid search over learning rates, we train all the Transformer models with learning rates 1e-3 and the rest of the models with learning rates 3e-4. We run all the experiments on a server with 8 A100s and the estimated time to reproduce the results is within 2 days.

SECTION: Omitted Proof
SECTION: Building Blocks of FFNs Construction
We will first show some basic operations that multiple layers of feedforward neural networks with ReGLU activation can perform that will be used in the following proofs.

We can construct the following feedforward neural network with ReGLU activation:

âˆŽ

âˆŽ

Then,

âˆŽ

We can calculate, and then scaleindicator functions to get the value of the lookup table at the key.
âˆŽ

Then,

âˆŽ

The interval function here can be written as the difference of two threshold functions. We can use the same construction as into approximate the indicator functionandand then take the difference.
âˆŽ

SECTION: Building Blocks of Transformers Construction
We will show in this section some construction for basic functionality using Transformer Blocks. This construction will be used in the following sections to prove the main results.

We will always useas the input to the Transformer Block, whereis the dimension of the input, andis the length of the sequence.
We will first outline all the building functionality and then show how to implement them.

For integer, index setsatisfying, a copying functionsatisfies the following,, then

For index setand index, a counting functionsatisfies the following, ifand, then

For index set, a matching functionsatisfies the following, if, then

For index set, a matching closest functionsatisfies the following, if, then

For index setand index, a matching nearest functionsatisfies the following, if, then

Given any interger constant, assuming, for index set, and a special counting index, a matching next functionsatisfies the following, ifsatisfies the following condition:

,

,

For any, given any, the counting index multisettakes consecutive and disjoint values in, that is, there existssuch that.

then, we have

Now we will show how to implement these functions using Transformer Blocks. The construction here is motivated by the construction inwith some modifications.

We will use the feedforward block to calculateand() and have

We will useto denote.
Then we will choosesuch that

Hence

Hence we have

Also, for any, we have

Hence after the column-wise softmax and rounding tobit, we have

We will then choosesuch that

This then concludes that

The proof is complete.
âˆŽ

We will use the feedforward block to calculate() and have

We will useto denote.
Then we will choosesuch that

Hence,

Hence we have

Equality holds whenorfor all.

Also, for anyorfor some, we have

Hence after the column-wise softmax and rounding tobit, we have

Here theterm comes from the fact that the softmax is rounded tobit.

We will then choosesuch that

This then concludes that

âˆŽ

We will use the feedforward block to calculateas in the proof ofand.

We then choosesuch that

The detailed construction ofis omitted here since it is similar to the proof ofand.

We will discuss several cases for the distribution of. It always holds that.

If there doesnâ€™t exists, such that, then for any, we have.

If there exists, such that, then for such, we have. The rest of the entries are all smaller than. Finally, the largestsatisfying thatwill corresponds to athat is at leastlarger than the second largest, as in the proof of.

Concluding the above discussion, we have after the column-wise softmax and rounding tobit,

Further, we will choosesuch that

This then concludes that

This concludes the proof.
âˆŽ

The proof is similar to the proof of, and one can design the attention pattern as

The rest of the proof is omitted here.
âˆŽ

The proof is similar to the proof of, and one can design the attention pattern as

The rest of the proof is omitted here.
âˆŽ

We will use the feedforward block to calculate the followingfunction, where

The value can be arbitrary for.
This function is achievable by a feedforward block through combination ofand.

The purpose of this is to approximate thefunction for, and we have the following lemma.

We always have.
We will discuss several cases for.

If, then.

If, it holds that, then

This then concludes the proof.
âˆŽ

We then choosesuch that

Again, the detailed construction ofis omitted here since it is similar to the proof ofand.

We will discuss several cases for the distribution of. It always holds that.

If there doesnâ€™t exists, such that, then for any, we have.

If there exists, such that, then for such, we have. The rest of the entries are all smaller than.

It remains to discuss the distribution offorsatisfying. Whensatisfies the condition in, we have thattakes consecutive and disjoint values in. Hence, if, supposesatisfies that

We will discuss several cases for.

Ifandare both negative, then, we have,

.

Ifandare both positive, then, and same as above we have

.

Ifandhave different signs, then according to, we have,becausetakes consecutive and disjoint values in. This then implies that

Concluding, we always have for any

Concluding the above discussion, we have after the column-wise softmax and rounding tobit,

Further, we will choosesuch that

This then concludes the proof.
âˆŽ

SECTION: Building Blocks of RNNs Construction
We will now describe the building blocks of Linear RNNs construction. We will introduce some basic operations that will be used to build more complex RNNs family.

Suppose the input sequence is, and the dimensions that the state should memorize are. We can construct the following linear unit:

âˆŽ

Suppose WLOG the input sequence is, and the dimension that the state should memorize is. We can construct the following linear unit:

âˆŽ

This is a direct combination ofandand. The FFN can assign all the input vectors with position greater thanto, and permute the corresponding dimensions of firstinput vectors to the firstdimensions of the state. The linear unit can then maintain the state.
âˆŽ

This is a direct consequence of.
âˆŽ

SECTION: Proof of
We will first restate the theorem for clarity.

*

We will discuss by cases.

Whenis Index, we will first show why RNN cannot solve the Index question withoutmemory. The key observation is that the recurrent form of RNNs allowed the algorithm to be run in a streaming fashion withbit memory. Here streaming means that the algorithm gets to look at each bit of the memory sequentially and can only update a constant size of memory.

Suppose there exists a communication protocol whereonly receivesbit and can perfectly decide. Because Alice doesnâ€™t know, the protocol must send the same message to Bob for all. Hence Bob can reconstruct the whole stringwithbit withbit communication. This is a contradiction.
âˆŽ

Now if RNN can solve the Index problem withbit memory, then it can also solve the Index problem withbit communication complexity. This is because Alice can simply run the RNN on inputand send the hidden state to Bob. Then Bob can run the RNN with the hidden state andto get the answer. This is a contradiction to. Hence RNN cannot solve the Index problem withbit memory.

On the other hand, we will show that Transformers can solve the Index problem withbit parameters. This is because using 2 layers of Transformer, we will implement a Match Block () that can match the last query token with the position of the previous token and retrieve the type of the matched token to the query token.

Whenis AR, wthout loss of generality, we assume thatis even. The proof is similar to the proof of the proof of theIndex problem. As there aredifferent types of tokens, we can label them as. Now for any boolean sequence, solving AR for the following input is equivalent to solving the Index problem for:

This then implies that RNN cannot solve AR withbit memory. Transformers, on the other hand, can still solve AR withbit parameters, we will use one layer of copying function to copy each tokenâ€™s previous tokenâ€™s type to it. Then we can use the Match Block to match the last query token with the position of the previous token and retrieve the type of the matched token to the query token.

Whenis-gram retrieval, without loss of generality, we assume thatis a multiple of. The proof is similar to the proof of. As there aredifferent types of tokens, we can label them as. Now for any boolean sequence, solving AR for the following input is equivalent to solving the Index problem for:

This then implies that RNN cannot solve-gram retrieval withbit memory. Transformers, on the other hand, can still solve-gram retrieval withbit parameters, we will use one layer of copying function to copy each tokenâ€™s previoustokensâ€™ type to it. Then we can use the Match Block to match the last query token with the position of the previous token and retrieve the type of the matched token to the query token.

Whenis Counting, we will first show why RNN cannot solve the Counting question withoutmemory. Consider the following setup, given any, the input string iswhere, then solving the Counting question for this input string for queried thresholdis equivalent to solving the Index problem for. This then implies that RNN cannot solve the Counting question withbit memory.

On the other hand, we will show that Transformers can solve the Counting question withbit parameters. This is because using 2 layers of Transformer, we can first use ablock to copy the last query token to the token corresponds to the threshold, and then use ablock () that can count the numberof the appearance of the last query token in the input sequence, and then writeto one of the dimension. Finally, we can use the Feed Forward Network on the last layer to multiply thresholdwith this value and compare the result toto get the answer.
âˆŽ

SECTION: Proof of
We will first prove a lemma assuming.

We will prove this by contradiction. Assuming for every Turing machinewith linear space complexity, there exists a polynomial-size circuit familythat can simulate. We will construct a polynomial-size circuit familythat can decide, which contradicts the assumption that.

Given any language, we can decideby a Turing machinewith spacefor some constant. We can consider another language. We can decideby a Turing machinewith linear space complexity by checking the length of the input and then simulating. By the assumption, there exists a polynomial-size circuit familythat can simulate. We can then construct a polynomial-size circuit familythat can decideby first counting the length of the input and then simulatingon the extended input. This contradicts the assumption that.
âˆŽ

Now we are ready to prove our theorem.*

By, we know that if, then there exists a Turing machinewith linear space complexity that cannot be simulated by a polynomial-size circuit family. We will use this result to prove.

We design the task as follows, for any, let, for any boolean inputof length, we will choose input sequence asand the label asifandotherwise.

Because we are considering regular RNN withmemory, we know that we can compute the result of RNN without CoT through a circuit family with size. However, we know thatcannot be simulated by a polynomial-size circuit family. Hence, no RNN family withmemory can solve the task for all.

On the other hand, we can simulateby the RNN by maintaining the state, the pointer, and the tape of theinside the state of the RNN. The RNN can then maintain the transition function of the Turing machine in its output function as a lookup tableand write down the updated state, the direction of the pointer movement, and the updated cell value at the pointer in its context. Paired with the ability to memorize the recent input, the RNN can then simulate the running of the Turing machine.

Because the space complexity ofis linear in, the time complexity ofiswhich is polynomial in. Hence, we can solve the task by an RNN with CoT andmemory and polynomial-size circuit family.
âˆŽ

SECTION: Proof of
We will now proceed to prove our main theorem, which states that Transformers with chain-of-thought can solve IsTree perfectly, while RNNs cannot. We will first restate the theorem here.

*

We will prove this theorem by proving the following lemmas.

This proof is a direct combination ofand.
âˆŽ

We first reduce another problem in communication complexity to IsTree.

We can reduce this problem to the problem in. Considering the game in, given any, we can construct. Thenis a string of lengthwith onlyand. Moreover,if and only if. Hence, if Bob can solve the problem inwithbit, he can solve the problem in. This is a contradiction.
âˆŽ

Now suppose that we have a streaming algorithm for IsTree with onlymemory. We shall prove Alice and Bob incan use it to solve the original question withmemory.

Consider the following graph withnodes. There is a nodecorresponding to eachforand two special nodes. Nodewill be connected toifand toif. Moreover,andwill be connected. Now the original answer Bob wants is False if and only if the graph is a Tree. Hence, given access to the streaming algorithm, Alice can run it on the edges that she knows exist and send the memory to Bob. Bob can then run it on the edges that he knows exist. Combining they will be able to solve the original problem. This is a contradiction.

Moreover, as RNN with CoT is also a streaming algorithm, it also requiresmemory.
âˆŽ

The proof is two-folded. We will first define an algorithm that can solve IsTree by generating a sequence of vertices of length, and then we will show that this sequence can be generated by a Transformer with constant depth and width, andprecision as a Chain of Thought.

We defineas a depth-first search algorithm that can generate a sequence of vertices of lengththat can be used to solve IsTree. We will use two stacksto store the vertices.will be used to store the vertices that are not yet visited, andwill be used to store the vertices that are already visited. The algorithm will start withand. At each step, the algorithm will pop the top ofand push it to. Then it will push all the neighbors of the popped vertex that are not into. The algorithm will terminate whenis empty. We will denote the yielded vertice sequence foras. The following lemma shows the connection between the result of the algorithm and the IsTree problem.

First, every vertex in the connected component ofcontainingwill be visited. This is because the algorithm will always push all the neighbors of the popped vertex that are not into. Hence, the algorithm will terminate when all the vertices in the connected component ofcontainingare visited.

Second, every two consecutive vertices in the yielded sequence will be connected by an edge. This is because the algorithm will always push one of the neighbors of the popped vertex that is not into. Hence, every two consecutive vertices in the yielded sequence will be connected by an edge. On the other hand, the combination of these edges will form a tree because the algorithm will never push a vertex that is already into. Hence, the yielded sequence is a tree traversal of a spanning tree of the connected component ofcontaining.
âˆŽ

We will now show that the yielded sequence ofcan be generated by a Transformer with constant depth and width, andprecision as a Chain of Thought. The Transformer will generate a valid yielded sequence but can terminate early if the graph is not a tree. We will now describe the Transformer in detail. We will assume the input token sequenceis as follows,

for someandis a valid yielded sequence. The length ofiswithtokens for each edges andspecial token. We will further denote the input to the first layeras. We will similarly denote the input to layeras. We will also denote the output of the last layer as.

The attention at Layer 1 will output zero and the FFN at Layer 1 and Attention at Layer 2 will implement a counting function () to count the number of verticesappears in the previous token sequence and writein a new dimensionas a result.

The FFN at Layer 2 and Attention at Layer 3 will implement a copying function () copying the first dimension and the counting dimensionof each token to its successor at two new dimensionsand. For each edge, this moves the type of the first vertice and the number of times the first vertice appears to. For every vertice in the chain of thought, this moves the type of the previous vertice to them.

The FFN at Layer 3 and Attention at Layer 4 will implement another copying function, copying the dimensionsandof each token to its successor at two new dimensionsand. Especially, for each edge, this moves the type of the first vertice and the number of times the first vertice appears to the position corresponding to the second vertices.

This FFN will process the information gathered from the previous layer and prepare for the next layer. It will make sure the following properties hold for,

For every token, the position number, its square, andwill be kept in the last three dimensions.

For the first vertices in each edges,andThe rest dimension will be zero.

For the second vertices of each edges, there will be four dimensionswith valueand, where.

For verticein, there will be four dimensionswith valueand().

Combining with the previous Layer 4 FFN layer, we will implement two match functions with two attention heads matchingorwithat Layer 5 Attention, i.e. finding the edge in input for each step in the chain of thought, we will then copyto dimensionsand.

We will use Layer 5 FFN and Layer 6 Attention to implement the match function that matches dimensionof the current token toin the previous token. This will matchto the first appearance ofin the chain of thought and we will copyof the matched token to. This dimension will be the first predecessor ofin the chain of thought (0 for). We will denote this predecessor ofto beas it is the father ofin the tree. Now we will need to split into two cases depending on whetheris. Ifor(for), we will set dimensionto beandto be. Otherwise, we will keep dimensionandas.

Now we will use Layer 6 FFN and Layer 7 Attention with two attention heads to implement twofunctions() which useoras the counting index, and matchattoorrespectively. We will then copy dimensionstoof the matched tokens toto(because there will be two of them).

The match next function will be able to retrieve the first edge containing. For any, one of the matches next function will be able to retrieve the next edge containingafterif it exists. If it doesnâ€™t exist, the corresponding counting dimension will either be zero or no smaller than. We will use Layer 6 FFN to decide whether the next edge exists and set dimensionof the output of Layer 6 to be the other edge in the next edge if it exists, orotherwise, andof the output of layerto be the counting dimension of the next edge if it exists, orotherwise. For each edge in the original input, we will also set dimensionto be the counting dimension of the edge.

We will grab the next edge again, in the same manner as Layer 6, but this time using dimensionand. The necessity of this step is that the next edge containingin the original graph can be the same as theand in such case we need to check whether the next edge after this edge.

We now have, at each position corresponding to, the first edgein the yielded sequence containingand the other vertex in the edge containingthat hasnâ€™t been visited if it exists. If they donâ€™t exist, the corresponding dimension will be zero. This allows us to use Layer 8s FFN to decide the next vertex in the yielded sequence, which is exactly the first vertex different within the two edges if they exist, orotherwise. We will use Layer 8 FFN to calculate the potential next vertex and put it in dimensionand its square in.

Combining with Layer 8 FFN, we will matchof the current token toof the previous token to find the first appearance of the potential next vertex in the chain of thought. We will then copy dimensionof the matched token to. This value beingwill imply this vertex has never appeared in the chain of thought before and any other value will imply the vertex has appeared before.

We can now check several cases,

If the potential next vertexis eitheror never appears in the chain-of-thought sequence, then Layer 9 will output, which will decodes to.

If the potential next vertexis notand appears in the chain-of-thought sequence, then Layer 9 will output, which will decodes to, because the chain of thought has already visitedand hence the graph is not a tree.

Ifand the potential next vertexis, this means the chain of thought has finished. In this case, layer 9 FFN will check whether the position isand outputif it is, or outputotherwise, which will decode toandrespectively.

This concludes the construction of the Transformer.
âˆŽ

SECTION: Proof of
We will now prove. We will first restate the theorem for convenience.

*

The proof is inspired byfrom.

However, direct utilization ofis not feasible because we are interested in (1)precision Transformer, and (2) simulating the RNN forstep, which would correspond to a circuit within size. However, as the calculation is recurrent, we can encode the RNN circuit inparameter instead.

To do so, we will unroll the circuit of each recurrent step of the RNN intogates. We will then assign each gate a unique id inand assume the circuit is calculated in the order of the gate id in the following manner.

Each gate has a type, which is either a constant gate outputting, an input gate, a hidden state gate, an AND gate, or an XOR gate.

Each gateâ€™s output depends on two valuesand. Ifis a constant gate, thenandare assigned to be. When it is an input gate,will be assigned to be the coordinate of the input embedding andwill be assigned to be the index of the bit of the value atcoordinate. When it is a hidden state gate,will be assigned to be the coordinate of the hidden state embedding, andwill be assigned to be the index of the bit of the value atcoordinate. If it is an AND gate or an XOR gate,andwill be assigned to be the id of the two gates that it depends on.

We will further assume without loss of generality that the hidden state gate is the firstgate. The output of the lastgate will be the next hidden state. We will also assume that the lasttogates are the output gates.
We will now first describe the chain of thought that the Transformer will output and then construct the Transformer.

Taking any inputwith length, the Transformer will output a sequence ofandtokens. The firsttokens will be the same as the input sequence. For eachand, thetoken is

the output of gatewhen RNN circuit is calculating the output atposition plus, if.

thetoken in the RNN chain of thought, if.

The first attention layer will output zero and the first FFN layer will be of width, encoding all the gate information. The output of the first layer at positionwill have the following coordinate:

The inputwill be encoded in the first dimensions.

will be encoded in four different dimensions.

The gate typewill be encoded in the next dimension, whereIf, then the gate type will be encoded as.

The necessary dependenceandwill be encoded in the next two dimensions.

A constantwill be encoded in the next dimension.

Together with the Layer 1 FFN, the Layer 2 Attention will implement two match functions () to copy the output of gateandwhen RNN circuit is calculating the output atposition. When the type of gateis not AND or XOR, the result will not be used in the following calculation.

Layer 2 FFN will be of width. The output of the layer will be

Whenandis AND or XOR or constant, one dimension of the output will be the output of gatewhen RNN circuit is calculating the output atposition.

Whenandis an input or hidden state gate or, one dimension of the output will be the position in the current chain of thought where the input bit or hidden state bit is copied from and the other dimension will be the square of that position

When, the output remains the same as the input to Layer 2 FFN.

Layer 3 Attention will be of width. Together with Layer 2 FFN, Layer 3 Attention will implementheads () to copy the output at the position where the input bit or hidden state bit is copied from. When the type of gateis not input or hidden state gate, the result will not be used in the following calculation.

Layer 3 FFN will be of width. The output of the layer will be

When, one dimension of the output will be output of gatewhen RNN circuit is calculating the output atposition.

When, the output remains the same as the input to Layer 3 FFN.

Layer 4 Attention will haveheads and each head will be of width. Headwill copy the first dimension of the output of Layer 3 FFN at positionand weight each of them byand add them in one dimension. The Layer 4 FFN will calculatewhen the first dimension of the input isandotherwise. Hence, for each, thetoken contains a dimensionwhich is thedimension of the output of the RNN at position.

Layer 5 Attention will haveheads and each head will be of width. Headwill copy the dimensionof the output of Layer 4 FFN at positionto a disjoint dimension. The Layer 5 FFN will then make sure the output of Layer 5 satisfies the following:

When, one dimension of the output will betimes the output of gatewhen RNN circuit is calculating the output atposition plus, which will decode to the corresponding value.

When, the firstdimension of the output will be the same as the output of the RNN at position, and the rest dimension will be, which will decode to the same token as the chain of thought of the RNN at position.

This concludes the construction of the Transformer.
âˆŽ

SECTION: Proof of
We will first state the theorem for clarity.

*

When taskis Index, given an input sequence that ends with a query token, RNN will generate the following search query sequence:,^(?:\S\s*){}(\S),.

Then the regular expression will match the-th token in the input sequence. The RNN needs to recite the format of the query, remember the indexand calculateto generate the regular expression. As we have shown inand, RNN can recite a fixed sequence at fixed position and memorize the recent input, the above sequence can be generated by an RNN. The explicit construction of the RNN is omitted here.

When taskis AR, given an input sequence that ends with a query token, RNN will generate the following search query sequence:,\b\b(\S+)\b,.

Then the regular expression will match the next token after the first occurrence ofin the input sequence. The RNN needs to recite the format of the query and remember the query tokento generate the regular expression. The explicit construction of the RNN is omitted here.

When taskis c-gram retrieval, given an input sequence that ends with query tokens, RNN will generate the following search query sequence:,\b\b(\S+)\b,.

Then the regular expression will match the next token after the first occurrence ofin the input sequence. The RNN needs to recite the format of the query and remember the query tokensto generate the regular expression. The explicit construction of the RNN is omitted here.

When taskis Counting, given an input sequence that ends with a query tokenand a query threshold, RNN will generate the following search query sequence,(\b\b){},.

Then the regular expression will match the-th occurrence ofin the input sequence. The RNN needs to recite the format of the query and remember the query tokenand the thresholdto generate the regular expression. The RNN can then check whether the retrieval result isto determine if the count is less than. The explicit construction of the RNN is omitted here.

âˆŽ

SECTION: Proof of
In this section, we will prove. We will first state the theorem for convenience.

*

We will first define the sequence that the retrieval-augmented RNN will generate and then construct an RNN that can generate such a sequence.

We will use a variant ofto generate the sequence and we will use the concatenation of the tokenization of the sequence returned byas the sequence that the retrieval augmented RNN will generate.

We can use similar proof inby having the RNN memorize local sequences and determine the phase ofit is in. The RNN will maintain the length of() and the top ofin the state () and it is easy to check that the retrieval function will retrieve the correct result for each search query. The way to determine the next vertex in the stack is the same as in the proof of. We will omit the simple but tedious detailed construction here.
âˆŽ

SECTION: Proof of
In this section, we will prove. We will first restate the theorem for convenience.

*

We will denote the state ofas(we will useas the initial state) and the vocabulary ofas. We will assumeoperates on an infinite tape, which is a sequence of cells indexed by. We will also assume that the tape is initialized with all cells beingexcept for thecell starting at. The Turing machine also has a pointerthat points to a cell in the tape. The pointer is initialized to. At each time step, the Turing machine reads the cell pointed byand updates the cell pointed byand the pointeraccording to the transition function, which takes the current state and the current cell value (could be empty, which corresponds to) as input and outputs the new state, the new cell value and the direction to move the pointer. The Turing machine halts when the state is. Because, the Turing machine will always halt insteps. We will useas the value on the-th cell onbefore thetimestep. We will useas the value of the pointer before thetimestep andas the state of the Turing machine before thetimestep. We will further useas the direction of the pointer movement before thetimestep.

We will first define the sequence that the retrieval-augmented RNN will generate and then construct an RNN that can generate such a sequence.

The input token sequencewill be as followed,

Here all the symbols on the tape are represented by one special symbol in the vocabulary. Given this input token sequence, the retrieval augmented RNN will generate the following output token sequence,

Hereis defined as

The output token sequence simulates the Turing machineon the tapedue to the following lemma.

The proof is by induction, for, the result holds. For any, we only need to notice thatis the only cell that can be updated at time.
âˆŽ

Given the input, the RNN can first iterate overtoand generate the firstsearch queries and results by maintaining a counter in its state and memorizing the most recent search result (). Then it is easy to see that the retrieval oracle will generate the correctgiven the input. Therefore, we will only need to construct an RNN that can generate the rest part of.

We will assume the RNN maintains the state and pointer of the Turing machine in its state and show that they can be updated.

Based on, the RNN can maintain constant recent token types in its state, we will assume the RNN memorize the last tokens up to the most recentand also calculate the position relative to the most recent. By a lookup table in the FFN, the RNN can output the fixed format of the search query. Similarly, RNN can output the. To generate the update, the RNN can use a FFN withwidth to memorize the transition function of the Turing machine and output the update. Then, the RNN can use the memorized recent input to update the state and the pointer of the Turing machine at the next. The proof is then complete.

âˆŽ

SECTION: Proof of
*

The proof here is essentially the same as the construction of the Transformer in. We would use the same Transformer layer to solve. The only difference is that we would use the output of the RNN, instead of FFN, as the input of the Transformer layer.Also for Counting, instead of using afunction, we write the query token in the state of the RNN ().
âˆŽ

SECTION: Proof of
*

The proof is similar to the proof of. However, instead of using regular expressions to retrieve the next neighbor and parent, we will need to use the Transformer layer. The Transformer layer can retrieve the parent through an attention head implementing the match closest head () if the RNN part maintains the predecessor of each node in the chain of thought.

Retrieving the next neighbor is more complicated and we will usesteps of the chain of thought to do that. Given an edge, we will first use one match head to retrieve the positionofin the input sequence and write it to the chain of thought. Then we will use twoheads to retrieve the edge that containsand is closest toforiteratively until the heads return an edge that is notorreaches. Herecan be computed through doubling one of the dimensions in the state of the RNN and reset that dimension toafter termination.
We will then compare the retrieved edge with the father ofto check if it is the same. If it is the same, we will search the next neighbor ofafter the parent ofin the same way. The other part of the proof is similar to the proof of.
âˆŽ

SECTION: Proof of
*

Under the same formulation of proof of the. The hybrid RNN will output the following sequence.

Note thatstill holds. We only need to prove that the hybrid architecture can generate the above sequence.

The way RNN maintains the pointers and the states is the same as the proof of. Given each pointer value, we can retrieve the last value of the cell at the pointer through the one layer of attention by implementing a match closest head ().
âˆŽ