SECTION: DCT-HistoTransformer: Efficient Lightweight Vision Transformer with DCT Integration for histopathological image analysis
In recent years, the integration of advanced imaging techniques and deep learning methods has significantly advanced computer-aided diagnosis (CAD) systems for breast cancer detection and classification. Transformers, which have shown great promise in computer vision, are now being applied to medical image analysis. However, their application to histopathological images presents challenges due to the need for extensive manual annotations of whole-slide images (WSIs), as these models require large amounts of data to work effectively, which is costly and time-consuming. Furthermore, the quadratic computational cost of Vision Transformers (ViTs) is particularly prohibitive for large, high-resolution histopathological images, especially on edge devices with limited computational resources. In this study, we introduce a novel lightweight breast cancer classification approach using transformers that operates effectively without large datasets. By incorporating parallel processing pathways for Discrete Cosine Transform (DCT) Attention and MobileConv, we convert image data from the spatial domain to the frequency domain to utilize the benefits such as filtering out high frequencies in the image, which reduces computational cost. This demonstrates the potential of our approach to improve breast cancer classification in histopathological images, offering a more efficient solution with reduced reliance on extensive annotated datasets. Our proposed model achieves an accuracy of 96.00% ± 0.48% for binary classification and 87.85% ± 0.93% for multiclass classification, which is comparable to state-of-the-art models while significantly reducing computational costs. This demonstrates the potential of our approach to improve breast cancer classification in histopathological images, offering a more efficient solution with reduced reliance on extensive annotated datasets.

SECTION: 
Automated detection and classification of breast cancer from histopathological images plays a pivotal role in the clinical diagnosis and prognosis within the framework of the Precision Medicine Initiative. The World Cancer Report issued by the World Health Organization delineates breast cancer as the predominant neoplasm among women globally, with unparalleled morbidity and mortality rates. Statistically, breast cancer constitutes 25.2% of cancer incidences among female patients, occupying the foremost rank, whereas its morbidity rate stands at 14.7%, securing the second rank subsequent to lung cancer in recent epidemiological studies on cancer mortality. Annually, approximately half a million fatalities are attributed to breast cancer, alongside the emergence of nearly 1.7 million new cases, a figure anticipated to escalate markedly. Moreover, histopathologic imagery is acclaimed as the quintessential standard over alternative medical imaging techniques such as mammography, magnetic resonance imaging (MRI), and computed tomography (CT) for the delineation of breast cancer. Notably, the determination of an optimal therapeutic regimen for breast cancer hinges significantly on advanced multi-classification processes. This is attributed to the fact that clinicians, armed with knowledge of the specific subclasses of breast cancer, can promptly intervene in the metastasis of tumor cells, thereby devising tailored therapeutic strategies based on the unique clinical manifestations and prognostic outcomes of various breast cancer subtypes.Indeed, conducting manual classification on breast cancer histopathological images poses significant challenges. The challenges stem from three primary factors: (1) the difficulty in passing down or enhancing the specialized knowledge and extensive experience of pathologists, which results in a scarcity of proficient pathologists in entry-level hospitals and clinics, (2) the laborious nature of the task, which is both costly and time-intensive, and (3) the risk of misdiagnosis due to pathologist fatigue. Therefore, there is a critical and pressing need for implementing computer-aided multi-classification for breast cancer, which could significantly alleviate the workloads of pathologists and minimize the risk of diagnostic errors.Over the past decade, deep learning methodologies have elicited a paradigm shift across multiple sectors, notably within healthcare, by facilitating tasks such as precise disease diagnosis, prognostication, and the advancement of robotic-assisted surgical procedures. Research leveraging deep convolutional neural networks (CNNs) has been conducted to detect breast cancer (BC) utilizing the medical imaging techniques aforementioned. Nevertheless, CNNs are predisposed to innate inductive biases, exhibiting sensitivity to the translation, rotation, and spatial positioning of the target object within images. Consequently, image augmentation techniques are commonly employed during the training of CNN models to introduce variability, although these techniques may not always yield the anticipated diversity within the training dataset. In response to these limitations, there is an emergent focus on the development of deep learning models predicated on self-attention mechanisms. These models demonstrate enhanced resilience to variations in the orientation and location of the object of interest within images, thereby representing a significant advancement in the field.In recent years, ViTs, have emerged as a powerful alternative to traditional Convolutional Neural Networks (CNNs) in computer vision tasks such as image classification, object detection, and image segmentation. The core component of Transformer is the attention mechanism, which computes dependencies between all pairs of tokens in a sequence. However, for a sequence of length, the expressiveness of pairwise attention comes at a quadratic costin both time and memory consumption. As a result, the computational demands of ViTs become a bottleneck, hindering their efficient deployment. This problem holds great importance for deep learning applications tackling high-resolution images and those functioning on edge devices with limited computational resources.In this paper, we pursued two objectives to address this challenge:

SECTION: 
The first objective of this research is to leverage the concept of Frequency Transformers to transform spatial data into the frequency domain. This transformation allows us to remove high-frequency components, effectively reducing the size of the input. By decreasing the input size, we can alleviate the computational demands associated with self-attention weight calculations. The novelty of this objective lies in the application of frequency-based transformations for reducing input data size which has the potential to significantly reduce computational costs while preserving essential information for accurate predictions.

SECTION: 
The second objective is to tackle the potential loss of local interactions that may occur when reducing the input size. To overcome this challenge, we propose incorporating MobileBlock convolutions into the architecture. We aim to preserve the ability of the model to capture fine-grained details and local patterns in the data. This objective ensures that the proposed model maintains its capability to handle local interactions while benefiting from the computational efficiency achieved through the reduction of input size.Our proposed DCT-Conv block places two parallel branches at the forefront - DCT-Attention and MobileConv, each providing independent preprocessing to raw image data. Figure 2 shows the overall architecture.

SECTION: 
Vision Transformers (ViTs) were first introduced by Dosovitskiy et al., marking the beginning of a new era in image recognition tasks. However, these initial ViTs were associated with high computational costs and quadratic complexity for both time and memory due to their global self-attention mechanism. As a result, subsequent research efforts shifted towards proposing lightweight ViTs that aimed to strike a balance between performance, computational cost, and speed.One approach has been to tune the transformer part of the model, enhancing self-attention mechanisms and redesigning the Transformer blocks while maintaining lower computational costs. Swin-Transformerproposed an efficient self-attention mechanism, employing window-based and shift mechanisms to capture long-range dependencies efficiently. While effective, its high parameter and FLOPS count hindered its suitability for resource-constrained edge devices and mobile applications. In response, PSLTproposed ladder self-attention blocks with a progressive shift mechanism which divides the input feature map into multiple branches, where each branch shifts the obtained features in different directions and divides them into multiple windows for window-based self-attension layer. This enables the aggregation of output features from diverse windows, allowing for efficient modeling of long-range interactions. This approach significantly reduces the number of parameters and FLOPS. However, further exploration of alternative computation methods for faster operations is needed, as its slower inference rate suggests. In order to address the challenges associated with slow inference speeds, EfficientViT, a high-speed vision transformer family adopts a sandwich layout that improves memory usage, placing a single memory bound Multi-Head Self-Attention layer between efficient Feed-Forward layers. Furthermore, computation costs are minimized through the integration of a novel cascaded group attention module that feeds each attention head with different splits of the complete feature. The EfficientViT excels in enhancing computational efficiency and speed, offering a practical solution for real time edge applications.
Another research approach that aims to address the challenges of lightweight ViTs involves exploring hybrid models that combine the strengths of convolutions capturing local dependencies and transformers in handling global and long-range dependencies.. MOATintroduces a novel building block that includes an MLP after the self-attention layer with a mobile convolution, and then reorders and places it before the self-attention layer. By doing so, this approach enhances representation capacity and delegates downsampling to strided depth-wise convolution. However, further optimization is needed considering its relatively high parameter and FLOPS count compared to other state-of-the-art (SOTA) lightweight hybrid models.Studies in the field of lightweight models like the ones mentioned earlier have primarily focused on designing delicate network architectures, often neglecting the optimization of training strategies. However, it has been observed that proper pre-training can lead to comparable performance of even vanilla lightweight ViTs compared to more complex designs. In this context, MAEevaluated self-supervised pre-training methods on ViT-Tiny and proposed MAE-Tiny, which is a vanilla ViT-Tiny model pre-trained using Masked Image Modeling (MAE). MAE-Tiny exhibited limitations, as it struggled to effectively leverage large pre-training datasets and performed poorly on downstream tasks with limited data. To overcome these limitations, DMAE-Tiny was introduced which involved distilling knowledge from a larger MAE model into the smaller MAE-Tiny model during the pre-training phase. It significantly improved results on data-limited classification and detection tasks where MAE-Tiny had previously shown deficiencies. However, these methods do not fully exploit the advantages of large-scale pre-training data.In histopathology applications, several studies have utilized ViTs to improve classification accuracy. Wang et al.proposed a semi-supervised learning procedure based on ViT, incorporating adaptive token sampling (ATS) and consistency training (CT) strategies. This approach combines supervised and unsupervised learning with image augmentation, achieving an average test accuracy of 0.98 on the BreakHis dataset. Tummala et al.used a variant called Swin Transformer (SwinT), which uses non-overlapping shifted windows to improve efficiency. Their model is an ensemble of pre-trained SwinT models which includes the tiny, small, base, and large variants. Alotaibi et al.introduced an ensemble model merging ViT and Data-Efficient Image Transformer (DeiT) Its novelty lies in the addition of DeiT, which uses an extra input called the distillation token. The learning process of the distillation token involves backpropagation, where it interacts with classes and patch tokens through the self-attention layers.

SECTION: 
SECTION: 
The BreaKHis dataset comprises 7,909 microscopic RGB images derived from the surgical biopsy of breast tumors across 82 patients, captured at magnifications of 50×, 100×, 200×, and 400×. Figure 1 displays representative images from the dataset across these varying levels of magnification. The dataset encompasses both benign and malignant tumor subtypes. Specifically, the benign subtypes consist of fibroadenoma, tubular adenoma, phyllodes tumor, and adenosis, while the malignant categories include ductal carcinoma, papillary carcinoma, lobular carcinoma, and mucinous carcinoma. Table 1 provides detailed information regarding the dataset, organized by tumor type and magnification levels.

SECTION: 
The Discrete Cosine Transform (DCT) is a widely used transformation technique in image processing, primarily known for its ability to compactly represent the energy of a signal in the frequency domain. This characteristic makes the DCT particularly useful for tasks such as image compression and noise reduction. While the DCT is originally defined for one-dimensional (1D) signals, it can be extended to two-dimensional (2D) images to analyze and manipulate spatial information in a more effective manner.

The DCT of a one-dimensional signalof lengthis defined by the following formula:

In this formula,represents the DCT coefficient at frequency index,is the input signal, andis the length of the input signal.

In the two-dimensional (2D) case, the DCT is applied to images as a matrix of pixel values. It extends the 1D DCT to analyze both spatial dimensions of an image. The 2D DCT is defined as:

X[u, v] =(u)(v) ∑_x=0^M-1 ∑_y=0^N-1 x[x, y] ⋅cos[M (x + 12) u ] ⋅cos[N (y + 12) v ]

where:

denotes the pixel value at positionin the image.

is the DCT coefficient at frequency.

andare normalization factors:

Our proposed DCT-Conv block places two parallel branches at the forefront - DCT-Attention and MobileConv, each providing independent preprocessing to raw image data. Figureshows the overall architecture.

SECTION: 
DCT-Attention branch: The DCT-Attention branch (Figrue 2 (b)) consists of three key steps: DCT and Low Pass Filtering, Self-Attention with DCT Output, and IDCT.
First, we apply the Discrete Cosine Transform (DCT) to convert the input image into frequency components. The DCT separates the image into different frequency bands, where low-frequencies represent important global patterns and high frequencies capture fine details. Then we normalize the DCT coefficients to handle the DC component effectively. This ensures that the DC component, which represents the overall energy of the image, is retained while preventing it from dominating the feature space. By normalizing, we balance the influence of the DC component with other frequency components, preserving essential global information without overemphasis. After the DCT, we perform a lowpass filtering operation, which scales down the input dimensions by a factor of ”r” (resulting in an input size of to retain only the low-frequency components and discard the high-frequency components. This filtering helps reduce the input size for attention layer by focusing on preserving only low-frequency components. Next, Inspired by the Cascade Group Attention layer (CGA), we modify the self-attention mechanism to enhance efficiency. Instead of using the same full feature for all attention heads, we split the feature into channel-wise partitions. Each attention head receives a different split, promoting the learning of distinct patterns and reducing computational redundancy. The attention maps of each head are computed in a cascaded manner, progressively refining the feature representations (Figure 2 (c)). After the self-attention, we apply the Inverse Discrete Cosine Transform (IDCT) to reconstruct the spatial representation from the frequency domain. This step bridges the gap between the frequency based information obtained and the spatial information required for subsequent processing. Finally, we introduce a padding operation to adjust the size of the image and match it with the output size of the MobileConv block. This padding ensures compatibility between the feature maps from the DCT-Attention branch and the subsequent processing in the MobileConv branch.

SECTION: 
MobileConv Block Branch: To capture local dependencies that may be lost during the reduction of input size after low pass filtering, we introduce a MobileConv branch (Figure 2 (a)) to extract local information from the raw input data. This branch complements the global patterns learned through the self-attention mechanism by capturing detailed local features, thereby enhancing the overall representation.
Finally, the outputs from both branches are combined to leverage both the global contextual information obtained from the DCT-Attention branch and the local features captured by the MobileConv branch. This integration allows us to enhance the overall representation by benefiting from the strengths of both branches. We organize the blocks into different stages within the architecture which allows for customizing and adjusting the depth and complexity of the model based on the specific needs of the task or dataset. The stage layout serves as a framework for structuring the model and optimizing its performance by effectively allocating computational resources and managing the flow of information through the different stages.

SECTION: 
SECTION: 
Due to the class imbalance present in both datasets, utilizing solely accuracy (Acc) as a measure does not accurately convey the actual effectiveness of a model. Hence, along with Acc, we incorporate additional metrics such as precision (Pre), recall (Rec), and F1 scores for a thorough assessment of performance. The formulation of these metrics is provided in equations 1-4.

In this context, TP, TN, FP, and FN denote the counts of true positives, true negatives, false positives, and false negatives, correspondingly. Precision (Pre) measures the proportion of false alarms, with a higher Pre indicating a reduced number of false alarms generated by the model. Recall (Rec), on the other hand, quantifies the number of positive instances that were not detected. Therefore, a higher Rec signifies a decrease in missed positive cases. The F1 score is the harmonic mean of Pre and Rec, offering a metric more appropriate than accuracy (Acc) for evaluating classification tasks when dealing with datasets that are not balanced.

SECTION: 
We evaluated the proposed model on the BreakHis dataset with a batch size of 32. The model’s architecture consists of a total of 7966589 parameters. It includes four blocks, each with MobileConvolution branches and DCT-Attention branches. Figureshows the model configuration. During training, we applied data augmentation techniques, including RandomHorizontalFlip, RandomRotation, and RandomCrop, to the training images, while the validation images were used without augmentation. The model was trained from scratch using the AdamW optimizer for 50 epochs, with an initial learning rate of 0.0001 and a weight decay of 0.01. The cross-entropy loss function was used to measure performance.

SECTION: 
Our proposed model demonstrates a high-performance level, achieving an accuracy of 0.9698 ± 0.0048, a F1-score of 0.9564 ± 0.0049, a precision rate of 0.9565 ± 0.0049, and a specificity rate of 0.9566 ± 0.0049 (Table II, Fig. 4). This level of accuracy is comparable to other state-of-the-art models while significantly reducing computational costs. The model leverages parallel processing pathways in the initial stages, where the DCT-Attention and MobileConv modules operate. This dual-pathway approach transforms the data from the spatial domain to the frequency domain, capturing more global information about the entire image rather than focusing solely on individual pixels.
In transitioning to the frequency domain, we apply low-pass filtering, which effectively reduces the input size by eliminating high-frequency components typically containing detailed information, noise, and other less important details. This selective retention prioritizes critical low-frequency components, enhancing the recognition and classification accuracy of our model.
Additionally, the separated local features from the MobileConv branch are integrated back into the final output. This integration further boosts the model’s ability to recognize and classify images accurately across various tasks. The results from Tables IV and V below support these claims.
As shown in Table III, In multi-class classification tasks, the model achieved an accuracy of 0.8785 ± 0.0093 , a recall of 0.8735 ± 0.0093, a precision of 0.8771 ± 0.0091, and an F1 score of 0.8731 ± 0.0096 .(Fig. 5) These results illustrate the model’s competency in handling more complex classification tasks while maintaining reasonable performance metrics.
The performance comparison table(Table IV) shows that our model outperforms other existing methods across all magnification levels (40X, 100X, 200X, 400X). Notably, at 200X magnification, our model achieves the highest performance with an accuracy of 97.68%, illustrating its superior ability to handle different resolutions and magnifications in image classification tasks.
These comprehensive results highlight the significant advancements our model offers in terms of accuracy and computational efficiency, confirming its effectiveness across a broad range of classification tasks.

SECTION: 
In this study, we have presented a novel approach to breast cancer classification from histopathological images using a lightweight Vision Transformer model. Our approach combines Discrete Cosine Transform (DCT) with self-attention mechanisms to achieve a balance between performance and computational efficiency. By incorporating DCT-Attention and MobileConv branches, our model successfully reduces the computational burden associated with high-resolution images while maintaining a high level of classification accuracy. The effectiveness of our approach lies in its ability to leverage frequency-domain information for efficient data representation and its ability to capture both global and local features through the integration of the DCT-Attention and MobileConv components. Our results highlight the potential of hybrid transformer architectures in the context of medical image analysis, specifically for histopathological image classification.

SECTION: 
While our model demonstrates strong performance and efficiency, there remains considerable room for improvement.Our current approach uses a simple low-pass filtering mechanism to discard high-frequency components. Future research could investigate more sophisticated methods for selecting DCT coefficients that focus on identifying and preserving those that have the most significant impact on the model’s decision-making process. Techniques such as adaptive thresholding or coefficient pruning based on feature importance could be explored to better balance the trade-off between computational efficiency and model performance.Although the DCT-Attention mechanism has proven effective, there is potential for further refinement of attention mechanisms to improve both the accuracy and efficiency of the model. Exploring advanced attention techniques or hybrid attention mechanisms that integrate DCT with other forms of attention could offer new insights into optimizing the model for specific types of histopathological features.

SECTION: APPENDIX
SECTION: References