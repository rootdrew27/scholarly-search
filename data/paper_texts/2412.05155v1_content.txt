SECTION: Multimodal Fact-Checking with Vision Language Models: A
Probing Classifier based Solution with Embedding Strategies

This study evaluates the effectiveness of Vision Language Models (VLMs) in representing and utilizing multimodal content for fact-checking. To be more specific, we investigate whether incorporating multimodal content improves performance compared to text-only models and how well VLMs utilize text and image information to enhance misinformation detection. Furthermore we propose a probing classifier based solution using VLMs. Our approach extracts embeddings from the last hidden layer of selected VLMs and inputs them into a neural probing classifier for multi-class veracity classification. Through a series of experiments on two fact-checking datasets, we demonstrate that while multimodality can enhance performance, fusing separate embeddings from text and image encoders yielded superior results compared to using VLM embeddings. Furthermore, the proposed neural classifier significantly outperformed KNN and SVM baselines in leveraging extracted embeddings, highlighting its effectiveness for multimodal fact-checking.

Multimodal Fact-Checking with Vision Language Models: A
Probing Classifier based Solution with Embedding Strategies

Recep Firat Cekinel1,
Pinar Karagoz1,
Çağrı Çöltekin2,1Middle East Technical University, Turkiye2University of Tübingen, GermanyCorrespondence:rfcekinel@ceng.metu.edu.tr

SECTION: 1Introduction

Social media platforms are increasingly becoming the primary source of news for many people. However, these platforms are susceptible to the rapid spread of fake stories, which can be used to manipulate public opinionAllcott and Gentzkow (2017). Fabricated posts may include false text, images, videos, or speech contentAlam et al. (2022); Akhtar et al. (2023); Comito et al. (2023), designed to deceive social media users. Therefore, automated fact-checking systems should be able to consider information from different modalitiesAbdali et al. (2024). For instance, on the Snopes website, a claim111https://www.snopes.com/fact-check/hitler-trump-image-fake/about an edited image was proven to be fake by providing the original image and explaining how it was fabricated to manipulate public opinion about public figures. To verify the truthfulness of such content, it is essential to process both text and image information (see Figure1).

A vision language model (VLM) consists of an image encoder, a text encoder and a mechanism such as contrastive learningBordes et al. (2024)and cross attentionChen et al. (2022)to fuse text and image information. By this way, the model leverages the text and visual information while generating a response text. VLMs consist of billions of parameters and fine-tuning these models requires significant computational resources. Although parameter-efficient fine-tuning approachesHu et al. (2022); Liu et al. (2024c)have proven to be very effective for large language models, VLMs do not scale well horizontally. Consequently, such VLMs cannot be fine-tuned with moderate batch size and sequence length on a single GPU for problems like fact-checking that requires long text inputs.

Instead of fine-tuning, probing classifiers are trained on the representations of a pre-trained modelKunz and Kuhlmann (2020)to predict linguistic features such as dependency parsingAdelmann et al. (2021)and POS taggingKunz and Kuhlmann (2021). A key advantage of probing classifiers is their ability to assess how well the pre-trained model has captured linguistic properties. In this study, we aim to evaluate how VLMs leverage both text and images for the fact-checking task by training a probing classifier. The following research questions are addressed in the paper.

RQ1: Validating the need for multimodality:Does incorporating multimodal data improve performance in the fact-checking task or are text-only models sufficient?

RQ2: Leveraging multimodal content:How effectively do VLMs utilize both text and image information to enhance fact-checking performance?

RQ3: Evaluating probing classifiers:How does a probing neural classifier compare to baseline models in the context of the fact-checking task?

This study proposes a probing classifier that involves extracting the last hidden layer’s representation and using it as input for a neural network. By introducing this pipeline, we aim to elaborate on the utilization of multimodal information, text and image, compared to embeddings extracted from discrete text-only and image-only models for the fact-checking problem. The source code is available at the following anonymousGitHub repository222https://github.com/firatcekinel/Multimodal-Fact-Checking-with-Vision-Language-Models

SECTION: 2Related Work

Shared tasks such as FEVERThorne et al. (2018), CLEF2018Nakov et al. (2018)and AVeriTeCSchlichtkrull et al. (2023)evaluate fact-checking systems on textual claims. Although LLMs achieved high success rates on fact-checking with English data even in zero-shot settingsHoes et al. (2023),Zhang et al. (2024)emphasize the need for language models that are specifically pre-trained on the target language. SimilarlyCekinel et al. (2024)investigate cross-lingual transfer learning using LLMs. Additionally,Cheung and Lam (2023)incorporate external evidence during instruction-tuning to enhance the knowledge of LLMs. Moreover,Yue et al. (2023)focus on cross-domain knowledge transfer with in-context learning.Tang et al. (2024)verify the factuality of synthetically generated claims against grounding documents. LLMs are also used for explanation generationBangerter et al. (2024); Zeng and Gao (2024); Mediratta et al. (2024)and neuro-symbolic program generationPan et al. (2023)for fact-checking. While these works primarily focus on enhancing models’ knowledge, we aim to explore how they can leverage different modalities.

While SpotFake+Singhal et al. (2020)concatenates extracted text and image features for further processing through feed-forward layers, CARMNSong et al. (2021)fuses multimodal information using a cross-modal attention residual network. Pre-CoFactv2Du et al. (2023)implements a multi-type fusion model that uses cross-modality and cross-type relations. COOLANTWang et al. (2023)implemented a contrastive learning based fusion method for image-text alignment.Gao et al. (2024)incorporates the information extracted from the tweet graph with text and image embeddings for improving fake news detection.Liu et al. (2024b)examined the impact of audio in multimodal fact-checking by proposing a framework that fuses text, video and audio information with the cross-attention mechanism.Wang et al. (2024a)align news text with images by cross-modal attention model.

Geng et al. (2024)propose an evaluation framework for VLMs that assesses the pre-trained knowledge of these models in fact-checking without evidence. RAGARKhaliq et al. (2024)presents a RAG-based model that reframes the problem as question-answering for retrieved evidence pieces. MMIDRWang et al. (2024b)trains a distilled model to generate explanations. SARD frameworkYan et al. (2024)applies multimodal semantic alignment to integrate multimodal network features. LVLM4FVTahmasebi et al. (2024)is an evidence-ranking approach and was evaluated on two benchmark datasets using LLMs and VLMs with zero-shot setting.

Although recent studies have focused on developing multimodal models for fact-checking using various fusion approaches, we aim to explore how effectively VLMs utilize different modalities.Geng et al. (2024)also evaluated the robustness of recent VLMs for this problem by comparing the pre-trained knowledge of selected models and their prediction accuracy and confidence rates in zero-shot and few-shot settings. In contrast, we aim to leverage VLM representations by proposing a pipeline that trains a classifier using these embeddings. Furthermore, our primary focus is on utilizing multimodal information. In the experiments, we evaluate the intrinsic fusion of multimodal information against the extrinsic fusion of separate text-only and image-only representations.

SECTION: 3The Proposed Method

SECTION: 3.1Feed-Forward Veracity Classifier

We introduce a probing classifier to examine the efficiency of multimodal embeddings compared to separate embeddings extracted from text-only and image-only models for veracity prediction. The VLM embeddings fuse text and image modalities intrinsically but distinct text and image encoder embeddings are fused extrinsically by the probing classifier as illustrated in Figure2.

First, the last hidden layer representation is extracted from a VLM or a text/image encoder. The neural classifier either receives the VLM representation or embeddings from the corresponding text encoder and image encoder, then predicts veracity classes. If multiple input tensors are fed to the neural classifier, they are processed by a linear layer and after the first layer, all tensors are resized to a "hidden_size" — a hyper-parameter determined by validation experiments — and then concatenated. We concatenate after the first layer because the text and image embedding sizes vary significantly. To utilize both types of information equally, we resize these embeddings to the same dimension and concatenate them afterward. On the other hand, if only the VLM embedding is given to the network as input, two linear layers process the tensor sequentially without any concatenation.

In both of the probing classifier architectures, we implement a weighted cross-entropy loss, with weights determined by inverse class ratios to penalize the majority class more. Since PyTorch’s cross-entropy loss implementation combines softmax with negative log-likelihood loss, the output tensor predicts class probabilities. Consequently, the classifier predicts the class with the highest probability for a given instance.

SECTION: 3.2Models

The primary goal of this study is to examine whether merging image and text information provides gains for the fact-checking problem. To this end, we selected three multimodal models with different fusion mechanisms, as explained below.

Qwen-VLBai et al. (2023b)is a multimodal model introduced by Alibaba Cloud. Qwen-VL is based on the Qwen-7BBai et al. (2023a)language model and Openclip’s ViT-bigGIlharco et al. (2021)vision transformer. The model leverages both modalities through a cross-attention mechanism. Information from the vision encoder is fused into the language model using a single-layer cross-attention adapter with query embeddings optimized during the training phase. In this study, we employedQwen-VL-Chat-Int4checkpoint which was the 4-bit quantized version.

Idefics2Laurençon et al. (2024)is a general-purpose multimodal VLM introduced by Huggingface. It is based on the Mistral-7BJiang et al. (2023)language model and SigLIP’s vision encoderZhai et al. (2023)(SigLIP-So400m/14). The model employs a vision-language connector that takes the vision encoder’s representation as input, using perceiver pooling and MLP modality projection. After these operations, the image information is concatenated with the encoded text representation and fed into the language model decoder.

PaliGemmaBeyer et al. (2024)is introduced by Google and is based on the Gemma-2BTeam et al. (2024)language model and SigLIP’s vision encoderZhai et al. (2023)(SigLIP-So400m/14). Since Gemma-2B is a decoder-only language model, the vision encoder’s representation is fed into a linear projection, concatenated with text inputs, and then fed into the Gemma-2B language model for text generation. In this study, we employedpaligemma-3b-mix-448checkpoint that was fine-tuned on a mixture of downstream tasks.

SECTION: 3.3Datasets

MochegYao et al. (2023)consists of 15K fact-checked claims from Politifact333https://www.politifact.com/and Snopes.444https://www.snopes.com/These websites employ journalists to verify claims who collect evidence documents and write ruling comments. The Mocheg dataset includes both text and image evidence which were crawled from the reference articles linked on the fact-checked claims’ webpages. In cases where multiple evidence images were available for a claim, some collected images were found to be irrelevant. Therefore, for the experiments, only the first image was used as the evidence image.

Factify2Suryavardan et al. (2023)is a challenge dataset containing 50K claims. The authors collected true claims from tweets by Indian and US news agencies and false claims from fact-checking websites. They scraped text and image evidence from external articles and also collected claim images from the headlines of the claims. The fact-verification task was reformulated as an entailment problem where claims were annotated to indicate whether the claim text and image were entailed by the evidence text and image.

SECTION: 4Experiments

We conducted experiments on compute nodes with 4x40GB Nvidia A100 GPUs. While evaluating the models on the datasets, we ignore the instances that have missing text evidence or images. For the Mocheg dataset, we used the original train-dev-test splits. The dataset has three labels"supported","refuted"and"not enough info (NEI)"and we used the labels as it is.

Regarding the Factify2 dataset, since the labels in the test set were unavailable, the original validation data was kept for testing. Instead, we randomly selected 10% of the training set for validation but kept the same percentages of classes in each split. Similar toTahmasebi et al. (2024), we reduced the original five labels to three classes:Support(Support_Multimodal & Support_Text),RefuteandNot enough info(Insufficient_Multimodal & Insufficient_Text) to evaluate the proposed approach.

During the training of the probing classifier using the embeddings, validation experiments were conducted through grid search within the parameter space detailed below. Note that only the best parameter settings are presented in AppendixA. Last but not least, we reported F1-macro scores and F1 scores for each class in the following experiments.

SECTION: 4.1Zero-Shot Inference

In this experiment, we evaluated the zero-shot inference performance of text-only language models and multimodal VLMs on selected datasets. The text-only models were the same language models used in the VLMs for text processing. The purpose of reporting the results on text-only models is to examine the necessity of image content for the fact-checking problem.

For the text-only models, the claim and evidence text were provided as a single prompt, as illustrated in Figure3. Similarly, for each claim statement, the evidence text and evidence image were fed to the VLMs using a similar prompt template. Note that we reported results only for instances where the models responded with "supported," "refuted," or "not enough info." In other words, if the models did not provide a relevant justification, these cases were excluded from the reported results.

Assess the factuality of the following claim byconsidering evidence. Only answer "supported","refuted" or "not enough info".Claim: {claim}Evidence: {evidence}

We also reported the performance of two baseline models, LVLM4VTahmasebi et al. (2024)and MOCHEGYao et al. (2023), for comparison. MOCHEG concatenates the claim, evidence and image to generate CLIPRadford et al. (2021)representations, employing attention mechanisms to update the claim representation based on the evidence. LVLM4V uses two-level prompting, formulating the problem as two binary questions and utilizing the MistralJiang et al. (2023)and LLaVaLiu et al. (2024a)models.

F1-macro scores along with F1 scores for each class are presented in Table1for both text-only and multimodal models. The results show that multimodality can enhance performance depending on the dataset and model configuration. For example, both Idefics-8b and LVLM4FV consistently outperformed their text-only counterparts, while Qwen-VL performed slightly better on the Factify2 dataset but worse on the Mocheg dataset. In contrast, PaliGemma consistently responded with, "sorry, as a base VLM I am not trained to answer this question" to test queries, suggesting that specific policies were implemented in the base VLM to prevent responses to ambiguous queries. As a result, PaliGemma’s inference performance was significantly lower than that of its language model counterpart, Gemma-2b (see AppendixBfor response frequencies). The inference scores of Idefics2-8b suggest that images may provide additional information for fact-checking, likely due to its fine-tuning on a mixture of supervised and instruction datasets, which could explain its success on these datasets. Additionally, LVLM4V’s prompting strategy appears more efficient, as it first checks whether the evidence is sufficient for verification before issuing a second prompt to verify or refute the claim.

A qualitative analysis was conducted to explore the types of claims that were correctly predicted by multimodal models but incorrectly predicted by text-only models. In this analysis, the predictions from both the text-only (Mistral-7B) and multimodal (Idefics2-8b) models were employed on the Mocheg dataset. Although for the fact-checking problem, textual contents are the primary source, images are shown to be useful. After examining the instances that are correctly predicted by the VLM but misclassified by the LLM, we found that such instances required image information to accurately verify the claims, as illustrated in Figure4.

Fact-checking requires long evidence with supporting images, making it computationally challenging to fine-tune the VLMs with moderate batch sizes and sequence lengths on a single GPU. Therefore, we fine-tuned only thePaliGemma-3b-pt-224checkpoint using claim, evidence and claim image as input. The experimental details are given in AppendixC.

Evidence in the Mocheg dataset was collected from reference web articles. In contrast, Factify2 used the justifications provided by fact-checkers as evidence. As a result, Factify2’s evidence is more concise and self-explanatory. However, models should interpret the knowledge from Mocheg’s evidence sources to make a final decision. Because of the GPU memory considerations, evidence texts were cropped if they exceeded 768 words.

Fine-tuning results, presented in Table2, show a significantly lower score of 0.366 on the Mocheg dataset compared to inference results, due to cropping of the evidence text. However, on the Factify2 dataset, the evidence texts were shorter and the model leveraged the key information for making a decision and achieved 0.835 F1-macro score. Note that, on the Factify2 challenge the best-performing model was LogicallyGao et al. (2021)which was also fine-tuned on Factify2 dataset and it achieved 0.897 F1-macro score. Due to computational constraints, we were unable to utilize the long text evidence, particularly in the Mocheg dataset. As a result, we introduced a probing classifier instead of fine-tuning the selected VLMs.

SECTION: 4.2Intrinsic Fusion of VLM Embeddings

In this experiment, we examined whether inherently multimodal models effectively utilize both text and image information. First, we extracted embeddings from selected VLMs and fed these vector representations into a feed-forward multi-class classifier. We extracted the last hidden states and applied mean pooling to each token’s embedding. In other words, the extracted embedding size was(1, ntokens, ndim), wherentokensis the number of tokens andndimis the dimension of each token embedding. Mean pooling provided a single embedding for each instance.

We provided two sets of inputs for extracting embeddings:mm_claimandmm_evidence. Themm_claiminput consists of a claim and a corresponding image while themm_evidenceinput consists of text evidence and an evidence image. For the second setting, we fed two input vectors to the classifier network: themm_claimembedding and themm_evidenceembedding. This is becausemm_evidenceincludes only the evidence representation - evidence image and evidence text - so we provided the claim information by feeding a second input to the classifier.

According to Table3, themm_evidenceinput setting improved F1-macro scores consistently for all models. This indicates that using both text and image evidence improved classification performance on both datasets. The results suggest that the selected VLMs effectively leverage information from evidence text and images on both the Mocheg and Factify2 datasets.

SECTION: 4.3Extrinsic Fusion of Language Model and Vision Encoder Embeddings

Separate embeddings were extracted for text and image information from the vision encoders and language models, respectively. Afterward, we performed mean pooling to obtain one-dimensional vector representations for each instance. For this experiment, we had four input setups:

Input1 (claim+image):The claim representation was taken from the language model and the corresponding image representation was taken from the vision transformer.

Input2 (claim+claim_image+text+text_image):In addition to Input1, the evidence text representation was extracted from the language model and the evidence image representation was extracted from the vision transformer.

Input3 (mm_claim+mm_image):The embeddings extracted when the claim text is given to the VLM and the embeddings extracted when only the claim image is given were used separately.

Input4 (mm_text+mm_image):The embeddings extracted when all textual content is given to the VLM and the embeddings extracted when only the images are given were used separately.

Inputs, except Input2, had two separate text and image embeddings. Only the second setup had four embeddings: claim embedding, claim image embedding, text embedding, and text image embedding. After extracting the embeddings, we trained the proposed probing classifier as described in Section3.1for multi-class veracity prediction. We extracted the embeddings for Input1 and Input2 using the selected multimodels’ text and vision encoders that were also mentioned in Section3.2.

According to Table4, Idefics2 with the third input setup outperformed the other models on both datasets. Note that Idefics2 also performed better in zero-shot evaluations which could indicate that the model might have encountered similar data during pre-training. Therefore, it may leverage its pre-training knowledge while processing these claims.

SECTION: 4.4Ablation Study

Our feed-forward classifier, illustrated in Figure2, consists of two sequential linear layers. The first layer resizes each input tensor to a "hidden size" before concatenating the tensors. We chose this approach because there was a significant difference between the image and text embedding sizes. By reshaping each tensor to the same size before concatenation, we aimed to utilize both types of information more effectively.

However, this approach has some limitations. If concatenation were performed before the first hidden layer, linear layers would be common for all models and input setups. In our approach, only the layers after concatenation are common so as the number of inputs increases, the number of learned parameters for the non-common layers also increases. Additionally, we did not validate the depth of the neural classifier and the network depth might be too shallow for the veracity detection task.

To assess whether the neural classifier effectively learns the intended task, we conducted an experiment using KNN and SVM classifiers with the same training embeddings as mentioned in Section4.2. We set the number of neighbors (k), to seven which was decided after exploring consecutive values. Similarly, we trained SVM classifier with a linear kernel. As shown in Table5, our approach outperformed the baselines on both datasets which implies that the proposed neural classifier leveraged the embeddings much better than the KNN and SVM classifiers on both datasets.

SECTION: 5Discussion

First, we addressed RQ1 by conducting a zero-shot experiment to verify that multimodality improves performance depending on the dataset and model configuration, with models like Idefics-8b and LVLM4FV outperforming their text-only counterparts. Idefics2-8b benefits from image information while LVLM4V’s efficient prompting strategy further enhances verification accuracy.

Additionally, the proposed intrinsic fusion pipeline which utilizes VLM embeddings, outperformed the VLMs’ base inference performance (see Table1and Table3). The only exception was the Idefics2 model on the Mocheg dataset, which had a 0.517 F1-macro inference score while the classifier achieved only a 0.501 F1-macro score. Since the probing classifier has only two layers, it might be too shallow for this dataset and model. Note that the primary goal of this study is not to achieve state-of-the-art scores for the selected datasets. Instead, we aim to evaluate whether recent VLMs improve performance on the fact-checking problem through multimodality or if fusing externally the information from distinct models achieves superior results.

Secondly, we addressed RQ2 by assessing how VLMs leverage text and image information. According to the results, for Idefics2-8b and Qwen-VL, multimodal embeddings were outperformed by discrete models (see Table3and Table4). In other words, extracting separate embeddings resulted in higher F1-macro scores across all models. To be more specific, on the Mocheg dataset, the highest F1-macro scores for Qwen-VL and Idefics-8b were 0.514 and 0.528 respectively. Similarly, on the Factify2 dataset, the highest F1-macro scores were 0.629, 0.670 and 0.590 respectively. Although the best results were achieved with different input setups, for all of the best results, we extracted separate text and image embeddings. In contrast, when embeddings were extracted from inherently multimodal VLMs (as shown in Table3), the maximum F1-macro scores were lower except PaliGemma-3b on Mocheg dataset. This indicates that for the given evaluation framework, using discrete text and image embeddings yielded higher F1-macro scores.

Besides, RQ3 was addressed by conducting an ablation study to examine how the proposed classifier leverages embeddings against KNN and SVM baselines. According to our evaluations, the proposed classifier utilized the extracted embeddings significantly better than the baseline approaches.

Finally, on the Mocheg dataset, the selected models struggle more on "not enough info" cases, as their lowest success rates, even in the best settings, were consistently associated with this class. This may be due to class relabeling, where the authors of the Mocheg dataset reannotated the "Mixture," "Unproven," and "Multiple" cases as "Not Enough Info" which may lead to confusion for the models. In contrast, on the Factify2 dataset, the trained classifier was more successful in distinguishing fake claims compared to other classes. This could be linked to the difference of data domains, as the genuine news was sourced from news agencies while fake claims were crawled from fact-checking sites and satirical articles.

SECTION: 6Conclusion

In this study, we utilize VLMs for multimodal fact-checking and propose a probing classifier-based approach. The proposed pipeline extracts embeddings from the last hidden layer of selected VLMs and fuses multimodal embeddings (extrinsic or intrinsic) into a simple feed-forward neural network for multi-class veracity classification. The experiments show that employing a probing classifier is more effective than the base VLM performance and extrinsic fusion usually outperforms the intrinsic fusion for the proposed approach. As future work, we plan to employ VLMs as assistants rather than as primary fact-checkers. To be more specific, the VLM can be used as an assistant that reviews the given text and image and returns a summary or justification to guide the text-only model for the fact-checking task. Since the LLMs are prone to hallucination and their accuracy depends on the quality of their training data which may be outdated or biased, incorporating knowledge grounding could be a more reliable strategy for real-world deployment.

SECTION: 7Limitations

We tested a limited number of models which may not fully capture the variability across different models and configurations. Additionally, the evaluations were performed on English datasets, restricting the assessment of multilingual capabilities. Furthermore, there is a potential risk that some dataset instances may overlap with the training data of the VLMs which could bias the evaluation results.

Moreover, while extracting embeddings from the selected VLMs and corresponding LLMs, we encountered some computational overhead. More specifically, for some claims, the evidence field exceeded the sequence length of the models or could not fit within our memory constraints. Therefore, we cropped the evidence fields for such instances. Furthermore, while LLMs and VLMs are prone to hallucination, we did not perform any analysis on this phenomenon within the scope of this study.

SECTION: Acknowledgments

This research is supported by the Scientific and Technological Research Council of Turkey (TUBITAK, Prog: 2214-A) and the German Academic Exchange Service (DAAD, Prog: 57645447). We would like to thank the anonymous reviewers for their suggestions to improve the study. We also appreciate METU-ROMER and the University of Tübingen for providing the computational resources.

This project is partially supported by METU with grant no ADEP-312-2024-11484. Parts of this research received the support of the EXA4MIND project, funded
by the European Union´s Horizon Europe Research and Innovation Programme,
under Grant Agreement N° 101092944. Views and opinions expressed are
however those of the author(s) only and do not necessarily reflect those
of the European Union or the European Commission. Neither the European
Union nor the granting authority can be held responsible for them.

SECTION: References

SECTION: Appendix AHyperparameter Values for the Best Models

We set the number of epochs to 20, enabling early stopping with the patience of 5 and monitoring the validation loss. We used the Adam optimizer in combination with a cosine scheduler, employing a warm-up ratio of 0.05. Moreover, we adjusted the cross-entropy loss weight of the neural network according to the inverse class ratios. In this way, the classifier was penalized more for the misclassifications of the minority classes.

We performed a grid search to explore the following parameter space for the results given in Table3and Table4:

learning rate: { 0.00001, 0.0001, 0.001, 0.01, 0.1},batch size: {32, 64, 128},hidden size(h in Figure2): {128, 256, 512 } anddropout: {0.05, 0.1, 0.2, 0.4}.

The parameter settings for the best results are detailed in Table6.

SECTION: Appendix BZero-shot Model Response Frequencies

We used the prompt template shown in Figure3for all models in the zero-shot inference experiments. We expected the models’ responses to contain either "supported," "refuted," or "not enough info." If a model’s response did not contain these labels, we ignored those instances. Additionally, we observed that PaliGemma consistently responded with "sorry, as a base VLM I am not trained to answer this question," which could be due to injected policies. The frequencies of considered cases for each model (with percentages in parenthesis) are given in Table7.

SECTION: Appendix CFine-tuning Parameter Settings

We employed QLoRADettmers et al. (2024)adapter on top of attention weight matrices and fine-tuned only the LoRAHu et al. (2022)adapters for 3 epochs. The batch size was set to 2 with an initial learning rate of 2e-5 using a cosine scheduler and the Adam optimizer. We used the checkpoint with the lowest validation loss. Additionally, we set warm up to 0.02, gradient accumulation to 4 and evaluated on validation set 10 times during fine-tuning. We set the rank of matrices for LoRA adapters to 16, the scaling factor (lora_alpha) to 16 and the dropout rate for the adapters to 0.05. Besides, 16-bit mixed precision, bfloat16, was employed for memory efficiency and faster fine-tuning.