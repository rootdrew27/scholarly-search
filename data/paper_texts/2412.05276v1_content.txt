SECTION: Sparse autoencoders reveal selective remapping of visual concepts during adaptation
Adapting foundation models for specific purposes has become a standard approach to build machine learning systems for downstream applications. Yet, it is an open question which mechanisms take place during adaptation. Here we develop a new Sparse Autoencoder (SAE) for the CLIP vision transformer, named PatchSAE, to extract interpretable concepts at granular levels (, shape, color, or semantics of an object) and their patch-wise spatial attributions.
We explore how these concepts influence the model output in downstream image classification tasks and investigate how recent state-of-the-art prompt-based adaptation techniques change the association of model inputs to these concepts. While activations of concepts slightly change between adapted and non-adapted models, we find that the majority of gains on common adaptation tasks can be explained with the existing concepts already present in the non-adapted foundation model. This work provides a concrete framework to train and use SAEs for Vision Transformers and provides insights into explaining adaptation mechanisms.Code and Demo:

SECTION: Introduction
Foundation models excel at fast adaptation to new tasks and domains with limited extra training data. In the space of vision-language models, CLIPbecame an important backbone for numerous applications.
The CLIP model consists of two transformer networks to encode text and image inputs. Various parameter-efficient adaptation techniques, such as adopting learnable tokens, have been proposed targeting either of the systems. While early works targeted the text encoder, more recently it was shown that joint adaptation of both the text and image encoders can further improve classification performance. Despite these advances in adaptation methods, it remains an open question ofhowrepresentations in a foundation model change during adaptation.

Recently, Sparse Autoencoders (SAEs;) have gained emerging attention as a tool of mechanistic interpretabilityafter showing its effectiveness in widely used LLMs. SAEs map dense model representations, which are difficult to interpret because multiple unrelated concepts are entangled (), to sparse and interpretable () concepts.

In this work, we develop PatchSAE, a new SAE model for the CLIP Vision Transformer (ViT) (Fig.(a)). PatchSAE extracts interpretable concepts and their patch-wise spatial attributions, providing localized understanding of multiple visual concepts that can be simultaneously captured from different regions of a single image (Fig.(b)).
Before analyzing model behaviors through the SAE, we first validate PatchSAE as an interpretability tool. Our PatchSAE identifies diverse interpretable concepts, provides localized interpretation, and performs well across multiple datasets (§). We then explore the influence of interpretable concepts on the final output of the CLIP model through classification tasks.

We use our PatchSAE to shed light on the internal mechanisms of foundation models during adaptation tasks. Recent state-of-the-art adaptation methods for CLIPadd trainable, dataset specific tokens for adaptation, akin to a system prompt in LLMs.
Through extensive analysis, we reveal a wide range of interpretable concepts of CLIP, including simple visual patterns to high-level semantics, employing our PatchSAE as an interpretability tool (Fig.(b)). We also localize the recognized concepts through token-wise inspections of SAE latent activations, while extending it to image-, class-, and task-wise understandings. Furthermore, we demonstrate that the SAE latents have a crucial impact on the model prediction in classification tasks through ablation studies. Lastly, we show evidence that prompt-based adaptation gains the performance improvement by tailoring the mapping between recognized concepts and the learned task classes (Fig.(c)).

Our paper proceeds as follows:
First, we introduce PatchSAE, a sparse autoencoder which allows to discover concepts for each token within a vision-transformer with spatial attributions (§). We train this model on CLIP, and show the interpretability and generalizability of ImageNet-scale trained SAE across domain-shifted and finer-grained benchmark datasets.
We explore the CLIP behavior in classification tasks and how adding learnable prompts changes the model behavior using PatchSAE (§). In the end, we discuss conclusions and broader implications (§).

SECTION: Related Work
Mechanistic interpretabilityaims to interpret how neural networks infer their outputs. To achieve this, it is natural to seek a deeper understanding of which feature (concept) is recognized by each neuron in the neural network. For instance, the logit lensapproach attempts to understand the intermediate layer output by mapping it into the final classification or decoding layer. However, understanding neurons in human interpretable form is challenging due to the polysemanticnature of neurons, where each neuron activates for multiple unrelated concepts. This property is attributed to superposition, where neural networks represent more features than the number of dimensions.

To overcome the superposition phenomenon in neural network interpretation, sparse autoencoders (SAEs)have recently gained significant attention. SAEs decompose model activations into a sparse latent space, representing them as dictionary of high dimensional vectors. Several studieshave applied SAEs to language models. Using SAEs,discovered bias- and safety-related features in large language models (LLMs), demonstrating that these features can be steered to alter the behavior of LLMs. Recent research extended the application of SAEs to vision-language models, such as CLIP.andextracted interpretable concepts from the vision encoder of CLIP andutilized these features to edit image generation in a diffusion model.named the SAE concepts using word embeddings from the CLIP text encoder and used them for a concept bottleneck model.

Distinct from previous works, we propose to use patch-level image tokens for SAEs which allows intuitive and localized understanding of SAE latents and easily transformable to higher (image- / class- / dataset-) level of analysis. Furthermore, we adopt SAE latents masking method to examine the relationship between interpretable concepts and downstream task-solving ability. For the first time, this allows precise investigation of how foundation models behave during adaptation, and how concepts are re-used across datasets.

Adapting vision-language foundation models like CLIP through fine-tuning requires large datasets and significant computation resources. In addition, the generalization ability of the model may be compromised after fine-tuning. As an alternative, prompt-based adaptation has recently emerged, training only a few learnable tokens while keeping the weight of pre-trained models fixed. CoOpproposed adapting CLIP in few-shot learning setting by optimizing learnable tokens in the language branch, whileapplied prompt adaptation to the vision branch. MaPLeimproved few-shot adaptation performance by jointly adding learnable tokens to both the vision and language branches and considered as a base structure for more recent multimodal prompt adaptation methods. Although these studies demonstrate that prompt learning is effective for adapting CLIP, there is still a lack of research focusing on how and why prompt learning enables such adaptation.

Our work focuses on exploring the CLIP image encoder using an SAE on a vision transformer. We choose MaPLeas a representative structure of multimodal prompt adaptation and investigate the internal work of adaptation methods.

SECTION: PatchSAE: Spatial attribution of concepts in VLMs
In this section, we revisit the basic concept of sparse SAE and introduce our new PatchSAE model in §. We then discuss how we discover interpretable SAE latent directions in §. Our analysis includes computing summary statistics of SAE latents, that indicate how often and how strongly one latent is activated, detecting model-recognized concepts by inspecting reference images with high SAE latent activations, and spatially localizing SAE concepts in the image space (Fig.).

SECTION: PatchSAE Architecture and Training Objectives
SAEs typically consist of a single linear layer encoder, followed by a ReLU non-linear activation function, and a single linear layer decoder.
To train an SAE on CLIP Vision Transformer (ViT), we hook intermediate layer outputs from the pre-trained CLIP ViT and use them as self-supervised training data. We leverage all tokens including class () and image tokens from the residual stream output(Fig.(c)) of an attention block and feed them to the SAE.
Formally, we take ViT hook layer output as an SAE input, multiply it with the encoder layer weight, pass to the ReLU activation, then multiply with the decoder layer weight.
The column (or row) vectors of the encoder (or decoder),vectors size of, correspond to the candidate concepts,,. We call the output vector of the activation layer (size of) as.
For simplicity, we usefor the encoder andfor the decoder:

To train the SAE, we minimize the mean squared error (MSE) as a reconstruction objective, and use L1 regularization on the SAE latent activations to learn sparse concept vectors (Fig.(a)):

An ideal SAE encoder maps the dense model representations into multiple monosemantic concepts and an ideal decoder reconstructs the original vector by linearly combining these distinct concepts.

We use a CLIP model with an image encoder of ViT-B/16, which results inimage tokens and atoken as input. It has 12 attention layers with model dimensionof 768. For PatchSAE, we set the expansion factor to 64 that multiplies with, which results in a SAE latent dimensionof 49,152. We take the ViT output from the residual stream of the second last attention layer (i.e., 11-th layer output). Note that we use all image tokens, so the input and output of SAE have a size of (number of samples, token length, model dimension). We average the training loss across all individual tokens. We evaluate the trained SAE for reconstruction ability and the sparsity. We report the training performance of different configurations (§) and show variations for training the SAE on different layers (§) in the Appendix.

SECTION: Analysis method and evaluation setup
After training, we validate our PatchSAE model by interpreting the activated SAE latents from input examples. We first discover the candidate concepts – the–by collecting reference images that maximally activate each SAE latent and compute their summary statistics. Then, we investigate theto see how much the given input aligns with the corresponding SAE latent directions. The token-wise (, patch-wise) investigation allows spatially localized understandings of an image. To derive a global interpretation of the image for an SAE latent, we aggregate the patch-level activations into an image-level activation by counting the number of patches activating the corresponding latent. Similarly, we extend it to class- and dataset-level analysis (Eq.).

As a first step of discovering the interpretable concepts that SAE represents, we consider a set of images that maximally activate each SAE latent as. Given a trained SAE and a dataset, we keep the top-images having the highest SAE latent activation value for each latent dimension,, we havereference images in total. Here, we use image-level activations to select top-images. Fig.(a) illustrates the procedure.

To inform the general trend of an SAE latent, we compute summary statistics of the activation distribution. We use theand theover a subset of training images. Using the class label information from a classification benchmark dataset such as ImageNet, we compute theandfrom the reference images.
Specifically, we compute and interpret the statistics as follows:

represents how frequently this latent is activated. We count the number of images having positive SAE latent activations and divide by the total number of seen images. An SAE latent with a high frequency either represents a common concept or is an uninterpretable (noisy) latent.

is computed by averaging the positive activation value among the activated samples. The mean activation value implies the SAE model’s confidence. A latent direction is more likely to represent a meaningful concept if it has a high mean activation value.

measures how many unique labels activate the latent. Precisely, we compute the probability of a label based on its activation value and compute the entropy as

whereis the summed activation values for label. The entropy being equal to zero indicates that all reference images have exactly the same label. Higher entropy indicates that more labels contribute to the latent’s activation.

. In ImageNet, class labels are organized in a hierarchical structure based on WordNet’s semantic relationships. We leverage this label structure and use the label standard deviation of reference images as a clue for the semantic granularity besides the label entropy when exploring the latents. We discuss more in §.

Using the reference images as an interpretable proxy for SAE latent directions and the SAE latent activation of an input as the similarity with the corresponding latent, we examineconcepts areactive for the input. As depicted in Fig.(b) and (c), theSAE latent activations inform the recognized concepts from each patch. For example, for the input patch containing the “dog’s nose”, the SAE latent having the reference images of “dog’s nose” is active.

To obtain a global concept from the image, we transform the patch-level activations into anactivation.
In specific terms, we first binarize the SAE latent activation of the-th image at-th token for the-th SAE latentusing a small positive numberas a threshold. We call the SAE latent above the threshold as anlatent, otherwise anlatent. Then we count the number of patches that activate the latentand consider it as the image-level activation. Similarly, we obtain the class- () and dataset-level () activation by incorporatingfor the images with the same class or dataset, respectively. Formally, we represent this as below:

From the class- or dataset-level activations, we discover the shared concept within the group. We use these group-wise active SAE latents to analyze the relationship between the interpretable concepts and the model behavior in §.

PatchSAE allows localizing an active latent in the image space. We treat the patch-level latent activation as a soft segmentation mask. Precisely, given an imageand an SAE latent index, we multiply each patchwith the corresponding latent activation valuefor visualization. For example in Fig.(c), we highlight “yellow flowers” or “dogs with black, white, and brown colors” concepts from the input image. Separating the patches relevant to the targeting latent from the input and reference images shows a clearer view of the concept

SECTION: PatchSAE discovers spatially distinct concepts in CLIP
More examples in the following sections are provided in the(Fig.).

As depicted in Fig., we explore the SAE latents guided by the statistics. We observe two big clusters of rarely activated with low activation value (bottom left) and frequently activated with high activation (top right), and one small cluster near the center. Although the statistics do not ensure the interpretability of the latents, we find several interesting patterns. Many latents from the bottom left region with high label entropy are uninterpretable (Fig.(a)). We find more interpretable latents from the second large cluster (top right region). For interpretable latents, lower entropy (Fig.(d)) indicates more distinctive semantics such as a specific class, while the higher entropy latent (Fig.(c)) represents the shared style of reference images. We also observe multimodal latents that activate when certain text appears in the image. For example, Fig.(b) latent detects the text. More examples are in Fig..

As a case study, we compare a pair of images having the same class label but different domains (covariate shift) in Fig.. The commonly activating SAE latent from both images shows the shared concept(Fig.(c)). From both images, thelatent is activated by the relevant regions. Exclusively activating latents (Fig.(c) & (d)) represent discrete concepts such asor. The segmentation map highlights the contributing patches for the concepts.

Although we train our PatchSAE model only using ImageNet training data, we find that the interpretability of SAE latents is transferrable to different datasets. We show that an SAE latent retrieves a consistent concept from different datasets if such concept exists in the dataset. Otherwise, the mean activation value is low and/or the retrieved images are uninterpretable (§). Fig.shows reference images from ImageNet and four fine-grained datasets for two SAE latents and Fig.shows reference images of top-1 task-wise latent.

SECTION: Analyzing CLIP Behavior via PatchSAE
In this section, we seek the relationship between SAE latents and model behaviors under classification tasks. By replacing the model’s intermediate layer output with SAE reconstructed one and ablating the latents used for the SAE decoder, we find that SAE latents contain class discriminative information (§). Comparing the behaviors of CLIP models before and after adaptation on downstream tasks, we explain the major performance gain stems from adding new mappings at SAE latents to downstream task classes, rather than firing additional class discriminative concepts (§).

SECTION: Impact of SAE Latents on Classification
We explore the influence of SAE latents on the final model prediction in classification tasks. We replace the intermediate layer representation of CLIP image encoder with the SAE reconstructed output to steer the model outputby selectively using a subset of SAE concepts. Then we compute cosine similarity between text and image encoder outputs for the classification task. Fig.(a) summarizes the procedure.

We conduct ImageNet-1Kzero-shot classification using OpenAI CLIP ViT-B/16 (§) with an ensemble of 80 OpenAI ImageNet templatesto compute text features for each class and conduct classification by computing cosine similarity between image features out text features (Fig.(a)).

To select the subset of SAE latents that are used for the linear combination in the SAE decoder, we search for class-wise representative concepts. We utilizeactivations (Eq.). In short, we aggregate SAE latent activations from a group of images having the same class label using the training split of each downstream task dataset and find the top-most frequently activated latents. We then control the active latents via masking the SAE latent activation vector before feeding it into the decoder. We replace the original model representations with the reconstructed ones by the masked SAE. We compare the classification accuracies by varying the mask. For example, “on top-” refers to using the mask of a one-hot vector where only the top-latent indices are 1s (active) and the others are 0s (inactive). Contrastingly, “off top-” refers to using a mask filled with 1s except for top-indices being 0s. As ablation, we provide comparisons on using the same number ofselected indices andrepresentative latents (i.e., frequently activated across all classes within the same dataset).

The results for the top-SAE latent masking experiments are shown in Fig.(b)&(c). Using all SAE latents (on all; identity mask) recovers the original classification performance (i.e., using the original model representation without replacing it with the SAE output) with small reconstruction errors (64.82% for the identity mask and 68.25% for the original). Using the all-zero mask (off all) removes all relevant information, and hence results in the accuracy of 0.1%. Ablating randomly selected or task-wise latents do not show significant affect to the classification accuracy until we use sufficient number of latents. On the other hand, ablating the per-class top activating latents shows a crucial impact on classification performance. We observe a clear performance improvement or degradation in accordance with the increased or decreased number of active SAE latents, respectively.
The results of this analysis show that some SAE latents contain rich information that is critical for class discrimination. Moreover, the search for such latents can be narrowed down to the top activating SAE latents that are frequently activated across inputs with the same ground-truth class.

SECTION: Understanding Adaptation Mechanisms
To understand how models are adapted to downstream tasks based on our findings in §, aim to address the following questions: Does adaptation make modelsclass discriminative concepts? Or, do theybetween the used concepts and the downstream task classes? The former question refers to the model improving itsby adaptation (i.e., adapted models capture additional class discriminative latents). The latter implies that both models recognize similar concepts, but the adaptationbetween the activated concepts and the downstream task classes (i.e., adaptation uses concepts that are not closely related to certain classes previously, as class discriminative information).

To answer the questions, we investigate whether the class discriminative SAE latents of zero-shot and adapted models overlap or not. We observe a large overlap between before and after adaptation, which indicates that similar concepts are recognized by the two methods even though the adaptation shows distinctive performance improvement. We thereby conclude our analysis that the major performance gain via prompt-based adaptation stems from tailoring the mapping between (commonly) activated concepts and the downstream task classes.

Following the setup introduced by, we split the downstream task dataset classes into two groups and consider the first half asand the remaining asclasses, then conduct classification on two groups. We use total 11 benchmark datasets (§): ImageNet-1K, Caltech101, OxfordPets, StanfordCars, Flowers102, Food101, FGVC Aircraft, SUN397, DTD, EuroSAT, and UCF101.

append learnable tokens to a frozen pretrained CLIP and train the added tokens on downstream tasks. Specifically, MaPLeadds learnable tokens at the input layer and the first few layers both for text and image encoders (Fig.(a)). In the base-to-novel setting, MaPLe uses few-shot samples from each of the base classes to train the learnable tokens. We use officially released MaPLe weights (§) for the experiments.

Table(the last four columns) summarizes the reproduced classification results of CLIP and MaPLe in base-to-novel settings. We notice that both zero-shot performance and performance improvement by adaptation vary in a wide range across different datasets. To measure the improvement apart from its zero-shot performance, we compute the improvement rate as (adapted - zero-shot) / (100 - zero-shot) and denote it as.measures the improvement via adaptation relative to the remaining potential improvement from zero-shot performance.

Leveraging that prompt-based methods keep model parameters intact as frozen while adding the learned parameters as additional input tokens, we share our PatchSAE trained on the default CLIP model to both CLIP and MaPLe. This allows us to understand the internal mechanisms of the adaptation method by comparing the model behaviors in the shared SAE latent space. We demonstrate the transferability of CLIP-based trained PatchSAE to MaPLe by repeating the top-SAE latent masking experiment with variations to SAE training backbones, SAE latent computing backbones (image encoder), and classification inference backbones (text and image encoder) as CLIP or MaPLe (see §). The results validate the transferability of our PatchSAE to the adapted models under all base-to-novel settings.

We compute class-level top activating SAE latents using the shared PatchSAE for backbone image encoders CLIP and MaPLe for each task. We plot the comparison of class-level latent activations as a scatter plot (Fig.), where each point represents the class-level activation of the latent;(Eq.) with backbones CLIP and MaPLe inandaxes, respectively. We divide the points into several groups including(high in both),(high before and low after adaptation), and(low before and high after adaptation) (Fig.). We set the upper and lower bounds using top-50 and top-100 values, and we use class- and dataset-level activations to analyze class- and task-wise performance improvement, respectively.

The first six columns of Tableshow the normalized count of top activating SAE latents in three groups: high, high-to-low, and low-to-high. For example of EuroSAT base classes, 6.9 latents are highly active in both before and after adaptation, 0.5 previously highly activated latents become non-active after adaptation, and 4.1 previously not active latents became active after adaptation on average. In most cases, SAE latents rarely place in off-diagonal regions. We provide scatter plots for the same comparison in Fig., where we can observe the SAE latent activations are highly correlated. We notice that EuroSAT shows distinctive improvement through MaPLe and the highest count in the low-to-high group and discuss in §.

We compare the impact of the same SAE latent for CLIP and MaPLe. Similar to §, we conduct the top-masking experiment. We provide the results in Fig.(c) and deeper case studies in Fig..
In Fig.(c), regardless of the top-latent selection backbones, MaPLe consistently show better performance using the same number of SAE latents. The result implies that MaPLe makes a better use of the same (number of) SAE latents for classification than CLIP does.
For deeper analysis, we choose two cases in Flowers102 (case 1) and EuroSAT (case 2) that show large performance improvements by adaptation and top-masking while the former task is a finer-grained classification and the latter is classification in special domains. We observe that zero-shot and adapted models activate SAE latents in similar patterns for images of the same class, while the two models show different predictions. In both cases, top activating latents recognize visual attributes that seem providing representative information relevant ground-truth class. Using this same set of latents, MaPLe makes correct predictions while CLIP does not. This result exemplifies MaPLe adding new connections between commonly activated concepts and the downstream tasks.
In essence, our analysis shows that the performance gain of prompt-based adaptation on CLIP can be explained by adding new mappings between the recognized concepts, which do not change much by adaptation, and the downstream task classes.

SECTION: Discussion
By adopting SAEs originally studied on LLMs to vision models, this work contributes to the understanding of the vision part of vision-language foundation models. By following basic settings in previous works, evaluating the training performance including reconstruction and sparsity objectives, and performance comparison with the original model in downstream tasks, we validate our design choices. We propose PatchSAE that provides spatial attribution of the candidate concepts, which advances the interpretability. Through extensive qualitative analysis, we demonstrate the interpretability of our SAE. Furthermore, we provide an interactive demo to share abundant results with transparency. The scope of this work focuses on CLIP vision encoder. We believe that the analysis of different vision encoders and extending it to jointly analyzing the multimodality could provide a more comprehensive understanding of large vision-language models. We leave this as future work.

We use SAEs to shed light on model behaviors and adaptation mechanisms. In order to use the same SAE for both non-adapted and adapted approaches, we focus on prompt-based adaptation method which does not directly update the model parameter but appends learnable tokens as inputs. We choose MaPLe as the prompt-based method because this approach uses learnable tokens in the vision encoder (note that simpler baseline CoOp uses learnable tokens only in text encoder) and shows competitive performance with state-of-the-art methods regardless of its simplicity. Exploring different adaptation methods (e.g., full fine-tuning) as future work could provide deeper insights to adaptation mechanisms of foundation models.

SECTION: Conclusion
Adapting foundation models to specific tasks has become a standard practice to build machine learning systems. Despite this widespread use, the internal workings of models and adaptation mechanisms to target tasks remains as an open question. To address this question, we introduced PatchSAE, a sparse autoencoder that extracts interpretable concepts with spatial attributions from a database of reference images. We provide a detailed framework to train and analyze PatchSAE models on vision transformers. Through controlled experiments on 11 adaptation tasks, we study how adaptation changes the relation between class outputs and concepts.
Surprisingly, our analysis finds that on almost none of the studied tasks, drastically new concepts are introduced for adaptation. Adaptation rather assigns the rightexistingconcepts to the correct classes, and in only one task with more notable distribution shift (EuroSAT), we found a non-negligible number of concepts that got suppressed or newly introduced by the adaptation mechanism.
Our analysis is an example for leveraging PatchSAE to “debug” inner workings of a vision model on the level of concepts. We believe that methods like PatchSAE will become useful in categorizing algorithms to edit, interpret and adapt foundation models, and to build more effective models on particular downstream tasks.

We used publicly available model checkpoints for CLIP () and MaPLe (). We used OpenAI ImageNet templates for zero-shot classification ().
We will open source our code, SAE model weights and raw results for secondary analysis upon publication of the paper.
We only used publicly available datasets following the official implementation of MaPLe () which are cited in the main text.

StS, HsL with comments from JgC;HsL, StS;HsL, JhC;HsL, StS;HsL, JhC;HsL, JhC;HsL, StS, JgC.

HsL was supported through a DAAD/NRF fellowship in the NRF Summer Institute Programme, 2024 (57600422).
This work was supported by the Helmholtz Association’s Initiative and Networking Fund on the HAICORE@KIT partition.

SECTION: References
SECTION: SAE training details and ablation studies
In this section, we provide details about training SAE.

§summarizes the training performance of SAEs used for hyperparameter tuning.

§shows SAE layer ablation study results.

§shows SAE’s generalizability to different datasets.

§justifies SAE’s transferability to adapted models.

SECTION: Training performance
We follow the literature on SAEsto set quantitative metrics. The mean squared error (MSE) loss indicates reconstruction ability. Contrastive loss with and without SAE informs the reconstruction ability as well. Close contrastive losses indicate that the SAE reconstructs the input better. The L1 loss and the L0 metric indicate the sparsity of SAE. The lower the values are, the less number of SAE latents are activated for the given input. We start by reproducing training performance as reported in previous studies. Then we ablated training hyperparameters, expansion factor, ghost gradient technique, and the initialization of decoder bias term (mean vs. geometric median of the training dataset). Furthermore, we ablated model architectures (ViT-B/16 and ViT-L/14), training tokens (vs. all; Fig.), and hook layers (§).

We set the coefficient for L1 regularizeras 8e-5, the learning rate as 4e-4 with a constant warmup scheduling with warmup step of 500, and initialized decoder bias with geometric median. We train SAE using 2,621,440 samples from ImageNet training dataset using ghost gradient. We set the threshold, that we use for transforming patch-level activations into global views (Eq.), to 0.2 (value of -0.7).

In addition to the frequency and the mean value of activation distribution, we use the label entropyand the label standard deviation that can give an intuition about concept granularity. The label standard deviation is tailored for a labeled dataset such as ImageNet, where the label structure contains a hierarchical structure of English words. In this case, the standard deviation indicates whether the latent is capturing a distinct label from ImageNet dataset or other attributes such as the style (or domain) or patterns of image. For example, thelatent might be activated by different breeds of dogs, so the number of unique labels is high (high entropy) but the gap between labels might be low (low standard deviation). On the other hand,latent might be activated by diverse blue objects or scenes whose labels can be very far away (high entropy and high standard deviation).

Although the quantitative metrics of reconstruction and sparsity validate that SAE is trained as intended, they do not provide rich information about the validity and interpretability of SAEs. Therefore, we utilize SAE latent summary statistics and reference images for qualitative evaluation. We mostly follow the configurations as selected by, confirming that the chosen setup shows reasonable performance. To be compatible with both zero-shot and adapted method MaPLe, which releases official weight on ViT-B/16, we choose model architecture as CLIP ViT-B/16. For a deeper understanding, we use all image tokens in addition to thetoken. We also note recent progress of SAE architectures and training techniques such as gated SAE, using SAE on other components’ output (such as attention output or MLP output), but we focus on the base architecture of SAEstreating the advanced techniques as out-of-scope.

SECTION: SAEs on different layers
We ablated the model layers to train SAEs. Using ViT-B/16 that has 12 residual block layers, we choose four layers: 2, 5, 8, and 11. We train SAEs for each layers (Fig.).than the ones on the shallower layers. Segmentation masks show that top-3 SAE latents represent the semantic of the major object (the Golden Gate Bridge) in layer 11 while layer 8 and 5 see the triangle shape of the object. Top latents are less interpretable for layer 2. This is unsurprising as the segmentation mask and the activation value indicate the latent is activated by the specified token (local attention), not interpreting a meaningful pattern from the entire semantics. The segmentation mask in reference images separates the concept from the reference images, which enhances the interpretability. We use the same training configuration for all four SAEs.

SECTION: SAE transferabiltiy to different datasets
In Fig., we show reference images from ImageNet and four fine-grained datasets (Flowers102, Caltech101, OxfordPets, and Food101) for two SAE latents. For thelatent (Fig.(a)), we retrieve images containing Christmas-related objects or styles. Fig.(b) latent representsand/or. From Flowers102 and OxfordPets, the mean activation value of this latent was low, which explains unclear relationship between reference images and the concept of the latent. The mean activation value is higher in Food101, where the retrieved images are more related to the concept: the left top image showsgame in the background and the left bottom image shows a game character wearing. The results demonstrate that a SAE latent retrieves a consistent concept from different datasets if such concept exists in the dataset). Otherwise, the mean activation value is low and the retrieved images are uninterpretable. See another example in Fig..

SECTION: SAE transferabiltiy to adapted models
To justify using CLIP-based trained SAE for analyzing MaPLe, we repeat top-SAE latent masking experiment under various settings. We observe consistent results whether using CLIP or MaPLe-based SAE latents and demonstrate the transferability of SAE for multimodal prompt-based adaptation method.

In Fig., we use SAE trained on MaPLe (trained on ImageNet-1K). We compare four settings where classification backbone can be either CLIP or MaPLe and SAE can be either CLIP-based or MaPLe-based trained models.

Fig.andshows top-SAE latent masking results on 11 datasets. Here, we fix to use class-level activations to select top-latents and to use CLIP-based trained SAE. We compare four settings of adopting CLIP or MaPLe as SAE activation computing backbone and classification backbone. Using different classification backbone showed different patterns while selecting SAE activation backbone does not show significant difference, which supports the transferability of our SAEs under all base-to-novel settings.

SECTION: Additional results and supporting Figures
Fig.shows a screenshot ofshort instruction.

Fig.shows an example ofSAE latents.

Fig.reference images across datasets.

Fig.andshow full results ofexperiment using MaPLe adapted on each dataset.

Fig.explains three groups in top activating SAE latent comparison.

Fig.andshowscase studies in EuroSAT, DTD and UCF101 datasets.

Fig.shows theof aggregated class-level SAE latents in 11 datasets.

Fig.shows a detailedof Flowers102.

Fig.supplements Fig.and provides more case studies for remapping.

SECTION: Top activating SAE latents
We notice that the EuroSAT dataset shows distinctive performance improvement through MaPLe, low correlation in Fig., and the highest count in the low-to-high group (Table). We conduct case studies for classes showing large class-level performance improvement. As shown in Fig., we assess confusion matrices and choose class 2, where the class accuracy improves from 42.61% to 73.07%. We find the class-specific concept latents are found from low-to-high regions (i.e., get activated by adaptation), while concepts irrelevant to the classes are deactivated (high-to-low). We find concepts that are generally related to the task (satellite or pictures from an airplane) in the high (diagonal) group. We provide more case study results in Fig.. The results yield positive initial findings for the first research question–– and suggest the need to examine the possibility of adaptation improving perceptual ability. We leave this as future work.