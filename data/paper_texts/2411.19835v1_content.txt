SECTION: Feedback-driven object detection and iterative model improvement for accurate annotations

Automated object detection has become increasingly valuable across diverse applications, yet efficient, high-quality annotation remains a persistent challenge. In this paper, we present the development and evaluation of a platform designed to interactively improve object detection models. The platform allows uploading and annotating images as well as fine-tuning object detection models. Users can then manually review and refine annotations, further creating improved snapshots that are used for automatic object detection on subsequent image uploads—a process we refer to assemi-automatic annotationresulting in a significant gain in annotation efficiency.

Whereas iterative refinement of model results to speed up annotation has become common practice, we are the first to quantitatively evaluate its benefits with respect to time, effort, and interaction savings. Our experimental results show clear evidence for a significant time reduction of up to53%for semi-automatic compared to manual annotation. Importantly, these efficiency gains did not compromise annotation quality, while matching or occasionally even exceeding the accuracy of manual annotations. These findings demonstrate the potential of our lightweight annotation platform for creating high-quality object detection datasets and provide best practices to guide future development of annotation platforms.

The platform is open-source, with the frontend and backend repositories available on GitHub111https://github.com/ml-lab-htw/iterative-annotate.

*These authors contributed equally

SECTION: 1Introduction

Object detection has become a critical component in various computer vision applications, including but not limited to autonomous driving[19], surveillance[7], robotics[9], microscopy[13], and manufacturing[3]. These applications rely on high-quality labeled datasets to train and fine-tune models. Traditionally, creating such datasets requires manually tagging images—a process that is both labor-intensive and prone to human error. As object detection models continue to evolve, so do the strategies for efficiently generating these annotated datasets, which has become a key research focus[21,8].

While accurate, manual annotation is not scalable for large datasets. Consequently, there has been growing interest in methods that integrate machine learning techniques to assist with image labeling, particularly in semi-automatic annotation workflows[5,4]. Semi-automatic annotation bridges the gap between fully manual and fully automated annotation by combining the strengths of machine learning and human expertise. Fully automated systems often struggle with rare or ambiguous cases, while semi-automatic methods allow human annotators to correct these mistakes, resulting in higher accuracy and efficiency. This approach promises to significantly improve efficiency without compromising the quality of annotations.

In this paper, we introduce a platform designed to enhance the object detection labeling process through an interactive, feedback-driven loop that iteratively refines model-generated annotations. Our platform allows users to create and manage annotation projects, upload image bundles, and apply a pre-trained Single Shot Detector (SSD)[12]to perform initial object detection and labeling. Users can then manually adjust the generated labels, which are used to incrementally fine-tune the model’s performance, creating snapshots for future predictions. This iterative process, which we refer to as semi-automatic annotation, aims to reduce annotation time while maintaining high accuracy. The workflow of the labeling process is illustrated in Fig.1.

We evaluate the platform by conducting quantitative experiments to assess the efficiency of semi-automatic annotations versus fully manual annotation workflows. Our results demonstrate a significant time-saving of up to 53% without compromising annotation quality.
The findings presented in this paper provides further quantitative insights and best practices with respect to semi-automatic annotation.

SECTION: 2Related Work

Efficient image annotation is essential for training high-quality object detection models. Manual annotation tools such as LabelMe[16]and VGG Image Annotator[6]laid the groundwork by providing interfaces for human annotators to label images. While accurate, manual annotation is time-consuming and not scalable for large datasets.

Iterative refinement or semi-automatic annotation is indeed a common strategy (see[1,17,2,14]just for a few examples). However, a extensive quantiative evaluation of the resulting speed-up as well as suitable best practices are missing in the literature.
Semi-automatic annotation systems, such as Amazon SageMaker Ground Truth[4], combine machine learning with human-in-the-loop corrections, speeding up the annotation process without sacrificing quality. Similarly, Papadopoulos et al.[15]demonstrated how interactive tools could reduce the time required by allowing users to refine pre-labeled bounding boxes generated by object detectors.

The concept of iterative refinement has also been extensively explored from an active learning perspective, where algorithms select suitable yet unlabeled examples to be annotated or reviewed[18]. Tools like LabelStudio[20]apply this feedback loop in object detection, enabling the model to learn from progressively refined annotations.

Our platform builds on these advancements by combining the strengths of semi-automatic annotation with an iterative refinement mechanism. This approach reduces the manual annotation workload while continuously improving model accuracy. Our approach can be of course combined with active learning techniques, however, we decided to focus on semi-automatic annotation only.

SECTION: 3Platform Design and Implementation

Building on these previous advancements, our work focuses on the integration of semi-automatic annotation with iterative model refinement to address the limitations observed in manual workflows. The platform leverages semi-automatic annotation, where users correct initial model predictions, and these corrections are used to incrementally improve the model. This process is shown in Fig.1. It involves:

Generating predictions using the SSD model.

Allowing users to refine the annotations.

Using the refined annotations to fine-tune the model incrementally.

SECTION: 3.1Object Detection Model

At the core of the platform’s object detection capabilities is a standard SSD model[12]. We employ an SSD300 model pre-trained on the COCO dataset[10], chosen for its balance between speed and accuracy. SSD predicts bounding boxes and object classes in a single forward pass, making it ideal for real-time annotation tasks.

SECTION: 3.2User Interface

The platform’s user interface (UI) is intuitive and user-friendly, facilitating efficient annotation of large image datasets. It consists of several components guiding users through project creation, annotation, and model fine-tuning.

The project management interface allows users to create, manage, and monitor annotation projects. Fig.2(left) shows a summary of active projects, with key statistics like uploaded image bundles and fine-tuning status. Users can start new projects, upload images, and review progress through detailed metrics.

The annotation editor enables users to review and refine bounding boxes generated by the model. Fig.2(right) shows the interface for adjusting, resizing, or deleting boxes. Users can also add new boxes for missed objects, improving annotation quality through real-time feedback.

After refining annotations, users initiate model fine-tuning. The user interface allows selection of image bundles, model snapshots, and training configurations. Performance metrics, such as accuracy and loss values, help users track progress and make decisions on further training.

Snapshot management ensures version control during fine-tuning. Users can view different versions, compare performance, and roll back when needed, aiding experimentation.

The UI supports keyboard shortcuts for actions like navigating images and adjusting labels, improving efficiency. The platform is also optimized for minimal interaction, ensuring that users can complete the annotation and fine-tuning processes with as few clicks as possible.

SECTION: 4Quantitative Evaluation

SECTION: 4.1Datasets

The platform was evaluated using the agricultural dataset of[11], selected for its challenging conditions that closely simulate real-world agricultural environments. This dataset, consisting of 200 images captured from an autonomous agricultural robot, includes annotated rows of crops from multiple angles and under varying lighting conditions. For our experiments, the dataset was divided into 20 bundles, each containing 10 images, allowing systematic testing of the platform’s adaptability across diverse scenarios. As shown in Fig.3, these variations—including dense foliage, occlusions, and inconsistent lighting—created a complex environment for the object detection model. Such characteristics enabled us to rigorously assess the platform’s performance in handling domain-specific object detection tasks in realistic agricultural monitoring scenarios.

SECTION: 4.2Performance Measures

To evaluate the performance of the platform, we employed several key metrics commonly used in object detection tasks: a) mean intersection over union (IoU), for measuring bounding box overlap between detection and ground-truth. We also calculated the b) F1 score, which is the harmonic mean of precision and recall. Precision and recall were calculated using an IoU threshold ofto determine true positives for single detections. The F1 score was computed per image bundle. For the semi-automatic approach, the F1 score was calculated after the automatic labeling was completed, to quantify how much manual intervention was still required to improve accuracy.

Another critical aspect of the platform’s evaluation was annotation efficiency. We measured the time required for manual annotation and compared it to the time spent using the semi-automatic annotation process. In addition, we tracked specific user interactions, including the number of bounding boxes created, adjusted, or deleted during the process, allowing for a detailed analysis of the effort saved by using the platform. All measurements were performed on the dataset described in section4.1.

SECTION: 5Results

The experiments conducted in this study demonstrate the effectiveness of the proposed semi-automatic annotation platform. The key findings are summarized as follows:

The semi-automatic annotation process reduced annotation time by up to 53.82% compared to manual labeling, especially in later iterations as the model’s predictions improved.

User interaction effort, significantly decreased in the semi-automatic process, with nearly no manual box creation needed by the 20th bundle.

F1 scores for semi-automatic annotations matched or exceeded the quality of manual annotations, ensuring that the efficiency gains did not compromise accuracy.

SECTION: 5.1Efficiency Gains

A key objective of this work was to reduce the time required for object annotation. Our experiments, show significant time savings using the semi-automatic annotation platform compared to manual labeling. Fig.4provides a comparison of the annotation duration across different bundles.
It shows, that the semi-automatic process reduces the time spent on annotation by up to 53.82%, with the most significant reductions observed in later bundles, where the model’s accuracy improved. So as the model becomes more refined, user intervention decreases.

SECTION: 5.2Interaction Time Reduction

In addition to overall time savings, we measured the time required for individual user interactions, such as creating, adjusting, or removing bounding boxes. Fig.5illustrates the time taken for each interaction in both manual and semi-automatic workflows. The results show that the semi-automatic process required less time on almost every bundle, with up to 67% less time needed for the creation of bounding boxes compared to the manual approach.

SECTION: 5.3Model Performance

As shown in Fig.6(left), F1 score improved consistently over the course of multiple feedback loops, with the semi-automatic approach achieving comparable performance to the manual approach after just 5 iterations. This indicates that the semi-automatic method significantly reduces the need for manual corrections and can function as a fully automated process after sufficient iterations.

SECTION: 5.4Annotation Quality

In contrast to detection performance, annotation quality was assessed using Intersection over Union (IoU), a widely recognized metric for object detection tasks. IoU measures the overlap between the predicted bounding box and the ground truth bounding box, providing a value between 0 and 1. A higher IoU indicates better alignment between the predicted bounding boxes and the actual objects in the image, thus reflecting higher annotation accuracy.

In this study, the mean IoU was calculated per image bundle, allowing us to track the improvements in annotation quality over the iterative annotation steps. For the semi-automatic approach, mean IoU was computed after the automatic labeling, to assess how accurately the model was able to label objects without requiring further manual adjustments.

The results indicate a steady increase in mean IoU from 0.68 in the first bundle to 0.83 in the final bundle, as illustrated in Fig.6(right). The comparison shows that automatically generated bounding boxes quickly catch up to the accuracy of manual annotations, with the final iterations producing IoU scores that are on par with fully manual approaches. Eventually, the semi-automatic approach transitions into a fully automated process as the model becomes capable of producing high-quality labels independently.

SECTION: 6Conclusion

In this work, we presented a feedback-driven, semi-automatic annotation platform designed to iteratively improve object detection models. The primary goal was to evaluate the efficiency of such a platform in reducing the time and effort required for creating high-quality labeled datasets compared to traditional manual annotation methods. Through the integration of user feedback and iterative model fine-tuning, we demonstrated that the proposed system achieves significant gains in annotation efficiency without compromising accuracy.

Our experimental results show that the semi-automatic annotation process can reduce annotation time by up to 53.82% compared to manual labeling, while maintaining or even improving the quality of the annotations. Intersection over Union (IoU) showed consistent improvements across multiple iterations, indicating that the platform effectively enhances the model’s performance over time. These results validate the effectiveness of the semi-automatic approach, especially for large-scale datasets, where manual annotation becomes impractical.
The ability to provide feedback to the model in a dynamic, iterative manner highlights the importance of a human-in-the-loop approach, where machine learning techniques complement human expertise to accelerate the dataset creation process.

SECTION: Funding Acknowledgment

This research was funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) – Project Number528483508.

SECTION: References