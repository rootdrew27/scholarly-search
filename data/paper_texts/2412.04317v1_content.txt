SECTION: FlashSloth: Lightning Multimodal Large Language Models viaEmbedded Visual Compression

Despite a big leap forward in capability,multimodal large language models(MLLMs) tend to behave like a sloth in practical use,i.e., slow response and large latency. Recent efforts are devoted to building tiny MLLMs for better efficiency, but the plethora of visual tokens still used limit their actual speedup. In this paper, we propose a powerful and fast tiny MLLM calledFlashSloth. Different from previous efforts, FlashSloth focuses on improving the descriptive power of visual tokens in the process of compressing their redundant semantics. In particular, FlashSloth introduces embedded visual compression designs to capture both visually salient and instruction-related image information, so as to achieving superior multimodal performance with fewer visual tokens. Extensive experiments are conducted to validate the proposed FlashSloth, and a bunch of tiny but strong MLLMs are also comprehensively compared, e.g., InternVL2, MiniCPM-V2 and Qwen2-VL. The experimental results show that compared with these advanced tiny MLLMs, our FlashSloth can greatly reduce the number of visual tokens, training memory and computation complexity while retaining high performance on various VL tasks. Our code is released at:https://github.com/codefanw/FlashSloth.

SECTION: 1Introduction

Recent years have witnessed the remarkable breakthroughs made by extendinglarge language models(LLMs)[56,20,71]to more modalities, e.g., buildingmultimodal large language models(MLLMs) for vision-language tasks[28,35,42]. Among these advancements, one main research focus is on enhancing the visual perception of MLLMs, and the widely recognized solution is to use a larger number of visual tokens[34,32,45]. For instance, LLaVA-NeXT[34]uses 5 times more visual tokens compared to LLaVA-1.5[33]by subdividing input images into multiple tiles. Similarly, recent MLLMs, such as InternVL1.5[10]and Qwen2-VL[58], can support up to thousands of visual tokens for high-resolution image understanding via dynamic-resolution encoding. Although effective, the excessive use of visual tokens further execrates already high computation of MLLMs, limiting practical use.

In this case, more and more efforts are devoted to the research of lightweight and efficient MLLMs[12,51,16,65]. In particular, with the emergence of small-scale LLMs,e.g., Phi[20]and Gemma[4], recent endeavors start to explore their use in building tiny MLLMs, such as MobileVLM[11,12], Imp[51]and Mini-Gemini[31]. Meanwhile, representative MLLM families also launch their slim versions for better mobile applications,e.g., Qwen2-VL[58], InternVL[10]and MiniCPM-V[16]. With a much smaller LLM structure, these tiny MLLMs typically scale to about 2-3 billion parameters, so their training expenditure as well as memory overhead are also much cheaper than previous MLLMs[33,28,42]. However, to retain general multimodal capability, most tiny MLLMs[51,65,10]still adopt a large number of visual tokens, making it hard to achieve actual speedup. As shown in Fig.1, with more visual tokens used, tiny MLLMs even have a slower response time111The time for the first answer token.than common MLLMs like LLaVA-1.5-7B[33].

By revisiting the development of vision-language research[59,73,60,2,43], we can see that the way to achieve better visual capability is not confined to a singular paradigm. In principle, the key to addressing visual shortcoming is to make “vision” matter in MLLMs[15,52], thereby helping them better understand visual information and also reduce the impact oflanguage bias[72,22]. From this perspective, the use of enough visual tokens does contribute more to self-attention modeling in MLLMs, but recent studies[7,62]also show that this paradigm is often inefficient and obviously redundant. In addition, various attempts have been successfully made in improving visual capability before the era of MLLMs. For instance, enriching the visual semantics[21,70,44]or refining complex image information based on visual saliency or question dependency through various attention-based approaches[64,2,73,41]. To this end, we believe that a good balance between the performance and efficiency of MLLMs is feasible.

In this paper, we propose a tiny and fast MLLM calledFlashSloth. The principle of FlashSloth is to improve the descriptive power of visual tokens in the process of refining and compressing their redundant semantics, thereby achieving actual speedup during inference. Concretely, FlashSloth first introduces aspatial-aware attentive poolingto compress the redundant image information while capturing visually salient semantics. Meanwhile, a novel and lightweightQuerymodule is equipped to grasp instruction-related image information, thereby compensating the loss of image details in the attention pooling process. Notably, this query module is embedded into the architecture of FlashSloth rather than as an independent bridge branch that requires another language modeling[74,13,61], e.g., Q-Former[28]. Thus, we term itEmbQ. In addition to the compact structure designs, EmbQ also consumes much lower training and inference costs, well facilitating the efficiency goal of FlashSloth. For instance, EmbQ does not require dedicated large-scale VL alignment pretraining[28]. With these intuitive designs, FlashSloth can not only greatly reduce the number of input visual tokens but also improve their discrimination for better multimodal reasoning.

To validate the proposed FlashSloth, we conduct extensive experiments on a set of highly-competitive VL and MLLM benchmarks[36,14,66,25,30], and compare it with a bunch of least tiny MLLMs, including Qwen2-VL-2B[58], Intern-VL-2[10], MiniCPM-V2[17], and MM1.5[68]. Experimental results demonstrate that compared to these advanced tiny MLLMs, our FlashSloth can reduce the number of visual tokens, training memory and inference computation by 80-89%, 61-80% and 70-98%, respectively, while shortening the actual response time by about 2to 5times. Retaining high efficiency, FlashSloth also exhibits competitive ness against these SOTA methods, and even perform slightly better on several common VL tasks,e.g., MMB[36]and MMMU[66], well confirming our motivation and the designs of FlashSloth.In summary, our contributions are three folds:

We propose a strong and fast tiny MLLM in this paper, coined asFlashSloth, showing that a good balance between performance and efficiency is feasible.

In FlashSloth, we introduce embedded visual compression designs to efficiently capture both visually salient and instruction-related semantics, namelySpatial Attention PoolingandEmbedded Querymodules.

The extensive experiments not only show the strong multimodal capability of FlashSloth, but also confirm its competitiveness with a set of advanced MLLMs while retaining higher efficiency.

SECTION: 2Related Works

SECTION: 2.1Multimodal Large Language Models

Based on the rapid development oflarge language models(LLMs)[56,71,20]and visual encoders[50,48],multimodal large language models(MLLMs) also achieve significant strides in variousvision-language(VL) tasks. Numerous open-source MLLMs[27,li2023blip,42,33]emerges in recent years, some of which even achieve outstanding capability comparable to GPT-4[1]in specific fields. However, this advancement is often support with increasingly larger parameter sizes, which also results in heavy burden to the training and application of MLLMs. Therefore, more recent research resorts to smaller LLMs to build tiny MLLMs, such as Phi[20], Gemma[4], and Qwen2[3]. For instance, MobileVLM[11]first realize the attempt of extending tiny LLMs to multimodal tasks with a simple yet effective visual projection after image encoder. Additionally, more efforts are devoted to explore the design and training strategies of tiny MLLMs based on small LLMs, such as LLaVA-Phi[75], Imp[51], and PaliGemma[9]. Meanwhile, the influential MLLM families, such as MiniCPM-V[17], Qwen2-VL[58]and InternVL[10], also develop their slim but also powerful versions via exploring high-resolution image encoding and high-quality data collection. Overall, the advancement of tiny MLLMs well facilitate the real-world applications of MLLMs. However, a magnitude of visual tokens still used also slow down the response time of tiny MLLMs in addition to high expenditure[29,7],i.e., the first token prediction, hindering application.

SECTION: 2.2Visual Token Compression

Most existing MLLMs[53,58,31,10,45]usually rely on a large number of visual tokens for superior visual capability, whether they are large or tiny. However, this paradigm is often criticized for excessive computation and obvious visual redundancy, which also attracts an influx of interest in efficient visual learning of MLLMs[7,62,18].
In terms of network designs,Q-Former-like methods[28,13,74,61]uses learnable tokens to control the number of visual tokens, with a purpose of capturing instruction-related visual information via visual compression. However, they often use another language model like BERT[57]to interpret the text instruction and require dedicated vision-language pretraining. Some methods like Abstractor[6]and LDP[11,12]employ convolutional layers to learn local visual compression. Similarly, methods[10]like InternVL apply pixel shuffle to directly reduce the number of visual tokens. However, the information loss in these local compression methods are often not further compensated. The other main paradigm for visual efficiency is to apply external methods for effective visual token pruning during inference[7,62,18]. For example, FastV[7]determines each token’s importance based on average attention,and FitPrune[62]selects retained features by minimizing the attention distribution difference before and after pruning. However, the contributions of token pruning methods are orthogonal to this paper, and we focus on improving the discrimination of visual tokens via investigating network structure design.

SECTION: 3Method

SECTION: 3.1Overall

In this paper, we propose a tiny and fast MLLM calledFlashSloth, of which framework is illustrated in Fig.1. FlashSloth aims to improve the descriptive power of visual tokens with embedded visual compressions,i.e., thespatial attention pooling(SAP) andembedded query(EmbQ) modules, thereby achieving superior multimodal capability with a shorter visual token sequence.

Concretely, given an input image, FlashSloth uses the image encoder to extract its visual token features, denoted, wheredenotes the resolution andis the feature dimension. And the input text instructionis first truncated into a set of tokens, which are then vectorized by the corresponding word embeddings, denoted as. Here,is the length of text sequence.

In existing MLLMs[33,51], the number of directly output visual tokensis often large, especially for the high-resolution images[34,10,17]. Thus, we apply the SAP module to attentively capture the salient visual semantics while compressing the token redundancy. The processed visual tokens by SAP are denoted by, which has a much smaller number of tokens than.

Afterwards, the compact visual tokensafter linear projection and the text tokensare fed to the MLLM structure, which are also padded withlearnable query tokens at the end of the sequence, denoted as.

In FlashSloth,serves to supplementin terms of the instruction-related image details. Particularly, to avoid another language modeling[13,74,61],will first attend the multimodal interaction in the MLLM, and then engage in EmbQ for visual querying at the-th layer:

Here, the output of EmbQ is pure visual attention featureswith the same length of. Overall, the objective of FlashSloth’s decoding can be defined by:

whereis the prediction probability distribution,is the answer sequence andis its length.denotes the answer tokens before the-th step.

From the above introduction, we can see that FlashSloth is different from most MLLMs[33,10,58]in two main aspects. First, FlashSloth refine and compress visual tokens in terms of both visual saliency and instruction-related semantics, which can well collaborate with each other for different VL tasks. Second, all compression designs are lightweight and embedded in the architecture of FlashSloth without the requirement of specific tuning or pretraining[28]. In the following section, we will describe them in detail.

SECTION: 3.2Embedded Visual Compression

As discussed above, the main principle of FlashSloth is to improve the discrimination of visual tokens while squeezing their length. To approach this target, we perform the visual compression in two aspects,i.e.,visual saliencyandinstruction dependency. Moreover, these designs are lightweight and can be embedded into the architecture of FlashSloth, serving the target of model efficiency.

We first introduce aspatial attention pooling(SAP) method to refine and compress the semantics in local visual tokens, borrowing the successful attention designs in previous VL research[73]. The intuition is that the visual tokens extracted by the encoders like ViT[67,49]already have a large receptive field as well as obviously overlapping information. Thus, an MLLM can mainly focus on the most visually salient information in each image regions.

Specifically, given the extracted visual tokens, we first spatially divide them into a set of region tokens with a size of, denoted. Thus, for each image region, SAP directly use a two-layerto predict its visual attention weights:

Then, the visually salient feature of each image regionis directly obtained via weighed combinations:

Lastly, those salient features are tiled to form the new visual tokensand fed to FlashSloth.

Considering the varying difficulties of VL tasks[15,8,23], the salient semantics provided by SAP is prone to insufficient for multimodal reasoning. In this case, we further propose anembedded query(EmbQ) module towards instruction-aware visual compression.
In broad terms, EmbQ is similar to previous attempts like Q-Former[28],i.e., querying the text-related visual information, but it still exhibit obvious differences in design and operation.

Above all, our requirement for EmbQ is to accomplish coarse-grained visual grounding rather than accurate VL alignment. By revisiting previous VL research[59,60], we note that this requirement is easy to achieved without complex network structure and large-scale pretraining. Therefore, the design of EmbQ is neat and efficient, which is directly embedded into FlashSloth, as shown in Fig.2.

Concretely, a set of learnable tokensare used asqueriesand padded in the input sequence of FlashSloth. Afterlayers of transformation,are fed to EmbQ for visual querying. In particular, we expect this operation will allowto obtain enough instruction information from the text tokens via self-attention. But during experiments, we note that more visual semantics are received since the length of visual tokens is much longer than that of the text ones, which contradicts the target of EmbQ.

Thus, we first interactandvia cross-attention:

whereare the obtained text queries, andare the projection weight matrices, anddenotes their dimension.

Then, we can useto query visual information from the uncompressed visual tokens, defined by

Lastly,are up projected and then combined withfor the following multimodal inference of FlashSloth, as described in Sec.3.1. Notably, the process of EmbQ takes into account of the discrimination of well-learned visual tokens, so only one up-projection is used for visual tokens for scaling to the MLLM’s dimension. Besides, we also use the embedding of ‘dot’ token to initialize queries, making them easier to accommodate to the semantic space of MLLM.

SECTION: 3.3Training and Other Settings

Under the default setting, FlashSloth apply a two-stage training paradigm[35].The pretraining stage.Only the projector and spatial attention pooling are optimized for the alignment between visual and text tokens, while the LLM is fixed.The SFT tuning stage.Except vision encoder, FlashSloth are optimized, including LLM and EmbQ.

To tackle OCR tasks with a high demand on image resolution[34,26,10], we also propose a high-resolution version, termedFlashSloth-HD. In particular, FlashSloth-HD inputs images ofresolutions. In terms of image processing, we follow LLaVA-NeXT[34]to divide the images into four parts and a low-resolution thumbnail, of which visual tokens are extracted in parallel. Similarly, FlashSloth use SAP to squeeze their length greatly and EmbQ for visual querying. To save training expenditure, we only use high-resolution images in the SFT tuning of FlashSloth-HD, where the vision encoder is unfreezed for better accommodation. Details can refer to our project.

SECTION: 4Experiment

SECTION: 4.1Implementation Details

In terms of the default FlashSloth, we usesiglip-so400m[67]as the visual encoder with an input resolution of 384, andphi2-2.7B[20]as the LLM. The downsampling rate of SAP is set to 3, generating 81 visual tokens. The number of queries in EmbQ is 9, and an embedding query module with a dimension of 576 is inserted at the 8th layer of the MLLM by default. The model is trained byAdamW[37]optimizer and cosine learning rate scheduler for a total of 1 epoch. The initial learning rates for pre-training and instruction tuning are 1e-3 and 2e-5 with batch sizes of 256 and 128, respectively. All training is conducted on 8 NVIDIA A800 GPUs. For the FlashSloth-HD, the input image resolution is 768, and the image tokens are compressed to 405, while the other settings remain the same as FlashSloth.

SECTION: 4.2Benchmarks and Metrics

We evaluate the model on seven multimodal benchmark datasets, including MMB[36], MME[14], mm-vet[63], Pope[30], SEED[25], MMMU[66], and MathVista[40]. And seven general visual-language datasets, including SQA[39], AI2D[24], GQA[19], TextVQA[54], ChartQA[46], DocVQA[47], and RealWorldQA. These benchmarks assess MLLMs from diverse perspectives, such as hallucination, multimodal perception and cognition, and multidisciplinary question answering. All evaluations are conducted using thelmms-eval[69].

SECTION: 4.3Quantitative Analysis

We first compare the efficiency of FlashSloth with advanced tiny MLLMs, and also use the representative MLLM LLaVA-1.5-7B[33]as reference.Inference Efficiency.We first compare the inference efficiency of FlashSloth with three advanced tiny MLLMs[58,10,51]in Tab.1. For better comparisons, we use LLaVA-1.5-7B[33]as reference. From these statics, we can first observe that tiny MLLMs have a lower requirement of GPU memory than LLaVA due to the use of much smaller LLMs. Likewise, their theoretical computation (FLOPS) is also less than LLaVA. However, their actual inferences are not obviously faster,i.e., the response time or throughput. To explain, the large number of visual tokens will greatly slow down the decoding of first token,i.e., response time, making their advantages inKV caching[5]based decoding become not that obvious, especially that the answers of VL examples are often short[54,46,47]. We can also see that the dynamic resolution designs of Qwen2-VL and InternVL can help to adjust the number of visual tokens for different images, but still keeps a relatively large number, which also result in large latency. Lastly, we can see that with about 80-89% fewer tokens, FlashSloth exhibits obvious merits than these MLLMs in terms of all inference metrics. For instance, its response time is about 2 and 5 times faster than LLaVA and InternVL, respectively.
In terms of FlashSloth-HD, its overall efficiency is also superior than the compared MLLMs, even through it uses more visual tokens than FlashSloth. These results well confirm the advantages of FlashSloth in inference and visual compression.

Training Efficiency.We further report the training expenditures of FlashSloth and the other four MLLMs in Fig.3. For a quick comparison. we use the pretraining and tuning splits of LLaVA[33], and the per-GPU batch size is set to 32 for pretraining and 8 for instruction tuning. From these plots, we can first find that FlashSloth consumes much less training time than the other MLLMs, especially pretraining. In practice, its pretraining using the LLaVA split only takes 6.4 GPU hours, about 76% and 68% less than LLaVA and IMP, respectively. Its SFT tuning time (52 GPU hours) is longer due to more examples used, and it is also slightly affected by the input queries. However, the cost is still lower than than IMP (65.6 GPU hours) and MobileVLM (96 GPU hours). Similarly, with a well designed training scheme, FlashSloth-HD has a slightly longer training time (6.4+65.6 GPU hours), which is still cheaper than the other MLLMs. The other observation from Fig.3is that tiny MLLMs require GPU memories close to that of LLaVA except our FlashSloth. During pretraining, both FlashSloth and FlashSloth-HD only use about 81 visual tokens, making its memory overhead much lower than the other MLLMs. During instruction tuning, their GPU memories increase greatly. To explain, the queries are given for each instruction, and an SFT example is often a multi-round conversation, so the multiple paddings will bring in a larger sequence. With effective compression, this overhead is still lower than the compared methods. Overall, these results show the merits of FlashSloth in training efficiency.

We make performance comparisons between FlashSloth-HD and a bunch of advanced tiny MLLMs on 14 highly-competitive VL and MLLM benchmarks, as shown in Table2. From this table, we can first observe that FlashSloth is very competitive on common VL tasks with a much smaller number of visual tokens. For instance, its performance on MMB[36], GQA[19]and SQA[39]is much better than several previous tiny MLLMs with similar training data amount,e.g., MobileVLM[12], Mini-Gemini[31]and Imp[51]. Compared to the SOTA tiny MLLMs, such as InternVL[10]and Qwen2-VL[58], FlashSloth also exhibits good competitiveness on these tasks, but it still lags behind them on the OCR tasks like DocVQA[47], which requires high-resolution image inputs. In this case, we can see that its HD version,i.e., FlashSloth-HD, can well compensate this shortcoming. Overall, retaining better efficiency, FlashSloth-HD can generally reach the capability of InternVL2, and well shorten its gap to Qwen2-VL. Moreover, FlashSloth can even achieve new SOTA performance among tiny MLLMs on several benchmarks, such as MMB, GQA and AI2D. Considering the much smaller amount of training data for FlashSloth-HD, these results are in fact very notable, well confirming our motivation and designs.

In the first block of Tab.3, we first compare different visual compression designs. The most simple solution isaverage pooling, but it tends to lose key visual information, leading to obvious declines in most benchmarks,e.g., MME and MMB for multimodal perception and recognition. In contrast, our SAP can well keep the visual saliency, so as to obtaining better performance than simple pooling. In addition, we can also see that with the combination of EmbQ and SAP, FlashSloth can obtain very marginal performance drops compared to the baseline, while its efficiency is much better. This result confirms the supplement of EmbQ to SAP.
In the second and last blocks of Tab.3, we examine the settings of EmbQ. We can first see that the initialization of queries has impact on EmbQ. The random initialization can serves the target of EmbQ, but the initialization of textdottoken further improves performance slightly, suggesting their better interactions with other input tokens in MLLMs. Besides, we can also see that the number of queries required by EmbQ is very small, and 9 tokens are enough for visual querying. To explain, EmbQ serves to capture instruction-related information at a coarse granularity, as discussed above. As a supplement to SAP, this design only requires a few queries, especially considering the short questions in MLLM tasks. Overall, these results well confirm the designs of EmbQ.

SECTION: 4.4Qualitative Analysis

To gain deeper insight into FlashSloth’s process of enhancing visual feature description, we visualize the attention results of SAP and EmbQ, as shown in the figure4(a). As observed, SAP distributes attention more broadly, allowing the model to focus on salient information from different regions of the image. This helps the model capture salient details in images , such as the three small birds in the left image, seabirds in the middle image, and tiny text in the right image. In contrast, the embedded query focuses more narrowly on key, text-related information in the image, such as the elephant in the left image, the surfer in the middle image, and the hair in the right image. By combining these two attention mechanisms, the model can effectively prioritize the most important information in the image, enhancing the expressiveness of visual features. This demonstrates that the synergy between SAP and EmbQ allows FlashSloth to fully leverage visual information, resulting in improved performance across multi-task scenarios.

In the Figure4(b), We visualize the predictions of FlashSloth , FlashSloth-HD, Qwen2-VL and InternVL-2 for different VL examples. First, FlashSloth exhibits extremely fast response times, with latency significantly lower than the other two models, providing a good user experience. Second, for coarse-grained real-world QA and scientific QA tasks, FlashSloth’s performance is on par with or even surpasses that of the other two models. In the top-left example, FlashSloth identifies grapes that the other models miss, and FlashSloth-HD answers in more detail. In the bottom example, FlashSloth provides the most accurate answer to a biological question. However, due to its lower resolution, FlashSloth underperforms on the OCR tasks. For instance, in the top-right example, FlashSloth fails to recognize correctly, but upon increasing the input resolution, FlashSloth-HD handles fine-grained OCR tasks effectively.

SECTION: 5Conclusion

In this paper, we introduce FlashSloth, a powerful and fast tiny MLLM. By incorporating effective embedded visual compression designs, FlashSloth effectively captures both visual saliency and instruction-related semantics, achieving an optimal balance between performance and efficiency. Extensive comparisons with existing tiny MLLMs on various benchmarks demonstrate that FlashSloth significantly enhances both training and inference efficiency while maintaining competitive performance, which well validates its motivation and designs.

SECTION: 6Acknowledgments

This work was supported by National Science and Technology Major Project (No. 2022ZD0118201), the National Science Fund for Distinguished Young Scholars (No. 62025603), the National Natural Science Foundation of China (No. U21B2037, No. U22B2051, No. 623B2088, No. U23A20383, No. U21A20472, No. 62176222, No. 62176223, No. 62176226, No. 62072386, No. 62072387, No. 62072389, No. 62002305 and No. 62272401), and the Natural Science Foundation of Fujian Province of China (No. 2021J06003, No. 2022J06001).

SECTION: References

SECTION: Appendix AQuantitative analysis

SECTION: A.1Impact of EmbQ configuration

Table4examines the impact of EmbQ’s dimensions, number of layers, and insertion positions within the LLM on performance. The first and second blocks show that EmbQ achieves optimal results with a simple configuration of 576 dimensions and a single layer, aligning with FlashSloth’s efficiency goals. The third block explores the effects of inserting EmbQ into shallow, middle, deep, or multiple layers. Excessively shallow insertions limit Query Tokens to self-interaction, restricting their ability to capture rich image and text features, while overly deep insertions hinder effective propagation of newly learned supplementary information to generated tokens.

SECTION: A.2Impact of Feature Fusion Methods

Table5compares methods for fusing features learned by EmbQ with Query Token features: direct replacement, addition, and gated fusion[45], which adjusts the contribution of each feature. Results show that direct addition achieves the best performance by effectively integrating EmbQ’s features while retaining the original feature information.

SECTION: A.3Comparison with Other Vision Compression Methods

Table6compares our method with various visual feature compression approaches and evaluates the performance gains from incorporating EmbQ. The combination of SAP and EmbQ achieves the best results, validating the integration of SAP’s saliency features with EmbQ’s instruction-related features. Additionally, embedding EmbQ into any compression method consistently improves performance, highlighting its effectiveness.

SECTION: Appendix BQualitative Analysis

This section showcases practical examples of FlashSloth and FlashSloth-HD, demonstrating their real-world performance in multidisciplinary question answering, code generation, real-world scene reasoning, fine-grained text information extraction, and chart analysis reasoning. FlashSloth delivers accurate responses and exceptional performance across these tasks.