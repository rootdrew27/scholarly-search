SECTION: The Survey of Surveys: machine learning for stellar parametrization

We present a machine learning method to assign stellar parameters (temperature, surface gravity, metallicity) to the photometric data of large photometric surveys such as SDSS and SKYMAPPER. The method makes use of our previous effort in homogenizing and recalibrating spectroscopic data from surveys like APOGEE, GALAH, or LAMOST into a single catalog, which is used to inform a neural network. We obtain spectroscopic-quality parameters for millions of stars that have only been observed photometrically. The typical uncertainties are of the order of 100K in temperature, 0.1 dex in surface gravity, and 0.1 dex in metallicity and the method performs well down to low metallicity, were obtaining reliable results is known to be difficult.

SECTION: 1INTRODUCTION

In the last few years large spectroscopic surveys provided a huge amount of photometric measurements for hundreds of millions (up to billions) of stars, either at low-medium resolution such as the RAdial Velocity Experiment (RAVE) [1], the Sloan Extension for Galactic Understanding and Exploration (SEGUE) [2] and the Large sky Area Multi Object fiber Spectroscopic Telescope (LAMOST) [3], or at high-resolution such as the Galactic Archaeology with HERMES (GALAH) [4], the Apache Point Observatory Galactic Evolution Experiment (APOGEE) [5] and the Gaia-ESO survey [6]. The data provided by these surveys allow to give a precise estimates of key parameters such as effective temperature (T), surface gravity (log) and iron metallicity ([Fe/H]) for few millions of stars in the Milky Way.
The availability of high-quality spectroscopic measurement, together with a good estimation of distance and reddening, is of paramount importance to derive high quality estimates of the above parameters from photometric surveys.In recent years, these surveys spawned a many works focused on Machine Learning (ML) methods (i.e. Neural Networks or simpler methods), such as the Cannon [7], the Payne [8] and StarNet [9]. ML saw a huge development in the last decades of XX century and rose to a widespread usage in the first decades of XXI century. The term can be used as a general hat to cover different disciplines from Artificial Intelligence to Neural Networks and Computational Statistics. In general we refer to ML techniques when based on algorithms that make use of heterogeneous data to automatically “learn” and build a “model” that is used to produce a desired output, using statistical methods.ML methods applied to spectroscopic star catalogues mainly focus on the analysis of the provided data to many different purposes, i.e. trying to enhance the physical model used to compute the parameters, thus this field is already mature in the case where the previous parameters are directly derived from spectroscopic data.However spectroscopic quality measurements are hard to perform on large stellar samples, requiring a lot of telescopes time and the scientific community is thus lacking high-quality estimates on large stellar samples, thus limiting the impact of studies that use the ML-derived spectroscopic data.There are however many photometric surveys that include hundreds of millions (up to billions) of stars that provide observations in many different bands, such as the Sloan Digital Sky Survey (SDSS) [10], the SkyMapper Southern (SMSS) [11] and the Two Micron All-Sky Survey (2MASS) [12]. While not as accurate as spectroscopic data, photometry can indeed be used to derive a rough estimate of T, logand [Fe/H], and is used by many astronomers to analyze huge star samples.With this in mind, in this contribution we provide a first preliminary ML application that aims to enhance low-quality photometric measurements and thus improve the accuracy from pre-determined analytic estimates of the star parameters. The strength of this method is that we do not try to produce estimates from scratch, thus we are less impacted by large deviations of the model from the real measurement. To achieve this goal and train our ML model, we used the spectroscopic estimates of T, logand [Fe/H] provided by the SoS catalogue [13], which is a critical compilation of spectroscopic parameters from survey data. Also this allow us to make use of the astrometric, photometric, and spectroscopic data from the Gaia mission, combined with large photometric datasets such as SkyMapper Southern Survey (SMSS), SDSS and others that provide us huge samples with hundreds of millions of stars, allowing us to maximize the applicability of the method itself.

SECTION: 2Input catalogs and training set

Our neural network (NN) model was designed to produce survey-quality values for effective temperature (Teff), surface gravity (log g), and iron metallicity ([Fe/H]) with spectroscopic-like accuracy using solely standard photometric data.

Initially, we integrated Gaia data (Gaia Collaboration et al. 2016) with Gaia distances (Bailer-Jones et al. 2021). This was supplemented by data from the SkyMapper Southern Survey (SMSS, Keller et al. 2007), forming a comprehensive input dataset that encompass much of the visible spectrum.
We performed a set of training cycles utilizing the NN framework. The supervised training methodology was guided by the ’true’ values for Teff, log g, and [Fe/H], which the NN sought to accurately predict.

SECTION: 2.1Input dataset

We started to build our input dataset from theGaiaDR3 dataset111https://gea.esac.esa.int/archive/[14]. We used the software from Marrese et. al. [15,16] to perform a precise cross-matching between catalogs, a process greatly enhanced by the detailed proper motions available, with all coordinates conforming to theGaiaDR3 system. For sample purification, we applied metrics such as stellar blending, binarity, non-stellarity, and variability from theGaiacatalog. During the neural network’s training phase, we leveraged data from the catalog including magnitudes, colors, and other stellar parameters, aiming to enrich the data input and improve the precision and accuracy of our results. Crucially, we incorporated distances derived by [17], which are critical for accurately determining logand [Fe/H] values.

We applied several selection criteria toGaia DR3catalog in order to start from very clean data in this preliminary NN test:

sources lacking eitherphot_bp_mean_magorphot_rp_mean_magwere removed;

sources withipd_frac_multi_peak10oripd_frac_odd_win10were removed, to avoid disturbance by neighboring objects [18], as well as sources withruwe1.4[19];

sources with thenon_single_starorVARIABLEflags were removed, as well as those with thein_qso_candidatesandin_galaxy_candidatesflags;

we only included sources having a distance in [17] and in particular we decided to use the geometric distance determinations because we noted, a posteriori, that they produced slightly better results than the photo-geometric ones;

we removed stars with a spectroscopic rotational broadening (vbroad) of more than 30 km s-1, for the few stars for which this parameter was available, because these stars can have altered colors and less reliable spectroscopic parameters;

we removed all stars with a photometric temperature (teff_gspphot) greater than 7500K;

finally, we also removed stars with G18 mag, parallax_error0.1 or astrometric_sigma5d_max0.1, which have worse astrometric quality parameters, because they do not always allow for a reliable distance determination. After the first tests, in fact, we noticed that13% of the stars in the training sample had conflicting properties in different catalogs. In the spectroscopic surveys they were clearly giants, while they appeared to lie on the main sequence of the absolute and dereddenedGaiacolor-magnitude diagram. This conflicting information confused the algorithm, providing wrong logdeterminations for a large part of the training set. Therefore, for this experiment, we decided to use the cleanest possible sample. The adopted cut reduced the stars with conflicting information to about 1%.

As a result, we pre-selected an initial sample of almost 27 million stars from theGaiaDR3 catalogue, that we further selected as described in the following.

We then moved to integrate the SkyMapper DR2 dataset222https://skymapper.anu.edu.au/table-browser/dr2/[20], which offers photometry in sixuvgrizbands for approximately half a billion stars. We performed a cross-matching of SMSS data with a selected set of sources fromGaiaDR3, employing algorithms developed by [21,22]. We eliminated all stars in each catalog that corresponded to multiple entries in the other, keeping only the cleanest sources.

To filter out non-stellar objects from the SkyMapper catalog, we applied the criterionand excluded problematic data using the conditionsand. Stars with any magnitudes exceeding 25 mag were removed. Following these filtering steps and cross-matching with the refinedGaiaDR3 sample, we retained a significant sample of nearly 11.4 million stars, spatially distributed in the southern hemisphere. In subsequent work we plan to add other surveys to cover the Northern hemisphere as well. In a future paper we already planned to add other surveys (e.g. SDSS) to cover also the northern hemisphere and expand the dataset.

The absolute magnitudes were computed using the distances fromGaia(), according to the formula:

whereindicates the magnitudes, whether absolute or relative, for each band. The extinction coefficients () were derived fromextinction maps, indicated by the parameter ebmv_sfd, and adjusted using the transformation coefficient for each of the SMSS bands333https://skymapper.anu.edu.au/filter-transformations/.

In table1we list all the parameters selected to be used as an input to the NN model.

SECTION: 2.2Training data

Our star sample with known spectroscopic parameters—T, log, and [Fe/H]—was derived from the first SoS data release [13]444http://gaiaportal.ssdc.asi.it/SoS/query/form. This release, primarily featuring radial velocities (RV), also included an early version of a stellar parameter catalog that we will call SoS-spectro, for brevity. This catalog was utilized to examine the influence of various parameters on RV trends across different surveys and incorporated data from APOGEE DR16 [24], GALAH DR2 [25], Gaia-ESO DR3 [26], RAVE DR6 [27,28], and LAMOST DR5 [29]. Despite the straightforward nature of its homogenization method [13], SoS-Spectro displays typical uncertainties around100,K for Tand about0.1,dex for logand [Fe/H], encompassing nearly 5.5 million stars. SoS-Spectro has been effectively used to characterize both Landolt and Stetson secondary standard stars [23], providing reliable results for challenging parameters such as logand [Fe/H].

Finally, after cross-matching the SoS Catalogue with our previously collected input samples, we developed a new datasets with 624410 stars. These contain the spectroscopic measurements for Teff, log, and [Fe/H], which will be pivotal for the reference outputs during the machine learning algorithm’s training and testing phases.

SECTION: 3Neural network model

Prior to the research presented in this paper, we evaluated several machine learning (ML) and deep learning (DL) architectures to identify the most effective approach. In previous work [23], we tested basic machine learning algorithms such as Random Forest (RF), K-Neighbours, and Support Vector Regression (SVR) on a relatively small dataset of approximately 6000 stars from Landolt and Stetson photometry. From this evaluation, SVR emerged as more effective than more complex deep learning techniques like Multilayer Perceptron (MLP) networks, despite its simplicity. SVR, a linear model used for both regression and classification, operates by identifying a hyperplane that maximizes the margin between the support vectors while minimizing the regression error, effectively separating the N-dimensional data points onto an N-1 dimensional hyperplane, which then acts as the regression function.

Conversely, MLP utilizes a nonlinear approach involving multiple layers of artificial neurons, each with specific activation functions, making it part of the broader DL category. This network includes an input layer that receives data vectors, several hidden layers, and an output layer that generates the regression results. Each neuron across the layers is linked to all inputs, with weights that influence the activation of the neuron based on a nonlinear activation function. MLP networks are trained using a dataset to fine-tune these weights to minimize output errors compared to known true values, as determined by a loss function (e.g., mean error, RMSE).

Despite MLP’s ability to handle larger datasets more effectively as they often perform better with increased data volume, they are less interpretable and considered “black boxes” because it is challenging to discern how each layer’s calculations contribute to the final outcomes. Additionally, MLPs are prone to overfitting, which means they might perform excellently on training data but generalize poorly to new datasets. To combat this, techniques such as weight regularization and dropout layers are employed to prevent over-strong connections and enhance network robustness by randomly deactivating neurons during training (see section3.1).

SECTION: 3.1Implementation

For this study, we adopted an MLP network architecture using the Keras Python interface for the TensorFlow library. After establishing a clean, cross-matched training and testing catalogue (see section4), the entire training process was executed on a standard desktop PC, with each session taking only a few hours of CPU time.

The MLP was constructed with sequential layers: a fully connected (dense) layer with a Leaky Rectified Linear Unit (Leaky ReLU) activation function for improved performance and convergence, followed by a batch normalization layer to stabilize the mean output near zero with a standard deviation of one, and a dropout layer to prevent overfitting. This setup is particularly crucial given the potential noise in the dataset, which could lead to overfitting rather than learning from the actual physical properties.

Our network design included 18 hidden layers of varying sizes, from 80 to 160 elements, arranged in a diamond shape to optimize processing. During the initial phase of training, we minimized the mean absolute error to address the maximum error across the dataset and mitigate the impact of outliers. For the subsequent phases, we adjusted to minimize the symmetric mean absolute percentage error (SMAPE), defined by the equation:

whereis the actual value,is the predicted value, andprevents function explosion.

Each network was tailored to predict specific parameters—Teff, log, and [Fe/H]—with a single-element wide output layer for each. Additional strategies were implemented to handle missing data effectively, ensuring the network could still operate robustly when encountering NaN values in large catalogues.

SECTION: 4Training and Testing

During the training of the MLP model we allocated 80% of the data for training, 10% for internal validation, and the remaining 10% for final testing. These subsets were randomly selected, and the networks were run multiple times to ensure consistent and repeatable results across different test subsamples.

During the training, the network cycles through the dataset multiple times in what are called epochs, each time adjusting its internal parameters (or coefficients) to minimize the loss function tested against the validation subset, which is not used for training. The goal is to align the network’s output as closely as possible with the known spectroscopic values (Teff, log, and [Fe/H]) during training. Once the network achieves satisfactory performance or runs for a predetermined number of epochs, its final performance is evaluated on the test subset, which is entirely separate from the training and validation data, to verify the accuracy of the network’s output against the desired parameters post-training.

It is important to recognize that any significant biases or deviations in the test subset’s Teff, log, and [Fe/H] measurements will similarly influence the neural network’s output, as the machine learning algorithm’s accuracy cannot exceed that of the reference measurements used during training. If this happens this is a clear sign of overfitting.

In Fig.1, we compare the spectroscopic measurements from SoS with the ML predictions on the test subsample for each parameter. To quantify the model’s performance, we present both the mean and median errors in Tab.2to identify potential biases in the ML predictions. We also provide the standard deviation and the Median Absolute Deviation (MAD), defined as:

whereis the normalization constant to compare MAD to standard deviation.

Our findings indicate that the results of this initial implementation of the MLP model are quite accurate. Predicted Teffaligns with the SoS spectroscopic measurements within a 80-85 K range, depending on the statistical measure used. Logaccuracy is within 0.14-0.09 dex, and the error for [Fe/H] predictions ranges from 0.12-0.08 dex. The residual bias on the parameters, when compared with standard deviation, is completely irrelevant. We are still trying to understand if we can further improve the NN model to improve performance on the rare outliers.

Although the average and median performances are very reliable, it’s noteworthy that the disparity between the standard deviation and the MAD suggests occasional large errors by the model. In Fig.2, we illustrate the complete error distribution on a logarithmic y-scale, highlighting these extremely rare but largely discrepant results.

SECTION: 5Final sample

SECTION: 6Conclusions

In this contribution we presented a preliminary study that shows the feasibility of using an ML model trained on high-quality spectroscopic data to derive key stars parameters, such as T, logand [Fe/H], from lower quality photometric data. The performance of this method applied to photometric data from Gaia and SMSS merged catalogues shows extremely good performance in determining estimates for the above parameters, with errors (on the test set) characterized by a standard deviation of 85.0for T, 0.14for logand 0.12for [Fe/H]. We managed to obtain this thanks to the high-quality spectroscopic measurements provided by the SoS catalogue which were used for the training. This allow us to apply the method to a potentially huge star datasets with hundreds of millions and up to billions of stars, thus providing to the astronomic community the largest high-quality data sample up to date. Future improvements of this work will include an extension to datasets covering also the northern hemishpere (such as SDSS) and a more detailed statistical analysis on the ML predictions outside the test set, where no spectroscopic-quality reference measurements are available. A comparison with other sources of high-quality spectroscopic measurements will also allow us either to improve or to better characterize the model performance.

SECTION: References