SECTION: Enhanced Astronomical Source Classification with Integration of Attention Mechanisms and Vision Transformers
Accurate classification of celestial objects is essential for advancing our understanding of the universe. MargNet is a recently developed deep learning-based classifier applied to the Sloan Digital Sky Survey (SDSS) Data Release 16 (DR16) dataset to segregate stars, quasars, and compact galaxies using photometric data. MargNet utilizes a stacked architecture, combining a Convolutional Neural Network (CNN) for image modelling and an Artificial Neural Network (ANN) for modelling photometric parameters. Notably, MargNet focuses exclusively on compact galaxies and outperforms other methods in classifying compact galaxies from stars and quasars, even at fainter magnitudes. In this study, we propose enhancing MargNet’s performance by incorporating attention mechanisms and Vision Transformer (ViT)-based models for processing image data. The attention mechanism allows the model to focus on relevant features and capture intricate patterns within images, effectively distinguishing between different classes of celestial objects. Additionally, we leverage ViTs, a transformer-based deep learning architecture renowned for exceptional performance in image classification tasks. We enhance the model’s understanding of complex astronomical images by utilizing ViT’s ability to capture global dependencies and contextual information. Our approach uses a curated dataset comprising 240,000 compact and 150,000 faint objects. The models learn classification directly from the data, minimizing human intervention. Furthermore, we explore ViT as a hybrid architecture that uses photometric features and images together as input to predict astronomical objects. Our results demonstrate that the proposed attention mechanism augmented CNN in MargNet marginally outperforms the traditional MargNet and the proposed ViT-based MargNet models. Additionally, the ViT-based hybrid model emerges as the most lightweight and easy-to-train model with classification accuracy similar to that of the best-performing attention-enhanced MargNet. This advancement in deep learning will contribute to greater success in identifying objects in upcoming surveys like the Vera C. Rubin Large Synoptic Survey Telescope.

SECTION: Introduction
Classifying celestial objects is a longstanding challenge in observational astronomy. The segregation between stars, quasars, and galaxies in astronomical images is pivotal to harnessing the best science out of the data. Efficient and robust ways of classification have become of topical importance due to the plethora of ongoing and upcoming astronomical surveys such as the Dark Energy Survey (DES)-, the Sloan Digital Sky Survey (SDSS)-, the Zwicky Transient Facility (ZTF)-, Euclid, the Subaru Prime Focus Camera, etc. The upcoming Vera C. Rubin Observatory’s LSSTwill further contribute to this data deluge, collecting approximately 15 terabytes of data every night. Given this large volume of data, manual analysis becomes impractical, necessitating the development of automated tools for accurate classification of astronomical sources. These tools are crucial for advancing our understanding of the universe, especially cosmology and structure formation. A more detailed discussion of the impact of source separation on cosmological analyses can be found in.

Over the past two decades, the evolution of Machine Learning (ML), especially Deep Learning (DL), has significantly impacted astronomy. ML, with its data-driven training for learning a task, and DL, utilizing multi-layer neural network architectures to acquire hierarchical feature representations, have revolutionized tasks in many areas of astrophysics. A non-exhaustive list of applications of ML to astrophysics includes the classification of stellar spectra, photometric light curves, and galaxy morphologies, photometric redshift estimation, gravitational wave analysis, identification of strong lenses, separation of pulsars signals from radio frequency interference, etc. Moreover, DL algorithms have been shown to be very effective in detecting contaminants in astronomical images such as cosmic ray hits, spurious reflections (“ghosts”), and light scattering. This integration enhances efficiency and accuracy in various astronomical tasks, showcasing the ongoing potential for data-driven advancements in the field.

The growing availability of data has elevated ML as a powerful tool for astronomical source classification tasks. In the past, methods like Random Forest (RF)-, Support Vector Machines (SVM)-, and Gradient Boosting classifierswere favoured for their ease of training with limited data but suffered from weaker performance, typically achieving an accuracy between 80and 90for the star-galaxy separation. More recent approaches utilizing Convolutional Neural Networks (CNNs)-have yielded significantly improved results. These advancements were facilitated by the ML community’s progress in developing enhanced image classification algorithms, as exemplified by the ImageNet dataset.

Star-galaxy classification is pivotal to achieving the science requirements from ongoing and next-generation photometric surveys. Although many metrics based on point-spread function (PSF) information have been used for star-galaxy separation, they cannot easily separate stars and quasars. Therefore, many works have explored star-galaxy-quasar separation using ML. While some earlier studies addressed the entire star, quasar and galaxy separation problem using classical ML and DL algorithms, it is essential to note that there is often a faint limit in source classification, beyond which accurate categorization becomes challenging. For instance, current methods experience performance degradation when dealing with objectsthanand. With the advent of new sky surveys like the Vera Rubin LSST, which observe even fainter celestial objects, achieving accurate classification at these faint magnitudes becomes increasingly paramount.

In this study, we leverage DL to distinguish stars, quasars, and compact galaxies at faint magnitudes, where stars and quasars appear as point sources convolved with the PSF. On the other hand, galaxies are extended sources. Traditionally, differentiating stars and quasars relied on spectra, a resource-intensive process. ML, particularly color-based methods, provides an efficient alternativefor the star-quasar classification. However, distinguishing high-redshift quasars remains challenging due to intrinsic color variations. Morphological approaches for galaxiesface limitations with fainter and more compact objects, prompting the exploration of spectral energy distributions, although hindered by data availability. Our investigation extends to studying attention-augmented CNNs and Vision Transformer (ViT) architectures, offering potential enhancements for image-based source classification tasks in astronomy and addressing challenges posed by traditional methods.

Recently, a novel DL framework called(C23, hereafter) was introduced for classifying stars, quasars and compact galaxies at faint magnitudes with enhanced accuracy. This method employs a DL architecture that combines photometric features-based and image-based classification, using a stacking ensemble of an Artificial Neural Network (ANN) and a CNN, respectively. The model’s training dataset incorporates photometric features and images in five photometric bands (,,,,) from the SDSS Data Release (DR)16. The objects have been classified as stars, quasars or galaxies spectroscopically. Selection criteria outlined in Section 2 of C23 ensured the inclusion of compact galaxies and faint sources by applying constraints on the de Vaucouleur radius and photometric magnitude. Their study conducted three experiments to assess the performance in the faint and compact regime, emphasizing the importance of precise source separation.

This work addresses the classification challenge of faint and compact astronomical objects by enhancing the “MargNet” model architecture initially used in C23. We extend our investigation to incorporate attention-enhanced CNNs and ViT architectures to better handle image data. In particular, we integrate the Squeeze and Excitation (SE)-attention module into CNNs, a technique that has previously shown substantial performance improvements in image classification tasks.

First introduced by, ViTs excel in image classification, object detection, and semantic segmentationtasks. ViTs represent a revolutionary paradigm in computer vision, diverging from traditional CNNs by adopting self-attention mechanisms. Their unique approach to processing image patches through self-attention allows them to capture global dependencies and intricate spatial relationships, leading to state-of-the-art performance. ViTs have also proven effective in transfer learning, particularly in scenarios with limited labelled data, and found applications in medical image analysis, remote sensing, and various domains where understanding global context is critical.

While attention-enhanced CNNs have improved performance over baseline CNN models, their application in astronomy remains relatively unexplored, except for contaminant detection. Although ViTs have been investigated for galaxy morphology classification, their potential to address the source separation problem remains untapped. Further, by leveraging the capabilities of ViT, we aim to extract and analyze intricate features from diverse astronomical data types, including photometric features and FITS images. This approach facilitates the identification of unique signatures associated with various classes of celestial objects.

Our primary contributions in this work are described as follows:

Exploring the modality-specific source classification models tailored to handle photometric features and FITS images separately.

Investigating the optimal approach to source classification using FITS images solely, comparing the performance of CNNs with and without attention augmentation and ViT-based models.

Unraveling the “MargNet” model and expanding its capabilities by integrating attention-enhanced CNNs and ViT models for the image-based classifier.

Exploring the synergistic potential of integrating photometric features and FITS images through fusion in ViT-based architectures. This augmentation aims to enhance astrophysical source detection capabilities within the baseline ViT framework.

The remainder of this paper is structured as follows: Sectioncovers data acquisition from the SDSS DR16 Catalogues. Sectiondescribes the preprocessing steps, the methods employed for the classification problem and details on the models. In Section, we present the results and discussion; in Section, we present the conclusions and further possibilities arising from this work.

SECTION: Dataset and Experiments
The primary objective of this study is to reliably classify the astrophysical sources with faint and compact characteristics.
For this study, we rely on SDSS DR 16, specifically utilizing SDSS photometric data in the,,,, andpassbandsas model inputs. Spectroscopically assigned classes from the SDSS pipeline serve as labels for supervised training, ensuring the accuracy of our model predictions. We use both photometric features and FITS images from the SDSS DR 16 for our analysis.

SECTION: Compact and Faint source dataset
To create a sample of galaxies which can be identified as compact, we use the same compactness parameter () introduced in C23 and choose the threshold ofto separate the two.
To constitute the faint object dataset, we impose the following constraint on the average magnitude across the five passbands from SDSS data, similar to C23:

Using these cuts, we create both a Compact source dataset as well as a Faint and Compact source dataset using DR16 data in the same way as C23.

After retrieving the photometric features and FITS images from both the datasets, we are set up to conduct the experiments. We split the dataset into training, validation, and test sets, ensuring an equal representation of each class. We prepared three different experiments, which we label as,, andand the details of each of these experiments can be found below:

We obtain all three sets: training, validation, and test from the Compact source dataset. Importantly, the test set is representative of the training set in this context.

The training, validation, and test sets are drawn from the Faint and Compact source dataset.

The training and validation sets are selected from the Compact source dataset. However, the test set is chosen from the Faint and Compact source dataset.

SECTION: Methodology
The primary objective of this work is to explore optimal models for source classification, utilizing both the photometric features and FITS images. Our study includes the independent and joint analysis of models leveraging these two types of features.

SECTION: Classification using Photometric Features
We utilized 24 distinct features, as outlined in Table 1 of C23, to represent the photometric information derived from celestial source images. This phase capitalizes on the wealth of photometric features to extract insights into celestial objects, encompassing brightness, color, and variability. The classification task using photometric features involves employing classical supervised ML classifiers alongside ANNs. ML classifiers, including Decision Trees (DT), RF, and XGBoost (XGB), play a pivotal role in classifying entities such as star-galaxy-quasars. Furthermore, adhering to the model proposed in C23, incorporating ANN classifiers offers an additional approach for achieving precise and accurate classification of celestial objects.

, a prevalent ML technique in astronomy for tasks like classification and regression, operate by recursively partitioning the data based on feature conditions and establishing a tree. Each split in the tree represents a decision, and collectively, they form a hierarchical structure. DTs are known for their interpretability and simplicity, enabling clear insights into decision-making. However, they may be prone to overfitting and capturing noise in the training data. Regularization techniques, ensemble methods like RF, and careful tuning help mitigate these issues, making DTs valuable in astronomical analysis.

, a widely used ML technique in astronomy, creates ensembles of DTs through bootstrap resampling, preventing overfitting by leveraging diverse subsets of features during training. Introduced in, RF excels in efficiency, scalability, and competitive performance for classification and regression in astronomy compared to other ML algorithms.

, an influential ML algorithm widely applied in astronomy for tasks such as classification and regression, stands out for its exceptional predictive performance. An optimized gradient boosting framework, XGB sequentially builds a strong predictive model by combining weak learners (typically DTs). It incorporates regularization techniques to control model complexity and prevent overfitting, and it includes advanced features like parallel processing and handling missing values. Renowned for its speed, efficiency, and ability to handle large datasets, XGBoost has become popular in astronomy for achieving accurate and robust predictions.

, inspired by biological neural networks, ANNs emulate biological neurons, receiving inputs, processing information, and producing outputs. The ANN model in this study, (same as in C23), is a feed-forward network with stacked dense layers activated by ReLU units. Five layers provide ample depth for directly classifying astronomical sources from selected parameters. Dropoutswith a 0.25 fraction are employed to combat overfitting in deep networks. The final layer, softmax-activated with three (or two) outputs for each category, utilizes categorical (or binary) cross-entropy as the loss function for the star-quasar-galaxy classification.

SECTION: Classification Using FITS Images
The SDSS FITS images in all five passbands are preprocessed in the same way as C23. We then apply the following methods to these images for astronomical source separation.

are essential for image processing in computer vision. Their strength lies in specialized layers like convolutional and pooling layers, effectively capturing hierarchical image features. CNNs excel at recognizing patterns, edges, and textures, enabling detailed visual analysis. Parameter sharing in convolutional layers reduces model complexity, enhancing generalization across image regions. This adaptability makes CNNs robust for object recognition, image classification, and segmentation. Additionally, CNNs have been successfully applied to astronomical source classification.

In C23, a CNN architecture inspired by the Inception Network, as introduced byfor the ImageNet Challenge, is employed for compact and faint astronomical source separation. This adaptation of InceptionNet, previously successful in astronomy, utilizes layer sizes from, initially designed for photometric redshift estimation.

Including the inception module is crucial, aiding in extracting discriminative features from various convolution filter sizes (11, 33, and 55). These filters are applied at a specific depth, and their outputs are concatenated to create a consolidated layer. We considered the same CNN model from C23 as our baseline and tried to improve the performance over the baseline models across all three experiments. Figureillustrates the chosen inception module in this work. CNN processes pre-processed FITS image files for data objects, providing initial classification output. With 80,000 images per class and spectroscopic labels, the CNN uses convolutional layers with varying kernel sizes ([1,1], [3,3], [5,5]) activated by ReLU units. Dense layers distil information, and the final layer employs softmax (or sigmoid) for multi-class (or binary) classification. It uses categorical cross-entropy as the loss function. The complete details of the CNN architecture used can be found in Table 3 of C23.

As discussed in Section, attention mechanisms play a pivotal role in computer vision in enhancing the capability of models to focus on relevant information within an image. Analogous to human visual attention, these mechanisms enable models to selectively emphasize specific regions or features during processing. By dynamically assigning weights to different input parts, attention mechanisms improve the model’s ability to capture intricate patterns, relationships, and contextual information. Integrating attention mechanisms has become a fundamental aspect of modern computer vision architectures, contributing to overall advancement and adaptability in image-based tasks. Furthermore, the attention blocks can be effortlessly incorporated into pre-existing CNN models, allowing for seamless integration without requiring extensive modifications to the architecture.

Inspired by the success of attention-enhanced CNN models, we experimented with various attention mechanisms to enhance the performance of our baseline CNN model for celestial object classification. In particular, we investigated the SENet. We also explored the Simple Attention Module (SimAM;) and Convolutional Block Attention Module. The SENet introduces a mechanism to adaptively recalibrate the importance of different channels within feature maps. The “squeeze” operation captures global information by applying global average pooling, while the “excitation” operation learns channel-wise dependencies. This enables SENet to dynamically assign weights to channels based on relevance, enhancing the network’s discriminative power and performance in tasks such as image classification and object detection. The block diagram representing operations in SENet is illustrated in Figure.

SE networks have emerged as a powerful enhancement to CNNs for image classification tasks. They aim to improve model performance by explicitly modelling channel-wise dependencies, allowing the network to adaptively recalibrate the importance of different channels. One crucial parameter in SE networks is the reduction ratio, often denoted as. This parameter controls the dimensionality of the intermediate representations within the SE block. Specifically, during the squeeze phase, which involves global average pooling to obtain channel-wise statistics, the reduction ratio determines the number of intermediate features used to capture channel dependencies. Mathematically, if the input tensor haschannels, the reduction ratio () reduces the number of channels tobefore applying the excitation phase. Typically,is chosen as a hyperparameter, and commonly used values range from 2 to 16, although smaller values like 2 or 4 are prevalent.

Choosing an appropriate reduction ratio is crucial for balancing model complexity and performance. A higher reduction ratio decreases computational cost but may limit the model’s capacity to capture complex channel interactions. Conversely, a lower reduction ratio may improve expressiveness but at the expense of increased computational overhead. The reduction parameter is often determined through experimentation and validation on the specific dataset and task. It is essential to balance model capacity and efficiency to achieve optimal performance.

represent a paradigm shift in computer vision, leveraging transformer architectures fromfor image classification tasks. Unlike traditional CNNs, ViTs treat images as sequences of patches, allowing them to capture local and global contextual information. This approach enables ViTs to scale efficiently to large image sizes, contributing to their success in various image classification challenges. ViTs have demonstrated state-of-the-art performance on benchmarks such as ImageNet, showcasing their potential to outperform traditional CNNs in certain scenarios. On a related note, Linformeris a variant of the ViT architecture that specifically addresses the quadratic complexity of the self-attention mechanism in ViT, making it more computationally efficient. While ViTs have excelled in image classification tasks, Linformer’s advancements in attention mechanisms within ViTs make it particularly relevant for applications requiring the processing of extensive image datasets. ViTs and Linformer demonstrate the adaptability of transformer-based architectures in handling diverse image-related tasks, significantly contributing to the evolution of machine learning methodologies in computer vision.

The Classification (CLS) token in the ViT architecture is a pivotal element at the model’s beginning. It serves as a representative summary of the entire input image, capturing global contextual information. Traditionally initialized randomly during training, some approaches consider deriving the CLS token intentionally from specific features. This is studied specifically when multimodal data is available in remote sensing applications. This token is crucial in aggregating essential information for downstream tasks, contributing significantly to ViT’s visual recognition and understanding performance. In Figure, the architecture of ViT is illustrated, and the CLS token is extracted randomly while working with images solely.

SECTION: Classification Using Photometric Features and FITS Images
We systematically explored hybrid models that combine photometric features with images, investigating various integration approaches. Our experimentation involved deploying stacking ensemble models, similar to MargNet, which were trained separately using photometric features and images. Later, these models were combined using ensemble techniques. The second idea is to implement nuanced adjustments within the ViT architecture (proposed). This included training a unified model that leverages both photometric features and images. Specifically, we adopted the multi-modal fusion-based Vision Transformer (MM ViT) paradigm, inspired by the insights introduced by. This comprehensive investigation aimed to optimize the fusion of visual and photometric information to enhance model performance. Further details on these approaches are provided below.

The CNN (for images) and ANN (for photometric features) models were trained using an early stopping criterion. These pre-trained models were then integrated in parallel through stacking to create the final network, MargNet. The models were fine-tuned for their inputs, eliminating the need for retraining MargNet models. CNN and ANN outputs, generated from images and photometric data of each class, were used as inputs for the ensemble,. We also explored alternative architectures for MargNet by replacing the CNN model with attention-augmented CNNs and ViTs, maintaining the ANN model unchanged.

This represents a pioneering approach in computer vision by seamlessly integrating visual information from diverse modalities. Drawing inspiration from ViT’s success in image classification, MM ViT extends its capabilities to concurrently handle various image modalities, such as multi-spectral and hyper-spectral data. Unlike traditional models that process each modality separately, MM ViT employs a unified architecture, enabling joint learning and information fusion. This framework empowers the model to capture intricate relationships and dependencies across different modalities, leading to more robust and holistic representations. By leveraging self-attention mechanisms and hierarchical feature extraction, MM ViT showcases the potential for enhanced performance in tasks requiring synthesising information from various sources, making it a versatile and powerful tool in multimodal learning scenarios.

Our study treated photometric features and images as distinct modalities and conducted experiments to fuse them seamlessly. We strategically designated the CLS token from the ViT architecture as a pivotal hyperparameter to achieve this. Traditionally, the CLS token in ViT training is selected randomly. However, our methodology advocates deriving the CLS token from photometric features. Simultaneously, the model receives images as input in patch form, facilitating the comprehensive incorporation of both data types. This approach contributes to a more refined and comprehensive understanding of the combined visual and photometric information. The architecture of the proposed MM ViT is illustrated in Figure. The CLS token is extracted by passing the photometric features through a simple ANN model. The final dense layer (withneurons) dimension is adjusted to match the dimension of the flattened image patches. Finally, the CLS token is concatenated with the projected and flattened patches before being fed into the ViT encoder.

SECTION: Performance Evaluation
To assess the effectiveness of classification models, it is essential to utilize three cornerstone metrics: Precision, Recall, and Accuracy. Precision measures the proportion of correctly identified positive instances (TP) out of all instances classified as positive (TP + FP), providing insight into the model’s ability to avoid false positives (FP). Conversely, to prove insight into the model’s ability to avoid false negatives (FN), Recall evaluates the model’s capability to capture all positive instances by calculating the ratio of true positives (TP) to the total number of positive instances (TP + FN). Accuracy, another crucial metric, measures the accuracy of the model’s predictions by calculating the ratio of correctly classified instances (TP + TN) to the total number of instances. Together, these metrics offer a comprehensive understanding of a model’s performance across different aspects of classification tasks, thereby facilitating informed decision-making in model selection and optimization.

For a specific class, precision () and recall () are calculated as follows:

c

We compute the aggregate precision () and recall () by averaging the precision () and recall () values across all classes. Precision for galaxies signifies the classifier’s proficiency in avoiding misclassifying stars/quasars as galaxies. In contrast, recall for galaxies denotes the classifier’s effectiveness in identifying all galaxies in the dataset.

Accuracy can be calculated for a single class or all classes combined together, and is given by:

wheredenotes the true negatives.

SECTION: Results and Discussion
For each experiment described in Section, we investigate two scenarios: star-galaxy classification and star-galaxy-quasar classification. Subsequent sections delve into the specifics of each methodology, focusing on the corresponding data type and the quantitative performance metrics. We emphasize the results obtained from models utilizing photometric features and FITS images as input.

SECTION: Performance Analysis with Photometric Features
The photometric features extracted from the source images exclusively serve as inputs for models that do not incorporate image data. We utilized classical ML algorithms, including DT, RF, and XGB, alongside an ANN model to accommodate these explicit features.Since this analysis does not consider images, no additional preprocessing is necessary apart from standard data normalization. The performance of each model is summarized in Table, and our deep learning models are implemented using theframework. Tableshows that XGB consistently outperforms DT and RF classifiers across all experiments and classification settings. The best performance is achieved by either XGB or ANN. For the ANN model, we employed a dropout fraction of 0.25, and the model comprises 331,331 neurons for the architecture employed.

SECTION: Performance Analysis with FITS Images
We initially employed the inception-based CNN model from C23 as our baseline while working with the FITS images alone. We then enhanced this baseline CNN by integrating attention mechanisms to improve its performance. Particularly, we investigated the incorporation of the SENet as detailed in Sectioninto our baseline model after each inception module. Moreover, our study represents the pioneering exploration of source classification using attention-augmented CNNs and ViT architectures in the literature. Preprocessed images with dimensions of 32325 serve as input for all these models, excluding the photometric features.

In our investigation of the SENet-enhanced CNN, we explored the ‘reduction (r)’ hyperparameter within the SENet. We experimented with values ranging from 2, 8, 16, and 32, studying the SENet-CNN with eachvalue in separate experiments and selecting the best-performing model for each case. Similarly, while exploring the ViT, we considered various patch sizes such as 4, 8, and 16, with a 44 patch size proving the most effective. We implemented Linformer to alleviate the computational burden, using 12 encoder layers, 8 attention heads, and 64 as the dimensions of the flattened patches. Results are presented in Tablefor the baseline CNN model and the proposed attention-enhanced CNN (with corresponding hyperparameter values for the best-performing model) and ViT-based models. Tabledemonstrates that the SENet-augmented CNN outperforms the baseline CNN across all the experiments and classification tasks, albeit with marginal performance gain across all the metrics. Further, the proposed SENet-CNN shows significant gains over the ViT models, specifically for Experiments 2 and 3. The proposed SENet-CNN demonstrate decent performance gain in Experiment 3, while models trained on bright sources are applied to faint sources. Tabledetails the computational complexity for all image-based models. The performance gain with the SENet-augmented CNN comes at the expense of an additional 0.1to 3parameters. The highest performance gain in terms of accuracy of around 1is observed in Experiment 3 for both star-galaxy and star-galaxy-quasar classification, and the lowest gain of 0.1is noticed in Experiment 1 compared to the baseline model. Conversely, the ViT model, comprising only 3.4of the parameters of the baseline CNN, achieves comparable performance across all settings, with a maximum performance drop of around 2.9and 3.7observed in Experiment 3 for the star-galaxy-quasar classification, when compared with the baseline CNN and best-performing SENet-CNN respectively. The lowest performance drop of 0.3and 0.4is observed in Experiment 1 for the star-galaxy separation, compared to the baseline CNN and best-performing SENet-CNN. However, the proposed ViT stands out as the most lightweight image-based model for source classification.

SECTION: Performance Analysis with Photometric Features and FITS Images
FITS images and photometric features are provided as inputs when working with the ensemble models and the proposed MM ViT. Ensemble model (MargNet), comprises a parallel stack of photometric feature-based and image-based models. In this study, alongside the baseline CNN model for images (proposed in C23), we explored the proposed SENet-augmented CNN and ViT performance for the image-based model while maintaining the ANN fixed. These pre-trained models were then integrated in parallel through stacking to construct the final network, MargNet. We meticulously fine-tuned the CNN and ANN models for their respective inputs, obviating the need to retrain MargNet. Each class’s objects, comprising images and photometric data, are provided as inputs for the CNN and ANN models. The outputs from these models subsequently feed into the ensemble. The outputs from both components are merged, and a statistical combination of these outputs yields the final result. MargNet’s architecture at concatenation encompasses 103 neurons, and the corresponding trainable parameters with different image-based models are detailed in Table.

While working with the proposed MM ViT model, we incorporated photometric features and FITS images into the same architecture. We used the same ViT architecture studied for image-based classification to achieve this, as detailed in Section. However, a key distinction in using the MM ViT lies in the derivation of the CLS token. Instead of relying solely on image data, we generated the CLS token for the ViT by incorporating photometric features, as depicted in Figure. The parametric specifications for the ViT remain consistent with those discussed in Section. Notably, we adjusted the dimension of the final dense layer, withneurons, to 64, matching the dimension of the flattened image patches. Tableillustrates the computational complexity of the proposed MM ViT. Hybrid models that take both photometric features and images (MargNet-based and MM ViT from Table) as input perform better than individual ANN (for photometric features, from Table) and CNN or ViT (for images, from Table) models for both star-galaxy and star-galaxy-quasar classifications. The performance of hybrid models across all the experiments described in Sectionare discussed in detail in the sections below.

This experiment selects all the three sets: training, validation, and testing from the Compact Source dataset. Performance metrics are computed for star-galaxy and star-galaxy-quasar classifications as illustrated in Table. For star-galaxy classification, quasars are excluded to facilitate comparison with only star-galaxy separation models used in other studies. The proposed SENet-augmented MargNet achieves an overall accuracy of, while MargNet, ViT-MargNet, and MM ViT each achieve an average accuracy of. We can also see from Figurethat the performance on both stars and galaxies is similar (98.31for galaxies and 98.12for stars) and higher with SENet-MargNet. MargNet exhibits a slightly higher accuracy for galaxies at 93.32while maintaining an accuracy of 98.01for stars. Meanwhile, ViT-MargNet and MM ViT demonstrate performance with an accuracy of 98.1and 97.8for galaxies 98.08and 98.52for stars, respectively. Notably, misclassifications between stars and galaxies are minimal, with MargNet, SENet-MargNet, ViT-MargNet and MM ViT achieving rates as low as approximately 1.83, 1.78, 1.91and 1.84, respectively.

For the task of star-galaxy-quasar classification, the proposed SENet-MargNet achieves an average accuracy of, with similar accuracies of,andobserved for the MargNet, ViT-MargNet and MM ViT models, respectively. However, detailed analysis reveals variations in performance across the individual classes. The confusion matrices in Figurehighlight distinct accuracies for stars, galaxies, and quasars. Quasars exhibit the lowest accuracy when classified by the ViT-MargNet model, achieving 90.63accuracy, while MM ViT achieves the best accuracy of 92.35. An accuracy of 90.79and 91.15are achieved using MargNet and SENet-MargNet, respectively. Notably, the MM ViT model demonstrates the lowest accuracy for stars, with a rate of 91.18, and ViT-MargNet demonstrates the lowest accuracy for galaxies at 95.63. Further analysis reveals specific misclassifications within each model. For SENet-MargNet, quasars are misclassified as stars in approximately 5.85of cases and galaxies in around 3. The MargNet model misidentifies quasars as stars in 6.13of cases and galaxies in 3.08. The ViT-MargNet misclassifies quasars as stars at 6.13and galaxies at 3.23of rates. MM ViT misclassifies quasars as stars and galaxies at the same rates of 3.84and 3.81, respectively. The more frequent identification of quasars as stars is understandable, as both are point sources. For the same reason, stars are misclassified as quasars in, 5.20, 5.31and 7.54of the cases for the MargNet, SENet-MargNet, ViT-MargNet and MM ViT, respectively. Galaxies have the best individual accuracy among the three classes and across all the models. Galaxy-quasar misclassifications (galaxy-quasar: 2.90and quasar-galaxy: 3.08for MargNet, galaxy-quasar: 3.04and quasar-galaxy: 3.0for SENet-MargNet, galaxy-quasar: 3.13and quasar-galaxy: 3.23for ViT-MargNet and galaxy-quasar: 2.80and quasar-galaxy: 3.81for MM ViT being the individual misclassification rates) are more common than galaxy-star misclassifications (galaxy-star: 1.16and star-galaxy: 1.26for MargNet, galaxy-star: 1.07and star-galaxy: 1.29for SENet-MargNet, galaxy-star: 1.24and star-galaxy: 1.39for ViT-MargNet and galaxy-star: 1.13and star-galaxy: 1.28for MM ViT). This arises from the quasar host galaxy fuzz, which may be observable particularly with low-luminosity quasars, sometimes resulting in misclassification as a compact galaxy. Stars, devoid of structure beyond the Point Spread Function (PSF), are generally less susceptible to being misclassified as galaxies.

In Experiment 2, all datasets; training, validation, and test—are derived from the Faint and Compact Source dataset. Similar to Experiment 1, various models are trained and evaluated using the Experiment 2 dataset, and their performance metrics are summarized in Table. Across all metrics, there is a decline in performance compared to Experiment 1, attributed to the lower signal-to-noise ratio (SNR) as objects become fainter. This noise affects photometric measurements and feature calculations, posing challenges in classification. For star-galaxy separation, SENet-MargNet achieves 97.10.1accuracy, while MargNet, ViT-MargNet, and MM ViT achieve 96.90.1, 96.80.1, and 96.90.1, respectively. Confusion matrices depicted in Figurereveal SENet-MargNet’s excellence with 96.82accuracy for galaxies and 97.39for stars. MargNet shows 96.76accuracy for galaxies and 97.19for stars. ViT-MargNet and MM ViT score 96.72and 96.78for galaxies and 96.86and 97.04for stars, respectively. Minimal misclassifications between stars and galaxies are observed, with rates of approximately 2.89, 3.02, 3.21, and 3.09for SENet-MargNet, MargNet, ViT-MargNet, and MM ViT, respectively.

SENet-MargNet achieves an average accuracy of% for star-galaxy-quasar classification, similar to MargNet, ViT-MargNet, and MM ViT, which achieve accuracies of%, 86.90.2, and 86.30.2, respectively. Individual class accuracies are depicted in Figure. Quasars display the lowest accuracy with MM ViT at 81.28and the highest with SENet-MargNet at 83.37. Stars achieve the lowest accuracy with MM ViT at 82.6and the highest with MargNet at 85.1. Similarly, for galaxies, the lowest accuracy of 94.1is achieved using ViT-MargNet and the highest with MM ViT at 95.01. Notably, misclassifications among stars, quasars, and galaxies are minimal, with SENet-MargNet, MargNet, and MM ViT achieving error rates as low as approximately 6.21, 6.29, 6.5, and 6.85, respectively. We further evaluate the performance statistics as a function of magnitude to assess model behaviour at fainter levels. The test set is divided into bins of 0.1 magnitudes within therange, ensuring each bin contains at least 50 objects per class for robust analysis. Metrics are assessed for each bin and visualized in Figureand Figure. For star-galaxy-quasar classification, Figure(MargNet model) shows accuracy steadily decreasing up to, in line with diminishing SNR.

Similar trends are observed for quasar precision, quasar recall, and star recall, with a rise following a drop. Star precision and galaxy recall remain consistently high. However, galaxy precision behaves differently; at, no increase is observed, plateauing. A similar trend is observed for SENet-MargNet (in Figure), ViT-Margnet (in Figure) and MM ViT (in Figure) models.

Experiment 3 selects the training and validation sets from the Compact Source dataset, while the test set is drawn from the Faint and Compact Source dataset. As in Experiments 1 and 2, a range of models are trained and evaluated using the Experiment 3 test dataset, and their performance metrics are summarized in Table. Given that the training data remains consistent between Experiments 1 and 3, we directly employ the models trained on Experiment 1’s training data to assess their performance on the Experiment 3 test data. However, across all metrics, a decline in performance is evident compared to Experiment 1, attributed to the lower SNR as objects become fainter. For star-galaxy separation, SENet-MargNet achieves an accuracy of 92.90.1, while MargNet, ViT-MargNet, and MM ViT achieve 92.00.1, 91.70.1, and 91.80.1accuracy, respectively. The confusion matrices illustrated in Figurehighlight SENet-MargNet’s excellence with an accuracy of 88.48for galaxies and 98.43for stars, followed closely by MargNet with accuracies of 87.67for galaxies and 98.62for stars. ViT-MargNet and MM ViT achieve accuracies of 86.75and 86.81for galaxies and 98.46and 98.37for stars, respectively. Notably, minimal misclassifications between stars and galaxies are observed, with rates of approximately 6.54, 6.85, 7.39, and 7.41for SENet-MargNet, MargNet, ViT-MargNet, and MM ViT, respectively.

Similarly, for the star-galaxy-quasar classification, SENet-MargNet achieves an average accuracy of 73.20.2, on par with MargNet, ViT-MargNet, and MM ViT, which achieve accuracies of 73.10.2, 72.70.2, and 71.80.2, respectively. Figureillustrates the individual class accuracies. Quasars exhibit the lowest accuracy, ranging from 66.2with MM ViT to 69.04with SENet-MargNet. For stars, SENet-MargNet achieves the lowest accuracy at 95.14, while ViT-MargNet achieves the highest at 95.5. Similarly, in galaxies, the lowest accuracy of 64.72is attained using MM ViT, while SENet-MargNet achieves the highest at 65.24. Notably, misclassifications among stars, quasars, and galaxies remain minimal, with SENet-MargNet, MargNet, and MM ViT demonstrating error rates as low as approximately 11.76, 11.87, 11.81, and 12.28, respectively. Across all experiments and classification settings, the proposed SENet-MargNet consistently outperforms all other models. While the performance enhancement achieved with SENet augmentation on MargNet incurs an additional computational cost of approximately 0.1to 3in trainable parameters, integrating ViT into MargNet and MM ViT requires only 3.4and 5.8of the parameters of the baseline MargNet model, respectively. Notably, MM ViT performs comparably to ViT-MargNet.

SECTION: Conclusions and Future Work
Our study presents the results aimed at enhancing the performance of the previously introduced MargNet model, utilizing attention mechanism and ViT-based architectures for classifying stars, galaxies, and quasars in SDSS photometric images. Within the SDSS dataset, our proposed MargNet variants, particularly SENet-MargNet, exhibit superior classification accuracy compared to prior neural network-based approaches, particularly for compact and faint galaxies. Additionally, our ViT-MargNet and MM ViT models offer lightweight alternatives. A significant advantage of CNN and ViT-based models is their ability to learn useful features directly from images automatically. This alleviates the need for separate feature engineering, as typically required in traditional machine learning algorithms. Notably, a portion of MargNet’s architecture incorporates inception net modules, which have demonstrated promising results in various image classification tasks and have gained widespread adoption within the computer vision community. While conventional networks tend to exhibit reduced accuracy as sources become fainter and more compact, SENet-MargNet showcases the promising performance, achieving an overall accuracy of 93.5, with MargNet achieving 93.5, and ViT-MargNet and MM ViT models achieving accuracies of 93.2each. When considering faint and compact sources, individual CNN and ANN components achieve approximately 91.6and 93.0accuracy, respectively. However, notable improvement in accuracy is observed when these components are combined within MargNet and MM ViT. The applicability of MargNet-based models can be extended to future surveys, given their ability to effectively capture numerous faint and compact sources. Furthermore, MargNet holds promise for surveys like GAIA and ZTF, which rely on crossmatching with SDSS and employ transfer learning techniques. Even in scenarios where photometric features differ from SDSS, adjustments to the ANN network within MargNet can accommodate alternative features. Alternatively, the image-based CNN component of MargNet can be utilized independently if photometric features are unavailable. In conclusion, the methodology underlying MargNet proves valuable due to its high accuracy in object classification, making it well-suited for deployment in forthcoming astronomical surveys with deeper observational reach.

SECTION: Acknowledgements
Srinadh Reddy was supported by Tata Consultancy Services (TCS) and the Department of Science and Technology - Interdisciplinary Cyber-Physical Systems (DST-ICPS) (under the grant T-641). He is grateful to Ajit Kembhavi and Yogesh Wadadekar for useful feedback on his thesis defense presentation.
We thank the Sloan Digital Sky Survey for making their data free and open source. The Alfred P. Sloan Foundation, the U.S. Department of Energy Office of Science, and the Participating Institutions have funded SDSS IV. SDSS-IV acknowledges support and resources from the Center for High-Performance Computing at the University of Utah. The SDSS website is www.sdss.org. SDSS-IV is managed by the Astrophysical Research Consortium for the Participating Institutions of the SDSS Collaboration including the Brazilian Participation Group, the Carnegie Institution for Science, Carnegie Mellon University, Center for Astrophysics — HarvardSmithsonian, the Chilean Participation Group, the French Participa- tion Group, Instituto de Astrofísica de Canarias, The Johns Hopkins University, Kavli Institute for the Physics and Mathematics of the Uni- verse (IPMU) / University of Tokyo, the Korean Participation Group, Lawrence Berkeley National Laboratory, Leibniz Institut für Astro- physik Potsdam (AIP), Max-Planck-Institut für Astronomie (MPIA Heidelberg), Max-Planck-Institut für Astrophysik (MPA Garching), Max-Planck-Institut für Extraterrestrische Physik (MPE), National Astronomical Observatories of China, New Mexico State Univer- sity, New York University, University of Notre Dame, Observatário Nacional / MCTI, The Ohio State University, Pennsylvania State University, Shanghai Astronomical Observatory, United Kingdom Participation Group, Universidad Nacional Autónoma de México, University of Arizona, University of Colorado Boulder, University of Oxford, University of Portsmouth, University of Utah, Univer- sity of Virginia, University of Washington, University of Wisconsin, Vanderbilt University, and Yale University.

SECTION: Software
Astropy, Numpy, Scipy, Matplotlib, Seaborn, Pandas, Jupyter, scikit-learn, Python3, and Pytorch.

SECTION: Model Training Details
We have implemented our deep-learning models in PyTorch 1.9.0 using the Adam optimizer. The CNN, SENet-CNN, and ViT (for images) and ANN (for photometric features) models underwent training with an early stopping criterion, ceasing training if there was no improvement over 30 and 100 consecutive epochs, respectively. Finally, the ensemble models (MargNet, SENet-MargNet and ViT-MargNet) were trained for 100 epochs for the last layers freezing the corresponding photometric feature-based and image-based models, with the best-performing models being saved. For the MM ViT, an early stopping criterion was applied, ceasing training when there was no further improvement over 100 consecutive epochs. Each model (ANN, CNN and ViT-based) was trained five times during training, and the average results are presented.

SECTION: Data Availability
The PyTorch implementation of the proposed framework has been made open-source and is publicly available on. Photometric data was acquired from SDSSDR16 using a specific query, while image data was obtained fromthrough a Python script. The fully preprocessed dataset is accessible on.

SECTION: References