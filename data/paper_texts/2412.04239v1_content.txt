SECTION: On Deep-Learning-Based Closures for Algebraic Surrogate Models of Turbulent Flows

A deep-learning-based closure model to address energy loss in low-dimensional surrogate models based on proper-orthogonal-decomposition (POD) modes is introduced. Using a transformer-encoder block with easy-attention mechanism, the model predicts the spatial probability density function of fluctuations not captured by the truncated POD modes. The methodology is demonstrated on the wake of the Windsor body at yaw angles of, withas a test case. Key coherent modes are identified by clustering them based on dominant frequency dynamics using Hotelling’son the spectral properties of temporal coefficients. These coherent modes account for nearly 60% of the total energy while comprising less than 10% of all modes. A common POD basis is created by concatenating coherent modes from training angles and orthonormalizing the set, reducing the basis vectors from 142 to 90 without losing information. Transformers with different size on the attention layer, (64, 128 and 256), are trained to model the missing fluctuations. Larger attention sizes always improve predictions for the training set, but the transformer with an attention layer of size 256 overshoots the fluctuations predictions in the test set because they have lower intensity than in the training cases. Adding the predicted fluctuations closes the energy gap between the reconstruction and the original flow field, improving predictions for energy, root-mean-square velocity fluctuations, and instantaneous flow fields. The deepest architecture reduces mean energy error from 37% to 12% and decreases the Kullback–Leibler divergence of velocity distributions fromto below.

SECTION: 1Introduction

Surrogate models are data-driven computational techniques used in various scientific and engineering fields to approximate complex systems or functions. These models serve as simpler substitutes for both experiments and computationally expensive simulations, thus providing quicker, yet sufficiently accurate results(Sun & Wang,2019). Surrogate models are mainly utilized to estimate the optimum-product solution or as instrumental tools to evaluate the performance in the initial stages of the vehicle development because they reduce the resource requirements for design exploration(Kuyaet al.,2011; Yondoet al.,2018).

In the particular case of fluid-dynamics applications, surrogates are tipically built on a reduced space due to the complexity and high dimensionality of the original phenomenon(Yondoet al.,2018). The dimensionality reduction can be done either with algebraic methods,e.g.the proper-orthogonal decomposition (POD)(Lumley,1981), or employing deep-learning-based techniques. POD was first introduced in fluid dynamics byLumley (1981)to express the chaotic turbulent motions into modes representing some portion of the total fluctuating energy of the flow.Sirovich (1987)explored the relationship between POD and the dominant features of the flow, and showed that POD is a relevant tool for the study of vortex dynamics in all types of fluid flows. Recently, other modal decompositions have been introduced in order to obtain modes that are associated with a single frequency instead of the range of frequencies present in the time series of the temporal coefficients in POD. Among these new techniques, the most popular are dynamic-mode decomposition (DMD)(Schmid,2010)and spectral proper-orthogonal decomposition (SPOD)(Towneet al.,2018). Note that while POD and SPOD rank the modes in terms of their contribution to the reconstruction of the original flow, DMD obtains modes classified in terms of their dynamical importance to minimize errors in the reconstruction.

Alternatively, deep-learning methods for dimensionality reduction are based on unsupervised-learning methodologies such as autoencoders. There are application examples of several autoencoder architectures for dimensionality reduction in fluid dynamics, including vanilla(Eivaziet al.,2020; Murataet al.,2019), hierachical(Fukamiet al.,2020), physics-assimilated(Zhang,2023)and variational autoencoders(Eivaziet al.,2022; Wanget al.,2024; Solera-Ricoet al.,2024; Akkariet al.,2022). All of them are able to capture the non-linear behaviour of dynamical systems with a higher compression capacity than any POD-based methodology thanks to the excellent capabilities of spatial convolutions for non-linear feature extraction(Bruntonet al.,2020; Vinuesa & Brunton,2022).

It is particularly relevant to mention that-variational autoencoders based on convolutional neural networks (CNN-VAEs) have been used successfully to obtain a disentangled latent representation of turbulent fluid flows. For instance,Eivaziet al.(2022)compressed the turbulent flow around a simplified urban environment into 5 orthogonal latent variables containing more than the 85% of the flow energy. However, the need of convolutional layers restricts the usage of this technique to geometries that can be represented on a regular grid. On the other hand, algebraic decompositions can be used on unstructured grids at the cost of losing a significant amount of the energy of the system. A good illustration of this is the aforementioned study fromEivaziet al.(2022), where 5 POD modes barely recover 30% of the flow energy. Accurately capturing all the fluctuations in a turbulent flow would require selecting nearly all the modes of the system.

Coupletet al.(2003)proved that large-index POD modes drain energy from the more significant modes, yielding an energy-cascade structure. Such a modal-energy redistribution suggests that reduced-order models (ROMs) can be built on a small number of significant modes that represent the majority of flow features and the contribution of the rest of modes can be modelled as an additional term to the ROM. This conclusion has led to an intense research on closures for reduced-order models based on Galerkin and Petrov–Galerkin projections of the Navier–Stokes equations. These models constitute a fundamental pillar for the stability of the projection(Stabile & Rozza,2018; Kaptanogluet al.,2021)and have been traditionally inspired by sub-grid scale models such as the ones used in large-eddy simulations (LESs)(Wanget al.,2012; Hijaziet al.,2020; Imtiaz & Akhtar,2020). More recently, such closures have been modelled with data-driven techniques such as probabilistic neural networks(Mauliket al.,2020). A recent review and comparison of data-driven methods for ROM closures can be found inPrakash & Zhang (2024).

The main goal of this manuscript is to present a new data-driven model capable of recovering the energy loss due to modal truncation in POD. Instead of working in the reduced space as the aforementioned closures, this work is focused on learning the spatial probability density function of the difference between the original field and the POD reconstruction using only the most significant modes with a transformer model(Vaswaniet al.,2017). A transformer is a deep-neural-network architecture initially developed in the field of natural-language processing (NLP). Since then, it has revolutionized many areas of machine learning thanks to its attention mechanism, which enables identifying long-range dependencies in the data more effectively than traditional models(Yousifet al.,2023). The rationale behind the approach proposed is to build a reduced-order model capable of predicting the most significant features of the flow, which are fully dependent on the geometry and initial conditions, and then add a separate correction for the smaller turbulent scales. The methodology is tested on the turbulent wake of the flow past the Windsor body(Littlewood & Passmore,2010), which is a simplified square-back vehicle. The model is designed to produce a closure valid for any free-stream-velocity direction in a yaw-angle range. The objective of the closure is to improve the POD reconstruction of the root-mean-square values of the stream-wise velocity fluctuations. This test case is highly relevant for the automotive industry because in any road vehicle the drag force increases linearly for yaw angles in the range of(Howell,2015). This drag increase is completely independent of the zero-yaw drag, thereby making it impossible to extrapolate the performance in cross-flow conditions from the parallel-flow case(Howell,2015). Hence, car manufacturers need to evaluate the aerodynamic performance under yawed flows in the development loop of a new vehicle(D’Hoogeet al.,2014). The development could be massively accelerated by using a surrogate model instead of re-running the simulations and wind-tunnel tests that are needed to characterize the aerodynamic performance of a road vehicle(Zhanget al.,2006)at every angle of interest.

There are a number of studies in the literature that propose models able to evaluate dependence of the forces and moments on the yaw angle. For instance,Gonget al.(2012)built a surrogate model based on the Kriging interpolation technique, which obtains the optimal wind-deflector geometry in a tractor trailer to reduce drag in crosswind situations. Similarly,Ghoreyshi & Cummings (2014)developed a model able to predict the dependence of the forces and moments on the Mach number, the angle of attack and the side-slip angle of an aircraft, whileZhanget al.(2021)presented a model to predict the derailment coefficient of a train in crossflows. Lately,Eiximenoet al.(2024c)developed a model to interpolate the mean base pressure in the Windsor body. To the authors’ knowledge, there are no models that can evaluate changes in high order statistics on unseen flow conditions, hence, there are no models able to predict the velocity fluctuations with the yaw angle.

The rest of this manuscript is organized as follows:section 2describes how the closure is formulated, how the significant POD modes are selected and how the model is extended to multiple flow conditions; then,section 3shows the accuracy of the closure in the wake behind the Windsor body; and finally,section 4summarizes the main findings of the manuscript.

SECTION: 2Methodology

This section describes the methods used in the manuscript, including the Windsor-body dataset employed to test the methodology, a mathematical definition of POD and the selection of the most significant modes, together with an explanation of the model used to add the energy from the truncated modes.

SECTION: 2.1Dataset description

The test dataset is the turbulent wake behind the Windsor body, the simplified square-back vehicle depicted inFigure 1, at a Reynolds number of, whereis the magnitude of the free-stream velocity,is the length of the model andis the kinematic viscosity of the fluid. The data was generated by means of wall-modeled large-eddy simulations at yaw angles of. For the simulations, the spatially filtered incompressible Navier–Stokes equations,

were numerically integrated using SOD2D (Spectral high-Order coDe 2 solve partial Differential equations)(Gasparinoet al.,2024b), a low-dissipation spectral-element-method (SEM) code(Gasparinoet al.,2024a).

In the equations aboveare the spatial coordinates (or,, and),(or,, and) stands for the velocity components andis the pressure. Note thatis the density of the fluid. The filtered variables are represented by.
The right-hand-side term inEquation 2represents the sub-grid stresses, and its anisotropic part is expressed as,

where the large-scale rate-of-strain tensoris evaluated as, withandbeing the Kronecker delta. Here, the unresolved scales are modelled using the local formulation of the integral length-scale approximation (ILSA)(Lehmkuhlet al.,2019). The near wall region was modelled using the Reichardt wall-law(Reichardt,1951)with an exchange location in the 5th node(Lehmkuhlet al.,2018).

After the initial transients had been washed out, all simulations were run for 60 additional convective time units,, to collect 660 snapshots. The data for the model assessment was interpolated into the plane represented inFigure 1. This plane is perpendicular to the vertical axis, therefore it contains the dynamics of both the leeward and windward sides of the wake. It is located at, which is half of the vehicle height when measured from the bottom of the body.

In the present work only a brief comparison of the fluid flow at the different yaw angles is shown to illustrate the different conditions in which the closure needs to be valid. For more details on the numerical model, grid and simulations accuracy, the reader is referred to the previous work byEiximenoet al.(2024c)on the development of a surrogate model for the base pressure of the Windsor body.

In terms of the averaged flow, the wake of square-back bluff bodies in a yawed free stream flow is dominated by two vortices: one on the leeward side () and one on the windward side (), as it was shown byBooysenet al.(2022). InFigure 2, the flow streamlines are plotted forand. As reported byBooysenet al.(2022), the vortex on the leeward side dominates the recirculation and gains intensity over the windward vortex as the yaw angle increases. This effect moves the vortex centers and the saddle point to the leeward side of the vehicle and closer to the body.

The changes in the vortex intensity have an effect on the recirculation length.Lorite-Díezet al.(2020)identified, in a square-back Ahmed body, that this vortex interaction leads to a decrease on the recirculation length and, to a deflection of the recirculation bubble towards the leeward side of the wake. Similar to the square-back Ahmed body, in the Windsor body, this trend is also observed. This can be seen inFigure 3, where the mean streamwise velocityfor the cases atandis plotted. Here the recirculation length varies from 0.41L to 0.28L.

InFigure 4changes in the velocity fluctuations brought about with the yaw angle are illustrated by comparing the root-mean-square of the streamwise velocity fluctuations,at bothand.Figure 4shows that a larger yaw angle increases the entrainment of irrotational free-stream into the near wake, resulting in larger fluctuation intensity and a steeper shear layer angle on both sides of the vehicle. The latter leads to a narrower wake. This is in agreement with the findings fromLiet al.(2019)on a square-back Ahmed body.

The above changes in the mean flow with the yaw angle can also be observed when the mean streamwise velocity and its fluctuations are plotted along a streamwise line atand over a cross-stream line at, respectively (seeFigure 5). For the objectives of the current work, it is relevant to remark that neither the fluctuations maxima nor their positions in the domain have a linear evolution with the yaw angle. Thus, it is not possible to derive a linear model to predict them.

SECTION: 2.2Proper-orthogonal decomposition (POD)

POD is used in this work as a dimensionality-reduction technique. It is an efficient way to capture an infinite-dimensional process with a reduced number of modes(Holmeset al.,1997). This method is based on finding a set of deterministic functions that characterize the dominant features of the system given by the field. This decomposition can be written as,

whereis the number of functions to decompose the field into. POD requires that the basis for the spatial modes is orthonormal, i.e.,

and optimal, so that the the firstvectors are the ones that reconstruct the database with the minimum possible error.

In this work, the chosen method to perform POD is the singular-value decomposition (SVD). The SVD decomposes the initial snapshot matrix,, into the left singular vectors,, the singular values,, and the right singular vectors,,

Each column ofcontains a spatial mode,and each column ofgives the evolution of the time coefficient,, of the corresponding mode. The singular values are given in a diagonal matrix and are associated with the energy contribution of each mode in descending order. The higher the singular value, the more energy is contained in the mode. The POD analysis has been performed using pyLOM(Eiximenoet al.,2024a), a high-performance-computing reduced-order-modelling code that has a parallel and scalable algorithm for the singular-value decomposition(Eiximenoet al.,2024b).

SECTION: 2.3On the significance of POD modes

Turbulent flows are characterized by a flat-tail of singular values, making it difficult to set an energy threshold to select the modes onto which the data has to be projected. This threshold is set arbitrarily and is decided based on a trade-off between accuracy of the model and evaluation cost. To overcome this issue, in this work the selection of the relevant modes is based on their frequency content. The objective is to select only the modes that contain relevant information on the frequency of the coherent structures of the flow. This is achieved by identifying outlier modes in the power-spectral density (PSD) matrix of the temporal coefficients,. In other words, the selected modes are those that exhibit a frequency spectrum significantly different from the rest.

The PSD matrix ofis computed by performing the Lomb-Scargle periodogram to the temporal coefficient of each mode of the system. Then, the outlier modes are identified with principal-component analysis, PCA. PCA is analogous to POD once the data has been normalized with its variance and centered to its mean. Since the PCA model may contain numerous components, its information is summarized using Hotelling’s:

whereis the projection of the PSD of modeinto the PCA componentandis the covariance of that component. Note thatcan be seen as the distance from the center of the hyperplane formed by the components to the projection of the observation onto the hyperplane. The larger thevalue is, the more relevant frequency content the mode will have. Hence, the modes now can be selected with athreshold value that contains all the outliers.

SECTION: 2.4POD projection and reconstruction

The POD basis for data projection is built using the spatial correlations of themodes corresponding to the frequency outliers. When working withdifferent inlet conditions, one can find an optimal POD basis among them by concatenating the spatial correlations of the outlier modes from each case to create the following matrix:

Then, POD is applied to matrixto find an orthonormal basis that contains the information of the selected modes for each of the inlet conditions:

The resulting basis can be truncated as long as there are no information losses, i.e. the selected modes are able to recover more than 99% of the energy.

The data matrixcan be projected now ontoas follows:

with the assurance that all coherent modes inside the inlet-conditions range are included in the reduced-order model. This operation reduces the dimensionality of the numerical data and sets a latent space for any surrogate-modelling applications. Such a surrogate model can be used to perform temporal predictions of the system or to evaluate its response to any condition in the evaluated range.

When a prediction,, is reprojected back into the full-order space:

the main behavior of the system is captured, however, the model lacks the energy from the modes that were discarded during its construction.

SECTION: 2.5Closure model

The missing energy of the prediction arises from the error between the original data and the reconstruction from the truncated POD modes:

To build a closure for the missing scales in the POD projection and reconstruction process, it is essential to understand the spatial and temporal distribution of this error. In other words, it is necessary to determine where and when this error is more likely to occur. The strategy followed in this work involves learning the evolution of the error as a function of the recovered fluctuations,, since this field contains all relevant information about the system’s state at all points in the domain for the studied timestep. To achieve this, a transformer(Vaswaniet al.,2017)encoder block is trained to minimize the difference between the actual error field,, and the predicted one, using the temporal series ofacross all points in the domain. The training process employs a mean-squared-error loss function. Thus, ifis known for a given timestep, the transformer can predict the corresponding error field,. From now on, the error predicted by the transformer is represented asand the error of the model after considering the closure is defined as:

The choice of using a transformer-based model is motivated by their ability to identify and predict the temporal dynamics of chaotic systems by capturing long-term dependencies in the data(Wuet al.,2022; Geneva & Zabaras,2022; Sanchis-Agudoet al.,2023). Additionally, transformers are well-suited for forecasting time series based on other spatial variables(Wang,2023)through their variant known as visual transformers (ViT). Transformers can be seen as universal approximators to probability density functions(Furuyaet al.,2024). Hence, the proposed model actually learns the joint probability density function (PDF) ofgiven,. Furthermore, there exists an attention-only, Transformerwith attention normalizationsuch that, for any auto-regressive sequenceconverges exponentially fast asgoes to infinity, whereis the number of attention layers. Denoting

one has

For a more detailed study of the transformer’s universality and the analytic intrinsics when approximating the theoretical measure the reader is referred toGeshkovskiet al.(2024); Sander &
Peyré (2024).

The latter definition ensures that the system modeled by the transformer is statistically equivalent to the original one and that the closure will be generalizable as long as the joint PDFfor a new set of data is similar to the original one. Such similarity is quantified using the Kullback–Leibler divergence,:

whererepresents the joint PDF of the error for a single snapshot, whileis the joint PDF for all snapshots included in the training.

In this study, the input signal has a time-delay dimension of 48 steps, which means that the input to the transformer is a sequence of 48 consecutive time steps of the POD reconstruction. The output is the prediction of the error of the first time instant of the input series. A time-space embedding module is added to each point time signal to incorporate temporal and spatial information before passing it to the transformer blocks, allowing the model to distinguish between the evolution of the velocity in different points at different time steps. An average pooling and a max-pooling layer are added to the time-space embedding. Both of them are one dimensional and have a stride of two steps.

Three different architectures (Table 1) are tested to see the effect of the number of parameters on the closure accuracy. All of them are based on a single transformer encoder block (Figure 6) with eight attention heads followed by a feed-forward layer. The only change between the three architectures is the size of the attention layers. The shallowest architecture has an attention size of 64 and then increases to 128 and 256. Those layers are in charge of measuring the importance of different parts of the input sequence when making predictions(Bahdanauet al.,2014). Note that, the dimension of the feed-forward layer is set to 128 in all three cases. This layer learns complex non-linear relationships between the input and output sequences.

The choice of using multi-head attention is based on its oustanding performance over scaled dot product attention as it allows the model to jointly attend to information from different representation subspaces at different positions(Vaswaniet al.,2017). In particular, the current architecture employs the easy-attention mechanism(Sanchis-Agudoet al.,2023), which has demonstrated promising performance in predicting the temporal dynamics of chaotic systems, significantly outperforming the self-attention transformer(Solera-Ricoet al.,2024). The easy-attention mechanism originally presented bySanchis-Agudoet al.(2023)is defined by the mapping, given by the equation, where both the pseudo-input, input after embedding, and output matrices have the same dimensions, the attention size (Table 1). In this formulation,andare matrices of trainable parameters, withrepresenting the temporal feature dimension andthe spatial feature dimension. Following the standard notation used in transformer architectures,denotes the values, while the matrixrepresents the attention weights. This mechanism, expressed as a kernel operation, can be formulated as:

To extend the easy-attention mechanism to the multi-head attention strategy, we consider multiple attention heads so that each of them focus on different parts of the input space. In this case, the inputis projected into multiple subspaces, allowing the model to attend to different sources of information simultaneously. For each attention head, we perform the same kernel operation as defined in the original mechanism, but with distinct sets of trainable parameters for the attention weights and value projections.

The multi-head version of the kernel operation can be written as:

wheredenotes the index of the attention head,is the number of attention heads, and eachandare distinct trainable parameters for the-th attention head.
The final output of the multi-head attention is obtained by concatenating the outputs of each attention head:

After the transformer block, a one-dimensional convolutional network of the same size as the attention layer and a fully connected layer of size of the number of points are added to decode the transformer output and form the final spatial prediction of the POD reconstruction error.

The training of each architecture was conducted along 3500 epochs, which required up to 8 hours and 15 minutes using an NVIDIA H100 GPU from the accelerated partition of the supercomputer MareNostrum 5(Barcelona Supercomputing Center,2024). An extensive discussion on the accuracy of each architecture is presented next in the results section.

SECTION: 3Results

This section presents the performance of the designed closure model for the POD reconstruction of the turbulent wake behind the Windsor body. The dataset described insection 2is split between the training,, and test,, sets. The training set is used to build a common POD basis along the yaw-angle range and to train the transformer which predicts the reconstruction error. Then, the high-fidelity results atare projected into that POD basis and reconstructed with the additional closure term from the transformer to assess its performance on unseen data. All results are obtained for the streamwise velocity fluctuations, therefore,inEquation 6is equivalent to.

SECTION: 3.1POD common basis

The first step to build the common basis is to perform the POD of each of the training angles individually. After that, the PSD-based mode-selection process described insection 2is applied.Figure 7shows theclustering results for each of the angles. The threshold for the coherent-mode selection is set to. Table2shows the number of selected modes in each case and the amount of energy recovered. It can be seen that the tendency is to have between 30 and 40 modes per angle containing coherent structures that represent a somewhat larger amount than half of the total energy. The case atis the one in which the coherent modes account for the smallest energy percentage, 52.0%, while forthey account for up to 58.1%. It is important to note that the rest of the energy is shared among the remaining 600 non-coherent modes, therefore, each of them has a small individual contribution to the total energy of the system.

Two modes of the case atare used to illustrate the clustering process.Figure 8compares the spatial correlation of a coherent mode with the one of a non-coherent mode. Note that the chosen coherent mode is the fifth most energetic one and the non-coherent mode is the 450th most energetic one. The coherent mode is clearly dominated by four large correlated regions linked to the vortex shed from the windward side of the vehicle, whereas the non-coherent mode depicts multiple small scales.

Figure 9compares the temporal coefficient and its spectrum for both modes. The spectrum of the coherent mode (9(a)right) exhibits a peak at the non-dimensional frequency of. This peak corresponds to the windward vortex-shedding frequency(Eiximenoet al.,2024c; Booysenet al.,2022). Note that no dominant frequencies can be observed in the spectrum of the non-coherent mode (9(b)right), which is completely flat as those from pure white noise signals. These modes are seen as noise in the reduced system as their temporal coefficients are completely uncorrelated. The temporal coefficients of the non-coherent modes (9(b)left) suggest that the lack of correlation might come from an inadequate sampling frequency, this one being lower than the dominant frequency of these modes. The noisy and random evolution of the non-coherent modes, together with their small individual energy contribution, would increase the cost of a surrogate model if they were included in the reduced system. However, they cannot be discarded without an efficient closure that accounts for the large energy percentage that they contain as a group.

The spatial correlations of the selected modes are then concatenated to create the matrixas inEquation 8. As some coherent modes might be repeated in the yaw angle range, POD is applied to matrixto find the optimal and orthonormal basis that contains the information of the selected modes for the four angles.Figure 10shows the cumulative singular values to prove that instead of using all the 142 coherent modes, 90 vectors are enough to represent the information of all the coherent modes in the yaw-angle range under study.

11(a)presents the kernel density estimate of all the training snapshots for the original field,, its reconstruction after being projected into the common POD basis,, and the error between both of them,. The most likely situation is to have fluctuations close to zero in the original and reconstructed fields. This is explained by the large unperturbed area in the leeward () side of the domain. The source of error is then the filtering performed by the POD reconstruction of the high-amplitude fluctuations. Such filtering yields a field that is more likely to have points with velocity fluctuations close to zero than in the original case.11(a)also confirms that this is holds true for the test case at, bringing evidence that the common basis is valid for any angle in the studied range.11(b)compares the joint probability density function of the error given the reconstruction from the common basis,, for the training and tests fields. As stated insection 2, this is the probability density function learnt by the closure model as it ensures that the predicted error yields a statistically equivalent system to the original one. Consistently with the results shown in11(a), the most likely case in both the training and test datasets is to have a state with the velocity reconstruction and its error with the original field being close to zero. The most probable values for the test set match the ones of the training set, however, the limits offor the training set are wider than those at.

SECTION: 3.2Statistical closure accuracy

The three different architectures described inTable 1are tested in order to assess the correct size of the attention layer. InFigure 12, the probability density function,, given by the transformer output with the original one, represented in11(b), for both the training and test datasets are compared. Architecture 1, with an attention layer of size, performs poorly in learning both the center and the limits of the distribution. The Kullback–Leibler divergence between the transformer prediction and the original probability for the training data is ofand for the test data is. Both values are the highest ones obtained during the architecture refinement process. In this case, the main source of error is that the PDF learnt by the transformer is much narrower than the original one, meaning that the model fails to recover the fluctuations with larger amplitude.

Increasing the attention layer to, with its subsequent duplication of the number of parameters, allows the transformer to learn a wider area of. This reduces the KL divergence with the original data tofor the training snapshots andfor the test snapshots. It is relevant to mention that this architecture nearly matches the output distribution for the test set as the limits offorare narrower than the ones found in the training set.

Duplicating the attention size toleads to the best match of the training dataset of the three architectures. The learnt PDF expands for a wider area of fluctuations and the KL divergence is reduced to. Now the KL divergence on the test set is. The negative sign accounts for the larger fluctuations from the training set that are not present in the case ofand are already learnt by the transformer. Moreover, in this case the absolute value ofis slightly larger than the one found with architecture 2. This is the last step of architecture refinement because the evaluation of the test set has already crossed the ideal prediction in which the KL divergence would be null as the model shows the firsts signs of overfitting.

The wider area oflearnt by the architectures with larger attention size can be linked to the amount of turbulent kinetic energy (TKE),

recovered by the closure model. The TKE recovered in each case is quantified with the kernel density estimate among all the snapshots of the training and test sets separately.Figure 13effectively showcases that the most likely energy value,, after the reconstruction from the POD common basis is significantly lower than the one of the original flow. For the training snapshots, it is reduced fromtoand for the test dataset it decreases fromto.

Figure 13also shows that the most likely energy value when adding the closure term increases with the attention layer size of the transformer used to model the missing fluctuations. In the training angles,increases fromtowhen the attention sizes changes fromto. It finally reaches the value ofwith the largest architecture of. A similar behavior is observed with the case at, for the architectures withandasgoes up toand, respectively. However, for the architecture withthe most likely energy value,, is slightly higher than in the original flow. This can be explained by the fact that the probability density function of the fluctuations predicted by the transformer is wider than the ones of the real case (Figure 12).

This analysis brings evidence that the closure model actually reduces the offset between the energy of the POD reconstruction and the one of the original system.
It is important to note that the accuracy on the energy prediction is directly linked with the KL divergence between the predictedby the transformer and the ground truth. When the KL divergence is positive, the energy added by the closure is still smaller than the gap between the original flow and the POD reconstruction. Zero KL divergence would mean a perfect match between the model and the ground truth with no energy deviation. On the last scenario, a negative KL divergence indicates that the model is overshooting the predicted fluctuations, and with it the turbulent kinetic energy. Either with a positive or negative KL divergence, a larger absolute value indicates a larger deviation in the additional turbulent kinetic energy.

SECTION: 3.3Spatial field reconstruction

Up to this point of the discussion, it has been proven that adding a field of fluctuations based on thelearnt by the proposed transformer architectures is enough to close the energy gap of a POD reconstruction and the original flow field. However, it remains to be proven that the closure model can distribute these fluctuations adequately across the spatial and temporal domains.

Figure 14shows the root mean square (rms) of the velocity fluctuations in the spatial domain for the reconstruction from the common POD basis, the closure term trained withandtogether with the ones of the original field. The rms of velocity fluctuations is closely related with the local contribution to the total turbulent kinetic energy of the flow, hence, a field with the closure term matching the rms fluctuations of the original case could be considered accurate in space and statistically equivalent in time. In the figure, the case atis used to illustrate the performance on the various training angles. The results atare also plotted to show the performance of the model on unseen data. Moreover, the four mentioned reconstructions together with the one atare evaluated along the line atinFigure 15. The reader is referred toAppendix Afor the equivalent figures toFigure 14andFigure 15corresponding to the training cases at.

Both figures illustrate that the common basis captures the positions of the fluctuation maxima and their correct distribution along the domain, ensuring that the main flow structures are preserved throughout the projection and reconstruction processes (Equation 10andEquation 11). This is also valid for the case at, despite its features were not explicitly included in the basis.

In all analyzed angles, the reconstruction from the common basis misses the actual value by an offset associated with the filtered fluctuations.Figure 14andFigure 15show that increasing the attention size helps to close the gap in the rms fluctuations in all areas of the domain. The larger range of fluctuations learnt by the deeper architecture () and its additional kinetic energy added to the flow, translates to a nearly perfect match of the rms of the velocity fluctuations. It is worth mentioning that in the training cases most of the differences between the original field and the closure prediction arise from the model underestimating the fluctuations, however, atall the error of the closure is attributed to a slight overprediction.

As the offset between the POD reconstruction and the original field is not constant throughout the whole domain,Figure 14andFigure 15are also the evidence that the closure learns how much energy the POD reconstruction missed depending on the domain region. This proves that the closure model does not only close the energy gap statistically over all the points of all snapshots, but also that it can give accurate predictions of what happens in every point in the domain.

SECTION: 3.4Instantaneous-field reconstruction

After discussing how the closure model can emulate a field which is statistically equivalent to the original one in all points of the domain, it is time to discuss its impact on the reconstruction of the instantaneous fields. In this case, all comparisons are done with architecture 3 () as it is the only one able to close all the energy gap between the reconstruction and the original flow.

The first step is proving that the closure learns the amount of energy missing in each timestep. To do so,Figure 16shows the temporal evolution of the total turbulent kinetic energy together with its POD reconstruction and the reconstruction corrected with the closure model for the case at. This case is the evidence that common POD basis successfully captures the instants of all energy maxima and minima of the original field. Once again, this is still valid even if the flow condition was not included in the database.

Figure 16also shows that the closure model represents the energy missing in each snapshot instead of adding the same energy to all of them. However, in the particular case of, the actual energy predicted by the closure is consistently higher than the one of the original flow in each snapshot, as have been discussed in the previous paragraphs. Table4links the better prediction of the energy temporal evolution with the mean relative error regarding the energy of the original field. In all angles this error has been reduced from overto a margin betweenand.

Closing the energy gap appropiately in each snapshot also comes with a better prediction of the instantaneous fluctuations. To exemplify this,Figure 17compares the original instantaneous field and its reconstruction for a handpicked snapshot at, effectively showcasing that the POD reconstruction exhibits large deviations from the original data. In fact,18(a)illustrates that the reconstruction from the standard POD basis leads to a relative error larger thanin 57.8% of the points in the domain. After adding the closure term, the accuracy of the reconstruction is increased so that only 13.7% of the points have a relative error higher than.

The comparison between the probability density functions (PDF) of the fields,18(b), agrees with11(a)on showing that the POD reconstruction filters the high-amplitude fluctuations by increasing the points with fluctuations close to zero. When adding the closure term, the probability density function of the velocity fluctuations is nearly identical to the one of the original field. In fact, the Kullback–Leibler (KL) divergence between the reconstructed PDFs and the original one is reduced fromtoafter adding the term predicted by the transformer. Table5shows the meanover all snapshots to show that adding the closure term reduces the KL divergence with the original field of instantaneous velocity fluctuations regardless of the yaw angle.

SECTION: 4Conclusions

This manuscript presents a deep-learning-based closure model for truncated POD modes. The main objective is to provide a methodology to recover the energy lost when building a surrogate model on a low dimensional space. To do so, a transformer model is used to learn the spatial probability density function of the difference between the original flow field and the POD reconstruction from the modes that would be included in a surrogate. The methodology is tested for the streamwise velocity fluctuations on a slice in the wake of the Windsor body at the yaw angles of. As the model has to be generelizable for unseen data, the case atis used as a test dataset and the rest of angles are used for training.

Before working on the transformer model, a set of POD modes at each of the training angles is selected. Those modes have to be the most meaningful ones in the system as they would constitute the core of a reduced-order model. The selection process is based on performing principal-component analysis on the power-spectral density of the temporal coefficients. Then the modes with an outstanding frequency behavior are clustered with Hotelling’s. Those modes are named as coherent modes and this selection process ensures that they are the only ones that present relevant frequency dynamics.

In the particular case of the Windsor body, less than ten percent of the modes are coherent, however, they account for nearly 60% of the energy. The remaining 40% is distributed along the more than 600 non-coherent modes and is the one that needs to be modelled by the closure. The clustered modes from each training angle are concatenated to form a common basis that preserves the coherent structures inside the studied yaw angle range. POD is applied to the concatenated modes to ensure that all vectors from the basis are orthonormal between themselves and that they are the optimal representation of the coherent modes in that range. This operation reduced the number of basis vectors from 142 to 90 without any additional information loss.

Projecting any snapshot (regardless of whether it was included in the training set or not) into the common POD basis, filters the high-amplitude fluctuations. A transformer-encoder block with an easy-attention mechanism is used to learn the probability density function of the missing fluctuations depending on the reconstructed value from the POD common basis. Three different transformer architectures are trained in order to assess its effect on the recovered fluctuations. The main difference between the architectures is the change on the attention size. In the shallowest architecture it takes the value of. Then it is doubled twice to get an attention size ofand.

The larger the attention size, the more fluctuations from the training set are recovered. The accuracy of the prediction is quantified with the KL divergence between the transformer output and the original field. For the training set it reduces fromtowhen the attention size changes fromto. In the case of the test set, there is also an accuracy improvement when doubling the attention size up to, but then, the first signs of overfitting to the training data are seen with the deepest architecture. The evaluation of the closure atfor the architecture withis the only case in which the KL divergence has a negative value,. Note that a negative KL implies that the transformer has learnt larger fluctuation amplitudes from the training set that are not present in the test set.

Adding the fluctuations field predicted by the transformer reduces the energy gap between the POD reconstruction and the original field. The architectures with larger attention size recover more energy than the shallower transformers. For instance, in the training set, the most likely energy value after adding the closure withis of, and it rises tofor. In this case, since the fluctuations from the training set are larger than the ones of the test set, the overfitting observed when comparing the probability density function leads to an overshoot in the energy prediction. The evaluation of the test set withyieldsbut the most likely energy value in the original flow is.

Adding these fluctuations also leads to an improvement on the prediction of the root mean square value of the velocity fluctuations. The reconstruction from the common POD basis is able to capture the distribution of all local maxima and minima, but it falls short when matching the correct value. Then, the closure model helps to recover the missing fluctuations in the correct part of the domain. Once again, an increase on the attention size leads to a better closure of the offset. The evaluation of the deepest architecture atis the only case in which the rms prediction is larger than the original flow value. This is related to the energy overshoot discussed in the previous paragraph.

Finally, this manuscript proves that the energy added via the predicted fluctuations also reduces the error in the instantaneous flow field prediction. This is particularly true for the architecture with. The temporal mean of the energy prediction error is reduced on all angles from more than theto less than the. Moreover, the KL divergence between the velocity distribution reconstructed by POD and the one of the original field is consistently larger than, but the closure reduces it to less than.

[Acknowledgements]The authors acknowledge the Barcelona Supercomputing Center for the usage of MareNostrum 5 during the development of this manuscript. The authors also acknowledge the insightful conversations with Fermín Mallor during the development of the initial ideas giving place to this work.

[Funding]The research leading to this work has been partially funded by the project TIFON with reference PLEC2023-010251/ AEI/10.13039/501100011033. B. Eiximeno’s work was funded by a contract from the Subprograma de Ayudas Predoctorales given by the Ministerio de Ciencia e Innovación (PRE2021-096927). Oriol Lehmkuhl has been partially supported by a Ramon y Cajal postdoctoral contract (Ref: RYC2018-025949-I). The authors acknowledge the support of Departament de Recerca i Universitats de la Generalitat de Catalunya to the Research Group Large-scale Computational Fluid Dynamics (Code: 2021 SGR 00902) and the Turbulence and Aerodynamics Research Group (Code: 2021 SGR 01051). We also acknowledge the Barcelona Supercomputing Center for awarding us access to the MareNostrum IV machine based in Barcelona, Spain.

Marcial Sanchis-Agudo and Ricardo Vinuesa would like to acknowledge the support from Marie Sklodowska-Curie Actions project MODELAIR, funded by the European Commission under the Horizon Europe program through grant agreement number 101072559.

[Declaration of interests]The authors report no conflict of interest.

[Data availability statement]The data that support the findings of this study is available upon request. See JFM’sresearch transparency policyfor more information

[Author ORCIDs]B. Eiximeno, https://orcid.org/0000-0001-7018-6371; M. Sanchis-Agudo, https://orcid.org/0009-0000-1194-7900; A. Miró, https://orcid.org/0000-0002-2772-6050; I. Rodriguez, https://orcid.org/0000-0002-3749-277X; R. Vinuesa, https://orcid.org/0000-0001-6570-5499; O. Lehmkuhl, https://orcid.org/0000-0002-2670-1871

[Author contributions]B. E.:Writing – review & editing, Writing – original draft, Visualization, Validation, Software, Methodology, Investigation, Formal analysis, Data curation, Conceptualization.M. S-A.:Writing – review & editing, Writing – original draft, Software, Methodology, Investigation, Conceptualization.A. M.:Writing – review & editing, Supervision, Software.I. R.:Writing – review & editing, Supervision, Methodology.R. V:Writing – review & editing, Supervision, Resources, Project administration, Funding acquisition.O. L.:Writing – review & editing, Validation, Supervision, Software, Resources, Project administration, Methodology, Investigation, Funding acquisition.

SECTION: Appendix AAccuracy of the closure on the training angles

This appendix presents the comparison of the root mean square value of the streamwise velocity fluctuations for the angles of. Those angles were also included in the common basis as the case of. Moreover, the error of their reconstruction was also included in the dataset used for the training of the transformer.Figure 19complementsFigure 14andFigure 20complementsFigure 15as in those figures only the case atwas used to illustrate the effect of the closure on the cases used during training.

SECTION: References