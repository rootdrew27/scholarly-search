SECTION: Efficient-means with Individual Fairness via Exponential Tilting

In location-based resource allocation scenarios, the distances between each individual and the facility are desired to be approximately equal, thereby ensuring fairness.
Individually fair clustering is often employed to achieve the principle of“treating all points equally”, which can be applied in these scenarios.
This paper proposes a novel algorithm, tilted-means (TKM), aiming to achieve individual fairness in clustering to ensure fair allocation of resources.
We integrate the exponential tilting into the sum of squared errors (SSE) to formulate a novel objective function called tilted SSE.
We demonstrate that the tilted SSE can generalize to SSE and employ the coordinate descent and first-order gradient method for optimization.
We propose a novel fairness metric, the variance of the squared distance of each point to its nearest centroid within a cluster, which can alleviate theMatthew Effecttypically caused by existing fairness metrics.
Our theoretical analysis demonstrates that the well-known-means++ incurs a multiplicative error ofwith our objective function, and we establish the convergence ofTKMunder mild conditions.
In terms of fairness, we prove that the variance in each cluster generated byTKMdecreases with, whereis a hyperparameter that adjusts the trade-off between utility and fairness.
In terms of efficiency, we demonstrate the time complexity ofTKMis linear with the dataset size. Moreover, we demonstrate the monotonicity of the tilted SSE with respect toin a simple case.
Our experimental results demonstrate thatTKMoutperforms state-of-the-art methods in effectiveness, fairness, and efficiency.
Specifically,TKMexhibits a better trade-off between clustering utility and fairness than six baselines and achieveshundreds or even thousandsof times acceleration in running time.
Moreover,TKMcan overcome the RAM overflow issue that other methods encounter with a large dataset size.

SECTION: 1Introduction

In the era of big data, the scale of data is increasing exponentially[47,65], emerging from diverse fields[29,30]with rich information and potential value[64].
Clustering algorithms have become powerful tools for exploring the internal structure of datasets by partitioning data points into different clusters[7], where data points within the same cluster are similar to each other, while those in different clusters are dissimilar[34,66].-means is one of the most classic clustering algorithms, which measures the similarity between data points using Euclidean distance and is suitable for various types of data[46,17,23,61].
This characteristic makes-means widely applicable in various location-based resource allocation scenarios, such as opening new facilities to serve residents[38,31,62,54].

However, applying-means to resource allocation scenarios may lead to the issue of unfairness[53,35,5]. Consider the scenario in Figure1(a): when setting up public facilities such as hospitals for residents,-means tends to place these facilities closer to densely populated areas, resulting in sparsely populated areas having difficulty accessing public resources and unfair treatment for minority residents.
Individual fairness is a promising concept that can ensure that within the same cluster, each data point is treated approximately equally[42,18]. Figure1(b) shows the clustering results obtained by-means with individual fairness, where the distances from each resident to the centroid are approximately equal. In this case, we consider the clustering result to be fair.

One of the most widely studied concepts in individual fairness for-clustering is the “service in your neighborhood” proposed by Jung et al.[35].
This concept ensures that each data point has a centroid within a small constant factor of their neighborhood radius.
The neighborhood radius is defined as the minimum radius of a ball centered at each data point that includes at leastdata points, whereis the total number of data points.
Several studies[48,40]have made significant improvements in clustering utility and yielded tighter theoretical bounds based on this individual fairness concept. Mahabadi and Vakilian[40]introduced a local search method for-clustering that notably surpasses[35]in effectiveness.
Negahbani and Chakrabarty[48]proposed leveraging linear programming to develop improved algorithms for individually fair-clustering, both theoretically and practically.

However, the fairness definition of these methods faces a similar issue. To illustrate this, let us consider the scenario in Fig.2: point B resides in a sparsely populated area, with its neighborhood radius larger than point A situated in a densely populated area. This tends to result in opening more facilities in densely populated areas while only opening a few facilities in sparsely populated areas[48].
Within the same radius, individual A in densely populated areas has more opportunities to choose facilities, while individual B in sparsely populated areas may have only a single facility available.
Moreover, densely populated areas attract more individuals due to abundant resources. To meet the needs of these individuals, additional facilities must be opened. This results in the development of sparse areas continually lagging behind.
This phenomenon, also known as theMatthew Effect[44,10], is a sociological concept describing how the distribution of resources, wealth, and opportunities tends to favor individuals who already possess them.

Moreover, existing individually fair clustering methods suffer from the efficiency issue: their running time heavily depends on the dataset size. The most promising theoretical finding suggests a time complexity of[48].
Based on data from the U.S. Census, the population of New York City was 8.468 million in 2021[4]. Due to the high time complexity of existing algorithms, no individual fair clustering algorithm can effectively perform clustering analysis on such a large-scale dataset.
Moreover, as the dataset size increases, existing methods suffer from the issue of RAM overflow since they require computation of the distance between each pair data point, necessitating the storage of anarray in memory.
Additionally, in the clustering results obtained by these algorithms, each centroid must be selected from the data points, which is often unreasonable in real-world applications.

Exponential tiltingis a widely used technique to induce parametric shifts in distributions in various disciplines, including statistics[16,56,59], probability[25], information theory[43,11], and optimization[51,55].
Li et al.[37]first proposed using exponential tilting in machine learning to ensure the fairness of empirical risk minimization (TERM).
The flexibility of TERM lies in its ability to adjust the impact of individual losses using a scale parameter and thus enables us to effectively tune the influence of minority data points as required[63].
TERM provided examples of exponential tilting in supervised learning, such as linear regression and logistic regression. However, the practical applications of exponential tilting in unsupervised learning, especially in clustering algorithms, remain unresolved.
Furthermore, some theoretical analysis of TERM relies on the assumption that the objective function follows a generalized linear model, which does not hold for clustering algorithms.

We aim to utilize the ability of exponential tilting to induce parametric shifts in distribution to ensure individual fairness for clustering analysis.
Building on this concept, we propose a novel loss function, tilted SSE, for the individually fair-means problem based on exponential tilting, and suggest solving this problem effectively through coordinate descent (CD) and stochastic gradient descent (SGD), ensuring that the centroid in each cluster is closer to minority data points, thus guaranteeing individual fairness. Moreover, we demonstrate that tilted SSE can generalize to SSE when the scaled parameter inTKMis set to 0.
Due to the fact that existing fairness metrics may exacerbate the Matthew Effect in location-based resource allocation scenarios, we propose a novel criterion for evaluating fairness within clusters, utilizing thevarianceof distances between each data point and its centroid. Our fairness metric aims to treat each individual more equitably compared to existing metrics, thereby mitigating the Matthew Effect.

Our theoretical analysis comprises five parts: approximation guarantee, convergence analysis, fairness analysis, efficiency analysis, and monotonicity analysis.
Our approximation guarantee indicates that the centroids obtained through the well-known-means++ incur a multiplicative error of.
We establish the convergence analysis forTKMunder mild conditions.
Specifically, we demonstrate that the expected tilted SSE is non-increasing with respect to iterations.
For fairness analysis, we demonstrate that the variance of distances in each cluster decreases as the increase of hyperparameterinTKM.
A smaller variance indicates greater fairness, implying that asgrows, clustering becomes fairer.
For efficiency analysis, we demonstrate that the time complexity ofTKMis, similar to that of-means.
Note that the time complexity of the state-of-the-art method is[48], while that ofTKMis linear with respect to. Therefore,TKMexhibits a significant advantage in efficiency compared to other methods.
For monotonicity analysis, we demonstrate that the tilted SSE monotonically increases within a simple case. This property may guide the choices offorTKMin practical applications.

Our experimental evaluations demonstrate the effectiveness, fairness, efficiency, and convergence ofTKMover ten real-world datasets with five measurements. Our experimental findings indicate thatTKMoutperforms the state-of-the-art methods regarding the trade-off between clustering utility and fairness.
Specifically, we use SSE to measure clustering utility. The SSE ofTKMis lower than that of the state-of-the-art method and is very close to clustering algorithms that do not consider individual fairness on some datasets. To evaluate fairness, we use not only variance as a metric but also the maximum distance from each sample point to the centroid within each cluster.
Our results show thatTKMoutperforms the state-of-the-art fair clustering methods on both metrics.
Moreover,TKM’s performance in efficiency is remarkably impressive. Due to the linear time complexity with dataset size,TKMachieves acceleration ofhundreds or even thousands of timescompared to other fairness-aware clustering methods. Furthermore,TKMcan overcome the RAM overflow issues in other methods when dealing with large-scale data.
Additionally, we validate the impact of different hyperparameters inTKMon its convergence.

Our contributions are summarized as follows:

We incorporate exponential tilting into SSE to propose a novel method for individually fair-means:TKM.

We theoretically analyzeTKM’s approximation guarantee, convergence, fairness, efficiency, and monotonicity.

We experimentally validated the effectiveness, fairness, efficiency, and convergence ofTKM.

The remaining sections are structured as follows: Section2presents the notations used in this paper, Section3presents the related work, Section4introduces the preliminaries used in our study, Section5outlines our proposed method,TKM, Section6validates our algorithm through experiments, Section7concludes our paper, and Section8presents the proofs of our theories.

SECTION: 2Notations

We use different text formatting styles to represent different mathematical concepts: plain letters for scalars, bold letters for vectors, and calligraphic letters for sets. For instance,represents a scalar,represents a vector, anddenotes a set. Without loss of generality, all data points in this paper are represented using vectors.
We useto represent the set. The symboldenotes the expectation of a random variable, and we use “” to indicate a definition.
We useto denote the identity matrix.
We useto denote the Euclidean norm of a vector. We use the symbol “” to denote the natural logarithm with base. TableIlists the notations appearing in this paper and their interpretations.

SECTION: 3Related Work

We provide an overview of previous studies on fair clustering and the application of exponential tilting in various fields and highlight the limitations of these studies.

Fair Clustering.Fairness in clustering algorithms is typically divided into two categories:group fairnessandindividual fairness[15,18,42,27]. The goal of group fairness is to achieve clustering of a given set of points with minimal cost while ensuring that all clusters are balanced with respect to certain protected attributes, such as gender or race. Group fairness is not the focus of this paper, so interested readers can refer to[20,12,22,67,26].

The concept ofindividual fairnesswas initially introduced by Dwork et al.[28]in the context of classification, which posits that “similar individuals should be treated equally”. Several studies have explored this definition of individual fairness in clustering, such as[14,19]. Another widely used and researched concept of individual fairness is referred to as “service in your neighborhood”, which was initially suggested by Jung et al.[35].
This concept aims to ensure that each data point has a centroid within at most a small constant factor of their neighborhood radius, where the neighborhood radius is the minimum radius of a ball centered at the data pointthat includes at leastdata points.
Subsequently, various methods addressing the individually fair-clustering were based on this paradigm[48,40,21], along with numerous improved theoretical upper bounds[32,60].
Mahabadi and Vakilian[40]introduced a local search algorithm for-clustering, which significantly outperforms the method proposed by Jung et al.[35]in terms of clustering utility. Negahbani and Chakrabarty[48]proposed leveraging linear programming techniques to develop improved algorithms for individually fair-clustering, both theoretically and practically.
The fairness metric used in these methods can alleviate some of the unfairness in location-based resource allocation scenarios by ensuring that facilities are within a neighborhood radius of each data point.
However, this metric might exacerbate the Matthew Effect, as it tends to result in more facilities being opened in densely populated areas while fewer facilities are opened in sparsely populated areas.
Moreover, existing individually fair clustering methods encounter the same issue: they suffer from prohibitively high computational time. Specifically, the time complexity of[40]is, and[48]is.
To address the running time issue, Chhaya et al.[21]proposed a method to reduce the dataset size by constructing a coreset. However, this approach results in diminished clustering utility and fails to mitigate the inherent dependency of the computational complexity of existing individual fairness clustering on dataset size.

Exponential Tilting.We elucidate the concept of exponential tilting and explore its applications across various disciplines.
Letbe a set of probability distributions with parameter,denote a random variable drawn from the probability distribution, then for any, the information ofunder[24]is defined as

Whenis not specified, we assume thatis a random variable drawn from the distribution. Then the cumulant generating function of[25]is defined as

whereis commonly referred to as an exponential tilting of the information density, and can induce the probability distribution with parametershifting.
Exponential tilting has been applied in numerous fields, such as statistics[16,56,59], applied probability[25], information theory[43,11], and optimization[51,55]. Interested readers can refer to[37]for a more detailed introduction.
Currently, there are relatively few applications of exponential tilting in machine learning[37,63,58].
Li et al.[37]proposed tilted empirical risk minimization (TERM), which allows flexible tuning of individual losses, marking a pioneering move in machine learning.
TERM offers several examples of supervised learning, including linear regression and logistic regression, as illustrated in Fig.3. Recent research has also concentrated on supervised learning, such as the additive model[63]and semantic segmentation[58].

Remarks.
1) Current fairness metrics might exacerbate the Matthew Effect, as they tend to lead to more facilities being opened in densely populated areas while fewer facilities are opened in sparsely populated areas.
2) The efficiency of existing individually fair-clustering algorithms heavily depends on the number of samplesof the dataset.
3) Existing individually fair clustering algorithms cannot flexibly tune the trade-off between utility and fairness. Moreover, these algorithms require cluster centroids to be one of the data points.
4) The current application of exponential tilting is still limited to supervised learning, and it has not been applied in unsupervised learning, especially in clustering.

SECTION: 4Preliminaries

We begin by introducing the definition of-means. Then, we present the well-known-means++ initialization method.

SECTION: 4.1-means

-means is a widely used clustering algorithm designed to partition a dataset intodistinct clusters based on similarities among data points. Letbe a dataset ofpoints,-means aims to find a setofclusters such that the sum of squared error (SSE) is minimized,

whereis a set of centroids,is the centroid of cluster,denotes the square of the Euclidean distance from a data pointto the centroid.
The commonly used method for solving-means is the well-known Lloyd’s heuristic[39], which iteratively computes the assignment of each data point and the centroids through coordinate descent. Next, we provide a detailed description of the optimization process of Lloyd’s heuristic. We begin by presenting the equivalent form of Problem (3) as

wheredenotes the assignment of each data point, for example, if, then, else,represents the assignment of data points in the-th cluster, anddenotes the SSE in the cluster.
To solve Problem (4), one may iteratively assign each point to its nearest centroid and refineusing Lloyd’s heuristic. Following initialization, withholds constant, the solution forcan be obtained as

Whenholds constant, solve foras follows:

whereis an operator to calculate the weighted mean.

SECTION: 4.2-means++

-means++ is an improved version of-means by providing a more effective strategy for selecting initial centroids, thus enhancing the speed and accuracy[8]. We provide the details of-means++ in Algorithm1.
Its process involves selecting the first centroid randomly from the dataset (Step1in Algorithm1). Letbe the shortest distance from a data pointto its closest centroids that we have already chosen. The subsequent centroid is chosen from the data points based on their squared distances to the nearest existing centroids, with a probability(Step1in Algorithm1). This iterative process is repeated untilcentroids are chosen (Step1in Algorithm1).
After selectingcentroids, the subsequent update ofandis performed through coordinate descent, which is identical to Lloyd’s heuristic (Steps1-1in Algorithm1).

SECTION: 5ProposedTKM

In this section, we begin by proposing the objective function of tilted-means (TKM) and presenting the corresponding optimization method. Then, we theoretically analyze the convergence, approximation guarantee, fairness, efficiency, and monotonicity ofTKM.

SECTION: 5.1Objective Function ofTKM

Due to the characteristic of exponential tilting inducing parametric shifts in distributions, we consider incorporating exponential tilting into SSE to obtaintilted SSE.
The objective of tilted-means is to minimize the tilted SSE within each cluster as follows:

whereis a hyperparameter.
Note that whenin (1) is an exponential set of distributions parameterized byand, the cumulant generating function can be written as:

Therefore, it is clear that the objective function ofTKMcan be considered as a properly scaled summation version of the cumulant generating function in Equation (8).

Next, we consider the case ofinTKM.
When, according to L’Hôpital’s rule, it holds that:

Therefore, when, tilted SSE generates to SSE. Without loss of generality, we define

SECTION: 5.2Solving Tilted-means

Since Problem (5.1) involves a highly non-convex objective function with multi-block variables, we consider using coordinate descent (CD) to solve it. We begin by fixingto solve.
Due to the monotonically increasing nature of the objective function with respect to, the solution foris identical to that of Equation (5).
Next, we consider fixingto solve.
Since the tilted SSE is convex with respect to(this property will be proven in Section5.3), we can derive the optimality condition for the tilted SSE with respect to. We then present the first-order gradient ofwith respect toas follows,

whereis the first-order gradient ofwith respect to.
Then setting Equation (11) equal to zero yields the optimal condition of:

We define a tilted mean operator, whererepresents the values ofthat satisfy Equation (12). Note that obtaining the closed solution forfrom Equation (12) is nontrivial, therefore, we employ the first-order gradient method to solve. Letbe a batch data from, thenis updated as follows:

whereis a learning rate, and.
Note that the first-order gradient method is a commonly used optimization method for solving such problems. Interested readers may consider trying second-order gradient methods such asNewton method[49]for solving.

Algorithm Description.The algorithmic process ofTKMcan be summarized into three parts: initialization, assignment, and refinement.
We provide algorithm details forTKMin Algorithm2and an example in Fig.4. Firstly, the centroids setis initialized using-means++ (Line2in Algorithm2). Subsequently, we employ CD to iteratively solve(assignment) and(refinement) (Lines2-2in Algorithm2). We setepochs for solving, where in each epoch, a batchdata is sampled from, and the data points withinare used to solveusing Equation (13).

SECTION: 5.3Theoretical Analysis

Our theoretical analysis consists of five parts. The first part provides an approximation guarantee for the initial centroids obtained by-means++ with respect to the tilted SSE. Then we present a convergence analysis ofTKM. Next, we delve into a fairness analysis ofTKM. In the fourth part, we explore the time complexity ofTKM. Finally, we analyze the monotonicity of the tilted SSE using a simple case.

We begin by providing some definitions and assumptions used throughout our theories.

Given a clusterand a centroid, the tilted weightof a data pointis defined as

Letbe a set of squared Euclidean distances of points into the centroid, then the tilted empirical mean and variance in the clusterare defined as

Note that when, tilted empirical mean and variance generalize to the standard mean and variance in statistics.

The objective functionis continuously differentiable and the gradient function of,
namely,, is gradient Lipschitz continuous with Lipschitz constant, if for any, it holds that

For any, we define the Tilted Hessianas the Hessian ofwith respect to. That is

andis an identity matrix of appropriate size.

For any, the tilted SSE is strongly convex with respect to. That is

Note that the first term in tilted Hessian is positive semi-definite, and the second term is positive definite and lower bounded by, which completes the proof.
∎

For any,is-Lipschitz with respect to, where, anddenotes the largest eigenvalue.

Letdenote the mini-batch gradient of, then the following conditions hold:

There exist scalarssuch that for any,

There exist scalarsandsuch that for any, it holds that

The first requirement in Assumption1states that in expectation, the vectoris a direction of sufficient descent forfromwith a norm comparable to the norm of the gradient. The second requirement in Assumption1, states that the variance ofis restricted, but in a relatively minor manner.

Letrepresent the optimal value of, we aim to prove that-means++ can ensure the resulting initial centroids setsatisfy, whereis a multiplicative error. Next, we mathematically obtain the value of.

Letbe the optimal value of tilted SSE, Letbe the optimal value of SSE, then for any dataset, centroids setinitialized by-means++, and induced clusters, it holds that

The proof of Theorem1can be found in Section8.1.-means++ has been proven to generate initial centroids with a multiplicative error ofin-means whenfairness constraints are not considered[8].
Theorem1demonstrates thatwith individual fairness constraints,-means++ achieves the multiplicative error of.

Next, we provide the convergence analysis ofTKMby proving that the assignment and refinement steps ensure that the expected value of the tilted SSE decreases.

Let,and,be the solutions in the-th and)-th iterations ofTKM. Under Assumption1and by choosing the learning rate, it holds that

The proof of Theorem2is provided in Section8.2.
Theorem2demonstrates that with the selection of an appropriate learning rate, the expected value of the tilted SSE can decrease until reaching convergence.

We propose using the variance of each data point’s squared distance to the centroid within each cluster to measure the fairness of clustering algorithms.
Note that whenin the tilted weight, the tilted empirical variance generalizes to standard variance.
We employ variance as a measure of fairness because it quantifies the extent to which sample points in a dataset are distributed around the mean, with smaller variance indicating reduced fluctuation in distances from the mean and thus greater fairness.
Next, we consider the monotonicity of the tilted empirical variance with.

For any cluster, any corresponding centroid, and any, suppose all data points are normalized to a unit norm, then it holds that

The proof of Theorem3is provided in Section8.3. Note thatis a constant in the calculation of tilted empirical variance, where it contributes to the tilted weight adjustment.
Theorem3states that the-tilted empirical variance among the distances between each data point inand their corresponding centroid will decrease with an increase in.
Therefore, there exists a potential trade-off between SSE and variance, enabling solutions to flexibly achieve desirable clustering utility and fairness.
While Theorem3suppose all data points are normalized to a unit norm which is not satisfied in some datasets, we observe favorable numerical results motivating the extension of these results beyond the cases that are theoretically studied in this paper.

We provide the time complexity ofTKMand analyze whyTKMis suitable for individually fair clustering analysis in big data scenarios.

The time complexity ofTKMis, whereis the number of attributes of each data point,is the epoch size, andis the total number of iterations.

The proof of Theorem4is provided in Section8.4. Note that the time complexity ofTKMis linear with the dataset size, which is the same as that of vanilla-means algorithms without fair constraints such asLloyd’s heuristic[39]and SGD-based-means[52]. In contrast, existing individual fair clustering methods exhibit a time complexity of[40,48]. In the context of big data, employing these methods for clustering becomes impractical, as the required running time becomes difficult to estimate when dealing with dataset sizes reaching the order of millions.
Moreover, these methods encounter RAM overflow issues due to the necessity of computing distances between each data point, requiring storage of anarray in RAM. Conversely,TKMonly necessitates distance calculations between each data point and corresponding centroids during the assignment, thus requiring the computation of only anarray, effectively mitigating the risk of RAM overflow.

In this section, we provided a monotonicity analysis for tilted SSE in a simple case.

When, suppose all data points are normalized to a unit norm, then for any, it holds that,

Proof of Theorem5is provided in Section8.5. When,-means simplifies to a point estimation problem. In this case, Theorem5shows that the tilted SSE increases asincreases.
While the monotonicity of the tilted SSE is restricted to the scenario when, our experiments suggest that the tilted SSE also exhibits a monotonically increasing trend for other values of.

SECTION: 6Experiments

Goals.In this section, we verify the effectiveness and efficiency ofTKMby comparing it with various methods. We also examine the impact of various hyperparameters on the convergence ofTKM. Moreover, we provide visualizations of the centroids’ variations with varying.

SECTION: 6.1Settings

Datasets.We employ ten real-world datasets and two synthetic datasets to validate the performance ofTKM.
To compare the effectiveness and fairness ofTKMwith various methods and parameters, we utilizeAthlete,Bank,Census,Diabetes,Recruitment,Spanish,Student, and3D-spatial. To compare the efficiency ofTKMwith other methods, we employCensus1990andHMDA. For visualizingTKM, we use two synthetic datasets.
We sampled numerical features from ten real-world datasets and then standardized these features (the names of these features are provided in our repository).
A comprehensive overview of the datasets can be obtained from TableII.

Baselines.We experimentally evaluate the performance ofTKMagainst six methods, namely,-means++[8],JKL[35],MV[40],FR[48],SFR[48], andNF[52].
As explained in our related works,JKLfirst introduced the concept of individual fairness for-means.MV,FRandSFRare three state-of-the-art methods for individually fair-means. Note thatSFRis a sparsed version ofFR.-means++andNFare two clustering methods that do not take individual fairness into account. It is worth noting thatNFis a method different from the classicalLloyd’s heuristic. It is solved through SGD and can be considered as the case ofinTKM.
ForTKMandNF, we employed-means++for initialization.

Measurements.We employ several metrics to evaluate the performance of clustering algorithms.
We useSSEto measure the utility of different clustering algorithms, where a smaller SSE is considered a better clustering utility.
To measure fairness among different clustering algorithms, we consider using two metrics. The first is thevarianceof each point’s distance to its nearest centroid within a cluster. A smaller variance indicates a fairer algorithm. The second metric is themaximum distancefrom each point in a cluster to the centroid, where a smaller maximum distance signifies greater fairness.
As for efficiency evaluation, we measure it using therunning timeof each algorithm.
To verify the impact of different hyperparameters on the convergence ofTKM, we usetilted SSEas the metric.

Implementations.Our algorithms were executed on a platform comprising an Intel i9-14900KF CPU with 24 cores, 64 GB of RAM, and operating on the CentOS 7 environment.
The software implementations, including our methods and the comparison methods, were realized in Python 3.7 and open-sourced (https://github.com/zsk66/TKM-master).

SECTION: 6.2Comparison among Various Methods

Fig.7compares the SSE of six methods asvaries on eight datasets:Athlete,Bank,Census,Diabetes,Recruitment,Spanish,Student, and3D-spatial. Due to the long running time required by our comparison methods, we need to sample the datasets to accommodate them. We sampled 1000 data points from each dataset, repeated this process 10 times, conducted experiments on the resulting 10 sampled datasets, and averaged the obtained SSE values. We set the parameterofTKMto be 0.01, 0.05, 0.1, and 0.2, respectively. The learning rate forNFandTKMwas set to 0.05, the number of epochs was set to 5, the batch size was set to 100, and the number of iterations was set to 500.JKL,MV,FR, andSFRadopted the default hyperparameter settings in their papers.

Observations.
We can see that asincreases, the SSE ofTKMalso increases. This is because an increase ininevitably brings the centroids closer to the minority data points, resulting in an increase in SSE.
Comparing the SSE of different methods, we can observe that the SSE ofJKLis consistently the highest across all datasets except forBankandSpanish.
In these two datasets,TKMhas a large SSE at, which is due to excessively largecausing the centroids obtained byTKMto be too close to those minority data points.
The SSE ofSFRis always larger thanFRbecauseSFRis a version ofFRthat applies the sparsification technique.
The SSE for3D-spatialandRecruitmentinFRis lower than inMV, but on the other six datasets,MVhas a lower SSE compared toFR.
Meanwhile,TKM’s SSE atis consistently lower thanJKL,MV,FR, andSFR, and even performs nearly as well as-means++andNFon theCensusandRecruitment, which reflects the outstanding effectiveness ofTKM.

Fig.7and Fig.7illustrate the variance and maximum distance within each cluster for various methods when.
The variance and maximum distance values within Clusters 1-4 are arranged in descending order.
The data processing and hyperparameter configurations for all methods remain consistent with those outlined in Section6.2.1.

Observations.
From Fig.7, it can be seen that forTKM, asincreases, the variance of each cluster decreases, which is consistent with our theoretical results. Next, without loss of generality, we examine the variance of each method on Cluster 1. It can be observed thatJKLhas the largest variance across all datasets except forBank,Recruitment, andSpanish, while-means++andNFhave the largest variance onBankandSpanish, andSFRhas the largest variance onRecruitment. It is worth noting that in some datasets, such asDiabetes,Recruitment,Student, and3D-spatial, even when, the variance ofTKMis smaller than other comparison methods. Moreover, in other datasets, by adjusting, it is always possible to make the variance ofTKMsmaller than the comparison methods.
From Fig.7, we observe that the maximum distance within each cluster decreases asincreases.
This occurs because the greater maximum distance is caused by the centroids being farther from the minority points. With a higher, the centroids shift towards the minority points, thereby reducing the maximum distance.
ComparingTKMwith other methods reveals thatTKMachieves the smallest maximum distance, demonstrating its fairness.
Moreover, we observe that in3D-spatial, the variance and maximum distance ofJKL,MV,FR, andSFRare all larger than those of-means++, indicating that existing individually fair clustering methods might even exacerbate unfairness in our scenario.

TableIIIpresents a comparison of the running time ofTKMwith three state-of-the-art methods,MV,FR, andSFR(
Due to the poor performance ofJKLandNFin effectiveness and fairness, we do not consider these two methods in the comparison of efficiency). We sampled theCensus1990andHMDAwith sizesof 1K, 2K, 5K, 10K,15K, 20K, 25K, 30K, 40K, 50K, 60K, 70K, 80K, 90K, 2M, and 5M, respectively. We set the number of iterations forTKMto 500, the batch size to, the number of epochs to 5, and the learning rate to 0.05. The hyperparameters forMV,FR, andSFRwere set to their default values in their papers.

Observations. Experimental results demonstrate that regardless of the number of data points sampled, the running time ofTKMis always significantly shorter than that ofMV,FR, andSFR.
It can be observed thatTKMcan cluster 5 million data points in about 30 minutes, whileMVcan only cluster 20,000 samples within 30 minutes, andFRcan only cluster 5,000 data points. Moreover, it is worth noting that as the sample size increases,TKM’s running time increases by hundreds or even thousands of times compared toMVandFR. For example, when the number of sampled points is 1,000,TKMachieves 8.0and 19.1acceleration compared toMVandFRinCensus1990, respectively. When the number of sampled points is 10,000,TKMachieves 62.7and 2673.2acceleration compared toMVandFRinCensus1990, respectively. Furthermore, although the running time ofSFRis significantly shorter thanMVandFR,TKMstill achieves approximately a 10.7and 12.1acceleration with 80,000 data points inCensus1990andHMDA, respectively. Furthermore, when the sample size reaches 90,000, the algorithmic characteristic ofSFR, which requires computing distances between each sample point, can lead to a memory overflow issue, causing the algorithm to terminate. This issue also arises inMVandFR.

We have provided the changes of SSE with respect tofor different methods, the variance results in four clusters for different methods, and a comparison of the efficiency of different methods. Our experimental results have led us to draw the following conclusions:

TKMoutperforms state-of-the-art methods in terms of effectiveness. Specifically,TKMachieves smaller SSE compared to state-of-the-art methods across different values ofand. In some datasets, the SSE ofTKMis almost the same as methods that do not consider individual fairness.

TKMoutperforms state-of-the-art methods in terms of fairness. Specifically,TKMcan achieve smaller variance and maximum distance than state-of-the-art methods when an appropriate value ofis chosen.

TKMsurpasses state-of-the-art methods in terms of efficiency. Specifically,TKMcan cluster more data points in a shorter time, and as the sample size increases, this acceleration effect becomes even more pronounced.
Moreover,TKMcan overcome the RAM overflow issue that existing methods encounter when dealing with large-scale data.

SECTION: 6.3Comparison among Various Parameters

Fig.10illustrates the convergence of tilted SSE with iterations atvalues of 0.01, 0.05, 0.1, 0.2, 0.5, and 1.
We randomly select 1000 data points from each dataset, repeating this process 10 times. We then conduct experiments on these 10 subsampled datasets, calculating the average of the resulting tilted SSE values.
For other hyperparameters, we set the learning rate to 0.05, the number of iterations to 500, the batch size to 100, and the epoch size to 5.

Observations.
We observe that despite using SGD to update the centroids, the tilted SSE ofTKMstill decreases steadily with iterations, which confirms the convergence ofTKM.
Asincreases, the tilted SSE also increases. This confirms that our theoretical analysis of the monotonicity of the tilted SSE with respect toholds not only for.
When, the tilted SSE remains nearly unchanged with iterations. This indicates that the tilted SSE is insensitive to variations inwhenis small.

Fig.10illustrates the convergence of tilted SSE with different numbers of epochs during iterations.
The data preprocessing forTKMhere follows the same procedures outlined in Section6.3.1.
To visualize the curve of tilted SSE ofTKMover iterations more intuitively, we set, learning rate to 0.03, number of iterations to 500, batch size to 50, and epoch size to 1, 3, 5, 7, 9.

Observations.
From Fig.10, it can be observed that as the number of iterations increases, the tilted SSE ofTKMdecreases and tends to stabilize after reaching a certain value on all datasets.
With an increase in the epoch size, the convergence speed ofTKMaccelerates, and its convergence performance improves. This is because increasing the epoch size allows for higher precision in the solution obtained through SGD during each iteration, as more data can be utilized.
When the epoch size is 7 and 9, the convergence and convergence speed ofTKMare not significantly different. Therefore, selecting 7 as the epoch size is an appropriate choice.
However, in some datasets, we found that increasing the epoch size does not necessarily improve convergence. For example, inRecruitment, a smaller epoch size of 7 yields better convergence compared to an epoch size of 9. This is attributed to the risk of overfitting when the epoch size is too large. Therefore, choosing an epoch size of 7 is deemed appropriate for these datasets.

Fig.10illustrates the convergence of tilted SSE with various learning rates during iterations. The data preprocessing forTKMhere is the same as in Section6.3.1.
For the parameter settings ofTKM, we set, epoch size as 5, batch size as 50, number of iterations to 500, and learning rate as 0.01, 0.02, 0.03, 0.04, and 0.05.

Observations.
From Fig.10, we can see that, across the eight datasets, the convergence speed generally increases with the increase in learning rate. However, when the learning rate increases to a certain extent, the increase in convergence speed becomes slower. For example, when, the convergence speed and the converged tilted SSE value on theBankare almost indistinguishable. Additionally, if the learning rate is excessively high, it can result in poorer convergence, as demonstrated inDiabeteswhereproduces a smaller tilted SSE. This occurs because an overly large learning rate may cause the SGD step size to become excessive, hindering the achievement of locally optimal solutions.

Fig.11demonstrates how centroids change overin two synthetic datasets when the number of clusters is set to 2 and 3, respectively.
We set the number of epochs forTKMto 5, the number of iterations to 1000, and the learning rate to 0.01, the batch size to 20.
For the values of, we take a total of 60 geometrically spaced values betweenand.
We employ a blue-to-red gradient to depict the rising values of, and we use the same color to represent data points within the same cluster.

Observations.
It can be observed that asincreases, the positions of the centroids tend to shift towards the minority data points in each cluster.
This ensures data points in each cluster can guarantee “treat all points equally”, aligning with the concept of individual fairness.
Furthermore, we observe that asincreases, the centroids do not shift excessively towards minority data points, ensuring that the distance from majority data points to the centroids remains reasonable. This demonstrates thatTKMensures equal treatment of each data point.

We have provided the convergence behavior ofTKMunder different epoch sizes and learning rates, as well as visualizations ofTKMon 2-dimension synthetic data. These experiments lead us to the following conclusions:

TKMis a convergent algorithm, and the tilted SSE increases monotonically with. Specifically, for different values of, the tilted SSE inTKMsteadily decreases to a stable value. Moreover, asincreases, the tilted SSE increases.

The convergence ofTKMis influenced by the epoch size and learning rate. Specifically, selecting an appropriate epoch size and learning rate can lead to faster convergence speed and better convergence ofTKM. However, choosing larger epoch sizes and learning rates does not necessarily improve the performance.

TKMindeed can ensure individual fairness for-means. Specifically, asincreases, it can guarantee that those minority data points can be closer to the centroids, achieving the goal of treating each individual equally.

SECTION: 7Conclusions and Future Work

This paper investigated the individually fair-means in the context of location-based resource allocation.
To address the issue where existing individually fair clustering methods and fairness metrics may exacerbate unfairness, we proposedTKM, an algorithm designed to effectively solve the individually fair-means problem via exponential tilting.
We constructed the tilted SSE as the objective function and proposed solving the optimization problem using CD and SGD. Moreover, we proposed to employ variance to measure fairness.
Our theory and experiments have validated that the effectiveness, efficiency, and fairness of our proposed algorithm outperform existing state-of-the-art methods.
It is noteworthy that existing individually fair clustering methods encounter challenges in their application to large-scale data clustering analysis scenarios, primarily due to their computational complexity, which depends on the dataset size. In contrast,TKM, due to its excellent efficiency performance, can be applied in many big data clustering analysis scenarios, such as resource allocation.

Due to privacy concerns, data is often stored on different devices and cannot be shared among them. Therefore, a hot topic of research is how to perform clustering analysis without sharing data. In the future, we will investigate individually fair-means in the framework of federated learning to address this issue.

SECTION: 8Proofs

SECTION: 8.1Proof of Theorem1

Before proving Theorem1, we present some useful lemmas.

Given a cluster, letandbe the corresponding assignment and centroid, then for any, it holds that,

Following from (5.1), we have

where (24) follows from the Jensen’s inequality.
∎

Given a set of clustersand a set of centroids, let, then for any, there exists a scalar, such that the following inequality holds:

Consider the case when, according to L’Hôpital’s rule, it holds that,

which implies that for any,is bounded. Then there must exist a scalarsuch that

which completes the proof.
∎

Letbe the optimal solution of SSE, letbe the optimal solutions of tilted SSE, and let,be the corresponding optimal objective function values, then for any, we have.

Based on Lemma3and optimal conditions, we obtain

Summing over (31) from 1 toimplies Proposition1.
∎

Letbe the optimal SSE of-means, letbe the centroids set constructed by-means++, and letbe the corresponding induced assignment, then for any set of data points, it holds that.

Next, we are ready to prove Theorem1based on the above lemmas.

Letbe the centroids set constructed by-means++, and letbe the corresponding induced set of clusters, then following from Lemma4, we have

Then we can boundas

where (33) follows from Lemma5, and (34) follows from Proposition1. According to Lemma4, we have, then we can derive that, which completes the proof.
∎

SECTION: 8.2Proof of Theorem2

By the Mean Value Theorem, the gradient Lipschitz continuity indicates the following proposition.

For any, and, it holds that

Following Lemma2, it holds that

which completes the proof.
∎

Next, we show the proof of Theorem2.

We consider proving the decreasing property ofTKMfrom two parts: refinement and assignment. Our proof with respect to the refinement follows from[13]which establishes the convergence for gradient Lipschitz continuous objective functions.
Under the gradient Lipschitz continuous property ofwith respect to, the iterations of SGD satisfy the following inequality by applying Proposition2:

According to Cauchy-Schwarz inequality and Assumption1, it holds that

Next, we consider boundingunder Assumption1as follows,

where. Then by applying Assumption1and (8.2) into (8.2), we obtain

To ensure that the objective function value decreases within refinement, we need,
which implies. Next, we consider proving the decreasing property in the assignment. Following the optimal condition with, the following inequality holds

Combining (8.2) and (39) yields

Summing over (40) fromtoproves Theorem2.
∎

SECTION: 8.3Proof of Theorem3

We begin by defining the tilted weight, tilted empirical mean, and tilted empirical variance when all data points are normalized to a unit norm.

Suppose the dataset is normalized, then the tilted weight is defined as

where.

Suppose the dataset is normalized, the tilted empirical mean and variance in each cluster are defined as

where, and

For any, and any, it holds that

Letbe the solution of (5.1), then substituting,andinto the tilted weight denoted as, we can obtain the tilted empirical mean and variance for each cluster as

whereandare constants. Then, by taking derivative ofwith respect to, we have

Based on the optimal condition with, we have

Divide both sides of (46) by, and differentiate with respect toyields

where (47) follows from the chain rule, and (48) follows from Lemma6.
Then we can infer from (48) that

Substituting (49) into (8.3), we obtain

which completes the proof.
∎

SECTION: 8.4Proof of Theorem4

When initializing the centroids with-means++, the required number of multiplications is.
The number of multiplication needed for assignment and refinement areand, respectively.
When we set the number of iterations to, we can obtain the multiplication required forTKMis.
∎

SECTION: 8.5Proof of Theorem5

When, we obtain

whereandare the unique cluster and centroid.
We directly take the partial derivative ofwith respect to, yielding:

where (51) follows from the fact that all data points are normalized, and (52) defines. Next, we consider

where (8.5) follows from (41), (43) and the chain rule.
Given thatfor any, thereforeis a monotonically increasing function with, and its minimum value is attained at. When, we have

where (8.5) follows from (41), (43) and L’Hôpital’s rule.
Then we obtain, and consequently inferfor any. In conjunction with Equation (52), Theorem5is implied.
∎

SECTION: References