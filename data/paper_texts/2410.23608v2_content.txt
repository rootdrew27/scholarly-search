SECTION: Context-Aware Token Selection and Packing for Enhanced Vision Transformer
In recent years, the long-range attention mechanism of vision transformers has driven significant performance breakthroughs across various computer vision tasks. However, the traditional self-attention mechanism, which processes both informative and non-informative tokens, suffers from inefficiency and inaccuracies.
While sparse attention mechanisms have been introduced to mitigate these issues by pruning tokens involved in attention, they often lack context-awareness and intelligence. These mechanisms frequently apply a uniform token selection strategy across different inputs for batch training or optimize efficiency only for the inference stage.
To overcome these challenges, we propose a novel algorithm: Select and Pack Attention (SPA). SPA dynamically selects informative tokens using a low-cost gating layer supervised by selection labels and packs these tokens into new batches, enabling a variable number of tokens to be used in parallelized GPU batch training and inference. Extensive experiments across diverse datasets and computer vision tasks demonstrate that SPA delivers superior performance and efficiency, including a 0.6 mAP improvement in object detection and a 16.4% reduction in computational costs.

SECTION: Introduction
Recent advancements in computer vision tasks such as image classification, segmentation, and object detection have seen Vision Transformers (ViTs) surpass traditional convolutional approachesdue to their powerful self-attention mechanisms. ViTs are particularly effective at capturing long-range dependencies, enabling the learning of global features that are crucial for complex visual understanding. However, this strength comes with a significant drawback: the computational overhead increases quadratically with the number of tokens, leading to excessive and often unnecessary computations among irrelevant tokens. This not only raises the computational burden but also risks degrading performance by incorporating extraneous, often redundant, information in typical computer vision tasks.
As illustrated in, the use of self-attention in ViTs inadvertently processes a large amount of superfluous data, exacerbating computational inefficiency and potentially degrading task performance by introducing irrelevant information into the model’s learning process. This issue is particularly severe when dealing with sparse data, where most pixels are not informative.

Numerous approaches have been proposed to address this issue by performing self-attention only on the most informative tokens. However, these methods still encounter significant challenges in both efficiency and performance.

Efficiency: The constraints of GPU batch training, where images within a batch often contain non-uniform numbers of informative tokens, pose challenges to parallelizing computation effectively. Some methods, such as SparseViT, address this by padding all effective tokens to match the maximum number in the batch, leading to inefficiencies, as illustrated in. Other approaches, like DynamicViTand EViT, reduce computation only during inference by discarding a fixed number of tokens. However, these methods still attend to all tokens during training, employing an attention mask to focus on informative tokens, which, along with the mask prediction module, can result in training costs that exceed those of a standard ViT. The Deformable Attention Transformer (DAT), inspired by Deformable Convolutional Networks (DCN), merely reduces the receptive field of query tokens while still computing all tokens, yielding minimal improvements in computational efficiency.

Performance: Existing methods demonstrate effectiveness primarily in simpler tasks like image classification, where some degree of information loss is tolerable. However, their performance degrades in more complex tasks, such as object detection, which demand richer semantic information. For example, DynamicSwin, a Swin-based DynamicViT, struggles in these scenarios due to inaccurate token selection, leading to significant information loss.

To address these challenges, we propose a novel Select and Pack Attention (SPA) mechanism that dynamically selects varying numbers of informative tokens from input batches, supervised by selection labels, and packs them into new batches for parallelized training. Specifically, we introduce a linear gating layer to generate scores for token selection, supervised by a multi-scale selection label derived from object labels (e.g., bounding boxes, instance segmentation labels). After selection, the chosen tokens are placed into uniform-sized package containers to form new batches, as illustrated in. For attention computation within each container, tokens attend only to those from the same original image, ignoring tokens from other images by using attention masks.
Additionally, SPA can be effectively integrated with the window-based attention proposed by Swin Transformer, benefiting from the window shifting operation that captures information across windows. To prevent information loss across package containers, we shift the feature maps every two transformer blocks, ensuring that token pairs placed into containers vary, allowing the attention computation to encompass all tokens.
Based on SPA, we propose a backbone network, Select and Pack Transformer (SPT), featuring a hierarchical architecture to generate image representations at various scales for downstream computer vision tasks. Similar to DAT, to avoid mis-selection at the early stage which may cause serious information loss, we leverage our SPA from the third stage with the adapted image features.
Ultimately, SPA addresses the efficiency issue by selecting only informative tokens and packing them into new batches, enabling efficient parallelized computation for both training and inference. Moreover, by leveraging selection label supervision, SPA improves performance in complex computer vision tasks, such as object detection.
Comprehensive experiments on four well-known datasets demonstrate the efficacy of SPA across multiple computer vision tasks.

To summarize, our main contributions are as follows:

We propose a novel sparse attention mechanism, Select and Pack Attention (SPA), to enhance both the efficiency and performance of Vision Transformers. For efficiency, SPA dynamically selects informative tokens from images in a batch using a linear gating layer and packs them together to enable efficient GPU batch training and inference. For performance, we introduce a multi-scale selection label to explicitly supervise token selection, thereby outperforming existing methods even in complex computer vision tasks.

By effectively integrating our SPA mechanism with Swin blocks, which use a window shifting trick to capture information across packages, we propose a backbone network with a hierarchical structure called the Select and Pack Transformer (SPT). SPT can generate features at various scales, making it suitable for many computer vision tasks.

Through extensive experiments on four diverse datasets, we demonstrate the superior performance of our Select and Pack Transformer (SPT) across a range of computer vision tasks. SPT consistently outperforms state-of-the-art methods with a 0.6 mAP improvement in object detection, a 0.24 mAP increase in multi-label classification, a 7.05 boost in top-1 accuracy for image classification, and a 16.4% reduction in computation cost.

SECTION: Related Work
SECTION: Transformer in Computer Vision
Given the remarkable success of transformers in natural language processing (NLP), this architectural paradigm is progressively permeating diverse computer vision tasks. For instance, Vision Transformer (ViT) divides input images intopatches, which are subsequently treated as tokens for the application of the attention mechanism.
In image segmentation, SAMintroduces a prompt-based algorithm, setting new benchmarks across state-of-the-art methods. For object detection (OD), DETR conceptualizes it as a direct set prediction problem and designs a transformer-based network. DINO advances self-supervised learning to propose a novel network rooted in the ViT architecture. Additionally, in the domain of super-resolution, transformers such as SwinIR have demonstrated exceptional capability in capturing long-range dependencies for improved visual representations.

SECTION: Efficient Transformers
Despite their advantages in global feature extraction via self-attention across all tokens, Vision Transformers (ViTs) are hindered by significant computational overhead. This overhead primarily arises because the computation of attention weights scales quadratically with the number of tokens. To address this challenge, two main approaches have been proposed: pruning the number of tokens for sparse attention or developing mechanisms with linear complexity.
For sparse attention, Swin Transformerintroduces window-based and shifted window-based self-attention mechanisms, significantly reducing computational demands within localized windows. MAEemploys random masking to decrease token computation. Sparse Transformerproposes two new attention mechanisms to limit the number of tokens in each attention computation. Besides these data-agnostic sparse attention methods, DynamicViTproposes a dynamic token sparsification framework to prune redundant tokens progressively and dynamically based on the input, SparseViToptimizes computation by selecting tokens based on thenorm of window activations, prioritizing features with higher scores. Inspired by Deformable Convolutional Networks (DCNs), DATemploys an offset network to refine the query token’s receptive field, further enhancing computational efficiency. Our Select and Pack Attention (SPA) mechanism also belongs to this category.
For linear transformers, Transformer-VQachieves efficient attention with linear complexity using vector-quantized keys and a novel caching mechanism. FLASH attentionproposes a new transformer with linear time complexity, utilizing Gated Linear Units (GLUs).

SECTION: Methodology
SECTION: Overall Architecture
As illustrated in, our Select and Pack Transformer (SPT) features a hierarchical structure composed of four stages. Each stage generates image representations of varying sizes, resulting in a total of four different scales of representation.

Specifically, suppose we consider a smallpatch as a single token, the input image(andare the input image height and width), are progressively embedded into representations,,,(is the embedding dimension of the first patch embedding layer) stage by stage. Each stage is structured around an embedding block for feature map downsampling, followed bytransformer blocks tasked with feature learning (signifies the block count in theth stage).
Similar to Swin, the embedding block of the first stageemploys a convolution layer while the subsequent embedding block consists of a patch merging layer that concatenates features as groups ofpatches and a linear layer for feature projection.
For the transformer blocks, the first two stages,utilize standard Swin Transformer blocks, whereas the latter two stages,incorporate our Select and Pack Attention (SPA) block. This design decision is informed by observations from DAT, which noted that early-stage transformer block replacement diminishes accuracy due to the model’s inability to efficiently distinguish positive tokens based on shallow features.
Our SPA blocks in the second and third stages not only generate outputs for subsequent layers but also transfer the score map to the next stage for the multi-scale supervisionandfor computing select loss. With the selection mapgenerated in the last stage, there are a total of three different scales. The complete process is as follows:

where,,, andare output representations of four stages.,,, anddenote the four stage models. And,anddenote the predicted score map for selection from the last three stages, separately.is the gating layer to generate scores for the output of stage 2.

SECTION: Select and Pack Attention (SPA)
Inspired by the gated networks in Mixture of Experts (MoE)and heterogeneous federated learning, which adeptly guide models in selecting appropriate computational paths and enhancing task-specific generalization, we design a Select and Pack (SnP) block. This block utilizes a linear gating layer to select informative tokens and pack them into fixed-size package containers, generating new batches for GPU training or inference. While positive tokens undergo multi-head self-attention (MSA), negative tokens are directly passed to the feedforward network, as illustrated in.

Although token selection can be implicitly guided by the final objective, our experiments reveal that the gating layer tends to assign large values to all tokens, leading to the selection of too many tokens and reduced efficiency. To address this, we introduce a selection label based on object labels, which directly indicate areas of interest, such as instance segmentation masks or object detection bounding boxes. For segmentation, a binary mask assigns a value of 1 to all object pixels and 0 otherwise. For object detection, an aggregated binary mask is formed by stacking all bounding boxes.
However, a single-scale label overly restricts token selection, causing significant information loss and poor performance. To mitigate this, we reduce the Gumbel-Softmax function’s threshold and integrate multi-scale select labels. As shown in, each SPA block in SPT not only uses the selection scale matching the representation but also incorporates scores from up-scaled features, adjusted via max-pooling to match the correct feature size. This approach selects the maximum scores from two scales to include more informative tokens, thereby enhancing performance.

Specifically, given flattened input batch(,andare the batch size, the length of each image representation and the number of channels, separately), the gateassigns scoresto each token. Then, we element-wise multiply the normalized scores by a sigmoid layer with the input representations to obtain the gated representations. After that, we leverage the Gumbel-Softmax functionto separate positive tokens (i.e.informative tokens),(is the number of positive tokens from all images in the batch).
The procedure unfolds as follows:

where,,,,denotes the input representation, scores for up-scale features, scores for this scale, gated representation, and output positive tokens, separately. Andis element-wise multiplication with boardcasting.is the linear gating layer.

After the dynamic selection for each input image, the lengths of selected tokens vary. To avoid padding all tokens to the maximum length, which would introduce significant computational overhead, we pack the selected tokens into new batches. Inspired by, we set a series of package containers with a fixed lengthand fill them with the selected tokens.
After packing all selected tokens, if the total number of tokens is not a multiple of the packing length, we only pad the last package. This approach is significantly more efficient than padding the selected tokens for all images in the batch. Consequently, we obtain packed tokens,(is the batch size of packed tokens), and the number of tokens is much smaller than the original input, especially for sparse data. And the attention computation depends on, similar to the window sizeof Swin. And we setto be. Specifically, for input representation batch, the complexity of regular multi-head self-attention (MSA), window-based multi-head self-attention (W-MSA), and our SPA are as follows:

Compared to MSA, W-MSA is more efficient since the complexity is linear to the original token length. However, our SPA is not only linear to, the new batch sizeis also much smaller than, resulting in higher efficiency.
Additionally, for the self-attention of the packed tokens, we employ an attention mask to ensure that all tokens attend only to tokens from the same image, as illustrated in.

SECTION: Loss Function
The loss function of our SPT Transformer comprises the loss for the target task and the selection loss. For the selection loss (Details in Appendix A), we adopt binary cross-entropy and sum over all SPA blocks as follows,

whereis the normalized score map by Sigmoid layer, andis the ground truth label.

The loss function of our SPT is, whereis hyperparameter to adjust the weights of losses.
And we summarize our algorithm in Appendix B.

SECTION: Experimental Results
SECTION: Data and Experimental Setup
To demonstrate the efficacy of our SPT Transformer in complex computer vision tasks, we primarily conducted experiments on object detection, using the BDD100K and COCO2017 datasets.
Beyond these standard datasets, we explored our method on sparse data, where token selection is more challenging, further validating the effectiveness of our approach.
We generated sparse datasets by selecting images with low object ratios.
Specifically, we selected images from COCO2017 with object pixel ratios smaller than 20%, creating the COCO-S dataset. Similarly, we selected images from BDD100K with object ratios smaller than 25%, resulting in the BDD-S dataset.
Additionally, to further showcase the robustness of our SPT Transformer, we extended our experiments to a range of simpler computer vision tasks.
We evaluated multi-label classification using the PASCAL VOC 2012 dataset, selecting images with object pixel ratios smaller than 25% to create VOC-S. For image classification, since datasets for this task are typically dense, we instead padded the original images with black pixels to make them sparse. Specifically, we selected the Tiny ImageNet subset of ImageNet-1K, which contains 100,000 training images and 10,000 validation images, and provides object labels for selection supervision. We padded black background pixels to the original images to make them sparse, called IN-S. After thepadding, the original images were positioned in the upper-left corner, while the remaining 75% of the area was filled with black pixels.

To evaluate OD, we utilize the Mask RCNN framework, and replace the backbone network with our SPT or other baselines. We adopt the default training settings, such as 36 max training epochs, batch size of 2. In addition, we set the threshold of Gumbel-Softmax to 0.01, and set the select loss weightto 0.01.
Experiments were performed on two Linux servers, each outfitted with dual NVIDIA L40S GPUs.

For object detection and multi-label classification, we employ mAP as the evaluation metric. And for image classification, we adopt Top-1 accuracy. In addition, we evaluate the select ratios for each SPA block, which is the number of selected tokens over the total number.

SECTION: SPT for Object Detection
In, we compare our SPT with other baselines for the tiny, small and base configurations of the Swin Transformer (i.e., Swin-T, Swin-S and Swin-B) on BDD100K.
The GFLOPs are computed over backbone, FPN and detection head with RGB input image at the resolution offor training stage. For a clearer comparison, we evaluate the throughput (i.e., FPS) only over the backbone network on a machine with an NVIDIA L40S GPU, as including other components would result in values that are too small. Under the same settings, our approach achieves the best performance with the lowest computation cost across all three configurations. Specifically, our SPT-B model improves object detection results on BDD100K from 22.5, achieved by DynamicSwin—an existing state-of-the-art sparse attention method—to 23.1, while reducing computation by 16.4% for both training and inference.

When dealing with the more challenging BDD-S dataset, as shown in, the GFLOPs were reduced from 272 to 251 for SPT-T and from 363 to 320 for SPT-S, representing reductions of 7.72% and 11.8%, respectively. These models also achieved performance improvements of 19.1% and 9.6%, respectively. In experiments on COCO-S, our SPT also outperformed other methods while requiring less computation in. These results collectively demonstrate the superiority of our SPA in accurately selecting informative tokens under the supervision of multi-scale selection labels.
Additional results can be found in Appendix C.

SECTION: SPT for Other Computer Vision Tasks
In addition to the complex object detection task, we also evaluated our SPT on simpler tasks, including multi-label classification and image classification.

For multi-label classification on VOC-S, Swin outperforms ViT, improving the mAP from 43.24 to 44.36. However, with our SPA, performance is further improved to 44.6 with a much lower computational cost. In, we show the mean of select ratios for SPA blocks (Detailed ratios are included in Appendix D). Overall, SPT reduces the GFLOPs for VOC-S by 10.2%.

On the original Tiny ImageNet dataset (Tiny IN-1K), as shown in, the high selection ratios indicate minimal efficiency improvement, as this dataset is very dense. However, we observed an increase in Top-1 accuracy from 36.12 to 43.17, further demonstrating the effectiveness of our proposed attention mechanism in focusing on informative tokens.

For the more challenging IN-S dataset, our SPA selects approximately 23% of the tokens for attention computation, aligning with the ground truth object pixel ratio. This approach not only improves Top-1 accuracy but also achieves a 10.5% reduction in computation by disregarding background information, as shown in.

SECTION: Ablation Study
To illustrate the effectiveness of informative token selection, we designed experiments where all informative tokens were selected based on ground truth (GT) selection. As illustrated in, for both plain ViT and window-based attention mechanisms, selecting tokens to disregard background information improves both accuracy and efficiency, as confirmed across two different tasks.

Even though we know that token selection works, a critical challenge is how to correctly select these informative tokens without ground truth labels. As discussed earlier, previous methods (e.g.SparseViT) commonly adopt uniform token selection, applying a fixed ratio for all images in each batch. However, the results indemonstrate that our SPA with dynamic selection performs better. Additionally,provides both numeric and visual comparisons to illustrate the effectiveness of our proposed multi-scale select label.

explore the optimal number of SPA blocks in SPT. The results match with the findings in. Starting from the third stage yields the best performance. Early-stage selection leads to information loss, resulting in worse performance.

SECTION: Conclusion
In this paper, we analyze the current issues with sparse attention mechanisms and propose a novel Select and Pack (SPA) mechanism to address these challenges for both efficiency and performance. Our SPA focuses attention computations solely on informative tokens using a supervised gating block in Vision Transformers. This mechanism packs the selected tokens for parallelized GPU batch training and inference. Integrated into the Swin Transformer’s hierarchical architecture, SPA forms the efficient Select and Pack Transformer (SPT), which works as image backbone network for various computer vision tasks and generates multi-scale representations. To enhance selection accuracy and ensure effectiveness in complex computer vision tasks, we employ multi-scale selection labels for explicit supervision using object labels. Extensive experiments across four datasets and a range of vision tasks validate the effectiveness of SPT. For object detection, SPT achieves a 0.6 mAP improvement and a 16.4% reduction in computational cost compared to state-of-the-art sparse attention mechanisms. Additionally, SPT outperforms baselines in other computer vision tasks, with a 0.24 mAP improvement in multi-label classification and a 7.05 increase in top-1 accuracy for image classification.

SECTION: References