SECTION: Less Discriminatory Alternative and Interpretable XGBoost Framework for Binary Classification

Fair lending practices and model interpretability are crucial concerns in the financial industry, especially given the increasing use of complex machine learning models. In response to the Consumer Financial Protection Bureau’s (CFPB) requirement to protect consumers against unlawful discrimination, we introduce LDA-XGB1, a novel less discriminatory alternative (LDA) machine learning model for fair and interpretable binary classification.
LDA-XGB1 is developed through biobjective optimization that balances accuracy and fairness, with both objectives formulated using binning and information value. It leverages the predictive power and computational efficiency of XGBoost while ensuring inherent model interpretability, including the enforcement of monotonic constraints.
We evaluate LDA-XGB1 on two datasets: SimuCredit, a simulated credit approval dataset, and COMPAS, a real-world recidivism prediction dataset. Our results demonstrate that LDA-XGB1 achieves an effective balance between predictive accuracy, fairness, and interpretability, often outperforming traditional fair lending models. This approach equips financial institutions with a powerful tool to meet regulatory requirements for fair lending while maintaining the advantages of advanced machine learning techniques.Keywords: Fairness, Interpretability, Binning, Information value, Less discriminatory alternative, Mixed-integer programming, Monotonicity, XGBoost.

SECTION: 1Introduction

The financial industry increasingly relies on complex machine learning models for critical decision-making processes, such as credit scoring, loan approvals, and risk assessments. Though these models substantially improve predictive accuracy and efficiency, they also bring two major concerns: fairness and interpretability.

To address these concerns, the Consumer Financial Protection Bureau (CFPB) has enacted stringent regulations that require lenders to uphold fair lending practices and provide clear explanations for their decisions. On the fairness front, the CFPB mandates that lenders actively search for and adopt less discriminatory alternatives (LDAs). This ensures that models do not unfairly disadvantage protected groups in decisions like credit approval. Simultaneously, the adverse action reason code (AARC) requirement emphasizes model interpretability. Lenders must provide clear, specific, and understandable reasons for rejecting loan applications. This is crucial not only for transparency but also to meet legal obligations and maintain customer trust.

The dual mandates of fairness and interpretability present a formidable challenge for financial institutions. Traditionally, machine learning models have been favored for their high accuracy and efficiency in solving binary classification problems. However, their use in regulated industries has raised concerns due to the complexity and potential for bias in decision-making. As a result, there is a pressing need for new approaches that strike a balance between accuracy, fairness, and interpretability.

In response to this challenge, we propose LDA-XGB1, a novel framework designed to meet both fairness and interpretability requirements while maintaining strong predictive performance. LDA-XGB1 builds upon the widely used XGBoost algorithm but ensures inherent interpretability by using shallow trees of depth 1, making each feature’s contribution to the decision easily understandable. To further enhance interpretability, monotonic constraints can be imposed on key features, ensuring logical and transparent behavior in predictions. LDA-XGB1 employs biobjective optimization that balances accuracy and fairness, incorporating XGB1-based binning techniques and information value to measure and mitigate potential disparate impact. Disparate impact occurs when a seemingly neutral model disproportionately affects a protected group (e.g., race, gender) without discriminatory intent. This ensures equitable treatment for protected groups in the decision-making process. As a result, the proposed LDA-XGB1 model offers a comprehensive solution that balances accuracy, fairness, and interpretability, making it highly applicable for compliance with regulatory requirements.

The rest of this paper is structured as follows. In Section 2, we provide a detailed review of related work, focusing on fairness-enhancing methods and interpretability techniques in machine learning. Section 3 outlines the methodological framework of LDA-XGB1, including its biobjective optimization and fairness metrics. In Section 4, we describe the datasets used for evaluation and present experimental results demonstrating the model’s effectiveness in balancing accuracy, fairness, and interpretability. Finally, Section 5 discusses the implications of our findings for financial institutions and concludes with future directions for research.

SECTION: 2Background and Related Work

SECTION: 2.1Interpretability in Machine Learning

In high-stakes domains like finance, model interpretability is essential for building trust, ensuring compliance with regulatory standards, and comprehending model behavior. With advancements in machine learning, two primary approaches have emerged: post-hoc explainability techniques and inherently interpretable models.

Post-hoc techniques aim to explain the behavior of complex, often black-box, models after they have been trained. Popular methods such as LIME (Locally Interpretable Model-Agnostic Explanations)(Ribeiro et al.,,2016)and SHAP (SHapley Additive exPlanations)(Lundberg and Lee,,2017), provide model-agnostic local explanations by approximating complex models with simpler surrogate models. However, studies have shown that post-hoc explanations may not always faithfully reflect the decision-making process of the underlying model, leading to misleading or incomplete interpretations(Kumar et al.,,2020; Bilodeau et al.,,2024). Moreover, the added layer of complexity in post-hoc analysis can be difficult to justify under regulatory scrutiny, especially in financial services where transparency is critical. These explanations are approximations, and there is no guarantee that they faithfully capture the complex interactions present in the black-box model. See also CFPB Circular 2022-03(CFPB,,2022)for a footnote comment that creditors must ensure the accuracy of any post-hoc explanations, as such approximations may not be viable with less interpretable models.

In contrast to post-hoc methods, inherently interpretable models are designed to be transparent by nature(Sudjianto and Zhang,,2021). These models ensure that their structure and behavior can be directly understood by users. One of the most prominent frameworks in this regard is the generalized additive model (GAM), which was first introduced byHastie and Tibshirani, (1990)and later re-introduced for interpretable machine learning by(Vaughan et al.,,2018; Yang et al.,,2020; Agarwal et al.,,2021). GAMs provide a balance between flexibility and interpretability by assuming the input-outputrelationship to be of the form

whereis the link function,is the overall mean and’s are the main effects of individual features. This additive structure allows each feature’s contribution to the prediction to be independently analyzed and understood, making GAM models inherently interpretable and easy to visualize. Recent extensions of GAMs include GAMI-Net(Yang et al.,,2021)with structured pairwise interactions based on neural networks and GAMI-Lin-Tree(Hu et al.,,2023)based on model-based trees.

In summary, GAM and its extensions represent an ideal model structure for interpretable machine learning in regulated industries. Unlike post-hoc methods that attempt to explain complex models, GAMs are inherently interpretable by design, making them highly suitable for scenarios where model transparency and simplicity are paramount.

SECTION: 2.2Fairness in Machine Learning

Fairness has become a critical concern in the application of machine learning models, particularly in sectors such as finance, healthcare, and criminal justice, where decisions can significantly impact individuals’ lives. In machine learning, fairness refers to the principle that models should not disproportionately harm or disadvantage specific groups defined by protected attributes such as race, gender, or socioeconomic status.Caton and Hass, (2024)provides a comprehensive overview of fairness in machine learning, exploring the various definitions, metrics, and methodologies for integrating fairness into model development.

In the context of model fairness, protected groups and reference groups are defined as follows:

Protected Groups:These are groups of individuals who are safeguarded against discrimination and bias based on legally recognized characteristics, also known as protected attributes. These attributes often include race, gender, age, religion, disability, sexual orientation, and ethnicity. Fairness in machine learning aims to ensure that models do not disadvantage these protected groups in their predictions, recommendations, or outcomes. For example, when evaluating the fairness of a credit decision algorithm, women or certain racial minorities might constitute a protected group if the concern is that the algorithm could unfairly disadvantage them.

Reference Groups:These are groups against which the protected groups are compared when assessing fairness. They typically represent the dominant or non-disadvantaged population in relation to a specific attribute. For example, in gender-based fairness assessments, men might serve as the reference group when evaluating fairness toward women. The reference group serves as a benchmark for fairness, helping to identify disparities in outcomes between different groups.

In fairness assessments, models are examined to ensure that protected groups receive fair and equitable outcomes compared to reference groups, helping to prevent biases that can arise from societal inequalities being reflected in algorithmic decisions. Techniques such as fairness metrics, bias mitigation strategies, and group-based audits are used to measure and address these disparities.
The concept of Less Discriminatory Alternatives (LDA) is crucial as it underscores the obligation of financial institutions to actively seek out and implement strategies that reduce discrimination against protected groups. This proactive approach aligns with the requirements established by the Consumer Financial Protection Bureau (CFPB), which mandates that lenders identify and adopt less discriminatory alternatives whenever feasible.Gillis et al., (2024)discusses the search for LDA in fair lending, providing a method to audit models for less discriminatory alternatives. Their approach allows regulators, auditors, and lenders to systematically discover alternative models that reduce discrimination or refute the existence of such alternatives.

One effective approach to ensure fairness in machine learning is the incorporation of fairness constraints directly into the model’s design and training process(Donini et al.,,2018; Zafar et al.,,2019). These constraints help to align the model’s predictions with ethical considerations and regulatory requirements, ensuring that protected groups are treated equitably.
In model development, fairness constraints can be mathematically formulated to minimize disparate impacts across different protected groups. For instance, one might impose constraints on the false positive rates or true positive rates to ensure that these rates are comparable across groups. By integrating these fairness constraints into the optimization process, models can be trained to balance predictive accuracy with fairness, effectively reducing bias in decision-making. Note that such fairness optimization problems can often be equivalently formulated as biobjective or multi-objective programming (MOP) tasks also known as-constraint scalarization(Haimes,,1971), where
fairness objectives are explicitly considered alongside performance objectives. This approach enables model developers to explore trade-offs and find solutions that provide both high accuracy and improved fairness.

In summary, biobjective or multi-criteria optimization that integrates fairness objectives has proven to be an effective approach in developing LDA for machine learning models, particularly in the domain of fair lending.

SECTION: 3Methodology

In this section, we outline the development of LDA-XGB1, a novel framework designed to balance fairness and accuracy while ensuring model interpretability. LDA-XGB1 builds upon the widely-used XGBoost algorithm, but it incorporates several enhancements to address the dual objectives of fairness and interpretability. The model employs biobjective programming that balances predictive performance and fairness, with built-in mechanisms to ensure that decisions remain transparent and understandable.

SECTION: 3.1XGB1 Base Model

XGB1 is an inherently interpretable model advocated by PiML toolbox(Sudjianto et al.,,2023)and it serves as the base model for the proposed method. XGB1 is built upon the widely used XGBoost framework(Chen and Guestrin,,2016), a powerful machine learning algorithm that utilizes gradient-boosted decision trees. XGBoost (Extreme Gradient Boosting) is known for its high accuracy, speed, and efficiency, making it a popular choice for various predictive tasks, including classification and regression.

The XGB1 base model employs depth-1 trees (also known as decision stumps) during the boosting process. A key characteristic of this model is its interpretability, as it can be viewed as a Generalized Additive Model (GAM) with piecewise constant main effects. The following procedure outlines how XGB1 achieves this interpretability:

Collecting Unique Splits for Each Feature:In each boosting step, XGB1 adaptively selects a feature and determines the best split point. For a given feature, the collected set of unique splits (across different boosting steps) would divide the feature into discrete intervals (bins), with each interval representing a specific range of values.

Calculating Accumulated Leaf Node Values:After determining the split points for each feature, the accumulated leaf node values for each bin generated by the unique splits are calculated. These values correspond to the contribution of each bin toward the final prediction.

The key feature of XGB1 is its ability to adaptively select split points, which enables the model to capture feature behavior in a data-driven manner. This adaptive selection allows XGB1 to achieve superior predictive performance compared to spline-based GAMs, which rely on pre-defined smooth functions for feature effects.

Support for Regularization:To prevent overfitting and enhance generalization ability, XGBoost incorporates built-in regularization mechanisms, including L1 (lasso) and L2 (ridge) regularization. These regularization techniques penalize large coefficients in the model, ensuring a balance between complexity and performance.

Support for Monotonic Constraints:XGBoost supports the implementation of monotonic constraints, which ensures that the relationship between specific features and the model’s predictions remains consistent with domain knowledge. For example, in financial applications, it is expected that an increase in income should not lead to a decrease in the likelihood of loan approval. By imposing these constraints during the training process, XGB1 guarantees that predictions follow logically consistent patterns.

SECTION: 3.2Binning and Information Value

Binning and Information Value (IV) are fundamental tools in assessing the predictive strength of explanatory variables in models, especially in binary classification tasks such as credit scoring or risk modeling. These methods help us evaluate how well input variables (features) distinguish between two outcome classes (e.g., default vs. non-default). Importantly, they can also be adapted to promote fairness by minimizing the influence of variables that may cause disparate impacts to protected groups.

Binning refers to dividing continuous variables into discrete intervals, or “bins”, which group similar values together. This simplification helps reduce noise while preserving the variable’s predictive power. We begin the binning process by using XGB1 base model. These initial bins serve as a starting point for understanding the distribution of values.

To further refine the binning process, we utilizeoptbinning, an optimization technique based on mixed-integer programming. This method identifies the best binning strategy by maximizing a measure known as Jeffreys’ Divergence, which reflects how well the variable differentiates between two classes. In predictive modeling, this divergence is commonly represented as IV.

IV measures the strength of a variable’s ability to distinguish between different outcomes. It is calculated by comparing the proportion of “events” (e.g., defaults) and “non-events” (e.g., non-defaults) within each bin. The formula for IV is:

where

and

is the number of events in bin

is the number of non-events in bin

is the total number of events

is the total number of non-events.

This formula captures the difference between the event and non-event distributions across the bins, with larger differences contributing more to IV. Higher IV indicates that the variable is better at separating the two outcomes, making it more predictive.

The general rule for interpreting IV(Mironchyk and Tchistiakov,,2017; Siddiqi,,2006)is

: the variable has little to no predictive power.

: the variable provides weak predictive power.

: the variable has moderate predictive power.

: the variable is highly predictive.

This ranking helps us determine which variables are most useful in a model, allowing us to focus on features that provide the greatest predictive value.

While maximizing IV for binary outcomes is essential for improving model accuracy, fairness considerations require a different approach for certain variables that may cause disparate impacts. As discussed in Section2.2, fairness means that the model should eliminate or reduce the influence of predictive variables with disparate impacts to protected groups.
In this context, our goal is to minimize the IV for such variables with respect to their ability to discriminate between reference and protected groups in such a way they don’t unduly influence model predictions.

Thus, it creates a balance between

Maximizing the IV for explanatory variables to improve accuracy,

Minimizing the IV for explanatory variables with disparate impacts to reduce bias and promote fairness.

By minimizing the IV of variables with disparate impacts, we ensure that they play a minimal role in model decision-making, promoting fairness and reducing potential bias in the model.

Theoptbinningalgorithm(Navas-Palencia,,2020)helps us achieve this balance by optimizing the binning process for each explanatory variable. The goal is to merge or adjust bins in ways that increase IV for binary outcomes, for improving the model’s predictive ability. However, for fairness, the strategy is the opposite: the binning is optimized to reduce IV of variables with disparate impacts.

This dual-objective binning process helps ensure that models are both accurate and fair. For example, a variable like debt to income ratio might have its bins adjusted to increase IV and better predict risk, while a variable like mortgage size would have its bins merged or adjusted to reduce IV, preventing it from having a large influence on the predictions.

In summary, binning and IV are not just tools for improving model accuracy, they also play a crucial role in ensuring fairness. By carefully managing the IV’s for both binary outcomes and protected groups, we can build models that are both effective and equitable.

Consider a set ofbins. For any two specific binsand, letsuch thatindicates that binsthroughare merged, whileindicates otherwise. Refer to(Navas-Palencia,,2020)for the commonsense constraints onsuch that noncontiguous bins cannot be merged without all the bins between them, and a single pre-bin cannot be merged into multiple superbins.
With the definition of such binary variables, the IV becomes the following function of:

Note in (3) that the interior of the summation is the IV over each merged bin, even though, for some integer,for all.

We followNavas-Palencia, (2020)to defineas the IV for a single superbin containing theth throughth bins for all,

Then, construct a linear representation ofas the accuracy metric for binary classification,

Note that the second summation is required, in order to remove IVs for bins that have been merged. In Appendix we provide a thorough demonstration of this result as a theorem, along with a technical proof by induction.

Denote bythe indicator of the reference group (0) or the protected group (1). For an explanatory variable used in model development, in the process of merging bins, this time we want to minimize the IV with respect to. That is, the model should not be able to discriminate between reference and protected groups. In the similar fashion, defineto be the total number of events in the protected group (i.e.,),to be the total number of events in the reference group (i.e.,). For each bin, let,be the number of protected events and the number of reference events in bin, respectively. Then, write

Similar to (4), by expressing

we can define the following fairness metric,

It is important to note that, while we maximizeto obtain as much information about the target as possible, we want to minimizeto ensure that the binning yields as little information as possible.

SECTION: 3.3Biobjective Programming

When balancing two objective functions – accuracy and fairness, we utilize biobjective programming (BOP) techniques, which can be expressed as follows:

whereare the accuracy and fairness metrics, andis the feasible set defined by a finite list of constraints. The solutions to (BOP) are the pointswhich result in the bestor. To find these, we implement the-constraint methodHaimes, (1971).

The-constraint method exploits the fact that the two objective functions conflict by incorporating one objective function as a constraint. Let; we choose to optimizewith the fairness constraint:

Letminimizeandrespectively over. By adjusting the FIV (fairness IV) bound

we are able to obtain all solutions to (BOP). Frequently, the solutions obtained are plotted against one another in what is called a Pareto front so as to demonstrate the trade-off. In practice, the choice ofis typically selected based on some previously determined thresholds.

SECTION: 3.4The Proposed LDA-XGB1

The proposed algorithm for constructing the model sequence and the approach to demonstrate interpretability for the resulting model are explained below. We begin by developing a binary classification ML model, with the option of applying increasing and decreasing monotonicity constraints as appropriate to aid interpretability, followed by construction of the feature importance and main effect plots for model interpretation.

Suppose we have a dataset consisting of sampleswhereare-dimensional predictors,is the target, andis the protected group indicator. Partition the dataset into training and testing sets and begin by applying XGB1 base model(Sudjianto et al.,,2023)to the training set. This provides us with sets of prebins for each predictor.

Algorithm1outlines the key algorithmic steps. Step 2 is to optimally merge prebins for each predictor by (Con) with or without the fairness constraint according to the input FIV bound. Without the fairness constraint, it is identical to the default optbinning approach(Navas-Palencia,,2020), which serves as the baseline for model comparison. In Step 3, it fits a binning logistic regression model based on the optimized superbins upon the use of one-hot encoding. Finally, the LDA-XGB1 algorithm outputs such a BinLogistic model.

Note that in both Steps 1 and 3, the monotone constraints (i.e., ascending/descending trends) can be imposed on some key variables in order to achieve proper interpretability. Step 1 takes the advantage of XGBoost framework in XGB1 model training with monotone constraints, while in Step 3 we use theoptimizefunction in Python Scipy to compute for a constrained logistic regression model.

To interpret the resulting BinLogistic model from the LDA-XGB1 algorithm, we present another algorithm of model interpretability. Following(Yang et al.,,2021), we develop the procedures to generate the FI (feature importance) and ME (main-effect) plots. The ME plot visualizes the univariate function for each predictor to display the partial input-output relationship. For LDA-XGB1 based on optimal superbins, each ME plot is a step function with piece-wise constant values. The FI plot is presented by a bar chart, with each predictor’s importance index quantified as the sample variance of the step function. See Algorithm2for the concrete steps.

SECTION: 4Numerical Experiments

In this section, we demonstrate the efficacy of the proposed algorithm using both simulated and real world datasets. We consider two datasets, a simulated credit dataset, SimuCredit, fromSudjianto et al., (2023), as well as the well-known, more complicated, COMPAS dataset, obtained fromLutz, (2020). For each dataset, we present the LDA-XGB1 model from Algorithm1. We consider the main effect plots to observe the behaviour implied by this model, add monotone constraints to induce sensible behaviour, and follow up by considering fairness.

To confirm the performances of the resulting LDA-XGB1 models, we employ the widely used metrics, AUC (area under the ROC curve) to measure the accuracy, and AIR (adverse impact ratio) to measure the fairness, with the latter defined as follows:

Note that the AIR compares the number of timesis accepted against the number of timesis accepted. The goal is to have the AIR as close to 1 as possible.

SECTION: 4.1SimuCredit

SimuCredit is a simulated credit approval dataset from the PiML toolboxSudjianto et al., (2023). It contains the following nine features:

Mortgage:The original amount of the mortgage payment

Balance:The amount left on the mortgage payment

Amount Past Due:the amount of money in missed payments

Delinquency:the amount of time spent with missed payments

Inquiry:The number of applications for credit which required companies to check the credit score of the datapoint

Open Trade:The number of accounts on which payments have to be made (credit cards, car payments, etc.)

Utilization:the ratio of balance to credit limit

Gender:binary variable denoting male or female

Race:binary variable denoting black and white

The target variable isStatus, with 1 being approved and 0 otherwise. In this experiment, we considerRaceas the protected group indicator. As a standard practice in fair lending, we remove bothRaceandGenderin model development, so that there will be no disparate treatment (intentional discrimination). Even when the demographic variables are not used in the modeling process, the model can still have disparate impacts (unintentional discrimination) caused by other variables. Thus, there is need to search for less discriminatory alternatives.

We split the data into training and test sets, using SKlearn’s ‘train_test_split’ function with a constant random seed (23) for reproducibility. We employ Algorithm1using the XGBoost.XGBClassifier method with max_depth = 1, n_estimators = 1000 , learning rate = 0.3, tree method = ‘auto’, max_bins = 256, reg_lambda = 1, and reg_alpha = 0.

We run three rounds of the LDA-XGB1 algorithm with the following settings:

Plain XGB1 Model.We applied Algorithm1without fairness or monotone constraints. It fits a plain XGB1 model with optbinning for merging prebins. The feature importance and main effect plots are shown in Figures2and2, respectively. The main effect plots show apparent monotonic patterns forUtilization,BalanceandMortgage.

Monotonic XGB1 Model.As observed from the unconstrained XGB1 model, we impose monotone constraints such thatBalanceandMortgagemust be increasing, whileUtilization,Inquiry,Amount Past Due,Delinquency, andOpen Trademust be decreasing.
Refit the LDA-XGB1 model and we have the feature importance and main effect plot results in Figures4and4, respectively. Note thatMortgagebecomes far more important with the introduction of monotonic constraints, whileUtilizationloses importance to the point that it’s now fourth important. The monotonic shapes of the main effect plots match our expected interpretability based on reality.

Figure6tabulates the AIR and AUC results for both the plain XGB1 and monotonic XGB1 baseline models in the bottom rows. It is worth noting that for the training data, imposing monotonicity makes XGB1 perform better in AUC and worse in AIR, while for the testing data, the monotonic version has better performances in both AUC and AIR. When the monotonic constraints are imposed wisely, the resulting model can have both better generalization and better interpretability.
Nevertheless, the AIR values are far from the normal acceptable range, which means both baseline models have disparate treatment to the protected ‘race’ groups.

Fair Monotonic XGB1 Model.Consider the protected/reference groups defined by ‘race’, for which we define the fairness metric (7). We run the LGD-XGB1 algorithm with the fairness constraints and varying FIV bounds. The trade-off between fairness and accuracy are shown in Figure6for the testing performances, as well as the tabular results in Figure6for both training and testing performances. It is evident that as FIV bound decreases, the model tends to fairer with higher AIR values, while the prediction performance gets sacrificed with lower AUC values.

In search of LDA models with fairness constraints, suppose the resulting model is desirable to have AIR value at least 0.8. By checking Figure6or Figure6, we should choose the FIV boundwith the corresponding model testing performances AIR 0.8521 and AUC 0.7882. Note that we can also run finer FIV bounds betweento search for a better trade-off. This final model atcan be interpreted in the same way by feature importance and main effect plots, as shown in Figures8and8. It can be found that the predictorMortgageis substantially more important when considering fairness, and its binning effect gets coarsened in order to mitigate the bias between the protected and reference ‘race’ groups.

SECTION: 4.2COMPAS

For the Correctional Offender Management Profiling for Alternative Sanctions (COMPAS) dataset(Lutz,,2020), we attempt to predict whether a specific data point will re-commit a crime, i.e., predicting recidivism. The dataset contains the following ten features:

age: how old the datapoint is

juv_fel_count: the number of felonies committed as a juvenile

juv_misd_count: the number of misdemeanours committed as a juvenile

juv_other_count: the number of other crimes committed as a juvenile

priors_count: the number of crimes committed in the past

age_cat_25 - 45: a binary variable denoting within the age range

age_cat_Greater than 45: a binary variable denoting within the age range

age_cat_Less than 25: a binary variable denoting within the age range

c_charge_degree_F: a binary variable denoting if the current charge is a felony

c_charge_degree_M: a binary variable denoting if the current charge is a misdemeanour

Thegenderandraceinfo are not used for model development and they are saved in a separate file. The target istwo_year_recid, which is 1 if the data point committed another crime within two years of being released.

Given the complexity of this dataset, we decrease the number of n_estimators from 1000 to 100 so as to avoid overfitting. Other than that, all the XGBoost hyperparameters are set the same as in Section4.1.

We again run three rounds of LDA-XGB1 algorithm with the following settings:

Plain XGB1 Model.We applied Algorithm1without fairness and monotone constraints, fitting a plain XGB1 model, usingoptbinningto merge the bins. The feature importance and main effect plots are shown in Figures10and10, respectively. The main effect plots show apparent monotonic patterns forpriors_count, andage.

Monotonic XGB1 Model.As observed from the unconstrained XGB1 model, we impose monotone constraints such thatpriors_count,juv_fel_count,juv_misd_count, andjuv_other_countmust be increasing, whileagemust be decreasing.
Refit the LDA-XGB1 model and we have the feature importance and main effect plot results in Figures12and12, respectively. Note thatpriors_countandageboth become far more important with the introduction of monotonic constraints, whilejuv_fel_countdrops enough to change places withageandjuv_misd_countloses all importance completely. At the same time,priors_count, andagenow match our expectation based on reality.

Fair Monotonic XGB1 Model.Again consider the protected and reference ‘race’ groups and define the fairness metric (7). We run the LGD-XGB1 algorithm with the fairness constraints and varying FIV bounds as shown in Figure14. The trade-off between fairness and accuracy is shown in Figure14for the testing performances, as well as the tabular results in Figure14for both training and testing performances. It is also evident that as FIV bound decreases, the LDA-XGB1 model tends to fairer with higher AIR values, while the prediction accuracy gets sacrificed with lower AUC values.

In our exploration of LDA-XGB1 models with fairness metrics, we observe significant bias in binary classification for the COMPAS dataset. The baseline models demonstrate low AIR values, indicating substantial fairness concerns. When fairness constraints with stringent FIV bounds are applied, the AIR improves, but it comes at the cost of significant reduction in model prediction accuracy. Figures16and16show a selected LDA-XGB1 model with FIVfor model interpretation. In this model, whereageandpriors_countemerge as the two most important features, withageshowing the decreasing trend andpriors_countexhibiting an increasing trend.

SECTION: 5Conclusion

In this work, we introduced LDA-XGB1, a novel framework for developing less discriminatory and interpretable machine learning models for binary classification. By leveraging the predictive strength of XGBoost while integrating fairness constraints and monotonicity for interpretability, LDA-XGB1 achieves an effective balance between accuracy and fairness, particularly in high-stakes applications like financial lending and criminal justice.

Using both simulated (SimuCredit) and real-world (COMPAS) datasets, we demonstrated that LDA-XGB1 can mitigate biases against protected groups. Our biobjective optimization framework, which balances accuracy and fairness, allows for the flexible tuning of trade-offs, making LDA-XGB1 adaptable to various practical use cases.

The adoption of LDA-XGB1 presents a promising pathway for financial institutions to comply with regulatory standards, such as the CFPB’s requirements for fair lending practices and model transparency.
Recently, CFPB provided comments in response to the U.S. Department of the Treasury’s Request for Information on the use of artificial intelligence (AI) in the financial services sector(Frotman and Meyer,,2024), The CFPB emphasized that AI adoption must comply with existing consumer protection laws, including the Equal Credit Opportunity Act (ECOA) and the Consumer Financial Protection Act (CFPA). These laws prohibit discrimination and ensure transparency in AI-driven decision-making processes, particularly in areas like lending and fraud detection.

The proposed LDA-XGB1 framework aligns well with CFPB’s emphasis on less discriminatory alternatives and the importance of model interpretability.
As machine learning continues to transform decision-making processes in the financial sector, LDA-XGB1 can serve as a standard for incorporating ethical considerations into predictive modeling.

In summary, LDA-XGB1 not only addresses current regulatory demands for fairness and interpretability but also sets the stage for future research on integrating fairness constraints into machine learning algorithms. As regulatory standards evolve and the demand for less discriminatory alternatives, the LDA-XGB1 framework can be expanded and refined to incorporate multiple fairness metrics, two-dimensional optbinning strategy for two-way interaction effects, and more complex use cases.

SECTION: Acknowledgement

This work is supported through a generous gift from the Wells Fargo Bank and partial funding from the Center for Trustworthy Artificial Intelligence through
Model Risk Management (TAIMing AI) at UNC Charlotte which is funded through the Division of Research, the School of Data Science, and the Klein College of Science.

SECTION: Appendix

We present the result that (5) is a linear representation of the IV as a theorem, along with two detailed proofs thereof, one by induction, and one directly.

The representation of the IV, (3), which is nonlinear in, is equivalent to the linear representation, (5).

We proceed by induction. Suppose that there is a specific set (possibly infinite) of bins of pre-determined length. Letbe some number of those bins, and denote the IV in (3) and proposed objective function in (5) for binsthroughas follows:

Note that

Further, for a given row of matrix, let the IV over that row be denoted as

Then (9) can be represented as

Consider the base case. Then there are two cases: the bins are separate, that is,,; and the bins are merged, that is,,.

If the bins are separate,,, then

Similarly, if the bins are merged,,, then

Suppose the induction step, that is, suppose. Since

we need only show that

Based on the constraint that a merged bin must be continuous, we have that, for each, there existssuch thatfor allandfor all. Then

Therefore. At the same time, sinceis fixed, we get that

Consequently,, and the proof by induction is complete.
∎

SECTION: References