SECTION: STDCformer: A Transformer-Based Model with a Spatial-Temporal Causal De-Confounding Strategy for Crowd Flow Prediction
Crowd Flow Prediction is critical to urban management, with the goal of capturing the arrival and departure characteristics of crowd movements under different spatial and temporal distributions, which is fundamentally a spatial-temporal prediction task. Existing works typically treat spatial-temporal prediction as the task of learning a functionto transform historical observations to future observations. We further decompose this cross-time transformation into three processes: (1) Encoding (): learning the intrinsic representation of observations, (2) Cross-Time Mapping (): transforming past representations into future representations, and (3) Decoding (): reconstructing future observations from the future representations. From this perspective, spatial-temporal prediction can be viewed as learning, which includes learning the space transformationsbetween the observation space and the hidden representation space, as well as the spatial-temporal mappingfrom future states to past states within the representation space. This leads to two key questions:To address Q1, we propose a Spatial-Temporal Backdoor Adjustment strategy, which learns a Spatial-Temporal De-Confounded (STDC) representation space and estimates the de-confounding causal effect of historical data on future data. This causal relationship we captured serves as the foundation for subsequent spatial-temporal mapping. To address Q2, we design a Spatial-Temporal Embedding (STE) that fuses the information of temporal and spatial confounders, capturing the intrinsic spatial-temporal characteristics of the representations. Additionally, we introduce a Cross-Time Attention mechanism, which queries the attention between the future and the past to guide spatial-temporal mapping. Finally, we integrate the process of learning the STDC representation space and the spatial-temporal mapping into an--skeleton for spatial-temporal prediction. The skeleton is further instantiated with a Transformer model, building a Transformer model with Spatial-Temporal De-Confounding Strategy (STDCformer). Experiments on two real-world datasets demonstrate that STDCformer achieves state-of-the-art predictive performance and exhibits stronger out-of-distribution generalization capabilities.

SECTION: Introduction
Crowd flow prediction aims to use historical flow data in a region to predict the inflow and outflow of during future time periods. It is a typical spatial-temporal prediction task that plays a crucial role in urban planning, traffic management, public safety, and other fields. The essence of spatial-temporal prediction lies in capturing the mappingfrom historical data to future data, enabling the inference of future from historical observations. In recent years, a series of spatial-temporal prediction models have emerged with the goal of solving, and the deep learning-based models have become mainstream. Among these, models based on Spatial-Temporal Graph Neural Networks (STGNNs) and Spatial-Temporal Transformers (ST Transformers) have demonstrated exceptional performance in various spatial-temporal prediction tasks, such as traffic flow prediction and crowd flow prediction. These two types of models follow a unified paradigm, where temporal representation modules (e.g., RNN, CNN, and Transformer) and spatial representation modules (e.g., CNNs, GNNs, and Transformers) are used to separately capture the temporal and spatial representations from the observed data. A spatial-temporal fusion module then combines these representations to obtain the final representation, which is used to infer unobserved spatial-temporal data.

needs to be realized through three subprocesses. First, the encoderis used to encode historical observations into representations in a latent space. Second, a cross-time mappingis applied to transform the representation of past states into the representation of future states. Third, the decoderprojects the future representations back into the observation space to reconstruct the future observations. From this perspective, the objective of learningis to learn, which includes the space transformationbetween the observation space and the representation space, as well as the spatial-temporal mappingfrom future states to past states within the representation space. The purpose of space transformation is to identify a representation space that can capture the essential information in the observations, serving as the foundation for ensuring the feasibility of the subsequent spatial-temporal mapping. The purpose of spatial-temporal mapping is to learn the intrinsic, dynamic transformation relationships between the past and future from their essential representations. These two processes lead to two key questions behind spatial-temporal prediction:The answers to these questions and the corresponding methods are as follows.

The premise of inferring the future from the past is based on the assumption that there exists some form of ”influence” of the past on the future, which determines the mapping relationship between the past and future. Historical data and future data are not simply related as input and output, but implicitly involve assumptions about the causal relationship between the past and future. Therefore, fusing the representation of causality can capture the essential information within the observational data. Under this assumption, such ”influence” can be characterized through causal effects. Existing methods typically build neural network models to fit the associationsbetween historical and future data distributions from the observational data to learn the representation space. However, when uncontrolled confounding biasis present, the model fails to capture the true causal effect.

As shown in Figure, the basic unit in spatial-temporal data is analogous to a token in text, which serves as the trial unit for estimating, referred to as the ST token (STT), represented as. It can be observed that, similar to different participants with varying physical conditions in a drug trial, each STT also has temporal and spatial characteristics. These characteristics reflect the attributes of the spatial region(e.g., functionality, travel cost, travel safety, etc.) and the temporal window(e.g., travel necessity, travel suitability, etc.), which are potential factors influencing human movement. These characteristics simultaneously affect both historical and future crowd flow observations, serving as confounders in the spatial-temporal prediction process. If the distribution bias of these confounding factors in the sample data is ignored, the model will learn incorrect patterns.

Estimating the correct causal effect requires eliminating the distributional bias of confounders, a process known as de-confounding. In this paper, we argue that the ideal spatial-temporal representation space should enable de-confounding, i.e., learn a Spatial-Temporal De-Confounded (STDC) representation space. The challenge of spatial-temporal observational data de-confounding lies in simulating the intervention operationon the observed samples, with the key being sample stratification and confounder control. Sample stratification entails classifying samples into treatment groups based on their differences, while confounder control then re-weight the samples in each stratum to estimate the true causal effect. However, due to the complexity of spatial-temporal activities and the limited nature of observational data, confounders are often unobservable, and the distribution of such hidden confounders in the observational samples is unknown, which makes it difficult to directly measure the differences between samples, consequently, challenging to accurately pre-define the number of sample strata and their weights. In order to obtain the representation of hidden confounders, existing methods often learn the representations confounders from the original observation time series. However, these abstract representations are hard to be aligned to the specific semantics in the real world, and the separability between different strata heavily depends on the sampling and quality of the observation data. Some methods also introduce Point-of-Interest (POI) data to learn the classification of confounding representations, but they need to pre-define the number of categories k of confounding and their distribution. The ideal hidden confounding stratification in crowd data should accurately indicate the key influence factors behind crowd movements, and different confounding representations should be naturally distinguishable and assigned appropriate weights. To approximate this ideal hidden confounder stratification and allow for subsequent control, this paper proposes a Spatial-Temporal Backdoor Adjustment strategy, specifically:

(1) To improve the reliability and semantics of confounders’ representations, we collected auxiliary information that can separately characterize the temporal and spatial characteristics of STTs based on the analysis of human movement behavior. This auxiliary information is fused into the prediction model to learn the representations of hidden confounders. Compared to one-dimensional time series data from uncontrollable sampling process, this type of information has more stable quality and clearer semantics.

(2) To reduce the bias caused by assumptions on the distribution of confounding factors, and based on the two most critical attributes of human mobility, ”When”, ”Where”, we naturally categorize the confounders into temporal and spatial confounders. This approach avoids the need to predefine the number of clusters k while ensuring the completeness of the stratification. Additionally, both temporal and spatial confounders are fed into learnable modules to derive the weights, eliminating the need to assume a predefined distribution for the confounders.

Changes in both time and spatial sampling values can lead to shifts in the spatial-temporal mapping relationships. As shown in Figure, taking the crowd flow data from Manhattan Island, New York City, as an example, the flow data is divided into different samples for prediction, with a time window size of 6 hours. For the same time sampling, the future trends of both samplesandare similar to their historical trends, showing an overall downward trend. In contrast, the future trend offirst follows the historical trend and then reverses. For the same region sample, the future trend ofis opposite to its past trend, whileshows the same future and past trends.

This difference arises from the spatial-temporal characteristics of the observation samples, i.e., the different spatial-temporal contexts, and spatial-temporal confounders can portray this information. For example, due to the different functionalities of Zone 79 compared to the other two zones, even at the same time, their ability to attract crowd flow differs, leading to different trends. While for Zone 75, the travel demand of people varies at different time windows, resulting in different trends in how the spatial region is visited across different time periods.

Therefore, we propose a Cross-Time-Attention-based Past-to-Future Mapping mechanism. To better capture the inherent spatial-temporal characteristics of STT representations, we construct Spatial-Temporal Embeddings (STE) of STTs by fusing time and spatial confounders, which preserve the global positional and structural information of the STTs in both time and space dimensions. To better capture the dynamic transformation relationship between past and future, which is a pair of cross-time entities, we introduce a Cross-Time-Attention (CTA) mechanism to enable querying between future STEs and past STEs. The queried relationships are future used to guide the transformation of past representations to future representations.

By combining A1 and A2, we constructed a prediction skeleton with the structure of STDC Encoder → CTA-based Past-to-Future Mapping → STDC Decoder, and instantiated this skeleton based on the Transformer architecture to obtain the STDCformer, which simultaneously achieves de-confounding and spatial-temporal prediction. Compared to existing ST Transformer architectures (e.g., Figure), STDCformer introduces confounder’s information in both STE encoding and spatial-temporal representation learning (Figure), allowing the model to use confounder’s information to simultaneously guide spatial-temporal mapping and spatial-temporal fusion. This approach maximizes information sharing between de-confounding representation and spatial-temporal prediction, thereby enhancing information utilization efficiency.

Our contributions are as followed:

(1) We propose a new perspective for formulating spatial-temporal prediction, and construct a novel Encoder-Past-to-Future Mapping-Decoder prediction skeleton. Based on this skeleton, a spatial-temporal prediction model, STDCformer, is developed, which achieves de-confounded crowd flow prediction.

(2) We introduce a spatial-temporal backdoor adjustment strategy, partitioning the confounders behind crowd flow prediction into two categories: temporal confounders and spatial confounders, and incorporating auxiliary information to accurately build confounders. The representation of confounders is used to encode spatial-temporal embeddings and guide spatial-temporal fusion, enhancing the utilization of confounders’ information and simultaneously achieving de-confounding.

(3) We conducted prediction experiments under independent and identically distributed (IID) conditions on two real-world datasets. The experimental results show that STDCformer outperforms state-of-the-art (SOTA) models, including STGNNs and ST Transformers. Moreover, STDCformer generalizes better to out-of-distribution (OOD) datasets in a zero-shot manner. We aligned the wights of learned confounders with real-world data and discussed their physical meaning, explaining how the deconfounding strategy removes confounding bias from the data by reweighting.

(4) We build two crowd flow datasets for Manhattan and Brooklyn in New York City, which include auxiliary information required to build confounders for all STTs within the datasets. These datasets provide a foundation for future research on de-confounded crowd flow prediction.

SECTION: Related Works
SECTION: Spatial-Temporal Graph Prediction
Crowd Flow Prediction in Urban Areas is a typical spatial-temporal graph prediction task, and existing spatial-temporal graph prediction methods can potentially be applied to crowd flow prediction. Therefore, this paper provides a general review of spatial-temporal graph prediction methods. Spatial-temporal graph prediction aims to forecast the future states of nodes, edges, or graphs in data constructed as graphs. In such graphs, nodes typically represent spatial entities (e.g., regions), while edges represent relationships between spatial entities (e.g., adjacency, interaction). Typical spatial-temporal graph prediction tasks include traffic flow prediction, crowd flow prediction, epidemic prediction, and air quality prediction. Unlike spatial-temporal data constructed as regular grids (e.g., remote sensing images), the spatial regions in graphs are mostly irregular units, and the relationships between them are often more complex. Unlike purely time-series prediction or graph learning tasks, spatial-temporal graph prediction requires the simultaneous modeling of temporal and spatial features, and thus involves modules for capturing both temporal dependencies and spatial dependencies.

Currently, mainstream spatial-temporal graph prediction frameworks can be divided into two categories: Spatial-temporal Graph Neural Networks (STGNNs)and
Spatial-temporal Transformers (ST Transformers). As shown in Figure, STGNNs utilize time-series representation models and GNNs to learn temporal representation, and spatial representation, then fuse these two representations to obtain the final spatial-temporal representation, for prediction. In another way, ST Transformers introduce temporal
embeddings, and spatial embeddingsto preserve the positions and structural information of tokens in the original spatial-temporal sequence, and use temporal self-attention and spatial self-attention modules to derive temporal representationsand spatial representations, respectively. These two representations are then fused to generate the final spatial-temporal representation for prediction (as shown in Figure).

The main goal of the spatial representation module is to model the relationships between spatial regions and encode these relationships into representations. Based on the fundamental structure of spatial representation models, existing methods can be categorized into CNN-based, GNN-based, and Transformer-based spatial-temporal prediction approaches. CNN-based models partition spatial regions into regular grids and use spatial convolutions to achieve spatial representation. For example, DeepSTuses a deep neural network (DNN) to extract the spatial features of the urban grid. However, this approach is not suitable for spatial-temporal prediction in irregular regions. GNN-based spatial representation modules and Transformer-based spatial representation modules are separately the core component of existing STGNNs and ST Transformers.

. DCRNNuses a diffusion convolution to extract graph structure signals by walking randomly on the graph; STGCN, T-GCN, and KST-GCNuse graph convolution networks to represent graph structures; STFGNNuses a simplified graph matrix multiplication to aggregate spatial dependencies; Graph wavenetciteRN13 generalizes the form of GCN to diffusion convolution to build graph convolution layer; ASTGCRNproposes an adaptive graph convolution to learn node representations; ASTGCNuses graph attention convolution to obtain spatial representations based on spatial attention; HGCNuses spectral clustering and spatial gated graph convolution to obtain micro and macro spatial representations; MTGNNuses a mix-hop propagation layer in the graph convolution module to select and aggregate information from neighboring nodes.

. ASTTGNleverages node2vecto retain graph structure information, and inputs it as the embedding of the node sequence into the spatial transformer model. Then, a multi-head attention mechanism is used to capture dynamic and implicit spatial dependencies. STTNinputs the representation obtained by graph convolution into the transformer structure to capture dynamic graph signals; GLSTTNconstructs a spatial transformer similar to STTN to learn spatial representations; Traffic transformerconstructs an encoder-decoder framework based on multi-head attention, and introduces the original graph structure as a mask in the decoder.

The primary goal of the temporal representation module is to model temporal features such as trends and periodicity in time series data. According to the fundamental structure of temporal representation models, existing methods can be categorized into RNN-based, CNN-based, and transformer-based spatial-temporal prediction models. RNN-based models primarily utilize Recurrent Neural Networks (RNNs)and their variantsto extract temporal features. CNN-based models extract temporal features by applying convolutions over the time dimension. Transformer-based temporal representation models, on the other hand, use self-attention mechanisms to capture dependencies across different time steps.

. Models such as ASTGCRN, T-GCN, and KST-GCNall employ Gated Recurrent Units (GRU). DCRNNutilizes GRU to construct a Diffusion Convolutional Gated Recurrent Unit for extracting temporal features. ST-MetaNetemploys a GRU as the first layer in a Seq2Seq framework. Traffic Transformeruses a simple Long Short-Term Memory (LSTM) to obtain temporal embeddings for extracting temporal features. MASTGNuses LSTM to process features after transformations based on spatial and temporal attention.

. STGCNutilizes 1-D causal convolution and Gated Linear Units (GLU) to construct temporal convolutional layers. Graph Wavenetuses Gated Temporal Causal Convolution to expand the temporal receptive field. STFGNNproposes a new gated CNN module for extracting temporal features. ASTGCNcombines temporal attention with standard convolution to extract temporal features. HGCNreplaces the original gated temporal convolution with dilation convolution whose dilation factor is 2 to construct temporal gated convolution.

. ASTTGNemploys an adaptive temporal transformer module to capture long-range temporal dependencies. STTNconstruct a temporal transformer to capture bidirectional, long-range temporal dependencies. LSTTNtrains a transformer-based encoder to represent temporal features of subsequences through a self-supervised task of mask reconstruction.

Existing methods for spatial-temporal fusion can be divided into two categories: direct fusion and adaptive fusion. The former typically connects the temporal and spatial representation modules sequentially, interleaves them in a block or simply adding the two types of representations. In contrast, the latter involves inputting both representations into a learnable module for fusion.

. In TGCN, the temporal representation module follows the spatial representation module, with the learned spatial representations being fed into the temporal representation module. While Graph Wavenetplaces the temporal module before the spatial module. G-SWaNfirst uses a WaveNet to obtain the temporal representation and then feeds it into the spatial graph transformer to learn the spatial representation. Similarly, Traffic Transformeruses LSTM to obtain representations, which are then input into subsequent global and local spatial graph representation modules. STGCNand HGCNbuild the temporal gated convolutional layer and spatial convolutional layer into a ”sandwich” structure. MTGNNinterleaves the temporal and spatial convolution modules for spatial-temporal fusion, while STTNinterleaves the spatial transformer and temporal transformer. DCRNNreplaces matrix multiplications in the GRU with spatial diffusion convolutions, using spatial representations as recurrent units to learn temporal representations. Similarly, ASTGCRNreplaces the multilayer perceptron (MLP) in GRU with adaptive graph convolution. MASTGNconcatenates the representations transformed by spatial attention and internal attention, using this concatenated representation as input for the followed layers.

. ASTTGN, MVSTTand GMANuse a gated fusion mechanism to adaptively perform weighted fusion of temporal and spatial representations. PDFormerallocates different attention heads to the temporal and spatial representations for attention learning and then uses a multi-head attention mechanism to fuse the temporal and spatial representations.

SECTION: Spatial-Temporal De-confounding Representation Learning
Spatial-temporal de-confounding representation learning involves modeling causal variables in spatial-temporal data, and estimating the causal effect of the treatment variable on the outcome variable by removing the influence of confounding variables (confounders). In different data and task scenarios, the confounders within the system vary, and the de-confounding methods differ accordingly. Existing research can further be categorized into methods based on structural causal model (SCM) and potential outcome framework. The underlying ideas of the two types of approaches are unified, differing only in the causal language used.

Methods based on SCM can directly implement interventions on causal graphs, transforming causal problems into statistical language that can represent the data through front-door, backdoor adjustments and other methods. Backdoor adjustment realizes de-confounding by stratifying and balancing confounding factors, being adopted by many studies. Some works categorize confounding factors into pre-defined layers. For instance, STCTNconsiders regional attributes in the region network as confounders, which may cause existing spatial-temporal prediction models to absorb spurious correlations between the historical and future data. To address this, spectral clustering is used to partition regional attributes, and an unbiased prediction model is constructed in an independent parameter space for each partitions to achieve de-confounded predictions. SEADaims to eliminate the confounding effects of social environments on pedestrian trajectories. This work assumes that the confounders follow a uniform distribution, and the joint distribution of confounders and causal variables is learned through a cross-attention module. CISTGNNlearns representations of confounders from external weather environments and applies backdoor criteria for de-confounding. STEVEfocuses on the finiteness of the number of confounders and the completeness of their categories, dividing confounders into invariant and variant layers. CaSTdiscretizes the confounders through a temporal environment codebook, so that the representations of confounder fall into the most appropriate layers. CTSGIleverages images at pedestrian trajectory points as the source of confounder representations, extracting environmental representations through a semantic segmentation model, and then adjusting confounding effects using backdoor criteria. Front-door adjustment, on the other hand, applies to causal graphs where no back-door paths exist, providing a de-confounding strategy different from the backdoor criterion. STNSCMuses GLUs to obtain the distribution of causal variables after intervention from historical observational data and external environment data. In this distribution, causal variables can combine with different confounders obeying an unbiased probability, allowing for de-confounded effect estimation. A similar strategy is used in vehicle trajectory prediction, where the contextual scene is treated as a confounder and eliminated through a counterfactual representation inference module based on front-door adjustment strategies. CASPERuses front-door adjustment to transform time-series completion tasks into summing over subcategories of input data representations, and calculates de-confounded causal effects through learnable prompt vectors.

The main idea of methods based on the potential outcome framework is to treat different observational units as samples and simulate interventions by processing these samples comprehensively, thus obtaining de-confounded causal effects. Sample re-weighting is one of the classical methods for achieving sample balance, which is used to assign different cities and regions to different treatment groups and calculate the causal effects corresponding to each treatment. For example, it has been used in studies to assess the impact of changes in POI on regional pedestrian flow. With the development of representation learning, balancing confounders based on the obtained unified representations of the samples through representation learning can also ensure that the background attributes distribution across different experimental groups are similar. For instance, CAPEgenerates representations for different locations from historical event sequences and measures the differences in the representations using the Integral Probability Metric (IPM), minimizing the overall differences between samples in different experimental groups to achieve representation balance. SINetuses the Hilbert-Schmidt Independence Criterion (HSIC) as a regularizer to guide representation balance and remove the effect of hidden confounders. CIDERapplies the Wasserstein distance to balance representations across different administrative regions.

In the spatial-temporal prediction module, the proposed method constructs temporal and spatial representations based on transformer models, and employs adaptive spatial-temporal fusion. In the spatial-temporal de-confounding module, the de-confounding strategy proposed in this paper is based on SCM, utilizing the backdoor criterion for de-confounding. The distinctions between this method and the aforementioned related studies are as follows: 1) The strategy employed in this paper innovatively categorizes confounders into two abstract types: temporal confounder and spatial confounder. Furthermore, the probability distributions of the confounders are learned adaptively, which avoids the pre-setting of the number of categories and the distribution of confounders; 2) The constructed spatial-temporal de-confounded representations can simultaneously participate in both spatial-temporal representation learning and spatial-temporal fusion, maximizing information sharing between the two modules.

SECTION: Methodology
SECTION: Hypothesis on Spatial-Temporal Prediction
Spatial-temporal prediction aims to infer the future state forsteps based on the known previous observations forsteps. Taking the prediction of crowd flow as an example, the spatial-temporal prediction task is typically formalized as, whererepresents the number of spatial regions to be predicted, andrepresents the number of features to be predicted. For the case of predicting the inflow and outflow,. As shown in Figure, existing methods typically use spatial-temporal prediction model to approximate, where the underlying assumption is that there exists some learnable pattern between past and future observations. However, in this paper, we argue that the nature of this pattern is to describe the transformation relationship between the past and future in different spaces, rather than simply viewing it as a mapping between input and output values. Therefore, as shown in Figure, we further decompose the spatial-temporal prediction task into space transformationsbetween the observation space and the hidden representation space, and the spatial-temporal mappingfrom future states to past states within the representation space. The past inflow and outflow values (IO) are first mapped into a high-dimensional hidden space through a representation model. Then, in the high-dimensional latent space, they undergo a spatial-temporal transformationto obtain the future IO vector, which is finally transformed back to the observable IO values through another representation model. Therefore, the crowd flow prediction problem can be formalized as Eq.:

Based on this decomposition, the spatial-temporal prediction model must address two key questions. Q1:Q2:For Q1, we argue that the predictability of spatial-temporal data is ensured by the transmitting the influence of the past on the future, and this influence can be measured through causal effects. Therefore, based on causal assumptions, we propose a spatial-temporal backdoor adjustment strategy to learn a de-confounded representation space (Section), where past representations being encoded and future representations being decoded. For Q2, we believe that the transformation relationship between the past and the future depends on their spatial-temporal. The spatial-temporal embeddings (STE), which fuse the information of both temporal and spatial confounder can model this spatial-temporal context. Therefore, we propose a Past-to-Future Mapping Module based on a Cross-Time-Attention (CTA) mechanism, which facilitates the mapping between past and future through the query of STE from both the future and the past (Section).

The foundation of the ability to infer future states from historical states for spatial-temporal prediction models is that the past has some form of ”influence” on the future. This influence determines the mapping relationship between historical input data and future output data, which is the basis of the predictability of spatial-temporal data. Causal inference can measure the influence of the past on the future using causal effects, aiming to build causal relationships between variables rather than merely correlations, enabling a more accurate and stable representation of the variables. The underlying assumption is that causality is a more reliable form of association, where information flows along the causal links, transmitting the causal effect of the cause on the outcome. Based on this assumption, the spatial-temporal prediction process can be viewed as the process of estimating the causal effect of historical data representations on future data representations, inferring the outcome from the values of causes. However, the hidden confounding bias makes it impracticable to estimate the true causal effect directly from the observational data. This section takes the basic unit of spatial-temporal prediction, STT, as a starting point, to describe the causality underlying crowd flow prediction. We also propose a spatial-temporal backdoor adjustment strategy to learn a de-confounded representation space.

. Based on the form of input data, the basic unit in spatial-temporal data is the observation on a spatial entityat a specific time step. In this paper, this smallest unit in spatial-temporal data is analogized to a token in text, referred to as the Spatial-Temporal Token (STT), represented as:. whereis the variable to be predicted, andanddescribe its spatial and temporal characteristics, respectively. These can be considered as background attributes of STTs, and they influence the estimation of the causal effect.

. The prediction model infers future data representations from historical data representations, where the historical representation serves as the cause for the future representation. However, for each STT, the background attributes influence both the past and future IO representations, makingandserve as confounder in the prediction process of. The causal graph underlying spatial-temporal prediction can be illustrated as Fig., which suggests that the causal effect frommay vary under different values of confounders. This is similar to how the effect of a drug may differ for patients of different ages in a medical trial. To estimate an accurate and fair causal effect, it is necessary to configure the trial groups in such a way that the distribution of confounders across all groups becomes uniform, i.e., to remove the confounding bias. The intervention operation on the causal graph provides an intuitive representation of de-confounding, where the arrowis removed to make the distribution ofindependent of. The causal effect after intervention is expressed as.

. Backdoor adjustment modifies the weights of observational samples with different values of confounders based on their distribution, simulating the balanced sample distribution under an intervention experiment, and is an effective way to de-confounding in observational data. It can be applied when the confounder satisfies the backdoor criterion for. After backdoor adjustment, the causal effect ofis expressed as:. where the confounder is discretized intocategories. However, in spatial-temporal prediction, confounders are often hidden, making it difficult to predefine the value ofand the distribution of each category. To address this issue, we consider the fundamental decision factors of human movement in the real world-namely, ”where” and ”when”-to construct the minimal descriptive set of human movement: ”when”, ”where”. We then categorize the confounders into two abstract types: temporal confoundersand spatial confounders. Furthermore, since STTs naturally contain the background variablesand, which can serve as information sources forand, respectively. Based on this partition, we propose a spatial-temporal backdoor adjustment method as our de-confounding strategy, which is expressed as Eq.:

The relationship between past and future states is very abstract, if using a function to approximate this kind of relationship, the parameters of this mapping function will have different value in different temporal periods. Past and future can be viewed as a pair of Cross-Time spatial-temporal entity. Capturing the transformation relationship between these entities requires addressing the following two issues:

(1). Based on the observations in Fig., we argue that spatial confounder and temporal confounder can jointly characterize the contextual differences between different STTs, which represent the intrinsic characteristics of tokens. Therefore, fusing the representations of temporal confounders with spatial confounder effectively captures the spatial-temporal characteristics of different spatial-temporal entities. For each STT that constitutes the Past and Future, we integrate their temporal and spatial confounder’s representations to obtain a Spatial-Temporal Embedding (STE). This embedding retains the temporal and spatial characteristics of each STT within the entire dataset, thereby representing the contextual features of different spatial-temporal entities.

(2). The relationships between Cross-Time entities are diverse and dynamically evolving. Therefore, attention mechanism can be effective for capturing their similarities. Since this type of querying spans across time, we refer it to Cross-Time Attention (CTA). We use the STEs of Past and Future as the Key and Query, respectively, and employ the attention mechanism to perform the query. The spatial-temporal mapping is then implemented based on the results of queried attention.

SECTION: Overview
Based on the assumptions in Section, we combine the physical institutions of space transformation and spatial-temporal mapping in the spatial-temporal prediction process. We implement the representation modelsand the spatial-temporal mapping functionthrough an Encoder-Decoder architecture and the Past-to-Future Mapping module, respectively. The prediction skeleton with architecture ofis built. The encoder and decoder typically share similar model structures and perform predictions in an end-to-end manner. The transformation processes involved in each part are described as follows:

: The encoder projects the past datainto a hidden representation space, producing the intermediate representation. This is expressed as:.

: The spatial-temporal mapping function captures the relationship between past and future states in the hidden representation space, transforming the past representation in the hidden space into the corresponding future vector. This is expressed as:.