SECTION: DiaMond: Dementia Diagnosis with Multi-Modal Vision TransformersUsing MRI and PET
Diagnosing dementia, particularly for Alzheimer’s Disease (AD) and frontotemporal dementia (FTD), is complex due to overlapping symptoms.
While magnetic resonance imaging (MRI) and positron emission tomography (PET) data are critical for the diagnosis,
integrating these modalities in deep learning faces challenges, often resulting in suboptimal performance compared to using single modalities.
Moreover, the potential of multi-modal approaches in differential diagnosis, which holds significant clinical importance, remains largely unexplored.
We propose a novel framework, DiaMond, to address these issues with vision Transformers to effectively integrate MRI and PET. DiaMond is equipped with self-attention and a novel bi-attention mechanism that synergistically combine MRI and PET, alongside a multi-modal normalization to reduce redundant dependency, thereby boosting the performance.
DiaMond significantly outperforms existing multi-modal methods across various datasets, achieving a balanced accuracy of 92.4% in AD diagnosis, 65.2% for AD-MCI-CN classification, and 76.5% in differential diagnosis of AD and FTD.
We also validated the robustness of DiaMond in a comprehensive ablation study.
The code is available at.

SECTION: Introduction
Dementia presents a growing concern for individuals and society, with Alzheimer’s Disease (AD) constituting 60-80% of the cases and frontotemporal dementia (FTD) ranking as the second most common type in the younger-elderly population under 65 years old.
Accurately diagnosing AD and distinguishing it from other dementia types is crucial for patient management, therapy, and prognosis, but challenging as symptoms overlap.
To address this challenge, a variety of diagnostic tools are employed, including magnetic resonance imaging (MRI), positron emission tomography (PET), and cognitive tests.
Structural MRI provides details on neuroanatomy for identifying regional atrophy, while fluorodeoxyglucose (FDG) PET tracks the distribution of glucose metabolism in the brain.
Recent research into machine learning methods has revealed that using both MRI and PET scans improves the accuracy of diagnosing
AD and distinguishing it from other types of dementia, compared to single-modality imaging.
In deep learning (DL), CNNs have been widely employed to combine these two modalities, but recent results pointed out that the combination may not surpass the efficacy of using PET alone.
The emergence of Transformers and attention mechanisms gives rise to a surge of new techniques in multi-modal AD diagnosis, yet they commonly integrate CNNs for initial feature extraction with vision Transformers (ViTs) for feature fusion, which may not fully exploit the multi-modal potential of ViTs.
Moreover, although the main clinical objective for neuroimaging of dementia patients is differential diagnosis, current studies have been limited to consider one single type of dementia, namely AD.

We address this gap by introducing DiaMond, a novel framework that leverages pure ViTs with multiple attention mechanisms for effective multi-modal classification, which exhibits high efficacy in both AD prediction and differential diagnosis of dementia.
DiaMond consists of independent branches incorporatingself-attentionandbi-attentionmechanisms, where the former extracts unique features from PET and MRI modalities independently while the latter exclusively captures correlations between modalities in their overlapping information, as illustrated in Fig..
Since the bi-attention block explicitly focuses on the potential similarity between modalities, we remove the redundant partial dependencies between modalities from the self-attention computation using the multi-modal normalization technique RegBN, enabling these branches to efficiently explore distinct features within each modality.
Evaluated on three distinct medical datasets, DiaMond shows significant improvement not only in the classification of AD progression but also in the differential diagnosis between AD and FTD.
In summary, our contributions are:

An efficient multi-modal classification framework using pure ViTs for feature extraction and interaction.

A novel bi-attention mechanism to exclusively explore the underlying similarities between PET and MRI data in high-dimensional feature space.

Integrating the multi-modal normalization technique RegBN to reduce redundant dependencies between multiple modalities for enhanced diagnosis accuracy.

Robustness of DiaMond evaluated on three distinct datasets for AD prediction and differential diagnosis of AD and FTD. DiaMond is the first method to leverage deep neural networks to enhance differential diagnosis of dementia with MRI and PET.

SECTION: Related Work
Recent studies have advanced AD diagnosis by combining structural MRI and FDG-PET through various DL approaches.
Different fusion strategies have been explored, including early, middle, and late fusion.
Luet al.use a multiscale deep neural network to fuse the extracted patch-wise 1D features from MRI and PET.
Liuet al.propose a cascaded framework including multiple deep 3D-CNNs to learn from local image patches and an upper 2D-CNN to ensemble the high-level features.
Fenget al.combine a 3D CNN and LSTM with a late fusion of MRI and FDG-PET for AD diagnosis.
Huanget al.propose an early and a late fusion approach for the two modalities based on a 3D-VGG.
Linet al.first propose a 3D reversible GAN for imputing missing data and then use a 3D CNN to perform AD diagnosis with channel-wise early fused MRI and PET data.
Wenet al.introduce an adaptive linear fusion method for MRI-PET fusion based on 2D CNNs.
Songet al.propose an early fusion approach by overlaying gray matter (GM) tissues from MRI with the FDG-PET scans and feeding them into a 3D CNN for classification.
However, after investigating several multi-modal methods with 3D CNN across image-level early, middle, and late fusion of MRI and PET, Narazaniet al.find that the diagnostic performance of these existing multi-modal fusion techniques may not yet outperform that of using PET alone for AD diagnosis.

As Transformers and attention mechanisms have shown promising results in various medical imaging tasks, recent applications have started to integrate them for multi-modal feature fusion with CNNs as encoders for feature extraction.
Liet al.combine a CNN and a Transformer module for multi-modal medical image fusion.
Zhanget al.propose an end-to-end 3D ResNet framework, which integrates multi-level features obtained by attention mechanisms to fuse the features from MRI and PET.
Gaoet al.introduce a multi-modal Transformer (Mul-T) using DenseNet and spatial attention for global and local feature extraction, followed by cross-modal Transformers for T1-, T2-MRI, and PET fusion.
Zhanget al.use MRI and PET for dementia diagnosis by first employing adversarial training with CNN encoders for feature extraction, then applying Transformers through the cross-attention mechanism for feature fusion and finally classification with a fully-connection layer.
Miaoet al.propose a multi-modal multi-scale transformer fusion network (MMTFN), combining CNN-based residual blocks and Transformers to jointly learn from multi-modal data for diagnosing AD.
Tanget al.first employ a 3D CNN to extract deep feature representations of structural MRI and PET images, then utilize an improved Transformer to progressively learn the global correlation information among features.
Note that these recent approaches typically combine CNNs for initial feature extraction with ViTs for feature fusion, which may not fully leverage the capabilities of ViTs for multi-modal learning.

Despite the extensive research focused on AD diagnosis, there has been limited exploration in multi-modal learning for differential diagnosis of dementia, which holds significant importance in clinical practices.
Current research in differential diagnosis primarily adopts machine learning algorithmsor focuses on single modality, yet the potential of employing multi-modal deep neural networks for this purpose remains unexplored.

SECTION: Proposed Method
presents the main steps of, a multi-modal ViT-based framework incorporating multiple attention mechanisms for effective dementia diagnosis. We begin by outlining the foundational concepts underlying our approach and subsequently delve into the specifics of each component.

SECTION: Preliminaries
Let’s denote the 3D MRI and PET images asandrespectively, with height, width, and depth. The objective is to classify the given multi-modal data into a set oflabels. In this study,includes CN (Cognitively Normal), MCI (Mild Cognitive Impairment), AD, and FTD labels.
MRI and PET data exhibit inherent dependencies that can be introduced during data collection and revealed in high-dimensional feature space. Hence, we divide the data space into three non-overlapping regions denoted by, as highlighted in.
Our proposed framework DiaMond consists of three branches based on pure ViTs to process each of the data space:

maps input MRI from data space(Fig.) into the latent encodingwith length;

maps PET from data spaceinto the latent encodingof length;

receives both MRI and PET, then captures their shared information in data space, and finally maps those to the latent encodingof length.

We extract regionsusing self-attention mechanisms, together with a recently-developed normalization technique RegBNto ensure feature independence. A novel bi-attention mechanism is introduced to explore the similarities between the two modalities in region.

SECTION: Self-Attention for Single Modality
andoperate over single modalities separately with self-attention mechanisms.
Letandbe partitioned intovoxel patches, wheredenotes the length of the feature embedding, andthe number of input patches.
The input patchis projected into the query, key, and valuematrices, giving:

Letdenote transpose, the self-attentionfor a single modalityis defined as

Each self-attention branch aims to independently extract unique features from one input modality. To ensure that each branch efficiently identifies distinct modality-dependent features, a normalization technique RegBNis later applied to the latent space, aiming to reduce redundant partial dependency between the self-attention branches.

SECTION: Bi-Attention for Multiple Modalities
A novel bi-attention mechanismis introduced into compute the interweaved attention between two modalities, uniquely designed to focus on capturing their similarities in the high-dimensional feature space:

where,, and.is the indicator function to threshold the correlation matrix between the features of the two modalities to be above a constant threshold.
Note that. Illustrated in, the bi-attention blocks aim to produce features for each modality conditioned on the other, targeting on their potential disease-specific similarities.
Distinct from the conventional cross-attention mechanism, which captures both similarities and dissimilarities between patches in the correlation matrices, our proposed bi-attention selectively preserves only the pronounced similarities, resulting in a sparse representation. This sparsity is achieved by applying a constant thresholdto the correlation matrix between the query and key matrices from two modalities, filtering out negligible values.
This mechanism enables efficient capture of dependencies between modalities for improved diagnostic accuracy and robustness, as validated in Sec.&.

SECTION: RegBN for Dependency Removal
RegBN is a normalization method devised for dependency and confounding removal from low- and high-level features before fusing those.
As discussed earlier, MRI and PET input images are mapped into latent space, represented asfor MRI andfor PET.
Dependencies between modalities are present during image acquisition; thus, such information can be transferred to the latent space, as illustrated byin Fig.. Since the proposed bi-attention block focuses explicitly on the underlying similarities between input modalities via the self-attention modules, it is essential to eliminate redundant shared information between them. Otherwise, the neural network may primarily optimize within the overlapped region, increasing the risk of getting trapped in local minima. Thus, we use RegBN to separate the latent encodingfrom.

RegBN represents one latent encoding in terms of another using a linear regression model:

in whichis a projection matrix, anddenotes the difference between the input latent encodings, so-called residual. The residual segment contains a portion ofthat is independent from.
RegBN uses the Frobenius norm as a regularizer to minimizeover mini-batches. As demonstrated in, RegBN enables a neural network to integrate data from all three non-overlapping regions, leading to improved diagnosis performance. The issue of falling into the redundant overlapping region regularly occurs in multi-modal data learning, yet is often overlooked despite its potential side effects.

In the final step, the attention maps from all three branches,, andpass through the feed-forward network and yield their latent feature vectors respectively.
All latent vectors of the three branches are then summed and unified through an MLP layer to obtain the diagnostic label, i.e.,.

SECTION: Backbone Architecture
We adopt a 3D multiple instance neuroimage Transformeras our backbone, while it is straightforward to generalize the backbone to any 3D ViT architecture.
The adopted backbone is a convolution-free Transformer-based architecture inspired by the multiple instance learning paradigm. Given the input image, the backbone first splits it intonon-overlapping cubiform blocks, where,, anddenotes the block size. Eachis then treated as one instance to be fed into the Transformer encoder independently. Learned block positional embeddings will be added to each, aiming to preserve the positional information of each block within the whole image.
The Transformer encoder consists ofTransformer layers. Each of the layers contains either a multi-head self-attention block or a multi-head bi-attention block, followed by a feed forward network including two linear projections with Gaussian Error Linear Units (GELU) non-linearityapplied in between. We apply layer normalizationbefore each block, and residual connections after each block within a Transformer layer.

SECTION: Experiment Setup
We use paired FDG-PET and T1-weighted MRI scans from three datasets:

Alzheimer’s disease neuroimaging initiative (ADNI) database, including 379 CN, 257 AD, and 611 MCI samples.

Japanese Alzheimer’s Disease Neuroimaging Initiative (J-ADNI) database, including 104 CN, 78 AD, and 124 MCI samples.

In-house clinical dataset containing two types of dementia, with 143 CN, 110 AD, and 57 FTD samples, from Klinikum rechts der Isar, Munich, Germany.

Due to the lack of a public multi-modal dataset for both AD and FTD subjects, we evaluate the differential diagnosis on a well-characterized, single-site in-house clinical dataset from Klinikum rechts der Isar, Munich, Germany.reports statistics of all used datasets.
Only scans from baseline visits are selected. We extract the gray matter (GM) density maps from MRI as input. All scans were normalized, registered to the MNI152 template withvoxel size, and rescaled to the intensity range between 0 and 1, with the final size of 128 × 128 × 128.

To avoid biased results due to data leakage and confounding effects, we split the data using only baseline visits and ensure that diagnosis, age, and sex are balanced across sets. Further information on data preprocessing and splitting can be found in Sec. A.1.

First, we compare DiaMond against single-modality inputs by employing a 3D ResNetor a 3D neuroimage ViTas the backbone.
Subsequently, we compare alternative multi-modal fusion techniques, including early, late, and middle fusion, using both backbones. In the end, we compare with the latest fusion methods Mul-Tand MMTFN.

The ViT-based backbone of DiaMond adopts a patch size of 8, a feature embedding dimension of 512, 8 attention heads, a model depth of 4, and a dropout rate of 0.0.
These parameters were selected after an exhaustive search and comparison using the validation set, with detailed information and results presented in.

We implement models with PyTorch, using the AdamWoptimizer with a learning rate of, a weight decay of, a batch size of, and cosine annealing as the learning rate scheduler. The models are trained on one NVIDIA A100 GPU with 40 GByte memory for 3,800 iterations, with early-stop to prevent overfitting. Tab. A.2
reports the detailed hyperparameters.

We perform 5-fold cross-validation, partitioning each dataset into training, validation, and test sets with ratios of 65%, 15%, and 20% respectively. We train all models for three different tasks:

Binary classification of healthy controls (CN) vs. patients with AD.

Three-way classification of CN vs. MCI vs. AD.

Three-way classification of CN vs. AD vs. FTD.

We use balanced accuracy (BACC) and area under the ROC curve (AUC) to evaluate the results for binary classification. For the three-way classification, we adopt BACC, F1-Score, Precision, and Recall. In addition, we illustrate the fairness evaluation on DiaMond across different demographics.

SECTION: Results
SECTION: Alzheimer’s Prediction
We compare DiaMond with other baseline methods for the task of Alzheimer’s prediction on two different datasets.reports the results for both binary (CN vs. AD) and three-way (CN vs. MCI vs. AD) classification on ADNI and J-ADNI datasets, respectively. For binary classification, PET scans generally yield better results than MRI as a single modality input, reaching a BACC of 89% on ADNI and 89.4% on J-ADNI, whereas MRI only achieves a BACC of 86.6% on ADNI and 84.7% on J-ADNI at the largest. Early, middle, and late fusion methods, together with Mul-Tand MMTFN, regardless of using ResNet or ViT as the backbone structure, can only achieve on-par performance as PET alone, which is aligned with the conclusion in, as the diagnostic performance of existing multi-modal fusion methods may not yet outperform that of using PET alone.
On the contrary, DiaMond outperforms all other methods and single modality input by a large margin for both datasets, achieving a BACC of 92.4% in ADNI, and 91.7% in J-ADNI.

As for the three-way classification between CN, AD, and MCI, despite an overall performance decline due to the complexity of MCI as a syndrome, DiaMond consistently surpasses all other methods, achieving a notable BACC of 65.2% on the ADNI dataset.
The results highlight the effectiveness of the multi-modal fusion mechanisms employed in DiaMond. The high and consistent accuracy is likely achieved through the efficient feature extraction and integration enabled by the attention mechanisms in our ViT-based framework.

SECTION: Differential Diagnosis of Dementia
Further, we conduct a three-way differential diagnosis of dementia, between subjects of CN, AD, and FTD. This task is challenging due to the overlapping symptoms between different types of dementia; however, it is highly important due to its distinctive clinical value.reports the results of DiaMond and other baseline methods on this task.

When using single modalities as input, MRI only achieves a BACC of 56.7% with ResNet, and 66.0% using ViT as backbone. PET can achieve a BACC of 68.7% with ResNet and 69.5% with ViT, confirming its higher sensitivity in the differential diagnosis. Using multi-modal input of both MRI and PET can elevate the diagnostic accuracy compared to using single modality alone, particularly when employing the late fusion strategy using either ResNet or ViT, achieving a BACC of 73.2% and 74.1%, respectively.
Recent fusion methods Mul-Tand MMTFNcombine CNNs for feature extraction with ViTs for fusion, however, their performance falls short compared to pure ViT-based late fusion.
Notably, utilizing ViT as the backbone consistently outperforms ResNet in differential diagnosis, whether employing a single modality or multiple modalities as input.
In the end, applying DiaMond further boosts the diagnostic accuracy to more than 2%, reaching the highest BACC of 76.5%. These outcomes confirm the efficacy of Transformers, particularly the efficient use of attention mechanisms in DiaMond, in effectively integrating multi-modal data for the challenging task of differential diagnosis of dementia, highlighting its substantial clinical value.

SECTION: Fairness Evaluation
Ensuring fairness is a paramount consideration in the domain of medical imaging. DL models employed in medical applications must minimize biases towards specific demographic groups, such as age, gender, and diagnostic labels. In this regard, we evaluate the fairness of the diagnostic results produced by DiaMond on the ADNI dataset for AD prediction, by examining its test accuracy across diverse patient cohorts.
The results presented inindicate that DiaMond achieves minimal variance in the diagnostic accuracy across different demographic categories, suggesting a uniform and equitable performance.

SECTION: Ablation Study
We conduct a comprehensive ablation study on the important components in DiaMond. This includes evaluating the inclusion of different ViT branches (self- and bi-attention), the integration of RegBN, and the application of the attention thresholdin our bi-attention design. In the end, we include the ablation on the network parameters.

SECTION: Different Branches
DiaMond comprises three independent ViT branches:with MRI as input,with PET, andreceives both MRI and PET as input simultaneously.
To validate the efficacy of the three branches in DiaMond, we conduct ablation studies on each of the branch and their different combinations. As shown in, usingoralone achieves a BACC of 86.2% and 88.8%, respectively, indicating that PET is slightly more effective as a single modality input. The model achieves a BACC of 90.87% when it relies solely on thebranch, suggesting the high benefits from our introduced bi-attention mechanism for integrating multiple modalities. The efficacy of the combination of multiple attention mechanisms is further evidenced whenis combined alongside eitheror, resulting in BACC scores of 91.0% and 91.5%, respectively. Finally, combining all three branches, as in our final DiaMond framework, achieves the highest BACC of 92.4%, confirming the synergistic effect of DiaMond with self- and bi-attention interaction.

SECTION: Integration of RegBN
RegBN is incorporated into DiaMond as a normalization technique to make self-attention branches independent, aiming to reduce the redundant partial dependency between the input modalities.
We further evaluate the impact of RegBN in our model. As shown in, the performance of DiaMond is affected by the presence of RegBN across all three datasets, with this normalization method enhancing classification results by up to 2%.
This improvement underscores two key issues in multi-modal classification. First, partial dependency and confounders in multi-modal data can mislead a classification neural network, causing it to fall into the overlapped region. Second, RegBN demonstrates a unique ability to counteract the negative impact of overlapping data, thereby contributing to a more efficient and robust framework.
As discussed in previous sections, the classification of multi-modal data may differ from that of single-modal data due to the heterogeneous nature of multi-modal data sources, which can exhibit positive or negative correlations affecting the distributions of learned features.
This study demonstrates and substantiates that accounting for the data space of each modality in classification leads to significant improvements in results. This topic has received limited attention and study thus far. Importantly, recent deep learning methods have shown an increased focus on multi-modal analysis, and the strategy employed in our model for managing multi-modal data demonstrates significant potential for future research.

SECTION: Bi-Attention Threshold
We use a constant thresholdinto filter out very small values in the correlation matrices within the bi-attention block, so that it focuses primarily on similarities between modalities. As illustrated in, avalue of approximately 0.01 typically results in better and more stable performance. In contrast, having no threshold (, equals to a conventional cross-attention mechanism) or setting the threshold too high causes a drop in performance or high variation in the outcomes. Thus, including an optimal attention threshold is crucial for the bi-attention block, as it reduces the redundancy of learning repetitive features as captured in the self-attention blocks, and efficiently helps to focus on the dependencies between modalities.

SECTION: Network Parameters
We conduct ablation studies on the network parameters, performing an exhaustive search over the following parameters: patch size in {4, 8}, embedding dimension in {128, 256, 512, 1024}, number of attention heads in {8, 16}, model depth in {1, 2, 4, 8}, and dropout rate in {0, 0.2, 0.5}.
We use the validation set to compare different configurations, with the results presented in. As a result, the combination of a patch size of 8, a feature dimension of 512, 8 heads, a depth of 4, and a dropout rate of 0.0 yields the highest validation accuracy. Therefore, we adopt these parameters for all experiments.

SECTION: Conclusion
We introduced DiaMond, a ViT-based framework for Alzheimer’s prediction and differential diagnosis of dementia using MRI and PET. DiaMond effectively learns from multi-modal data via self- and a novel bi-attention mechanism from a pure ViT backbone. The self-attention mechanism extracts distinct features from individual modalities, along with a normalization strategy to ensure feature independence; our novel bi-attention mechanism exclusively focuses on the similarities between multiple modalities, aiming to capture their disease-specific dependency. Across three distinct datasets, DiaMond consistently outperformed all competing methods, achieving a balanced accuracy of 92.4% for AD-CN classification, 65.2% for AD-MCI-CN classification, and 76.5% for differential diagnosis between AD, FTD, and CN subjects, highlighting its significant clinical value. We evaluated the fairness of our model, which indicated equitable performance across various demographic groups.
Our comprehensive ablation study validated DiaMond’s robustness and synergistic effect of integrating multiple modalities with its intricate design.
Overall, DiaMond demonstrated that leveraging the attention mechanisms in vision Transformers offers superior fusion compared to CNNs, enabling the combination of MRI and PET to significantly surpass the accuracy of PET alone.

SECTION: Acknowledgements
This work was supported by the Munich Center for Machine Learning (MCML) and the German Research Foundation (DFG).
The authors gratefully acknowledge PD Dr. Igor Yakushev and PD Dr. Dennis M. Hedderich from Klinikum rechts der Isar (Munich, Germany) for their invaluable provision of the in-house clinical data, as well as the Leibniz Supercomputing Centre for providing the computational and data resources.

SECTION: References