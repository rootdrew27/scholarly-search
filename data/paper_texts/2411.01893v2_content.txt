SECTION: A Global Depth-Range-Free Multi-View Stereo Transformer Network with Pose Embedding
In this paper, we propose a novel multi-view stereo (MVS) framework that gets rid of the depth range prior. Unlike recent prior-free MVS methods that work in a pair-wise manner,
our method simultaneously considers all the source images.
Specifically, we introduce a Multi-view Disparity Attention (MDA) module to aggregate long-range context information within and across multi-view images.
Considering the asymmetry of the epipolar disparity flow, the key to our method lies in accurately modeling multi-view geometric constraints.
We integrate pose embedding to encapsulate information such as multi-view camera poses, providing implicit geometric constraints for multi-view disparity feature fusion dominated by attention.
Additionally, we construct corresponding hidden states for each source image due to significant differences in the observation quality of the same pixel in the reference frame across multiple source frames.
We explicitly estimate the quality of the current pixel corresponding to sampled points on the epipolar line of the source image and dynamically update hidden states through the uncertainty estimation module.
Extensive results on the DTU dataset and Tanks&Temple benchmark demonstrate
the effectiveness of our method.
The code is available at our project page: https://zju3dv.github.io/GD-PoseMVS/.

SECTION: Introduction
Multi-view stereo matching (MVS) is a crucial technique in 3D reconstruction, which aims to recover robust and reliable 3D representations from multiple RGB images. Traditional methodsrely on hand-crafted similarity metrics and regularizations to compute dense correspondences between the input images. These methods are prone to degradation in challenging scenarios, such as varying illumination, textureless regions, and occlusion regions.
Recently, learning-based methodsdirectly learn discriminative features from the input images through neural networks such as CNN and Transformers. By sampling some possible depth hypothesis within a given depth range, they warp the features from the source images to the reference view (i.e., the plane sweep algorithm) and compute the cost volume, which is then regularized also through the neural network to obtain the final depth maps.
However, obtaining a suitable depth range is non-trivial when applied in real-world scenarios while these methods are generally sensitive to the depth range, which limits their application.

To get rid of the dependence on depth range, some methodstransform the regression problem in the given depth space into a matching problem on the epipolar lines. Similar to optical flowand feature matching, these methods also adopt a pair-wise manner. For example, DispMVScomputes the depth map of the source image multiple times through pairs that contain different source images and computes the final depth map by weight of sum. However, the pair-wise manner neglects the inter-image correspondence between the source images and could lead to sub-optimal solutions.
Meanwhile, although DispMVS mitigates the influence of depth priors on constructing the 3D cost volume, its initialization based on depth range can still lead to significant performance degradation when the depth range error is too large, as shown in Fig..

We argue that these methods need to consider all the source images at the same time. Our ideas are inspired by the recent methodsof optical flow which concurrently estimate optical flows for multiple frames by sufficiently exploiting temporal cues.
However, we find these frameworks cannot be trivially applied in the task of multi-view stereo.
The reasons are twofold.
First, a strong cue in the multi-frame optical flow estimation is that the flow originating from the same pixel belongs to a continuous trajectory in the temporal dimension. Additionally, the frames are sequentially aligned along this temporal dimension. Such inductive bias makes it easy to learn. But in the context of multi-view stereo, the source images may be captured in no particular order, lacking a similar constraint of continuity. Unlike optical flow, the input images in multi-view stereo are unordered. These distinctions pose a significant challenge when attempting to adapt the multi-frame optical flow framework for use in multi-view stereo.
Second, the arbitrary positions and viewing angles of the source images, coupled with potentially large temporal gaps between captures, exacerbate issues such as varying illumination, significant viewport differences, and occlusions which call for new designs.

Based on the above observations, in this paper, we propose a novel framework that gets rid of the depth range assumption. Unlike some recent methodsthat work in a pair-wise manner, the proposed method estimates the depth maps of a reference image by simultaneously considering all the source images.
To address the first issue, we design careful injection of geometric information into disparity features using 3D pose embedding, followed by multi-frame information interaction through an attention module.
Subsequently, we encode multi-view relative pose information and geometric relationships between specific sampled points into 3D pose embedding, which is subsequently transferred to the Multi-view Disparity Attention (MDA) module.
This method efficiently incorporates the relationship between depth and pixels within the network, facilitating improved information integration across multiple frames.
Second, to mitigate the challenge of fluctuating image quality stemming from occlusion and other factors, we maintain and update the disparity hidden features to reflect the depth uncertainty of the current sampling point for each iteration.
We design the disparity feature encoding module to learn disparity features along the epipolar lines of multi-view frames.
This approach enables us to explicitly characterize occlusion scenarios for each pixel across diverse source images and dynamically adapt them during epipolar disparity flow updates. Consequently, the auxiliary information is furnished for subsequent information fusion within the module.
Furthermore, we designed a novel initialization method to further eliminate the influence of the depth range compared to DispMVS.

In summary, our contributions can be highlighted as follows: (1) A multi-view disparity transformer network, which facilitates the fusion of information across multi-view frames, (2) A specially designed 3D pose embedding which is utilized to implicitly construct relationships of the epipolar disparity flow among multi-view frames, and (3) An uncertainty estimation module and dynamically updated hidden states representing the quality of source images during iterations. We evaluate our method against other MVS methods on the DTU datasetand Tanks&Temple dataset, and demonstrate its generalization in Fig..

SECTION: Related Work
SECTION: Traditional MVS
Multi-View Stereo has been developed for many years and has many downstream or related applications such as simultaneous localization and mapping (SLAM), visual localization, 3D reconstruction, 3D generationand scene understanding.
Traditional methods for Multi-View Stereo (MVS) can generally be categorized into three classes: volumetric, point cloud-based, and 2D depth map-based methods.
Volumetric methodstypically partition the 3D space into voxels and annotate them as either interior or exterior to the object surface.
Point cloud-based methodsdirectly optimize the point cloud coordinates of objects in 3D space.
Depth map-based methodsfirst estimate 2D depth corresponding to images and then fuse the 2D depths of the same scene to obtain a 3D point cloud.
However, these traditional methods remain constrained by manually crafted image features and similarity matrices.

SECTION: Deep learning based MVS
generally leverage convolutional neural networks to construct and refine 3D cost volume.
For instance,uses isotropic and anisotropic 3D convolution-based learning networks to estimate the depth map.introduces a pixel-wise network to obtain visibility.applies a multi-stage CNN framework to enable reconstruction.andbuild a kind of pyramid to realize 3D cost volume.
Similarly,proposes a sparse-to-dense CNN framework when constructing the 3D cost volume.

mainly exploit recurrent network structuresto regularize 3D cost volume.
For example,utilizes recurrent encoder-decoder structure and 2D CNN framework to solve large-scale MVS reconstruction.introduces a scalable RNN-based MVS framework.
IterMVSuses a GRU-based estimator to encode the probability distribution of depth.
Compared with 3D CNN, RNN highly reduces the memory requirement, which makes it more suitable for large-scale MVS reconstruction.

is popular in 3D vision tasks, and first introduced into the field of MVS reconstruction bydue to its ability to capture global context information. 
Transformer is incorporated into feature encodingto capture features within and between input images.
The succeeding workimplements a transformer to assign weights to different pixels in the aggregating process.employs an Epipolar Transformer to perform non-local feature augmentation.
However, these deep learning-based MVS methods commonly exhibit sensitivity to the depth range, thereby restricting their broad applicability.

infer the depth information from the movement along epipolar lines to reduce the heavy dependence of depth range priors.
Several methodsperform 2D sampling between two frames and iteratively update flows to find the matching points.
Specifically, DispMVSis randomly initialized within the depth range and performs depth fusion by utilizing a weighted sum. RAMDepthselects a random source image in each iteration.
However, both methods fail to fully exploit multi-frame constraints during the flow updates due to the mismatch of 3D information at sampling points.
In this paper, we enhance the epipolar matching process by simultaneously considering multi-frame information.

SECTION: Method
Given a reference imageand multi-view source imagesas input, the task of MVS is to calculate the depth map of the reference image.
We treat MVS as a matching problem: for a pixel pointin the reference image, we identify the corresponding pointin the source image, then we can get depth by triangulation.
Given the initial matching pointobtained by the initial depth, we adopt an iterative update strategy.
Since the matching point lies on the epipolar line of the source image, the one-degree-of-freedom epipolar disparity flow is used to represent the network’s iterative updates.
The epipolar disparity flowis 1-d flow along the epipolar line on the source image during each iteration:

whereis the normalized direction vector of the epipolar line,is the dot product of vectors, andis the iteration time.

Different from previous methods,
we fully eliminate the dependence on depth range during initialization and achieve synchronous updating of the epipolar disparity flow across multi-view images.
This is done by our design of disparity information interaction.

The overall pipeline of our method is illustrated in Fig..

The proposed method starts from a feature extraction module to extract multi-scale features (Sec.).
Then, we discuss how to initialize the depth map without depth range (Sec.) and perform feature encoding (Sec.).
To facilitate information fusion across multi-view source images, we introduce the Multi-view Disparity Attention (MDA) module (Sec.), enhanced with Pose Embedding.
Finally, the features enhanced by the MDA module are fed into a GRU module to update the epipolar disparity flow, as described (Sec.), which is then fused to generate the depth map.

SECTION: Feature Extraction
Following previous methods, we employ convolutional neural networks (CNNs) for image feature extraction.
Moreover, we adopt a coarse-to-fine strategy to extract multi-scale image features.
Specifically, we utilize two share-weighted feature extraction modules to extract image featuresandand a context feature extraction module to extract context features.

SECTION: Initialization
Differing from DispMVS, we design a novel initialization method without depth range to further mitigate the influence of depth priors.
Specifically, we select an initial position along the epipolar line and then convert it into the depth map.
First, we derive the correspondence between depth and position along the epipolar line.
Given a pixelof the reference image, the geometric constrain between it and the warped pixelof the source imagecan be written as:

wheredenotes the depth in reference view,denotes the depth in source view.anddenote the rotation and translation between the reference and the source view,anddenote the intrinsic matrices of the reference and the source view.
Let,and, we can associateandwith pixel coordinates:

whereis a 2D flow vector along the epipolar line that provides flow in the x dimensionand y dimension.
To obtain an appropriate initial position, we first determine the geometrically valid range along the epipolar line, which has not been considered in other works.
If a point is observable in the current view, it must have physical significance, meaning it must lie in front of the camera.
Therefore, we identify the search range along the epipolar line on the source image that satisfies the condition.
We obtain the initial positionby selecting the mid-point in search range along epipolar line.

SECTION: Disparity Hidden State Based Feature Encoding
Due to occlusion, moving objects, blurring, or other factors violating the multi-view geometry assumptions, the quality of sampling points from different source images varies, which limits the network’s performance in depth estimation.
To address this issue, we extract uncertainty information from the sampling point feature and encode it with cost volume as epipolar disparity feature.
As shown in Fig., we design the disparity hidden stateto maintain the sampling information of the current source image and update it during iterations by incorporating new uncertainty information.

For each source image, after determining the positionfor the current iteration, we uniformly samplepoints aroundalong the epipolar line at each scale with a distance of one pixel. 
By constructing a 4-layer pyramid feature using average pooling, uniform pixel sampling at different levels allows for a larger receptive field. The sampling interval in 2D is fixed.
Given image featuresand, we obtain the features ofsampled points in the source image through interpolation and calculate the visual similarity.
The cost volumeis constructed by computing the dot product between pairs of image feature vectors:

whererepresents the set of sampling points uniformly sampled along the epipolar line in the source image, anddenotes the number of sample points.

When estimating the epipolar disparity flow from multi-view frames, it is essential to encode the differences between source images caused by variations in occlusion situations and image quality.
Motivated by this, we conduct disparity hidden stateto explicitly represent the situation of pointrelative to the source image.
Motivated by this, we introduce a disparity hidden stateto explicitly represent the condition of points relative to the multi-view source images.is randomly initialized and consecutively updated throughout the iterative process.
We introduce a variance-based uncertainty estimation module to encode the correlation features, which is formulated as follows:

wheredenotes the cost volume of source image,denotes the average value of, andis the sigmoid function.
Then, the uncertainty, the disparity hidden state of the previous iteration, the correlation features and the epipolar disparity flows are fed into the convolutional layers to generate epipolar disparity featureand update the disparity hidden state.

SECTION: Multi-view Stereo Transformer
DispMVS estimates the epipolar disparity flow from each two-frame image pair, which overlooks the abundant multi-view information.
Inspired by VideoFlow, we estimate the epipolar flow of multi-view images simultaneously.
However, since multiple source images are not sequentially arranged and
points uniformly 2D sampled across source images can not establish robust 3D spatial correspondences,
directly learning the continuity between flows, as, does not work.

Therefore, unlike, etc., we design some special structures for information aggregation among multi-view images.
Although the depths of sampled points along epipolar lines do not correspond, we observe that there is a regular pattern in the direction of depths along epipolar lines.
As shown in Fig., we design Multi-view Disparity Attention to learn the global information and utilize pose embedding to implicitly model the correspondence between pixel coordinates and depth on multiple source images, enabling the network to learn the direction and scale relationship of corresponding flows across different source images.

To effectively capture extensive global information across epipolar disparity features from different views, we leverage the Multi-view Disparity Attention (MDA) module to further enhance the disparity features.
We utilize an attention module to globally interact with disparity features of multi-view source images, thereby achieving multi-view feature fusion.

Given epipolar disparity features, we first use self-attention to achieve intra-image information interaction.
We concatenate epipolar disparity featuresand setthe as sequence length, generating.

Then we use cross-attention to achieve inter-frame information interaction and learn the relations among multi-view.
We concatenate epipolar disparity featuresand set the number of source imagesthe as sequence length, generating.

To reduce computation cost, for the self-attention we use a linear transformer to compute attention, which replaces the original kernel function with:

whereandrepresents the activation function of exponential linear units.

Due to the depths of sampling points varying for different source images, we utilize pose embedding to construct implicit disparity relationships among multi-view frames.
To effectively convey useful information to the attention module, we categorize the features of pose embedding into two types: multi-view relative pose information and geometric information between specific sampled points.
Fig.illustrates the variables used to construct the pose embedding.

On one hand, the multi-view relative pose information between cameras contains crucial information about disparity features.
By explicitly injecting relative poses into the attention module, the network can learn image-level geometric constraints.
We represent the anglebetween rays as embedding.
Inspired by, we encode the rotation matrix and translation matrix between the reference and the source view into the relative pose distance:

On the other hand, we encode the geometric information between specific sampled points.
Due to our incorporation of pixel-level attention in addition to inter-frame attention, it is necessary to encode not only image-level camera poses but also the pixel-level information corresponding to sampled points.
It is important to note that for each pixel in the reference image and its corresponding sampled point in the source image, we can obtain the corresponding 3D point coordinatesthrough triangulation based on stereo geometry.
Accordingly, we encode the 2D coordinatesof the source image, the depthfrom the perspective of the reference image, and the depthfrom the perspective of the source image, thereby transforming the 3D information into corresponding relationships on the 2D plane.
Moreover, we encode the normalized directionto the 3D location of a point.

SECTION: Iterative Updates
In the GRU updating process, we iteratively update the epipolar disparity flowobtained from the MDA module for each source image.
In each iteration, the input to the update operator includes 1) the hidden state; 2) the disparity feature output from the MDA module; 3) the current epipolar flow; and 4) the context feature of the reference image.
The output of the update operator includes 1) a new hidden state; 2) an increment to the disparity flow; and 3) the weight of disparity flow for multi-view images.
We derive the depth from the disparity flow and employ a weighted sum to integrate the depth across multi-view source images.
After fusion, the depth is converted back to disparity flow to perform the next iteration.

SECTION: Loss Function
Similar to, we output depth after each iteration and construct the loss function accordingly. We construct the depth L1 loss. The loss function is represented in Eq.:

where,are iterations at the coarse and fine stage,is a hyper-parameter which is set to 0.9.

SECTION: Experiments
In this section, we first introduce the datasets (Sec.), followed by the implementation details of the experiment (Sec.). Subsequently, we delineate the experimental performance (Sec.) and conduct ablation experiments to validate the efficacy of each proposed module (Sec.).

SECTION: Datasets
DTU datasetis an indoor multi-view stereo dataset captured in well-controlled laboratory conditions, which contains 128 different scenes with 49 views under 7 different lighting conditions. Following MVSNet, we partitioned the DTU dataset into 79 training sets, 18 validation sets, and 22 evaluation sets. BlendedMVS datasetis a large-scale outdoor multi-view stereo dataset that contains a diverse array of objects and scenes, with 106 training scenes and 7 validation scenes. Tanks and Templesis a public multi-view stereo benchmark captured under outdoor real-world conditions. It contains an intermediate subset of 8 scenes and an advanced subset of 6 scenes.

SECTION: Implementation Details
Implemented by PyTorch, two models are trained on the DTU dataset and large-scale BlendedMVS dataset, respectively. On the DTU dataset, we set the image resolution asand the number of input images as 5 for the training phase. On the BlendedMVS dataset, we set the image resolution asand the number of input images as 5 for the training phase. For all models, we use the AdamW optimizer with an initial learning rate of 0.0002 that halves every four epochs for 16 epochs. The training procedure is finished on two A100 with,.
For depth filtering and fusion, we process 2D depth maps to generate point clouds and compare them with ground truth.

SECTION: Experimental Performance
In this section, we compare our method with other state-of-the-art methods and scale-agnostic methods.
Existing methods are categorized into traditional methods, 3D cost-volume methods, RNN-based methodsand scale-agnostic methods.
Methods that leverage scene depth range have an advantage as they can utilize accurate and robust information, thereby mitigating outliers, especially in textureless regions.

We evaluate the proposed method on the evaluation set of DTU dataset. We set the image resolution asand the number of input images as 5.
As shown in Table, our method has the best overall performance among depth-range-free methods.
CER-MVSand MVSFormer++demonstrate superior performance; however, they are heavily dependent on the accuracy of the depth range.
Our approach outperforms when compared with depth range-free methods like DispMVSand RAMDepth, which demonstrates the effectiveness of our method in exploiting correlations among multi-view frames.

Since the Tanks and Temples dataset does not provide training samples, we use a model pre-trained on the BlendedMVS dataset for testing. We set the image resolution asand the number of input images as 7 for the evaluation phase.
Tablepresents the comparison between our method and other state-of-the-art methods.
Our method achieves the best performance among scale-agnostic methods.
Since RAMDepthhas not provided results on the Tanks and Temples dataset and source code, we are unable to make a comparison.
Although our method exhibits a certain gap when compared to state-of-the-art methodsbased on precise depth priors, it demonstrates superior robustness across a broader depth range.

We visualize point clouds generated on DTU and Tanks and Temples dataset in Fig., which demonstrates that our method is capable of constructing a comprehensive and precise point cloud.

SECTION: Ablation Study
In this subsection, we conduct ablation studies of our model trained on DTUdatasets to discuss the effectiveness of core parts of our method.
The implemented baseline is basically based on DispMVS.
All the experiments are performed with the same hyperparameters.

We conducted ablation experiments to validate the effectiveness of the pose embedding. 
Specifically, within the multi-view attention module, we remove 3D pose embedding and retain only the original 2D position encoding of attention.
As shown in Table, after applying pose embedding, the overall performance improves by 9.46%.
The result indicates that the current task heavily relies on the relative pose and geometric information contained in the pose embedding.
Without incorporating geometric constraints across multi-view source images, typically achieved through pose embedding, the performance of Transformer in this task may degrade significantly.

Following, we further attempt to remove uncertainty estimation and disparity hidden state and directly perform feature encoding on disparity flow and cost volume.
As shown in Table, with the disparity feature encoding coupling with uncertainty, the overall performance improves by 9.95%.
The result validates the effectiveness of the module, demonstrating that explicitly estimating the quality of sampled points on the epipolar line of source images and updating the disparity hidden state in the network is effective.
Additionally, we designed two separate ablation experiments, removing the uncertainty and disparity feature hidden states, to further evaluate the impact of these two modules on the network.
The uncertainty and disparity feature hidden states improved the overall performance by 6.16% and 7.46%, respectively.
Compared to the performance without disparity feature encoding coupling with uncertainty, this demonstrates the effectiveness of the uncertainty and disparity feature hidden state updating modules.

SECTION: Depth Range
In this section, we compare the generalization of different networks to depth range.
We don’t compare with RAMDepthin the ablation studies due to the lack of its source code.
We select several state-of-the-art methods (GeoMVS, MVSFormer++), RNN-based methods (IterMVS, CER-MVS) and depth-range-free method (DispMVS) to conduct experiments to evaluate the generalization of depth range.
Our main comparison is with depth-range-free methods, which reduce dependence on depth priors through network design.

All methods are trained on DTU dataset with the same depth range and subsequently inference under different depth ranges.

For methodsthat rely on depth range prior for depth sampling, whether based on RNN or Transformer, they may exhibit better performance with accurate depth priors.
However, as shown in Table, there is a marked decline in performance for these methods with larger depth range.

Although DispMVSshows insensitivity to depth range, its performance still exhibits a certain degree of decline with larger depth ranges.
In contrast, our method, which is independent of depth range, maintains consistent performance regardless of changes in depth range.

It is crucial to emphasize that the depth range provided by the dataset is exceptionally accurate.
For instance, the ground truth for the Tanks-and-Temples dataset is captured using an industrial laser scanner.
However, in practical applications, while Structure-from-Motion (SfM) can derive depth ranges from sparse feature points, the resulting depth estimates are often prone to inaccuracies. These inaccuracies arise from the inherent sparsity of feature points, as well as challenges such as occlusion and suboptimal viewpoint selection.
To verify the robustness of the MVS models in practical applications,
we use the depth range obtained from COLMAP to replace the depth range ground truth (GT).
As shown in Table, there is a significant decline in performance for GeoMVS, MVSFormer++, IterMVSand CER-MVSwhen we use the depth range obtained from COLMAP. DispMVSalso exhibits a certain degree of decline. In contrast, our method maintained consistent performance.
This result further demonstrates the necessity of eliminating the depth range.

SECTION: Conclusion
We propose a prior-free multi-view stereo framework that simultaneously considers all the source images. To fully fuse the information from disordered and arbitrarily posed source images, we propose a 3D-pose-embedding-aided and uncertainty-driven transformer-based network. Extensive experiments show that our methods achieve state-of-the-art performances among the prior-free methods and exhibit greater robustness to the depth range prior.: The proposed method cannot run in real-time (i.e., 30 FPS), which could limit its application in mobile devices or other time-sensitive scenarios.

Besides, our method shows a performance gap compared to SOTA cost-volume-based methods on the mainstream benchmark, despite these methods relying on highly precise depth range priors. In the future work, we hope to close the gap.

SECTION: Acknowledgement
This work was partially supported by NSF of China (No. 61932003).

SECTION: References