SECTION: An Adaptive Cost-Sensitive Learning and Recursive Denoising Framework for Imbalanced SVM Classification

Category imbalance is one of the most popular and important issues in the domain of classification. Emotion classification model trained on imbalanced datasets easily leads to unreliable prediction. The traditional machine learning method tends to favor the majority class, which leads to the lack of minority class information in the model. Moreover, most existing models will produce abnormal sensitivity issues or performance degradation. We propose a robust learning algorithm based on adaptive cost-sensitivity and recursive denoising, which is a generalized framework and can be incorporated into most stochastic optimization algorithms. The proposed method uses the dynamic kernel distance optimization model between the sample and the decision boundary, which makes full use of the sample’s prior information. In addition, we also put forward an effective method to filter noise, the main idea of which is to judge the noise by finding the nearest neighbors of the minority class. In order to evaluate the strength of the proposed method, we not only carry out experiments on standard datasets but also apply it to emotional classification problems with different imbalance rates (IR). Experimental results show that the proposed general framework is superior to traditional methods in Accuracy, G-mean, Recall and F1-score.

SECTION: 1Introduction

Support Vector Machine (SVM), a typical statistical learning method, was first proposed by Vapnik and Cortes in 1995[1]. SVM has gained significant popularity as a classification and regression algorithm. It has been used in various real-world problems widely and successfully, including particle recognition, text classification[2,3,4], visual recognition[5], feature selection[6,7], and bioinformatics[8]. SVM encounters various obstacles, including the incorporation of previous knowledge from the datasets[9,10]and the identification of a suitable kernel function for mapping the datasets to a separable space of high dimensions[11], and expediting the resolution of quadratic programming (QP) problems[12,13,14]. Our exclusive focus lies in utilizing SVM to address the challenges posed by large-scale imbalanced datasets. In doing so, we employ the technique of assigning distinct weights to individual samples. The SVM classification hyperplane is uniquely determined by support vectors (SVs). A sample that satisfiesis called a boundary support vector. The two planes that are parallel to the optimal classification hyperplane and intersect the boundary support vector are referred to as classification interval hyperplanes.

The error support vector lies between the hyperplanesandin the classification interval. Once the optimal classification hyperplane is acquired, the decision function can be expressed as

whererepresents the sign function.

Noise frequently contaminates the training data in several practical engineering applications. Furthermore, certain data points within the training dataset are erroneously positioned into the feature space alongside other categories, which are referred to as outliers. As indicated in[15]and[16], outliers frequently transform into support vectors that have higher Lagrangian coefficients throughout the training process. The classifier created using the support vector machine exclusively depends on support vectors. The existence of outliers or noise in the training set leads to a large deviation of the decision border from the ideal plane, when using the standard SVM training technique. This high sensitivity of SVM to outliers and noise is a consequence of this deviation. This work proposes the inclusion of weights to showcase the contributions of various samples.

Our primary focus is on the fundamental problem of soft-margin SVM. Wang et al.[17]improved the soft-margin support vector machine and proposed a new fast support vector machine with truncated square hinge loss, which fully improved the calculation speed of large-scale datasets. Descent techniques can be employed to minimize the objective function of a conventional SVM, which is known for its high convexity. Nevertheless, descent methods necessitate accurate calculation of the gradient of the goal function, a task that is frequently arduous. The Stochastic Gradient Descent (SGD) method addresses this issue by employing unbiased gradient estimates using tiny data samples[18,19]. It is the primary approach for solving large-scale stochastic optimization issues. Nevertheless, the practical attractiveness of SGD approaches remains restricted due to their need for a substantial number of iterations to achieve convergence. Indeed, SGD exhibits sluggish convergence due to its reliance on gradients, a drawback that is further amplified when gradients are substituted with random approximations. Various stochastic optimization strategies have been employed thus far to address SVM models. Aside from first-order techniques such as SGD, second-order approaches are also extensively employed. Second-order stochastic optimization methods impose greater demands on functions compared to first-order methods, as they are capable of quadratic differentiation. Consequently, studying these approaches is a demanding task. Second-order stochastic optimization methods inherently address the issues of limited precision and sluggish convergence that are associated with first-order approaches. This paper considers solving SVM using second-order stochastic optimization methods to further reduce the impact of outliers and dataset imbalance while ensuring accuracy.

In this paper, we introduce a novel generalized framework with adaptive weight function for imbalanced SVM classification (AW-WSVM). Our main contributions are as follows:

1. This work proposes a novel cost-sensitive learning technique. In the kernel space, the contribution of training samples to the optimal classification hyperplane is determined by their kernel distance from the hyperplane. Specifically, the closer a sample is to the hyperplane, the greater the penalty term for its loss, while the farther a sample is from the hyperplane, the smaller the penalty term for its loss. We suggest implementing a new adaptive weight function that can adjust itself according to the changes in the decision hyperplane.

2. We introduce a new soft-margin weighted support vector machine. The model no longer focuses on the overall loss but instead emphasizes the loss of each sample.

3. We filter the data during each iteration, paying more attention to the samples in the vicinity of the decision hyperplane. As a small number of samples are sensitive to noise, we propose a recursive method to find the nearest neighbors and eliminate noisy data.

4. Employ the adaptive cost-sensitive learning and recursive denoising technique we suggested in conjunction with various optimization algorithms, and conduct experiments using datasets with different characteristics. In the field of emotion classification, we have done many experiments under different imbalanced ratios. The results demonstrate that the suggested weighting technique can enhance the performance of SVM even more.

The remaining sections of this paper are structured in the following manner. Section 2 provides a concise overview of the recent literature on the classification of imbalanced data. Section 3 presents the introduction of a novel adaptive cost-sensitive learning and recursive denoising technique. Section 4 involves conducting experiments on various datasets by integrating different algorithms with our suggested technique. The discussion and conclusions can be found in section 5.

SECTION: 2Related Work

SECTION: 2.1Imbalanced Data

Data imbalances occur in the real world, often leading to overfitting. Faced with the problem of imbalance in SVM, there are two popular solutions. The first is to reconstruct the training points. Increase the number of points of a few classes in different ways (oversampling) or decrease the number of points of a majority class (undersampling)[20,21]. The second is to introduce appropriate weight coefficients into the loss function. The error cost or the decision threshold of imbalanced data is adjusted to increase the weight of the minority class and reduce the weight of the majority class to achieve balance. An initial approach to incorporating weighting into SVM is through the use of fuzzy support vector machine (FSVM)[22]. Fuzzy SVM assigns different fuzzy membership degrees to all samples in the training set, and retrains the classifier. Shao et al.[23]introduced an extremely efficient weighted Lagrangian twin support vector machine (WLTSVM) that utilizes distinct training points to create two nearby hyperplanes for the classification of unbalanced data. Fan et al.[24]proposed an improved method based on FSVM to determine fuzzy membership by class determinism of samples, so that distinct samples can provide different contributions to the classification hyperplane.Kang et al.[25]introduced a downsampling method for Support Vector Machines (SVM) that uses spatial geometric distances. This method assigns weights to the samples depending on their Euclidean distance from the hyperplane. The weights highlight the samples’ influence on the decision hyperplane. In order to improve the performance of SVM to generate samples, paper[26]combines oversampling with particle swarm optimization algorithm to find the best sample position.

SECTION: 2.2Outliers of SVM

In the real world, datasets often possess substantial size and exhibit imbalances, hence posing challenges to the problem-solving process. Yang et al.[27]proposed Weighted Support Vector Machine (WSVM), which addresses the issue of anomaly sensitivity in SVM by assigning varying weights to different data points. The WSVM approach utilizes the kernel-based Probability c-means (Kpcm) algorithm to provide varying weights to the primary training data points and outliers. Wu et al.[28]introduced the Weighted Margin Support Vector Machine (WMSVM), a method that combines prior information about the samples and gives distinct weights to different samples. Zhang et al.[29]proposed a Density-induced Margin Support Vector Machine (DMSVM), for a given dataset, DMSVM needs to extract the relative densities of all training data points. The densities can be used as the relative edges of the corresponding training data points. This can be considered as an exceptional instance of WMSVM. In[30], particle swarm optimization (PSO) is used for the time weighting of SVM. Du et al.[31]introduced a fuzzy-compensated multiclass SVM approach to improve the outlier and noise sensitivity issues, proposing how to dynamically and adaptively adjust the weights of the SVM. Zhu et al.[32]introduced a weighting technique based on distance, where the weights are determined by the proximity of the nearest diverse sample sequences, referred to as extended nearest neighbor chains. The paper[33]presents a novel approach called the weighted support vector machine (WSVM-FRS), which involves incorporating weighting coefficients into the penalty term of the optimization problem. Inadequate samples possess minimal weights, while significant samples possess substantial weights.

SECTION: 2.3Stochastic Optimization Algorithm for SVM

Gradient-based optimization techniques are widely used in the development of neural systems and can also be applied to solve SVM. First-order approaches are commonly employed due to their straightforwardness and little computational cost. Some examples of optimization algorithms are stochastic gradient Descent (SGD)[34], AdaGrad[35], RMSprop[36], and Adam[37]. Although the first-order approach is easy to understand, its slow convergence is a significant disadvantage. Utilizing second-order bending information enables the improvement of convergence. The Broyden-Fletcher-Goldfarb-Shanon (BFGS) approach has garnered significant attention and research in the training of neural networks. Simultaneously, an increasing number of scholars are employing the BFGS algorithm to address Support Vector Machine (SVM) problems. However, a significant drawback of the second-order technique is its reliance on substantial processing and memory resources. Implementing quasi-Newtonian methods in a random environment poses significant challenges and has been a subject of ongoing research. The Online BFGS (oBFGS) approach, mentioned[38], is a stochastic quasi-Newtonian method that has demonstrated early stability. In contrast to BFGS, this method eliminates the need for line search and alters the update of the Hessian matrix. An analysis is conducted on the worldwide convergence of stochastic BFGS, and a novel online L-BFGS technique is introduced[39]. A novel approach is suggested, which involves subsampling Hessian vector products and collecting curvature information periodically and point-by-point. This method can be classified as a stochastic quasi-Newtonian method[40]. In[41], the authors introduce a stochastic (online) quasi-Newton method that incorporates Nesesterov acceleration gradients. This strategy is applicable in both complete and limited memory formats. The study demonstrates the impact of various momentum rates and batch sizes on performance.

In the following sections, we apply the proposed framework to SGD, oBFGS and oNAQ to do comparative experiments. We pay more attention to these three algorithms, SGD and oBFGS are summarized in Algorithm1and Algorithm2respectively. ONAQ will be discussed in the third section.

SECTION: 3Proposed Method

SECTION: 3.1Support Vector Machine(SVM)

We consider a classical binary classification problem. Suppose there is a training set, for any,is the sample,is the label. The purpose of the SVM classifier is to find a function:that separates all the samples. A linear classification function can be expressed as, whereis a weight vector andis a bias term. If,is assigned to class; Otherwiseis divided into class.

The SVM problem is based on the core principle of achieving classification by reducing the error rate and optimizing the hyperplane with the highest margin. This is illustrated in Fig.1, where the solid line represents the classification hyperplane. SVM employs the kernel functionto transform low-dimensional features into higher-dimensional features in order to address nonlinear situations. The kernel approach is used to directly calculate the inner product between high-dimensional features, eliminating the requirement for explicit computation of nonlinear feature mappings. Afterwards, linear classification is conducted in the feature space with a high number of dimensions. Soft-margin SVM utilizing kernel techniques can be expressed in the following manner:

where, andis a hyperparameter.

Considering imbalanced datasets, the solution of SVM may appear overfitting. Standard SVMs tend to assign any sample that comes into the class-heavy side. Fig.2illustrates the above situation. When the dashed line is used as the classification hyperplane, the error rate is as high as 4.53%. When the solid line is used (the solid line is learned through the method proposed in this paper), the error rate is reduced to 2.13%. This illustrates the importance of improving data imbalances.

In numerous real-world engineering scenarios, the training data frequently encounters noise or outliers, resulting in substantial deviations in the decision bounds of SVM. The outlier sensitivity problem of ordinary SVM algorithms refers to this phenomena. Fig.3demonstrates the atypical susceptibility of SVM. In the absence of outliers in the training data, the SVM algorithm can identify the optimal hyperplane, shown by a solid line, that maximizes the margin between the two classes. If an outlier exists on the other side, the decision hyperplane, indicated by a dashed line, deviates considerably from the optimal selection. Consequently, the training procedure of a typical SVM is highly susceptible to outliers.

SECTION: 3.2Soft-Margin Weighted Support Vector Machine (WSVM)

Soft-Margin Weighted Support Vector Machine (WSVM), aims to apply distinct weights to individual data points based on their relative significance within the class. This enables individual data points to exert varying influences on the decision hyperplane. Setas the weight of thea sample, then the training data set into, theis continuous and. Then the optimization problem can be written as:

where

We no longer focus on the overall loss, but on the loss of each sample. In other words, we can regard the weight as a penalty term. The greater the punishment for the sample, the less the corresponding loss is required. In fact, the SVM is more accurate and targeted by punishing each sample separately by setting the weight. It satisfies the spacing maximization principle.

In order to make the expression of WSVM more concise, we give the following explanation

Therefore, problem (4) can be expressed more concisely as follows

Obviously, the standard SVM and the proposed WSVM are different in weight coefficient. The higher the value of, the more significant the sample’s contribution to the decision hyperplane. In conventional SVM, samples are considered equal, meaning that each sample has an equal impact on the decision hyperplane. However, that is not true. In the real datasets, there are imbalanced samples and outliers. The large number of samples and the presence of outliers will result in the decision hyperplane deviating, which is undesirable. Consequently, it is necessary to diminish or disregard the impact of these samples, while amplifying the significance of the limited categories of samples, support vectors, and accurately classified samples. This is what WSVM does. Both WSVM and SVM ensure maximum spacing, facilitating the differentiation between the two categories of data. Furthermore, the former also prioritizes the analysis of samples that have a greater impact.

SECTION: 3.3Adaptive Weight Function(AW Function)

The weight of sample is developed based on the distance. Suppose that the distance between the sample and the hyperplane is d in the feature space, thenobeys a Gaussian distribution with a mean value of 0. Its probability density function is

The initial decision hyperplaneis obtained. Setis the distance between theth sample and the initial decision hyperplane, in theth iteration. In order to summarize the expression of, we give several new definitions.

We can easily get different kinds of clustering points in the kernel space. Set the total number of positive sample population asand its clustering point as. In addition, the total number of negative sample groups is recorded as, and the clustering point is recorded as. The expressions ofandare as follows:

Then the normal vectorof the decision hyperplane can be expressed as

In order to calculate the distance between the sample in the kernel space and the decision hyperplane, we set a point on the hyperplane, which is marked as. Finally, we give the expression of.

We stress the significance of the distance between the sample and the hyperplane. And according to the above assumption, we can give the definition of Adaptive Weight function(AW function).

(AW function) In theth iteration, letbe the weight of theth sample. Set. Accordingly, the following Adaptive Weight function(AW function) is proposed:

wherecan weaken the influence of difference or variation amplitude on distribution to some extent.

According to differentvalues, we draw the AW function image, as shown in Fig.4. The function exhibits an axis of symmetry atand demonstrates a decreasing trend from the center toward the periphery. In the proposed framework, the x-axis indicates the spatial separation between the sample and the decision hyperplane, while the y-axis signifies the magnitude of the weight assigned to the sample. Therefore, the y-values should be in the range of 0 to 1. It is evident from this analysis that the value ofthat yields the greatest match for our weight argument is 1.

Demonstrate the reasonableness of the weighting function for the given set. The weight function should be expressed as a probability density function within each class, meaning that the integral value of the function is equal to 1 within each class. Furthermore, it is important to mention that, to enhance efficiency during the experiment, we will refrain from computing the distance based on the category. Hence, we do not differentiate between the two categories during the process of proof. In brief, we prove that the definite integral of the function equals 2.

The first term is the probability density function of the Gaussian distribution, so

The second term is

So

∎

For every category of samples, the weight function assigns a weight of 1 when the distance is 0. The weight diminishes proportionally with the distance. It fulfills the condition that samples close to the hyperplane have a greater impact.

SECTION: 3.4Generalized Framework for AW-WSVM

On the basis of the theories in Sections 3.2 and 3.3, we give the most important contribution of this paper, a generalized framework named AW-WSVM. We apply this framework to standard stochastic optimization algorithms, such as Algorithm1and Algorithm2, to solve WSVM and update the weights with AW function. The standard second-order algorithm oNAQ will be used in the following sections, so it is summarized in Algorithm3.

All samples are initially assigned the same weight. The assumption in Section 3.3 is given to illustrate the rationality of the above statement.

Now, we can utilize the AW function mentioned before to produce weights for training the AW-WSVM algorithm. Enter the distanceobtained by thetraining into the (12). We filter the data by recursively finding the nearest neighbor. Then update the weight of each sample. Finally, we put it in the objective function again. The framework includes the following steps:

(1) Solve the WSVM to get the hyperplane

Substitute the weight vector obtained from each update into formula (6), and get a new hyperplane expression through iterative solution of random optimization algorithm;

(2) Filter samples

The minority is more sensitive to noise. We can calculate the distance between each sample and the hyperplane, and only pay attention to the samples near the decision boundary (set the threshold according to experience). Because the samples far away from the decision boundary will not have a great influence on the optimal hyperplane. Remember, the majority sample set around the decision hyperplane is, and the minority sample set is. Traverse the samples inand find the nearest neighbor of. If the nearest neighbor belongs to, it meansis the noise to be removed. We describe this process in detail in Fig.5;

(3) Update weights

Substitute the obtained sample distance into AW function, and get the weight of each sample.

The hyperplane acquired at theiteration is used as the beginning input for theiteration. The weight coefficientand distanceof the decision hyperplane are obtained again. Proceed with the aforementioned instructions to iterate. In this way, the outlier will have minimal weight, approaching zero, thereby greatly diminishing its impact on the choice hyperplane. Fig.6displays the weights obtained for a certain class with the AW function. The data that was used for training points located in the middle region of the data group possess a substantially greater weight (close to 1), while the outliers have a much lower weight (close to 0). The proposed AW-WSVM is summarized in Algorithm4.

We can draw a conclusion that the normal vector of the decision hyperplane in each iteration can be solved by most stochastic optimization algorithms such as Algorithm1,2or3. In the case of Algorithm3oNAQ, we can easily provide important steps:

SECTION: 4Numerical Experiments

In this section, we present a comparison between the proposed method AW-WSVM and typical first-order and second-order approaches. The comparison is conducted on different datasets. We use experimental data to illustrate the effectiveness of the proposed framework in solving the problem of imbalanced data classification. These experiments were performed on python3 on MacOS with the Apple M2 chip.

To evaluate the proposed method comprehensively, it is insufficient to solely rely on a single indicator. Before providing the assessment criteria, it is necessary to establish the confusion matrix for the binary classifier. The confusion matrix provides fundamental details on the classifier’s actual and expected classifications (see Table1). The discriminant criteria we offer include accuracy, precision, recall, specificity, F1 score and G-mean.(1)Accuracy=(2)Precision=(3)Recall=(4)Specificity=(5)F1-score=(6)G-mean=

SECTION: 4.1Experiments on Different Datasets

In this section, to illustrate the universality of the proposed framework, we compare our proposed generalized framework, which combines standard first-order optimization algorithms SGD and second-order optimization algorithms oBFGS and oNAQ, with these algorithms on 12 different datasets. These datasets are diverse, including sparse, dense, imbalanced, high-dimensional, and low-dimensional. They were collected from the University of California, Irvine (UCI) Machine Learning repository111https://archive.ics.uci.edu, Libsvm data222https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/. Details of the features are shown in Table2. The second column indicates the source of the dataset, the third column indicates the quantity of the dataset, the fourth column indicates the dimension, and the fifth column indicates the quantity of data categories.

In this section, the parameters are set as follows. The learning rate of SGD optimization algorithm on different datasetsis selected from {0.1,0.3,0.5}; The parameterrelated to the learning ratein oBFGS optimization algorithm is set to 10,(represents theiteration). In addition, to ensure the stability of the value, the change value of the independent variable is introduced in the calculation of the step difference, where the difference coefficient of the independent variableis set to 0.2 on all datasets. Parametersrelated to the learning ratein the oNAQ optimization algorithm are set to 10, momentum coefficientto 0.1, and difference coefficient of independent variableto 0.2 on all data sets. The highest limit of iterations and batch size of all algorithms are selected based on the size of the datasets. Refer to Table3for a comprehensive explanation.

The variancein the AW function is set to 1 empirically. We conducted 100 iterative experiments, taking the total amount of iterations as the horizontal coordinate and the test accuracy as the vertical coordinate to depict the performance of each algorithm, as shown in Fig.7. Fig.7(a)-7(l)respectively plotted the experimental results of the 12 datasets in Table2. No matter whether the proposed method in the initial result performs well or not, it tends to be stable after a certain number of iterations. We can conclude that the proposed approach effectively improves the performance of SGD, oBFGS and oNAQ on all 12 datasets.

We additionally evaluate the efficacy of the proposed framework, AW-WSVM, by using the confusion matrix. To ensure the stability of the results, we iterated 100 times. The metrics of different algorithms obtained from the confusion matrix can be presented in Table4-7, with the highest metrics bolded. Conclusions are easy to come by, and the proposed method outperforms SGD, oBFGS, and oNAQ on 12 datasets. The proposed method exhibits lower results on very few datasets. However, when considering the entirety of the recommended technique, it has successfully attained the intended outcome.

In order to intuitively illustrate the effectiveness of our proposed method in processing these 12 datasets, we plotted a histogram of G-mean values. We randomly sampled 20 experimental results for each dataset, and the height of each bar in the histogram represents the expectation of G-mean value over these 20 sampled results. As shown in Fig.8, we can easily observe that our method performs better on all datasets.

SECTION: 4.2Statistical Comparison

This section makes a statistical comparison[42]of the experimental results. The significance of performance differences between classifiers, including test accuracy and prediction accuracy, were tested using Friedman tests and Nemenyi post hoc tests[43,44]. The Friedman test is utilized to compare the mean ranking of several algorithms on N datasets. Defineas the ranking of theth algorithm on theth dataset out of a total of N datasets, and. According to the null hypothesis, all algorithms are considered to be identical, so their average rankingshould be the same. The Friedman statisticfollows a Chi-square distribution with K-1 degrees of freedom

whereis the number of algorithms involved in the comparison.

Table8shows the average rankingof the six classifiers in test accuracy. The p-value of Friedman’s test is. The P-value is less than the threshold for significance level, indicating that the null assumption should be discarded. The above results show that there are great differences among all classification methods from a statistical point of view. We compared the g-mean of the six classifiers (see Table9), with a p-value offor Friedman’s test, indicating significant differences among all classification methods. Table10-11record the Recall and F1-score of six classifiers. The Friedman’s test p-values for Recall and F1 areand, respectively, which indicates significant differences between the methods.

Consequently, to determine the classifiers that exhibit substantial differences, it is essential to perform a post-hoc test in order to further differentiate between the methods. Nemenyi post hoc test is selected in this paper. The Nemenyi test calculates the critical range of the difference in average order values

If the discrepancy among the mean rank measurements of the two distinct algorithms exceeds the critical range (CD), the hypothesis that the two algorithms exhibit comparable results is refuted with the equivalent level of confidence. The variablesandcorrespond to the quantities of algorithms and datasets, correspondingly. When the significant level=0.05,=2.850. So we get CD = 2.85×=2.1767.

Fig.9(a)-9(d)respectively report the analysis results of six classifiers in terms of test accuracy, G-mean, Recall and F1-score. The test accuracy of the standard SGD, oBFGS, and oNAQ algorithms does not exhibit any substantial variation, but there is a notable disparity between the proposed approach in this research. Furthermore, our analysis indicates that the integration of all three algorithms with AW-WSVM leads to enhanced algorithm performance. This improvement is evident from the overall higher ranking of the combined algorithms across all datasets, showcasing varied progress. The results indicate a negligible disparity in the classification accuracy across standard SGD, oBFGS, and oNAQ. Each of the three algorithms was examined independently. The performance of SGD was enhanced by using the weight update approach suggested in this research (AW-WSVM+SGD). Notably, there were notable disparities between the two algorithms when compared to the original SGD. The same applies to second-order approaches such as BFGS and NAQ.

SECTION: 4.3Experiments on Emotional Classification with Different Imbalance Ratio(IR)

In this section, we collect the dataset of emotion classification, which comes from gitte333https://gitee.com/zhangdddong/toutiao-text-classfication-dataset.(see Tabel12). The last column in the table indicates the imbalance ratio (IR). In daily life, different kinds of news data are always imbalanced, for example, entertainment news will account for more than science and technology news.

In this section, the parameters are set as follows. The learning rateof SGD optimization algorithm is 0.1. The parameterin oBFGS and oNAQ optimization algorithm is set to 10 andis set to 0.2. The momentum coefficientto 0.1. The highest limit of iterations is set to 500 and batchsize of all algorithms is set to 256.

Table13and Table14record the test accuracy and G-means after 100 iterations, respectively, in which bold indicates the best results compared with the two. It is well known that G-means can be used as an effective indicator to measure the quality of the model when the data is unbalanced. Therefore, we can draw the conclusion that our framework, AW-WSVM, will effectively improve the performance of the standard stochastic optimization algorithm, no matter whether the IR is high or low.

In order to further illustrate the effectiveness of the proposed AW-WSVM framework, we process the same data and get G-means under different IRs[45]. Taking 102vs100 and 109vs100 as examples, we plot the results in Fig10. It is not difficult to find that our proposed method performs best in both low IR and high IR.

SECTION: 5Conclusion

We proposes a novel generalized framework, referred to as AW-WSVM, for SVM. The purpose of its development was to address the complexities of real-world applications by effectively handling uncertain information. It ensures that training remains effective even when dealing with challenging and extensive datasets, without compromising on speed.

Considering the massive data, we propose an Adaptive Weight function (AW function), through which the weights of samples can be dynamically updated. Our analysis demonstrates that the weights generated by the AW function obey a probability distribution in each class. We propose a new soft-margin weighted SVM and provide a more concise matrix expression, so that any stochastic optimization algorithm can iterate through the matrix, saving computational space. Since minority is extremely sensitive to noise, we propose an effective noise filtering method based on nearest neighbors, which improves the robustness of the model. Subsequently, we can allocate greater weights to samples that are in close proximity to the decision hyperplane, while assigning lower weights to the other samples. Our allocation technique is appropriate since the support vectors, which are placed near the decision hyperplane, correspond with the concept that samples close to the decision plane have higher influence.

The proposed framework, AW-WSVM, was evaluated on a total of 12 different datasets sourced from UCI repositories and LIBSVM repositories. The experiments conducted confirm that the proposed framework AW-WSVM surpasses the conventional first-order approaches SGD, second-order methods oBFGS, and oNAQ. Furthermore, it exhibited superior performance in terms of classification accuracy, g-mean, recall and F1-score. In addition, we also conducted statistical experiments on these six classifiers. Friedman tests and Nemenyi post hoc tests demonstrate that the standard algorithms SGD, oBFGS, and oNAQ all perform better within our proposed framework. We not only do experiments on standard datasets, but also apply the framework to emotion classification datasets. From the G-means and other indicators, the framework proposed in this paper can effectively improve the performance of standard optimization algorithms, no matter whether the imbalance index is high or low. Moreover, the core of the proposed generalized framework, AW-WSVM, is to look for data samples near the classified hyperplane. Consequently, it may be effectively integrated with any soft-margin SVM solution techniques.

SECTION: Acknowledgement

This work is supported by the National Natural Science Foundation of China (Grant No.61703088), the Fundamental Research Funds for the Central Universities (Grant No.N2105009)

SECTION: Declaration of Generative AI and AI-assisted technologies in the writing process

During the preparation of this work, the authors used ChatGPT in order to translate and revise some parts of the paper. After using this tool, the authors reviewed and edited the content as needed and took full responsibility for the content of the publication.

SECTION: References