SECTION: Vision Transformers for End-to-End Vision-Based Quadrotor Obstacle Avoidance
We demonstrate the capabilities of an attention-based end-to-end approach for high-speed vision-based quadrotor obstacle avoidance in dense, cluttered environments, with comparison to various state-of-the-art learning architectures. Quadrotor unmanned aerial vehicles (UAVs) have tremendous maneuverability when flown fast; however, as flight speed increases, traditional model-based approaches to navigation via independent perception, mapping, planning, and control modules breaks down due to increased sensor noise, compounding errors, and increased processing latency. Thus, learning-based, end-to-end vision-to-control networks have shown to have great potential for online control of these fast robots through cluttered environments. We train and compare convolutional, U-Net, and recurrent architectures against vision transformer (ViT) models for depth image-to-control in high-fidelity simulation, observing that ViT models are more effective than others as quadrotor speeds increase and in generalization to unseen environments, while the addition of recurrence further improves performance while reducing quadrotor energy cost across all tested flight speeds. We assess performance at speeds of up to 7m/s in simulation and hardware. To the best of our knowledge, this is the first work to utilize vision transformers for end-to-end vision-based quadrotor control.

SECTION: 
Quadrotor unmanned aerial vehicles (UAVs) are small, agile vehicles that can fly with high levels of acceleration and agility. With the miniaturization of onboard sensing and computing, it is now common to treat quadrotors as a single, independent robot that senses its environment, performs high-level planning, calculates low-level control commands, then tracks these commands using an onboard-calculated state estimate. Prior to recent work, most research in this field has been done with the traditional robotics pipeline of individual and modular sensing, planning, and control blocks, each with some computational requirements and error tolerances.
However, quadrotors reach their highest utility when flown fast, thereby achieving more coverage with limited battery life;
at these speeds, perception-to-planning via the modular approach can break down. Recent works have explored using learned policies to develop a single sensing-to-planning algorithm that feeds into a tracking controller; this has fast reaction times and generalizes to sensor noise and real-world artifacts when learned models are trained with domain randomization techniques. In contrast, we explore a fully end-to-end approach, encapsulating more of the tracking task into a vision transformer (ViT) model.

We aim to explore the utility of attention-based learning models, specifically vision transformer architectures, that take depth images as input for end-to-end control of a quadrotor for the high-speed obstacle avoidance task. We adopt the task and simulator from ICRA 2022 DodgeDrone Challenge: Vision-based Agile Drone Flight, showcasing various teams’ efforts to develop obstacle avoidance policies for a quadrotor flying through a cluttered simulation environment.
Against the ViT architecture, we select as baselines a range of learning backbones representing varying use-cases: convolutional, U-Net, and recurrent networks. While this is not an exhaustive list of all popular learning architectures available today, we believe this covers a wide range of model types typically used for vision-based control-related tasks, including object detection, image segmentation, and temporal sequence modeling.

The favorable qualities of convolutional networkshave driven much of the work in vision-based UAV control. These networks are able to learn feature maps that, when trained with gradients computed relative to a desired objective function, outperform hand-engineered feature maps and additionally are translationally invariant. However, when used for the control of dynamical systems, they do not maintain any internal state and thus do not have any memory of past states or actions. In robotics tasks where both the robot and thus the sensory input are dynamic, it has yet to be shown that convolutional networks are the best method for perception.
Recurrent architectures (e.g., long short term memory) are models that maintain an internal state and may provide smoother, more feasible commands to dynamic robots such as quadrotors. More recent sequence modeling techniques use the attention mechanismto construct transformersthat have been adapted from the natural language processing domain for visionand show superior performance in object recognition and tracking. These vision transformers take as input the patches of an image as a sequence, and when provided with large training datasets, outperform convolutional or recurrent networks in a growing number of computer vision tasks. Further motivation lies in prior work which suggests that using recurrent or attention-based models might improve performance due to the temporal nature of dynamic robot navigation.

This work’s aim is to study the ViT as an end-to-end framework for quadrotor control, with comparisons to baseline architectures. We choose to train models via behavior cloning in simulation from a privileged obstacle-aware expert, as is common among existing learning-to-dodge works. This additionally eliminates potentially confounding factors that may exist in a reinforcement learning framework, such as the exploration/exploitation tradeoff and additional hyperparameter tuning.
We use depth sensing data to train and evaluate these models, as across multiple sensor types, robotic platforms, and environments, depth data has shown in prior work to be both ubiquitous and effective at high-level scene understanding as well as navigation tasks. We evaluate models against each other by comparing collision metrics during forward flight through a cluttered scene, across trials with increasing forward velocity. We further present the paths taken by each model through a given simulated scene, the command characteristics of each model (acceleration and energy cost), and compare our end-to-end approach to modular baseline methods. Representative hardware experiments further demonstrate agile avoidance around multiple obstacles and at high speeds (Figure).

The first use of vision transformer models for high-speed end-to-end control of a quadrotor.

A comparison of a vision transformer model against various state-of-the-art, learning-based architectures for end-to-end depth-based quadrotor control.

Real experiments demonstrating and comparing models on hardware.

Open-source code, datasets, and pretrained weights to reproduce and extend the results in this paper.

SECTION: 
End-to-end approaches for control tasks have been investigated in a variety of settings such as autonomous driving, and partially so in autonomous quadrotor flight in the wild. Both works demonstrate that such approaches have high sample complexity, and in the more structured self-driving task, it does not generalize as well as modular approaches.
However, both end-to-end and structured methods degrade in performance as agility increases. Existing works for learned end-to-end quadrotor flight largely focus on flight at a relatively slow speed and use raw images directly.

Some research has used a combination of architectures to surpass the performance of individual components for the object detection task. Swin Transformerand a ResNet-based modulehave been combined previously, showing that this attention+convolutional network architecture improved object detection performance. We similarly explore combination models between basic backbone architectures (convolution and U-Net, recurrence, and attention) in this work. When looking at downstream tasks such as control from pixels, transformer architectures such as ViT have been compared to convolutional neural networks (CNNs) and shown to perform worse than the CNN architecture due to the weaker inductive bias and need for more training data. ViT has been studied extensively for its use in downstream control tasks as a representation learner in many reinforcement learning (RL) works. Our method investigates leveraging similar architectures using imitation learning from a privileged expert as opposed to RL.

Since the development of the ViT, some works have used such models with onboard imagery from quadrotors but not in the same module used for planning or control. Some perform object detection onboard a UAV. Object tracking and then servoing has been attempted in the past, where a Siamese Transformer Network performs object tracking onboard a UAV, then independently a PD visual servoing algorithm commands yaw angle and forward velocity from the bounding box. To our knowledge, our work represents the first investigation for using ViT in an end-to-end manner with quadrotor control.

When navigating in uncertain environments, learning from depth images offers an input modality for reactive control. Various approaches have been taken to learn control outputs from depth image inputs such as mapping then planning, using motion primitives, and sampling collision-free trajectories. These methods use depth images to determine some intermediate representation which is then used to control the quadrotor. Compared to these methods, we demonstrate purely end-to-end control using various architectures learning from depth images directly, without the need for mapping, planning, or trajectory generation.

SECTION: 
SECTION: 
We formulate the obstacle avoidance task as flying forward a fixed distance at various forward velocities while avoiding static obstacles in a cluttered environment that is unstructured and unknown.
The privileged teacher expert policy is a reactive planner designed to be short-sighted and mimic data available to low-skill pilots, thereby avoiding expensive real-world, high-risk, in-situ data collection which may rely on trained-pilot data but be more time-optimal. Supervisory data is collected during expert policy rollouts to train the end-to-end learning-based student models. As this work aims to study ViTs against other models for developing reactive obstacle avoidance behavior, all models output linear velocity commands in the world frame, in contrast to long-horizon trajectory planning often used for less reactive policies in prior work. To compare model performance, we consider multiple obstacle collision metrics, computational complexity and inference time, and other quantitative and qualitative factors (see Section).

Formally, we consider an obstacle-aware experttaking as input quadrotor stateand local obstacle locations, and producing actionat every timestep. A datasetis collected by conducting many trajectoriesunder which the transition dynamicsare produced under the expertand the depth imageis recorded at every timestep. We seek to train a student policynot directly aware of obstacle locations but rather relying on depth images to reproduce the actions of the expert policy ().

SECTION: 
The student model takes as input a single depth image(similar to) of size 6090, quadrotor attitude, and forward velocity, and predicts a velocity vector. As models have access to the forward velocity, we do not include variations of model input such as stacked image sequences; while this input might improve temporal predictions of convolution-only models, we instead present recurrent models that are meant to model sequential data. Theare learned from the privileged expertin a supervised behavior cloning fashion, with a standard L2 loss over all timesteps in a trajectory, given student model parameters.

SECTION: 
We use the open-source Flightmare simulatorthat contains fast physics simulations of quadrotor dynamics and a bridge to the high-visual-fidelity game engine renderer Unity. In addition to being ROS-enabled, this simulator package allows us to collect training data from and deploy quadrotors in various environments with accurate depth sensor measurements with real-time computations on a laptop computer. Specifically, we use a modified version of the simulator supplied for the DodgeDrone ICRA 2022 Competitionwhich contains an environment with floating spherical obstacles of various sizes (later referred to as “Spheres” environment). The simulated quadrotor has a mass of 0.752kg and a diameter of 0.25m.

To gather supervisory data pairs offor behavior cloning, the privileged expert runs its short-horizon collision avoidance algorithm through randomized obstacle fields. Note that bold variables are multi-dimensional.
In total, 588 expert runs yielded 112k depth images, where 27.5k images are sampled from trials where there is at least one collision. Since the expert is reactive and lacks dynamics-feasible planning, collisions are not absent from the data; however, we find that some models (see Section) outperform this expert at high speeds.

The privileged expert can access obstacle position and radius informationof those obstacles within 10m of the current drone position. It searches for straight-line collision-free trajectories from the drone’s current position to each waypoint along a 2D grid in the lateral-vertical plane set at some horizon ahead of the drone, as demonstrated in Figure. The waypoint closest to the center of the grid is chosen, and a gain is applied to this relative position to generate a velocity command action. Transition dynamicsare provided by the Flightmare quadrotor simulation.

We present five student models for a comparison of ViT-based models against other state-of-the-art image processing architectures. Each model takes as input a single depth image, and uses an encoder-like module composed of either convolutions or attention to generate a lower-dimensional middle representation, which is then concatenated with the quadrotor attitude (quaternion) and forward velocity (scalar) before feeding into either LSTM or fully-connected layers. With the exception of the ConvNet model, all models are roughly3M parameters (Table). This model size ensures fast enough computation for in-flight processing in a robotics control loop (30Hz on the hardware platform presented in Section). Certain models such as a fully transformer-based architecture were attempted, but performed poorly and are not presented (unless a fully-connected head was used, as is described below as ViT).We utilize a transformer based encoder inspired by Segformer, and apply two transformer blocks in hierarchical fashion, followed by upsampling using pixel shuffle, and mixing of information across hierarchies using a convolution operation. This incorporates information at multiple scales. This ViT is followed by fully connected layers.This is identical to themodel but includes a multi-layer LSTM before the fully connected layers.A lightweight CNN serving as a reference model resembling the first, and still widely used, successful deep learning architecture for object detection or end-to-end vision-based control for robots. This model is the only learned baseline significantly smaller than 3M parameters. ResNet-18, a large CNN model that learns residuals of desired layer outputs from inputs, was also trained but found to perform similarly to the ConvNet model and worse than UNet model variations (below), demonstrating that simply increasing the number of parameters in the ConvNet does not lead to better performance.A multi-layer LSTM module placed between the convolutional and fully connected layers is included to encourage temporal consistency in velocity predictions.The symmetric encoder-decoder architecture popular in segmentation tasks includes skip connections that incorporate local and global information and help prevent vanishing gradients. The reduction to a lower-dimensional feature space also may improve generalizability to unseen environments. A multi-layer LSTM follows the UNet. Our model includes multiple skip connections to improve performance and form a strong comparison.

SECTION: 
We use the Falcon250 custom quadrotor platform(Figure) running theopen-source quadrotor control stack. All experiments are performed in a motion capture arena for accurate state estimate. The Intel Realsense D435 camera utilizes structured infrared light and stereo matching to calculate depth. These images are resized and invalid depth pixels are set to a fixed value; model inference is done on the Intel NUC 10 onboard computer (i7-10710U CPU). The velocity command prediction is sent to the control stack, which outputs a SO(3) command (orientation and thrust) that is sent to the open-sourcecontroller. While we train models on floating spherical objects in simulation, we test with free-standing cylinders and blocks in hardware experiments.

SECTION: 
SECTION: 
We present collision rates in Figurefor two simulation environments; Spheres () has obstacles similar to those seen during training, and Trees (contains trees that are previously unseen to the models. Mean collision rates are calculated over multiple trials performed in varying obstacle configurations, with velocities of 3-7m/s and 90 trials for each model in the Spheres environment, and 50 trials each in the Trees environment. As supported by prior work, collision rates increase as forward velocity increases. However, ViT+LSTM outperforms other models as flight speed increases beyond 6m/s, and is the only model to consistently outperform the expert beyond 5m/s. This is likely due to the recurrent nature of the ViT+LSTM, in contrast to the privileged expert which independently calculates a desired waypoint and corresponding velocity at each re-planning timestep. Notably, models only composed of the ViT+LSTM components, namely the ViT and LSTMnet models, present worse collision rates than ViT+LSTM as speeds reach 7m/s. Success rate (i.e., zero-collision rate) plots for both environments can be found in the supplementary material of this paper’s online version.

Mean time spent in collision per obstacle, per trial was also used to analyze model behavior (a plot is presented in online material). Since any collisions during expert rollouts causes the collision-free waypoint search to fail, briefly stalling the quadrotor planner, the expert presents large values for this metric over multiple trials (1.20s at 3m/s, 0.45s at 7m/s). However, all end-to-end learned models exhibit much lower values at this metric (worst values of 0.30s at 3m/s, 0.16s at 7m/s), demonstrating that they do not learn this stalling behavior, and again the ViT+LSTM model outperforms all other models at speeds of 4.5m/s and greater.

SECTION: 
The trajectory paths in Figureexhibit the path distribution of each model during 60m of forward flight through a fixed Spheres environment.
The ConvNet and ViT+LSTM demonstrate significantly less variance in paths through the scene, while
the ViT model exhibits changing variance in the chosen path as it progresses towards the goal. Notably, the ViT+LSTM takes the most direct path through the obstacles, with a maximum mean lateral deviation of 0.75m from the startingposition.

Figurepresents the calculated energy cost as described in, which for real drones translates to longer flight time with a limited battery life (lower energy cost is better). The ViT model has a drastically improved energy cost with the addition of recurrence (ViT+LSTM). And interestingly, while all models and the expert policy present worsening energy cost as forward velocity increases, ViT+LSTM does not increase by an appreciable amount. Acceleration and energy cost plots for both Spheres and Trees are shown in the online material.

SECTION: 
Figurehighlights areas of the input, for simulated and real experiments, that return the strongest signal by the convolutional layers in ConvNet and UNet (via spatial activation maps), and by the attention layers in ViT (via attention maps). ConvNet highlights full obstacles with little specificity to their shape; UNet specifically highlights edges and disregards surrounding areas; the ViT model seems to both capture obstacle edges and surrounding context, thereby capturing nearby obstacles more than UNet, likely a result of the known behavior of ViTs to learn relationships between parts of an image. In real experiments (Figures,) we find that the ConvNet attention map is more diffuse on the obstacle while ViT is much more specific.

SECTION: 
To test model generalization capabilities, we zero-shot deploy the models in a simulation environment containing realistic tree models placed randomly in the scene (“Trees” environment). We present collision rates in Figurewith additional metrics shown in the online version of the paper. We see that ViT-based models outperform all others, generalizing well. We further find that ViT-based models are the only models capable of generalization to a fly-through-window trial (often called a “narrow-gap” trial), where only a tight distribution of viable collision-free paths exist from start to goal, compared to many such viable paths in the Spheres or Trees environments.

SECTION: 
We compare the proposed end-to-end ViT+LSTM to two state-of-the-art modular baselines in the simulated Spheres environment (Figure). FastPlannerfinds an optimal initial path given multiple depth image observations in a receding horizon fashion, and fits a B-spline that is iteratively optimized to guarantee dynamic feasibility and be non-conservative. However, as this method is designed for flight around 3m/s, we additionally create a FastPlanner-scaled method which appropriately scales the output velocity to be closer to the desired velocities in the range 1-5m/s. However, this forced decrease or increase in flight speed either causes the FastPlanner algorithm to time-out (for3m/s only valid trials’ metrics are presented) or not keep up with observations (causing increased collisions), similar to the FastPlanner baseline metrics reported in. Double Descriptionis similarly designed for 3m/s flight, but produces fewer collisions, on par with ViT+LSTM. In contrast to these modular baselines, we find that our end-to-end method is the only one that not only appropriately flies at any desired speed (Figure), but also scales better with changing speed by maintaining consistently-low collision rates compared to modular baseline methods (Figure).

SECTION: 
We perform an ablation against providing quadrotor orientation and forward velocity to the models by re-training without this data; success rates are presented in Tablefor Spheres and Trees across three velocities. This ablation is particularly relevant to high-speed flight since unstructured or windy environments may cause noisy state estimates and variable velocity. We find that ViT+LSTM performed betterthe state information across slow and fast speeds and across environments, while ConvNet only benefited in the Spheres environment but performed substantially worse when tested in the unseen, generalization Trees environment, particularly as speeds increase.

SECTION: 
We zero-shot deploy ViT+LSTM in single and multi-obstacle configurations requiring collision avoidance and flight-through-gap behaviors at high speeds up to 7m/s. The model runs inference onboard at 30Hz (CPU) during all trials. Figureshows representative examples of two such trials, withshowing the depth image input to the model with a red arrow representing the model’s corresponding output velocity command. The quadrotor avoids obstacles in the real world with no model re-training or extensive system fine-tuning; while depth camera images exhibit artifacts not seen in the simulated training data, such as high-frequency patterns, varying background appearance, and propellers in-view (uncropped depth images viewable on project website) our end-to-end learned models performed effectively in many scenes at speeds of 1-7m/s. Figureadditionally demonstrates real-world differences in collision avoidance between the ConvNet and ViT+LSTM models.

SECTION: 
We present the first use of a vision transformer for end-to-end quadrotor control from depth images, with a comprehensive discussion of collision rates and command characteristics in simulation, comparison to other learned architectures and traditional modular methods, as well as deployment in challenging real-world trials. We find that the attention-based models (ViT and ViT+LSTM) outperform other models at high speeds in in-distribution simulation environments, and greatly outperform other architectures across all speeds in generalization environments, even extending from obstacle avoidance in forward flight to a fly-through-window task where other architectures fail. The best model combines attention and recurrence (ViT+LSTM), and consistently presents low collision rates while also maintaining the lowest energy cost of all models at speeds ranging 3-7m/s. Ablations show that across various speeds and environments, the attention-recurrent model improves with the inclusion of quadrotor state information. We also found that, in contrast to the presented end-to-end architecture, modular baseline methods fail to maintain low collision rates when scaling to varying flight speeds. We provide additional experiments and visualizations, as well as all code, datasets, and pretrained weights on the project website (Section) to enable further research of attention-based end-to-end robotics control.

SECTION: ACKNOWLEDGMENTS
This work was funded by NSF SLES-2331880 and NSF CAREER-2045834.