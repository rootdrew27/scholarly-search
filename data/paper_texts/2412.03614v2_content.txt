SECTION: Deep Learning in Single-Cell and Spatial Transcriptomics Data Analysis: Advances and Challenges from a Data Science Perspective

The development of single-cell and spatial transcriptomics has revolutionized our capacity to investigate cellular properties, functions, and interactions in both cellular and spatial contexts. Despite this progress, the analysis of single-cell and spatial omics data remains challenging. First, single-cell sequencing data are high-dimensional and sparse, often contaminated by noise and uncertainty, obscuring the underlying biological signals. Second, these data often encompass multiple modalities, including gene expression, epigenetic modifications, metabolite levels, and spatial locations. Integrating these diverse data modalities is crucial for enhancing prediction accuracy and biological interpretability. Third, while the scale of single-cell sequencing has expanded to millions of cells, high-quality annotated datasets are still limited. Fourth, the complex correlations of biological tissues make it difficult to accurately reconstruct cellular states and spatial contexts. Traditional feature engineering-based analysis methods struggle to deal with the various challenges presented by intricate biological networks. Deep learning has emerged as a powerful tool capable of handling high-dimensional complex data and automatically identifying meaningful patterns, offering significant promise in addressing these challenges. This review systematically analyzes these challenges and discusses related deep learning approaches. Moreover, we have curated 21 datasets from 9 benchmarks, encompassing 58 computational methods, and evaluated their performance on the respective modeling tasks. Finally, we highlight three areas for future development from a technical, dataset, and application perspective. This work will serve as a valuable resource for understanding how deep learning can be effectively utilized in single-cell and spatial transcriptomics analyses, while inspiring novel approaches to address emerging challenges.

KeywordsSingle-cellSpatial transcriptomicsDeep learning

SECTION: 1Introduction

The advancement of single-cell and spatial transcriptomics techniques has facilitated in-depth investigations of cellular characteristics, functions, and interactions, considering both cellular activity and spatial context within tissues. Single-cell RNA sequencing (scRNA-seq) quantifies gene expression at the cellular level, thereby elucidating cellular composition, gene expression patterns, and molecular characteristics[174,54]. Recognized as the Method of the Year by Nature Methods in 2013[8], scRNA-seq has significantly advanced research into complex biological questions, including mechanisms of disease resistance[114,194], tissue heterogeneity[97,219], targeted therapies[221], and embryonic development[196]. However, tissue dissociation disrupts spatial cell distribution and intercellular interactions, thereby constraining our understanding of the intricate processes occurring within multicellular organisms.

Spatial transcriptomics (ST) generates spatially resolved transcriptomic data to create detailed tissue maps at the subcellular level. This technique represents a significant advance in the field of transcriptomics, transitioning from cellular resolution to spatially sub-cellular resolution. In recognition of its importance in biomedical research, Nature Methods named spatially resolved transcriptomics as the Method of the Year in 2020[139].

Single-cell and spatial transcriptomics are crucial for studying the microenvironment at cellular and spatial resolutions, respectively. However, the complexity of biological tissues and the limitations of current sequencing techniques present significant analytical challenges. In this review, we discuss four major challenges in single-cell and spatial transcriptomics from a data science perspective: data sparsity, diversity, scarcity, and correlation. Our aim is to elucidate the origins of these challenges, explore potential solutions, and provide insights into the underlying mechanisms of the methodology. In terms of data sparsity, we examine issues such as the curse of dimensionality, noise, and uncertainty. Concerning data diversity, we categorize the integration of single-cell and spatial transcriptomics data into two primary types: multimodal integration and multi-source integration. When dealing with data scarcity, we focus on missing data annotations and missing modalities. Finally, from a data correlation perspective, we analyze methods for modeling spatiotemporal dependencies and incorporating prior knowledge.

With the increasing volume and diversity of data, traditional analysis techniques for bulk RNA sequencing are becoming increasingly inadequate for single-cell and spatial transcriptomics[54]. Deep learning (DL), a powerful tool for modelling large-scale, high-dimensional complex data, has demonstrated its versatility across numerous scientific domains, including small molecule modeling[147,19], protein structure prediction[38,150], and drug development[31], etc.

Recent reviews[51,15,58,23]of deep learning (DL) applications in single-cell data have introduced methods such as multilayer perceptrons (MLP)[184], autoencoder (AE)[82], generative adversarial network (GAN)[68], convolutional neural network (CNN)[110], and graph neural network (GNN)[160]. These reviews explored both traditional and DL methods across various stages of the scRNA-seq analysis pipeline. But they do not summarize current technological advances and challenges from a data science perspective. Moreover, existing data analysis techniques may not always be effective for addressing novel problems as the number of modalities continues to grow. This review aims to discuss four major data science challenges and exploring relevant methods within these contexts. We highlight DL techniques by comparing them with traditional machine learning approaches and emphasize their advantages, particularly when integrated with statistical frameworks. Each algorithm is discussed alongside its mathematical foundations, focusing on both similarities and differences. Finally, we outline future directions in three key areas: the application of novel AI methodologies, the development of fair and robust benchmark datasets with biologically interpretable evaluation metrics, and the exploration of DL applications in practical scenarios. This review provides a comprehensive overview of DL applications in single-cell and spatial transcriptomics data analysis from a data science perspective, offering insights that could inspire innovative solutions to emerging challenges in biological and medical research. The overall structure of the article is shown in Fig.1.

SECTION: 2Transcriptomic data

Bulk RNA sequencing (RNA-seq) provides average gene expression profiles at the tissue level, limiting its ability to accurately represent cellular heterogeneity. Consequently, it becomes challenging to discern whether the observed differences are due to changes in cellular composition or variations in gene expression (Fig.2). The scRNA-seq addresses this limitation by profiling gene expression at the single-cell level. Additionally, spatial transcriptomics integrates sequencing data with spatial context, providing a more comprehensive understanding of tissue construction and function.

SECTION: 2.1Single-cell transcriptomes

In 2009, scRNA-seq technology emerged, making it possible to study the transcriptomes of individual cells. Single-cell sequencing technology requires four main steps: (1) isolation of single cells (2) reverse transcription (3) cDNA amplification (4) sequencing library preparation and sequencing.

Isolation of single cells.Isolation of single cells refers to the process of separating individual cells from a complex tissue or cell population. Accurate and reliable capture is essential for single-cell sequencing. The dissociation methods mainly include mechanical dissociation, enzymatic dissociation and chemical dissociation. Target cells are selected from single-cell suspensions based on specific characteristics such as size, fluorescence, or surface labeling.

Fluorescence-activated cell sorting (FACS)[80]is a widely used high-throughput technique that labels cells with fluorescent dyes or antibodies targeting specific molecules on or within the cell. In flow cytometry, a laser excites a fluorescence marker, which emits a signal that is measured to quantify the molecular content. However, this method is less effective for cells with low marker expression and struggles to distinguish a subset of cells with similar fluorescence markers. Typical sequencing methods include Smart-seq[71], VASA-seq[158], FLASH-seq[73].

Magnetic-activated cell sorting (MACS)[143]is another high-throughput isolation technique that separates and enriches specific cell types by binding magnetic beads to target cell proteins. The beads are conjugated with antibodies or other ligands, but unlike FACS, MACS isolates cells based on surface protein expression rather than gene expression, sorting them into positive and negative populations. MACS is primarily employed for the initial enrichment of cells and does not facilitate precise single-cell sorting as FACS does.

Microfluidic-based techniques exploit the inherent physical properties of cells for separation. These properties encompass cell size, shape, electrical polarizability, electrical impedance, density, deformability, magnetic susceptibility, and hydrodynamic characteristics[69]. In droplet-based microfluidics, individual cells are encapsulated within small droplets that are suspended in an immiscible fluid. Techniques such as InDrop[103], Drop-seq[136], and 10x Chromium[223]build upon this methodology. Microwell-based scRNA-seq methods—such as CytoSeq[53], Seq-Well[64], and Microwell-seq[76]—involve placing cells into discrete wells to ensure that each well contains either a single cell or none at all.

Reverse transcription.RNA cannot be directly sequenced within the cell. After cell lysis, the released RNA must be reverse transcribed to generate complementary DNA (cDNA). The poly(A) tailing method employs an oligo-dT primer that binds to the 3’-poly(A) tail of mRNA, thereby facilitating its reverse transcription into cDNA. During this process, additional nucleotide sequences, such as cell-specific barcodes and uniform molecular identifiers (UMIs) for mRNA, are incorporated to uniquely label each cell and distinguish individual mRNA molecules.

cDNA amplification.Since mRNA is typically present in very low quantities within individual cells, it often proves inadequate for sequencing purposes. Therefore, cDNA amplification is necessary to produce sufficient amounts for subsequent library preparation. The most widely utilized method for this process is PCR-based amplification.

Sequencing library construction.The first step in library preparation involves converting nucleic acids into a sequencing library, where DNA or RNA molecules are ligated to platform-specific adapters.

SECTION: 2.2Spatial transcriptomes

Spatial transcriptomic techniques can be broadly categorized into two main types based on whether positional information is encoded before sequencing: (1) next-generation sequencing-based methods and (2) imaging-based methods.

Next-generation sequencing-based approaches encompass both the earlier microdissection-based techniques and the more widely adopted barcode-based approaches. Microdissection techniques isolate regions of interest through physical segmentation or optical selection, followed by collection for library preparation and sequencing. Microdissection-based techniques include tomo-seq[95], STRP-seq[161], Geo-seq[33], PIC-seq[65], TIVA[130], NICHE-seq[140]. Of these, PIC-seq, TIVA, and NICHE-seq achieve cellular-level resolution[174]. However, physical segmentation is often performed manually, making it time-consuming. Additionally, optical selection requires the insertion of specialized markers into living cells or model organisms, which limits its application to FFPE human samples. Accurately locating spatial locations remains a significant challenge, often resulting in relatively low spatial resolution.

The barcode-based sequencing technique, inspired by scRNA-seq, utilizes cell barcodes to capture poly-adenylated RNA molecules in situ before reverse transcription. This process is facilitated by a capture probe that incorporates a spatial barcode, a unique molecular identifier (UMI), and poly-T oligonucleotides, followed by the synthesis of complementary DNA (cDNA). Spatial barcodes operate analogously to cellular barcodes, ensuring an accurate mapping of transcriptomes obtained from tissue slices back to their original locations. The spatial resolution of this method depends on the distance between adjacent spots, achieving a maximum resolution of approximately, which facilitates subcellular analysis. However, enhancing spatial resolution often leads to compromises in detection sensitivity and gene coverage. Examples of barcode-based sequencing technique include 10x Visium[170], Slide-seq (V2)[171], HDST[191], Stereo-seq[32], Seq-scope[37,101], Decoder-seq[26]. Moreover, Open-ST[162]can generate ST in 3D.

In contrast to barcode-based techniques, image-based methods directly leverage in-situ spatial information without the need for spatial barcodes. Techniques such as in situ hybridization (ISH) and in situ sequencing (ISS) utilize gene-specific complementary DNA or RNA probes that bind to target sequences within fixed cells or tissues. Subsequently, spatial mapping of gene expression is accomplished by imaging, typically employing fluorescence or other markers. However, the number of detectable transcripts is constrained by optical limitations, allowing to detect only a few hundred targets. Ongoing technological advances aim to enhance the multiplexing capability. For example, ISH-based MERFISH[207]utilizes 3-color imaging and can analyze approximately 10,000 genes with only 23 rounds of imaging. Other techniques include seqFISH+[49].

RNA techniques based on in situ sequencing (ISS) can be divided into targeted and untargeted approaches. Targeted ISS involves the binding of RNA or cDNA targets with specifically designed probes, such as padlock probes, followed by rolling-circle amplification (RCA) to replicate these targets for sequencing. In contrast, the untargeted ISS transcribes the transcript into cDNA using standard reverse transcription, which is followed by DNA amplification and sequencing. This approach does not require pre-selection of target genes, but may exhibit lower detection efficiency. Examples of untargeted ISS include STARmap[198], and ExSeq[6].

SECTION: 2.3Database

The volume of sequencing data has grown exponentially with the rapid advances in single-cell and spatial transcriptomics technologies, highlighting the need for curated databases, robust analysis pipelines, and effective visualization tools. This review collects 12 large-scale single-cell sequencing databases (Table1) and 7 spatial transcriptomics databases (Table2). A concise overview is provided in Table3.

Single-cell omics highlights the significance of spatial context, which will increasingly be incorporated to develop multi-omics databases. The establishment of such a database not only emphasizes the integration of data sets from diverse sources, but also necessitates data preprocessing, analysis, visualization, user interaction, and other critical components.

SECTION: 3Challenges and DL Methods in Single-Cell Data Analysis

SECTION: 3.1Data Sparsity

Single-cell transcriptomes typically encompass tens of thousands of genes and exhibit considerable variability in expression across individual cells. Many genes remain inactive within a particular cell type, and even within the same cell type, certain genes may be transiently unexpressed due to the dynamic nature of the transcriptome and fluctuations in the cell cycle state. Consequently, gene expression matrices tend to be highly sparse, which poses challenges in modeling feature spaces, including issues related to the curse of dimensionality, noise, and uncertainty (Fig.5). The overall structure of this section is shown in Fig.4.

The curse of dimensionality arises from the exponential increase in the volume of the space as the number of dimensions grows, which leads to sparsity and makes it more difficult to cover the space effectively with a limited number of observations. Consequently, the similarity between data points diminishes. To address this challenge, feature selection and dimensionality reduction techniques are frequently employed. Traditional methods for dimensionality reduction include parameterized approaches such as principal component analysis (PCA)[135], along with variants like scPCA[22]and FastRNA[111]. Additionally, non-parametric methods such as t-distributed stochastic neighbor embedding (t-SNE)[200,121,104]and uniform manifold approximation and projection (UMAP)[18,100]are also widely utilized.

Nonparametric methods aim to map high-dimensional data into a lower-dimensional space while preserving local structure, typically described in terms of probabilities or metric learning. In contrast, parametric methods model the relationships between data points through explicit mathematical formulations with parameters estimated from training data. DL is a powerful parametric approach that employs neural networks for automated modeling, enabling end-to-end parameter learning directly from data. Compared to traditional methods, DL has demonstrated superior effectiveness in managing complex and high-dimensional feature spaces[145,89,133]. Scvis[44], a nonlinear dimensionality reduction method based on the variational autoencoder (VAE) framework, integrates generative modeling with variational inference (Fig.5(a)). By using two distinct neural network structures, it facilitates bidirectional mapping from high-dimensional data to low-dimensional (i.e., cell embedding) space, thereby preserving the global structure of the data. Consider a high-dimensional scRNA-seq dataset, which comprisescells, wheredenotes the gene expression vector for each cell. It is assumed that the observed data is generated from a low-dimensional prior distribution given by; this is typically modeled as a factorized standard normal distribution expressed as, through a transformation parameterized by. This parameter is difficult to compute directly and is instead approximated using a neural network. For each cell, the generative distribution can be expressed as the following integral:

However, computing the posterior distributionbased on the observed data is intractable. To address this issue, a variational distributionis introduced as an approximation. It is assumed that the variational distribution follows a multivariate Gaussian distribution characterized by meanand standard deviation, both of which are functions of, parameterized by a neural network. The model is then optimized to ensure that similar cells exhibit analogous posterior distributions. Consequently, the low-dimensional latent space effectively preserves the distance relations of the high-dimensional data, leading to efficient dimensionality reduction.

for tree=
forked edges,
grow’=0,
draw,
rounded corners,
node options=align=center,,
text width=2.7cm,
s sep=6pt,
calign=edge midpoint,
text=black,
,
[
Data Sparsity,
[
Curse ofDimensionality, for tree= top_class
[
ParameterizationMethods,
for tree=fill=red!45,generation
[
PCA-based,
generation_more
[
scPCA[22];FastRNA[111],
generation_work
]
]
[
Neural network-based,
generation_more
[
scvis[44];Molho D. et al.[145];Hwang H. et al.[89],
generation_work
]
]
]
[
Non-parameterizationMethods,
for tree=fill=red!45,generation
[
tSNE-based,
generation_more
[
Wang Y. et al.[200];Linderman G. C. et al[121];Kobak D. et al.[104],
generation_work
]
]
[
UMAP-based,
generation_more
[
Seurat v3[18];Kim G. et al.[100],
generation_work
]
]
[
Ensemble-based,
generation_more
[
EDGE[175],
generation_work
]
]
]
]
[
Noise Issues,
for tree=top_class
[
Batch effect correct,
for tree=fill=green!45,gpa
[
NNM-based,
gpa_wide
[
Haghverdi L. et al.[72];Hie B. et al.[81],
gpa_work
]
]
[
Neural network-based,
gpa_wide
[
CLEAR[75];BERMUDA[197],
gpa_work
]
]
]
[
Imputation,
for tree=fill=blue!45,gpa
[
Matrix factorization,
gpa_wide
[
ALRA[122],
gpa_work
]
]
[
Nearest neighbor,
gpa_wide
[
MAGIC[188],
gpa_work
]
]
[
Statistical-based,
gpa_wide
[
ScImpute[118];SAVER[88],
gpa_work
]
]
[
Neural network-based,
gpa_wide
[
DCA[50];scIGAN[209],
gpa_work
]
]
[
Benchmark=fill=gray!45,top_class
topclass_wide
[
scIMC[41];SAE-Impute[14],
topclass_wide
]
]
]
]
[
Uncertainty,
for tree= top_class
[
Statistical-based=fill=green!45, pretraining_more,
pretraining_more
[
scVI[128],
pretraining_work
]
]
]
]

Biological noise in scRNA-seq data arises from the intrinsic randomness of biological systems and variations in cellular states. In contrast, experimental noise reflects non-biological fluctuations due to technical limitations or random errors. Systematic biases, commonly referred to as batch effects, occur due to discrepancies in experimental conditions, instruments, reagents, and procedures across data batches. Furthermore, technical constraints or low capture efficiency often lead to missing values, resulting in a large number of zeros in the gene expression matrix. These zeros can obscure the true biological signal, a phenomenon known as dropout events. Batch effects and dropout events always co-occur in real-world datasets. Research has increasingly focused on batch effect correction and imputation methods to address these challenges.

Batch effect correction improves the comparability of datasets derived from different batches, ensuring that observed differences reflect genuine biological variation. This process involves mapping the identical cell types from various experiments to a common region within a latent space. Traditional methods, such as nearest neighbor matching (NNM)[72,81], address this issue by aligning representations across batches. DL-based approaches focus on the hidden space where samples are mapped to semantic representations, facilitating better learning of the underlying patterns. These methods preserve essential biological signals while filtering out irrelevant features through data reconstruction. The CLEAR algorithm[75], which is based on contrastive learning, improves latent representations by constructing positive and negative sample pairs during training (Fig.5(b)). Similarly, BERMUDA[197]aligns batch distributions using the maximum mean discrepancy (MMD) loss, facilitating the integration of data in the latent space.

Imputation methods are designed to reconstruct missing gene expression values by distinguishing technical noise from real biological zeros, relying on observed data patterns. Various approaches have been developed to address this issue, including traditional matrix factorization techniques, similarity modeling, statistical approaches, and DL strategies. Matrix factorization methods mitigate noise by preserving the dominant low-rank signal while discarding extraneous components. Traditional matrix factorization techniques are based on singular value decomposition (SVD) and non-negative matrix factorization (NMF), such as ALRA[122]. Similarity-based approaches leverage relationships between cells to infer missing values. Actually, they leverage gene expression profiles from other cells. An example is MAGIC[188], which employs a k-nearest neighbor algorithm to smooth and impute data based on local similarities. ScImpute[118]also relies on local structure for imputation. Statistical modeling methods employ predefined or probabilistic models to fit observed data. For example, SAVER[88]assumes gene expression follows a negative binomial distribution modeled through a Poisson-gamma mixture. It estimates the parameters using an empirical Bayesian approach with Poisson LASSO regression, and outputs the posterior means as imputed values. DL methods reframe traditional matrix operations as neural network layers, transforming parameter estimation into an optimization problem. Common frameworks include autoencoder-based models and generative architectures that adaptively learn complex patterns to enhance imputation accuracy.

Autoencoders (AEs) are widely used for imputation, effectively integrating dimensionality reduction and denoising within a unified framework. As unsupervised learning models, AEs transform high-dimensional input data into a lower-dimensional latent space, preserving essential features while eliminating redundant information. This latent representation provides contextual insights that facilitate the imputation process. The decoder reconstructs the original input from this compact representation with the objective of generating an output that closely resembles the initial data. For example, DCA[50]employs an autoencoder with a ZINB noise model to infer key parameters such as the mean, dispersion, and dropout probabilities associated with gene expression data (Fig.5(c)). The decoder produces a denoised reconstruction that is well aligned with the modeled data distribution. Variants of AEs, such as variational autoencoders (VAEs)[102], conditional autoencoders[167], and sparse autoencoders[148], offer additional flexibility tailored to specific applications.

Generative models, such as GANs, address the limitations of similarity-based methods that often lead to over-smoothed imputations. GAN-based models are designed to learn the underlying data distribution and generate new samples that closely resemble the denoised data. ScIGAN[209]generates synthetic single-cell profiles instead of directly estimating missing values from observed data. This strategy minimizes overfitting to dominant cell types while improving imputation for rare cell populations. The distinctive design of scIGAN involves transforming real gene expression data into a two-dimensional image representation. The generator synthesizes gene expression profiles from latent variables, whereas the discriminator distinguishes between real and synthetic images. Both networks are trained adversarially and their performance is refined by iterative competition.

We have collected 12 methods, including scImpute[118], SAVER[88], ALRA[122], MAGIC[188], scTSSR[92], DCA[50], DrImpute[67], DeepImpute[10], AutoImpute[178], scIGANs[209], scGAIN[70], to evaluate their imputation performance on five benchmark datasets[41,14]. The results demonstrate that DCA and scIGANs each achieved the highest imputation consistency across the two benchmarks, with both methods displaying robust clustering performance in four out of five datasets (Fig.3).

Uncertainty issues typically arise from factors that contribute to ambiguity during analysis, decision-making, or prediction. This is caused by insufficient information, measurement errors, inaccurate model assumptions, or other sources of variability. In addition to the aforementioned approaches for addressing batch effects and dropout events, uncertainty quantification can enhance model selection and performance evaluation. This procedure facilitates the mitigation of uncertainties arising from experimental data and model assumptions[61]. An example is scVI[128], which explicitly incorporates batch annotations and addresses batch effects through conditional independence assumptions (Fig.5(d)). This approach effectively isolates batch-related factors from the data, thereby reducing the uncertainties associated with batch differences and improving gene expression analysis. scVI models the observed expressionof each genein each cellas a random sample from a zero-inflated negative binomial distribution (ZINB) denoted as. Here,represents a low-dimensional Gaussian vector that captures biological differences between cells;is a one-dimensional Gaussian variable that accounts for variation due to differences in capture efficiency and sequencing depth, serving as a cell-specific scaling factor; anddenotes the batch annotation of the cell (if available). Employing variational inference and reparameterization techniques, scVI optimizes the posterior distribution via a variational lower bound. By incorporating sources of uncertainty, including cell-specific and batch-dependent features, this approach effectively preserves the information inherent in the original data. In contrast, posterior correction methods may rely on fixed assumptions, which can lead to the loss of critical information or introduce bias.

SECTION: 3.2Data diversity

for tree=
forked edges,
grow’=0,
draw,
rounded corners,
node options=align=center,,
text width=2.7cm,
s sep=6pt,
calign=edge midpoint,
text=black,
,
[
Data Diversity,
[
MultimodalData Alignment, for tree= top_class
[
Multi-omicsData Alignment,
for tree=fill=red!45,generation
[
Feature selection,
generation_more
[
Seurat v3[18];LIGER[204];iNMF[60];scAI[94],
generation_work
]
]
[
VAE-based,
generation_more
[
MultiVI[11];Cobolt[66];scMVAE[230];scMM[144];scMVP[113],
generation_work
]
]
]
[
Alignment ofSC and ST Data,
for tree=fill=red!45,generation
[
Marker-based,
generation_more
[[159],
generation_work
]
]
[
Latent space-based,
generation_more
[
Seurat[173];LIGER[204];Harmony[105],
generation_work
]
]
[
Statistical modeling,
generation_more
[
RCTD[25],
generation_work
]
]
[
Neural network-based,
generation_more
[
soScope[112],
generation_work
]
]
]
[
Integration ofOther Omics Data,
for tree=fill=red!45,generation
[
Neural network-based,
generation_more
[
SpatialData[138];STAligner[226];ST-Net[78],
generation_work
]
]
]
]
[
Integration of Multi-Source Data,
for tree= top_class
[
VAE-based=fill=green!45,gpa,
gpa_wide
[
DAVAE[85],
gpa_work
]
]
[
GAN-based=fill=green!45,gpa,
gpa_wide
[
scAEGAN[98],
gpa_work
]
]
[
Graph-based=fill=green!45,gpa,
gpa_wide
[
GLUE[30],
gpa_work
]
]
[
Benchmark=fill=gray!45,top_class
topclass_wide
[
Luecken et al.[131],
topclass_wide
]
]
]
]

The "central dogma" delineates the flow of genetic information from DNA to RNA and subsequently to proteins, establishing a foundational framework for understanding how gene expression influences cellular functions. Omics data, obtained through high-throughput techniques, provide a systematic characterization of the various molecular components within an organism, including the genome, transcriptome, proteome, and metabolome. Recent advances in multichannel sequencing now allow simultaneous measurement of multiple types of omics data. Current transcriptome-focused multimodal techniques include combinations such as gDNA-mRNA[157,42], mRNA-methylation[7,84], mRNA-ATAC[27,35,134], mRNA-proteome[63,172], and mRNA-methylation-ATAC[199,39]. The observed heterogeneity encompasses intra-sample heterogeneity (resulting from differences in sequencing depth, coverage, and data type), inter-sample heterogeneity (caused by variations in experimental design, sample handling, and sequencing protocols), and variability across species and individuals. The diversity and complexity inherent in single-cell data present significant challenges, particularly when it comes to aligning and integrating paired and unpaired datasets. "Paired" data refers to multimodal datasets derived from the same sample, whereas "unpaired" data consists of multimodal datasets from different samples or platforms. Multi-omics analysis integrates these diverse data types to facilitate a comprehensive understanding of organismal heterogeneity and regulatory mechanisms. The overall structure of this section is shown in Fig.6.

This section focuses on the alignment of multimodal data, including multi-omics data as well as paired single-cell and spatial transcriptomics data, with the goal of uncovering intrinsic patterns in the alignment of homologous data.

Multi-omics data alignment aims to align similar features while preserving the unique characteristics of each modality. LIGER[204]employs non-negative matrix factorization (NMF) to uncover latent structures in the data, extracting both shared and modality-specific gene expression patterns while minimizing distances between datasets (Fig.7(a)). iNMF[60]builds on LIGER by enabling online learning for enhanced data integration. Other comparable methods include scAI[94]. MultiVI[11]adopts a VAE framework where the encoder processes different molecular attributes, such as protein abundance, to generate modality-specific latent representations, denoted asand. The cell state is estimated as the average of these two representations:, thereby forming a joint latent space of multiple modalities. Modality-specific decoders then generate the observed values using a negative binomial distribution for transcriptomic data and a Bernoulli distribution for chromatin accessibility data. A constraint is imposed on the latent space to minimize the distance between these two representations. Additional VAE-based models include Cobolt[66], scMVAE[230], scMM[144], and scMVP[113]. scMVP maximizes the likelihood of jointly generated probabilities across multi-omics data by introducing a Gaussian mixture model (GMM) prior to obtain a shared latent embedding. Each modality is encoded separately using an asymmetric GMM-VAE model that incorporates two clustering consistency modules to align each imputed dataset while preserving the shared semantic information.

Spatial mapping in ST involves aligning scRNA-seq data with physical spatial domains, matching the geometry of the spatial data. Traditional methods have aimed to reconstruct key marker genes by assuming continuity in gene expression or using local alignment information[159]. However, these methods are hindered by limited capture rates, significant dropout events, and sparse gene distribution, making them error-prone and unable to generalize across different experimental settings. More recent approaches, such as Seurat[173], LIGER[204], and Harmony[105], integrate scRNA-seq with ST data through shared latent spaces and mutual nearest neighbors (MNN). This integration facilitates the transfer of cell-type labels while enhancing weak ST signals. RCTD[25]integrates reference scRNA-seq data to model the average gene expression profiles of different cell types. It utilizes a hierarchical model to estimate the proportion of each cell type within individual spatial spots. The method applies a Poisson distribution to estimate gene expression counts and employs maximum likelihood estimation (MLE) for parameter estimation. soScope[112]adopts a multimodal DL framework that integrates spot-level omics maps (transcript, histone, DNA, protein), spatial relationships, and high-resolution morphological images. It jointly infers high-resolution spatial maps using a variational Bayesian inference network (Fig.7(d)).

Many open-source frameworks have been developed to facilitate the alignment and integration of multimodal spatial omics data, including SpatialData[138], SSGATE[132], STAligner[226]and SpatialGlue[127]. For example, SpatialGlue[127]can be used to integrate three modalities, including spatial epigenome–transcriptome and transcriptome–proteome modalities. For aligning 2D slices, STAligner[226]employs a triplet-based approach to identify mutual nearest neighbors that exhibit similar gene expression patterns across different slices. This method facilitates coordinate registration of stacked consecutive slices, enabling 3D tissue reconstruction (Fig.7(c)). ST-Net[78]integrates paired H&E-stained pathology images with ST data to train an end-to-end neural network for predicting spatially resolved transcriptomics from pathology images.

Recent studies have highlighted advancements in ST techniques[86], although the field is still challenged by a trade-off between spatial resolution and measurement throughput. spatial transcriptomics, bridging imaging and sequencing, holds great potential for integrating diverse modalities from histopathology and single-cell data, offering deeper insights into spatial organization, microenvironmental interactions, and histopathology. Moving forward, the integration of multimodal data remains a key challenge in single-cell and spatial transcriptomics analysis.

Integration of unpaired datasets, such as those derived from different samples or sequencing platforms, usually requires alignment of independent feature spaces. In this context, cross-modal integration aims to mitigate discrepancies in embeddings of the same cell type across heterogeneous modalities. Seurat v3[173]identifies common anchors (cell pairs) across datasets based on features (such as genes) that exhibit high variability among cells, integrating data using these anchors (Fig.7(b)). It can integrate scRNA-seq with scATAC-seq, allowing for an investigation into chromatin differences.

Generative DL models have been widely employed to capture complex semantic relationships in multi-source datasets. For example, DAVAE[85]integrates large-scale unpaired data through three essential components: a variational approximation network, a generative Bayesian neural network, and a domain adversarial classifier. This setup benefits the learning of latent representations, enhances data fitting, and mitigates batch effects to accurately represent cellular biological states across diverse datasets.
Similarly, scAEGAN[98]combines autoencoders (AE) with conditional generative adversarial networks (cGAN) to facilitate the translation between different single-cell datasets. It effectively transforms the dataset by using AE to remove random noise, while employing cGAN with recurrent consistency regularization. GLUE integrates omics-specific autoencoders with graph-based coupling and adversarial alignment to model regulatory interactions between omics layers, supporting integrated regulatory inference for unpaired multi-omics datasets (Fig.7(b)).

Recent benchmark for multi-source data integration has collected 19 methods for data integration on seven benchmark datasets, as detailed by Luecken et al.[131], indicates that NMF-based and nearest neighbor approaches exhibit superior performance in average bio-conservation and batch correction across all datasets, respectively. We also observed the effectiveness of the VAE-based and GAN-based model in preserving biological consistency (Fig.8).

SECTION: 3.3Data scarcity

The overall structure of this section is shown in Fig.9.

for tree=
forked edges,
grow’=0,
draw,
rounded corners,
node options=align=center,,
text width=2.7cm,
s sep=6pt,
calign=edge midpoint,
text=black,
,
[
Data Scarcity,
[
Missing Data Annotation, for tree= top_class
[
Statistical modeling=fill=green!45,gpa,
gpa_wide
[
scDesign3[168],
gpa_work
]
]
[
GAN-based=fill=green!45,gpa,
gpa_wide
[
GRouNdGAN[229],
gpa_work
]
]
[
Benchmark=fill=gray!45,top_class
topclass_wide
[
Pratapa A. et al.[151];Cao Y. et al.[29],
topclass_wide
]
]
]
[
Missing Modalities,
for tree= top_class
[
VAE-based=fill=green!45,pretraining,
pretraining_wide
[
TotalVI[62];UniPort[28];POE[206];MOE[166];CGVAE[123],
pretraining_work
]
]
[
Benchmark=fill=gray!45,top_class
topclass_wide
[
Hu Y. et al.[86];Makrodimitris S. et al.[137],
topclass_wide
]
]
]
]

Single cell data has reached sequencing scales of hundreds of millions. Due to the significant labor and time required in laboratory settings, existing datasets often present challenges in obtaining large-scale biological annotations. In addition, single-cell data contains many complex biological factors, making it difficult to obtain a reasonable and reliable ground truth. For instance, benchmark datasets for experimental analysis of single-cell population evolution require follow-up samples with known evolutionary trajectories and developmental timelines, which are difficult to obtain under experimental conditions[106]. Moreover, the reliability of model evaluation often depends on high-quality data annotations. For example, since regulatory interactions in databases are aggregated from broad datasets and lack specificity to particular biological systems, it is unreliable to evaluate the performance of gene regulatory network (GRN) inference algorithms. A key technical solution to this problem is the construction of simulation datasets.

It has been extensively employed to evaluate and compare computational methods, concentrate on specific biological features, and establish more precise benchmarks[151]. Cao et al.[29]conducted a comprehensive review of various simulation approaches and proposed a framework for systematic benchmarking, highlighting their ability to capture biological signals and higher-order interactions, such as the mean-variance relationship among genes. However, most existing methods generate simulated data tailored to specific evaluation objectives, such as clustering or differential gene expression analysis. There are few tools specifically designed to create datasets that are applicable across diverse scenarios.

scDesign3[168]employs statistical modeling methods to generate single-cell multi-omics data and spatial transcriptomics data with known cell proportions. It standardizes generative modeling approaches across various data modalities, rather than focusing on only one modality. Given a cell state covariate(factors such as cell type, cell pseudotime, and cell spatial locations) and experimental design covariates(such as batch effects and experimental conditions), the measurement valuesare modeled according to a specific distribution. This is formulated as a generalized additive model for location scale and shape (GAMLSS) , characterized by its position, proportion, and shape parameters.

The model is parametrically represented, incorporating specific link functions for each feature(distribution functions, such as Gaussian (Normal), Bernoulli, Poisson, ZINB) These link functions correspond to the mean parameter, the scale parameter(for example, standard deviation or dispersion), and zero-inflation proportion parameter. For instance, the specific link functionsfor featuresmaps the mean parameterto the model’s linear predictor. This mapping depends on the chosen distribution function and consists of four key components:

The specific interceptfor feature, represents the mean of featurein the absence of other influencing factors. The batch effect, captures the influence of batchon feature. The conditional effectdenotes the impact of conditionon feature. The cell state covariates, such as the effects associated with different cell types on feature, are also considered. In scDesign3, the joint distribution of cellular features is constructed using a marginal cumulative distribution function and a copula model with parameters estimated by a maximum likelihood method. These parameters can be adjusted to generate synthetic data reflecting varying sequencing depths and cell states. Furthermore, scDesign3 is capable of producing spatial transcriptomic data based on cell type proportions derived from single-cell sequencing, simulating realistic ATAC-seq datasets at both count and read levels, and generating multi-omics datasets by integrating separate omics datasets like RNA expression or DNA methylation.

In summary, statistical modeling provides a highly interpretable framework for data generation and analysis, and its integration with DL is emerging as a significant trend.

GlouNdGAN[229]is a causal model-based data generation method that allows the generation of realistic simulated data aligned with the underlying principles of GRN. The architecture of GlouNdGAN consists of five sub-networks: Causal Controller, Target Generator, Critic, Labeler, and Anti-labeler. The Causal Controller generates expression values for transcription factors (TFs), while the Target Generators produce expression values for target genes based on the causal GRN framework. The Critic estimates the Wasserstein distance between the generated data and the real data to ensure that the target gene expression is causally related to TF expression. The Labeler predicts TF expression based on generated and real target gene expression, while the Anti-labeler estimates TF expression solely from generated target gene expression. This model pre-trains the TF expression generation module and subsequently generates expression values for other genes via the Target Generator. GlouNdGAN allows researchers to simulate interference by manipulating TF expression values during the inference phase, enabling an accurate comparison of gene expression before and after interference while maintaining constant parameters. Additionally, by performing mutation experiments on TF expression for certain cell types, the researchers observed alterations in the characteristics of the generated cells, thus validating the function of TFs and their relationship to phenotypic labels. This capability positions GlouNdGAN as an ideal tool for in-situ interference experiments.

As discussed above, simulation data generation serves as a valuable tool for elucidating biological mechanisms in contexts where high-quality data is lacking. It allows the creation of diverse datasets with controllable parameters and facilitates the evaluation of model performance.

Although genetic information is transferred from DNA to RNA and then to proteins, each modality captures distinct biological information, making it impossible for one modality to substitute for another. It has been demonstrated that multimodal analysis enhances the overall understanding of cellular heterogeneity. However, multi-channel sequencing typically incurs higher costs compared to single-channel sequencing. DL-based solutions commonly rely on VAE architectures that use either single-modality or multi-modality joint embeddings for shared latent space modeling[137]. The difference lies in how the latent variables are modeled (Fig.10). TotalVI[62]is trained on the joint embeddings of the two modalities with separate reconstruction. UniPort[28]trains a single-modality embedding to reconstruct two different modalities, compelling the encoder to learn features that are predictive of both. In the Product of Experts (POE) model[206], the joint latent variable is derived as a product of each modality’s. Unlike POE, Mixture of Experts (MOE)[166]employs the sum of the joint latent variables for data reconstruction. Constrained Graph Variational Autoencoders (CGVAE)[123]learns feature embeddings for each modality individually while applying constraints to ensure that each modality can reconstruct both itself and the other modalities. Based on the benchmark results, Makrodimitris, S. et al.[137]concluded that different joint embeddings can be used for different downstream tasks.

Scenarios of modality completion are often related to data sparsity. Monae[181]employs data imputation to perform data denoising and modality completion simultaneously, constrained by a cross-modal prediction loss. In the feature extraction phase, a graph encoder-decoder reconstruction process extracts embedding features from multiple modalities, and contrastive learning is applied to minimize the spatial distance between embeddings of the same cell type. Therefore, when discriminative information from one modality is lacking, the latent space embeddings of other modalities can be leveraged for reconstruction. UnitedNet[180]combines multimodal ensemble with cross-modal prediction in a multi-task learning framework, trained with cross-modal prediction loss alongside generator and discriminator losses.

We have collected 17 methods, including BABEL[205], CMAE[213], LIGER[204], Seurat[173], cTP-net[227], scArches[129], scMoGNN[108], scVAEIT[47], sciPENN[107], totalVI[62], Generalized Linear Model (GLM), MCIA[142], MOFA[9], CGVAE[123], ccVAE[176], PoE[206], MoE[166], to evaluate their modality prediction performance on four benchmark datasets[86,137]. Among all the methods, totalVI shows highest cell-cell PCC on predicted modalities and PoE shows better performance than other VAE-based models (Fig.11).

SECTION: 3.4Data correlation

Understanding the relationship between biological systems and external factors is crucial to gain deeper insights into cellular dynamics and the interactions between cells and their environment. Modeling data correlation involves capturing these complex interactions and dependencies, which are affected by both spatial and temporal variations, as well as biological prior knowledge. The overall structure of this section is shown in Fig.12.

for tree=
forked edges,
grow’=0,
draw,
rounded corners,
node options=align=center,,
text width=2.7cm,
s sep=6pt,
calign=edge midpoint,
text=black,
,
[
Data Correlation,
[
Modelingspatiotemporaldependencies, for tree=top_class
[
Regression-based=fill=green!45,gpa,
gpa_wide
[
Walter F. C. et al.[192];Townes F. W. et al.[186];Äijö T. et al.[3];MEFISTO[189],
gpa_work
]
]
[
Graph-based,
for tree=fill=green!45,gpa,
[
MRF-based,
gpa_wide
[
Zhu Q. et al.[228];Giotto[46],
gpa_work
]
]
[
GNN-based,
gpa_wide
[
DeepLinc[117];NCEM[57],
gpa_work
]
]
[
CNN-based,
gpa_wide
[
Tan X. et al.[179],
gpa_work
]
]
[
RNN-based,
gpa_wide
[
Almagro A. et al[5],
gpa_work
]
]
]
]
[
Modeling prior knowledge,
for tree= top_class
[
Neural network-based,
for tree=fill=green!45,pretraining,
[
Pathway information,
pretraining_wide
[
Yan H. et al.[211],
pretraining_work
]
]
[
Regulatory networks,
pretraining_wide
[
GLUE[30];Yan H. et al.[211];stImpute[220];GRNInfer[115],
pretraining_work
]
]
]
[
Benchmark=fill=gray!45,top_class
topclass_wide
[
Xie Z. et al.[208];CITEdb[164];DeepCCI[214];Liu Z. et al.[125],
topclass_wide
]
]
]
]

Modeling temporal and spatial dependencies is crucial for the analysis of single-cell and spatial transcriptomics data, as numerous biological processes exhibit dynamic spatiotemporal correlations. Examples include cell differentiation during development[16], the spatial organization of cells within tissues[109], disease progression pathways[163], and variations in immune responses over time and space. Capturing these features can reveal dynamical shifts in cell states, cell-cell interactions, and complex tissue or disease structures. Spatiotemporal omics data encompass longitudinal molecular profiles from patients, molecular profiles across developmental stages, and continuous spatiotemporal omics maps. However, making comparisons across varying scales, biological samples, or conditions remains a challenging task. For example, establishing statistical correlations between samples requires the alignment of temporal and spatial coordinates across individuals or biological scales. Current approaches mainly rely on regression-based and graph-based models[190].

Among regression-based models, Gaussian process-based probabilistic models are widely used[192,186,3]. These models are effective in capturing trends of continuous variability for both time series and spatially distributed data. MEFISTO[189]leverages the Gaussian process to model latent factors by incorporating temporal and spatial information, as well as grouped data, into the covariates within the Gaussian kernel. The covariance function consists of two components: one that describes relationships across different groups (e.g., sample sets or experimental conditions), and another that captures smooth variations such as spatial locations or time points. This dual structure allows Gaussian processes (GP) to account for both inter-group heterogeneity and intra-group covariate variation. By ensuring that samples located closer together in the covariate space share more similar latent factors, the Gaussian process effectively models the continuity of relations between data points.

Another approach for jointly modeling omics latent featuresinvolves the use of graph models. Markov random fields (MRF) are undirected graphical models that assume the distribution of each node depends only on the labels of its neighboring nodes. Compared to non-parametric regression models like GP, MRF offer greater computational efficiency, as they do not require inference of the complete covariance structure across all samples. Qian Zhu et al.[228]proposed a Markovian property for spatial patterns, which constrains the correlations between neighboring nodes. By assuming that labels of neighboring cells, including gene expression states or cell types, exhibit a degree of similarity, the joint probability distribution over the spatial domain can be factorized into a product of local neighborhood probability distributions. The probability distribution for the label associated with the current nodeis jointly modeled using both its neighboring nodes’ labelsand its own gene expression:

Giotto[46]utilizes MRF with conditional probability distributions, such as Gaussian or Poisson, to enhance spatial clustering. This approach effectively captures smooth and continuous expression changes, thereby facilitating the identification of spatially structured cell populations.

DL-based graph frameworks are increasingly employed to explore cell-cell interactions, including recurrent neural network (RNN), CNN, and GNN.
In GNNs, cells are represented as nodes, with edges denoting potential interactions, such as those between ligand-receptor pairs. This approach effectively integrates spatial data and gene expression profiles to reveal interaction patterns. For instance, DeepLinc[117]posits that neighboring cells are more likely to interact than cells further apart (Fig.13(a)). It constructs a cell adjacency graph based on the physical distance between cells and learns embedding features that reflect the likelihood of interactions by aggregating information from each cell along with its neighbors. Using variational graph autoencoders (VGAEs) and adversarial networks, DeepLinc employs unsupervised learning techniques to uncover intrinsic associations between the cell adjacency graph and gene expression profiles, thereby reconstructing interaction networks. NCEM[57]utilizes GNN to reconstruct gene expression vectors from cell type labels and niche composition, which are represented through graph-level predictors and adjacency matrices derived from spatial proximity. While NCEM incorporates a linear model for mathematical interpretability, experimental results demonstrate that its nonlinear variant significantly outperforms the linear one. Different from GNNs, CNNs[179]aggregate features from neighbouring regions in images through convolution operations. RNNs[5], on the other hand, propagate information from adjacent spatial points using recurrent connections.

Overall, DL frameworks exhibit considerable flexibility in analyzing spatiotemporal omics data.

Single-cell data is characterized by sparse features, multi-source heterogeneity, and lack of high-quality labels, making it unreliable to draw experimental conclusions solely from the observed data. However, the incorporation of prior biological knowledge such as gene regulatory networks, cell type characteristics, and developmental trajectories, can significantly enhance the accuracy and interpretability of the analysis. Nonetheless, effectively integrating prior knowledge while avoiding potential biases and overfitting remains a challenging task.

Prior knowledge mainly refers to pathway information and regulatory networks[211]obtained from databases. Pathway information concerns to molecular interactions and biochemical reactions that drive specific biological processes, such as signal transduction, metabolism, and cellular activity. This information aids in elucidating how cells respond to external stimuli or internal changes. In the context of single-cell and multi-omics analyses, pathway information is used to infer gene-gene interactions, characterize cell types, and determine cell states. It is typically sourced from well-established databases such as KEGG[96], Reactome[52], and WikiPathways[2]. Regulatory networks involve the interactions among genes, transcription factors, proteins, and other biomolecules that regulate gene expression and cellular function. These networks are commonly represented as graph where nodes denote biomolecules (e.g., genes or proteins) while edges indicate regulatory relationships (e.g., activation or inhibition). Databases such as STRING[177]and GeneMANIA[202]provide valuable insights into protein-protein interactions and gene-gene interactions, respectively. Regulatory networks facilitate the understanding of the mechanisms governing gene expression regulation, the identification of key regulatory factors, and the revelation of cell-specific patterns in gene expression.

GLUE[30]integrates multi-omics data through a guidance graph, where nodes represent features from various modalities, such as genes in scRNA data and accessible chromatin regions in ATAC-seq data(Fig.13(b)). Graphs establish connections between ATAC peaks and RNA genes based on overlapping gene bodies or promoter regions. A variational posterior is employed to reconstruct the guidance map and its latent space is used as a prior for multi-omics data reconstruction. The decoder computes an inner product of feature and cell embeddings to ensure consistent embedding directions across different modalities. Hongxi Yan et al.[211]aggregate gene features within the same biological pathway to obtain pathway-level features for predictive modeling. They utilize the KEGG database together with an ensemble gradient method to identify key pathways, which significantly enhances model interpretability.

Some methods use prior knowledge from existing databases to initialise edge features in gene-gene interaction networks. For instance, DeepCCI[214]uses the constructed LRIDB database to define receptors and establishes interaction networks between cell clusters based on known ligand-receptor (L–R) pairs, predicting interactions by a combination of ResNet and graph convolutional network (GCN) outputs (Fig.13(c)). stImpute[220]leverages the ESM-2 protein language model to embed proteins and constructs a network of gene relationships using cosine similarity. GRNInfer[115]incorporates gene regulatory relationships from RegNetwork[126]as prior information to construct a gene graph network.

Furthermore, we have collected 10 methods, including CellChat[93], CellPhoneDB[48], iTALK[201], LIANA[43], NATMI[83], scMLnet[36], SingleCellSignalR[24], Connectome[155], CytoTalk[87], and CellCall[222]. These methods leverage existing L-R pair knowledge to infer cell-cell communication, and we evaluate their cell-cell interaction prediction performance on five benchmark datasets[208,164,214,125]. Among all the methods, CellPhoneDB ranks among the top across all benchmark datasets, demonstrating the robustness of extracting cellular context (Fig.14).

SECTION: 4Conclusion and Future Perspectives

SECTION: 4.1Innovative AI Method for Single-Cell and spatial transcriptomics data analysis

The rapid expansion in the size, depth, and complexity of single-cell and spatial transcriptomics data requires the development of algorithms capable of effectively capturing complex gene expression patterns and spatial distributions.

Recently, foundational models have emerged as a focus of single-cell omics. By leveraging self-supervised pre-training on large unlabeled scRNA-seq datasets, these models capture complex features and patterns, producing unified representations that can be fine-tuned for specific downstream tasks[212,185,77,40]. For instance, SCimilarity[79]enables rapid querying of cell states for cell type annotation. scMulan[20]converts single-cell transcriptomic data along with rich metadata (e.g., cell type, spatial context, and temporal aspects) into "cell sentences" (c-sentences), achieving superior performance in tasks like zero-shot cell annotation and batch correction. scGPT[40], which is based on the GPT architecture, employs self-supervised pretraining with condition tokens to model gene interactions within cells, incorporating cell type labels for tasks such as cell type prediction, and applies a "binning" technique to ensure semantic alignment across diverse datasets.

Future advancements in large language models, such as OpenAI’s O1111https://openai.com/o1/, and agent-based methods, are expected to further enhance single-cell and spatial transcriptomics analysis. O1 leverages large-scale reinforcement learning algorithms to achieve chain-of-thought (COT) reasoning, thereby improving inference accuracy. Agent-based approaches, such as ReAct[216], integrate real-time observations to guide decision-making, facilitating more efficient and proactive error correction. These models offer considerable potential for providing interpretable inferences across a variety of downstream tasks in single-cell analysis.

However, in contrast to parametric modeling approaches, end-to-end networks often operate as "black boxes," providing limited interpretability of the inference processes. Moreover, these networks are typically trained on specific datasets and lack dynamic updates, which constrains their generalizability and robustness on unseen or uncertain data. For scientific scenarios, it is crucial to strike a balance between interpretability and accuracy. As indicated by previous studies, it may be feasible to enhance interpretability by establishing connections between prior models and observed outcomes through interpretable parametric rules.

SECTION: 4.2Benchmark Datasets and Evaluation Metrics

Relevant benchmarks have been established for various stages of single-cell sequencing data analysis workflows, including imputation[41], cell identification[1], clustering[193], gene regulatory networks[13], cell-cell interactions[195], and multi-omics data integration[12]. However, several studies have mentioned that the datasets and evaluation metrics employed in these benchmarks do not accurately reflect the strengths and weaknesses of contemporary algorithms[151]. Moreover, as large-scale sequencing data continues to evolve, algorithms that previously demonstrated strong performance may no longer be applicable in different application contexts[99]. These datasets may exhibit increased heterogeneity due to samples derived from diverse sequencing platforms or continuous-time and continuous-space settings.

Current evaluation metrics mainly emphasize performance metrics such as accuracy, AUC, and RMSE, with a limited focus on biological relevance. To ensure the biological validity of model predictions, systematic validation through in vitro experiments is essential. For example, trends in gene expression, molecular properties, or cellular behavior can be compared to experimental results to confirm biological significance. Therefore, it is important to establish benchmarks that provide a more comprehensive and objective assessment of the generalizability and applicability of algorithms. Data simulations that generate benchmark datasets based on well-defined rules can provide a diverse array of labeled data for evaluation. This approach has already been applied to tasks such as cell identity recognition and modelling of gene regulatory networks[168,229], highlighting its potential as a robust tool for evaluating model performance.

SECTION: 4.3Application of DL in practical scenarios

Here, we summarize the applications of single-cell and spatial transcriptomics in biology, medicine, and clinical practice, providing an overview of the background for DL applications in these fields.

In biology, single-cell and spatial transcriptomics focus on embryonic[152], tissue[45], and organ development[21]. These techniques facilitate the identification and classification of various cell types and lineages, while providing insights into the evolution of cell populations throughout organogenesis.

In precision medicine, the analysis of single-cell transcriptomic data is critical for investigating disease heterogeneity[74], identifying distinct subclones within diseases[154], discovering critical disease biomarkers[4], characterizing interactions between normal and diseased cells[215], elucidating relevant signaling pathways[91], and predicting resistance[203]. Single-cell transcriptomics facilitates the construction of comprehensive cellular maps that significantly enhance the discovery of novel biomarkers and therapeutic targets[187]. In additon, scRNA-seq has demonstrated significant potential for improving patient outcomes and accelerating the development of personalized therapies[182,90].

In clinical applications, scRNA-seq plays a crucial role in characterizing patient-specific features[169]. It assists in the identification of biomarkers for patient stratification[149], elucidates the underlying mechanisms of drug action and resistance[153], supports the development of personalized treatment strategies, and enables monitoring of drug response and disease progression[124].

SECTION: 5

Key Points•This review discusses four major challenges and related deep learning approaches in single-cell and spatial transcriptomics data analysis.•This review curates 21 datasets from 9 benchmarks covering 58 computational methods and compares their performance on their respective modeling tasks.•This review outlines three future research directions regarding data, methods, and applications for single-cell and spatial omics data analysis.

SECTION: 6Competing interests

No competing interest is declared.

SECTION: 7Author contributions statement

Shuang Ge and Zhixiang Ren collected and reviewed literature. Shuang Ge, Shuqing Sun, Qiang Cheng and Zhixiang Ren drafted the manuscript. All authors read and approved the final manuscript.

SECTION: References