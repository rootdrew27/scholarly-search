SECTION: YOLO11 and Vision Transformers based 3D Pose Estimation of Immature Green Fruits in Commercial Apple Orchards for Robotic Thinning
In this study, a robust method for 3D pose estimation of immature green apples (fruitlets) in commercial orchards was developed, utilizing the YOLO11 object pose detection model alongside Vision Transformers (ViT) for depth estimation. For object detection and pose estimation, performance comparisons of YOLO11 (YOLO11n, YOLO11s, YOLO11m, YOLO11l and YOLO11x) and YOLOv8 (YOLOv8n, YOLOv8s, YOLOv8m, YOLOv8l and YOLOv8x) were made under identical hyperparameter settings among the all configurations. Likewise, for RGB to RGB-D mapping, Dense Prediction Transformer (DPT) and Depth Anything V2 were investigated. It was observed that YOLO11n surpassed all configurations of YOLO11 and YOLOv8 in terms of box precision and pose precision, achieving scores of 0.91 and 0.915, respectively. Conversely, YOLOv8n exhibited the highest box and pose recall scores of 0.905 and 0.925, respectively. Regarding the mean average precision at 50% intersection over union (mAP@50), YOLO11s led all configurations with a box mAP@50 score of 0.94, while YOLOv8n achieved the highest pose mAP@50 score of 0.96. In terms of image processing speed, YOLO11n outperformed all configurations with an impressive inference speed of 2.7 ms, significantly faster than the quickest YOLOv8 configuration, YOLOv8n, which processed images in 7.8 ms. This demonstrates a substantial improvement in inference speed over previous iterations, particularly evident when comparing YOLO11n and YOLOv8n. Subsequent integration of ViTs for the green fruit’s pose depth estimation revealed that Depth Anything V2 outperformed Dense Prediction Transformer in 3D pose length validation, achieving the lowest Root Mean Square Error (RMSE) of 1.52 and Mean Absolute Error (MAE) of 1.28, demonstrating exceptional precision in estimating immature green fruit lengths. Following this, the DPT showed notable accuracy improvements with a RMSE of 3.29 and an MAE of 2.62. In contrast, measurements derived from Intel RealSense point clouds exhibited the highest discrepancies from the ground truth, with a RMSE of 9.98 and an MAE of 7.74. These findings emphasize the effectiveness of YOLO11 in detecting and estimating the pose of immature green fruits, illustrating how Vision Transformers like Depth Anything V2 adeptly convert RGB images into RGB-D data, thus enhancing the precision and computational requirement of 3D pose estimations for future robotic thinning applications in commercial orchards.

KeywordsYOLO11Vision TransformersDepth AnythingDense Prediction TransformerYOLO

SECTION: Introduction
In United States (US) agriculture, labor shortage is one of the biggest problem, and automation and robotics have been instrumental in reducing labor efforts and enhancing crop production. For example, precision planting systems automate seed placement with exact spacing and depth, optimizing growth conditions and reducing the need for manual labor. Likewise, robotic weed control systems identify and eliminate weeds without human intervention, using advanced sensors and precise application of herbicides or mechanical removal techniques. Furthermore, in harvesting, autonomous combine harvesters and fruit-picking robots increase efficiency by performing tasks faster and with greater accuracy than human workers. Similarly, automated irrigation systems use weather forecasts and soil sensors to regulate water delivery, minimizing wastage and labor associated with manual watering.

However, the commercial production of specialty tree fruit crops remains heavily reliant on extensive manpower because of paramount need of precision and care for crop-load management operations such as tree pruning, fruit thinning, and harvesting. For example, apple (Malus domestica) orchards require significant labor inputs for pruning to shape the trees and improve sun exposure to the fruits, which is essential for enhancing fruit quality and yield. Similarly, the peach (Prunus persica) industry depends on manual thinning to regulate fruit size and prevent branch damage due to overbearing. Likewise, cherries (Prunus avium), known for their delicate nature, necessitate careful hand-picking to avoid damage to both the fruit and the trees, a task that machines are currently unable to perform with sufficient gentleness. Additionally, in the cultivation of pears (Pyrus communis), precise manual thinning is critical to avoid alternation in bearing years and to ensure consistent fruit quality. Additionally, plums (Prunus domestica) also require extensive manual labor for thinning and pruning to maintain fruit quality and manage tree health.

Among these crop-load management operations, immature green fruit thinning, a centuries-old practice is essential in the commercial cultivation of tree fruit crops for optimization of fruit size and quality by reducing competition for water, sunlight, and nutrients among fruits. This practice is particularly crucial for apple production, which ranks second in consumption worldwide among major fruitsand the third most consumed fruit in the United States. The challenges of manual thinning, combined with persistent labor shortages and the increasing costs associated with securing skilled labor, underscore the urgent need for innovative solutions. Each year, over a hundred thousand of worker come to the US for farm labor. Example of such farm workers are shown in Figure, which was taken in the commercial apple orchard in Prosser, Washington State, USA. The Figurevisually encapsulates the labor-intensive nature of fruit thinning in commercial orchards, highlighting the traditional methods still in use. Figurea illustrates the painstaking process of manual fruitlet thinning performed by a worker in a commercial ’Scilate’ apple orchard in Prosser, Washington State, USA, during the year 2024. The image captures a laborer navigating between trees with an aluminum ladder, which he uses to access fruit clusters up to 14 feet high. This method not only demands physical agility to climb and reposition the ladder continuously but also poses safety risks inherent in working at heights.

In contrast, Figureb depicts an alternative approach where multiple workers operate from a mechanized platform that navigates the rows of the orchard. While this method reduces the need to climb, it still requires significant manual effort. Workers must bend and stretch extensively to reach fruitlets in dense clusters, as demonstrated in Figurec. In manual fruit thinning, workers are shown actively managing high-density areas of immature green fruits, a necessary task given the overproduction tendencies of fruit trees. This physically demanding work not only entails high labor costs but also poses significant health risks to workers, particularly related to spinal injuries and other musculoskeletal disorders. Studies have highlighted that the repetitive and strenuous postures required in manual thinning, such as excessive back bending and upper arm elevation, can lead to severe physical strain. Specifically, while platforms can reduce upper arm strain compared to ladders, they do not significantly alleviate the stress on workers’ backs, who must still engage in substantial torso flexion. Furthermore, the agricultural sector is notorious for the prevalence of upper limb injuries, with a systematic review revealing that a substantial number of these injuries occur during manual tasks like harvesting, often leading to long-term disabilities and economic consequences.

Despite the advent of some mechanization, the reliance on manual labor in fruit thinning continues due to the precision required in these tasks, underscoring an urgent need for more innovative, automated solutions. Such advancements would not only enhance efficiency but also crucially reduce the physical strain on workers, potentially decreasing the incidence of work-related injuries in commercial fruit production. The current state of labor in agriculture calls for a shift towards systems that prioritize worker health and safety by integrating technology capable of replicating the delicate nature of manual thinning without the associated physical risks.

The primary goal of this study is to address the challenges associated with manual thinning in tree fruit agriculture, especially during the immature stages of the fruit when they are all green and similar in appearance. We employ advanced machine vision techniques to facilitate automation in managing orchards. Using the latest variant, YOLO11 (You Only Look Once, version 11), a sophisticated deep learning algorithm, our study aims to develop a system that can accurately detect and determine the position and orientation, or ’pose’, of immature fruitlets in standard RGB (red, green, blue) image space. Further, we enhance the precision of our pose estimations by integrating the Depth Anything model, a tool that performs monocular depth estimation from RGB images. By comparing these estimations to actual ground truth measurements in RGB-D (RGB + Depth) space, we validate the effectiveness of our approach, setting the stage for subsequent automation efforts in orchard management.

The specific objectives of this study can be summarized into following points:

To deploy a robotic platform to collect high-resolution RGB images of immature fruitlets within a commercial orchard environment for assembling a comprehensive dataset.

To train and assess the performance of the YOLO11 and YOLOv8 models under similar hyperparameter settings to detect and estimate the pose of immature green fruits. Implement the best-performing model to estimate the pose of fruitlets from RGB images.

To convert the RGB images into 3D point clouds to enhance the pose estimations provided by the best-performing deep learning model using Vision Transformers: Dense Prediction Transformer (DPT) and Depth Anything V2.

: To validate these depth estimations against field-level measurements taken in commercial orchards, comparing the major axis values of immature green apple fruits, collected with vernier caliper, to their respective point cloud data.

SECTION: Related Works
SECTION: Deep Learning-based Fruit Detection and Pose Estimation:
Traditional methods for apple detection, crucial for robotic thinning, have predominantly focused on mature fruit suitable for harvesting, utilizing basic image processing techniques. These methods, however, often fall short in complex natural environments where young fruits exhibit colors and textures similar to foliage, complicating their detection. Recent advancements in machine learning and deep learning have introduced more sophisticated models, such as YOLOv3 and its variations, which significantly enhance detection capabilities by learning from diverse datasets without explicit programming. Despite these technological advances, few studies have targeted the detection systems necessary for robotic thinning of early-stage apples. The specific challenges during this growth stage include the obscurity and overlap of fruits by leaves and branches, as well as variable lighting conditions, which require a robust detection system capable of operating under such complexities. Recent efforts have shown promising results using deep learning techniques for young apple detection, yet the need for higher accuracy and faster processing speeds remains.

Likewise, Eizentals et al.introduced a 3D pose estimation method for green peppers using Lidar-based model matching. Although effective under controlled conditions, this method’s dependency on precise laser range findings and its moderate error rates highlight the need for more accurate technologies in cluttered and variable outdoor environments.explored guava detection and pose estimation using a low-cost RGB-D sensor, achieving high precision and recall. However, the pose error and the 0.565 seconds per fruit processing time underscore potential limitations in real-time applications, indicating a demand for faster and more precise pose estimation techniques.utilized binocular imagery and deep neural networks for grape cluster-harvesting robots. Their approach demonstrated high accuracy in fruit detection and pose estimation. Yet, the 1.7 seconds processing time per grape and the reliance on complex setup for binocular imagery point to a need for simpler and more efficient methods.described a citrus pose estimation method using a single RGB image, focusing on fruit rotation vector prediction. While innovative, the method’s reliance on specific image annotations and the moderate harvesting success rate suggest room for improvement in pose accuracy and operational efficiency.developed a pose estimation method for sweet peppers by detecting symmetry axes in point clouds. Despite its precision, the method’s performance varies significantly with the physical state of the peduncle, highlighting a limitation in consistency and the potential for errors in uncontrolled environments.

Additionally, Kim et al.developed a 2D pose estimation method for tomatoes, emphasizing pedicel detection for robotic harvesting. While this approach effectively grasps the poses of tomato-pedicel pairs, the reliance on visible keypoints may falter under occlusion or varied postures, indicating the need for more robust methods in cluttered environments.employed deep learning for apple pose estimation in logistic centers, achieving high accuracy in rotation regression. However, the method’s reliance on single-perspective imaging may limit its applicability in more complex orchard settings, where multi-angle approaches could enhance detection accuracy.utilized the association between tomato bodies and sepals for pose estimation, achieving promising accuracy. Nonetheless, the method’s dependency on high-quality association metrics like IoU scores could be problematic in less controlled field conditions, suggesting a gap in versatility and robustness.presented a 6D pose estimation strategy using point cloud registration for various fruits. While effective, the need for extensive pre-processing and 3D model matching might not scale efficiently across diverse fruit types without significant computational resources.combined point cloud segmentation and geometric analysis for in-field grape cluster pose estimation. Though the method is functional, the reported error angles in pose estimation and the time required per cluster highlight the ongoing challenges in achieving real-time, precision-oriented harvesting solutions.

SECTION: Vision Transformers Application in Agriculture
Vision Transformers (ViT) have emerged as powerful alternatives to convolutional neural networks for various image analysis tasks, leveraging the self-attention mechanism to process images as sequences of patches. The Dense Prediction Transformer (DPT), an adaptation of ViT, is designed for dense prediction tasks such as semantic segmentation and depth estimation. DPT modifies the traditional transformer architecture to handle high-resolution inputs directly, maintaining spatial hierarchies and enabling precise localization capabilities crucial for detailed image understanding tasks.

In the emerging field of precision agriculture, vision transformers are increasingly recognized for their efficacy in addressing complex challenges such as plant disease classification. Yet, despite their burgeoning utility, the application of vision transformers within agricultural contexts, especially beyond disease identification, remains relatively sparse.introduced a lesion-aware visual transformer specifically designed for the detection of paddy diseases, marking a significant advance with its multi-scale learning and novel lesion localization capabilities that guide disease detection. This study showcased an exceptional model performance with accuracy and F1-score averaging 98.74% and 98.18%, respectively, underscoring its potential in precision agriculture.developed the FormerLeaf, a transformer-based model that employs the Least Important Attention Pruning (LeIAP) algorithm and sparse matrix-matrix multiplication (SPMM) to enhance the detection of leaf diseases. The innovations presented not only reduced training time by 10% but also increased accuracy, demonstrating the transformer’s ability to adapt and perform efficiently in agricultural settings.presented the PlantViT, a hybrid model that combines CNNs with a Vision Transformer to diagnose plant diseases from leaf images. This model, which leverages multi-head attention modules, achieved notable accuracy on extensive datasets, emphasizing the transformer’s robustness and generalizability in diverse agricultural environments.proposed an automatic pest identification method using a Vision Transformer, trained on an enhanced dataset to prevent overfitting and improve accuracy. The results indicated that the ViT network outperformed traditional CNN-based methods, highlighting the transformer’s precision in classifying plant diseases and pests. Lastly,exploited an advanced vision transformer model to identify diseases in maize leaves, achieving a groundbreaking accuracy rate of 99.24%. By integrating a MaxViT model with novel architectural adjustments, this study set a new benchmark for deep learning applications in agriculture, proving that vision transformers can significantly enhance crop productivity and sustainability.

Beyond plant disease detection, the utilization of vision transformers in agriculture extends to innovative applications such as crop yield prediction and multi-object tracking, as demonstrated by two notable studies.introduced the Multi-Modal Spatial-Temporal Vision Transformer (MMST-ViT), a pioneering model designed for precise crop yield prediction. This model uniquely integrates visual remote sensing data with meteorological inputs to assess the impact of both short-term weather variations and long-term climate change on crop yields. The MMST-ViT consists of three specialized components: a Multi-Modal Transformer for processing diverse data types, a Spatial Transformer for analyzing spatial dependencies across regions, and a Temporal Transformer for capturing long-range temporal dynamics. The novel multi-modal contrastive learning technique employed allows for effective pre-training without extensive human supervision. This approach has been validated across over 200 counties in the United States, where it consistently outperformed existing models on several performance metrics. Similarly,developed a Multi-Object Tracking (MOT) framework tailored for agricultural environments using a Vision Transformer, enhancing the robustness and accuracy of robotic tracking applications. Their framework utilizes the local feature matching transformer (LoFTR), to establish spatial associations among plants within cluttered fields, significantly improving tracking performance. Tested on the LettuceMOT and AppleMOT benchmark datasets, their method showed substantial enhancements in tracking accuracy, with improvements up to 44% over existing solutions. This underscores the transformative potential of Vision Transformers in managing the spatial dynamics of plants, facilitating advancements in robotic agriculture.

ViTs have demonstrated significant promise across various domains, including precision agriculture, primarily focusing on plant disease detection. The groundbreaking contributions by,,,, andhave showcased the capabilities of ViTs in enhancing classification accuracy and processing efficiency in this specific area. However, the broader application of these models in agriculture, particularly in tasks such as crop yield prediction and multi-object tracking, is still in its nascent stages. The introduction of the MMST-ViT model byand the multi-object tracking framework byare pioneering efforts that expand the use of ViTs beyond disease detection.

These studies illustrate the adaptability of ViTs to diverse agricultural needs. Despite these advances, the integration of ViTs into wider agricultural applications, such as real-time automated systems for crop management and harvesting robotics, remains limited. The agricultural sector faces challenges in integrating ViTs with existing technologies, addressing compatibility with various sensor types, and meeting real-time processing demands. To fully leverage the potential of ViTs in agriculture, further research is essential. This research should focus on overcoming the current limitations and expanding the application of ViTs to more complex agricultural environments. Specifically, there is a compelling opportunity to explore the use of ViTs in environments such as apple orchards and other specialty crops, which undergo similar phases of management, pruning, and thinning throughout their growth cycles. These environments, where crops present similar challenges to those faced in apple orchard management, could greatly benefit from the advanced image analysis capabilities of ViTs. Such studies could pave the way for comprehensive, efficient, and adaptable agricultural practices, maximizing the transformative potential of Vision Transformers in agriculture.

SECTION: Methods
The overall methodology of this study is summarized in Figure. In this study, RGB images were collected using a robotic system, as depicted in Figure. This figure comprehensively illustrates the workflow, encompassing data collection, preprocessing, and the deployment of deep learning models for pose estimation. The images obtained were manually annotated through extensive human labor, which involved the precise delineation of bounding boxes and pose lines on immature green fruits, crucial for the subsequent model training phase. These annotated datasets were employed to train various configurations of the YOLO11 and YOLOv8 models, with the comparative performance analysis of each configuration presented in the middle part of Figure. The aim was to ascertain the most efficacious model configuration based on precision and inference speed for applications in robotic agricultural settings. The optimal model configuration was subsequently utilized for the transformation of RGB images into RGB-D point clouds. This transformation was facilitated by two advanced vision transformers: the Dense Prediction Transformer (DPT) and Depth Anything V2. The conversion from RGB to RGB-D data was critical for the validation process, detailed on the right side of Figure. The estimated poses derived from the YOLO11 model were validated against actual field measurements obtained using a digital caliper. This validation process involved measuring the major axis length of each fruitlet in the RGB-D point clouds and comparing these measurements with the corresponding field data to ensure the accuracy and reliability of the pose estimations.

The methodologies employed in this study are detailed in the following five sections. Initially, the Data Acquisition section discusses how images were collected. This is followed by an overview and training of the YOLO11 models, as described in the YOLO11 Overview and Model Training section. The performance of these models is then evaluated in the Detection and Pose Estimation Performance Evaluation section. Subsequently, the Vision Transformers for RGB to RGB-D Mapping section is divided into two subsections: Depth Prediction Transformer (DPT) and Depth Anything V2. Finally, the Pose Validation in a 3D Environment section details how the accuracy of our pose estimates was assessed against actual measurements.

SECTION: Data Acquisition and Preparation
Data acquisition was carried out using a robotic platform that was equipped with an Intel RealSense D435i camera, mounted on a UR5e robotic arm as depicted in Figure. This setup facilitated the capture of RGB images of immature fruitlets in a commercial "Scifresh" apple orchard located in Prosser, Washington State, USA. The images were collected prior to thinning in May 2024, with the appropriate timing for fruitlet thinning determined through regular monitoring of the commercial apple orchard and ongoing communication with growers and laborers. The orchard was planted with tree row spacing of 10 feet apart and a spacing of 3 feet between trees.

A total of 1147 images were collected using Intel RealSense D435i camera as shown in Figure. This camera features a depth-sensing system utilizing active infrared (IR) stereo vision and an inertial measurement unit (IMU). The camera’s depth sensor operates using structured light technology, which employs a pattern projector to create disparities between stereo images captured by two IR cameras. With a resolution of 1280 × 720 pixels, the camera can capture depth information up to a range of 10 meters. It supports a frame rate up to 90 frames per second (fps), offering a 69.4° horizontal field of view (HFOV) and 42.5° vertical field of view (VFOV). Additionally, the 6-axis IMU provides precise orientation data, enhancing the alignment of depth data and scene understanding.

After the image collection phase, the images were then preprocessed to enhance their quality. The preprocessing was followed by a manual annotation process conducted by experts. Each image was annotated to identify crucial keypoints on the immature fruits, specifically the calyx and peduncle, as illustrated in Figuresand. A bounding box and a line were used to denote the object and its pose, respectively. This extensive manual annotation process required continuous effort over several days by multiple individuals. It was vital for generating accurate data essential for training deep learning models. After this labor-intensive phase, the dataset was carefully divided into 918 images for training, 114 for validation, and 115 for testing, adhering to an 8:1:1 train:validate ratio for data preparation. This organized dataset was then exported using Roboflow, ensuring structured access for model development.

SECTION: YOLO11 Overview and Model Training
YOLO11 (Glenn Jocher, Ultralytics, 2024, Link: https://docs.ultralytics.com/models/yolo11/ ) (Figure) represents the latest advancement in the YOLO family, building on the strengths of its predecessors while introducing innovative features and optimizations aimed at enhancing performance across various computer vision tasks. Specifically, YOLO11 architecture optimized feature extraction capabilities as depicted in Figure, enabling it to capture intricate details in images. This model supports a range of applications, including real time object detection, instance segmentation, pose estimation, and which allows it to accurately identify objects regardless of their orientation, scale, or size, making it versatile for industries like agriculture and surveillance. The YOLO11 model utilizes enhanced training techniques that have led to improved results on benchmark datasets. Notably, YOLO11m achieved a mean Average Precision (mAP) score of 95.0% on the COCO dataset while utilizing 22% fewer parameters compared to YOLOv8m, demonstrating greater efficiency without compromising accuracy. With an average inference speed that is 2% faster than YOLOv10, YOLO11 is optimized for real-time applications, ensuring quick processing even in demanding environments. These specifications position YOLO11 as a powerful tool for advancing AI applications, particularly in sectors where rapid and accurate image analysis is crucial.

The training process for the YOLO11 models was structured to optimize performance across various computer vision tasks. The model was trained with a batch size of 8 and a uniform image resolution of 640x640 pixels. The training utilized an automatic optimizer and implemented a total of 100 epochs, with a patience setting of 100 to avoid overfitting. Key hyperparameters included an initial learning rate of 0.01, momentum at 0.937, and weight decay set at 0.0005. A warmup phase of 3 epochs was integrated to stabilize training parameters, alongside specific loss settings including box loss of 7.5 and class loss of 0.5. Image augmentation techniques used were flipping, translation, and a unique mosaic approach with a probability of 1.0. Moreover, the model was designed to operate with advanced features like overlap masks and a dynamic workspace of 4. In this work, pre-initialized weights were not used, ensuring that the models were trained to adapt solely based on the synthesized dataset, thus providing a fresh foundation to assess the effectiveness of the training and data augmentation strategies employed. The training process was conducted on a high-performance computing workstation, which was crucial for managing the substantial computational demands of the study.

The workstation was equipped with an Intel Xeon(R) W-2155 CPU, which features a base clock speed of 3.30 GHz and 20 cores, providing the necessary processing power for intensive data processing tasks. For graphics processing and machine learning computations, the system utilized NVIDIA Corporation GP102 [TITAN Xp] graphics cards, which are well-suited for complex image processing tasks. Additionally, the workstation boasted a large storage capacity of 7.0 TB, allowing for extensive data management and analysis. It operated under Ubuntu 20.04.6 LTS, a robust and stable 64-bit operating system. The system’s graphical interface was managed by GNOME version 3.36.8, with X11 serving as its windowing system, ensuring a stable and efficient environment for conducting the computational analyses required for training the YOLO11 configurations.

After the YOLO11 experiments, its predecessor, YOLOv8, along with its five configurations (developed by the same company, Ultralytics), were thoroughly evaluated. Each configuration of YOLOv8 was tested under identical hyperparameter settings and using the same workstation to ensure a controlled and fair comparison.

SECTION: Performance Evaluation for Detection and Pose Estimation
The detection and pose estimation performance of the YOLO11 algorithm for immature green apples was evaluated using key metrics: precision, recall, and mean Average Precision at 50% Intersection over Union (mAP@50). These metrics are calculated based on the following terms:

The number of immature green fruits correctly identified by the model.

The number of instances where the model incorrectly identifies an object as an immature green fruit.

The number of instances correctly identified as not being immature green fruits. (Note: This term is generally not used in the calculation of precision and recall for object detection tasks.)

The number of immature green fruits present in the image that the model fails to detect.

for both box detection and pose estimation is defined as the ratio of correctly predicted positive observations to the total predicted positives (Equation):

, shown in Equation, measures the model’s ability to detect all relevant instances:

considers the intersection over union (IoU) between the predicted box and the ground truth box, where a detection is considered correct if the IoU is greater than 50%. It is averaged over all classes and IoU thresholds (Equation):

These metrics, delineated through Equations,, and, provide a comprehensive view of the model’s performance in detecting and estimating the pose of immature green fruits, highlighting the effectiveness and accuracy of the detection algorithms.

Additionally, the evaluation of image processing speeds for green fruit pose detection was systematically segmented into three phases: pre-processing, inference, and post-processing. These phases encompass the complete cycle from initial image manipulation before detection, through the detection process itself, to the final steps following the detection output. Each phase is critical in understanding the overall efficiency and responsiveness of the pose detection system.

This stage involves preparing the images for detection, which may include resizing, normalization, and augmentation to enhance the model’s ability to detect immature green fruits accurately.

At this core phase, the model processes the image to detect and estimate the pose of the fruits. The speed of this stage is crucial for real-time applications and is quantified by the time taken to process a single image, as shown in Equation:

This phase involves operations after the initial detection, such as applying non-maximum suppression to refine the detection boxes and extracting pose information for further analysis.

Based on achieving the highest precision and the fastest inference speed among all configurations of YOLO11 and YOLOv8, the optimal model was selected for further image analysis and validation of the detected pose in 3D image space.

SECTION: Vision Transformers for RGB to RGB-D Mapping
The two Vision Transformers, DPT and Depth Anything V2, are explored in this section. Detailed workflows for each model are illustrated in Figure, and the specific processes for DPT and Depth Anything are further elaborated in Figuresand.

DPT, designed primarily for depth estimation, utilizes a transformer architecture that excels in processing high-resolution RGB images to generate depth-enhanced RGB-D data efficiently. This model leverages pre-trained weights and a sophisticated feature extraction process to offer precise depth information, crucial for accurate 3D mapping of objects. In contrast, Depth Anything V2 extends the capabilities of depth prediction to various scenes by adapting to different datasets without extensive retraining. It uses a lightweight transformer model that supports faster computation while maintaining high accuracy. This model is particularly effective in generating detailed depth maps from RGB images, which are instrumental in calculating the exact dimensions and orientations needed for robotic thinning operations in orchards.

The DPT model utilizes a ViT as its backbone, which processes images in a sequence of patches. Unlike standard transformers, DPT assembles tokens from different stages of ViT into multi-resolution feature maps, which are then progressively fused using a convolutional decoder to produce full-resolution predictions. This method ensures that the model captures both fine details and global context effectively. The model operates on a high constant resolution throughout the process, enhancing its ability to generate detailed and coherent predictions across the entire image.

In this study, the DPT was directly applied to high-resolution RGB images collected from commercial apple orchards to generate detailed RGB-D (depth) data. The depth maps produced by DPT provided the 3D spatial context necessary for validating the pose estimation performed by the YOLO11 algorithm. Specifically, YOLO11’s pose estimations of the major axes of immature green fruits were cross-verified against measurements obtained from 3D point clouds generated by DPT. The actual diameters of the fruitlets were measured in the field using digital calipers to provide ground truth validation. This integration of DPT not only enhanced the accuracy of pose estimation but also demonstrated the potential of transformers in automating and refining agricultural operations through precise 3D mapping and measurement capabilities.

The entire workflow, as depicted in the accompanying diagram (refer to Figuresand) , was meticulously designed to ensure a streamlined process. Initially, high-resolution RGB images were captured using specialized cameras mounted on a robotic platform. These images were then preprocessed to enhance quality before being input into the DPT model. The model processed these images through several stages, including patch extraction, transformer encoding, and feature map creation, culminating in the generation of detailed RGB-D outputs. The subsequent steps involved using these outputs to validate the pose estimations made by the YOLO11 algorithm, focusing on the accuracy of keypoint detection within the regions of interest. Each step of the process was critical in achieving a high degree of precision in the 3D spatial analysis, ultimately facilitating a robust assessment of the pose estimation techniques used. The methodology outlined ensured that the study leveraged advanced imaging and deep learning techniques to address the challenges posed by manual labor in fruitlet thinning operations.

Depth Anything V2 is an advanced monocular depth estimation model designed to convert standard RGB images into depth-mapped (RGB-D) images. This model employs a cutting-edge neural network architecture that integrates depth estimation directly into the image processing pipeline, enabling it to predict the depth information from a single image with high precision. The model utilizes a fully convolutional network, leveraging the power of deep learning to interpret and transform visual data into spatial mappings. The Depth Anything V2 architecture is characterized by its depth prediction capabilities, which are crucial for applications requiring detailed three-dimensional understanding from two-dimensional data. The model processes images through several layers, where each layer extracts and refines features at different scales. This method allows the model to capture both high-level details and fine-grained elements, making it exceptionally good at handling the complexities of natural scenes. It uses a series of encoder-decoder networks, where the encoder compresses the input image into a feature-rich representation, and the decoder expands these features back into an image format, assigning depth values to each pixel. This is enhanced by skip connections that help preserve spatial details that are often lost during down-sampling. The output is a dense depth map that closely aligns with the real-world geometry of the photographed scene.

In this study, the Depth Anything V2 model was utilized to significantly enhance the pose estimation of fruitlets, which is crucial before the thinning process in agricultural operations. This methodology was systematically executed in a sequence of defined steps, as depicted in the associated diagram (refer to Figuresand). Initially, high-resolution RGB images of immature green fruitlets were captured using a specialized camera setup in commercial orchards, as outlined in the "Fruitlet Image Collection" step. These images were then subjected to preprocessing to optimize quality for further analysis, a process detailed in the "Image Preprocessing" step. Following this, the images were inputted into the Depth Anything V2 model, starting the depth estimation process, as indicated in the "Depth Estimation Initialization" step. Subsequent steps involved extracting features from the images ("Feature Extraction"), compressing these features via an encoder to generate a dense representation ("Encoder Processing"), and then expanding these representations through a decoder to form detailed depth maps ("Decoder Processing"). Integration of skip connections was employed to preserve crucial spatial details, a key feature described in the "Skip Connection Integration" step. The depth maps generated were used to synthesize 3D spatial data about the fruitlets ("3D Information Synthesis"), which enhanced the YOLO11 algorithm’s ability to accurately localize and measure each fruit in three dimensions. This critical phase is captured under the "Pose Estimation Integration" step. The synthesized depth data provided a detailed understanding of the spatial arrangement of fruitlets, crucial for identifying precise engagement points for robotic arms during thinning.

SECTION: Field-Level validation of estimated pose
Following the conversion of RGB images to RGB-D format via Vision Transformers, the resultant 3D point clouds were meticulously extracted, as depicted in Figurea for the corresponding images. Ground truth values were precisely measured using a vernier caliper, as illustrated in Figureb. These in-field measurements served as a benchmark for validating the poses estimated by the YOLO11 deep learning model in 3D image space, where Vision Transformers were employed to transform standard RGB images into detailed RGB-D data without additional depth information from the camera.

The evaluation of the predicted poses against actual field measurements was performed using CloudCompare (Seattle, Washington, USA), a tool for assessing the quality of 3D point clouds. Root Mean Square Error (RMSE) and Mean Absolute Error (MAE) were the primary metrics used to evaluate the accuracy of the pose estimations, specifically measuring discrepancies in the length of green fruits. The RMSE is defined as:

and the MAE as:

whereis the number of fruits analyzed,is the diameter as estimated from the depth maps generated by the DPT and Depth Anything V2 models, andis the diameter measured in the field using a caliper. These metrics, RMSE and MAE, provided a quantitative measure of the model’s precision in estimating the pose of immature green fruits based on their size, facilitating an objective comparison between the predicted and actual dimensions.

SECTION: Results and Discussion
SECTION: Detection and Pose Estimation Results
The performance metrics of the YOLO11 and YOLOv8 in terms of box detection and pose estimation across all configurations are summarized in Table. The results highlight the superior performance of the YOLO11 models, particularly YOLO11n, which achieved the highest precision among all tested configurations. Specifically, YOLO11n recorded a precision score of 0.91, surpassing the best performing YOLOv8 configuration, YOLOv8m, which scored 0.90. Moreover, the inference speed of YOLO11n was noted at 2.7 milliseconds, markedly faster than the quickest YOLOv8 configuration, YOLOv8n, at 7.8 milliseconds.

In terms of recall, YOLOv8n outperformed all other configurations with a score of 0.905, the highest among its counterparts, while the highest recall within the YOLO11 series was observed for YOLO11x at 0.891. Regarding the mean Average Precision at 50% Intersection over Union (mAP@50), YOLO11s led all configurations with a score of 0.94, closely followed by YOLOv8n, which achieved an mAP@50 of 0.934. These metrics highlight critical aspects of model performance in complex agricultural environments, where not only precision and speed but also recall and mAP@50 are vital for comprehensive performance assessment. The robustness of YOLO11 in detecting and estimating the pose of immature green fruits suggests its applicability across a range of agricultural operations. Potential applications extend beyond robotic thinning to include automated pest management, crop monitoring, yield estimation, and precision irrigation, where the detailed understanding of crop status and dynamics is crucial. These applications demonstrate the broad utility of high-performing detection models in enhancing the efficiency and effectiveness of agricultural practices.

Figuresa,b, andc illustrate the comparative effectiveness of the YOLO11 and YOLOv8 models in detection and pose estimation, highlighting their applicability in precision agricultural operations. Specifically, Figurea displays the mAP@50 values, whereas Figuresb andc detail the image processing speeds for YOLO11 and YOLOv8 configurations respectively. Given YOLO11n’s superior precision and faster inference speeds, as demonstrated in the processing speed comparisons, it has been chosen as the best-suited model for our robotic thinning project. This decision is scientifically backed by its performance metrics, which promise to enhance efficiency and accuracy in field implementations.

!

In the results depicted in Figure, the examples of the YOLO11n model in the detection and pose estimation of immature green fruits within a commercial orchard environment is illustrated. The immature green fruits, even those occluded by shadows or canopy foliage, were robustly identified, as evidenced in Figurea. In this subfigure, a green fruit located in a shadowed and low-light area was accurately detected, an ability highlighted by the red arrow, indicating the model’s robust performance under diverse environmental conditions.

Furthermore, within the bounding boxes of detected fruits, keypoints were identified by the model, although not all keypoints could be detected when parts of the fruit were not visible, demonstrating a realistic limitation given the partial visibility of some fruits. Such instances are portrayed in Figuresa andb, showcasing the YOLO11n model’s consistent accuracy in detection and pose estimation. Remarkably, the model also identified fruits that were only marginally visible; an example is depicted in Figureb, where approximately 10-15% of an immature green fruit at the edge of the image frame was detected. This capability emphasizes the YOLO11 model’s refined sensitivity to minimal visual cues, asserting its robustness in complex agricultural settings.

Additionally, in Figure, further examples of the YOLO11n model’s detection and pose estimation capabilities for immature green fruits are demonstrated. The left side of each image in the figure displays the original scenes, while the right side presents the detection and pose estimation results by YOLO11n. Although the model performed commendably in terms of detection and pose estimation, areas for improvement were identified.

For instance, in Figurea, the depiction of the detection and pose estimation in a densely clustered environment of immature green fruits reveals critical challenges. In the heavily clustered setting, which is typical before thinning operations in commercial ’Scifresh’ orchards, the model managed to detect the majority of immature green fruits. Notably, strong pose detection was observed, particularly in the green-circled regions of the left side, which corresponds to the original imagery.

However, the red-circled regions on the right side of Figurea present a limitation. While the model detected fruits and estimated keypoints up to the visible sections of the fruits’ major axes, the accuracy of these estimations might be compromised by occlusions. Specifically, the fruit positioned above the red-circled apple complicates the detection scenario.

For robotic thinning operations, a technical solution would involve prioritizing the thinning of fruits that occlude others to enhance visibility and accuracy in subsequent detections. This approach would improve the practicality of robotic systems designed for fruitlet thinning, enabling more effective deployment in commercial orchard environments. Future research should focus on developing algorithms that can adaptively prioritize detection and thinning sequences based on occlusion and fruit arrangement, enhancing the efficacy of robotic thinning operations.

Likewise, in the upper part of Figurea, particularly within the red circle, the orientation of an immature green fruit is captured in an orientation that predominantly displays its minor axis rather than the major axis from calyx to peduncle. The YOLO11 model successfully identified the presence of the fruit, a demonstration of its robust detection capabilities. However, the pose estimation within the bounding box, as depicted on the right side of the figure, did not accurately reflect the major axis of the fruit. This inaccuracy is understandable, given the natural complexity and density of fruitlets within apple orchards. Apple trees often produce a surplus of fruitlets and flowers, far exceeding what is necessary for optimal fruit development, as highlighted in historical agricultural studies. Such conditions pose significant challenges for pose estimation algorithms, particularly in scenarios where the fruit’s orientation does not align with the typical major axis configurations.

Moreover, in the depicted results ofb, the YOLO11n model accurately recognized the object as an immature green apple; however, the pose estimation within the bounding box revealed inaccuracies. Although one keypoint (at the calyx) was correctly identified, the other necessary for a complete pose estimation (at the peduncle) was obscured due to occlusion, a persistent challenge in dense orchard environments. This partial visibility compromises the effectiveness of even the most advanced models like YOLOv10, which demonstrated the highest precision in our study. The upper part ofb illustrates this limitation: the visible calyx, while detected, does not provide sufficient information to infer the full fruit pose. Future work might benefit from integrating machine learning technologies such as Generative Adversarial Networks (GANs), which have shown potential in reconstructing complete object profiles from limited views. Such approaches could potentially address the occlusions inherent in agricultural settings by extrapolating the unseen portions of fruits, enhancing the accuracy and utility of robotic thinning tools in commercial orchards.

!

Furthermore, the lower portion of the red-circled area in Figureb presents another challenge where the YOLO11n model, despite its accurate detection of immature green fruits, failed to correctly identify the keypoints for pose estimation. This limitation was notably influenced by the insufficient diversity and volume of the training dataset used in this study, which comprised only 1147 images. The lack of representative data from densely occluded or complex environments may hinder the model’s learning algorithm, leading to less accurate pose estimation in real-world scenarios. Enhancing the training dataset with a broader array of images capturing a wider range of fruit orientations and occlusion levels could significantly improve the model’s ability to generalize and perform more reliably in commercial orchard settings. Such an expansion of the dataset is anticipated to provide the necessary variability that would enable the model to better learn and predict fruit poses accurately, thereby supporting more effective robotic applications in agriculture.

SECTION: Evaluation of Visual Transformers in Pose Length Validation
The performance of visual transformers in pose length validation was rigorously assessed by comparing the Root Mean Square Error (RMSE) and Mean Absolute Error (MAE) between estimated and actual measurements of immature green fruit lengths. The validation process involved data from Intel RealSense point clouds, Dense Prediction Transformer (DPT), and Depth Anything models, all cross-referenced against ground truth measurements. Initially, measurements derived from the Intel RealSense exhibited a RMSE of 9.9845 and an MAE of 7.7444, indicating substantial variance from the ground truth. Significant improvements in accuracy were observed with the application of the DPT model, where the RMSE was reduced to 3.2935 and the MAE to 2.6237. The most notable enhancements in pose estimation accuracy were achieved through the Depth Anything model, which demonstrated the lowest errors with a RMSE of 1.5261 and an MAE of 1.2809. These findings underscore the efficacy of using advanced visual transformers like Depth Anything and DPT in enhancing the precision of pose length estimations in agricultural applications, particularly in the context of YOLO11-based fruit detection and pose estimation tasks.

The utilization of visual transformers for RGB to RGB-D mapping is exemplified in the Figure, which shows examples of DPT in Figurea and Depth Anything V2 in Figureb.
In Figurea, a notable observation in the black circled region of the original RGB image reveals three fruits, one of which is partially illuminated by sunlight, impacting the depth perception in the generated images. In the corresponding feature RGB-D maps for both DPT and Depth Anything, although the structural details are well-preserved, it is the Depth Anything model that demonstrates superior refinement. The 3D point cloud heatmap from Depth Anything distinctly captures the intricate structure of the canopy foliage and the spatial arrangement of the fruits with greater clarity and detail compared to DPT. This is particularly evident in the enhanced delineation of the fruit affected by the sunlight, where Depth Anything accurately models the depth variations caused by the lighting conditions. This figure serves as a compelling demonstration of Depth Anything’s advanced capabilities over DPT in processing complex visual data from agricultural environments. The refined depth mapping provided by Depth Anything is instrumental for applications in green fruit automation, where precise spatial data is crucial for effective robotic thinning.

Likewise, Figureb, focuses on a densely packed cluster of seven immature green fruits, a typical scenario crucial for robotic thinning. The original RGB image, highlighted in black dotted circles, showcases the high density of fruitlets, emphasizing the necessity for precise thinning operations. Depth Anything V2’s performance in this intricate setting is particularly notable. The model’s output, as reflected in the heatmap of the 3D point clouds, reveals an exceptional level of detail not only in identifying the target fruits but also in mapping the surrounding canopy foliage (Figureb). This enhanced depth perception by Depth Anything is crucial for robotic applications, where distinguishing between closely clustered fruits and their environmental context is vital for effective operation. The superior depth mapping capabilities of Depth Anything, compared to DPT, are distinctly visible in the black dotted regions of Figure b. It successfully delineates the spatial relationships and depth variations within the cluster, providing a clearer understanding of the physical layout essential for robotic maneuvering and decision-making in thinning operations. This example underscores Depth Anything V2’s robustness and accuracy in handling complex agricultural environments, making it a valuable tool for advancing the precision and efficiency for robotic fruit thinning technologies.

SECTION: Discussion
The integration of 3D data has become a cornerstone in the field of robotics, serving as a pivotal element in the advancement of automation technologies. Numerous studies have emphasized that without robust 3D information, effective automation remains unattainable, highlighting the critical role of precise spatial data in enhancing robotic functionalities. The utilization of the Depth Anything V2 transformer, in particular, has proven highly effective in generating detailed 3D mappings from standard RGB images, as illustrated in Figure. Subfigurea presents the original RGB images captured in a Scifresh orchard, while Subfigureb displays the corresponding Intel RealSense point clouds, and Subfigurec showcases the Depth Anything V2 generated point clouds. These point clouds were produced at a data collection threshold of 2 feet, demonstrating the transformer’s capacity to accurately interpret and convert 2D images into spatially informative 3D data. This transformation process is crucial for tasks that require precise depth perception, such as robotic fruitlet thinning in densely populated orchards. By providing a clear 3D representation of the environment, Depth Anything V2 facilitates more accurate and efficient robotic interactions, significantly reducing the operational complexity and enhancing the efficacy of automated procedures. The results presented highlight the potential of these advanced vision transformers to not only improve the accuracy of automated operations in agricultural settings but also to drive innovations that could revolutionize the sector.

In the exploration of RGB to RGB-D mapping through vision transformers, particular attention is drawn to Figurea where specific regions marked by red, green, and blue dotted circles are critical. These areas, highlighted across the top, middle, and bottom images, contain key fruitlets and essential canopy features integral to the operational success of future robotic applications like fruitlet thinning and canopy management. The associated point clouds, as depicted in Figureb, extracted directly from the Intel RealSense machine vision camera, unfortunately, do not adequately capture the detailed structure of fruitlets and canopy elements within these circled regions. Interestingly, these missing details in the camera-captured point clouds are effectively compensated for by the Depth Anything vision transformer model, as evidenced in Figurec. This model adeptly generates the necessary 3D data for areas where the camera-based point clouds fall short, ensuring that even obscured or partially visible fruitlets are accounted for. This capability of Depth Anything not only enhances the precision of 3D mapping but also supports more accurate robotic interactions with the environment by providing a more comprehensive spatial understanding.

In the middle section of subfigures in Figure, a critical observation regarding the tree trunk—a vital element for collision avoidance and robotic navigation in orchard settings is highlighted. Due to excessive sunlight exposure or possible intrinsic camera limitations, the Intel RealSense camera failed to capture depth information for the tree trunk on the rightmost part of the image, as shown in Figureb. Remarkably, this missing data was effectively reconstructed by the Depth Anything vision transformer, which successfully generated the requisite point clouds for this region. This capability underscores the robustness and utility of Depth Anything in enhancing environmental perception, crucial for the safe and efficient operation of agricultural robots.

In past several studies that have explored pose estimation in various agricultural contexts, a broad spectrum of approaches has been highlighted along with their respective challenges. For instance, Eizentals et al.employed a Lidar-based method for green peppers which, while effective under controlled settings, underscores the limitations inherent in relying on precise laser range findings for cluttered outdoor environments. Similarly, methods leveraging RGB-D sensorsand binocular imageryhave demonstrated high precision in fruit detection and pose estimation. However, these approaches often suffer from prolonged processing times and complex setups, hindering their feasibility for real-time applications. Further, studies like those by Sun et al.and Li et al.have introduced innovative techniques using single RGB images and symmetry detection in point clouds, respectively. While these methods show promise, they are constrained by their specific requirements for image annotations and environmental conditions, which may not be consistently replicable in dynamic field settings. Moreover, recent advancements by Kim et al.and Giefer et al.in 2D pose estimation and apple rotation regression highlight the ongoing need for methods that can robustly handle occlusions and offer versatility across different orchard environments. In contrast to these computationally intensive methods, our study harnesses straightforward, low-cost RGB imaging techniques to reconstruct RGB-D data with high accuracy, significantly simplifying the data collection and processing pipeline. By implementing a critical threshold for data input into vision transformers like Depth Anything, we achieve refined 3D mappings essential for precise robotic operations. This approach not only reduces the computational load but also enhances the scalability and applicability of pose estimation technology in real-world agricultural settings, offering a more practical solution for field-level validation and operational efficiency.

SECTION: Conclusion
This research has highlighted the growing interest and advancements in robotic solutions for labor-intensive agricultural tasks, such as the thinning of immature green fruits in commercial apple orchards. While consumer-level automation for harvesting red apples, which presents a simpler vision problem due to color differentiation, remains underdeveloped, the challenges in detecting and estimating the pose of similarly colored immature green fruits amidst canopy foliage are more complex. The study successfully utilized advanced machine vision and deep learning techniques, particularly the YOLO11 model and Visual Transformers, to address these challenges. The application of the YOLO11 model demonstrated high precision in both detection and pose estimation of immature fruitlets. Coupled with the depth insights provided by Visual Transformers, such as DPT and Depth Anything V2, the study enhanced the capabilities of automated thinning systems significantly. This integration not only improves the efficiency of orchard management but also reduces the physical strain and health risks associated with manual labor, thereby indicating a promising direction for enhancing precision agriculture practices, especially in handling immature fruitlets. This comprehensive experimental approach in commercial orchards provided performance comparisons between two widely used deep learning models (YOLO11 and YOLOv8) and investigated the efficacy of emerging visual transformers for cost-effective RGB to RGB-D mapping to validate fruitlet poses. The findings from these comparisons and validations lead to the following specific conclusions:

The YOLO11n variant showcased superior precision among all YOLO11 and YOLOv8 models, achieving a precision score of 0.91. In terms of recall, YOLOv8n led with a score of 0.905. Additionally, YOLO11n and YOLOv8x both recorded high mAP@50 values of 0.95, highlighting their effective performance in complex environments.

YOLO11 Pose Estimation Performance: Similarly, YOLO11n also excelled in pose estimation precision, matching its detection precision of 0.91, while YOLOv8x displayed the highest recall among the models at 0.91. YOLO11s notably achieved the highest mAP@50 value of 0.94, underscoring its robustness in accurate pose estimation.

Image Processing Speeds: YOLO11n was markedly faster in inference speeds, clocking in at 2.7 milliseconds, significantly outpacing the fastest YOLOv8 variant, YOLOv8n, which registered at 7.8 milliseconds. This substantial speed advantage underscores YOLO11n’s suitability for real-time applications in agricultural automation.

Visual Transformers Evaluation: The evaluation of Visual Transformers revealed that Depth Anything V2 markedly outperformed the Dense Prediction Transformer in pose length validation. This was evidenced by its superior ability to convert low-cost RGB images into detailed RGB-D maps and generate accurate 3D point clouds. Initial measurements with Intel RealSense showed significant discrepancies, with a RMSE of 9.9845 and an MAE of 7.7444, highlighting substantial variations from ground truth data. However, a notable improvement was observed with the DPT model reducing the RMSE to 3.2935 and the MAE to 2.6237. Depth Anything V2 achieved the best results, demonstrating the lowest errors with a RMSE of 1.5261 and an MAE of 1.2809, showcasing its efficacy in enhancing precision in pose estimation of immature green fruitlets.

SECTION: Future Work
Building on the findings of this study, future research will aim to develop a robust motion planning system and an advanced robotic end effector, paving the way for a fully automated fruitlet thinning solution in commercial orchards. This vision system has demonstrated potential beyond just immature green apples, applicable to other tree fruits that resemble immature green apples during their pre-thinning stages, such as pears, plums, apricots, cherries, and peaches. The workflow validated here could be adapted to manage these fruits as well, enhancing the breadth and applicability of the technology. The successful deployment of a vision system capable of precise detection and pose estimation sets a solid foundation for future work, which will concentrate on developing and testing a working prototype in field conditions. This next phase will focus on integrating these technological advancements into a cohesive system that can operate efficiently within the dynamic environment of an orchard, aiming to revolutionize how fruit thinning is conducted, reducing labor costs and improving crop management.

SECTION: Acknowledgements and Funding
This research is funded by the National Science Foundation and United States Department of Agriculture, National Institute of Food and Agriculture through the “AI Institute for Agriculture” Program (Award No.AWD003473). We extend our heartfelt gratitude to Zhichao Meng, Martin Churuvija, Astrid Wimmer, Randall Cason, Diego Lopez, Giulio Diracca, and Priyanka Upadhyaya for their invaluable efforts in data preparation and logistical support throughout this project. Special thanks to Dave Allan for granting orchard access. We also acknowledge the contribution of open-source platforms Roboflow (https://roboflow.com/), Ultralytics (https://docs.ultralytics.com/models/yolo11/), Hugging Face (https://huggingface.co/), and OpenAI (ChatGPT) for the models and implementation assistance in our project through their open-source platform.

SECTION: Author contributions statement
R.S conceptualization, data curation, software, methodology, validation, writing original draft. M.K editing and overall funding to supervisory

SECTION: References