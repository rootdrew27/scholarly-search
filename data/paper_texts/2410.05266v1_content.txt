SECTION: Brain Mapping with Dense Features: Grounding Cortical Semantic Selectivity in Natural Images With Vision Transformers
Advances in large-scale artificial neural networks have facilitated novel insights into the functional topology of the brain. Here, we leverage this approach to study how semantic categories are organized in the human visual cortex.
To overcome the challenge presented by the co-occurrence of multiple categories in natural images, we introduce BrainSAIL (emanticttribution andmageocalization), a method for isolating specific neurally-activating visual concepts in images. BrainSAIL exploits semantically consistent, dense spatial features from pre-trained vision models, building upon their demonstrated ability to robustly predict neural activity. This method derives clean, spatially dense embeddings without requiring any additional training, and employs a novel denoising process that leverages the semantic consistency of images under random augmentations. By unifying the space of whole-image embeddings and dense visual features and then applying voxel-wise encoding models to these features, we enable the identification of specific subregions of each image which drive selectivity patterns in different areas of the higher visual cortex. This provides a powerful tool for dissecting the neural mechanisms that underlie semantic visual processing for natural images. We validate BrainSAIL on cortical regions with known category selectivity, demonstrating its ability to accurately localize and disentangle selectivity to diverse visual concepts. Next, we demonstrate BrainSAIL’s ability to characterize high-level visual selectivity to scene properties and low-level visual features such as depth, luminance, and saturation, providing insights into the encoding of complex visual information. Finally, we use BrainSAIL to directly compare the feature selectivity of different brain encoding models across different regions of interest in visual cortex. Our innovative method paves the way for significant advances in mapping and decomposing high-level visual representations in the human brain.

SECTION: Introduction
Understanding how the human brain processes and represents visual information from natural experience is a fundamental challenge in neuroscience. The vast majority of our knowledge of the visual system comes from tightly controlled experiments using simplified, hand-crafted images or, at best, real-world photographs of objects against noise backgrounds. Although this paradigm has revealed a pattern of preferential neural responses to semantic categories such as faces, places, bodies, words, objects, and food, the visual world we actually experience consists of rich, complex scenes containing many co-occurring objects, textures, and contextual associations. As such, using minimal or single-object stimuli narrows the space of hypothesis testing and limits the ecological relevance of any conclusions, leaving us with an incomplete characterization of how the brain represents and processes real-world visual stimuli.

Recent developments in computer vision models trained on web-scale datasets have enabled learning rich multimodal representations that capture semantic concepts
in a human-aligned manner. In this work, we introduce a novel methodology that leverages the power of such models to decompose selectivity patterns in visual cortex by analyzing responses to dense, localized semantic features present in naturalistic images:emanticttribution andmageocalization (“BrainSAIL”). BrainSAIL allows us to isolate the specific image regions that activate different cortical areas when viewing naturalistic scenes. This method allows us to focus on selectivity within complex naturalistic images, thereby enabling a richer decomposition grounded in the full semantic complexity of natural visual experiences.

The core of BrainSAIL involves extracting spatially dense semantic embeddings from images using state-of-the-art models such as CLIP, DINO, or SigLIP. These embeddings bridge the traditionally disparate domains of raw vision data, dense deep semantic features, and measured neural responses. Within this rich embedding space, we can isolate and identify the specific visual features and corresponding image regions that drive selectivity effects in different cortical areas during perception of naturalistic visual scenes. By concurrently modeling localized semantic information, high-level semantic categories, and observed brain activity patterns, BrainSAIL can tease apart the image-level visual drivers of neural tuning preferences across higher visual areas. We validate this dense feature mapping method on a large-scale fMRI dataset consisting of human participants viewing many thousands of diverse natural images that span a wide range of semantic categories and visual statistics.

BrainSAIL’s dense embedding framework offers an interpretable view of feature representations across visual regions of the brain. Critically, this view explicitly grounds neural selectivity to localized semantic characteristics inherent in real-world visual experiences. First, we demonstrate the utility of our model for natural images applied to known category-selective regions of the cortex. Second, we show that our model can be used to identify the preference of brain regions sensitive to scene statistics. Finally, we use our model to compare and contrast the feature selectivity for different vision foundation models.
In sum, the dense semantic grounding realized in BrainSAIL enables exciting new directions towards understanding and modeling high-level visual representation in humans.

SECTION: Related Work
A growing body of work leveraging computational modeling and machine learning has explored semantic representation in the higher visual cortex. Approaches include generative image modelsand the decoding of visual stimuli. These diverse studies are united by their consideration of the stimulus image as a whole, primarily focusing on the global information contained within the image rather than the individual scene components. In contrast, the method we introduce explicitly decomposes an image into its semantic components, enabling the identification of individual, semantically meaningful activating concepts within complex natural images.

Using hand-crafted image stimuli, functional mapping studies have identified regions in the human brain that respond preferentially to stimuli representing distinct semantic concepts such as faces, places, bodies, words, objects, and food. One limitation of this simplified approach is that it may not fully capture the contextual complexity of natural vision. Addressing this concern, recent work on image-computable encoders has enabled computational tests of visual selectivity using naturalistic images. Building on this work, our method leverages state-of-the-art brain encoding backbones based on vision transformersto further explore finer-grained semantic representation in visual cortex.

Self- or weakly-supervised vision models that use contrastiveand masked prediction objectivesare scalable and can be trained on massive, diverse datasets to achieve high zero-shot performance on downstream tasks. Contrastive models such as CLIP, DINO, and SigLIP demonstrate strong classification performance without further fine-tuning. Models that jointly train on language and vision (CLIP/SigLIP) can also classify images using text-based descriptions without fine-tuning. Interestingly, this high level of performance is mirrored in the fact that contrastive models show high performance for predicting neural responses in visual cortex when paired with linear probes.

There has been strong interest in leveraging generative models for decoding (reconstructing) visual stimuli conditioned on brain activations either directly or via intermediate language-based captions. A related approach generates novel stimuli that are posited to best to activate a target brain region (as opposed to reconstructing the original stimulus)with recent attempts utilizing GANs or Diffusion models to constrain the synthesized output. While these models have shown positive results, they all rely on images as a whole, whereas BrainSAIL seeks to disentangle complex images into their semantically meaningful components and localize those parts of the image that elicit activation for different brain voxels or regions.

SECTION: Methods
Our aim is to generate spatial attribution maps for arbitrary voxels in the higher visual cortex. Unlike the early visual cortex, which is believed to be primarily selective for “simple features”, the higher visual cortex exhibits semantic selectivity – a pattern that, at present, is best predicted by deep networks. As illustrated in Figure, to create spatial attributions maps for brain voxels, we first train voxel-wise fMRI encoders to map images to brain activations. Second, we derive dense features from pre-trained vision transformers (ViT) used as the backbone for these encoders. Third, we demonstrate that an artifact-free dense feature map can be derived for high-throughput exploration of selectivity with the visual cortex.

SECTION: Image-to-Brain Encoders for the Higher Visual Cortex
A voxel-wise image-computable fMRI encoder is a modelthat predicts fMRI activations (betas) forwhererepresents the number of voxels in the brain. The encoder is conditioned on image input, where. Recent work has demonstrated that encoders that rely on features extracted from large vision foundation models achieve excellent predictive performance, where higher visual cortex is best predicted by deeper layers in the model. In this setting, the backbone model is usually frozen, while a per-voxel adapter typically parameterized as a linear layer is trained to map from network features to voxel activations. In that we focus on the higher visual cortex exclusively, we utilize a two component design for our encoder: (1) a frozen vision foundation model backbonewhich outputs adimension embedding vector for each image; (2) a per-voxel adapter parameterized as a linear probe with weightand bias, which takes as input a unit-norm image embedding.

It should be noted that BrainSAIL is not restricted to linear probes, and can work with arbitrary voxel-wise parameterizations, including MLPs. Linear probes are used here as they are widely adopted in fMRI encoder literature and empirically achieve good performance. BrainSAIL is compatible with any Vision Transformer (ViT)-based model, making it readily applicable to the vast majority of modern visual foundation models which predominantly employ ViT architectures. Additional results are presented in the supplemental. We train our model with MSE loss, and evaluate the encoder on the test set. In Figurewe show that our encoder achieves state-of-the-art.

SECTION: Deriving Dense Features from ViT backbones
The emergence of vision models trained on a contrastive image-text objective has fueled interest in zero-shot open-vocabulary image classification methods. For example, CLIP has shown that images can be classified without foreknowledge of the test time classes during training; instead the category of interest can be described using language during test time. Of late, this capability has been extended from classification to segmentation. Compared to methods that require human annotationand perform poorly on out-of-distribution images, these new methods require no further training and directly extract dense features that lie in the same space as the image/text embedding.

These dense feature extraction methods operate by modifying the last self-attention (SA) block within the typical ViT architecture (MaskCLIP,; SCLIP,; NACLIP,). For vision models of this sort trained on a contrastive objective, the output is composed of a singletoken, which is supervised using a contrastive loss; and numerous patch tokens which correspond to specific spatial locations. Letbe the query, key, value features respectively for a single image patch, with a total ofspatial patches. For a given patchat the final layer, where f denotes any function applied to theafter the last self-attention, thetoken and each dense token is a convex combination offeatures:(2)MaskCLIP proposes to directly remove the convex re-weighting and output the value feature for each patch token directly. SCLIP and NACLIP reintroduce the weighting to reduce output artifacts, but modify it with correlative self-attention (CSA); or by using CSA with a spatial attentive bias. Here, we utilize NACLIP as the dense adaptor for CLIP. The other two backbones in Sectionuse an updated ViT architecture with “register tokens”. As these have not been explored in the context of CSA, we utilize MaskCLIP as the dense adaptor.

SECTION: Learning-Free Feature Distillation
As only theis supervised in these contrastive models, as shown in Figuresand, the extracted dense embeddings often have artifacts – even when using the latest NACLIP method which seeks to reduce artifacts. While methods such asimprove spatial consistency via architectural improvements, they require training the model with architecture modifications that are computationally costly. Consequently, in order to facilitate high-throughput characterization of the visual cortex over large datasets, we propose an efficient learning-free distillation module. Given an image, we first generateaugmentation parameters, whereconsists of a horizontal/vertical offsetand horizontal. We further generate the image space coordinates, wheregoes from top-to-bottom, whilegoes left-to-right. We describe our full transform in Algorithm. Our method distills a clean semantic map, as visual semantics are equivariant to shift and horizontal flips. We note that averaging over the number of augmentation is extracting anembedding under mean squared error (squared euclidean). Letbe the optimal embedding under MSE for a given patch, andwithbe the feature candidates under image augmentation:

The objective can be expressed as, then.

In Table, we compare pre- and post- smoothing results. Under the Pearson metric, which does not assume prior category knowledge, smoothing yields the best performance in three of four datasets. We apply the voxel-wise adapters to the per patch dense features to derive the final relevance map.

SECTION: Results
We utilize BrainSAIL to localize the semantic selectivity of different brain regions and demonstrate that the relevance maps are interpretable throughout the brain and correlate well with the known category-selective regions. We then explore the selectivity of higher visual cortex with respect to localized scene structure and image properties. Finally, we compare and contrast the localization results from three different vision foundation models. These results establish BrainSAIL as a novel technique for mapping and understanding the semantics of visual representations in the brain.

SECTION: Setup
We use the Natural Scenes Dataset (NSD;), the largest 7T fMRI dataset of human visual responses, focusing on four subjects (S1, S2, S5, S7) who viewed the full 10,000 image set (a subset of COCO images) three times each. fMRI activations (betas) were derived using GLMSingleand normalized per session (). Responses to repeated images were averaged. A brain encoder for each subject was trained onunique images per subject, with the remainingimages viewed by all subjects being used forvalidation as the test set. Supplementary results for other subjects are included in the appendix.
Face, place, body, and word regions were defined using independent category localizer data from NSD with a threshold of. Food regions were defined using masks provided by.

We train three encoders based on different neural network backbones. For all three, we utilize themodel size.For CLIP, we utilize OpenAI’s officialweights. This is a network trained on an infoNCE contrastive image-text objective.For DINO, we utilize the latest official, and is a network trained on image-only self-supervision.For SigLIP, we utilize NVIDIA’s implementation based on, as the original Google variant used a non-standard architecture. SigLIP utilizes a pairwise non-contrastive image-text objective. All fMRI encoders are trained using MSE loss, with the backbone frozen. We validate the test timein Figureand find that we achieve state-of-the-art results similar toand. We use CLIP for Sectionsand, as it is the most widely used backbone in fMRI literature. We useaugmentation steps unless otherwise noted.

SECTION: Image Factorization using the Brain
To explore how different areas in higher visual cortex align to different image parts we apply UMAPwith an angular metric to linear brain weights and apply the same UMAP basis to dense features as produced by BrainSAIL. Note that during dimensionality reduction– the region of interest outlines on the cortex in Figurea are forand are derived from independent NSD functional localizers. As shown in Figure, we find that the factorization of the brain is well aligned to pre-identified functional regions, and broadly segments the cortex into axes along “people”, “scenes” and “food”. In particular, place regions, including the retrosplenial cortex (RSC), occipital place area (OPA), and parahippocampal place area (PPA), show selectivity for scene components (). People regions, including the extrastriate body area (EBA), fusiform face area (FFA), occipital face area (OFA), show selectivity for face and body parts in the image (-). Finally, we find that the recently identified food region that roughly surrounds FFA ()strongly corresponds to food in images. These results establish that BrainSAIL can be used to characterize higher-level selectivity to individual semantic categories in complex natural images without prior knowledge of their semantic selectivity.

We further quantify the feature relevance maps for broad category selective regions in Figureand Table. We use the brain encoder to predict the top-5 images for the place/word/face/body regions, and the top-10 images for the food region. We find that our method can effectively localize the objects relevant to each category- selective brain region. Note that the word region is known to have cross-selectivity to facesand food.

SECTION: Cortex Selectivity to Image Features
Going beyond semantic categories, we seek to explore the low- and mid-level image feature correlates that correspond to different brain regions. Prior work explored this by training a convolutional encoder on each NSD subject, which is limited toimages each. One concern is that using a small dataset with a convolutional backbone can lead to overfitting to the dataset’s specific features and exacerbate the inherent biases of convolutional networks. To address this limitation, our method leverages vision transformers trained on massive datasets of hundreds of millions of images, thereby avoiding the hard-coded inductive biases present in CNNs. We visualize BrainSAIL feature dissection results in Figure. Our method can successfully identify the known scene selective regions (RSC/OPA/PPA) as preferring high depth, and is successful even in OPA wherefails. We believe this is likely because OPA processes higher-level associative content and affordances.
Similarly, we identify the region surrounding FFA as being selective to high color saturation, which correspond to the food regions identified byand others. In OPA, we identify a split in color luminance preference, which is similar to the indoor/outdoor preferring regions identified by,, and. These results demonstrate that our method can identify fine-grained selectivity with more broadly characterized brain regions.

SECTION: Are Brain Encoders Equivalent?
Recent high-performing models such as CLIP, DINO, and SigLIP differ in their training objectives, architectures, and datasets: CLIP employs a contrastive image-language objective, DINO utilizes a self-supervised image loss without explicit linguistic guidance, and SigLIP leverages a non-contrastive pairwise image-language loss. Despite these differences, when employed as the backbone for fMRI encoders, these models exhibit similar performance in predicting brain responses, achieving comparablevalues on the test set as shown in Figure. This observation raises an important question about the nature of each model’s learned features and their alignment with one another: Do these models converge upon similar feature representationsfor category selective brain regions despite their varied training paradigms?

To investigate the representational differences between the models, we perform BrainSAIL analysis for scene, face, and food-selective brain regions and qualitatively visualize the results in Figure. While all three models exhibit broad similarities in their grounding maps, DINO, trained without language supervision, demonstrates a stronger sensitivity to low-level visual features compared to CLIP and SigLIP. This is evident in the food region (Figure), where DINO’s grounding map for a pizza image excludes the toppings and misses differently colored vegetables on a metal plate, suggesting a focus on color and texture rather than the concept of “food” itself. Similarly, in the face region, DINO’s grounding map exhibits less reliance on semantically relevant features such as eyes, nose, and mouth. We hypothesize that this greater sensitivity to visual features in DINO stems from its lack of language guidance during training, preventing it from learning the higher-level semantic correlations that link visually disparate parts and objects within a category. As high-performing “proxy models” of visual brain representation, these and other underlying model characteristics – architecture, training objective, training dataset, etc. – are important considerations for developing more robust encoding models that can bridge the gap between artificial and biological vision systems.

SECTION: Discussion
BrainSAIL achieves strong localization performance and benefits from a pre-trained vision transformer, reducing reliance on the fMRI dataset for backbone training. However, it is still necessary to train the fMRI encoder on these data, and thus potential dataset biases in the human neural data and how it was collected can influence the learned representations and conclusions. Future work should explore training on larger and more diverse neural datasets to mitigate this limitation and enhance the generalizability of our findings.

We propose BrainSAIL, a method that leverages vision foundation models to interrogate which semantic components of complex natural images lead to the neural activation of specific regions of the brain. Based on the vision transformer architecture, we: (1) semantically attribute and localize relevant objects in complex compositional images; (2) jointly factorize images and semantically selective regions in the human brain; (3) identify the feature correlates of depth, saturation, and luminance that underlie semantic selectivity; (4) explicate differences in fMRI encoders that achieve similar overall brain prediction performance.these results establish that BrainSAIL is a powerful new approach to data-driven explorations of the human higher visual cortex.

SECTION: References
SECTION: Appendix
Additional encoder test setfor CLIP, DINO, and SigLIP ()

Visualization of ground truth functional localizer statistics ()

Visualization of encoder weight UMAP for all subjects using CLIP backbone  ()

Visualization of encoder weight UMAP for all subjects using DINO backbone  ()

Visualization of encoder weight UMAP for all subjects using SigLIP backbone  ()

Additional visualization of fMRI grounding using CLIP backbone encoder  ()

Additional visualization of fMRI grounding using DINO backbone encoder  ()

Additional visualization of fMRI grounding using SigLIP backbone encoder  ()

Implementation details ()

SECTION: Additional encoder test setfor CLIP, DINO, and SigLIP
Subjects 1, 2, 5, 7 are those that completed the full scan ofimages – where each image was viewed 3 times. They are also the subjects reported to have the highest noise ceiling in NSD (higher is better).

SECTION: Visualization of ground truth functional localizer statistics
In this section, we show the ground truth functional localizer t-statistic provided by NSD. As NSD does not provide a food localizer, we obtain the food mask from. During the UMAP, we do not provide our method with the ground truth functional localizer information, and these t-statistics are provided here for reference only.

SECTION: Visualization of encoder weight UMAP for all subjects using CLIP backbone
We utilize OpenAI’s officialas the encoder backbone, and visualize the UMAP as applied to the fMRI encoder weights and dense features.

SECTION: Visualization of encoder weight UMAP for all subjects using DINO backbone
We utilize Meta’s officialas the encoder backbone, and visualize the UMAP as applied to the fMRI encoder weights and dense features. The UMAP method is nonparametric , and the output can depend on the random seed. Similar colors for UMAP applied to DINO encoder weights does not indicate any specific similarity to CLIP results in the prior section.

As DINO does not have a text encoder, we cannot visualize text-based localization probes.

SECTION: Visualization of encoder weight UMAP for all subjects using SigLIP backbone
We utilize Nvidia’simplementation for SigLIP. Nvidia’s implementation was choosen as the original Google implementation did not utilize a ViT with a attentiontoken. We visualize the UMAP as applied to the fMRI encoder weights and dense features. The UMAP method is nonparametric , and the output can depend on the random seed. Similar colors for UMAP applied to SigLIP encoder weights does not indicate any specific similarity to DINO or CLIP results in the prior section.

SECTION: Additional Visualization of Grounding using CLIP backbone
SECTION: Additional Visualization of Grounding using DINO backbone
SECTION: Additional Visualization of Grounding using SigLIP backbone
SECTION: Implementation Details
Our experiments utilize a mixture of GPUs including Nvidia V100 (16GB and 32GB), 2080 Ti, A6000, 6000Ada, L40S, and 4090 cards. The network training code is implemented in PyTorch. For encoder training, we employ the Adam optimizer with a decoupled weight decay of. The initial learning rate is set toand decays exponentially toover 100 epochs. Each subject is trained independently. All backbone networks are frozen, and operate in fp16 mode.

For encoder training, we always utilize the network’s native input resolution. For CLIP, we resize images to. For DINO, we resize images to. For SigLIP using the AM-RADIO backbone, and we resize images to. After resizing, we augment the images with random pixel-wise value scaling between 0.95 and 1.05, followed by normalization using each network’s respective image mean and variance. Before network input, images are randomly offset by up to 4 pixels horizontally and vertically, with edge padding filling the resulting empty pixels. Independent Gaussian noise (,) is added to each pixel.

We perform positional encoding interpolation for CLIP and DINO networks in order to extract higher resolution embeddings. Forwe modify the network to accept images of(4x upsampling;final patch resolution); Forwe modify the network to accept images of(2x upsampling;final patch resolution); For SigLIP based on Nvidia’s, we do not perform upsampling, and havefinal patch resolution. As noted in the AM-RADIO paper, the spatial patch features of these networks are highly robust to positional encoding upsampling.
We performaugmentation steps for CLIP, where the first augmentation step is null (no shift or flipping). For DINO and SigLIP, as these networks contain registers which mitigate the artifacts to a certain degree, we performaugmentation steps.

For CLIP, we utilize the NACLIP self-attention modification, with gaussian std set to. For DINO and SigLIP, we use the MaskCLIP self-attention modification. For SigLIP specifically, since we are using the AM-RADIO implementation, we apply their provided SigLIP adapter head to the extracted features. As shown in section, this enables zero-shot probing with the official SigLIP text encoder using the extracted dense features.

Our “Learning-Free Distillation Module” is highly efficient. With pre-extracted dense features for each augmentation, we observe our procedure generally takes less thanseconds on a GPU, this is roughlyfaster than “Denoising Vision Transformers” on the same hardware and similarly ignoring the feature extraction cost.

We define a set of natural language captions to help us evaluate the alignment between fMRI region-wise relevance maps and concepts. For every caption and every image, we compute a relevance map using the CLIP text encoder. The category that contains the caption with the highest pearson correlation to the fMRI relevance map is assigned as the category.

face_class = [”A face facing the camera”, ”A photo of a face”, ”A photo of a human face”, ”A photo of faces”, ”A photo of a person’s face”, ”A person looking at the camera”, ”People looking at the camera”,”A portrait of a person”, ”A portrait photo”]
body_class = [”A photo of a torso”, ”A photo of torsos”, ”A photo of limbs”, ”A photo of bodies”, ”A photo of a person”, ”A photo of people”, ”A photo of a body”, ”A person’s arms”, ”A person’s legs”, ”A photo of hands”]
scene_class = [”A photo of a bedroom”, ”A photo of an office”,”A photo of a hallway”, ”A photo of a doorway”, ”A photo of interior design”, ”A photo of a building”, ”A photo of a house”, ”A photo of nature”, ”A photo of landscape”, ”A landscape photo”, ”A photo of trees”, ”A photo of grass”]
food_class = [”A photo of food”, ”A photo of cuisine”, ”A photo of fruit”, ”A photo of foodstuffs”, ”A photo of a meal”, ”A photo of bread”, ”A photo of rice”, ”A photo of a snack”, ”A photo of pastries”, ”A photo of vegetables”, ”A photo of pizza”, ”A photo of soup”, ”A photo of meat”, ”A photo of candy”]
text_class = [”A photo of words”, ”A photo of glyphs”, ”A photo of a glyph”, ”A photo of text”, ”A photo of numbers”, ”A photo of a letter”, ”A photo of letters”, ”A photo of writing”, ”A photo of text on an object”]