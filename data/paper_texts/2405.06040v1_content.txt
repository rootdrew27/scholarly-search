SECTION: Searches for the BSM scenarios at the LHC using decision tree based machine learning algorithms: A comparative study and review of Random Forest, Adaboost, XGboost and LightGBM frameworks

Machine learning algorithms are now being extensively used in our daily lives, spanning across diverse industries as well as academia. In the field of high energy physics (HEP), the most common and challenging task is separating a rare signal from a much larger background. The boosted decision tree (BDT) algorithm has been a cornerstone of the high energy physics for analyzing event triggering, particle identification, jet tagging, object reconstruction, event classification, and other related tasks for quite some time. This article presents a comprehensive overview of research conducted by both HEP experimental and phenomenological groups that utilize decision tree algorithms in the context of the Standard Model and Supersymmetry (SUSY). We also summarize the basic concept of machine learning and decision tree algorithm along with the working principle ofRandom Forest,AdaBoostand two gradient boosting frameworks, such asXGBoost, andLightGBM. Using a case study of electroweakino productions at the high luminosity LHC, we demonstrate how these algorithms lead to improvement in the search sensitivity compared to traditional cut-based methods in both compressed and non-compressed R-parity conserving SUSY scenarios. The effect of different hyperparameters and their optimization, feature importance study using SHapley values are also discussed in detail.

SECTION: 1Introduction

Supersymmetry (SUSY)Martin:1997ns;drees2004theory;baer2006weakis one of the most compelling extensions of beyond the Standard Model (BSM) scenario, and the pursuit of supersymmetric partners of the SM particles (sparticles) remains as a primary objective at the Large Hadron Collider (LHC). Since the inception of the LHC, both the ATLAS and CMS collaborations have already conducted numerous searches to explore the SUSY particles utilizing the LHC Run-I and Run-II datasetatlas_susy;cms_susy. In the absence of any statistical deviations from the SM predictions, the LHC has set stringent lower bounds on the masses of particles. For example, in the R-parity conserving (RPC) SUSY111The RPC SUSY model offers a stable lightest supersymmetric particle (LSP), which can be a promising candidate for dark matter.
The weakly interacting massive particle (the most popular choice is the lightest neutralino) is able to evade detection, and this results in a distinct signature of significant missing energy.scenarios with relatively light neutralinos (), the LHC Run-II data has extended the lower bounds on the masses of gluino (), first two generations light squarks (), the lightest stop () and the lightest chargino () upto2.3 TeV, 1.6 TeV, 1.2 TeV and 1.2 TeV respectivelyatlas_susy;cms_susydepending on the branching ratios and the simplified models assumptions.

It is important to note that the accumulated luminosity of Run-II data is approximately140, which is about 5% of the luminosity of planned upgrade of high luminosity LHC (HL-LHC) run (). The LHC Collaboration initiated the Run-III operation in July 2022, and it will continue the operation until the planned three-year-long shutdown (LS3) in preparation for the high luminosity upgrade, which is scheduled to commence in 2026. The constraints on sparticle masses, derived from LHC Run-I and Run-II data, are primarily derived within the framework of simplified SUSY scenarios, which involve specific assumptions regarding decay modes, branching ratios, etc. Also, the majority of the analyses have concentrated on prompt decay scenarios. However, there are several pockets of SUSY parameter spaces with light sparticles that remain less explored, e.g., quasi-degenerate SUSY scenarios where the next to LSP (NLSP) - LSP mass gap is very smallBhattacherjee:2013wna;Dutta:2015exw;Chakraborti:2017dpu;Chowdhury:2016qnz;Dutta:2017jpe. Such compressed SUSY models are also highly motivated in the context of Dark Matter (DM) relic densityKumarBarman:2020ylm;Barman:2022jdg;He:2023lgi;Chakraborti:2015mra;Chakraborti:2014gea;Chakraborti:2017dpu;Chowdhury:2016qnz;Bhattacharyya:2011se;Choudhury:2012tc;Baer:2021aax, Muon (g-2) anomalyChakraborti:2022vds;He:2023lgi;Chakraborti:2015mra;Chakraborti:2014gea;Baer:2021aax;Athron:2021iuf;Endo:2021zal;Chakraborti:2021bmv;Choudhury:2017acn;Choudhury:2017fuu;Banerjee:2018eaf;Banerjee:2020zvi;Chakraborti:2021dli;Frank:2021nkq;Ali:2021kxa;Kowalska:2015zja;Chakrabortty:2015ika;Choudhury:2016lku. It has been observed that conventional cut and count analyses are highly competitive for non-degenerate SUSY searches compared to machine learning (ML) analyses. But ML based analyses give better sensitivity in both degenerate and non-degenerate BSM physics searches.

The term “machine learning” (ML) was first proposed by A. Samuel in 1959Samuel1988. After six decades, ML algorithms are now being widely used in our daily lives and across various industries, e.g., filtering email, social media recommendation, cyber security, image analysis and disease detection in healthcare, data analysis in the finance sector, autonomous vehicles, marketing and advertising, etc. In the future, the applications of ML will continue to expand rapidly with the advancement of technology and the availability of more data.
high energy physics (HEP) experimental and phenomenological analyses deal with large amount of data and it has been observed that the use of ML techniques leads to improvements in the data analyses of several HEP fields. For particle identification, event selection, object reconstruction, event classification, etc., the experimental high energy physics (HEP-Ex) collaborations have been using the conventional ML algorithms for more than three decades. boosted decision tree (BDT) is one of the most popular algorithms which has been widely used by the HEP-Ex collaborations e.g., the search of single top quark prediction by the CDF and D0 collaborationsCDF:2010eor;D0:2008wma;D0:2006ngk, Higgs discovery at the LHCCMS:2014afl. For a long time, the HEP community has used BDT and other algorithms implemented in the Toolkit for Multivariate Data Analysis (TMVA) software packageTMVA:2007ngywhile XGBoost (Extreme Gradient Boosting) has gained immense popularity in recent yearsChen:2016btl. However, it should be noted that the deep neural networks (DNN) or deep learning (DL) techniques, which are based on multilayer NN, are becoming more popular nowadaysdoi:10.1142/S0217751X20020030;dnn;Bhat:2010zz;Guest:2018yhq;Bourilkov:2019yoi;Schwartz:2021ftp;Carleo:2019ptp;Shlomi:2020gdn;Abdughani:2019wuv.
There are several broad reviews in the literature on the applications of
BDTBhat:2010zz;Cornell:2021gut;Coadou:2022nsh, deep learning algorithmdoi:10.1142/S0217751X20020030;dnn;Bhat:2010zz;Guest:2018yhq;Bourilkov:2019yoi;Schwartz:2021ftp;Carleo:2019ptp;Shlomi:2020gdn;Abdughani:2019wuvlike convolutional neural networks (CNN), recurrent neural networks (RNN) etc. in the context of high energy physics.

The collider analyses in HEP typically involve with searches of new physics signals and the precise measurements of the existing known SM processes. To achieve this, one has to look for faint signal from a large amount of background
where the distribution of signal and backgrounds have significant overlap. Traditional cut-based analysis shows less sensitivity for significant overlap scenarios. In such cases, the ML algorithms are more powerful in discriminating signals from backgrounds. In this article, we will focus on the various kinds of decision tree (DT) based algorithms and study how these algorithms lead to improvement in the search sensitivity compared to cut-based methods considering a case study of electroweakino pair production at the HL-LHC. The structure of the article is as follows. In Sec.2, we introduce the basic concepts of machine learning along with different kinds of metrics. In Sec.3, we briefly summarize the major important analyses involving the use of ML algorithms with an emphasis on DT-based algorithms. The basic concepts of decision trees are discussed in Sec.4. Also, we present a concise overview of Random Forest, AdaBoost, and extreme gradient boosting algorithms such as XGBoost and LightGBM in this section. In Sec.5, we investigate the improvement of search sensitivity by these four ML algorithms with comparison to cut-and-count analysis using a SUSY scenario with(wino-like) pair production. We also present the role of hyperparameters in different ML framework along with the feature importance study using SHapley values. Finally, we summarize the paper in Sec.6.

SECTION: 2Basic concepts of machine learning

Machine learning, as a subset of artificial intelligence (AI), involves the development of various models that can learn from diverse datasets and execute tasks without requiring explicit programming.
To perform a machine learning (ML) analysis, it is essential to collect data, which comprises information about the desired output that we want the computer to learn. The ML algorithms can be broadly categorized into supervised and unsupervised learninginbook;hastie2009elements;carbonell1983overview.
In addition to these two, there is also a third category known as semi-supervised learning. In supervised learning, each input data point (, i = 1,….n, for n input data samples) is accompanied by a target variable or output label () and the algorithm learns to extrapolate patterns from the provided training data in order to predict the output labels for unseen (testing) data points.
In other words, the aim of the supervised learning is to acquire a mapping functionfrom the input data to output label:such that it can accurately predictfor a new data sets (testing) which do not have output labels.

Supervised learning algorithms like decision trees (DT), support vector machines (SVM), logistic regression (LR), neural networks (NN), etc, are commonly used for two major tasks: classification and regression. In the classification task, the output/target is discrete, categorical, and finite, while the output is continuous and infinite for the regression task.
For the HEP problems, the simplest example of binary classification is the categorization of
the signal and backgrounds for the sensitivity study and one example of a regression task is object tagging. In this paper, we will concentrate on supervised DT algorithms. On the other hand, for the unsupervised learning algorithms (e.g., k-means clustering, autoencoders, etc.) the data only contains input features
without any target output variables/values and the algorithms explore the hidden
structures or patterns to perform tasks like clustering, anomaly detection, etc.
For a recent review of unsupervised machine learning in the context of particle physics, see Ref.Bardhan:2024zla.

In high energy physics (HEP), each and every particle of an event has different four-momentum information along with different energy depositions in the detector. These attributes serve as the basis for constructing different input features for analysis so that the ML algorithm can distinguish the signal and background efficiently. The available dataset is commonly divided into two subsets: training data and testing data. The training data is used to train the model, while the testing data is used to assess the model’s performance on unseen data. Sometimes, we use another set of independent data, known as validation data, to fine-tune the hyperparameters, such as learning rate, regularization strength, or the number of layers in a neural network, etc.

SECTION: 2.1Loss Function

In supervised learning, the ML algorithm learns using aloss function222It is also known as cost function or error function and depends on the actual and predicted output..
Using optimization techniques like Gradient Descent, the loss function is
minimized during training by adjusting parameters, which are known as “weights”. For example,
the predicted outputin a linear regression model is calculated as, where b is the bias term. For decision trees, weights may indicate how much each feature contributes to the decision at each node of the tree.
The minimization of theloss functionspecified that the model predicted values are close to the actual value/label. For different types of tasks or data,
the choices of loss function vary. The commonly used function for regression
tasks is the mean squared error (MSE) and is defined ashastie2009elements;article;bentejac2021comparative

whereis the number of samples in the dataset,andare
the predicted and true target value of thesample.
Another common loss function for regression tasks is mean absolute error and is defined as

For binary classification tasks the commonly used the most commonly used function
is Binary Cross-Entropy Loss or Log Loss, which is defined as:

whereis the predicted probability andis the true binary label (0 or 1). Categorical cross-entropy loss
(log(y)) is used in multi-class classification tasks.

SECTION: 2.2Overfitting and underfitting

After training the dataset and adjusting the “hyperparameters”, one proceeds to assess the model’s ability togeneralizeor to make accurate predictions for unseen testing data. However, the complexity of the trained model and the amount of training data can lead to two major issues:overfittingandunderfitting.

arises when a model becomes too complex and learns the training data
too precisely. Then, the model captures noise in the data instead of the underlying patterns. As a result, the loss computed for the training data tends to approach zero, while it becomes significantly higher for unseen or new testing data. This lack of generalization leads to poor performance on the testing data. Bias represents the deviation of the predicted value from the actual value, while variance indicates the model’s sensitivity to small fluctuations in the training data. Therefore, anoverfittedmodel typically exhibits low bias and high variance. Conversely, if the model is too simple and fails to capture the underlying patterns in the training data, it performs poorly on both the training and testing data. This high bias and low variance scenario is known asunderfitting. A good/robust model is a trade of both the parameters - bias and variance. In other words, the right balance between model complexity and the amount of training data is essential for building models that generalize well to unseen data.
In Fig.2we demonstrate an example ofunderfitting,overfittingand appropriate fitting by considering a dataset consisting of signal (circular magenta points) and background events (square cyan points). In the left panel, the model is too simple, underfitted, and has a very high bias. On the other hand, the lowest loss is obtained for the middle panel. However, the prediction of this overfitted model is not reliable for the new dataset. The right panel of Fig.2represents a robust model that predicts accurately for untrained data.

Class

True

SECTION: 2.3Measures of classification performance

After training, tuning of hyperparameter and testing the HEP data,
our objective is to evaluate the effectiveness and accuracy of the
machine learning models in solving a specific task, e.g.,
classify or distinguish between signal and background events despite their similar
signature. Performance metrics are quantitative indicators used to measure
the model’s ability to make accurate predictions or classifications. By
evaluating performance metrics, we can compare different ML models
and optimize the hyperparameter tuning to improve the model’s effectiveness. Sometimes, the highly skewed distribution of classes in the dataset leads toclass imbalance, which affects the performance of the ML algorithm.
Techniques like oversampling of the minority class or undersampling of the majority class, adjustment of class weights, cost-effective learning, etc., can be employed to address the class imbalance in MLCornell:2021gut;weiss2003learning;Chawla2005;sulaiman;satio;LUQUE2019216;10.1007/978-3-642-04962-0_53. To evaluate the performance
metrics, the algorithm checks the actual and predicted values of observations
in the dataset by forming confusion matrix333It is also known as error matrix or classification table..
When the signal/background events are correctly classified as
signal/background events, then those events are called True Positive (TP) or
True Negative (TN). On the other hand, False Positive (FP),
False Negative (FN) are the number of events where the actual background/signal events are incorrectly classified as signal/background events. In the Table.1, we present the confusion matrix graphically for binary classification. With these definitions, we can define the following
various performance metrics to evaluate the performance of the model:

Sensitivity or Recall or True Positive Rate (TPR):Recall, also known as sensitivity or TPR, is defined as the ratio of the number of true positives (correctly predicted signal events) to the total number of actual positives/signal events (TP + FN). Sensitivity is useful
for minimizing the occurrence of false negatives.

Precision:Precision measures how many of the events predicted as signal events are actually signals. It is useful when the purpose is
to limit the number of false positives or incorrectly classified signal events.

Accuracy:Accuracy measures the overall correctness and is defined as the ratio of correct
predictions to the total no of events/samples.

Specificity or True Negative Rate (TNR):Specificity is the ratio of true background to the total number of background events.

ROC curve and area under the curve (auc):The ROC (Receiver Operating Characteristic)
curve444The ROC term originated in the context of electrical engineering
during World War II, when electrical signals were used for the prediction of enemy objects.is a graphical representation of TPR or recall against FPR or (1 - specificity) for various threshold settings. In HEP, the ROC curve relates the signal efficiency versus background efficiency/rejection plane as shown in Fig24and the curve illustrates the ability of a binary classifier to separate signal and background events. The ROC curve ends at (1,1)
for a perfect classifier that accepts 100% signal events and rejects 100% background events.
The area under the ROC curve is known asaucmetric and it varies from 0 to 1. For a perfect classifier, theaucbecomes one and random guessing
leads toaucvalue = 0.5. Thus theaucmetric is a powerful tool for the evaluation of the overall ranking performance of a binary
classifiersulaiman.

F-scoremetric:F-score is a measure that combines recall and Precision, and it is basically
the harmonic mean of them. F-score can be tuned via a real parameter () and the generic expression is given by:

For, it is known as-score metric and expressed as.
The F-score can be a better measure than the accuracy metric on imbalanced datasets. A high recall and precision rates indicate low FN and low FP rates andscore can be useful for imbalanced HEP dataset where the signal events are very rare compared to backgrounds.

Approximate Median Significance (ams) score:In the field of high energy physics, the primary goal is to optimize the discovery significance. To estimate the discovery significance, the formulasorare commonly used555For discovery of a new particle the significance should beand for exclusion it should be, where s and b denote the numbers of signal and background events, respectively, that remain after the signal selection cuts.
It may be noted that in a typical Poisson counting experiment, whereevents are observed, the Poisson distribution often features a large mean value,. However, the formulais valid only forand it
overestimates the discovery significance when the background events are
smallCowan2012DiscoverySF;Cowan:2010js. For a Poisson counting experiment with negligible uncertainty, the Asimov approximation for the median significance (ams score) is given byCowan2012DiscoverySF;Cowan:2010js:

For the scenarios with very small background events,is replaced by, whereis a regularization term typically set to stabilize
the calculation. Expanding the logarithm in s/b, the Eq.9reduces to. In the context of particle physics experiments, there is generally an uncertainty with the background and this
uncertainty () reduces the significance or the ams score. Due to the
presence of the uncertainty, the Eq.9modifies asCowan2012DiscoverySF

SECTION: 3Machine learning in High Energy Physics

In this section, we primarily review the studies that have utilized decision tree-based algorithms666We will also briefly mention the relevant works where algorithms other than DT have been used.within the context of Standard Model (SM) and Beyond Standard Model (BSM) physics scenarios. A comprehensive list of references, grouped into a minimal number of topics, is regularly updated in theLiving Review(hepmllivingreview,), covering various categories like ML review works, classification and regression in supervised or unsupervised learning, generative models and more.

Nearly two decades ago, the first notable use of a boosting algorithm and its performance comparison with the artificial neural network (ANN) and other algorithms was made in the MiniBooNE experiment at Fermilab, which was designed to explore neutrino oscillationsROE2005577;YANG2005370.
It was observed that particle identification (PID) with AdaBoostFREUND1997119algorithm is better than the standard ANN PID technique or Random ForestROE2005577;YANG2005370. The separation between signal and background events from AdaBoost algorithms is shown in Fig.3where the choice of parameters was:= 0.5, the number of trees () = 1000 and the number of leaves () = 45. In the next few years, Dand CDF experiments at Fermilab used the BDT algorithm along with other algorithms like neural network, matrix elements, etc. for the analyses of single top quark production from the Tevatron dataD0:2006ngk;D0:2008wma;CDF:2010eor. The performance of the BDT algorithm was slightly better than others and the combined results of different techniques led
to the initial evidence and subsequent observation of single top quark productionD0:2006ngk;D0:2008wma;CDF:2010eor. The importance of BDT analysis for this observation has been discussed in a recent reviewCoadou:2022nsh. Over the past twenty years, the HEP community has been applying the BDT and other more advanced algorithms extensively for event triggering, event generation, parameter space exclusion/scan, jet identification/tagging, event classification and more.

Event Triggering:In particle physics experiments, the event triggering is very crucial for managing the enormous volume of data, reducing the event rates for storage, conducting real-time analyses, enhancing the sensitivity of new physics searches, etc. The CMS collaboration has implemented
BDT in the Endcap Muon Track Finder (EMTF) at the Level 1 (L1) trigger level for the LHC Run-II data collectionCMS:2018wav. Also, after Phase-2 upgrade, the CMS experiment will use a dedicated
BDT classifier at the HGCAL to achieve optimal signal efficiency while rejecting pileup-induced backgroundsZabi:2020gjd. The high-level trigger (HLT) algorithms run in an online environment and they must be
very fast. The LHCb collaboration has reoptimized the HLT using a bonsai BDT (BBDT) algorithmGligorov:2012qtwithin the Adaboost frameworkLikhomanenko:2015aba.
It may be noted that for exotic events with long-lived particles (LLPs) searches, the existing
triggers are not suitable enough to select the displaced events at the HL-LHC. In such cases, modern machine learning algorithms like lightweight graph autoencoder can be more promisingBhattacherjee:2023evs.

Event Simulation and Parameter space scanning:Monte Carlo event generations, along with fast detector simulations, are becoming
more and more computationally expensive as the size of the LHC data is increasing.
Also, for the generation of events with multiple outgoing particles or the simulations of next-to-leading (NLO) order processes with a large number of additional jets at the LHC/HL-LHC, the evaluation of matrix elements becomes computationally intensive. A new machine learning algorithm based on gradient BDT (GBDT) has been proposed and tested for Monte Carlo integration in Ref.Bendavid:2017zhk.
Most commonly, the exploration of the parameter space of a new physics model involves the evaluation of some complex likelihood function. Using popular
approaches like frequentist and Bayesian statistics coupled with Markov Chain Monte Carlo (MCMC) methodology or MultiNestFeroz:2007kg;Feroz:2008xx;Trotta:2008bpalgorithm based on Nested Sampling10.1111/j.1365-2966.2009.14548.x, several phenomenological groups have analyzed and constrained the SUSY parameter spaceTrotta:2008bp;GAMBIT:2017snp;GAMBIT:2017zdo;Choudhury:2023lbp.
A recent study has explored the effectiveness of Random Forest (RF) classifier, which is a decision tree based algorithm, in accurately predicting whether a particular SUSY model is excluded by LHC data or notCaron:2016hib.
For the Monte Carlo simulation and scanning or recasting of SUSY parameter space, the HEP community has also extensively used more modern/advanced machine learning techniques
like Multilayer Perceptron (MLP)Bridges:2010de;Buckley:2011kcBayesian Neural Network (BNN)Kronheim:2020vct, Graph Neural NetworkMullin:2019mmh, Generative Adversarial Network (GAN)DiSipio:2019imz;Butter:2019cae;Lin:2019htn;Musella:2018rdi, active learning (AL)Ren:2017ymm;Caron:2019xkxetc. These studies showed that the use of ML algorithms reduces CPU time and storage. For a recent review on the sampling of BSM parameter spaces subjected to available experimental data using machine learning algorithms, please see Ref.Baruah:2024gwy.

Jet tagging:In collider experiments, the reconstruction and identification of hadronic jets are integral components for physics analyses. Jets originating from
massive particles (e.g., bottom quark, top quark,bosons or supersymmetric partner of top quark ()) are usually boosted and collinear. Jets emerging from b-quark are associated with long lifetime and secondary vertex and can be differentiated from other jets coming from light quarks. Motivated by computer vision techniques,jet imageswas first introduced in Ref.Cogan:2014ouaand then many groups have used modern deep neural network (DNN) architectures
for jet tagging withjet imagesdeOliveira:2015xxd;Baldi:2016fql;Komiske:2016rsd;Chakraborty:2019imr;ATLAS:2017dfg. The CMS collaboration has developed DeepCSVCMS:2017wtu;Bols:2020bkb, DeepFlavour2020HeavyFI;deepflvand DeepJetBols:2020bkbtaggers for multiclass classification for light jets, gluon jets, c-jets and b-jets using a deep learning algorithm. In a recent workATLAS:2019bwq, the ATLAS Collaboration has developed two high-level b-tagging algorithms - MV2 and DL1Krizhevsky2012ImageNetCW. MV2Krizhevsky2012ImageNetCWalgorithm is based on BDT777This same algorithm has been used in another analysisATLAS:2020hpj, where the ATLAS collaboration has reported the first evidence ofproduction.and is trained within the TMVA frameworkTMVA:2007ngy. The performance of b-jets tagging using the simulatedevents has been presented in Fig.1 inATLAS:2019bwqfor BDT and deep feed-forward neural network algorithms.
Although for jet images, tagging, and substructure studies, deep learning algorithms are the most efficient ones, the ATLAS collaboration recently showed that W-boson tagging and top quark tagging ML algorithms lead to
a significant gain in efficiency compared to cut-based analysis, as presented
in Fig.4ATLAS:2018wis. This analysis has been performed using LHC Run-II data with. It may be noted that both the BDT and DNN-based algorithms perform similarly to each other for all signal efficienciesATLAS:2018wis.

SECTION: 3.1Signal and background events classification

As mentioned earlier, the most important goal of a collider experiment is the production and search of new particles by identifying rare (mostly) signal events from huge backgrounds. The last missing piece of the Standard Model, aka the Higgs boson, was discovered by the CMS and ATLAS collaboration in 2012ATLAS:2012yve;CMS:2012qbp. The CMS Collaboration has used the BDT algorithm, implemented within the TMVA frameworkTMVA:2007ngy, for the Higgs discovery analysis in RefCMS:2012qbp. Using the full dataset collected in 2011 and 2012 from 7 & 8 TeV LHC run, CMS has updated theanalysis in Ref.CMS:2014afl, where BDT has been used extensively for several tasks such as - photon identification, photon vertex reconstruction, signal-background classification with good diphoton mass resolution, classification of VBF, Vh,tagged events etc. ATLAS Collaboration has also performed an analysis ofproduction, where the Higgs boson (h) decays to apair, using Run-II data withATLAS:2017fak. In this analysisClassification BDThas been trained to separate the signal ()
from backgrounds and ATLAS has usedReconstruction BDTto select the best combination of jet-parton to reconstruct the Higgs boson and top quark candidatesATLAS:2017fak. In the next subsection, we will summarize the R-parity conserving (RPC) and R-parity violating (RPV)Barbier:2004ez;Choudhury:2024ggysearches using DT/BDT-based algorithms. The most distinct features between RPC and RPV SUSY scenarios are large missing energy from a stable LSP in the RPC case and higher lepton and jet multiplicity (arising from RPV couplings) in the latter case. Numerous experimental and phenomenological studies have explored ML algorithms to enhance the discovery reach and exclusion limit in contrast to traditional cut-based analyses. We outline some of the results below.

Among different SUSY models, the RPC SUSY888RPC SUSY can provide the possible Dark Matter (DM) candidate as the LSPKumarBarman:2020ylm;Barman:2022jdg;He:2023lgi;Chakraborti:2015mra;Chakraborti:2014gea;Chakraborti:2017dpu;Chowdhury:2016qnz;Bhattacharyya:2011se;Choudhury:2012tc;Baer:2021aaxand also can explain the muon (g-2) excessChakraborti:2022vds;He:2023lgi;Chakraborti:2015mra;Chakraborti:2014gea;Baer:2021aax;Athron:2021iuf;Endo:2021zal;Chakraborti:2021bmv;Choudhury:2017acn;Choudhury:2017fuu;Banerjee:2018eaf;Banerjee:2020zvi;Chakraborti:2021dli;Frank:2021nkq;Ali:2021kxa;Kowalska:2015zja;Chakrabortty:2015ika;Choudhury:2016lkuis most widely studied by both the ATLAS and CMS collaborationsatlas_susy;cms_susy. In several analyses, both collaborations have used the BDT algorithms to improve the sensitivity of sparticle
searches. In this section, we will summarize the works where BDT algorithms have been used by the experimental and phenomenological groups in the context of RPC SUSY models.

Searches for first two generation squarks and gluinos:The ATLAS collaboration has searched for squarks and gluinos in fully hadronic channel (+ jets +final states) based on full Run-II dataset () using three strategies:multibin search, BDT searchandmodel-independent searchATLAS:2020syg. Results were interpreted in various simplified scenarios where gluino can decay directly () or via one step () as shown in Fig. 1 of Ref.ATLAS:2020syg. Depending on the mass difference,
ATLAS has separated the events into four categories for these gluino decay modes. Eight independent BDT were trained to obtain the optimal sensitivity (see Table 7 of Ref.ATLAS:2020syg). Among the three search strategies, theBDT searchwas the most robust tool, particularly effective in scenarios involving gluino decays via, resulting in complex event topology with a large number of jets. The optimized BDT regions, chosen based on their BDT score, achieved the best sensitivity due to their ability to exploit the correlations between variables. In regions of the parameter space where the mass difference between the LSP - NLSP is small and approaches the kinematic limit, themulti-bin searchis particularly effective and has excluded gluino and neutralino masses upto900-1000 GeV (see Fig.5)
from gluino pair production. For relatively light neutralino masses, the observed lower limit on the gluino mass reaches upto 2.3 (2.2) TeV for direct (one-step) gluino decayATLAS:2020sygand is derived from the optimized BDT regions (see Fig.5). The same analysis has also excludedupto 1.85
TeV for massless lightest neutralino considering the pair production of mass degenerate first and second generation squarks.

Searches for Stop:The light stop scenario is theoretically well-motivatedBalazs:2004buand also has a rich phenomenology. Both the ATLAS and CMS collaborations have looked for the lightest stop squarks () through all possible decay modes with various final states.
The possible decay modes of stop are,,,,. The last one (four body mode) may dominate in the compressed SUSY scenario where the other decay modes are kinematically forbidden andis suppressed. In a recent study, the CMS collaboration
has used the BDT algorithmsTMVA:2007ngyto optimize the separation between background and signal eventsCMS:2023ktc. The discriminating variables of signal and background processes have different correlations and CMS has performed the BDT analysis for different values of. The distribution of the score of multivariate analysis (the BDT discriminator value or BDT output) is shown in Fig.6as obtained by CMS forGeV. This BDT search has excludedupto 480 and 700 GeV for= 10 and 80 GeV at 95CLCMS:2023ktc.
CMS had also implemented a BDT multivariate approach to define the signal regions for the analysis of stop pair production with subsequent, anddecays in Ref.CMS:2013phu. The comparison between the exclusion plots obtained by CMSCMS:2013phufordecay using cut-based and BDT methods are presented in the left and right panels of
Fig.7.

A recent phenomenological workJorge:2021vpohas expanded and improved the ATLAS analysisATLAS:2020xzuforpair production in the semileptonic channel where the stop squarks decay via 3 body mode. ATLAS has used a recurrent neural network (RNN) algorithm and excluded stop mass upto 710 GeV using 13 TeV LHC data withATLAS:2020xzu. The authors compared the performance of
Logistic Regression, Random Forest, XGBoost and Neural Network algorithms
in the Ref.Jorge:2021vpoat 13 TeV LHC with. It has been shown that in such compressed scenarios,
XGBoost and Neural Network algorithms improve the signal significance
more efficiently compared to other algorithms as well as the cut-and-count approach.

Searches of Electroweakinos and sleptons:Very recently, for the first time, the ATLAS Collaboration has reported the sensitivity ofonly RPC SUSY scenarios withfinal states using the LHC Run-II dataset withATLAS:2024fub. In this analysis, other scenarios withandpair productions also have been considered. The LightGBM package has been used
to train multiple BDTs on the sensitivity of stau-LSP phase space. The ATLAS experiment has excludedmasses upto 350 GeV by improving the signal background separation through the use of BDTATLAS:2024fub. The training was done for four BDT and for illustration purpose, we present the post-fit BDT score distribution for the signal region BDT3ATLAS:2024fubin Fig.8In another recent study, the ATLAS collaboration has looked for the chargino pair production via 2l +final states where the mass gap between the charginos and lightest neutralinos are close to W boson massATLAS:2022hbt. For this analysis, ATLAS has performed a multiclass GBDT classification using LightGBMNIPS2017_6449f44a, and signal regions are defined according to BDT scores (see Table 3 of Ref.ATLAS:2022hbt). The observed and expected numbers of events are presented in Fig.9along with the significance.
In the absence of any significant excess, ATLAS has excludedup to 140
GeV at 95% CL forGeVATLAS:2022hbt.

In the RefCornell:2021gut, the authors have shown that gradient boosting algorithm can extend the 95% C.L. exclusion limits derived from traditional cut-based methods using an example of smuon pair production at the 13 TeV LHC. The authors have also studied the utility of various performance metrics (auc, ams, F-score etc.) usingXGBoosttoolkit, feature importance using SHAP packageShapley+1953+307+318;lundberg2018consistentin great detailsCornell:2021gut. The possibility of constructing a general machine learning model that may be applied to probe a two-dimensional mass plane has also been examined in this work.
InAlvestad:2021sje, the authors pointed out
thatXGBoostalgorithm can significantly increase the detectability of a SUSY models
with a gravitino type LSP and a metastable sneutrino NLSP by analyzing the events
coming from the electroweakinos production along with the slepton productions.
The possibility of probing the light Higgsinos, which are still allowed by LHC Run-II and LZ experiment data, at the upcoming HL-LHC run viaXGBoostframework has been studied in Ref.Barman:2024xlc

Searches of Heavy Higgs:The Minimal Supersymmetric Standard Model (MSSM) consists of two Higgs doublets and after electroweak symmetry breaking, the Higgs sector contains two CP even Higgs (h and H), one CP odd Higgs boson (A) and two charged Higgs bosons.
Extensive studies from LHC collaborations and phenomenological groups have already been performed for the Heavy Higgs searches. The implication of LHC results from
125 GeV Higgs boson searches and other heavy Higgs searches have been analyzed
in Ref.Bagnaschi:2017tru;GAMBIT:2017zdo;Bhattacherjee:2015sga;Barman:2016jov. The ATLAS collaboration has used the multivariate BDT analysis for the searches of charged Higgs () in Ref.ATLAS:2018ntn;ATLAS:2018gfm. Forand in the alignment limit,is the dominant decay mode and for large values of tan, the branching ratio ofbecomes.
ATLAS and CMS both have looked for these final states where the dominant production mode isATLAS:2018ntn;ATLAS:2018gfm;CMS:2020imj;CMS:2019bfg. CMS collaboration has used a a multivariate BDT with gradient boost (BDTG) classifier within TMVA toolkit formodeCMS:2020imj. ATLAS has performed the training of the BDTs with the TMVA toolkit for different values ofin various signal regions to discriminate thesignal from the SM backgrounds withfinal statesATLAS:2018ntn. The BDT score distribution obtained from this analysis for signal benchmark points with= 200 and 800 GeV along with SM backgrounds are shown in Fig.10(left panel) from the3b channel. For thefinal states, training of the BDT
has been performed using the FASTBDT algorithmKeck:2017gsv. The resultsATLAS:2018gfmwere interpreted in the-mass plane (see the right panel in Fig.10). The limit comparison plot shows that at high tan, stringent limits come from thechannel and at low tan, tb channel is more effective forGeV.

The authors in Ref.Baer:2021qxahave studied the prospect of Heavy Higgs search in a radiatively-driven natural supersymmetric model where
the neutral Heavy Higgs decays toorand give rise tosignature.
Detecting this signature at the HL-LHC will be very difficult. But it is shown the conventional cut-based method can probeupto 1.65 TeV at the future 100 TeV pp colliderBaer:2021qxawith signal significance greater than 5. Using a multivariate analysis (BDT) within the TMVA toolkit, the authors also pointed out that the BDT algorithm improves the signal significance and heavy Higgs can be probed upto 2 TeV.

One of the key motivations for going beyond the RPC MSSM is its inability to provide an explanation for the neutrino oscillation phenomena. The RPV SUSY scenarios can explain the light neutrino masses and mixingGrossman:2003gq;Davidson:2000uc;Roy:1996bua;Allanach:2007qc;Diaz:2014jta;Choudhury:2023lbp.
The RPV SUSY models can also address the muon (g-2) anomalyChakraborty:2015bsk;Altmannshofer:2020axr;Chakraborti:2022vds;Hundi:2011sior flavor anomaliesTrifinopoulos:2019lyo;Domingo:2018qfg;Das:2017kfo.
Due to the presence of lepton and/or baryon number violating terms, the LSP becomes unstable and decays to SM particles. As a result, the final state
contains less missing energy and a higher number of leptons and/or jets, depending upon the coupling. Although the ATLAS and CMS collaboration have mainly presented results in the RPC SUSY scenarios, there exist a few analyses, based on cut based method, involving strong or electroweak sparticles pair production with subsequent RPV decaysATLAS:2021yyr;ATLAS:2021fbt;ATLAS:2024kqk;CMS:2017szl;CMS:2021knz;CMS:2018skt. A recent workDreiner:2023bvshas summarized the possible gaps in
RPV-MSSM searches at the LHC in great detail by meticulously classifying
the various possible RPV-MSSM signatures at the LHC and analyzing
both direct and indirect production of various LSPs, the authors have derived limits on sparticle masses.

In the presence of non-zero trilinear couplings, the LSP decays into, where. Thus, the final states become leptonically enriched.
For different choices ofcouplings, the LHC collaborations have
already derived limits on chargino and slepton masses(ATLAS:2021yyr,).
The yellow and red regions in the left and right panels of Fig.11represent the excluded regions on chargino-LSP and slepton-LSP mass plane.
The authors in Ref.Choudhury:2023eje;Choudhury:2023yfghave extended the similarfinal states in the context of the electroweak sparticle
searches at the 14 TeV high-luminosity LHC (HL-LHC) and the proposed
high-energy (27 TeV) upgrade of the LHC (HE-LHC). It has been shown that
an optimized cut-based analysis will be able to excludeupto
2.18 TeV at the HL-LHC from wino likeandproductionsChoudhury:2023eje. The authors have further explored
a multivariate analysis based on an Extreme Gradient BDT algorithm to
improve the results further.
The projectedexclusion limit reaches up to
2.37 TeV and 4.0 TeV at the HL-LHC and HE-LHC, respectively, from
the ML-based analysisChoudhury:2023eje(the dark violet color represents thereach for
HL-LHC in Fig the left panel of Fig.11).
In another recent workChoudhury:2023yfg,
considering the pair and associated production of mass degenerate
sleptons, the authors have studied the sensitivity of similar final state
at the future LHC. The right panel of Fig.11shows the
discovery and exclusion reach on L-type slepton masses obtained for
nonzeroand/orcoupling valuesChoudhury:2023yfg.
It is observed that
the projected exclusion limits on slepton and sneutrino masses at the HL-LHC (HE-LHC) are1.85 (3.0) TeV from ML-based analysis. The ML algorithm
shows significant improvement over cut-and-count method (see Fig.11).
The work in Ref.Bhattacherjee:2023kxwhas addressed the search prospect of long-lived particles in the RPV SUSY scenarios from the electroweakino pair production,
where the LSP decays via UDD-type couplings. Utilizing the
Gradient Boosting algorithm withinXGBoosttoolkit,
the authors obtained that wino-like (higgsino like)with a mass of 1900 (1600) GeV andwith a mass greater than 800 (700)
GeV can be probed for decay length ranging from 1 cm to 200 cmBhattacherjee:2023kxw.

SECTION: 4Decision Tree algorithms

The decision treesBreiman:1984jkaare one of the simplest and most widely used
classification and regression tools, which use inverted tree like structure with root on the top, to predict the value of an output by applying binary cuts in the feature space. Breiman et al.breiman;Breiman:1984jkaproposed“Classification and Regression Trees” (CART)algorithm for the implementation of decision trees (DT). The CART algorithm
begins with the root-node consisting of the entire training dataset. It then proceeds to split the parent node into several branches or child nodes
until the model can make a decision regarding the class of a given data point
or it satisfies a predetermined stopping criteria.
Through this binary splitting, the algorithm effectively partition the data into subsets. When the splitting criterion is met by the node, it is termed as a leaf.

We demonstrate this process with a medical science example in the Table.2regarding the risk of heart disease of a person. In medical science, the risk of being diagnosed with a heart disease primarily depends on the person’s age, blood pressure, weight, whether the person is diabetic or not etc999For the prediction of heart disease using ML algorithms see Ref.hossain2023heart. The initial five columns denote input features, while the last column signifies the resulting outcome. A decision tree, depicted in Fig.12, is constructed using this dataset. In the tree visualization, the root node is depicted in brown, leaf nodes in green, and general nodes in blue.

In the context of HEP, the goal for DT algorithm is mainly to classify an entry as a signal or background. For classification problem, the journey from theroot-nodeto theleaf-nodesignifies a series of cuts which determines whether an entry is classified as signal or background depending upon
the characteristics of theleaf-node.
Each leaf can be assigned a purity value, calculated aswhereandrepresent the sum of weights of the signal and background events contained within that leaf. Depending upon this value of purity the events in that leaf can be identified as signal or background ( if, it is identified as signal; otherwise, it is classified as background.)
At each node, the feature and its corresponding threshold value determine the subsequent splitting of the node. The selection of the feature and its threshold value depends on the decrease in impurity.
The commonly used impurity functions includethe misclassification error,the cross entropybreiman;Breiman:1984jkaandthe Gini index of diversitygini;ceriani2012origins, all of which are determined by the purity of signal and background. Among these impurity function, the most popular one is theGini indexwhich performs similar to entropy.
In HEP, generally we try to optimize the signal significanceorby applyingCross section significanceorexcess significancewhich is defined asandrespectively. On the other hand, regression tress trees split nodes based on minimizingthe Sum of Squared Errors (SSE). The tree-growing process is stopped usingstopping criteria.
Thestoppingcondition may include several criteria, such as reaching a maximum tree depth, maximum number of leaf nodes, ensuring a minimum number of instances within each leaf node, insufficient improvement through further splitting, or achieving complete splitting, where all events within the node belong to the same class.
Sometimes, even before reaching the maximum allowed depth or number of nodes,
early stopping criteria are used to prevent overfitting and improve the generalization.

After constructing the tree, predictions are generated by navigating from the root node to a leaf node that matches the input data. Although decision trees are adept at modeling training data, they are known for their susceptibility to instability. Overfitting to the training sample can make a decision tree overly sensitive to minor variations in inputs, reducing its effectiveness for unfamiliar events or data, as it heavily relies on the training set.
Pruning refers to the process of cutting irrelevant branches or reducing the size of DT by removing nodes and branches that do not significantly improve the predictive performance of unseen data. Pruning a tree is crucial for reducing instability, avoiding overfitting, and enhancing the model’s ability to generalize with new data. “Pre-pruning” is similar to the early stopping we have already discussed above. “Post-pruning” involves constructing an extensive tree initially and subsequently eliminating irrelevant branches. These branches are pruned by converting an internal node and all its offspring into leaves, thereby eliminating the corresponding subtree.
There are several pruning algorithms like expected error pruning, reduced error pruningQUINLAN1987221and cost-complexity pruning which is a part of the CART algorithmBreiman:1984jka.

There is another method to handle the instability of the decision trees,
which is called “ensemble learning”. Ensemble learning addresses this by combining predictions from multiple trees, potentially boosting discriminatory power. Techniques such as bagging, boosting, and random forests fall under this framework, offering various ways to improve model performance.
The combination of aggregation and bootstrapping is known as “bagging” or bootstrap aggregating. Initially, a new dataset is created by randomly sampling data from the original dataset, ensuring that the new dataset contains the same number of samples as the original dataset.
One important point is that individual samples from the original dataset can appear multiple times in the new dataset. This process is known as ”bootstrapping.” Subsequently, multiple trees are constructed based on the features of the new dataset. This procedure is repeated several times until the algorithm generates and aggregates a considerable number of trees.
In the upcoming subsection, we will summarize the following DT algorithms that utilize “ensemble learning” techniques. Specifically, we will focus on Random Forest, which is an extension of bagging and boosting algorithms. Within the boosting category, our attention will be on AdaBoost, as well as two Gradient boosted decision tree (GBDT) algorithms: XGBoost and LightGBM101010It may be noted that there are several other variations ofAdaBoostalgorithm, e.g.,BrownBoostfreund1999adaptive,-LogitBoostfriedman2000additive,-HingeBoostYang:2005nz, etc. Also, a new GBDT algorithm with categorical feature support known asCatBoostdorogush2018catboosthas been proposed recently..

SECTION: 4.1Random Forest

The Random forest algorithmBreiman:2001hzmis a robust ensemble learning method used for regression and classification jobs. The key concept of the Random forest algorithm is to create an ensemble (forest) of decision trees and to select the prediction voted by the majority/averaging of all those trees depending upon the classification/regression task. For a better overall outcome, a subset of input data is passed through each decision tree. Randomizing the training variables helps to obtain better accuracy.
Initially, a new dataset is generated for a given input dataset withsamples. The algorithm randomly selects a sample and adds it as the first entry of the new dataset. This process iterates fortimes. It may be noted that one particular sample can occur more than once in the new dataset. This process is called “bootstrapping”. For a given input feature setwhere, decision trees are made taking a random subset of features, such that. Based on these features, a tree is made.
This process repeats several times till the algorithm generates and aggregates a large number of trees. The aggregation, together with bootstrapping, is referred to as “bagging”. Among all the trees, the outcome with the majority of votes is selected as the model prediction for classification problems.
For regression tasks, the final outcome is calculated by averaging the predictions of all the decision trees.
The model outcome depends on the hyperparameters, such as the number of trees the algorithm grows, the maximum depth of a tree, the maximum number of leaf nodes a tree can have, etc. The creation of a new tree is an iterative process, and it does not stop until the algorithm meets its stopping criterion (a pre-determined number of trees the algorithm can grow, which is fixed by the user). A flow chart for the Random Forest algorithm is shown in Fig.13.

To illustrate this point, let us consider a sample dataset (Table.3) for a supersymmetric (SUSY) signal with the Standard Model background, where the signal and background are labeled as 1 and 0, respectively. We have considered the transverse momentum of leading and sub-leading lepton (and) and the leading jet (), missing transverse energy () and(where,andare the pseudorapidity and azimuthal angle, respectively.) between leading and sub-leading lepton () as input features.

The label (class) corresponding to each entry is written in the gray-shaded column at the right. The first six entries are training data, and the light-blue colored data is test data. A forest of decision trees is made (Fig.14) from the training data of Table.3.

Test data generates predictions as it passes through each tree in the forest. For our example, the outcome of each tree is marked with a blue color when the test data from Table.3passes through them. In our example, two of the three trees classify that entry as a background (), and one classifies it as a signal (). Since the majority is “0”, the prediction for this particular entry is “” or background. It is worth noting that the predicted output matches the actual data, which signifies the accuracy of the algorithm.

Random forest can deal with both classification and regression problems and it can handle high dimensional data very well. The accumulation of numerous decision trees and random feature selection provides better accuracy and reduces overfitting. On the other hand, the random forest algorithm is computationally expensive and time-consuming.

SECTION: 4.2AdaBoost

Decision trees are the base learner for Random Forest and many other algorithms. The learners can be weak or strong depending on their prediction accuracy. Poor prediction accuracy is associated with weak learners, whereas a strong learner yields better model prediction accuracy. While it is hard to predict an output only through a weak learner, a combination of such weak learners can be made into a strong learner. At each step, the misclassified instances are provided with a larger weightage, resulting in better accuracy at the final step. This process is called “Boosting”.AdaBoostfreund1999short;wang2012adaboostworks on the basic principle of the boosted decision tree (BDT) algorithm. The termAdaBoostoriginates fromAdaptive Boosting, as the misclassified events are given a larger weightage, while the correctly classified events are given a smaller weightage in this algorithm. Freund and Schapire introduced theAdaBoostalgorithmFreund:1997xnain 1995 and from then, it has been widely used in many scenarios including the HEP analysis.

A set of labeled training datawheredenotes the input features andstands for the label, is fed into the algorithm. Here, and eachandbelong to domainsand, respectively (). For simplicity, here we have chosen binary classification to elaborate on, and the classes are denoted asand. The base learner is repeatedly called by the algorithm foriteration. It may be noted thatdenotes the hyperparameternumber of trees/estimators. The weight of thesample initeration is denoted by. In the beginning, all samples are assigned the same weight. At each iteration, the learner is compelled to focus on the incorrectly classified event by giving larger weightage to the incorrectly classified events. The learner constructs a weak hypothesisforwith the lowest error, where the error is given by. The algorithm next assigns a parameterto the weak hypothesis,, which is a measure of performance of. Here,acts as thelearning rateorshrinkage coefficient, which is a measure of the strength of boosting. It is worth noting that for,, andincreases with decreasing. The weight,, is then modified. A larger weight (multiplyingby) is given for the misclassified events () and a smaller weight (multiplyingby) is given to the correctly classified events (). After completion ofiteration (stopping criterion), the weighted majority of allweak hypotheses is chosen to be the final hypothesis or strong learner,. The flow chart of theAdaBoostalgorithm is presented in Fig.15.

The ability ofAdaBoostto create a strong learner by combining several weak learners by weighted data sampling makes it more efficient than the Random forest algorithm. Furthermore, theAdaBoosttraining process leads to faster convergence in many scenarios. However, while dealing with complex datasets or when the base learner is too complex,AdaBoostis susceptible to overfitting.

SECTION: 4.3XGBoost

A general gradient boosting decision tree (GBDT) algorithm uses gradient descent to minimize the loss function. In addition to the boosting, it optimizes the algorithm to find the local minima of a differentiable loss function. A loss function signifies the quantitative goodness of the model prediction. The “eXtreme Gradient Boost” orXGBoostChen:2016btlis an extension of the GBDT algorithm. Here, the algorithm utilizes the second-order derivatives of the convex loss function, in contrast to a general GBDT algorithm where only the first-order derivative is used. The flow of theXGBoostalgorithm can be divided into two parts: the first consists of boosting by obtaining optimal leaf score or weight for tree growth, and the second is optimizing the algorithm for smoother operation. In the following, we discuss them.

XGBoostuses a regularized objective; in simpler terms, one regularization or penalty term is added with the objective function, and it helps to prevent overfitting. For a given datasetwithfeatures andentries such that, the predicted output fornumber of trees isChen:2016btl

whereis the tree function space. The mapping for a featureto the corresponding leaf index is denoted by the functionandwhereis the number of leaves attree. Here,is the score ofleaf. A regularization term () is added to the loss function () and the new objective function becomesChen:2016btl

where. Here,is the minimum loss reduction term used to further divide a leaf node, andis a ridge regularization term acting on the leaf weights. A higher value ofandresults in a more conservative algorithm. As the number of leaves increases within a tree, the objective function is subsequently increased. Consequently, this makes minimizing the objective more challenging. To overcome this issue, we addto minimize the objective.

whereanddenote the objective and prediction forentry atiteration. The next step is to optimize the objective function, and for that,is expanded in the Taylor series up to the second orderChen:2016btl

whereis called the gradient andis called the hessian. Letbe the set of instances whereinstance is mapped toleaf with tree structure. Eq.(14) can be written asChen:2016btl

The optimal weight of theleaf,can be written as

and the minimum value ofis

and it represents a quantitative measure of the goodness of the tree structure, which is analogous to the impurity score. A smaller value ofmeans a better tree structure. While it is non-trivial to count all possible trees to find the tree with the maximum impurity score, the use of a greedy algorithm eases the scenario. From a single leaf withinstances, left and right branches are made with instancesand, respectively, such that. It chooses the split with the highest reduction in loss. After the splitting, the loss reduction can be quantified asChen:2016btl

As we can see,XGBoostuses the second order derivative () along with gradient () of the loss function for boosting, which is different from other GBDT algorithms. We present the flow chart of theXGBoostalgorithm in Fig.16. Finding the optimal split for a node is a non-trivial task, especially for large and complex datasets.XGBoostuses several split-finding algorithms. For a relatively simple dataset, it uses a “basic exact greedy algorithm”Chen:2016btlwhere it goes over all possible splits and finds the best one. However, for a large dataset, this technique results in a trade-off with reduced efficiency. To handle this, the algorithm divides the dataset into several percentile or weighted quantile buckets, treats each bucket separately through parallel learning and finds the best split. To deal with sparse data,XGBoostinvokes the “sparsity-aware split algorithm,” where it enumerates the gain by going in both directions in a split and chooses the one with maximum gain. These features ofXGBoostto tackle such complexities in a dataset make it scalable to almost all scenarios and popular among data scientists.

SECTION: 4.4LightGBM

Simple gradient-boosting techniques have been quite successful so far. A common attribute of GBDT is to enumerate over all instances in dataset for each feature to find the optimal split. As data complexity and size increase, GBDT faces challenges in efficiency and accuracy. To overcome this issue, novel techniques are implemented upon simple GBDT algorithm, namely, (i) leaf-wise tree growth, (ii) Gradient-based one-sided sampling (GOSS), (iii) histogram splitting, and (iv) Exclusive feature bundling (EFB). The new algorithm that incorporates these new techniques with GBDT, developed by Microsoft, is referred to as Light Gradient Boosting Machine-learning orLightGBMke2017lightgbm.
In the following, we discuss the key concepts ofLightGBM:

Leaf-wise tree growth:The attribute that fundamentally separatesLightGBMfrom common GBDT is that it is based on “leaf-wise tree growth” (demonstrated in the right panel of Fig.17). The leaf node that provides maximum loss is split by the algorithm, and this process is repeated. In this case, one leaf may branch deeper than the other while both of them are coming from the same root node. The general GBDT algorithm is based on “depth-wise tree growth” or “level-wise tree growth”(as shown in the left panel of Fig.17), where trees are split horizontally at each level, and it expands every node at the same level before going to the next round.

Gradient-based one-sided sampling (GOSS):In GBDT, the smaller the gradient is, the better the fit becomes. In this technique, a subset of dataset is created by mostly retaining the instances with higher gradients and discarding (by random sampling) instances with smaller gradients. GOSS retains a percentage () of data with the highest gradient and randomly selectsdata from the rest and creates a new subsample, given. While calculating the information gain, the algorithm multiplies a factorto the instances with a smaller gradient. The under-trained instances thus get more attention even though no initial weight is associated with the original dataset.

Histogram splitting:For a tree branching,LightGBMalgorithm bins the feature values into two or more sets rather than evaluating all potential split points for every feature.
Let us suppose for a given dataset, there exists an input feature, “Age,” where the entries are 40, 45, 47, 50, 65, 72, 75, 80. The common approach for GBDT while creating a decision tree is to sort the value for which the optimal prediction is achieved. In the case of histogram splitting, the algorithm splits the dataset into two or more bins or buckets, say, age group 40-60 and 61-80 and then proceeds with the analysis. It makes the algorithm faster.

Exclusive feature bundling (EFB):The complexity of an algorithm increases with increasing input features. To improve efficiency, the algorithm looks for exclusive features in a given dataset (features that do not take non-zero values at the same time). To illustrate this point, let us suppose a binary dataset has a feature column for the gender of the person in a given entry. If we set the male tothen the female is automatically set toand vice-versa. Both cannot take the same value simultaneously. EFB carefully combines these exclusive features and creates a new bundle, thus effectively reducing the input features without sacrificing accuracy.

The flow-chart ofLightGBMalgorithm is shown in Fig.18.LightGBMalgorithm turns out to be more efficient in terms of time consumption compared withXGBoostwhile maintaining a competitive accuracy. Due to its histogram-based approach,LightGBMuses less memory, making it more efficient and scalable while dealing with complex large dataset. Nevertheless, asLightGBMsplits the trees leaf-wise, it can produce much more complex tree structures, resulting in overfitting in some cases.

SECTION: 5Performance of different Decision Tree based algorithms - a RPC SUSY case study at the HL-LHC

In this section, we study the effectiveness of various decision tree-based techniques for the search for direct pair production of charginos and neutralinos within the RPC MSSM scenario. To accomplish this, we consider a model where, ensuring that both theandexhibit wino-like characteristics and become mass degenerate, while the LSP (), remains predominantly bino-like. For such a scenario, the dominant production mode is, which gives rise to three lepton final state from subsequent decays as shown in Fig.19.

We explore both the scenarios where both winos promptly decay to either on-shell or off-shell as follows:

On-shellscenarios: the mass difference between the chargino () or the second lightest neutralino () and the lightest neutralino () exceeds theboson mass (), bothandwill undergo decays involving realandbosons, such asand.

Off-shellscenarios: In this case the mass difference betweenandis less thanboson mass () ,andwill undergo decay mediated by virtual bosons, such asand.
It may be noted that we have not considered the case where the mass gap is greater thanand less than.

Thesebosons decay into leptons with an equal branching fraction for each flavor (, and), resulting in a final state comprising precisely three light leptons ()111111For the rest of the analysis only electron and muon () will be classified as leptons () unless stated otherwise.and missing energy coming from two LSPs. The decay processes for both realand virtualare depicted in Fig.19121212For earlier works on electroweakino searches in the context of LHC, see Ref.Choudhury:2013jpa;Chakraborti:2014gea;Chakraborti:2015mra;Choudhury:2016lku;Barman:2020azo;Datta:2018lup.. Thisfinal state can originate from several Standard Model backgrounds such as,,,,, and. ATLAS has conducted a similar analysis using data from Run-I, Run-II, and high-luminosity LHC (HL-LHC). The current bounds on the electroweakinos from ATLAS Run-II results is 640 GeV for a masslessATLAS:2021moawhile the simulation by ATLAS indicated that future HL-LHC will be able to probe1.1 TeVin such scenario with a cut-based analysisTheATLAScollaboration:2014nwe.
Our aim is to study the sensitivity of wino searches using optimized cut-based and ML methods.
Signal events are simulated usingPythia6Sjostrand:2006za, while all background events are simulated usingMadGraph5-aMC@NLOAlwall:2014hcaat the leading order (LO). The NLO+NLL cross-sections for signals are calculated usingResummino-3.1.1Fuks:2013vua, and the NLO cross-sections for all backgrounds are generated byMadGraph5-aMC@NLOwhich are listed in two Tables7and8in the Appendix7. For this analysis, we consider a flatfactor = 1.4 for all the SM backgrounds and SUSY signals. For the detector simulation,Delphes-3.5.0deFavereau:2013fsais utilized. Event reconstruction employs the anti-algorithm with a jet radius of 0.4, imposing criteria ofandfor jet selection. Leptons are identified withandfor electrons (muons) respectively. For b-jets,is considered, with an 85% b-tagging efficiency and a 25% miss-tagging rate for light jets. Lepton-lepton isolation, lepton-jet isolation, etc, also have been implemented as described in the Refs.ATLAS:2021yyr;Choudhury:2023eje.

Initially, a cut-based analysis is conducted, followed by machine learning (ML) analysis using various DT algorithms. Then, we look for the improvement in signal significance achieved through ML algorithms, along with a comparison of results from different ML approaches. Additionally, the impact of various hyperparameters such as learning rate () and number of trees is discussed.
Furthermore, the significance of different kinematic features in discriminating signal and background events is illustrated using SHapley Additive ex-Planations (SHAP) values obtained through the SHAP packageShapley+1953+307+318;lundberg2018consistent;pedregosa2018scikitlearn.To do the further analysis, we choose four different benchmark points, allowed by LHC Run-II data, based on the mass difference between NLSP and LSP () with valuesand. The benchmark points are :Benchmark points(GeV)(GeV)(GeV)BP1400                            350                            50BP2500                            430                            70BP3800                            100                            700BP41200                            100                           1000Pre-selection CutsPre-selection cuts:We select the events with exactly three leptons, which must have separation among them with. Also, we choose the events with at least one same flavor opposite sign (SFOS) pair. We also impose that the events with no tau-jets and no b-tagged jets are selected for the analysis. After passing these selections with the remaining events, we do the cut-based analysis and ML analyses.

SECTION: 5.1Cut-and-count analysis

In this section, we investigate the search for wino-likeproduction at the HL-LHC with= 3000using the traditional cut-and-count method. We present the transverse momentum () distributions of the three chosen leptons, illustrated in upper panel of Fig.20, for three distinct benchmark points (BP1,BP2,BP3) corresponding to, and 700 GeV, alongside three prominent backgrounds:,, and. The leptondistribution for signal and backgrounds are quite similar for the compressed benchmark points and distinct in the cases with large.
We identify the same-flavor opposite-sign (SFOS) lepton pair with mass closest to theboson mass and exhibit the distribution of their invariant mass as, depicted in Fig.20(bottom left panel). Thedistribution shows that the benchmark pointshave a significant overlap withbackground while the compressed cases have peaked aroundand 70 GeV with a dominant overlap ofbackground. Subsequently, we proceed to reconstruct the variable transverse mass, such as.

denotes the remaining third lepton subsequent to selecting a lepton pair for the reconstruction of theboson, whilesignifies the angular disparity between the missing energy () and said third lepton. This parameter exploits the disparity between the distribution of the standard model background, characterized by a Jacobian peak sharply diminishing around.
Additionally, the distributions offor both signals and backgrounds demonstrate that signal benchmark points generally possess larger missing energy compared to the standard model backgrounds. We have also defined another parameter as the ratio of the scalar sum of three leptons and the missing energy (), which can discriminate the signal and backgrounds.

From the cut optimization, we have found that distinct sets of cuts become necessary for various benchmark points due to their varyingvalues. For benchmark points with substantial, cuts involving solely,,,, number of jets () andvariables prove adequate. Here, the same cut set is effective for both the benchmark points with. Three different signal regions are defined in the Table4asSR-A,SR-BandSR-Ccorresponding toGeV,GeV and& 1100 GeV. We provide the signal yield, individual background yields, and total background yield, along with the signal significance without uncertainty () and with 10% () and 20% uncertainty () in Table5. The signal significance without and with uncertainty are calculated using the formulas mentioned in the Equations.9and10(see section.2). The() values corresponding to BP1, BP2, BP3, and BP4 are 4.72 (0.19), 2.04 (0.07), 10.77 (7.98), and 1.59 (1.30), respectively. The cut-based method shows thatBP1,BP2andBP3are within reach of HL-LHC131313Due to the presence of huge background or smallerratio, the compressed scenario (BP1,BP2) will not be sensitive even we consider a 10% systematic uncertainty..

SECTION: 5.2Machine Learning based analysis

We will now conduct a similar analysis employing Machine Learning algorithms. For this purpose, we have compiled a dataset comprising events after the pre-selection cuts. In order to fully harness the benefits of utilizing a boosted decision tree approach over the cut-and-count method for distinguishing signal from background, it’s crucial to incorporate not just the primary kinematic variables such as the number of leptons (), number of jets (), and transverse momenta of leptons (), but also to generate additional derived variables like the mass of same-flavor opposite-sign (SFOS) pairs, the sum of transverse momenta () of leptons and jets, as well as variables such as effective mass. Therefore, we have developed 21 distinct features tailored for the specific analysis required in machine learning. These features variables are mentioned below :

The transverse momenta of the three chosen leptons, denoted as,, and, are represented by three variables.

The angular difference between each pair of the three leptons, denoted as,, and(three features), is measured. (3 features)

Azimuthal angular difference between leptons and missing energy represented aswhere i=1,2,3 (3 features)

No of jets,and missing energy,(2 features)

The SFOS pair number () and the mass of SFOS pair () with mass closest ofboson mass (2 features)

After selecting the SFOS pair, with the remaining lepton, we construct the transverse mass (), which is defined in Equation.19. Also, we calculate the invariant mass of three leptons (). Then we calculate the difference between the masses ofanddenoted as. (3 features)

Next, we compute the scalar sum of the transverse momenta of three leptons () and the scalar sum of transverse momenta of jets (). Additionally, we determine the sum of missing energy andas, and similarly for jets as. We introduce the effective mass variable, defined as, comprising a total of 5 features.

We incorporate the relevant weight into both the signal and background during the preparation of the data file. Subsequently, we proceed with the machine learning analysis using four distinct algorithms.

In this section we present the role of hyperparameters for the performances of different algorithms. In Sec.4, we have already summarized the mechanics of these algorithms. Each algorithm offers a range of hyperparameters for customization. For instance, with theXGBoostalgorithm, one can adjust parameters such as the learning rate (), the number of trees, the maximum depth of each tree (), the minimum weight required for node splitting (Min. child weight), a regularization parameter () to prevent overfitting, anparameter for further regularization via a penalty term within the loss function, and the subsample size of the dataset used for training, which is randomly selected etc..

Similarly, theLightGBMalgorithm shares many of these hyperparameters, alongside additional parameters like the number of leaves, which should be set to a value less thanto mitigate overfitting and enhance model accuracy.AdaBoostfollows a similar optimization strategy, with parameters such as the number of estimators, which corresponds to the number of trees. However, unlike other algorithms,Random Forestdoesn’t include a learning rate parameter in its optimization process.

We have estimated the best set of hyperparameters effective for each algorithm and mentioned in the Table.9(see Appendix.7). Upon determining the optimal hyperparameters for each algorithm, we investigate how varying the learning rate parameter affects the signal significance across three algorithms:AdaBoost,XGBoost, andLightGBM. We examine the range of theparameter from 0.0001 to 1.0 and depict the resulting variations in Fig.21(left). The plot clearly indicates thatAdaBoostdemonstrates peak performance around= 0.02-0.08,XGBoostperforms optimally within= 0.05-0.15, andLightGBMshows higher significance around= 0.01-0.1.

Additionally, we explore the impact of the number of trees, ranging from 1 to 5000, on the algorithms, as shown in Fig.21(middle), while keeping all other parameters constant. It becomes evident from the plot that forRandom ForestandXGBoostalgorithms, the signal significance reaches its maximum after approximately 300 trees, with minimal variation thereafter. However, forAdaBoostandLightGBMalgorithms, the signal significance declines rapidly after 100 and 500 trees, respectively.
Furthermore, we analyze the effect of the number of training events in the dataset on all algorithms, presented in Fig.21(right), to ensure that we have taken an adequate amount of data for the ML analysis. Upon reaching around 3training events, all algorithms achieve maximum significance, with marginal variation as the number of training events increases.

As discussed in Sec.5.2, for this analysis, more than 20 distinct features have been used to train the ML model and the impact of these features in discriminating the signal and background can be determined using SHAP packageShapley+1953+307+318;lundberg2018consistent;pedregosa2018scikitlearn. In the context of tree ensemble methods like gradient boosting methods or random forests, it is common to assign importance values to each input feature in order to comprehend the predictions. These values can be calculated for an individualized prediction or the global prediction of an entire dataset. Popular packages like XGBoostShapley+1953+307+318;lundberg2018consistent;pedregosa2018scikitlearnoffer implementations of tree ensembles that enable users to calculate a metric of feature importance. These metrics aim to condense the complexity of ensemble models and offer an understanding of the key features influencing the model’s predictions. Global feature importance is derived through three main methodologies gain, split count, permutation and Saabas method. However, most of these methods of calculating feature importance metrics are inconsistent and ineffective.However, SHapley values, which stem from principles of game theory, demonstrate the contribution of an individual player within a group.
As indicated in Ref.lundberg2018consistent, among various techniques mentioned above, SHapley values emerge as the most dependable measure for selecting feature importance.
This method of local feature attribution, founded on SHapley values, was pioneered by S. Lundberg and S. Lee, as outlined inlundberg2017unified.
The technique of calculating SHapley values involves training the model on various subsets of features, where each subset is a subset of the full feature set. It assigns an importance score to each feature, indicating its impact on model predictions when included. To calculate this impact, two models are trained: one with the feature in the model and another without it. The difference in predictions between these models, considering the current input, helps quantify the feature’s influence. Since the influence of a feature’s absence is influenced by other features in the model, these differences are computed across all possible feature subsets, excluding the feature under consideration. Subsequently, SHapley values are computed and utilized to attribute importance to each feature. The formula used for the calculation of SHapley value looks likelundberg2018consistent

In this context, N denotes the complete set of features and “i” represents the feature under consideration for SHapley value calculation. M signifies the overall count of features. S denotes a subset of N that excludes i. The functionrepresents the model’s prediction. Consequently, the algorithm evaluates a weighted sum of the variances in model predictions when including or excluding the ith feature across all potential combinations of the subset S.Various kinds of visualizations can be generated using SHapley values to illustrate how different features impact the model. SHAP summary plots utilize individualized feature attributions to encapsulate the significance of each feature effectively while maintaining visual simplicity. Initially, features are arranged based on their overall impact, represented by the sum of absolute SHapley values. Subsequently, the corresponding SHapley values are horizontally plotted, with stacking occurring vertically if space becomes insufficient. The SHAP summary plot (dotted) is shown in the Fig.22. Every point is shaded according to the corresponding feature’s value, ranging from a low (blue) hue to a high (red) one. Also, we consider the absolute SHapley values and show a bar-type summary plot where each color represents each class and the bar width shows the effect of each feature on classifying each class. In the left panel of the Fig.22, we have displayed the SHapley value distribution for the signal benchmark pointBP3only. The right panel of Fig.22represents the combined effect of the signal point and all SM backgrounds. In Fig.22, we plot only the first 15 feature variables according to importance. The variable () is the top most important feature, while thevariable is the next effective variable when the model tries to discriminate the signal and background (see the right panel of Fig.22.
and the combined effect of the signal and all the background (right). It is obvious that when we try to discriminate the signal and background, variablewill be very effective and that’s why it has more importance value in the bar plot (right) than the dotted plot (left). The other variables likeandshow the higher SHapley value, which is quite similar to the cut-based analysis.

After optimization of hyperparameters for each algorithm, we proceed to visualize the distribution of signal and background events corresponding to two signal benchmark pointsBP1andBP3, which represent two different scenarios like small(50 GeV) and large(700 GeV). The event distributions ofBP3(BP1) are displayed in Fig.23(Fig..25in the Appendix.7). Analysis of the distributions corresponding toBP3reveals that theXGBoostandLightGBMalgorithms outperformRandom ForestandAdaBoostin discriminating signal from background, as evidenced by the higher number of background events at larger probability cuts for the latter two algorithms. For smallalso, these two algorithms work better, but for all the algorithms, there is significant overlapping between signal and background which is evident from the Fig.25. In the Fig.20, the distribution of the kinematic variables also displays the same behavior about the significant overlapping.

We also represent the ROC curve for two different benchmark pointsBP1(left) andBP3(right) in the Figure.24. Here we can see thataucvalues are a little bit different for each algorithm corresponding toBP1whereas forBP3, theaucvalues are pretty close.

In the Table.6we provide the signal yield, total background yield, and signal significance without systematic uncertainty (), and with uncertainties of 10% () and 20% ().
For benchmark points with low(BP1abdBP2), there are significant overlap between signal and background events and the dominant contribution comes fromandbackgrounds. In these compressed scenarios, due to the smallratio (), we observe a sharp decrease in the significance when we consider the systematic uncertainties.
ForBP1andBP2, the signal significance becomes3-4 times larger as compared to the cut-and-count method. ForBP3andBP4, the signal significance increases by30-60% in ML method. Specifically, when considering signal significance with uncertainty, we can exclude the point where= 400 GeV for= 350 GeV. Similarly, we can also exclude= 1200 GeV for= 100 GeV, regardless of uncertainty. It should be noted that the performances ofXGBoostandLightGBMare almost similar andLightGBMgives a slightly better signal significance. The Table.6indicates that even with 20% systematic uncertainty chargino mass with 800 (1200) GeV will be withinandreach for a LSP mass of 100 GeV.

SECTION: 6Summary

Over the years, machine laerning techniques have significantly boosted efficiency and accuracy in HEP data analysis for both experimental and phenomenological works.
For the analyses of event triggering, jet tagging, particle identification,
event selection, object reconstruction, event classification etc., the HEP community has been widely using the boosted decision tree (BDT) algorithms for a long time.
We have briefly summarized the major important analyses performed by the ATLAS and CMS collaboration utilizing LHC data as well as the works of the phenomenological groups involving the use of ML algorithms with emphasis on DT-based approaches. We have also outlined the basic concepts of machine learning
along with different kinds of loss functions and their roles, issues of underfitting, overfitting, etc.
Evaluation of the effectiveness and accuracy of the ML models in solving a specific task, e.g., classifying signal and background events, can be done using
performance metrics. Different kinds of metrics relevant to particle physics analysis, like ROC curve, F-score, ams score, etc., are discussed in detail.

In this article, we have focused solely on a prominent machine learning
technique, namely Decision Tree (DT), specifically boosted decision tree, which is primarily used for tasks in supervised classification and regression.
We particularly explore four decision tree-based ML algorithms, namely,Random Forest,AdaBoostand two gradient boosting frameworks such asXGBoost, andLightGBM, in the context of Supersymmetry which is a well-promising candidate for beyond the Standard model framework.
We summarize the basic concepts and working principles of these four
algorithms along with the flowcharts.
In numerous analyses, both the ATLAS and CMS collaborations, HEP phenomenological groups have employed BDT algorithms to enhance the sensitivity of sparticle searches within the framework of RPC and RPV SUSY models. However, exploring the compressed SUSY parameter space still reamsins as a significant challenge.

Using an example of wino type electroweakino productions at the high
luminosity LHC, we demonstrate how these algorithms lead to improvement
in the search sensitivity compared to traditional cut-based methods in both
compressed and non-compressed R-Parity conserving SUSY scenarios.
The optimization of hyperparameters and its role in signal significance
are studied in detail for these four algorithms.
We have also discussed how to find out the individualized and global feature importance in ML methods usingSHAPpackage. We have found that there are30-60% gains in the significance for the signal benchmark points with a large mass gap between NLSP-LSP pair, whereas, for the compressed region (mass gap 50 and 70 GeV), the signal significance improves by3-4 times compared to the cut-based analysis. We have obtained that theLightGBMandXGBoostalgorithms perform better than the other two algorithmsRandom ForestandAdaBoost.
Although the deep neural networks (DNN) or deep learning (DL) techniques, which are based on multilayer NN, are gaining popularity, BDTs continue to be
highly relevant and important in High Energy Physics due to their user-friendly nature, interpretability, computational efficiency, and more.

Acknowledgments:Authors would like to thank Ritik Pal for fruitful suggestions. The work of A.C is supported by the SERB Core Research Grant CRG/2023/008570.

Data availability statement:No data associated in the manuscript.

SECTION: References

SECTION: 7Appendix