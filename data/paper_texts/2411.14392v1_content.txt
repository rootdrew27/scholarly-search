SECTION: Convolutional Vision Transformer for Cosmology Parameter Inference

Parameter inference is a crucial task in modern cosmology that requires accurate and fast computational methods to handle the high precision and volume of observational datasets. In this study, we explore a hybrid vision transformer, the Convolution vision Transformer (CvT), which combines the benefits of vision transformers and convolutional neural networks. We use this approach to infer theandcosmological parameters from simulated dark matter and halo fields. Our experiments indicate that the constraints onandobtained using CvT are better than the traditional vision transformer (ViT) and CNN, using either dark matter or halo fields. For CvT, pretraining on dark matter fields proves advantageous for improving constraints using halo fields compared to training a model from the beginning. However, ViT and CNN do not show these benefits. The CvT is more efficient than ViT since, despite having more parameters, it requires a training time similar to that of ViT and has similar inference times. The code is available athttps://github.com/Yash-10/cvt-cosmo-inference/.

SECTION: 1Introduction

An enthralling task in cosmology is accurately estimating the cosmological parameters that describe the Universe from observational data, i.e., cosmological parameter inference. The most widely accepted cosmological model, theCDM (Cold Dark Matter), describes the Universe using a few parameters:(the matter density, including normal and dark matter),(the dark energy density;, the cosmological constant, represents dark energy),(the Hubble parameter),(the spectral index of density perturbations),(the variance in the matter distribution smoothed over spheres of radius 8Mpc). The overwhelming amount of cosmological information from current and upcoming observational surveys[e.g.,12,10]will require sound statistical methodologies to achieve this goal.

Parameter inference aims to determine the posterior distributions of cosmological model parameters given a set of observations. Traditionally, this has been achieved by comparing the two-point correlation functions or power spectra of the tracers of large-scale structure (LSS) with theoretical predictions, or by using higher-order summary statistics that extract non-Gaussian information[e.g.,4,26]. Such methods have analytically tractable likelihoods. However, predefined summary statistics inevitably fail to fully capture the rich non-Gaussian information at non-linear scales, which makes them suboptimal. Recently, ‘field-level inference’[5]has gained a lot of attention as a potential alternative to these traditional techniques due to its ability to produce tighter constraints[see, e.g.,14,1,6,15]. In this case, the likelihood is untractable since cosmological parameters are directly derived from the full, non-linear distribution of matter fields. Field-level inference allows access to higher-order information (e.g., from the phases of the fields), which is otherwise inaccessible through conventional summary statistics. Neural networks are a promising solution for field-level inference due to their demonstrated capabilities to extract features from complex data efficiently.

Since neural networks use the entire non-linear (and thus noisy) distribution of matter, they must effectively extract informative multi-scale features that help link those features to the underlying cosmological model parameters. Convolutional Neural Networks (CNNs) have consistently excelled in various tasks, primarily due to their localization through convolutional kernels, translation invariance property, and learning features hierarchically (i.e., local features in earlier layers and increasingly global features in later layers as its receptive field enlarges). Consequently, CNNs have become a dominant choice for cosmological parameter inference[e.g.,19,20,13,24].

CNNs also have limitations because their receptive fields are constrained to grow larger as depth progresses, but that may not be necessary. The relatively newer Vision Transformers (ViTs)[7]do not take advantage of the strong inductive biases induced by convolutions in CNNs, allowing them to learn global spatial relationships through their self-attention layers even in the earlier layers of the model. ViTs break down an image into several patches, which are then flattened and considered as tokens, similar to terminology used in natural language processing. ViTs lack some biases, so they require large datasets for training, but given this constraint is satisfied, they have shown comparable or better performance than state-of-the-art CNNs such as ResNets. The applications of ViT for cosmological parameter inference are thus compelling but are at a nascent stage, with only a handful of studies exploring their value[8,9]and finding them competitive with CNNs. We hypothesize that combining the benefits of CNNs and ViTs may improve parameter inference by alleviating their individual deficiencies.

In this study, we perform likelihood-free inference to predict the marginal posterior mean and variance of the two cosmological parameters,and, from simulated dark matter and halo distribution using the publicly availableQUIJOTEsimulation suite as our dataset. We use a convolutional vision transformer (CvT) that combines the advantages of both CNNs and ViTs.

SECTION: 2Method

SECTION: Data

We use the-body simulations of the publicly-availableQUIJOTEsimulation suite[22]to obtain the DM density and halo catalogs; we use the friends-of-friends (FoF) halo catalogs. These simulations are performed with a box size ofGpc. We use standard Latin-hypercube simulations with massless neutrinos that containcold dark matter particles and use outputs at.
This simulation set contains 2000 simulation data, and we divide it into training, validation, and testing sets using an 80-10-10% split. Since the splitting is performed at the simulation level, all data from a simulation are either in the training, validation, or testing set; such a non-random splitting is crucial to prevent obvious bias in the results[21]. All 1600 DM data are used for pretraining, but only 200 Halo data are used for finetuning.
All simulations have different random seeds withvaried in [0.1, 0.5],varied in [0.6, 1.0], and the other astrophysical parameters (,,) varied within their appropriate ranges.

We project the particle and halo positions from the simulation snapshots onto agrid using the cloud-in-cell (CIC) scheme. All the halos detected inQUIJOTEare contained in halo maps, i.e., we do not apply any mass cuts. Ten random two-dimensional maps (each of thickness3.9Mpc) along each projection direction (X, Y, and Z) are selected, producing 30 maps from each simulation (these maps are individually used as inputs to the neural network). Overdensities are first calculated (), followed by a logarithm-like transformation given byto reduce the dynamic range of the pixel values, followed by standardization using the mean and standard deviation of the transformed fields from the training set. Each cosmological parameter is individually normalized tousing the corresponding minimum and maximum values calculated across the training set.

Fig.1shows an example comparison of the two-dimensional DM density and halo maps. It shows that the halos are biased tracers of the DM density as the former captures high local overdensities in the DM distribution, and thus leads to a visually sparser distribution.

SECTION: Approach

We use a Convolutional vision Transformer (CvT)[25], which uses a multi-stage hierarchical structure (see AppendixAfor visualization of the architecture), where each stage contains a convolutional token embedding layer followed by convolutional transformer blocks. The convolutional token embedding layers learn a convolution operation transforming input tokens into a new set of tokens. Its placement across different stages allows progressive spatial downsampling (i.e., reducing the number of tokens) together with increasing feature dimensions (i.e., increasing the width of tokens), and thus allows capturing local information as in CNNs. The convolutional transformer block uses a convolutional projection, implemented as a depth-wise separable convolution layer, instead of a linear projection used in traditional ViT. Ideologically, this transformer block generalizes the transformer in traditional ViT. Since local spatial relationships are modeled through the convolutional token embedding and projection, no positional encoding is required, which allows CvT to adapt to variable spatial resolution images. The use of efficient convolutions within the transformer in CvT also makes it computationally and memory-wise more efficient than traditional ViTs. The implementation is adapted from thevit-pytorchcode111https://github.com/lucidrains/vit-pytorch.
Specifically, we use the lightweight CvT-13 model, i.e., with a total of 13 transformer blocks containing 17.6M parameters and the default model hyperparameters as used in the original paper.

SECTION: Training details

We modify the CvT architecture to perform a regression task to predict the two cosmological parameters,and. Our model predicts the marginal posterior mean and variance forand. The loss function is designed under the framework of moment networks[11]and used previously in works such as[23]and[24], and is given by:

whereis the true value of parameterfrom simulationandandare the network prediction of the mean and standard deviation of the marginal posterior of parameter, respectively.
During training, the 2D maps are rotated randomly by 90, 180, or 270 degrees. A batch size of 16, Adam with decoupled weight decay optimizer (AdamW;Loshchilov and Hutter16) with weight decay ofand a learning rate ofis used. The learning rate is reduced by a factor of 0.3 if the validation loss does not improve for five epochs. Dropout is not used during training. Training is performed for 30 epochs in all experiments, and the model weights corresponding to the lowest validation loss are used for inference. Weights & Biases[3]was used to track training and evaluation. No specific hyperparameter tuning has been performed.

SECTION: 3Results

SECTION: Experimental details and evaluation

We pretrain CvT on DM fields and report the test results. We also show test results when the pretrained model is finetuned on halo fields and compare its performance against the model trained from scratch on halo fields. Before finetuning, we only reinitialized the fully connected MLP head. We chose not to freeze any weights, as doing so only provided marginally better constraints on: the RMSE was almost the same, with error bar increasing by approximately 0.014 reflecting the RMSE better, but constraints onwere significantly worse, with the RMSE increasing by about 0.014 and error bar underpredicted by about 0.03, than not freezing. The optimal pretrained model on DM data was found after 28k iterations, the optimal finetuned model on halo data after 2.4k iterations, and the optimal model trained from scratch on halo data after 1.4k iterations, although it had a higher validation loss than using transfer learning.

We used two metrics to evaluate the prediction for each cosmological parameter: the root mean squared error (RMSE), defined as, whereis the number of test examples, anddenotes the averaged errors.determines the accuracy of the predictions, anddenotes the 1error in the prediction of the parameter value.

SECTION: Prediction performance

Fig.2shows the predictions versus the true parameter values for DM pretraining (a), halo transfer learning (b), and halo training from scratch (c), from left to right in that order.predictions for (a) show excellent agreement with a near 1:1 relationship (RMSE = 0.005 and appropriately small error bars), whilepredictions are moderately good (RMSE = 0.059) and appropriate error bars. To put these values in context, we report the RMSE in the case of a constant prediction equal to the mean of the true value of the parameter; we obtain RMSE = 0.118 for both parameters.

Using the pretrained model from dark matter and transfer learning on halo fields (case b) gives slightly worse constraints forthan (a) (RMSE = 0.064 and underestimated error bars), but theconstraints are more prominently deteriorated (RMSE = 0.079 and slightly underestimated error bars). This deterioration is not unexpected, as halos are biased tracers of the underlying dark matter field, so they contain less pertinent information. It is currently elusive why the error bars forare severely underestimated, but the fact that this also happens for (c) suggests that this is not necessarily due to transfer learning but a characteristic feature when using halo fields. The error bars forare also underestimated, but this is less severe than.

For (b), it can be seen that largevalues tend to be underestimated, smallvalues are overestimated, whereas largevalues are underestimated. So, predictions near the edges are affected and there is tendency to regress towards the mean of the cosmological parameter set222Although we intend to talk about the mean of the ‘test’ parameter set here, we have checked the mean of the training parameter set is also similar which is because we randomly split the simulations.. The RMSE for constant prediction is 0.121 and 0.109 forand, respectively. Thus, this accuracy is still better than simply predicting the mean value. We do not have a clear explanation for these biases, but they may be due to insufficient expressiveness of the MLP head (since we only use a single-layered MLP) or due to overfitting (see[18]and[17], respectively).

The constraints in (c) are worse than in (b) as seen by the lower values ofRMSEand, and thus the transfer learning approach (DM pretraining followed by halo finetuning) seems more beneficial than training on halo data from scratch. This may be expected because the large-scale features in the DM and halo fields are similar, so the pretrained weights of the model trained on DM data serve as a better starting point to learn features from the halo fields.

SECTION: Comparison with traditional ViT

We compare the CvT (used in this work) with the simpler version of the traditional ViT architecture discussed inBeyer et al. [2], which we dub the ‘ViT’. We used a patch size of 8 for the ViT, but other common hyperparameters are the same as CvT. The results for (a), (b), and (c) forare as follows: RMSE = 0.066 and= 0.254, RMSE = 0.068 and= 0.299, RMSE = 0.074 and= 0.281. Thus, for (a) and (b), ViT is less accurate than CvT. For (c), the RMSEs are similar, but the error bar for CvT is more representative of the accuracy. For, the results for (a), (b), and (c) are: RMSE = 0.1 and= 0.24, RMSE = 0.106 and= 0.314, RMSE = 0.112 and= 0.247. However, the predictions are ‘near-flat’333The predicted parameters are visually similar irrespective of the true parameter value when visualized like in Fig.2.in all cases. Thus, CvT can constrain the cosmological parameters more tightly than ViT, especially.

SECTION: Comparison with CNN

The CNN architecture consists of five convolutional layers and batch normalization, followed by a fully connected layer that predicts the mean and standard deviation, just like the ViT, and other common hyperparameters are the same as CvT. The results for (a), (b), and (c) forare as follows: RMSE = 0.073 and= 0.086, RMSE = 0.21 and= 0, RMSE = 0.106 and= 0.139. CNN is less accurate than ViT and CvT, and also yields an overconfident prediction for (b) (= 0). For, the results for (a), (b), and (c) are: RMSE = 0.035 and= 0.075, RMSE = 0.151 and= 0, RMSE = 0.116 and= 0.137. CNN is better than ViT to predictfor (a), but is worse than both ViT and CvT for all other cases.

SECTION: Execution time

The ViT used in this work contains far fewer parameters (1.6M vs. 17.6M) but requires only marginally smaller training time than CvT (2.6 vs. 2.7 hours) for the DM pretraining. Thus, the operations in CvT are more efficient than the ViT, likely due to the introduction of convolutions and the convolutional projection operation[25]. At test time, CvT requires24 seconds, whereas ViT requires19 seconds for inference on 6000 maps (this experiment was performed when the model is trained on halo data from scratch). Although CvT is cumulatively slightly slower, the per-map inference times are almost the same.

SECTION: 4Conclusion

We have applied the convolution-based vision transformer (CvT) to infer theandcosmological parameters using data obtained from theQUIJOTEsimulation. We find that CvT constrains both parameters better than CNN and ViT when using dark matter and halo fields. CNN is found to be more beneficial than ViT only for inferringfrom dark matter fields, whereas ViT outperforms in all other cases. Pretraining CvT on dark matter fields has proven beneficial in better constraining the parameters when finetuned on halo fields rather than training a model from scratch on halo data, but these benefits are not apparent for CNN and ViT. One possible interpretation is that CvT is able to effectively leverage the large-scale structure similarities between dark matter and halo fields; however, more detailed tests are necessary to validate this finding. The demonstrated constraining power of CvT is noteworthy given that it was finetuned using 8lesser data than pretraining. As a result, it may be advantageous to apply CvT on data such as galaxy distribution, which require hydrodynamic simulations that are often computationally prohibitive. We also briefly demonstrate that CvT is more efficient than ViT due to the use of convolutions and has a similar inference time to it.

Some future aims of this work are to interpret CvT, apply it to real data and develop guidelines for observational cosmologists instructing the regions to look at in the data, and integrate it with data simulation approaches based on deep learning (i.e., emulators).

SECTION: Acknowledgments and Disclosure of Funding

This work was supported by JSPS KAKENHI Grant Number 23K03446.

SECTION: References

SECTION: Appendix AArchitecture of the convolutional vision transformer

Fig.3shows the architecture of the CvT network. The primary modifications in CvT compared to the traditional ViT are the presence of a convolutional token embedding layer, whose presence across multiple stages resembles the design of CNNs, and the presence of a convolutional projection instead of a linear projection.