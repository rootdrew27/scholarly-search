SECTION: 
SECTION: Introduction
Traditionally, the classification of SSS imagery has relied heavily on manual interpretation by experts, supplemented by conventional machine learning techniques that utilize hand-crafted features. These methods, while effective to a degree, are time-consuming and often fall short in capturing the complex, varied textures and structures present in underwater environments. The advent of Convolutional Neural Networks (CNNs) marked a significant advancement in this field, offering more robust feature extraction capabilities and automating the classification process to a large extent.

However, the introduction of Vision Transformers (ViTs) has opened new avenues for SSS imagery analysis. ViTs leverage the power of self-attention mechanisms to process images as sequences of patches, allowing for a more flexible and comprehensive understanding of the spatial hierarchies within the data. This approach holds promise for capturing the intricate details and global context of SSS images, potentially surpassing the performance of traditional CNNs in classifying complex underwater scenes. Unlike CNNs, which primarily focus on local features due to their convolutional nature, ViTs can process entire image patches at once, allowing them to better understand the broader context of a scene. The self-attention mechanism in ViTs may be of benefit for particular classification scenarios where seafloor bottom types such as rocky and ripple sand negatively impact CNNs in reporting false alarms when searching for man-made objects.

The objective of this paper is to rigorously compare the efficacy of ViTs with established CNN models for binary image classification tasks in SSS imagery. By conducting a comprehensive evaluation based on a variety of metrics, as well as considerations of computational efficiency, we aim to provide a detailed empirical assessment of each model architecture’s strengths and limitations to benefit future sonar machine learning research.

SECTION: Related Work
Vision Transformers have shown excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train. The main contribution of Dosovitskiy et al.was to adapt the concept of attention, common in natural language processing applications to images. They were not the first to try, however, instead of computing attention between every pairwise set of pixels as predecessors had done, Dosovitskiy et al.proposed computing the metric at two distinct spatial scales; first in complete 16x16 local blocks, then computing the metric between individual blocks at the scale of the whole image. This technique exceeded existing benchmarks on natural images from ImageNet, CIFAR, and on subsets of Pets and Flowers.

ViTs have also been shown to have superior classification performance in remote sensing images, which are more similar to SSS images in that they are downward looking and are absent in most deep learning image libraries. Multiple surveys in ViTs have been conducted recently, including a survey on transformers in remote sensing. Applications of ViTs in image classification tasks have exploited additional metadata sources using a multi-modal approach in medical imaging, remote sensing, and in SSS.

From the domain of underwater acoustics in automated target recognition tasks, a studyintroduces the Spectrogram Transformer Model (STM), a novel application of the Transformer architecture for recognizing underwater acoustic targets exhibiting superior performance over traditional CNNs. A different paper presents the Dual-Path Vision Transformer Network (DP-ViT), designed for accurate target detection in sonar imagery, including both forward-look and side-scan sonar.

SECTION: Methodology
SECTION: Model Architectures
In this study, multiple architectures are empirically evaluated against one another, namely two CNN architectures and two ViT architectures: ResNet, ConvNext, ViT, and SwinViT. Model sizes that are considered tiny(T), small(S), and base(B) are chosen, such as ViT-T or ConvNext-S, ignoring larger variants like large and huge. Every model was configured to operate on dual channel, low and high frequency, SSS imagery with 224x224 snippet sizes along with the last fully-connected layer changed to the number of target classes.

SECTION: Dataset
Data was collected using a dual frequency synthetic aperture sonar (SAS) from several different geographical locations with varying seafloor bottom types. Due to the nature of SAS producing large resolution imagery, snippets were extracted and resized to a common 224x224 size that either contained a man-made object or not. All datasets are perfectly balanced with positive and negative instances.

SECTION: Training
Each model is trained using PyTorch on a Nvidia A6000 graphics processing unit. Pretrained weights on ImageNet-1K were used to initialize each deep neural network. To reduce overfitting, each model is optimized against the loss produced from the validation dataset to alleviate overfitting to the training dataset.

Various data augmentations are used during training such as horizontal flip, +- 5 degree rotation, subtle affine, and zoom. No data augmentations are applied on the validation and test dataset as the objective is to evaluate on real imagery. Careful consideration has to be placed on what augmentations make the most sense for sonar imagery. For example, vertical flipping essentially reverses the direction of the sonar’s line of sight, which is not a realistic representation of how SAS data is acquired.

SECTION: Evaluation Metrics
SECTION: Classification Performance
The effectiveness of the models are measured with four widely applied evaluation criteria: f1-score, recall, precision, and accuracy. These metrics provide a comprehensive evaluation of the model’s performance, quantifying its ability to accurately identify true positives(TP) and its capacity to limit false positives(FP). Depending on the scenario, the cost of classifying a scene as a false negative(FN) or simply not classifying a scene correctly when it is actually a true negative (TN) may be quite costly.

Precision measures the models ability to return only relevant instances, calculating the fraction of instances correctly identified as objects out of all instances that the model classified as such.

Recall measures the model’s ability to identify all relevant instances, calculating the fraction of actual objects that the model correctly identified.

The F1-score is the harmonic mean of Precision and Recall, providing a balanced representation of these two metrics.

Accuracy measures the overall effectiveness of the model in capturing both positive and negative instances. However, in the case of SSS imagery, accuracy can be misleading in imbalanced scenarios where man-made objects are rare compared to the abundant background clutter such as rocks. Since all the datasets are balanced, this is not an issue.

SECTION: Computational Efficiency
Inference time is measured by averaging each model’s inference acrossruns. To ensure accurate measurements, inference time is only recorded when the data is loaded on the device, and the GPU warm-up process has been completed. The GPU warm-up involves running a few inference iterations to allow the GPU to reach a stable state and optimize its performance. Time is measured asynchronously, capturing the duration from the start to the end of the GPU computation using CUDA events.

Throughput measures the maximum amount of parallelization, indicating the number of instances that can be processed per second. It is calculated as:

Floating Point Operations per Second (FLOPS) quantifies the number of floating-point operations (addition, subtraction, multiplication, and division) performed in one second. This unit of measure is used to assess the computational cost or efficiency of the ViT. FLOPS are calculated using the python libraryby Meta AI Research.

The number of parameters, or weights, in a model is closely related to its capacity to learn. A larger number of weights generally requires a larger dataset for training and thus results in a larger model size. The model size has implications for memory requirements and the feasibility of deploying the ViT in resource-constrained environments. Only the learnable parameters, ones that require gradients, are counted.

SECTION: Discussion of Results
ViTs performed the best compared to popular CNN architectures in terms of binary image classification performance as seen in Table. Despite having better classification performance, deployment of such models often fall under constraint of memory especially on edge devices located in underwater vehicles. As a reminder, this study did not include other ViT variants such as large and huge which are impractical for real-time inference in modern underwater vehicles.

ViTs in general require more computational resources due to their higher parameter count. ResNet-101 is considered fairly steep in modern deep learning CNNs however when compared to ViT-B in Table, it has less than half the parameter count. It should also be noted that inference speed is generally slower for ViTs as well. Potential future directions would dive deeper into model optimization techniques such as quantization and pruning to further examine classification performance over computational efficiency trade-offs.

It is widely accepted that ViTs require more training data than CNNs to obtain good predictive performance. The requirement of larger training data for ViTs distill down to what are called inductive biases which encode assumptions or prior knowledge into the design of each machine learning algorithm. Inductive biases in CNNs are local connectivity, weight sharing, hierarchical processing, and spatial invariance. These inductive biases make machine learning algorithms computationally more feasible and/or exploit domain knowledge. Because CNNs have more inductive biases than ViTs, less training data is required to learn patterns or features in an input image.

SECTION: Conclusion
Final results revealed that ViT-based models outperformed classification performance over popular CNN models in diverse environmental settings captured with SSS. However, ViTs do have computational efficiency trade-offs that prohibit them from real-time deployment applications especially in limited resource environments such as underwater vehicles. CNNs still have a strong role to play in the practical deployment of image classification tasks using SSS due to their built-in inductive biases.

Future ViT research goals in SSS will examine the benefits of self-supervised learning with unlabeled data and multi-modal applications where additional underwater vehicle data is fused in order to inform predictive performance.

SECTION: References