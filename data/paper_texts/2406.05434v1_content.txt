SECTION: Unsupervised learning of Data-driven Facial Expression Coding System (DFECS) using keypoint tracking

The development of existing facial coding systems, such as the Facial Action Coding System (FACS), relied on manual examination of facial expression videos for defining Action Units (AUs). To overcome the labor-intensive nature of this process, we propose the unsupervised learning of an automated facial coding system by leveraging computer-vision-based facial keypoint tracking. In this novel facial coding system called the Data-driven Facial Expression Coding System (DFECS), the AUs are estimated by applying dimensionality reduction to facial keypoint movements from a neutral frame through a proposed Full Face Model (FFM). FFM employs a two-level decomposition using advanced dimensionality reduction techniques such as dictionary learning (DL) and non-negative matrix factorization (NMF). These techniques enhance the interpretability of AUs by introducing constraints such as sparsity and positivity to the encoding matrix. Results show that DFECS AUs estimated from the DISFA dataset can account for an average variance of up to 91.29 percent in test datasets (CK+ and BP4D-Spontaneous) and also surpass the variance explained by keypoint-based equivalents of FACS AUs in these datasets. Additionally, 87.5 percent of DFECS AUs are interpretable, i.e., align with the direction of facial muscle movements. In summary, advancements in automated facial coding systems can accelerate facial expression analysis across diverse fields such as security, healthcare, and entertainment. These advancements offer numerous benefits, including enhanced detection of abnormal behavior, improved pain analysis in healthcare settings, and enriched emotion-driven interactions. To facilitate further research, the code repository of DFECS has been made publicly accessible.

SECTION: 1Introduction

Interpersonal communication encompasses both verbal (speech) and non-verbal modes, such as facial expressions and body language.111Code available here:https://github.com/Shivansh-ct/DFECS-AUsAmong these non-verbal cues, facial expressions play a pivotal role in social interactions and emotional intelligence, serving as a window into the human mind. Examples of commonly observed facial expressions include smiles, frowns, wrinkled noses, and raised eyebrows. Additionally, facial expressions can reveal the genuine emotional state of a person, making them valuable for applications such as lie detectionporter2008reading;ekman2009lieand the examination of medical conditions like depressionhuang2021elderly. Moreover, they are essential for effectively conveying and understanding emotions in complex social interactionszhi2020comprehensive. The study of facial expressions extends across diverse disciplines, including psychology, neuroscience, sociology, human-computer interaction, healthcare, and affective computingekman1978facial;ekman2002facial;rosenberg2020face;tripathi2024consistent;zhi2020comprehensive.

Research indicates that our ability to interpret emotions from facial expressions relies on specific movements of facial muscleswehrle2000studying;matsumoto2008facial. These subtle variations in facial muscle movements play a crucial role in various studies on prototypical emotions such as happiness, sadness, surprise, anger, and moreekman2002facial. Additionally, they are instrumental in distinguishing between genuine and fake smiles, characterizing conditions like pain, depression, stroke, Parkinson’s, autism, and schizophreniahamm2011automated;ekman1988smiles;hess1994cues;zhi2020comprehensive;heilman1993emotional;blonder2005affective;schimmel2011quantitative;tripathi2023protocol;jin2020diagnosing;trevisan2018facial;kohler2010facial;williams2016updating. For example, the expression of happiness often involves the muscles pulling the lip corners diagonally upwards and creating wrinkles around the eyes, while disgust is characterized by muscles causing nose wrinklingekman2002facial. A genuine smile includes muscles that raise the cheeks towards the eyelids, producing wrinkles around the eyes known as crow’s feet, which is often absent in fake smilesekman1988smiles;hess1994cues. Furthermore, paralyzed or drooping facial muscles can serve as indicators of neurological disorders such as strokeheilman1993emotional;blonder2005affective;schimmel2011quantitative;tripathi2023protocol. In conditions like Parkinson’s disease, there is a significant reduction in facial expression movements, and local tremors in facial muscles may occurjin2020diagnosing. Variations in facial expressions between individuals with autism spectrum disorder and healthy subjects have been documentedtrevisan2018facial. Subjects with schizophrenia exhibit abnormal expressions and also report impaired perception of facial expressions of emotionhamm2011automated;kohler2010facial. Facial muscle movements also play a role in characterizing pain, aiding in pain detection, especially when patients are unable to communicate verballychen2018automated. Therefore, the analysis of subtle differences in facial muscle movements holds the potential for diverse applications in facial expression analysis.

Psychologists have developed facial coding systems to objectively describe facial muscle movements consistently across various studies on facial expressionszhi2020comprehensive. These systems define a set of atomic expressions which we refer to as the Action Units (AUs), that encode the observable movements of facial muscles rather than delving into the underlying meaning of the displayed expressionzhi2020comprehensive. For instance, a specific AU might represent the upward movement of the eyebrows or a particular motion of the lips, such as pulling them diagonally upward in a smile. These AUs can occur independently or in combinations, and a collection of AUs within a facial coding system can effectively capture a broad spectrum of facial expressions.

Existing facial coding systems include Facial Affect Scoring Technique (FAST)ekman1971facial, Maximally Discriminative Facial Movement Coding System (MAX)izard1979maximally, Monadic Phases Coding System (MP)izard1979maximally;matias1989comparison, Face Animation Parameters (FAP)ostermann2002face, Facial Expression Coding System (FACES)kring2007facial, Facial Action Coding System (FACS)ekman1978facial;ekman2002facial, Neonatal Facial Coding System (NFCS)peters2003neonatal, and Child Facial Coding System (CFACS)gilbert1999postoperative. These encoding systems have been designed for a variety of applications, for instance, the coding system FASTekman1971facialis dedicated to encode facial expressions representing six distinct emotions: happiness, anger, surprise, sadness, fear, and disgust. MAXizard1979maximallyand MPizard1979maximally;matias1989comparisonare designed to encode facial expressions related to infants’ affective behavior. FAPostermann2002faceis integrated into the MPEG-4 Face and Body Animation (FBA) standard, serving the purpose of generating facial expressions on virtual faces. FACESkring2007facial, on the other hand, focuses on valence, measuring the intensity of positive or negative emotions conveyed by a facial expression. FACSekman1978facial;ekman2002facialstands out as a comprehensive coding system capable of encoding various facial expressions, not limited to emotional expressions alone. Extensions of FACS, such as NFCSpeters2003neonataland CFACSgilbert1999postoperative, find applications in pain studies involving infants. Among all these systems, FACS is recognized as the most comprehensive coding system for facial expressions to datezhi2020comprehensive;ekman1978facial;ekman2002facial.

All of the existing coding systems, including FAST, MAX, MP, FAP, FACES, FACS, NFCS, and CFACS, were developed by manually analyzing facial expression videos. For instance, Ekman and Friesen dedicated years to closely examining recorded facial muscle movements in order to define the FACS AUsekman1976measuring. Given the labor-intensive nature of this manual approach, the creation of a comprehensive coding system requires a significant investment of time and effortekman1976measuring. Moreover, among the manually devised systems, even the most comprehensive one, FACS, covers the majority of visible expressions but not all of themekman1976measuring;ekman1978facial;ekman2002facial, suggesting the potential for discovering a facial coding system that is even more comprehensive.

With the advent of deep learning-based technologies for computer vision, a substantial amount of standardized facial expression videos are readily available. The face images in these videos have been annotated with (x, y) coordinates of selected keypoints representing prominent facial features such as eyes, eyebrows, lips, mouth, and jawline. The collection of these keypoints coordinates within a given face image is illustrated in Fig.2. The annotation of these keypoints can be achieved automatically using deep learning-based computer vision algorithmstrigeorgis2016mnemonic;peng2016recurrent;guo2018stabilizing;sun2019fab;liu2017two;nagae2020iterative;wu2021design;dong2018supervision;dong2020supervision.

The keypoint-labeled facial expression videos represented using keypoint motion vectors (KPMs) (see section2.2) can be encoded with a small number of basis vectors that are biologically interpretable. To our knowledge, the work of Tripathi et al.chandra2023pcamarks the first attempt to explore an unsupervised learning approach for deriving a low dimensional representation of KPMs using principal component analysis (PCA) on keypoint labeled videos. Using the principal components of the KPMs as novel Action Units, they proposed the discovery of a new facial coding system called PCA AUs.

The objectives of PCA AUs in their coding system were twofold. Firstly, it should represent any facial keypoint movements with high accuracy. Secondly, the system should maintain biological interpretability. The results of their study indicate that PCA AUs, derived from the keypoint-labeled DISFA datasetmavadati2012automatic;mavadati2013disfa, can effectively encode various facial expression samples across different datasets (CK+kanade2000comprehensive;lucey2010extendedand BP4D-Spontaneouszhang2013high;zhang2014bp4d) with an average reconstruction error of 7.17 percent. Furthermore, the coding ability demonstrated by PCA AUs is comparable to that of FACS AUs.

However, a notable drawback of PCA AUs is that half of the PCA AUs are non-interpretable. Additionally, the encoding matrix, which contains the linear weights for the linear combinations of PCA AUs to represent a given sample, includes both positive and negative values. This suggests that each PCA AU can exhibit movement in both positive and negative directions from a neutral position. This may not be biologically plausible if AUs are intended to comply with facial muscles that typically move unidirectionally from a neutral face. Furthermore, the dense nature of the encoding matrix implies that all PCA AUs are employed to represent the spatial movement of 68 facial keypoints in any given sample. This is unlikely to be biologically plausible because sometimes only a few of the facial parts move, suggesting that only a few AUs are expected to be present in specific cases.

In this work, we aim to enhance the interpretability of AUs in unsupervised automated coding systems by introducing sparsity and positivity constraints to the encoding matrix. To achieve this, we propose a Full Face Model (FFM) that employs a two-level decomposition using advanced dimensionality reduction techniques such as Dictionary Learning (DL)mairal2009onlineand Non-negative Matrix Factorization (NMF)fevotte2011algorithms. We present a novel coding system named Data-driven Facial Expression Coding System (DFECS) and derive its AUs by employing the Full Face Model (FFM) on the DISFA dataset. Our results indicate that, while both PCA AUs and DFECS AUs exhibit similar performance in terms of variance explained on test datasets (CK+ and BP4D-Spontaneous), the latter demonstrates a notable improvement in interpretability. Specifically, 87.5 percent of DFECS AUs are interpretable, representing a substantial enhancement compared to the 50 percent achieved by the PCA AUschandra2023pca. In the future, automated keypoint-based learning of facial coding systems with more accurate and stable tracking can pace up facial expression analysis. For example, in security, it might speed up abnormal behavior analysis for forensic investigations; in healthcare, it could aid in managing conditions like pain, depression, or schizophrenia. Similar to existing coding systemszhi2020comprehensive;chandra2023pca, DFECS may be a versatile tool with numerous applications in entertainment, marketing, education, animation, facial expression synthesis, emotion quantification, and robotics.

The following section outlines the preprocessing steps before generating facial keypoints and the estimation of DFECS AUs. In Section 3, we delve into the experimentation and present the results, and Section 4 provides a conclusion, including the limitations of our work and potential directions for future research.

SECTION: 2Unsupervised learning of DFECS

The essence of facial expressions within video frames can be extracted through tracking the facial keypoints (Fig.1). To eliminate the effect of geometric variabilities across multiple subjects, such as head movement, the difference in face sizes, or relative position of face parts on our model, we first standardize the keypoints as in Tripathi et al.chandra2023pca. The standardized keypoints are finally converted into Keypoint Motion (KPM) vectors representing the changes of facial keypoints from a neutral frame capturing the movements of facial expressions. Finally, we present our FFM model to estimate our DFECS AUs from the KPM dataset. The remainder of this section presents the details of the above steps.

SECTION: 2.1Geometric corrections

To prevent any potential bias or inaccuracies in our model due to geometric variabilities across different subjects—such as head movement, the difference in face sizes, or the relative position of face parts—we preprocess facial keypoints. The preprocessed keypoints are called standardized keypoints. The process of eliminating geometric variations involves three key steps, outlined below:

Facial Keypoints Frontalization:Head movements can obscure pure facial muscle movement in an image. For instance, to quantify a facial muscle movement at a keypoint position, the displacement of the keypoint cannot be used unless there is no head movement. Head movements must be eliminated to obtain accurate facial muscle movements. To achieve this, we employ the algorithm introduced by Vonikakis et al.9190989, which effectively aligns facial keypoints of all the video frames to a front-facing orientation, thereby eliminating keypoint movements due to head motion.

Affine Registration:Subsequently, we perform six-parameter affine registration (translation, rotation, scaling, and shearing)hartley2003multipleto standardize all keypoints into a common space, thereby mitigating variabilities related to face size and position. For this step, we select a specific set of six anchor facial keypoints to estimate the six geometric parameters of affine transformation within a face image. The remaining keypoints in the image are then registered based on these parameters. The chosen anchor keypoints remain nearly stationary on the face regardless of facial expression. Specifically, for the 68-keypoints template, these fixated keypoints are zero (0), sixteen (16), twenty-seven (27), thirty-three (33), thirty-nine (39), and forty-two (42), as shown in yellow color in Fig.2222Right image is modified form ofhttps://github.com/Fang-Haoshu/Halpe-FullBody/blob/master/docs/face.jpg.

Similarity Registration:Finally, we apply four-parameter similarity registration (translation, rotation, and scaling)hartley2003multipleindividually to face parts to further eliminate variabilities, such as the distance between the eye corners, the length of the nose and the eyebrows, among others. The anchor keypoints used to estimate the four geometric parameters are as follows: (42,45) keypoints for the left eyebrow and left eye, (36,39) for the right eyebrow and right eye, (27) for the nose, and (0,16) for the jawline (Fig.2). While similarity registration typically requires at least two point correspondenceshartley2003multiple, the nose, having only one fixated keypoint, is translated to the fixated keypoint without undergoing similarity registration. Moreover, since lips lack fixated keypoints, they are excluded from similarity registration.

SECTION: 2.2Keypoint Motion (KPM) vectors

We define a Keypoint Motion (KPM) vector as a representation of changes in facial keypoint positions from a neutral face. The dimension of this vector is two times the number of keypoints tracked in the videos, denoting the x and y direction movement for each keypoint. To generate KPM vectors, we calculate the difference between the coordinates (x, y) of the standardized neutral frame keypoints and the corresponding standardized coordinates of those keypoints in all other frames of the same subject (Fig3).333The tool used for facial image morphing “Pychubby” is available at:https://github.com/jankrepl/pychubby/Consequently, the facial expression at any frame in a video is captured by an r-dimensional vector (where r is twice the dimensions of the number of keypoints tracked), encoding the movements of the keypoints from the subject’s neutral frame. To compactly represent the data from all subjects, we utilize a matrix, whererepresents the (x, y) coordinate movements of thekeypoints from the neutral frame, andsignifies the sum of the total number of video frames for each subject in the dataset.

SECTION: 2.3Estimating DFECS AUs

To estimate DFECS AUs, we employ an unsupervised algorithm that aims to learn a small set of “basis vectors” from a large sample of KPM vectors, which can accurately represent any KPM sample. This task is similar to the classical framework of dimensionality reductionsorzano2014survey;van2009dimensionality;reddy2020analysis. The dimensionality reduction problem may be restated in terms of the matrix factorization problem as follows. Consider m-points (or KPM samples) in a r-dimensional space, represented as a matrix. Our objective is to represent these points in a k-dimensional space (generally), using a set ofbasis vectors inrepresented as a matrix. These-basis vectors (where) represent the data in a much more meaningful manner. Any pointmay thus be approximated as a linear combination of these basis vectors (,are the linear weights). This may be compactly written in matrix form as. The corresponding matrix factorization thus becomessubject to constraints on.is the component matrix, and its columns are called components.is the encoding matrix, and its columns are called encodings. The k basis vectors depict the movements of facial keypoints from a neutral facial expression. Like FACS AUs, which can capture any facial expression with only 26 units (excluding head movements), these basis vectors efficiently represent the entire dataset,.

The PCA algorithm emerges as a natural choice for generating low-dimensional representations of KPM vectorschandra2023pca. The PCA decomposition problem may be written assubject to, whereis adiagonal matrix and columns ofandare constrained to be orthonormal. Definingasandas, the PCA decomposition can also be represented as. However, the PCA-based estimation of AUs suffer from the problem of interpretability due to dense values in the principal components and encoding matrixchandra2023pca.

To improve the interpretability of the DFECS system, we propose a Full Face Model (FFM) to estimate DFECS AUs using advanced dimensionality reduction techniques such as dictionary learning (DL) and non-negative matrix factorization (NMF)mairal2009online;hoyer2004non;cichocki2009fast;fevotte2011algorithms. FFM performs a two-level decomposition to the data. We first create partitions of the input datarepresenting the seven face parts- left eyebrow, right eyebrow, left eye, right eye, nose, lips, and jawline. The first level decomposition is applied to these seven parts individually and are called Part Face Models (PFMs) and capture localized movements of the face parts. To capture the correlations of localized face part movements, the decompositions of the seven PFMs are aggregated and decomposed further and is called Hierarchial Face Model (HFM). For simplicity, we call the aggregated PFMs and HFMs decomposition together as Full Face Model (FFM) (Fig.1). Below, we present the details of both the decompositions and how they combine to represent our final FFM model.

This model applies decomposition individually to the seven face parts () - left eyebrow, right eyebrow, left eye, right eye, nose, lips, and jawline. Letdenote the number of dimensions incorresponding to the face part; for example, for the left eyebrow, 5 keypoints are tracked, leading to. Letdenote the submatrix ofcorresponding to the set of rows in the face part. For each face-part, the submatrixis decomposed as,(where) using Dictionary Learning (DL) algorithmmairal2009onlinewhich utilizes the positive LASSO objective given by:

The basis vectors inare expected to represent facial muscle movements, which typically move unidirectionally from a neutral face. Therefore, the DL applies constraints that ensure positive coefficients infor the unidirectional movement of basis vectors from a neutral face. Further, the DL intends to enforce sparsity in the part-face encodingsto capture finer muscle movements; for example, in an eyebrow movement, sometimes, only the inner part is pulled upwards, or only the outer part instead of the full eyebrow.

For any face part(= 1, 2, … 7), PFMs are represented ashavingcomponents in its component matrix (). Each of the p-dimensional PFM component inis expanded to a full face (facial keypoints) template, i.e., into r-dimensional vector (by adding a zero to keypoints not present in the corresponding face part). Thecorresponding to the seven face parts are then stacked together in the matrix, where. Similarly,are stacked vertically in a matrix. Then, the PFM decomposition can be compactly represented as.

In reality, the movement of keypoints across different face parts are expected to be correlated (e.g., movement of the left eye is expected to accompany movement of the left eyebrow). To capture correlations of the keypoint movements in different face parts, we apply a 2nd level decomposition called the Hierarchical Face Model (HFM). HFM decomposition is applied on the matrix, such that,(where) using modified NMF algorithmhoyer2004non;cichocki2009fast;fevotte2011algorithmswhich solves the objective:

The positivity ofensures positively correlated movements of facial parts; for instance, in a happy expression, lip corner pulling often accompanies cheek raising and not lowering. The positivity ofensures the unidirectional movement of facial keypoints from a neutral position in the correlated part face components. L1 regularization onenforces sparsity intended to capture correlated face part components in which only few face parts are moving, for example, a mild smile may only involve the movement of brows, eyes and lips. Similarly, L1 regularization onis enforced to allow the possibility of generating sparse codings where only a few parts are moving.

The final model termed FFM can be written as.represents the part face components andcaptures the correlations between them. Therefore, our final DFECS AUs estimated by the FFM model are compactly represented by, andrepresents the encoding matrix.

In this section, we provide the details of hyperparameter tuning for both PFM and HFM to train an FFM model. The model learning aims to achieve a specific givenpercent of variance explained in the final FFM model. Note that FFM performs dimensionality reduction using matrix decomposition twice, first for PFM and then for HFM. Each decomposition introduces some error due to reduced dimensions. Consequently, the PFM decomposition aims to attain an explained variance higher thanso that the subsequent error, resulting from reduced dimensions during the HFM, can reach a variance explained percentage of.

For the PFM represented as, two hyperparameters are tuned: the number of face part componentsand the sparsity controller ()(Eq.1) of the encoding matrix. Training is halted when the percent variance explained reaches. Grid search is conducted for each PFM, where rows signify the variation offromto(wheredenotes the number of dimensions in the face part), and columns signify the variation of the sparsity constraint fromtowith a decrement step of.

For the HFM, which utilizes the NMF algorithm, three hyperparameters are tuned: the number of components, the sparsity controller of components, and the sparsity controller of the encoding matrix(Eq.2). NMF training employs a three-dimensional grid search, where the first dimension coversto(, is the total number of face part components across all face parts), the second dimension variesfromtowith a decrement step of 0.5, and the third dimension variesfromtowith a decrement step of 0.5. The training of HFM () concludes when the percent variance explained (betweenand, refer section2.3.2and3.4) reaches.

SECTION: 3Evaluation methodology

The FFM learns a decomposition of the input KPMs (represented as matrix) as, where. The columns of matrixrepresent the different DFECS AUs learned by the FFM. To compare this AU representation with FACS, one needs to (a) generate corresponding component matrixfor FACS AUs, (b) The encoding matrix, such that. The next subsection describes the methods used to generate the component matrix for FACS AUs and the encoding matrix to compare DFECS AUs, FACS AUs, and PCA AUs. We further outline the datasets and the evaluation metrics used.

SECTION: 3.1Computing component matrix for FACS AUs

We first categorize FACS AUs into two subsets: pure AUs, representing pure atomic expressions, and comb AUs, encompassing both pure AUs and some of their combinations mentioned in the FACS manualekman1978facial;ekman2002facial. To isolate pure facial muscle movements, we identify 26 pure AUs and 113 comb AUs from the AUs specified in the FACS manual, excluding any AUs related to head movement. Next, we focus on a single subject and extract images for the expression (apex) frame of each pure AU and comb AU from the FACS manualekman2002facial. Additionally, a neutral image of the same subject is selected. Using a keypoint tracking algorithmdong2020supervision, we generate 68 facial keypoints on each image. These keypoints then undergo the same preprocessing steps outlined in sections2.1and2.2, resulting in a standardized representation of the 136-dimensional KPM vector for each AU. The compiled KPM vectors for the 26 pure AUs are denoted in a matrix, while those for the 113 comb AUs in a matrix.

SECTION: 3.2Encoding matrix computation

Given a KPM dataset, represented as a matrixand an action unit (DFECS AUs, PCA AUs or FACS AUs) represented using a matrix, our goal is to compute the encoding matrixsuch thatapproximates(). For the DFECS AUs the encoding matrix is constrained to be non-negative. This is obtained by solving the following optimization problem:

s.t.for alland. The parameterand the L1-norm penalty are used in the objective to promote and control sparsity in the encoding matrix. The sparsity ensures the presence of fewer AUs in the coding of a sample where only a few facial muscles are moving. The above problem can be decomposed intoindependent identical problems on different KPM samples as follows:

The above problem may be solved using positive lassoefron2004least, which also gives the complete solution path with different solutions for different values of.

Note that there was no constraint on the encoding matrix for PCA AUs in Tripathi et al.chandra2023pca, which contained both positive and negative values. Therefore, PCA AUs were allowed to move in positive as well as negative directions. For a fair comparison with DFECS AUs, we must also learn positive encodings for PCA AUs. However, upon constraining the encoding matrix of PCA AUs to be positive values only, we need to include the negative of the components in the PCA AUs to represent the original set. Therefore, the component matrixof PCA AUs is updated as.

SECTION: 3.3Evaluation datasets

To evaluate the performance of our FFM model, we use three datasets: CK+kanade2000comprehensive;lucey2010extended, DISFAmavadati2012automatic;mavadati2013disfa, and BP4D-Spontaneouszhang2013high;zhang2014bp4ddescribed next.

CK+:This dataset comprises 593 posed facial video recordings of 123 subjects (aged 18-50) recorded at 30 frames per second (FPS). It captures facial expressions displaying emotions such as anger, contempt, disgust, fear, happiness, sadness, and surprise. The dataset contains around 10,100 video frames, resulting in a data matrixof size.

DISFA:In this dataset, there are spontaneous facial expression videos of 27 subjects (aged 18-50) recorded at 20 FPS. These expressions are elicited using a 4:01 minute emotionally evocative video targeting five distinct emotions: disgust, fear, happiness, sadness, and surprise. The dataset comprises 1,14,000 video frames, generating the matrixof size. To ensure a fair comparison with CK+, we randomly sample 10,100 KPMs from DISFA for training, denoting it as DISFA (train).

BP4D-Spontaneous:This dataset features spontaneous facial expression videos of 41 subjects (aged 18-29) recorded at 25 FPS. Expressions are elicited through eight different activities, including interview, watching a video clip, startle probe, improvisation, threat, cold pressor, insult and smelling. A total of 1,36,000 frames in this dataset results in a data matrixof size. For a fair comparison with training on CK+ and DISFA, we randomly sample 10,100 KPMs from BP4D and designate it as BP4D (train). Note that, in the case of BP4D-Spontaneous, all the keypoints on the jawline are missing (Fig.2). Therefore, the affine registration step when converting the tracked facial keypoints to standardized keypoints utilizes the following anchor points: twenty-seven (27), thirty-three (33), thirty-nine (39), forty-two (42), thirty-six (36), and forty-five (45) were used.

These datasets come pre-labeled with facial keypoints. These keypoints were tracked using different computer vision techniques: Active Appearance Model (AAM)cootes2001activefor DISFA (66 keypoints), Active Appearance Model (AAM)cootes2001active;matthews2004activefor CK+ (68 keypoints), and the Constrained Local Model (CLM)saragih2009deformable;saragih2011deformablefor BP4D-Spontaneous (49 keypoints). To ensure uniform representation, all datasets are vectorized into 68 keypoint templates, with missing keypoints being filled with zero values. Further, incorporating spontaneous datasets like DISFA and BP4D-Spontaneous enables us to capture a wide range of facial expressions, enhancing the model’s ability to generalize. Additionally, we include the dataset CK+ containing posed expressions and spontaneous smiles to investigate potential improvements in the final FFM model.

SECTION: 3.4Evaluation metrics

To assess the performance of our FFM model, we adopt and modify some of the metrics proposed by Tripathi et al.chandra2023pca, which were originally designed for evaluating the PCA model performance. These have been presented below.

Train Variance Explained (VE (train)) -Consider a model trained on the dataset, where. VE (train) measures how well the DFECS AUs reconstruct the training data and is defined as the percentage of variance explained, calculated as, whereis the low-dimensional representation ofgiven by.

Test Variance Explained (VE (test)) -Given a test datasetand AU component matrix, we compute its encoding matrix (section3.2) such that. VE (test) assesses how accurately the AU matrix () reconstructs the test data. It is defined as the percentage of variance explained, computed as.

Interpretability -Theinterpretabilitymetric evaluates the degree of biologically plausible movements within the DFECS AUs. Firstly, all DFECS AUs are visually depicted by projecting them onto a neutral facechandra2023pca. This involves selecting facial keypoints from a neutral image and then adding a 136-dimensional KPM vector representing a DFECS AU to these keypoints, resulting in an expressive frame. Facial components are then morphed to align with the expression’s keypoints, yielding a projected AU image (see SI Fig. 1). Three independent volunteers then examine each keypoint movement in any DFECS AU projected image by comparing it with the feasible directions of muscle movements depicted in Fig.4at that keypoint location. If any keypoint movement within any AU deviates from the possible movements of facial muscles in that facial area, the volunteer labels it as a non-interpretable AU. Finally, we apply majority voting, where a DFECS AU is considered non-interpretable if at least two of the volunteers label it as non-interpretable. Letdenote the number of non-interpretable AUs from the totalAUs in a DFECS model. Then, theinterpretabilitymetric is defined as.

SECTION: 4Results

In this section, we first compare the DFECS AUs generated by the three training datasets (DISFA (train), BP4D (train), and CK+) for their coding power and interpretability using the evaluation metrics. We finally compare the DFECS AUs with PCA AUs and the component matrix of FACS AUs.

We first compare the train and test performance of DFECS AUs on different training and testing datasets. For this, an FFM model was first learned on a train dataset, then the encoding matrices for train and different test datasets were computed (section3.2). The hyperparameters for the final FFM model that achieved a Var (train) of 95 percent when trained on the DISFA (train), BP4D (train), and CK+ datasets can be found in the SI Table. 1 and SI Table. 2.

For a given training dataset, and the value, representing the number of AUs utilized by a sample, we compute the mean of VE (train) over all samples and the mean VE (test) across all samples for the test datasets. Subsequently, for a given train dataset, at each, we take the average VE (test) over the remaining two test datasets.
The results are shown in Fig.5for the datasets- DISFA, CK+, and BP4D-Spontaneous. It’s important to note that, on the x-axis of the graph, each corresponding VE (train) and VE (test) for a givensignifies the variance explained withor fewer thancomponents. This is due to the fact that not every sample can utilize up toAUs, owing to the constraints imposed by positive encodings within the subspace spanned by the AUs.

The average VE (test) across the three datasets remains comparable when a small number of AUs is utilized (). However, asymptotically, DFECS AUs estimated from DISFA exhibit the best average VE (test).
Consequently, we chose the FFM components from DISFA as the final DFECS AUs, comprising sixteen components in the component matrixand having Var (train) of 95 percent.

We further inspect theinterpretabilitymetric for the sixteen components of DFECS AUs. These components represented as KPM vectors, are visualized by projecting them onto a neutral face (see Fig.6). The labeling of these AUs into interpretable or non-interpretable categories by the three volunteers (see section3.4) is shown in Table1. The majority vote reveals that Component 6 and Component 9 are non-interpretable; therefore, 14 out of the 16 components are fully interpretable, or theinterpretabilitymetric is 87.5 percent. For demonstration purposes, the non-interpretable keypoint movements coded by volunteer 1 are depicted as blue dots in Fig.6and detailed in SI Table. 4.

SECTION: 4.1Comparison of PCA AUs, DFECS AUs, and FACS AUs

For a given test dataand AUs in the component matrix, we compute the encoding matrix (see section3.2) such that. Further, at eachrepresenting the number of AUs, whereranges from 1 to 136, we calculate the mean of the VE (test) across all samples. The results for both CK+ and BP4D-Spontaneous test datasets are shown in Fig.7.

In both the BP4D-Spontaneous and CK+ datasets, PCA AUs outperform DFECS AUs and FACS AUs (pure AUs and comb AUs) when a small number of AUs are utilized (). Asymptotically, DFECS AUs exhibit the best performance. Additionally, we assess the performance with respect to the L1-norm of the encoding matrix forto, computing the mean of the VE (test) across all samples at each L1 value (SI Fig. 2). The results reveal that PCA AUs and DFECS AUs demonstrate comparable performance, and consistently outperform pure AUs and comb AUs.

DFECS AUs demonstrate a significant improvement in interpretability compared to the top eight PCA AUs, which only achieved 50 percentinterpretabilityin Tripathi et al.chandra2023pca. However, since the encoding matrix of PCA includes both positive and negative values, while our FFM’s encoding matrix comprises only positive values, we analyze the new component matrix for PCA AUs (see section3.2) for interpretability for a fair comparison. This results in PCA AUs with sixteen components.

We examined theinterpretabilitymetric of sixteen components of PCA AUs, that account for 95 percent VE (train). The interpretability labeling by the three volunteers and their majority vote is presented in Table2. Component 2 (-ve), Component 5 (-ve), Component 6 (+ve), Component 6 (-ve), Component 7 (+ve), and Component 8 (+ve) of PCA AUs are non-interpretable. Consequently, 10 out of 16 PCA AUs are fully interpretable, or theinterpretabilitymetric is 62.5 percent. For demonstration purposes, the non-interpretable keypoints movements identified by volunteer 1 are indicated by blue dots in Fig.8and detailed in SI Table. 3.

Recall that KPM representation of FACS AUs, being directly derived from human facial expressions, have 100 percentinterpretability. Overall, we find that DFECS AUs exhibit superior performance in explaining variance compared to FACS AUs. However, they are less interpretable compared to KPM-based FACS AUs. Notably, theinterpretabilityof DFECS AUs stands at 87.5 percent, a significant improvement over the 62.5 percentinterpretabilityof PCA AUs, while having variance explained comparable to that of the PCA AUs.

SECTION: 5Conclusion

In this work, we propose an automated approach to discovering a novel facial coding system DFECS using advanced algorithms (NMF, DL), and estimate its AUs from facial expression videos labeled with keypoints. The DFECS AUs estimated with the DISFA dataset demonstrate comparable variance to PCA AUs and superior variance compared to KPM-based FACS AUs in the BP4D-Spontaneous and CK+ test datasets. Notably, 87.5 percent of DFECS AUs are interpretable, a substantial improvement over the 62.5 percent interpretability achieved with a PCA model in the previous workchandra2023pca. Furthermore, the utilization of positive encodings enhances the interpretation of AUs as facial muscle movements, which typically move unidirectionally from a neutral face.

The applications of an automated data-driven facial coding system such as DFECS can be extensive and impactful across various domains. For instance, in the realm of security, such a system can pace up abnormal behavior analysis of facial and gesture behaviors, aiding in forensic investigations. In clinical settings, the system’s capabilities in automatic pain analysis and long-term monitoring can offer valuable tools for medical professionals, such as in mental state research related to depression, schizophrenia, and other psychosomatic disorders. Entertainment experiences can be greatly enriched through affect-based interaction, tailoring evoked expressions to users’ emotions such as boredom, depression, and happiness. Overall, the automated facial coding system presents a new tool with diverse applications in security, healthcare, entertainment, commerce, education, and robotics, bringing innovation and efficiency to various real-world scenarios.

However, certain limitations of the DFECS AUs should be acknowledged. Firstly, we have utilized only 68 facial keypoints, which may limit the system’s ability to capture subtle expressions such as wrinkles and microexpressions. Future improvements could involve incorporating more keypointswang2021deep;liu2019grand, thereby enabling the system to capture a broader range of subtle expressions. Moreover, future efforts should focus on refining the accuracy of keypoint-tracking algorithms, as precise tracking is crucial for capturing subtle expressions such as microexpressions.

Secondly, our interpretability analysis maybe limited because facial expressions are complex and can allow for large number of possible movements, therefore, robust metrics for gauging interpretability can be developed.
Finally, comparing DFECS AUs with KPM-based FACS AUs may not be the best method. Original FACS offers a qualitative, thorough description of facial muscle movements, allowing to code AUs universally in any expression. Hence, KPM-based FACS AUs were expected to have high coding power. However, they demonstrate lower coding power than DFECS AUs in terms of variance explained in datasets like CK+ and BP4D-Spontaneous. Creating KPM-based FACS AUs using responses from multiple subjects and comparing them with DFECS AUs may be a topic for future exploration.

SECTION: 6Funding

This work was supported by the project “Machine Learning Model for Early Diagnosis of Stroke in Resource Limited Settings” funded by the Department of Biotechnology, Government of India. Grant No.: BT/PR33179/AI/133/16/2019

SECTION: References

SECTION: 7Supplementary material

SECTION: 7.1Generation of a projected AU image

A neutral image, along with its 136-dimensional facial keypoints (comprising 68 x and y coordinates), is chosen. Next, a DFECS AU represented as a KPM vector is added to the 136-dimensional keypoints of the neutral image, resulting in projected AU keypoints. These keypoints are then utilized to morph the neutral image into the projected AU image. The below figure shows a projected AU image. The KPM image in the figure below is an equivalence of the projected AU image with neutral keypoints and direction of movement marked on it.

SECTION: 7.2Comparing AUs with increasing L1 norm

Comparing different Action Units (FACS AUs, PCA AUs, and DFECS AUs) for their performance, i.e., variance explained on the test datasets CK+ and BP4D-Spontaneous with increasing L1 norm from L1=0 to L1=1000.

SECTION: 7.3Final NMF and DL hyperparameters for DFECS AUs

Hyperparameters for the final FFM model trained on the DISFA dataset.

SECTION: 7.4Artifacts in projected AUs

Description of the artifacts labeled by volunteer 1 in the projected AU images of PCA AUs and DFECS AUs by comparing the keypoint movements with the direction of facial muscle movements in SI Fig.3. Blue dots represent non-interpretable keypoint movements called artifacts in any projected AU image (see more about projected AUs in SI section7.1).