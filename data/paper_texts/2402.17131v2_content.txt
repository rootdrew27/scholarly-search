SECTION: Predicting O-GlcNAcylation Sites in Mammalian Proteins with Transformers and RNNs Trained with a New Loss Function

Glycosylation, a protein modification, has multiple essential functional and structural roles. O-GlcNAcylation, a subtype of glycosylation, has the potential to be an important target for therapeutics, but methods to reliably predict O-GlcNAcylation sites had not been available until 2023; a 2021 review correctly noted that published models were insufficient and failed to generalize. Moreover, many are no longer usable. In 2023, a considerably better recurrent neural network (RNN) model with an F1score of 36.17% and a Matthews Correlation Coefficient (MCC) of 34.57% on a large dataset was published. This article first sought to improve these metrics using transformer encoders. While transformers displayed high performance on this dataset, their performance was inferior to that of the previously published RNN. We then created a new loss function, which we call the weighted focal differentiable MCC, to improve the performance of classification models. RNN models trained with this new function display superior performance to models trained using the weighted cross-entropy loss; this new function can also be used to fine-tune trained models. A two-cell RNN trained with this loss achieves state-of-the-art performance in O-GlcNAcylation site prediction with an F1score of 38.88% and an MCC of 38.20% on that same large dataset.

SECTION: 1Introduction

Glycosylation, a co- and post-translational modification, occurs when glycan(s) are added to proteins. O-linked glycosylation occurs when a glycan is added to an oxygen of an amino acid (usually serine or threonine in mammals). O-GlcNAcylation, a subtype of O-linked glycosylation, occurs when the first glycan added is an N-Acetylglucosamine (abbreviated GlcNAc)(Schjoldager et al.,2020). Unlike other forms of glycosylation, O-GlcNAcylation may be viewed similarly to phosphorylation, for no other glycans are further added to a site after GlcNAc and the process is catalyzed solely by two enzymes, OGT and OGA. O-GlcNAcylation is important functionally and structurally(Schjoldager et al.,2020; Chang et al.,2020); conversely, incorrect O-GlcNAcylation or its improper removal is associated with multiple diseases such as cancers(Shi et al.,2022), infections(Chang et al.,2020), and heart failure(Umapathi et al.,2021). According to recent research, O-GlcNAcylation can be a powerful target for therapeutics(Zhu & Hart,2021), further emphasizing its relevance.

While O-GlcNAcylation is essential for human health and very important for biotherapeutics, challenges still exist. Because O-GlcNAcylation is complete after the addition of a single glycan, the problem of predicting glycan distributions in a glycosylation site does not exist; thus, the main predictive task is to determine where and when (if ever) an amino acid will be O-GlcNAcylated. As summarized bySeber & Braatz (2024), this task is challenging for “multiple reasons”, which include “a low frequency of events (only about 2% of S/T sites are O-GlcNAcylated) and a lack of a motif to guide predictive efforts”. Moreover,Seber & Braatz (2024)also note that “the effects of neighboring amino acids likely influence whether an S/T is O-GlcNAcylated”, a fact corroborated by their interpretability studies done with their final model, but many previous works use model architectures that cannot directly take into account any type of sequential information. The pioneer classifier machine-learning model for this challenging task is YinOYang (YoY)(Gupta & Brunak,2002), which also had one of the highest performances until the work ofSeber & Braatz (2024), first published in 2023. However, until at least 2021, O-GlcNAcylation site prediction models had insufficient performance to help advance research in the area(Mauri et al.,2021). Specifically,Mauri et al. (2021)found that no published model until then could achieve a precision9% on a medium-sized independent dataset, suggesting that models to predict the location of O-GlcNAcylation sites failed to generalize successfully in spite of the high metrics achieved with their training data. The models evaluated byMauri et al. (2021)also have low F1scores and Matthew Correlation Coefficients (MCCs), another sign that their performance is insufficient.Seber & Braatz (2024)used recurrent neural network (RNN) models (specifically, long short-term memory [LSTM] models) to predict the presence of O-GlcNAc sites from mammalian protein sequence data. That work used two different datasets (fromMauri et al. (2021); Wulff-Fuentes et al. (2021)), the latter of which is among the largest O-GlcNAcylation datasets available. These RNNs obtained vastly superior performance to the previously published models, achieving an F1score more than 3.5-fold higher and an MCC more than 4.5-fold higher than the previous state of the art. Furthermore, Shapley values were used to interpret the predictions of that RNN model through the sum of simple linear coefficients(Shapley,1951). These predictions with Shapley values maintained most of the performance of the original RNN models (Supplemental data ofSeber & Braatz (2024)). Conversely, two other models that were also released recently, LM-OGlcNAc-Site(Pokharel et al.,2023)and O-GlcNAcPRED-DL(Hu et al.,2024), failed to obtain high metrics in independent test sets despite their complex architectures (an ensemble of transformers pre-trained for proteins in LM-OGlcNAc-Site’s case, and a CNN+LSTM model in O-GlcNAcPRED-DL’s case) and the direct use of some structural information. These two models still cannot reach precision9% on independent test sets, and an analysis bySeber & Braatz (2024)(Table 4 of that work) noted that their model had a 2-fold performance improvement over O-GlcNAcPRED-DL, despite the fact that the latter was operating with a few advantages.

While the work ofSeber & Braatz (2024)was a significant improvement over previous models and brought interpretability for the first time in a publication on data-driven O-GlcNAcylation site prediction models, the test-set F1score and MCC achieved by the best RNN ofSeber & Braatz (2024)was 36.17% and 34.57%. Thus, there is room for improvement, and we believed the transformer architecture could lead to a better model, but this hypothesis proved false, as LSTMs outperform transformers. This work also develops and explores using a novel loss function that aims to directly improve the models’ MCCs, and compares the superior results attained with new loss function against results obtained with the weighted cross-entropy loss function. 5-fold cross-validation is employed in the model training, ensuring the reliability of reported metrics and predictions. The improved model generated by this study is provided as open-source software, allowing the reproducibility of the work, the retraining of the models as additional or higher quality O-GlcNAcylation data become available, and the use of the models to further improve the understanding and applications of O-GlcNAcylation.

SECTION: 2Materials and Methods

SECTION: 2.1Dataset

The data used to train the model was first generated byWulff-Fuentes et al. (2021)and was modified bySeber & Braatz (2024). The modified version is available atgithub.com/PedroSeber/O-GlcNAcylation_Prediction/blob/master/OVSlab_allSpecies_O-GlcNAcome_PS.csv. This dataset contains 558,168 unique S/T sites from mammalian proteins. Out of these, 13,637 (2.44%) are O-GlcNAcylated. The same procedure as inSeber & Braatz (2024)was employed to filter homologous and isoform proteins, a sequence selection based on a window size of 5 AA on each side of the central S/T (11 AA total), even for the larger windows. 20% of each dataset is separated for testing, with the remaining 80% used for cross-validation with five folds. This procedure follows what was previously done inSeber & Braatz (2024). To ensure that the choice of train/cross-validation and test sets is not biased and to highlight the robustness of our architecture and novel loss function, this work also performs nested validation with the best model, similarly toSeber & Braatz (2024). In each nested validation round, 20% of the data are used as a test set and the remaining is used for hyperparameter selection via 5-fold cross-validation (as above).

SECTION: 2.2Transformer Models

Transformer encoder models are constructed using PyTorch(Paszke et al.,2019)and other Python packages(Harris et al.,2020; Pedregosa et al.,2011; McKinney,2010). Embed sizes (ES){24, 60, or 120}, learning rates{1or 1}, {2, 4, 6, or 8} stacked encoder cells, cell hidden sizes (widths){ES/4, ES/2, ES, ES2, ES3, ES4, ES5}, {2, 3, 4, 5, or 6} attention heads (when available in a given ES), post-encoder MLP sizes{0, ES/4, ES/2, ES}, and window sizes of {5, 10, 15, or 20} were used. Training is done in 100 epochs with the AdamW optimizer with a weight decay parameter{0, 1, 5, or 1}, Kaiming uniform initialization(He et al.,2015)with, and cosine scheduling(Loshchilov & Hutter,2017). Cosine scheduling has been used mostly in computer vision settings and has achieved significant results with imbalanced datasets(Kukleva et al.,2023; Mishra et al.,2019), and it was also used bySeber & Braatz (2024). The best combination of hyperparameters is determined by a semi-grid search111Combinations of hyperparameters that werea prioriclearly inferior were not tested., and the combination with the highest cross-validation average F1score is selected. Finally, the performance on an independent test dataset of a model using that combination of hyperparameters is reported. Multiple thresholds are used to construct a Precision-Recall (P-R) curve, and the F1scores and MCCs of the models are analyzed, allowing for a throughout evaluation of the best model’s performance.

SECTION: 2.3RNN Models

RNN models (specifically, LSTMs) are constructed using the same packages mentioned in Section2.2. RNN hidden sizes{225 to 1575 in multiples of 75} for the first cell,{0 to 300 in multiples of 75} for the second cell, and{0 to 225 in multiples of 75} for the third and fourth cells, learning rates{1, 5, 1}, post-LSTM MLP sizes{37 and 75 to 825 in multiples of 75}, and window sizes of {20} were used. Training is done in 70 epochs with the AdamW optimizer with a weight decay parameter{0, 1, or 2}, Kaiming uniform initialization(He et al.,2015)with, and cosine scheduling(Loshchilov & Hutter,2017). Hyperparameter selection and testing / evaluation procedures are as in Section2.2.

SECTION: 2.4Loss Functions

The cross-entropy (CE) loss is the most common loss function for classification tasks, and its weighted variant is suitable for unbalanced data. Despite its widespread use and efficacy, the CE loss has some important limitations. First, the CE loss is distinct from the evaluation metrics used. Users of classification models may desire to maximize the F1score or MCC metrics, but the model with the lowest CE loss is not necessarily the one with the highest metrics. Moreover, a model can lower its CE loss by manipulating the predictions of samples that are already correctly separated. For an extreme example, consider a model that predicts 0.8 for all positive samples and 0.2 for all negative samples and a second model that predicts 0.99 for all positive samples and 0.01 for all negative samples. Both models perform perfect classification, yet the second model will have a lower CE loss. In a less extreme scenario, a model may improve its CE loss by simply manipulating the probabilities of easy-to-classify samples, but such manipulations would not improve real metrics such as the F1score or MCC.

There are multiple ways to remedy these issues.Lin et al. (2020)created the Focal Loss, which provides a correction factorthat reduces the effect of increasing the certainty of a correct prediction, forcing the model to improve on difficult-to-classify samples to lower the focal loss.Berman et al. (2018)created the Lovász loss, which is an optimizable form of the Jaccardi Index and is claimed to perform better than the CE loss due to its better categorizing of small objects and lower false negatives. Other losses not evaluated in this work include the Baikal loss ofGonzalez & Miikkulainen (2020), the soft cross entropy ofIlievski & Feng (2017), the sigmoid F1ofBénédict et al. (2022), the work ofLee et al. (2021), and the MCC loss ofAbhishek & Hamarneh (2021). In particular, the approaches of this work resemble those ofLee et al. (2021)andAbhishek & Hamarneh (2021), but these references did not use weighting or focal transforms, and the definitions for the differentiable F1and MCC loss functions used in these works are slightly different from the definitions in this work.

The standard formulations of evaluation metrics (such as the F1score or MCC) are not differentiable, so they cannot be directly used as loss functions. However, it is possible to modify them for differentiability. One method is treating the true / false categories as prediction probabilities instead of binary values. For example, if the model predictsfor a positive (negative) sample, that sample would increase the true positives (negatives) by 0.7 and the false negatives (positives) by 0.3. This modification can be considered as a differentiable MCC loss function. The focal modification ofLin et al. (2020)can be added to this loss function by modifyingto, which leads to the focal differentiable MCC loss function. Finally, it is possible to add class weighting: by multiplying the true positives and false negatives by a scalar W, it is possible to prioritize predictions on the positive class (W > 1) or negative class (W < 1). All the above changes combined lead to theweighted focal differentiable MCCloss function (Algorithm1).

For the weighted CE loss, the weights for the positive class used were{15, 20, 25, 30}. For the focal loss, thevalues used were {0.9375, 0.95, 0.99, 0.999, 0.9999, 0.99999, 0.999999} and thevalues used were {0, 1, 2, 3, 5, 10, 15, 20, 30, 45, 50}. For the weighted focal differentiable MCC, the weights W used were{1, 2, 3, 4, 5, 10, 15, 20} and thevalues used were {1, 2, 3, 3.5, 4, 4.5, 5}.

SECTION: 3Results

SECTION: 3.1Transformer Encoders Have Good Predictive Power on This Dataset, but are Inferior to RNNs

The primary goal of this subsection is to compare transformer models with the best RNN fromSeber & Braatz (2024)using the modified dataset ofWulff-Fuentes et al. (2021)(Section2.1) and the weighted CE loss function. While our transformer models surpass models published beforeSeber & Braatz (2024)and achieve an F1score equal to 24.31% and an MCC equal to 22.49% (Table1), transformers exhibit inferior recall at the same precision level (Fig.1) than the RNNs fromSeber & Braatz (2024), leading to inferior F1and MCC metrics. It should be noted that the work ofPokharel et al. (2023)used an ensemble of three transformer models to predict O-GlcNAcylation and obtained considerably inferior F1and MCC metrics than the RNN ofSeber & Braatz (2024)and the transformer models in this work. It is possible that transformers are an ill-suited architecture for this task, potentially due to the amount of data available, which is why we train primarily RNNs in the following sections.

MetricTransformer-15; 5 win(This Work)Transformer-15; 10 win(This Work)Transformer-15; 15 win(This Work)Best Threshold0.600.200.50Recall (%)20.5426.0626.02Precision (%)15.6313.7721.28F1Score (%)17.7518.0223.41MCC (%)15.5016.0821.36

MetricTransformer-15; 20 win(This Work)RNN-225; 20 win(Seber & Braatz(2024))YinOYang(Gupta & Brunak(2002))No ModelBest Threshold0.500.600.60N/ARecall (%)30.8935.4713.59100Precision (%)20.0436.908.082.44F1Score (%)24.3136.1710.134.76MCC (%)22.4934.577.570.00

A surprising result is how the performance of the transformer models does not change significantly with the hyperparameters. Using large hidden sizes or adding another linear layer after the encoders slightly degrades the performance, while increasing the number of attention heads (to 4–5) and the number of stacked encoder layers (to 4) marginally improves the model. The most surprising result is the lack of significant improvement with increasing window size. As inSeber & Braatz (2024), our models perform better with larger window sizes for sizes up to 20 (Fig.1and Table1). The performance of a transformer encoder with a window size = 5 is only marginally inferior to the RNN fromSeber & Braatz (2024)using the same window size. However, whereas the RNNs’ performances increase quite significantly with increasing window size, the performance of the transformers barely improves. The only significant performance increase comes from increasing the window size from 10 to 15, but barely any performance is gained from further increasing this window size to 20 (Fig.1).

SECTION: 3.2The weighted focal differentiable MCC loss function is a superior alternative to the weighted CE

We hypothesized that changing the loss function can lead to improvements in the models’ metrics, as the weighted CE is optimizing a function that is only correlated with F1scores and MCC. Four other loss functions (as described in Section2.4) are also used to cross-validate transformer models through the same procedure used with the weighted CE function (Sections2.2and3.1).

The Focal, Lovász, and differentiable F1losses fail to achieve meaningful results. In the best-case scenarios, models trained with these loss functions can achieve F1scores equal to or slightly above 4.74%, equivalent to treating all (or almost all) sequences as positive. The differentiable F1, in particular, frequently gets stuck at this local F1maximum. Naturally, these models have an MCC, as they are no better than random classifiers.

Conversely, the weighted focal differentiable MCC loss is able to attain very good performance on this problem (Fig.2and Table2). For both architectures, its performance is superior to that of the weighted cross-entropy loss. As in Section3.1, RNNs are considerably better than transformer encoders in predicting O-GlcNAcylation. The RNN model trained with the weighted focal differentiable MCC loss achieves an F1score = 37.03% and an MCC = 36.58%. The relative and absolute increases in MCC are higher than those for the F1score, highlighting the effectiveness of the weighted focal differentiable MCC loss, as it seeks to directly optimize the MCC. The weighted focal differentiable MCC loss can also be used for fine tuning; an RNN model first trained using weighted cross-entropy, then fine-tuned using the weighted focal differentiable MCC is generated (“RNN-225; CEdiff MCC”). This model initially followed the optimal hyperparameters (including model size) found through cross-validation using the weighted cross-entropy loss, but the learning rate and loss hyperparameters are changed to the optimal values found through cross-validation with the weighted focal differentiable MCC loss function when that loss function is used for fine-tuning. This fine-tuning also leads to a model that is better than the previous state-of-the-art fromSeber & Braatz (2024)(Fig.2and Table2), reaching an F1score = 36.52% and an MCC = 36.01%, values slightly lower than that of the model trained only with the weighted focal differentiable MCC loss.

There is a significant difference between the optimal hyperparameters selected through the weighted cross-entropy and weighted focal differentiable MCC loss functions. Compared to the optimal values found through the weighted cross-entropy loss, the weighted focal differentiable MCC loss has an optimum at higher model hidden sizes (15 vs. 240 for transformers; 225 vs. 1425 for RNNs), higher batch sizes (256 vs. 512 for transformers; 32 vs. 128 for RNNs), lower learning rates (1vs. 1), and lower class weights (15 vs. 3 for transformers; 30 vs. 3 for RNNs).

MetricTransformer-15; CE(This Work)Transformer-240; diff MCC(This Work)RNN-225; CE(Seber & Braatz(2024))Best Threshold0.500.400.60Recall (%)30.8925.7035.47Precision (%)20.0425.4836.90F1Score (%)24.3125.5936.17MCC (%)22.4923.6734.57

MetricRNN-1425; diff MCC(This Work)RNN-225; CEdiff MCC(This Work)No ModelBest Threshold0.700.40N/ARecall (%)30.6730.35100Precision (%)46.7045.842.44F1Score (%)37.0336.524.76MCC (%)36.5836.010.00

SECTION: 3.3Stacked RNNs provide better performance, and the weighted focal differentiable MCC loss is still superior

Seber & Braatz (2024)trained RNNs with only a single cell, a restriction that was followed by this work in Section3.2. However, RNNs with multiple stacked cells may perform better than single-cell RNNs. To verify this hypothesis, RNN models with 2, 3, and 4 LSTM layers are tested (as per Section2.3). Similarly to Section3.2, three models are created: one trained only with the weighted CE loss, one trained only with the weighted focal differentiable MCC loss, and one first trained with the weighted CE loss, then fine-tuned with the weighted focal differentiable MCC loss. These models all surpass their single-RNN layer equivalents from Section3.2. The model trained with the weighted CE loss (“RNN-[450,75]; CE”) displays the greatest relative improvement over its single-cell version, reaching an F1score = 38.73 % and an MCC = 37.31%. Like with the single-cell RNNs, fine-tuning with the weighted focal differentiable MCC loss after training with the weighted CE leads to improvements in the model performance; this fine-tuned model (“RNN-[450,75]; CEdiff MCC”) reaches an F1score = 38.83% and an MCC = 37.33%. While the performance improvement of fine-tuning is smaller in this scenario, these improved results again confirm the efficacy of fine-tuning with the weighted focal differentiable MCC loss. Finally, a model trained directly with the weighted focal differentiable MCC loss (“RNN-[600,75]; diff MCC”) achieves the best results, reaching an F1score = 38.88% and an MCC = 38.20%. The increase in MCC is considerably higher, further corroborating the benefits of this new loss function discussed in Section3.2. Overall, the “RNN-[600,75]; diff MCC” model displays a 7.5% increase in F1score and a 10.5% increase in MCC over the “RNN-255; CE” model ofSeber & Braatz (2024).

MetricRNN-[450,75]; CE(This Work)RNN-[450,75]; CEdiff MCC(This Work)RNN-[600,75]; diff MCC(This Work)Best Threshold0.900.500.60Recall (%)36.6937.6233.11Precision (%)41.0240.1147.10F1Score (%)38.7338.8338.88MCC (%)37.3137.3338.20

The models with the highest average cross-validation F1score all had only two layers and second LSTM layers with 75 neurons. As in Section3.2, the optimal hyperparameters for models trained with the weighted focal differentiable MCC loss included higher model hidden sizes (600 vs. 450 for CE), lower learning rates (5vs. 1), and lower class weights (2 vs. 20). Surprisingly, both models had the same optimal batch size (128), and the hidden size and class weights for the model trained with the weighted CE loss moved towards those from the model trained with the weighted focal differentiable MCC loss.

Finally, we verified that our splits for the cross-validation and test sets do not bias the models by performing five-fold nested cross-validation, following the procedure in Section2.1, on the “RNN-[600,75]; diff MCC” model. The models generated by each fold had very similar performance to each other (Fig.A1and TableA1), highlighting the robustness of that architecture and loss function to changes in the training/test data.

SECTION: 4Discussion

This work generates transformer encoder (Section3.1) and RNN (Sections3.2and3.3) models to predict O-GlcNAcylation sites based on protein sequences using a modified version of an extensive public datasetWulff-Fuentes et al. (2021). These models are also compared to the state-of-the-art RNNs trained bySeber & Braatz (2024).

Although transformer models attain high performances on this dataset, they are inferior to the RNN models trained bySeber & Braatz (2024), reaching an F1score of 24.31% and an MCC of 22.49% (Section3.1, Fig.1, and Table1). A significant difference between transformers and RNNs is that the former do not scale well with increasing window sizes, while the latter display significant improvements with larger window sizes. With window size = 5, both transformers and RNNs possess similar performances, but a large performance gap arises with increasing window sizes. Multiple reasons could be causing this inferior transformer performance, including a lack of training data. Although the dataset fromWulff-Fuentes et al. (2021)is one of the largest (if not the largest) O-GlcNAcylation datasets published, it is possible that there simply are not enough data for the transformer architecture to perform well in this task.

Because the weighted cross-entropy loss function, which is used to train the models inSeber & Braatz (2024)and Section3.1of this work, only indirectly optimizes the classification performance metrics, we hypothesized that a function that could directly improve the F1score and MCC can lead to better-performing models.

We develop the weighted focal differentiable MCC loss to address this question (Section2.4; Algorithm1). This function allows the direct optimization of the MCC, yielding better classification models. We cross-validate and test transformer and RNN models using the same dataset and notice that the performance of the weighted focal differentiable MCC loss is similar to but higher than that of the weighted cross-entropy loss (Section3.2, Fig.2, and Table1). This loss function improves both the transformer and RNN architectures, and leads to the creation of an improved model with an F1score = 37.03% and an MCC = 36.58%. Furthermore, a model first trained using the weighted cross-entropy loss, then fine-tuned using the weighted focal differentiable MCC loss achieves improved performance in this prediction task relative to a model trained using only the weighted CE loss, reaching an F1score = 36.52% and an MCC = 36.01%. While these metrics are slightly smaller than the values obtained by a model directly trained with the weighted focal differentiable MCC loss, these results show how this new loss function can also be used to optimize and improve trained models at a low cost. The optimal hyperparameters for models trained with the weighted focal differentiable MCC loss are different from those of models trained with the weighted cross-entropy loss.

AsSeber & Braatz (2024)used only single-cell models in their work, the RNN models from Section3.2follow this restriction. Models with multiple stacked cells can have better performance, so we train RNN models with multiple LSTM cells in Section3.3. These models display superior cross-validation F1scores over models with one cell, and the best models have only 75 neurons in their 2nd cells. No matter the loss function used, these deeper models display better performance in the test set than the best RNN ofSeber & Braatz (2024)(Fig.3and Table3). As in Section3.2, fine-tuning using the weighted focal differentiable MCC loss after training with the weighted CE leads to improvements in performance over using only the weighted CE, corroborating one of the benefits of the weighted focal differentiable MCC loss. The model trained solely with the weighted focal differentiable MCC loss, “RNN-[600,75]; diff MCC”, achieves state-of-the-art performance in O-GlcNAcylation prediction, reaching an F1score = 38.88% and an MCC = 38.20%. Overall, this is a 7.5% increase in F1score and a 10.5% increase in MCC over the “RNN-255; CE” model ofSeber & Braatz (2024).

There are two main limitations to this work. On the O-GlcNAcylation prediction side, although the models developed in this work reach state-of-the-art performance in multiple independent datasets, that performance is still far away from (near-)perfect, possibly limiting the widespread usability of the models. It should be noted that the experimental detection of O-GlcNAcylation sites is a laborious and expensive process, so even an imperfect model may be useful. Having a usable model is particularly important given the relevance of O-GlcNAcylation to many diseases and the potential of O-GlcNAcylation as a target for medicines (Section1). On the weighted focal differential MCC loss side, although we earnestly believe that it can improve the performance of models in almost any classification task relative to the (weighted) CE, this work tests its performance on only one task. Future directions primarily include testing the weighted focal differential MCC loss on multiple classification tasks to show its wide usability, including in fine-tuning models that were previously trained with the CE loss. Another direction is analyzing the effects of dynamically changing this loss function’s hyperparameters over the epochs to determine whether it can be further improved.

The code used in this work is publicly available, allowing the reproduction, improvement, and reuse of this work. It is simple to install and run the best RNN model to predict O-GlcNAcylation sites based on the local protein sequence. Instructions are provided in SectionsA1.1andA1.2of the supplemental data or the README inour GitHub <redacted for review>repository.

SECTION: References

SECTION: A1Appendix

SECTION: A1.1Reproducing the models and plots

The transformer models can be recreated by downloading the datasets and running thetransformer_train.pyfile with the appropriate flags (runpython transformer_train.py --helpfor details).

The RNN models can be recreated by downloading the datasets and running theANN_train.pyfile (modified from the original ofSeber & Braatz (2024)) with the appropriate flags (runpython ANN_train.py --helpfor details).

The plots can be recreated by running themake_plot.pyfile (modified from the original ofSeber & Braatz (2024)) with the appropriate flag as an input (python make_plot.py cefor Fig.1,python make_plot.py diffMCCfor Fig.2,python make_plot.py multilayerfor Fig.3, andpython make_plot.py nestedfor Fig.A1).

SECTION: A1.2Using the RNN model to predict O-GlcNAcylation sites

Following the procedures set bySeber & Braatz (2024), the Conda environment defining the specific packages and version numbers used in this work is available asANN_environment.yamlonour GitHub <redacted for review>. To use our trained model, run thePredict.pyfile aspython Predict.py <sequence> -t <threshold> -bs <batch_size>.

Alternatively, create an (N+1)x1 .csv with the first row as a header (such as "Sequences") and all other N rows as the actual amino acid sequences, then run the Predict.py file aspython Predict.py <path/to/file.csv> -t <threshold> -bs <batch_size>. Results are saved as a new .csv file.

SECTION: A1.3Supplemental Figures and Tables

MetricFold 1Fold 2Fold 3Fold 4Fold 5Mean ()Best Threshold0.600.800.800.600.70N/ARecall (%)33.1135.2332.6831.0131.8132.771.43Precision (%)47.1042.6145.1547.9648.1246.192.08F1Score (%)38.8838.5737.9237.6638.3038.270.44MCC (%)38.2037.3737.1037.3837.9037.590.40