SECTION: Evaluating Vision Transformer Models for Visual Quality Control in Industrial Manufacturing

One of the most promising use-cases for machine learning in industrial manufacturing is the early detection of defective products using a quality control system. Such a system can save costs and reduces human errors due to the monotonous nature of visual inspections.
Today, a rich body of research exists which employs machine learning methods to identify rare defective products in unbalanced visual quality control datasets.
These methods typically rely on two components: A visual backbone to capture the features of the input image and an anomaly detection algorithm that decides if these features are within an expected distribution. With the rise of transformer architecture as visual backbones of choice, there exists now a great variety of different combinations of these two components, ranging all along the trade-off between detection quality and inference time.
Facing this variety, practitioners in the field often have to spend a considerable amount of time on researching the right combination for their use-case at hand. Our contribution is to help practitioners with this choice by reviewing and evaluating current vision transformer models together with anomaly detection methods. For this, we chose SotA models of both disciplines, combined them and evaluated them towards the goal of having small, fast and efficient anomaly detection models suitable for industrial manufacturing. We evaluated the results of our experiments on the well-known MVTecAD and BTAD datasets. Moreover, we give guidelines for choosing a suitable model architecture for a quality control system in practice, considering given use-case and hardware constraints.

SECTION: 1Introduction

In industrial manufacturing, early detection of defective products saves material and costs and enhances public trust in the manufacturer. Automating this process increases scalability, saves labour costs and reduces human error due to the monotonous nature of visual inspections[27]. To this end, the possibility of automating this process using machine learning methods has been subject of extensive research[24].
Anomaly detection (AD) in machine learning addresses the challenge of identifying rare defective products in unbalanced datasets, often utilizing unsupervised or semi-supervised training strategies. While AD refers to image-level classification of samples as either normal or anomalous, anomaly localization (AL) aims to identify anomalies on a more fine-grained level and indicates where the anomalous feature was detected within the image.
This provides interpretability of the model’s decisions, facilitating human-in-the-loop control by allowing focus on the anomalous regions.
Unsupervised AD architectures typically consist of an image encoding backbone and a detection algorithm that identifies if the extracted features are within an expected distribution.
Since the publication ofViTin 2020[9], vision transformers have emerged as an alternative to traditional CNN backbones, offering enhanced global dependency capture, making them particularly interesting for AD tasks. Emerging hierarchical transformer architectures promise to solve the problem of their excessive size by keeping their advantages[26].
Many approaches to solve AD problems in existing literature consider monolithic vision transformer but not hierarchical ones[21,31]. We believe hierarchical vision transformer models can be a great benefit for industrial visual quality control regarding their computational and memory demands. Moreover, we want to help practitioners with their choice of the best setup by reviewing and evaluating current vision transformer models together with AD methods. Our contributions can be summarized as follows:

First, we provide a comprehensive overview of current state-of-the-art (SotA) hierarchical vision transformer models and approaches for the task of visual quality control.

Second, we reproduce two of the most promising AD methods and combine them with different visual backbones to find small, fast and efficient AD models, suitable for industrial manufacturing, and evaluate our results on the well-known MVTecAD and BTAD datasets.

At last, we recommend guidelines for choosing a suitable architecture for deploying a quality control system in practice considering given use-case and hardware constraints.

The rest of this paper is structured as follows: In the next section, we review existing work regarding visual backbones, AD and AL.
Section3describes the setup we chose for our experiments, which are then discussed and analysed in Section4. Our source code is available on GitHub (https://github.com/visiontransformerad/vit-ad).

SECTION: 2Related Work

SECTION: 2.1Vision Backbone

Vision transformers have become a powerful alternative to CNNs for various computer vision tasks, with many studies highlighting their proficiency in capturing global dependencies, which are essential for AD and AL workloads[20,30,6,21,31].
Monolithical vision transformers such asViTfollow the architecture of NLP transformers, offering performance competitive with CNNs likeResNet[12]but demand more training data, memory, and longer inference times. By applying an enhanced training strategy using a teacher model and a distillation token to match it’s output, Touvron et al.[25]publishedDeiT, an improved monolithic transformer that is smaller thanViT, demands less training data and has a superior performance.
However, in light of the limited computational resources in production settings, the light-weight class of hierarchical vision transformers gained interest in recent work[26,28,19,16,33].
The main idea is to address the poor scaling of monolithic transformers to high-resolution images by reducing the image size across layers.
This leads to models that need less time and resources for training and inference but perform equal to existing vision transformer models[26].HaloNet[26]is one of the first approaches of using size-reducing layers in combination with attention in the encoder, inspired by the architecture used inResNet[12]. Wang et al. introducedPVT, a model with an attention backbone suitable not only for classification but also for dense prediction tasks such as object detection and segmentation[28]. Zhang et al.[33]propose withNesTa lightweight yet not overly complex architecture by applying theViTarchitecture[9]on distinct subsets (blocks) of patches and subsequently reducing groups of four neighbouring blocks into one.
Liu et al. developed a similar approach toPVTbut highlight, that their model has linear instead of quadratic complexity when it comes to scaling with image size[19]. TheirSwinTransformermodel alternates between a window-based and shifted window-based self-attention, which computes attention locally on a fixed number of patches within non-overlapping windows. To enable cross-window connections, non-overlapping neighboring windows from the previous layer are included in the calculations of each block. For dimension reduction, a patch merging layer is added before each stage, which itself can consist of two or more transformer blocks.
Li et al.[16]highlight the issue of multi-stage vision transformers with sparse self-attention failing to detect fine-grained inter-region dependencies. They introduce a label-free knowledge distillation strategy, using altered image views for training. Their approach, dubbedEsViT, combines view and region-level prediction losses. Their training procedure can be applied to various transformer architectures and pre-trained weights are available for[19,28]and[25].
Li et al.[18]introduced theEfficientFormer, an approach that is comparable in inference speed with lightweight CNN implementations such asMobileNetV2[23]and thus can be used in edge applications. To achieve this, they observed the main bottlenecks (e.g linear projection layers for patch embeddings, reshape operations) in the existing vision transformers and tried to improve efficiency with a few architecture modifications.

SECTION: 2.2Anomaly Detection and Localization

There are several different categories of approaches for implementing AD and AL in practice[24].
Reconstruction-based methods use an auto-encoder with the objective to compress the input to a lower dimensional latent space and subsequently reconstruct the original image, using the reconstruction error as anomaly score. They tend to deliver weaker results than other categories[30]. Self-supervised learning methods try to use synthetic data to train a model, which requires costly data augmentation strategies that often heavily depend on expert knowledge. In contrast, promising representation-based approaches try to model the distribution of normal features and classify defects as samples that are in regions with low probability density or outside of the modeled distribution[24].
Roth et al.[22]achieved excellent detection scores withPatchCoreby using a memory bank and measuring the distance of extracted features to the nearest feature in the bank. Bae et al.[2]enhancePatchCorewith position and neighborhood information to improve the detection of global anomalies. Hyun et al. introduceReConPatch, that uses multiple levels of a CNN encoder as base to create patch-level features, also enhanced by neighborhood information[13]. Li et al. introduceSemiREST, a model that uses supervised and semi-supervised learning in combination with theSwinTransformerarchitecture[17].

Gaussian Mixture Models (GMM) are based on the assumption, that the underlying distribution of normal features is more complex than a single Gaussian distribution and thus learns a set of distributions[5]. At inference time, a GMM estimates the probability that a feature belongs to the set of learned distributions[21].
Zong et al.[34]applied an auto-encoder model for AD on non-image data, integrating a GMM for both dense prediction and regularization to avoid local minima in training.
Zhang et al.[32]developed a three-component model for AD in high-resolution images, featuring class-specific training and evaluation. This model includes a patch embedding creator, a GMM for density prediction, and a multi-layer perceptron (MLP) for location prediction, jointly trained end-to-end. This approach yields a smaller model compared to pre-trained alternatives.
Mishra et al.[21]and Choi et al.[6]advanced auto-encoder networks for AD by integrating them with a GMM and using a vision transformer encoder, respectively. Mishra et al.’sVT-ADLmodel involves end-to-end training, feeding encoder output patch embeddings to a GMM for Gaussian calculations and a CNN decoder for reconstruction. They use mean squared error, log-likelihood and structural similarity (SSIM) losses as training objective and for the anomaly map generation. Choi et al.[6]adopt a similar methodology but employ a variational auto-encoder and a pre-trained transformer, emphasizing a workflow from image collection to expert validation. Both approaches underscore transformers’ superiority in capturing global dependencies over CNNs.
Fan et al.[10]integrate GMMs with variational auto-encoders for pinpointing anomalies in surveillance video streams, employing a model that samples from multiple distributions to accommodate complex feature spaces.

Normalizing Flow models (NF) are located in two categories, generative models and representation based approaches and are an efficient alternative to GMMs for estimating complex distributions. They have seen increasing popularity for AD due to their fast inference and strong downstream performance.
They estimate the exact likelihood of features by following any arbitrary distribution and computing the likelihood by using the KL divergence between a prior and a base distribution[11,8].
Gudovskiy et al.[11]introduceCFLOW-AD, an AD model pairing a pre-trainedResNetencoder with a conditional NF, training independently for each class with fixed position embeddings.
Yu et al.[31]critique fixed position embeddings for complex datasets, introducingFastFlow, an AD model with a 2D NF that eliminates the need for positional encoding by preserving spatial structures.FastFlowuses aResNetor a vision transformer encoder with selective embedding stages. The authors report AUROC scores on, among others, the MVTecAD and BTAD dataset, achieving speed gains over the previous models in[11]and[22]. Lei et al.[15]improve results with pyramid NFs and end-to-end trainedCNNs, while Kim et al.[14]enhanceFastFlowandCFLOW-ADstability and performance with a novel training approach.

SECTION: 3Experimental Setup

SECTION: 3.1Backbone Architectures

Because of their performance advantages, our analysis primarily focuses on transformer-based architectures. Given the importance of hardware efficiency in practical applications, the lightweight and efficient hierarchical transformers are of particular interest in our study. For a comprehensive perspective, we also evaluate a classicalResNet-50model as a baseline. We chose thisResNetvariant since it is a common choice when making a trade-off between the quality of representations and computational cost[11,31].
TheDeiTarchitecture was chosen as an example of a monolithic vision transformer that closely follows the architecture of the originalViTproposed in[9], but achieves better performance by using knowledge distillation[25]. We used the largest variantDeiT-basein our experiments since it has the highest performance and was also used in[31].
Moreover, we selected theEsViTapproach to investigate the performance of hierarchical transformer versions. We used the variant based on theSwin-Tarchitecture since it has a relatively small number of parameters which is comparable toResNet-50. A window size of 14 achieved the best results in[16], hence we adopted this choice for our experiments.EfficientFormeris another efficient transformer suitable for settings with limited computational resources.
To have a comparable parameter size toResNet-50and make a good trade-off between efficiency and performance, we picked the medium-sized model variantL3.
A comparison of the number of parameters of the different image encoder models is shown in Table1.

SECTION: 3.2Anomaly Detection Architectures

Based on the feature vector produced by the vision backbone a classifier needs to distinguish normal examples from defective anomalies.
We focused our experiments on GMMs, which were one of the first approaches using a vision transformer backbone and NFs, which showed promising performance in AD tasks (as discussed in Section2.2).

The performance of a GMM usually improves with a higher number of Gaussians but the use of a fully-connected MLP for every component can make them prohibitively expensive. Figure1illustrates how we used image-processing backbones together with a GMM to perform AD in the case ofDeiT. The procedure for the other transformer backbones is analogous with differing numbers for the embedding size. WithResNet, we followed the approach in[11]and trained two GMMs for the output of the last two blocks ofResNet-50to capture global and local dependencies.

Based on the patch embeddings we trained three MLPs representing the means (), standard deviations () and weights () of the Gaussian Mixture components to capture the distribution of the normal examples. The size of the MLPs depends on the number of Gaussians. During training we minimized the negative log-likelihood, while for inference we calculated an anomaly score for every patch as follows: The log-likelihood is normalized to a value between zero and one using min-max normalization over the batch to obtain a pseudo-probability. Our anomaly score is defined as. To obtain a 2D anomaly map we reshaped the 196 patches to amatrix and used bilinear interpolation to project the anomaly map to our original image size of. An image is deemed anomalous if its highest patch anomaly score surpasses a specific threshold, which is empirically determined using the validation set, as suggested by You et al.[30].

Normalizing Flows were chosen as models in view of limited compute. Our setup of a NF model combined with aDeiTimage-processing backbone is visualized in Figure2. The setup is analogous for other transformer backbones. ForResNet, we used the output of the last three blocks and averaged the results, similar to the GMM approach.

The patch embeddings produced by the vision backbone are passed through the NF model consisting of 20 flow steps for the transformer-based methods and eight forResNet-50, due to hardware limitations. Further, we chose a hidden ratio. Each flow step consists of a subnetwork of alternatingandconvolutional layers. These hyperparameters follow the recommendations of[31]. For the other hyperparameters see Table2. The flow steps were realized byAllInOneBlocks of theFrEIAframework[1]. The NF model outputs the log determinant of the Jacobian and the transformed patches. During training those two outputs are used for the loss objective. At inference,andare used to calculate the likelihood which is then used as anomaly map for localization, by upsampling the patches with bilinear interpolation. For more details on the NF architecture see[31].

SECTION: 3.3Datasets

We evaluate our methods on two datasets that are as close to real-life as possible.

The BTAD dataset consists of three different types of real-world industrial products and has a total of 2830 RGB images with a resolution of betweenand[21]. Each product category is split into a train set, consisting of only normal images and a test set, which includes normal and anomalous images. In total there are 1799 training images, the rest are test images. The proportion of anomalous images in the test set varies from 9% to 87% depending on the class. A ground-truth mask for each anomalous image is provided that can be used for AL. There is no further distinction between body and surface defect anomaly classes in the labels.

The MVTecAD dataset consists of 15 different product categories or textures and has a total of 5354 images[3]. Three classes are provided as grayscale images which is common in some real-life settings. The other classes are RGB images. The resolution varies betweenandand some classes have more examples than others.
Each product category is split into a training set with only normal instances and a test set that also contains anomalies. In total there are 3629 training and 1725 test examples. The proportion of abnormal images is, in most cases, about two to three times higher than the normal ones in the test set. The abnormal data is classified into different types of anomalies, for example in crack, hole, cut and print on thehazelnutclass. A ground-truth mask on pixel-level is given for each abnormal image.

SECTION: 3.4Implementation Details

Since both datasets do not provide a separate validation set for hyperparameter tuning and model selection we split the training set into 80% train and 20% validation data in both cases. We applied Min-Max scaling to every image. The minimum and maximum values were computed separately for every channel on the training set. We scaled the input images to an image size ofpixels which is the default training size of most transformer models[9].

Hyperparameters were optimized once for each model using thehazelnutclass of the MVTecAD dataset and the best configuration according to validation loss is adopted for all other experiments. We chose this class because it has the largest amount of training samples in the dataset. A summary of the considered hyperparameters and the best values found for them are shown in Table2. Note that the batch size of the GMM is relatively small due to hardware limitations.
We trained each model separately on every object class for a maximum of 500 epochs using early stopping based on the validation loss with a patience of 30 epochs. Only the parameters of the AD models were updated, the image-backbones were pre-trained on ImageNet1k[7]and kept frozen during training. For evaluation the model checkpoint with the best validation loss was selected.
All models were trained on a single NVIDIA GeForce RTX 2080 Ti TURBO GPU with 11GB VRAM.

SECTION: 3.5Metrics

For the evaluation of our approach, we use the AUROC since it is a common metric for visual quality control[31,2,22,13]. For evaluating AD we consider the AUROC on image and for AL on pixel level.

As a second metric we use the Per-Region-Overlap (PRO), which measures the overlap between ground truth and predicted anomalies on a pixel level. To calculate the PRO-score one needs to set a threshold at which level to classify a pixel as anomalous. This is usually done based on the ROC considering a trade-off between acceptable levels of False Positive Rates (FPR) and TPRs. This can depend on the use case, considering whether it is costly to allow a high FPR (e.g. if a false positive results in an entire production batch being discarded). All pixels with anomaly values below the threshold are set to zero, while the rest keep their original anomaly score.
We calculate the area under the PRO-curve up to a maximum false positive rate of 30% following the procedures of[4]and[21]for the same datasets.
We use the PRO-score to evaluate AL.

Furthermore, we report the precision recall area under curve (PRAUC) as supplement to the AUROC for a more detailed view of the overall performance and to compare results with other works that also use this score. PRAUC is only used to measure image-level AD performance.

SECTION: 3.6Experiments

Our first goal was to assess if similar outcomes can be achieved as theVT-ADLarchitecture by Mishra et al.[21]. While they trained a vision transformer with a GMM and a CNN from scratch on the BTAD and MVTecAD datasets, we used a pre-trainedDeiTmodel as a frozen image backbone and trained only a GMM with the likelihood loss. Chosen for its enhanced performance overViT, ourDeiTconfiguration includes twelve layers and twelve heads, versus the six layers and eight heads used byVT-ADL.
We used an image resolution ofinstead ofto match the requirements of our pre-trained image encoders and employed a smaller GMM with only 100 instead of 150 Gaussians according to our empirical hyperparameter search.
Additionally, we attempted to reproduce Yu et al.’s[31]promisingFastFlowresults for MVTecAD, despite the absence of their source code. We followed their experimental details. While the authors use the 7thDeiTencoder block’s embedding we additionally evaluate a model version which usesDeiT’s last layer. For compatability with our pre-trained backbones we scale the images to a size ofinstead ofpixels.

A second goal was to study the behavior of GMMs and NFs on a more fine-grained level to find out if there are differences in the performance depending on the object class. For this experiment we used our setup with a pre-trainedDeiTimage encoder and trained and evaluated the 15 classes of the MVTecAD dataset separately. For the NF model we investigated two different versions, NF i11 which uses the features from the last layer of the image encoder and NF i7 which uses the feature maps of the 7th encoder layer. For the sake of completeness we also compared the performance with the reported results from[21]and from[31].

Third, we examined the performance effects of replacing traditional monolithic transformers with hierarchical versions using theEsVitand theEfficientFormermodels described in Section3.1. We also consider the CNN-basedResNet-50backbone as a baseline. As proposed in[11]and discussed in Section3.2, we used the output of several blocks of theResNetbackbone and averaged the results. In case of the GMM, we trained only two models with 50 Gaussians for the last two blocks, due to hardware limitations.
We trained and evaluated the methods on five selected classes of the MVTecAD dataset. The classes were chosen separately for both the GMM and the NF i11 approaches based on the performance of our previous experiment with theDeiTencoder. As a representative sample for each model, we included classes with high, medium and low performance according to our experiments. For GMM these are the classescable,carpet,grid,hazelnutandtile. For the NF i11 we use the classesbottle,carpet,hazelnut,leatherandscrew.

SECTION: 4Results and Discussion

SECTION: 4.1Comparison with the VT-ADL and FastFlow models

Table3shows that our GMM model performs better thanVT-ADLin two of three classes of the BTAD dataset in the localization task and one class in the detection class. The bad detection performance on class three may result from the model highlighting anomalies correctly but also producing spots with high anomaly scores in normal images (see Figure3). This results in a high false positive rate when using the maximum of the patch anomaly scores to classify the image. Considering also the composition of the test dataset for this class can explain the gap between detection and localization performance, since it has about ten times more normal than anomalous samples. The overall localization performance of our model on the MVTecAD dataset is higher than the one ofVT-ADL. These results show, that using a pre-trained transformer encoder in scenarios with relatively small datasets can be beneficial compared to training a transformer end-to-end.

Table4shows, that our implementation of the NF model could not reach the values reported forFastFlowin[31]. However, the use of only 80% of the training data and a lower image size may have negatively affected the performance.

SECTION: 4.2Comparison of GMMs and NF models

Table3and4show that the overall performance of the NF model is better than the GMM. However, the GMM performs better in localization tasks on most of the surface classes. The high standard deviation between the classes on MVTecAD for the GMM is mainly caused by the classesbottleandscrew, which could not be learned correctly.
The NF-model has a relatively small standard deviation between the classes and thus is more robust when applied to new classes. Together with the smaller size this makes it more suitable for a use in production scenarios. The weakness of NF i11 in localization performance on some classes can be eliminated when using NF i7 instead (see Table4). However, the overall detection performance of NF i7 is about five percent points less. The anomaly maps show that when using the last block, the highlighted areas are more coarse grained but the model is also more certain. Using the seventh block highlights anomalies more precisely but is also more uncertain and produces small anomaly spots on anomaly-free images.
Regarding the model size (see Table1), the NF model has a clear advantage over the GMM.

SECTION: 4.3Performance of the Backbones

Table5shows that for the GMM the overall localization performance is the best with theDeiTbackbone. It outperforms the hierarchical backbones on all classes except for thecableclass. In contrast, theEsViTmodel performs best in three of five classes in the detection task and has the overall best detection performance. A possible reason for the gap between localization and detection performance can be seen on the generated anomaly maps in Figure4. TheEsViTmodel does locate the anomaly correctly but also highlights large areas in the background. On the normal image there is no area highlighted at all. A possible reason for the bad performance of theResNetbackbone is the usage of only 50 Gaussians and two output layers as discussed in Section3.6.

The results in Table6show, thatDeiTis the backbone that results in the best detection performance for the NF model. Nevertheless,EsVitfollows with the second best detection performance.
Interestingly, while in general the NF achieved the best results, forEsViTthe GMM resulted in a better performance.
TheResNetbackbone outperformsDeiTin two localization tasks but performs worse in detection tasks. Figure5shows the anomaly maps generated with the different backbones.

Hierarchical transformers create smaller models than CNNs, producing compact, information-rich patch embeddings, as highlighted in Table1, where the patch embedding size significantly affects the GMM and NF sizes. UnlikeDeiT, these models are also smaller or equal in size toResNet.
For high-resolution images, the adequacy of transformer patch embedding for identifying small anomalies needs evaluation. Smaller embeddings can result in coarser feature maps due to upsampling from the patch embedding level.

SECTION: 4.4Considerations and Limitations for Practical Application

Anomaly maps can be used in various manufacturing scenarios such as explanation of a model’s choice, making decisions based on the anomaly size or to double check a model’s decision[6]. However, it is important to notice that the experiments in this work are conducted on benchmark datasets that have high quality, which is hard to achieve in a real-world scenario. In manufacturing, metrics should be selected considering the composition of training data and the severity of false positives and false negatives. These aspects should also be considered when deciding on the thresholding strategy for the PRO-score and for AD. Our experiments on the detection performance show, that AUROC score should be preferred when false-negatives are expensive while PRAUC is more suitable when false-positives lead to problems. To evaluate the quality of a model when facing highly imbalanced datasets, both scores should be considered. For localization tasks, the PRO-score should be preferred. The model selection is highly influenced by the available data, hardware and real-time requirements. If enough hardware is available, the superior performance ofDeiTor a comparable monolithic transformer should be chosen. In case of strong hardware limitations and/or real-time requirements, smaller and faster hierarchical transformer are the models of choice. Yu et al. report an inference time of eight milliseconds withDeiTand theirFastFlowmodel[31], what can be regarded as a baseline for hierarchical transformer models. When processing images with a much higher resolution, adjusting the patch-size might be required and thus a training of the backbone becomes necessary.

SECTION: 5Conclusion

In our work, we provided a comprehensive overview of SoTA vision transformer models and evaluated different paradigms for visual anomaly detection in industrial visual quality control. We implemented two anomaly detection methods with four different image encoding backbones, all of them pre-trained on ImageNet1k. We trained our anomaly detection models on the datasets MVTecAD and BTAD and compared our results with two existing approaches from the literature. Finally, we discussed important aspects to consider when applying these approaches to production. We showed that using transformer models can improve the performance of anomaly detection models and reduce the overall size compared toResNet. Moreover, we showed that using pre-trained transformer models can have an advantage over training from scratch. Hierarchical transformer models are worth to evaluate for further use in production scenarios with limited computational capacity. Future research could perform experiments with using the output of more than one layer of the encoder model as done withResNetand proposed in[29]for their segmentation transformer.

This preprint has not undergone peer review or any post-submission improvements or corrections. The Version of Record of this contribution is published in „Machine Learning and Knowledge Discovery in Databases. Applied Data Science Track“ and is available online athttps://doi.org/10.1007/978-3-031-70381-2.Christoph Hönes has received funding from SAP SE. Christoph Hönes and Miriam Alber were employed by esentri AG who also provided computational resources.

SECTION: References

SECTION: Appendix 0.AGenerated anomaly maps from different model configurations

SECTION: Appendix 0.BClass distributions of the used datasets

SECTION: Appendix 0.CAblation study and production application