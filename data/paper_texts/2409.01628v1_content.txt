SECTION: CTG-KrEW: Generating Synthetic Structured Contextually Correlated Content by Conditional Tabular GAN with K-Means Clustering and Efficient Word Embedding

Conditional Tabular Generative Adversarial Networks (CTGAN) and their various derivatives are attractive for their ability to efficiently and flexibly create synthetic tabular data, showcasing strong performance and adaptability. However, there are certain critical limitations to such models. The first is their inability to preserve the semantic integrity of contextually correlated words or phrases. For instance, ‘skillset’ in freelancer profiles is one such attribute where individual skills are semantically interconnected and indicative of specific domain interests or qualifications. The second challenge of traditional approaches is that, when applied to generate contextually correlated tabular content, besides generating semantically shallow content, they consume huge memory resources and CPU time during the training stage. To address these problems, we introduce a novel framework,CTG-KrEW(Conditional Tabular GAN with K-Means Clustering and Word Embedding), which is adept at generating realistic synthetic tabular data where attributes are collections of semantically and contextually coherent words.CTG-KrEWis trained and evaluated using a dataset from Upwork, a real-world freelancing platform. Comprehensive experiments were conducted to analyze the variability, contextual similarity, frequency distribution, and associativity of the generated data, along with testing the framework’s system feasibility.CTG-KrEWalso takes around 99% less CPU time and 33% less memory footprints than the conventional approach. Furthermore, we developed KrEW, a web application to facilitate the generation of realistic data containing skill-related information. This application, available athttps://riyasamanta.github.io/krew.html, is freely accessible to both the general public and the research community.

SECTION: 1Introduction

In today’s digital landscape, data acts as the cornerstone of innovation, akin to oil in its transformative power across industries. Yet, accessibility challenges and privacy concerns impede the realization of data-driven initiatives, particularly for burgeoning academic and nascent organizations.

A prevalent strategy for addressing data scarcity is the adoption of fully synthetic data[1]. Synthetic data refers to fabricated datasets that mirror the structure and statistical characteristics of their original counterparts. However, while synthetic content is valuable, generating it in unstructured form may not fully suffice for data-centric applications. Structured datasets, particularly heterogeneous tabular data, emerge as pivotal assets[2]. Widely utilized and indispensable in numerous critical and computationally demanding applications, tabular datasets excel in various aspects. They not only facilitate interpretability and support extensive feature engineering but also enable benchmarking, reproducibility, scalability, and seamless integration with existing systems.

Hence, the generation of synthetic tabular data is a significant undertaking that has fascinated scholars for a long time. Previous literature has approached the issue of treating individual columns of a table differently by creating a joint multivariate probability distribution and subsequently sampling from the observed distribution. This has been accomplished through the utilisation of Bayesian networks as well as classification and regression trees[3]. Recently, there has been a growing interest in exploring the potential of Generative Adversarial Networks (GANs)[4]models for generating synthetic tabular data. The GAN framework comprises two primary components, namely a generator network and a discriminator network. The generator network is trained to produce synthetic data samples that exhibit a similarity to the actual data distribution. On the other hand, the discriminator network endeavours to differentiate between real and generated samples. Concurrently, the generator network endeavours to produce samples that deceive the discriminator network, while the discriminator network strives to precisely differentiate between real and synthesised samples. GANs have been extensively employed in diverse domains, including but not limited to image synthesis, style transfer, and text generation.

Despite the impressive achievements of GANs in producing unstructured data like images and text, they face notable obstacles in generating heterogeneous tabular data. The conventional GANs that predominantly function on uninterrupted noise vectors encounter difficulties in grasping the structured characteristics of tabular data and the complex relationships among its diverse attributes. Moreover, it is worth noting that the loss functions frequently employed in GANs may not be inherently compatible with the structured characteristics of tabular data, thus constraining their efficacy in this field. The outcomes of a recent survey carried out by Kaggle reveal the ubiquity of tabular data in both academic and business settings[5]. The survey also underscores the intricacies that arise from the varied types of data that are present in tables, such as numerical, categorical, time, text, and cross-table references. In addition, the variables’ distributions may manifest diverse shapes, including but not limited to multimodal, long-tail, and other configurations, thereby compounding the difficulties associated with producing tabular data[6].

TGAN (Tabular GAN)[6]and TableGAN[7]are two prominent variations of GANs that are widely utilised for generating tabular data. However, each of these models has its limitations. TGAN encounters difficulties related to mode collapse, an event that arises when the generator network cannot capture the complete data distribution and instead generates a restricted range of samples[8]. Furthermore, in[9], the authors have observed that the transformation of discrete variables with more than four categories into continuous values using TGAN may not produce favourable outcomes. Conversely, TableGAN postulates that the input data adhere to a continuous distribution and concentrate predominantly on producing continuous features. It may not be as effective in capturing and generating categorical features. Therefore, to address these challenges, a more definitive model was deemed necessary, leading to the development of the Conditional Tabular Generative Adversarial Network (CTGAN)[9]. CTGAN extends the Conditional GAN[10]framework by integrating conditional generation. This allows for the generation of samples that are conditioned on a particular attribute and includes an inherent mechanism for managing categorical data. The model employs an embedding layer to encode categorical variables, enabling the learning of continuous representations of categorical features[11,12,3,13,7,14].

The current CTGAN algorithms are constrained in their treatment of variable types, primarily addressing continuous and categorical variables. This oversight neglects a significant category of contextually correlated word sequences essential for capturing the structured organization and semantic cohesion found in descriptions of particular attributes, characterizations, skill sets, or qualities. Essentially, current iterations of CTGAN are unable to retain the semantic significance and correlation of words within such sequences within a tabular context. However, in various real-world dataset scenarios, such as customer reviews and ratings, social media post datasets, product descriptions and sales datasets, worker profile descriptions, job advertisements, and more, there is a crucial need to intentionally incorporate sequences of correlated words. This is essential for conveying a nuanced understanding of subjects within a tabular format. For example, the ‘skillset’ column in worker profiles serves as one such attribute where individual skills are semantically interconnected, offering insights into specific domain interests or qualifications.

Regrettably, in the case of research in the crowdsourcing category itself, there is hardly any availability of skill-oriented freelancer (or worker) and task profile description datasets that can be utilized in the testing and experimentation of skill-oriented task allocation problems. To our knowledge, MeetUp[15,16,17,18,19]and UpWork[20,21,22,23]are the only recognized datasets, both in tabular (CSV) format, extensively used for such procedures. These datasets are not accessible via any open source repository. The researchers have either scraped the data from the websites of the stakeholders or shared it on demand. However, a repository of the UpWork data has recently been made available on Kaggle[24]. This dataset was also collected by crawling the internet on January 11th, 2022, and contains only a sample of freelance jobs posted in January 2022, without any records of the workers’ or freelancers’ profiles. On the other hand, the UpWork data mentioned in[20,21,22]includes separate CSV files for tasks and workers. One positive aspect of Kaggle’s UpWork dataset[24]is that, in addition to being relatively recent, it contains 298 unique task profiles, whereas the former dataset only had 97. Some of the co-authors of this paper have access to the UpWork dataset mentioned in[21,22].

This study proposes a framework called CTG-KrEW (Conditional Tabular GAN with K-Means Clustering and Word Embedding) to produce extensive datasets from a limited, small-scale dataset. TheCTG-KrEW’s generated content is in a tabular context and supports continuous, categorical, and sequenced words with the semantic integrity of their contextual context. Further, theCTG-KrEWis designed to be system-feasible, consuming far less CPU time and memory resources than the conventional approach.CTG-KrEWemploys the word2vec method[25]to transform individual words (e.g. skills) into vector representations, followed by K-Means clustering[26]to group them. In this study, we used task data from Kaggle’s repository[24]and worker data from the aforementioned research work of[21,22], and subsequently referred to them astask-dataandworker-datathroughout the article. The respective snapshots of the data sets are represented in Figures2and2.
Instead of employing a preexisting word2vec model, we opted to train our word2vec model directly from thetask-dataandworker-datato maintain the associations among the skills that pertain to a specific skill set. We also developed a web-based application namedKrewthat allows users to generate worker-profile and job-description-related tabular content with skill information of any magnitude. Krew is also freely accessible to the public.

While CTG-KrEW effectively addresses the challenges of generating realistic synthetic datasets for skill-oriented profiles, its utility extends far beyond this specific application. CTG-KrEW is designed to generate generic synthetic tabular data where contextually correlated words are crucial, making it a versatile tool for any domain that requires maintaining the semantic integrity and contextual coherence of tabular data attributes. In this study, we demonstrate its capabilities using skill-oriented datasets as an example, but the framework is broadly applicable to a wide range of scenarios where realistic, contextually aware synthetic data is needed.

The rest of the paper is structured in the following manner: Section2provides a concise overview of the existing literature related to the generation of synthetic tabular data using GAN models. Section3provides an overview of the dataset description and the challenges associated with generating synthetic tabular data of the same distribution. Section4presents the preliminaries required for this study. Section5delineates the CTG-KrEW framework. In Section6, data from experiments are presented to assess the framework’s efficacy. A description of the implementation details of the web-based application is given in Section7. Next is a separate discussion ( Section8) about the practical use cases of CTG-KrEW. The paper is concluded in Section9.

SECTION: 2Related Work

This section will provide a comprehensive review of the current literature on use cases in which synthetic tabular data has been generated using GAN-based approaches.

The authors of the paper[3], provide an extensive analysis of models based on the generative adversarial network (GAN) to synthesize tabular intrusion detection system (IDS) data. Specifically, the study uses CTGAN, TableGAN, and CopulaGAN on the widely recognized NSL-KDD dataset. Based on the analysis conducted, it can be inferred that TableGAN demonstrates satisfactory performance when handling continuous data. However, its effectiveness is limited in scenarios where discrete values are involved. In contrast, CTGAN and CopulaGAN exhibit satisfactory performance for data sets containing both continuous and discrete variables. Despite the promising capability of GAN models, they are vulnerable to various privacy attacks that could reveal information about individuals from the training data.
The authors of[11]have proposed DP-CTGAN to maintain data privacy while ensuring the accuracy of the generated data. This approach involves integrating differential privacy into a conditional tabular generative model. The authors employed nine authentic datasets, primarily sourced from the medical field, as a means of illustrating their use-case. In[12], the authors introduced GANBLR, a GAN model, which draws inspiration from the relationship between Naive Bayes and Logistic Regression. This model is designed to overcome the interpretation limitations of current tabular GAN-based models, while also enabling the explicit handling of feature interactions. In a recent publication[8], CTGAN is employed to produce a synthetic dataset for Attribute-based Access Control (ABAC). Additionally, the researchers have developed a software tool called ConGRASS, which facilitates the creation of extensive ABAC datasets.

However, to the best of our knowledge, none of the previously mentioned works has considered the generation of synthetic tabular datasets with attributes containing collections of words that are both highly correlated and contextually associated (similar to the skills in a skillset).CTG-KrEWstands as the pioneering effort in this domain.

SECTION: 3Dataset with Sequentially Correlated Word Attributes

The data used is in two main CSV files. Theworker-datafile, used in previous studies[21,27], contains worker profiles from UpWork. Thetask-datafile is sourced from Kaggle[24]and includes information on posted tasks on the UpWork platform. Both files feature a ‘skills’ column that lists the necessary skillsets for tasks or workers. These skills represent sequences of contextually correlated words capturing the entities’ competencies and cognitive qualities (refer to Table2). For instance, a ‘full-stack developer’ task might require the skillset ‘Java, JavaScript, and HTML’. Table1outlines the dataset attributes. Thetask-dataincludes categorical attributes like ‘skills’, client_location’, ‘experience’, and ‘project_type’, and a continuous ‘fixed_price’ attribute. Theworker-datahas categorical variables such as ‘worker_location’ and ‘Success_Rate’, and a continuous ‘Price’ variable.

The primary challenge arises in handling the ‘skills’. A simplistic approach would involve treating each unique entry in the ‘skills’ column as an independent discrete category. For instance, considering the previous example, a ‘full-stack-developer’ task requiring “Java, JavaScript, and HTML” would be treated as one category, while a ‘Data analyst’ task with skills “Python, R” would be treated as another separate category. It is known that datasets with columns having such categorical values are converted usingone-hot encodingby CTGAN. This may cause the CTGAN model to produce a considerable number of repetitive entries in the synthetic data. Consequently, skillsets such as “Java, JavaScript, and HTML” and “Python, R” may consistently appear as inseparable combinations, ultimately leading to a reduction in information variability and potentially causing an information imbalance within the generated data.

SECTION: 4Preliminaries

The primary challenge of this research is to generate content in a tabular context and support continuous, categorical, and sequenced words with semantic integrity. In this section, we will discuss the agenda and framework of CTG-KrEW.

Table2presents seven sample entities(which could be either tasks or workers). Each entitypossesses a set of skills, or more appropriately, askillsetand is represented by. For example, entityhas a skillset“Java, Javascript, HTML”. It is worth mentioning that the length of the skillsets may vary across different entities. Furthermore, two entities can possess either identical or disjoint skillsets. For instance, we observe that, but.

SECTION: 4.1Generic CTGAN Approach with Default Encoding

CTGAN’s default approach is to encode categorical values using aone-hot encodingtechnique. This involves identifying all the unique skillsets present in the dataset and creating binary columns for each distinct skillset. For instance, in Table2, there areunique skillsets. Consequently, 6 binary columns, denoted as, will be introduced. For a given entity, the value of its correspondingcolumn will be set to 1 if the respective skillsetrepresented byis possessed by. Conversely, all other binary columns associated with that entity will be assigned zero. Therefore, if the dimension of the source data frame in our cited example was, the dimension of CTGAN’s input data frame after the default transformation would be.

However, this default encoding scheme has limitations. One drawback is that it cannot generate a new set of correlated associative words (e.g. skillsets) for records that are not present in the source dataset. For instance, Table2, depicts that there are six unique skillsets. Thus, if we aim to generate 10 records, the maximum number of unique skillsets in the generated data will not exceed six. Hence, the synthetic data will have lots of redundancies and will have less diversity.

SECTION: 4.2Generic CTGAN Approach with Multi-hot Encoding

An alternative encoding approach calledmulti-hot encodingis proposed in place of the default one-hot encoding used in the generic CTGAN model. Multi-hot encoding (MHE) is a modification of one-hot encoding, except that the former employs a binary column for each unique word (i.e. skill) found in the dataset. Each binary column indicates the presence or absence of a specific skill within a skillset for the respective entity. Multi-hot encoding introduces high dimensionality to the data, particularly when dealing with a large number of unique words (i.e. skills). In the example, there are 9 unique skills: {C++, C, Java, HTML, Javascript, PHP, Node.js, Python, R} implying. However, MHE introduces the additional challenge of higher training time and memory consumption.

SECTION: 5Proposed CTG-KrEW Framework

CTG-KrEWpresents an innovative encoding technique that strives to incorporate greater variability in generating unique set of correlated and associative words (like skillsets) for synthetic data while simultaneously ensuring that the data dimensionality remains feasible for training CTGAN.

The data undergoes three main preprocessing steps before training CTGAN:unique skill (or word) identification, word2Vec encoding,andclustering. The K-means clustering algorithm[26]is utilised by specifying a user-defined hyperparameter, represented as, that establishes the intended number of cluster centres. In contrast to prior variations discussed in subsections4.1and4.2, inCTG-KrEW, new columns are introduced to represent the cluster IDs. Each’s skillsetis encoded so that the cluster-ID column is assigned the count of unique words (or skills) present in that specificcluster.

The word2vec[25]is employed for the encoding of the set of words (or skillset) before the clustering process. The word2vec methodology involves the transformation of individual skills into a vector representation. Rather than utilising any pre-trained word2vec model, we train the model directly using the providedtask-data. To construct the corpus utilised for training the
word2vec model, a distinct keyword (represented as) is incorporated both preceding and succeeding each skill within a given skillset. Table3displays the transformed corpus utilised to train the word2vec model, following the example illustrated in Table2.

Additionally, during the training of the word2vec model, a window size of unity is chosen. This ensures that skills within the same skill set are positioned closer to each other in terms of distance. As a result, the likelihood of these skills being assigned to the same cluster after the clustering process increases. This approach aims to preserve the associations among the skills belonging to a specific skillset. The algorithm1depicts the pseudo-code ofCTG-KrEWmethod.

Continuing with the previous example, after applying K-means clustering to the corpus presented in Table3, consisting of 9 unique skills, 4 clusters are obtained:(Python, R),(HTML, Javascript),(C++, C, Java), and(PHP, Node.js). The encoded clustered representation of these clusters is provided in Table4.

The analysis of Tables2and4reveals that in the case of, the three skills (C++, C, Java) are categorized under. Consequently, in Table4, the initial row aboutdisplays a value of three. Similarly, in terms of, HTML and JavaScript fall into the category of, while Java is classified under. Hence, the entry in Table4aboutis two, while that ofis one. This cluster-encoded representation form, as presented in Table4, is used as input for the training of our CTGAN model. Upon completion of the training process, the model can produce synthesized data, although in its encoded form. Therefore, to ensure that the synthesized data bears a resemblance to the source dataset adecodingfunction was incorporated into theCTG-KrEWframework.

During the encoding stage, alongside the allocation of cluster IDs to individual skills, probability values are also assigned to reflect the probability that a skill is present within a given cluster (see Steps 8 to 17 of Algorithm1). The decoding process uses these probability values linked to the cluster IDs to reconstruct the ‘skills’ column, thus mapping the cluster IDs to their respective skillsets. This process enables the reconstruction of the initial skillsets for each entity, ultimately allowing for the synthesised data to be converted from its encoded form to a format that resembles the source dataset. The data produced hold the original structure and integrity of the ‘skills’ attribute.

A visualization analysis (refer to Figures4and4) was performed to compare the source data set with the synthetically generated data produced by three variants. Our analysis centered on the values within the ‘skills column. The datasets possess a high-dimensional nature due to the variable number of unique skills they contain. To improve the clarity of the data, principal component analysis (PCA) was used to project the high-dimensional data onto a three-dimensional space[28,29]. To generate the three-dimensional projections, a sample size of 90 was used for each variant of the model, and the entities are randomly sampled from their respective data sets. Subsequently, PCA was employed on the source data to extract its fundamental structure and patterns. The same PCA transformation was then implemented on the synthesized data sets, thus aligning them with the established structure of the source dataset.

The generic CTGAN (refer subsection4.1) and CTGAN with MHE (refer subsection4.2) are considered the baselines for this study.

SECTION: 5.1Core Architecture of CTGAN

The implementation of CTGAN for all three variants was facilitated through the utilisation of the library offered bySDV-Synthetic Data Vault, an open-source ecosystem of libraries designed for synthetic data generation[30,6].

CTGAN utilises a conditional generator denoted as, whererepresents the conditional vector obtained through the one-hot encoding of discrete columns, andis a vector of random noise and a training-by-sampling technique to ensure equitable representation of feasible values for discrete attributes in the training stage.is designed to produce a replica of the conditional vector by introducing random noise. This is achieved by minimising the generator loss () by computing the cross-entropy between the input conditional vector and the generated vector. The discriminator () assesses the data samples(either real or generated) through Discriminator Loss () by evaluating the distance between the learned conditional distribution of the generated samples and the conditional distributions of real data. Fully connected networks are utilised in both the generator and the discriminator to capture correlations among columns.
The Generatorconsists of two fully connected hidden layers with batch normalization and ReLU activation functions:

whereandare weight matrices,andare bias vectors, andconcatenates the one-hot encoded conditional vectorand random noise. Synthetic data rowis generated using various activation functions, including scalar values from the hyperbolic tangent function (), mode indicators, and discrete values from the softmax function.

The utilisation of adversarial training methodology yields a generator that is capable of generating synthetic data through the implementation of Gumbel softmax. In contrast, the discriminator employs leaky ReLU functions and applies dropout regularisation to every hidden layer. The implementation of the PacGAN framework with 10 samples per pac is utilised as a measure to mitigate mode collapse.

The loss functions for training are defined as follows:

The training of the model is carried out by utilising the Wasserstein loss function, which is augmented with a gradient penalty.typically represents real data samples drawn from the true data distributionandrepresents generated data samples produced by the generator. Additionally, the Adam optimizer is utilised with a learning rate of.

Figure5outlines the CTG-KrEW workflow, beginning with data preprocessing, where unique skills are identified, encoded using word2vec, and clustered via K-Means. The CTGAN model then trains on this processed data to generate synthetic datasets that maintain the original data’s contextual integrity. The synthetic data is then decoded back into its original format. Finally, the KrEW application, deployed on a server, enables users to generate and download these datasets at any scale.

SECTION: 6Evaluation

The proposedCTG-KrEWand the baselines will be compared for their effectiveness using statistical and visual metrics in this study. The subsequent are the evaluation criteria with related metrics.

(i)Skillset variability:In order to capture the variability or randomness of theskillsetvalues in the synthetic dataset,Entropyis used.
The concept of entropy in information theory[31,32]pertains to the mean degree ofinformationoruncertaintyintrinsic to the potential outcomes of a random variable.

The distinctive skillsets are identified and their respective frequencies are recorded from both the source and synthetic datasets. As demonstrated in Table2, the count of distinct sets of skills is. Subsequently, Table5is generated for the skillsets. The formula for the entropy of a discrete random variable X, which is defined over a set of valuesand follows a distribution functiongiven by:

is the normalised frequency of distinct skillsets. As the value ofincreases, the variability of skillsets also increases.

The present investigation concerns the calculation of entropy for the synthetic datasets produced by each of the three variants. A significant limitation of the generic CTGAN model, when employed to generate skillsets or collections of associated skills is the introduction of repetitive structured skillsets. As a result, the information entropy, as shown in FiguresLABEL:entropy-tandLABEL:entropy-w, is the lowest. It is noteworthy that since the sourcetask-dataconsisted of only 297 unique skillsets, even when generating datasets of size 10k, only 297 distinct combinations of skillsets could be produced, leading to a significant repetition of skillsets. In contrast, the other two methods performed much better in terms of skillset variability for bothtask-dataandworker-data. As a consequence of this rationale, we have concentrated our analysis exclusively on the comparative efficacy of the CTGAN model with multi-hot encoding (abbreviated as CTGAN-MHE) andCTG-KrEW

(ii)Skillset matching:The introduction of theskillset matchingparameter aims to effectively capture the contextual similarity among skills within a given skillset while considering the interrelationships between these skills and the overall structure of the skillset as a cohesive unit. The study employed the utilisation of aSentence Transformer[33], a specific model of natural language processing (NLP) that encodes sentences into vectors or embeddings of fixed dimensions, thereby capturing the semantic meaning of the sentences.

We employed a pre-trained BERT-based sentence embedding model from the Sentence-Transformer library[34]to calculate thecosine similarity scorebetween the skillsets. The process involves comparing the similarity of each encoded skillset record in the synthetic dataset with every skillset record in the source dataset and selecting the highest similarity value as skillset matching score. As an illustration, let us consider two skillsets extracted from the synthetic dataset, denoted as(Java, C++) and(PHP, C++).

Upon computing the similarity scores betweenandwith the 7 skillset records presented in Table2, we obtain the following values: [0.985,0.847,0.845 ,0.817,0.853 ,0.568,0.847] and [0.896, 0.776, 0.761, 0.840, 0.874, 0.642, 0.776], respectively. It is observed that the maximum value foris 0.985, while the maximum value foris 0.896. These results indicate that (Java, C++) exhibits a higher degree of semantic similarity to (C++, C, Java) compared to (PHP, C++). Drawing upon this concept, we endeavoured to capture the contextual similarity between the skillsets of the source and synthetic domains. The mean value of all the scores is utilised for comparison. Box plots illustrate depicted in FiguresLABEL:skill_match-tandLABEL:skill_match-w. indicate that the average skillset matching score of CTGAN-MHE exhibited superior performance compared toCTG-KrEWabout both thetask-dataandworker-data.

(iii)Similarity of the skills distribution:To assess the likelihood that the skills present in the synthetic dataset exhibit a frequency distribution that closely aligns with the skills in the original dataset,KL divergence metric[35]is used. KL or Kullback–Leibler divergence metric, denoted as, is a statistical measure that represents a quantification of the dissimilarity between a given probability distributionand a reference probability distribution.

To compute the KL divergence of the dataset, we adopt a methodology similar to that employed for assessing the variability of the skillset. However, instead of determining the frequency of distinct skillsets, we ascertain the frequency of unique skills across all the skillsets. As demonstrated in Table2, the number of distinct skillsets was, while the count of the number of unique skills wasand thus generated Table6displays the frequency of the nine skills.

cccccccccc
&C++CJavaHTMLJavaScriptPHPNode.jsPythonR{block}c(ccccccccc)C++0  1  1  0  0  0  0  0  0C1  0  1  0  0  0  0  0  0Java1  1  0  1  1  1  1  0 0HTML0  0  1  0  3  1  0  0 0JavaScript0  0  1  3  0  1  0  0 0PHP0  0  1  1  1  0  1  0 0Node.js0  0  1  0  0  1  0  0 0Python0  0  0  0  0  0  0  0 1R0  0  0  0  0  0  0  1 0

Therefore, by referring to Table6, we obtain two distributions:, representing the normalized frequency of the source dataset, and, representing the normalized frequency of the synthetic dataset. Subsequently, we calculateusing the following formula:

wheredenotes the collection of all conceivable values of the random variable under examination. The unbounded nature of the KL divergence signifies that it can assume any non-negative value or even tend towards infinity. However, a KL divergence value of zero indicates that the two distributions being compared possess equivalent amounts of information.

FiguresLABEL:kl-tandLABEL:kl-windicate that the KL divergence score is lowest forCTG-KrEWin comparison to CTGAN-MHE which suggests that the synthetic data generated byCTG-KrEWexhibits the highest degree of similarity to the frequency distribution of the ‘skill’ attribute in the original dataset. This similarity applies to both the categories oftask-dataandworker-data. Moreover, as the size of the sample data increases, the KL divergence score decreases specifically for theworker-data. The observed incongruity can be attributed to the fact that the initial size of theworker-datawas notably larger than that of thetask-data, leading to increased diversity among the distinct skills (as well as other attributes).

(iv)Associativity among the skills:Maintaining the associativity between the skills presented posed a persistent challenge that remained unresolved by the generic CTGAN and CTGAN-MHE approaches. The development ofCTG-KrEWwas significantly influenced by this particular challenge.

To ensure that the skillsets produced are consistent with practical requirements and adequately maintain the joint distribution of the ‘skill’ column with other attributes in the synthetic dataset, it is imperative to remain attentive to the associativity or co-occurrence of individual skills within their respective skill sets. Consequently, thePearson correlation coefficient[36]metric is used to capture the associativity between skills that co-occur within skillsets. To find the Pearson’s correlation coefficient () in our datasets, both source and synthetic, an association matrix of sizeis first built, whereis the number of unique skills in the data set under consideration. For example, from Table2, the association matrix (see Table7) can be formed.

In Table7, the value in cellrepresents the number of times that skillco-occurs with skillin all skill sets and vice versa. If skillnever cooccurs with skill, then cellcontains zero. Thismatrix is then normalized and transformed into avector. A similar vector is created for the skills in the synthetic dataset. For a pair of random variables, the formula for the Pearson correlation coefficient is given by:

wheredenotes the covariance,is the standard deviation of, andis the standard deviation of. In this context,andcorrespond to the associative vectors of the source and synthetic datasets, respectively.

The results depicted in FiguresLABEL:rho-tandLABEL:rho-windicate that forCTG-KrEW, there is a greater degree of similarity in associativity among the individual skills within their respective skillsets when comparing the source and synthetic datasets, concerning CTGAN-MHE.

(v)System feasibility:For this analysis, we conducted a comparison of the variants based on theirmemory usageandtraining timeconsumption. It should be noted that after the training and deployment of the models, the process of generating a dataset comprising even 10,000 records is relatively quick, regardless of the variant utilized.

The system feasibility results are depicted in Figures11and 9, which include 350 epochs of training. Thememory profilerlibrary for Python was used[37]. Memory usage and training time for the different variants usingtask dataare depicted in Figure11, while Figure 11 shows the corresponding metrics forworker data. The empirical findings indicate that the training time required for CTGAN-MHE to complete 305 epochs is more than 300X longer than that ofCTG-KrEW. Furthermore, theCTG-KrEWmodel exhibits better memory consumption performance.

(vi)Quality of the data related to the remaining attributes:

The quality of the attributes of bothtask-dataandworker-data, excluding the ‘skills’ column, has been illustrated in Figures 12 and Figure13, respectively.

All other attributes are categorical, except ‘fixed_price’ and ‘success_rate’. The frequency of occurrence for each category or value in each attribute is calculated to compare the synthetic datasets produced by theCTG-KrEWmodel with the source datasets. This analysis was performed on synthetic datasets consisting of 1000 tuples. We created fixed-size bins for the values ‘fixed_price’ and ‘success_rate’ and generated frequency histograms to show the distribution of values in each bin. The frequency counts of each characteristic are normalised to the intervalto offer a clearer visualisation. The illustrations show that the patterns seen in the source data closely resembled the frequency distribution of attribute values in the synthetic data produced.

SECTION: 7KrEW Web Application

This section provides an overview of the procedural and technical aspects of implementing and using theKrEWapplication.CTG-KrEW, recognized for its superior performance compared to baselines, utilizes its pre-trained model within the application. The model is trained on real datasets by the application’s owner or administrator usingCTG-KrEW. Post-training, the model weights are stored in a pickle file, which is then loaded onto the application server.CTG-KrEWorganizes the initial dataset into distinct clusters, and training is conducted solely on attributes within these clusters. Consequently, any synthetic data generated byCTG-KrEWfollows this clustered format, necessitating a conversion back to the original raw form. The application manages these background tasks, abstracting complexities from end-users. Users interact with the application by selecting the dataset, the type of data (worker, task, or both) and the desired sample size through the application interface. Upon clicking “Download,” the request is sent to the server, where samples are generated using the pre-trained model. Upon completion, a CSV file containing the data is available for download to the user’s device. Users can generate and retrieve multiple samples as needed. The application server is deployed using Flask, a Python microweb framework, with the user interface developed using HTML, CSS, and
JavaScript. Currently, only the UpWork dataset is integrated, but the administrator can incorporate additional real datasets for training theCTG-KrEWmodel. Users can choose to extract tasks-related, worker-related, or both types of information based on their requirements. For reference, Figure14provides a snapshot of the KrEW application, accessible to users through the linkhttps://riyasamanta.github.io/krew.html.

SECTION: 8Discussion and Use-Cases

The CTG-KrEW framework introduces a novel approach to generating synthetic tabular data by addressing critical challenges in maintaining the semantic integrity of contextually associated word sequences. The integration of word2vec with K-Means clustering allows CTG-KrEW to preserve meaningful relationships within the data, ensuring that the generated synthetic datasets are both realistic and contextually accurate. While initially designed for skill-oriented datasets, CTG-KrEW’s capabilities extend far beyond this specific application, making it a versatile tool for a wide range of data generation tasks. Some of the practical uses cases of CTG-KrEW’s application are:

Customer Review Synthesis:CTG-KrEW can generate synthetic datasets of customer reviews, preserving the contextually linked sequences of product features, customer sentiments, and usage scenarios. This synthetic data can be used for training sentiment analysis models or enhancing recommendation systems without compromising real customer data.

Medical Report Generation:In healthcare, CTG-KrEW can be used to generate synthetic medical reports, maintaining the associations between symptoms, diagnoses, and treatment plans, along with patient demographics and lab results. This enables the development and testing of predictive models for disease diagnosis and treatment planning while ensuring patient privacy.

Legal Document Synthesis:CTG-KrEW can generate synthetic legal documents, such as contracts or case briefs, preserving the contextual relationships between legal terms, clauses, and conditions. This data can be used to train NLP models for legal text analysis or contract clause identification, aiding in the development of legal tech tools without risking confidentiality.

Financial Data Generation:In the finance sector, CTG-KrEW can be employed to generate synthetic financial datasets that maintain the relationships between transaction descriptions, account types, and transaction amounts. This data is invaluable for developing models for fraud detection, risk assessment, or financial forecasting, all while keeping real financial data secure.

SECTION: 9Conclusion

The limitations of the generic CTGAN model, particularly in handling contextually associated word sequences and its high computational demands, prompted the development of the CTG-KrEW framework. By incorporating three key preprocessing steps, unique skill identification, word2vec encoding, and K-Means clustering—CTG-KrEW enhances the core CTGAN’s ability to generate realistic synthetic tabular data. Experimental evaluations demonstrate that CTG-KrEW not only outperforms baseline methods but also significantly reduces memory usage and CPU time, making it a highly efficient solution.

Moreover, the development of the KrEW web-application underscores the practical utility of this framework, providing users with a freely accessible platform to generate synthetic data at any scale, tailored to the specific features of the training datasets. While this study primarily focused on skill-oriented datasets, CTG-KrEW’s versatility extends far beyond this niche, offering robust capabilities for generating a wide range of categorical, numeric, and contextually associated data types.

Looking ahead, the framework’s potential for broader application is immense. Future research will focus on expanding CTG-KrEW to support more complex and contextually nuanced data, such as phrases with specific tones, and exploring its application across various domains, thereby further solidifying its role as a comprehensive solution for synthetic data generation.

SECTION: Competing Interest

The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this article.

SECTION: Funding

No funding has been received for this research work.

SECTION: Data availability

The datasets used or analyzed during the study are available from the corresponding author upon reasonable request.

SECTION: References