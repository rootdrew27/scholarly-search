SECTION: Chain-linked Multiple Matrix Integration via Embedding Alignment

Motivated by the increasing demand for multi-source data integration in various scientific fields, in this paper we study matrix completion in scenarios where the data exhibits certain block-wise missing structures – specifically, where only a few noisy submatrices representing (overlapping) parts of the full matrix are available.
We propose the Chain-linked Multiple Matrix Integration (CMMI)
procedure to efficiently combine the information that can be extracted from these individual noisy submatrices.
CMMI begins by deriving entity embeddings for each observed submatrix, then aligns these embeddings using
overlapping entities between pairs of submatrices, and finally aggregates them to reconstruct the entire matrix of interest.
We establish, under mild regularity conditions, entrywise error bounds and normal approximations for the CMMI estimates.
Simulation studies and real data applications show that CMMI is computationally efficient and effective
in recovering the full matrix, even when overlaps between the observed submatrices are minimal.

Keywords:norm, normal approximations, matrix completion, data integration

SECTION: 1Introduction

The development of large-scale data collection and sharing has sparked considerable research interests in integrating data from diverse sources to efficiently uncover underlying signals. This problem is especially pertinent in fields such as healthcare research(Hong et al.,2021; Zhou et al.,2023),
genomic data integration(Maneck et al.,2011; Tseng et al.,2015; Cai et al.,2016),
single-cell data integration(Stuart et al.,2019; Ma et al.,2024),
and chemometrics(Mishra et al.,2021).
In this paper we consider a formulation of the problem where each sourcecorresponds to a partially observed submatrixof some matrix, and the goal is to integrate theseto recoveras accurately as possible.

As a first motivating example, consider pointwise mutual information (PMI) constructed from different electronic healthcare records (EHR) datasets.
PMI quantities the association between a pair of clinical concepts, and matrices representing these associations can be derived from co-occurrence summaries of various EHR datasets(Ahuja et al.,2020; Zhou et al.,2022).
However, due to the lack of interoperability across healthcare systems(Rajkomar et al.,2018), different EHR data often involve non-identical concepts with limited overlap, resulting in substantial differences among their PMI matrices.
The analysis of PMI matrices from different EHR datasets can thus be viewed as a
multi-source matrix integration problem. Specifically, letrepresent some concept set and
suppose there is a symmetric PMI matrixassociated with, where. For theth EHR, we denote its clinical concept byand let.
The PMI matrix derived from theth EHR,, then corresponds to the
principal submatrix ofassociated with. As it is often the case that the union of all the entries inconstitutes only a strict subset of those in, our aim is to integrate theseto recover the unobserved entries in.

For another example, consider single-cell matrix data where rows represent genomic features, columns represent cells, and each entry records some specific information about a feature in the corresponding cell.
A key challenge in the joint analysis for this type of data is to devise efficient computational strategies to integrate different data modalities(Ma et al.,2020; Lähnemann et al.,2020), as the experimental design
may lead to a collection of single-cell data matrices for different, but potentially overlapping, sets of cells and features. More specifically,
letbe the population matrix for all involved features and cells where(withanddenoting the sets of genomic features and cells, respectively). Each single-cell data matrixis then a submatrix ofcorresponding to someand; here we denoteand. Our aim is once again to integrate the collection ofto reconstruct the original.

The above examples involving EHR and single-cell data are special cases of the matrix completion with noise and block-wise missing structures.
However, the existing literature on matrix completion mainly focuses on recovering a possibly low-rank matrix based on uniformly sampled observed entries
or independently sampled observed entries which may be contaminated by noise; see, e.g.,Candès and Tao (2010); Candes and Recht (2012); Cai et al. (2010); Candes and Plan (2011); Koltchinskii et al. (2011); Tanner and Wei (2013); Chen et al. (2019); Fornasier et al. (2011); Mohan and Fazel (2012); Lee and Bresler (2010); Vandereycken (2013); Hu et al. (2012); Sun and Luo (2016); Cho et al. (2017); Chen et al. (2020); Yan et al. (2024); Srebro and Salakhutdinov (2010); Foygel et al. (2011); Cai and Zhou (2016); Keshavan et al. (2010)for an incomplete list of references.

These assumptions of uniform or independent sampling in standard matrix completion models are generally violated in applications of matrix integration, thus necessitating the development of efficient methods for tackling the block-wise missing structures.
Some examples of this development include the generalized integrative principal component analysis (GIPCA) ofZhu et al. (2020), structured matrix completion (SMC) ofCai et al. (2016), block-wise overlapping noisy matrix integration (BONMI) ofZhou et al. (2023), and symmetric positive semidefinite matrix completion (SPSMC) ofBishop and Yu (2014).
The GIPCA procedure operates under the setting where each data matrix have some common samples and completely different variables, and furthermore assumes that each entry in these matrices are from some exponential family of distribution, with entries in the same matrix having the same distributional form.
SMC is a spectral procedure for recovering the missing block of an approximately low-rank matrix when a subset of the rows and columns are observed; thus, SMC is designed to impute only a single missing block at a time.
BONMI is also a spectral procedure for recovering a missing block (or submatrix) in an approximately low-rank matrix but, in contrast to SMC,
assumes that this missing block is associated with a givenpairof observed submatrices that share some (limited) overlap.
SPSMC has a similar spectral procedure with BONMI to recover a low-rank symmetric positive semidefinite matrix using some observed principal submatrices. While BONMI combines submatrices pair by pair, SPSMC sequentially integrates each new submatrix with the combined structure formed by all previously integrated submatrices.
The key idea behind BONMI and SPSMC is to align (via an orthogonal transformation) the spectral embeddings given by the leading (scaled) eigenvectors of the two overlapping submatrices and then impute the missing block by taking the outer product of these aligned embeddings.
The use of embedding alignments also appeared in other applications including bilingual dictionary induction(Kementchedjhieva et al.,2018), knowledge graphs integration(Lin et al.,2019; Fanourakis et al.,2023), and vertex nominations(Zheng et al.,2022).

In this paper, we extend the BONMI procedure, which handles only two overlapping submatrices, tosubmatrices and propose the Chain-linked Multiple Matrix Integration (CMMI) for more efficient and flexible matrix completion. As a motivating example, suppose we have two overlapping pairs of submatricesand. Using the overlapping entries betweenand(resp.and) we can find an orthogonal transformation(resp.) to align the embeddingsand(resp.and). Then by combiningand, we can also aligntoand recover the missing block associated withandeven when these submatrices arenon-overlapping. Generalizing this observation we can show that as long asareconnectedthen we can integrate them simultaneously to recover all the missing entries; here two submatricesandare said to be connected if there exists a sequencewith,such thatandare overlapping for all.
The use of CMMI thus enables the recovery of many missing blocks that are unrecoverable by BONMI and furthermore allows for significantly smaller overlap between the observed submatrices. CMMI considers all possible overlapping pairs without relying on the integration order of submatrices, unlike SPSMC, enabling a more optimal recovery result.

The structure of our paper is as follows. In Section2we introduce the model for multiple observed principal submatrices of a whole symmetric positive semi-definite matrix, and propose CMMI to integrate a chain of connected overlapping submatrices.
Theoretical results for our CMMI procedures are presented in Section3.
In particular we derive error bounds in two-to-infinity norm for the spectral embedding of the submatrices and entrywise error bound for the recovered entries. Using these error bounds we show that our recovered entries are approximately normally distributed around their true values and that our algorithm yields consistent estimate even when there are only minimal overlap between the submatrices. We emphasize that the results in Section3also hold for BONMI (which is a special case of our results for) and SPSMC, thereby providing significant refinements over those inZhou et al. (2023)andBishop and Yu (2014), which mainly focus on bounding the spectral or Frobenius norm errors of the missing block and embeddings.
And our analysis handles both noisy and missing entries in the observed submatrices whileZhou et al. (2023)andBishop and Yu (2014)only consider the case of noisy entries.
Numerical simulations and experiments on real data are presented in Sections4and5. In Section6, we extend our embedding alignment approach to the cases of symmetric indefinite matrices and asymmetric or rectangular matrices.
Detailed proofs of stated results are provided in the supplementary material.
SectionAin the supplementary material explores more complex matrix integration challenges, such as scenarios where the connected submatrices do not form a single chain but rather multiple chains with possibly quite different lengths and thus we need to select a suitably optimal chain among these candidates. The theoretical results in Section3allows us to develop several effective strategies for addressing these issues.

SECTION: 1.1Notations

We summarize some notations used in this paper.
For any positive integer, we denote bythe set.
For two non-negative sequencesand, we write(resp.) if there exists some constantsuch that(resp.) for all, and we writeifand.
The notation(resp.) means that there exists some sufficiently small (resp. large) constantsuch that(resp.).
Ifstays bounded away from, we writeand, and we use the notationto indicate thatand.
If, we writeand.
We say a sequence of eventsholds with high probability if for anythere exists a finite constantdepending only onsuch thatfor all.
We write(resp.) to denote that(resp.) holds with high probability.
We denote bythe set oforthogonal
matrices.
For any matrixand index sets,, we denote bythe submatrix offormed from rowsand columns, and we denote bythe submatrix ofconsisting of the rows indexed by. The Hadamard or entrywise product between two conformal matricesandis denoted by.
Given a matrix, we denote
its spectral, Frobenius, and infinity norms by,, and, respectively.
We also denote the maximum entry (in modulus) ofbyand thenorm ofby

wheredenotes theth row of, i.e.,is the maximum of thenorms of the rows of. We note that thenorm isnotsub-multiplicative. However, for any matricesandof conformal dimensions, we have

see Proposition 6.5 inCape et al. (2019). Perturbation bounds using thenorm for the eigenvectors and/or
singular vectors of a noisily observed matrix had recently
attracted interests from the statistics community, see e.g.,Chen et al. (2021); Cape et al. (2019); Fan et al. (2018); Abbe et al. (2020)and the references therein.

SECTION: 2Methodology

We are interested in an unobserved population matrix associated withentities denoted by. We assumeis positive semi-definite with rank; extensions to the case of symmetric but indefiniteas well as asymmetric or rectangularare discussed in Section6. Denote the eigen-decomposition ofas,
whereis a diagonal matrix whose diagonal entries are the non-zero eigenvalues ofin descending order, and the orthonormal columns ofconstitute the corresponding eigenvectors.
The latent positions associated to the entities are given byand any entry incan be written as the inner product of these latent positions, i.e.,so thatfor any, whereanddenote theth andth row of, respectively.

We assume that the entries ofare only partially observed, and furthermore, that the observed entries can be grouped into blocks. More specifically, suppose that we havesources and
for anywe denote the index set of the entities contained in theth source by.
For ease of exposition we also requirefor allas otherwise there exists somesuch that it is impossible to integrate observations fromwith those from. We denoteand the population matrix for theth source by. We then have

whereis the submatrix offormed from rows and columns in,contains the rows ofin, andcontains the latent positions of.

We also allow for missing and corrupted observations in each source, i.e., for theth source we only get to observefor allHereindicates the indices of the observed entries andrepresent the random noise.
In particularandare both symmetric, and we assume the upper triangular entries ofare i.i.d. Bernoulli random variables with success probabilitywhile
the upper triangular entries ofare independent, mean-zero sub-Gaussian random variables
with Orlicz-2 norm bounded by.
For this model, the matrix

is an unbiased estimate of, and thus a natural idea is to use the scaled leading eigenvectorsas an estimate for, whereandcontain the leading eigenvalues and the leading eigenvectors of, respectively.
We now propose an algorithm to integrate and alignfor recovery of the unobserved entries in.

SECTION: 2.1Motivation of the algorithm

We first summarize the BONMI algorithm ofZhou et al. (2023). We start with thenoiselesscase for overlapping submatricesandto illustrate the main ideas. Our goal is to recover the unobserved entries in the white block of Figure1; this is part of.

Based onandwe can obtain latent position estimates for entities inand, which we denote asand.
Next note that

and hence there existssuch that

Eq. (2.2) then implies

where, and thus we only needto recover.

Note that for entities in, we have two equivalent representations of their latent positions. More specifically, letandbe the rows ofandcorresponding to entities in. Then by
Eq. (2.2) we haveand thuscan be obtained by aligningand.
The resultingis unique
whenever.

The same approach also extends to the case where theandare partially and noisily observed. More specifically, suppose
we observeandas defined in Eq. (2.1). We then obtain estimated latent positionsforandforfromand, respectively. To alignand, we solve the orthogonal Procrustes problem

and then estimate the unobserved block as part of

SECTION: 2.2Chain-linked Multiple Matrix Integration (CMMI)

We now extend the ideas in Section2.1to a chain of overlapping submatrices.
Suppose our goal is to recover the entries in the yellow block of Figure2, given a collectionsuch thatfor all.
Then for each pair, we align the estimated latent position matricesandby solving the orthogonal Procrustes problem

Note that the solution of the orthogonal Procrustes problem between matricesandis given bywhereandcontain the left and right singular vectors of, respecitvely; seeSchönenmann (1966).andcan be aligned by combining thesewhich then yields

as an estimate for. See Algorithm1for more details.

For, obtain estimated latent position matrix for, denoted by, whereand the diagonal matrixcontain theleading eigenvectors and eigenvalues of, respectively.

For, obtainby solving the orthogonal Procrustes problem

Compute.

Compared to BONMI inZhou et al. (2023), which handles only two submatrices, our proposed CMMI can combine allconnectedsubmatrices, where two submatricesandare said to be connected if there exists a path of overlapping submatrices between them. Indeed, for the example in Figure2, BONMI can only recover the entries associated with pairs of overlapping submatrices, namely, while CMMI can recover the whole matrix. In general, BONMI only recoversfraction of the entries recoverable by CMMI.
Moreover, our theoretical results indicate that increasinghas a minimal effect on the estimation error of CMMI (see Theorem2), and
simulations and real data experiments in Sections4and5show that accurate recovery is possible even when.
Our theoretical results also show that CMMI requires only minimal overlap betweenand, e.g.,can be as small as, the embedding dimension of; see Remark6and Section4.3for further discussion and experimental results.

For more general cases encountered in practice, there
may exist multiple chains to recover a given unobserved entry.
We will present in SectionAof the supplementary material several strategies to select and/or combine these chains.
Although in certain cases, such as when considering a simple chain, CMMI is identical to the sequential integration approach SPSMC inBishop and Yu (2014), CMMI offers a more optimal strategy in more complex scenarios by considering all overlapping pairs. In contrast, the restriction to sequential integration imposes limitations on SPSMC, and how to determine a valid and effective sequential integration order is an unresolved issue inBishop and Yu (2014). Furthermore, our theoretical results are significantly stronger than those inBishop and Yu (2014)andZhou et al. (2023); see Section3.1for further discussion.

SECTION: 3Theoretical Results

We now present theoretical guarantees for the estimateobtained by Algorithm1.
We shall make the following assumptions on the underlying population matricesfor the observed blocks.
We emphasize that, because our results address either large-sample approximations or limiting distributions, these assumptions should be interpreted in the regime whereis arbitrarily large and/or.

For each, the following conditions hold for sufficiently large.

We have. Letanddenote the largest and smallest non-zero eigenvalues of, and letcontain the eigenvectors corresponding to all non-zero eigenvalues.
We then assume

for some finite constant.

whereis a symmetric matrix whose (upper triangular) entries are independent mean-zero sub-Gaussian random variables with Orlicz-2 norm bounded byandis a symmetric binary matrix whose (upper triangular) entries are i.i.d. Bernoulli random variables with success probability.

Denote

We supposeand

For the entire population matrix, we have. Letanddenote the largest and smallest non-zero eigenvalues of, and letcontain the eigenvectors corresponding to non-zero eigenvalues. Suppose (1)has bounded condition number, i.e.,for some constant, andhas bounded coherence, i.e.,; (2) for each,are drawn uniformly at random from. Then

with high probability (LemmaD.11) and Eq. (3.1) holds.

We first consider the case where we only have two overlapping submatricesand.
Theorem1presents an expansion for.

Letandbe overlapping submatrices satisfying Assumption1. For their overlap, suppose, and define

Letfor any. We then have

whereandare random matrices satisfying

with high probability. Furthermore suppose

Thenis the dominant term and

with high probability.

The expansion in Eq. (3.5) consists of four terms, with the first two terms being linear transformations of the additive noise matricesand. The third termcorresponds to second-order estimation errors forand, and hence Eq. (3.6) only depends
on quantities associated withand. Finallycorresponds to the error when aligning the overlapsandand hence Eq. (3.7) depends on.

Zhou et al. (2023)requiresso that the number of overlapping entities must grow
with the sizes of the submatrices. However, our derivations of
Theorem1show that the overlap can be bounded or even
be as small as(the rank of), which is the minimum value
required to align the embeddings in; see Remark6and
Section4.3for more detailed discussion and
simulations verifying this result.

Next we consider a chain of overlapping submatrices as described in Algorithm1. Theorem2presents the expansion of the
estimation error forand
Theorem3leverages this expansion to derive an
entrywise normal approximation for. While the statements of Theorem2and Theorem3appear somewhat complicated at first glance, this is intentional as they make the results more general and thus applicable to a wider range of settings.
Indeed, we allow forto have different magnitudes as well as the overlaps
to be of very different sizes. For example we can havebutwhilebut.
Ifthen these results can be simplified considerably; see Remark6.

Consider a chain of overlapping submatricessatisfying Assumption1.
For all overlaps, suppose,
and defineas in Eq. (3.4).
Letfor all. We then have

whereandare random matrices satisfying

with high probability. Furthermore suppose

Thenis the dominant term and

with high probability.

We note that the only difference between Theorem1and Theorem2is in the upper bound forcompared to that for. Indeed,andin Theorem2only depend onand, but not on the chain linking them, and thus their upper bounds are the same as that in Theorem1forand. In contrast, from our discussion in Remark2, the termcorresponds to the alignment error betweenand. Asandneed not share any overlap, this alignment is obtained via a sequence of orthogonal Procrustes transformations betweenandfor. The accumulated error for these transformations is reflected in the termand depends on the whole chain.
When the length of the chainis not too large relative to the overlaps, the error inis negligible compared to that in the dominant term, and consequently, our entrywise error rate depends only onand, rather than on the chain linking them.

Consider the setting of Theorem2. For, letbe amatrix whose entries are of the form

Defineas

For any fixed, defineas

Furthermore, denote

whereanddenote the-th row and-th row ofandrespectively.
Note that.
Suppose the following conditions

are satisfied, whereandare upper bounds forandgiven in Theorem2.
We then have

as.

Ifthen all terms depending onare dropped from these conditions. Specifically,in Assumption1simplifies to, and
the condition in Eq. (3.10) simplifies tofor.
Ifthen all terms depending onare also dropped. Specifically, the condition in Eq. (3.10) simplifies to.

Using Theorem3we can also construct aconfidence interval foraswheredenotes the upperquantile of the standard normal distribution andis a consistent estimate ofbased on

withand;
we leave the details to the interested readers.

We now provide an example to illustrate the above results.
We first assumeare of the same order, i.e., there exists anwithfor all. We also assume,for all.
We further supposehasentries that are lower bounded by some constantnot depending on. Then, asis also low-rank with bounded condition number, we have, and thus.
By Eq. (3.3) we have,
sofor all.
For the overlaps we assumeand.
Under this setting, the condition in Eq (3.2) simplifies to

the error bounds in Eq. (3.8) of Theorem2simplify to

with high probability. Furthermore we also have

with high probability, provided that

All conditions are then trivially satisfied and the estimate error converges towhenever,, and.
Note that the overlap sizecan be as small as the minimal.

For the normal approximation in Theorem3, the condition in Eq. (3.10) is trivial, and the condition in Eq. (3.11) simplifies to

Note thatis bounded whenfor some constant(this condition is dropped when). Then Eq. (3.13) is satisfied whenand, i.e. the overlap size is slightly larger than the minimal overlap.

SECTION: 3.1Comparison with related work

As mentioned in Section2, our theoretical results are comparable toZhou et al. (2023)for two observed submatrices and toBishop and Yu (2014)for a simple chain of observed submatrices.
In particular, while the error bounds inZhou et al. (2023)are in the spectral norm and those inBishop and Yu (2014)are in the Frobenius norm, our error bounds are in the maximum entrywise norm and allow for heterogeneity among blocks.
In additional, the error bound in Theorem 4 ofBishop and Yu (2014)grows exponentially with the length of the chain (due to its dependency onwhereis the chain length), whereas our error bound forincludes only a non-dominant term that grows linearly with the chain length; see Eq. (3.8) or Eq. (3.12).
And the bound in Theorem 4 ofBishop and Yu (2014)is only applicable for smallwhere, whereas our noise model allowsto be of orderwithof order, rendering their result ineffective.

We emphasize that the block-wise observation models in this paper, BONMI(Zhou et al.,2023)and SPSMC(Bishop and Yu,2014)differ significantly from those in the standard matrix completion literature, which typically focuses on a single large matrix and assumes uniformly or independently sampled observed entries.
Nevertheless, the authors of BONMI have compared their results with other results in the standard matrix completion literature. For example, Remark 10 inZhou et al. (2023)shows that the upper bound for the spectral norm error of
BONMI matches the minimax rate for the missing at random setting.
As CMMI is an extension of BONMI to more thanmatrices, the above comparison is still valid.
Furthermore, our results for CMMI are in terms of the maximum entrywise norm and normal approximations, which are significant refinements of the spectral norm error in BONMI, and are thus also comparable with the best available results for standard matrix completion such as those inAbbe et al. (2020)andChen et al. (2021). More specifically, consider the case ofwith. Also supposeand. Then CMMI has the maximum entrywise error bound of, which matches the rate in Theorem 3.4 ofAbbe et al. (2020)and Theorem 4.5 ofChen et al. (2021)up to a factor of, as the number
of observed entries in our model is onlytimes that for the standard matrix completion models.
Finally the normal approximation result in Theorem3is analogous to Theorem 4.12 inChen et al. (2021), with the main difference being the expression for the normalizing variance as our model considers individual noise matricesandwhereasChen et al. (2021)consider a global noise matrix(which includesandas submatrices).

Another related work isChang et al. (2022)which considers matrix completion for sample covariance matrices with a spiked covariance structure. Sample covariance matrices differ somewhat from the data matrices considered in our paper as, while both our population data matrixin Section2and their population covariance matrixare positive semidefinite, the entries of the sample covariance matrixaredependent. Consequently, the settings in the two papers are related but not directly comparable.
Nevertheless, if we were to compare our results against Theorem C.1 inChang et al. (2022)(where we setin our model, asChang et al. (2022)assume that the sample covariance submatrices are observed completely) then (1) we allow block sizes(theirsare our) to differ significantly in magnitude; (2) more importantly, our error bounds depend at most linearly on the chain length, whereas their bounds grow exponentially with the chain length due to the dependency on, (theiris our). As(see Proposition C.2 inChang et al. (2022)), this results in a factor ofthat is highly undesirable asincreases.

SECTION: 4Simulation Experiments

We now present simulation experiments to complement our theoretical results and compare the performance of CMMI against
existing state-of-the-art matrix completion algorithms.

SECTION: 4.1Estimation error of CMMI

We simulate a chain ofoverlapping observed submatricesfor the underlying population matrixas described in Figure3, and then predict the yellow unknown block by Algorithm1.
Eachhas the same dimension, i.e.for all, and the overlap betweenandare set tofor all.
We generateby samplinguniformly at random from the set ofmatrices with orthonormal columns and set, so thatin this setting.
We then generate symmetric noise matriceswithfor alland allwith. Finally we setwhereis a symmetricmatrix whose upper triangular entries are i.i.d. Bernoulli random variables with success probability.

Recall that, by Theorem2, the estimation error forcan be decomposed into the first order approximationand the remainder term. Furthermore we also have

with high probability.
We compare the error rates foragainstandby varying the value of one parameter among,,,,andwhile fixing the values of the remaining parameters.
Empirical results for these error rates, averaged overMonte Carlo replicates, are summarized in Figure4. We note that the error rates in Figure4are consistent with the bounds in Eq. (4.1) obtained by Theorem2.

We next compare the entrywise behavior ofagainst the limiting distributions in Theorem3. In particular, we plot in Figure5histograms (based onindependent Monte Carlo replicates) of theth entries where, and it is clear that the empirical distributions in Figure5are well approximated by the normal distributions with parameters given in Theorem3.

SECTION: 4.2Comparison with other matrix completion algorithms

We use the same setting as in Section4.1, but with, so that the observed submatrices fully span the diagonal of the matrix.

We varyand compare the performance of Algorithm1(CMMI) with some existing state-of-art low-rank matrix completion algorithms, including generalized spectral regularization (GSR)(Mazumder et al.,2010), fast alternating least squares (FALS)(Hastie et al.,2015), singular value thresholding (SVT)(Cai et al.,2010), universal singular value thresholding (USVT)(Chatterjee,2015), iterative regression against right singular vectors (IRRSV)(Troyanskaya et al.,2001). Note that increasingleads to more
observed submatrices but, as each submatrix is of smaller dimensions, the total number of observed entries decreases withat rate of.
Our performance metric for recovering the yellow unknown block is in terms of the relative Frobenius norm error.
Plots of the error rates (averaged overindependent Monte Carlo replicates) for different algorithms and their running times are presented in the left and right panels of Figure6, respectively.
Figure6shows that CMMI outperforms all competing methods in terms of both recovery accuracy and computational efficiency.

SECTION: 4.3Performance of CMMI with minimal overlap

We now examine the performance of CMMI when the overlap between the submatrices are very small. More specifically, we use the setting from Section4.2withand; as, this is the smallest overlap for which the latent positions for thecan still be aligned.
We fixand compute the estimation errorfor several values of. The results are summarized in Figure7. Note that the slope of the line in the left panel of Figure7is approximately the same as the theoretical error rate ofin Remark6. In summary, CMMI can integrate arbitrarily large submatrices even with very limited overlap.

SECTION: 5Real Data Experiments

We compare the performance of CMMI against other matrix completion algorithms on the MNIST database of grayscale images and MEDLINE database of co-occurrences citations.

SECTION: 5.1MNIST

The MNIST database consists ofgrayscale images of handwritten digits for the numbersthrough. Each image is of sizepixels and can be viewed as a vector in. Letdenote thematrix whose rows represent these images, where each row is normalized to be of unit norm.
We consider a chain ofoverlapping blocks, each block corresponding to a partially observed (cosine) kernel matrix for some subset ofimages. More specifically,

for eachwe generate amatrixwhose rows are sampled independently anduniformlyfrom rows ofcorresponding to one of the digits, with the lastrows ofand the firstrows ofhaving the same labels;

we setfor all;

finally,whereis asymmetric matrix whose upper triangular entries are i.i.d. Bernoulli random variables with success probability.

Given above collection of, we compare the accuracy for jointly clustering the images in the first and last blocks. In particular, for CMMI we first construct an embeddingusing theleading scaled eigenvectors offor each, and aligntoviaand. We then concatenate the rows ofandinto amatrixand cluster the rows ofintoclusters using-means. Finally we compare the accuracy of the cluster assignment against the true labelsusing Adjusted Rand Index (ARI). Note that
ARI values range fromto, with higher values indicating closer alignment between two sets of labels. For other low-rank matrix completion algorithms, we first reconstructfrom. Lettingdenote the resulting estimate, we findsuch thatis the best rank-approximation toin Frobenius norm (among all positive semidefinite matrices). We then subsetto keep onlyrows corresponding to images inand, and finally cluster theserows intoclusters using-means and compute the ARI of the resulting cluster assignments.

Comparisons between the ARIs of CMMI and other matrix completion algorithms, for different numbers of submatrices, are summarized in Figure8. Note that the black dotted line in the left panel of Figure8are ARIs when applying-means directly onand, and thus represent the best possible clustering accuracy. We observe that CMMI outperforms all competing methods on this dataset, and its ARIs remain close to optimal even asincreases. Finally, the right panel of Figure8shows that the running time for CMMI is orders of magnitude smaller than that of other algorithms.

SECTION: 5.2MEDLINE co-occurrences

The MEDLINE co-occurrences database(National Library of Medicine,2023)summarizes the MeSH Descriptors that occur together in MEDLINE citations from the MEDLINE/PubMed Baseline over a duration of several years. A standard approach for handling this type of data is to first transform the (normalized) co-occurrence counts into pointwise mutual information (PMI), an association measure widely used in natural language processing(Church and Hanks,1990; Lu et al.,2023). More specifically, the PMI between two conceptsandis
defined aswhereandare the (marginal) occurrence probability ofand, andis the (joint) co-occurrence probability ofand.

For our analysis of the MEDLINE data, we first selectclinical concepts which were most frequently cited during the twelve years period fromto, and construct the total PMI matrixbetween these concepts. Next we split the dates intotime intervals of equal length, and for each time intervalwe construct the individual PMI matrix. We randomly sample, for each interval, a subsetofconcepts from thosecited concepts such thatfor all. Finally we setas the principal submatrix ofinduced by. The collectionforms a chain of perturbed overlapping submatrices of.

Given, we apply CMMI and other low-rank matrix completion algorithms to constructfor the PMIs between clinical concepts inand those inin the total PMI matrix. Note that we specifyfor both CMMI and FALS, where this choice
is based on applying the dimensionality selection procedure ofZhu and Ghodsi (2006)to. In contrast we setfor GSR as its running time increase substantially for larger values of. The values offor SVT and USVT are not specified, as both algorithms automatically determineusing their respective eigenvalue thresholding procedures.
We then measure the similarities between the estimated PMIs inand the true total PMIs inin terms of the Spearman’s rank correlation(note that, for ease of exposition, we only compare PMIs for pairs of concepts with positive co-occurrence). The Spearman’sbetween two set of vectors takes value inwith(resp.) denoting perfect monotone increasing (resp. decreasing) relationship andsuggesting no relationship.
The results, averaged overindependent Monte Carlo replicates, are summarized in Figure9. CMMI outperforms competing algorithms in terms of both accuracy and computational efficiency.

SECTION: 6Extensions to Indefinite or Asymmetric Matrices

We now describe how the methodologies presented in this paper can be extended to block-wise data integration of symmetric
indefinite matrices and asymmetric/rectangular matrices.

SECTION: 6.1CMMI for symmetric indefinite matrices

Supposeis a symmetric
indefinite low-rank matrix.
Letandbe the number of positive and negative eigenvalues ofand set.
We denote the non-zero eigenvalues ofbyLet,, and the orthonormal columns ofandconstitute the corresponding eigenvectors.
Then the eigen-decomposition ofis, whereand.

Thencan be written aswith, and thus the rows ofrepresent the latent positions for the entities. For any, letdenote the set of entities contained in theth source, and letbe the corresponding population matrix. We then haveFor each observed submatrixon, we compute the
estimated latent position matrix. Hereand,contain thelargest positive andlargest (in-magnitude) negative eigenvalues of, respectively.contains the corresponding eigenvectors.

We start with thenoiselesscase to illustrate the main idea. Consideroverlapping block-wise submatricesandas shown in Figure1. Now

and hence there exist matricesandsuch that

Hereis the indefinite orthogonal group.
Eq. (6.1) implies

where we defineand we can recoverby aligning the latent positions for overlapping entities,

Ifthenis theuniqueminimizer of Eq. (6.2).
Heredenotes the Moore-Penrose pseudoinverse.

Now supposeandare noisy observations ofand. Letandbe estimates ofandas described above.
Then to alignand, we can consider solving the indefinite orthogonal Procrustes problem

However, in contrast to the noiseless case, there is no longer an
analytical solution to Eq. (6.3). We thus replace Eq. (6.3) with the unconstrained least squares problem

whose solution is once again.
Given, we estimatebyExtending the above idea to a chain of overlapping submatrices is also straightforward; see SectionB.1in the supplementary material for the detailed algorithm and simulation results.

The following result extends Theorem2to the indefinite setting. We note that the main difference in this extension is in the upper bound forand this is due to the fact that the least square transformationshave spectral norms that can be smaller or larger than,
and the accumulated error induced by these transformations need not grow linearly with. Finally ifthen the bounds in Theorem4are almost identical to those in
Theorem1, but with a slightly different definition for.

Consider a chain of overlapping submatriceswhere, for each,haspositive eigenvalues andnegative eigenvalues, satisfying Assumption1.
Set. Here we define,for any, and supposefor.
For all overlaps, suppose, and define

Supposefor all.
We then have

whereandare random matrices satisfying

with high probability. Hereis a quantity defined recursively byand

for.

SECTION: 6.2CMMI for asymmetric matrices

Data integration for asymmetric matrices has many applications including genomic data integration(Maneck et al.,2011; Cai et al.,2016),
single-cell data integration(Stuart et al.,2019; Ma et al.,2024).
Supposeis a low-rank matrix.
Letbe the rank of, and write the singular decomposition ofaswhereis a diagonal matrix whose diagonal entries are composed of the singular values ofin a descending order, and orthonormal columns ofandconstitute the corresponding left and right singular vectors, respectively.
The left and right latent position matrices associated to the entities can be represented byand, respectively. For theth source we denote the index set of the entities for rows and columns byand, and letFor each noisily observed realizationof, we obtain the estimated left latent positionsfor entities inand right latent positionsfor entities in.

Letandbe two overlapping submatrices shown in Figure10without noise or missing entries. Suppose.
Now

Then there existmatricesandsuch that

Suppose we want to recover the unobserved yellow submatrix in Figure10as part ofwhere, and thus our problem reduces to that of recovering. By straightforward algebra, we have

andcan be obtained by aligning the latent positions for the
overlapping entities, i.e.,

Now supposeandare noisy observations ofandwith possible missing entries. Letandbe the estimated latent positions matrices obtained fromand.
We can align these estimates by solving the least squares problems

and setting.
We then estimatebyNote that the unobserved white submatrix in Figure10is part ofand can
be recovered using the same procedure described above.

Finally we emphasize that to integrate any two submatricesandof an asymmetric matrix, it is not necessary for them
to have any overlapping entries, i.e., it is not necessary that bothand. Indeed, the above
analysis shows that if,or(inclusive or)then we can recover. Consider, for example, the situation in Figure11and
supposeis of rank. We can
then set

Extending these ideas to a chain of overlapping submatrices is
straightforward; see SectionB.2in the supplementary
material for the detailed algorithm and simulation results.

Finally,
we note that extending Theorem4to the asymmetric setting is also straightforward if we assume the entries ofare independent and thatfor all. Indeed, we can simply apply
Theorem4to the Hermitean dilations
of. However, the asymmetric case also allows for
richer noise models such as the rows ofbeing independent
but the entries in each row are dependent, or imbalanced dimensions
whereor vice versa for some indices. We leave theoretical results for these more general settings to future work.

SECTION: References

Supplementary Material for “Chain-lined Multiple Matrix Integration via Embedding Alignment”

SECTION: Appendix ADiscussion for Multiple Matrix Data Integration

If we are given a chain of overlapping submatrices of some larger matrix, then Algorithm1provides a simple and computationally efficient procedure for recovering the unobserved regions of.
In practice, before applying Algorithm1we need to resolve two other crucial issues: 1) determining if a chain exists for a particular unobserved entry and 2) if there exists more than one feasible chain, how to select one chain or combine multiple chains.
We discuss the recoverability of entries in SectionA.1.
In SectionA.2and SectionA.3we discuss the issue of chain selection for two cases: 1) we want to recover a specific unobserved entry; 2) integrating several block-wise overlapping noisy matrices. Finally in SectionA.4we discuss the use of a preprocessing step proposed inZhou et al. (2023)to aggregate multiple observed values by introducing weights to each source.
The algorithms are illustrated for positive semidefinite matrices, and it can be easily extended to the cases of symmetric indefinite matrices and asymmetric or rectangular matrices.

SECTION: A.1Determine the recoverability of entries

For a matrix ofentities, suppose that we haveobserved submatrices associated with entities setsfrom different sources.
We then construct an undirected graphto determine which unobserved entries can be recovered. More specifically,hasverticessuch thatandare adjacent if and only ifandare overlapping, i.e.,.
For any unobserved entryin, we setand. As long as there existandsuch thatandare connected in, we can estimateby at least one chain of overlapping submatrices. Furthermore, the connected components ofalso provide information on all recoverable entries. More specifically, supposehasconnected components, which can be found by depth-first search (DFS) efficiently.
For each component, let, and construct the entity setto contain all entities involved in this component.
Then wheneverandare both elements offor some,
the entryis recoverable using only the observed. See Algorithm2for details.

SECTION: A.2Choosing among multiple chains

For any arbitrary unobserved entry, we can use the graphdescribed in SectionA.1to determine all feasible chains of overlapping submatrices for recovering; indeed, each path incorresponds to one such chain.
We now describe how our theoretical analysis in Section3can provide guidance for choosing a “good” chain.
For ease of exposition, we will henceforth assume thatis connected and thus all entries that appeare in our matrix is recoverable. Ifhas more than one connected component then we can consider each connected component separately.

First recall thatis the dominant error term in
Theorem2. And if the noise levels ofandare homogenous then

with high probability.
Note that the bounds on the right sides of Eq. (A.1) and Eq. (A.2) depend only on the first and last submatrices of the chain, respectively, and they can be estimated from the observed data. Indeed,andare given, whilecan be approximated by, with a similar approximation for.
We thus only need to specify the start and end points of a chain. Let

for any. Then for any arbitrary unobserved entry, set

As the influence of the middle nodes of the chain on the estimation error is negligible (see Theorem2), for simplicity we set these nodes to form the shortest path betweenandin, which can be obtained by breadth-first search (BFS). See Algorithm3for details.

SECTION: A.3Algorithm for holistic recovery

Suppose we have observed submatricesforand want to integrate them.
Given Algorithm3in SectionA.2, a straightforward idea is to recover all unobserved entries one by one, but this involves a significant amount of redundant and repetitive calculations.
We now describe an approach for simplifying these calculations.

Suppose we have a graphas visualized in panel (a) of Figure12. Recall that each vertexinis associated with an observed submatrixwith estimated latent position matrix. Furthermore, ifandare adjacent inthen the corresponding submatricesandare overlapping and we can find an orthogonal matrixto alignand. Note thatin panel (a) of Figure12has cycles, and thus there exists at least one pair of verticesandwith multiple paths between them. We now want to find a unique path from every vertex to every other vertex, so that all the latent position matricescan be aligned together and all the unobserved entries can be recovered simultaneously.
This problem can be addressed using a spanning tree of; see panel (b) of Figure12. Recall thatdefined in Eq. (A.3) reflects the magnitude of the error foras an estimate of, and thus we want our tree to have paths passing through vertices with small estimation errors. This can be achieved by setting the weight of any edgeintoand then finding the minimum spanning tree (MST) of; see panel (c) of Figure12.

Given a minimum spanning tree, we choose aat random and align the remainingtousing the unique path.
See Algorithm4for details.

havevertices, andare adjacent if and only if.

For each, compute, and
obtain estimated latent positions for, denoted by.

Set the weight of each edgeas.

Find the minimum spanning tree ofby Prim’s algorithm or Kruskal’s algorithm, and denote its edge set by.

For each edge, obtainvia the orthogonal Procrustes problem

Choose one of the vertex denoted by(for example,), and let.

For each, apply BFS to find a path fromto, denoted by, and let.

Sortin descending order and record their indices as.

Initializeas anmatrix, and

fortodo

;

end for

SECTION: A.4Aggregating multiple observed values for the same entry

Finally we note that as the same entry can appear in more than one observed submatrix, we can also apply a
preprocessing step proposed inZhou et al. (2023)to aggregate these multiple observed values. This preprocessing is especially useful when the observed submatrices have many overlapping regions as it can reduce the noise and proportion of missing observations for each submatrix.
Letbe arbitrary, and denote bythe set of matrices in which theentry appears.Zhou et al. (2023)show that one can weight each sourceby.
Asis generally unknown, it can be estimated by. Hereis the observed matrix,
andis the rank-eigendecomposition of.
Given, the aggregated observed value foris then set to

Hereis the value for theentry inandare the normalized weights.

See Algorithm5for details.

For each, estimateby

whereis the rank-eigendecomposition ofwith.

Initialize.

For each, set.

ifthen

Compute.

Setfor each.

Setand.

end if

For each, setand.

Estimatebyand set.

SECTION: Appendix BAlgorithms and Simulation Results for Section6

SECTION: B.1Symmetric indefinite matrices integration

For each, obtain the estimated latent positions.

For each, obtainby solving the least square optimization problem

Compute.

We compare the performance of Algorithm6with other matrix completion methods. Consider the setting of Section4.2, but
with, and thus.
Figure13shows the relative-norm estimation error results for CMMI against other matrix completion
algorithms.

SECTION: B.2Asymmetric matrices integration

For each, obtain estimated left latent positions forasand right latent positions foras.

For each, obtain:

ifandthen

Computewhere

else ifthen

Compute

else

Computeby

end if

Compute.

We compare the performance of Algorithm7with other matrix completion methods. We simulate a chain ofoverlapping observed submatricesfor the underlying population matrixas described in Figure14, and then predict the yellow unknown block by Algorithm7.
We let all observed submatrices have the same dimension, and let all overlapping parts have the same dimension.
For the observed submatrices, we generate the noise matricesbyfor alland all, and we let all observed submatrices have the same non-missing probability.
For the low-rank underlying population matrix, we randomly generateandfromand, respectively. We fix the rank as, and set.
We fix the dimensions of the entire matrix atand, and we vary, the length of the chain, while ensuring that the observed submatrices fully span the diagonal of the matrix Recall that asincreases, we have more observed submarices but each observed submatrix is of smaller dimensions, which then increases the difficulty of recovering the original matrix.
Figure15shows the relative-norm estimation error results of recovering the yellow region.

SECTION: Appendix CProof of Main Results

SECTION: C.1Proof of Theorem1

We first state two important technical lemmas, one for the error ofas an estimate of the true latent position matrixfor each, and another for the difference betweenand. The proofs of these lemmas are presented in SectionD.2and SectionD.5.

Fix anand consideras defined in Eq. (2.1).
Write the eigen-decompositions ofandas

Let.
Next defineas a minimizer ofover allorthogonal matrices.
Suppose that

is amatrix with bounded coherence, i.e.,

has bounded condition number, i.e.,

for some finite constant; hereanddenote the largest and smallest non-zero eigenvalues of, respectively.

The following conditions are satisfied.

We then have

where the remainder termsatisfies

with high probability.
If we further assume

then we also have

with high probability.

As, we generally have, e.g.,hasentries that are lower bounded by some constantnot depending onor. Thus, asis low-rank with bounded condition number, we also haveand the second condition in Eq. (C.1) simplifies to

Similarly, the condition in Eq. (C.3) simplifies to

Both conditions are then trivially satisfied wheneverand.
Finally when, the bounds in LemmaC.1simplify to

with high probability.

Consider the setting of Theorem1. We then have

with high probability.

We now proceed with the proof of Theorem1. Recall Eq. (C.2) and
letfor any. Also denote.
We then have

where we set

We now boundand. Note that,
for matricesandof conformal dimensions, we have

We therefore have

Next, by LemmaC.1, for anywe have

with high probability. Finally, by LemmaC.2we have

with high probability. Substituting the above bounds in Eq. (C.6) and Eq. (C.7) into Eq. (C.5), we obtain the desired bounds ofand.

SECTION: C.2Proof of Theorem2

Theorem2follows the same argument as that for Theorem1, with the only change being the use of
LemmaC.3below (see SectionD.7for a proof) to bound the difference betweenand.

Consider the setting of Theorem2and
let. We then have

with high probability.

SECTION: C.3Proof of Theorem3

By Theorem2, for any fixed, we have

As,
we have for anythat

Similarly, we also have that for any,.
Note thatare independent.
We thus have

Let

and note thatare mutually independent zero-mean random variables.
Let

We now analyzefor any fixed; the same analysis also applies tofor any fixed.
Rewriteas

Then by Eq. (C.10) we have

The condition in Eq. (3.9)implies

whereanddenote theth row andth row ofand, respectively.
Next we have

For any fixed but arbitrary, we have

where the last inequality follows from the fact thatwhenever.

Now ifthen by Eq. (C.11) and Eq. (C.12) we have

Similarly, ifwe have

Eq. (C.14) and Eq. (C.15) together imply

asymptotically almost surely, provided by the condition in Eq.3.10.
Returning to Eq. (C.13), note thatis a deterministic function ofand hence

We now bound the terms appearing in the above display.
First, by Eq. (C.14), we have

Next, asis sub-Gaussian with, we have by a similar analysis to Eq. (C.14) thatis also sub-Gaussian with

There thus exists a constantsuch that

see Eq. (2.14) inVershynin (2018)for more details on tail bounds for sub-Gaussian random variables.
We therefore have

Combining Eq. (C.13) and Eq. (C.16) through Eq. (C.19), we have

as, under the assumption thatprovided by Eq.3.10.
Using the same argument we also have

By Eq. (C.20), Eq. (C.21), and applying the Lindeberg-Feller central limit theorem (see e.g.,
Proposition 2.27 inVan der Vaart (2000)) we have

as. Finally, invoking Theorem2and the assumption in Eq. (3.11), we havein probability. Then applying Slutsky’s theorem we obtainas claimed.

SECTION: Appendix DTechnical Lemmas

SECTION: D.1Technical lemmas for LemmaC.1

Consider the setting of LemmaC.1.
Then for any, forwe have

with high probability.

First writeas the sum of two matrices, namely

Ifthen,
following the same arguments as that for Lemma 13 inAbbe et al. (2020)we obtain

with high probability. Next for any arbitrarywith, let

wheredenotes theth row offor any.
Then; note thatare independent, random, self-adjoint matrices ofdimension withand

Now for any square matrix, we have, wheredenotes the Loewner ordering for positive semidefinite matrices. Therefore for anywe have

Furthermore, asfor all,
we have

Therefore according to Theorem 1.4 inTropp (2012), for all, we have

and hence

with high probability.

Next note that, whereis theth row of.
Thus, for any fixed,; note thatare independent, random matrices of dimensionwithand

Let.
We then have

Therefore, by Theorem 1.6 inTropp (2012), for anywe have

and hence

with high probability, where the final inequality follows from the assumption. Taking a union over allwe obtain

with high probability.

For, its upper triangular entries are independent random variables. Because for anyis a sub-gaussian random variable with, we haveand it follows that

For sub-gaussian random variables, we also havewith high probability; hereis some finite constant not depending onor.
Then with high probability

Then by combining Corollary 3.12 and Remark 3.13
inBandeira and Van Handel (2016)with Proposition A.7 inHopkins et al. (2016),
there exists some constantsuch that for any

Letfor some, thenfromwe have

with high probability.

Forand, with the similar analysis withandwe have

with high probability
and

with high probability.

Finally we combine Eq. (D.2) and Eq. (D.5), Eq. (D.3) and Eq. (D.6), Eq. (D.4) and Eq. (D.7) and obtain the desired results for,, and.
∎

Consider the setting of LemmaC.1.
Then for anywe have

with high probability, and forandwe have

with high probability.

By perturbation theorem for singular values (see Problem III.6.13 inHorn and Johnson (2012)) and LemmaD.1, for anywe have

with high probability. Then the condition in Eq. (C.1) yields the stated claim for.
Next, by Wedin’sTheorem (see e.g., Theorem 4.4 in Chapter 4 ofStewart and Sun (1990)) and LemmaD.1, we have

with high probability, and hence

with high probability.
∎

Consider the setting of LemmaC.1.
Then for each, we have

with high probability.

For ease of exposition we will fix a value ofand thereby drop the index from our matrices.

For, because

by LemmaD.1and LemmaD.2we have

with high probability.

For, we notice

For the right hand side of the above display, we have bounded the second term in Eq. (D.8).
For the first term and the third term, by LemmaD.2we have

with high probability.
Combining Eq. (D.8) and Eq. (D.9), we have

with high probability.

For, for anytheentry can be written as

We defineas amatrix whose entries are. Eq. (D.11) means

wheredenotes the Hadamard matrix product, and by Eq. (D.10) and LemmaD.2it follows that

with high probability.

For, notice

whereis a matrix-valued function, and for anyinvertible matrix,. Then if, according to Theorem 1 inLi (1995)we have

We now bound.
By LemmaD.2and Eq. (D.12) we have

with high probability. Now by Eq. (C.1)
we havewith high probability.
In summary we have

with high probability.
∎

SECTION: D.2Proof of LemmaC.1

We first state a lemma for bounding the
error ofas an estimate of; see SectionD.3for a proof.

Consider the setting of LemmaC.1.
Defineas a minimizer ofover allorthogonal matrices.
We then have

whereandis arandom matrix satisfying

with high probability. Furthermore, suppose

We then have

with high probability.

Now recall that

As, there exists an orthogonalsuch that.
Define

and recall

Note that.
Next, by Eq. (D.13) we have

where the remainder termis

By LemmaD.4, LemmaD.2and LemmaD.3we obtain

with high probability and

with high probability.

SECTION: D.3Proof of LemmaD.4

For ease of exposition we fix a value ofand drop this index from our matrices.
First note that

Hence for anyorthogonal matrix, we have

Letbe the minimizer ofover allorthogonal matrices. We now bound the spectral norms ofwhen.

For, by LemmaD.2we have

with high probability.

For, by LemmaD.2and LemmaD.3we have

with high probability.

For, by LemmaD.1, LemmaD.2and LemmaD.3we have

with high probability.

For, by LemmaD.1, LemmaD.2and LemmaD.9, we have

with high probability.
Combining the above bounds we obtain

with high probability
asas implied by Eq. (C.3).

SECTION: D.4Technical lemmas forin LemmaD.4

Our bound forin the above proof of LemmaD.4is based on a series of technical lemmas which culminate in a high-probability bound for. These lemmas are derived using an adaptation of the leave-one-out analysis presented in Theorem 3.2 ofXie (2024); the noise model forin the current paper is, however, somewhat different from that ofXie (2024)and thus we chose to provide self-contained proofs of these lemmas here.
Once again, for ease of exposition we will fix a value ofand thereby drop this index from our matrices in this section.

We introduce some notations.
Forwhose entries are independent Bernoulli random variables sampled according to,
we define the following collection of auxiliary matricesgenerated from.
For each row index, the matrixis obtained by replacing the entries in theth row ofwith their expected values, i.e.,

Denote the singular decompositions ofandas

Consider the setting in LemmaC.1, we then have

with high probability. Furthermore, letbe the solution of orthogonal Procrustes problem betweenand. We then have

with high probability.

The proof is based on verifying the conditions in Theorem 2.1 ofAbbe et al. (2020), and this can be done by following the exact same derivations as that in Lemma 12 ofAbbe et al. (2020).
More specifically,inAbbe et al. (2020)corresponds toin our paper whileinAbbe et al. (2020)corresponds toin our setting, whereappears in the assumptions of LemmaC.1and is bounded,
andinAbbe et al. (2020)can be set to befor some sufficiently large constant, based on the bound offrom LemmaD.1. Then all desired results of LemmaD.5can by obtainedunder the conditions thatand condition in Eq. (C.3).∎

Consider the setting in LemmaC.1.
Recall in LemmaD.1, we decomposeas.
Letdenote theth row of.
Then for any deterministicmatrix, we have

with high probability.

For any, letanddenote theth row ofand. Following the same arguments as that forandin the proof of LemmaD.1, we have

We therefore have

with high probability.
Combining the above bounds yields the desired claim.
∎

Consider the setting in LemmaD.4, we then have

with high probability.

By the construction ofand LemmaD.1we have

with high probability, and hence

with high probability.
Then by Weyl’s inequality we have

with high probability. The condition in Eq. (C.3) impliesand hence.
Furthermore, by LemmaD.2we havewith high probability. Applying
Wedin’sTheorem (see e.g., Theorem 4.4 ofStewart and Sun (1990)) we have

with high probability.
We now bound. Note that

whereis theth row of.
For the first term on the right side of Eq. (D.18), by Cauchy-Schwarz inequality, LemmaD.5and LemmaD.1we have

with high probability.
For the second term on the right side of Eq. (D.18),
asandare independent, we have by LemmaD.6, LemmaD.5, and the assumptionthat

with high probability. Combining Eq. (D.18), Eq. (D.19) and Eq. (D.20), we have

with high probability. Substituting Eq. (D.21) into Eq. (D.17)
yields the desired claim.
∎

Consider the setting in LemmaD.4, we then have

with high probability.

Eq. (D.15) and Eq. (D.16) impliesandwith high probability.
Then by Wedin’sTheorem (see e.g., Theorem 4.4 inStewart and Sun (1990))
we have

with high probability.
Letbe the orthogonal Procrustes alignment betweenand. Then

with high probability.

Now let. By Eq. (D.22) and LemmaD.5we obtain

with high probability, where the final inequality follows from the fact that, under the conditions in Eq. (C.3) we have.

Finally, asandare independent, by LemmaD.6and the assumptionwe have

with high probability. ∎

Consider the setting in LemmaD.4, we then have

with high probability.

For each, letdenote theth row of. Notice

We now bound all terms on the right hand side of Eq. (D.23).
For, by LemmaD.2we have

with high probabilityasas implied by Eq. (C.3).

For, by LemmaD.1and LemmaD.2we have

with high probability.

For, by LemmaD.1and LemmaD.7we have

with high probability.

For, by LemmaD.8we have

with high probability.

Combining Eq. (D.23), Eq. (D.24), …, Eq. (D.27) we finally obtain

with high probabilityasas implied by Eq. (C.3).
∎

SECTION: D.5Proof of LemmaC.2

Recall that

Denote

We therefore have, by perturbation bounds for polar decompositions, that

Indeed, we supposeis invertible in Theorem1.
Now suppose. Thenis also invertible and hence, by Theorem 1 inLi (1995)we have

Otherwise ifthen, as, Eq. (D.28) holds trivially.

We now bound. First note that

Next, by LemmaC.1, we have

whereandcontain the rows inandcorresponding to entities, respectively. A similar expansion holds for.
We therefore have

For, by LemmaC.1we have

with high probability. For, by LemmaD.10we have

with high probability. The same argument also yields

with high probability. For, once again by LemmaD.10we have

with high probability. The same argument also yields

with high probability.
Combining the above bounds forand simplifying, we obtain

with high probability. Substituting the above bound forinto Eq. (D.28) yields the stated claim.

SECTION: D.6Technical lemmas for LemmaC.2

Consider the setting of Theorem1. We then have

with high probability.

From LemmaC.1we have

with high probability. Furthermore, following the same derivations as that forin the proof of LemmaD.1, we have

with high probability, provided that.
∎

Suppose the entities inare selected uniformly at random from allentities.
Write the eigen-decomposition ofaswhereare the eigenvectors corresponding to the non-zero eigenvalues. Letanddenote the largest and smallest non-zero eigenvalue of, respectively. Then forwe have

with high probability.

If the entities inare chosen uniformly at random from allentities then, by Proposition S.3. inZhou et al. (2023)together with the assumption, we have

with high probability. As, given the above bound we have

and hence

Finally, from

we obtain

as claimed.
∎

SECTION: D.7Proof of LemmaC.3

We proceed by induction on. The casefollows from LemmaC.2.
Now for any, define

and suppose the stated bound holds for, i.e.

with high probability. Next note that

and hence, by combining Eq. (D.31) and LemmaC.2(for),
we obtain

with high probability.

SECTION: D.8Extension to symmetric indefinite matrices

We first state an analogue of LemmaC.1for symmetric but possibly indefinite matrices.

Fix anand consideras defined in Eq. (2.1).
Write the eigen-decompositions ofandas

Letanddenote the number of positive and negative eigenvalues of, respectively, and denote.
Letand.
Suppose that

is amatrix with bounded coherence, i.e.,

has bounded condition number, i.e.,

for some finite constant; hereanddenote the largest and smallest non-zero singular values of, respectively.

The following conditions are satisfied.

Then there exists an matrixsuch that

where the remainder termsatisfies

with high probability. Recall thatanddenote the set oforthogonal andindefiniteorthogonal matrices, respectively.
If we further assume

then we also have

with high probability.

The proof of LemmaD.12follows the same argument as that for LemmaC.1and is thus omitted. The main difference between the statements of
these two results is that LemmaC.1boundswhile
LemmaD.12bounds. Ifis positive semidefinite thenfor some orthogonal matrixand thus we can combine bothandinto a single orthogonal transformation; see the argument in SectionD.2. In contrast, ifis indefinite thenfor some indefinite orthogonal. As indefinite
orthogonal matrices behave somewhat differently from orthogonal matrices, it is simpler to considerandseparately.

Consider the setting of Theorem4for just two overlapping submatricesand.
Letbe the least square alignment betweenand.
Heredenotes the Moore-Penrose pseudoinverse of a matrix.
Also noticeis the corresponding alignment betweenand. We then have

with high probability.

For ease of notation, we will letand, and defineand. We then have

where the first equality follows from the fact that ifis amatrix andis aorthogonal matrix then.

Now under the assumption, we havewith high probability, and hence bothandare invertible with
high probability. We thus haveand

Furthermore, asandshare the same column space, we haveso that

Combining Eq. (D.33) and Eq. (D.34) we obtain

and hence

We now bound the terms on the right side of Eq. (D.35).
By LemmaD.12we have

with high probability.
Recall that

ThusBecausewith high probability, we also have

with high probability.
Then by Theorem 4.1 inWedin (1973)we have

with high probability.
Next, by Eq. (D.32), we have

By similar results to LemmaD.10we have

with high probability. Therefore we have

with high probability.
Combining Eq. (D.35), Eq. (D.36), Eq. (D.37), and Eq. (D.38) we have the desired error rate of.
∎

We now prove Theorem4.
In the case of a chainwe have

whereis matrix product and. Furthermore, for anywe also have

Therefore, for,

Definefor. We then havewith high probability by LemmaD.13, and have

forwith high probability by Eq. (D.39), where

Finally, we have

where the last equality follows from the facts thatand.
Letfor. Following the same derivations as that for Eq. (C.4), with LemmaD.12replacing LemmaC.1,
we obtain

where we set

Substituting the bounds forandin LemmaD.12, we obtain

with high probability under the assumptionfor. Finally, asfor, with, we have after some straightforward algebra that

as desired.