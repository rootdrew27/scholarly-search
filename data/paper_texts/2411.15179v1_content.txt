SECTION: Random Forest-Supervised Manifold Alignment††thanks:Both authors contributed equally to this work. This material was supported by a grant from the Simmons Research Endowment.

Manifold alignment is a type of data fusion technique that creates a shared low-dimensional representation of data collected from multiple domains, enabling cross-domain learning and improved performance in downstream tasks. This paper presents an approach to manifold alignment using random forests as a foundation for semi-supervised alignment algorithms, leveraging the model’s inherent strengths. We focus on enhancing two recently developed alignment graph-based by integrating class labels through geometry-preserving proximities derived from random forests. These proximities serve as a supervised initialization for constructing cross-domain relationships that maintain local neighborhood structures, thereby facilitating alignment. Our approach addresses a common limitation in manifold alignment, where existing methods often fail to generate embeddings that capture sufficient information for downstream classification. By contrast, we find that alignment models that use random forest proximities or class-label information achieve improved accuracy on downstream classification tasks, outperforming single-domain baselines. Experiments across multiple datasets show that our method typically enhances cross-domain feature integration and predictive performance, suggesting that random forest proximities offer a practical solution for tasks requiring multimodal data alignment.

SECTION: IIntroduction

Data collected from an individual observation can come from multiple sources (multi-modal) which may be inherently connected, such as a document translated into multiple languages[1]. However, challenges arise when attempting to use multiple data domains in a single model, especially if the data lack clear connections, or when there is no originating source from which both domains are sampled[2].

Recent machine learning tools are capable of improving classification and regression problems by incorporating multiple domains in the learning paradigm[3]. Most recent multimodal models are large and computationally expensive deep learning models[4]which are well formulated for tasks such as generative natural-language processing or image generation[5], but are often overkill for smaller, scientific projects. For moderate data sets for scientific tasks, the field of manifold alignment forms a means of generating a shared representation across multiple domains for subsequent tasks.

Manifold alignment aims to find a shared, low-dimensional representation of data sources, enabling the learning of inter-domain relationships while preserving the structure of each domain. Semi-supervised manifold learning involves using known connections, or anchors, to help uncover relationships between domains. In some instances, it is possible to learn richer information by combining insights from multiple sources formed into a single representation, such as a manifold. For example, a classification model can be enhanced by integrating relevant information from multiple data domains. However, we show that many existing manifold alignment methods often yield representations on which predictive models perform poorly compared to single-domain baselines.

In this paper, we overcome this weakness by employing random forests[6]to initialize manifold learning algorithms. We show that using this initialization often improves the accuracy of subsequent supervised models when trained on the resulting joint embedding. We build on recent diffusion- and shortest-path methods[7], named, respectively, MASH (Manifold Alignment via Stochastic Hopping) and SPUD (Shortest Path on the Union of Domains). We show that the embedding quality, as measured by post-alignment classification accuracy, is generally improved over existing, semi-supervised alignment methods.

SECTION: IIManifold Alignment Background

Manifold alignment refers to methodologies aimed at discovering a common, low-dimensional structure, or manifold embedding, that forms a common or shared representation of data from multiple sources. The representation can be subsequently used to analyze relationships across different domains or enhance performance in downstream machine-learning tasks like label prediction. In some cases, alignment is aided by known connections or anchors, represented by data points shared across multiple domains[2]. In others, alignment relies on discrete, shared data labels[8,9]. These examples belong to a branch known as semi-supervised manifold alignment. Here, we assume that knowledge between points of other modalities or domains can be distilled through known anchor points. At a high level, we assume that point nearness in one modality transfers across domains, extending the idea that “the neighbor of my neighbors is also my neighbor” to multiple domains.

This concept was explored in[7]. In the paper, the authors described two graph-based manifold alignment methodologies that build new graph edges through either diffusion[10]or shortest paths. They showed that both the diffusion method, Manifold Alignment via Schochastic Hopping (MASH), and the shortest-path method, Shortest Paths on the Union of Domains (SPUD) improved alignment performance compared to state-of-the-art methodologies measured using two independent metrics: The fraction of samples closer than the true match (FOSCTTM)[11]and cross-embedding classification (CE) or label transfer[8]. We build upon this method by defining weighted graphs via random forest proximities to initialize the graph structure. In doing so, the aligned embedding more frequently improves classification accuracy over that derived from a single domain. In contrast, we show that most manifold alignment methods typically do not enhance the learnable information of classification tasks.

SECTION: IIIRandom Forest Proximities

A random forest is a supervised learning model designed to predict data labels using an ensemble of randomized decision trees[6]. Each tree is trained on a bootstrap sample, uniformly selected with replacement, with included points referred to as “in-bag”, and excluded points called “out-of-bag” or OOB.

Similarity measures can be derived from the terminal nodes between the in-bag points and a new observation. Since the similarities are determined by the decision space laid out by the terminal nodes, they can be viewed as a form of supervised similarity. Such similarity measures are often called random forest proximities. Random forest proximities define pairwise relationships between data points, leveraging supervised learning to reveal meaningful patterns guided by the supervision of the random forest model. They have proven to be useful in many applications. One of the original uses of random forest proximities is in data visualization, where they facilitate dimension reduction techniques such as multidimensional scaling (MDS)[12]. Random forest proximities have also been used to improve feature importance measures[13]and data imputation[14].

The connection between random forest regression models and a nearest-neighbor approach with an adaptive local metric was established in[15], framing the model’s predictions in terms of local instance relationships. Building on this concept, Random Forest-Geometry- and Accuracy-Preserving Proximities (RF-GAP)[16]provided a novel measure that extends this relationship classification contexts and can perfectly replicate random forest out-of-bag predictions. Thus, the RF predictions are described in terms of instance-level attribution; that is, they reflect the influence of adjacent training points. In this way, the model predictions are driven by the importance of local instances in shaping the decision space.

RF-GAP proximities have shown superior performance in proximity-based tasks, including visualization[17], imputation, and outlier detection[16]. This improvement is likely due to their being intentionally designed to generate random forest OOB predictions, which design also prevents overfitting when used in downstream tasks. By encoding the relationships between data points through random forest proximities, we can establish a meaningful connection between domains through neighboring points.

SECTION: III-ARelated Methods

Manifold alignment methods for semi-supervised learning focus on aligning two modalities or co-domains,and, where the partial correspondence between points is known. The correspondence is typically defined through points known to belong to both domains (i.e.,andform alternative representations of the same point inand, respectively), although, in some cases, correspondence is inferred via known class labels[9,8]. We briefly describe below existing semi-supervised alignment methods.

Semi-Supervised Manifold Alignment (SSMA)[18]uses constrained Laplacian Eigenmaps to align known correspondences between domains in low-dimensional embeddings. An alignment method based on Diffusion Maps was derived in[19], performing density normalization across domains and employing an affine transformation to match embeddings. However, limited implementation details preclude us from comparing this approach. Manifold Alignment via Procrustes Analysis (MAPA)[20]applies Laplacian Eigenmaps followed by Procrustes alignment on correspondences and generally outperforms SSMA.

Joint Laplacian Manifold Alignment (JLMA) builds on-NN similarities to create a joint Laplacian across domains, embedding known correspondences in a shared coordinate system[2]. MAGAN[21], a Generative Adversarial Network-based approach, aligns domains using reconstruction, discriminator, and custom correspondence losses. Diffusion Transport Alignment (DTA)[22]combines diffusion processes with optimal transport to align domains via cross-domain transition matrices, outperforming over MAGAN, MAPA, and SSMA.

Kernel Manifold Alignment (KEMA)[9]and Manifold Alignment with Label Information (MALI)[8]are both semi-supervised methods that perform alignment through common data labels rather than known data anchors. KEMA is a kernelized version of SSMA, while MALI uses entropic optimal transport initialized with prior class probabilities, and has been shown to outperform KEMA in label transfer tasks[8].

SECTION: IVMethods

Both MASH and SPUD[7]are typically initialized with a kernel matrix for each domain using the-decaying kernel[23]. Instead, we initialize each model using RF-GAP proximities[16]. The RF-GAP proximities form a row-stochastic diffusion operator which encodes similarities within a given domain. The supervised information, however, is encoded in a way that does not overfit to the training data, which is important for downstream classification tasks. We refer the reader to[16]for more on this aspect. To learn cross-domain similarity measures, we assign the similarities between anchor points to be maximal. That is,forin, whereis the normalized proximity, andis the representation ofin. We form a joint, cross-domain similarity matrix as

with=.

MASH powers the cross-domain similarity, forming new graph connections between the domains. We apply an information distance[24]to the powered diffusion operator,to perform a potential distance[23]upon which we can apply MDS to form an embedding.

RF-SPUD also starts by forming two RF-GAP similarity graphs,, and. Anchor points are then leveraged to form connections between domains. However, instead of following a diffusion process, RF-SPUD learns the shortest paths between the domain graphs by following geodesic curves through anchor points, thereby forming additional cross-domain connections and learning long-range dependencies. A potential distance function[23]is applied to the learned connections and an embedding is formed using MDS. For further details, we refer the reader to[7].

SECTION: VExperimental Results

We investigate the potential for manifold alignment models to produce alignments or embeddings capable of enhancing downstream classification. We compare RF-SPUD and RF-MASH to SSMA, MAPA, JLMA, MAGAN, DTA, KEMA, MALI, and MALI initialized using RF-GAP proximities (RF-MALI). We used 16 classification111Balance Scale, Breast Cancer, Crx, Diabetes, Ecoli (5 majority classes), Flare1, Glass, Heart Disease, Heart Failure, Hepatitis, Ionosphere, Parkinson’s, Seeds, Iris, Tic-Tac-Toe, Cancer Datafiles from the UCI Machine Learning Repository[25]. We removed observations with missing values and normalized all features from 0 - 1.

To simulate different modalities from each dataset, we perform feature-level splits (i.e., treat different sets of features as a different domain), and two types of feature distortions. In therandomsplit setup, each domain was assigned a randomly chosen set of features without any overlap. Theskewedsplit assigns domains according to feature relevance as assessed by a random forest model; the most relevant features formed one domain, while the remaining formed the other. For theevensplit, features were divided alternating between domains according to importance scores to balance prediction relevance. All remaining less relevant features are assigned randomly.

Two data-distortion domain adaptions were used for comparisons, as presented in[22]. For the first method,distort, Gaussian noise was added to the original features to form a second domain. For the second, random rotations via QR factorization were applied to the original features to produce therotatedomain adaptions.

Our experiments aim to identify alignment methods that produce embeddings with sufficient information to enhance downstream prediction tasks and to determine the contexts in which certain methods perform best. Specifically, we train classification models (random forests and-nearest neighbors) on each domain separately, record validation scores, and then train these models across the aligned, joint embedding space. Ideally, a joint embedding should capture inter-domain relationships, enabling the models to exceed baseline accuracy.

We used a staggered grid search strategy for model parameter selection, testing each parameter set on a fixed 10-nearest neighbor model across each dataset to ensure comparable performance evaluations. We explored combinations of variables with the greatest impact on model performance and reported the best results from the search for each model.

In Figure1, we compare baseline classification accuracy with the accuracy of models trained on aligned embeddings. For each dataset and split, we trained a random forest and-NN model, recorded validation accuracy, and then trained each model using manifold embeddings for each alignment method, fitting on one domain and predicting on the other. The proportion of models that outperformed one or both baselines, Overall, RF-MASH, KEMA, and RF-SPUD. Generally, methods initialized with random forests outperformed their unsupervised counterparts.

While KEMA generally performs well at improving multi-modal classification accuracy over the baselines, we note that this method underperforms at aligning specific data points to their known correspondences in another domain. For comparison, we include models that use label information during the alignment process using the combined metric (CE - FOSCTTM) as introduced in[7]in Figure3. In this Figure, we see that KEMA is outperformed by RF-SPUD and RF-MASH across all domain adaptations except for random rotations. For all other splits, RF-SPUD and RF-MASH are top performers.

SECTION: VIConclusion

In this paper, we introduced two semi-supervised alignment methods (RF-SPUD and RF-MASH) initialized using RF-GAP proximities. Although most semi-supervised manifold alignment methods generally do not improve classification models trained on the resulting embeddings, we showed that proximity-initialized models usually enhance aligned manifold embeddings by retaining information sufficient for improving multi-modal classification tasks.

SECTION: References