SECTION: Embed-Search-Align: DNA Sequence Alignment using Transformer models
DNA sequence alignment, an important genomic task, involves assigning short DNA reads to the most probable locations on an extensive reference genome.
Conventional methods tackle this challenge in two steps: genome indexing followed by efficient search to locate likely positions for given reads. Building on the success of Large Language Models (LLM) in encoding text into embeddings, where the
distance metric captures semantic similarity, recent efforts have encoded DNA sequences into vectors using Transformers and have shown promising results in tasks involving classification of short DNA sequences.
Performance at sequence classification tasks does not, however, guarantee, where it is necessary to conduct a genome-wide search to align every read successfully, a. We bridge this gap by developing a “mbed-earch-lign” (ESA) framework, where a novel Reference-Free DNA Embedding () Transformer model generates vector embeddings of reads and fragments of the reference in a shared vector space; read-fragment distance metric is then used as a surrogate for sequence similarity. ESA introduces: (1) Contrastive loss for self-supervised training of DNA sequence representations, facilitating rich reference-free, sequence-level embeddings, and (2) a DNA vector store to enable search across fragments on a global scale. RDE is 99% accurate when aligning 250-length reads onto a human reference genome of 3 gigabases (single-haploid),. RDE far exceeds the performance ofrecent DNA-Transformer model baselines, and shows task transfer across chromosomes and species.

SECTION: 1. Introduction
Sequence alignment is a central problem in the analysis of sequence data. It is used in various genomic analyses, including variant calling, transcriptomics, and epigenomics. Many DNA sequencers generate short reads that are only a couple of hundred bases long. In order to interpret this data, a typical first step is to align the reads to a genome. Genomes come in many sizes, and commonly studied genomes range from millions of bases (e.g., bacteria) to billions of bases (e.g., mammals, plants) in length. The resulting task of aligning short reads to large-size genomes is computationally challenging—akin to finding a needle in a haystack—and many decades of work have led to optimized approaches that can perform these tasks with great efficiency.
Since genomes are sequences where the alphabet consists of only a few symbols () they can be considered as Limited Alphabet Languages (LAL). We ask whether one can design a new paradigm to align reads to genomes by exploiting the Transformer architectures. The applications of the Transformer model—and, more generally “Large Language Models” (LLM)—in Bioinformatics applications are still in their infancy, yet hold substantial promise: Transformer models have demonstrated an unprecedented ability to construct powerful numerical representations of sequential data.

We establish a foundation model–Reference-free DNA Embedding (RDE)—tailored for embedding DNA sequences. The model(i.e., parameterized by weights) maps any genome subsequenceinto an embedding(e.g.,, such that ifandare similar subsequences of differing lengths—for example,is any noisy subsequence oforandhave significant overlaps—then theis small; whereis a distance metric in the embedding space, such as the cosine distance between two vectors. Another requirement of such RDE models is that they should generate embeddings that are reference free, i.e. once trained, it creates semantically-aware embedding (i.e. sequences that are close in edit distance are mapped close to each other) of any given genome subsequences. Given such a model, alignment of reads can be achieved by a near-neighbor search in the embedding space: only the fragments of genome with embeddings nearest to the embedding of the given read need to be considered. Thus, a global search in a giga-bases long sequence is reduced to a local search in a vector space.

SECTION: 1.1 Transformer models: Written Language to DNA Sequence Alignment
DNA sequences share remarkable similarities with written language, offering a compelling avenue for the application of Transformer models. Like written language, these are sequences generated by a small alphabet of nucleotides. Traditional DNA modeling efforts have already accommodated mature encoding and hashing techniques initially developed for written language – such as Suffix trees/arrays and Huffman coding– to successfully parse and compress DNA sequences. Alignment methods such as MinHash and MashMaphave incorporatedHashing (LSH)to construct fast, approximate representations of DNA sequences that facilitate the rapid matching of millions of reads onto the reference genome.

Within the last few years,

SECTION: 2. Our Contributions
SECTION: 2.1 Task Definition
SECTION: 2.2 RDE: Designing effective sequence representations
SECTION: 2.3 RDE: Self-supervision and contrastive loss
SECTION: 2.4 RDE: Encoder implementation
SECTION: 2.5 ESA: Search and retrieval
SECTION: 3. Transformer-DNA baselines
SECTION: 4. Sequence alignment of ART-simulated reads
SECTION: 4.1 Performance
SECTION: 5. Limitations and Future Work
SECTION: 6. Concluding Remarks
SECTION: References
SECTION: Appendix
SECTION: A. Training Convergence and Data Usage
SECTION: B. Complexity of computing alignment
SECTION: Cost of constructing a new representation
SECTION: Vector Store Upstream
SECTION: Retrieval Cost
SECTION: Fine-grained Alignment
SECTION: Total Complexity
SECTION: C. Ablation studies
SECTION: Without diversity priors in top-K
SECTION: Across read lengths
SECTION: D. Task transfer from Chromosome 2
SECTION: E. Potential for speed-ups in index creation and read alignment
SECTION: F. A setup for de-novo Genome Assembly using RDE