SECTION: A Survey of the Self Supervised Learning Mechanisms for Vision Transformers
Deep supervised learning models require high volume of labeled data to attain sufficiently good results. Although, the practice of gathering and annotating such big data is costly and laborious. Recently, the application of self supervised learning (SSL) in vision tasks has gained significant attention. The intuition behind SSL is to exploit the synchronous relationships within the data as a form of self-supervision, which can be versatile. In the current big data era, most of the data is unlabeled, and the success of SSL thus relies in finding ways to utilize this vast amount of unlabeled data available. Thus it is better for deep learning algorithms to reduce reliance on human supervision and instead focus on self-supervision based on the inherent relationships within the data. With the advent of ViTs, which have achieved remarkable results in computer vision, it is crucial to explore and understand the various SSL mechanisms employed for training these models specifically in scenarios where there is limited labelled data available. In this survey, we develop a comprehensive taxonomy of systematically classifying the SSL techniques based upon their representations and pre-training tasks being applied. Additionally, we discuss the motivations behind SSL, review popular pre-training tasks, and highlight the challenges and advancements in this field. Furthermore, we present a comparative analysis of different SSL methods, evaluate their strengths and limitations, and identify potential avenues for future research.

SECTION: Introduction
Deep learning based algorithms have exhibited impressive results across various disciplines specially in computer vision (CV)and natural language processing (NLP). Deep learning based models utilize a pre-training task on large datasets to enhance their performance. This primary step is often utilized as an initialization point and then the model is optimized for any specific use case. Self-supervised learning (SSL) is among the pre-training strategies for deep learning algorithms. Overall, SSL approach is driven by two major motivations. Firstly, a network that has been trained on an extensive data has already learned distinctive patterns that can be applied to subsequent tasks. This also helps to decrease the overfitting issue during training step. Secondly, parameters learned from extensive data provide effective parameter initialization, which enables faster convergence across different applications.

Although there is abundance of unlabeled web data in the era of big data, obtaining high-quality labeled data with human annotations can be costly. For instance, data labeling companies like Scale.comcharge $6.4 per image for image segmentation labeling. Creating an image segmentation dataset with over 1 million high-quality samples (JFT-300M) could cost up to a million dollars. Usually, this process is time-intensive and inefficient. Moreover, methods of supervised learning can easily learn misleading connections within dataset, which can result in mistakes and thus may be more sensitive to adversarial exploitations.

To tackle above-mentioned challenges in supervised learning, several learning strategies are presented, such as active learning, semi-supervised learning, and SSL. Recently, Transformershave emerged as an effective Neural Network (NN) architecture for leveraging deeper insights in the input data, specifically, the ViTsare considered to be the alternative of CNNs. ViTs have been successfully employed in tasks related to computer vision, including object detection, recognitionand semantic segmentation. Notably, ViTs have demonstrated superior performance in image classification, when trained with large volume datasets such as JFT-300M.

In essence, by designing pretext tasks, a pre-training phase of network that encourage the model to learn significant patterns in the data itself, SSL methods enable ViTs to understand underlying structures of data without any supervision. The motivation for applying SSL in ViTs is two-fold. Firstly, SSL enables ViTs to employ the vast amount of unlabeled data that is readily available, allowing them to develop more robust and generalizable representations. SSL is beneficial in situations where labeled data is scarce or costly to acquire. Secondly, SSL provides a pathway for pre-training ViTs on larger volume of datasets, followed by of fine-tuning on downstream tasks, leading to improved performance and faster convergence.

SECTION: Survey Scope and Contributions
Several interesting surveys related to SSL have been reported. However, these surveys on SSL, serve to particular domains like recommendation systems, graphs,, sequential transfer learning, videosand algorithms.
Vision Transformers (ViTs) are a prominent area of research in computer vision and have recently demonstrated several breakthroughs. However, the training of ViTs remains challenging. Therefore, this survey, contrary to existing surveys, explores the different SSL methods proposed for ViTs.

SECTION: Survey Structure
The paper begins with an introduction to SSL and a detailed discussion on importance of SSL in ViTs. The reported SSL methods are segmented into 5 classes according to their unique features. The paper discuss the recent advancements and applications of SSL across different computer vision tasks. It also presents a taxonomy for SSL, which classifies them based on how feature learning is applied within their architecture. According to this taxonomy, SSLs are divided into five groups, each representing unique way of making use of input features. Frequently used abbreviations are listed in Table. The structure of paper is depicted in Fig. Section 1 discuss a systematic understanding of the SSL architecture, highlighting its need in ViTs and outlined the advent of SSL architectures. Proceeding to section 2 which covers the advancements in SSL variants, while section 3 provide a taxonomy of the recent SSL architectures, respectively.

Section 4 focuses on the regularization techniques used in SSL, particularly in the area of computer vision. Section 5 presents various metrics and benchmarks for evaluating the effectiveness of SSL, while Section 6 provides a comparative analysis of state-of-the-art SSL mechanisms employing Vision Transformers. Section 7 highlights some open challenges in the domain of SSL, and Section 8 discusses different applications and future directions for SSL in Vision Transformers, including current challenges and potential developments. Finally, Section 9 concludes this survey.

SECTION: Evolution of SSL
In computer vision SSL methods are generally segmented into contrastive, generative, and predictive approaches. Contrastive methods, such as MoCoand SimCLR, aim to learn patterns by contrasting positive and negative samples. Generative methods, such as GANsand VAEs, focus on learning representations by generating samples from the learned representation space. Predictive methods, such as BERTand T5, learn representations by predicting some parts of the input data.

One SSL category is instance discrimination, in which the motto of learning is basically focused on learning patterns by differentiating each sample image from others. This method is rather challenging for large volume of datasets. Examples are SimCLR, SwAV, BYOL, MoCO.

SECTION: Evolution of ViTs
In the domain of computer vision, ViTs is newly arising development motivated by the success of transformers architecture and it is presented in seminal paper ”Attention Is All You Need”. Transformers has revolutionized the learning process by enabling models to learn patterns from substantial volume of unlabeled data.

In computer vision, the traditional approach in image processing tasks is the utilization of convolutional neural networks (CNNs). However, the CNNs have limitations in capturing dependencies across long spatial distances and global context, both of which are crucial for some vision tasks. ViTs address these limitations via self-attention mechanisms to learn and understand the global context and also dependencies over long spatial distances in the input data .

The initial vision transformer architecture ViT was presented in the paper ”An Image Is Worth 16x16 Words”Since then, various variants of ViTs are proposed, including DeiT, Swin Transformer, and TNT. These architectures have obtained state-of-the-art performance in different computer vision based tasks, including image classification, object segmentation and object detection.

SECTION: Importance of SSL in ViTs
SSL is especially crucial in the context of ViTs, as it empowers models to learn representations from large amount of unlabeled data. ViTs need vast amounts of data to learn effective representations, and SSL provides a pathway for pre-training on large-scale datasets. By pre-training on large-scale datasets, ViTs can learn robust representations.

SECTION: Evolution of SSL in Computer Vision
SSL technique is used for representation learning where a model is designed to learn the underlying relationship between its inputs. The framework of Energy-Based Models (EBMs)can effectively describe this objective, where the aim of assigning high energy to inputs that are incompatible and low energy to those that are compatible.
The foundation for many self-supervised learning techniques applied to images was laid by Geoffrey Hinton and others in the 1980s through the development of autoencoders and the proposal of greedy layer-wise pre-training, where layers of a deep neural networks are trained sequentially. However, autoencoders were basically used for learning features and reducing dimensionality. They were impactful at that time as they made it possible to train the first “deep” networks. An analogous approach was Restricted Boltzmann Machines (RBMs), that allows layer-wise training and then combine those layers to build deep belief nets. Although these techniques were eventually replaced by simpler initialization strategies and extended training procedures, they played a vital part in the development of early deep learning architectures.

In 2000s, types of unsupervised learning, including sparse coding and K-means, were employed for self-supervised learning (SSL). These techniques aimed to learn image features without labeled data, laying the groundwork for more complex self-supervised methods. The category of Spatial Context prediction gained attention in SSL for the training of CNNs. In this regard, Pathak et al. (2016)proposed inpainting using context prediction as a pretext task for SSL in images, where the model predicts missing parts of an image. Zhang et al. (2016)explored a straightforward method of the colorization task for self-supervised representation learning. They trained a model to colorize grayscale images, which significantly enhances its capability to understand textures, patterns, and object identities. Noroozi and Favaro (2016)introduced a method where the model learns to solve jigsaw puzzles, using the relative positions of image patches as supervision. Gidaris et al. (2018)introduced RotaNet, which employs a pretext task where the model predicts the rotation angle applied to an image, enhancing feature learning for various orientations. Meanwhile, clustering approaches also progressed in SSL. Caron et al. (2018)proposed DeepCluster, a method that iteratively clusters image features and trains a neural network to predict the cluster assignment.

Contrastive learning became significant approach in self-supervised learning during the era of convolutional neural networks (CNNs). The roots of contrastive learning can be found in the early work on metric learning and the development of Siamese networksin the 1990s and early 2000s. Siamese Network architecture, introduced by Bromley et al. in 1993, involved training twin networks to lessen the relationship among embeddings of matching pairs and expanding the gap between embeddings of constrasting pairs. The formalization of contrastive loss by Hadsell, Chopra, and LeCun (2006)was a significant step forward.

In the late 2010s, the concept of employing contrastive learning for self-supervised learning gained attraction. This period saw the development of several influential techniques that leveraged contrastive objectives to learn meaningful patterns from unlabeled data. Dosovitskiy (2016)used instance discrimination as a pretext task. Each image was handled as a different class, and the network was trained to differentiate between differnt variations of the same image. Wu (2018).refined the instance discrimination approach using a memory repository to save negative examples for the contrastive loss. Oord (2018).introduced method known as Contrastive Predictive Coding in 2018, which learns patterns by predicting future observations in latent space using contrastive learning. Chen (2020)introduced a framework known as SimCLR for contrastive learning of visual features that trains by contrasting similar and non-similar pairs of data. This significantly advanced the performance of self-supervised methods in computer vision. Over time, the idea of hybrid learning gained strength, and in 2020, Caron (2020)introduced SwAV, which combines contrastive learning with clustering by using swapped assignments between different augmentations of an image.

The idea of self-distillation give a new dimension to SSL. Grill (2020)boosted the idea of SSL by introducing BYOL, a self-supervised method that avoids the need for negative samples (simplifying contrastive learning) by using two networks to predict each other’s outputs. He et al.introduced MoCo, which extends the idea of SimCLR by using a dynamic dictionary and a momentum encoder for contrastive learning. Facebook AI Research (Caron) presented DINO, which uses self-distillation with no labels, leveraging VITs for SSL.

In 2020, the idea of transformers in vision was introduced. Vision transformers are high-capacity models and need a huge data for model tuning and good generalization. For improving the learning ability of the transformers, Dosovitskiy (2020)adapted SSL techniques like masked image modeling and contrastive learning for ViT. Inspired by BERT (2018), Dosovitskiy et al.masked certain segment tokens and replaced them with trained mask tokens. Later the model was further trained for predicting pixel values directly. However, they found unsupervised pre-training technique notably less useful than pre-training with supervised methods. Before ViTs, the same idea of inpainting was employed in CNNs, where portions of an image were masked out and the model was taught to reconstruct them, this approach to pre-training is termed as MIM (Masked Image Modeling). In ViTs, the MIM was framed as a regression task. This involved initially utilizing an autoencoder, which encode image segments to separate tokens, subsequently, the transformer is pre-trained to estimate the distinct token values for masked tokens. At that time, Bert pre-training of image transformers (BEiT)demonstrated substantial enhanced results in image classification and semantic segmentation compared to earlier supervised and self-supervised methods. However, its training process was intricate as it required a robust autoencoder to transform image patches into discrete tokens.

The SSL techniques based on cross-covariance (correlation) analysis was introduced in 2021. This family originates from the Canonical Correlation Analysis (CCA) framework developed by Hotelling in 1992. The main idea behind utilizing CCA is to determine the correlation among two variables by examining their cross-covariance matrices. SSL methods based on this idea include VICReg, Barlow Twins, SWAV, and W-MSE.
The latest of these methods is VICReg that balances three goals: variance, invariance, and covariance. Controlling the variance along each dimension of the representation helps to prevent collapse, ensuring that the representation maintains diversity. Invariance ensures that two views of the same data are encoded similarly, while covariance promotes various dimensions of the representation to learn distinct features. This balanced approach allows VICReg to effectively leverage cross-covariance matrices for improved self-supervised learning.

Recently, the idea of multi-modality gained attention with the introduction of CLIP by OpenAI in 2021. CLIP learns image and text embeddings simultaneously by matching corresponding image-caption pairs. Similarly, ALIGNuses self-supervised learning approaches to develop a shared feature space for images and their corresponding captions.
The concept of MIM pre-training was streamlined by two concurrent works: MAE by He et al.and SimMIM by Xie et al.. These approaches propose simplified algorithms that immediately rebuild masked parts of image patches instead of using separate image pieces obtained from an encoder as done in BEiT. This idea was inspired by masked language modeling in NLP. These straightforward pre-training strategies performs better as compared to BEiT on classifying images, object detection and semantic segmentation (downstream tasks). MIM has recently dominated this domain by obtaining state-of-the-art performance on ViTs. The essence of this technique is to enhance the network’s capability to capture visual context at patch level using a denoising auto-encoding mechanism. Invariant Joint Embedding Predictive Architecture (IJEPA)has emerged as a promising alternative, addressing some of the limitations inherent in other self-supervised learning methods. Unlike traditional self-supervised approaches that depend heavily on hand-crafted data augmentations, IJEPA focuses on invariant feature learning and joint embeddings. IJEPA relies on predicting the patterns of destination blocks within an image from a single context block, using a strategic masking method to guide the learning process.

SECTION: Comparison of SSL with Transfer Learning
In computer vision, transfer learning (TL)is employed in the development of CNN models to solve a new challenging problem by utilizing the pre-trained model’s weight sharing mechanism especially in the presence of data scarcity.
Initially, transfer learning was the dominant approach, but recently, SSL has demonstrated promising results across various applications.

Although TL and SSL are not mutually exclusive techniques, they have a few major differences, as discussed in Table.. Moreover, Fig .shows the TL and SSL workflow. TL and SL have two main steps: (i) training on a source task and (ii) adapting to a specific target task. The aiming of first step is to optimize the network’s weight parameters to obtain a better starting point.

Challenges associated with TL or SSL especially in complex vision problems are due to several factors including
the volume of pre-training data and class imbalances in the source task. A study byexamined this issue across multiple sources and target tasks, yielding the following key insights:

When the visual scenarios in the training and target applications are significantly different—say, moving from natural scenes to medical images— SSL tends to perform better. On the other hand, TL is more effective when both the training and target tasks involve similar visual elements.

Working with a limited set of images for pre-training SSL offers better performance. However, availability of massive dataset for pre-training TL seems to outperform SSL.

SSL offers a more robust performance in transitioning between various types of visual tasks. For example, a model trained using SSL might adapt more easily from facial recognition to gesture recognition. Meanwhile TL may offers better performance when switching between these tasks.

When both the training and target tasks involve similar types of images, TL performance can vary widely. Whereas, SSL remains more stable under these conditions.

In class imbalance problem especially in vision tasks, SSL shows resilient compared to TL.

In SSL, supplementing pre-training with some of the target task’s data, for instance, using some annotated images/data can enhance the model performance. This strategy is not as effective for TL.

It is concluded that there is a trade-offs between these two methods and should be carefully considered according to specific needs and constraints of the vision task at hand. Emerging research continues to provide valuable insights into these methodologies’ comparative strengths and weaknesses, guiding practitioners in making informed choices for their specific use cases.

SECTION: SSL based Techniques for ViTs
In computer vision, the success of ViTs is based on large-scale self-supervised pre-training. This paper categorizes the approaches for self-supervised pre-training in ViTs into five groups: Contrastive, Generative, Clustering, knowledge distillation and Hybrid SSL methods. Each methodology’s nuances and impact on transformer learning are discussed in detail in subsequent sections.

Contrastive Learning (CL) stands as a prominent technique in SSL. Its aim is to capture invariant semantics through pairs of random views, termed Contrastive multi-view coding. CL ensures that global projections of representations are similar for positive samples and dissimilar for negative samples. This section delves into literature reviews on Contrastive Learning applied in transformer frameworks.

Wang and co-workers in 2022introduced the method SRCL that is semantically-relevant contrastive learning for histopathology image analysis. Unlike traditional contrastive learning, SRCL evaluates similarity among instances to identify additional positive pairs by arranging various positive examples that share related visual concepts. This approach enriches the diversity of positive pairs and yields more informative patterns. They employee a hybrid model named CTransPath, in which they combines a CNN with a multi-scale Swin Transformer architecture, their methodology acts as a unified feature extractor, addressing both local and global features during pre-training, seeking to acquire global feature interpretations tailored for activities within the domain of histopathology images. The efficacy of SRCL-pretrained CTransPath was evaluated across five distinct downstream tasks spanning nine publicly available datasets, such as retrieving patches, classifying patches, performing weakly-supervised classification of whole-slide images, detecting mitosis, and segmenting glands in colorectal adenocarcinoma. The findings demonstrated that the visual representations derived from SRCL not just achieved state-of-the-art performance throughout all datasets but it also exhibited increased robustness and transferability compared to both supervised and self-supervised ImageNet pre-training techniques.

To address instability issues during training, Chen and his colleagues in 2021utilized SSL framework in MOCO, representing an advancement in Contrastive Learning (CL) methodologies. MOCO utilizes dictionaries to gauge similarity between encoded keys and queries. Empirical assessments suggested that self-supervised Transformers within the MOCO framework can demonstrate good performance with relatively fewer inductive biases compared to ImageNet supervised ViT. This approach aims for a better balance between simplicity, accuracy, and scalability, showcasing its superiority over supervised pre-training methods in multiple detection and segmentation tasks. The study proposes replacing the patch projection layer in ViT with fixed random patch projections and suggests removing positional embeddings in SSL setups to address instability.

Mo and his teamin 2023 introduced MCVT, which focuses on projecting class tokens to embedding space during the initial or final phases of the ViTs using multi-layer perceptrons. In this methodology, InfoNCE loss is applied to low-level features while ProtoNCE loss is introduced in high-level features, enhancing performance in downstream classification tasks. Extensive experimentation transferring MCVT pre-trained backbones to various downstream tasks confirmed its efficacy across multiple vision-related tasks.

Radford in 2021proposed a Contrastive Language-Image pre-training (CLIP) framework, which incorporates transformers, learns image representations through natural language supervision. In pre-training, CLIP concurrently trains image and text encoders to accurately predict corresponding pairs within a batch of (image, text) training instances. CLIP discerns among thepossible (image, text) pairings within a batch. It achieves this by learning a multi-modal embedding space, optimizing the image and text encoders jointly to increase the cosine similarity of embeddings for the N genuine pairs while decreasing the resemblance of embeddings for the
( N 2̂ - N )
incorrect pairings. Symmetric cross-entropy loss is used in this optimization strategy to adjust these similarity scores. After pre-training, the model uses natural language to reference acquired visual concepts, facilitating zero-shot transfer to downstream tasks, demonstrating substantial transferability to diverse computer vision tasks without the need for task-specific training data. Evaluation encompasses benchmarking across more than 30 diverse computer vision datasets. Remarkably, the model demonstrates substantial transferability to most tasks and frequently matches or surpasses fully supervised baselines without requiring task-specific training data.

Knowledge distillation transfers knowledge from a complex teacher model to a simpler student model without labeled data. This section explores literature on knowledge distillation in SSL applied to ViTs.

Caron et al.in 2021 introduced knowledge distillation into SSL to enhance the training of ViTs. Their method, DINO, comprises two identical networks termed student and teacher networks, each with distinct parameters. Both networks include an encoder or base network, utilizing either ViTs or ResNet50, along with a projection head placed on top of the encoder network. The DINO approach involves generating multiple crops from two perspectives: a local perspective and a global perspective. The global crop perspective is fed to the teacher network, while the local crop perspective is fed to the student network. The local crop is a subset of the global perspective, exhibiting overlap between the two. The teacher network guides updates in the student network, but gradient stop is applied to halt backpropagation in the teacher network. Meanwhile, the teacher network weights are adjusted using the EMA (exponential moving average) of the weights from student network.

Li et al. (2022)proposed Efficient Self-Supervised ViTs (EsViT) to achieve a optimized balance between accuracy and efficiency, reducing the cost in building state-of-the-art SSL vision systems while demonstrating enhanced scaling performance on accuracy versus throughput and model size. They introduced a multi-stage Transformer architecture, combining networks to address the issue of semantic information loss when transitioning from a single-stage transformer to a multi-stage architecture. Employing a distillation learning strategy targeting correspondence between patches, they introduced a new non-contrastive region-matching pre-training task, enhancing the model’s ability to learn intricate regional dependencies and substantially improve the quality of visual representations learned.

The Multi-stage ViT segments an input RGB image into individual non-overlapping patches, handling each patch as an individual token in the first stage. Later, the patch merging module concatenates the features of each group and applies a linear layer to reduce the number of tokens. Subsequently, a Transformer with a sparse self-attention module enables interactions among the merged features. This process is repeated multiple times, resulting in a hierarchical representation. EsViT, the proposed model, is validated across various tasks. EsViT achieves 81.3% top-1 accuracy, with the linear evaluation protocol, demonstrating 3.5× parameter efficiency and at least 10× higher throughput compared to previous state-of-the-art models. EsViT leads over its supervised counterpart, Swin Transformers, by outperforming on 17 of 18 datasets in downstream linear classification tasks.

Xie et al. (2021)introduced MoBY, a SSL framework tailored for ViTs. This approach combines MoCo v2 and BYOL approaches, yielding notably high accuracy on the ImageNet-1K dataset. MoBY adopts the key queue, momentum design, and contrastive loss from MoCo v2, and it also take asymmetric data augmentations, asymmetric encoders, and momentum scheduler from BYOL. Employing a knowledge distillation approach, MoBY employees a pair of neural networks (online and target) to enhance mutual learning. Both encoders includes a backbone and a projector head, with the online encoder featuring an additional prediction head, rendering the two encoders non-uniform. Gradients progressively optimizes the online encoder, while the target encoder is a moving average of the online encoder updated through momentum. MoBY is evaluated across diverse downstream tasks, such as object detection and semantic segmentation, showcasing robustness and transferability across varied visual tasks. When utilizing DeiT-S and Swin-Transformers as backbone architectures, MoBY achieves top-1 accuracy rates of 72.8% and 75.0%, respectively, on ImageNet-1K.

Clustering-based SSL utilizes unsupervised clustering algorithms to learn representations from unlabeled data. This technique groups similar data points based on specific criteria, such as feature similarity, and then predicts these clusters or cluster assignments. By encouraging the model to distinguish between different clusters, clustering-based SSL extracts semantically rich features for downstream tasks like classification or segmentation. This approach proves valuable when labeled data is limited or costly to obtain, enabling the model to learn useful representations directly from the unlabeled data.

Clustering-based methods segment the latent space by applying clustering. This clustering can be performed offline, utilizing the entire dataset, as in PCL, DeepCluster v2, and SeLa, or online, using mini-batches, as in DINOand SwAV. These approaches enforce consistent clustering assignments of positive pairs using cross-entropy loss. However, previous literature have not taken advantage of this beneficial property in semi-supervised scenarios. Our study aims to fill this gap.

Su et al. (2023)introduced a clustering-based SSL framework tailored for ViTs, aiming to enhance performance of tasks related to object detection and segmentation. Their proposed methodology, Feature-Level SSL (FLSL), works at two levels of semantics: intra-view, which deals with individual image, and inter-view which considers over an entire dataset. FLSL utilizes a bi-level feature clustering approach, integrating mean-shift clustering intrinsic to transformers for extracting modes as patterns with a k-means-based SSL technique, ensuring semantic coherence of extracted representations both locally and globally.

At the first semantic level, FLSL optimizes intra-cluster affinity using a self-attention layer to encourage semantic representations within clusters. On the second tier, semantic representations are cultivated through non-empty k-means clustering, with positive samples identified using a cross-attention layer. Experimental outcomes illustrate the efficacy of FLSL in dense prediction tasks, achieving significant improvements in object detection and instance-level segmentation on MS-COCO, utilizing Mask R-CNN with ViT-S/16 and ViT-S/8 as backbones, respectively.

Fini et al., (2023)leverage the limited annotations in the semi-supervised setting to enhance the quality of learned representations. The core idea involves substituting cluster centroids with class prototypes learned through supervised methods, guiding unlabeled data to cluster around these prototypes through a self-supervised clustering-based objective.
Their approach involves the joint optimization of a supervised loss on labeled data and a self-supervised loss on unlabeled data.
For a given an unannotated dataset{},
two versions of each input image, (x, xˆ), are created through data augmentation technique. These versions are then processed through an encoder network f. This encoder network is comprised of a projector, backboneand a set of centroids or prototypesimplemented as a bias-free linear layer.
The forward pass through this network yields two latent demonstrations (p, pˆ), corresponding to the two augmented views. The prototypesare then employed to produce logits (c, cˆ) for each representations. These logits are transformed into probability distributions over clusters using softmax functionresulting in cluster assignments.

Generative approaches, for example Generative Adversarial Networks (GANs) or Autoencoders, provide a promising avenue for pre-training ViTs. These models not only generate realistic images or reconstruct input images but also encode meaningful representations of visual content, facilitating enhanced learning of features in subsequent tasks.

Chen et al. (2020) introduced iGPT, an image-based version of GPT, for self-supervised visual learning. Unlike ViT, which uses patch embedding, iGPT directly resizes and flattens images into lower-resolution sequences, which are then fed into a GPT-2model for autoregressive pixel prediction. Despite its significant computational cost, iGPT achieves high accuracy on CIFAR-10, surpassing supervised Wide ResNet.

Bao et al. (2022)introduced BEIT, that is Bidirectional Encoder representation from Image Transformers is a self-supervised vision representation model. Instead of pixel-wise generation, they adopted MIM as a task to pretrain ViTs in a self-supervised manner. BEIT, grounded on a BERT-style visual Transformer, reconstructs masked images in the latent space. The approach involves randomly masking parts of image pieces and predicting the visual tokens relevant to these hidden pieces. The main motivation behind this pre-training technique is to restore the base visual tokens utilizing corrupted image patches. The standard visual Transformer serves as the backbone network. Evaluation using thorough fine-tuning for tasks such as image classification and semantic segmentation demonstrated competitive performance without human annotation.

He et al., (2022)proposed MAE (Masked Autoencoders), in this methodology parts of input images are randomly masked and subsequently reconstructed the pixels that were masked, enabling optimized training of large models that demonstrate strong generalize across downstream tasks. This method utilizes an asymmetric encoder-decoder setup, where the encoder only looks on the visible parts of image, and a small decoder rebuilds the original image usingthe learned representation and mask tokens. By hiding a large proportion of the input image, e.g., 75%, MAE creates a challenging and useful self-supervised learning task. This scalable method allows large models to train efficiently and effectively, resulting in powerful models that generalize well. For instance, a standard ViT-Huge model achieved the highest accuracy of (87.8%) among techniques using only ImageNet-1K data. Supervised pre-training was outperformed by transfer learning in downstream task, showing strong potential for scaling behavior.

Hybrid methods similar to generative approaches often combine the strengths of different techniques to achieve specific objectives.

Wu et al. (2024) implemented the hybrid SSL pre-training framework Enhanced SSL with Masked Autoencoders (ESLA). This innovative approach integrates both contrastive learning (CL) and MAE frameworks to enhance SSL. ESLA improves model generalization and representation while addressing issues stemming from competing features during data enhancement. This is achieved through a masking mechanism in the CL framework and by blending representations of different views in the latent space.

During training, ESLA conducts data augmentation and masking on input data to create two views.
The online encoder receives EMand target encoder get EM.
The ”mixer” combineswithand restoration masks to generate a ”mixed latent,” which is then used to compute the reconstruction loss. ESLA’s structure comprise of two Interconnected branches known as CL and MAE branch respectively. There are two kinds of encoders in CL branch including online and target encoder that generate distinct intermediate representations (IRs),and. To decrease the computational power, an encoder incorporating a masking technique is used to encode two modified variants, referring to the same image.
A contrast loss is generated by evaluating theandusing predictor and projector.
Rather than being trained with gradient descent, the target network uses an exact replica of online network,
following it with a delayed using an EMA that is exponential moving average.
In MAE branch, a feature map is generated by combining the information with the help of mixer that corresponds to orignal image.
This feature map is unmasked and forwarded to the decoder for computing the reconstruction loss. In this context shallow data enhancement occur in pixel space () that produces two distinct views of the same image while in the latent spacedeep data enhancement is executed that generate the ”mixed latent”, as an aspect of the decoder input.

Quan et al. (2023)introduced a novel SSL model, based on the global contrast-masked autoencoder (GCMAE), which adeptly extracts both global and local features from pathological images. GCMAE leverages masking image reconstruction and contrastive learning as self-supervised pretext tasks, enabling the encoder to proficiently depict local-global features. This model comprises of an initializer, an encoder, a tile feature extractor, and a global feature extractor facilitating image reconstruction and contrastive learning tasks. In this investigation, an asymmetric encoder-decoder architecture is employed. ViT serves as the backbone for the encoder, pretrained using MAE. Its module related to patches feature extraction is comprised of eight transformer blocks, forming an asymmetric structure in conjunction with an encoder featuring of about 12 transformer blocks (ViT-base). This asymmetry facilitates decoupling of the encoder and decoder, fostering the acquisition of more generalized representations. Extraction of global feature is achieved via utilizing contrastive learning. The hidden representationpatch() is used to reconstruct image through decoder and update a memory bank (). This updation ofis executed to store the global features utilizing momentum coefficient. The dynamically update is conducted to pursue the feature vectors of image data using memory bank as a fixed size queue. The various features that are randomly picked from the memory bank are used as a negative samples that helps to reduce the effects of the constraint caused by batch size on the heavy performance.
Notably, the momentum adjustment mechanism is designed distinctively from MoCo’s model parameter update, it employ an encoder and a memory bank to facilitate contrastive learning.
Vpath(vis) is depicted as the encoders output in the current epoch in the latent representation, where as the vpatch(vis) represented the previous epoch latent representation stored in.
Particularly, the latent demonstrations at the output of encoder aspatch(vis) in the current epoch,
The cost function combines both, the weighted sum of MSE loss for tile feature extraction and the NCE loss for global feature extraction. This formulation aims to minimize the gap between similar features, enhancing the model’s generalization and accuracy, particularly in cross-dataset transfer learning tasks.

Fang et al. (2023)proposed a hybrid SSL framework (HSL) for Hyperspectral Image (HSI) classification. Leveraging MIM and contrastive learning to enhance its effectiveness. Employing a ViT as the backbone network within an asymmetric encoder-decoder architecture with two branches,
HSL achieve improved HSI classification utilizing contrastive pre-training and self-supervised generative tasks by effectively extracting spatial and spectral details.
To facilitate image reconstruction through masked inputs, each branch connects intermediate features of the encoder to the decoder at the corresponding level.
Contrastive learning is accomplished by utilizing the intermediate features that are obtained from two encoders that share parameters across both branches. Utilizing standard data augmentation methods, HSL generates several views of input images. Then each image view undergoes random masking technique such as MAE.
Initially, image segments with mask are reorganised into a series of patches and later processed by HSL encoder and decoder to extract patterns. Extracted features are then pass through an adaptive average pooling layer and a projection head to achieve the contrastive learning task of capturing image resemblance. The decoder, which also includes multiple Transformer encoders, rebuilds patches after certain patches have been masked. Moreover, the Transformer Encoder retains the initial structure while the two output features from the encoder are used to compute contrastive loss. The loss is calculated using the decoded sequence produced by the decoder and the sequence that precedes the encoder masking, efficiently completing the masked image rebulding process.
In summary, HSL combines contrastive pre-training and self-supervised generative tasks, facilitating effectiveness for downstream HSI classification task.

SECTION: Regularization Techniques
In basic terminology, regularization techniques helps in making model simpler. As Occam’s razor principle state, simpler models usually work better, this means keeping things straightforward is key. By using different techniques, we narrow down the model’s options to make it more focused. The integration of SSL methods with traditional regularization techniques offers a nuanced approach to improving the generalizability and robustness of computer vision models. This analysis explores how various SSL methods incorporate regularization principles within their frameworks.

employs a teacher-student framework, where the student network predicts the output of a teacher network on augmented inputs. The teacher network is updated as a moving average of the student’s parameters, ensuring learning consistency. This method mirrors the stabilization effect seen in regularization techniques like batch normalization and parameter sharing.

leverages a momentum-based moving average of the query encoder, establishing a large and consistent dictionary. This approach acts as a dynamic regularization mechanism, ensuring stability in the representation of dictionary keys over time. This stability is vital for the extraction of stable and generalizable features, akin to how traditional regularization methods prevent overfitting by maintaining consistency in model parameters.

utilizes a straightforward contrastive learning framework, where its regularization is primarily through aggressive data augmentation techniques. These augmentations include random cropping, resizing, color distortion, and Gaussian blur, enabling the model to learn invariant and robust features. This is similar to how techniques like dropout and data augmentation in traditional settings prevent over-reliance on specific input features.

utilizes self-distillation for regularization, where the student network mimics the teacher network, updated as an exponential moving average of the student’s parameters. This method is reminiscent of techniques like early stopping and weight decay, which aim to prevent overfitting by smoothing the learning trajectory.

method aims to align the cross-correlation matrix between the outputs of twin networks with the identity matrix. This alignment encourages the model to learn features invariant to input distortions, functioning as a regularization technique by discouraging the learning of redundant features.

regularization is unique, involving clustering approaches and consistency enforcement between cluster assignments for different views of the same image. This approach is akin to enforcing a constraint (regularization) on the feature space to enhance generalizability.

focuses on learning temporally consistent representations, acting as a regularization mechanism by ensuring robustness to temporal variations in data. This is similar to entropy regularization, which encourages the model to learn more uncertain, hence generalizable, features.

regularizes learning by enforcing invariance to different views of the same scene. This invariance is a form of regularization, similar to dropout variations that encourage the model to learn features that are invariant to specific changes in the input.

approach involves pseudo-labeling, where the network assigns labels to its inputs and learns from these self-generated labels. This process regularizes the model by forcing it to generalize from its initial, potentially noisy, self-generated labels, akin to label smoothing which also deals with label noise.

In conclusion, these SSL methods embody various facets of regularization, each employing unique strategies to instill robustness and generalization in the learned representations. They extend the traditional notion of regularization beyond mere parameter adjustment, delving into feature space manipulation, consistency enforcement, and self-reflection in learning processes.

SECTION: Evaluation Metrics, Benchmarks and loss functions
In this section, we will discuss the evaluation metrics and benchmarks used to evaluate the performance of SSL (SSL) methods in ViTs.

SECTION: Performance Metrics for SSL in ViTs
The performance of SSL methods in ViTs is typically evaluated on downstream tasks, such as image classification, object detection, and semantic segmentation. The performance metrics used to evaluate these tasks include accuracy, precision, recall, F1 score, and mean average precision (mAP). These metrics provide a quantitative measure of the performance of the SSL method in comparison to other methods.

Accuracy is a measure that represents the true positives (both true negatives and true positives) of the under observation data. Accuracy and error rate are mutually exclusive and range between 0 to 100. Accuracy is computed by dividing the number of correct predictions by the total prediction number as:

It may be noted that accuracy is generally a good metric as long as the datasets are well-balanced. However, with imbalanced classes, it may shows erroneous readings as the numbers of rightly classified samples of different classes are not considered.

Precision, also known as a measure of quality, quantifies the total number of correct positive predictions.
More specifically, it is computed as the number of rightly predicted positive samples divided by the total number of positive examples that were predicted as:

Note that the precision is usually preferred in the cases when the false positives are rarely desireable whereas false negatives are tolerable in a learning system.

Recall is the total number of data samples that a learning network correctly predicts as belonging to the positive class as:

This gives an estimate of the sensitivity of the network. In other words, this provides an idea of the probability that a sample from actual positive class will be classified as positive. It may be noted that recall as a metric is preferable when it is not desired to miss any relevant sample even if it means classifying some samples incorrectly as positive class.

F1 score is computed by taking harmonic mean of Precision and Recall as:

In simple form, it becomes:

The reason harmonic mean is used instead of geometric mean is to to balance the values for both Precision and Recall. The more the precision and recall scores deviate from each other, the worse the harmonic mean.

Note that F1 Score is usually preferable when the classes are imabalanced in the dataset which is usually the case in real-world datasets.

SECTION: Benchmark Datasets for SSL in ViTs
Several benchmark datasets have been used to evaluate the performance of SSL methods in ViTs. Some of them are discussed below.

The ImageNet classification datasetcontains roughly 1.2 million images for training, fifty thousand images for validation, and 100000 images for testing. Overall, the dataset is divided into one thousand diverse object classes including tablelamp, goldfish, radiator, screwdriver, and so on.

COCO (Common Objects in Context)is a large-scale object detection, key-point detection, segmentation, and captioning dataset. The dataset contains over 330,000 images of which around 200,000 are fully labeled. The dataset is divided into 80 object classes and over 90 generic ‘stuff’ classes.

The CIFAR-10 a short for Canadian Institute For Advanced Research is a datasetthat contains sixty thousand
images divided into 10 classes in which every class contains 6000 images. In total, 50000 images are utilized for training purpose while the rest of 10000 images are utilized for testing purpose. The dataset comprises of 10 classes, including horse, airplane, frog, automobile, cat, deer, bird, dog, ship, and truck.

CIFAR-100 datasetis an extension of CIFAR-10. Although the total number of images are same on both datasetss, i.e., sixty thousand, CIFAR-100 is divided into 100 distinct classes with 600 images per class. Similar to CIFAR-10, a total of 50000 images are utilized for training and 10000 images are employed for testing. Every image contains two labels, a coarse label (general category) and a fine label (particular class). Some particular object classes include apple, bee, cattle, chair, rabbit, and wowan, whereas general categories include flowers, household_furniture, medium_mammals, people, etc.

SECTION: Loss functions
Machine learning algorithms must include loss functions to gauge how well a model is doing on the job at hand and acts as a feedback signal to adjust the model’s parameters during training. By identifying and quantifying the discrepancy between expected output and the actual target labels, they play a critical part in directing the learning process.
Over the past few years, contrastive learning has appeared as an effective approach in the realm of unsupervised representation learning, revolutionizing how neural networks can be trained without labeled data. This approach hinges on the idea of learning meaningful representations by increasing the similarity among positive pairs and decreasing the similarity of negative pairs. Among the several contrastive loss functions, the InfoNCE loss(Noise Contrastive Estimation) stands as a common choice. It operates by introducing noise to the input data to create negative pairs, enabling the model to differentiate between real positive pairs and artificially generated negatives. Another widely used loss is the NT-Xent loss(Normalized Temperature-Scaled Cross Entropy), which extends InfoNCE by incorporating a temperature parameter to scale the pairwise similarities. Through a softmax function, it computes the negative log-likelihood of positive pairs, enhancing the overall contrastive learning process. SimCLR loss (Simple Contrastive Learning) represents yet another approach, harnessing multiple augmentations of the input data to build diverse views of the same instance. It seeks to minimize the similarity between views of different instances while increasing the correspondence between varied augmented views of the same instance.
For learning representations or embeddings, the triplet losshas gained popularity. It utilizes an anchor, a positive example that is close to it, and a negative example that is distant. The objective of the loss function is to reduce the distance between the anchor and a positive example while increasing the distance from the negative example, effectively creating a margin between them. The concept of margin loss also focuses on pairwise comparisons, but its objective is to ensure a margin between the representations of positive and negative pairs. By creating pairs of instances and optimizing the model, the loss pushes positive pairings closer together while pushing negative pairings farther apart. In response to the shortcomings of triplet loss, the Circle Losswas recently developed. By establishing a decision boundary in a hypersphere around instances and incorporating a flexible buffer based on the instance distances, it encourages positive pairs to lie within the boundary while keeping negative pairs outside.
These are just a few instances of the wide array of contrastive loss functions employed in various research works. The selection of a loss function is determined by factors like the specific problem, the nature of the data, and the required characteristics of the learned representations. Researchers often experiment with multiple loss functions to determine the most suitable one for their specific task. As contrastive learning progresses, more innovative loss formulations and improvements are likely to emerge, opening up new possibilities for unsupervised representation learning .

SECTION: Comparison with Supervised Learning Approaches
SSL, particularly in the context of ViTs, has completely changed the area of computer vision. Large volumes of unlabeled data can be used with these techniques to acquire meaningful representations, reducing the substantial reliance on labeled instances observed in conventional supervised learning. The main benefit of SSL is its capacity to take advantage of the wealth of unlabeled data that is readily available online, allowing models to learn from big datasets without the need for explicit annotations. SSL is especially appealing for real-world applications because of its scalability and affordability given that manual data labeling can be resource-intensive, costly, and require significant effort.

The comparative evaluation of SSL and supervised learning methods using the same downstream tasks and evaluation metrics has shed light on the effectiveness of SSL in ViTs. In situations where the annotated data is limited or challenging to gather, SSL has demonstrated its potential to achieve competitive or even superior performance. This is especially valuable for real-world scenarios where manually annotating data might not be feasible, yet high-performance computer vision models are needed. A comprehensive learning framework known as ”SSL” employs pretext tasks derived solely from unsupervised data. These pretext tasks are designed to learn the meaningful visual representation in order to complete them effectively. Whereas, in the supervised learning paradigm, each input sample is connected to its matching target or output label, and the system is trained on a labeled dataset. In order for the model to accurately predict outcomes on data that has not yet been observed, supervised learning aims to establish a relationship between input data and their associated output labels.

SECTION: Comparative Analysis of SSL Mechanisms
In this section, we will provide a comparative analysis of state-of-the-art SSL (SSL) mechanisms employing ViTs. Various SSL methods used different networks as their backbone networks. For a fair comparison, we compare different SSL methods employing ViT-B/16as their backbone network.

SECTION: Performance Comparison
We compare SOTA SSL methods, in Table, where self-supervised pre-training is performed over ImageNet-1Ktraining dataset. Later, fully supervised training is performed to evaluate the feature representations using (i) linear probing and (ii) end-to-end fine-tuning. The inputs are resized to 224x224 crops and report top-1 validation accuracy.
From Table, we notice that DINO, MoCo-V3, MSG-MAE, iBOT, and EsViTachieve more than 75% top-1 accuracy, in terms of fine-tuning using linear probing. It is notable that the top-performing EsViTemployed Swin-Barchitecture as the backbone network. In contrast, almost all the methods obtain over 82.0% in terms of end-to-end fine-tuning. However, MSG-MAE, TEC, and dBOTpresent the top-1 accuracy of 85.3%, 84.7%, and 84.5%, respectively.

SECTION: Performance evaluation over downstream tasks
We further assess the effectiveness of self-supervised methods for downstream tasks such as object detection, semantic segmentation, and instance segmentation in Table. We also demonstrate the generalizability capabilities of visual representations learned by self-supervised methods by fine-tuning the
pre-trained models on smaller datasets in Table.

We present a comparative analysis of SOTA self-supervised approaches for downstream tasks such as object detection and instance segmentation (as shown in Table. For a fair comparison, we compare methods having ViT-Bas the backbone architecture. The methods are trained over COCOdataset considering Mask R-CNNas the task head for object detection and instance segmentation and evaluated using average precisionand, respectively. The results in Tabledemonstrate that TEC, dBOT, and MSG-MAEoutperform other SOTA SSL methods over object detection and obtainscores of 52.3, 52.7, and 52.3 respectively. On the instance segmentation, MSG-MAE, TEC, and dBOTshow significant promising performance compared to other SSL methods and achieve 48.8, 47.2, and 45.7scores, respectively.

In Table, we present a performance comparison of SSL methods for semantic segmentation, where UpperNetis trained with ViT-Bover ADE20Kdataset.
From Table, it is notable that BootMAE, Diet-III, Diet-III+AugMask, MSG-MAE, TECand dBOTexhibit above 49.0% mIoU. We also observe that top performing Diet-III+AugMask, TEC, Diet-III, and MSG-MAE, demonstate the 50.2%, 49.9%, 49.7%, and 49.7%, respectively.

To further study the generalizability of the SSL methods, we compare their transfer learning capabilities in terms of accuracy. To do so, the pre-trained methods are fine-tuned over small datasets such as CIFAR-10, CIFAR-100, iNaturalist18(iNa18), iNaturalist19(iNa19), Flowers, and Cars. The results in TableDeit-IIIdemonstrate outstanding performance over state-of-the-art SSL methods and achieve 99.3 and 92.5 accuracy over CIFAR-10 and CIFAR-100 datasets, respectively. We also note that dBOTillustrates better performance over
CIFAR-10, iNa18, and iNa19 datasets, and show accuracy of 99.3, 77.9, and 81.0, respectively. Similarly, iBOTexhibits the best performance over Flowers and Cars datasets while obtaining 98.9 and 94.3 accuracy, respectively.

SECTION: Robustness to Perturbations
We also present a comparative comparison of SSL methods over various robustness datasets including natural adversarial examples(IN-A), objects in different styles and textures (IN-R), controls in rotation, background, and viewpoints (ObjNet), and SI-scores(SI-size, SI-loc, and SI-rot).
The results in Tableshow that Diet-III+AugMaskhas better robustness across various robustness metrics.

SECTION: Advancements and Open Challenges
In this section, we will discuss the recent advancements and open challenges in SSL (SSL) for ViTs.

SECTION: Data Augmentation as supportive technique
Data augmentation is a crucial aspect of SSL for ViTs. Data augmentation strategies can assist the SSL models to achieve more robustness and generalizability representations from unlabeled data. Recent advancements in the field including generative models to generate augmented data, incorporating adversarial perturbations, and unsupervised clustering to generate diverse augmentations, to enhance learning capability of the model. However, there is still a need for more effective and productive data augmentation techniques which can help to improve the effectiveness of SSL methods.
In supervised and SSL environments in ViTs, data scarcity is one of the challenges for convolutional neural networks (CNN) models training and generalization. Usually, various conventional strategies of data augmentation such as rotation, contrast enhancement, cropping, and flipping are used in literature for improvement of the learning and generalization ability of the models.
The SSL approaches learned contextual representation to predict labels without any human interferences and learned the transformation from input data. In SSL the self-supervised labels are optimized by employing two loss functions namely, the original and the self-supervised, that utilized the feature space sharing strategy. There are mainly two types of data augmentation strategies: (i) traditional and (ii) advanced. Detail of these techniques are given as follows:

Augmentation using geometric transformation is being used for various image processing applications. By employing this transformation, the original positions of the image pixels are relocated to new positions without modifying intensity values. It modifies the training data by incorporating variations in viewpoint, non-rigid deformations, and scaling for better model learning on real world problems. The geometric transformations including the translation, rotation, reflection, shearing, and scaling are being commonly used for data augmentation. It is categorized into affine and non-affine transformations. The former one utilized linear mapping for adjusting the geometric disorder in the structure of the image. Whereas the later involved complex non-linear mapping functions.
The projective and perspective transformations are fall into the non-affine geometric transformations. These are effectively being used in computer vision and medical imaging tasks.
Similarly, the deformation transformations are non-affine which are used to simulate at higher degree of freedom such as Lens and non-rigid body deformations.
Photometric transformationis also fall under the traditional augmentation category and used for various computer vision tasks. It considers the camera and shooting artifacts such as optical noise, motion blur, image color artifacts degradations. These simulations are being used to mitigate the effects of data scarcity during model training.

The advanced augmentation strategies are used to solve complex computer vision problems and gained a significant attention of researchers. These augmentation operations are learned from the given data automatically. Several CNN based models are proposed in literature to perform various advanced transformations for data augmentation. These deep models have several cascading layers to learn the transformations, consequently obtained a better trained and generalized model. Jagerberg et al. (2015)proposed spatial transformer network (STN) data augmentation approach which is used for model training. Based on STN data augmentation model, several improved models are built which offer promising results. Although the advanced models are computationally expensive compared to the traditional methods. However, the accuracy and diversity of data augmentation of these models are very high. They are effective to solve various data scarcity related computer vision and SSL problems.

Data augmentation is an imperative task for improvement in performance of the SSL and ViTs. It is successfully applied to solve different computer vision problems. However, there are several open challenges related to uses of data augmentation in SSL and ViTs:

Designing of augmentation strategies:Generally, traditional methods for data augmentation like flipping, random cropping, and color jittering are used. It is not a simple task to design a strategy for effective data augmentation in SSL. Exploring novel and appropriate augmentations which exploit certain characteristics of ViTs is an open challenge.

Robustness to complex variations:Usually, real-world images having complex variations such as in lighting, occlusion, viewpoint, etc. It is a major challenge to validate that during SSL and data augmentation process such complex variations are truly accounted.

Maintaining of consistency and discriminability:Typically, SSL techniques use produced augmented visions of the same image and capture useful patterns to support consistency that is small changes in the input will reflect in the learned representations. In order to maintain discriminative information, it is challenging to discover the sufficient balance between produced augmented visions consistent for learning.

Scale Sensitivity:ViTs work excellent on large-scale datasets. The data augmentation methods well-behaved on data scarcity problems and it might not be effective on larger-scale datasets. Development of augmentation methods, which are capable to apply on larger-scale datasets is a challenge.

Computational Efficiency:Commonly, ViTs are computationally intensive, and incorporation of data through augmentation techniques would make problem more intensified. Exploring adequate data augmentation methods which are computationally efficient and offering valuable benefits to SSL is a practical challenge.

Robust against adversarial attacks:Data augmentation methods usually are not vulnerable against adversarial attacks. It is open challenge to develop efficient data augmentation approaches which improve developed model robustness against various forms of adversarial attacks.

SECTION: Incorporating Spatial Information
Incorporating spatial information into SSL-based ViTs is an active area of research nowadays. Spatial details are essential for grasping the structural relationships within images and can improve the model’s ability to capture meaningful features. One recent trend incorporating spatial information is utilizing positional embeddings using sine/cosine and axial position embeddings. Spatial relationships can also be better captured using grid masking. The attention mechanisms can also be used to capture the spatial dependencies, e.g., axial attentionwhich can help the model to focus on local and global spatial features. However, incorporating spatial information might be challenging due to increase the computational complexity of transformers. Further, it complicates when dealing with multi-modal data or tasks that require understanding spatial-temporal relationships. Therefore, spatial attention is one of the components of SSL and ViTs that can be encountered when solving a complex problem.

SECTION: Improving Sample Efficiency
The performance of sample efficiency can be improved by following some of the good practices. For instance, bigger training datasets can be further divided into subsets to capture fine details of input data. If the training dataset is already small, then labeled data can be combined with unlabeled data to perform SSL learning. Regularization methods such as dropout, weight decay, and layer normalization can also improve sample efficiency. Regularization helps to prevent overfitting and improve the model’s generalization. It is recommended to employ the transfer learning concept from larger, pre-trained models for training of smaller ViTs to mimic the behaviour of the larger models. By following this practice, smaller model’s performance can be improved while having limited dataset. Fine-tuning strategies such as layer freezing, gradual unfreezing, and learning rate schedules can also affect the sample efficiency.

SECTION: Interpretability and Explainability
Interpretability and explainability are critical for the utilization and veracity of the models. To understand the inner working mechanism of the model while solving real world problems, the following steps might be considered while designing the ViTs models. For interpretability, the important aspects include feature visualization, activation analysis, and attribution methods can be considered during model development. While, for explainability of ViTs models, the attention mechanism, attention maps, feature attribution, concept activation, and shapely values need to be accounted.

In summary, recent advancements and open challenges in SSL for ViTs include improving data augmentation strategies, incorporating spatial information, improving sample efficiency, and improving interpretability and explainability. Addressing these challenges will enable the development of more effective and efficient SSL methods for ViTs.

SECTION: Applications and Future Directions
In this part, we will explore various applications and potential future directions of SSL for ViTs.explores how SSL, coupled with climate-smart agriculture, helps in analyzing environmental data for sustainable farming practices that reduce greenhouse gas emissions and improve resilience to climate change.discusses the role of SSL and AI techniques in precision farming, optimizing nutrient use, and improving crop yield prediction, thereby aiding in environmental sustainability.made a Comparative Study in which they compares various optimization techniques aimed at reducing sidelobe levels (SSL) in antenna arrays. It evaluates the effectiveness of methods like Genetic Algorithm (GA), Particle Swarm Optimization (PSO), and Differential Evolution (DE) in minimizing sidelobes to improve signal clarity and performance.

SECTION: Few-shot Learning
Few-shot learning is a paradigm where the goal is to learn a task from limited labeled datasets.
The aim of few-shot learning is to overcome the challenge of data scarcity for training models and better generalization.
In Natural Language Processing (NLP), SSL pre-training has demonstrated exceptional efficacy in enhancing few-shot learning performance. However, extending this success to few-shot learning in computer vision remains a challenge. Most of the current research utilizes the pretext tasks inherent in SSL as additional loss functions to improve representation learning during the supervised pre-training phase. However, the effectiveness of these techniques often diminishes significantly in the absence of supervision. Another approachthat has gained attention is unsupervised few-shot learning, which adopts the InfoMax principle to optimize the shared information between the data points and their representations with a low-bias mutual information estimator to learn comprehensive feature representations.employs a clustering technique to generate pseudo-labels for samples before implementing a meta-learning algorithm. However, its performance still lags behind leading supervised few-shot learning methods due to inherent limitations in its downstream meta-learning. Meanwhile, recent research has assessed the effectiveness of existing self-supervised methods in a cross-domain few-shot image classification benchmark, highlighting significant domain differences between base and novel classes.indicate that current SSL methods, such as MoCo v2, can compete with supervised few-shot learning in a transductive setting. Nevertheless, this approach contradicts the core idea of few-shot learning as it requires test class data for unsupervised pre-training.

Few-shot learning and SSL have the potential to significantly broaden the scope and applicability of CNN models in complex vision tasks like 3D object recognition, video summarization, and action recognition. Research could focus on developing hybrid algorithms that seamlessly integrate both paradigms or creating self-supervised tasks that are inherently designed to improve few-shot learning performance. Despite the advances, challenges such as overfitting to small datasets, computational complexity, and the lack of effective evaluation metrics. These issues are often discussed but have not yet been fully resolved, marking them as crucial areas for future research.

Zero-shot learning (ZSL) is a paradigm that enables models to perform classification tasks on categories that were not present during the training phase. The mechanism behind this capability usually entails the utilization of semantic attribute spaces or embedding vectors to address the separation between known (trained) and unknown (untrained) categories.
Zero-shot learning enables models to generalize for unseen tasks. However, SSL allows efficient data utilization by using the data as labels.
The figure.shows the workflow for training models using zero-shot SSL. During the SSL phase, raw unlabeled visual data are collected and subjected to a predefined self-supervision task. This task could involve predicting occluded portions of an image or identifying spatial transformations. Utilizing this task, the model undergoes training to generate intricate feature representations of the visual data, thereby accomplishing the objective of unsupervised learning. A zero-shot task is then explicitly defined, such as classifying objects that the model has not encountered during the training phase. Leveraging the learned feature representation and the auxiliary information, the model is capable of generalizing its understanding to perform the zero-shot task. This generalization is instrumental in the model’s ability to adapt for new data categories without necessitating retraining, thus epitomizing the zenith of machine learning adaptability.

In literature, ZSL is used with SSL to identify both known and unknown classes by constructing correspondence between visual and semantic embedding.employs zero-shot learning, augmented with self-supervised methods, to create a technique that rearranges semantic embeddings. This produces artificial training data, enhancing the model’s capacity to adapt to new, unfamiliar classes.put forth a method for subject-specific rapid image reconstruction using zero-shot and SSL. They divide the dataset into three separate groups: two are used for maintaining data integrity and establishing the loss function during self-supervised training, while the third group is used for self-validation.propose a module for generating features across different domains, which creates high-quality synthetic samples using class embeddings. This includes an innovative discriminator focused on maintaining consistency within the target domain. Additionally, introduces a SSL component to explore relationships between different domains. To serve as a connecting link between known and unknown categories, it incorporate a set of anchor points.investigated the effectiveness of a Siamese neural network that employs contrastive loss to separate embeddings of different classes while bringing closer the embeddings of similar classes.

Although the fusion of these two paradigms can potentially revolutionize the way learning algorithms generalize, it is fraught with obstacles that must be tackled. Among them, the key difficulty is the semantic gap between the self-supervised features and the semantic attributes used in ZSL. Bridging this gap requires sophisticated mapping functions that are hard to optimize.
SSL often relies on the data distribution of the unlabeled dataset. However, zero-shot learning requires the model to generalize to unseen classes, which may not follow the same distribution. In real-world applications, the semantic attributes used for zero-shot learning may contain noise or inaccuracies, affecting the performance of the combined model. By addressing these challenges, the combined approach of ZSL and SSL can pave the way for more generalize and efficient vision models, thereby pushing the limits of what can be accomplished in the field of computer vision.

SECTION: Real-World Deployment
Real-world deployment is a crucial aspect of SSL for ViTs. SSL methods should be efficient, scalable, and effective in real-world scenarios. Future directions in real-world deployment include exploring the effectiveness of SSL methods on more diverse and complex datasets, developing more efficient and scalable SSL methods, and addressing the ethical and societal implications of SSL methods in real-world applications.

In summary, the applications and future directions of SSL for ViTs include transfer learning in vision tasks, few-shot and zero-shot learning, and real-world deployment. Exploring these applications and potential future paths will enable the development of more effective and efficient SSL methods for ViTs and their deployment in real-world scenarios.

SECTION: Conclusion
This survey paper has provided a comprehensive overview of SSL methods specifically for ViTs. The taxonomy of SSL methods has been illustrated, highlighting the four major categories as contrastive, generative, predictive, and hybrid methods. The exploration of SSL extends to a detailed summary of various pretext tasks, including spatial context-based methods, color and texture based methods, temporal and sequence based methods, contrastive and clustering methods, distillation and momentum-based methods, redundancy reduction methods, and cross-modal methods. By delving into these diverse aspects, this survey contributes to a deeper understanding of the landscape of SSL methods, providing valuable insights for both researchers and professionals in the domain of computer vision.

SECTION: References