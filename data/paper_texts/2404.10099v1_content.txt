SECTION: Feature selection in linear SVMs via hard cardinality
constraint:a scalable SDP decomposition approach

In this paper, we study the embedded feature selection problem in linear Support Vector Machines (SVMs), in which a cardinality constraint is employed, leading to a fully explainable selection model. The problem is NP-hard due to the presence of the cardinality constraint, even though the original linear SVM amounts to a problem solvable in polynomial time. To handle the hard problem, we first introduce two mixed-integer formulations for which novel SDP relaxations are proposed. Exploiting the sparsity pattern of the relaxations, we decompose the problems and obtain equivalent relaxations in a much smaller cone, making the conic approaches scalable. To make the best usage of the decomposed relaxations, we propose heuristics using the information of its optimal solution. Moreover, an exact procedure is proposed by solving a sequence of mixed-integer decomposed SDPs. Numerical results on classical benchmarking datasets are reported, showing the efficiency and effectiveness of our approach.

SECTION: 1Introduction

In supervised classification, Machine Learning (ML) models are trained on labeled datasets to learn how to predict the correct output for new, unseen data. Classification algorithms are prone to overfitting when the dataset is small and contains irrelevant features, or noise[19]. This problem can also occur with correlated features or features containing errors.
Further, interpretability in ML is gaining more importance, and remains a cutting-edge research topic. Indeed, designing models that can give explanatory insights to the end user for understanding the rationale behind the decisions is crucial from both the predictive and debugging perspectives[20].

It is therefore beneficial to use a smaller set of features that can give a good approximation of the target function, i.e., the classifier. Feature Selection (FS) aims to eliminate less significant and/or redundant features to identify the possibly optimal subset for the classifier, which becomes crucial when dealing with real data. Among various feature selection techniques, embedded methods integrate feature selection within the classifier training process.
In this paper, we study the embedded feature selection problem for SVMs.
Given a dataset withsamples, andfeatures,, the linear SVM classification problem defines a classifier as the function,

using the structural risk minimization
principle[25].
Here and in the sequel, given two integers, bywe denote the set of all integerssatisfying.

In traditional SVMs[10], coefficientsidentify a separating hyperplane (if it exists) that maximizes its margin, i.e. the distance between the hyperplane and the closest data points of each class. Accounting for classes not linearly separable, the tupleis found by solving the following convex quadratic problem:

The non-negative variablesallow points to be on the wrong side of their “soft margin”, as well as being ”too close” to the decision boundary.

The sumin the objective function represents an upper bound on the total misclassification error, which is weighted by the hyperparameterso to balance the maximization of the margin, which is inversely proportional to. Problem (1) is categorized as-regularized-loss SVM (see[13,18], for instance). When needed, we can also refer to it as-SVM to highlight the use of theregularization contrasting with the use ofregularization, as in[18,23], for instance.

The optimal solutionof (1) is usually dense, in the sense that usually all components ofare nonzero and thus every feature contributes to the definition of the classifier. Indeed, through duality theory,can be expressed as
the linear combination of a subset of samples, called support vectors. More precisely,, withbeing the dual optimal solution of problem (1) with nonzero value only if samplerepresents a support vector. Thus, it is unlikely to obtainfor some feature, and sparsity in the vectormust be forced.

In this paper, we aim to strictly control sparsity by imposing hard cardinality constraints on the number of nonzero components in. We approach the feature selection problem from a combinatorial perspective, using binary variables to clearly indicate which features are selected.

More specifically, the SVM problem with feature selection we are interested in is:

where the-pseudonorm is defined as.
The user-defined parameterconstitutes a budget on the number of features used, indicating that the number of features involved in the classifier is at most. Even though objective function and all other constraints are convex quadratic or linear, the cardinality constraintis of combinatorial type, which renders (2) NP-hard.

The main contributions of our work are:

We analyze two novel Mixed Integer Quadratic optimization Problem (MIQP) formulations for the FS-SVM problem (2), based on tackling the-pseudonorm constraint either by use of the big-M reformulation or by use of a complementarity constraint.

We present novel relaxations that decompose with respect to small blocks of variables, thus being easily solvable when dealing with datasets with a large number of features.

We propose both exact and heuristic algorithms that are easily implementable and exploit the proposed relaxations.

We conduct a battery of extensive numerical experiments that demonstrate the effectiveness of our algorithms in solving the proposed problem compared to off-the-shelf solvers.

SECTION: 1.1Related work

Embedding feature selection in linear SVM has been proposed in several papers. One of the most frequently used approaches consists in using the-regularized SVM, the-SVM, instead of the-SVM. Indeed, the-regularization, also known as LASSO regularization, leads to sparser solution and to a easier problem, being formulated as a Linear Programming problem
(see[14,18,23,28], for instance).

While replacing the-regularization by the-regularization has a significant impact on reducing the computational requirement for solving the optimization problem, the-SVM leads to relatively reduced classification accuracy due to the inability to maximize the margin of separation between the two classes of data. To overcome the disadvantages,
in[26,29]the elastic-net regularization technique has been proposed which uses both the- and the-regularization in the objective function.

Another approach consists in penalizing the-pseudonorm ofin the objective function and consider better approximation than the-norm.
In this framework,
Bradley and Mangasarian[8]studied the model in which the-regularization is substituted with the-pseudonorm of. They solve the resulting problem by approximating, where. The resulting problem is the minimization of a concave function over a polyhedral set and the authors proposed a successive linearization algorithm
to find a local solution. Weston and coworkers[27]tackled the the same problem as[8]and proposed to solve it by iteratively rescaling the input features values, multiplying them by the absolute value of the weights, obtained from the resolution of the SVM problem, until convergence is reached. Variables can be ranked by removing those features whose weights become zero during the iterative algorithm and computing the order of removal.

None of approaches described above have a direct and hard control on the numberof nonzero components in the vectorlike the property.

Facing the problem (FS-SVM), Chan and coworkers[9]proposed to replace the cardinality constraint by the non-linear constraint. This non-convex constraint can be relaxed, leading to two convex relaxations of the sparse SVM.
However, the resulting optimal solutions seem to be not as strong[15], and they violate the imposed desired features limit. To overcome these drawbacks, Ghaddar and Naoum-Sawaya[15]proposed an alternative relaxation, adjusting a parameter value iteratively to obtain improving bounds and employing a bisection algorithm to ensure thatdoes not exceed the maximum limit.

Another approach based on bilevel optimization is proposed by Agor and coworkers[2], where binary variables in the upper-level problem are used to
to explicitly control the number of features selected. They proposed a genetic algorithm for the solution of the resulting problem.

In[22], Maldonado and coworkers proposed a Mixed Integer Linear Optimization Problem (MILP) model, introducing feature selection through a budget constraint to limit the number of features used in the classification process, but focusing just on minimizing the sum of the deviationsrather than considering also the margin. Extending this work, Labbé and coworkers[21]proposed a MILP formulation which considers also the margin maximization, to select suitable features for constructing separating hyperplanes, incorporating the same budget constraint. The authors proposed both heuristic and exact approaches for solving their formulation when applied to large datasets.

As regards the-SVM, Aytug[6]proposed two feature selection (hard and soft constrained) models where binary variables act as a multiplicative ”mask” on the, and enter both the objective function and the constraints in a nonlinear way. An exact method based on Generalized Benders Decomposition is proposed to solve the two formulations.
Another approach employing-SVMs and cardinality constraints is used in[12], where the authors proposed a new MIQP model for building optimal classification trees with SVM sparse splits.

SECTION: 1.2Paper organization and notations

This paper is structured as follows: in Section2, two mixed-integer reformulations of the targeted problem (2) are proposed. Several tractable relaxations of these two mixed-integer problems are introduced in Section3with a theoretical comparison among them. In the same section, we further propose novel decomposed relaxations, exploiting the sparsity pattern of the problems. Section4focuses on algorithmic methods: a heuristic and an exact algorithm are proposed for solving the problem (2), based on the optimal solutions of the relaxations. Applying these algorithms to some benchmark datasets,
we report the numerical results in Section5, from the optimization point of view and from the ML perspective. The concluding Section6also discusses possible future research directions.

Throughout the paper, we denote by bold lower case, e.g., and bold upper case, e.g., letters a vector and a matrix respectively. Especially,andrepresent all-ones and all-zeros vectors of proper dimension. The dimension should be clear according to the context unless otherwise mentioned. We employ subscripts to indicate a specific element of vectors or matrices. For instance, bywe denote the-th element of the vector, and bywe denote the-th entry of the matrix.
Furthermore, bywe denote the vector of diagonal of.
Finally, byandwe denote the set of real symmetricmatrices, and the set of symmetric positive-semidefinite matrices of order, respectively.

Additionally, let us introduce the margin set

so that problem (2) reads as:

In the next sections, we will also make use of big-M reformulations, so it is convenient to introduce the two sets

and

where the big-M parameteris large enough
(see Remark3.2below) and the variablesare the negated, sometimes more convenient in notation.

SECTION: 2Exact Mixed-Integer approaches

In this section, we propose two exact mixed-integer formulations of the original problem (2), using two different mixed-integer systems to present the cardinality constraint with auxillary binary variables introduced as an indicator, namely a big-M strategy and a complementarity constraint strategy.

Differently from[21], we study the following MIQP model incorporating the standard-norm on the hyperplane weights as the regularization term:

whereandare defined in (3) and (4).

To show the equivalence between (2) and (6), we consider a-sparse, e.g.,. If, there exists asuch that, whereis given by the rule thatifandif; If, there also exists asuch that, whereis given by the rule thatforandotherwise, whereis a index set covers all of features with, satisfying.

It is worth noticing that the optimal value of model (6) remains the same if we substitute the equality constraintby the budget constraint, which is derived directly from the targeted problem (2).

Alternatively, instead of expressing the cardinality constraint by the big-M formulation, we employ a set of binary variablesand a complementarity constraint to count the nonzero elements ofand end up with another mixed-integer reformulation of (2):

Unlike (6), the binary variablescount the number of zero elements of. For a (7)-feasible point, we letand the pointis (6)-feasible. To show this, we only need to show, namely, thatsatisfies the big-M constraint in (4). It is clear that the constraint holds for the coordinates ofwith. On the other hand, the complementarity constraintimpliesif, and therefore the point. Hence, the problem (7) is equivalent to the problem (6).

In the above problem (7), the constraintcan be equivalently replaced by the constraintfor the same reason as Remark2.1, and the binary constraintcan be equivalently reduced to the box constraintdue to the existence of the complementarity constraint. To show this, we consider the model with the box constraint and assume thatis-sparse with. Due to the constraint,ifis nonzero andifis zero. Hence,, which contradicts the budget constraint. Therefore allvectors with cardinality strictly larger thanare rendered infeasible to the model (7) and its relaxation (8) below.

In the first formulation, we need a big-M parameterfor the reformulation. It is known that the choice ofhas a profound influence on the computational performance if we solve the problem with off-the-shell global solvers. In a branch-and-bound framework, we solve LP relaxation at each node, and in many cases, the LP relaxation can be as weak as the SVM problem without feature selection, if too largeis chosen, as shown later in Theorem3.1. In contrast, in the second formulation, the difficulty of the problem mainly goes to the complementarity constraint, and no big-M needed. The computational comparison between these two formulations will be presented in Sec.5.

SECTION: 3Tractable Relaxations

In this section, we propose several tractable relaxations, e.g., LP relaxations, and SDP relaxations, of the FS-SVM model, and a comparison is made among some of them. Moreover, a decomposition strategy is proposed by exploring the sparse pattern involved in the both objective and feasible region of the original problem, leading to a much smaller and scalable SDP-based relaxation.

SECTION: 3.1Box relaxation

In this subsection, we study the box relaxation (also known as a type of LP relaxation) of two exact mixed-integer formulations presented in the last chapter and point out the crucial role ofthat it plays in the first formulation.

After relaxing the binarity constraints to the box constraints in the problem (6), we arrive at:

The relaxation is a convex quadratic optimization problem solvable in polynomial time, therefore it has been widely used in many previous works, see references in[21]. However, the tightness of the relaxation is highly dependent on the choice ofand there is a possibility that it can be as weak as the original SVM model without feature selection (1). To be more precise, ifis larger than a data-based threshold, the relaxation (8) is equivalent to the problem (1), as presented in the following theorem.

If the parameterin the problem (8) satisfies

whereis an (1)-optimal solution, then (8) is equivalent to (1).

It is clear to see that (1) is a relaxation of (8), as the former includes the constraint. Hence the optimal value of (1) is not larger than that of (8).
To show the converse, we assume thatis optimal to the problem (1). Letfor all. Then the pointis (8)-feasible. Indeed, due to the construction of, the constraintis satisfied automatically for all. Since, the constraintis implied by the assumption we made. Since the objective functions in both problems coincide, the claim follows.
∎

On the other hand,can not be chosen smaller than:

whereis part of an optimal solution to (2). Otherwise, the optimal solution will be cut off in the sense that the problem (6) is not a reformulation for an insufficient choice of. Hence, the proper range of the parameterhas to be:

However, this set can be empty depending on the problem data itself, which means no matter whatwe chose, the box relaxation (8) ends up with one of the following situations: (i) no improvement compared to the original SVM problem (1); (ii) the optimal solution of the targeted problem (2) is cut off, becauseis too small to render (8) a reformulation of (2). This is quite problematic in practice and it encourages us to explore the other formulation (7).

Since estimating the range ofis as difficult as solving the problem itself, we usually chooseunder a safe threshold, which is much larger than the tightest upper bound. In most practical cases, the conditionholds then as well, which means the LP relaxation (8) does not provide any improvement compared to the plain problem (1).

Due to the presence of complementarity constraints, the box relaxation of (7) is equivalent
to (7) itself, as mentioned in Remark2.2. However, the combinatorial nature of a complementarity constraint is the main difficulty in both the unrelaxed and the relaxed version, so this continuous reformulation does not solve the problem immediately without any further processing.

SECTION: 3.2The Shor relaxation

In this subsection, we study the Shor relaxation, a type of SDP relaxation, of the two proposed mixed-integer reformulations.

Letting,and dropping the rank one constraint, the Shor relaxation of (6) reads as:

The Shor relaxation (10) is equivalent to the box relaxation (8).

For a (10)-feasible point, the projected pointsatisfies all of the constraints of (8). Moreover, we have, hence the relaxation (8) is weaker than the relaxation (10).
To show the other direction, we suppose that the pointis feasible to the problem (8), and letand, whereis
a diagonal matrix withfor all. The constructed pointthen satisfies all constraints of (10). Indeed, the last positive-semidefiniteness constraint follows by the construction ofdue to the Schur complement,
which therefore completes the proof.
∎

Letting,,and dropping the rank-one constraint, we arrive at the Shor relaxation of (7):

If there exists partof an optimal solution to (1) with, then the optimal value of (11) is strictly larger than the optimal value of (1).

For a given (1)-feasible pointwith, we restrict the corresponding variables of (11) to the same value and estimate the optimal value of the restricted version of (11). The matrix

has a principal submatrix

which implies that

It follows that

(observe thatimplies). Recalling the assumption, it follows that

Together with the constraint

we havefor some, which implies thatfor some. Hence, the optimal value of (11) is strictly larger than the optimal value of (1). ∎

There is no direct comparison between the relaxation (10) and the relaxation (11). But given the case we have a valid estimate ofat hand satisfying (9), the relaxation (11) can be tightened with a small computational cost by adding the big-M constraint via, which reads as:

The relaxation (12) is tighter than the relaxations (10) and (11).

It is clear that the relaxation (12) is tighter than the relaxation (11) due to the additional big-M constraint. To show the other claim, we construct a (10)-feasible point, given a (12)-feasible point. Suppose that the pointis (12)-feasible, letand, whereis a diagonal matrix withfor all. Thenis (10)-feasible with the same objective value.
∎

SECTION: 3.3Decomposed SDP-based relaxations

In this subsection, we propose a procedure for obtaining equivalent and much smaller SDP-based relaxations by exploiting the sparsity patterns of the Shor relaxations.

We notice that, for example: in problem (11), some sparsity patterns appear both in the objective and the constraints. e.g., only diagonal elements of matrices,, andare involved. Based on this observation, we write down the following decomposed version of (11):

The relaxation (11) is equivalent to the relaxation (13).

Suppose thatis (11)-feasible. It is clear thatis (13)-feasible. In the opposite direction, we assume that the pointis (13)-feasible. It is clear that all linear constraints in (11) are satisfied if we letand. The only thing that needs to be checked is if the following partial positive-semidefinite matrix is positive-semidefinite completable:

Bywe denote the specification graph of the above partial positive-semidefinite matrix. Since the vertexis not connected withandfor all, and the vertexis not connected withandfor all, the only cycle we can find in the graph is the trianglefor all. Thereforeis chordal. Due to[7, Theorem 1.39], if the specification graph of a partial positive-semidefinite matrix is chordal, it is always completable to a positive-semidefinite matrix.
∎

Following a similar fashion, we propose the decomposed version of the relaxation (12):

The relaxation (14) is equivalent to the relaxation (12).

See the proof of Theorem3.6.
∎

For the formulation (6), the decomposed version of its Shor relaxation (15) reads as:

The relaxation (15) is equivalent to the relaxation (10).

See the proof of Theorem3.6.
∎

SECTION: 4The algorithms

In this section, we introduce both a heuristic and an exact algorithm for solving the feature selection problem we are interested in. In particular, we will first present two heuristic strategies to find good upper bound values for our problem and a heuristic strategy to estimate a tight value for the big-M parameterfor the problem. All strategies are based on the resolution of the decomposed SDP-based relaxations presented in the previous section. These strategies will then be embedded in a heuristic algorithm which can ultimately improve the heuristic upper bound. Finally, we present an exact algorithm that implements one of the two upper bound procedures as a subroutine and consists of the resolution of a sequence of Mixed-Integer Second-Order Cone Optimization problems (MISOCPs).

SECTION: 4.1Two upper bounding strategies

In this subsection, we propose two strategies for obtaining an upper bound, namely a feasible objective value of a feasible solutionto (2), based on the optimal solution of relaxations.

The first strategy, denoted as Local Search, consists of three main steps and employs a user-specified excess parameterwhich will temporarily allow to work on justinstead of all features. We first solve the relaxation by off-the-shelf software such asMosek, an interior point solver. With the optimal solutionin hand, we want to search for a feasible point nearby. We next sort features according to increasing. Namely, ifis close to 1, then the featureis unlikely to be selected because of the complementarity constraint, while ifis closer to 0, then the featureis more likely to be selected. Based on it, we select the firstfeatures with small corresponding, denoted by a setwith. In the end, we solve the restricted mixed-integer problem:

which is much smaller than the original problem (7), as it only involvesbinary variables. The problem is solved by the global solverGurobi. This first upper bounding strategy is described in Algorithm1.

Slightly enlarging the search set at each iteration, we propose the following search strategy for a feasible solution of (2).
Our Kernel Search strategy consists in solving a sequence of small MIQPs considering an initial ranking on the features. This strategy takes inspiration from a heuristic proposed by Angelelli and coworkers[4]for the general multi-dimensional knapsack problem which has been applied to different kinds of problems such as location problems[16]and portfolio optimization[5]. Recently, Labbé and coworkers[21]also applied it to solve their embedded feature selection for-SVMs. In this strategy, based on some ranking, a restricted set of promising features is kept and updated throughout each iteration.
Usually, for ranking variables in general MILPs, LP theory is exploited using both relaxed solution values and reduced costs, a strategy followed by[21]for ranking features.

By contrast, in our case we decided to use as a ranking criterion the coordinates of an optimal solutionto the decomposed SDP relaxation (13). Features are sorted
with respect to this vector: the smaller the value of, the higher the relevance of feature.
Since in practice (13) leads to much tighter lower bound solutions in contrast to the plain relaxation (8) similar to the model used by[21], there is a justified hope that the ranking provided byis closer to the relevance of features.

Letbe the ordered set of features,a user-defined parameter and. Thenis divided intosubsets denoted asfor.
In particular, each subset,, will be composed offeatures, andwill contain the remaining features.

An initial value of the UB is set to. Also define a feature subsetas thekernel set, containing features that are kept at the next iteration. To initialize, set. At each iteration, the heuristic considers seti.e. the union of the features in the kernel set and the features in the set. To update the UB, at each iteration we solve problemCoP()plus the following two constraints:

Constraint (17) restricts the objective function to take a value smaller than or equal to the current upper bound, while constraint (18) makes sure that at least one feature from the new setis selected. Note that, due to the addition of these constraints, the optimization problems may potentially be infeasible. If the optimization problems are feasible, the new features selected are then added to the kernel setused in the next iteration since adding these features obtains an identical or better upper bound. Conversely, the set
of features ofthat have not been chosen in the optimal solution in the previous iterations is removed from the kernel set. The
removal of some of the features from the kernel set is decisive in that it does not excessively increase the number of binary variables considered in each iteration. We decided to remove the features that were not selected in the
previous two iterations. The set of added features is denoted asand the set of removed features as. The resulting
kernel set for the next iteration is.
Conversely, if the problem is infeasible, the kernel set is not modified and the procedure skips to the next iteration. The Kernel Search strategy is described in Algorithm2.

SECTION: 4.2A strategy to tighten the big-M parameter

Given an upper bound UB of problem (2), one can estimate the upper bound ofby solving the following problem:

Similarly, the lower bound ofcan be obtained by solving the same optimization problem with the max replaced by min and we denote the optimal value by. The proper choice ofis.

Letbe a feasible solution of (2) andbe the corresponding objective value. Then

is a valid bound, whereandare obtained
by (19).

obvious. ∎

The proposed boundis tighter than most of the currently used big-Ms.
Nevertheless, empirically, the tightening of any such bound is of little relevance unfortunately, because they still fall outside of the range specified in (9).

SECTION: 4.3The heuristic algorithm

In this subsection, we propose a procedure for generating a good feasible solution of (2) taking one of the two upper bound strategies and the big-M estimation strategy into account. At first, a feasible solution is computed with the chosen upper bound strategy. Then, according to Proposition4.1, we solve a series of 2many SDP problems to find, for each feature, values,and then obtain the big-M parameter. If this value satisfies (9), we know that the SDP formulation (14) is better than (13), thus we compute a new relaxed solutionof (14). We thus use vectoras a ranking criterion for the chosen upper bound strategy, we run the strategy again and we update the heuristic solution if a better one is found.

SECTION: 4.4The exact algorithm

In this subsection, we propose an exact procedure for (2) by solving a sequence of MISOCPs. With a slight abuse of notation, let us define, given a subset of features, the set

The subproblem considered in the exact procedure is

where SR is the abbreviation for semi-relaxation. In the problem (20), we essentially relax the complementarity constraints associated with features in the setvia decomposed SDP relaxation, and express the other by the big-M mixed-integer formulation. We note that the problem (20) is a mixed-integer positive semi-definite optimization problem (MISDP) and there is no off-the-shelf solver dealing with it. In the following part, we show a way to express the problem in the form of MISOCP, which can be handled by the off-the-shelf solvers likecopt.

Before specifying the MISOCP reformulation, we present a useful result: some special slicesof the positive-semidefinite cone are second-order cone representable, due to Lemma4.3below.

Noticing that the matrix in the left set has a chordal sparsity pattern, due to[1], we have the decomposition:

It follows that

Finally, noticing the fact that

we complete the proof.
∎

Applying Lemma4.3, the problem (20) can be rewritten of the form

where(actually,) more variablesandare introduced.

Now we can introduce the exact algorithm reported as Algorithm4. In this exact procedure, we solve a sequence of semi-relaxed problems SR-DLMP() associated with a subsetof features. In such problems, only variableswithwill be considered as binary and the remaining ones are relaxed. To obtain initial bounds on the objective value, the exact procedure exploits the upper bound strategies detailed above. The main step of the exact procedure consists of solving the sequence of semi-relaxed problems (21) to improve the lower bound of the objective value.

To start with, we must select a subset of featureswhose associatedvariables will be considered as binary variables in the first semi-relaxed problem. The upper-bound strategy run at the beginning provides a subset of features that allows us to obtain a good bound on the optimal objective value. Therefore, the exact procedure will consider the set provided by the heuristic as the initialand it will obtain an initial LB solving SR-DLMP(). Then the setis updated by adding and removing some of the features, in order to improve the lower bound of the objective value. To this end, two sets (denoted byand) are built in each iteration. Setconsists of some of the features inwhose associatedvariables will be considered as binary in the next iteration, i.e. features ofwill be added to. Similarly,consists of features inthat will not be considered as binary in the next iteration.

In addition and if possible, we will update the UB in the main step running the upper bound strategies and using vectoras the initial ranking. There could be many different ways of defining setand updating. In the strategy we adopted, we setas the set offeaturessuch that the relaxed solutionof SR-DLMP() was ”less binary”. This is why we first create a ranking on the features based on termsin descending order, and then select the first, which is a user-defined parameter.

SECTION: 5Numerical experiments

The various computational experiments were performed on an Apple M1 CPU 16 GB RAM computer. All models and algorithms were implemented inPython.
Results were performed usingGurobi 11.0for the resolution of the MIQPs,Mosek 10.1for the resolution of the SDP relaxations andCoptfor the resolution of the MISOCP models solved in the Exact algorithm.

The experiments were carried out on fifteen different datasets. Nine of them can be found in the UCI repository[11], (see Table2), whereis the number of data points,is the number of features and the last column shows the percentage of samples in each class. As can be observed, they contain a small number of features. The other six datasets used in the experiments have a larger number of features, as shown in Table2. The Arrythmia, Madelon, and MFeat datasets are available in the UCI repository as well. The remaining three datasets, Colorectal, DLBCL, and Lymphoma, are microarray datasets containing thousands of features but smaller sample sizes. Further descriptions of these last datasets can be found in[3,17,24].

SECTION: 5.1Computational analysis on the MIQP models

As starting results, we solved the two proposed MIQP models BigMP and CoP withGurobi. Initially, we created different instances of the problems for each dataset in Table2rangingvalues among all the possible values fromtoand rangingvalues in the set. For this first set of results,Gurobiwas able to reach global optimum in every case but formulation CoP was the one which was easier to solve. Figure1shows average computation times for solving the two MIQPs, CoP and BigMP. On average, among datasets in Table2, solving model CoP withGurobiwas 1.4 times faster.

The reason behind this first comparisons is thatGurobiis used in the two upper bound strategies we proposed. The two strategies, the Local Search and the Kernel Search, require the resolution of the FS-SVM model on instances created on a smaller subset of features, and this can be either done by using the BigMP or the CoP formulation.
From these initial tests, we understand that CoP is the model that is faster to solve withGurobiwhen dealing with datasets with a small number of features, thus, we will use the resolution of model CoP as the subroutine used by the heuristic algorithms to find ad update the upper bounds values.

In Table3we show the computational performances ofGurobiwhen tested on the larger instances related to the datasets with the larger number of features. For these tests, we set the hyperparameter, and, similarly to[21], we rangedvalues in. We can notice how the resulting optimization problems are much harder to solve in contrast to the problem related to the smaller datasets. A time limit of 1 hour was set, and the final MipGap values for the majority of these dataset are very high. This is certainly due to the fact that the lower bound values computed in the BranchBound tree ofGurobiare not very effective for these type of formulations. This is particularly the case for model CoP whose final MipGap values tend to be worse than the ones of model BigMP. In contrast,Gurobimanages to find much better incumbent solutions when solving model CoP for all datasets, apart from Colorectal and Madelon.

SECTION: 5.2Results on the heuristics

In this section we analyse the performances of the heuristic algorithm we proposed and we compare withGurobi. We have two versions of the heuristic algorithm depending on whether the Local Search or the Kernel Search upper bound strategies are used. We denote byH-LSthe heuristic algorithm which implements the Local Search strategy, and byH-KSthe heuristic implementing Kernel Search.

Table4shows the optimization performance ofH-LSandH-KS. Both heuristics where given a time limit of 600 seconds. In particular forH-LSwe set the parameter, which means that after getting the relaxed solution we solved model CoP on themost promising features. ForH-KSwe also set parameter, which means that the overall set of features was divided in subsets of ten features each. A smaller time limit of 60 seconds was given for the resolution of each CoP subproblem solved in the Kernel Search strategy.

As expected,H-KSmanages to find better upper bounds compared toH-LS, at the cost of generally longer computational times. In some instances like in Lymphoma, Madelon and Mfeat, both heuristics found the same solutions. The time limit of 600 seconds was reached byH-KSfor the instances related to datasets Madelon and Mfeat. For the rest of the datasets, the average computational times do not exceed 300 seconds for both heuristics. In general, compared to the objective function values of the solutions found byGurobiin 1 hour (Table3), the solutions found by the heuristics are much better.

Another set of results we carried out was the one of usingH-KSas a warm start heuristic in order to evaluate ifGurobiwas able to find better solution. In Table5we compare the optimal value of the solution ofGurobiafter 1 hour for model CoP, the value of theH-KSheuristic, and the value of the solution found byGurobiusing as a warm start solution the one ofH-KS(denoted by CoP∗).

Regarding Table5, we can see thatGurobiwas unable to find a better solution than the heuristic after 1 hour for all datasets, apart from Arrhythmia, and this is probably due to the fact that the heuristic solutions are already optimal or nearly-optimal. In this table, columnindicate the percentage of improvement of the objective function found byGurobiusingH-KSstarting solution, against the objective function of the solution found byGurobialone.

SECTION: 5.3Results on the exact approach

As shown in Table7, the proposed exact algorithm performs much better thanGurobiwith a warm start heuristic, both on optimal value and computation time. In particular, the exact algorithm is able to globally solve four problems under a time limit of 3600 seconds. For the unsolved problems, the exact algorithm provides a better objective value and a smaller MipGap.

SECTION: 5.4Validation

Finally we conducted a series of experiments to compare our feature selection model (2) based on the-norm, and the feature model studied in[21]which employs the-norm instead. We varied the parameteramong values in. Regarding the parameter, similarly to[21], we ranged it among all possible values in-betweenandfor the small datasets, while, for the larger ones,varied in the set.

Under a 10-fold cross validation framework, for each of the ten folds and for eachandin the grid, we computed the accuracy (ACC) values on the portion of the dataset left for validation. We then computed the average ACC values among the folds. In Figure3, we plotted the best mean ACC values for eachin the grid for the small datasets. We recall that in this case, all the optimization problems could be solved to optimality byGurobi. As we can see, in most of the cases, employing the-norm leads to better generalization performance, resulting in higher average accuracy values.

Similar results for the large datasets are reported in Figure4. In this case, each optimization problem was solved with the Kernel Search strategy alone. This, is because, as shown in the computational experience, the strategy was able to find in most cases very good, if not optimal, solutions. The same was done in[21]. To differentiate the two models, the FS-SVM model we studied in this paper is here named as the-FS-SVM model, while the one presented in[21]is referred to as the-FS-SVM. The Kernel Search heuristic we implemented for the-FS-SVM is the same as described in the relative paper. In order to conduct a fair comparison we set the same time limit to 600 seconds for each procedure.

SECTION: 6Conclusions

In this study, we addressed the NP-hard feature selection problem in linear SVMs under a cardinality constraint, which ensures that the model remains interpretable by selecting a limited number of features. Our approach involves formulating the problem as a MIQP problem and introducing novel SDP relaxations to handle its complexity and enhance scalability.

We developed two MIQP formulations: one employing a big-M method and another one using a complementarity constraint. These formulations accommodate the cardinality constraint by integrating binary variables to select features. To solve these challenging formulations, we proposed several tractable SDP relaxations and a decomposed SDP approach, exploiting the sparsity pattern inherent to the problems. This decomposition significantly reduces computational complexity, making the conic relaxations scalable even for datasets with a large number of features.

For practical implementation, we designed both heuristic and exact algorithms based on the SDP relaxations. The heuristic algorithms, informed by the solutions of the relaxations, efficiently search for good upper bounds and feasible solutions. Meanwhile, the exact algorithm iteratively refines these bounds by solving a sequence of MISOCPs, eventually converging to the global optimum.

Numerical experiments on various benchmark datasets demonstrate the effectiveness of our approach. The proposed methods not only outperformGurobibut also provide competitive classification accuracy. By allowing a flexible adjustment of the number of features, our approach enhances interpretability without significantly compromising predictive performance.

Future work may explore extensions of this framework to non-linear SVMs or Support Vector Regression problems. Moreover, the reformulations and algorithms proposed in this paper can be extended to other linear-SVM-based models where feature selection plays a crucial role in model interpretation and performance, e.g. optimal classification trees with margins[12].

SECTION: Acknowledgements

Laura Palagi acknowledges financial support from Progetto di Ricerca Medio Sapienza Uniroma1 (2022) - n. RM1221816BAE8A79.
Research of Bo Peng supported by the doctoral programme Vienna Graduate School on Computational Optimization, FWF(Austrian Science Funding), Project W1260-N35.

SECTION: References