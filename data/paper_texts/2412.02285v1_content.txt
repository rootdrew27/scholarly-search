SECTION: GQWformer: A Quantum-based Transformer for Graph Representation Learning
Graph Transformers (GTs) have demonstrated significant advantages in graph representation learning through their global attention mechanisms. However, the self-attention mechanism in GTs tends to neglect the inductive biases inherent in graph structures, making it chanllenging to effectively capture essential structural information.
To address this issue, we propose a novel approach that integrate graph inductive bias into self-attention mechanisms by leveraging quantum technology for structural encoding. In this paper, we introduce the Graph Quantum Walk Transformer (GQWformer), a groundbreaking GNN framework that utilizes quantum walks on attributed graphs to generate node quantum states. These quantum states encapsulate rich structural attributes and serve as inductive biases for the transformer, thereby enabling the generation of more meaningful attention scores. By subsequently incorporating a recurrent neural network, our design amplifies the model’s ability to focus on both local and global information.
We conducted comprehensive experiments across five publicly available datasets to evaluate the effectiveness of our model. These results clearly indicate that GQWformer outperforms existing state-of-the-art graph classification algorithms.
These findings highlight the significant potential of integrating quantum computing methodologies with traditional GNNs to advance the field of graph representation learning, providing a promising direction for future research and applications.

SECTION: Introduction
Inspired by the celebrated success of Transformers in modeling structured data across various domains, researchers have begun to explore the application of Transformers to graph data. Unlike traditional attention networks, which aggregate node features solely from adjacent nodes, Graph Transformers (GTs) consider pairwise relationships between all nodes within a graph. This approach allows each node to directly attend to every other node, thereby capturing long-range dependencies and interactions from distant nodes, significantly enhancing their expressive power.

However, the self-attention mechanism in GTs often neglects the inherent inductive biases of graphs, especially those related to graph topology.
The absence of structural information consideration leads to indiscriminate attention to all nodes, thereby failing to discern the instrinsic differences within the graph. To mitigate this, numerous studies have centered on encoding positional or structural information to model these inductive biases, thereby bolstering the Transformer’s capacity to handle graph data. While significant strides have been made, the full potential of integrating graph structure information with node features remains untapped. Notably, as illustrated in Figure, for structurally identical graphs with different features, their attribute-aware structural encodings should vary, but existing methods fail to model this differentiation. In addition, the over-globalizing problem in GTsindicates that attention patterns, when indiscriminately augmented with globalizing properties, can lead to the accumulation of redundant information from distant nodes. Therefore, it is imperative to focus more on the local nuances within the graph.

In order to resolve the aforementioned issues in GTs, we propose Graph Quantum Walk Transformer (GQWformer), an innovative framework that integrates GTs with intrinsic inductive bias by leveraging Quantum Walks (QWs) to encode the structural information of graphs. Specifically, GQWformer comprises two major components: the Graph Quantum Walk Self-attention Module (GQW-Attn) and the Graph Quantum Walk Recurrent Module (GQW-Recu).
Our approach begins with the execution of attribute-aware QWs on a node-attributed graph. This encoding approach is sentitive to both the topological and contextual information of the graph, capturing a comprehensive representation of the graph’s structural attributes. The learned QW encodings are then utilized by GQW-Attn to measure the correlations between node pairs. By directly incorporating QW encodings into the attention biases, GQW-Attn ensures precise control over each node’s attention weights relative to other nodes. This integration of QW encodings effectively embeds the graph’s structural attributes into the GT model.
Subsequently, GQW-Recu processes the complete sequences drived from the QWs, preserving and effectively utilizing the temporal and sequential dependencies to learn robust embedding for target nodes, which enhances the local processing power of the model.
Additionally, the length of the QWs dictates the extent of each node’s interactions, allowing for a flexible adjustment of the model’s receptive field. Importantly, the QWs are learnable, meaning they can dynamically adjust the encodings based on the specific characteristics of the graph data over time. This learnability ensures continuous optimization of QW encodings, providing a tailored inductive bias that effectively guides the learning process. As a result, the introduced QW encoding, rooted in the graph structural bias, significantly enhances the model’s performance in graph learning tasks. Our contributions are summarized as follows.

Addressing the general attributed node embedding problem, we propose a novel and potent framework-GQWformer, this tailored graph network harnesses the global structural information of the graph while distinguishing local information. It achieves this by ingeniously merging the advantages of Transformer and recurrent networks with QWs.

We use QW encoding to simultaneously extract topological and contextual information between pairs of nodes, leveraging the unique properties of QWs. This novel approach provides a comprehensive and nuanced understanding of the relationships within the graph.

We validate the effectiveness of our approach through empirical testing on five public benchmark datasets for graph classification where both network structure and node attributes information can be modeled. Our model exhibits a competitive performance advantage over RWC.

SECTION: Related Work
The transformer architecture have been applied to graph modeling, leading to the proposal of various encoding strategies. These strategies include laplacian eigenfunctions, node degree centrality, kernel distance, shortest paths, random walksand structure-aware methods. Additionally, The issue of over-globalizing in GTs suggests the necessity of integrating local modules. Consequently, multiple studieshave been conducted to capture local information either implicitly or explicitly.

Recently, quantum computing, an emerging research domain, has demonstrated substantial potential in machine learning, including graph learning. With unique principles like superposition and entanglement, quantum computing redefines traditional paradigms of information understanding and processing. When applied to graph data, these principles facilitate effective graph feature characterization within a high-dimensional Hilbert space, enabling the extraction of undetectable and atypical graph patterns.
Indeed, quantum computing offers a novel paradigm for handling graph-structured data, thereby revolutionizing graph learning. Quantum walks (QWs), the quantum equivalent of their classical counterparts, serve as a universal model for quantum computing. These walks provide a powerful method for encoding graph data using qubits and representing them with quantum states, proving effective in graph learning.

SECTION: Preliminary
In this paper, we focus on node-attributed graphs. Letdenote a graph with an adjacency matrixand a node feature matrix. In the following, Scalars or elements of a set are denoted by italic lowercase letters, vectors are denoted by boldface lowercase letters and matrices are represented by boldface captical letters.

SECTION: Transformer
Recent advancements have leveraged the Transformer architectureto aggregate node features using the self-attention mechanism. The self-attention mechanism is mathematically defined as follows:

Here, the node embedding ofat the-th layer, denoted as, is aggregated from the embeddings of all nodes in the graph. The termrepresents the node embedding from the previous layer. To compute the self-attention, each node embeddingis projected into three distinct vectors: the query vector, the key vector, and the value vector, using the projection matrices,and, respectively. The resultant attention vectoris then combined with the previous layer’s embedding, scaled by a factor, to form the new node embedding. This process enables the model to capture intricate relationships and dependencies across the entire graph by allowing each node to weigh the influence of every other node.

SECTION: Quantum Computing
In quantum computing, the basic unit of information is a qubit. Two possible states for a qubit are the computational basis statesand. Notation like ’’ is called bra-ket notation. A qubit is also possible to form linear combinations of states, often called superpositions:

where the numberandare complex numbers that satisfy. Formally, a quantum system onqubits lives in the-fold tensor product Hilbert spacewith resulting dimension. A quantum state is represented as a unit vector. A quantum gate is a unitary operationon, and the action of a unitary operator is denoted as. The state space of a composite system is the tensor product of the state space of its components.

SECTION: Graph Quantum Walk
Quantum walks (QWs), introduced by, represent a quantum analog of classical random walks. Unlike the stochastic evolution seen in classical random walks, QWs evolve through a unitary process.
This unitary evolution gives rise to fundamentally different behaviors compared to classical walks, primarily due to the phenomenon of interference between different trajectories of the walker. Such interference can lead to faster spreading and different probability distributions, which are key features exploited in various quantum algorithms. Two kinds of QWs have been introduced in the literature, namely, continuous time QWsand discrete time QWs. In this paper, we adopt the discrete time QW framework for general graphs as outlined in.

Given a graph, whereandare vertex set and edge set respectively. The formulation of QWs involves defining two crucial Hilbert spaces. One is the position Hilbert spacewhich encapsulates the superposition over various positions, i.e., nodes, in the graph. Formally,is defined as the span of the position basis vector. The position vector of a QW can then be expressed as a linear combination of position state basis vectors:

whereare the complex amplitudes satisfying the unit-norm condition,is the probability of finding the walker at vertex. This formulation allows for the QW to be in a superposition of multiple vertices simultaneously, a fundamental aspect of quantum mechanics that distinguishes QWs from their classical counterparts.

Another one is the coin Hilbert spacecapturing the superposition of possible directions in which the walker can move from each node. Formally,is defined as the span of the coin basis vectors, whereindexes the edges incident to a vertexandis the maximum degree of the graph. The coin state of a QW at vertexcan then be expressed as a linear combination of coin state basis vectors:

where the coefficientssatisfy the unit-norm condition. If a measurement is performed on the coin state of the walker at vertex, the quantityrepresents the probability of observing the walker
at nodeand selecting the-th neighbor offor its subsequent move. Overall, the comprehensive state space of the QW is described as the tensor product of the position and coin Hilbert spaces.

The time-evolution of QW over a graph is governed by two key unitary operators: the coin operator and the shift operator. Letdenote the state of the walker at time. A single step in the QW process involves the sequential application of these two operations. Firstly, the coin operatoris applied, which operates exclusively on the coin Hilbert space, effecting a unitary transformation of the coin state at each vertex. This transformation can be expressed as:

wheredenotes the identity operator. Subsequently, a unitary shift operatoris applied to swap the states of vertices connected by edges, thus facilitating the walker’s movement across the graph. The shift operator acts on both the coin and position Hilbert spaces. Formally, this step can be described as:

In shorthand notation, the unitary evolution of the walk is governed by the composite operator. If the initial state of the QW on the graph is denoted by, then aftertime steps, the state of the walks is described by.

It’s essential to note that QWs diverge significantly from classical random walks, primarily due to the pronounced influence of the initial superposition and the coin operator. These elements introduce additional degrees of freedom, enabling deep learning techniques to more effectively fit data through a controlled diffusion process.

SECTION: Method
This subsection presents the methodology of GQWformer, designed to capture global structural information while emphasizing local details within graphs. The central concept of this approach is to harness the power of QWs on attributed graphs to model and interpret the intricate interactions between nodes, thereby producing highly informative encodings. Figureprovides an illustrative overview of the GQWformer architecture, We will now delve into the specifics of each component.

SECTION: Attribute-aware Graph Quantum Walk
We employ the framework of multiple non-interacting QWs on arbitrary graphs, as introduced in, to facilitate the learning of structural encodings in graph data.

Consider a graph withvertices, the QW process involvesseparate, non interacting walks running in parallel, with each walk originating from a unique node in the graph. Suppose the maximum degree of the graph is, the initial state is encapsulated in the superposition tensor, which represents the collective state of thewalkers.
The evolution of the QW can be decomposed into a sequence of discrete steps.
At each step, the current superposition tensoris updated using a set of coin operators, following by swapping the states along the edges of the graph.

As previously mentioned, coin operators are pivotal in modifying the spin state of the QW, thereby governing the evolution of this non-classical walk process over the graph. These coin operators can exhibit spatial variability across different nodes in the graph or temporal variability along the steps of the walk. However, the basic QW framework does not inherently account for node feature information. To address this, we can learn a function that generates distinct coin operations at each node, based on the features of neighboring nodes. This ensures that even structurally identical graphs can produce different walks if their node features differ. Consequently, learning QWs on a graph is achieved by learning the appropriate coin operators. In general, the function that generates the coin operators could be an arbitrary function that yields a valid coin operator.

In this paper, we focus on the use of elementary unitary matrices for the coin operators. These matrices take the form:

whererepresents the identity matrix andis an arbitrary vector. This specific form of unitary matrix can be computed efficiently during the forward pass of the neural network and its gradients can similarily be computed efficiently during backpropagation, making it well-suited for integration into learning algorithms. For a given node, the coin operator can be generated as follows:

Here,is a function related to the features of node. This approach ensures that the coin operators are adaptively learned based on the node features, thereby enabling the QW process to capture both structural and feature-based information within the graph.

Inspired by graph attention networks and diverging from the approach presented in, we propose a novel functiondesigned to compute attention scores between the nodeand each of its neighbors. The function is designed as:

wheredenotes the features matrix of the neighbors of,is a matrix withrows, and each row is the feature vector ofitself,is a learnable weight matrix that linearly transforms the feature vectors of nodes, andis an attention function that computes the attention scores. By incorporating these elements, our proposed functioncan dynamically evaluate the influence of each neighbor and modify the spin states of the QW to prioritize specific neighbors.

A-step QW generates a sequence of superposition states. Now this sequence captures complex patterns and relationships within the graph, making it a powerful tool for tasks such as graph classification.

By summing the squares of the spin states of each superposition state, we derive a sequence of matrices, where eachfor, we then take the sequence as the input for GQWformer, as will be introduced in the following section.

SECTION: Graph Quantum Walk Transformer
While Transformer excel at modeling the global information, they often struggle to capture essential graph structural information. Here we extract structural infromation using quantum method. Building upon this foundation, we introduce GQWformer, which extends the original GT architecture by incorporating these QW encodings, thereby introducing a potent inductive bias into the self-attention mechanism. Recognizing that the GQW-Attn module may overlook certain local structure information, we further introduce the GQW-Recu module to supplement the focus on local structural details.

Compared to conventional GNNs, Transformer offers a more generalized framework by meticulously analyzing the pairwise correlations between every node. One of the significant advantages of Transformers is their global receptive field, which allows each node to attend to the information from any other node in the graph, thus enabling a comprehensive processing of node representations. However, the self-attention mechanism in Transformers primarily focuses on calculating the semantic similarity between nodes, often neglecting the crucial topological information inherent in the graph. Therefore, to fully harness the power of Transformers in graph-based tasks, it is imperative to effectively integrate the structural information of graphs into the model.

With the sequenceobtained in the previous section, we interpretas a structural encoding matrix, where the-th element of, denoted as, encodes the distance between nodeand node. As previous described, this distance is a trainable parameter that is sensitive to both the structural and attribute information of the graph. The proposed QW encoding serves as a bias term in the attention module, thereby incorporating an attention bias into the attention score calculation. Specifically, Graph Quantum Walk Self-attention Module (GQW-Attn) is formulated as:

By incorporating the QW encoding as an attention bias, the GQW-Attn ensures that the attention mechanism not only captures the semantic similarity but also respects the topological structure of the graph, leading to more expressive and robust node representations.

As the parameterincreases, the QW extends its exploration to larger regions of the graph. Consequently,encodes more valuable information between node pairs compared to the initial state. This extended exploration allows the model to capture long-range dependencies and intricate structural relationships within the graph. The ability to dynamically adjust the extent of this exploration via the parameterprovides the model with flexibility and adaptability to various graph structures and scales.

The sequenceencapsulates the interaction of nodes with their neighbors, dictated by the graph’s topological structure and node attributes. Recurrent neural networksform the foundation of many sequence-to-sequence methods, the work ofextends this paradigm to graph domains by introducing graph recurrent networks (GRN), where GCN can be interpreted as a special case of GRN. In this vein, we explore to utilize GRN to learn the quantum embedding for the initial nodes of QWs.

Specifically, we take the quantum encoding sequenceas the primary input. As depicted in Figure, the first input should be, which are the initial state of the first node, specifically the target node. To precess this sequence, we employ two sequences of GRU cells, they take the same input, but run in oppsite directions, one running forward and the other running backward. In this bidirectional GRU setup,denotes the learned representation for the-th backward cell, whilerepresents the corresponding forward cell.
We first concatenateandto form the output of the layer, denoted as. Notably,is the output of the target node, it already combines the information in the sequence. Subsequently, we apply a pooling operation to merge the outputsinto a single vector. This pooling step serves to further distill the information contained within the entire sequence. Then we obtainby concatenatingwith. Assuming the output of GQW-Attn is, the final embedding of the target nodes is defined as.

The pooling operation within GQW-Recu serves to inductively learn from the neighboring nodes. Moreover, GQW-Recu capitalizes on the inherent order information of the sequence, ensuring that temporal and directional dependencies are thoroughly integrated into the node representations. As a result, GQW-Recu yield a comprehensive and more informative representation of the target nodes.

The integration of GQW-Recu ensures that our model is not only aware of global structures but also deeply attuned to local intricacies, which are vital for accurate graph analysis.

Following the methodology outlined in, we add a special node to the graph, and make it connect to all other nodes within the graph. This virtual node facilitates efficient information exchange among nodes, thereby enhancing overall performance. Furthermore, since the virtual node aggregates information from all nodes in the graph, we adopt its hidden feature as the whole graph embedding and train an additional classifier for the downstream tasks.

SECTION: Experiments
In this section, we conduct a comprehensive empirical analysis of the proposed GQWformer framework, focusing on its performance in graph classification tasks. We evaluate GQWformer using five real-world network datasets, benchmarking its effectiveness against several state-of-the-art GNNs.

SECTION: Experimental Settings
For graph classification, we test GQWformer on five TUDataset benchmarksfrom various domains, including one biology (i.e., PROTEINS), two chemistry (i.e., MUTAG and PTC), and two social (i.e., IMDB-B and IMDB-M) datsets.

To demonstrate the effectiveness of our proposed method, we compare GQWformer with the followingbaselines: DGCNN, IGN, GIN, PPGNS, Natural GN, RWNN, CRaWl, CIN, GSN, GNN-AKand RWC.

In the task of graph classification, given the embedding representations of all graphs, and a set of training graphs with labels, the goal is to predict the labels of the remaining graphs. The classification performance is measured by the accuracy score.

We adhere to the conventional settings outlined into validate the performance of GQWformer. Specifically, for all experiments, we employ a linear learning rate scheduler with the end learning rate. The optimizer of GQWformer is AdamW with a weight decay of. To mitigate the risk of exploding gradient, we set a gradient clipping value to. Additionally, the dropout for FFN, GQW-Attn, and GQW-Recu modules is maintained at. The QW is configured to a length of, and the number of GQWformer blocks is also set to.

In line with, ten-fold cross-validation is employed to select the training and test sets. For each dataset, given the entire set of graphs, we randomly selectofas the training set, while the remaining is used as the test set.

SECTION: Effectiveness Evaluation
From Table, it is evident that our method demonstrates performance on par with, and often superior to, various baseline methods. Specifically, for the chemistry datasets, GQWformer outperforms all baselines by at leaston MUTAG, andon PTC in terms of accuracy. Similarly, GQWformer improves the accuracy byon the biological dataset PROTEINS, and for the social networks, it achieves an accuracy increase ofon IMDB-B andon IMDB-M. Especially, RWCis a recently proposed GNN model, which employs the RW-SAN to measure the global pairwise correlations and RW-Conv to carefully analyze the local substructures of graphs. Our GQWformer method consistently and significantly outperforms RWC on all five datasets by a large margin, underscoring its effectiveness and robustness in graph classification tasks. In summary, the empirical results presented in Tableclearly showcase the superiority of GQWformer over existing baseline methods, including the recent RWC model.

SECTION: Ablation Studies
We conduct ablation studies to evaluate the importance of different modules in GQWformer on the PTC dataset, as presented in Table. These studies provide a comprehensive analysis of each component’s contribution to the overall performance of our model.
Firstly, we validate the significance of the GQW-Attn module. The experimental results clearly demonstrate that incorporating GQW-Attn significantly enhances the performance of GQWformer. The adoption of GQW-Attn allows the model to effectively capture and utilize the structural information inherent in the graph data. Additionally, assigning attention based on QW further amplifies the model’s performance, highlighting the crucial role of this mechanism in improving graph representation and ensuring a more accurate classification.
Next, we investigate the effectiveness of the GQW-Recu module. GQW-Recu leverages the inherent temporal and sequential dependencies of the QW sequence to focus on local details, thereby bolstering the performance of GQWformer. This module’s ability to exploit these dependencies facilitates a more nuanced and comprehensive understanding of the graph’s local structure, which translates into better classification accuracy.
Furthermore, we compare different encoding strategies, including the vanilla QW, which operates independently of feature information, the invariant QW, which is also attribute-aware, as introduced in, and our proposed QW encoding, designed to be more sensitive to the graph’s attribute information. Our encoding strategy proves to be more powerful than both the vanilla and invariant QW encodings on the PTC dataset. This superiority underscores the effectiveness of our approach in capturing the essential features and relationships within the graph data. In conclusion, the ablation studies highlight the indispensable roles of GQW-Attn and GQW-Recu in enhancing the performance of GQWformer. Additionally, our proposed QW encoding strategy demonstrates its effectiveness in graph classification tasks.

SECTION: Sensitivity Analysis
In our study, a critical aspect of GQWformer is the evaluation of the impact of the QW length on model performance. To thoroughly investigate this, we conducted a series of experiments with varying walk lengths using the PTC dataset, as detailed in Table. The GQWformer mechanism benefits from longer walk sequences by exploring a greater number of nodes within the graph, thereby potentially capturing more intricate global structural patterns. However, this increased exploration comes with inherent tradeoffs. Specifically, there is a balance to be struck between the enhanced performance derived from a more comprehensive exploration and the computational efficiency that may be compromised with longer walks. Additionally, there is a delicate equilibrium between the ability to capture global structural information versus local structural details. Our experimental results underscore the significance of walk length, demonstrating that it indeed has a substantial impact on model performance. Longer walks allow the model to capture more extensive global structures, but they also risk diluting the focus on local details and increasing computational costs.
These findings highlight the necessity of carefully selecting the walk length to optimize the balance between performance and efficiency, as well as between global and local structural information.

SECTION: Conclusion
We have proposed a novel GNN, termed Graph Quantum Walk Transformer (GQWformer), this innovative model leverages the unique properties of QWs to capture rich topological and contextual information from graph structures. By integrating a GQW-Attn module, GQWformer effectively extracts global pairwise relationships, while the GQW-Recu component adeptly focuses on local details. Experimental results demonstrate that GQWformer consistently achieves state-of-the-art performance across a diverse range of domains, including biological networks, chemical compounds, and social interaction graphs, particularly excelling in graph classification tasks.

SECTION: References