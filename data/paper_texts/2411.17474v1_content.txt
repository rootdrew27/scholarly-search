SECTION: Learning 3D Spatial Perception from a Child’s Perspective

Young kids develop mature 3D spatial perception by age one that enables them purposefully navigate through the 3D world, organize retinal images into objects, and grasp and bite these objects, with little supervision from adults.
Can existing self-supervised representation learning method learn such capability from a child’s visual experience?
To investigate this, we train state-of-the-art neural networks on a realistic proxy of a child’s visual experience without any explicit supervision or domain-specific inductive biases.
Specifically, we train both embedding models and generative models on 200 hours of headcam video from a single child collected over two years and comprehensively evaluate their performance in downstream tasks using various reference models as yardsticks.
Even though prior work has shown that the best embedding models perform at a respectable 70% of a high-performance ImageNet-trained model, despite substantial differences in training data.
Surprisingly, we observe that the gap in 3D spatial perception becomes much smaller.

SECTION: 1Introduction

“Space: the First Frontier”

Philip Kellman

Mid-level vision capabilities come before high-level semantic understanding in the development of human visual intelligence.
Before age one, kids can construct a visual world in three dimensions, purposefully navigate through the 3D world, organize retinal images into objects, and grasp and bite these objects, without knowing the name and purpose of objects (i.e.semantics).
Such mid-level vision capabilities have far beyond the current state-of-the-art vision system.

Supplementary Material

0.8—p5cm—c—c—X—Model NameBackboneDatasetSource LinkJigsaw[Noroozi2016]ResNet-50  ImageNet-1KVISSL model zooRotNet[Gidaris2018]ResNet-50  ImageNet-1KVISSL model zooNPID[Wu2018a]ResNet-50  ImageNet-1KVISSL model zooSeLa-v2[caron2021unsupervisedlearningvisualfeatures]ResNet-50  ImageNet-1KSwAV repositoryNPID++[misra2019pirl]ResNet-50  ImageNet-1KVISSL model zooPIRL[misra2019pirl]ResNet-50  ImageNet-1KVISSL model zooClusterFit[yan2019clusterfitimprovinggeneralizationvisual]ResNet-50  ImageNet-1KVISSL model zooDeepCluster-v2[caron2021unsupervisedlearningvisualfeatures]ResNet-50  ImageNet-1KSwAV repositorySwAV[caron2021unsupervisedlearningvisualfeatures]ResNet-50  ImageNet-1KSwAV repositorySimCLR[chen2020simple]ResNet-50  ImageNet-1KVISSL model zooMoCo v2[chen2020improved]ResNet-50  ImageNet-1KMoCo v2 repositorySimSiam[chen2021exploring]ResNet-50  ImageNet-1KMMSelfSup model zooBYOL[grill2020bootstraplatentnewapproach]ResNet-50  ImageNet-1KUnofficial BYOL repoBarlow Twins[zbontar2021barlowtwinsselfsupervisedlearning]ResNet-50  ImageNet-1KMMSelfSup model zooDenseCL[wang2021dense]ResNet-50  ImageNet-1KDenseCL repositoryDINO[caron2021emerging]ResNet-50/ViT-B/16  ImageNet-1KDINO repositoryMoCo v3[chen2021empiricalstudytrainingselfsupervised]ResNet-50/ViT-B/16  ImageNet-1KMoCo v3 repositoryiBOT[zhou2021ibot]ViT-B/16  ImageNet-1KiBOT repositoryMAE[he2021maskedautoencodersscalablevision]ViT-B/16  ImageNet-1KMAE repositoryMaskFeat[wei2022masked]ViT-B/16  ImageNet-1KMMSelfSup model zoo

Sec.Aprovides an overview the self-supervised learning models (Tab.1) included in our study.
Sec.Bdetails the evaluation metrics and presents the quantitative results (Tab.C-C) for each mid-level vision task.
Sec.Cshowcases qualitative visualizations (Fig.1-2).

SECTION: Appendix ASelf-supervised Learning Models

In our experiments, we select 22 SSL models from a wide range of categories based on two criteria:
(1) coverage of the main approaches used for large-scale self-supervised training and
(2) comparable model architecture and training data to allow fair comparisons.
We primarily evaluate the publicly-available checkpoints pretrained on ImageNet1K[deng2009imagenet]— the links to each checkpoint are included in Tab.1.
We briefly describe each SSL below.

SECTION: Jigsaw.

Noroozi and Favaro[Noroozi2016]introduced a self-supervised learning approach for model pretraining based on solving jigsaw puzzles as a pretext task. This method trains a network to predict the correct arrangement of shuffled image patches, where the image is divided into a 3x3 grid. At its core, this approach encourages the model to learn spatial relationships and understand object structure by generating consistent embeddings for the spatially rearranged patches of the same image. In our study, we used the publicly available ResNet-50 checkpoint trained on the ImageNet-1k[imagenet15russakovsky]dataset.

SECTION: Rotnet.

Gidariset al.[Gidaris2018]proposed a self-supervised approach for model pretraining using a rotation prediction task, known as RotNet. This method trains a network to classify the rotation angle (0°, 90°, 180°, or 270°) applied to an input image, encouraging the model to learn semantic features and spatial structure within the image. At its core, this approach leverages rotation as a proxy task, pushing the network to recognize objects and their orientations. In our work, we evaluate the ResNet-50 architecture trained on ImageNet-1k[imagenet15russakovsky]using this pretext task and rely on the checkpoint released by the authors.

SECTION: NPID.

Wuet al.[Wu2018a]introduced a non-parametric instance-level discrimination approach for unsupervised feature learning. This method trains a network to distinguish between individual instances by treating each image as its own unique class, employing a memory bank to store and update embeddings for all instances in the dataset. At its core, this approach promotes the model to learn discriminative features by maximizing the similarity between augmentations of the same instance and minimizing it across others. In our work, we evaluate the ResNet-50 architecture pre-trained on ImageNet-1k[imagenet15russakovsky]using this instance discrimination task.

SECTION: NPID++.

Misraet al.[misra2019pirl]significantly improves upon the original implementation of NPID, achieving results that substantially outperform those reported in the original paper[Wu2018a].

SECTION: PIRL.

Misraet al.[misra2019pirl]introduced Self-Supervised Learning of Pretext-Invariant Representations (PIRL), a method designed to learn representations that remain invariant across various pretext tasks. The approach applies contrastive learning, where the model is trained to produce similar embeddings for multiple augmentations of the same image while distinguishing between different images. At its core, PIRL combines instance discrimination with pretext invariance to capture both semantic and structural features. In our work, we evaluate the ResNet-50 architecture pre-trained on ImageNet using the PIRL framework.

SECTION: ClusterFit.

Yanet al.[yan2019clusterfitimprovinggeneralizationvisual]proposed ClusterFit, a self-supervised learning approach that improves feature representations through clustering and re-training. This method begins by clustering embeddings of unlabeled images to capture the underlying data distribution, using these cluster assignments as pseudo-labels to retrain the model, thus distilling semantic information at the cluster level. At its core, ClusterFit follows a two-step process—clustering followed by supervised re-training—to develop robust and discriminative features. In our work, we evaluate the checkpoint using ResNet-50 architecture which is pre-trained on ImageNet.

SECTION: SimCLR.

Chenet al.[chen2020simple]proposed SimCLR, a contrastive self-supervised learning framework designed to learn visual representations by maximizing agreement between different augmented views of the same image. The method applies a series of data augmentations, including random cropping, color distortion, and Gaussian blur, and uses a contrastive loss to bring embeddings of the same image instance closer together while pushing apart embeddings of different images. At its core, SimCLR leverages a simple yet effective contrastive objective, removing the need for specialized architectures or memory banks. In our work, we evaluate the ResNet-50 architecture trained on ImageNet-1k[imagenet15russakovsky].

SECTION: SwAV.

Caronet al.[caron2021unsupervisedlearningvisualfeatures]introduced SwAV (Swapping Assignments between Views), a self-supervised learning approach that combines clustering with contrastive learning. Instead of directly contrasting augmented views, SwAV clusters the features of one view and assigns pseudo-labels, which are then used to predict the cluster assignments of another view. This method enables the model to learn representations without requiring negative samples or a memory bank. At its core, SwAV maximizes similarity between different augmentations by leveraging these swapped cluster assignments. In our work, we evaluate the ResNet-50 architecture trained on ImageNet 1k with SwAV.

SECTION: SeLa-v2.

SeLa[asano2020selflabellingsimultaneousclusteringrepresentation]proposes an alternative approach to clustering-based self-supervised learning by formulating the clustering process as an optimization problem. It uses the Sinkhorn-Knopp algorithm to solve this optimization efficiently, ensuring that cluster assignments are balanced across the dataset. This avoids degenerate solutions where all data points are assigned to a single cluster. Caronet al.[caron2021unsupervisedlearningvisualfeatures]re-implemented SeLa which improves upon the original SeLa by incorporating additional training improvements introduced in the self-supervised learning literature, such as stronger data augmentation, an MLP projection head, and temperature scaling for contrastive learning and yields better performance.

SECTION: MoCo-v2.

Chenet al.[chen2020improved]proposed MoCo-v2, an improved version of the Momentum Contrast (MoCo) framework for self-supervised learning. MoCo-v2 enhances the original MoCo by incorporating stronger data augmentations (such as color distortion and Gaussian blur) and using an MLP projection head to further improve representation quality. Similar to its predecessor, MoCo-v2 employs a memory bank to maintain a large pool of negative samples and uses a momentum-updated encoder to produce stable representations. At its core, this approach refines instance discrimination with updated augmentations and architecture adjustments. In our work, we evaluate the ResNet-50 architecture trained on ImageNet using MoCo-v2.

SECTION: SimSiam.

Chen and He[chen2021exploring]proposed SimSiam, a self-supervised learning framework designed to simplify contrastive learning by removing the need for negative samples, momentum encoders, or memory banks. Instead, SimSiam trains a Siamese network with two branches, where one branch predicts the representation of the other. By using only a stop-gradient operation on one branch, SimSiam prevents the network from collapsing to trivial solutions, allowing it to learn meaningful representations from positive pairs alone. At its core, SimSiam is a simple and efficient method that demonstrates the feasibility of contrastive learning without negatives. In our work, we evaluate the ResNet-50 architecture trained on ImageNet 1k with SimSiam.

SECTION: DenseCL.

Wanget al.[wang2021dense]introduced DenseCL, a self-supervised learning approach that extends contrastive learning to dense feature correspondences within images. Unlike traditional contrastive methods focused on global representations, DenseCL aims to learn pixel-level features by contrasting dense local regions between augmented views of the same image. This pixel-level contrastive objective encourages the model to learn spatially detailed representations, which benefit dense prediction tasks such as object detection and segmentation. At its core, DenseCL leverages fine-grained contrastive learning to produce more spatially aware features. In our work, we evaluate the ResNet-50 architecture trained on ImageNet 1k using DenseCL.

SECTION: BYOL.

Grillet al.[grill2020bootstraplatentnewapproach]proposed BYOL, a self-supervised learning framework that learns visual representations without requiring negative samples. BYOL employs two neural networks: a “student” network and a “target” network. The student learns to predict the target’s representation of an augmented view of the same image, and the target network is updated as an exponential moving average of the student. This setup enables the model to avoid trivial solutions by progressively refining representations through self-distillation. At its core, BYOL relies on bootstrap mechanisms and a momentum update to learn meaningful features without contrastive pairs. In our work, we evaluate the ResNet-50 architecture trained on ImageNet 1k using BYOL.

SECTION: DeepCluster-v2.

Caronet al.[caron2019deepclusteringunsupervisedlearning]introduced DeepCluster which uses k-means clustering on deep features to assign pseudo-labels to unlabeled data. These pseudo-labels are then used for training the network in an iterative process. However, DeepCluster suffers from the instability of cluster assignments between epochs, which requires reinitializing the classification layer repeatedly, disrupting the training of the convolutional network.
Caronet al.[caron2021unsupervisedlearningvisualfeatures]re-implement DeepCluster and address ealier issues by introducing explicit comparisons between features and cluster centroids instead of learning a classification layer for cluster assignments. This direct comparison increases the stability and performance of the training process. Additionally, DeepCluster-v2 incorporates modern self-supervised learning tricks and further enhances the method’s performances.

SECTION: Barlow Twins.

Zbontaret al.[zbontar2021barlowtwinsselfsupervisedlearning]proposed Barlow Twins, a self-supervised learning approach designed to reduce redundancy in representations by decorrelating feature dimensions. The method uses a loss function that encourages the cross-correlation matrix between two identical networks’ embeddings of augmented views to be as close to the identity matrix as possible, reducing redundancy across dimensions. This setup allows the model to learn diverse and informative features without the need for negative samples or memory banks. At its core, Barlow Twins promotes redundancy reduction, enhancing feature decorrelation. In our work, we evaluate the ResNet-50 architecture pre-trained on ImageNet 1k using Barlow Twins.

SECTION: MoCo-v3.

Chenet al.[chen2021empiricalstudytrainingselfsupervised]proposed MoCo-v3, an extension of the Momentum Contrast framework tailored for Vision Transformers (ViTs) in self-supervised learning. MoCo-v3 adapts the momentum contrastive learning strategy to ViTs, introducing optimizations such as an MLP projection head and advanced data augmentations. Similar to previous versions, MoCo-v3 leverages a momentum-updated encoder to generate stable features and uses a queue-based memory bank to manage negative samples. At its core, this approach refines contrastive learning by combining MoCo’s momentum mechanism with the ViT architecture. In our work, we evaluate the ViT-B/16 architecture trained on ImageNet using MoCo-v3 and employ the checkpoint released by the authors.

SECTION: DINO.

Caronet al.[caron2021emerging]proposed a self-distillation approach for model pretraining. The proposed approach trains a student network to generate features similar to a teacher network, where the teacher is an exponential moving average
of the student network. At its core, this approach relies on instance discrimination as the model is trained to learn to
generate similar embeddings for different crops of the same image instance. In our work, we evaluate the ViT-B/16 architecture trained on ImageNet-1k. We use the checkpoint released by the authors.

SECTION: MAE.

Heet al.[he2021maskedautoencodersscalablevision]showed that training vision transformers to reconstruct images based on randomly masked inputs is an effective pretraining task. Such models are trained with a large masking ratio; e.g., 75% of the input image patches are masked. In our experiments, we use the ViTB/16 and ViT-L/16 models trained on ImageNet-1k.

SECTION: MaskFeat.

Weiet al.[wei2022masked]introduced MaskFeat, a self-supervised learning approach that learns visual representations by predicting masked visual tokens in videos. MaskFeat leverages a Vision Transformer (ViT) and operates by masking random patches in input video frames, then training the model to predict feature embeddings of these masked regions. This strategy encourages the model to capture rich semantic and spatial features, which generalize well across various downstream tasks. At its core, MaskFeat combines masked prediction with a ViT backbone, making it particularly effective for dense prediction tasks. In our work, we evaluate the ViT-B/16 architecture trained on ImageNet-1k using MaskFeat.

SECTION: BEiT-v2.

Penget al.[peng2022beitv2maskedimage]proposed BEiT-v2, a self-supervised learning method that improves upon the original BEiT by introducing a more refined tokenization process for masked image modeling. BEiT-v2 leverages a teacher-student framework, where the teacher network generates discrete tokens from image patches, and the student network learns to predict these tokens from masked image patches. This approach enhances the model’s ability to capture fine-grained visual patterns and contextual relationships. At its core, BEiT-v2 combines masked image modeling with a new tokenization strategy to achieve state-of-the-art performance on image classification and downstream tasks. In our work, we evaluate the ViT-B/16 architecture trained on ImageNet-1k using BEiT-v2.

SECTION: iBOT.

Zhouet al.[zhou2021ibot]combine ideas from DINO and MAE by training a model to reconstruct masked dense features based on a teacher network. iBOT uses both an imagelevel and a dense distillation objective. We analyze the ViT-B/16 and ViT-L/16 architectures trained on ImageNet1k and ImageNet-22k. We evaluate the checkpoints released by the authors.

SECTION: Appendix BTask-Specific Metric Descriptions

SECTION: Generic Object Segmentation

We report the full results in Tab.Cusing the following metrics to evaluate generic object segmentation, which involves binary segmentation of foreground objects and background:

F1 Score:The F1 score provides a harmonic mean of precision and recall, offering a balanced evaluation of segmentation performance, particularly in the presence of class imbalance. It is defined as:

wherePrecisionmeasures the proportion of correctly predicted foreground pixels among all pixels predicted as foreground, andRecallmeasures the proportion of correctly predicted foreground pixels relative to all ground truth foreground pixels.

Accuracy:Accuracy quantifies the proportion of correctly classified pixels, encompassing both foreground and background classes. It is defined as:

While simple and intuitive, accuracy may be biased toward the majority class (e.g., background), particularly in cases of class imbalance.

Mean Intersection over Union (mIoU):mIoU assesses segmentation performance by averaging the Intersection over Union (IoU) across all classes (foreground and background). For a given class, IoU is defined as:

where,, anddenote the true positives, false positives, and false negatives for class. mIoU is computed as:

wherefor generic object segmentation. mIoU provides a robust evaluation of the model’s capacity to capture spatial overlap and resolve fine-grained boundaries.

These metrics collectively provide a comprehensive evaluation of the model’s performance in binary segmentation tasks, highlighting both pixel-level accuracy and the model’s ability to distinguish between foreground and background regions.

SECTION: Depth Prediction

We present the complete results for depth prediction in Tab.C. To evaluate performance, we adopt the setup described in[eigen2014depthmappredictionsingle], which includes computing the root mean square error (RMSE) and evaluating the prediction accuracy under different threshold criteria. The threshold-based accuracy, denoted as, measures the proportion of pixels for which the ratio between the predicted depth () and the ground-truth depth () lies below. Formally, this is defined as:

whereis the total number of pixels,represents the predicted depth, andis the ground-truth depth.

SECTION: Surface Normal Estimation

For each pixel in the image, the error is defined as the angular deviation (in degrees) between the predicted and ground-truth surface normals. To evaluate the model’s performance, we compute two primary metrics: (1) the root mean square error (RMSE), which measures the overall angular error, and (2) the accuracy of predictions at predefined angular thresholds. Specifically, the accuracy metric is calculated as the proportion of pixels whose angular error falls within thresholds of,, and, following established evaluation protocols[bae2021estimatingexploitingaleatoricuncertainty,piccinelli2023idisc,7410483].

SECTION: Geometric Correspondence

We report full results on object geometric correspondence in Tab.Cand scene geometric correspondence in Tab.C. Correspondences are evaluated using either 2D projection error or 3D metric error. For a correspondence between pixel locationsin image 1 andin image 2, the 2D projection error is computed as follows. First,is projected into 3D space, yielding a 3D point, using the depth value atand the camera intrinsics of image 1. The 3D pointis transformed to the coordinate frame of image 2 using the relative camera pose and projected back onto the image plane of image 2, yielding the pixel location. The 2D projection error is then defined as:

whererepresents the Euclidean distance in the image plane.

For 3D metric error, bothandare transformed into a shared 3D coordinate space, resulting inand, respectively. The 3D metric error is then computed as:

The 2D projection error is used for scene-level correspondences, while the 3D metric error is preferred for objects to better account for occlusions and thin structures.

To evaluate correspondence quality, we computecorrespondence recall, defined as the percentage of correspondences with error below a threshold:

Whereindicates the number of correspondences with error below the thresholdandis the total number of correspondences. We report recall values for variousvalues and analyze results across image pairs grouped by relative viewpoint changes.

SECTION: Mid-level Image Similarity

We present the full results for mid-level image similarity in Tab.C. In this task, a reference image is provided, and the model selects one of two candidate images based on mid-level image similarity. The evaluation metrics used are Accuracy (Acc), Precision (Prec), Recall (Rec), and F1 Score (F1), defined as follows:

Accuracy (Acc):The proportion of correctly predicted matches out of the total comparisons:

Precision (Prec):The proportion of correctly identified matches (true positives, TP) among all images predicted as matches:

Recall (Rec):The proportion of correctly identified matches (TP) among all actual matches in the dataset:

F1 Score (F1):The harmonic mean of Precision and Recall, providing a balanced measure of performance:

These metrics provide a rigorous evaluation of the model’s ability to identify mid-level image similarities accurately and consistently.

SECTION: Appendix CQualitative Comparisons

We present qualitative visualizations in Fig.1and Fig.2to assess model performance on mid-level vision tasks. These visualizations validate the models’ ability to learn and perform each mid level vision task effectively.

Xll ccc ccc
 
  VOC07[pascal-voc-2007]VOC12[pascal-voc-2012]ModelBackboneTaskF1-measure  mIoU  Accuracy 
F1-measure  mIoU  Accuracy\rowcolorGray!20Self-Supervised Models (SSL)Jigsaw[Noroozi2016]RN-50  IN-1k  71.13  63.03  83.24  81.51  71.48  89.41RotNet[Gidaris2018]RN-50  IN-1k  75.84  65.32  85.39  83.46  71.46  89.94NPID[Wu2018a]RN-50  IN-1k  76.92  66.38  85.99  84.34  72.66  90.35SeLa-v2[caron2021unsupervisedlearningvisualfeatures]RN-50  IN-1k  83.20  73.53  89.73  86.03  76.56  91.71NPID++[misra2019pirl]RN-50  IN-1k  80.75  69.59  87.84  85.46  75.24  91.29PIRL[misra2019pirl]RN-50  IN-1k  79.55  69.62  87.69  86.40  77.39  92.46ClusterFit[yan2019clusterfitimprovinggeneralizationvisual]RN-50  IN-1k  77.91  67.94  86.79  85.58  72.98  90.25DeepCluster-v2[caron2021unsupervisedlearningvisualfeatures]RN-50  IN-1k  79.33  71.08  88.14  88.29  79.91  93.01SwAV[caron2021unsupervisedlearningvisualfeatures]RN-50  IN-1k  79.72  71.95  88.59  87.38  78.72  92.91SimCLR[chen2020simple]RN-50  IN-1k  81.05  73.63  89.44  87.94  79.62  93.25MoCo v2[chen2020improved]RN-50  IN-1k  82.78  74.40  89.91  88.65  79.75  93.21SimSiam[chen2021exploring]RN-50  IN-1k  82.99  74.05  89.88  88.25  77.51  92.05BYOL[grill2020bootstraplatentnewapproach]RN-50  IN-1k  83.20  71.97  89.21  87.74  78.81  93.09Barlow Twins[zbontar2021barlowtwinsselfsupervisedlearning]RN-50  IN-1k  79.97  71.53  88.51  88.09  78.62  92.82DenseCL[wang2021dense]RN-50  IN-1k  79.32  70.71  88.03  87.19  78.75  92.47DINO[caron2021emerging]RN-50  IN-1k  78.13  71.95  88.32  88.81  79.86  92.99MoCo v3[chen2021empiricalstudytrainingselfsupervised]RN-50  IN-1k  82.56  71.48  88.88  85.44  77.41  92.06DINO[caron2021emerging]ViT-B/16  IN-1k  83.12  74.00  89.79  88.70  79.94  93.17iBOT[zhou2021ibot]ViT-B/16  IN-1k  82.85  75.74  90.50  90.51  84.72  94.90MoCo v3[chen2021empiricalstudytrainingselfsupervised]ViT-B/16  IN-1k  80.92  72.45  88.99  82.11  74.11  90.71MAE[he2021maskedautoencodersscalablevision]ViT-B/16  IN-1k  77.25  65.78  85.88  80.22  69.63  89.14MaskFeat[wei2022masked]ViT-B/16  IN-1k  78.84  70.28  87.76  84.27  75.14  91.00

Xll cccc cccc
 
  NYU 
  NAVIModelArchitectureDatasetRMSERMSE\rowcolorGray!20Self-Supervised ModelsJigsaw[Noroozi2016]RN-50  IN-1k  71.17  93.02  98.24  0.6282  29.48  55.45  73.66  0.1775RotNet[Gidaris2018]RN-50  IN-1k  73.18  93.41  98.23  0.6047  29.87  55.03  73.00  0.1804NPID[Wu2018a]RN-50  IN-1k  70.65  92.81  98.34  0.6191  37.88  65.46  80.82  0.1506Sela-v2[caron2021unsupervisedlearningvisualfeatures]RN-50  IN-1k  74.76  94.47  98.80  0.5684  34.72  61.97  78.64  0.1586NPID++[misra2019pirl]RN-50  IN-1k  71.89  93.27  98.34  0.6110  38.07  65.32  80.69  0.1525PIRL[misra2019pirl]RN-50  IN-1k  74.58  94.13  98.59  0.5780  38.55  65.36  80.86  0.1495ClusterFit[yan2019clusterfitimprovinggeneralizationvisual]RN-50  IN-1k  74.13  93.81  98.25  0.5850  39.45  66.47  81.45  0.1479DeepCluster-v2[caron2021unsupervisedlearningvisualfeatures]RN-50  IN-1k  73.63  93.62  98.39  0.5863  39.50  67.35  82.43  0.1448SwAV[caron2021unsupervisedlearningvisualfeatures]RN-50  IN-1k  76.17  94.96  98.81  0.5542  39.45  67.13  82.04  0.1457SimCLR[chen2020simple]RN-50  IN-1k  75.64  94.67  98.65  0.5698  42.86  70.04  83.68  0.1365MoCo v2[chen2020improved]RN-50  IN-1k  77.05  94.83  98.77  0.5467  45.42  72.55  85.42  0.1309SimSiam[chen2021exploring]RN-50  IN-1k  75.95  94.74  98.78  0.5628  43.03  70.01  83.94  0.1366BYOL[grill2020bootstraplatentnewapproach]RN-50  IN-1k  75.43  94.48  98.68  0.5711  42.19  69.22  83.54  0.1387Barlow Twins[zbontar2021barlowtwinsselfsupervisedlearning]RN-50  IN-1k  75.06  94.22  98.61  0.5791  41.83  68.74  83.01  0.1408DenseCL[wang2021dense]RN-50  IN-1k  76.30  94.69  98.65  0.5615  43.78  71.45  85.01  0.1332DINO[caron2021emerging]RN-50  IN-1k  77.68  95.89  99.09  0.5235  47.63  74.31  86.54  0.1241MoCo v3[chen2021empiricalstudytrainingselfsupervised]RN-50  IN-1k  75.56  94.63  98.86  0.5584  45.93  72.87  85.57  0.1309DINO[caron2021emerging]ViT-B/16  IN-1k  79.38  95.97  99.05  0.5278  47.75  74.65  87.02  0.1241iBOT[zhou2021ibot]ViT-B/16  IN-1k  81.32  96.90  99.34  0.4919  50.02  76.29  87.89  0.1199MoCo v3[chen2021empiricalstudytrainingselfsupervised]ViT-B/16  IN-1k  80.14  96.14  99.16  0.5109  51.07  76.96  87.95  0.1175MAE[he2021maskedautoencodersscalablevision]ViT-B/16  IN-1k  66.17  90.38  97.37  0.6898  26.78  51.82  71.69  0.1868MaskFeat[wei2022masked]ViT-B/16  IN-1k  80.39  96.18  99.07  0.5125  49.50  75.47  87.14  0.1195

Xll cccc cccc
 
  NYUv2 
  NAVIModelBackboneDataset11.25°22.5°30°RMSE11.25°22.5°30°RMSE\rowcolorGray!20Self-Supervised ModelsJigsaw[Noroozi2016]RN-50  IN-1k  44.27  67.65  76.23  28.8386  22.79  49.22  62.50  36.6169RotNet[Gidaris2018]RN-50  IN-1k  43.93  67.40   76.07  28.8557  23.70  50.20  63.46  28.8557NPID[Wu2018a]RN-50  IN-1k  40.80  64.68  73.97  35.4511  24.92  51.82  64.87  35.4511SeLa-v2[caron2021unsupervisedlearningvisualfeatures]RN-50  IN-1k  45.14  68.98  77.53  28.0449  25.73  53.19  66.22  34.7204NPID++[misra2019pirl]RN-50  IN-1k  41.57  65.98  75.14  29.2829  25.03  52.03  65.20  34.9940PIRL[misra2019pirl]RN-50  IN-1k  44.92  68.35  76.71  28.5771  27.01  54.06  66.85  34.1514ClusterFit[yan2019clusterfitimprovinggeneralizationvisual]RN-50  IN-1k  43.93  67.40  76.12  28.9261  25.49  53.21  65.98  34.8134Deepcluster-v2[caron2021unsupervisedlearningvisualfeatures]RN-50  IN-1k  44.48  68.29  76.98  28.2509  26.51  54.01  67.07  34.1514SwAV[caron2021unsupervisedlearningvisualfeatures]RN-50  IN-1k  44.08  67.98  76.81  28.2881  25.69  53.17  66.21  34.4863SimCLR[chen2020simple]RN-50  IN-1k  45.87  69.17  77.48  27.9438  26.70  54.21  67.07  34.1743MoCo v2[chen2020improved]RN-50  IN-1k  46.37  69.79  78.03  27.5874  29.02  56.86  69.42  32.7033SimSiam[chen2021exploring]RN-50  IN-1k  44.12  67.95  76.72  28.4032  28.06  55.71  68.18  33.5474BYOL[grill2020bootstraplatentnewapproach]RN-50  IN-1k  43.64  67.73  76.46  28.5432  26.51  54.29  67.17  34.1015Barlow Twins[zbontar2021barlowtwinsselfsupervisedlearning]RN-50  IN-1k  44.04  67.75  76.57  28.4161  27.21  54.70  67.46  33.9390DenseCL[wang2021dense]RN-50  IN-1k  45.30  68.74  77.16  28.2974  27.21  54.70  67.46  33.9390DINO[caron2021emerging]RN-50  IN-1k  47.64  70.96  79.12  26.8891  31.43  59.50  71.77  31.3895MoCo v3[chen2021empiricalstudytrainingselfsupervised]RN-50  IN-1k  43.03  67.15  76.20  28.6994  27.22  55.03  67.85  33.7240DINO[caron2021emerging]ViT-B/16  IN-1k  48.42  69.71  77.57  28.0873  31.66  58.58  70.68  31.9912iBOT[zhou2021ibot]ViT-B/16  IN-1k  52.02  72.43  79.53  26.9539  32.75  60.06  71.69  31.4563MoCo v3[chen2021empiricalstudytrainingselfsupervised]ViT-B/16  IN-1k  49.64  70.01  77.36  28.2596  31.72  57.84  69.20  33.0295MAE[he2022masked]ViT-B/16  IN-1k  43.89  66.13  74.56  30.1382  22.07  49.06  62.63  36.4724MaskFeat[wei2022masked]ViT-B/16  IN-1k  53.63  72.23  79.03  27.1797  32.40  58.92  70.43  32.3430

Xll ccc rrr rrrr
 
  3D Recall 
  2D Recall 
  Bin RecallModelArchitectureDataset0.01m  0.02m  0.05m 
5px  25px  50px 
0-30°  30-60°  60-90°  90-120°\rowcolorGray!20Self-Supervised Models (SSL)Jigsaw[Noroozi2016]RN-50  IN-1k  9.13  19.83  54.94  0.68  7.45  16.20  49.15  26.54  13.06  7.76RotNet[Gidaris2018]RN-50  IN-1k  11.97  23.21  55.13  0.92  9.83  19.38  58.44  29.82  14.85  10.26NPID[Wu2018a]RN-50  IN-1k  18.70  32.11  63.38  1.57  15.80  27.47  69.09  41.51  22.96  16.62SeLa v2[caron2021unsupervisedlearningvisualfeatures]RN-50  IN-1k  12.17  23.50  53.26  0.93  10.14  19.49  49.33  28.07  18.86  12.86NPID++[misra2019pirl]RN-50  IN-1k  13.20  25.86  58.25  0.87  10.52  21.20  53.10  32.17  19.75  14.41PIRL[misra2019pirl]RN-50  IN-1k  16.21  29.49  61.54  1.15  13.21  24.73  60.73  36.56  22.61  16.40ClusterFit[yan2019clusterfitimprovinggeneralizationvisual]RN-50  IN-1k  10.85  21.49  56.86  1.86  9.08  16.94  43.28  26.57  17.32  11.61DeepCluster v2[caron2021unsupervisedlearningvisualfeatures]RN-50  IN-1k  20.65  34.42  64.24  1.78  18.14  30.46  69.52  42.24  27.47  19.09SwAV[caron2021unsupervisedlearningvisualfeatures]RN-50  IN-1k  20.20  33.99  63.20  1.71  17.60  29.83  67.11  42.34  27.23  18.81SimCLR[chen2020simple]RN-50  IN-1k  16.57  30.68  61.75  1.09  13.49  25.80  60.53  37.77  23.67  18.27MoCo v2[chen2020improved]RN-50  IN-1k  21.85  37.76  68.76  1.63  18.17  32.94  75.85  48.73  28.47  20.50SimSiam[chen2021exploring]RN-50  IN-1k  23.47  38.16  68.41  2.07  20.16  33.57  76.05  48.63  29.90  20.46BYOL[grill2020bootstraplatentnewapproach]RN-50  IN-1k  10.81  21.11  56.81  2.26  9.02  16.64  46.24  26.45  15.82  10.65Barlow Twins[zbontar2021barlowtwinsselfsupervisedlearning]RN-50  IN-1k  12.71  23.27  58.22  2.97  10.92  18.83  52.25  29.38  17.00  11.41DenseCL[wang2021dense]RN-50  IN-1k  17.59  34.57  67.63  1.17  14.28  29.17  71.25  44.65  26.29  17.76DINO[caron2021emerging]RN-50  NAVI  30.57  47.36  75.43  2.61  26.79  42.41  84.37  61.43  39.01  26.82MoCo v3[chen2021empiricalstudytrainingselfsupervised]RN-50  NAVI  21.70  36.29  65.49  1.70  18.43  31.77  73.41  45.90  27.84  19.88DINO[caron2021emerging]ViT-B/16  IN-1k  25.91  43.00  74.66  3.16  22.54  36.86  84.78  56.28  33.20  22.54iBOT[zhou2021ibot]ViT-B/16  IN-1k  26.84  44.72  76.10  3.12  23.78  39.11  86.94  58.98  34.22  23.85MoCo v3[chen2021empiricalstudytrainingselfsupervised]ViT-B/16  IN-1k  26.99  44.46  75.22  2.17  23.45  39.54  85.95  58.96  34.45  23.20MAE[he2021maskedautoencodersscalablevision]ViT-B/16  IN-1k  19.21  32.59  66.82  2.74  17.16  27.72  78.17  46.12  21.16  11.85MaskFeat[wei2022masked]ViT-B/16  IN-1k  22.11  35.16  65.92  2.08  19.67  31.37  86.25  51.50  22.17  11.00

Xll rrr rrrr
 
 2D Recall 
  Bin RecallModelArchitecture5px  10px  20px 
0-15°  15-30°  30-60°  60-180°\rowcolorGray!20Self-Supervised Models

Jigsaw[Noroozi2016]RN-50  9.57  18.18  27.98  26.11  19.80  11.16  4.00RotNet[Gidaris2018]RN-50  15.74  25.46  34.15  37.56  28.52  13.73  4.29NPID[Wu2018a]RN-50  27.64  40.10  50.07  52.84  44.85  28.24  11.34SeLa-v2[caron2021unsupervisedlearningvisualfeatures]RN-50  12.21  22.70  33.36  31.73  24.61  14.61  6.54NPID++[misra2019pirl]RN-50  10.62  19.59  30.23  27.16  20.92  13.09  6.37PIRL[misra2019pirl]RN-50  17.89  30.43  41.35  45.37  35.12  19.67  7.55ClusterFit[yan2019clusterfitimprovinggeneralizationvisual]RN-50  26.31  40.92  51.96  54.96  46.61  26.67  10.45DeepCluster v2[caron2021unsupervisedlearningvisualfeatures]RN-50  17.30  27.90  37.57  38.25  30.90  18.09  7.87SwAV[caron2021unsupervisedlearningvisualfeatures]RN-50  25.41  38.74  49.86  52.34  44.20  27.48  10.23SimCLR[chen2020simple]RN-50  21.78  35.34  46.18  48.85  40.32  22.15  9.08MoCo v2[chen2020improved]RN-50  24.92  37.65  48.33  50.92  41.97  24.56  8.97SimSiam[chen2021exploring]RN-50  18.11  29.83  40.92  42.58  33.72  19.04  7.24BYOL[grill2020bootstraplatentnewapproach]RN-50  15.39  25.41  34.89  35.88  26.91  16.96  6.90Barlow Twins[zbontar2021barlowtwinsselfsupervisedlearning]RN-50  18.83  30.60  40.61  42.36  33.96  19.24  8.55DenseCL[wang2021dense]RN-50  17.23  31.17  44.98  42.41  34.80  20.36  8.56DINO[caron2021emerging]RN-50  26.63  40.64  51.49  54.07  45.63  27.80  11.19MoCo v3[chen2021empiricalstudytrainingselfsupervised]RN-50  15.23  26.06  35.87  37.24  28.23  15.94  7.05DINO[caron2021emerging]ViT-B/16  24.38  34.22  45.47  46.56  36.72  23.74  11.12iBOT[zhou2021ibot]ViT-B/16  20.04  29.45  41.07  41.13  30.95  20.00  9.47MoCo v3[chen2021empiricalstudytrainingselfsupervised]ViT-B/16  25.03  39.31  51.00  53.18  42.87  27.05  11.95MAE[he2022masked]ViT-B/16  6.64  10.31  18.42  15.64  9.81  6.63  3.81MaskFeat[wei2022masked]ViT-B/16  27.94  40.87  50.49  56.51  47.65  24.41  6.90

Xll ccc cccModelBackboneDatasetAccuracy  F1-Score  Precision  Recall\rowcolorGray!20Self-Supervised Models (SSL)Jigsaw[Noroozi2016]RN-50  NIGHTS  71.22  70.69  70.73  70.65RotNet[Gidaris2018]RN-50  NIGHTS  75.33  75.14  74.40  75.89NPID[Wu2018a]RN-50  NIGHTS  81.41  81.16  80.84  81.47SeLa-v2[caron2021unsupervisedlearningvisualfeatures]RN-50  NIGHTS  81.41  81.16  80.84  81.47NPID++[misra2019pirl]RN-50  NIGHTS  83.06  82.63  83.24  82.03PIRL[misra2019pirl]RN-50  NIGHTS  83.77  83.56  83.19  83.93ClusterFit[yan2019clusterfitimprovinggeneralizationvisual]RN-50  NIGHTS  81.58  81.42  80.70  82.14DeepCluster-v2[caron2021unsupervisedlearningvisualfeatures]RN-50  NIGHTS  85.25  84.93  85.26  84.60SwAV[caron2021unsupervisedlearningvisualfeatures]RN-50  NIGHTS  84.65  84.36  84.45  84.26SimCLR[chen2020simple]RN-50  NIGHTS  83.55  83.26  83.26  83.26MoCo v2[chen2020improved]RN-50  NIGHTS  84.43  84.22  83.85  84.60SimSiam[chen2021exploring]RN-50  NIGHTS  85.86  85.78  84.75  86.83BYOL[grill2020bootstraplatentnewapproach]RN-50  NIGHTS  85.86  85.75  84.90  86.61Barlow Twins[zbontar2021barlowtwinsselfsupervisedlearning]RN-50  NIGHTS  83.11  82.70  83.26  82.14DenseCL[wang2021dense]RN-50  NIGHTS  82.73  82.53  82.03  83.04DINO[caron2021emerging]RN-50  NIGHTS  83.83  83.31  84.50  82.14MoCo v3[chen2021empiricalstudytrainingselfsupervised]RN-50  NIGHTS  84.70  84.37  84.70  84.04DINO[caron2021emerging]ViT-B/16  NIGHTS  89.20  88.98  89.23  88.73iBOT[zhou2021ibot]ViT-B/16  NIGHTS  89.36  89.27  88.49  90.07MoCo v3[chen2021empiricalstudytrainingselfsupervised]ViT-B/16  NIGHTS  87.17  86.90  87.19  86.61MAE[he2021maskedautoencodersscalablevision]ViT-B/16  NIGHTS  83.39  82.91  83.81  82.03MaskFeat[wei2022masked]ViT-B/16  NIGHTS  76.10  75.70  75.61  75.78