SECTION: Internalist Reliabilism in Statistics and Machine Learning: Thoughts on Jun Otsuka’s
Otsuka (2023) argues for a correspondence between data science and traditional epistemology: Bayesian statistics is internalist; classical (frequentist) statistics is externalist, owing to its reliabilist nature; model selection is pragmatist; and machine learning is a version of virtue epistemology. Where he sees diversity, I see an opportunity for unity. In this article, I argue that classical statistics, model selection, and machine learning share a foundation that is reliabilist in an unconventional sense that aligns with internalism. Hence a unification under internalist reliabilism.

SECTION: Introduction
Epistemologists of the more traditional variety have had little interaction with their counterparts in philosophy of science and even less with those in philosophy of data science, broadly construed to include statistics and machine learning. This is unfortunate, and perhaps even a scandal, because data science seems to be scientists’ explicit attempt to develop an epistemology for their own inferential practices. Otsuka’s book is the first monograph to take up the important task of addressing this gap, and thereby deserves attention from a broad audience in epistemology and philosophy of science.

Otsuka examines various distinctive approaches in data science and argues that they correspond to different camps in traditional epistemology. He develops a thought-provoking thesis:

Bayesian statistics is internalist.

Classical (frequentist) statistics is externalist, owing to its reliabilist nature.

Frequentist model selection is pragmatist.

Machine learning aligns with virtue epistemology.

However, it seems to me that all these four approaches in data science are internalist, or at least can be interpreted as such. I even think that those mentioned in (ii)-(iv) fall under what could be termed, a combination that many traditional epistemologists might view as incoherent or even impossible.

Before elaborating on my disagreement with Otsuka, let me briefly review the internalism-externalism debate in traditional epistemology.

SECTION: Internalism vs. Externalism
Internalism was originally proposed as a thesis about the justification of belief rather than inference; it holds that all factors determining whether an agent’s belief is justified “reside within the agent” (Otsuka 2023: 90). Internal factors include, for example, the beliefs one holds, the assumptions one is willing to take for granted, the propositions one treats as reasons for beliefs, and the deductive logical relations among propositions. In contrast, a paradigm example of an external factor is the actual reliability of a belief-producing process, which depends on whether the external world, so to speak, cooperates. The external world encompasses anything outside one’s mental life, such as stars, apples, and even the neurons in one’s brain.

While the above are relatively uncontroversial examples, the exact boundary between internal and external factors remains debated. The key point is that internalists focus exclusively on internal factors to ensure that one can in principle evaluate one’s own beliefs and justify themone’s first-person perspective. Externalists disagree, rejecting the need for such a requirement. For more on the nature of this debate, see the survey by Bonjour (2005).

The term ‘internalist’ is most commonly used to describe theories of justified belief, but it can be applied more broadly. We can ask whether an inference method is justified for an agent, or whether an agent’s adoption of a particular inference method is justified. If you believe that the answer depends solely on internal factors—those residing within the agent—you are an internalist about the justification of inference methods. Conversely, if you think that it depends on at least one external factor, you are an externalist.

In fact, most externalists seem to think that inference methods and other belief-producing procedures are theobjects of assessment, with the assessment of beliefs being. Consider this paradigm approach to externalism:

The underlying idea can be understood as a two-step justification: high reliability justifies an inference method or belief-producing procedure, which, once justified, is able to justify the beliefs it produces.

The first issue I want to address is whether classical statistics, as a theory for evaluating inference methods, is internalist or externalist. This is where I disagree with Otsuka. He argues that classical statistics is externalist, while I believe it is internalist—or at least can be naturally and plausibly interpreted as such.

I will first explain why it might appear to be tempting to view classical statistics as externalist before I argue that it is, in fact, internalist. I will start with examples from hypothesis testing and then proceed to other inference tasks, such as estimation and model selection.

SECTION: Hypothesis Testing
Suppose that a scientist is testing a hypothesiswith a prescribed sample size. An inference method for this task, or a, is a function that outputs a verdict—either “Reject” or “Don’t”—whenever it receives a data sequence of the given length. Classical statisticians widely accept a norm for assessing tests:

But how are significance levels defined? This is the crux. Let me walk you through two alternative definitions.

SECTION: Informal Definition
The following informal definition is quite common, being a primary source of the externalist impression of classical statistics. It is adopted by Otsuka (2023: 23), and can be found in many textbooks such as Rosner (2016: 213-214):

Notably, if the Type I error probability of a test exists, it is a physical chance—an objective property of some process in the actual world. Therefore, a test is automatically unjustified if it fails to have a low Type I error probability, irrespective of the first-person perspective of the scientist conducting the test. It does not matter whether the scientist can provide a good reason for believing that the Type I error probability of the test in use is high or low. Thus, classical hypothesis testing is rendered externalist.

However, the above presentation is actually misleading. No such externalist interpretation is readily available once we turn to a more formal definition.

SECTION: Formal Definition
The following definition is taken from a standard textbook by Casella & Berger (2002). They define significance levels as follows (pp. 383, 385):

The parenthetical clauses are my interpretations, but they seem to be quite plausible. Like philosophers of language, mathematical statisticians formally represent a hypothesis or proposition as a set—the set of the possible ways in which it is true.

Now it should be clear what is required by a low significance level: the probability of a test’s erroneous rejection ofbe kept uniformly low () across all possible worlds inwhereis true. This criterion is often conjoined with the following one (Casella & Berger 2002: 388):

So, put intuitively, uniform maximum power requires that the probability of (correct) rejection attains maximum among all tests with a low significance level. This finishes a brief recap of the key formal definitions in the classical, Neyman-Pearson approach to hypothesis testing.

Here is the crux: these formal definitions refer to a set, called a parameter space. How shouldbe interpreted?

SECTION: Interpretations of the Parameter Space
In practice, the exact identity ofis highly content-dependent. Consider a scientist testing the hypothesis that the proportion of red marbles in an urn falls within a certain interval, under the background assumption that there are exactly 100 marbles in the urn, and under the usual assumption of IID (independent and identically distributed) data. In this context, the parameter spaceis formally identified with the set of all (epistemically) possible proportions of red balls:

More precisely,represents the possible world in which the proportion of red marbles equalsand the background assumptions (including IID data) are true, which determines a unique probability distribution(over possible data sequences)—the distribution true in world. If the scientist’s background assumptions are weaker—such as only assuming that there are no more than 100 marbles in the urn—the parameter spacemust expand to accommodate more possible worlds. Thus,can be reasonably identified with the set of possible worlds compatible with the background assumptions adopted in a context of inquiry. Under this interpretation, the Neyman-Pearson theory of hypothesis testing is internalist.

This internalist interpretation is applicable to frequentist statistics across the board, extending beyond hypothesis testing. Indeed:

First focus on the underlined part (i): reliability. The relevant reliability in hypothesis testing is the probability of freedom from error, as we have seen above. In point estimation, the relevant reliability is the probability of producing a point estimate close to the true, unknown estimand. It is the use of standards of reliability that make frequentist statistics a reliabilist theory.

But being reliabilist doesautomatically imply being externalist. It depends on the underlined part (ii): the parameter spaceas a domain of quantification. To be sure, frequentist statistics can be rendered externalist by restricting the parameter spaceto a singleton containing only the actual world—whatever the actual world turns out to be, independent of anyone’s background beliefs or assumptions. Then frequentist statistics becomes not just externalist, but also reliabilist in the conventional sense—it is all about reliability in the actual world. Yet such an externalist interpretation does not hold automatically: it relies on a substantive view of whatis.

In practice, the choice ofis highly context-dependent, as seen in the urn example, andcan be plausibly interpreted as the set of the possible worlds compatible with the background assumptions that one takes for granted in one’s context of inquiry—in short, the set of the cases that. This feature is crucial for serving a need of working scientists: namely, allowing them assess inference methods from within their first-person perspectives. I hasten to add that a first-person perspective can also be first-person: a group of scientists can work together to assess inference methods. The parameter spacecan represent the common ground of those scientists—the assumptions commonly accepted within that group.

Therefore, under a plausible interpretation, frequentist statistics is reliabilist in an unconventional sense that aligns with internalism.

SECTION: Estimation
Now, let me provide additional examples to illustrate the same point: frequentist statistics as internalist reliabilism. I will shift the focus from hypothesis testing to estimation. This transition will also help set the stage for our next topic: model selection and machine learning, which are closely connected to estimation.

Imagine a scientist aiming to estimate a certain quantity. Suppose there is no prescribed sample size, possibly because this estimation project is ongoing, with the estimate updated as new data arrive. In this context, an estimatoris a function that produces a point estimate when given a data sequence of any finite length.

It would be great if an estimatorcould come with a guarantee of a specific sample sizethat suffices for the desired reliability in estimation: a high probability of producing an estimate close to the unknown estimand. Here, a guarantee means a guarantee under the background assumptions that the scientist takes for granted, formalized by. This idea leads to the following criterion:

In some problem contexts, this standard is too high to be unachievable. For instance, consider the context in which one seeks to estimate the mean of a normal distribution with anvariance—hence a large parameter space.
In this case, uniform consistency is unachievable, necessitating a shift to a lower standard, such as the following:

Both uniform and pointwise consistency serve as standards of reliability, and an internalist interpretation is available owing to the quantification over—a point reiterated.

Yet there is ansense in which frequentist statistics is reliabilist. There is actually a hierarchy of standards of reliability, from high to low. A simple version looks like the following, whereis a desideratum added to pointwise consistency as a minimum qualification:

In fact, it seems to me that frequentist statistics operates under a somewhat tacit norm, which can be formulated as follows:

This norm embodies a serious pursuit of reliability,, which can be understood as an additional sense in which frequentists statistics is reliabilist. The above hierarchy is for point estimation; hypothesis testing has its own.

Returning to the hierarchy: an example of the additional desideratumconcerns the rate at which the outputs of an estimator converge to the truth—the. This means, roughly, a rate of convergence as fast as the rate achieved in a classic case, in which the sample mean is used to estimate the mean of a normal distribution with an unknown variance. In practice, the hierarchy is much richer, incorporating a multiplicity of additional desiderata and their combinations.

The main point—frequentist statistics as internalist reliabilism—extends from point estimation to interval estimation almost immediately. When it is possible to achieve the high standard of uniform consistency in point estimation, we know how to achieve a correspondingly high standard in interval estimation:(Siegmund 1985: chapter 7). When it is possible to achieve the intermediate standard of pointwise consistency with the-rate of convergence in point estimation, we know how to achieve a weaker, asymptotic variant of the above standard in interval estimation, called(Casella & Berger 2002: section 10.4).

Thus, classical (frequentist) statistics—encompassing hypothesis testing, point estimation, and interval estimation—is both reliabilist and internalist across the board.

SECTION: Model Selection and Machine Learning
The scope of internalist reliabilism extends further. As we will see shortly, the foundations of model selection and machine learning are actually extensions of the classical theory of estimation, which, as I have argued, is both internalist and reliabilist.

Imagine an economist aiming to predict the price of a house based on four factors: square footage, number of bedrooms, location, and age of the property.
Or consider a computer scientist who wants to determine whether a givenimage depicts a cat, based on the colors of its 1.2 million () pixels. More generally, consider a scientist seeking to construct a functionto predict the value ofbased on any given input(which could be a number or a vector). Whenis binary, this task is known as; whenis real-valued, it is called. Classification is more frequently explored in machine learning, while regression has long been a traditional focus in statistics.

A very popular approach to classification and regression is. Examples include, as shown below, where thes are adjustable parameters:

In general, a parametric modelis a class of functions that share a parametric formwith finitely many adjustable parameters. When these parameter values are fixed, the model produces a specific function fromto(which can be visualized as a curve on the-plane). In machine learning, the most popular models are. Technical details aside, a neural network model is still a parametric model—a class of functions sharing a form, where the parameters, known as, represent the strengths of signal transmission between neurons.

Given any datasetofpoints on the-plane as, a parametric modelis put to use as follows: the parametersare adjusted and fixed to produce a specific function that best fits the given dataset. This resulting function is called a, also called a, as it can be directly used for prediction.

Confronted with multiple models to choose from, we face an estimation problem: estimating the predictive power of each model. How is the predictive power defined? Suffices it to know that, under the usual assumption of IID data, a given modelhas a well-definedwith respect to two factors: (i) a training sample size, and (ii) a possible world, which provides a probability distributionfor defining expected values. So, when a modeland a set ofdata points on the-plane are provided as input, an estimator for the present purpose outputs an estimate of’s (actual) expected predictive accuracy for the training sample size.

The above raises some questions: What estimators may be used, and how should they be evaluated? These are foundational questions for model selection and machine learning. It is at this point that these two fields reconnect with the classical theory of estimation.

In the present context, it still makes sense to consider the classical standards for assessing point estimators, only with two differences. First, we need to be careful about the target of estimation: in the definitions of evaluative standards for point estimation, ‘the true value (to be estimated) in possible world’ needs to be replaced by ‘the expected predictive accuracy of the given modelwith respect to possible worldand training sample size’.

Here is the second difference: In the present context, the spaceof possible worlds is intended to be very large, representing very weak background assumptions, such as the usual IID assumption and the assumption that the true probability distribution on the-plane is smooth. This makesa very large space of possible probability distributions, which doshare any particular parametric form. This is quite different from the classical theory in statistics, which traditionally employed strong background assumptions, often assuming that the true probability distribution takes a specific parametric form—hence the term ‘parameter space’ associated with. Now that the background assumptions are weaker, I will refer todirectly as a space of possible worlds rather than a parameter space to avoid confusion with the parametersin a model.

Let’s return to the task of estimating the expected predictive accuracies of models. This was pioneered by Akaike (1973), who proved that, under certain background assumptions, the AIC estimator meets at least a standard called.Although asymptotic unbiasedness is a relatively low standard—lower and weaker than pointwise consistency, as shown in the following hierarchy—Akaike’s theorem marks the beginning of an important research program.

The question left by Akaike is whether it is possible to achieve a higher standard. Progress has been made in climbing the hierarchy, thanks to a series of breakthroughs since 2020 (Austern et al. 2020, Bayle et al. 2020, Wager 2020, Li 2023, and Bates et al. 2024). We now know that it is possible to design a single estimator that achieves at least the second standard—pointwise consistency with the-rate of convergence—for a broad range of models, extending well beyond polynomial models and neural network models.Moreover, this has been shown to be achieved by certain versions of cross validation, specifically-fold cross validation, whereis any positive integer held constant.

To sum up: Although the fields of model selection and machine learning are relatively new, their theoretical foundation aligns with the classical theory of estimation. This foundation addresses specific needs: the need to estimate the expected predictive accuracies of the models under consideration and, consequently, the (higher-order) need to evaluate estimators, such as the AIC estimator and the cross-validation estimator. These estimators are assessed in the same style as in classical estimation: an internalist evaluation sensitive to background assumptions and framed in terms of standards of reliability. It employs the same hierarchy of standards and is guided by the same pursuit of the highest achievable standard. Thus, the underlying philosophy remains internalist reliabilism. The main difference is that when the focus shifts from the classical theory to model selection and machine learning, the target of estimation becomes less familiar, and the mathematics more complex. Yet, it remains internalist reliabilism throughout.

SECTION: Closing
While Otsuka maintains that classical statistics is a form of externalist epistemology because it is reliabilist, I hold a different view. I have argued that classical statistics is most plausibly interpreted as internalist reliabilism.

Otsuka also contends that model selection is a form of pragmatist epistemology, and that machine learning aligns with virtue epistemology. I agree, but this diversity should not be overemphasized. As explained above, model selection and machine learning actually have a common foundation shared with classical statistics: a frequentist foundation, which can be interpreted as both internalist and reliabilist. Where Otsuka sees diversity, I see an opportunity for unity—a unification under internalist reliabilism.

My disagreement with Otsuka on those finer points should not distract you from the bigger picture: Otsuka’s admirable efforts to promote a great tradition in epistemology. It is the tradition of addressing epistemological questions by perusing the most exciting sciences of one’s time. For Plato, the most exciting science was Euclidean geometry: he reflected on how knowledge is gained in Euclidean geometry (as in), and used it as a model for acquisition of ethical knowledge (as in). In Kant’s time, the leading science was Newtonian physics, to which he contributed early in his career, and hissought to establish an epistemological foundation of Newtonian physics along with the (Euclidean) geometry it presupposes. Today, the most exciting science is data science, broadly construed to conclude statistics and machine learning. Otsuka’s book is the first monograph that systematically and comprehensively examine data science in relation to some of the deepest issues in epistemology. This article represents my attempt to follow his lead.

SECTION: Acknowledgements
I am indebted to Jun Otsuka, I-Sen Chen, and Konstantin Genin for their valuable discussions.

SECTION: References
Akaike, H. (1973). Information theory and an extension of the Maximum Likelihood Principle. In B. N. Petrov, & F. Csaki (Eds.),(pp. 267-281). Budapest: Akademiai Kiado.

Austern, M. & Zhou, W. (2020). Asymptotics of cross-validation. arXiv preprint
arXiv:2001.11111.

Bates, S., Hastie, T., & Tibshirani, R. (2024). Cross-Validation: What does it estimate and how well does it do it?.,(546), 1434-1445.

Bayle, P., Bayle, A., Janson, L., & Mackey, L. (2020). Cross-validation confidence intervals for test error.,, 16339-16350.

Bonjour, L. (2005). Internalism and externalism. In P.K. Moser (Ed.),(pp. 234-263). Oxford University Press.

Casella, G & Burger, R. (2002)., 2nd Edition. Duxbury Press.

Li, J. (2023). Asymptotics of K-fold cross-validation.,, 491-526.

Lin, H. (2022). Modes of convergence to the truth: Steps toward a better epistemology of induction.,(2), 277-310.

Otsuka, J. (2023).. Routledge.

Rosner, B. A. (2006).. Belmont, CA: Thomson-Brooks/Cole.

Siegmund, D. (1985).. Springer.

Shao, J. (2003).. Springer.

Steup, M. (2004). Internalist Reliabilism.,, 403-425.

Wager, S. (2020). Cross-Validation, risk Estimation, and model Selection: Comment on a Paper by Rosset and Tibshirani.,(529), 157-160.