SECTION: Deep Learning application for stellar parameters determination:III- Denoising Procedure

In this third paper in a series, we investigate the need of spectra denoising for the derivation of stellar parameters. We have used two distinct datasets for this work. The first one contains spectra in the range of 4,450-5,400 Å at a resolution of 42,000 and the second in the range of 8,400–8,800 Å at a resolution of 11,500. We constructed two denoising techniques, an autoencoder, and a Principal Component Analysis. Using random Gaussian noise added to synthetic spectra, we have trained a Neural Network to derive the stellar parameters,,,, andof the denoised spectra. We find that, independently of the denoising technique, the stellar parameters accuracy values do not improve once we denoise the synthetic spectra. This is true with and without applying data augmentation to the stellar parameters Neural Network.

SECTION: 1Introduction

Observations in astronomy have always been associated with noise. Trying to minimize the noise is one of the needs of astronomers. Several observation techniques have been suggested to reduce the noise in spectra, however, once the observation is performed, the only way to proceed is to apply mathematical algorithms that can improve the Signal-to-Noise Ratio (SNR) of the data. These techniques involve but are not limited to Gaussian smoothing(Chung,,2020), median filtering(Kumar and Sodhi,,2020), wavelet denoising(Halidou et al.,,2023), and Principal Component Analysis(Bacchelli and Papi,,2006; Zhang et al.,,2010; Murali et al.,,2012; Li,,2018). More recently, and with the advancement of computational power, Deep Learning algorithms started to be used in that purpose.Gheller and Vazza, (2022)used a Convolutional Denoising autoencoder to decrease the noise of synthetic images of state-of-the-art radio telescopes like LOFAR(Offringa et al.,,2013), MeerKAT(Jonas,,2009), and MWA(Tingay et al.,,2013). The technique was applied on different kinds of corrupted input images. The autoencoder was able to effectively denoise images identifying and extracting faint objects at the limits of the instrumental sensitivity. The authors state that their autoencoder was capable of removing noise while preserving the properties of the regions of the sources with SNR as low as 1.Scourfield et al., (2023)used a variational autoencoder to denoise optical SDSS spectra of galaxies (York et al.,2000).Their main goal was to denoise the spectra while keeping the important information they can retrieve from low SNR galaxy spectra and avoiding the use of sample averaging methods (smoothing or spectral stacking). They tested the method in the context of large optical spectroscopy surveys by simulating a population of spectra with noise to mimic the ones at galaxies at a redshift of. Their results showed that the technique can recover the shape and scatter of the mass-metallicity relation in this sample.

In this work, we introduce two types of spectral denoising techniques, autoencoders(Ballard,,1987; Baldi,,2011)and Principal Component Analysis (PCA,Wold et al.,1987; Maćkiewicz and Ratajczak,1993). We test the need of the denoising technique on the derived stellar parameters: effective temperature, surface gravity, equatorial projected rotational velocity, microturbulence velocity, and the overall metallicity. These stellar parameters are derived using the Neural Network introduced our previous work(Gebran et al.,,2022,2023; Gebran,,2024). The paper is divided as follows: Sec.2introduces the calculation of both datasets and noisy spectra, Sec.3explains the autoencoder construction used in the denoising procedure, and Sec.4describes the denoising technique using Principal Component Analysis. Section5shows the results of the denoising technique using both procedures and the effect on the derived stellar parameter accuracy values. Finally, we conclude in Sec.6.

SECTION: 2Datasets

Two datasets were used in the context of the present study. The one analyzed inGebran et al., (2023)and the one ofGebran, (2024). The characteristics of these two datasets are described in Tab.1. The reason for selecting these diverse datasets is to check the procedure over different wavelength ranges and different resolving power.

The steps of calculating the datasets are detailed inGebran et al., (2022,2023)andGebran, (2024). In summary, line-blanketed model atmospheres are calculated using ATLAS9(Kurucz,,1992). The models are plane parallel and in Local Thermodynamic Equilibrium (LTE). They are in hydrostatic and radiative equilibrium. We have calculated the models using the Opacity Distribution Function (ODF) ofCastelli and Kurucz, (2003). Convection was included according to Smalley’s prescriptions(Smalley,,2004). Convection is included in the atmospheres of stars cooler than 8,500 K using the mixing length theory. A mixing length parameter of 0.5 was used for 7,000 KTeff8,500 K, and 1.25 for Teff7000 K.

We have used the radiative transfer code SYNSPEC(Hubeny and Lanz,,2017)to calculate the synthetic spectra. As mentioned previously, two datasets were calculated with each one containing around 200,000 spectra. In both datasets, metal abundances were scaled with respect to theGrevesse and Sauval, (1998)solar value from -1.5 dex up to +1.5 dex. The effective temperature, surface gravity, projected equatorial velocity, and microturbulence velocity were also modified according to the values displayed in Tab.1. The first dataset consists of spectra having a resolution of 42,000 and a wavelength range between 4,450 and 5,400 Å. As explained inGebran et al., (2022,2023), this wavelength range is sensitive to all stellar parameters in the spectral range of AFGK stars. The second dataset has spectra computed between 8,400 and 8,800 Å at a resolution of 11,500. This region includes the Gaia Radial Velocity Spectrometer (RVS,Cropper et al.,2018). The RVS spectra contain lines sensitive to the stellar parameters and to the chemical abundance of many metals ( Mg, Si, Ca, Ti, Cr, Fe, Ni, and Zr, among others) at different ionization stages. The linelist used in this work is the one used inGebran et al., (2022,2023). It contains updated values for the atomic parameters such as the wavelength of the transitions, the oscillator strengths, the damping constants, and others.

In summary, we ended up with two datasets of around 200,000 synthetic spectra each, with,,,, andrandomly chosen from Tab.1. Figure1shows a color map of a sub-sample of the datasets. The Balmer line is detected in the left color map for dataset 1 and the absorption lines of the calcium triplet (= 8,498, 8,542, 8,662 Å) are also shown in the color map of dataset 2 in the bottom part of the figure.

For each dataset, a set of spectra were calculated with random Gaussian noise between 5 and 300. This SNR is used to mimic the noisy observations that we will be denoising later on as they represent the average SNR encountered in real stellar spectra. An example of a spectrum calculated with and without noise in the parameter range of dataset 2 is shown in Fig.2.

SECTION: 2.1Data Augmentation

We have also tested the effect of data augmentation in this work, and for that reason, we have calculated extra dataset as suggested inGebran et al., (2022). Data augmentation is a regularization technique that by increasing the diversity of the training data by applying different transformations to the existing one, helps in avoiding over-fitting and improves the predictions of stellar labels when applied with real observed data(Gebran et al.,,2023). We have used the same approach ofGebran et al., (2022)in which 5 replicas of each spectrum in the dataset were performed. These replicas consist of

Adding to each spectrum a Gaussian noise with a SNR ranging randomly between 5 and 300.

The flux of each spectrum is multiplied with a scaling factor selected randomly between 0.95 and 1.05.

The flux of each spectrum is multiplied with a new random scaling factor and noise was added.

The flux of each spectrum is multiplied by a second-degree polynomial with values ranging between 0.95 and 1.05 and having its maximum randomly selected in the wavelength range of the dataset.

The flux of each spectrum is multiplied by a second-degree polynomial and Gaussian noise added to it.

For more details about data augmentation, we refer the reader toGebran et al., (2022).

SECTION: 3Auto-Encoders

Autoencoders, usually used in denoising and dimensionality reduction techniques(Lecun,,1987; Fogelman Soulie et al.,,1987; Ballard,,1987; Baldi,,2011; Schmidhuber,,2014; Einig et al.,,2023; Scourfield et al.,,2023), are a type of Neural Networks that work in an unsupervised way. They consist of two distinct yet similar algorithms, an encoder and a decoder. The encoder's role is to transform the spectra from a dimension offlux point to a smaller size ofinside a Latent Space. The decoder re-transform theto the original spectrum offlux point. The choice ofdepends on the characteristics of the dataset. However, using the two datasets in this work, we found that the optimal size for the Latent Space is. This is found by minimizing the difference between the output spectra and the input one during the training process. It is true that different values ofcould be used, but our choice ofwas based on the smallest value that gives a reconstruction error less than 0.5% as will be explained in the next steps.

The classical architecture of an autoencoder is shown in Fig.3where the initial spectrum is introduced having 19,000 or 4,000 data points depending on the dataset and is then reduced topoints through successive hidden layers. This first step defines the encoder part of the autoencoder. Then, thepoints are transformed to 19,000 or 4,000 data points while passing through different hidden layers. This second step defines the Decoder part of the autoencoder. The hidden layers are usually symmetrical in the encoder and decoder parts.

Two autoencoders were used in this work, one for each dataset. In both cases, the spectra are
reduced to 10 parameters in the Latent Space. The architecture of the used autoencoders is displayed in Tab.2. We have used an Adam optimizer with a Mean Squared Error (MSE) loss function.

Calculations were performed usingTensorFlow111https://www.tensorflow.org/with theKeras222https://keras.io/interface and were written inPython.

The training of the autoencoders was performed using the 2 datasets containing the synthetic spectra with no noise. The convergence is achieved when the difference between the output and the input spectra is minimized through the MSE. Convergence usually occurs after around 500 epochs. For both datasets, we achieved an R2score larger than. Meaning that the reconstruction of the spectra is performed with an error0.5%. Once the training is done, the denoising is performed when the trained autoencoders are applied to the noisy spectra.

SECTION: 4Principal Component Analysis

PCA is a non-parametric mathematical transformation that extracts relevant information from a dataset(Wold et al.,,1987; Maćkiewicz and Ratajczak,,1993). Its goal is to compute the most meaningful basis to represent a noisy dataset. expressanoisydataset. The new basis usually reveals hidden structure and filters out the noise(Shlens,,2014). PCA has been used for denoising(Bacchelli and Papi,,2006; Zhang et al.,,2010; Murali et al.,,2012; Li,,2018)or spectral dimension reduction(Maćkiewicz and Ratajczak,,1993;Paletou et al., 2015a,; Gebran et al.,,2016,2022,2023). The main power of PCA is that it can reduce the dimension of the data while maintaining significant patterns and trends.

The basic idea behind the use of PCA is to derive a small number of eigenvectors and use them to recover the information in the spectra. The steps of PCA calculation are

The matrix containing the Training dataset hasflux points per spectrum, therefore the dataset can then be represented by a matrixMof sizewhererepresents the number of spectra in the dataset.

The matrixMis then averaged along the-axis and this average is stored in a vector.

The variance-covariance matrixCis calculated as

where the superscript "T" stands for the transpose operator.

The eigenvectorsofCare then calculated.Chas a dimension of. The Principal Components (PC) correspond to the eigenvectors sorted in decreasing magnitude.

Each spectrum ofMis then projected on these PCs in order to find its corresponding coefficientdefined as

The original "denoised spectrum" can be calculated using

The PCA can reduce the size of each spectrum fromto. The choice ofdepends on the many parameters, the size of the dataset, the wavelength range, and the shape of the spectra lines. We have opted for a value forthat reduces the mean reconstructed error to a value <0.5% according to the following equation:

We have opted to a value forthat reduces the mean reconstructed error to a value <0.5%. This value if found to be=50. A detailed description of all steps of the PCA can be found inPaletou et al., 2015a;Paletou et al., 2015b; Gebran et al., (2016,2022,2023); Gebran, (2024). For both datasets, we achieved an R2score larger than.

SECTION: 5Denoising and parameters determination

The datasets that contain the synthetic spectra without any added noise are used to train the autoencoder and to find the eigenvectors of the PCA procedure. These two techniques are then used on the set of noisy spectra that are calculated in Sec.2. The evaluation of the denoising procedure is tested in two ways. First, we checked the similarity of the denoised spectra with the original one with no noise added. Second, we checked the accuracy of the derived stellar parameters when we applied the procedures ofGebran et al., (2022,2023)on the denoised spectra from the autoencoder and PCA.

Autoencoders usually replace PCA because of their non-linear properties, however, both techniques showed a good reconstruction power as shown by the R2score in Secs.3and4. A way to visualize the denoising of spectra is shown in Fig.4. The figure is divided into two parts, the upper one displays a spectrum having the parameters of dataset 1 and the bottom one has the parameters of dataset 2. In each part, the noisy spectrum is in black, the original one without noise is in dashed blue, the denoised spectrum using the autoencoder (left panel) or PCA (right panel) technique is in red, and the difference between the denoised spectrum and the original one without noise is in dash-dot green.

InGebran et al., (2022,2023)we have introduced a technique to derive the stellar parameters of spectra using a Neural Network. We have used the same procedure to derive the accuracy of the stellar parameters once we apply the same technique to the denoised spectra. The main purpose of this step is not to evaluate if the derivation technique is accurate or not but it is to check how similar are the derived stellar parameters of the noisy spectra to the ones derived from the original spectra with no noise added.

The networks that we used are made of several fully dense layers and are trained to derive each parameters separately. The layers are described in Tab.3. The first step of the analysis is to reduce the dimension of the spectra using a PCA procedure. This PCA is not related to the one used for denoising, it is just a step for optimizing the network and making the training faster (SeeGebran et al.,2022for more details).

Two different training are performed for each dataset. The first one is done using a dataset of only synthetic spectra with no noise added and the second one consists of applying data augmentation with spectra having a range of SNR between 3 and 300.

Because we already know the stellar parameters of the spectra, the evaluation is performed by calculating the difference between the predicted parameter and the original one using the equation

whereis the total number of noisy spectra used in the evaluation. This is done for,,,, and. Tables4and5display the accuracy values for the parameters for the two datasets when deriving the stellar labels of25,000 with no noise added (column 2), with random noise (column 3), with random noise then denoised using autoencoder of Sec.3(column 4) and using PCA of Sec.4(column 5). Each table is divided into two, one part when data augmentation is performed and one without it.

A detailed analysis of Tabs.4and5show that:

Data augmentation is an important step to be applied if we need to derive the stellar parameters of noisy spectra. Without it, the model will only learn to derive the parameters of synthetic spectra without any noise added. A similar conclusion was also found inGebran et al., (2023).

PCA denoising is capable of recovering the line profile and the details in the spectra. This is reflected by comparing the accuracy values of the derived parameters using the denoised spectra from the autoencoders and PCA (i.e. comparing Cols. 4 and 5).

The parameters derived using the PCA denoising technique are more accurate than the ones derived using the autoencoder denoising.

No denoising technique is capable of improving the accuracy of the stellar parameters for the one directly derived from noisy spectra (displayed in Col. 3).

The stellar parameter algorithm is capable of deriving the stellar labels without the need for a denoising technique.

These tests show mainly that data augmentation is very important when Neural Networks are used to derive the stellar parameters of noisy spectra, a results already found byGebran et al., (2022,2023). As an example, Fig.5displays the predictedwith respect to the original one for the data with noise from the augmented dataset 2 (left panel) and the denoised data using autoencoder (right panel) from the same dataset. The data are color-coded to the SNR values. The straight black line represents the best prediction line (). The left panel shows that the highly dispersed results are the ones for the low SNR spectra. Once the spectra are denoised, the dispersion appears to be present for all SNR values with no specific trend or deviation. This is true for all stellar parameters. Independently of the denoising technique, there is no improvement found in the accuracy values of the derived parameters of denoised spectra when the networks were trained on noisy spectra. Applying the networks on noisy data gives more accurate results then when it is applied on denoised data.

SECTION: 6Conclusion

In this work, we have applied two different denoising techniques, an autoencoder, and a PCA, on spectra with random Gaussian noise added to derive the stellar parameters using Neural Networks ofGebran et al., (2022,2023). The method was applied to two different spectra ranges, one in 4,450–5,400 Å and one in the Gaia RVS range from 8,400–8,800 Å. In this study, we do not constrain the stellar parameter derivation technique, this was done previously inGebran et al., (2022,2023).
Interestingly, when applying the model to denoised spectra, there was no noticeable improvement in the accuracy of the derived fundamental parameters, such as,,,, and. This outcome was unexpected, as denoising is typically thought to enhance the precision of predictions. However, the results indicate that data augmentation plays a more crucial role. When the model is trained on datasets that include noise, the accuracy of predictions for noisy spectra improves significantly, suggesting that the network becomes better equipped to handle real observed spectra. This highlights the importance of incorporating noisy data into training rather than relying on post-processing techniques like denoising to improve accuracy.
To further validate these findings, it would be valuable to explore other denoising techniques and assess their impact on prediction accuracy. Techniques such as those presented inAlsberg et al., (1997),Koziol et al., (2018), andZhao et al., (2021)could be tested to see if they yield better results in reducing noise while maintaining or enhancing the precision of derived parameters. These additional experiments would help solidify the conclusion that data augmentation is more effective than denoising in improving the accuracy of noisy spectra predictions, offering deeper insights into how best to model real observational spectra.

Acknowledgment: The authors acknowledge Saint Mary's College for providing the high-power computing cluster used in this work. The authors are grateful for the reviewer’s valuable comments that improved the manuscript.

Funding information: Authors state no funding involved.

Author contributions: All authors have accepted responsibility for the entire content of this manuscript and consented to its submission to the journal, reviewed all the results, and approved the final version of the manuscript. MG and RB designed the code and carried out the calculations. MG prepared the manuscript with contributions from all co-authors.

Conflict of interest: The authors state no conflict of interest.

SECTION: References