SECTION: SVM: Soft-margin SVMs with-norm Hinge Loss
Support Vector Machines (SVMs) based on hinge loss have been extensively discussed and applied to various binary classification tasks. These SVMs achieve a balance between margin maximization and the minimization of slack due to outliers. Although many efforts have been dedicated to enhancing the performance of SVMs with hinge loss, studies onSVMs, soft-margin SVMs with-norm hinge loss, remain relatively scarce. In this paper, we explore the properties, performance, and training algorithms ofSVMs. We first derive the generalization bound ofSVMs, then formulate the dual optimization problem, comparing it with the traditional approach. Furthermore, we discuss a generalized version of the Sequential Minimal Optimization (SMO) algorithm,SMO, to train ourSVM model. Comparative experiments on various datasets, including binary and multi-class classification tasks, demonstrate the effectiveness and advantages of ourSVM model and theSMO method. Code is available at https://github.com/CoderBak/pSVM.

SECTION: Introduction
Support Vector Machine (SVM) algorithm, which is known for its simplicity and effectiveness, has been extensively utilized not only in practical classification tasks, such as image classification, healthcare, and cybersecurity, but also in learning algorithms, such as federated learning, and deep stacking networks.

Originally designed for binary classification, SVMs can be extended to handle multiclass classification tasks using One versus One (OvO) and One versus Rest (OvR) techniques. In a-class classification scenario, OvO constructsbase classifiers to determine hyperplanes separating each pair of classes, whereas OvR buildsbase classifiers to distinguish each class from the rest. Previous studies have indicated that OvO generally outperforms OvR in large-scale problems. However, the efficiency of OvO is constrained by the training time of base classifiers, and its performance heavily depends on the accuracy of these classifiers. Therefore, improving training efficiency and accuracy in SVMs can enhance the effectiveness of multiclass classification.

In the context of soft-margin SVMs, the parameterdetermines the trade-off between margin-maximization and the minimization of slack penalty. While most studies focus on SVMs with, defined as, recent studies have also explored SVMs with. For instance,extended Crammer and Singer’s multiclass SVMto apply L2 loss, achieving superior accuracy but at the cost of longer training time. This suggests the potential of introducing a more generalized-norm hinge lossto improve the accuracy and efficiency of SVMs. Nonetheless, studies on-norm hinge loss in SVMs remain limited.

To address these challenges, we propose a novel model,SVM, i.e. soft-margin SVMs with-norm hinge loss. This model generalizes both L1 and L2 loss SVMs and allows the selection of an optimal parameterto enhance the performance. However, this extension may increase training time. To mitigate this problem, we introduce theSMO method to efficiently solve the optimization problem associated withSVM training. Our approach facilitates the development of effective binary and multiclass classifiers.

Our contributions are summarized as follows.

We introduce a novel modelSVM, by incorporating a hyperparameterinto soft-margin SVMs, which enhances the flexibility in balancing margin maximization with the minimization of slack caused by outliers. Furthermore, we establish a generalization bound through the lens of margin theory, offering theoretical insight into the motivation behind the-norm hinge loss.

We develop theSMO method to address the challenges of solving the dual optimization problem in theSVM model. In particular, we present practical implementations ofSMO andSMO, and integrate them with the one-vs-one (OvO) strategy for effective multiclass classification.

Through extensive experiments on diverse datasets, our proposed methods demonstrate significant improvements in classification performance. These experiments on binary and multiclass classification tasks highlight the properties ofSVM models and the effectiveness ofSMO methods.

This paper is organized as follows. In Section 2, we review the fundamental concepts relevant to our topic, define key terms, and introduce our proposedSVM model. Section 3 explores the generalization bound and dual optimization problem of theSVM model, proving that it minimizes the upper-bound of the generalization error. In Section 4, we present theSMO method, aiming at effectively solving the dual optimization problem ofSVMs, and we described how multiclass classifiers can be constructed based on theSVM model andSMO method. Section 5 presents experimental results, highlighting the properties of theSVM model in binary classifications and evaluating the efficiency of theSMO method in comparison to state-of-the-art algorithms on multiclass classification tasks. Section 6 then concludes the paper.

SECTION: Preliminaries
Before delving into the main discussion, we first review some fundamental concepts relevant to our topic. The setis abbreviated asfor simplicity.

SECTION: SVMs for binary classification
A classification task can be framed as an optimization problem: Given a training sample, the learner aims to determine a hypothesis, or classifier,, that minimizes the generalization error. In the simplest case involving hyperplane classifiers, the objective is to find a hyperplane that optimally separates data points belonging to different classes, which is precisely the task of the Support Vector Machine (SVM) algorithm.

For training samples that are not linearly separable, the soft-margin SVMwas introduced, balancing the trade-off between maximizing the margin and minimizing the slack variables associated with outliers. This optimization problem is formulated as:

where slack variables can also be viewed as hinge loss, i.e.

By applying the Karush–Kuhn–Tucker (KKT) conditions and kernel methodsto Eq. (), we obtain the dual optimization problem,

and its solution directly yields the classifier:

In the context of SVMs, thefor each data pointin the training set is defined as its distance from the hyperplane. The SVM algorithm aims to identify a hyperplane that maximizes the confidence margin for the least confident data point. Consequently, the behaviour of soft-margin SVM can be estimated through margin theory.

SECTION: Margin Theory
The following definitions extend the concept ofby introducing a hyperparameter.

(Margin loss function)

(Empirical margin loss)

(Generalization error)

Fig.provides an intuitive view of the loss function. Note thatis upper-bounded by the-norm hinge loss:

SECTION: SVM Model
OurSVM model, which generalizes the soft-margin SVM by introducing the-norm hinge loss, can be formulated as the following optimization problem, whereis a hyperparameter that can be selected via cross-validation.

thus, ourSVM model is actually the soft-margin SVM with-norm hinge loss (), i.e.

which can be used to train binary classifiers.

SECTION: Theoretical Derivation
In this section, we discuss the generalization bound and dual optimization problem of ourSVM model.

SECTION: Generalization Bound
derived the following bounds on the generalization error and Rademacher complexity.

:

:

However, these results pertain specifically to the tradition soft-margin SVM, where. In our work, we extend these findings by deriving new theoretical results on the generalization bounds forSVMs.

.

This lemma, which highlights an important property of the margin loss function, serves as a foundation for the following theorem.

:

Theorem 3 leads to the following corollary, which establishes the generalization bound ofSVMs.

(Generalization bound ofSVMs):

Following a similar method in, we analyze our result as follows. Consider the right-hand side of Eq. (). Asincreases, the first term decreases while the second term increases. This trade-off highlights the significance of the hyperparameterin ourSVM model. By appropriately selecting, one can achieve an optimal balance between these two terms, thus minimizing the right-hand side of Eq. ().

Since only the first term of the right-hand side depends on, for any, one can choose the best hyperplane by selectingas the solution of the following optimization problem:

Introducing a Lagrange variable, the optimization problem can be equivalently written as

which precisely coincides with ourSVM model Eq. (), indicating that the learning performance ofSVMs is guaranteed by minimizing the right-hand side of Eq. ().

SECTION: Dual Optimization Problem
Applying the Karush–Kuhn–Tucker (KKT) conditions and kernel methods to Eq. () yields the dual optimization problem (which is concave) as follows:

where,. The traditional soft-margin SVM can be viewed as a special case where.

SECTION: Method
Previous derivations have revealed the generalization bound and dual optimization problem ofSVMs. A comparison between Eq. () and Eq. () reveals that extendingSVM toSVM primarily involves introducinginto the optimization problem. However, this additional term brings barriers to efficient optimization in practical scenarios.

The Sequential Minimal Optimization (SMO) algorithmis widely recognized for its efficiency in training SVMs by solving large-scale quadratic programming problems. Nevertheless, SMO is specifically designed for theSVM model, i.e.. In this section, we extend the SMO method toSMO: an SMO-style training algorithm designed forSVMs.

SECTION: SMO Method
The fundamental principle of SMO is to optimize two variables at a time, keeping the others fixed. Suppose we fixand focus solely onand. The new constraints arewhereis a constant. Letand, where

The objective function in Eq. () can be written as:

Solving Eq. () underyields the update algorithm presented in Algorithm. This algorithm takesas inputs, updatingandtoandin a way that maximizes the objective function while ensuring thatand. Lemma 2 guarantees that.

.

: Index

It is evident that the update algorithm coincides with SMO algorithm when, suggesting that ourSMO method generalizes the SMO method to a broader context.

The core update algorithm has been discussed in Algorithm, and the pseudocode of theSMO algorithm is presented in Listing. According to the original SMO method, the first indexis selected based on the violation of the KKT conditions, and the second indexis selected to maximize. However, in our experiments, we randomly selectbecause we observed that it is not worthwhile to allocate time to computingsolely for the potential theoretical improvement in convergence speed.

SECTION: Updating the Bias Parameter
In theSMO algorithm, parameterin Eq. () needs to be updated during each iteration. In this part, we discuss the implementation of this step.

In the original SMO method, the index setis used to define the, andcan be computed immediately from anyas follows:

However, due to the modification of the original optimization problem, this property no longer holds in theSMO method. Specifically,

Thus, in our algorithm, an alternative solution is applied:

where. Or equivalently,is set to the average bias on all the support vectors.

SECTION: Discussions on Special Cases:
While ourSMO algorithm is designed for all, one of the key challenges in its practical implementation lies in computing the unique solution to. This involves solving the equation:

where. For, Eq. () can be solved analytically. Whenor, Eq. () is simplified to a linear equation, and when, it becomes a quadratic equation. In this cases, implementing ourSMO method is more straightforward and retains the same time complexity as the original SMO algorithm. Consequently, in our experiments, we implementedSMO andSMO and evaluated their performances.

SECTION: Towards Multiclass Classification
Previous studies have shown that one-vs-one method may be more suitable than one-vs-rest method for large-scale problems in practical scenarios. Accordingly, we implemented our multiclass classifier using the one-vs-one strategy, which is based on binary classifiers optimized usingSMO.

SECTION: Experiments
In this section, we empirically evaluate the effectiveness of ourSVM model andSMO method on both binary and multiclass classification tasks. To ensure the reproducibility of our experiments, we set the random seed to 42 for all procedures involving randomness.

SECTION: Binary Classification Tasks
First, we evaluate the effectiveness of ourSVM model on binary classification tasks and explore how the value ofaffects the accuracy and the number of support vectors (nSV) on different datasets.

To ensure a fair comparison across different values of, we train ourSVM model using the cvxpy package, which utilizes the Gurobi optimizerto solve Eq. ().

Our method is compared with eight classic binary classification algorithms implemented in the scikit-learn package, including SVM (RBF SVM,), Gaussian Naive Bayes, Random Forests,-Nearest Neighbours, Gradient-boosted Trees, Linear Discriminant Analysis, Decision Trees, and AdaBoost (with Decision Tree estimator,).

The datasets selected for evaluation include Cancer (Breast Cancer Wisconsin,), Heart (Heart Disease,), Ionosphere (), Wine (Wine Quality,), and Banknote (Banknote Authentication,). Their number of featuresare reported in Table. More details of the datasets are provided in the Appendix.

OurSVM model and the baseline classifiers are evaluated using the same train-test split. The hyperparameterin both our model and the baseline SVM is selected via 5-fold cross-validation, with the optimal valuereported in Table. Both our model and the baseline SVM use Gaussian kernel as the kernel function, with its hyperparameterset to, which is the default setting of the scikit-learn package. Other baseline classifiers are evaluated over 20 runs due to their sensitivity to randomness, with the average accuracy reported in Table.

cells = c,
cell11 = r=2,
cell12 = r=2,
cell13 = r=2,
cell31 = r=5,
cell32 = r=5,
cell33 = r=5,
cell64 = r=2,
cell65 = c=2,
cell75 = c=2,
cell81 = r=5,
cell82 = r=5,
cell83 = r=5,
cell114 = r=2,
cell115 = c=2,
cell125 = c=2,
cell131 = r=5,
cell132 = r=5,
cell133 = r=5,
cell164 = r=2,
cell165 = c=2,
cell175 = c=2,
cell181 = r=5,
cell182 = r=5,
cell183 = r=5,
cell214 = r=2,
cell215 = c=2,
cell225 = c=2,
cell231 = r=5,
cell232 = r=5,
cell233 = r=5,
cell264 = r=2,
cell265 = c=2,
cell275 = c=2,
hline1,28 = -0.08em,
hline3,8,13,18,23 = -0.05em,

Setting & Train  Test1.00  1.25  1.29  1.33  1.40  1.50  1.67  2.00  3.00/  5  4.5  4  3.5  3  2.5  2  1.5Cancer()398(70%)  171(30%)/  5  5  5  5  5  5  5  10nSV%  31.2  31.2  30.9  31.2  31.7  31.7  32.2  34.9  39.4acc%baseline  SVM ()   NB  RF  KNN  GB  LDA  DT  AB93.57  96.93  95.91  95.91  95.32  93.13  97.08Heart()  189(70%)  81(30%)/  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.1nSV%  78.3  79.4  79.4  79.4  84.1  86.2  89.4  93.7  100acc%  82.7283.95  83.95  83.95  83.95baseline  SVM ()   NB  RF  KNN  GB  LDA  DT  AB82.72   83.95  80.93  80.25  78.58  83.95  72.47  81.48Ionosphere()  245(70%)  106(30%)/  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1nSV%  59.2  89.0  89.0  90.6  91.4  94.3  98.8  100  100acc%  95.28  96.23  96.23  96.23baseline  SVM ()   NB  RF  KNN  GB  LDA  DT  AB96.23   88.68  95.57  83.96  94.76  84.91  88.73  92.45Wine()  649(10%)  5848(90%)/  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.1nSV%  79.0  79.5  79.8  79.5  82.4  84.3  87.2  91.7  100acc%  74.49  74.86  74.91  74.88  75.07  75.15  75.4174.91baseline  SVM ()   NB  RF  KNN  GB  LDA  DT  AB74.56   70.09  75.50  69.82  75.48  73.58  67.42  73.12Banknote()  411(30%)  961(70%)/  0.5  0.5  0.5  0.5  0.5  0.5  0.5  1nSV%  26.3  40.6  41.6  45.5  48.4  49.1  56.2  66.4  82.2acc%baseline  SVM ()   NB  RF  KNN  GB  LDA  DT  AB99.38   85.02  98.47  98.86  98.96  97.29  96.93  98.96

The evaluation results are reported in Table, with the highest performance on each dataset highlighted in bold. The training process on the Wine Dataset is depicted in Fig., where “Train size” denotes the proportion of the training sample used for training. Our experimental findings lead to several key conclusions:

Compared to other widely used classification algorithms,SVM achieves the best classification performance on all selected datasets. This is largely due to the introduction of the free parameter, which brings more flexibility to the traditional SVM. Notably, the best accuracy of ourSVM model on the Heart Disease Dataset beats MeanMap, InvCal, conv-SVM and alter-SVM. However, introducing the termincreases the training time, especially when. Therefore, introducing theSMO method is essential towards making large-scale training more feasible.

When, the optimization problem (Eq.) has no upper-bound on, which can be interpreted as a traditional soft-margin SVM where. Therefore, ourSVM is actually a “hard” SVM which hasaccuracy on the training sample which can be linearly separated (using Gaussian Kernel) as reported in Fig., leading to its lower accuracy than traditional soft-margin SVM as reported in Table.

Asincreases, nSV increases accordingly, suggesting that the model learns better on the training set, but at the cost of an increased risk of overfitting. The results in Tablesupport this conclusion: largervalues lead to higher nSV, however, the test accuracy initially increases and then decreases, indicating that the model initially learns the features of the training set well, but begins to overfit ascontinues to grow. Based on our experiments, we infer that ourSVM model performs optimally when. Therefore, one can determine the bestvalue within this range using cross-validation, or simply selectorand applySMO orSMO for faster training.

SECTION: Multiclass Classification Tasks
Next, we evaluate the effectiveness of ourSVM model andSMO method on multiclass classification tasks.

Recent advances in multiclass classification have led to the development of powerful methods, such as CappedSVMand the state-of-the-art methodSVM. To facilitate a comparative analysis, we draw on the data fromto benchmark our model against these established approaches.

Our model was evaluated against the same set of baselines as used in, which includes eight multiclass classification algorithms, including one-vs-rest (or one-vs-all, OvR), one-vs-one (OvO), Crammer, M-SVM, Top-k, Multi-LR, Sparse Multinomial Logistic Regression (SMLR,), andSVM.

cells = c,
hline1,6 = -0.08em,
hline2 = -0.05em,

Dataset & Instances  Features  ClassesGlass  214  9  6Vehicle  845  18  4Dermatology  358  34  6USPS  9298  256  10

The datasets chosen for evaluation include Glass, Vehicle, Dermatology, and USPS. The properties of these datasets are summarized in Table. Additional details about these datasets are provided in the Appendix.

cells = c,
hline1,6 = -0.08em,
hline2 = -0.05em,

Train & Test  Methods  OvR  OvO  Crammer  M-SVM  Top-k  Multi-LR  SMLR1.5L  2L171  43  Glass  0.656  0.685  0.594  0.629  0.674  0.664  0.679  0.744  0.744676  169  Vehicle  0.794  0.756  0.757  0.762  0.778  0.780  0.771  0.8000.828286  72  Dermatology  0.939  0.971  0.933  0.868  0.891  0.965  0.9657438  1860  USPS  0.887  0.898  0.769  0.910  0.825  0.932  0.937  0.956  0.959

We follow the same experimental settings outlined in. Both ourSMO algorithm and the baseline algorithms are evaluated using the same 8:2 train-test split, with the test accuracy reported in Table. All the algorithms are trained using a linear kernel. In Table, “1.5L” stands for “SMO + Linear Kernel”, and “2L” stands for “SMO + Linear Kernel”. We report the performance ofSMO onselected from grid search.

The evaluation results are presented in Table, with the highest performance on each dataset highlighted in bold. Compared to other multiclass classification algorithms, ourSMO algorithm demonstrates superior performance, particularly on the USPS Dataset.

Our experimental findings lead to two key conclusions: Firstly, the OvO method shows improved performance when the base classifier is replaced with ourSVM model. Secondly, ourSMO algorithm surpasses the state-of-the-art method across various datasets.

SECTION: Conclusions
Soft-margin SVMs are well-established in various classification tasks and algorithms, yet the optimal normfor hinge loss has received limited attention. To address this, we introduced theSVM model and theSMO algorithm, designed to deliver both precise classification and efficient training. Through theoretical analysis and empirical experimentation, we derived the generalization bounds forSVMs and detailed the practical implementations ofSMO andSMO. Our experimental results on multiple datasets highlight the superior performance of our methods compared to traditional and state-of-the-art approaches.

SECTION: References
SECTION: Appendix
SECTION: A.1 Proof
.

Assuming.

, we have

, we have

, we have

using the fact thatis monotonically increasing on.

, we have

using the fact thatis monotonically increasing on.

:

.

:

. Use the same proof method as in Theorem 5.8 of, and the only difference is thatis-lipschitz.

:

. Use the same proof method as in Theorem 5.9 of, and the only difference is thatis-lipschitz.

:

. Letin Claim 1 and use it in Claim 3 directly to get the upper-bound.

(Generalization bound ofSVMs):

.is upper-bounded by the-norm-hinge loss:

Thus, letand assume that. Fix, then, for any, with probability at leastover the choice of a sampleof size, the following holds for anyand:

Here we need. However, if we considerand, we can directly derive Corollary 1.

The dual optimization problem of ourSVM model is as follows.

where,.

. We introduce Lagrange variables, so the Lagrangian can be defined by

The KKT conditions are as follows.

By plugging the equations into the Lagrangian, we have

When we choose the traditional-norm penalty in SVMs, we have

so our optimization problem is

which has been widely discussed in previous works.

Now we consider the SVMs with-norm loss and. Consider the set of indexes, we can infer that, and, leading to a common conclusion that

So our optimization problem becomes

We can rewrite our objective function as follows, assuming that, then

The key idea of SMO is: freeze all the variables except two of them, and look at only two variables. Assume that we are freezingand we only considerand. New constraints arewhereis a constant. Letand, where

The optimization problem can be written as follows:

.

Using the Cauchy-Schwarz inequality on the given reproducing kernel Hilbert space (RKHS), we have

We want to updateandintoand. It is obvious that

or equivalently,

Plugging the equation above intotransfersinto a single variable optimization problem, i.e.as follows.

So we have

Let. For any, we have.

Let, thenis monotonically decreasing on. We want to find outandwhich maximize. We consider the following cases and discuss them separatedly.

In this case, we have. We can immediately derive that

..

..

.satisfies.

In this case, let, we have. We can immediately derive that

..

.satisfies.

We can summarize our case analysis above as follows. Let

1..

..

..

Otherwise, solveonto get.

2.. Let.

..

Otherwise, solveonto get.

SECTION: A.2 More on Experiments
Figure 3 illustrates the training process on USPS withand.

All the datasets used in our experiments were cleaned and scaled. We removed any rows with missing values and applied the sklearn.preprocessing.scale function to standardize the data.

For binary classification datasets, labels were standardized to. In the case of the Wine Quality dataset, where labels represent a quality level from, we categorized samples with quality levels in the rangeas negative and those inas positive.

All the results presented in this paper can be reproduced using the provided code. To run our code, please ensure that all dependencies listed in requirements.txt are installed. Please note the following:

The cupy package is utilized to accelerate matrix computations on GPUs. If you prefer to use numpy, please refer to the code for instructions on how to adapt it accordingly.

A Gurobi license (e.g., Academic License) is required to run our code, as the Gurobi optimizer is used in the experiments.

To reproduce our results, please run “python eval.py” under “binary” and “multiclass” folder in the code we provided.