SECTION: Modular Duality in Deep Learning

An old idea in optimization theory says that since the gradient is adual vectorit may not be subtracted from the weights without first being mapped to theprimal spacewhere the weights reside. We take this idea seriously in this paper and construct such a duality map for general neural networks. Our map, which we callmodular dualization, forms a unifying theoretical basis for training algorithms that are a)fastand b)scalable. Modular dualization involves first assigning operator norms to layers based on the semantics of each layer, and then using these layerwise norms to recursively induce a duality map on the weight space of the full neural architecture. We conclude by deriving GPU-friendly algorithms for dualizing,andlayers—the latter two methods are based on a rectangular Newton-Schulz iteration(Kovarik,1970; Björck & Bowie,1971). A variant of our methods was used to set speed records for training NanoGPT. Overall, we hope that our theory of modular duality will yield a next generation of fast and scalable optimizers for general neural architectures.

SECTION: 1Introduction

In this paper, we pursue a rigorous and first-principles theoretical framework for designing neural network training algorithms. We hope that building such a framework will facilitate the design of a next generation of fast and scalable optimizers that are automatically tailored to different neural architectures.

While gradient descent is the workhorse of modern machine learning, the most vanilla form of the algorithm does not, in our view, pass a basictype check. For a gradient update to type check, we insist that the gradient must be passed through a duality map before being multiplied by a learning rate and applied to the weights:

Why? The reason is that the loss function may not be equally smooth in all directions in weight space, and there is no reason for the sizes of different components of the raw gradient vector to respect this heterogeneity. In other words,the geometry of the loss function may be non-isotropic. Insisting on a type check should force the user to become cognizant of this issue and to find a suitable duality map. A good duality map should adjust the size and direction of the gradient to respect the smoothness structure of the loss function.

Duality maps on vector spaces are commonplace in physics and applied math. Examples include themusical isomorphismin differential geometry(Grosse,2022),raising and lowering indicesin general relativity(Carroll,2019)and thebra-ket notationin quantum mechanics(Sakurai & Napolitano,2020). Duality maps are also central to several optimization theories includingmirror descent(Nemirovsky & Yudin,1983),natural gradient descent(Amari,2016)andsteepest descent on a normed space(Boyd & Vandenberghe,2004). Despite the efforts of some prescient papers(Carlson et al.,2015b; Flynn,2017), the latter kind of duality map involving normed vector spaces is yet to puncture the deep learning mainstream.

We believe that duality is a key theoretical concept that will help in building performant large-scale machine learning systems. To support this belief, we show in this paper that two important and seemingly disparate methods in contemporary optimization research may be seen as approximations to a single duality map. These methods aremaximal update parameterization(Yang & Hu,2021), which is aimed at scalable training, andShampoo(Shi et al.,2023), which is targeted at fast training. We show inSection4.1that both methods emerge as partial approximations to a single duality map induced by the RMS–RMS operator norm.

The main contribution of this paper is to describe a procedure for constructing duality maps for general neural architectures. The procedure, which we callmodular dualization, works in three steps:

Operator norms are assigned to individual layers based on the input-output semantics of each layer;

Based on these operator norms, duality maps are constructed for individual layers;

Given the layerwise duality maps and the structure of the neural architecture, a single duality map is recursively induced on the full weight space of the architecture.

To instantiate this procedure for a rich family of neural architectures—including convolutional networks and transformers—we write down duality maps for,andlayers. We also provide GPU-friendly algorithms for computing these duality maps. Overall, we hope that modular dualization will help in the principled design of the machine learning systems of the future.

SECTION: 2Related Work

This paper constructs a duality map for general neural architectures. Our approach is based on assigning operator norms to individual network layers and using these layerwise norms to recursively induce a duality map on the full neural architecture. The most closely related prior work is a series of papers onspectral descent(Carlson et al.,2015a;b;2016)and a paper onduality structure gradient descent(Flynn,2017).

Spectral descent has been applied to restricted Boltzmann machines(Carlson et al.,2015a)and discrete graphical models(Carlson et al.,2016), but let us focus on the more closely related paper on spectral descent for deep learning(Carlson et al.,2015b). In that paper, the authors propose assigning the Schatten-norm (a.k.a. spectral norm) to individual linear layers. This assignment is based on the observation that neural networks admit natural majorization bounds in the Schatten-norm. The authors call the corresponding duality map for linear layers the “#-operator”—a name presumably inspired by the musical isomorphism(Grosse,2022). The authors propose a cheap approximation to the #-operator based on sketching(Martinsson & Tropp,2020), and they also propose a way to mix RMSprop-style pre-conditioning information(Tieleman & Hinton,2012)into the weight updates. In contrast to our work, the authors only derive duality maps for single linear layers, and these maps are then heuristically extended to all-layer updates. Nonetheless, the authors achieve substantial wall clock speedups using variants of spectral descent to train small networks.

Now, let us turn our attention to duality structure gradient descent(Flynn,2017), which constructs a duality map on the full weight space of the neural architecture based on identifying aFinsler structure(Deimling,1985)inherent to neural networks. Similar to modular dualization,Flynn (2017)’s duality map works by assigning duality maps to each layer and then inducing a duality map on the full weight space. The substantial difference to our approach is thatFlynn (2017)leverages a weighted sum (combination) of layerwise norms to construct his full duality map. This leads to optimization methods that only update a single layer at each iteration, and the methods need to be heuristically extended to achieve all-layer updates. In contrast, we leverage the modular norm(Large et al.,2024), which takes a weighted max (combination) of layerwise norms. In turn, our duality map leads directly to more conventional
all-layer optimizers.

Another important difference between our work on modular duality and prior work on duality structure gradient descent is that we fully “modularize” our theory—meaning that our construction is explicitly recursive—and as such it is easy to code up into a software package. In this regard, we are inspired by a line of work that attempts to build optimization algorithms that automatically adapt to the structure of general computation graphs. The earliest work we know of in this category is the PhD thesis ofGrant (2004)on disciplined convex programming, which aims to infer the convexity properties of general functions by breaking them up into subexpressions and applying composition theorems from convex analysis. More recent progress in this vein includes work on universal majorization-minimization algorithms(Streeter & Dillon,2022; Streeter,2023)and related papers on automatic majorization(Tran et al.,2015; Bernstein et al.,2023).

SECTION: 3Theoretical Preliminaries

In this section, we introduce duality maps, a means of constructing duality maps based on norms, and finally a norm called themodular normthat is well-suited to describe the geometry of general neural architectures.

SECTION: 3.1Duality Maps

Given a vector space, we say that a functionis alinear functionalonifis linear. We define thedual spaceto be the set of linear functionals on the vector space. The dual space is itself a vector space provided that addition is defined pointwiseand scalar multiplication is defined pointwisefor any scalar. Byduality mapwe simply mean any function that sends members of the dual vector spaceto the primal vector space. The function need not be an involution.

Letdenote the loss of a differentiable machine learning model with weight space. The Taylor expansion of the loss at weight settingis given by:

Observe that, in the first-order term, the gradientis acting as a linear functional: it is pairing with the weight vectorin a linear way to produce a real number. As such, we shall say that the gradient belongs to the dual weight space:. We shall forbid ourselves from directly subtracting a member of the dual weight spacefrom the weight space. If we would like to conduct a gradient descent update, then we had better find a duality map to send the gradient back to the primal space.

This restriction may seem absurd! After all, here the weight spaceand its dualare both just. However, insisting upon this type check serves to remind us that the curvature of the loss function may be highly heterogeneous. The next section will show one way to construct duality maps to account for this.

SECTION: 3.2Steepest Descent on a Normed Space

Suppose that we have found anormand asharpness parameterthat serve as a good model of the higher-order terms in the Taylor expansion of the loss function given inEquation3:

In other words, the norm provides a good characterization of the heterogeneity in curvature of the loss function. Then it makes sense to solve for a weight updateby minimizing the right-hand side ofEquation4. We will show that the minimizer can be expressed in terms of adual normand aduality map:

Given a norm, the dual normof a vectoris given by:

Given a norm, we consider the duality map:

where, if theis not unique,returns any maximizer.

Given these definitions, minimizing the expression in the right-hand side ofEquation4can be done using the following standard proposition, for whichBernstein & Newhouse (2024)provide a proof:

For anythought of as “the gradient”, anythought of as “the sharpness”, and any normwith dual normand duality map:

In words: to find the minimizer of a linear term penalized by a squared norm, we need only evaluate the dual norm and a duality map. In this paper, we focus on constructing a duality map for themodular norm, which is defined on general neural architectures. The next section reviews duality maps for more standard norms.

SECTION: 3.3Basic Norms and Duality Maps

Many basic norms and duality maps are already covered in prior work(Carlson et al.,2016;2015a;2015b; Flynn,2017). For some warmup examples, the following duality maps for vector norms are standard:

For a vector, we have.

For a vector, we have, where the sign function is applied entrywise and we are free to take.

In neural networks, the weight spaces of individual layers tend to have matrix structure. And layers with the same shape weight matrix may have semantically different input and output spaces—thinkembeddingversuslinearlayers in a transformer. As such, we will need duality maps for differentinduced operator norms:

Given a matrixand two normed vector spacesand, the “to” induced operator norm is given by:

For tensors, we define the duality map via. For linear layers, we will need the duality map for theinduced operator norm. This ends up as a rescaled version of the spectral norm duality map from prior work(Carlson et al.,2015b; Flynn,2017).

For a vector, we define the RMS norm to be the normalized Euclidean norm:. Given a matrix, theinduced operator norm resolves to a rescaled spectral norm:, wheredenotes the standard spectral norm. For a matrixwith reduced singular value decomposition, the corresponding duality map is given by

And for embedding layers, we will need the duality map for theoperator norm:

Given a matrix, theinduced operator norm resolves to the maxnorm of the columns:. For a matrix, the corresponding duality mapsimply normalizes each column ofto have unit RMS norm:for each.

SECTION: 3.4The Modular Norm

Themodular norm(Large et al.,2024)is intended to help characterize the heterogeneous curvature of general neural architectures. The construction first defines an abstractmoduletype along with a notion of what is a good, orwell-normed, module. Thencombination rulesare given for constructing new well-normed modules from a library of existing well-normed modules. So modules are a special case ofcombinator patternfrom functional programming(Haskell Wiki Contributors,2007). Modules are also related to amonoidal categoryfrom category theory(Fong & Spivak,2019). We begin by defining the abstract notion of amodule:

Given input vector space, output vector spaceand weight vector space, a moduleis an object with the following four attributes:

a function,, which maps an input and a weight vector to an output;

a number,, which is used to set the proportion of feature learning that this module contributes to any supermodule;

a number,, which estimates the module’s sensitivity to input perturbations;

a norm over the weight space,, sometimes abbreviated to just.

We shall care most about modules that arewell-normed, which amounts to requiring that the forward function is Lipschitz-continuous in the weights with constant 1 and in the inputs with constant:

Letbe a module on, where the input and output spaces have respective normsand.is well-normed if for all inputsand weights:

Theoperator denotes summation over any shared tensor indices. This definition of well-normed-ness can be used as a guiding principle in the design of a library of atomic (i.e. handwritten) modules. First, norms should be assigned to the input and output space of each module based on the semantics of. Then a normshould be assigned to the module’s weight space and a numbershould be chosen to make the module well-normed. Examples are given inSection4.1.

Given such a library of well-normed atomic modules, a compound module built through any arbitrary sequence ofmodule compositionsandmodule concatenationsis automatically well-normed(Large et al.,2024). And if the atomic modules in the library are not only well-normed but are alsosmoothin an appropriate sense, thenLarge et al. (2024)give an automatic procedure for computingsharpness coefficientsfor any compound module built from the library. The relevant definition of module composition is as follows:

Consider modulewith input, output and weight spaceand modulewith input, output and weight space.andare composable if. Their composite modulehas input, output and weight spaceand attributes:

;

;

;

given by:

where iforis zero, the corresponding term in theis set to zero.

So the composite norm is taken to be a weighted max over the norms of the two sub-modules, where the weight space of the first module is coupled to the input sensitivity of the second module. The module masses provide freedom to tune the importance of each sub-module in the norm, andLarge et al. (2024)prove that module mass provides precise control over the amount of feature learning that can happen in each sub-module.

Module concatenation is defined in a similar way to module composition:

Consider modulewith input, output and weight spaceand modulewith input, output and weight space. We say thatandare concatenatable if their input spaces match:. The tuple modulehas input, output and weight spaceand the following list of attributes:

;

;

;

given by:

where iforis zero, the corresponding term in theis set to zero.

A shortcoming of the paper byLarge et al. (2024)is that the power of the modular norm is not fully leveraged. In particular, the authors domodular normalizationof training, where weight updates to modules are sometimes just naïvely divided by their norm. In this paper we make fuller use of the geometry implied by the modular norm by constructing the corresponding duality map, which we callmodular dualization.

SECTION: 4Modular Dualization

In this section, we construct a duality map for general neural architectures. Our strategy is to first write down duality maps for atomic modules, i.e. individual layers. We then extend to arbitrary compound modules, i.e. full neural networks, by showing how duality maps should pass through composition and concatenation.

SECTION: 4.1Duality Maps for Atomic Modules

To construct a duality map for an atomic module, the idea is to first fix norms on the input and output spaces that respect the semantics of. We should select norms that describe both how large we would like the inputs and outputs to be, and in what geometry we would like the outputs to evolve. Then we place a norm on the weight space such thatis well-normed: this is typically the operator norm (Definition3) induced by the input and output norms. Finally we are in position to solve for the duality map, which we shall call. We now give some examples of this procedure for the basic layer types of,and. The results are summarized inTable1.

We start with the canonical example of an atomic module:

Themodule sends inputs fromto outputs in. The weight space is given by the matrix space. We endow themodule with attributes:

, the matrix-vector product;

;

, whereis a hyperparameter;

, theinduced operator norm.

Since themodule is intended to map to and from vectors of roughly unitnorm, we place thenorm on both the input and output space:and. Thenis well-normed if the inputs and weights belong to the unit ballsand. Referring back toSection3.3, the duality map corresponding tois then given by:

, where the gradienthas reduced SVD.

This single duality map recovers essential features of bothmaximal update parameterization(Yang & Hu,2021,P)andShampoo(Gupta et al.,2018). In particular, the factor ofinrecovers spectral update scaling(Yang et al.,2023)that leads toP. (Initializing such thatalso recoversP initialization scaling.) And the mappingis equivalent to Shampoo without accumulation(Bernstein & Newhouse,2024). As such, we believe that duality maps may help reconcile different strands of deep learning research and provide a unifying basis for fast and scalable training algorithms.

Themodule provides a useful counterpoint to themodule. The difference between the two modules stems from the fact that the input spaces ofandhave different semantics.

Themodule sends inputs fromto outputs in. The weight space is given by the matrix space. We endow themodule with attributes:

, the matrix-vector product;

;

, whereis a hyperparameter;

, theinduced operator norm.

is intended to map from one-hot vectors to vectors of roughly unitnorm, so we place thenorm on the input space and thenorm on the output space:and. Thenis well-normed if the inputs and weights belong to the unit ballsand. Referring back toSection3.3, the duality map foris:

performs the mappingfor each column index.

Finally, we consider amodule with akernel.has a more involved tensor structure thanand. The calculations work by slicing up the weight tensor into a collection ofmatrices.

Themodule sends inputs fromto outputs in. We think of this as mapping an input image of width, heightand withcolor channels to an output image of width, heightand withcolor channels. The weight space is given by the tensor space, whereis the kernel size. We endowwith attributes:

, wheredenotes 2D convolution;

;

, whereis a hyperparameter;

, the maxnorm over kernel indices.

We would like pixel intensities in the inputs and outputs to be order one and undergo order one change. We formalize this by taking the input and output norms to be the spatial maximum of the RMS norms of all the color channel vectors:and. Thenis well-normed if the inputs and weights belong to the unit ballsand. Since the duality map for a max of norms decouples into one duality map per sub-norm, the duality map corresponding tois given by:

does, wherehas reduced SVD.

SECTION: 4.2Duality Maps for Bond Modules

Large et al. (2024)define another class of basic modules:bond modules. Bonds are handwritten modules without weights. An example of a bond is thenonlinearity. For a bond, the weight space is the zero vector spaceand the modular norm. As such, the corresponding duality map is also. In a software package, one need not write norms or duality maps for bond modules.

SECTION: 4.3Duality Maps for Compound Modules

First, given two composable modulesand, the duality map for the compositeis given by:

And second, given two concatenatable modulesand, the duality map for the tupleis:

The proofs ofEquations11and12follow in a straightforward manner fromDefinitions6and7.

SECTION: 5Fast Duality Maps

For modular dualization to be practically feasible, we need ways of computing duality maps quickly. Inspecting the duality maps listed inTable1, we see thatis easy to implement since it just involves computing vector norms of matrix columns. Butandinvolve the projection:

whereis the reduced SVD of the matrix. Since computing SVDs can be slow(Carlson et al.,2015b; Flynn,2017), here we discuss three fast approximations toEquation13via sketching, iterations for inverse matrix roots, and a family ofrectangular Newton-Schulziterations. Which method works best in practice may depend on the condition number of the matrixor the available computational resources.

SECTION: 5.1Sketching

Sketching is a randomized method(Martinsson & Tropp,2020)that can be used to build low-rank approximations to the SVD.Carlson et al. (2015b)already used sketching to provide a fast approximation to their-operator. More recent papers have experimented with sketching in the context of Shampoo-type algorithms(Feinberg et al.,2023). A potential downside of approximatingEquation13via sketching is that randomized SVD methods usually try to accurately approximate the largest singular values of a matrix(Martinsson & Tropp,2020, Section 11.2)while the value ofEquation13may lie in its action on the small singular values.

SECTION: 5.2Iterations for Inverse Matrix Roots

Given a full rank matrixwith reduced SVD, we have that:

This provides a route to approximatingEquation13since one can compute inverse matrix roots such asvia Newton iteration(Lakić,1998). This is discussed in Chapter 7 ofHigham (2008)’s book and also seeAnil et al. (2020)’s paper. Care must be taken with inverses whenever the matrixis ill-conditioned.

SECTION: 5.3Rectangular Newton-Schulz Iteration

We developed a “rectangular Newton-Schulz iteration” for computingby adapting Equation 5.22 inHigham (2008)’s book for computing the “matrix sign function”. We later discovered that this iteration has a long history(Kovarik,1970; Björck & Bowie,1971). In short, the method works by first normalizing the matrixaccording to(or alternatively) and then iterating:

then as, the sequence. To see this, one can plot the univariate cubic functionand see that, for, iterating this cubic will pushcloser and closer to. The final step is to realize that the effect of the iteration inEquation15is to apply this cubicto each singular value of. This shows that the spectral normalizationis stronger than what is required: we need only ensure thathas singular values no greater thanfor the iteration to converge.

The iteration inEquation15has the advantage over sketching that it always works on all singular values, and since the iteration does not compute inverse matrix roots it is well-behaved even on low-rank matrices.

Finally, there are in fact a family of degreepolynomial iterations of the form

for suitablethat could be used instead ofEquation15. One should choose coefficientsso that the univariate polynomialis a suitable approximation to. One may try to further accelerate the iteration by “tuning” the coefficientsempirically.

SECTION: 6Discussion

This paper develops the theory ofmodular dualityand the procedure ofmodular dualizationas means to construct duality maps for general neural architectures. Here, we comment on implications and connections.

SECTION: 6.1A Type System for Deep Learning

Part of the inspiration for this work is the idea of building a fully-fledgedtype systemfor deep learning. We think that activation spaces should be typed by their intended norm and the intended size of activations in that norm. This information would help in the construction of well-normed modules (seeSection4.1). Modules should be typed according toDefinition4. And, as suggested in the introduction, gradients should be explicitly typed as dual vectors. A duality map should flip the type of a dual vector to a primal vector. We plan to use the Modula deep learning package(Large et al.,2024)as a testbed for these ideas.

SECTION: 6.2Neural Network Speedrunning

We believe that the ideas in this paper can help in the design of faster training methods. In fact, a new NanoGPT training speed record was recently set(Jordan,2024)using a Newton-Schulz-based duality map. We communicated the method to Keller Jordan through our workshop paper(Bernstein & Newhouse,2024).

SECTION: 6.3Modular Duality: A Unifying Theoretical Framework for Fast and Scalable Training

An important topic in contemporary optimization research is the design of fast and scalable training methods for neural networks. In fact, the theme of the Optimization for Machine Learning workshop at this year’s NeurIPS conference is “scaling up optimization”(OPT,2024). Two popular methods in this research space aremaximal update parameterization(Yang & Hu,2021,P), which allows for increasing network width without changing the optimal learning rate, andShampoo(Gupta et al.,2018), a variant of which(Shi et al.,2023)won a speed challenge at the inaugural AlgoPerf optimization competition(Dahl et al.,2023).

We showed inSection4.1that essential features of bothP and Shampoo are recovered from the single duality map. We think that, on a basic theoretical level,P and Shampoo should be viewed as partial approximations to this duality map. This observation helps putP and Shampoo on a consistent theoretical footing, orients the methods with respect to overlooked prior work on spectral descent(Carlson et al.,2015b)and duality structure gradient descent(Flynn,2017), and suggests new ways to generalize these methods to arbitrary layer types and network architectures via the modular norm and modular dualization.

SECTION: 6.4On the Alignment of Activations and Updates

Recent work(Yang et al.,2023; Everett et al.,2024; Large et al.,2024)has singled out the following question as important to the design of scalable deep learning systems:to what extent do gradient updates to neural network layers align with incoming activation vectors?This question is important since it helps inform how large weight updates need to be to induce a certain amount of change in layer outputs. Duality maps such asandmay help simplify the answer to this question, since they project gradients to scaled semi-orthogonal matrices for which all singular values have the same magnitude.

SECTION: 6.5A Numerical Paradox:The Weights Don’t Change!

Past work(Lee et al.,2019; Jesus et al.,2021)has pointed out an apparent paradox in deep learning: the weights seem to move a vanishing amount from initialization in the limit of large network width. This finding has motivated a substantial amount of work on linearized training dynamics(Jacot et al.,2018). We attempted to resolve this paradox in prior work by showing that the weights move a roughly constant amount at any width when the change is measured in spectral norm(Yang et al.,2023). But duality maps change the story again:ramps up the stable rank of updates, so the weights should move a non-trivial relative amount at large widtheven in the Frobenius norm—provided the batch size is not too small.

SECTION: 7Conclusion

This paper has proposed a recursive procedure calledmodular dualizationfor building duality maps for general neural architectures. The procedure unifies past strands of optimization research on Shampoo(Gupta et al.,2018)andP(Yang & Hu,2021). Partial implementations have already led to significant wall-clock speedups in transformer training(Jordan,2024). The rectangular Newton-Schulz iteration provides a GPU-friendly and numerically stable means of dualizing under theoperator norm, while avoiding some of the downsides of sketching-based approaches(Carlson et al.,2015b). Overall, we hope that our theory ofmodular dualityprovides a clarifying toolkit for the design and analysis of deep learning systems.

SECTION: Acknowledgements

Many ideas in this paper were developed jointly with Tim Large before he left to work at a tech company.
We are grateful to Phillip Isola for invaluable discussions. We also thank Jack Gallagher, Keller Jordan, Simo Ryu, Rogier Brussee, Tongzhou Wang, Victor Butoi, Jeffrey Cider and Volkan Cevher for helpful conversations.

SECTION: References