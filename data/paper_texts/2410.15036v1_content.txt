SECTION: EViT-Unet: U-Net Like Efficient Vision Transformer for Medical Image Segmentation on Mobile and Edge Devices
UNet, Vison Transformer(ViT), Segmentation, Attention, Computational Efficiency

SECTION: Introduction
With the rapid development of deep learning, the field of medical image analysis has made significant progress, especially in image segmentation tasks. U-shape networks have become the mainstream model for segmentation networks. The structure integrates a symmetric encoder-decoder configuration with a bottleneck layer and skip-connection, constituting the classic U-shaped segmentation network. The classical U-Net architecture employs convolutional downsampling to capture multi-level features. It incorporates skip connections to preserve spatial details, followed by decoder-based upsampling to reconstruct image resolution for precise pixel-level segmentation. This structure has succeeded greatly in various medical image segmentation tasks, such as heart, organ, and lesion segmentation. Some works based on U-Net aim to enhance network performance, such as U-Net++, which improves network efficiency by refining the skip connection mechanism. Furthermore, DeepLabV3enhances the processing of multi-scale features by optimizing convolutional operations. However, CNN-based UNet still needs to improve in capturing global semantic information and handling complex feature interactions.

The introduction of Vision Transformers (ViTs) addresses the limitations of CNNs in capturing global information. The self-attention mechanism in ViTs effectively captures global context, providing significant advantages over traditional CNNs in managing global features and long-range dependencies. Researchers have started exploring their application in medical image segmentation. For instance, Att-UNetand TransUNetintroduced Transformers to the UNet network, and hybrid CNN-Transformer models like HiFormerand UCTransUNetwere designed to improve network speed. Furthermore, SwinUNetand MedTemploy specialized Transformer architectures to enhance network speed and performance. The self-attention mechanism in ViTs improves segmentation accuracy and robustness, further advancing effectiveness in medical image segmentation tasks.

Although ViTs exceptionally perform in vision tasks, their high computational complexity limits their applicability on resource-constrained devices. In medical image segmentation tasks, reducing computational complexity while maintaining accuracy has always been a key pursuit in segmentation tasks for resource-constrained devices.Thus, we propose EViT-UNet, an efficient U-shaped network based on ViT for medical image segmentation on mobile and edge devices. It inherits ViT’s ability to capture global information while reducing computational complexity through the combination of convolution and self-attention mechanisms, ensuring high accuracy while minimizing computational cost, making it ideal for mobile and edge devices. Tested on multiple datasets, EViT-UNet demonstrated superior segmentation accuracy and outperformed other popular segmentation frameworks. Our key contributions are as follows: (1) Developing an efficient U-shaped segmentation framework based on ViT that integrates an encoder, a decoder, and skip connections, which has shown outstanding performance across diverse datasets. (2) Achieving the best computational efficiency in comparative analyses with multiple networks. (3) Successfully reducing computational complexity while maintaining high accuracy, enhancing the feasibility of deploying this technology in resource-limited settings for medical image segmentation tasks.

SECTION: Method
SECTION: Architecture overview
The overall architecture of our network is illustrated in Fig.(a). The design consists of an encoder, decoder, bottleneck layer, and skip connections during the upsampling. Both the encoder and decoder are structured into four stages, and we employ EfficientFormerV2 blockas the basic unit. The input through initial feature extraction block(stem) and downsampling to the size tp. Then, the input is downsampled after the block in each stage of the encoder, with downsampling rates of. The encoder adopts the channel configuration shown in Fig.(a). Global feature fusion occurs in the encoder’s final stage, and the features are passed to the decoder. We designed a decoder that is symmetric to the encoder. The decoder features are combined with the encoder features through skip connections, restoring the image features, and then performing 2x upsampling in each stage. Finally, the upsampling module performsupsampling and outputs pixel-level predictions.

SECTION: Efficientformer block
Different from models that purely use Transformer and self-attention as encoders, our network adopts a hybrid approach, combining convolution with self-attention modules. In the high-resolution stages, where self-attention requires calculating interactions between all pixels, leading to significant computational overhead, our blocks employ the depthwise(DW) convolutionto construct its feed-forward network (FFN) to extract local features, as illustrated in Fig(b). Compared to standard convolution, DW convolution applies one filter per input channel, significantly reducing the computational complexity and enhancing the local features. The process can be described as:

where theis theth layer in thestage, And theis a learnable layer scale.

In the low-resolution stages, the computational burden of the self-attention mechanism is significantly reduced. Our blocks introduce the multi-head self-attention (MHSA) mechanism(Fig.(c)), which enhances the ability to capture global features and enriches multi-scale features in the encoder. In the decoder, the multi-head attention mechanism improves the accuracy of image reconstruction by aggregating global and local features. This approach effectively balances accuracy and computational efficiency, allowing the model to capture complex global dependencies without significantly increasing the computational load. This process can be described as follows:

The input features are projected through the mapping functionvia linear transformations to obtain the query (Q), key (K), and value (V) in the attention mechanism:

whereis the learnable attention bias for position encoding.

SECTION: Downsampling and Upsampling
During the downsampling process, convolution is similarly employed in the high-resolution stages for efficient downsampling and to reduce the size of feature maps. In the low-resolution stages, we adopt the self-attention mechanisms for downsampling, which adjusts the number of query tokens to effectively capture global dependencies and multi-scale features during downsampling. This approach balances the computational complexity of the self-attention mechanism due to the reduced resolution. We propose a symmetric design for the decoder, utilizing self-attention upsampling in the low-resolution stages by adjusting the number of query tokens. At the same time, convolution operations are used to upsample the high-resolution stages. While ensuring accurate image reconstruction, it reduces computational complexity. The self-attention downsampling/upsampling can be described as:

whereis the scaling factor, When, it represents downsampling, when= 2, it represents upsampling.

SECTION: Skip Connection
In U-shaped segmentation networks, skip connections play a crucial role by passing the features collected from the encoder to the decoder to help retain low-level features effectively. However, recent studies found some limitations of traditional skip connections. Simply concatenating the encoder and decoder features may introduce redundancy, and since skip connections primarily pass local features, they struggle to capture global dependencies in more complex segmentation tasks. However, some studies introduced attention mechanisms into skip connections and have achieved promising results. Based on the investigation and research, we introduced channel attentioninto skip connections. This method enhances feature fusion by applying attention to emphasize important feature channels and suppress redundancy. It also facilitates better global dependency modeling across different feature scales, all while introducing minimal additional computational overhead. The channel-based skip connection can be described as follows:

Here,is the current feature map from the previous layer, andis the skip connection feature map from the encoder during the downsampling process.

SECTION: Experiments And Results
consists of 30 cases, with a total of 3779 axial abdominal clinical CT images. The dataset is split into 18 training and 12 testing samples. Our method is evaluated on eight abdominal organs, including the aorta, gallbladder, spleen, left kidney, right kidney, liver, pancreas, and stomach.

contains 85 images for training, 80 for testing, and 30 images for training and 14 for testing, respectively. We perform 5-fold cross-validation on the GlaS and MoNuSeg datasets.

Our network is implemented based on Python 3.10 and PyTorch 2.0. Input images with size of 224x224, with a batch size of 32, and training is conducted on a single Nvidia A100 GPU. We employed the pre-trained weights of EfficientFormerV2 from ImageNet, adapting and loading them into both the encoder and decoder to initialize the model parameters as much as possible. Optimization is performed using the SGD optimizer with backpropagation.

The results are shown in Tablefor Synapse dataset, and Tablefor Glas and MoNuSeg datasets. Our model outperforms many popular current methods, achieving superior performance with 80.87%. Specifically, it surpasses the best performance in our comparison method by 0.33% in average DSC in the Synapse dataset. The results on the Glas and MoNuSeg datasets show that our method performs well on both datasets. Specifically, on the Glas dataset, our model achieved the best DSC of 92.44% and an IOU of 86.50%. On the MoNuSeg dataset, our model also achieved a DSC of 79.27% and an IOU of 65.87%, outperforming many popular comparison methods. We also obtain the visualization results for the Synapse dataset(Fig.(A)), Glas dataset(Fig.(B)), and MoNuSeg dataset(Fig.(C)) to illustrate the performance of our method. Most importantly, we conducted a comparison of computational complexity on the Synapse dataset; our method outperforms all compared approaches in terms of computational efficiency, and the computational complexity is only(Fig.).

SECTION: Conclusion And Discussion
In conclusion, we have developed a segmentation framework that achieves outstanding performance and offers superior computational efficiency. Our model surpasses numerous state-of-the-art methods in accuracy while maintaining a lower computational burden, making it particularly suited for limited computational resources devices, such as medical devices. These qualities emphasize the model’s suitability for performance-critical, real-world applications.

Despite the model’s outstanding performance and high efficiency, there are still some limitations regarding adaptability and practicality for medical devices. Medical devices have complex requirements, and while our framework performs well in experiments, further optimization is needed for broader deployment in embedded and portable systems. Looking ahead, our research can focus on fine-tuning the model for specific hardware implementations, thereby enhancing its applicability in real-world medical devices.

SECTION: COMPLIANCE WITH ETHICAL STANDARDS
This research study was conducted retrospectively using human subject data available in open access by Synapse, GlaSand MoNuSeg. Ethical approval was not required as confirmed by the license attached with the open-access data.

SECTION: ACKNOWLEDGMENTS
This work was supported by grants from the National Institutes of Health (R01EY032125 and R01DE030286) and the State of Arizona via the Arizona Alzheimer Consortium.

SECTION: References