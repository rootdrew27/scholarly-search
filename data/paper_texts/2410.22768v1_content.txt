SECTION: Emergence of Human-Like Attention in Self-Supervised Vision Transformers: an eye-tracking study
Many models of visual attention have been proposed so far. Traditional bottom-up models, like saliency models, fail to replicate human gaze patterns, and deep gaze prediction models lack biological plausibility due to their reliance on supervised learning. Vision Transformers (ViTs), with their self-attention mechanisms, offer a new approach but often produce dispersed attention patterns if trained with supervised learning. This study explores whether self-supervised DINO () training enables ViTs to develop attention mechanisms resembling human visual attention. Using video stimuli to capture human gaze dynamics, we found that DINO-trained ViTs closely mimic human attention patterns, while those trained with supervised learning deviate significantly. An analysis of self-attention heads revealed three distinct clusters: one focusing on foreground objects, one on entire objects, and one on the background. DINO-trained ViTs offer insight into how human overt attention and figure-ground separation develop in visual perception.

Kattentioneye-trackingvision transformerself-supervised learningDINOfigure ground separation

SECTION: Introduction
Biological visual systems use attention to efficiently extract information from the environment. Traditional models of bottom-up attention, such as saliency models, highlight salient points based on low-level visual features like edges, color, and luminance, using hand-crafted filters that mimic early visual processing. However, these models, such as the Graph-based Visual Saliency (GBVS) model, have limitations. For instance, GBVS failed to replicate human gaze patterns when viewing video clips.

Recently, convolutional neural networks (CNNs) have been used to predict human gaze using supervised learning on human gaze data. While these deep gaze prediction models perform well, they lack biological plausibility, as human attention is learned without explicit instruction. In biological systems, attention serves visual perception, meaning that attention models must be integrated with broader models of visual perception.

In this context, the vision transformer (ViT), introduced by Dosovitskiy, et al., is notable for its use of attention mechanisms in image classification. ViT divides an image into patches, which are treated as tokens analogous to those used in the original transformer model developed for natural language processing. In addition to the patch tokens, ViT uses a class token to aggregate information across patches via self-attention, which connects to a classification head. However, the supervised training of the original ViT reduces its biological plausibility, and it often produces noisy, unfocused attention.

Caron, et al.addressed this by training ViTs with the self-supervised DINO method (), which produced more focused attention confined within object boundaries, closely resembling human-like attention. However, this observation lacks quantitative validation.

In this study, we quantitatively compared human overt attention, measured through eye-tracking, with the attention generated by ViTs trained under two conditions: DINO and conventional supervised learning (Fig.). To enhance ecological validity, we used video stimuli, as human gaze behavior remains remarkably consistent across individuals during dynamic viewing. We show that DINO-trained ViTs closely match the overt attention of neurotypical adults, while ViTs trained using conventional methods diverge significantly. Additionally, an analysis of individual self-attention heads revealed three distinct clusters: one focusing on the “center” of foreground objects (e.g., a face), another on entire objects (e.g., bodies), and a third on background areas. We discuss these findings in relation to psychological and physiological attention theories.

SECTION: Results
SECTION: DINO ViTs exhibit attention like TD adults
We projected the temporo-spatial “gaze” patterns of 36 ViTs onto a two-dimensional MDS plane determined by landmark gaze profiles from human participants (Fig.a). These landmark gaze data, collected from 104 human participants in a previous study (N2010), were based on a 77-second video composed of 12 short clips featuring varying numbers of human characters. The same video was presented frame-by-frame to the ViTs—18 trained with DINO (cyan symbols) and 18 trained with supervised learning (SL; green symbols)—and the best head of each ViT was plotted on the MDS plane.

The origin of the MDS plane represents the median of all human gaze patterns. Adults with typical development (TD adults, red circles,) clustered near the origin, indicating high consistency in their gaze profiles, which serve as the standard for human gaze patterns. Notably, the DINO ViTs, particularly the 8- and 12-layer models (cyan squares and diamonds, six for each), were distributed near the origin, closely resembling the TD adults. In contrast, SL ViTs (green symbols) were distributed farther from the center, even more distant than adults with ASD (orange circles,), indicating a larger divergence from both TD adults and human participants in general. Additionally, the gaze profiles predicted by graph-based visual saliency (GBVS) models were located in the most distant region of the plane (gray inverted triangles), further underscoring the limitations of traditional models.

To gain a more comprehensive view of these temporo-spatial gaze profiles, we increased the MDS dimension from 2 to 32, positioning the top five heads of each ViT in the 32-dimensional MDS space. We then quantitatively compared the MDS distance, defined as the distance from the origin of this 32-dimensional space, across four human participant groups, six DINO and SL ViT groups, and the GBVS model (Fig.b). The MDS distances for TD adults were concentrated within the smallest range, serving as the benchmark against which other distributions were compared (horizontal shading in red, Fig.b). Remarkably, the 12-layer DINO ViT (DINO ViT-12) was the only group whose MDS distances were statistically comparable to those of TD adults (, Wilcoxon rank-sum test, after Bonferroni correction). All other groups exhibited significantly greater MDS distances ().

Two additional points merit attention. First, SL ViTs produced substantially larger MDS distances compared to DINO ViTs, despite both having identical transformer architectures (, in all 9 comparisons). This result suggests that DINO significantly outperforms conventional supervised learning in replicating human-like attention. Second, MDS distances decreased as the number of layers increased from 4 to 8 and 12.

These results are further illustrated in Fig.c, using two typical frames from the video. In clip No. 2, TD participants focused their gaze on the face of the woman on the left, while in clip No. 4, their gaze was directed to the face of the boy on the right. In contrast, participants with ASD displayed a broader distribution of attention. Similarly, the 12-layer DINO ViTs (DINO ViT-12) directed their attention to the same faces as TD participants, whereas SL ViTs-12 exhibited more dispersed attention, covering faces, bodies, and other areas of the scene.

SECTION: Gaze pattern similarity generalizes to drama but less to animation or nature films
To test generalizability of the above-mentioned MDS analysis, we applied the same approach to a different data set (CW2019, 63 healthy adult participants). The dataset contained 200 videos across three genres: “drama/other” (featuring human actors), “cartoon/animation” (featuring human-like characters) and “documentary/nature” films (featuring animals or natural scenes), as illustrated in Fig.a.

For the drama/other genre, which featured human actors as in N2010, the results were largely consistent with the findings with N2010. The adult participants formed a cluster in the center of the MDS plane, with DINO ViTs positioned close by, while SL ViTs and GBVSs were located farther from the center (Fig.b and c, left panels). However, discrepancies emerged between the attention patterns of human participants and DINO ViTs for the cartoon/animation genre. These discrepancies became even more pronounced in the documentary/nature genre without human characters. Conversely, the performance of GBVS predictions approached the human cluster when the documentary/nature films were used. This suggests that human attention may rely more on visual saliency cues when no human characters are present in the scene.

The extent of generalization was further quantified by calculating correlation between the MDS distances obtained from N2010 and those from each of the three genres in CW2019 (Fig.d). As expected, the correlation was highest for the drama/other genre (0.91–0.96), followed by cartoon/animation (0.88–0.95), and documentary/nature films (0.79–0.91). To conclude, findings with N2010 dataset generalized to those with CW2019 dataset as long as the video clips featured human characters.

SECTION: Gaze pattern similarity generally peaks before the final layers
We then examined how MDS distance changes across different layers of ViTs (Fig.a). Each ViT has six attention heads for the class token in each layer, and we analyzed six individual ViTs, giving us a total of 36 attention heads per layer. In DINO ViT-12, the median MDS distance (open circles) decreased from layer 1 to layers 8 and 9, then increased, forming a U-shaped curve. Additionally, the attention heads formed two clusters in layers 9 and 10, with most of the best heads (cyan diamonds) found in the smaller cluster.

These observations were generally applied to DINO ViT-4 and DINO ViT-8 as well (Fig.a). Similar patterns were found for SL ViTs, although the MDS distances in SL ViTs were significantly larger compared to their DINO ViTs counterparts (Supplementary Fig.). These observations remained largely unchanged when using the CW2019 dataset (Supplementary Fig.).

SECTION: Three distinct groups of attention heads emerge in DINO ViTs
To further investigate whether distinct clusters exist in the optimal layers of ViT-8 (layers 7 and 8) and ViT-12 (layers 9 and 10), we analyzed the cosine similarity of the attention maps across the 144 attention heads in these four layers (4 layers6 heads6 models). A hierarchical cluster analysis based on cosine similarity revealed that the distribution was best represented by three clusters (Fig.b), as determined by voting across 24 indices using NbClust. The G1 heads showed the smallest MDS distance, while the G2 heads formed an intermediate peak, and the G3 heads exhibited the largest peak (Fig.c). G1, G2, and G3 consisted of 24, 92, and 28 heads, respectively, out of the 144 heads (Fig.d).

SECTION: G1, G2, and G3 heads focus on the face, body, and background
We compared the attention patterns of the three groups of heads (G1, G2, and G3) on typical frames taken from five video clips (Fig.a). As expected from the smallest MDS distance, the G1 heads focused on human faces, or the faces of penguins, with a peak on the face of the primary character in each scene. In contrast, the G2 heads distributed attention over the bodies of the attended characters, forming sharp contours that segregated them from the background. By comparison, the G3 heads focused on the rest, namely the background. Notably, the G3 heads often highlighted vertical and oblique lines that delineated the borders of walls and ceilings (e.g., clip No. 2, MISSP_9a; G3).

Figureb compares the attention patterns on images of non-human animals sampled from ImageNet-1k dataset. G1 heads attended to the faces, while the G2 heads attended to the entire body, and the G3 heads focused on the background. This pattern held across various species, including mammals (tabby, macaque, Indian elephant, and sea lion), birds (indigo bunting), fish (goldfish), reptiles (Komodo dragon), and insects (rhinoceros beetle).

SECTION: G1 heads focus more on faces than TD adults and children
We further examined the duration of attention (viewing time) that each G1 head allocated to the eyes, mouth, or face (Fig.a), similar to the analyses conducted for human participants by Nakano et al.. The viewing time of the eyes by the G1 heads (median 42%) was comparable to that observed in TD adults (median 43%) but exceeded that of TD children (median 26%). In contrast, attention to the mouth of the G1 heads (median 32%) was two to three times higher than that in both TD adults (median 12%) and children (median 19%). Consequently, G1 heads devoted approximately 90% of their viewing time to the face (median 92%), which was significantly greater than those observed in TD adults (median 65%) and children (median 66%). These patterns are illustrated in a frame from clip No. 11 (Fig.b). The G1-heads preferred to view faces of non-human animals as well (median 85%, Fig.c).

SECTION: G1 heads partially replicated gaze switching between human faces
In our previous studies, we observed that typically developing (TD) participants switched their gazes between characters in a remarkably synchronized manner. Figured presents the most thoroughly analyzed example from clip No. 4, where two boys are engaged in conversation. During this 6-second clip, the gazes of TD adults were initially centered around the boy on the left, who was closer to the center of the monitor (1). However, they quickly shifted their focus to the boy on the right at approximately 1 second (2). The gaze then switched back to the boy on the left around 2 seconds (3, 4), before returning to the boy on the right at 2.3 seconds for about 2.5 seconds (5), and finally settling back on the left boy at 5.6 seconds and remaining there (6). The graphs in Fig.e display the face-viewing proportions for the two boys (red: right boy, blue: left boy), which alternated clearly, marking four distinct transitions (indicated by vertical dashed lines). By subtracting the viewing proportion of the right boy from that of the left boy, a single temporal profile was generated, crossing the time axis four times and delineating five distinct periods of dominant face viewing (Fig.f).

The G1 heads of DINO ViTs failed to replicate these distinct transitions, instead consistently showing dominant attention to the boy on the right (Fig.e and f, bottom row). According to Suda and Kitazawa, the switching process is well explained by a model that includes the size of each face, head motion, and mouth motion during vocalization. In this case, the face of the boy on the right, who was shown in a frontal view, appeared larger than the profile view of the boy on the left. Because motion signals from the face and mouth—critical cues for turn-taking—were not available in the current application of ViTs, the G1 heads were unable to reproduce the distinct gaze-switching patterns. However, there was a significant correlation between the single temporal profiles of TD adults and the G1 heads of DINO ViTs, with a coefficient of 0.425 (Fig.f,, comparison between means of each group). Since the size of the faces remained almost constant throughout the 6 seconds, it suggests that the G1 heads of DINO ViTs may have detected some subtle cue, other than size, that guides our attention to a face.

SECTION: G1 heads also responded to visual saliency
The results mentioned above demonstrate that the G1 heads exhibited a strong preference for faces. This raises the question of whether the G1 heads were merely face detectors. To address this, we also tested whether the attention of the G1 heads exhibited “pop-out” properties, meaning the ability to focus on an exceptional region (called singletons) based on physical properties (first-order features) such as orientation, color, size, and shape (Fig.). The G1 heads were highly sensitive to visual saliency, characterized by color (first row), size (second row), shape (third row), and orientation (fourth row), performing as well as the GBVS model. Notably, the G1 heads were able to detect a pattern-defined object (a square distinguished from the background by the orientation of the hatching) involving a second-order feature, which the GBVS model failed to detect (fifth row). The attention of the G2 heads also displayed pop-out properties similar to those observed in the G1 heads. However, the G3 heads focused more on the background, though they also exhibited pop-out properties for first-order features.

SECTION: Discussion
We have revealed that self-attention from the class token of ViT exhibits a temporo-spatial pattern remarkably similar to that of human adults, especially when trained with DINO, but not when trained with conventional supervised learning with labels. Notably, a classical model based on feature-based attention, often referred to as a saliency model, yielded attention much less similar to human attention. These results indicate that not only the architecture of ViT but also the learning algorithm is essential for achieving human-like attention.

We also found that self-attention heads can be divided into three groups (G1, G2, and G3), each showing a preference for faces, bodies, and backgrounds, respectively. The G1 heads showed strong attention to human and animal faces but were also able to attend to singletons of artificial images with both first- and second-order features. This is remarkable because previous research, which used SL ViTs, has claimed that the attention mechanisms of ViTs do not exhibit attentional properties such as pop-out. These results suggest that ViTs trained with DINO self-acquired attention comparable to the multifaceted attentional processes of humans.

SECTION: Is self-attention of ViTs biologically plausible?
The biological plausibility of self-attention in ViTs, and in transformers in general, is often questioned. One concern is whether the brain’s visual areas have long-range connections comparable to the unrestricted range of self-attention in ViTs. The answer is yes—the brain has long-range connections within visual areas, allowing neurons to be influenced by both nearby and distant stimuli

Another criticism is that transformers lack the top-down, or feedback, connections that exist in human brain networks, which play an essential role in generating top-down attention. Transformers may seem to lack recurrent connections, but ViTs are equipped with the class token, which aggregates information from all the input tokens. The class token sends queries to these tokens, and those queries are correlated with the keys in the input tokens to form attention weights. The weighted sum of values is then sent back to the class token. The biological plausibility of dot products between queries, keys, and values has also been questioned, but several researchers have proposed ways to achieve these by using biologically plausible components. Thus, the entire process is, in principle, comparable to widespread backward projections in actual neural networks and forward projections that integrate information from lower layers to higher layers. From this, we argue that top-down connections do exist in ViT, from the class token to all the others, capturing the essence of top-down attention.

It could still be argued that the class token does not send signals back across multiple layers to the input layer. However, the class token’s attention in a single layer still resembles human attention. This raises the question of how far back human top-down attention extends in the brain. Given the frequent and rapid shifts in retinal images due to saccades, it seems unlikely that top-down signals must return to the primary visual cortex to finalize attention. Our finding that class token attention in layers 9 and 10 of DINO ViT-12 closely mirrors human attention (Fig.a) suggests that a more localized top-down process within higher cortical layers may be sufficient to explain top-down attention.

SECTION: Why do DINO ViTs exhibit human-like attention?
The question arises: why do DINO ViTs exhibit attention patterns much more similar to human attention than SL ViTs? We speculate that two key aspects of DINO’s self-supervised learning method reflect a learning process akin to that of a newborn.

DINO’s loss function,, is the sum of the Kullback-Leibler (KL) divergence between the probability distribution produced by the teacher ViT,, and that by the student ViT,, and the entropy of:

First, DINO learning minimizes the KL divergence between the student and the teacher. The student ViT observes many different portions of an original image, similar to how the human eye makes saccades, while the teacher ViT observes a larger portion of the image. This process leads to view-invariant representation learning, promoting visual stability across saccades.

Second, the DINO minimizes the entropy of the teacher ViT’s output,, while its “centering” procedure flattens the mean of, effectively maximizing the entropy of its prior distribution. Together, these procedures allow DINO to maximize Shannon information the difference between the entropy of the prior and posterior distributions, before and after observing a new image.

We might call this the “DINO information maximization principle,” though it should not be confused with conventional mutual information maximization like InfoMaxor DeepInfoMax. Traditional InfoMax maximizes mutual information between input data and its neural network representation, requiring both input and output distributions. DINO, on the other hand, uniquely focuses only on the output distribution.

It could be argued that the conventional SL method also maximizes information about predefined labels from an image. However, a fundamental difference exists between DINO and SL: SL deals with a fixed set of 1,000 labels, leaving no flexibility for the ViT to improve or adapt those labels, even if they lack optimal informativeness. DINO, by contrast, operates with an output dimension of up to 65,536, allowing the ViT to autonomously organize this vast space to optimally represent the complexity of the world.

Now, consider which process is more akin to a newborn’s learning experience. A newborn knows little about how the visual world is organized, let alone the classifications defined by human experts. Maximizing information through observation, as DINO does, seems more ecological for a baby adapting to an unpredictable environment. Indeed, the mammalian visual system has sensitive periods during which experiences shape sensory filters in primary sensory cortices.

Although DINO uses non-physiological components—like the student-teacher system—it is tempting to speculate that the brain may have evolved a more efficient, autonomous mechanism for maximizing information without relying on such structures. The remarkable similarity between DINO ViTs’ attention and human attention suggests that DINO’s information maximization could reflect a fundamental principle of learning in a novice brain.

SECTION: How do G1 heads learn to focus on a face?
A striking feature of G1 heads is their keen interest in focusing on faces within a scene. Their viewing proportion of faces was as high as 90%, surpassing the 60-70% observed in TD adults and children (Fig.). Suda and Kitazawademonstrated that a gaze scanpath, generated by randomly selecting one of multiple faces in a scene, can significantly differ from the actual scanpaths generated by TD adults. The fact that the scanpaths of G1 heads were comparable to those of TD adults suggests that G1 heads chose the same primary face in the scene as TD adults. How could the G1 heads identify an appropriate face without being taught what a face is?

To address this issue, we examined how many images in the ImageNet-1k dataset, used for training DINO ViTs, contained a human face or a human. Of 1,000 images randomly drawn from the dataset, 159 contained some parts of humans or human-like figures (e.g., images shown with red rectangles in Supplementary Fig.). There was a wide variety in the size and numbers of humans in these pictures, but more than half (98/159) contained at least a recognizable face. In addition, the dataset featured a wide variety of animals (388 out of 1,000) with recognizable heads and faces. Taken together, nearly half of the dataset images contained a face, whether human or non-human animal. Consider obtaining information by discriminating between the presence and absence of elements in an image. The maximal information of one bit is obtained when an object appears in an image with a probability of 1/2. We speculate that DINO ViTs autonomously developed a face detector because faces appeared with an approximate probability of 1/2 in their virtual world of ImageNet. This speculation is supported by the fact that the G1 heads paid attention to both human and non-human animal faces (Figs.and).

SECTION: Implications of G1, G2, and G3 attention heads in human visual perception
It was unexpected that self-attention heads in the critical layers of DINO ViTs clustered into three groups rather than two, given that conventional literature in visual psychology often emphasizes dichotomies, such as figure-ground segregation. In this context, G2 aligns with figures and G3 with the ground. G2 attention is evenly distributed over human and animal bodies or main objects, maintaining precise boundaries (e.g., Fig.). This boundary precision suggests that DINO ViTs may address the border ownership problem, typically managed by lower visual areas like V2. Additionally, G2, comprising 60% of the critical attention heads (Fig.d), should be useful for “semantic segmentation”, which assigns a label to each segmented area.

The role and neural basis of G3 also merit consideration. A recent study has indicated that many neurons in the precuneus—a major hub of the neural network—encode the retinotopic location of a large background, closely paralleling the role of G3 heads. Furthermore, significant numbers of the precuneus neurons encode both retinotopic information of a dot (figure) and allocentric information of the figure relative to the background. Taken together, these precuneus neurons seem to include both G2- and G3-like functions, representing stabilized allocentric information that remains view-invariant across saccades.

If G2 and G3 attention heads explain most findings in figure-ground segregation, what role do G1 heads play? In typical psychological experiments with only one main figure, G2 and G3 heads may suffice. However, natural scenes often contain multiple figures, and the visual attention system continuously selects one as the focal point. G1 heads likely serve this purpose by identifying the “center” within figures represented by G2 heads. Neural correlates of G2 heads, if they exist, may act as a “reservoir” of figures from which G1 heads select the central focus.

Several research studies has shown general correspondence between layers of convolutional artificial neural networks and the hierarchies of visual cortical areas. Consequently, neural correlates of G1, G2, and G3 heads, found in layers 9 and 10 of DINO ViT-12, are likely to be found at high levels in the visual hierarchy from the retina to the hippocampus as illustrated by Felleman and Van Essen. Future research will examine these neural correlates in the human brain by comparing the effects of selective ablation or inhibition in both DINO ViTs and biological networks.

SECTION: Methods
SECTION: Video stimuli and eye tracking data
The video stimuli and human eye tracking data used in this study were derived from Nakano, et al., N2010, and Costela and Woods, CW2019. The N2010 data were obtained from 104 participants (27 adults and 25 children with typical development, and 27 adults and 25 children with Autism Spectral Disorder, ASD) while they were viewing a 77-second-long video clip, consisting of 12 short video clips taken from TV programs and movies, each lasting for 5-6 s and featuring one or more human characters. Gaze positions were measured with the Tobii X50 system (Tobii Technology AB), with a sampling frequency of 50 Hz.

We used a part of the CW2019 data (), obtained from 63 adult participants while they were viewing “Hollywood” video clips, each lasting for approximately 30 seconds, using the EyeLink 1000 system (SR Research Ltd.), with a sampling frequency of 1,000 Hz. The dataset contained 200 videos across three genres: “drama/other” featuring human actors (), “cartoon/animation” featuring human-like characters (), and “documentary/nature” films featuring animals or natural scenes (). It should be mentioned that each participant viewed approximately 40 of the 200 video clips.

SECTION: Vision Transformers (ViTs)
The architecture of ViTs used in this study was based on the model called “DeiT-S”or “ViT-S/16”which has the following parameters: patch sizepixels, embedding dimensions, number of heads, and 12 transformer layers. Additionally, we developed two smaller ViT models withorfor comparison.

A ViT processes an image first partitioning it intopatches, each of size. Each patch is then linearly transformed into a-dimensional embedding vector (in this case, 384-dimensional), referred to as a patch token. This transformation is performed using 384 learnable linear filters. After training, many of these filters resemble Gabor-like filters, reminiscent of the receptive fields of V1 neurons. Position information is then added to each patch token. Notably, ViTs include a class token (), which is appended to the beginning of thepatch tokens (Fig.). This token aggregates information across the entire set ofpatches.

The resulting input is an-dimensional matrix, where each row is fed into each ofmodules of the first layer of the transformer encoder (Fig.). Each ofmodules of the transformer layer has“attention heads”, each of which combines information across thetokens in parallel: the matrixis divided intosub-matrices, whereand.
Each attention headmixes information across thetokens inusing self-attention mechanisms as follows:

whereare the query, key and value matrices, respectively, andare the learnable linear transformation matrices. Equation () shows that the-th row of, referred to as the attention weight matrix, defines a set of attention weights for the headin the-th token. These weights determine how each column of the value matrix (of dimension) is weighted and averaged. Thus, the first row of the attention weight matrix gives the attention weights for the class token.

The self-attention map of the class token in the-th head,, is given by:

whereis the first row of, andmeans taking the second to-th elements of the vector. This produces an attention map over thepatch tokens, which is then reshaped into two dimensions with an aspect ratio that matches the input image. Henceforth, when we refer to an attention map, we specifically mean the attention map of the class token over thepatch tokens.
In the first layer, the outputs from the six attention heads are concatenated and passed through additional processing using a multilayer perceptron (MLP) and skip connections, resulting in an-dimensional matrix. The output matrix of the first layer is fed into the second layer. This process is repeated across all layers up to the final-th layer. The final output is a-dimensional vector derived from the class token, which is then passed through an MLP, known as the projection head, to generate the final output. For supervised learning, the output dimensionality was, whereas for DINO-based learning it was.

We resized video frames or images so that their dimensions (length and width) were multiples offor input into ViTs. The resized video frames were presented independently to the trained ViTs. Attention maps obtained from the trained ViTs were up-sampled to match the size of the input image using nearest-neighbor interpolation and then smoothed using a box blur with a kernel size of. Following the winner-take-all principle, we deterministically defined the peak positions of the attention map as the gaze positions of each attentional head (Fig.). For comparison purposes, these gaze positions were then mapped to the coordinates of the original image.

We trained a total of 36 ViTs, with six models for each structure (, and 12), using both supervised learning and self-supervised learning (DINO). For both training approaches, we used the training set from the ImageNet-1k (ILSVRC2012) dataset, consisting of 1,281,167 images labelled across 1,000 classes.

In the supervised learning setup, we trained the ViTs to minimize the cross-entropy loss between the output of the projection heads and the 1000 labels. We used the training code from Touvron, et al., available at:. The models were trained using default parameters, without distillation.

For the DINO training, we used the code from Caron, et al., available at:. We trained the model for 300 epochs (the same as in supervised learning) using default hyperparameters.

DINO training, introduced by Caron, et al., is unique in that it does not rely on pre-defined labels. Instead, the ViT learns to discover “optimal classifications” of visual images, leveraging a large output dimension of 65,536. In DINO, two ViTs with identical architectures are employed: one acting as the student and the other as the teacher. Both models output a probability distribution over the 65,536 bins, denoted(student), and(teacher). The loss function,, is the cross-entropy betweenand, which can be decomposed into the entropy ofand the Kullback-Leibler (KL) divergence betweenand:

Three key mechanisms are crucial in DINO learning. First, the input images are augmented by cropping smaller regions (local views) and larger regions (global views). All the crops are shown to the student, while only the global views are shown to the teacher. Minimizing the KL divergence betweenandencourages the model to learn view-invariant representations.

Second, a centering procedure is applied to the teacher’s output to prevent model collapse, where the teacher would otherwise produce the same output for all inputs. This centering ensures that the model uses the full 65,556 dimensions for classification. By assuming a uniform prior distribution across the bins, minimizing the entropy term maximizes the Shannon information, defined as the difference between the entropy before and observing a new image.

Third, the update rules of the parameters differ between the student and teacher models. The student’s parameters are updated directly by minimizing the loss, while the teacher’s parameters are updated with the exponential moving average (EMA) of the student’s parameters. This EMA approach introduces a model ensemble effect, enhancing stability and gradually leading to convergence of both models’ parameters.

In summary, DINO training encourages the ViT to discover optimal classifications, maximizing the information gained from observing new images.

In addition to the 36 ViTs we trained ourselves, we used publicly available two pre-trained 12-layer models, one trained with supervised learning (DeiT-S) and the other with DINO (ViT-S/16). We refer to these as “official models”. The trained weights of the official models are available from the links mentioned above.

The ViT models were trained on a workstation equipped with an AMD EPYC 7513 CPU, 512 GB of RAM, and eight NVIDIA A40 GPUs. The software used for training was Python 3.8.8 with PyTorch 1.8.1.

SECTION: Graph-Based Visual Saliency model
A Graph-Based Visual Saliency (GBVS) modelwas used as a control. This model includes six feature channels—color, intensity, orientation, contrast, flicker and motion—to capture various aspects of saliency. For the color channel, we used DKL (Derrington-Krauskopf-Lennie) color space, which aligns with human visual perception. Gaze positions in the GBVS were defined by the peak saliency values in the attention maps, similar to the method used for ViTs.

SECTION: MDS analysis for gaze positions
To quantify differences and similarities in the temporal pattern of gaze movements among human participants, ViTs, and GBVS models, we computed pairwise distances between gaze positions at each time point across the video clips. Prior to calculation, gaze position sampling rates were adjusted to 50 Hz for the N2010 data and 720 Hz for the CW2019 data using nearest-neighbor interpolation.

We defined the distancebetween two time series of gaze positions,from the-th and the-th participant (or model) as follows:

where,denotes the Euclidean norm of a vector, andis a function that calculates the median while ignoring missing values (NaNs).

For the CW2019 data, distanceswere computed separately for each video clip and normalized by the median distance, as each participant viewed a different set of clips. The resulting distance matrices were then averaged across clips. Missing values inwere interpolated using the following formula:

First, we applied metric multidimensional scaling (MDS) on the distance matrix between human participants to establish a standard MDS space where the origin represents the general gaze profile of human participants in general. Data from ViTs and GBVS models were then plotted relative to these landmarks using the following formula:

whereis the set of landmark data (human participants),is the embedding vector of the-th participant in the MDS space, andis the embedding vector of the-th model. We performed MDS analysis and nonlinear optimization of equation () using MATLAB R2023a (MathWorks, Natick, MA, USA), specifically with the functions “mdscale” and “lsqnonlin” with “MultiStart”.

To quantify deviation from standard human gaze behavior, we defined the MDS distance, representing the distance from the median of the human data. While we used two-dimensional MDS for visually representing similarities and differences in gaze data (e.g., Fig.a), we increased the dimensionality to 32 for formal MDS distance calculations to minimize underestimation of distance. Our analysis indicated that using two dimensions could lead to an underestimation of up to 9%, whereas a 32-dimensional space reduced this error to less than 1%.

SECTION: Hierarchical clustering of self-attention heads
We applied hierarchical clustering to classify self-attention heads based on the attention maps they generated. The distance matrix was calculated by averaging the cosine distances (one minus cosine similarities) of the attention maps across the N2010 video frames. This matrix was then used for hierarchical clustering. To determine the optimal number of clusters, we used 24 clustering indices provided by the NbClust package (ver. 3.0.1)in R 4.3.2. To calculate these cluster indices, we also used a matrix of attention maps for N2010 video frames concatenated by each self-attention head and compressed into 128 dimensions with principal component analysis (PCA).

SECTION: Viewing proportion analysis
We calculated the viewing proportion, a metric raging between 0 and 1, indicating whether a human or an artificial model is looking at a specific target, such as the eyes, mouth, or face. The viewing proportion for the-th target at time point, denoted by, is given by:

whereis a gaze position,is the position of the-th target, andis the standard deviation parameter of the Gaussian function, set to 30 pixels. When the-th target is absent,is set to zero.
The average viewing proportion ofover time, calculated by summingand then dividing by the total time the-th key point was present, indicates the proportion of time spent viewing the-th target. We did not compute the viewing proportions of self-attention heads in ViTs directly from the attention map, to ensure consistency in analytical methods between data from human participants and models.

For human face-viewing proportions, we used clips No. 3, 7, and 11 from the N2010 data (a total of 571 frames), each manually annotated with target coordinates (eyes, mouth, nose, ears, and hands) for every frame. The face-viewing proportion was defined as the sum of viewing proportions for the eyes, mouth, nose, and ears.
For non-human animal face-viewing proportions, we used the Animal Parts Datasetcontaining eye and foot annotations across 14,711 vertebrate images from the Imagenet-1k dataset. We selected 875 images based on the following criteria: images were from the Imagenet-1k validation set (not used in model training), had visible faces, contained no humans or multiple faces, and depicted biological (non-artificial) objects. We manually annotated mouth positions for these images; for animals with elongated beaks or mouths (e.g., birds, crocodiles) the mouth position was set approximately at the center. For animal faces, the face-viewing proportion was defined as the sum of viewing proportions for the eyes and the mouth. To obtain the gaze positions of the ViTs, the dataset images were cropped and resized to a resolution of 256256 pixels.

SECTION: Data availability
The video stimuli and eye-tracking data from N2010are available upon request. The data from CW2019are openly available ().

SECTION: Code availability
The code is publicly available at:.

SECTION: Acknowledgments
This work was supported by KAKENHI 23K17462, 21H04896, and 18H05522 from the Japan Society for the Promotion of Science (JSPS) to SK. Portions of Figure 1 were created using BioRender (). Language checking was conducted using ChatGPT-4.

SECTION: Author contributions
T.Y., H.A. and S.K. designed the study. T.Y. and H.A. conducted model training and data analysis. T.Y. and S.K. wrote the manuscript. All authors reviewed the manuscript.

SECTION: Competing interests
The authors declare no competing interests.

SECTION: References
SECTION: Supplementary material
SECTION: Supplementary figures