SECTION: Automated Medical Report Generation for ECG Data: Bridging Medical Text and Signal Processing with Deep Learning

Recent advances in deep learning and natural language generation have significantly improved image captioning, enabling automated, human-like descriptions for visual content. In this work, we apply these captioning techniques to generate clinician-like interpretations of ECG data. This study leverages existing ECG datasets accompanied by free-text reports authored by healthcare professionals (HCPs) as training data. These reports, while often inconsistent, provide a valuable foundation for automated learning. We introduce an encoder-decoder-based method that uses these reports to train models to generate detailed descriptions of ECG episodes. This represents a significant advancement in ECG analysis automation, with potential applications in zero-shot classification and automated clinical decision support.

The model is tested on various datasets, including both 1- and 12-lead ECGs. It significantly outperforms the state-of-the-art reference model by Qiu et al., achieving a METEOR score of 55.53% compared to 24.51% achieved by the reference model.
Furthermore, several key design choices are discussed, providing a comprehensive overview of current challenges and innovations in this domain.

The source codes for this research are publicly available in our Git repository111https://git.zib.de/ableich/ecg-comment-generation-public.

Attention mechanisms, Biomedical text generation, Clinical decision support, Deep learning, Electrocardiography (ECG), Encoder-decoder architectures, Image captioning adaptation, Long Short-Term Memory (LSTM), Medical datasets, Medical signal processing, Natural language processing (NLP), Neural networks, Transformers.

SECTION: 1Introduction

Cardiovascular diseases remain a leading cause of morbidity and mortality worldwide, highlighting the importance of effective and timely diagnostic methods. Traditional ECG analysis, while indispensable, often requires manual interpretation by trained physicians, which can be time-consuming and subject to human error. At the same time, advances in AI-driven text generation algorithms present a novel opportunity to take advantage of existing ECG data sets for which physician-written free-text comments are available. These models have the potential to generate informative text about ECG episodes, which could offer decision support to clinicians during diagnostic tasks and thus reduce cognitive load on medical professionals and potentially improve the speed and accuracy of diagnosis.

Significant progress in encoder-decoder architectures, particularly in image captioning, has demonstrated their capacity for generating coherent descriptions[1,2]. While these methods have succeeded in domains such as image processing, their application to medical data, specifically electrocardiography (ECG), remains underexplored[3]. One major challenge is the scarcity of purpose-built, high-quality labeled datasets for ECG analysis, as generating such datasets requires the involvement of highly trained individuals, making the process resource-intensive[4]. However, a promising alternative lies in leveraging existing ECG records paired with free-text comments, which are often generated as a byproduct of routine clinical practices[5]. Although these free-text comments were not originally created for training machine learning models, they represent a rich source of data that can be adapted to train encoder-decoder architectures, capable of generating concise textual summaries of ECG episodes, as demonstrated in this work.

This study introduces an innovative approach to text generation designed for ECG analysis, validated on the publicly available PTB-XL dataset[6]. In addition to achieving state-of-the-art performance, a key contribution of this work is the establishment of a reproducible benchmark for future studies, as our work is, to the best of our knowledge, first of its kind to be tested on the official test subset of the PTB-XL dataset, ensuring both transparency and comparability.
Furthermore, we present a case-study on a more challenging and larger dataset of single-lead, variate, and subcutaneous ECG (sECG) signals obtained from implantable cardiac monitors (ICMs)[4]. Unlike purpose-built datasets, this dataset comprises labels and free-text reports generated as a byproduct of routine clinical and administrative processes. These reports, often created to fulfill procedural requirements, have not been cross-checked for accuracy, introducing additional noise and complexity that make the dataset more difficult to leverage for automated learning. This case study highlights the robustness and adaptability of our method to diverse and less curated real-world data sources.

The method introduced in this study employs an encoder-decoder architecture, utilizing a ResNet-based encoder[7]paired with either an LSTM[8]or Transformer[9]decoder.

In the following sections, we provide an overview of related work to contextualize this study within the broader scientific landscape and facilitate comparison with existing methods. We then present a comprehensive description of our proposed model, detailing its architecture, tokenization techniques, data preprocessing procedures, and the hyperparameter optimization and training strategies employed. This is followed by a description of the datasets used, along with the specific preprocessing steps applied. Next, we outline the experiments conducted to evaluate the performance of our model and analyze the results obtained. Finally, we discuss the key findings and suggest directions for future research.

SECTION: 2Related Work

Various methods have been proposed in recent years for the automatic diagnosis of ECG data (e.g.,[10,11,12]) and other feature-based heart data (e.g.,[13]). Stracina et al.[14]provide a comprehensive review of advancements and possibilities in ECG analysis since the first successful recording of the electrical activity of the human heart in 1887, including recent developments leveraging deep learning techniques.

A key takeaway from these reviews is that the majority of existing ECG analysis methods focus primarily on the classification of ECG episodes, such as identifying atrial fibrillation (AF) or other abnormal heart rhythms. In contrast, the field of automated report generation for ECG data remains largely unexplored. Meanwhile, significant advancements in the task of generating textual descriptions from visual data (commonly referred to as image captioning) have been achieved through the development of sophisticated machine learning models and language models in particular. Seminal works in this domain include works by Vinyals et al.[1], and subsequently Xu et al.[2], which introduced an encoder-decoder architecture utilizing ResNet[7]as the encoder and a Long Short-Term Memory (LSTM) network[8]as the decoder to successfully generate descriptive text for images. This framework has since been extended to various domains, including medical signal processing. Pang et al.[3]conducted a survey of recent technologies in medical report generation, primarily addressing medical imaging, such as X-ray screenings, demonstrating the versatility and potential of such architectures. However this work offers limited insight into one-dimensional signals like ECG or EEG. This highlights a critical gap in the application of LLMs to medical data, emphasizing the need for further exploration in this area.

Given the substantial progress in automated data captioning and the relative scarcity of work in applying these techniques to ECG data, this area represents a significant research opportunity. Building on the foundational work of Vinyals et al. and Xu et al., this study explores the application of similar encoder-decoder architectures to the generation of free-text descriptions for ECG episodes, aiming to bridge this gap in the field.

One notable effort in this direction is the work by Qiu et al.[15], who investigated the potential of pre-trained large language models (LLMs) for ECG classification. They demonstrated that knowledge from natural language processing could be effectively transferred to ECG data, enabling the detection of cardiovascular diseases. Their approach involved fine-tuning pre-trained LLMs on the publicly available PTB-XL dataset[6], achieving highly encouraging performance. However, their method faced certain limitations, such as random data splitting, which can lead to overfitting, as evident by the minimal performance gains after incorporating the ECG data itself into the model (refer to Section6.4for further details). Since their work was not tested on the PTB-XL official splits and no other studies have reported results on publicly available datasets, we used their method as a reference model for performance comparison of our model (refer to Section5.2).

Bartels et al.[16]explored a method using a private dataset of ECG records with captions, partially machine encoded and corrected by healthcare professionals (HCPs), in addition to those generated by HCPs during routine medical practices. Their method used a pre-trained ECG classification ResNet (ECGNet[17]) with an additional classification layer to embed signals and feed the embedded signals into an LSTM or a Transformer. However, their approach did not include testing on publicly available datasets, which limits the generalizability of their model evaluations. Furthermore, they did not explore using non-pre-trained models or single-lead ECG signals, and their proposed model architecture lacked detailed explanation.

In conclusion, while caption generation for images has achieved satisfactory levels, the task remains complex for medical images and even more so for ECG data. Current advances in applying LLMs to ECG analysis are promising, but a proven approach to successfully evaluate public datasets with trustworthy evaluation metrics is still being investigated. Our work aims to address this gap by leveraging free-text data and exploring the potential of modern machine learning techniques in the challenging domain of ECG signal interpretation.

SECTION: 3A New Method for ECG Caption Generation: Architecture and Training Optimization

This section introduces a novel approach for generating text descriptions of ECG data through encoder-decoder architectures, adapted from successful image captioning frameworks[1,2]. The method leverages a ResNet-based encoder for embedding ECG signals and incorporates Transformer or LSTM decoders to produce descriptive, clinically relevant reports.

The entire encoder-decoder architecture is visualized in Fig.2.

SECTION: 3.1Encoder for ECG Embedding

To create an effective embedding for ECG signals, we employed a modified 34-layer ResNet architecture tailored for 1D inputs. The standard classification layer was removed and replaced with an average pooling layer, allowing for adjustable output sizes to preserve temporal information across 512 output channels. This adaptation enables the retention of temporal relationships, which together with attention mechanism in the decoder, can be utilized for ECG interpretation. The output embedding from this encoder forms the input to the decoding stage.

SECTION: 3.2Decoder Architectures for Generating ECG Description

The decoder’s task is to generate meaningful, contextual descriptions from the ECG embeddings. We tested two architectures for this purpose: a Transformer-based decoder and an LSTM-based decoder, described in detail below.

The Transformer decoder[9]receives a combined input of ECG embeddings and tokenized report sequences, leveraging self-attention to dynamically focus on different parts of the ECG signal during token prediction. The concatenation is formalized as follows:

wheredenotes the embedding of the-th token in the report, andrepresents the features of the-th ECG segment provided by the encoder.is set by the user and determines the extent of downsampling of the ECG signal.

Using the combined input, the transformer calculates the context vectorat each time step:

Whererepresents each element inwhich can be either an ECG-segment feature vectoror a token embedding. The attention weightsare calculated following the standard Transformer attention mechanism described in[9].

The LSTM decoder[8]generates one word at a time, conditioned on a context vector, the previous LSTM hidden state, and the last predicted token (or ground truth token during training). The attention mechanism dynamically weights different segments of the encoded ECG signal, creating a context vectorat each time step:

whereis the attention weight at time stepfor temporal segment, defined by:

Here,,andare learnable weights within linear layers, andis the previous hidden state.

Ultimately, both the transformer and LSTM based models are trained using cross-entropy loss.

Attention:In the realm of natural language processing (NLP), attention mechanisms have been shown to enhance the recognition of objects in images, as demonstrated by Ba et al.[18]. These mechanisms, which focus on relevant parts of the data, are hypothesized to enable the model to dynamically focus on relevant portions of the ECG signal and thus improve the quality of generated descriptions.

While self-attention is natively integrated into the Transformer’s structure and can be adjusted to attend to different segments of the ECG (refer to equation2), it is externally added in our LSTM-based model. In this case, in each prediction stepwe use anvector, sized to match the size of each channel output by the encoder (240 in our best-performing model, which represents temporal encoding of the ECG signal), and perform weighted aggregation of it. This results in a single weighted value for each encoder output channel, conditioned on the networks previous state. Refer to equation4for details about the calculation of. By applying weighted aggregation to the channels of the downsampled ECG signal using weights that account for both the ECG signal and the current network state, the model is able to attend to different parts of the ECG when predicting each word in the generated report. An example to this attention driven prediction process is visualized in Fig.1. Finally, as in[2], doubly stochastic attention regularization is applied to prevent overfitting. This encourages the model to distribute its attention more uniformly, avoiding a tendency to over-focus on specific time steps and ensuring diverse attention across the signal.

SECTION: 3.3Training Optimization and Enhancement Techniques

Our training pipeline incorporates several optimization strategies to improve model performance and ensure reliable generation of ECG descriptions. Key techniques include encoder pre-training and targeted hyperparameter tuning.

Encoder Pre-Training:In one configuration, we pre-trained the encoder using rhythm class labels as a preliminary task to strengthen ECG embeddings before fine-tuning on free-text reports. This pre-training phase was intended to leverage rhythm labels for additional contextual learning; Moreover, as described in Section4.1the ICM dataset includes approximately 738K episodes with rhythm class labels but no accompanying reports, enabling us to tap into this large dataset, which would otherwise not be utilized. Pre-training resulted in an F-score of 0.69 on the PTB-XL dataset (6 classes) and 0.49 on the ICM dataset (11 classes).

Hyperparameters Tuning:Extensive hyperparameter tuning was conducted to optimize model performance and stability. Key parameters were adjusted as follows:

Encoder architecture: We tested both ResNet-18 and ResNet-34, aiming to optimize both model performance and computational efficiency.

Decoder architecture: We varied the layer depth for both LSTM and Transformer decoders, from a single layer up to 20 layers.

Learning rates: Separate learning rates were optimized for the encoder and decoder.

Stopping Criteria: We explored various early stopping criteria such as cross-entropy loss (which was also used for back-propagation), BLEU-1, BLEU-4 and METEOR scores.

Additional Parameters:We also tuned various parameters, including embedding sizes, batch sizes, teacher-forcing probabilities[19], normalization techniques (at batch, dataset, and episode levels), and top-k sampling. These adjustments contributed to enhancing model robustness and performance consistency.

This comprehensive tuning process involved multiple repetitions of experiments to reduce the impact of random variations, resulting in a stable and reliable evaluation of model performance. The best-performing parameter configurations are as follows:

Encoder:A learning rate ofwas optimal, with a stem kernel of 9 in the first convolutional layer and subsequent ResNet stage kernel sizes of 9, 7, 7, and 5, respectively. Each of the 512 output channels was set to an output size of 240.

Transformer:The best configuration used 12 layers and 8 attention heads, a learning rate of, and an ECG input size of(as described in Section3.2, subsectionTransformer Decoder).

LSTM:We used a single LSTM layer with a learning rate ofa teacher forcing probability of 1 during training (disabled during validation and testing), and an attention layer of size 512.

For both our LSTM and Transformer based models we used batch size of 32, no normalization, cross-entropy as the loss function and Adam optimizer. The token embedding size was set to 512, the dropout rate to 0.5 and the learning rate decay factor to 0.8 every 8 epochs without improvement, with the METEOR score (see section5.4under METEOR) as the target metric. Top-k sampling of 1 was used for deterministic token selection.

SECTION: 3.4Complexity & Runtime

The model was trained on a single node equipped with 8 CPU cores, 62.5 GB of memory, and one NVIDIA A100 GPU (80 GB memory). The runtime was estimated based on training both the encoder and decoder from scratch (without pre-training) on the PTB-XL dataset, using the optimal hyperparameters described in section3.

Encoder:The ResNet-34 architecture contains 13.76M parameters.

Decoder:The Transformer decoder, with 38.36M parameters, has an estimated runtime of approximately 7 hours and 10 minutes on the specified hardware for training the entire model, including the encoder (without encoder pre-training). By comparison, the LSTM decoder—with 5.52M parameters—completes training in roughly 1 hour and 10 minutes (in both cases, runtime depends on the number of epochs before early stopping).

While the best-performing Transformer model uses 12 layers, reducing this to 8 layers provides a runtime-optimized alternative with minimal impact on performance. This modification reduces the parameter count to 25.75M and shortens the runtime accordingly. Similarly, using ResNet-18 instead of ResNet-34 reduces the encoder’s parameter count to 6.93M without substantially affecting accuracy.

Note: Both the Transformer and the LSTM training are not optimized in terms of runtime and, presumably a relatively minor effort could decrease it significantly.
Furthermore, using a pre-trained encoder reduces the runtime by about 10% (not including the time for pre-training, which, if included, results in a longer training time overall).

SECTION: 4Data & Preprocessing

In this section, we introduce the datasets used for training and evaluating our ECG caption generation model. We outline the main characteristics of each dataset, followed by the preprocessing techniques employed to ensure compatibility with our model architecture. These steps are critical for enhancing model performance and ensuring robust, generalizable results.

SECTION: 4.1Data

The success of our model relies on access to annotated ECG data that captures a wide range of cardiac conditions and data quality.

To achieve this, we selected two complementary datasets: the publicly available PTB-XL dataset, which provides clinical-grade, multi-lead ECG recordings, and the proprietary dataset provided by BIOTRONIK SE & Co KG, comprising single-lead sECG data from implantable cardiac monitor (ICM) devices. This combination allows us to evaluate the model’s adaptability to varying ECG formats, sampling rates, and recording environments, thus ensuring its relevance for both clinical and personal health monitoring applications.

The PTB-XL dataset[6,20]comprises 21,801 10-second ECG episodes, with two optional sampling frequencies: 500 Hz (resulting in 5000 data points per ECG record) and 100 Hz (1000 data points per record). These episodes were collected from 18,869 unique patients, giving aunique patient proportionof 0.87 - indicating that 87% of the total episode count corresponds to unique individuals, while the remaining episodes are contributed by patients with multiple recordings. Each episode is annotated with a rhythm label and a corresponding report (for details see[6]).

The dataset includes 9839 unique reports, where uniqueness is determined by ignoring minor differences such as whitespace, case sensitivity, and punctuation. This results in a unique report proportion of 0.45, meaning that 45% of the total reports are distinct, with the remainder being repeated across multiple episodes. Using this dataset served multiple purposes in our research. Firstly, it enabled us to test our model on a publicly available dataset, ensuring the reproducibility of our results. Additionally, it allowed us to assess the model’s performance on 12-lead, 500 or 100 Hz, 10-second ECG records and evaluate how it manages a relatively small sample size. Furthermore, we used this dataset for comparability with the reference model, which was also tested on PTB-XL[15](on 100Hz).

The ICM dataset contains records from Implantable Cardiac Monitors (ICMs) BIOMONITOR III and BIOMONITOR IIIm[21], which consists of 60-second sECG episodes recorded at a sampling frequency of 128 Hz, resulting in 7680 data points per sECG record. The report and rhythm class (a single label selected by HCPs out of given list) data were assembled as a byproduct of routine medical procedures conducted worldwide. The anonymized data underwent additional filtering to ensure that all personal information was removed from the manually entered reports. To avoid translation issues only reports in English were used. The final report dataset (Part 2below) comprised data from 1033 clinics. The dataset is divided into two parts:

Part 1:737,999 episodes, each labeled with a rhythm class provided by an HCP. These labels are a byproduct of routine medical procedures and were not cross-checked for accuracy and are therefore potentially inaccurate.

Part 2:206,768 episodes, where each episode includes both a rhythm class and a free-text report: a sentence written by an HCP as part of routine medical procedures, not necessarily intended as an ECG caption for learning or training purposes. These episodes come from 6687 implanted devices. Among these 200k episodes, there are 38,652 unique reports, (here too, ignoring minor differences like whitespace, case sensitivity, and punctuation), resulting in a unique report proportion of 0.19.

SECTION: 4.2Data Pre-Processing

Consistent data preprocessing ensures compatibility with our model architecture and optimizes performance by standardizing data across both datasets. Steps such as train-validation-test splits, abbreviation unification, and tokenization were carefully designed to maintain data integrity while enhancing comparability.

In the ICM dataset, we applied an episode deduplication process. An episode was considered duplicated if it originated from the same implanted device, had the same report, rhythm class, and recording date. During deduplication, one episode from each group of duplicates was randomly selected, and the others were removed from the dataset.

To prevent data leakage and enhance model generalizability, we applied device-level splits for training, validation, and testing. This approach ensures that each patient’s data is confined to one split, reducing the risk of overfitting and supporting a more meaningful evaluation of the model’s performance.

PTB-XL:We tested two splitting approaches for this dataset. The first approach used the official splits provided by PhysioNet, which maintain patient exclusivity across splits and mitigate data leakage risks. These splits allocate 80% of data for training, 10% for validation, and 10% for testing, as further detailed in[6]. The second approach, used for comparability with the reference model[15], involved a random split into training, validation, and testing sets with proportions of 64%, 16%, and 20%, respectively. Results from both approaches are presented in Section6, Tables2and3.

ICM:For this dataset, splits were applied at the implanted device level to ensure no overlap between the training, validation, and test sets, preventing the model from leveraging morphological similarities across episodes from the same device. After deduplication, data was divided into training (80%), validation (10%), and test (10%) sets.

Due to medical terminology variations in the datasets, an abbreviation unification process was applied to reduce prediction errors. Additionally, the OPUS-MT model[22,23]was used to translate German reports from the PTB-XL dataset into English, allowing for consistent application of abbreviation standardization and enhancing model compatibility. The abbreviations and their unified form are presented in Table1.

SECTION: 4.3Report Tokenization

Report tokenization was conducted by splitting comments based on non-letter characters (such as spaces, commas, slashes, etc.). These splitting characters were tokenized and retained within the text, with the exception of spaces. Spaces were not tokenized but assumed to occur between every two tokens to avoid skewing evaluation metrics (e.g., METEOR or BLEU-score) due to the abundance of spaces, which would otherwise inflate scores and reduce the relative significance of other tokens.

This process yielded an initial vocabulary size of 5,304 for the ICM dataset and 2,282 for the translated PTB-XL dataset, or 7,015 and 2,455, respectively, without applying the abbreviation script. As abbreviations only apply to English, they were not used on the non-translated PTB-XL reports. Filtering out words occurring fewer than twice further reduced the ICM and PTB-XL datasets vocabularies to 3,194 and 1,383 tokens, respectively. To ensure comparability across datasets, we truncated each vocabulary, retaining only the 1,024 most frequent tokens. Note that after vocabulary truncation, the least frequent word of the PTB-XL has a frequency of 3 and that of the ICM dataset a frequency of 14. Therefore, in the presented case filtering tokens based on their frequency deemed redundant.

Each report was prefixed with a start token and suffixed with an end token, with padding tokens added to a maximum report length of 300 tokens (although in practice, no reports exceeded this length). A special token was assigned for unknown tokens: words appearing only once or outside the top 1,024 most common tokens.

The Byte-Pair-Encoding (BPE) method[24]was also tested, but it performed suboptimally compared to word tokenization, likely due to the specialized terminology in these datasets.

In summary, the PTB-XL and ICM datasets provide diverse ECG data well-suited for training and evaluation of our caption generation model. By carefully preprocessing the data, including patient/device-level splits, translation and abbreviation unification, we enhanced our model’s ability to generate precise, contextually relevant ECG descriptions.

SECTION: 5Experiments

In this section, we summarize extensive evaluations of our model across multiple design configurations (Section5.1) and its comparison with a state-of-the-art method (see Section5.2). Additionally, we present the results of our case study (Section6.3) and a sanity check conducted to confirm that the model’s performance is attributable to its ability to learn ECG morphology (Section6.4).

SECTION: 5.1Key Experiments

Throughout our experimentation, we addressed several key research questions by running a series of targeted experiments. Table2, summarizes the performance of our model on the publicly available PTB-XL dataset across the following main variations:

Decoder Architecture (LSTM vs. Transformer):As the choice of decoder architecture is fundamental to our model, we tested all configurations with both LSTM and Transformer decoders, allowing us to compare overall performance between the two.

Encoder Depth (ResNet18 vs. ResNet34):To determine whether the depth of the ResNet encoder affects model performance, we experimented with both ResNet18 and ResNet34 encoders.

Effect of Encoder Pre-Training:To quantitatively evaluate whether pre-training the encoder on rhythm class labels improves model performance, we compared results with and without encoder pre-training. In our case-study, as discussed in Section4.1, such pre-training also leverages an additional 738K episodes.

Translation to English:Since our study primarily uses English as the target language, we conducted all experiments with text translated to English. To maintain reproducibility, however, we also ran an experiment without translation or abbreviation unification, testing the model on raw text.

Abbreviation Unification vs. Raw Text:To test the impact of the abbreviation unification process described in Section4.2, we compared results on such standardized text against results on the raw text. As the abbreviation unification applies exclusively to English, we included translation for this comparison as well.

SECTION: 5.2Comparison with the Reference Model

To evaluate the effectiveness of our approach, we benchmarked its performance against the reference model proposed by Qiu et al.[15]. This model employs a ResNet-based encoder for feature extraction from ECG signals, which are then aligned with pretrained embeddings from large language models (LLMs) such as GPT-2 and BERT. Additionally, Qiu et al. introduced an Optimal Transport (OT)-based objective complementing the standard cross-entropy loss, to enhance alignment between ECG embeddings and language embeddings. Their method has achieved strong results in ECG disease classification and report generation tasks, particularly on the PTB-XL dataset, making it a relevant benchmark for comparison.

However, Qiu et al.’s use of random train/validation/test splits raises concerns regarding patient data overlap, potentially inflating results due to shared patient data across splits. Such overlap is problematic for generalizability, a critical factor for clinical applications. To address this, we adopt more rigorous splitting methods in our experiments, including the official PTB-XL splits that ensure patient exclusivity across sets, thereby enhancing reliability and comparability.

A detailed comparison between the reference model and our model’s performance is provided in Section6, particularly in Table3.

SECTION: 5.3Experimental Setup

Our primary experiments were conducted on the PTB-XL dataset, which serves as a comprehensive benchmark in this domain. To ensure reproducibility and generalizability, we used the train/validation/test splits recommended by the PTB-XL authors. These fixed splits enhance reliability as they are not random, and the validation and test datasets are considered gold standards, comprising reports manually reviewed by professionals[6].

To facilitate a direct comparison with the reference model, which is evaluated on random splits, we also tested our best-performing models on comparable random splits, applying translation but excluding abbreviation unification to align with the reference approach. Results from this experiment are presented in Table3.

Additionally, as a case-study, we evaluated our model on the ICM dataset. This dataset represents an increasingly relevant data source generated by personal ECG devices, and its automated analysis is of growing importance[4]. The results for this case-study can be found in Table4.

For detailed explanation of the different datasets used in the different setups please refer to section4.

Lastly, we performed asanity checkby substituting the ECG input with a uniform vector in which all entries are set to 1, effectively removing the ECG signal to assess whether the model was generating reports based on repetitive textual patterns rather than meaningful ECG data. The performance of our model in this test is discussed in Section6.4.

SECTION: 5.4Evaluation

Evaluating generated text is a challenging task, as even humans may struggle to assess the similarity in meaning between two sentences. We used three key metrics suited for such evaluation:

BLEU[25]measures the precision of N-grams (1- to 4-grams) between generated and reference texts, applying a brevity penalty for shorter outputs to account for recall. This metric is widely used for its simplicity, but it emphasizes precision over recall.

METEOR[26]improves on BLEU by incorporating both precision and recall, offering a more balanced assessment. It also includes word-order sensitivity and explicit word-matching (exact, stemmed, and synonym-based), making it suitable for tasks where accurate phrasing is critical.

ROUGE[27], originally designed to emphasize recall (R), is particularly useful for assessing the completeness of information, as is often required in summarization tasks. However, beyond Recall, ROUGE also includes Precision (P) and F1-score (F), providing a more comprehensive evaluation of generated text.

Another metric worth noting isMRScore[28](preprint), which utilizes large language models (LLMs) such as GPT to evaluate generated reports based on human-like criteria. While it has not yet gained widespread adoption in clinical text generation, it represents a promising direction for incorporating LLM-based assessments in the future.

METEOR was chosen as our primary metric due to its ability to evaluate semantic similarity more effectively than BLEU or ROUGE. BLEU’s reliance on exact matches makes it less suitable for tasks like clinical text generation, where paraphrasing or synonym usage is common. ROUGE, while including precision and F1-score, is primarily recall-focused, which can bias evaluations towards longer outputs. METEOR, on the other hand, balances precision and recall while incorporating linguistic features like stemming and synonyms, making it better suited for capturing subtle nuances in clinical reports.

SECTION: 6Results

SECTION: 6.1Key experiments

The results of our main experiments, as shown in Table2, indicate that our model outperforms the current state-of-the-art reference method across all metrics. This improvement is achieved even under a more rigorous setup, using patient-exclusive test sets, compared to the random splits used in the reference model.

Our findings suggest that the optimal configuration employs a non-pretrained ResNet34 as the encoder and an LSTM as the decoder. However, performance across other configurations did not differ significantly. Notably, the performance gap between ResNet34 and the more compact ResNet18 encoder was minimal, suggesting that ResNet18 is a practical alternative when hardware or time constraints are considerations. This setup, combined with an LSTM decoder and bypassing time-intensive translation steps, still yielded comparable results.

Moreover, pre-training the encoder on episode rhythm labels did not improve model performance; in fact, it appeared to slightly diminish predictive accuracy. Translating the reports to English, however, resulted in a noticeable METEOR score boost from 50.72% for the non-translated version to 55.01% for the translated version in the top-performing architecture. Abbreviation unification had a minor yet positive impact (about 0.5%) on the scores for translated reports.

Fig.3shows the progression of the METEOR score over training epochs for key experiments. The plot suggests the absence of overfitting and reveals that the LSTM-based model achieves accuracy comparable to the Transformer-based model in nearly half the epochs.

SECTION: 6.2Experiment with the Reference Model Setup

To ensure a fair comparison, we conducted an additional experiment replicating the setup used by the reference model (detailed in Section5.3). The results, presented in Table3, demonstrate that both our LSTM-based and Transformer-based models outperform the reference model, with the LSTM-based model showing a slight advantage over the Transformer-based approach.

SECTION: 6.3Case Study - Single lead, subcutaneous ECG

In our case-study, we evaluated performance on single-lead, subcutaneous ECG data with reports from ICM devices (See Section4.1). The model’s performance on this dataset, though lower than on PTB-XL, still surpassed current state-of-the-art benchmarks, demonstrating the model’s robustness in challenging data scenarios. Notably, the Transformer and LSTM-based models performed similarly, with the Transformer showing a slight edge, likely due to minor random variations.

Notably, despite the 738K episodes containing only rhythm labels and thus available only for pre-training (alongside200K episodes with labels as well as reports which are used for both pre-training and training), pre-training the encoder did not enhance performance on this dataset.

The comparatively lower BLEU-4 score for the ICM dataset may be due to the brevity of the reports (median length of 4 tokens and an average of 5.67), reducing 4-gram match probability between generated and reference texts. This is supported by the relatively higher BLEU-1 and BLEU-2 scores, indicating more accurate matching in shorter text segments.

Lastly, abbreviation unification markedly improved performance on this dataset. While its effect was minimal on PTB-XL, it led to a pronounced increase (from 15.57% to 32.59%) on the ICM dataset. This is likely because a significant portion (31.2%) of the PTB-XL dataset is automatically generated, which may result in a higher degree of uniformity in terms and language. In contrast, the ICM dataset exhibits greater variability in terminology, an inconsistency mitigated effectively by abbreviation unification.

SECTION: 6.4Sanity Check

To ensure our models were generating text based on ECG data rather than relying on common text patterns, we performed a sanity check. Here, we replaced the ECG input with a uniform vector in which all entries are set to, effectively removing ECG information. A successful sanity check would result in a substantial drop in performance, confirming that the model’s output is informed by ECG morphology. As expected, this experiment led to prediction of uniform comments of optimal length comprising the most common words in the corpus.

The sanity check was conducted both on the PTB-XL and the ICM datasets, post-abbreviation unification.
The results on the ICM dataset demonstrate an 8.52% drop in METEOR score, from 32.59% in the regular experiment to 24.07% in the sanity check and a drop of 2.11% in BLEU4 score, from 10.61% in the regular experiment to 8.5% in the sanity check, reflecting 26% and 19% reduction in these scores respectively. While the relatively high performance in the sanity check suggests some dependency on recurring tokens, the observed drop highlights the model’s ability to incorporate meaningful ECG data, even in the relatively challenging ICM dataset.

As expected, the sanity check performed on the official splits of the PTB-XL dataset resulted in a more pronounced performance drop across various metrics. For instance, BLEU4 decreased from 0.35 in the original experiment to 0.08 in the sanity check, reflecting a 77% reduction. METEOR score dropped from 0.56 to 0.31, a 45% reduction.

These results further validate the model’s reliance on ECG morphology.

SECTION: 7Discussion and Future Work

The successful application of free-text generation techniques to ECG episodes using an architecture traditionally employed in image captioning represents a notable advancement. It underscores the potential for adapting image-captioning methods to analyze and generate free-text descriptions for medical signals, thereby expanding the scope of cross-domain applications of such techniques.

A central challenge identified in this study is the absence of a well-established, large-scale benchmark dataset specifically for ECG-based report generation. Although the PTB-XL dataset provides a foundation for evaluation, its relatively limited size poses constraints on training more sophisticated language models. An anticipated alternative is the MIMIC-IV-Notes dataset, which is expected to be linked with the MIMIC-IV-ECG dataset[29]. Future evaluation of our proposed model on this dataset could yield a more comprehensive performance assessment, and we anticipate that with larger datasets, the efficiency and accuracy benefits of Transformer-based models may become more pronounced. We also encourage other research groups to use this dataset upon its release to foster a standardized benchmark for the field.

Furthermore, we would like to highlight the importance of our experiments that did not produce our best-performing results, as we believe these acknowledgments are valuable for future research, guiding it toward or away from potential approaches. Examples include our pre-training strategies, translation of reports versus non-translation, lack of abbreviation unification, and the exploration of different combinations thereof. Furthermore, we conducted numerous hyperparameter tests—though not the entire field—due to time constraints and limited computational resources. We encourage future researchers to examine these suboptimal experiments carefully and learn what might or might not be worth attempting in their own studies.

Finally, building on the demonstrated adaptability of image captioning architectures to 1-dimensional medical data in this work, a particularly promising avenue for further exploration lies in applying these methods to other 1D datasets such as EEG (electroencephalogram), as well as data from other domains.

SECTION: 8Conclusion

This work demonstrates the feasibility of generating clinically relevant free-text comments for ECG episodes by employing architectures commonly used in tasks like image captioning. Both LSTM-based and Transformer-based models achieved strong performance, with the LSTM-based model displaying overall advantage due to efficiency and training time.

While these results are promising, they only represent an initial step toward fully automated ECG report generation. The current model, though effective in generating insightful text, is not yet sufficiently refined for standalone use in clinical settings. However, its performance suggests that even in its current state, the model could serve as a valuable assistive tool, providing diagnostic insights to physicians or enabling efficient reporting for large-scale ECG datasets.

In conclusion, this work introduces a state-of-the-art approach to generating ECG reports across a diverse range of episode characteristics, including variations in lead count, resolution, and report quality. The comprehensive pipeline and methodology developed here offer a robust foundation for further research and advancement, supporting future investigations into automated medical report generation and related topics.

SECTION: Acknowledgment

This work was supported by the German Ministry for Education and Research (BMBF) within the Berlin Institute for the Foundations of Learning and Data—BIFOLD (project grants 01IS18025A and 01IS18037I) and the Forschungscampus MODAL (project grant 3FO18501).

SECTION: References

SECTION: References