SECTION: :usion Vsionransformers for Image Generation
Diffusion models with their powerful expressivity and high sample quality have achieved State-Of-The-Art (SOTA) performance in the generative domain. The pioneering Vision Transformer (ViT) has also demonstrated strong modeling capabilities and scalability, especially for recognition tasks. In this paper, we study the effectiveness of ViTs in diffusion-based generative learning and propose a new model denoted as Diffusion Vision Transformers (DiffiT). Specifically, we propose a methodology for finegrained control of the denoising process and introduce the Time-dependant Multihead Self Attention (TMSA) mechanism. DiffiT is surprisingly effective in generating high-fidelity images with significantly better parameter efficiency. We also proposelatentandimagespace DiffiT models and show SOTA performance on a variety of class-conditional and unconditional synthesis tasks at different resolutions. The Latent DiffiT model achieves a new SOTA FID score ofondataset while having,less parameters than other Transformer-based diffusion models such as MDT and DiT, respectively.

SECTION: Introduction
Diffusion modelshave revolutionized the domain of generative learning, with successful frameworks in the front line such as DALLE 3, Imagenand Stable diffusionand achieving state-of-the-art (SOTA) performance in various tasks. They have enabled generating diverse complex scenes in high fidelity which were once considered out of reach for prior models. Specifically, synthesis in diffusion models is formulated as an iterative process in which random image-shaped Gaussian noise is denoised gradually towards realistic samples. The core building block in this process is athat takes a noisy image and predicts the denoising direction, equivalent to. This network, which is shared across different time steps of the denoising process, is often a variant of Convolutional Neural Network (CNN)-based U-Net. However, with a lack of standard design pattern, several architecture variantshave been proposed for the denoising network.

Vision Transformers (ViTs)have demonstrated SOTA performance for various recognition tasks and offer compelling advantages such as long-range dependency modeling and scalability. Recently, a number of efforts such as Diffusion Transformers (DiT)and Masked Diffusion Transformer (MDT)

have proposed to leverage the strong modeling capability and scalability of ViTs for diffusion-based image generation. In DiT and MDT, Adaptive LayerNorm (AdaLN)is used for input noise conditioning. However, this scheme significantly increases the number of parameters and does not effectively model the unique temporal dynamics of the denoising process. Specifically, in the beginning of denoising, the high-frequency content of the image is completely perturbed as the denoising network primarily focuses on predicting the low-frequency content. Towards the end of denoising, in which most of the image structure is generated, the network tends to focus on predicting high-frequency details. The conditioning in DiT is realized by modulating the input with channel-wise scale and shift parameters predicted by adaLN layers. However, this mechanism cannot optimally capture the dynamics of the denoising process since it does not effectively model the joint spatial and temporal dependencies. In this work, we introduce the Time-dependant Multihead Self-Attention (TMSA) mechanism which allows for fine-grained control over spatial and temporal dependencies and their interaction during the denoising process. Specifically, our TMSA proposes to integrate the temporal component into the self-attention where the key, query, and value weights are adapted per time step during denoising. This allows the denoising network to dynamically change its attention mechanism in different stages by considering both spatial and temporal components and their correspondence. In Fig., we visualize attention maps from a token at the center of a feature map to all surrounding tokens during the sampling trajectory of a models that are trained on CIFAR10dataset. The DiffiT model with TMSA has a better image generation quality and its attention maps demonstrate a progressive localization towards detailed salient features. However, the model without TMSA is not capable of recovering such details.

In addition, employing TMSA significantly improves the parameter efficiency as it only learns three temporal components for query, key and value in each block. In comparison, AdaLN requires learning the shift, scale and gate parameters for self-attention as well as MLP (six components per Transformer block). We also extend TMSA to a window-based scheme without cross-communication among the local regions. This design is surprisingly effective and decreases the computational cost of self-attention by reducing the token sequence length.

Using TMSA as a core building block, we introduce a novel ViT-based diffusion model, called DiffiT (pronounced), for image generation in latent and image space. DiffiT achieves a new SOTA performance in terms of FID score using ImageNet-256dataset (see Fig.) with,less parameters than MDT and DiT models, respectively. DiffiT also achieves SOTA performance for image space generation tasks on FFHQ-64and CIFAR10datasets.

The following summarizes our contributions in this work:

We introduce TMSA which is a novel time-dependent self-attention mechanism and is specifically tailored to capture both temporal and spatial dependencies as well as their interaction. Our proposed time-dependent self-attention dynamically adapts its behavior over sampling time steps.

We introduce a new ViT-based diffusion model, denoted as DiffiT, which unifies the design patterns of denoising networks and can be used in a variety of image generation tasks in the latent and image space.

We demonstrate that DiffiT can achieve SOTA performance on a variety of datasets for both conditional and unconditional generation tasks in the latent and image space. The latent DiffiT model achieves a new FID score of 1.73 with significantly less number of parameters than competing approaches.

SECTION: Related Work
Diffusion modelshave driven significant advances in various domains, such as text-to-image generation, natural language processing, text-to-speech synthesis, 3D point cloud generation, time series modeling, molecular conformal generation, and machine learning security. These models synthesize samples via an iterative denoising process and thus are also known in the community as noise-conditioned score networks. Since its initial success on small-scale datasets like CIFAR-10, diffusion models have been gaining popularity compared to other existing families of generative models. Compared with variational autoencoders, diffusion models divide the synthesis procedure into small parts that are easier to optimize, and have better coverage of the latent space; compared with generative adversarial networks, diffusion models have better training stability and are much easier to invert. Diffusion models are also well-suited for image restoration, editing and re-synthesis tasks with minimal modifications to the existing architecture, making it well-suited for various downstream applications.

Transformer-based models have achieved competitive performance in different generative learning models in the visual domain. A number of transformer-based architectures have emerged for GANs. TransGANproposed to use a pure transformer-based generator and discriminator architecture for pixel-wise image generation. Gansformerintroduced a bipartite transformer that encourages the similarity between latent and image features. Styleformeruses Linformersto scale the synthesis to higher resolution images. Recently, a number of effortshave leveraged Transformer-based architectures for diffusion models and achieved competitive performance. In particular, DiTproposed a latent diffusion model in which the regular U-Net backbone is replaced with a Transformer. Using the DiT architecture, MDTintroduced a masked latent modeling approach to effectively capture contextual information. In comparison to DiT, although MDT achieves faster learning speed and better FID scores on ImageNet-256 dataset, it has a more complex training pipeline. Recently with a similar architecture to DiT, SiTwas proposed to incorporate flow matching. Unlike DiT, MDT or SiT, the proposed DiffiT does not use shift and scale, as in AdaLN formulation, for conditioning. Instead, DiffiT proposes a time-dependent self-attention (TMSA) to jointly learn the spatial and temporal dependencies. In addition, DiffiT proposes both image and latent space models for different image generation tasks with different resolutions with SOTA performance.

SECTION: Methodology
SECTION: Diffusion Model Preliminaries
Diffusion modelsare a family of generative models that synthesize samples via an iterative denoising process. Given a data distribution as, a family of random variablesforare defined by injecting Gaussian noise to,,, whereis a Gaussian distribution. Typically,is chosen as a non-decreasing sequence such thatandbeing much larger than the data variance. This is called the “Variance-Exploding” noising schedule in the literature; for simplicity, we use these notations throughout the paper, but we note that it can be equivalently converted to other commonly used schedules (such as “Variance-Preserving”) by simply rescaling the data with a scaling term, dependent on.

The distributions of these random variables are the marginal distributions of forward diffusion processes (Markovian or not) that gradually reduces the “signal-to-noise” ratio between the data and noise. As a generative model, diffusion models are trained to approximate the reverse diffusion process, that is, to transform from the initial noisy distribution (that is approximately Gaussian) to a distribution that is close to the data one.

Despite being derived from different perspectives, diffusion models can generally be written as learning the following denoising autoencoder objective

Intuitively, given a noisy sample from(generated via), a neural networkis trained to predict the amount of noise added (,). Equivalently, the neural network can also be trained to predictinstead. The above objective is also known as denoising score matching, where the goal is to try to fit the data score (,) with a neural network, also known as the score network. The score network can be related tovia the relationship.

Samples from the diffusion model can be simulated by the following family of stochastic differential equations that solve fromto:

whereis the reverse standard Wiener process, andis a function that describes the amount of stochastic noise during the sampling process. Iffor all, then the process becomes a probabilistic ordinary differential equation(ODE), and can be solved by ODE integrators such as denoising diffusion implicit models (DDIM). Otherwise, solvers for stochastic differential equations (SDE) can be used, including the one for the original denoising diffusion probabilistic models (DDPM). Typically, ODE solvers can converge to high-quality samples in fewer steps and SDE solvers are more robust to inaccurate score models.

SECTION: DiffiT Model
At every layer, our transformer block receives, a set of tokens arranged spatially on a 2D grid in its input. It also receives, a time token representing the time step. Similar to,
we obtain the time token by feeding positional time embeddings to a small MLP with swish activation. This time token is passed to all layers in our denoising network. We introduce our time-dependent multi-head self-attention, which captures both long-range spatial and temporal dependencies by projecting feature and time token embeddings in a shared space. Specifically, time-dependent queries, keysand valuesin the shared space are computed by a linear projection of spatial and time embeddingsandvia

where,,,,,denote spatial and temporal linear projection weights for their corresponding queries, keys, and values respectively.

We note that the operations listed in Eq.toare equivalent to a linear projection of each spatial token, concatenated with the time token. As a result, key, query, and value are all linear functions of both time and spatial tokens and they can adaptively modify the behavior of attention for different time steps. We define,, andwhich are stacked form of query, key, and values in rows of a matrix. The self-attention is then computed as follows

In which,is a scaling factor for keys, andcorresponds to a relative position bias. For computing the attention, the relative position bias allows for the encoding of information across each attention head. Note that although the relative position bias is implicitly affected by the input time embedding, directly integrating it with this component may result in sub-optimal performance as it needs to capture both spatial and temporal information. Please see Sec.for more analysis.

The transformer block is a core building block of the proposed DiffiT architecture and is defined as

where TMSA denotes time-dependent multi-head self-attention, as described in the above,is the time-embedding token,is a spatial token, and LN and MLP denote Layer Normand MLP respectively.

Recently, latent diffusion models have been shown effective in generating high-quality large-resolution images. In Fig., we show the architecture of latent DiffiT model. We first encode the images using a pre-trained variational auto-encoder network. The feature maps are then converted into non-overlapping patches and projected into a new embedding space. Similar to the DiT model, we use a vision transformer, without upsampling or downsampling layers, as the denoising network in the latent space. In addition, we also utilize a three-channel classifier-free guidance to improve the quality of generated samples. The final stage is a linear layer to decode the output.

As shown in Fig., DiffiT uses a symmetrical U-Shaped encoder-decoder architecture in which the contracting and expanding paths are connected to each other via skip connections at every resolution. Specifically, each resolution of the encoder or decoder paths consists ofconsecutive DiffiT blocks, containing our proposed time-dependent self-attention modules. In the beginning of each path, for both the encoder and decoder, a convolutional layer is employed to match the number of feature maps. A convolutional upsampling or downsampling layer is also used for transitioning between each resolution. We speculate that the use of these convolutional layers embeds inductive image bias that can further improve the performance. In the remainder of this section, we discuss the DiffiT Transformer block and our proposed time-dependent self-attention mechanism. We use our proposed Transformer block as the residual cells when constructing the U-shaped denoising architecture.

The quadratic cost of attention scales poorly when the number of spatial tokens is large, especially in the case of large feature maps. Without loss of generality, the above Transformer block can be applied to local regions, in which the self-attention is computed within non-overlapping partitioned windows. Although these partitioned windows do not allow information to be propagated between different regions, the U-Net structure with bottleneck layers permits information sharing between different regions.

We define our final residual cell by combining our proposed DiffiT Transformer block with an additional convolutional layer in the form:

where GN denotes the group normalization operationand DiffiT-Transformer is defined in Eq.and Eq.. Our residual cell for image space diffusion models is a hybrid cell combining both a convolutional layer and our Transformer block.

SECTION: Results
SECTION: Latent Space
We have trained the latent DiffiT model on ImageNet-512 and ImageNet-256 dataset respectively. In Table., we present a comparison against other approaches using various image quality metrics. For this comparison, we select the best performance metrics from each model which may include techniques such as classifier-free guidance. In ImageNet-256 dataset, the latent DiffiT model outperforms competing approaches, such as SiT-XL, MDT-G, DiT-XL/2-Gand StyleGAN-XL, in terms of FID score and sets a new SOTA FID score of. In terms of other metrics such as IS and sFID, the latent DiffiT model shows a competitive performance, hence indicating the effectiveness of the proposed time-dependent self-attention. In the ImageNet-512 dataset, the latent DiffiT model significantly outperforms DiT-XL/2-G in terms of both FID and Inception Score (IS). Although StyleGAN-XLshows better performance in terms of FID and IS, GAN-based models are known to suffer from issues such as low diversity that are not captured by the FID score. These issues are reflected in sub-optimal performance of StyleGAN-XL in terms of both Precision and Recall. In addition, in Fig., we show a visualization of uncurated images that are generated on ImageNet-256 and ImageNet-512 dataset. We observe that the latent DiffiT model is capable of generating diverse high quality images across different classes.

SECTION: Image Space
We have trained the image space DiffiT model on FFHQ-64and CIFAR10datasets. In Table., we compare the performance of our model against a variety of different generative models including other score-based diffusion models as well as GANs, and VAEs. DiffiT achieves a state-of-the-art image generation FID score of 1.95 on the CIFAR-10 dataset, outperforming state-of-the-art diffusion models such as EDMand LSGM. In comparison to two recent ViT-based diffusion models, our proposed DiffiT significantly outperforms U-ViTand GenViTmodels in terms of FID score in CIFAR-10 dataset. Additionally, DiffiT significantly outperforms EDMand DDPM++models, both on VP and VE training configurations, in terms of FID score. In Fig., we illustrate the generated images on FFHQ-64 dataset. Please see supplementary materials for CIFAR-10 generated images.

SECTION: Ablation
In this section, we provide additional ablation studies to provide insights into DiffiT. We address different questions such as: (1) What strikes the right balance between time and feature token dimensions ? (2) How do different components of DiffiT contribute to the final generation performance, (3) What is the optimal way of introducing time dependency in our Transformer block? and (4) How does our time-dependent attention behave as a function of time?

SECTION: Time and Feature Token Dimensions
We conduct experiments to study the effect of the size of time and feature token dimensions on the overall performance. As shown below, we observe degradation of performance when the token dimension is increased from 256 to 512. Furthermore, decreasing the time embedding dimension from 512 to 256 impacts the performance negatively.

SECTION: Effect of Architecture Design
As presented in Table, we study the effect of various components of both encoder and decoder in the architecture design on the image generation performance in terms of FID score on CIFAR-10. For these experiments, the projected temporal component is adaptively scaled and simply added to the spatial component in each stage. We start from the original ViTbase model with 12 layers and employ it as the encoder (config A). For the decoder, we use the Multi-Level Feature Aggregation variant of SETR(SETR-MLA) to generate images in the input resolution. Our experiments show this architecture is sub-optimal as it yields a final FID score of 5.34. We hypothesize this could be due to the isotropic architecture of ViT which does not allow learning representations at multiple scales. We then extend the encoder ViT into 4 different

multi-resolution stages with a convolutional layer in between each stage for downsampling (config B). We denote this setup as Multi-Resolution and observe that these changes and learning multi-scale feature representations in the encoder substantially improve the FID score to 4.64. In addition, instead of SETR-MLAdecoder, we construct a symmetric U-like architecture by using the same Multi-Resolution setup except for using convolutional layers between stages for upsampling (config C). These changes further improve the FID score to 3.71. Furthermore, we first add the DiffiT Transformer blocks and construct a

DiffiT Encoder and observe that FID scores substantially improve to 2.27 (config D). As a result, this validates the effectiveness of the proposed TMSA in which the self-attention models both spatial and temporal dependencies. Using the DiffiT decoder further improves the FID score to 1.95 (config E), hence demonstrating the importance of DiffiT Transformer blocks for decoding.

SECTION: Time-Dependent Self-Attention
We evaluate the effectiveness of our proposed TMSA layers in a generic denoising network. Specifically, using the DDPM++model, we replace the original self-attention layers with TMSA layers for both VE and VP settings for image generation on the CIFAR10 dataset. Note that we did not change the
original hyper-parameters for this study. As shown in Tableemploying TMSA decreases the FID scores by 0.28 and 0.25 for VE and VP settings respectively. These results demonstrate the effectiveness of the proposed TMSA to dynamically adapt to different sampling steps and capture temporal information.

SECTION: Impact of Self-Attention Components
In Table, we study different design choices for introducing time-dependency in self-attention layers. In the first baseline, we remove the temporal component from our proposed TMSA and we only add the temporal tokens to relative positional bias (config F). We observe a significant increase in the FID score to 3.97 from 1.95. In the second baseline, instead of using relative positional bias, we add temporal tokens to the MLP layer of DiffiT Transformer block (config G). We observe that the FID score slightly improves to 3.81, but it is still suboptimal compared to our proposed TMSA (config H). Hence, this experiment validates the effectiveness of our proposed TMSA that integrates time tokens directly with spatial tokens when forming queries, keys, and values in self-attention layers.

SECTION: Time Token in TMSA
We investigate if treating time embedding as a separate token in TMSA is a beneficial choice. Specifically, we apply self-attention to spatial and time tokens separately to understand the impact of decoupling them. As shown in Table, we observe the degradation of performance for CIFAR10, FFHQ64 datasets, in terms of FID score. Hence, the decoupling of spatial and temporal information in TMSA leads to suboptimal performance.

SECTION: Time Embedding
We study the sensitivity of the DiffiT model to different time embeddings such as Fourier and positional time embeddings. As shown in Table, using a Fourier time embedding leads to degradation of performance in terms of FID score for both CIFAR10and FFHQ-64datasets.

SECTION: Computational Efficiency
In Table, we study the significance of model capacity in generating high-quality images by comparing the number of parameters for models that are trained on ImageNet-256 dataset. All models use the same number of function evaluations for sample generation for fair comparisons. We also use the same global window size for computing self-attention. We observe that DiffiT has,andless number of parameters and,andless number of FLOPs in comparison to MDT-G, SiT-XL and DiT-XL/2-G models, respectively while demonstrating a better FID score.

SECTION: Effect of Classifier-Free Guidance
As shown in Fig.(a), we investigate the effect of classifier-free guidance scale on the quality of generated samples in terms of FID score. For the ImageNet-256 experiment, we used the improved classifier-free guidancewhich uses a power-cosine schedule to increase the diversity of generated images in early sampling stages. This scheme was not used for the ImageNet-512 experiment, since it did not result in any significant improvements. The guidance scales of 4.6 and 1.49 correspond to best FID scores of 1.73 and 2.67 for ImageNet-256 and ImageNet-512 experiments, respectively. Increasing the guidance scale beyond these values results in degradation of FID score.

SECTION: TMSA and DiT Modulation
We directly compare the performance of TMSA and DiT modulation mechanisms in Fig.(b). For this purpose, we employ a DiT-XL as the base model with TMSA as well as its original modulation and train both models for 1000K iterations on ImageNet-256 dataset. The model with TMSA consistently shows better FID scores in different training iterations, hence validating the effectiveness of TMSA.

SECTION: Effect of Window Size
As illustrated in Fig., we study the impact of window size in TMSA on the FID score of generated images for models that are trained on CIFAR10 (resolution) and FFHQ-64 (resolution) datasets. Increasing the TMSA window size fromtodecreases the FID score byandfor CIFAR10 and FFHQ-64 models, respectively. As expected, increasing the effective receptive field seems to improve the generation performance. However, increasing the window size further fromtoonly results in marginal improvement ofandfor CIFAR10 and FFHQ-64, respectively. This is due to the spatial redundancy of adjacent pixels which may not contribute significantly to the generation quality upon increasing the receptive field. As also discussed in the supplementary materials, for image space experiments, we have used our window-based TMSA formulation to benefit from the efficiency gains while maintaining high image generation quality.

SECTION: Conclusion
In this work, we presented DiffiT which is a novel ViT-based diffusion model for both latent and image space generation tasks. Specifically, we proposed the TMSA which allows self-attention to dynamically adapt to different stages of denoising while learning spatial and temporal dependencies and their interaction. The proposed TMSA also significantly improves the parameter efficiency. DiffiT achieves a new SOTA performance on ImageNet-256 dataset while having significantly less number of parameters in comparison to other competitive Transformer-based diffusion models such as SiT, MDT and DiT.

SECTION: References
SECTION: Appendix
SECTION: Ablation
SECTION: Comparison to DiT and LDM
On contrary to LDMand DiT, the latent DiffiT does not rely on shift and scale, as in AdaLN, or concatenation to incorporate time embedding into the denoising networks. However, DiffiT uses a time-dependent self-attention (TMSA) to jointly learn the spatial and temporal dependencies. In addition, DiffiT proposes both image and latent space models for different image generation tasks with different resolutions with SOTA performance. Specifically, as shown in Table, DiffiT significantly outperforms LDMand DiTby 31.26% and 51.94% in terms of FID score on ImageNet-256dataset. In addition, DiffiT outperforms DiTby 13.85% on ImageNet-512dataset. Hence, these benchmarks validate the effectiveness of the proposes architecture and TMSA design in DiffiT model as opposed to previous SOTA for both CNN and Transformer-based diffusion models.

SECTION: Architecture
SECTION: Image Space
We provide the details of blocks and their corresponding output sizes for both the encoder and decoder of the DiffiT model in Tableand Table, respectively. The presented architecture details denote models that are trained with 6464 resolution. Without loss of generality, the architecture can be extended for 3232 resolution. For FFHQ-64dataset, the values of,,andare 4, 4, 4, and 4 respectively. For CIFAR-10dataset, the architecture spans across three different resolution levels (32, 16, 8), and the values of,,are 4, 4, 4 respectively. Please refer to the paper for more information regarding the architecture details.

SECTION: Latent Space
In Fig, we illustrate the architecture of the latent DiffiT model. Our model is comparable to DiT-XL/2-G variant which
032 uses a patch size of 2. Specifically, we use a depth of 30 layers with hidden size dimension of 1152, number of heads dimension of 16 and MLP ratio of 4. In addition, for the classifier-free guidance implementation, we only apply the guidance to the first three input channels with a scale ofwhereis the input latent.

SECTION: Implementation Details
SECTION: Image Space
We strictly followed the training configurations and data augmentation strategies of the EDMmodel for the experiments on CIFAR10, and FFHQ-64datasets, all in an unconditional setting. All the experiments were trained for 200000 iterations with Adam optimizerand used PyTorch framework and 8 NVIDIA A100 GPUs. We used batch sizes of 512 and 256, learning rates ofandand training images of sizesandon experiments for CIFAR10and FFHQ-64datasets, respectively.

We use the deterministic sampler of EDMmodel with 18, 40 and 40 steps for CIFAR-10 and FFHQ-64 datasets, respectively. For FFHQ-64 dataset, our DiffiT network spans across 4 different stages with 1, 2, 2, 2 blocks at each stage. We also use window-based attention TMSA with local window size of 8 at each stage. For CIFAR-10 dataset, the DiffiT network has 3 stages with 2 blocks at each stage. Similarly, we compute attentions on local windows with size 4 at each stage. Note that for all networks, the resolution is decreased by a factor of 2 in between stages. However, except for when transitioning from the first to second stage, we keep the number of channels constant in the rest of the stages to maintain both the number of parameters and latency in our network. Furthermore, we employ traditional convolutional-based downsampling and upsampling layers for transitioning into lower or higher resolutions. We achieved similar image generation performance by using bilinear interpolation for feature resizing instead of convolution. For fair comparison, in all of our experiments, we used the FID score which is computed on 50K samples and using the training set as the reference set.

SECTION: Latent Space
We employ learning rates ofandand batch sizes of 256 and 512 for ImageNet-256 and ImageNet-512 experiments, respectively. We also use the exponential moving average (EMA) of weights using a decay of 0.9999 for both experiments. We also use the same diffusion hyper-parameters as in the ADMmodel. For a fair comparison, we use the DDPMsampler with 250 steps and report FID-50K for both ImageNet-256 and ImageNet-512 experiments.

SECTION: Qualitative Results
We illustrate visualization of generated images for CIFAR-10and FFHQ-64datasets in Figuresand, respectively. In addition, in Figures,,and, we visualize the the generated images by the latent DiffiT model for ImageNet-512dataset. Similarly, the generated images for ImageNet-256are shown in Figures,and. We observe that the proposed DiffiT model is capable of capturing fine-grained details and produce high fidelity images across these datasets.