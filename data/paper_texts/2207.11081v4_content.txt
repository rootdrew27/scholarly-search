SECTION: Emotion Separation and Recognition from a Facial Expression by Generating the Poker Face with Vision Transformers
SECTION: Emotion Separation and Recognition from a Facial Expression by Generating the Poker Face with Vision Transformers
Representation learning and feature disentanglement have garnered significant research interest in the field of facial expression recognition (FER). The inherent ambiguity of emotion labels poses challenges for conventional supervised representation learning methods. Moreover, directly learning the mapping from a facial expression image to an emotion label lacks explicit supervision signals for capturing fine-grained facial features.
In this paper, we propose a novel FER model, named Poker Face Vision Transformer or PF-ViT, to address these challenges. PF-ViT aims to separate and recognize the disturbance-agnostic emotion from a static facial image via generating its corresponding poker face, without the need for paired images. Inspired by the Facial Action Coding System, we regard an expressive face as the combined result of a set of facial muscle movements on one’s poker face (i.e., an emotionless face). PF-ViT utilizes vanilla Vision Transformers, and its components are firstly pre-trained as Masked Autoencoders on a large facial expression dataset without emotion labels, yielding excellent representations. Subsequently, we train PF-ViT using a GAN framework. During training, the auxiliary task of poke face generation promotes the disentanglement between emotional and emotion-irrelevant components, guiding the FER model to holistically capture discriminative facial details.
Quantitative and qualitative results demonstrate the effectiveness of our method, surpassing the state-of-the-art methods on four popular FER datasets.

SECTION: 
Humans typically exhibit non-verbal emotions through their facial expressions, whether consciously or unconsciously.
Given a single facial image, the task of facial expression recognition (FER) aims at recognizing the basic expression on the human face, such as happiness, sadness, and contempt, which can facilitate many applications in computer vision, mental health assessment, etc. Predicting the discrete emotion category from a static facial image in the wild is quite challenging due to several factors, such as the inherent ambiguity of expression, occlusion and head pose variation. Other intertwined irrelevant factors, such as background illumination, identity bias, and imbalanced, noisy emotion labelsin real-world FER datasets make the problem of in-the-wild FER more difficult to address.

Apoker facerefers to a deliberately-induced blank expression on the face of a person that does not show any thinking or feeling of that person.
The well-known Facial Action Coding System (FACS), studied and developed for many decades, describes a facial expression as the combination of a set of distinct local action units (AUs). In other words, a facial expression can be roughly decomposed to an expressive component and a neutral component. Building upon this understanding,we consider an expressive face as the comprehensive result of a set of facial muscle movements on someone’s poker face. Here,. Neutral expression and happy expression are the two most frequently occurring expressions in popular real-world FER datasets, such as AffectNet, RAF-DBand FERPlus.

Recently, researches have made substantial progress in the field of FER and transfer their research focus from lab-controlled settings to in-the-wild circumstances, benefiting much from the development of deep learning techniques and the availability of more representative FER datasets. However, it is important to note that deep neural networks typically require a large amount of training data, and the presence of noisy emotion labels can significantly hinder their generalization performance.
To address this challenge, many state-of-the-art (SOTA) methodsadopt a two-step approach. They first pre-train their deep models on a large scale and less noisy dataset, for instance, Ms-Celeb-1Mfor face recognition (FR), in order to gain better facial representations and then fine-tune the deep models on specific FER datasets.
Various network architectures have been explored for FER, including Convolutional Neural Networks (CNNs), Graph Convolutional Networks (GCNs) and some hybrid Vision Transformers (hybrid ViTs). A hybrid ViT, in particular, consists of a pre-trained CNN backbone and multiple Transformerblocks. CNN-based FER models still dominate this field. SCN, RUL, and EACadopt the ResNet-18 modelpre-trained on the MS-Celeb-1M datasetas the backbone to extract facial features.
Additionally, some advanced methods leverage off-the-shelf facial knowledge such as facial landmarksor hand-crafted image featuresto aid their FER models for higher evaluation metrics.

Compared with CNNs, vanilla ViTsneed much more training data to obtain excellent feature representations, for they use weaker inductive biases and capture more long-range dependency relationships.Taking into account that different facial expressions can share similar local appearences (i.e., facial muscle movements), such global attribute of Transformers may help alleviate the common overfitting issue in FER. This issue arises when the model tends to memorize local facial features instead of learning from the global features.
TransFERand VTFFemploy ResNet-based hybrid ViTs to perform the task and achieve competitive FER performance. FG-AGRintroduces a fine-grained association graph representation framework for FER, combining CNN, GCN and Transformer simultaneously.
Inspired by Generative Adversarial Networks (GANs), MVTproposes a pure Transformer-based framework called Mask Vision Transformer, which aims to filter out the background and occlusion in face images.
It is worth noting that the current methods based on vanilla (pure) ViTs still lag behind those using CNNs or hybrid ViTs in terms of FER accuracy, model size and pre-training cost. Consiquently, we raise the first question.

This problem has not been well explored thus far, and our answer is yes.

In addition to the network structure design and representation learning methods, identity-invariant FER has attracted more and more attention most recently.
Previous studieshave attempted to disentangle facial emotion apart from person identity and even head pose, leveraging pre-trained FR modelsor direct supervision of subject labels. CNN-based GANs are employed to generate new facial expressions, thereby encouraging the FER model to learn a disentangled expressional representation and also enlarging the FER training set. However, other irrelevant factors except identity and pose still exist as before and are detrimental to FER. Some novel identity-aware FER methodsessentially aim to mitigate the adverse influence of personal identity as well. Unfortunately, these existing identity-invariant or identity-aware methods either need paired facial expression images for each subject or can not generate high-quality facial expressions for unseen subjects. In-the-wild FER datasets, such as AffectNetand RAF-DB, contain a large number of person identities and consist of unpaired facial images only. Hence, we proceed to raise the second question.

Our answer to this question is also positive.

However, training a plain ViT-based GAN (ViTGAN) for image generation is a challenging task. And how to make the de-expression (or emotion disentanglement) taskenhance FER remains to be further studied.
Masked autoencoder (MAE), which reconstructs the missing parts of an input image using an asymmetric encoder-decoder architecture, is a very powerful self-supervised representation learing framework. MAEs have proven to be effective in various tasks like image inpainting and denoising. They can then be utilized in downstream tasks such as image generation, classification, and feature extraction.
Inspired by MAE, we transform and develop a vanilla ViT-based Masked Autoencoder into a GAN framework to accomplish a specific image-to-image translation task, which is to generate the emotionless counterpart, a poker face, given a facial image with an arbitrary expression.
As illustrated in Fig., our work explores how to separate the emotion-related features from the emotion-irrelevant features in a latent representation space through the auxiliary task of adversarial poker face generation. By introducing the auxiliary task, our aim is to fully disentangle the emotion details from the expressive face and compel the FER model to capture more fine-grained features of facial muscle movements, leading to a better understanding of facial expressions.

Motivated by the analysis above, we propose a plain ViT-based GAN model, primarily developed from MAEs. Our GAN model, referred to asokeraceAE-AN (), is depicted in Fig.. After training, we can take out a multi-task ViT model from PF-MAG capable of simultaneously perfroming facial expression classification and poker face generation. And we name this ViT model asokeraceisionransformer ().
PF-ViT mainly incorporates an image encoder, a token separator, an image generator, and a facial expression classification head. PF-ViT and a composite discriminator constitute our GAN model, in which the encoder, generator and discriminator are pure ViTs. Specifically, our method firstly trains the ViT encoder to learn fine-grained, robust facial expression representations using MAE pre-trainingon a large facial expression dataset without emotion labels.
After that, PF-ViT is trained with the discriminator in an adversarial manner to separate the discriminative emotion features from the learned representations and synthesize realistic, emotionless poke faces using its generator, deceiving the discriminator within the GAN framework. Meanwhile, FER supervision signals applied to the classification head of PF-ViT prevent the information collapse in the pure emotion representations separated by the token separator.
During testing, the generator of PF-ViT can be removed, and let PF-ViT solely perform the FER task.

Our main contributions are summarized as follows:

We propose a novel in-the-wild FER model called PF-ViT, which utilizes plain ViTs to separate and recognize facial expressions and generate corresponding emotionless faces simultaneously, given a facial image. We train PF-ViT in a GAN framework without the need for paired images, where PF-ViT is encouraged to disentangle the pure emotion-related features, leading to an obvious improvement in FER performance.

We demonstrate the high potential of vanilla ViTs for FER by employing the MAE representation learing framework to pre-train ViT encoders using only unlabeled FER data. Furthermore, we show a tiny ViT with only 5.6M parameters has achieved decent performance, compared to previous advanced FER models.

Our method achieves SOTA performance on popular in-the-wild FER datasets, obtaining the highest accuracy of 92.07%, 67.23%, 64.10% and 91.16% on RAF-DB, AffectNet-7, AffectNet-8 and FERPlus datasets, respectively. Moreover, to the best of our knowledge, this is the first attempt to synthesize a realistic emotionless face from an arbitrary expressive face using plain Transformer backbones without paired data.

The remainder of this paper is organized as follows. Sec.reviews the related work briefly. Details of our method are elaborated in Sec.. Sec.provides the experimental results and analysis. Finally, we conclude this paper in Sec..

SECTION: 
Over the past few decades, extensive efforts have been devoted to addressing the problem of facial expression recognition (FER). Classical approaches concentrate on this problem under lab-controlled conditions and rely on handcrafted features.
With the rapid advancements in deep neural networks and representation learning, substantial progress has been achieved and robust FER in real-world scenarios has become feasible, as discussed in Sec.. In this section, we briefly review the most relevant studies on feature disentanglement and representation learning for FER.

SECTION: 
To mitigate the disturbance caused by emotion-irrelevant factors, such as age, race, gender, illumination and occlusion, explicit and implicit feature disentanglement has been introduced into this field to extract disentangled facial expression representations. DDLproposes a disturbance-disentangled learning method to disentangle several disturbing factors simultaneously. FDRLpresents a feature decomposition and reconstruction learning method that models expression similarities and expression-specific variations, enabling the extraction of fine-grained expression features. To approach the goal of identity- and pose-robust FER, IPFRemploys adversarial feature learning to extract expression features while avoiding subject and pose variations. Subsequently, IPD-FERintroduces a GAN-based FER model to disentangle identity, pose and expression. IPD-FER employs three independent CNNs to encode identity, pose and expression information, respectively.
However, these methods requires the dataset with multi-task labels, which are unavailable for in-the-wild FER datasets.

Zhang et al.and Xie et al.employ CNN-based GANs to disentangle expressions and even generate new faces with different expressions (i.e., expression transfer) to enlarge the training set. However, the synthesized facial expressions lack realism and their FER performance falls short of expectations, indicating that the intrinsic expressional information is not effectively separated.
By contrast, DeRL, a two-phase method, firstly trains a conditional GAN (cGAN) model to generate the corresponding neutral face for a given input, and then learns the residual expressive component recorded in the intermediate layers of the generative model. However, the emotion-irrelevant disturbance still persists in the intermediate layers and DeRL requires paired images for each subject. Although our method draws inspiration from DeRL, we make a different assumption for emotion extraction.

SECTION: 
Owing to the inherent ambiguity (or uncertainty) of facial expressions, real-world FER datasets often contain mislabeled annotations, also known as noisy labels.
Recently, much attention has been focused on learning reliable and discriminative expression features from noisy data. To this end, some prior work modifies the primary loss function or selects cleaner samples to train FER models. SCNpresents a self-cure network to suppress the uncertainties in facial expression data and promote the learning of robust features. DMUEtackles the ambiguity problem by exploring the latent label distribution and estimating pairwise relationships between instances. RULtreats facial expressions uncertainty as a relative concept and learns it based on the relative difficulty of two samples using feature mixup. EACbuilds an imbalanced framework and utilizes an erasing and flipping consistency loss to prevent the model from memorizing noisy samples during training.

The presence of noisy labels in FER datasets can significantly impair the performance of deep networks when conventional supervised learning is applied.
Previous workhas experimentally demonstrated that FER models trained solely on a specific in-the-wild FER datasetfrom scratch are inferior to those have already pre-trained on a large-scale upstream dataset like Ms-Celeb-1M for FRand ImageNet for image classification. Further, Face2Expleverages large unlabeled FR datasets to imrpove FER performance through a meta optimization framework. In contrast, our method explores the effectiveness of MAE pre-training for enhancing facial representations in ViTs using facial expression images without ambiguous emotion labels.

SECTION: 
SECTION: 
The proposed method consists of two steps.
Firstly, we pre-train plain ViTs using unlabeled facial expression images in a self-supervised manner, obtaining strong facial expression representations. These pre-trained ViTs are then fully fine-tuned on specific FER datasets to serve as decent and transparent baselines.

Next, we introduce the multi-task FER model, PF-ViT, which separates and classifies the emotion of the input face by generating the corresponding poker face that preserves all emotion-irrelevant details. As illustrated in Fig., we train PF-ViT in a GAN framework without the requirement for paired images of subjects, where the pre-trained ViTs from the previous step are reused.

SECTION: 
. Vision Transformers (ViTs)with the global attention mechanism have gained significant popularity as an alternative to Convolutional Neural Networks (CNNs) in various computer vision tasks. They excel at capturing long-distance relationships across the entire image. Here, we select three vanilla ViT variants, namely ViT-Base, ViT-Small and ViT-Tiny, to serve as image encoders for the FER task, where ViT-Small and ViT-Tiny were originally introduced in DeiT.

Given an input facial imageof width, heightand channel, we reshape it into a sequence of flattened 2D image patches, in which the resolution of each patch is, and. Fixed 2D sin-cos position embeddingsare used to retain positional information for Transformer blocks.
The ViT encoder, denoted as, maps the input to a latent representation, i.e., a sequence of visual tokens, where the first tokenis the appended learnabletoken andrepresents the feature dimension of each latent token across its layers.
In our method, ViT-Base (ViT-B), ViT-Small (ViT-S) and ViT-Tiny (ViT-T) all use a patch size ofpixels and they are only different in the embedding dimensionand the number of attention heads, which are summarized in Table.

Unfortunately, ViTs typically require a larger amount of data to train compared with CNNs, as they lack strong inductive biases. The widely used FER datasets in the wildare much smaller than general image classification datasets like ImageNet. Consequently, pre-trained Transformers and even CNNs for face recognition task are utilized by prior workfor satisfactory FER performance. It has been experimentally demonstrated that MAE pre-trainingcan effectively help ViTs learn a wide range of visual semantics and acquire excellent performance in different downstream tasks.

. We train ViTs as Masked Autoencoderson AffectNetdataset (the largest one for FER task publicly available up to now) without noisy and biased emotion labels, enabling us to obtain stronger representations for expressive faces. Letdenote the ViT encoder,represent the lightweight ViT decoder, anddenote the learnable mask tokens introduced in MAE structure. The corresponding configurations of decoderused in our experiments are provided in Table.For facial expressions, different emotions often involve shared facial muscle movements. Therefore, during the pre-training stage, facial expressions with low occurrence frequencies in datasets, for example, fear, can benefit from the structure learning of more frequent facial expressions, for example, happiness. The MAE pre-training encourages the encoder to learn both local and global facial structures by reconstructing the missing pixels of randomly masked image patches, which is achieved by minimizing the loss as follows:

whereis the token-level random mask that decides where to drop the image patches, anddenotes element-wise multiplication. Thus, the mask ratiois computed as:

As a consequence, only a subset of the image patches from the input are visible to the lightweight MAE decoder.
Following He et al., we reconstruct the normalized pixel values of each masked image patch as the target output.

.
After completing the MAE pre-training, we append a single-layer linear classifier to the outputtoken of each pre-trained ViT encoder variant and proceed with fine-tuning on specific FER training sets. Subsequently, we evaluate the performance of these classification ViTs on FER testing sets, establishing our ViT baselines for FER. All training configurations remain consistent with those in MAEunless mentioned otherwise. For convenience, we refer to these fine-tuned ViTs as MAE-ViTs.

SECTION: 
The framework ofokeraceE-AN () is shown in Fig., from which we can detachokeraceisionransformer () for simultaneous facial expression recognition (FER) and poker face generation. PF-MAG is composed of PF-ViT and a composite discriminator. PF-ViT consists of four key components: 1) an image encoder, which maps the input face imageto a representation, which is a sequence of visual tokens encoding all the information about the facial image, 2) a token separator, which decomposes the representationtoken-wisely into a pure emotional componentand an orthogonal residue, 3) a image generator, responsible for reconstructing the original input and synthesizing the emotionless poker face usingand mask tokens, 4) a classification head, which infers the separated emotion component. The discriminatordistinguishes the facial expression image generated byand is trained adversarially with PF-ViT. During testing, bothandare removable.

PF-MAG greatly benefits from the MAE pre-trainingconducted in Sec..
In this case,is simply aof the MAE encoder,andare the copies of the MAE decoder. Also, we reuse the mask tokens, with each mask token representing a shared, learned vector.
Assuming, whererepresents the embedding dimension of each token, andcorresponds to the total number of image tokens plus atoken.
Bothandutilize self-attention operations on the latent tokens. By contrast,utilizes cross-attention operations on the decomposed latent tokens(or) and mask tokens.
The mask tokens introduced in MAEindicates the presence of a missing image patch to be predicted, whilewe use mask tokensto prompt the generatorof PF-ViT to “edit” all the image patches of the input face.
Our experiments suggest that mask tokensare crucial forto successfully reconstruct the original face and generate the poker face simultaneously.
The emotional features can be enhanced by comparing the synthesized poker face and original input face.

Letdenote an in-the-wild FER dataset, in which each facial imagebelongs to one of thebasic expression categories, andrepresents the ground-truth label of. Supposing the neutral label in the current FER dataset is.
The ViT encodermaps the flattened 2D patchesreshaped from the input imageto the latent representation, briefly denoted by:

whereis a trainabletoken taken as image represent,represents the linear projection layer, anddenotes the fixed 2D sin-cos position embeddings.

The token separator S incorporates the standard squeeze and excitation block (SE)to generate a feature selection mask. However, we modify the activation function in the SE block from ReLU to GELU, resulting in a modified version known as Adapter.
The token separatordecomposes the representationtoken-wisely into an emotion-relevant componentand an orthogonal residue. During training, we encourageto contain pure emotional features yetto retain the details of the input image except for the facial muscle movements, as shown in Fig.. This process is simply described as follows:

whererepresents the sigmoid function,is the resulting element-wise selection mask for the emotional features, and we forceandto be perpendicular in the latent space. Although the selection mask is applied independently to each token of, the earlier multi-head self-attention and MLP blocks within ViT make sure each output token has already captured the global information across all other tokens.

The image generatorreceives the visual tokens decomposed by the separator(i.e.,and), as well as the mask tokens borrowed from MAEs, as input. It then produces two outputs: the reconstructed face image, which retains the original emotion of the input, and the synthesized poker face, which is an emotionless version of.
This computation can be expressed as follows:

in whichis a cross-fusion module implemented by a multi-head self-attention block () and mask tokensplay the role of query matrix. The cross-attention mechanism is employed to capture the relationships among the input tokens,,, which is defined as:

whereis the embedding dimension per attention head used in the lightweight ViT generator, and,,are projected from the three input token sequences described in Eqn., respectively. As mentioned earlier, the mask tokensalong with the position embeddingsguide the generatorto predict all the image patches with or without emotion content, according to the specific requirements. In this case, all image pathes of the input are visible to the generator.

The discriminatorplays a dual role in our GAN model, PF-MAG:

It distinguishes between the synthesized facial images (,) and the real facial image (),

It recognizes the expressed emotions in images using a multi-label classification manner.

As illustrated in the right of Fig.,has a total ofoutput units. The firstunitis are dedicated to class-wise emotion discrimination, with each unit representing one of thebasic emotion categories. And the last unit is responsible for discriminating between real and fake images. The proposed multi-output structure is crucial.
If we were to discard the last output unit responsible for real/fake image discrimination,the generatorcould potentially exploit a “shortcut” to deceive the discriminator. In such a scenario,might prioritize masking important local regions of the input face that convey emotional information, resulting in a “masked” facial image that appears emotionless tobut does not look real.
Bothandare lightweight ViTs derived from the MAE decoder pre-trained in Sec.. However, they are trained to fulfill different roles in the GAN model.

Finally, the single-layer linear classifierpredicts the facial expression class based on thetoken in the separated emotion representation. The inferred emotion class is determined as follows:

During training, the classifieris supervised to output the ground-truth emotion, ensuring thatcontains underlying emotional factors.

The decomposed representationis fed into the classifier, encouraging the encoderand separatorto extract and separate fine-grained emotion representations.
The cross entropy loss is applied to supervise the-emotion classification task:

whereis the indicator function, which equals 1 if the condition is true, and 0 otherwise.

The generatordecodes the tokens processed by the cross fusion moduleinto synthetic facial images, namelyand, which are regarded as fake images by the discriminator.
And we define the discrimination loss as follows:

where, andif the input is a real image, otherwise, indicating the current input is a generated image. Forin the equation above, we set:

With the supervision of loss, the discriminatorlearns to recognize the emotion displayed in the real facial image whenand further guides the generatorto generate the corresponding neutral faceas realistically as possible or high-quality reconstructionof the original input face.
During training, when,the discriminatorplays an adversarial game with PF-ViT(strictly speaking, its encoder, separatorand generator).

To alleviate the challenge of generating facial expressions without paired images, we introduce a reconstruction loss:

This loss function comprises two terms. The first term measures the pixel-level difference between the reconstructed faceand the original input face. Meanwhile, the second term focuses on the discrepancy between the generated poker faceand the original input face and only applicable when the current input is a neutral expression with class label.

The face reconstruction task described in Eqn.can provide pixel-level supervision signals, favoring the representation learning for both the encoderand generator. Additionally, it serves as a constraint for the token separator, ensuring that the combination of the decomposed components, i.e.,, preserves all the essential information of the input face, as described in Eqn..

During training, the image encoder, token separatorand FER classification headof PF-ViT are trained collaboratively to extract discriminative expression features,the loss. FER supervision signals applied toprevent the information collapse in the separated emotional feature space.
The discriminatorof PF-MAG is trained adversarially with,and the generatorof PF-ViT:tries tothe losswhen, while,andmake best toit when.
Meanwhile, the orthogonal constraint in Eqn.aids in the separating ofand.
In order to fool the discriminator, the synthesized poker faceneeds to resemble a true neutral face, and the reconstructed faceshould look nearly the same as the original input(refer to Eqn.).

As a result, the representationretains all the facial details except for emotion, whereas the representationonly contains the emotional information.
In other words, the representation space ofprimarily emphasizes and extracts the emotional component. The inclusion of the auxiliary task of image generation compels our FER model to capture more facial and emotional details.

To the best of our knowledge, training vanilla ViT-based GAN for image generation is quite challenging, even with sophisticated techniques. Fortunately,PF-ViT with the face generation task can be easily trained by leveraging our MAE representations(including the mask token) learned from a large number of facial expression images without labels.

SECTION: 
SECTION: 
is a widely used in-the-wild FER dataset that contains 29,672 facial images collected from the Internet. Each sample in the dataset is independently labeled by around 40 annotators. RAF-DB provides labels for seven basic expressions and eleven compound expressions. In our experiments, we focus on the seven basic expressions, namely neutral, happiness, surprise, sadness, anger, disgust, and fear, involving 12,271 images for training and 3,068 images for testing.

is currently the largest real-world FER dataset publicly available containing over 1 million (1M) facial images collected from the Internet. It holds manually and automatically labeled samples. And our experiments use the manually labeled images, following the convention in prior work. There are two different choices for result comparison denoted byand, considering the said seven basic expressions and the seven basic expressions plus contempt, respectively. AffectNet-7 includes 283,901 images for training and 3,500 images for testing, while AffectNet-8 contains 287,568 images for training and 4,000 images for testing.

is an extended version of FER2013, including 28,709 training, 3,589 validation and 3,589 testing grayscale images, collected using the Google search engine. Each image in the dataset, with the spatial extent of 4848 pixels, is labeled by 10 annotators to one of the eight emotion classes (the seven basic expressions plus contempt). The validation set is commonly used for model training as well.

SECTION: 
Following the conventional practice, we perform in-the-wild FER in two steps: (1) the face is detected and aligned, and further resized to 224224 pixels, (2) the facial expression is classified.
We implement our method using Python and PyTorch, and use automatic mixed precision (Amp) supported by NvidiaApexPyTorch library to seep up the experiments.

.
AffectNet dataset contains 400,000 manually labeled images. We clean the dataset based on the label file and perform face alignment using OpenFace. This results in 269,316 facial images for our self-supervised MAE pre-training, dubbeddtaset.
Each variant of ViT (ViT-Tiny, ViT-Small and ViT-Base) is pre-trained as the MAE encoder for 300 epochs. Subsequently, the pre-trained ViTs are fine-tuned on the downstream FER training setsrespectively, using the Adam optimizer with a batch size of 32 and an initial learing rate of 1e-4. During training, we reduce the learning rate by a factor of 10 if no improvement is observed for 30 epochs. Every training process is finished within 100 epochs. Common data augmentation techniques, such as random scaling and flipping, are employed unless mentioned otherwise.
In addition, we train and evaluate the ViT variants initialized with the regular ImageNet1K weightsprovided by Steiner et al.for the FER task, obtaining more baseline results.

.
According to our experimental findings, directly applying the orthogonal constraint in Eqn.toandimmediately after the token separatorfails to yield satisfactory disentanglement. To address this issue, we leverage the pre-trained Incept-ResV1, a face recognition (FR) model introduced by Schroff et al., to extract the facial features of the images generated bybased onand. Then, these extracted features are flattened and used to comput a cosine similarity, which indirectly measures the distance betweenandin the off-the-shelf latent space of the well-trained FR model. The pre-trained FR model is fixed and incorporated into the orthogonal constraint loss during training. 
It’s important to note that we reuse the pre-trained MAE encoders and decoders, as described in Table, to construct our GAN model, PF-MAG.

The encoder, separator, generator, and classifierare optimized using the Adam optimizer with an initial learing rate of 2e-5. The discriminatoris optimized using the Adam optimizer with,, and an initial learing learning rate of 1e-4. We approximately balance the different loss terms according to their magnitudes. The remaining training configurations follows our ViT baselines described earlier.
It is worth noting that we did not employ grid search to subtly optimize the hyperparameters for each dataset. Nevertheless, we have achieved decent accuracy performance.

“SL” represents the common supervised learning method for pre-training, while “MAE” stands for the self-supervised MAE pre-training. The models above undergo initial pre-training and subsequent fine-tuning for the task of FER.

SECTION: 
.
We show the FER fine-tuning results of the pre-trained ViTs in Table, whose initial representations are obtained by conventional supervised learning (SL) or self-supervised MAE learning(MAE).The MAE representation brings about remarkable and consistent accuracy improvementsto ViT-B on the RAF-DB (+3.35%), AfectNet-8 (+3.77%) and FERPlus (+2.51%) testing sets, compared to the widely used SL representations learned on ImageNet1K dataset (the second row vs. the first row).
The FER model based on the ViT-B variant pre-trained on AffectNet-270K dataset using the MAE method achieves respectable accuracy numbers: 90.22%, 61.73% and 89.00% on the RAF-DB, AffectNet-8 and FERPlus testing sets, respectively, without data augmentation.

Based on the comparison between the second and third entries in Table, it can be concluded that the ViT-B FER model pre-trained with MAE self-supervised learning on a facial dataset (AffectNet-270K) significantly outperforms the one pre-trained with MAE self-supervised learning on a general image recognition dataset (ImageNet1K). It is worth noting that the AffectNet-270K dataset we utilize only contains about 1/5 of the images compared to the ImageNet1K training set.
This suggests thata more relevant representation, which includes facial knowledge, is superior to a more general image representation.
Furthermore, the accuracy numbers of the ViT-B model increase to 91.07% on RAF-DB, 62.42% on AfectNet-8, and 90.18% on FERPlus testing sets, respectively, when data augmentation is employed. This observation aligns with the fact that Transformers are known to benefit from larger amounts of data, indicating their “data hunger”.

Benefiting from the strong facial expression representations obtained through MAE pre-training using AffectNet-270K images without noisy emotion labels, vanilla ViTs are able to achieve decent FER performance, which we refer to as MAE-ViTs for simplicity. Several MAE-ViT and SL-ViT baselines based on different ViT variants (ViT-B, ViT-S and ViT-T) are provided in Table. The experiments consistently demonstrate that “AffectNet-270K-MAE” representations outperforms the commonly used “ImageNet1K-SL” representations across different ViT backbones. Even with data augmentation techniques, the “AffectNet-270K-MAE” representation still leads to significant accuracy improvements of +3.85% on the RAF-DB, +4.14% on the AffectNet-8, and +1.27% on the FERPlus testing tests when ViT-B is utilized (comparing the 6th row and the 3rd row of Table).

Incidentally, in Fig., we show some reconstruction examples of the maksed facial images produced by MAE models based on ViT encoders and decoders of different scales. These MAE models are trained from scratch using AffectNet-270K dataset. It can be seen that MAE models are very good at reconstructing the missing pixels of the masked patches of the original facial image. The details of the MAE encoders and decoders configurations are presented in Table.

.
We show the evaluation results of the proposed PF-ViT based on ViT-B, ViT-S and ViT-T in Table, leveraging our MAE representations learned from AffectNet-270K data. In contrast to the fine-turning results of SL-ViT baselines in Table, large accuracy gaps exist among the PF-ViT models with the three ViT variants as the image encoder. This indicates thata moderately-sized ViT model with adequate capacity is more suitable for PF-ViT to effectively perform poker face generation and emotion classification simultaneously. The inclusion of the auxiliary task of poker face generation further improves the performance of our PF-ViT model, compared with MAE-ViT.
Comparing the first and last rows in Table, PF-ViT outperforms MAE-ViT by 1%, 1.68% and 0.98% in accuracy on the RAF-DB, AffectNet-8 and FERPlus testing sets, correspondingly.
The confusion matrices of PF-ViT are computed and presented in Fig..
Additionally, in Appendixof our supplementary material, we show several typical examples of misclassifications made by PF-ViT. These errors primarily arise from the ambiguity present in facial expressions.

To help understand how PF-ViT works, we visualize the emotional component (denoted as “Emotion”) and emotionless poker face (denoted as “PokerFace”) in theof Fig..
This visualization is achieved by separately feeding the emotional componentand emotionless componentinto the generatorof PF-ViT.
By comparing the Original and PokerFace images, it can be seen that the generated poker faces retain the emotion-irrelevant factors of the original input images while the emotional details are almost eliminated.
Thanks to the orthogonal constraint applied toand, described in Eqn., the visualized Emotion images exhibit significant differences compared to the Original and PokerFace images, even though some facial residues are sitll restored by the generatorfrom.
Incidentally, the reconstruction loss defined by Eqn.helps stableize the the adversarial training of poker face generation, for it provides pixel-level supervision signals from paired images.

Just for reference, the poker faces generated by our PF-ViT, based on ViT-B, obtain an FID score (Fréchet Inception Distance)of 100.38 when compared to the real faces on RAF-DB dataset.
However, the FID score may not reliably indicate the quality of the generated facial images.In cases where the de-expression process totally fails, the FID score may improve significantly. This is because the generated poker face would look nearly the same as the original input, leading to a much smaller distribution distance between the generated and real images.

Enforcing strict orthogonality between the decomposed componentsandin the high-dimensional spaceimmediately after the token separator, even though the cosine similarity (absolute value) between them significantly reduces during training. However, the poker faces synthesized by the generatorstill retain noticeable emotional details in this scenario.
To evaluate the effectiveness of the FR model (Incept-ResV1) in measuring feature distance, we conducted ablation experiments by either replacing it with a image classification ResNet pre-trained on ImageNet1K dataset or removing it entirely. The results are presented in Table.
According to the first row of the results, applying the orthogonal constraint toanddirectly leads to a significant decrease in accuracy on the RAF-DB and AffectNet-8 testing sets.
Referring to the last entry in Table, utilizing the pre-trained FR model to firstly extract the high-level facial features from the images derived fromandbefore applying the orthogonal constraint significantly improves the accuracy by large margins, compared to other ResNet models used for general image feature extraction.

Further,to demonstrate the necessity of token separation in our method, we remove the token separatorand conduct the ablation experiments as outlined in Table, in which only original and reconstructed images are utilized for training. Consequently, PF-ViT degrades to the original Masked Autoencoder (MAE). Here, we train and evaluate two variant GAN frameworks: 1) the MAE model trained adversarially with a discriminator, which only identifies real and fake images (presented in the second row); 2) the MAE model trained adversarially with a discriminator, which not only distinguishes between real and fake images but also verifies whether the facial expression of the reconstructed image is consistent with the original face (presented in the third row). The results of these two variants exhibit noticeable decreases in accuracy, with reductions of more than 1.4%, 3.5%, and 1.5% accuracy on the RAF-DB, AffectNet-8, and FERPlus testing sets, respectively. In addition,enforces emotional consistency during the reconstruction of facial images in training, and results in a slightly higher FER accuracy compared to. Overall, the ablation experiments in Tableindicate that direct adversarial training with the discriminator (or) is not beneficial for the final FER model to capture nuanced expressive features but rather introduces negative factors.

Similar to the mask tokens introduced in MAE, our mask tokensalong with the position embeddingsindicate to the generatorwhere to “edit” the image patches when only the emotion-irrelevant token sequenceis provided. Interestingly,the generatorcannot remove emotional contents from facial images without utilizing the mask tokens. In this case, themodule, described in Eqn., degrades to amodule. Then, we retrained our GAN model, but the adversarial training did not converge, and feature disentanglement failed. As a result, bothandretained the input facial contents. The resulting examples generated fromare shown in theof Fig..

We firmly believe thatall image patches of the input face are crucial for accurate and fine-grained facial expression recognition. To validate this belief, we present a multi-task MAE (Masked Autoencoder) model for simultaneous poker face synthesis and emotion classification, as detailed in Appendixof our supplementary material. During training, the multi-task MAE only observes a subset of the image tokens, resulting in inferior performance compared to PF-ViT. This further reinforces the effectiveness and superiority of the PF-ViT model in capturing all facial information and achieving superior performance in both poker face synthesis and emotion classification tasks.

In our experiments, we have observedfluctuations in FER accuracy. The varying accuracy can be attributed to several factors: 1) the current in-the-wild FER datasets, such as AffectNetand RAF-DB, are significantly class-imbalanced, 2) noisy emotion labels are prevalent in these datasets, introducing additional challenges for accurate classification, and 3) the limited size of the testing set can contribute to variations in accuracy. In Fig., we present the mean and standard deviation of the overall accuracy across 3 additional runs per dataset and per model, which are trained for the same epochs as the corresponding entries listed in Table.
Here, we provide cross-dataset evaluations following. The PF-ViT model trained on AffectNet-8 achieves 73% accuracy on RAF-DB, while the model trained on RAF-DB drops to 47.8% when tested on AffectNet-8.

. We show the attention maps of our different models based on ViT-B in Fig., where SL-ViT and MAE-ViT represent the plain ViTs pre-trained using ImageNet1K data and AffectNet-270K data, respectively, which are selected from Table.
The attention maps are generated usingGradCAM++and resized to match the original input size. It can be concluded that the proposed PF-ViT pays more attention to the global information and captures more expressional details compared to SL-ViT and MAE-ViT, allowing for a comprehensive understanding of different facial expressions. Moreover, this characteristic helps alleviate the issue of overfitting specific facial regions cased by the dataset bias, enhancing the model’s generalization capability.

We further visualize the high-level features of the RAF-DB dataset using t-SNE in Fig.. The feature embeddings are extracted from the penultimate layers before the classification heads of SL-ViT, MAE-ViT and PF-ViT, respectively. From the visualization, it can be conclude that our PF-ViT has learned more clusterable and discriminative expression features from the disentangled emotional component, leading to more accurate FER performance.

SECTION: 
. In Tableand Fig., we compare our MAE-ViT and PF-ViT, both utilizing the ViT-B backbone as thedefault option, with the existing SOTA FER methods. The comparison involves metrics such as accuracy, computational complexity during testing, and model size.
Equipped with MAE representations learn on the introduced AffectNet-270K datast, vanilla ViTs achieve competitive FER performance without the need for large-scale upstream datasets for representation learning. As shown in Fig., our MAE-ViT baseline, based on ViT-T and comprising only 5.6M parameters, is even comparable to the earlier advanced FER methods. For reference, the widely used lightweight ResNet-18 backbone, which is included in Table, contains approximately 12M parameters.

During inference for the FER task only, the image generatorof PF-ViT can be removed. Thus,is not taken into account when computing the computational complexity of our PF-ViT, as shown in Fig..
Based on the results presented in Table, ourPF-ViT has achieved a good trad-off between accuracy and computational cost, seting a new state of the artwith the highest accuracy numbers of 92.07%, 67.23%, 64.10% and 91.16% on the RAF-DB, AffectNet-7, AffectNet-8 and FERPlus testing tests, respectively.
Compared to the pre-trained MAE encoder detailed in Table, the additional classification headand token separatorin PF-ViT only contribute a small increase in the number of model parameters () or computational cost (FLOPs).
Finally, our results demonstrate the high potential of plain ViT models for the task of FER.

. During the training phase, the inclusion of the cross-fusion module, generator, discriminatorcontributes to increased training cost. We compare our entire GAN framework, i.e., PF-MAG, with other SOTA works in Table, focusing on model parameters and computational complexity during the training stage. For fair comparison, all trainable components of the frameworks in the table are taken into account. Considering we are the first to apply pure ViTs for FER and have achieved the best accuracy performance, our framework’s parameter size (145.1M) and computational cost (15.2 GFLOPS) are still tolerable to some extent, despite this shortcoming.
Fortunately, Our final FER model comprising only (,,), with 89.7M parameters and 9.0G FLOPs as illustrated in Fig., can obtain high testing speed on both 2080Ti and 3090 GPUs in practical settings. Tablepresents the practical speed metrics.
Keep in mind that FLOPs (Floating Point Operations) does not always accurately depict the actual speed of running on a GPU.

SECTION: 
In this paper, we firstly demonstrate that vanilla ViTs with MAE pre-training are able to achieve competitive performance for the task of FER, without the need for extra labeled data.
Then, we propose PF-ViT based on vanilla Transformer backbones to simultaneously perform facial expression classification and poker face generation, although block artifacts may exist in the generated images. Our quantitative and qualitative results suggest that the introduced auxiliary task of poker face generation guides PF-ViT to capture fine-grained facial features holistically, leading to improved FER performance. PF-ViT is trained using unpaired images in a GAN framework, and during testing, its image generator can be removed to avoid
additional computational cost. Compared to previous methods, our method has achieved a good balance between accuracy and inference cost, setting a new state of the art on in-the-wild FER datasets.
In future work,
it would be interesting to investigate the feasibility of transfering expressions to tackle the challenge of class imbalance in real-world FER datasets by extending our method, as the emotional component can be effectively separated and plausible emotionless faces can be synthesized.

SECTION: Failure examples produced by Our PF-ViT
Some ambiguous facial expressions, which are difficult to classify, are shown in Fig.. The inference scores for the top-2 predicted classes are displayed below the images and thelabels are annotated within the images.

SECTION: Multi-Task MAE for Simultaneous FER and Poker Face Generation
Here, we present a variation of PF-ViT called Multi-Task MAE, which attempts to follow the computation flow of Masked Autoencoders (MAEs). The structure of Multi-Task MAE is illustrated in Fig.. In PF-ViT, all image tokens from the input are utilized, and a multi-head cross-attention block (denoted as “Cross Fusion” in Fig.) is employed to process the visual tokens and mask tokens (obtained from our earlier MAE pre-training). By contrast, Multi-Task MAE only sees the visible subset of the orgianl image patches and utilizes a multi-head self-attention block (denoted as “MSA” in Fig.) to process the visible tokens and mask tokens, obtaining a latent representation that is more aligned with the practice in MAEs, thereby narrowing the gap between the upstream MAE pre-training and our specific application.

We train Multi-Task MAE with random masking on the input image. To investigate the influence of different mask ratio values, we have tried various options, namely 0 (no mask tokens), 0.3, 0.5, and 0.75. The results on the RAF-DB testing set are summarized in Table, demonstrating that a smaller mask ratio leads to higher FER accuracy during testing, while a moderate value ofperforms better during training. These findings emphasize the significance of choosing an appropriate mask ratio for Multi-Task MAE. Additionally, we show some qualitative results in Fig..

The, denoted by, is defined in Eqn.. An MAE model pre-trained with a mask ratio can be appliedto input images with a different mask ratio during testing.

Based on the results shown in Tableand Fig., we can conclude that PF-ViT (with an accuracy of 92.07% on the RAF-DB testing set), which utilizes all the image patches to capture all the facial details, significantly outperforms Multi-Task MAE in terms of the facial expression classification accuracy and the image quality of synthesized poker faces. Multi-Task MAE with the best accuracy is even inferior to our MAE-ViT baseline, which utilizes the ViT-B backbone (91.07%).

SECTION: References