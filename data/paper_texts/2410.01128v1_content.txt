SECTION: Using Interleaved Ensemble Unlearning to Keep Backdoors at Bay for Finetuning Vision Transformers
Vision Transformers (ViTs) have become popular in computer vision tasks. Backdoor attacks, which trigger undesirable behaviours in models during inference, threaten ViTs’ performance, particularly in security-sensitive tasks. Although backdoor defences have been developed for Convolutional Neural Networks (CNNs), they are less effective for ViTs, and defences tailored to ViTs are scarce. To address this, we present Interleaved Ensemble Unlearning (IEU), a method for finetuning clean ViTs on backdoored datasets. In stage 1, a shallow ViT is finetuned to have high confidence on backdoored data and low confidence on clean data. In stage 2, the shallow ViT acts as a “gate” to block potentially poisoned data from the defended ViT. This data is added to an unlearn set and asynchronously unlearned via gradient ascent. We demonstrate IEU’s effectiveness on three datasets against 11 state-of-the-art backdoor attacks and show its versatility by applying it to different model architectures.

SECTION: Introduction
Vision Transformers (ViTs,) have emerged as a powerful alternative to Convolutional Neural Networks (CNNs) for a wide range of computer vision tasks. ViTs have achieved state-of-the-art performance in various downstream tasks such as image classification, object detection, and semantic segmentation. However, the widespread deployment of ViTs have also raised concerns about their vulnerability to adversarial threats, particularly backdoor attacks, which typically modify images and/or labels in the training dataset to trigger attacker-controlled undesirable behaviour during inference. Backdoor attacks such as the BadNets attack incan compromise model behavior by embedding malicious triggers during training, leading to security risks in real-world applications. As ViTs become increasingly popular in security-sensitive domains such as autonomous driving and face recognition, it is important to understand these vulnerabilities and develop robust backdoor defences for ViTs.

ViTs are often pretrained using self-supervised learning (SSL) on large datasets and then finetuned to be deployed on specific tasks. Backdoor defences have been proposed to defend foundation models pretrained on large datasets and can either prevent backdoor injection during the SSL process or encourage removal of backdoors after pretraining; these thwart backdoor attacks that occur during pretraining, such as a practical real-world attack on web-scraped datasets inand an SSL-specific imperceptible attack. The finetuning process for adapting ViTs to downstream tasks using supervised learning is equally vulnerable to backdoor attacks. The rationale behind developing ViT-specific defences for finetuning are two-fold:shows that there are few defences specifically designed for ViTs for image classification in existing literature (andbeing notable examples of such defences); in addition, although existing defences designed for CNNs can defend ViTs after modifying the defence implementations, they still lead to high ASR and/or low CA when defending different flavours of ViTs (Tables 4 & 5 in).

To fill the gap, this work takes advantage of theand propose a novel backdoor defence that uses an ensemble of two ViTs to perform interleaved unlearning on potentially poisoned data. Informed by the designs inand, we use a shallow ViT denoted by the “poisoned module” to defend the main ViT, which we call the “robust module”. Our designhas two stages as shown in. In, the poisoned module (), a shallow ViT, is tuned on the attacker-controlled finetuning data. Intuitively, shortcut learningleadsto learn shortcuts in the dataset, which are most prevalent in poisoned images. In addition, the simplicity ofdiscourages it from learning clean data that have fewer shortcuts. Therefore, the poisoned module is confident (where confidence is the maximum class probabilitypredicted by) when classifying poisoned data and not confident otherwise. In, images pass through the tuned, which either queues data onto the unlearn set or allows the defended main ViT (the robust module) to learn data normally based on the confidence threshold. Whenever the unlearn set accumulates enough potentially poisoned data, a batch is unlearned by the robust module using a dynamic unlearning rate.

Instead of using a pre-determined unlearn set, our defence accumulates the unlearn set during stage 2. The benefits are two-fold: compared to using ABL’smethod which isolates poisoned samples using the defended model, finetune-time unlearn set accumulation usingensures that the robust module learns as little poisoned data as possible; in addition, online accumulation ofis adaptive in the sense that the frequency of unlearning is high when more potentially poisoned images are encountered, quickly erasing the impact of a large number of poisoned data. In addition, we argue that core concepts developed in our method, namely applying interleaved ensemble unlearning, can defend other model architectures in image classification. Here are our main contributions:

We propose a new backdoor defence method, IEU,during finetuning. The defence uses an ensemble of two models to perform interleaved unlearning of high-confidence data using a dynamic unlearning rate. We show that our interleaved ensemble unlearning framework is

We argue that, in contrast to ABL, a fixed-sizedis not suitable for our design and demonstrate that Local Gradient Ascent (LGA) and loss Flooding presented in ABLare orthogonal to our poisoned module (). We show that the interleaved unlearning framework serves as a model-agnostic alternative to other unlearning-based defences ().

We empirically demonstrate that our design out-performs existing state-of-the-art defences on challenging datasets using 11 backdoor attacks by comparing to SOTA methods such as ABL and I-BAU; Attack Success Rate (ASR) improved byandon average for TinyImageNet and CIFAR10, respectively, while maintaining high Clean Accuracy (CA). Our method requires no clean data and we perform extensive ablation studies.

We explore potential points of failure of unlearning-based defence mechanisms to defend againstattacks where “weakness” corresponds to lower ASR. We propose potential solutions to address these failures. In our opinion, weak attacks are as insidious as powerful attacks.

SECTION: Related work
. The goals of backdoor attacks in image classification tasks are (a) to encourage a specific classification when the image is perturbed by an attacker-specified transformation and (b) to maintain normal performance when images without a backdoor trigger are classified. A great variety backdoor attacks of all flavours for both SSLand supervised learninghave been proposed. There are three categories of backdoor attacks for supervised learning, which is the learning phase that this work defends: dirty-label attacks which includes visible and invisible attacks, clean-label attacks which do not modify the label of backdoored images, and clean-image attacks which only modify data labels during learning. Authors have also developedbackdoor attacks. For example,inserts a Trojan into a ViT checkpoint, whilemodifies the finetuning procedure by using an attacker-specified loss function.

. The defender’s goal is to ensure that backdoor images do not trigger attacker-specified model behaviour whilst maintaining high CA. A popular class of defences is model reconstruction where defenders cleanse poisoned models of backdoors. Early worksin this area such as the fine-pruning defence prunes neurons that are dormant on clean images and tunes on clean data. Later works that aim to remove backdoor neurons include Neural Attention Distillation (NAD,), Adversarial Neuron Pruning (ANP,) Adversarial Weight Masking (AWM,), Shapley-estimation based few-shot defence, and Reconstructive Neuron Pruning (RNP,). Another such cleansing defence, I-BAUconnects the two optimisation problems in the minimax formulation of backdoor removal using an implicit hypergradient. Authors invariably design insightful methods for recovering clean models after backdoor injection. Certified backdoor defences have also been developed. Another broad class of defences involves reconstructing the trigger in order to unlearn backdoor images. Notable examples include Neural Cleanse, DeepInspectwhich checks for signs of backdooring without a reserved clean set, and BTI-DBFwhich decouples benign features for backdoor trigger inversion. Mitigating backdoor attacks during tuningis also popular. Our method is closely related to Anti-Backdoor Learning (ABL,), which isolates poisoned data and unlearns them after training.

. Few backdoor defences are specifically designed for defending ViTs during tuningand existing defences that have CNNs in mind perform worse on ViTs. Two notable defences arewhere backdoor images are identified using patch-processing, andwhich is a test-time defence that uses GradRollout (, an interpretation method for ViTs) to block high-attention patches in images.

SECTION: Method
In this section, we first define our threat model and describe IEU in detail. We conclude this section by briefly exploring the drawbacks of using a fixed-size unlearn set.

. We focus on finetuning ViTs for image classification tasks and assume that the pretrained model checkpoint initially given to the defender is not benign. We follow the threat model of. We assume that the finetuning procedure is controlled by the defender, which means attacks that modify finetuning loss or the model’s gradientare out of scope. On the other hand, finetuning data is gathered from untrusted sources and may contain backdoor data. The attacker knows the model architecture and the pretrained checkpoint’s parameter values, and may poison the finetuning dataset by modifying images and/or labels. The defender aims to tune a benign checkpoint for downstream tasks usingand does not know the distribution/proportion of backdoor data in the attacker-supplied.

. The finetuning setcontain an unknown proportion of backdoor samples; this proportion (i.e., poisoning rate) is denoted by. The defender unlearns data in the unlearn set, whose size as a fraction ofis defined as. The two sub-networks in the ensemble are the poisoned module and robust module, denoted byand, respectively. Data points, which consist of unaugmented () and augmented () views of the original image (as in “data augmentation”), and potentially poisoned data pointsare used to finetune and defend, respectively. For simplicity, we useto denote images when data augmentation is not relevant. The logits produced by the two modules are referred to asand, whereare the potentially tunable parameters ofand, respectively. The two logits vectorsandcombine to form the logits vectorbased on(Equation). We useandto represent the softmax function and the cross-entropy loss, respectively. The confidence thresholddetermines whether an image is asynchronously unlearned or immediately learned. The number of classes inis denoted byfor CIFAR10, GTSRB, and TinyImageNet, respectively. The learning rates used to finetuneand unlearnareand, respectively.

SECTION: Interleaved Ensemble Unlearning ()
summarises our method, which has two stages. During stage 1, the poisoned moduleis pre-finetuned using finetuning data. During stage 2,is used to determine whether incoming data is learned by the robust moduleor added tofor asynchronous unlearning based on the unlearn ratein Equation.

. This step applies vanilla tuning (for hyperparameters seein) usingon, whereis initialised as the first few layers of a pretrained checkpoint. Stage 1 solves the following optimisation problem:, whereis the cross entropy loss,is the one-hot ground truth vector, andis the poisoned module. As explained in, the intuition of overfittingon poisoned data is based on shortcut learning. These shortcuts are found overwhelmingly in unaugmented backdoored images, where the attacker-specified trigger acts as an easily identifiable artifact that causesto easily learn the connection between the trigger and attacker-specified label. Therefore, the tunedis likely to be confident when predicting images with the trigger. The poisoned moduleis designed to be complex enough to learn shortcuts and shallow enough to avoid learning much from benign data. The goal is forto be small for clean images and large for backdoor images after tuning in stage 1.

. This stage applies defended finetuning onwhich optimises the objectives in Equationsand;is initialised as a pretrained checkpoint. During this stage,is frozen and onlyis tuned. The logitsfor the unaugmented view of each image is produced by the poisoned module in order to compute maximum class probability, which is then compared toto determine whether the data point should be learned byor added onto. Ifis above the confidence threshold, the data point is added to. Otherwise,learns the augmented views as would happen in regular finetuning. To prevent potentially poisoned data from being learned, we applyon the output logits ofandas shown in Equation

whereis the binary logit mask,is the indicator function,is the ground truth,is the logits vector, andare the logits produced byand, respectively. Whenis detected byas a potentially poisoned image, the logits for optimising the objective in Equationcome from; otherwise,for optimisation. In other words, optimising the objective in Equationrequires bothandto contribute to the logits.

The unlearn setaccumulates data until it has enough data for one batch containing potentially poisoned images, which is then unlearned byduring finetuning. Unlike the continuously decaying learning rateused for normal finetuning, the unlearning ratedoesn’t depend on just the decay schedule. Givenwhich follows the cosine annealing decay schedule, thebatch with potentially poisoned images, the number of classes in the dataset, and the robust module, the dynamic unlearning rate for the current batchis defined in Equationand can be viewed as a function of cross entropy loss of the previous batch of potentially poisoned images.

The two indicator functions ensurewhen, which occurs at an epoch’s beginning. The term that scalesin Equationis an exponentially decreasing function with respect to increasing loss, causingto be large when the previous batchproduces low cross entropy loss on. This keeps the backdoor from being learned by. Although it is shown inthat usingfor someperforms better than using Equation, one benefit of definingusing a fixed function is thatis no longer a hyperparameter that needs to be tuned. In addition to solving Equation, finetuning during stage 2 also optimises for the objective in Equation, which is implemented using gradient ascent performed ongiven. Seeinfor a precise description of stage 2.

SECTION: Why Not a Fixed-Sized Unlearn Set?
In this subsection, we argue that our method of usingto isolate poisoned data is orthogonal to similar methods used in ABLand is more suitable for our IEU. Authors in ABL isolates a fixed fraction (called “”) of the tuning set where(they used). In their method, images whose cross entropy loss rank amongst the lowestfraction ofis collected to formof size. ABL uses techniques such as Local Gradient Ascent (LGA) or loss Floodingto encourage poisoned images to have low loss. Compared to using(ABL), there are two main advantages for using(our method) to produce the unlearn set in our method: (a) the effectiveness (FPR or FNR) of our isolation method is not significantly affected by the value of the poisoning rate, which is unknown to the defender (), and (b) the unlearn set size varies asvaries, which increases defence success using IEU since a high proportion of poisoned data should be added tofor low ASR and high CA (in).

shows that the FPR/FNR are similar for bothandat low poisoning rate. However, asincreases, usingcauses more poisoned data to be left out of. For example, atand(meaning), the FNR is. This results in instability during defence and worse performance as shown in(column 0.5) since a large fraction of poisoned data is not in. As less poisoned data is included in, our defence becomes less effective with ASR increasing and CA decreasing (). Since using a fixedleaves many poisoned images outside ofwhen, we useto select a variable-sized.

We show in() that our shallowis not compatible with LGA/Flooding for isolating poisoned data; they are meant to be orthogonal methods.

SECTION: Experiments
Seefor more details regarding baselines, attacks, datasets and defence parameters.

. We use three baseline methods for comparison with our defence. Specifcally, we compare against I-BAU, ABL, and AttnBlockwhich is a ViT-specific defence. We report results for AttnBlock indue to high ASR. I-BAU and ABL are state-of-the-art general defences not specifically designed for ViTs; authors insuggest that I-BAU is the most competitive baseline compared to others. We attempt and fail to reproduce the RNP defencefor ViTs despite following recommendations into mask features of linear layers instead of those of norm layers.

. We evaluate the performance of our design on 11 backdoor attacks. Specifically, we consider 9 out of 10 attacks in: BadNets-white (white lower-right corner), BadNets-pattern (grid pattern in lower-right corner), Blended, l0-inv, l2-inv, Smooth, Trojan-SQ, Trojan-WM, and a clean label attack, SIG. In addition, we consider the sample-specific invisible attack ISSBAand an image transformation-based attack BATT. Please refer tofor visualisations of backdoored images. Although ViT-specific backdoor attacks exist in literature, we did not include these attacks due to their focus on inference-time attacks. Moreover, they use threat models that are incompatible with ours. For example,injects a trigger at inference-time (which does not concern finetuning), whilemodifies the finetuning procedure (which is controlled by the defender in our threat model) by using an attacker-specified loss function.

. We used three datasets to evaluate our defence (CIFAR10, GTSRB, and TinyImageNet). Defence parameters are shown in.

SECTION: Main Results
We show the results of our IEU compared to other baselines in. Our method’s ASR out-performs I-BAU bypercentage points (pp) on CIFAR10 and out-performs ABL bypp on TinyImageNet. In addition, our IEU’s CA for CIFAR10 and TinyImageNet are generally better than the corresponding values of the baselines. Our method has the lowest ASR in all attacks and 9 out of 11 attacks in CIFAR10 and TinyImageNet, respectively. Moreover, our method produces the highest CA for 9 out of 11 attacks for both CIFAR10 and TinyImageNet. We explore the limitations of IEU infor weaker attacks and for the GTSRB dataset. Note that I-BAU uses the highest amount of GPU memory (39 GB on an NVIDIA A100) when compared to ABL and IEU (GB).

SECTION: Ablation Study on Hyperparameters
is shown in, which demonstrates performance degradation of IEU when logit masking is not used. Without logit masking,both learns and unlearns, which by default is not learned in IEU. If the robust module performs poorly on potentially poisoned data because of asynchronous unlearning, the absence of logit masking allows the model to relearn the poisoned data during parameter updates for finetuning. Therefore, the model repeatedly learns and unlearns the same data, resulting in low performance on non-poisoned data. This reasoning also guides our decision to useinstead ofto isolatefor Interleaved Unlearning.

is shown in. We test a wide range of poison rates to determine the effectiveness of IEU under different attack settings. Overall, our IEU is able to defend against backdoor attacks with both high and low poisoning rate. The decrease in CA when poisoning rate is low is due to the worse performance ofat detecting potentially poisoned samples as shown in. We believe that better isolation methods for collatingwill result in higher CA.

are shown in. As one of the important hyperparameters in our IEU, varyingdoes not significantly affect model performance for both CIFAR10 and TinyImageNet. Looking at the “Poison” and “Clean” columns of, we generally see less data (ether poisoned or clean) having maximum class probability above the confidence threshold asincreases. Based on this observation, we argue that the performance of IEU remains stable even asdecreases in size.

as represented by the depth ofsignificantly affects the defence performance of IEU as shown in. As the depth ofincreases, it becomes more complex and more adept at learning non-poisoned samples. Sinceis confident about a larger number of clean images, this causes the number of clean data into become higher and reduces the amount of data learned by. Therefore, asbecomes deeper, CA decreases since the robust module unlearns more clean data (rows for CIFAR10 and TinyImageNet of). This effect is especially pronounced for simpler datasets (e.g., CIFAR10) because simpler datasets are more easily learned given the same model complexity, leading to more clean images being directed to. Tuningfor different depth leads to better performance as shown in the last two rows of.

SECTION: Ablation Study on Defence Design
in. We evaluate IEU where the following Vision Transformer variants are used as the robust module: CaiT-XXS, DeiT-S, PiT-XS, ViT-S (default architecture,), and XCiT-Tiny. In addition, we use ResNet-18and WideResNet-50-2to evaluate our defence on non-ViT architectures. The Interleaved Ensemble Unlearning framework generally performs well for most architectures. In addition, IEU trains high-performing models whenwhereis clean (see “No Attack” column of).

is shown in. Our defence is slightly more effective whenanddiffer by a small constant factor (first three rows of CIFAR10 & TinyImageNet in). However, on average there is only a small difference between the performance of Dynamic and constant. For example, Dynamicon average achievesCA on TinyImageNet, onlypp lower than the best CA at; ASR is comparable. To have fewer hyperparameters, we use Dynamicinstead offor hyperparameter.

SECTION: Discussion and Limitations
Theis that the Interleaved Unlearning Framework is a high-performing defence for tuning benign models on backdoored datasets. Theis that this framework is anin terms of stability and performance over existing unlearning-based methods that aim to cleanse modelstuning on backdoored data.

Weakness [a](e.g., GTSRB). Our IEU fails against ISSBA when using the GTSRB dataset as shown in(cell coloured red) and underperforms on GTSRB in general. We suggest that this happens because the GTSRB dataset is easily learnt by. Evidence is shown in, which plots the CDF of the maximum class probability values for poisoned/clean data for all three datasets using the ISSBA attack. Compared to CIFAR10 and TinyImageNet, clean images from the GTSRB dataset is only marginally more difficult to learn than backdoored GTSRB images. In, a higher proportion of clean data hasand a lower proportion of poisoned data hasfor GTSRB when compared to the other two datasets. Therefore, using a shallow ViT as the poisoned module is insufficient for discerning poisoned data from clean data for GTSRB.

Weakness [b]We believe that defending/detecting weak attacks is as important as defending strong attacks. A key difficulty in designing performant unlearning-based backdoor defence methods is identifying and mitigating weak attacks. An example of a relatively weak attack is WaNet, and as shown in, the two unlearning-based methods (our IEU and ABL) we consider are less performant. Weakness [a] and [b] have a similar root cause. Both weaknesses are caused by a less effectivefor isolating backdoored data. This effect is also seen to a smaller extent against the clean label SIG attackon TinyImageNet, where the ASR without defence is. As shown in, although ASR for SIG when defended using IEU is the lowest when comparing across different defences, the ASR for SIG is higher compared to the ASR on other attacks when defending using our IEU.

for weaknesses [a] and [b]. A better isolation method can be used in place of, such as in. We surmise that using’s isolation method would make interleaved unlearning even more effective: although our IEU does not require poisoned data isolation rate to be close to 100% (see the ASR and CA values inwhere adjacent “Poison” values areandwhere), a more effective isolation method causes the robust module to learn less backdoored data and unlearn less clean data.

Weakness [c]in stage 2 occurs when defending the VGG-11 model architecture against the Smooth attack. The instability causes NaN loss values during finetuning.

Potentialfor weakness [c]: either (1) usingfor hyperparameterinstead of the Dynamicexplained in Equationor (2) replacingwith a weighted moving average of successive cross entropy losses in Equationmay prevent instabilities from being introduced when alternating between finetuning and unlearning steps.

SECTION: Conclusion
This work presents a novel and effective method for finetuning benign ViTs on backdoored datasets called Interleaved Ensemble Unlearning (IEU). We use a small and shallow ViT (the poisoned module) to distinguish between clean and backdoored images and show that alternating between learning clean data and unlearning poisoned data during defence is an effective way of preserving high clean accuracy whilst foiling the backdoor attack. We demonstrate that our defence is effective for complicated real-world datasets and discuss ways to make IEU more robust.

. This paper’s impact goes beyond defending Vision Transformers against backdoor attacks. We believe that the Interleaved Ensemble Unlearning framework, which extends ABLand Denoised PoE, can be used to tune benign models with a great variety of different model architectures. In addition, we encourage future work to consider and remedy the weaknesses we point out infor unlearning-based backdoor defences.

SECTION: References
SECTION: More Implementation Details
SECTION: Backdoor Attack Details
For SIGwe poison 100% of the chosen target class regardless of the dataset. We mostly base our data poisoning code on BackdoorBox’s attacks; for attacks that are not available in BackdoorBox, we adapt our code from the attack authors’ source code. We modify the code for BATTimplemented in BackdoorBox by moving the attacker-specified transformation to before the data augmentation step. For ISSBA, one encoder/decoder pair is trained for each of the three datasets.shows visualisations of backdoored images on CIFAR10. Throughout the paper, the target class used by the attacker is class 1.

SECTION: Backdoor Defence Details
The data augmentations used for finetuning without defence and in our method (IEU) are based on the augmentations inwhereandare set to 0.3 and 0.0, respectively. Three views of each image are produced: an unaugmented image, a clean crop with only colour jitter, and a corrupted crop with colour jitter and patch-based corruption. We used the pretrained checkpoint infor finetuning. Note that only the non-corrupted view is used to finetune models when demonstrating the effectiveness of IEU with different model architectures in. Each image’s spatial dimension ispixels. For all experiments except for those found in, ViT-S is used as the base architecture: patch size of the ViT is; we use, and LayerNorm.

On all datasets, the ViT is finetuned for 10 epochs using the Adamoptimizer with initial learning rateand a cosine annealing learning rate scheduler that terminates at, weight decay using a cosine annealing scheduler starting fromand increasing to, andbatch size 128 (recall that the two augmented views of each image is used during finetuning).

For, the differences in the hyperparameters used for CIFAR10/TinyImageNet and GTSRB to pre-finetune the poisoned module are shown in. A weight decay ofis applied to the poisoned module throughout stage 1 and no data augmentations are applied. If learning rate warmup is used, the learning rate scheduler performs a linear warmup for one epoch starting from. The batch size is 64. Recall that poisoned module is a shallow ViT of depth 1. For, we use the same parameters during finetuning as for finetuning without defence. Please refer toand Equationfor an explanation of the unlearning rate.

Since the model architecture we use is different compared to the architectures used in, we perform hyperparameter tuning using the BadNets-white attack on all three datasets. The isolation ratio(fraction ofto unlearn) is set to 0.01 and every image inis poisoned by default since we verify that LGA/Flooding can accurately select poisoned images on BadNets-white using CIFAR10.shows the ABL hyperparameter tuning results for all three datasets. We chooseas the unlearning rate for CIFAR10, GTSRB, and TinyImageNet, respectively. In addition, we use the Adam optimiser.

We also perform hyperparameter tuning for I-BAUfor the same reasons above. Following suggestions in the appendix of, we tuneandfor CIFAR10 and TinyImageNet. GTSRB is not separately tuned since good eprformance is reached using CIFAR10’s hyperparameters. We choose as the hyperparametersfor CIFAR10, GTSRB, and TinyImageNet, respectively. In addition, we use the Adam optimiser as the outer optimiser for I-BAU. For every dataset,images from theare used for the unlearning step (in their code). Note thatclean images taken from theis the default setup in the defence code of I-BAU.

This is the defence referred to as the “” defence in. We use GradRolloutto compute the interpretation map on imageusing the backdoored checkpoint and find the coordinates of the interpretation map’s maximum. Apatch centred at the coordinatesis zeroed out from the original imageto form. If this centred patch goes outside of the image, the patch is shifted so that it is on the image’s border. Then, the backdoored checkpoint is used to classify. The results for the test-time interpretation-informed defence proposed inis shown in. Generally, the defence does not defend against the non-patch-based attacks evaluated in our work.

SECTION: Hardware Resources
Most experiments are conducted on one NVIDIA A100 GPU. A few experiments are (and can be) conducted on one NVIDIA Quadro RTX 6000 GPU. We did not reproduce I-BAUon the RTX 6000 GPU due to GPU memory constraints.

SECTION: Algorithm for Stage 2
The steps for Stage 2 of IEU is shown in.

SECTION: Ablation Study (cont’d)
.shows the effects of adding normally distributed zero-mean noise towhen accumulating the unlearn set. Whether noise is added or not has almost no effect on the model performance.

is shown in. The short-hand&means applying methodwhen tuningduring stage 1 of our method. Given the high FNR for most attacks with CIFAR10 and TinyImageNet when using LGA/Flooding together with our, we argue that using poisoned module is orthogonal to LGA/Flooding for isolating poisoned data.