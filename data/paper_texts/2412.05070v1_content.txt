SECTION: Enhancing Fourier pricing with machine learning

Fourier pricing methods such as the Carr-Madan formula or the COS
method are classic tools for pricing European options for advanced
models such as the Heston model. These methods require tuning parameters
such as a damping factor, a truncation range, a number of terms, etc.
Estimating these tuning parameters is difficult or computationally
expensive. Recently, machine learning techniques have been proposed
for fast pricing: they are able to learn the functional relationship
between the parameters of the Heston model and the option price. However,
machine learning techniques suffer from error control and require
retraining for different error tolerances. In this research, we propose
to learn the tuning parameters of the Fourier methods (instead of
the prices) using machine learning techniques. As a result, we obtain
very fast algorithms with full error control: Our approach works with
any error tolerance without retraining, as demonstrated in numerical
experiments using the Heston model.Keywords:Machine learning, computational finance, option
pricing, Fourier pricing, error control, Heston modelMathematics Subject Classification:65T40, 91G20, 91B24,
68T05

SECTION: 1Introduction

Fourier methods, such as the Carr-Madan formula and the COS method,
seeCarr and Madan (1999)andFang and Oosterlee (2009), are widely
used to price European options. In order to speed up option pricing,Liu et al. (2019a,b),Yang et al. (2017)andSirignano and Spiliopoulos (2018)propose a prediction of option prices using
neural networks.Ruf and Wang (2020)provide a comprehensive review
of neural networks for option pricing.Liu et al. (2019a,b)use a parametric approach and consider an advanced stock price model,
such as the Heston model, seeHeston (1993). They use
a set of market parameters, including strike price and maturity, as
well as model parameters, to predict the corresponding option prices.De Spiegeleer et al. (2018)use machine learning techniques based on Gaussian
process regression for prediction of option prices.

WhileDe Spiegeleer et al. (2018)andLiu et al. (2019a,b)were able to accelerate the existing Fourier methods to some extent,
their approaches also exhibited certain limitations.Liu et al. (2019a,b)obtain a mean absolute error (MAE) of about.De Spiegeleer et al. (2018)also obtains a MAE of aboutand a maximum absolute error
of approximatelyon their sample.De Spiegeleer et al. (2018, Table 2)compare the numerical effort with the Carr-Madan formula and obtain
an acceleration factor between 10 and 40 for European options.

However, the approaches described inLiu et al. (2019a,b)andDe Spiegeleer et al. (2018)suffer from a lack of error control: To
achieve higher numerical pricing accuracy, deeper neural networks
are necessary and the machine learning methods need to be retrained
with more samples, which is very time-consuming and impractical in
most situations.

In this paper, we propose an indirect use of machine learning methods
to improve the accuracy and efficiency of existing pricing techniques
with full error control. We focus on the COS method, but our approach
is also applicable to other methods, i.e., we also discuss the Carr-Madan
formula.

We describe the main idea of the COS method, details can be found,
e.g., inFang and Oosterlee (2009); Oosterlee and Grzelak (2019); Junike and Pankrashkin (2022):
Given only the characteristic function of the log-returns of the underlying,
the density of the log-returns is approximated in two steps: i) truncate
the density on a finite intervaland ii) approximate the
truncated density by a finite Fourier-cosine approximation withterms. There is a clever trick to obtain the cosine-coefficients of
the truncated density efficiently from the characteristic function.
The CPU time of the COS method depends linearly on the number of terms. Note that the choice of the truncation range has a significant
influence on the number of terms required to achieve a certain accuracy.
There are explicit formulas for the truncation range and the number
of terms depending on an error tolerance, seeJunike and Pankrashkin (2022)andJunike (2024). However, the truncation range formula
requires evaluating higher-order derivatives of the characteristic
function, which can be very time-consuming, e.g., in the case of the
Heston model. The formula for the number of terms requires integration
of the product of the characteristic function and a polynomial, which
is also very time consuming. Fortunately, the time-consuming part
required to obtainanddoes not depend on the
required error tolerance.

In this paper, we use machine learning techniques to learn the-th
derivatives of the characteristic function evaluated at zero and learn
the integral of the characteristic function times a polynomial, which
is independent of the required error tolerance. Then, we use these
predicted values and the error tolerance to obtain the truncation
range and the number of terms. The COS method can then be applied
to price European options.

Different traders may use different error tolerances, but our machine
learning techniques do not require retraining. This error control
is an advantage over direct prediction of option prices by machine
learning techniques. The actual calculation of the option price using
the COS method is then very fast.

The paper is structured as follows. Section2gives an overview of the Heston model, which will be used in the numerical
experiments. In Section3, we introduce the COS
method and the Carr-Madan formula and machine learning techniques.
Section4provides the numerical experiments
to demonstrate the performance of the proposed method. Section5concludes the paper.

SECTION: 2The Heston model

Consider a financial market with a riskless bank-account and a stock
with deterministic pricetoday and random priceat some future date. In the Heston model with parameters,,,and, the stock
price is described by the following system of differential equations

andare correlated Brownian motions such that,
seeHeston (1993).

The CIR process, described by Equation (2), stays positive
if, which is known as theFeller
condition, seeAndersen and Piterbarg (2007). The characteristic function
of the log stock price, seeBakshi et al. (1997), is given
by

where

SECTION: 3Algorithms: Numerical tools and machine learning

SECTION: 3.1The Carr-Madan formula

Carr and Madan (1999)showed that the price of a European call option
with strikeand time to maturityis given by

whereis a damping factor such thatandis the characteristic function of.denotes the real part of a complex numberandis the complex unit. The integral in Eq. (3)
can be truncated to, for some, and then be evaluated
using, e.g., Simpson’s rule withgrid points.

SECTION: 3.2The COS method

We summarize the COS method. This section is based onFang and Oosterlee (2009),Junike and Pankrashkin (2022)andJunike (2024). Letbe the expectation ofunder the risk-neutral measure
and assume that the characteristic functionof the
centralized log-returnsis given in closed-form.
The functionis explicitly given for many models such
as the Heston model. The price of a European put option with maturityand strikeis given by

whereis the density of. The price of a call option can
be obtained by the put-call-parity. Very often,is not explicitly
given and the COS method can be used to approximateand the price
of the option.

For some, the densityis truncated and the truncated density
is approximated by a cosine series expansion:

where for, the coefficientsare defined by

The second Equality in (6) follows from a simple analysis,
seeFang and Oosterlee (2009). The price of a European put option can
be approximated by replacingin (4) with its approximation
(5), which gives

where

The coefficientsare given in closed form whenis given analytically and the coefficientscan also be computed
explicitly in important cases, e.g., for plain vanilla European put
or call options and digital options, seeFang and Oosterlee (2009). This
makes the COS method numerically very efficient and robust.

We provide formulas for the coefficientsfor a European put
option: Let. For a
European put option, it holds thatifand otherwise

where

and

seeJunike and Pankrashkin (2022, Appendix A). To price a call option
it is numerically more stable to price a put option instead and use
the put-call parity, seeFang and Oosterlee (2009).

To apply the COS method, one has to specify the truncation rangeand the number of terms. For a given error tolerancesmall enough, both parameters can be chosen as follows to ensure an
approximation error smaller than, seeJunike and Pankrashkin (2022)andJunike (2024). Ifis small enough andhas semi-heavy tails, the truncation range of a put option can
be chosen using Markov’s inequality by

whereis even andis the-th root
of the-th moment of, which can be obtained using a computer
algebra system and the relation

Often,is a reasonable choice, seeJunike and Pankrashkin (2022, Cor. 9).
Ifis alsotimes differentiable with bounded
derivatives, then the number of terms can be chosen by

where

seeJunike (2024, Eq. (3.8)). The last integral can be
solved numerically by standard techniques and in some cases it is
given explicitly. One should choosesuch that the left-hand side
of Inequality (9) is minimized.
For the Heston model,is set toinJunike (2024).
An implementation of the truncation range, the number of terms and
the COS method for the Heston model can found in AppendixA.3.

SECTION: 3.3Machine learning techniques

Decision trees (DT), seeBreiman et al. (1984), operate
by recursively partitioning the input data into subsets, thereby forming
a tree-like structure, see Table1and Figure1. At each internal
node of the DT, the algorithm selects a feature and a threshold value
to split the data into two subsets.

For example, in the first row of Table1,
all input values with maturityless than or equal toare assigned to node, all other values are assigned to node.
The goal of these splits is to create child nodes with greater homogeneity.
The recursive splitting process continues until a stopping criterion
is met, such as a maximum tree depth or a minimum node size for splitting.

To build a DT for regression, the splitting is based on variance reduction.
The algorithm selects the features and thresholds that most strongly
reduce the variance at each node for splitting.

Given new samples, predictions are made at the leaf nodes, where the
model assigns the average of the data points within the node. This
simplicity and transparency make DT highly effective at handling complex
data sets while maintaining interpretability.

Random forests (RF), seeBreiman (2001)are an ensemble
of DTs to improve the accuracy and robustness of predictions. Each
DT in the RF is trained on a random subset of the data using bootstrap
aggregation. At each node, a random subset of the features is used
for the splitting. In a RF, each DT makes a prediction independently
and the final output is determined by averaging the individual predictions
of each single tree.

A neural network (NN) consists of one or more layers, each consisting
of a number of artificial neurons, seeGoodfellow et al. (2016).
A single neuron transforms its multidimensional inputinto a one-dimensional output. For some weights,
the weighted mean of the input is then transformed by an activation
function, i.e., the output of a neuron
is given byExamples
of activation functions are the ReLU functionor
the Sigmoid function. In the first layer
of the NN, the neurons receive the input data and the output of each
neuron is passed to all neurons in the following layers until the
last layer is reached.

At the start of training, the weights of the NN are randomly initialized.
During the training phase, the weights are chosen in such a way that
the functional relationship between input and output data is mapped
as well as possible.

In this work, we test the following regularization techniques that
can improve the robustness of the NN: Dropout means randomly deactivating
some neurons. Gaussian noise is a regularization technique that adds
normally distributed numbers with zero mean and small variance to
each weight at each update step. Batch normalization standardizes
the inputs of each layer. These and other regularization techniques
are discussed in detail in, for example,Goodfellow et al. (2016).

SECTION: 4Numerical experiments

In this section, we use the machine learning techniques DT, NN and
RF to predict the tuning parameters of the Carr-Madan formula and
the COS method. For training, we randomly generate parameters of the
Heston model. The ranges of the six parameters are shown in Table2. The wide ranges of these parameters
include parameters that are typically used for the Heston model, seeAndersen (2008); Crisóstomo (2015); Cui et al. (2017); Engelmann et al. (2021); Fang and Oosterlee (2009); Forde et al. (2012); Levendorskiĭ (2012)andSchoutens et al. (2003).

For each sample (consisting of the five parameters for the Heston
model and the maturity), we computeandandfor the entire data set, using Eqs. (8,10). The derivatives ofare calculated using a computer algebra system. As a side note: One
may also approximate the moments as inChoudhury and Lucantoni (1996)to avoid the computation of the derivatives.

We exclude all the model parameters for which Eq. (8)
gives negative results, assuming that the moments do not exist in
these cases and we remove all parameters for which the Feller conditionis not satisfied.

In the following numerical experiments, we price a European call option
with, strikeand interest rate. We also
tested other strikes, i.e.,and obtained similar
results. For each sample, we calculate a reference price. To obtain
the reference prices we use the COS method with truncation rangeand number of terms, where we set.
To confirm the prices we use the Carr-Madan formula with truncation
range,and appropriate damping factors. We remove
a few samples where the prices were too unstable and the COS method
and the Carr-Madan formula give completely different results. For
all remaining options, the COS method and the Carr-Madan formula coincide
at least up to seven decimal place.

We receive a cleaned data set ofsamples. We takesamples for training and validation and use the remainingsamples as a test set. All experiments are run on a laptop with an
Intel i7-11850H processor and 32 GB of RAM.

SECTION: 4.1On the tuning parameters of the COS method

To apply the COS method, we use the formulas for the truncation range
and the number of terms in Eq. (7)
and (9). For the Heston model,
it is time-consuming to computein Eq. (8)
and to solve the integralin Eq. (10).
Therefore, we use the machine learning techniques DT, RF and NN for
a fast estimation ofand.

To identify an appropriate architecture for the different machine
learning techniques, we perform a rough hyperparameter optimization.
For the DT, we optimize over the maximum depth and the minimum node
size. In addition, the number of DTs in the RF is optimized, resulting
in the hyperparameters shown in Table3.
The R packagerangeris used for both DT and RF. We consider
a big DT (bDT) of arbitrary depth and a small DT (sDT) of depth 5.
The sDT forand the sDT forare tabulated in
AppendixA.3andA.3and
could be implemented directly without using additional software packages.

The architectural specifications of the NN are described in Table4. The NN is trained with
100 epochs, a validation split ofand the mean squared error
(MSE)

as the loss metric. For the starting values of the weights we use
the He initialization, seeHe et al. (2015). For the NN, we
use tensorflow via the keras package called from R.

Table5shows the MSE on the test set for the different
machine learning techniques. It can be observed that for,
the NN has a smaller MSE than the RF, while the bDT has a comparatively
large MSE. With regard to, the RF has the smallest MSE,
while the MSE of the NN and the bDT are aboutlarger. The
sDT has a significantly larger MSE for bothand.

Next, we calculate the price of the call option for different model
parameters. We use the COS method withorand, where.

The Table6shows the percentage of samples
in the test set for which the required accuracy is achieved by obtaininganddirectly from Eqs. (8,10), which is very time-consuming,
or by estimatingandvia DTs, RF or a NN, which
is very fast. The direct way of obtainingandand the estimation by the RF result inaccurate option prices
on the test set for all. The NN also achieves a high
accuracy of aboutfor all. This result could
be further improved with a different NN architecture and additional
training. It can be observed that a single bDT is also able to estimateandwith sufficient accuracy to price the call
option with different error bounds for at leastof the
samples. And even a simple technique like the sDT already achieves
an accuracy of at leaston the test set.

These very good results are a consequence of the fact that the formulas
in Eq. (7) and (9)
are derived using many inequalities, thus overestimating the minimum
truncation rangeand the number of termsneeded to accurately
price the option. Therefore, a rough estimate ofandis sufficient for precise option pricing.

The Table7illustrates the CPU
time of the COS method, whereandare obtained by different
error tolerances. The COS method is implemented in C++ using for-loops
without parallelization. It is well known, thatis usually closer to the optimal truncation range than,
seeJunike and Pankrashkin (2022). It is therefore not surprising that
the average CPU time is about 10 times faster using the truncation
rangecompared to,
see Table7.

Let us setand let us consider two scenarios:
i) A trader estimatesanddirectly. (Estimatingdirectly is too time consuming for the Heston model). ii)
A trader estimatesandusing machine learning
techniques. From Table6, we can see that
both approaches will price the options very accurately for different
error tolerances and parameters of the Heston model. What is the impact
on the total CPU time? As shown in Table8,
the CPU time to obtainanddirectly takes about
0.011sec. (Most of the time is used to estimate, we used
R’s functionintegratewith default values for numerical integration).
The computation ofanddominates the total CPU
time, since the pure application of the COS method takes aboutsec., see Table7. On the other
hand, the CPU time to estimateandusing machine
learning techniques is about a factor oftotimes
faster than the direct computation ofand. The
total CPU time of the COS method estimatingandvia a NN is aboutsec. In summary, approach ii)
is almosttimes faster than approach i).

SECTION: 4.2On the tuning parameters
of the Carr-Madan formula

In order to apply the Carr-Madan formula, one must specify three parameters,
namely the damping factor, the truncation rangeand
the number of grid points. In the following, we use a NN and
a RF to estimate these parameters. We setand determine
optimal parametersandfor the entire training set,
such thatis minimal to achieve an error bound of.
We then train a NN and a RF to learn these optimal parameters. Since
the estimateof the NN and the RF sometimes significantly
underestimates the true, we double the output of the NN and the
RF to improve the accuracy of the Carr-Madan formula. This step was
not necessary for the COS method, since the theoretical formulas for
the truncation range and number of terms are larger than the minimal
truncation range and number of terms.

To measure the accuracy of the Carr-Madan formula, we price a call
option withand, using the predicted values
forandof the NN and the RF. We obtain the required
accuracy offorandof
the samples in the test set for the RF and the NN, respectively.

To compare these results, we also use standard parameters of the Carr-Madan
formula:Carr and Madan (1999)suggest the default valuesandas a rule of thumb. The Carr-Madan formula is very
sensitive with respect to the damping factor, we choose.
For these default values, the accuracy ofis reached in
onlyof the samples in the test set (any other fixedleads to an even lower proportion). Consequently, RFs and NNs are
a useful tool for improving the accuracy of the Carr-Madan formula,
since there is no single damping factorand number of grid
pointsfor all cases.

SECTION: 5Conclusion

In this paper, we proposed an indirect use of machine learning to
improve the efficiency and accuracy of the Carr-Madan formula and
the COS method for option pricing.Junike and Pankrashkin (2022)andJunike (2024)provide explicit bounds on the truncation
range and the number of terms to apply the COS method. These bounds
ensure that the COS method prices a European option within a predefined
error tolerance. It is generally time-consuming to obtain these bounds
using classical numerical tools. In this paper, we instead estimate
these bounds using machine learning techniques such as RF, DT and
NN. We summarize the advantages:

Compared to directly estimating the option prices using machine learning
techniques as inLiu et al. (2019a,b)andDe Spiegeleer et al. (2018),
our approach allows for full error control.

Compared to estimating the bounds using classical numerical methods,
our approach is much faster: about a factor.

Compared to using a fast rule of thumb (as proposed inFang and Oosterlee (2009)andCarr and Madan (1999)) to estimate the tuning parameters of
the COS method or the Carr-Madan formula, our approach is much more
reliable. For the COS method, seeJunike and Pankrashkin (2022)for examples
where a rule of thumb based on cumulants leads to serious mispricing.
For the Carr-Madan formula, see Section4.2.

We tested RF, DT and NN to estimate the bounds to obtain the truncation
range and the number of terms to apply the COS method. Among these
techniques, the RF works best (accurate on 100% of the test set).
The NN has a similar performance. But even a small DT gives very satisfactory
results (accurate on 98.2% of the test set). Estimation of the tuning
parameters of the Carr-Madan formula by a RF or a NN works in about
90% of all samples in a test set.

SECTION: Appendix AAppendix

SECTION: A.1Decision tree of depthto predict

SECTION: A.2Decision tree of depthto predict

SECTION: A.3Simple implementation

The following algorithm implements the COS method in R for the Heston
model to price European put and call options.

Algorithm 1Implementation details of the COS method in the
Heston model

#Characteristic function of log-returns in the Heston
with parameters params.

#The characteristic function is taken from Schoutens
et. al (2004).

psiLogST_Heston = function(u, mat, params, S0, r){

kappa = params[1] #speed of mean reversion

theta = params[2] #level of mean reversion

xi = params[3] #vol of vol

rho = params[4] #correlation vol stock

v0 = params[5] #initial vol

d = sqrt((rho * xi * u * 1i - kappa)^2
- xi^2 * (-1i * u - u^2))

mytmp = kappa - rho * xi * u * 1i

g = (mytmp - d) / (mytmp + d)

expdmat = exp(-d * mat)

tmp0 = 1i * u * (log(S0) + r * mat)

tmp1 = (mytmp - d) * mat - 2 * log((1 - g
* expdmat) / (1 - g))

tmp2 = theta * kappa * xi^(-2)
* tmp1

tmp3 = v0 * xi^(-2) * (mytmp
- d) * (1 - expdmat) / (1 - g * expdmat)

exp(tmp0 + tmp2 + tmp3)

}

library(Deriv) #There are much faster alternatives
like SageMath.

psiLogST_Heston1=Deriv(psiLogST_Heston, "u")

#mu is equal to E[log(S_T)]

mu = function(mat, params, S0, r){

Re(-1i * psiLogST_Heston1(0, mat, params,
S0, r))

}

#Characteristic function of centralized log-returns
in the Heston model.

phi = function(u, mat, params, S0, r){

psiLogST_Heston(u, mat, params, S0, r) * exp(-1i
* u * mu(mat, params, S0, r))

}

#cosine coefficients of the density.

ck = function(L, mat, N, params, S0, r){

k = 0:N

return(1 / L * Re(phi(k * pi / (2 * L),
mat, params, S0, r) * exp(1i * k * pi/2)))

}

#cosine coefficients of a put option, see Appendix
Junike and Pankrashkin (2022).

vk = function(K, L, mat, N, params, S0, r){

mymu = mu(mat, params, S0, r) #mu = E[log(S_T)]

d = min(log(K) - mymu, L)

if(d <= -L)

return(rep(0, N + 1)) #Return zero vector

k = 0:N

psi0 = 2 * L / (k * pi) * (sin(k * pi
* (d + L) / (2 * L)))

psi0[1] = d + L

tmp1 = k * pi / (2 * L) * sin( k * pi
* (d + L) / (2 * L))

tmp2 = cos(k * pi * (d + L) / (2 * L))

tmp3 = 1 + (k * pi / (2 * L))^2

psi1 = (exp(d) * (tmp1 + tmp2) - exp(-L)) /
tmp3

return(exp(-r * mat) * (K * psi0 - exp(mymu)
* psi1))

}

#approximation of put option by COS method

put_COS = function(K, L, mat, N, params, S0, r){

tmp = ck(L, mat, N, params, S0, r) * vk(K,
L, mat, N, params, S0, r)

tmp[1] = 0.5 * tmp[1] #First term
is weighted by 1/2

return(sum(tmp))

}

#approximation of call option by COS method using put-call
parity

call_COS = function(K, L, mat, N, params, S0, r){

return(put_COS(K, L, mat, N, params, S0, r)
+ S0 - K * exp(-r * mat))

}

#Derivatives of the characteristic function of the
centralized log-returns in the Heston model.

phi1 = Deriv(phi, "u")

phi2 = Deriv(phi1, "u")

phi3 = Deriv(phi2, "u")
#Takes very long but has to be done only once.

phi4 = Deriv(phi3, "u")
#Takes very long but has to be done only once.

save(phi4, file = "phi4.RData")
#save for later use. Load with load("phi4.RData").

#Price a put option in the Heston model by the COS
method.

eps = 10^-6 #error tolerance

K = 90 #strike

S0 = 100 #current stock price

r = 0.1 #interest rates

params = c(0.6067, 0.0707, 0.2928, -0.7571, 0.0654)

mat = 0.7 #maturity

mu_n = abs(phi4(0, mat, params, S0, r)) #4-th moment
of log-returns.

L = (2 * K * exp(-r * mat) * mu_n / eps)^(1
/ 4) #Junike (2024, Eq. (3.10)).

s = 20 #number of derivatives to determine the number
of terms

integrand = function(u){1 / (2 * pi) * abs(u)^(s
+ 1) * abs(phi(u, mat, params, S0, r))}

boundDeriv = integrate(integrand, -Inf, Inf)$value

tmp = 2^(s + 5 / 2) * boundDeriv
* L^(s + 2) * 12 * K * exp(-r * mat)

N = ceiling((tmp / (s * pi^(s + 1)
* eps))^(1 / s)) #Number of terms, Junike (2024,
Sec. 6.1)

put_COS(K, L, mat, N, params, S0, r) #The price of
put option is 2.773954.

SECTION: References