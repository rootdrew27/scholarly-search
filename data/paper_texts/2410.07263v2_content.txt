SECTION: Memory-augmented Transformers can implement Linear First-Order Optimization Methods
We show that memory-augmented Transformers () can implement linear first-order optimization methods such as conjugate gradient descent, momentum methods, and more generally, methods that linearly combine past gradients. Building on prior work that demonstrates how Transformers can simulate preconditioned gradient descent, we provide theoretical and empirical evidence that Memformers can learn more advanced optimization algorithms. Specifically, we analyze how memory registers in Memformers store suitable intermediate attention values allowing them to implement algorithms such as conjugate gradient. Our results show that Memformers can efficiently learn these methods by training on random linear regression tasks, even learning methods that outperform conjugate gradient. This work extends our knowledge about the algorithmic capabilities of Transformers, showing how they can learn complex optimization methods.

SECTION: Introduction
In-context learning (ICL) allows large language models (LLMs) to generate contextually appropriate outputs based solely on examples and queries provided in a prompt, without requiring any parameter adjustments. This remarkable ability has spurred research into understanding how Transformers can implement algorithms, with recent studies focusing on their capability to simulate optimization algorithms. Transformers have been shown to implement gradient-based optimization during their forward pass, such as preconditioned gradient descent for linear regression tasks.

More recently, studies have demonstrated that Transformers can learn even more advanced optimization methods. For instance,showed that Transformers exhibit convergence rates comparable to Iterative Newton’s Method, a higher-order optimization technique that converges exponentially faster than gradient descent for in-context linear regression. Additionally,proved that Transformers can, in fact, learn a variant of gradient descent that approximates second-order methods, such as, achieving convergence rates similar to Newton’s method. These findings lead to the central question of our paper:

Can Transformers efficiently “learn" more advanced gradient-based optimization methods?

We aim to address this question by revealing some of the representational power of Transformers as “algorithm learners,” further motivating the use of machine learning for discovering new optimization algorithms. To make our investigation more precise, we focus on learning the class of gradient-based algorithms obtained by linearly combining past gradients, known asLinear First-Order Methods (LFOMs), where thest iterate is

and whereare diagonal matrices. Model () is quite general, as it includes, as special cases, standard methods such as gradient descent (GD), momentum GD, Nesterov’s accelerated gradient, conjugate gradient, and in a stochastic setting, AdaGrad, ADAM, among others.

By, we mean two key things:

1.This means that its architecture and parameterization are sufficiently expressive to execute these optimization methods as part of its computation.

2.The surprising aspect lies in the Memformer’s ability to achieve competitive—and in some cases even superior—performance compared to CGD, despite using a relatively small number of learned parameters shared across all test samples drawn independently of the training data.

Our key insight for efficiently learning LFOMs is to leverage memory-augmented Transformers, known asMemformers, which retain intermediate attention values across layers. This memory enables Memformers to store past gradients, facilitating the execution of advanced first-order methods such as conjugate gradient descent and momentum methods. The same mechanism allows Memformers to implement more general LFOMs.

While unconditional learning of gradient methods remains out of reach, we build on related work demonstrating that Transformers can learn gradient descent in the context of linear regression tasks. Inspired by these findings, and extending the work of, we conduct a theoretical analysis of the loss landscape for memory-augmented linear Transformers that omit softmax activation.

In the Appendix, we also include our experiments that Memformers can outperform Nesterov Accelerated Gradient (NAG) and momentum GD. In summary, our main contributions are as follows:

SECTION: Main Contributions
We provide a rigorous theoretical framework showing that Memformers, when trained on linear regression tasks, can be configured to perform iterations of LFOMs in their forward pass, encompassing advanced algorithms like CGD. By leveraging their memory mechanisms, Memformers can store and effectively combine past gradients, enabling them to implement these sophisticated optimization methods within their architecture.

Through extensive experiments, we demonstrate that Memformers can, in a general sense, by training on random linear regression tasks.

This finding is particularlybecause CGD tailors its optimization individually for each data sample, whereas the Memformer applies a general optimization strategy learned from the training data across all samples. The ability of Memformers to generalize optimization strategies across data samples using shared parameters highlights their generalization capabilities, which have not been fully recognized in prior research.

We show empirically that multi-headed attention improves Memformers’ test performance and offer a heuristic explanation for why increasing attention heads enhances loss performance on test data.

Instead, we aim to shed light on the algorithmic capabilities of Transformers, inspiring further exploration into how these architectures can learn and generalize complex algorithms. We believe our results contribute to a deeper understanding of how augmented Transformers can facilitate optimization, which may ultimately lead to the discovery of new and practical gradient-based algorithms.

SECTION: Related Work
Research on Transformers is extremely active, and we cannot hope to fully capture the breadth of the related literature. Below, we summarize the most immediately relevant topics.

The ability of Transformer models to perform in-context learning (ICL) has been extensively studied since its introduction by. Subsequent works have explored how these models adapt to new tasks without requiring parameter updates. This foundational research has paved the way for studies investigating how Transformers can implement specific algorithms, such as gradient-based methods.

analyze the learning of gradient descent within Transformers, particularly in the context of ICL for linear functions. Empirical studieshave shown that Transformers can learn gradient descent after being trained on random linear regression tasks. Expanding on these results,demonstrate that Transformers can implement preconditioned gradient descent for solving linear regression problems presented in input prompts. Notably, these works—as well as ours—utilize Linear Transformers as discussed in.

Transformers have also been shown to learn higher-order optimization techniques, such as Newton’s method, expanding their capabilities beyond first-order methods.

Memformers were introduced by. These models retain intermediate attention values across layers through memory registers, enabling more complex computations and optimization methods to be learned. While significant progress has been made in understanding how Transformers can learn gradient descent, their potential for learning more sophisticated LFOMs remains largely unexplored. Our work addresses this gap by showing how Memformers can efficiently implement a wide range of advanced first-order and quasi-second-order optimization techniques, including CGD and momentum methods, thereby pushing the boundaries of Transformer-based architectures.

SECTION: Background and Problem Setup
SECTION: Linear Transformers on Random Linear Regression
We follow the setup of training Transformers on random instances of linear regression, following the prior works. We largely use the notation and formal setup of, which we now proceed to recall.

Letrepresent covariates drawn independently from a distribution, and letbe drawn from. The matrix of covariatescontains rows. The responses are. Define the input matrixas:

where the zero corresponds to the unknown response for. The task is to predictusing. The training data consists of pairsforand.

We focus on the linear self-attention layer, building on. Letbe the input matrix oftokens in. Standard self-attention layer is defined as

whereare weight matrices, anddenotes the column-wise softmax. The masking matrixensures that the label foris excluded is given by

Omitting softmax, the attention mechanism becomes

whereand. This simplified form, as shown in, can implement preconditioned gradient descent, and it is the one we also use.

As in the related work, we also simplify the Transformer to consider only attention layers, usinglayers of linear self-attention with a residual connection. Therefore, for each layer, the output is updated as

Using updates (), with the input, the final transformer output is

The set of parametersis then learned by minimizing the following training objective:

Here, the scaling factoris used only for ease of notation and does not influence the expressive power of the Transformer.

We will utilize the following lemma from, which demonstrates that multi-layer Transformers simulate preconditioned gradient descent under suitable parameterization. We have provided the full proof of this Lemmain the Appendix for completeness.

SECTION: Linear First-Order Methods
Linear First-Order Methods (LFOMs)are a class of optimization algorithms that lineary combine past gradients for minimizing smooth objective functions. They iteratively update a parameter vectorusing the gradient of the objective function. The general update rule is

whereis the step size andis the update direction, typically related to the gradient. Algorithms within this family differ in how they computeand choose.

LFOMs can be expressed in a cumulative form. For gradient descent, unrolling () we get

while common momentum methods need an additional term incorporating past gradients, yielding

where the coefficientsweight previous gradients. More advanced methods, or general LFOMs, use diagonal matricesto coordinate-wise scale each gradient component, i.e.,

Momentum methods accelerate convergence by incorporating a momentum term, modifying the gradient to account for past updates and achieving faster convergence in relevant directions. Conjugate Gradient Descent (CGD), on the other hand, is a first-order method optimized for quadratic minimization, serving as a benchmark for large-scale, sparse linear systems. After an initial steepest descent, CGD generates directions conjugate to previous ones, leading to faster convergence than standard gradient descent. Both are core methods within the LFOM class, summarized below:

Momentum methods provide fast convergence by accumulating gradient history and are widely used in modern optimization. CGD converges in at mostiterations for quadratic functions, whereis the number of variables, and is effective for ill-conditioned problems.

SECTION: Memformers Can Implement LFOMs In-Context
Memformers can “learn" LFOMs in the specific sense discussed earlier in Section. Each layerof the Memformer has learnable parameters such as(), and() or().

Theoretically, in Propositionsandbelow, we show that in their forward pass, under certain parameter configurations, Memformers can implement exact CGD and LFOM iterations. This is indicative of the algorithmic capacities of these architectures.

As noted in, the termin the update for() corresponds to the preconditioned gradientof the in-context loss () in the update for.

We will henceforth call the class of algorithms that the following architecture () can implement as, and the class of algorithms that architecture () can implement as.

SECTION: Dynamic Memory for CGD-like Algorithms
A memory-augmented Transformer can implement Conjugate Gradient Descent (CGD) in its forward pass through a dynamic memory mechanism that recursively refines search directions, where the update rules are:

whereandcontrol the influence of past updates and the step size, respectively.

Heredenotes the state of amemory registerat different layersduring a forward pass. CGD refines search directions using current gradients and previous directions. The Transformer simulates this by usingas the current update, analogous to the gradient in CGD, andto refine the previous search direction, corresponding to the recursive update ofin CGD.

The recursive update forthus mimics, the search direction in CGD. The update foruses, scaled by, similar to how CGD iterates are updated using. With, this process matches CGD applied to the loss(), using both current and previous gradients to refine the search direction. ()

SECTION: ImplementingSteps of LFOM with Memory Registers
We extend our analysis to show how Transformers can simulatesteps of Linear First-Order Methods (LFOMs). This is achieved by maintaining a memory register at each layer, which stores accumulated updates from previous layers, simulating iterative optimization.

A memory-augmented Transformer can implementsteps of LFOM in its forward pass by maintaining memory registers across layers, where the update rules are:

wheregoverns the contribution of previous layers, andis the Hadamard product for scaling.

Here eachdenotes amemory register for each layer. Memformers with this architecture simulate iterative optimization by refreshing the memory registerat each layer with, capturing the current update. The cumulative update toincorporates past layers through a weighted sum of previous memory registers, with weights, mimicking LFOM’s cumulative iterative process. We will henceforth refer to this architecture () as “".

The Hadamard productmodulates the influence of, analogous to gradient preconditioning. This setup subsumes the case of diagonal preconditionersacting on gradients, which in the general form looks like:

The matricesandserve similar roles, but their dimensions differ. We expect this Hadamard product memory architecture to be able to perform richer algorithms than LFOMs, though a formal characterization of its full potential remains to be done.

The full proof follows from the cumulative memory structure and the connection between attention and preconditioned gradients, as discussed in the proof steps of Lemma. ()

The update () could be interpreted as a type ofgated memory, related to gating in LSTMs and GRUs that also use the Hadamard product to modulate information flow through gates. This similarity suggests that principles from these architectures could help refine memory mechanisms in Transformers, potentially enhancing their ability to handle long-term dependencies in optimization tasks. However, further exploration is needed to fully understand this relationship.

SECTION: Experimental Results: Memformer Performance vs. CGD
In this section, we present our empirical results for Memformers “learning" conjugate gradient descent (CGD), general linear first-order methods (LFOMs), and general LFOMs with. The methodis a quasi-Newton method where the inverse Hessian in Newton’s method is approximated by a truncated Neumann series; for more details on, refer to Section A.10 of.

We consider the in-context loss function () for linear regression. The input dimension is set to, and the number of training observations in the prompt is. Both the inputsand the target weight vectorare sampled from Gaussian distributions:and, where. Here,is a uniformly random orthogonal matrix, andis a fixed diagonal matrix with entries.

We optimize the function() for a three-layer linear transformer using the ADAM optimizer. The matrices,, and(as in ()) are initialized with independent and identically distributed (i.i.d.) Gaussian entries. Each gradient step is computed using a batch of size 1000, and we resample the batch every 100 steps. We clip the gradient of each matrix to have a maximum norm of 0.01. All plots are averaged over five runs, each with a different randomly sampled(and thus different).

Figureillustrates the implementation of a CGD-like algorithm under the architecture given by (). In Figure, the line-search parametersand deflection parametersfor each layerare obtained by training using ADAM. By “CGD-like,” we mean that upon training the Memformer using ADAM, the Memformer layers learn general parametersandwhich, while they may not match the exact CGD parameters for individual observations, perform well enough on each observation to be comparable to, if not competitive with, CGD. We further explain the important issue of learning general parameters in Section.

Figurepresents the same experiment as Figure, using the architecture in (), but with the parametersfor each layer not restricted to scalars. Thus, past gradients are accounted for, similar to CGD, but with preconditioners. This is therefore not a “CGD-like” algorithm. We aim to demonstrate that once we allow preconditioned gradients, a Memformer implements a certain “LFOM-like" algorithm that distinctly outperforms CGD.

Figurepresents the performance of LFOM Memformer under the architecture in (), where the matrix parametersfor each layerare obtained by training using ADAM. In our experiments, we consider the special case of, which is more natural, if we consider that each layerof the Memformer has an associated. Figureshows the results on non-isotropic data, and Figureshows the results on isotropic data. Note that this algorithm is quite similar in nature to the previous case in Figure. Here, the’s essentially act as preconditioners of the gradients computed in each layer. Consequently, the graphs of Figuresandare nearly identical. In the isotropic data experiment (Figure), we observe that the Memformer does not perform better than a linear transformer. In quadratics with isotropic data, there is no significant variation in curvature across directions; thus, incorporating past gradients via momentum offers little advantage. Momentum is more beneficial in cases with non-isotropic data.

Figurepresents LFOM Memformer withunder the architecture in (), where theblocks in thematrices for each layer() are allowed to be non-zero. Once again, the matrix parameters for each layerare obtained by training using ADAM. In this case, thematrices resemble a heavily truncated Neumann series of the inverse(Hessian of ()), resulting in a quasi-Newton method. The experiments are conducted on both non-isotropic data (Figure) and isotropic data (Figure).

SECTION: Experiments: Influence of Batch Size on Performance
We emphasize here that the results presented in Sectioncompare the performance of Transformers and Memformers (which learn shared generic parameters upon training) against CGD that runs on fresh observations of batch size, independently resampled from the same distribution. But unlike CGD that computes specific parameters for each observation, the Transformer and Memformer models learn shared parameters,(and,, or) for each layer, and these parameters are applied uniformly across all 1000 observations in the batch. In contrast, CGD is executed individually on each of the 1000 observations in the batch, and the average log-loss versus layers is plotted.

The strength of LFOM Memformers () (with matricesrestricted to scalar multiples of the identity) becomes even more pronounced when tested on training data with small batch sizes, such asand. In these scenarios, the Memformers learn parameters that significantly outperform CGD running in parallel on each of the observations in those small batches. Figuredemonstrates this comparison. We further provide an experimental comparison of LFOM Memformer performance vs. Nesterov Accelerated Gradient Method and Momentum GD in the Appendix.

SECTION: Experiments: Impact of Using Multi-Headed Attention
Our experiments show that increasing the number of attention heads improves test loss performance. Multi-head attention enables Transformers to learn diverse preconditioning matrices, better adapting to varying data covariance structures. In our architecture (), attention values from each head are summed into the memory registerat each layer. Heuristically, each head captures different aspects of the data, estimating gradients from multiple perspectives. This ensemble-like behavior reduces variance in gradient updates by averaging out individual noise and biases, leading to faster convergence and more stable optimization. Acting as implicit regularization, it prevents overfitting and enhances generalization on test data. This phenomenon is also supported by recent studies.showed that multi-head attention is essential for effective context preprocessing in sparse linear regression, aligning with our findings. Similarly,provided theoretical and empirical evidence that multi-head attention outperforms single-head attention in in-context learning.

Figurecompares models with 1-head and 5-head attention, illustrating the benefits of multiple heads on convergence speed and test loss performance.

SECTION: Discussion and Future Work
This work demonstrates the capability of memory-augmented Transformers (Memformers) to implement a broad range of first-order optimization methods, opening several research directions. We briefly comment on some of these aspects below.

: Small modifications, such as (gated) memory registers, significantly enhance Transformers’ ability to learn and implement diverse optimization algorithms. Future research could explore further architectural innovations to unlock even greater capabilities.

: While our approach successfully makes Transformers implement LFOMs on quadratic functions, future work should extend this to more general objective functions. Doing so may require novel training strategies, and possibly architectural adjustments to handle non-quadratic functions. The role of nonlinear attention and the MLP component of Transformers may also prove to be useful here.

: Attention-based methods require more computation than directly implementing conjugate gradient descent or momentum GD. However, Transformers excel in learning general parameters, enabling LFOMs to generalize across new data without needing per-instance optimization. Exploring practical use of such “learned optimizers” to either warmstart a solver, or to potentially even bypass it, is a tantalizing research topic.

: Strengthening the theoretical basis of Transformers’ optimization capabilities, including convergence analysis and their alignment with classical optimization theory, is another important direction for future research.

: The ability of Transformers to learn and generalize optimization algorithms offers exciting potential for meta-learning and transfer learning, providing new opportunities in areas where traditional optimization methods fall short.

SECTION: Limitations
We briefly remark on some limitations of our current framework. For instance, while Memformers are quite versatile, our experiments (Figures,) indicate they do not radically outperform preconditioned GD on general quadratic problems as in (), where the preconditioner matrix(and likewise,) for the current layeris the main contributor to loss performance at each update step(). On the other hand, this behavior is likely due to the task being quadratic, and a future study that tackles more general ICL formulations will likely shed light here.

Transformers can implement second-order methods like Newton’s method, which typically outperform LFOMs in convergence speed and accuracy. However, we reiterate that the main focus of our paper is to explore the space of first-order optimization algorithms that augmented Transformers can learn, as opposed to looking for “the best” algorithm.

SECTION: Reproducibility Statement
We believe the following points provide a clear path for replicating our results:

: The code for our experiments, including Memformers and LFOM implementations, is available at.

: Detailed descriptions of the training setup, model architecture, parameter initialization, and optimization methods are included in Sectionsand.

: Random seeds were fixed across all experiments to ensure consistency, and they are provided in the code repository for replication.

: All experiments were conducted on NVIDIA T4 GPUs in Google Colab.

SECTION: References
SECTION: Supplementary Material
SECTION: Proofs
SECTION: Proof of Lemma 1: Equivalence to Preconditioned Gradient Descent
This proof already exists in the literature, for instance, in Subsection C.1 of. However, we repeat it here, to make this paper as self-contained as possible.

Consider a set of fixed samples, along with a fixed vector. Letandrepresent fixed weights, and letevolve as per equation (). Defineas the firstrows of(under equation (), we havefor all), and letbe the-th row of. Now, letbe a function such that forand, the function is defined as. It’s worth noting that.

We can verify that, under equation (), the update rule foris given by:

whereis a mask matrix of the form:

The following points can be verified:

1.. To see this, note that for each, we have:

Thus,does not depend onfor any. For, the update becomes:

which clearly shows that the dependence onis additive. Through a simple induction, we can establish:

2. The functionis linear in. To see this, note that for,does not depend onfor any,, or. Therefore, the update fordepends linearly onand. Sinceis linear in, we conclude by induction that the result holds.

Considering these points, we can confirm that for each, there exists a vectorsuch that:

for alland. It follows that, so that, implying.

We now focus on the third key fact: for each, we have:

To prove this, letfor some. Then:

therefore,when. This completes the induction, given thatby definition.

Letbe the matrix whose columns are, excluding, and letbe the vector of. It follows that:

Using this, the update formula forbecomes:

leading to the update:

Sinceis arbitrary, we derive the general update formula:

Treatingas a preconditioner, and letting, we can express the update as:

Finally, let. We can verify that, implying that:

We also confirm that for any, the prediction ofis:

This concludes the proof. We have simply followed the update rule () to its logical conclusion.

SECTION: Full Proof of Proposition 1
Our goal is to demonstrate that, under appropriate parameter configurations, the memory-augmented Transformer updates given by equations () and () correspond precisely to the Conjugate Gradient Descent (CGD) algorithm when applied to the quadratic loss function:

We will establish a mapping between the Transformer’s operations and the steps of the CGD algorithm, demonstrating that the Transformer can implement CGD under certain parameter settings.

For minimizing a quadratic function, the CGD algorithm proceeds as follows:

We first recall that in the proof of Lemma 1 (), theupdate rule

is a direct downstream consequence of theupdate rule ()

under the parameterization given in equation (). Thus, theterm in theupdate equation is, in a precise sense, paralleled by theterm in theupdate equation ().

The initial statein () parallelsin ().

The memory registeris initialized to, i.e.,, corresponding to.

We set, consistent with CGD initialization.

Identifying,, and, the Transformer’s memory update matches CGD.

The scaling factoraccounts for the gradient’s scaling, consistent with the CGD update when considering the Hessian.

: Scalar values computed based on residuals and the Hessian.

:

andare treated as parameters, ensuring structural correspondence.

The Transformer’s architecture allows these as fixed or learnable parameters.

Therefore, under suitable parameter configurations, the memory-augmented Transformer can implement CGD, demonstrating the feasibility of using the Transformer’s architecture to perform CGD-like updates.

SECTION: Full Proof of Proposition 2
Our goal is to show that the memory-augmented Transformer with updates given by equations () and () can implementsteps of an LFOM, whose general formulation is:

whereare diagonal matrices that scale the gradients.

We will proceed by establishing a correspondence between the variables and updates in the memory-augmented Transformer and those in the LFOM, and by showing that, under appropriate parameter settings, the Transformer updates replicate the LFOM updates.

The first order of business is to realize that, in the proof of Lemma 1 (), theupdate rule () is a direct downstream consequence of theupdate rule (), under the parameterization given in equation ().

Setper (). Then the consequence of theupdate rule is that eachis coordinate-wise scaled by. But ifis coordinate-wise scaled by, then theupdate rule in () now instead looks like, wheredenotes the-th row of. This is because, by definition,is the-th row of().

From the basicupdate rule in (), the update formula forin () follows as a consequence. Except that now, this update formula will include a coordinate-wise scaling as well, which we will denote by:

which in turn leads toin place of () andin place of (). The negative signs can, of course, be incorporated within thes.

If we simply rewriteas a diagonal matrix in, this setup then subsumes the case of diagonal preconditionersacting on the gradients, which in the general form looks like:

whereare diagonal matrices.

The memory-augmented Transformer performs exactly these updates in the special case when the preconditionersare scalar multiples of the identity. If the preconditionersare non-trivial, then this architecture performsalgorithms that lie in a class richer than LFOMs ().

SECTION: Comparison to Nesterov Accelerated Gradient Method (NAG) and Momentum Gradient Descent (MGD)
SECTION: Nesterov Accelerated Gradient Method (NAG)
NAG is a commonly used optimization technique that builds on classical gradient descent by incorporating a momentum term that anticipates the next update. Specifically, the weights are updated using the following update rules:

Here,controls the influence of previous updates (momentum), andis the learning rate. In our experiments, we selectedandafter testing various values of these parameters on the given distribution, as in Section. These values provided the best performance. The momentum term allows NAG to “look ahead" in the optimization trajectory, which often leads to faster convergence than vanilla gradient descent.

SECTION: Momentum Gradient Descent (MGD)
Momentum Gradient Descent operates similarly to NAG but without the anticipation of future steps. The algorithm updates the weights based on a momentum term that accelerates convergence in directions with consistent gradients. The update rule for MGD is given by:

In our experiments, the learning rateand momentum parameterprovided the best results on the given distribution, as in Section. Momentum helps to mitigate oscillations in directions with high curvature, stabilizing the optimization trajectory and leading to faster convergence compared to gradient descent.

SECTION: Memformers vs. NAG and MGD
In our experiments, we observed that Memformers () outperform both NAG and MGD on non-isotropic data. Figuresandcompare the performance of Memformer with NAG and MGD, respectively, on the same non-isotropic data. As shown, the Memformer achieves faster convergence and much better loss performance compared to both algorithms.

SECTION: Memformer Experiments With More Than 4 Layers
In our experiments, we observed that Memformers with more than 4 layers continue to demonstrate impressive performance in learning optimization strategies. We conducted experiments with Memformers having up to 7 layers and dimension. Training beyond this point becomes impractical due to extensive iteration requirements and significant convergence times, which can span several hours. This limitation is a consequence of computational constraints (e.g., available GPUs) rather than any inherent deficiency of the Memformer architecture itself.

Here,refers to the rank of the square matrixin the empirical loss quadratic as described in Equation.

1.(Dimension, Layers = 5): As expected, Conjugate Gradient Descent (CGD) converges withinsteps due to the dimensionality constraint. Remarkably, even though the Memformer only learns general parameters(Equation 9) and(Equation 20), it manages to keep up with CGD for up to 4 steps, showcasing its efficiency.

2.(Dimension, Layers = 7): In this case, CGD does not converge until beyond 7 steps, which aligns with theoretical expectations. Nevertheless, the Memformer remains highly competitive, matching CGD’s performance for 6 steps and even performing comparably at 7 steps. This demonstrates the Memformer’s robust generalization capabilities, even under more complex conditions.

SECTION: Experiment on Convergence Verification for Memformer Parameterto
Our strategy to train the Memformer () was to first train the’s () in each layeron the training batch and then to “fine-tune" the’s on the training batch. Therefore, we present here an empirical verification of our results perin.

We evaluated the in-context learning (ICL) loss for linear regression withand, whereand. The covariancewas generated as, withbeing a random orthogonal matrix and. A three-layer linear transformer was trained using ADAM, withinitialized as i.i.d. Gaussian matrices. Each gradient step used minibatches of size 20,000, resampled every 100 steps, and gradients were clipped to 0.01. Results were averaged over 5 runs with independentandsamples.

To measure convergence, we computed the normalized Frobenius norm distance:

which quantifies the deviation offrom a scaled identity. The distance, averaged over 5 runs, is shown in Figures,, andas a function of training iterations.