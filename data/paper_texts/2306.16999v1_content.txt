SECTION: Spectral Batch Normalization: Normalization in the Frequency Domain

Regularization is a set of techniques that are used to improve the
generalization ability of deep neural networks. In this paper, we introducespectral batch normalization(SBN), a novel effective method to improve
generalization by normalizing feature maps in the frequency (spectral) domain.
The activations of residual networks without batch normalization (BN) tend to
explode exponentially in the depth of the network at initialization. This leads
to extremely large feature map norms even though the parameters are relatively
small. These explosive dynamics can be very detrimental to learning.
BN makes weight decay regularization on the scaling factorsapproximately equivalent to an additive penalty on the norm of the feature maps,
which prevents extremely large feature map norms to a certain degree. It was
previously shown that preventing explosive growth at the final layer at
initialization and during training in ResNets can recover a large part of Batch
Normalization’s generalization boost.
However, we show experimentally that, despite the approximate additive penalty
of BN, feature maps in deep neural networks (DNNs) tend to explode at the
beginning of the network and that feature maps of DNNs contain large values
during the whole training. This phenomenon also occurs in a weakened form in
non-residual networks. Intuitively, it is not preferred to have large values in
feature maps since they have too much influence on the prediction in contrast to
other parts of the feature map.
SBN addresses large feature maps by normalizing them in the
frequency domain. In our experiments, we empirically show that SBN prevents
exploding feature maps at initialization and large feature map values during the
training. Moreover, the normalization of feature maps in the frequency domain
leads to more uniform distributed frequency components. This discourages the
DNNs to rely on single frequency components of feature maps. These, together
with other effects (e.g. noise injection, scaling and shifting of the feature
map) of SBN, have a regularizing effect on the training of residual and
non-residual networks.
We show experimentally that using SBN in addition to standard regularization
methods improves the performance of DNNs by a relevant margin, e.g. ResNet50
on CIFAR-100 by 2.31%, on ImageNet by 0.71% (from 76.80% to 77.51%) and VGG19 on CIFAR-100 by 0.66%.

[remember picture,overlay]\node[anchor=south,yshift=10pt] at (current page.south)© 2023 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes,creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.;

SECTION: IIntroduction

Deep neural networks contain multiple non-linear hidden layers which make them
powerful machine learning systems[1]. However, such networks
are prone to overfitting[2]due to the
limited size of training data or the high capacity of the model.
Overfitting describes the phenomenon where a neural network
(NN) perfectly fits the training data while achieving poor performance on the
test data. Regularization is a set of techniques used to reduce overfitting and
is therefore a key element in deep learning[3]. It
allows the model to generalize well to unseen data.

Many methods have been developed to regularize DNNs, weight penalties as-regularization[4]and weight decay[5], soft weight sharing[6],
dropout[1], data augmentation[7]and
ensemble learning methods[8].

Normalization techniques[9,10,11,12]like batch normalization (BN)[9]normalize features by
subtracting the mean and dividing by the standard deviation computed across
different dimensions of a feature map. In some cases such normalization
techniques act as regularizers, eliminating the need for dropout[9]. There are different explanations for the regularizing
effects of BN: (i) the stochastic uncertainty of the batch statistics can
benefit generalization[11], (ii) BN reduces the explosive growth of
feature maps in deeper layers which acts as a regularizer[13]and (iii) BN discourages reliance on a single neuron
and encourages different neurons to have equal magnitude in the
sense that corrupting individual neurons does not harm generalization[14].

The feature maps (i.e. activations) of residual networks without BN tend to
explode exponentially in the depth of the network at initialization[15]. This leads to
extremely large feature map norms even though the parameters are relatively
small[13]. Large and exploding feature maps also occur in a
weakened form in non-residual networks. These explosive dynamics can be very
detrimental to learning[15].

Intuitively, it is not preferred to have large values in the feature maps since
they have too much influence on the prediction in contrast to other parts of the
feature map. Therefore some sort of scaling is needed to restrict the influence
of single outputs and to distribute the decision making process on
a larger part of the feature map.

With ormalization techniques is weight decay regularization on the scaling and
shifting factorsapproximately equivalent to an additive penalty
on the norm of the feature maps[13]. This prevents
exploding features during training to some degree. Dauphin et al.[13]showed experimentally that preventing explosive growth
at the final layer at initialization and during training can recover a large
part of BNs generalization boost.

However, in our analysis of feature maps for different networks with BN we show
that feature maps tend to explode at the beginning of the training and that
they contain large values during the whole training despite the
additive penalties. Figure1compares
the feature map norm over the course of training of a BN layer (“BN”), two
subsequent BN layers (“2BN”) and aSpectral Batch Normalization(SBN) layer following a BN layer (“BN + SBN”) in a ResNet50 and VGGtrained on CIFAR-for an arbitrary batch. The figure shows that SBN
prevents exploding feature maps at initialization and large feature map norms
during the whole training. Note that the Subfigure1has logarithmic scaling.

To prevent exploding feature maps at initialization and large feature maps
during the whole training, SBN normalizes the feature maps in the frequency
(spectral) domain.

Firstly, SBN computes the 2-dimensional discrete Fourier transform (DFT) of the
feature maps. Secondly, it computes the channel-wise mean and standard deviation
of the DFT coefficients across all frequencies and the mini-batch. Thirdly, the
frequency components are normalized. After a re-scaling and re-shifting by
learned parameters, the feature map is transformed back into the spatial domain
using the inverse DFT.

SBN prevents exploding feature maps at initialization and large values in
feature maps during the whole training which would destabilize the training and
would lead to a more complex model compared to DNNs with small feature maps.
Furthermore, SBN has more positive effects: (i) it inserts stochastic
noise in the frequency domain which makes the network more robust to small
perturbations in the frequency domain and (ii) re-scaling and re-shifting gives
the network the opportunity to weight channels in the frequency domain and (iii)
it leads to more uniform distributed frequency components in the feature maps
which discourage the network to rely on single frequency components.

In SectionIVwe will show experimentally that using SBN in
addition to BN prevents large values in the feature maps during the whole
training and therefore reduces overfitting in DNNs. The feature maps are scaled
down to smaller values in comparison to only using BN. Furthermore, we will show
that SBN increases the accuracy of networks by a relevant margin compared to
the original models.

SECTION: I-AContributions of this Paper and Applications

In this work, we presentspectral batch normalization, a novel effective
method to improve generalization by normalizing feature maps in the frequency
(spectral) domain. Our core contributions are:

Introducing spectral batch normalization.

Analyzing the impact of our method on the weights and feature maps (in the spatial and frequency domain) during the training process.

Showing experimentally that using spectral batch normalization in
addition to standard regularization methods increases the performance of various
different network architectures on CIFAR-/CIFAR-and on ImageNet. The
additional gains in performance of ResNetis on CIFAR-by, on
CIFAR-by, on TinyImageNet byand on ImageNet byare worth noting. Moreover, the performance of non-residual networks is also
improved, e.g. VGGon CIFAR-by.

SECTION: IIRelated Work

SECTION: II-ARegularization

Regularization is one of the key elements of deep learning[3], allowing the model to generalize well to unseen
data even when trained on a finite training set or with an imperfect
optimization procedure[16]. There are several techniques to
regularize NNs which can be categorized into groups. Data augmentation
methods like cropping, flipping, and adjusting brightness or sharpness[7]and cutout[17]transform the training dataset to avoid overfitting.
Regularization techniques like dropout[1],
dropblock[18], dropconnect[2]drop neurons or weights from the NN during training to
prevent units from co-adapting too much[1]. Furthermore, NNs
can be regularized using penalty terms in the loss function. Weight decay[5,19]encourages the weights of the NN to be small in magnitude. The-regularization[4]forces
the weights of non-relevant features to zero.

SECTION: II-BNormalization

Normalization methods are an essential building block of the most successful
deep learning architectures and have enabled training very deep residual
networks[15].

Batch normalization (BN)[9]normalizes features by
subtracting the mean and dividing by the standard deviation computed within a
mini-batch. The normalization is done to reduce the internal covariate shift,
i.e. the change in the distribution of network activations due to the change in
network parameters during training. The features are then scaled and shifted by
learned parameters to maintain the expressive power of the network[3]. During the training, a moving average of the means
and variances is computed which are then used during inference. Batch
normalization enables faster and more stable training of deep NNs. Moreover, it
also acts as a regularizer, in some cases eliminating the need for dropout.

The effect of batch normalization is dependent on the mini-batch size and it is
not obvious how to apply it to recurrent neural networks. Layer normalization[10]computes the mean and variance used for normalization from all
the features in a layer on a single training case. It improves the training time
and generalization performance of several sequential models (RNN[20]).

Instance normalization[12]normalizes across the width and
height of a single feature map of a single example, i.e. not across the channels
like layer normalization. It works well for generative models.

Layer normalization and instance normalization have limited success in visual
recognition. Therefore, Wu et al.[11]present group normalization
which avoids normalizing along the batch dimension but divides the channels into
groups and computes the mean and variance within each group for normalization.

Weight normalization[21]is a weight reparameterization
approach that accelerates the convergence of SGD optimization. It
reparameterizes the weight vectors of each layer such that the length of those
weight vectors is decoupled from their direction. In detail, they express the
weight vectorbywhereis the new weight vector andthe
scalar parameter.

There is a number of works that analyzed the reasons for the success of
normalization methods. In the case of BN, Ioffe et al.[9]stated that it reduces the internal covariate shift, i.e. the change in the
distribution of each layer’s input during training. However, Santurkar et al.[22]demonstrate that the distributional stability of
layer inputs has little to do with the success of BN. Instead, they point
out that BN makes the optimization landscape significantly smoother. This would
induce a more predictive and stable behavior of the gradients, allowing faster
training. It has been shown that the activations of residual networks without BN
tend to explode exponentially in the depth of the network initialization[15]. Preventing the explosive growth
at the final layer at initialization and during the training can recover a large
part of BNs generalization effect[13].

SECTION: II-CFrequency

Due to the dual of the convolution theorem, stating that multiplication in the
time/spatial domain is equivalent to convolution in the frequency domain, the
DFT is used in deep learning to provide a significant speedup in the computation
of convolutions. Moreover, Rippel et al.[23]showed that the
frequency domain also provides a powerful representation to model and train
CNNs. They introduced spectral pooling which performs a dimensionality
reduction by truncating the representation in the frequency domain.

The DFT is also used to regularize DNNs. Spectral dropout[24]prevents overfitting by eliminating weak Fourier domain coefficients below a
fixed threshold and randomly dropping a fixed percentage of the remaining
Fourier domain coefficients of the neural network activations.

Furthermore, the frequency domain can be used to prune NNs[25].

SECTION: IIIMethod

In order to counteract exploding or large feature maps, we want to use a
frequency decomposition. It allows manipulating an input across its various
length-scales of variation, and as such provides a natural framework for the
manipulation of data with spatial coherence[23].

Inspired by batch normalization[9], we introducespectral batch normalization, which has a regularizing effect on DNNs, by
normalizing feature maps in the frequency domain. SBN prevents explosive
dynamics during the initialization and leads to smaller feature maps during the
whole training. To introduce spectral batch normalization, we need the discrete
Fourier transformation.

SECTION: III-AThe Discrete Fourier Transform

The discrete Fourier transform converts an input array of real (or
complex) numbers into another sequence of complex numbers. In our case, it is a
frequency domain representation of the original spatial input sequence. The 2D
DFT of a matrixis defined by

forand. Its inverse transform is given by, i.e. the conjugate of the transform normalized by.

Intuitively, the DFT decomposes an input array into the frequencies contained
in the input sequence. Roughly speaking, it compares a basis of complex
sinusoidal functions with the input sequence and computes the similarity between
the complex sinusoidal functions and the input sequence.

The DFT of a real signal is Hermitian-symmetric, i.e.. Therefore, onlycomplex numbers are needed to represent the real signal in the frequency domain.
This does not reduce the effective dimensionality of the transformed input
since each DFT coefficient consists of a real and imaginary component.

Before we explain spectral batch normalization, we have to discuss how to
propagate the gradient through a Fourier transform layer. This is well described
by Rippel et al.[23]. Letbe the input andbe the output of a DFT. Moreover, letbe a real-valued loss function applied
towhich can be considered as the remainder of the forward pass.
The DFT is a linear operator, therefore its gradient is the transformation
matrix itself. During back-propagation, the gradient is conjugated[26]. This corresponds to the application of the inverse transform

We use the real 2D DFT implementation of PyTorch[27], which
uses the FFT algorithm to compute the transformation. The implementation of the
inverse real 2D DFT in PyTorch uses zero-padding to get the original array sizefor signals with odd length in a transformed dimension.

SECTION: III-BSpectral Batch Normalization

Letbe a feature map.
The introduced spectral batch normalization block first computes the real
2-dimensional discrete Fourier transformof the last two
dimensions of the feature map. Secondly, it computes the
channel-wise mean and standard deviation of the DFT coefficients across all
frequencies and a mini-batch. Hence, the resulting mean and
variance have size. During the training, a
moving average of the means and standard deviations is computed which are then
used during inference. Then, the transformed feature maps are normalized
channel-wise using the computed mean and standard deviation (like in batch
normalization[9]). To recover the representation power of
the layer, the feature maps are scaled and shifted channel-wise by learnable
parametersand. After the
normalization and scale/shift step, the feature map is transformed back using
the inverse real DFT. SBN is presented in Algorithm1.

At this point, the question could arise why we compute thechannel-wisemean and standard deviation of the feature maps across a mini-batch, i.e. the
resulting mean and variance has size.

Different channels in a feature map are often independent and represent
different information depending on the input. Therefore, it is not beneficial to
normalize across different channels. Moreover, computing the mean and
standard deviation across a mini-batch and the channels andnotover the frequency components leads for some datasets (e.g. ImageNet) to a huge
number of additional scaling and shifting weights (i.e.).

We now have a short look at the effects of SBN on the frequency
components in the feature map. The resulting values of a DFT
are complex numbers. Letbe a complex number. Every
nonzero complex number can be written in the form,
whereis themagnitudeof, andis thephase, angleorargumentof. The magnitude ofrepresents the amplitude of the corresponding
frequency. Hence, the magnitude gives us information about which frequency
components are mainly represented in our feature map. The phase ofrepresents the phase of the corresponding frequency, i.e. the spatial delay for
that frequency in the feature map. Due to the normalization and
scaling/shifting, the magnitude and the phase of the frequency components are
affected by SBN.

SBN computes the mean and variance of the complex numbers using

Then the mean is substracted from the DFT coefficients. The real and complex
part of the DFT coefficients are changed separately to have zero mean (see
Equation (3)). This leads to DFT coefficients which are distributed
around. Subsequently, the DFT coefficients are divided by the standard deviation.

These steps make the magnitudes of the frequency components smaller which then
leads to smaller values in the feature maps in the spatial domain. Moreover, the
frequency components become more uniformly distributed, i.e. the importance of
frequency components with large magnitudes is reduced. On the other hand, the
frequency components with low magnitudes get a higher influence on the feature
map. The effects on the phase are difficult and cannot be stated clearly.
The effects of SBN on the the frequency components are discussed
empirically in SectionIV-D2.

Lastly, we want to point out where to insert a SBN layer. Empirically, it is
preferred to insert the SBN layer in deeper layers after the BN layer. More
details are discussed in SectionV-C.

SECTION: III-CRegularizing effects

There are several components of SBN which act as a regularizer during training.

As stated in the introduction, the activations of residual networks without BN
tend to explode exponentially in the depth of the network at initialization[15]which can be
detrimental to learning.

Batch normalization prevents exploding feature maps during the training to a
certain degree by normalizing the feature maps. Dauphin et al.[13]showed that most of the regularizing effect of BN comes
from the prevention of explosive growth from cascading to the output during
training.

Our analysis of feature maps for different networks (with and without residual
connections) trained with BN on different datasets showed that large values in
feature maps occur during the whole training despite the usage of BN.

Using SBN in addition to BN reduces the explosive growth of feature
maps further due to the normalization process in the frequency domain. The
feature maps are scaled down to smaller values compared to only using
BN (see SectionIV).

SBN subtracts a random value (the mean of the mini-batch across each feature map
and mini-batch) from each DFT transformed feature map. Moreover, SBN divides
each DFT transformed feature map by a random value (the standard deviation of
the mini-batch across each feature map and mini-batch) at each step of training.
Because different examples are randomly chosen for inclusion in the mini-batch
at each step, the standard deviation and mean randomly fluctuate.
Both of the sources of noise manipulate the feature maps in the frequency
domain. Hence every layer has to learn to be more robust to a lot of variation
in its inputs[3]. Therefore the stochastic
uncertainty of the batch statistics acts as a regularizer during training[10,11].

As in the standard BN, we introduce weightsand biasesto
recover the representation power of feature maps in the frequency domain by
multiplying the weights and biases with the DFT coefficient. Since weights and
biases are learned through SGD, the network learns to prioritize the channels of
the feature maps in the frequency domain.

As stated in SectionIII-B, SBN leads to more uniformly distributed
frequency components. Hence, the influence of strong frequency components
characterized via high magnitudes is reduced. On the other hand, the importance
of weak frequency components is relatively increased. The effect has a
regularizing effect on the training, since it discourages the DNN to rely on
single frequency components. This is the key difference between SBN and BN,
because BN normalizes feature maps only in the spatial domain.

SECTION: IVExperiments

We experimentally validate the usefulness of our method in supervised image
recognition. We compare DNNs using SBN in addition
to standard regularization methods against DNNs using only standard
regularization methods.

ResNets consist of four modules made up of basic
and “bottleneck” building blocks, a convolutional layer at the beginning of the
network and a linear layer at the end of the network. VGGs have a similar module
structure. Through our experiments, we figured out that SBN should preferably be
inserted in deeper layers of DNNs. The additions to the names of the models
describe in which modules SBN is applied. For example ”ResNet+ SBN“
means that, a ResNetnetwork is used where in the third and fourth module a
SBN layer is inserted after each BN layer.

In order to achieve a fair comparison, we also added results of ResNets with two
BN layers in the same modules where SBN is inserted, e.g. “ResNet+BN” is the abbreviation for a ResNetwith two BN layers, one after
the other, in the third and fourth module.

SECTION: IV-AImage Classification on CIFAR-/

We evaluate the performance of SBN on the CIFAR-/classification
datasets[28]. The CIFAR-/datasets consist of 50,000 training and 10,000 testcolor images.
We used various different network architectures, the CNNs ResNet,
ResNet, ResNet[29]and VGG/-BN[30].

The experiments were run five times with different random seeds forepochs, resulting in different network initializations, data orders and
additionally in different data augmentations. For every case we report the mean
test accuracy and standard deviation. We used a-split between training
examples and validation examples and saved the best model on the validation set.
This model was then used for evaluation on the test dataset.

The baseline networks ResNet, ResNetand ResNetwere trained with
data augmentation, weight decay and early stopping. The baseline network
VGG-BN and VGG-BN were trained with data augmentation, weight decay,
early stopping and dropout in the fully connected layers.

For our experiments we used PyTorch 1.10.1[27]and one
Nvidia GeForce 1080Ti GPU.

All networks were trained with a batch size of. We used the SGD optimizer
with momentumand initial learning rate. For ResNet, ResNetand ResNettrained on CIFAR-we decayed the learning rate byat
epochandand used weight decay with the factor(as in[29]). For all networks trained on CIFAR-and for
VGG/with BN trained on CIFAR-, we decayed the learning rate byat epochs,andand used weight decay with the factoras in[31]. Dropout is used in the fully connected
layers in VGG16-BN and VGG19-BN with a dropout rate of. We want to point
out that we have not done a hyperparameter search for the learning rate, weight
decay, etc. We used the same setting as in[29,31].

The training data is augmented by using random crop with
sizeand padding, random horizontal flip and normalization[7]. The test set is only normalized. The ResNet networks are
initialized Kaiming-uniform[32]. In the VGG networks the
linear layers are initialized Kaiming-uniform. The convolutional layers are
initialized with a Gaussian normal distribution with meanand standard
deviationwhereis the size of the kernel multiplied with the
number of output channels in the same layer.

TableIpresents the results of our experiments on CIFAR-.
Using SBN improves the accuracy for all ResNets and VGGs. The additional gain in performance by using SBN in the ResNet50 network () is worth noting. Using two BN layers increases the performance in some cases.
However, the performance gains are small compared to those of SBN.

TableIIshows the results of our experiments on CIFAR-.
Similar to the experiments on CIFAR-, ResNets generalize worse if more
parameters are trainable. Additionally, the gain in performance by using SBN in
the ResNet50 network () and VGG19 () are worth noting. The use
of two BN layers slightly increases the performance in some cases. However,
the performance gains are small compared to those of SBN.

SECTION: IV-BImage Classification on TinyImageNet

TinyImageNet[33]contains
100,000 images of 200 classes downsized tocolored images. Each class has 500 training images and 50 validation images.

The experiments were run five times with different random seeds for 300 epochs
using ResNetand ResNet. Following the common practice, we report the
top-1 classification accuracy on the validation set.

For our experiments we used PyTorch 1.10.1[27]and one
Nvidia GeForce 1080Ti GPU.

We trained all the models using the SGD optimizer with momentumand
initial learning ratedecayed by factorat epochs,, and. Moreover, we used weight decay with the factor. The batch size is
set to.

The training data is augmented by using random crop with size, random
horizontal flip and normalization. The validation data is augmented by resizing
to, using center crop with sizeand normalization.

TableIIIpresents the top-1 accuracy of our experiments on
TinyImageNet. ResNet18 using SBN in the third and fourth block outperforms the
standard ResNet18 by. Using two BN layers in the third and fourth block
slightly improves ResNetbut slightly worsens the performance of ResNet.
The additional gain in performance for ResNetis worth noting since using SBN improved the baseline by.

SECTION: IV-CImageNet

The ILSVRC 2012 classification dataset[34]contains 1.2
million training images, 50,000 validation images and 150,000 test images.
They are labeled with 1,000 categories.

Regularization methods on ImageNet require a greater number of training epochs
to converge[18]. The experiments were run three times
with different random seeds for 400 epochs using ResNet. Following the
common practice, we report the top-1 classification accuracy on the validation
set.

For our experiments we used PyTorch 1.10.1[27]and 4
Nvidia GeForce 1080Ti GPU.

We used the same hyperparameter setting as[35]. We trained all
the models using the SGD optimizer with momentumand initial learning
ratedecayed by factorat epochs,,,and. Moreover, we used weight decay with the factor. The batch size is
set to 256.

The training data and validation data is augmented as described for
TinyImageNet.

TableIVpresents the top-1 accuracy of our experiments on
ImageNet. Using SBN in the last block of ResNet50 increases the accuracy byon ImageNet. The usage of two BN layers increases the performance only
by. This shows that it is advantageous to use SBN.

SECTION: IV-DEffects on the Training

We used different measures to analyze the impact of SBN on the training of the
networks: (i) we tracked the weight/gradient norm and weight/gradient
distribution for every layer, (ii) we measured the feature map norm and
distribution after every convolution, BN and SBN layer, (iii) we analyzed the
effects of SBN on the magnitude and phase of the frequency components of feature
maps.

We did not see any differences in the weight/gradient norm and weight/gradient
distribution between networks with and without SBN. Moreover, there were no
clear differences in the phases of the frequency components of feature maps
before and after a SBN layer.

However, there were strong differences in the feature map norms and
distributions between DNN with and without SBN. Furthermore, there were large
differences in the magnitudes of the frequency components of feature maps before
and after a SBN layer.

We measured the feature map norm and distribution by looking at the feature map
for an arbitrary batch during training. Figure1shows the feature map norm per epoch of three
networks: (i) a standard ResNet, (ii) a ResNetusing two BN layers and
(iii) a ResNetusing BN and SBN. The output of a SBN layer has a much
smaller feature map norm compared to the output of one or of two BN layers. This
can be seen also in1for the non-residual network
VGG.

Figure2shows the distribution of values in a
feature map after a BN layer of a ResNettrained on CIFAR-without SBN
and after the corresponding SBN layer of a ResNettrained
on CIFAR-with SBN for an arbitrary batch. Without SBN feature maps have
high values during the whole training. Using SBN prevents large and exploding
values in feature maps.

We analyzed the effects of SBN on the magnitudes of the frequency components of
feature maps. We looked at the feature maps of different images before and after
the SBN layer in fully trained networks. The feature maps were transformed using
the DFT and shifted such that the DC term is moved to the center of the tensor.

Figure3shows the magnitudes of frequency
components of different channels in a feature map after a BN layer and after the
SBN layer behind it for one arbitrary image from CIFAR-. It shows
how the magnitude are manipulated due to the SBN layer. SBN reduces the
relevance of frequency components with high magnitudes, i.e. the importance of
frequency components with low magnitude is increased. Hence, the magnitudes of
the frequency components become more uniform distributed. This discourages
reliance on a single frequency component and encourages the network to focus on
more frequency components in the prediction process. Figure3generalizes to other images and is not an unique
phenomenon.

SECTION: VAblation Study / FAQ

SECTION: V-ADoes SBN allow higher learning rates?

Similar to BN, SBN allows the usage of higher learning rates. We tested
initial learning rates ofandwith ResNeton
CIFAR-/over three runs and could achieve better results using BN and SBN than only using BN (see TableV). The accuracy of ResNetwith SBN on CIFAR-using a learning rate ofis lower than the baseline, however, the baseline algorithm did not perform well either.

SECTION: V-BIs there an acceleration of the training? How high is the computational overhead of SBN?

For easy tasks (e.g. training ResNet//on CIFAR-) we saw a huge
acceleration of the training measured per epoch (see Figure4). However, due to the Fourier transformations,
the training time is longer compared to the baseline. The training time overhead
of SBN depends on the network architecture. Clearly, if more SBN layers are
used, the training time overhead increases. TableVIshows the
training time overhead of ResNets on CIFAR-/CIFAR-and ImageNet. Note
that we use the real 2D DFT implementation of PyTorch[27],
which uses the FFT algorithm to compute the transformation.

SECTION: V-CWhere to insert SBN?

We performed an ablation study to find the perfect position for inserting a SBN
layer. Firstly, it is preferred to insert a SBN after a BN layer. Inserting SBN
prior to a BN layer does not improve generalization since the BN layer scales
the feature maps up again. Hence, the input feature map of the following
convolutional layer again has a large feature map norm due to the normalization
process of BN. Using a SBN without a BN layer does not improve the
generalization since SBN does not replace BN. The normalization process in the
spatial domain is needed for the training. However, using SBN in addition to BN
increases the performance as shown in the previous experiments.

Beyond that, it is favorable to insert SBN in deeper layers, e.g. in ResNets
for example in modules 3/4. The reason for that is the phenomenon of large
or exploding feature maps happening more often in deeper than in shallow layers[15]. However,
inserting SBN after every BN layer in ResNet18/34/50 did also improve the
accuracy on CIFAR-and CIFAR-but we got the best results applying SBN
only in the modules 3/4. When the task gets even harder (e.g. ImageNet), it is
preferred to insert it only in the fourth block. Otherwise, the regularizing
effects are to strong.

SECTION: V-DIs it sufficient to only subtract the expectation?

The generalization boost can be partly reproduced by only subtracting the
expectation. However, it does not work as well as applying the whole SBN
algorithm.

SECTION: V-EIs it sufficient to only divide by the standard deviation?

Same answer as for the question above.

SECTION: V-FIs it sufficient to only normalize the feature map in the frequency
space without re-scaling and re-shifting?

Same answer as for the question above.

SECTION: V-GIs it sufficient to only down-scale the feature map in the frequency
representation?

The generalization boost can be partly reproduced by downscaling the frequency representation. However, it does not work as well as
applying the whole SBN algorithm. Moreover, this introduces one or more
hyperparameters depending on whether one decides to divide all feature maps by one single value or different feature maps by different values.

SECTION: V-HIs it sufficient to only-down scale the feature map?

Same answer as for the question above.

SECTION: V-IIs it sufficient to only use weighting of the feature maps in the
frequency representation?

The generalization boost cannot be reproduced by only weighting the feature
maps in the frequency domain. However, we need the weighting in the algorithm
since only normalizing the feature map in the frequency domain does not
reproduce the whole generalization boost.

SECTION: V-JIs it favorable to normalize real and imaginary part of the frequency components separately?

It is not favorable to normalize the real and imaginary part separately as it
has negative effects on the performance.

SECTION: VIConclusion and Future Work

We presented spectral batch normalization (SBN), a novel effective method to
improve generalization by normalizing feature maps in the frequency (spectral)
domain. SBN prevents large feature maps, introduces stochastic noise in the
frequency domain of the feature maps and leads to more uniform distributed frequency components. These effects act as regularizers during training.

Using SBN in addition to commonly used regularization methods (e.g.[7,9,5,19,1]) increases the performance of ResNets and
VGGs on CIFAR-, CIFAR-and ImageNet. The additional gain in performance
of ResNeton CIFAR-by, on CIFAR-byand on
ImageNet byare worth noting.

We have not explored the full range of possibilities of SBN. Our
future work will include applying SBN to other normalization techniques, e.g.
Layer Normalization[10], Group Normalization[11], Instance
Normalization[12]. Moreover, we will perform further
investigations to analyze the impact on the loss landscape as in[22]. This could be extended to a theoretical analysis of
SBN where we analyze the impact on the Lipschitz continuity of the loss. Furthermore, the weightsand biasescould be learned in the complex domain which could be advantageous for training.

SECTION: References