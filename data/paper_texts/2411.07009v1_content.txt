SECTION: Hierarchical Conditional Tabular GAN for multi-tabular synthetic data generation

The generation of synthetic data is a state-of-the-art approach to leverage when access to real data is limited or privacy regulations limit the usability of sensitive data. A fair amount of research has been conducted on synthetic data generation for single-tabular datasets, but only a limited amount of research has been conducted on multi-tabular datasets with complex table relationships. In this paper we propose the algorithm HCTGAN to synthesize multi-tabular data from complex multi-tabular datasets. We compare our results to the probabilistic model HMA1. Our findings show that our proposed algorithm can more efficiently sample large amounts of synthetic data for deep and complex multi-tabular datasets, whilst achieving adequate data quality and always guaranteeing referential integrity. We conclude that the HCTGAN algorithm is suitable for generating large amounts of synthetic data efficiently for deep multi-tabular datasets with complex relationships. We additionally suggest that the HMA1 model should be used on smaller datasets when emphasis is on data quality.

KeywordsGAN, HCTGAN, Multi-tabular data, Relational data, Synthetic data, Deep learning, Machine Learning

SECTION: 1Introduction

Data is ubiquitous in todayâ€™s modern society, and being able to opt-out of sharing your data with services is made more and more difficult each day. Nevertheless, we have seen great initiatives and regulations in the last decade, like the GDPR[9]in the EU and PIPA[16]in South Korea, whose goals are to enforce standards for individuals data privacy. But providing greater privacy standards for individuals does not come cheap for parties that aim to leverage the data, as new processes for using the data has to be established, and a lot of the data is now rendered unusable or inaccessible for development and analytical initiatives. Synthetic data is today being actively researched within the field of machine learning, and is an approach to leverage in businesses where usability of real data is limited. Furthermore, it has showed to be an effective method to use for speeding up internal data processes whilst minimizing privacy risks[24,34].

Tabular data is a de-facto standard used throughout the industry to store data in a column and row format. Typically, each row represents an object and each column represents an attribute of the object. The rows are usually identified by a unique identifier, ID, which is stored as a column in the table and is referred to as a primary key of the table. If data is stored in multiple tables which are connected, this is referred to as relational data, or multi-tabular data. To represent a relationship between two tables, the child table has to contain a reference to some object in the parent table. This is accomplished by introducing a new column in the child table which stores so called foreign keys. The foreign keys reference an object in the parent table, more specifically, they reference the parents primary key. Parent tables can have multiple children, and vice versa for children, which means that there could be highly complex relationships in a multi-tabular database.

Machine learning research on synthetic data has thus far showed promising results, but the large majority of that research only deals with non-relational data[38,37,27]. Hence, there is a lack of established research on multi-tabular algorithms, and the little research that does exist mainly proposes to use probabilistic models. However, these models do not scale well for largely connected multi-tabular datasets, neither in the training nor in the sampling process. Furthermore, there is no guarantee that the generated synthetic data has referential integrity between tables[3]. Apart from probabilistic models, plenty of research has been conducted on Generative Adversarial Networks (GANs) for synthetic data generation, but mainly on single table datasets[13]. The proposed GAN models are an attractive choice whenever sensitive data is to be synthesized, because the generator never operates on any real data, and these models can be extended to incorporate various levels of differential privacy[4,36,7,33].

SECTION: 2Related work

The GAN framework is comprised of two neural networks that are trained adversarially in a zero-sum game. The generatoris trained to decode latent space noise to resemble the real target data, and a discriminatoris trained to distinguish between the generated data and the real data[12]. The framework has been researched extensively in a wide range of areas, e.g., speech generation with HiFi-GAN[2,20], image generation with DCGAN[29,15], image-to-image translation with SPA-GAN and Pix2Pix[8,17], and has seen a number of extensions in the form of StackGAN and InfoGAN to name a few[28,30,26,22,39,31]. The GAN framework has been employed for tabular data generation tasks, where an initial model TGAN showed promising results and has subsequently been iteratively improved[38,21].

The CTGAN model improves the TGAN model by introducing three novel concepts: mode-specific normalization, a conditional generator, and training by sampling. These concepts together drastically improves both model training and the quality of the synthesized tabular data, by dealing with the problems of class imbalance and complicated column distributions[37]. The CTAB-GAN model improves CTGAN by including the so-called classification and information loss. These modifications are said to improve semantic column integrity and stability in training[40]. These tabular models are based on the WGAN modifications and incorporates the Wasserstein metric in the loss function and modifies the discriminator to not discriminate instances. Instead, it is often referred to as a criticbecause it outputs a number representing the credibility of the generated sample. The WGAN extension improves gradient behavior and simplifies the optimization process of the generator[1]. Techniques from the PacGAN framework were also utilized in the CTGAN model in order to prevent mode collapse, thus, improving the diversity of the generated samples[23].

The RC-TGAN model was proposed to deal with complex multi-tabular datasets by conditioning the input of the child table generation process on previously generated parent rows, specifically, the features of the parents. This is done in order to transfer relationship information from the parent to the child table. However, conditioning the child generator on row features from the parent tables leads to the generators being exposed to real data, and thus, potentially vulnerable to leakage of sensitive data[13,10]. Utilizing transformers for sequence-to-sequence modeling of relational tabular data was recently proposed and shows promising results as a substitute to real data for machine learning tasks. The authors utilize the parent tables sequence-to-sequence network as encoder for training its child tables, speeding up training times. Furthermore, utilizing the trained parent encoder allows information learned about the parent table to transfer to the decoder of the child table. However, exposing the encoder and decoder networks to the real data samples again leaves the model vulnerable[32,11,35].

SECTION: 3HCTGAN

The Hierarchical Conditional Tabular GAN (HCTGAN) is our extended version of the CTGAN model and introduces information transfer between the parent- and child table generator networks to promote relational learning. This work introduces two novel algorithms, one for training and one for sampling. Both algorithms are suitable for synthesizing large, and arbitrarily complex, multi-tabular datasets. The sampling algorithm retains the original database structure and ensures referential integrity in the synthesized data between relating tables. We will first present some technical background on the CTGAN model and then introduce the hierarchical elements which make up the HCTGAN model.

SECTION: 3.1CTGAN

The generator and critic networks in the CTGAN model aim to respectively minimize and maximize the Wasserstein GAN losswith gradient penalty and packing, which is formally described as

whereis the generator distribution,is the real data distribution, andis the uniform distribution of samples between pairs of points fromand[14].

Mode-specific normalizationis a technique used to model columns with complicated distributions[6]. The number of modesfor each continuous columnis estimated with a variational Gaussian mixture model. The estimated number of modesis used to fit a Gaussian mixture for the columnsuch that the learned mixture becomes

whereis valuein,is modeof the mixture model,is the probability density for the mode,andrepresents the mode weight and standard deviation respectively. One mode is sampled from the probability densitiesof the mixture model and is then used to normalize the valueto produce the scalar

The scalar is used together with the one-hot vector, which indicates the sampled mode, to represent the row data according to

where categorical and discrete values are represented as the one-hot vectoranddenotes concatenation.

Conditional generatorintroduces three primary concepts: the conditional vector, the generator loss, and training-by-sampling. The conditional vectoris a masked concatenation of all categorical and discrete one-hot encoded vectorsaccording to

where the mask vectormarks the index of columnwith valuewith aandfor all other values, when subject to the condition. For example, given the two discrete columnsandsubject to the conditionthe resulting conditional vector would become. The sampling of the conditional vector is drawn from a log-frequency distribution of each category. Conditioning the input forevenly on categorical and discrete values is necessary in order to learn the real conditional distribution as

The generator loss is introduced to allowto produce any set of discrete vectorsduring training . Meaning, the conditioncan be violated such that. The cross-entropy betweenand the generatedis introduced to penalize the loss, drivingto make a direct copy ofinto[37].

SECTION: 3.2Hierarchical modeling

Letdenote the set of parents for table. Given a tablewith no cyclic relations, and at least one relation, the HCTGAN algorithm generates synthetic data row-wise according to the following conditional probability distribution

whereis a concatenation of column-wise Gaussian noise for each column in the parent tables, such that

Becauseneeds to condition on the categorical and discrete values evenly to learn the real data distribution, we adhere to the generator loss and training-by-sampling process proposed by CTGAN. However, in HCTGAN we also condition on the noise vectoras a means to introduce information from parent tables. Since the conditional vectorandare independent, the joint conditional distribution in equation7can be marginalized independently. This leads to two tractable probability distributions and whenconditions evenly onthe real conditional distribution can be learned as in equation6.

The generatorsfollow a similar residual decoder network architecture as proposed in the CTGAN paper, with batch normalization andReLUbeing applied before each residual connection, activation functiontanhfor numerical columns andgumbel softmaxfor categorical and discrete columns[18]. The criticsalso follow the same network architecture as proposed in the CTGAN paper, with dropout andLeakyReLUbeing applied after each affine layer[37]. A schematic overview of the HCTGAN training process for a tablecan be seen in figure1, and pseudo code for the training and sampling algorithms can be found in appendixAandBrespectively.

SECTION: 4Results

The following sections describes the dataset and metrics used for evaluating our proposed HCTGAN algorithms, followed by the experimental results and a discussion.

SECTION: 4.1Datasets

Given that the HMA1 model scales poorly for datasets with many relational tables, the amount of datasets available for comparison is limited. Further research on larger datasets when solely using the HCTGAN model would be interesting to investigate in future work. We therefore used three datasets in the evaluation process which are manageable by the HMA1 algorithm and are publicly available through the SDV Python library[25]:

University_v1is a two-level dataset with 5 tables in total, containing three parent- and two child tables. The child tables both have two parents each, and one parent table has two children. They contain both one-to-one and one-to-many relationship types.

Hepatitis_std_v1is a two-level dataset with 7 tables in total, containing four parent- and three child tables. The child tables all have two parents each, one parent table has three children, one has two children, and the other two parents only have one child table. The tables contain both one-to-one and one-to-many relationship types.

Pyrimidine_v1is a two-level dataset with 2 tables, one parent- and one child table. The relationship type is one-to-many.

SECTION: 4.2Metrics

Several established metrics from the SDMetrics Python library by SDV were used to evaluate the performance of our proposed algorithms[5]. These are defined as follows:

KSComplement (CS)computes the similarity between realand synthetic numerical databy using the Two-sample Kolmogorv-Smirnov statistic. Given the two estimated CDFsand, whereandare the CDFs respective sample sizes, the KS statistic is calculated accordingly

whereis the supremum of the set of absolute distances between the two CDFs. The KSComplement metric is normalized to the range, and the KS statisticis inverted such that a higher score means higher quality data.

TVComplement (CS)computes the similarity between realand synthetic categorical databy utilizing the Total Variation Distance (TVD). The first step is to compute the frequency of all categories, and then one can calculate the TVD as follows

whererepresents all possible categories in column,andrepresents the frequency of those categories for the real and synthetic data respectively. The TVD score is normalized to the rangeand inverted such that a higher score means higher quality data.

CorrelationSimilarity (CPT)measures the correlation between a pair of numerical columnsby computing either the Pearson- or Spearman Coefficient. The metric is calculated accordingly

whereis the Pearson- or Spearman Coefficient. The metric is normalized towhere a higher score indicates that the pairwise correlations are similar.

ContingencySimilarity (CPT)measures the correlation between a pair of categorical columnsby computing a normalized contingency table for the realand synthetic data. The table represents the frequencies of categorical combinations inand. The total metric is calculated in the following way

whereandrepresents all possible categories in columnsand,andrepresents the frequency of those categories for the real and synthetic data respectively. The metric is normalized towhere a higher score means that the contingency table is similar between the real and synthetic data.

CardinalityShapeSimilarity (PCR)measures how similar the cardinality of primary- and foreign keys are between realand synthetic data. Practically it computes the number of child foreign keys that each parent primary key is connected to, denoted by. Doing this for all primary- and foreign key relations yields a numerical distribution. The KSComplement is then used to measure how similar the real and synthetic data cardinality distributions are as follows

We denote this metric as Parent Child Relationship (PCR) in the results. The metric is bounded by the rangewhere a high score indicates that the synthetic data relation cardinalities is similar to the real data.

RangeCoverage (RC)measure how well numerical synthetic datacovers the entire range of real values, column-wise. The score is calculated with the following formula

The metric can take on values belowif the synthetic data has terrible range coverage. Nonetheless, the resulting score is thresholded to always be positive and in the range, where a higher score indicates that the synthetic data covers the entire range of numerical values.

CategoryCoverage (RC)measures how well categories are present in the categorical synthetic datacompared to the real data, column-wise. The metric is calculated by computing the number of unique categoriesin the synthetic column, and dividing by the number of unique categoriesin the real column. The score is defined on the rangewhere a high score indicating that the synthetic data covers the real data categories well.

NewRowSynthesis (NRS)measures the total amount of synthetic data rows which are novel by looking for matching rows. The metric works differently depending on the type of the data. For categorical data the values must exactly match, whereas for numerical data the values are scaled and considered a match whenever the real data is within some % of the synthetic data (default is 1%). The score is defined on the rangewhere a high score indicates that the synthetic data is novel.

BoundaryAdherence (BA)measures how well the numerical synthetic datacomplies with the min- and max values of the real data. The min- and max values of the real data are found, and the frequency of synthetic datawhich lie within the valid range constitutes the metric. The metric takes values in the rangewhere a high score indicates that the synthetic data follows the min- and max values of the real data.

SECTION: 4.3Experimental results

The HMA1 model was fit
according to its specified procedure, and the HCTGAN models were trained for 50 epochs using the Adam optimizer for all GANs with weight decay, learning rate, and decay rates[19]. The models were set to synthesize approximately the same amount of rows for each dataset respectively. All experiments were run fortimes using different seeds, and the presented metrics are aggregated mean and standard deviation of each experiment.

SECTION: 4.4Discussion

A clear trend is that the HMA1 model achieves slightly better quality scores (CS & CPT), or performs almost equally to, the HCTGAN model on all three datasets3. However, sampling using the HMA1 model takes significantly longer time than with the HCTGAN algorithm, primarily due to the fact that HMA1 uses recursion to model relating tables. Furthermore, the HMA1 model does not guarantee referential integrity (RI) on the generated synthetic data. Although we can see that the HMA1 model manages to achieve referential integrity for the Pyrimidine_v1 dataset, generating synthetic data that does not have referential integrity could potentially break important database systems if used without care. This is why we believe referential integrity is a critical attribute of synthetic data that should always be considered during evaluation, and is an aspect that has unfortunately so far been overlooked in prior research.

The HCTGAN sampling algorithm produced data that always covered the numerical and categorical ranges better than the HMA1 model. However, HCTGAN also produced data that not always adhered to the boundaries of the real data. One could attribute the good RC scores to the HCTGAN models being poor at following min- and max boundaries, which the results suggest. However, a dataset is usually only a subset of all possible and valid observations, so it does not mean that the data in reality would be bounded to the same min- and max values that the dataset is bounded by. Thus, by using the HCTGAN models for synthetic data generation one could find outliers and new types of samples that otherwise would be missed with the HMA1 model.

Both models tended to produce a similar amount of novel synthetic rows (NSR), but the HCTGAN algorithm consistently produced a larger percentage of novel rows. This could also be attributed to the fact that the HCTGAN algorithm produces data which can lie outside of the min- and max values present in the training dataset. The HMA1 model outperforms HCTGAN when it comes to modeling accurate parent child relationships (PCR). This was expected since the sampling algorithm in HCTGAN utilizes heuristics for estimating how many related child rows should be generated based on the number of parent rows.

SECTION: 5Conclusions

In this paper we proposed the HCTGAN model for multi-tabular synthetic data generation. We conclude that the proposed model sufficiently well captures the characteristics of the real data, whilst the sampling algorithm also guarantees referential integrity in the generated synthetic data. The proposed sampling algorithm for HCTGAN allows it to efficiently sample large amounts of synthetic data for arbitrarily complex relational datasets, whereas the HMA1 algorithm is limited in this respect due to its recursive nature. Because the results indicate that the HMA1 model produces higher quality synthetic data, but does not scale for larger relational datasets, we suggest that the HCTGAN algorithm should be used whenever large amounts of synthetic data is needed, or whenever the dataset is deep and contains complex table relations.

For future work we propose to investigate the effect of the chosen parent column distribution which the child tables condition their generator on. One could estimate these distributions similarly to how the columns in the dataset are modelled using mode-specific normalization. This could potentially increase the quality of the generated synthetic data due to better capturing relating information between the tables. Furthermore, performing an ablation study on the conditioning of parent information would be insightful to determine its effect.

Finally, we conclude that more research in multi-tabular synthetic data algorithms is needed to establish a foundation for robust, and privacy preserving, synthetic data algorithms, where specific importance should be put on researching any privacy related issues with GAN based synthetic data models.

SECTION: References

SECTION: Appendix AHCTGAN training algorithm

SECTION: Appendix BHCTGAN sampling algorithm