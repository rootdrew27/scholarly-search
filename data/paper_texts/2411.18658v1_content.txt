SECTION: HDI-Former: Hybrid Dynamic Interaction ANN-SNN Transformer for Object Detection Using Frames and Events

Combining the complementary benefits of frames and events has been widely used for object detection in challenging scenarios. However, most object detection methods use two independent Artificial Neural Network (ANN) branches, limiting cross-modality information interaction across the two visual streams and encountering challenges in extracting temporal cues from event streams with low power consumption. To address these challenges, we propose HDI-Former, a Hybrid Dynamic Interaction ANN-SNN Transformer, marking the first trial to design a directly trained hybrid ANN-SNN architecture for high-accuracy and energy-efficient object detection using frames and events. Technically, we first present a novel semantic-enhanced self-attention mechanism that strengthens the correlation between image encoding tokens within the ANN Transformer branch for better performance. Then, we design a Spiking Swin Transformer branch to model temporal cues from event streams with low power consumption. Finally, we propose a bio-inspired dynamic interaction mechanism between ANN and SNN sub-networks for cross-modality information interaction. The results demonstrate that our HDI-Former outperforms eleven state-of-the-art methods and our four baselines by a large margin. Our SNN branch also shows comparable performance to the ANN with the same architecture while consuming 10.57less energy on the DSEC-Detection dataset. Our open-source code is available in the supplementary material.

SECTION: 1Introduction

DAVIS cameras[4,20,32,53], namely hybrid vision sensors, combine a bio-inspired event camera and a conventional frame-based camera within the same pixel array to complement each other. Event cameras[15], with their high temporal resolution, dynamic range, and low power consumption, detect light changes through dynamic events to overcome conventional camera limitations in high-speed motion blur and challenging lighting conditions. Conversely, frame-based cameras capture absolute brightness with fine textures, addressing event camera shortcomings in static or texture-less scenarios. A trend is emerging in leveraging the complementary advantages of frames and events for various computer vision tasks[18,35]. Nevertheless, how to design a high-accuracy and energy-efficient object detection model for two heterogeneous visual streams remains a challenging problem.

Most multimodal object detectors rely on two separate branches of Artificial Neural Networks (ANNs)[5,6,8,27,26,39,35,36,46,50,51,66,77], limiting the cross-modality interaction between two visual streams and resulting in high energy consumption. More specifically, most existing methods typically convert asynchronous events into an image-like representation[17,67]and then directly input them into the ANN detection backbone (e.g., SSD[47]and YOLO[60]). However, Convolutional Neural Networks (CNNs) have limited global spatial modeling capabilities, resulting in lower performance compared to Transformers[29], and processing event streams with ANNs involves high computational complexity and energy consumption. Additionally, two separate ANN branches process images and events individually and then generate the final result using a fusion module[5,46,35]in this detection paradigm. Nevertheless, processing two visual streams independently ignores cross-modality information interaction which is crucial for the complementary utilization of fine texture frames and dynamic events. In fact, the biological visual system processes fine texture and dynamic sensing in a more interconnected manner than previously thought[28,64]. Yet, there is no work to design a dynamic interaction neural network with lower power consumption specifically for object detection.

Spiking Neural Networks (SNNs)[11,41,62,78]excel in capturing temporal dynamics for event-based vision with low power consumption, but they are rarely used for multimodal object detection tasks. In general, SNNs utilize binary spikes for neural communication, potentially enabling energy-efficient computation[21,52]. Most SNN object detectors[7,30,24,40]directly convert ANNs to SNNs, relying on large timesteps and the performance of the original ANN. Recent advancements[3,9,14,65,72]using surrogate gradients[54]allow for direct training of SNNs, achieving energy-efficient object detection with fewer timesteps. Nonetheless, these single-modality SNNs, processing only event streams, may be hard to achieve high-accuracy detection. A promising method[73]is to design hybrid neural networks by combining ANNs and SNNs to leverage the strengths of both. Although some hybrid neural networks[2,71]integrate ANNs and SNNs to process two heterogeneous visual streams, they are not explicitly designed for object detection. Furthermore, their sub-networks operate independently and are not directly trained, which may hinder them from fully leveraging the complementary benefits of frames and events.

To address the aforementioned problems, we propose a Hybrid Dynamic Interaction ANN-SNN Transformer, namely HDI-Former, which is the first to directly train a hybrid ANN-SNN architecture for high-accuracy and energy-efficient object detection using frames and events. Indeed, this work does not optimize two independent CNN-based branches. In contrast, it seeks to overcome the following challenges: (i) How can we design a novel attention mechanism to achieve high accuracy in the ANN Transformer branch? (ii) How do we create an energy-efficient SNN Transformer branch for event-based object detection? (iii) What is the interaction mechanism for hybrid ANN-SNN branches that benefits two streams? To this end, we first present a novel semantic-enhanced self-attention mechanism to enhance the correlation between image encoding tokens within the ANN Transformer branch. Then, we design an energy-efficient Spiking Swin Transformer branch to use rich temporal cues from event streams. Finally, we introduce a bio-inspired dynamic interaction mechanism between ANN and SNN sub-networks to achieve cross-modality interaction. The results show that our HDI-Former outperforms the state-of-the-art methods by a large margin and our SNN branch achieves comparable performance to the corresponding ANN with less energy consumption. As shown in Fig.1, our HDI-Former reaches the best balance between performance and energy consumption. The main contributions of this paper are summarized as follows:

We propose a directly trained Hybrid Dynamic Interaction ANN-SNN Transformer (i.e., HDI-Former), which achieves high-accuracy and energy-efficient object detection using frames and events.

We present a novel semantic-enhanced self-attention in the ANN branch to enhance the correlation between image encoding tokens for better detection performance.

We design an energy-efficient Spiking Swin Transformer branch that achieves the best performance in directly-trained SNNs for event-based data while matching ANN performance with 0.1energy consumption.

We introduce a bio-inspired dynamic interaction mechanism between ANN and SNN sub-networks to enhance cross-modality information interaction for leveraging the complementarity of frames and events.

SECTION: 2Related Work

Neuromorphic Object Detection.The existing object detectors[23,35]utilizing event cameras can be broadly classified into two categories. The first category[25,37,56,59,69,80]involves single-modality approaches that solely process the event stream. These methods typically convert events into 2D image-like representations, which are then fed into feed-forward CNN-based backbones[25,69,80]or RNN-based object detectors[37,56,59]. While these ANNs exhibit high performance, they also come with high computational complexity and energy consumption. More recently, there has been exploration into energy-efficient SNN-based object detectors, including ANN-to-SNN conversion models[7,30,24,40]and directly trained SNNs[3,9,14,65,72]. However, it’s evident that relying solely on dynamic event streams may not achieve satisfactory performance in static or slow-motion scenarios that require precise detection of static fine textures. The second category comprises multimodal approaches that combine frames and events. Existing multimodal object detectors[27,26,35,36,51]utilize two independent CNN branches to handle each visual stream and then predict joint detection results using post-processing or end-to-end aggregation fusion modules. However, one drawback is that the limited spatial modeling ability of these CNN models results in a significant performance gap compared to mainstream Transformer models[19,57]. Another issue is that two independent ANN branches processing heterogeneous visual streams in the same manner would neglect cross-modality interaction, leading to incomplete utilization of the complementary attributes from frames and events. Thus, this work aims to design a hybrid Transformer with low power consumption to fully leverage the complementarity of two heterogeneous visual streams.

Hybrid Neural Networks.Hybrid neural networks[73]aim to achieve both high accuracy and energy efficiency by combining the strengths of ANNs and SNNs. Due to the development of hybrid vision sensors[4,20,32,53]and neuromorphic computing hardware, there is a growing trend to design hybrid neural networks. In one class of methods[33,34,44,1], SNNs are employed for efficient encoding and feature extraction of spatiotemporal events, followed by different ANN backbones for various visual tasks. However, these single-modality hybrid neural networks are only suitable for processing mere event streams. Another type of methods[2,71,73]adopts two branches of ANNs and SNNs to process multimodal frame and event streams simultaneously. The ANN is tasked with extracting features from the frames while the SNN processes events directly. For example, Aydin et al.[2]design an energy-efficient hybrid ANN-SNN architecture for human pose estimation. Zhao et al.[73]propose the general design and computation of hybrid neural networks by hybrid units for object tracking. Nevertheless, these hybrid models have not explored the object detection task. Meanwhile, the backbones of their two branches are usually independent of each other, making it difficult for them to interact with each other. Therefore, we propose a dynamic interaction hybrid ANN-SNN model to process two heterogeneous visual streams.

SECTION: 3Method

SECTION: 3.1Dynamic Interaction ANN-SNN Framework

This work aims to design a directly-trained Hybrid Dynamic Interaction ANN-SNN Transformer (HDI-Former), which makes complementary use of frames and events to maximize object detection performance. As shown in Fig.2, our HDI-Former consists of three novel modules: Semantic-Enhanced Swin Transformer for processing RGB frames (ANN), Spiking Swin Transformer for modeling event streams (SNN), and Dynamic Interaction between them. More precisely, we adopt the Swin Transformer[48]as the backbone since it achieves state-of-the-art performance among Transformer architectures. We then enhance the correlation of tokens by involving relative semantic information to further extract fine-grained spatial features from RGB frames. Then, we present a Spiking Swin Transformer to leverage the rich temporal cues from the continuous event stream. Meanwhile, a bio-inspired dynamic interaction between ANN and SNN sub-networks is designed to facilitate cross-modality information interaction. Finally, an asynchronous fusion module[35]is utilized to combine two visual flow features complementarily, feeding them into the detection head to predict the final results.

Aiming to implement cross-modality interaction for better utilization of the complementarity of frames and events, we propose a bio-inspired dynamic interaction mechanism between ANN and SNN sub-networks as motivated by the biological vision system in perceiving dynamic and fine textures[55,64]. Specifically, this operation makes complementary use of the two modalities and integrates spatiotemporal representations by referencing each other’s attention maps. To illustrate our dynamic interaction, we consider an arbitrary block (i.e.,) in both branches. We first compute self-attention in blockof the SNN branch and the ANN branch to obtain attention weight maps, which can be summarized as follows:

whereare the attention weight maps for events and frames.andare queries and keys for the two modalities.refers to the relative positional embedding, andare relative semantic embeddings (see Sec.3.2).stands for the number of windows, andis the window size.are two scalars to reweightandbefore adding to the attention weights.

In practice, the attention weight map from each modality is passed to the other and mapped to a unifying attention space using specially designed functions. This projection is necessary because, with multi-head attention, the weights resemble normal feature maps, treating the head dimension like a channel dimension and the attention weight maps for different modalities are distributed over different attention spaces. We refer to this meticulously handcrafted function as the attention kernel function, denoted as. A simple yet effective implementation of the attention kernel function can be mathematically described as:

wheredenotes the head number in multi-head self-attention, andis the attention weight map. Finally, we add the mapped attention weights to the attention weights of the other modality to obtain the final attention weightsandas:

wheredenotes the element-wise sum operation. We use attention kernel functionsandfor frames and events respectively due to the disparity of their attention maps.

SECTION: 3.2Semantic-Enhanced Swin Transformer

In HDI-Former, the ANN branch acts as the spatial feature encoder to extract fine static textures from frames. While the Swin Transformer[48]effectively exploits geometric information in images through relative positional embedding, relying solely on this aspect is insufficient[13,38]. Thus, we introduce Relative Semantic Embedding, referred to as RSE, to improve data utilization and token correlations by explicitly integrating semantic information into self-attention. Concretely, we first obtain the feature mapinput to self-attention, whereis the sequence length andis the channel dimension. Then, we compute the relative semantic distance between any two tokens by subtraction, which can be formulated as:

whereis the relative semantic distance matrix. Then we convertto RSE using a Multi-Layer Perceptron (MLP) with a single output channel. Given there are only two semantic relations between tokens in object detection (i.e., belong to the same object or not), the RSE is supposed to be symmetric. Thus, we feed only the upper triangular part ofinto the MLP and symmetrically populate the lower triangles with the resulting RSE. The above-mentioned operations can be described as follows:

wheredenotes the proposed RSE.andare learnable weights.

Upon RSE, we introduce a novel self-attention mechanism called Semantic-Enhanced Multi-head Self-Attention (SEMSA). By replacing the original self-attention in the Swin Transformer with SEMSA, we create the Semantic-Enhanced Swin Transformer (SEST) as the ANN branch of our HDI-Former. SEST comprises multiple stacked blocks, each executing shifted window partitioning, SEMSA, and feed-forward (FFN) operations. Besides, layer normalization[70]and dropout[63]are also applied within each block. In summary, two consecutive blocks in our SEST can be formulated as follows:

wheredenote the outputs of W-SEMSA and SW-SEMSA modules.andare the outputs of the two blocks. W-SEMSA and SW-SEMSA represent window-based SEMSA with regular and shifted window partitioning configurations, respectively.

SECTION: 3.3Spiking Swin Transformer

To efficiently process event streams with low energy consumption and enable dynamic interaction with the ANN branch, we design a novel spiking version of the Swin Transformer, which is the first trial to design a directly-trained SNN-based Transformer model for energy-efficient object detection. In contrast to ANN, SNN neurons transmit binary spikes for communication[76], so we need to convert the feature mapinto binary spikes and make the core self-attention mechanism compatible with spikes. To achieve this, we generate the binary feature map by adding a spiking neuron layer after the first convolutional layer in the patch splitting stage. Moreover, the Q, K, V produced by self-attention are quantized by binarization (see Fig.3). Meanwhile, we introduce the Spiking Self-Attention (SSA) to extract the spiked feature maps. By removing softmax normalization, SSA completely avoids multiplications, aligning with the energy-efficient calculations of SNNs. The operational steps in our Spiking Swin Transformer can be mathematically represented as follows:

whererefer to spiked query, key, and value derived from.represents spiking neuron layer, andis a scaling factor.andare the output of SSA and the block. Batch normalization is implemented for re-parameterization convolutions as[12]suggests.

Due to the need for a higher frequency of processing events compared to frames during asynchronous inference and the challenges in training SNNs, we propose a new training strategy for object detection regression tasks. In short, we introduce Q-K attention to make our Spiking Swin Transformer more efficient and easier to train. Q-K attention only uses two spike-form components query and key. By performing row summation, Q-K attention derives a token-wise attention vectorfrom, which models the binary importance of different tokens. Then, Hadamard productis conducted betweenand. We can formulate these operations as follows:

wherestands for the Q-K attention operation. In real implementation, we utilize both QKA and SSA for a trade-off between speed and accuracy.

SECTION: 4Experiments

SECTION: 4.1Experimental Settings

Dataset.To verify the effectiveness of our HDI-Former, we conduct experiments primarily on the DSEC-Detection dataset[16]. It provides paired frames and events at a resolution of 640480, along with 390k boxes at 20 Hz for object detection in driving scenarios. For fair comparisons, we also evaluate our Spiking Swin Transformer on the Gen1 dataset[59]which is widely used in SNNs for single-modality event-based object detection. The Gen1 dataset contains events at a resolution of 304240 and 255k boxes with a frequency of 1 Hz, 2 Hz, or 4 Hz.

Implementation Details.We select the Swin Transformer-Tiny[48](i.e., Swin-T) as the backbone owning to accuracy-speed trade-off. The patch size and window size are set to= 4 and= 8. In the ANN branch, we set the channel number of the hidden layers in the first stage= 96, while the query dimension of each head is= 32. For the convenience of training SNN, we reduce the number of channels to= 64 and= 16. The timestep in SNN is set to= 5, and the number of blocks to perform dynamic interaction is= 4. Following Swin Transformer, we use Mask R-CNN[22]as our detection head and utilize random resize as data augmentation. With batch size set to 16, we adopt an AdamW optimizer[31]with an initial learning rate ofand weight decay of 0.05. All experiments are conducted on NVIDIA A100 GPUs. We report the mean average precision (e.g., COCO mAP[42]) as evaluation metrics, containing mAP at IoU = 0.5 (i.e., mAP50) and the average AP between 0.5 and 0.95 (i.e., mAP). We also compare the SNNs from the perspective of energy consumption.

SECTION: 4.2Comparison with State-of-the-Art Methods

To verify the effectiveness of HDI-Former, we compare it with eleven state-of-the-art methods and four baselines in Table1and Fig.4to show the advantages of our hybrid framework from three perspectives.

Frame Modality.We present a comparison between our SEST branch and seven state-of-the-art methods, including Faster R-CNN[61], RetinaNet[43], CenterNet[75], YOLOv5[79], YOLOv7[68], ConvNeXt[49], and Swin Transformer[48]. Note that, our SEST outperforms the former six frame-based feedforward methods by a large margin. This is caused by the fact that the self-attention operation is better at extracting global features compared to standard convolution, indicating the potential of Transformer in the object detection task. Furthermore, by explicitly leveraging both geometric and semantic information, our SEST module comprehensively enhances the correlations between image tokens. Table1shows that our SEST achieves a remarkable mAP improvement of 0.8% with comparable parameters over the Swin Transformer.

Event Modality.We compare our Spiking Swin Transformer in the SNN branch with EMS-YOLO[65]which achieves state-of-the-art performance among directly trained SNNs to the best of our knowledge. From the event modality in Table1, our Spiking Swin Transformer achieves a significant mAP improvement of 3.4% over EMS-YOLO. In addition, our SNN branch performs relatively comparable to its ANN counterpart while significantly reducing energy consumption (see Sec.4.3).

Benefit From Multimodal Fusion.We further compare our HDI-Former with state-of-the-art multimodal methods (i.e., FPN-fusion[66], SFNet[50]and SODFormer[35]) and our baseline (i.e., HDI-Former* without dynamic interaction). Note that our HDI-Former outperforms FPN-fusion and SFNet by a large margin. Moreover, our HDI-Former, using the SNN branch to process the event stream, dramatically reduces energy consumption compared to SODFormer. Furthermore, we present some representative visualization results on the DSEC-Detection dataset[16]in Fig.4. The three rows from top to bottom represent low-light, high-speed motion blur and static scenes, respectively. Obviously, our HDI-Former, making complementary use of frames and events, can perform robust object detection in various scenarios. The first row in Fig.4also shows that our HDI-Former, utilizing continuous event streams with rich temporal cues, successfully detects the occluded car on the left, overcoming single-frame object detection challenges.

SECTION: 4.3Performance Evaluation of SNNs

Evaluation on Gen1[10].As illustrated in Table2, we present a comparison of our Spiking Swin Transformer with some directly-trained SNN-based object detectors on the Gen1 dataset[10], the most widely used dataset for single-modality event-based object detection. We can find that our Spiking Swin Transformer achieves the best mAP of 0.291 and mAP50of 0.580 with the same timestep, which outperforms the state-of-the-art methods to our knowledge. It is worth noting that despite the higher performance achieved by the ANN counterpart, our Spiking Swin Transformer exploits a sparse representation with a spike firing rate of 19.84%, implying lower energy consumption. According to the quantitative formula for energy consumption in[65], we conclude that our Spiking Swin Transformer reduces up to 6.34energy consumption compared to its ANN counterpart. Besides, some representative visualization results in Fig.5also indicate the effectiveness of our Spiking Swin Transformer on the Gen1 dataset[10].

Evaluation on DSEC-Detection[16].As shown in Table3, We further explore the effectiveness of our Spiking Swin Transformer on the DSEC-Detection dataset[16], which contains more classes of objects and is relatively more complex. By substituting CNN-based ResNet with the Swin Transformer which is more capable of modeling, our model gains greater expressive ability and outperforms EMS-YOLO[65]by a large margin of 3.4% mAP. Besides, our Spiking Swin Transformer maintains a low firing rate when facing more complicated data while EMS-YOLO increases significantly. As a result, the energy consumption can be saved up to 10.57compared to ANN Swin Transformer. In other words, our SNN branch achieves comparable performance to the ANN with the same architecture while significantly reducing power consumption.

SECTION: 4.4Ablation Study

Contribution of Each Component.We use the ANN Swin Transformer as the baseline to assess the contributions of each module. As illustrated in Table4, three methods namely (a), (b) and our HDI-Former consistently achieve higher performance by relying on the proposed components. To be specific, method (a), enhancing the correlations between tokens in self-attention by explicitly utilizing semantic information, obtains a 0.8% mAP improvement over the baseline. By involving events through the Spiking Swin Transformer, method (b) is capable of exploiting rich temporal cues from continuous event streams and making complementary use of frames and events, so the detection mAP is further enhanced by 0.7% compared to (a). By adopting dynamic interaction, our HDI-Former achieves an mAP improvement of 0.7% over method (b) and 1.4% over method (a). This reveals that our dynamic interaction mechanism favors complementary context capturing across frames and events while improving the model’s capability to extract features at a more fundamental level during the feature extraction stage. Besides, our HDI-Former introduces few ACs and almost no MACs compared to the baseline, ensuring high accuracy and energy efficiency.

Influence of SNN Timesteps.To explore the impact of SNN timesteps, we report the performance of our Spiking Swin-T on the Gen1 dataset by first fixing the length of temporal window and then setting various timesteps(see Table5). Note that, the performance is subpar whenis 1 or 3 as the small timestep hinders feature extraction. Conversely, it may be unsatisfactory with a high timestep (e.g.,= 7) due to increased training difficulty. The limited number of events within each timestep may also contribute to the inability to convey valid information whenis too large.

Influence of the Number of Interaction Layers.To further analyze the impact of dynamic interaction, we vary the number of layers where dynamic interaction is applied to 2, 4, 6, and 8, respectively, and evaluate the performance of our HDI-Former. From Table6, it’s evident that our HDI-Former achieves optimal performance with 4 dynamic interaction layers (= 4). This suggests that dynamic interaction increases the model’s expressiveness. Interestingly, the performance seems to vary marginally across different numbers of dynamic interaction layers, implying that a few layers are sufficient to achieve our objectives.

SECTION: 4.5Scalability Test

Asynchronous Inference Validation.Conventional cameras with global shutters face limitations in output frame rate, resulting in gaps between adjacent frames. Our HDI-Former introduces an event stream of high temporal resolution to effectively fill these gaps and achieve continuous object detection. To achieve high-frequency detection, we first sample from the event stream using a sliding window of 0.05s length shifting forward by 0.0125s, resulting in an event stream of 80 Hz. We then input frames at 20 Hz and event temporal bins at 80 Hz into our HDI-Former to produce detections at 80 Hz. As depicted in Fig.6, the middle three figures display the predictions, showing a smooth transition of the predicted box from the first frame to the second. This indicates that our HDI-Former can leverage the event stream of high temporal resolution to overcome the limitation of inference frequency and achieve continuous object detection in real-world applications.

Dynamic Interaction Analysis.To investigate the interpretability of the dynamic interaction mechanism, we present a scenario involving objects moving at different speeds in Fig.7. We observe a car on the left experiencing high-speed motion blur, while a car in the middle appears static. The attention map reveals that the ANN branch fails to highlight the left object, and the event modality struggles to recognize the middle object. However, these unattended regions contain valuable information crucial for accurate detection. Leveraging attention weights from each other, our dynamic interaction mechanism effectively addresses this challenge. In other words, the proposed dynamic interaction mechanism can leverage the complementarity of frames and events via cross-modality interaction.

SECTION: 5Conclusion

This paper proposes a novel Hybrid Dynamic Interaction ANN-SNN Transformer, namely HDI-Former, which is a pioneering approach to integrate ANNs and SNNs to utilize their complementary strengths dynamically. To achieve this, we first present a novel semantic-enhanced self-attention mechanism to strengthen the correlation between tokens in the ANN branch. Then an energy-efficient Spiking Swin Transformer branch is proposed to extract temporal cues from events. Finally, the dynamic interaction mechanism enables cross-modality interaction between the ANN and SNN. Experimental results demonstrate that our HDI-Former significantly surpasses the state-of-the-art methods and the SNN branch achieves comparable performance to the ANN counterpart at lower energy consumption. To our knowledge, this is the first attempt to directly train a hybrid ANN-SNN Transformer for high-accuracy energy-efficient multimodal object detection. We believe that conducting effective cross-modality interaction is an exciting breakthrough in object detection with frames and events, inspiring directions for future research.

Limitation.The power consumption is simulated based on existing academic papers rather than neuromorphic chips, potentially increasing inference time and skewing power usage estimates. Future research will focus on deploying algorithms onto neuromorphic chips to meet the high-speed and energy-efficient requirements of event cameras.

SECTION: References

Supplementary Material

SECTION: ADetailed DSEC-Detection Dataset

The DSEC-Detection[16]dataset is collected using 2× Prophesee Gen3.1 cameras with a spatial resolution of 640480 pixels. It includes detection labels for 60 hybrid sequences amounting to 70,379 frames and 390,118 bounding boxes. It is then divided into 47 sequences for training and 13 for testing. The labels are produced by applying an advanced object tracking algorithm and manually reviewing and adjusting to eliminate any incorrect tracks. There exist 8 classes, i.e., pedestrian, rider, car, bus, truck, bicycle, motorcycle, and train.Notably, the results in the main text are all evaluated on the full 8 classes, while some of the methods[16]involve only 2 classes (i.e., including only pedestrians and grouping cars, trucks, and buses as cars).For comprehensiveness, we further compare HDI-Former with several state-of-the-art methods on the DSEC-Detection dataset with the two-class setup in Table7. The results show that HDI-Former achieves significant performance improvement against the other methods with mAP increases by 4.8% compared to the second optimal method. Furthermore, HDI-Former utilizes the energy-efficient Spiking Swin Transformer to extract temporal cues from event streams, which reduces energy consumption.

SECTION: BPreliminaries

SECTION: B.1Event Camera Sensing Mechanism

Event cameras, called neuromorphic cameras, report pixel-wise values oforfor positive or negative log intensity differences, respectively. Each eventcan be described as a four-attribute tuple, which is generated for a pixelat the timestamponce the log-intensity change exceeds the processing threshold, and it can be depicted as:

whereis the temporal sampling interval at a pixel, and the polarityrepresents the increasing or decreasing change in the brightness, respectively.

Consequently, asynchronous events=are sparse and discrete points in the spatio-temporal window, which can be mathematically described as:

whereis the number of events during the spatio-temporal window, andrefers to the Dirac delta function, with, and. Event cameras offer the advantages of low latency, high dynamic range, and low redundancy.

SECTION: B.2Theory of Spiking Neural Networks

In SNNs, spiking neurons are the basic units with more biological plausibility as they mimic the membrane potential dynamics and the spiking communication scheme[65]. We choose the Leaky Integrate-and-Fire (LIF) model as the spiking neuron in our work as a good trade-off between bio-plausibility and complexity. The dynamics of a LIF neuron can be formulated as:

whereis the membrane time constant, andis the current input at time step. Whenever the membrane potentialexceeds the firing threshold, the spiking neuron triggers a spike.is the Heaviside step function, which equals 1 whenand 0 otherwise.represents the membrane potential after the triggered event, which equalsif no spike is generated and otherwise equals the reset potential. We can observe that LIF neurons are charged by input stimuli and remain at resting potential at other moments, which accounts for their high efficiency and low energy consumption.

Eq.21can be interpreted as a recurrent neural network that can be unrolled over multiple forward Euler steps and then trained using backpropagation through time. However, the Heaviside step functionis not differentiable near 0 which impedes backpropagation. To solve this problem, we use surrogate gradients[54]to replace the gradient of the Heaviside function with the approximate function as:

SNNs showcase energy-efficient properties with neurons engaging in Accumulation Calculation (AC) operations exclusively during spikes. Hence, the energy consumption of SNNs is a vital metric which can be calculated[65]as:

whereandare the total time steps and the firing rate in theth block, respectively.andrepresent the number of accumulation calculation operations (AC) and Multiply-Accumulate (MAC) operations in theth block.anddenote the computational energy of each operation and directly reference the power consumption of a neuromorphic chip with specific nanometer (nm) technology. In this work, we assume that various operations involved are 32-bit floating-point implemented in 45nm technology, where= 0.9pJ and= 4.6pJ.

SECTION: CDetailed Network Architecture

SECTION: C.1Network Specifications

We show more details of the network architecture in Table8. It can be seen that we set the architectures of the two branches approximately the same to ensure dynamic interaction. The output sizes are obtained presuming a 640480 input which is exactly the case in the DSEC-Detection dataset. ”concat” represents splitting the feature map into patches of sizeand merging the features within each patch into a token, which results in downsampling of feature map by a rate of. In the ANN branch, we achieve this through a Conv2D layer with kernel size and stride both set toin the patch embedding stage, while patch merging is achieved by concatenating the neighboringtokens and reducing the channels by 2 with a Linear layer. In the SNN branch, we follow[74]to use a {Conv2D-BN-MaxPooling-} combination for downsampling with a shortcut connection for both patch embedding and patch merging stage. Note that we add an extra {Conv2D-BN-} layer before the patch embedding stage of the SNN branch to transform the input into spikes.

SECTION: C.2Framework of Relative Semantic Embedding

For a more intuitive clarification of the proposed Relative Semantic Embedding (i.e., RSE), we show its computational procedure in Fig.8. The upper part of Fig.8indicates the computation of the original self-attention in Swin Transformer. On the bottom part, we calculate the relative semantic distance between any two tokensby subtraction as formulated in Eq.6. We denote this subtraction operation by the notationin Fig.8. After that, we convertto RSE through an MLP with the output channel set to 1. Remarkably, considering that there are only two kinds of semantic relations between tokens in the object detection task: belonging or not belonging to the same object, the RSE here is supposed to be symmetric. Therefore, only the upper triangular part is fed into the MLP to ensure symmetry, which can be summarized as:

SECTION: DMore Details on Experimental Setting

SECTION: D.1Gen1 Dataset

The Gen1 dataset is captured using a PROPHESEE GEN1 sensor with a resolution of 304×240 pixels. It consists of 2358 sequences, each containing recordings of open roads and various driving scenes for 60 seconds, encompassing urban environments, highways, suburban areas, and picturesque countryside landscapes. The object labels were meticulously assigned through a manual process and are available for two classes present: pedestrians and cars. The number of bounding boxes for pedestrians and cars is 228k and 27k, respectively.

SECTION: D.2More Implementation Details

Here we elaborate on some implementation details that are not covered in the Sec.4.1. First, the spiking neuron adopted in our HDI-Former is LIFNode. In all our experiments, the reset value of LIF neuronsis set to 0, and the membrane time constantto 2. We set the thresholdfor the neurons following the output of self-attention to 0.5 and the rest to 1. The surrogate function engaged is the inverse trigonometric function (i.e., arctan function). The scalars attached to RPE, RSE, and dynamic interaction shown in Eq.1-5(i.e.,are set to, which are decided by the experiments in Sec.E.2. Owning to a trade-off between speed and accuracy, we simply use the attention kernel function described in Eq.3asin Sec.3.1. Since QKA is equivalent to SSA with a diagonal attentional weight matrix, it benefits little from dynamic interaction. Therefore, the layers we conduct dynamic interaction start at Stage 3 in Fig.2. We use a learning rate scheduler that decreases the learning rate by 0.1 at epochs 27 and 33.

SECTION: EMore Experiments

SECTION: E.1Object Detection on PKU-DAVIS-SOD Dataset

To verify the effectiveness and generality of our HDI-Former, we further conduct an extensive experiment on the PKU-DAVIS-SOD dataset[35]. As Table9shows, our HDI-Former consistently outperforms the state-of-the-art methods and the baselines* on the PKU-DAVIS-SOD dataset. Note that the mAP50improvement of 1.2% compared to SODFormer[35]is even more significant than the 0.9% improvement on the DSEC-Detection dataset. We attribute this to that the PKU-DAVIS-SOD dataset contains more challenging scenarios, and our HDI-Former shows a better ability to extract valid information from degraded modality with the help of dynamic interaction. We further present some instances under challenging scenarios (i.e., low light and high-speed motion blur) in Fig.9to verify the effectiveness of integrating events and frames and the dynamic interaction mechanism in HDI-Former.

SECTION: E.2More Ablation Studies

Influence of Scalars for RPE and RSE.To explore the influence of the hyperparameters in self-attention (i.e.,-) and choose the best hyperparameter setting, we compare the performance of HDI-Former with varyingsettings in Table10. Particularly, we first follow the setting in[48]to setandto 1. Forandwhich are involved in the dynamic interaction mechanism, we modify their settings to identify the best choice. From Table10, we can observe that the performance is sub-optimal when. Moreover, the comparison between the first and the third rows in Table10indicates that the performance is not especially sensitive to. Statistically, we conclude that relatively smallandare sufficient to achieve our objective of cross-modality information interaction.

Influence of Attention Kernel Functions.The core of the dynamic interaction mechanism lies in the fact that the sharing of attention weights across modalities contributes to feature extraction. To achieve this, the attention kernel function (see Sec.3.1) works similarly to the kernel function, projecting the attention weight of one modality into the attention weight space of the other modality since different modalities may have different attention weight spaces regarding head dimension as the channel. Akin to the kernel function, our attention kernel function is a set of functions. To explore the influence of attention kernel functions on the dynamic interaction mechanism, we compare the Baseline in Eq.3with more sophisticated functions like Conv2D or MLP in Table11. As we can see, the HDI-Former achieves better performance with more powerful attention kernel functions. However, we state that the mAP improvements are marginal (i.e., 0.3%) while increasing inference time. Therefore, our implementation in Eq.3reaches a good trade-off between performance and complexity.

SECTION: E.3More Scalability Tests

Temporal Representation AnalysisSince event streams contain rich temporal cues, HDI-Former is capable of utilizing the temporal cues to boost detection performance. To validate this, we conduct additional experiments on the DSEC-Detection dataset and provide some comparative visualization results of the HDI-Former and the feed-forward baseline (i.e., SEST) about whether they utilize rich temporal cues or not (see Fig.10). Obviously, The baseline that utilizes single RGB frames for feed-forward object detection encounters challenges in scenarios involving occluded objects, as illustrated in the first row in which the occluded truck and the second car from the left are difficult to detect. However, our proposed HDI-Former effectively addresses this challenge by harnessing rich temporal information from continuous event streams, and thus successfully detects both objects with high confidence. Moreover, in contrast to existing models that utilize temporal cues, our approach generates temporal representation by processing event streams using SNN, reducing energy consumption.

Inference Time AnalysisFollowing the same experimental setup as in Sec.4.1, we tested the inference time of the baselines and our model on an NVIDIA A100 GPU. Unfortunately, the computation architecture of most existing computing hardware is not designed for SNN, and the A100 GPU is no exception. On one hand, the inference speed of SNN on such hardware is vastly reduced; on another hand, the two branches of our HDI-Former can be executed simultaneously, while the two branches can only be executed sequentially under the existing resources, which further reduces the inference frequency. Since our HDI-Former merely increases addition operations taking advantage of SNN and few multiplications, it is without doubt that the inference speed of our HDI-Former will increase significantly on neuromorphic computation hardware.

Failure Case AnalysisWhile our HDI-Former demonstrates promising performance even under challenging conditions, certain failure cases remain unresolved. In fact, some scenarios cannot be fully covered by the complementary properties of frames and events. This stems from the limited dynamic range of conventional frame-based cameras, resulting in poor image quality in such extreme lighting conditions. Conversely, event cameras excel at capturing moving objects but hardly output events for static or slow-motion objects. Therefore, some scenes pose a detection challenge to the current object detection paradigm, such as static objects or small objects in extreme lighting conditions. As illustrated in Fig.11, the four columns from left to right represent two unimodal baselines, our HDI-Former, and ground truth, respectively. The first row displays the difficulty of detection in the presence of small objects under low light even with our HDI-Former. In the second row, our HDI-Former fails to figure out the static car under low light on the right. Addressing these challenges remains an important area for future research, as they extend beyond the capabilities of our HDI-Former model.