SECTION: Data structurelabels? Unsupervised heuristics for SVM hyperparameter estimation

Classification is one of the main areas of pattern recognition research, and within it, Support Vector Machine (SVM) is one of the most popular methods outside of field of deep learning – and a de-facto reference for many Machine Learning approaches. Its performance is determined by parameter selection, which is usually achieved by a time-consuming grid search cross-validation procedure (GSCV). That method, however relies on the availability and quality of labelled examples and thus, when those are limited can be hindered. To address that problem, there exist several unsupervised heuristics that take advantage of the characteristics of the dataset for selecting parameters instead of using class label information. While an order of magnitude faster, they are scarcely used under the assumption that their results are significantly worse than those of grid search. To challenge that assumption we have proposed an improved heuristics for SVM parameter selection and tested it against GSCV and state of art heuristic on over 30 standard classification datasets. The results show not only its advantage over state-of-art heuristics but also that it is statistically no worse than GSCV.

[iitis]organization=Institute of Theoretical and Applied Informatics, Polish Academy of Sciences,addressline=Bałtycka 5,
city=Gliwice,
postcode=44100,
country=Poland

SECTION: 1Introduction

Classification is among the most frequently encountered problems within the field of pattern recognition. It is utilized, among many other fields, in computer visionKoklu and Ozkan (2020), document analysisShah et al. (2020), data scienceAlloghani et al. (2020)and biometricsYang et al. (2021). The classification itself is a wide area that contains both traditional machine learning methods and, recently increasingly popular, deep learning models.
However, even with formidable results achieved by the deep learning approaches, e.g.Li et al. (2021),Cheng et al. (2020), the classical methods still have a role to play. The high computational cost, large data volume required and the open-ended difficulty of finding a combination of a suitable architecture, hyperparameters and a learning algorithm for the deep learning model is prohibitive for many current applications of pattern recognition. This situation occurs e.g. for Internet of Things devicesMenter et al. (2021), edge computingGupta et al. (2022), medical devicesPires et al. (2018)or with limited training labelsRomaszewski et al. (2016). Additionally, classical methods – notably Support Vector Machines – are selected for their robustnessCervantes et al. (2020)or theoretical considerationHuang et al. (2022).

Support Vector Machine (SVM) is a supervised classification scheme based on ideas developed by V. N. Vapnik and A. Ya. Chervonenkis in 1960sVapnik and Lerner (1963)and later expanded on in works such asSchölkopf and Smola (1998b),Cortes and Vapnik (1995)orDrucker et al. (1997). It is based on computing a hyperplane that optimally separates training examples and then making classification decisions based on the position of a point in relation to that hyperplane. The SVM have been consistently used in various roles – as an independent classification scheme e.g.Sha‘abani et al. (2020),Głomb et al. (2018),Direito et al. (2017), part of more complex engines e.g.Kim et al. (2003),Cholewa et al. (2019)or a detection engine e.g.Ebrahimi et al. (2017),Chen et al. (2005). It has been also employed in unsupervised setting in works such asLecomte et al. (2011),Song et al. (2009). This flexibility allows SVM to be one of the most frequently used machine learning approaches in medicineSubashini et al. (2009), remote sensingRomaszewski et al. (2016), threat detectionParveen et al. (2011), criminologyWang et al. (2010), and is often utilized in photo, text, and time sequence analysisLi and Guo (2013). In numerous studies, SVM is consistently marked as one of the top performing methodCervantes et al. (2020).

The popularity and versatility of SVM is to a large degree due to its controllability by the key hyperparameters. The first is a label error regularization coefficient, which balances training error and margin width. It allows to classify non-linearly separable datasets or preserve margin width at the cost of misclassification of some training examples. The second is related to extension with the ’kernel trick‘ to kernel-SVM, which is much more effective in working with complex data distributions; it introduces a kernel function value computation as an extension of a dot-product. Various kernel functions have been investigated, however, overwhelming majority of applications use Gaussian radial basis function as it provides best classification performances on a large range of datasetsFernández-Delgado et al. (2014)and assumes only smoothness of the data, which makes it a natural choice when knowledge about data is limitedSchölkopf and Smola (1998a). Values of these hyperparameters are typically found through supervised search procedures, cross-validation (CV) on the training set and grid-search through a range of predefined parametersAn et al. (2007)Zhang and Wang (2016). However, major disadvantage of the CV is thecomplexity in the number of hyperparameter values to be evaluated, each requiring training a separate model. This is a burden for performing pattern recognition in distributed edge computing devices in Industry 4.0Gupta et al. (2022)or optimization of battery usage for mobile devices with limited connectivity, e.g. in monitoring of ageing peoplePires et al. (2018).

An alternative for hyperparameter selection is to derive their values from a statistical analysis of the data. Those approaches range from simple ’rule of thumb‘ statistics, e.g.Smola (2011), to more complex approaches involving e.g. cluster assumptions and graph distances between datapointsChapelle and Zien (2005). Through those approaches, values ofandcan be estimated based on structure of entire available dataset, in a unsupervised way – without the requirement of labels. This is especially useful for applications that acquire large amount of data with limited supervision, e.g. IoT devicesMenter et al. (2021). Additionally, this estimation is one-pass computation much less intensive than cross-validation, allowing for much greater applicability, e.g. in IoT/edge/medical supervision devices. Unsupervised estimation avoids the issues of optimizing parameters on the same set as the one used for training, which can lead to overfittingSchölkopf and Smola (1998b). It is known that in some cases, e.g. where classes indeed do conform to the cluster assumption and Gaussian distributionVarewyck and Martens (2010), optimal or close to optimal parameter values can be analytically derived from data without knowledge about class labels. This approach is also very helpful when training data is very limited and may poorly reflect true class distributions – a situation typically encountered in semi-supervised hyperspectral classification, e.g.Romaszewski et al. (2016). The robustness of this approach has lead unsupervised heuristics to be a default parameter setting in SVM programming libraries, e.g. scikit-learnGelbart (2018).

In this work, we experimentally verify the performance of unsupervised heuristical hyperparameter estimation for an SVM classifier (UH-SVM) against a grid search CV trained SVM (GSCV-SVM). We evaluate a large set of unsupervised heuristics, and propose an extension aimed at improving performance of one of the most general approach – Chapelle‘s heuristicsChapelle and Zien (2005). Our experiments show, that without specific prior knowledge of a dataset, there‘s a significantly higher chance of a number of UH-SVM approaches having similar or better accuracy than GSCV-SVM – in terms of statistical significance of the results – than to have a worse accuracy. Considering the significantly lesser requirement of computation power of UH-SVM with respect to GSCV-SVM, this in our opinion validates the conclusion of UH-SVM parameter estimation being in many application cases on par with the grid search. As part of results we show that our proposed extension of Chapelle‘s heuristics obtains results practically equivalent to GSCV. Additionally, while there are numerous works investigating individual heuristics, to the best of the authors‘ knowledge, there is no work that collects them together and compares them with each other.

SECTION: 2Methods

In the following section, we will recall both the ideas behind the Support Vector Machines classifier and the heuristics that we include in our experiments. In some cases our unified presentation of them allows us to derive natural generalizations, e.g. a scaling ofChapelle and Zien (2005)in high dimensional datasets or correction forSoares et al. (2004).

SECTION: 2.1Kernel SVM

A kernel SVMSchölkopf and Smola (1998b)is a classifier based on the principle of mapping the examples from the input space into a high-dimensional feature space and then constructing a hyperplane in this feature space, with the maximum margin of separation between classes. Letbe a set of data and letbe the set of labelled examples. Let alsobe a set of labels. We define a training set as a set of examples with labels assigned to them,

The SVM assigns an exampleinto one of two classes using a decision function

Here,andare coefficients computed through Lagrangian optimization – maximization of margin, or distance from hyperplane to classes‘ datapoints on the training set. Training exampleswhere the corresponding values ofare called support vectors (SV). Since SVM is inherently a binary classifier, for multi-class problems several classifiers are combined e.g. using one-against-one methodHsu and Lin (2002).

The functionis called the kernel function and it is used to compute the similarity between the classified exampleand each training instance. It is a generalization of a dot product operation used in the original linear SVM derivation, i.e., taking advantage of the ’kernel trick‘Schölkopf and Smola (1998b)– a non-linear mappingto a feature spacewhere the dot product is computed by evaluating the value. The kernel trick allows the SVM to be effectively applied in the case where classes are not linearly separable in the data space. A number of positive definite symmetric functions can be used as kernels, such as polynomial,,; Laplaceor Gaussian radial basis function (RBF):

whererepresents the variance of the data andis an Euclidean distance in. This kernel has been found to be versatile and effective for many different kinds of dataPrajapati and Patle (2010)and it will be the focus of our research. By substituting, it can be written:

wherecan be viewed as scaling factor, which is one of the parameters of the SVM classifier.

The parametercontrols the impact of individual SV as the kernel distance between two examples decreases with higher values of. Therefore, small values ofwill result in many SV influencing the point under test, producing smooth separating hyperplanes and simpler models. Very small values will lead to all SV having a comparable influence, making the classifier behave like a linear SVM. Large values ofresult in more complex separating hyperplanes, better fitting the training data. However, a too high value ofmay lead to overfitting (see Figure1).

In practice, even using a kernel trick, a hyperplane that separates classes may not exist. Therefore, SVM is usually defined as a soft margin classifier by introducing slack variables to relax constraints of Lagrangian optimisation, which allows some examples to be misclassified. It introduces the soft margin parameterwherea constraint oncontrolling the penalty on misclassified examples and determining the trade-off between margin maximization and training error minimization. Large values to the parame terwill result in small number of support vectors while lowering this parameter results in larger number of support vectors and wider margins (see Figure1).

SECTION: 2.2Setting the SVM parameters

One of the early discussions about SVM parameters was provided inSchölkopf and Smola (1998b). In the chapter 7.8, the authors mentioned the grid search CV (GSCV) as a common method of SVM parameter selection. As an alternative, in order to avoid the CV, the authors suggested a number of general approaches including scaling kernel parameters such as the denominator of the RBF kernel so that the kernel values are in the same range. They also suggested that the value of the parameterscan be estimated aswhereis some measure of data variability such as standard deviation of the examples from their mean, or the maximum/average distance between examples. Model selection by searching the kernel parameter space was later discussed inChapelle and Vapnik (2000), where authors proposed two simple heuristics based on leave-one-out CV.

Unsupervised heuristics are relatively less discussed than their supervised counterparts. A simple heuristic that estimatesas an inverse of some aggregate (e.g. a median) of distances between data points has been proposed in a blog postSmola (2011). In fact, when searching the Internet for a method to choose kernel parameters in an unsupervised way, this post – which refers to the idea from a thesis of B. Schölkopf
– is a common find. This heuristics is similar to the ’sigest‘111Implemented e.g. in R, seeCarchedi et al. (2021)methodCaputo et al. (2002). However, even in surveys comparing heuristics for SVM parameter selectionWainer and Fonseca (2021)when sigest is considered it is applied to the training set and complimented with cross-validation for the value of theparameter.

Sometimes, unsupervised heuristics supplement more complex methods, e.g. inChapelle and Zien (2005)authors propose a method for parameter selection inspired by the cluster assumption, based on graph distances between examples in the feature space; a heuristic for unsupervised initialisation of SVM parameters is provided as a starting point of a grid search. Another example are initialisation methods used in well-known ML libraries, e.g. scikit-learn222https://scikit-learn.orgemploys its own implementation of heuristic for theparameterGelbart (2018)Shark333http://www.shark-ml.org/uses the heuristic fromJaakkola et al. (1999)and while this one is supervised, it can be used in an unsupervised waySoares et al. (2004).

As a baseline method for model selection in this article, Grid Search Cross Validation (GSCV)Berrar (2019)is used. This method is based on dividing the dataset intopartsand then repeat the experiment using partsfor training andfor testing and averaging the results. This method allows to mitigate the variance resulting for random train/test set selection.

In case of this research, the additional layer is used for model selection – called an internal layer. It is designed to detect the best set of parametersfrom given grid.
Similarly to external layer, each training setis divided intosubparts, withused for training with given parameters from gridandused for testing (hence Grid Search Cross Validation). The parameters forare determined by the results of this second level of cross validation.

SECTION: 2.3Unsupervised heuristics for

Unsupervised heuristics usually assume thatshould be relative to ’average‘ distance (measured by) between the examples from, so that the two extreme situations – no SV influence or comparable influence of all SV – are avoided. For example,can be assigned the inverse of the data variance, which corresponds e.g. with heuristics described inGelbart (2018)orSmola (2011)). Intuitively then kernel value between two points is a function of how large is the distance between two given points compared to the average distance among the data. Differences between heuristics can be thus reduced to different interpretations of what that average distance is.

Considering a pair of examplesfrom Gaussian-distributed data, it has been noted inVarewyck and Martens (2010), that the squared Euclidean distanceis Chi-squared distributed with a mean of, assuming that every data feature has varianceand mean. This observation could be used as a heuristics to estimate the value ofas

If we further assume that, this simplifies to, as noticed by authors ofWang et al. (2014).

This approach relies on an underlying assumption that data covariance matrix is in the form, which, in turn, means that in a matrix of examples, every feature has an equal variance. In practice, data standardisation is used, which divides each feature by its standard deviation. However, the standard deviations are estimated on the training set, and on the test set will produce slightly varying values that are only approximately equal. To take that into account, we use another formula for estimation of the value ofas:

wheredenotes a trace of a matrix. This heuristic is denoted in the experiments ascovtrace.

A well-known heuristics for computing the initial value of a parameterwas provided by A. J. Smola in an article on his websiteSmola (2011). Given examples, he considered a kernel function in the form

where a scaling factorof this kernel is to be estimated and. The Smola‘s kernel form is consistent with the RBF kernel given by Eq. (4) – it as special case of (7), withwhereand.

He proposes to select a subset of (e.g.) available pairsand to compute their distances. Then, the value ofcan be estimated as the inverse ofquantile (percentile) of distances where one of three candidatesis selected through cross-validation. The reasoning behind those values extends the concept of ’average‘ distance: the value ofcorresponds to the high value of a scaling factor which results in decision boundary that is ’close‘ to SV,corresponds to ’far‘ decision boundary,aims to balance its distance as ’average‘ decision boundary. The author argues that one of these values in likely to be correct i.e. result in an accurate classifier. Those threevalues are included in the experiments asSmola_10,Smola_50andSmola_90.

A heuristic for choosing SVM parameters can be found inChapelle and Zien (2005). Interestingly, to the best of our knowledge it is the only method that estimates bothandin an unsupervised setting (see2.4.1). The heuristics take into account the density of examples in the data space. Authors introduce a generalization of a ’connectivity‘ kernel, parametrized by, which in the case ofdefaults to the Gaussian kernel. This kernel proposition is based on minimal-path distancewhich, forbecomes Euclidean distance i.e..

Authors use the cluster assumption, by assuming that data points should be considered far from each other when they are positioned in different clusters.
InChapelle and Zien (2005)authors consider three classifiers: Graph-based, TSVM and LDS. As this approach introduces additional parameters, which would make cross-validated estimation difficult, authors propose to estimate parameters through heuristics.
The value of(Equation3) is computed as-th quantile ofwhereis the number of classes. For Gaussian RBF kernel this results in

Note that we consider only the case, as only under this condition heuristics proposed inChapelle and Zien (2005)are comparable with other heuristics presented in this Section and compatible with our experiment. However, the authors‘ original formulation allows for other values of. This heuristic, along with the complimentary for theparameter (see Section2.4.1) are denoted in the experiments asChapelle.

While the original Jaakkola‘s heuristics, described inJaakkola et al. (1999)andJaakkola et al. (2000), was supervised, in this article we will focus on its unsupervised version proposed inSoares et al. (2004).

The original heuristics based on median inter-class distance and is computed as follows: for all training exampleswe defineas a distance to its closest neighbour from a different class. Then a set of all nearest neighbour distances is computed as

and the value of.

This approach, however, has been interpreted differently inSoares et al. (2004), which resulted in an unsupervised heuristic based on what was proposed inJaakkola et al. (1999). The approach to estimateis similar, however, it is calculated without any knowledge about labels of examples, which means that not inter-class but inter-vector distances are used. Considering an unlabelled distanceof an exampleto its closest neighbour, the set of all neighbour distances is computed as

and the value of. This heuristic is denoted asSoares.

The use of mean instead of median in an approach proposed inSoares et al. (2004)results in larger values ofin the case of outliers in the data space. Therefore, following the reasoning in the original manuscriptJaakkola et al. (1999), we propose to compute, which in case of the Gaussian RBF kernel results in:

This heuristic is denoted asSoares_med.

The heuristic used to estimate the initial value ofin a well-known Python library scikit-learn, was proposed by Michael Gelbart inGelbart (2018)444https://github.com/scikit-learn/scikit-learn/issues/12741. The scaling factor of Gaussian RBF kernel is computed as

whereandis a variance of all elements in the data set.
It is easy to see that this heuristic is similar to the one discussed in Section2.3.1, based onVarewyck and Martens (2010): provided that every data feature has varianceand meanthe value of Gelbart‘s heuristics is equal to the one described by Equation5.
The advantage of this heuristics is its computational performance, and it has the potential to perform well when the variance of elements in the data array reflect the variance of the actual data vectors. This heuristic is denoted in our results asGelbart.

SECTION: 2.4Unsupervised heuristics for

Unsupervised heuristics for theparameter are much less common than for; inSchölkopf and Smola (1998b), there is a suggestion that parameter, whereis a measure for a range of the data in feature space and proposes examples of suchas the standard deviation of the distance between points and their mean or radius of the smallest sphere containing the data. However, to the best of our knowledge, the only actual derivation of this idea was presented inChapelle and Zien (2005), which we discuss below.

Given avalue (originally computed as described in Section2.3.3),Chapelle and Zien (2005)calculate the empirical variance:

which, withbeing the value of RBF kernel (4), under the sameassumption as Section2.3.3, evaluates to

Theparameter value is then estimated as

This heuristic is denoted in our experiments as:Chapellewhen used in combination with authors‘heuristic (see Section2.3.3) and+Cwhen used withcovtraceheuristic.

Our observations suggest that values of parameter, when dealing with high-dimensional data such as hyperspectral images, should be higher than estimated with the heuristic proposed in Section2.4.1. Therefore we propose a new version of the heuristic, by modifying the formula14. Since in formula14the factor, higher values ofcan be achieved by substitutingwith.

The value ofin Equation14is an average of kernel values for all data points, which, for the RBF kernel, is a function of the average distances between the data points. By selecting a subset of the data points based on values of their distances, we can arbitrarily raise or lower the value of. We start by considering a set of distances between the data points

Then we define a subset of distancesasquantile ofand we select a relevant set of data points pairs

This leads to a modified version of the heuristic

with. The rationale of usingquantile is that with increased dimension, the proposed condition will restrict the set of pairsto the distances between close points. This modified Chapelle‘s heuristic is denoted as+MC, when used withcovtraceheuristic for.

SECTION: 3Experiments

In this section we will present our method for experimental verification of unsupervised heuristics: the datasets that we use for tests, experimental procedure and finally our approach to statistical testing of obtained results.

SECTION: 3.1Datasets

Experiments were performed using 31 standard classification datasets obtained from Keel-dataset repository555https://sci2s.ugr.es/keel/category.php?cat=clas, described inAlcalá-Fdez et al. (2011). Instances with missing values and features with zero-variance were removed, therefore the number of examples/features can differ from their version in the UCIDua and Graff (2017)repository. The datasets were chosen to be diverse in regards to the number of features and classes and to include imbalanced cases. In addition, followingDuch et al. (2012), the chosen set includes both complex cases where advanced ML models achieve an advantage over simple methods as well as datasets where most models perform similarly. Reference classification results can be found inMoreno-Torres et al. (2012)or through OpenML projectFeurer et al. (2021). The summary of the datasets used in experiments can be found in TableLABEL:tab:datasets, along with the Overall Accuracy (OA) results of naive classifier (or zero-rule classifier, 0R) that classifies every point as the member of most frequent class.

Before the experiment, every dataset was preprocessed by centering the data and scaling it to the unit variance. This operation was performed using mean and variance values estimated from the training part of the dataset.

[
cap = Datasets,
caption = Datasets used in the experiment. Balance is the ratio between size of the smallest and largest class. OA(0R) denotes the accuracy of a zero-rule, naive classifier that predicts the label of the most frequent class.,
label = tab:datasets,
pos = h,
doinside = ]
lrrrrrl[a]As the dataset is named in KEEL repositoryhttps://sci2s.ugr.es/keel/datasets.php\FLName\tmark[a]&ExamplesFeaturesClassesBalanceOA(0R)Notes or full name\MLappendicitis106720.2580.2\NNbalance625430.1746.1Balance Scale DS\NNbanana5300220.8155.2Balance Shape DS\NNbands3651920.5963.0Cylinder Bands\NNcleveland2971350.0853.9Heart Disease (Cleveland), multi-class\NNglass214960.1235.5Glass Identification\NNhaberman306320.3673.5Haberman‘s Survival\NNhayes-roth160430.4840.6Hayes-Roth\NNheart2701320.8055.6Statlog (Heart)\NNhepatitis801920.1983.8\NNionosphere3513320.5664.1\NNiris150431.0033.3Iris plants\NNled7digit5007100.6511.4LED Display Domain\NNmammographic830520.9451.4Mammographic Mass\NNmarketing68761390.4018.3\NNmonk-2432620.8952.8MONK‘s Problem 2\NNmovement-libras36090151.006.7Libras Movement\NNnewthyroid215530.2069.8Thyroid Disease (New Thyroid)\NNpage-blocks54721050.0189.8Page Blocks Classification\NNphoneme5404520.4270.7\NNpima768820.5465.1Pima Indians Diabetes\NNsegment23101971.0014.3\NNsonar2086020.8753.4Sonar, Mines vs. Rocks\NNspectfheart2674420.2679.4SPECTF Heart\NNtae151530.9434.4Teaching Assistant Evaluation\NNvehicle8461840.9125.8Vehicle Silhouettes\NNvowel99013111.009.1Connectionist Bench\NNwdbc5693020.5962.7Breast Cancer Wisconsin (Diagnostic)\NNwine1781330.6839.9\NNwisconsin683920.5465.0Breast Cancer Wisconsin (Original)\NNyeast14848100.0131.2\LL

SECTION: 3.2Choosing SVM parameters for a given dataset

The experiments used either one or two stages of cross-validation – ’external‘ and ’internal‘ or ’external‘ only – depending on whether the grid search or heuristics were used. Let the heuristicsfrom the set of tested heuristicsbe a function that generates SVM parametersbased on a supplied training seti.e.. We denote bya heuristic which always returns a pair, which are commonly assumed defaults, and thus a reference values which are not data-dependent. Theheuristic is denoted in our experiments asdefault.

For every training setcorresponding with a givenfold of the external CV, and for every heuristicsparameters of the SVM were selected in three ways:

by performing a grid-search around the initial parametersand selecting the best model in the internal CV on.

by applying the heuristics,

by performing a grid-search around the initial parametersand selecting the best model in the internal CV on.

The range of parameters for GSCV to test is not always easy to determine as different studies propose different ranges - inMatheny et al. (2007)the rangeis taken into consideration for, while forits. Authors ofSchuhmann et al. (2021)proposewhile in research conducted inBudiman (2019)the selected range wasforandfor. InLameski et al. (2015), the authors decided to use the grid offor bothand.

In this research, similar approach was selected, with range of parameters set as, and the parameter gridfor the heuristicsgenerated as

where. For the external CV, the number of folds, for the internal CV the number of folds; both were stratified CVs, by which we mean the approach often used towards unbalanced sets which selects training and test sets maintaining similar percentage of datapoints from each class666we used implementation provided by https://scikit-learn.org/.

For assessing classification performance, the Balanced Accuracy measureBrodersen et al. (2010)(BA) was employed. BA can be expressed as the mean of classification accuracies in classes i.e. the mean between a ratio of correctly classified examples to the total number of examples in every class. Compared to the Overall Accuracy (OA), which is the ratio between a number of correctly classified examples to the total number of examples in dataset, it less sensitive to unbalance in class size.

The final performance of the classifier in an experiment is the mean BA between external folds. Every experiment was repeated 10 times and the final values of BA were obtained by averaging the performance values of individual runs.

SECTION: 3.3Statistical verification of results

A typical approach to verify statistical significance of results is to use null hypothesis significance testing (NHST). While common, the NHST has several disadvantages explained in detail inBenavoli et al. (2017). Two particular ones are: the fact that point-wise null hypotheses are usually false, provided that sufficiently large number of data points is available, as in practice no two classifiers have perfectly similar accuracy; NHST does not allow to reach conclusion when the null hypothesis is rejected, which limits its usefulness. As an alternative, authors ofBenavoli et al. (2017)propose a new methodology based on Bayesian analysis that was adapted for analysing our results. This methodology compares classifiers by estimating and querying the posterior distribution of their mean difference. The methodology introduces theregion of practical equivalence(rope) which refers to the value of mean difference that implies that classifiers are practically equivalent e.g. when their accuracies differ by less then 1%. This allows to infer the probabilityof the mean difference between classifiers being practically negative which implies that classifieris more accurate, as well as the probability of the opposite inequality and the probabilitythat both classifiers are practically equivalent with regards to the rope value. In addition the methodology allows for drawing conclusions through the simultaneous analysis of multiple data sets and it has a dedicated, clear visualisation of test results.

Since we perform experiments using multiple datasets, the approach employing hierarchical models, described in Section 4.3.1 ofBenavoli et al. (2017)was employed. Following the suggestion inBenavoli et al. (2017), the value of rope was set to 1%.

SECTION: 3.4Implementation

SVM implementation was from the scikit-learn library v1.0.2. Bayesian comparison of classifiersBenavoli et al. (2017)and its visualisation was performed using baycomp library v. 1.0.2777https://github.com/janezd/baycomp. Matplotlib and seaborn libraries were used for data visualisation.

SECTION: 4Results and discussion

Our experiments compared the accuracy of the previously discussed UH-SVM approaches, to the GSCV-SVM, on the 31 Keel datasets. For each approach, the individual scores were aggregated into an estimated probability of practical advantage/disadvantage/equivalence of the heuristics and GSCV with regards to classifier accuracy. The summary of results for the Balanced Accuracy (BA) measure888For the reference, results of experiments for OA measure are presented in the AppendixLABEL:tab:results_bayes.is presented in TableLABEL:tab:results_bayes_aa. Since most of the heuristics only estimate theparameter, and only two of them estimate the(Chapelle,MC), we present results as a combination of everyandheuristics including the ’default‘ value of,. The advantage or any disadvantage of any one method corresponds to a sufficiently large difference between means of accuracies over all datasets, as described in Section3.3; their practical equivalence corresponds to sufficiently small difference, with regards to rope value of 1%.

When no heuristics or onlyheuristics are used, parameters obtained by GSCV result in significantly higher accuracy. There‘s only a marginal improvement whenChapelleheusitics is used for selecting aparameter value. However, when using our proposed extension, theMCheuristics, five of theheuristics tested obtained the accuracy very close, or practically equivalent to CV. The combination ofCovtrace+MCresulted in the highest estimated value of this probability which indicates, that on average this heuristics results in classification accuracy no worse than GSCV. Visualisation of results for example heuristics is presented in Figure3. The improvement in accuracy arising from the use of the two heuristics (three, if including the default) for theparameter is clearly evident in plots (a–c). Notably, the more effective the heuristic, the more equivalent are the scores of UH-SVM and GSCV-SVM.
Plot (d) presents similar results for an overall accuracy (OA) measure compared to the BA in plot (c). The use of OA measure usually results in slightly higher probabilities of practical equivalence between heuristics and GSCV. This suggests the class imbalance negatively affects GSCV‘s performance.

The practical equivalence in the accuracy of classifiers whose parameters were chosen by GSCV and heuristics, is also visible during the inspection of the parameter values obtained from heuristics plotted on the graph showing the relationship between the classifier‘s effectiveness and its parameters (estimated through a dense grid of parameters). In the selected representative examples on Figure2, it can be seen that most of these points, especially for the best heuristics, are usually located in areas of high accuracy.

Interestingly, out ofSmolaheuristics, the result ofSmola+MCresulted in the BA value most equivalent to GSCV. This indicates that the median distance between examples in the data space is of particular importance when choosing theparameter.

Comparison of execution time for heuristics and GSCV is presented in TableLABEL:tab:time. The values express a ratio of mean computation time of an experiment with GSCV parameter selection to experiment with parameters selected with heuristics. The average time was calculated over ten iterations of the experiment across all datasets. The use of heuristics allows, on average, to speed up calculations 100–200 times. Differences in times result not only from calculating the parameter values, but also from the impact of these values on the classifier – increasing the value of theandparameter extends the calculation time.

[
caption = Results of experiments – performance of different UH-SVM approaches with respect to GSCV-SVM. The numbers correspond to probabilities computed with the Bayesian analysis with methodology fromBenavoli et al. (2017). Three right columns present probabilities of cross-validation being on average more/ equivalently / less accurate than heuristics. Results were obtained for the balanced accuracy measure and rope value of 1%. Note that, with proposed (MC) heuristic, several ofheuristics achieve results very close to GSCV.,
label = tab:results_bayes_aa,
pos = !h]
lllll\FLC heuristics&heuristicsP()P()P()\MLdefault  1.00  0.00  0.00Gelbart  0.85  0.13  0.02\NNSmola_10  0.97  0.02  0.01\NNSmola_50  0.86  0.11  0.03\NNdefault Smola_90  0.99  0.00  0.01\NNSoares  1.00  0.00  0.00\NNSoares_med  1.00  0.00  0.00\NNChapelle  0.98  0.01  0.01\NNcovtrace  0.93  0.06  0.01\MLdefault  1.00  0.00  0.00\NNGelbart  0.60  0.36  0.04\NNSmola_10  0.96  0.03  0.01\NNSmola_50  0.68  0.24  0.08\NNChapelleSmola_90  0.84  0.12  0.04\NNSoares  0.99  0.00  0.01\NNSoares_med  0.99  0.00  0.01\NNChapelle  0.83  0.14  0.03\NNcovtrace  0.55  0.41  0.05\MLdefault  1.00  0.00  0.00\NNGelbart  0.19  0.76  0.05\NNSmola_10  0.68  0.28  0.04\NNSmola_50  0.17  0.81  0.02\NNMCSmola_90  0.34  0.60  0.07\NNSoares  0.98  0.00  0.02\NNSoares_med  0.99  0.00  0.01\NNChapelle  0.21  0.76  0.02\NNcovtrace  0.12  0.84  0.03\NN

[
caption = Performance of heuristics as ratio of CV/heuristics execution time i.e. how many times heuristics is faster than CV. Times were estimated from 10 experiments and averaged over all datasets. Note that in almost all cases the speedup is 100–200 times.,
label = tab:time,
pos = h]
llll\FL& C heuristics\NNheuristicsDefaultChapelleMC\MLdefault  136.88 106.52  97.91Gelbart  248.72  182.27  149.33\NNSmola_10  153.92  136.47  121.71\NNSmola_50  169.13  149.29  131.29\NNSmola_90  158.75  149.64  131.19\NNSoares  132.01  105.50  94.59\NNSoares_med  125.46  101.45  92.51\NNChapelle  163.59  148.40  132.23\NNcovtrace  237.38  188.61  154.86\ML

To summarise: estimation of both parameters, in particular withCovtrace+MCheuristics, leads to accuracy practically equivalent to GSCV (see Figure2(c)) with parameters obtained in only0.006 of its working time (see TableLABEL:tab:time).
Unsupervised heuristics for SVM parameters are likely effective because the test datasets conform to the clustering assumption, where data space forms structures/clusters useful to the classification problem and data point distributions reflect class divisions. However, the same assumption is the basis of training set selection with GSCV. As datasets deviate from the clustering assumption, the effectiveness of both approaches decreases, especially when training data is limited.

GSCV is by no means inferior to the heuristics, especially if supplied with proper number of labelled datapoints. In practice, however, the differences are often very small. and while it is natural to use GSCV when standard approach is preferable (small number of examples, training time is not an issue), in many scenarios i.e. processing on edge IoT devices, proposed heuristics offers practically equivalent accuracy in fraction of time..

SECTION: 5Conclusions

In this study we evaluated unsupervised heuristics for SVM parameter selection on over thirty benchmark datasets, comparing their performance with GSCV. We have also proposed a modification to Chapelle & Zien heuristics for theparameter as optimisation of both parameters is vital for accurate classifiers. We compared results with methodology based on Bayesian analysis, described inBenavoli et al. (2017). Our results indicate that heuristics and in particular the proposedcovtrace+MC, are usually practically equivalent to GSCV i.e. obtained accuracies differ by less than 1% (see Figure2(c)and probabilities of equivalence in TableLABEL:tab:results_bayes_aa). Moreover, these heuristics offer a computation time reduction, achieving a 100-200 times speed-up (see TableLABEL:tab:time). This makes unsupervised, heuristic approach to parameters selection a compelling alternative for GSCV for rapid SVM calibration.

[
caption = Results of experiments – performance of different UH-SVM approaches with respect to GSCV-SVM, for Overall Accuracy (a supplement to TableLABEL:tab:results_bayes_aa) The numbers correspond to probabilities computed with the Bayesian analysis with methodology fromBenavoli et al. (2017). Three right columns present probabilities of cross-validation being on average more/ equivalently / less accurate than heuristics. Results were obtained for the rope value of 1%. Note that, with proposed (MC) heuristic, several ofheuristics achieve results very close to GSCV.,
label = tab:results_bayes,
pos = h]
lllll\FLC heuristicsheuristicsP()P()P()\MLdefault  1.00  0.00  0.00\NNGelbart  0.70  0.25  0.05\NNSmola_10  0.94  0.05  0.01\NNSmola_50  0.61  0.35  0.04\NNDefault Smola_90  0.95  0.03  0.02\NNSoares  0.99  0.00  0.00\NNSoares_med  1.00  0.00  0.00\NNChapelle  0.94  0.04  0.02\NNcovtrace  0.74  0.23  0.04\MLdefault  1.00  0.00  0.00\NNGelbart  0.56  0.40  0.04\NNSmola_10  0.92  0.07  0.01\NNSmola_50  0.66  0.27  0.07\NNChapelle Smola_90  0.85  0.10  0.05\NNSoares  1.00  0.00  0.00\NNSoares_med  1.00  0.00  0.00\NNChapelle  0.74  0.21  0.04\NNcovtrace  0.68  0.24  0.08\MLdefault  1.00  0.00  0.00\NNGelbart  0.10  0.90  0.01\NNSmola_10  0.54  0.44  0.01\NNSmola_50  0.13  0.86  0.00\NNMC Smola_90  0.41  0.58  0.02\NNSoares  0.99  0.00  0.00\NNSoares_med  1.00  0.00  0.00\NNChapelle  0.19  0.80  0.01\NNcovtrace  0.10  0.89  0.01\LL

SECTION: References