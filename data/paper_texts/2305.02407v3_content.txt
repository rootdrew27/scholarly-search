SECTION: Predicting Stellar Rotation Periods Using XGBoost

Context.The estimation of rotation periods of stars is a key problem in stellar astrophysics.
Given the large amount of data available from ground-based and space-based telescopes, there is a growing interest in finding reliable methods to quickly and automatically estimate stellar rotation periods with accuracy and precision.

Aims.This work aims to develop a computationally inexpensive approach, based on machine learning techniques, to accurately predict thousands of stellar rotation periods.

Methods.The innovation in our approach is the use of the XGBoost algorithm to predict the rotation periods ofKeplertargets by means of regression analysis.
Therefore,
we focused on building a robust supervised machine learning model to predict surface stellar rotation periods from structured data sets built from theKeplercatalogue of K and M stars.
We analysed the set of independent variables extracted fromKeplerlight curves and investigated the relationships between them and the ground truth.

Results.Using the extreme gradient boosting method, we obtained a minimal set of variables that can be used to build machine learning models for predicting stellar rotation periods.
Our models are validated by predicting the rotation periods of aboutstars.
The results are compatible with those obtained by classical techniques and comparable to those obtained by other recent machine learning approaches, with the advantage of using much fewer predictors.
Restricting the analysis to stars with rotation periods of less than 45 days, our models are on averagecorrect.

Conclusions.We have developed an innovative approach, based on a machine learning method, to accurately fit the rotation periods of stars.
Based on the results of this study, we conclude that the best models generated by the proposed methodology are competitive with the state-of-the-art approaches, with the advantage of being computationally cheaper, easy to train, and relying on small sets of predictors.

SECTION: 1Introduction

Measuring how stars rotate is an essential part of stellar astrophysics.
One of the main methods for quantifying the rotation periods of stars is to analyse their light curves and look for modulations that can be related to the stellar rotation rate.
Solar-type stars,i.e., low-mass stars with convective outer layers, are known to develop spots on their surfaces.
The origin of these spots is related to stellar magnetism(e.g.,  Brun & Browning,2017).
Similarly, high-mass(e.g.,  Cantiello & Braithwaite,2011)and fully convective low-mass stars(e.g.,  Bouvier & Bertout,1989; Damasso et al.,2020; Bicz et al.,2022)also exhibit magnetic spots on their surfaces.
Such spots can induce a modulation in stellar light curves which, in principle, allows the determination of both stellar rotation periods and long-term stellar magnetic activity cycles(e.g.,  Strassmeier,2009).
The rotation period of a star is essential for the understanding of the transport of stellar angular momentum, a process that is still poorly understood(Aerts et al.,2019), and is important for the correct estimation of the age of stars(Eggenberger et al.,2009).
The latter can be very important for the characterisation of planetary systems(Huber et al.,2016).
The role of stellar rotation in driving stellar dynamos and determining magnetic cycles is also still much debated(e.g. Bonanno & Corsaro,2022).
Any theory on this subject requires an accurate calculation of rotation periods for all types of stars.

The accuracy and precision with which stellar rotation is measured are crucial for the study of stellar evolution.
The rotation period of a star correlates with its age: solar-type stars are known to spin down during their main-sequence evolution, and so the rotation period of young solar-like stars can be used to constrain stellar ages usinggyrochronology relations(Barnes,2003; Skumanich,1972; García et al.,2014; Messina,2021; Messina et al.,2022).
However, for stars older than the Sun, gyrochronological ages do not agree with asteroseismic ages(Van Saders et al.,2016; Hall et al.,2021)and those inferred from velocity dispersion(Angus et al.,2020).
Such a discrepancy may then be solved only by developing new methods for accurately measuring stellar rotation.

The vast amount of astronomical photometric data released over the last three decades has recently motivated the use of machine learning (ML) techniques to process and analyse it.
The advent of new large-scale sky surveys and the need to process large numbers of targets simultaneously make manual handling of astrophysical data impractical, and the use of artificial intelligence (AI) techniques is becoming increasingly popular(e.g.,  Pichara Baksai et al.,2016; Biehl et al.,2018).
A stellar light curve is nothing more than atime seriesof photometric data from a star,i.e., a sequence of stellar surface fluxes collected at successive points in time.
The last decade has seen the emergence of many observations that provide high-quality, long-term and near-continuous stellar photometric data.
Examples include theKeplerspace observatory(Borucki et al.,2009)and the rebornKeplerK2 mission(Howell et al.,2014), which together have observed more than half a million stars(Huber et al.,2016),111https://exoplanets.nasa.gov/resources/2192/nasas-kepler-mission-by-the-numbers/and the Transiting Exoplanet Survey Satellite(TESS, Ricker et al.,2014), which has collected light curves with time spans of 25 days to one year for tens of millions of stars.
Such a large number of observations requires automated procedures to process and extract information from them, and machine learning methods can be used to do this.

The first step in choosing an ML technique is to select the data from available sources and study them carefully.
Typically, two approaches can be adopted: using the photometric time series data directly as input, or first converting the light curves into structured data that can be represented by a set of variables or features in tabular form.
Machine learning models can be trained from these two types of data (unstructured and structured), automating processes that would otherwise be tedious or require too much human effort.

The first case—unstructured data—is usually tackled with special types ofartificial neural networks(ANN, Haykin,2009), reinforcement algorithms that require little or no pre-processing of the data.Blancato et al. (2022)used adeep learning(DL) approach and appliedconvolutional neural networks(CNN) to predict stellar properties fromKeplerdata.Claytor et al. (2022)applied a CNN to synthetic light curves to infer stellar rotation periods, and then estimated the rotation periods ofmain-sequence TESS stars, with periods up to 35 days for G, K dwarfs and up to 80 days for M dwarfs(Claytor et al.,2024).
This was achieved using a CNN, which allowed them to remove light curve systematics related to the telescope’s 13.7-day orbit.

However, training neural networks typically requires large computational resources.

The second scenario—using structured data—is solved by resorting to algorithms that can use tabular data to perform unsupervised (clustering) and supervised (classification and regression) tasks.Lu et al. (2020)developed a model that predicts the rotation periods of TESS stars with an uncertainty of, by training arandom forest(RF, Breiman,2001)regressor onKeplerdata.
The most important features for predictions in their model were the range of variability in the light curve, the effective temperature, theGaiacolour, the luminosity, and the brightness variation on timescales ofor less.Breton et al. (2021)applied RFs to tabularKeplerdata to produce three ML classifiers that can detect the presence of rotational modulations in the data, flag close binary or classical pulsator candidates, and to provide rotation periods of K and M stars from theKeplercatalogue(Brown et al.,2011).
The method was then applied to F and G main-sequence and cool subgiantKeplerstars bySantos et al. (2021).Breton et al. (2021)used 159 different inputs to train the classifiers: rotation periods, stellar parameters such as mass, effective temperature, and surface gravity (just to name a few), and complementary variables obtained from wavelet analysis, the autocorrelation function of light curves and the composite spectrum.
They claim accuracies ofwhen willing to accept errors withinof the reference value, and ofafter visual inspection ofof the observations.Breton et al. (2021)used a classification approach,i.e., their algorithm was trained to select the most reliable rotation period among these candidate values.
A particularity of their work is therefore that they used rotation periods as input variables for training their models.
We expected these features to be highly correlated with the response or target variable (the rotation periods considered as ground truth), and so we decided to(i)explore their data set and determine what is this level of correlation, and(ii)try to train ML models without these rotation period input variables and compare the performance of our models with theirs.

In this paper, we focus on the prediction of rotation periods of K and M stars from theKeplercatalogue(Borucki et al.,2009,2010), using an ML method.
We follow a regression-based approach,i.e., we propose to predict the rotation period of several targets, rather than selecting the best candidate among some previously computed values, as in a classification problem.
We address the selection of suitable data, the size of the data set, the optimisation of the ML model parameters, and the training of the latter.
Time and computational resources were also important constraints in the development of this project, especially during the learning phase of the models.

The paper is structured as follows: insections2and3, we describe the materials—the data—and the methods used for the experiments;section4is a discussion of our experimental design;
insection5, we present the results of our contributions by applying a supervised ML approach to predict the rotation periods of stars;section6is dedicated to the discussion of the results, and we summarise our contributions insection7.

SECTION: 2Materials

The main objective of this project is to develop robust yet computationally inexpensive supervised ML models from tabular astronomical data for the prediction of stellar rotation periods of K and M stars from theKeplercatalogue.

We used sets of structured, tabular data, containing measurements for thefeaturesorpredictorsand for theresponseortargetvariable.
Computationally, these tabular data have been organised intodata frames, where the columns correspond to the variables (predictors plus response) and the rows correspond to the observations.
Each of the rows, consisting of measurements for one star, will be referred to as anobservation, aninstance, acase, or anobject.

These data sets were divided intotrainingandtestingsets.
The former were used to build predictionmodelsorlearners, which in turn were used to predict the rotation period of unseen stars, provided they were input as structured data sets similar to the training set,i.e., in tabular form, containing at least some of its features.
The testing sets acted as the never-before-seen objects, with the advantage of containing the outcome (not given to the model), which could be used to assess the predictive performance and quality of the models by comparing the predicted values with the true outcomes orground truth.
We consider a good model to be one that accurately predicts the response.

The following sections describe the specific materials and methods used to build the models and assess their performance.222TheRprogramming language(R Core Team,2023)was used for all code and data analyses.

SECTION: 2.1Description of the Data

We focused on real data, including features extracted from the light curves, and “standard” predictors,i.e., variables that are commonly obtained directly or indirectly from astronomical observations.

We started by analysing structured data from theKeplercatalogue of K and M dwarf stars, already in tabular form, published bySantos et al. (2019)andBreton et al. (2021), whose targets were selected from theKeplerStellar Properties Catalogue for Data Release 25(Mathur et al.,2017).
Both catalogues—hereafter referred to as S19 and B21, respectively—contain all the predictors and targets considered in our work: B21 was used as the source for most of the predictors; S19 was used specifically to extract the rotation periods and features obtained directly fromKeplerobservations.
The latter are commonly known as “stellar parameters” in the astronomical community—examples include the mass of the star and its effective temperature, to name but a few.
However, we will not use the word “parameter” in this context to avoid confusion withmodel parameters.
Instead, we will use the terminology ofastrophysical variableswhen referring to them.

B21 is available in tabular form, without any particular clustering or classification of the variables.
However, we decided to group the predictors according to their nature and/or the method by which they were obtained.
We have identified six groups, with the following characteristics:333Words in brackets, intypewriter font, refer to variable names as they appear intable1and in the data frames.

Astrophysical(Astro,astro) — predictors related to the physical properties of the stars, derived directly or indirectly from the observation, to wit:

effective temperature,(teff), and its corresponding upper and lower errors,(teff_eup) and(teff_elo) respectively;

the logarithm of the surface gravity,(logg) and its upper and lower error limits,(logg_eup) and(logg_elo);

the mass of the star,(m), and its upper and lower errors,(m_eup) and(m_elo);

the magnitude from theKeplerinput catalogue,(kepmag); and

the Flicker in Power orFliPervalues,(f_07),(f_7),(f_20), and(f_50), respectively corresponding to cut-off frequencies of.

Time Series(TS,ts) — quantities that are related to the properties of the time series:

the length of the light curve in days,(length);

the start and end times of the light curve,(start_time) and(end_time) respectively;

the number of bad quarters in the light curve,(n_bad_q);

the photometric activity proxy or index,(sph), and its error,(sph_e), according toSantos et al. (2019);

the photometric activity proxy computed from the autocorrelation function (ACF) method to obtain the rotation period,(sph_acf_xx), for each of the-day filters, where, as provided byBreton et al. (2021), and its corresponding error,(sph_acf_err_xx);444InBreton et al. (2021),is divided into several values, each corresponding to the process from which it was obtained (ACF, CS, or GWPS) and to the filter applied to the light curves (20, 55, or 80 days).

the height of the(i.e., the period of the highest peak in the ACF at a lag greater than zero),(g_acf_xx), for each of the-day filters; and

the mean difference between the heights ofat the two local minima on either side of,(h_acf_xx), for each of the above filters.

Global Wavelet Power Spectrum(GWPS,gwps) — quantities obtained from a time-period analysis of the light curve using a wavelet decomposition:

the amplitude (gwps_gauss_1_j_xx), central period (gwps_gauss_2_j_xx), and standard deviation (gwps_gauss_3_j_xx) of theGaussian fitted to the GWPS with the-day filter;

the mean level of noise of the Gaussian functions fitted to the GWPS for the-day filter (gwps_noise_xx);

theof the fit of the Gaussian function to GWPS for the-day filter (gwps_chiq_xx);

the number of Gaussian functions fitted to each GWPS for the-day filter (gwps_n_fit_xx); and

the photmetric proxy computed from the GWPS method,(sph_gwps_xx), as provided byBreton et al. (2021), and its corresponding error,(sph_gwps_err_xx).

Composite Spectrum(CS,cs) — variables obtained from the product of the normalised GWPS with the ACF:

the amplitude of the(the period of the fitted Gaussian with the highest amplitude),(h_cs_xx), for the-day filter;

the average noise level of the Gaussian functions fitted to the CS,(cs_noise_xx), for the-day filter;

theof the fit of the Gaussian function to the CS for the-day filter (cs_chiq_xx);

the photometric proxy computed by the CS method,(sph_cs_xx), and its error,(sph_cs_err_xx) for the-day filter; and

the amplitude (cs_gauss_1_j_xx), central period (cs_gauss_2_j_xx), and the standard deviation (cs_gauss_3_j_xx) of theGaussian fitted to the CS with the-day filter, for.

Rotation Periods(Prot,prot) — rotation periods obtained by combining the ACF, CS, and GWPS time-period analysis methods with the KEPSEISMIC light curves, for the-day filters:(prot_acf_xx),(prot_cs_xx), and(prot_gwps_xx).555The KEPSEISMIC time series are available from the Mikulski Archive for Space Telescopes (MAST), via the linkhttps://dx.doi.org/10.17090/t9-mrpw-gc07.

The list of 170 explanatory variables, as they appear in the final data set after exploratory data analysis, cleaning, and engineering, is referenced intable1.
In total, there are 14 Astro, 18 TS, 66 GWPS, 63 CS, and nine Prot predictors.

We chose to use previously published real-world data because, on the one hand, they are readily available;
and, on the other hand, because we wanted to compare distinct machine learning approaches using different sets of predictors—thus analysing how much the predictions are affected by the variables closely related to the method used to estimate the reference rotation periods—and to compare the results with the state of the art—comparisons that can only be made fairly if the data sets are built from the same reference set of observations.

In the following section, we will describe the steps we applied to S19 and B21 to build a master data set and several subsets which we used to build the ML models.

SECTION: 2.2Data Engineering

S19 and B21 were cleaned and merged to create a master data set, from which several subsets were generated.
During the cleaning process,(i)columns containing only null values were dropped,(ii)rows with less thanof non-null predictors were removed,(iii)rows containing extreme standard deviation values were removed,(iv)very extreme values of variables other than standard deviations were flagged asNaNand subsequently imputed,(v)rows corresponding to targets with rotation errors greater thanof the respective rotation period were removed,(vi)flag-type variables from S19 and B21 were dropped,(vii)all variable names were converted to lowercase, and(viii)some names were changed, for clarity.

In the merging process, we selected all stars that were present simultaneously in both data sets.
As explained in detail insection3.1, we chose to use the XGBoost (XGB) version of the tree-based ensemble approach known asgradient boosting(GB) to train our models.
Missing values were imputed using a random forest (RF) with 500 trees and a sample fraction of 0.5.
Following data imputation, we decided to create a parallel data set with standardised predictors for further analysis.
We note that because we used a tree-based (as opposed to a distance-based) ensemble method, normalisation of the predictors would not be strictly required(Murphy,2012; Chen & Guestrin,2016).
Nevertheless, since XGBoost uses a gradient descent algorithm, we decided to use the normalised version of the data set to create the subsets from which the XGB models would be built, in order to optimise the computations during the training phase.

Several features, mainly in the CS and GWPS groups (recall the group in variables in the previous section), contained values with extremely large or infinite amplitudes.
They are derived variables, corresponding to some of the amplitudes, central periods, standard deviations, and mean noise levels of the Gaussian functions fitted to the CS and GWPS.
Their extreme values may have different origins, such as instrumental errors, and possibly they may have been taken into account when they were created.
In principle, the presence of outliers would not be a problem, as there are no restrictions on extreme points in GB, and XGB can handle missing values.
However, as we are validating a methodology and want to leave open the possibility of comparing the performance of XGBoost with other methods that cannot handle missing values (such as some implementations of RFs), we chose to flag these outliers as missing values and subsequently impute them.
We note that the main data set still contains large values from which the models could learn.

After the aforementioned transformations, we ended up with a data set containingobservations and 170 predictors.
This is the full or master set, labelledData Set 0(DS0).

Several subsets were created from the full data set, each containing a different number of predictors and/or instances:

The first subset was obtained by removing the nine rotation period variables (Prot group), resulting in a data set of 163 variables. This is referred to asData Set 1(DS1).

A second subset was created by removing from DS1 redundant variables, correlated between them, resulting in a data set with 152 predictors —Data Set 2(DS2).

The third subset was obtained from DS2, by removing stars with surface rotation periods greater than, leaving a data set ofrows — this isData Set 3(DS3).

The fourth subset is obtained by removing from DS3 stars with surface rotation periods less than, resulting in a data set ofrows —Data Set 4(DS4).

A fifth subset was built on top of DS3, using the twenty-eight most important variables as assessed by the previous models —Data Set 5(DS5).

A sixth subset was obtained from DS4, using the twenty-eight most important variables as scored by the models built with DS0 to DS4 —Data Set 6(DS6).

A final subset was created from DS1, where we used the same twenty-eight most important variables used to build DS5 and DS6 —Data Set 7(DS7).

DS2 was constructed by performing a simple univariate filtering of the features, where the predictors were cross-validated with the response variable in order to remove redundant or uninformative variables from the learning process.
The analysis of correlations between variables is detailed insection2.3.

DS3 was built because ofKepler’s 90-day quarters(Mullally,2020), to ensure at least two full cycles of observations per object, thus preventing any problems that may arise from stitching together data from different quarters and other potential long-term issues related to the telescope, such as those related toKepler’s orbital period and its harmonics.
AlthoughBreton et al. (2021)have identified and distinguished rotating stars from other types of objects, we also removed stars with surface rotation periods less than, because these targets can easily be mistaken for close-in binary systems,  classical pulsators, or other false positives, whose signals may mimic stellar objects manifesting surface rotation, and we wanted to check how the model would behave by removing these potentially misclassified targets.
By including only stars with rotation periods betweenand, as we did in DS4, we expected to improve the predictive performance of the models.
We ended up withstellar objects, a reduction of aboutcompared to the DS0, DS1, and DS2 data sets.

We also selected the top-20 most important variables in the models trained with the data sets DS0 to DS4, and took their union.
This resulted in a set of 28 predictors, which we used to build DS5, DS6, and DS7.
These three data sets contain only the most important features identified during the learning of the previous XGB models.

SECTION: 2.3Statistical Analysis of Relevant Variables

All variables present in DS0, including the response,(the stellar rotation period, in days), are continuous.
The histograms and box-plots of the target variable, as extracted from DS0 (top panels) and its training set (bottom panels), are shown infig.1.
The sample distributions are very similar to each other.
The aforementioned plots suggest unimodal, slightly platykurtic, right-skewed distributions, with several outliers towards large quantiles, corresponding to the largest stellar rotation periods, above approximatelyup to about.
The medians are equal to, with minima and maxima of, respectively.
The histograms show that the majority of the stars in the sample have rotation periods roughly between zero and, but the right tail of the distributions extends well beyond this interval.
Therefore, there is no statistical evidence of uniformity in the distribution of.

Figure2is similar tofig.1, but for DS4.
The distributions on the full and training sets are very similar to each other.
There is no statistical evidence of uniformity in either set, although the distributions are flatter than their DS0 counterparts.
The plots suggest unimodal, platykurtic, slightly right-skewed distributions.
For both DS4 and its training set, the median isand the minimum and maximum are, respectively;
the mean is equal toand the standard deviations are, respectively.
From the histograms, we can see that the sample lacks stars with rotation periods of up to aboutand above about.
The fact that the distribution ofis not uniform affects the learning of the models, especially in the less represented regions.

In DS0, after removing targets with rotation errorslarger than the corresponding rotation periods, as described insection2.2, the distribution of the rotation period errors (,fig.3) is unimodal, right skewed, and slightly platykurtic.
The errors vary between, with a median of.
The outliers indicated by the box-plot are not significant, as all the errors are less thanof their corresponding rotation periods.
A significant number of points in the QQ-plot are outside theconfidence band, and the graph has two pronounced tails.
Therefore, there is no statistical evidence of normality in the distribution of.

The scatter plot of the rotation period errors as a function of the rotation periods is illustrated infig.4.
The majority of the measurements correspond to targets with rotation periods of less than 50 days.
Two branches can be identified, one starting at the lowest rotation periods and another starting at.
The nature of these branches is not known.
Both branches show a linear relationship betweenandwithout much dispersion up to a certain rotation period of abouton the upper branch andon the lower branch.
Beyond these limits, the dispersion increases with.
The upper branch goes up to the maximum values of, while the lower branch stops at about.
An outlier atseems to belong to the lower branch, but as with the branches, its nature is unknown.
For rotation periods of around 50 days, the amplitude of the dispersion in the upper branch is about 7 days.
Increasing dispersion withcan lead to a decrease in model performance and be a source of outliers in the predictions.

The correlations between the Prot, TS, and Astro families of variables and the response are shown infigs.5,6and7.
The correlation between some of the variables belonging to the CS and GWPS families and the Astro group and Prot is shown infigs.19and20.

Reddish colours indicate a positive correlation between any two features, blueish colours indicate a negative correlation, and white indicates no correlation at all.
As expected, most of the Prot variables are strongly positively correlated with the response, since the latter is obtained directly from the estimation of the former.
This was the motivation for creating the DS1 data set.
Some of the TS features are negatively correlated with the response (fig.6), such ash_acf_20,g_acf_20, orsph_acf_20.
These predictors are expected to be important in building the models, as opposed to variables that have little correlation with, such asstart_time,end_time, andn_bad_q.
All TS variables extracted from the ACF are highly correlated with each other, which in principle is not a problem because we used tree-based ensembles (seesection3.1).

Classical Astro variables are not correlated with the response (fig.7), apart from the mass, some cut-off frequencies of the Flicker in Power, and the effective temperature.

However, some features of the CS and GWPS families, such ascs_gauss_,gwps_gauss_, and their errors, are strongly correlated with the FliPer metrics(Bugnet et al.,2018, seefigs.19and20).
This means that, in principle, any one of these variables could be replaced for the other without affecting how well the model performs.
For the CS and GWPS families of variables, we identified three groups of correlation with the response: low correlation, strong positive and strong negative correlation.
Examples of strong positive correlations arecs_gauss_2_1_20,cs_gauss_3_1_20, andgwps_gauss_2_1_55, and examples of strong negative correlations areh_cs_20andsph_gwps_20, just to name a few.

SECTION: 3Methods

SECTION: 3.1Problem Formulation

Stellar rotation periods are positive real numbers, that can range from almost zero up to tens or hundreds of days(e.g.,  Santos et al.,2019; McQuillan et al.,2014), and so we framed the problem as a regression task.
Assuming that there is a relationship between a quantitative responseand a set ofpredictors, our problem can be described mathematically by the equation

whereis an unknown fixed function of, andis a randomerror termindependent of, with zero mean.
The functioncan be estimated, but it is never fully known.
If the inputsare readily available, given an estimate for,, the output can be predicted by

In the context of this project, the target variableand thepredictors are the rotation period,, and the set of variables described insections2and1, respectively, so thateq.1becomes

In this equation, the indexis an integer that varies up to 170, according to the number of variables that make up the data set used to train the model (DS0 to DS7).
The models, trained by ML methods, can be expressed generically as

whereis the estimate of.
We were not concerned with the exact form of, but rather with the accuracy of the predictions made by the models.

An important condition for us was that the models should simultaneously be robust, have good predictive performance, and be computationally cheap.
Given the tabular nature of the available data, we decided to use a tree-based ensemble method, specifically XGBoost, to tackle the research problem, as it is fast and has recently been shown to be the best for predictive tasks from structured data(Géron,2017; Raschka & Mirjalili,2017).

The proposed method aims to improve the predictive performance of existing models, such as the RF classifier published byBreton et al. (2021), so that rotation periods can be estimated for thousands of K and M stars from theKeplercatalogue, in an efficient and timely manner, with little human interaction.

SECTION: 3.2The Extreme Gradient Boosting Approach

Boosting can combine several weak learners,i.e., models that predict marginally better than random, to produce a strong model, an ensemble with a superior generalised error rate.Gradient Boosting(GB, Friedman,2001)is a method that can be applied to classification and regression tasks, based on the idea ofsteepest-descent minimisation.
Given a loss functionand a weak learner (e.g., regression trees), it builds an additive model

which attempts to minimise.
In this equation,is theweight of the weak base model, andis the predictor, as defined insection3.1.
The algorithm is typically initialised with the best estimate of the response, such as its mean in the case of regression, and it tries to optimise the learning process by adding new weak learners that focus on the residuals of the current ensemble.
The model is trained on a set consisting of the cases, whereis theresidual, given by

The residuals thus provide the gradients, which are easy to calculate for the most common loss functions.
The current model is added to the previous one, and the process continues until a stopping-condition is met (e.g., the number of iterations, specified by the user).

We used XGBoost, a successful implementation of GB, which was designed with speed, efficiency of computing resources, and model performance in mind.
The algorithm is suitable for structured data.
It is able to automatically handle missing values, it supports parallelisation of the tree-building process, it automatically ensures early stopping if the training performance does not evolve after some predetermined iterations, and it allows the training of a model to be resumed by boosting an already fitted model on new data(Chen & Guestrin,2016).
The method implements the GB algorithm with minor improvements in the objective function.
A prediction of the response is obtained from a tree-ensemble model built fromadditive functions:

where the functionbelongs to the functional spaceof all possible decision trees.
Eachrepresents an independent tree learner.
The final prediction for each example is given by the aggregation of the predictions of the individual trees.
The set of functions in the ensemble is learned by minimising a regularised objective function,, which consists of two parts:

The first term on the right-hand side ofeq.8is thetraining loss,, and the second term is theregularisation term.
Here,is a differentiable convex loss function that measures the difference between the predicted and the true values of the response.
The training loss measures how good the model is at predicting using the training data, and it may be selected from a number of performance metrics, some of which will be described in detail insection3.3.
The regularisation term controls (penalises) the complexity of the model.
It helps to smooth the final learning weights, to avoid overfitting.
In practice, the regularised objective function tends to select models that use simple and predictive functions(Chen & Guestrin,2016).
A model is learned optimising the objective function above.

The XGBoost algorithm has severalhyperparameters, that are not considered ineq.4.
Hyperparameters are tuning parameters that cannot be estimated directly from the data, and that are used to improve the performance of an ML model(Kuhn et al.,2013).
They are parameters of a learning algorithm, not of a model, and are mostly used to control how much of the data is fitted by the model, so that, ideally, the true structure of the data is captured by the model, but not the noise.
Optimising them using a resampling technique is crucial and the best way to build a robust model with good predictive performance.
They are set in advance and remain constant throughout the training process.
Hyperparameters are not affected by the learning algorithm, but they do affect the speed of the training process(Géron,2017; Raschka & Mirjalili,2017).
There is no formula for calculating the optimal value nor a unique rule for tuning the parameters used to estimate a given model.
The optimal configuration depends on the data set, and the best way to build a model is to test different sets of hyperparameter values using resampling techniques.Section4describes the strategy we used to optimise the XGBoost hyperparameters.

SECTION: 3.3Performance Assessment

The performance of the ML models built with the data sets described insection2.2was assessed by quantifying the extent to which the predictions were close to the true values of the response for the set of observations.
Given the regression nature of the problem in this project, we used six metrics to assess the predictive quality and the goodness of fit of the learners:(a)theroot mean squared error(RMSE) and themean absolute error(MAE);(b)an interval-based error or residual,;(c)an interval-based “accuracy”,, calculated using the interval-based error;(d)the mean of the absolute values of the residuals,; and(e)the adjusted coefficient of determination,.Furthermore, the error bars associated with the predictions were estimated in accordance with the specifications outlined insection3.3.1.
The RMSE, the MAE, andwere used to measure the statistical dispersion, whileandwere used to assess the predictive performance of the models.
The mean of the residuals,, was used simultaneously as a measure of the dispersion and of the quality of the models, in the sense that we could infer from it how wrong the models were on average.

During the learning phase, a model’s performance was estimated using the-fold cross-validation (CV), where a subset of observations was used to fit the model, and the remaining instances were used to assess its quality.
This process was repeated several times, and the results were aggregated and summarised.

After the training phase, predictions were made on each of the testing sets, and the resulting rotation periods were compared with the reference values contained in the S19 catalogue.
The mean of the relative absolute values of the residuals was used as a measure of goodness of fit, giving us an idea of how wrong the models were on average.
The RMSE and the MAE were used together to measure the differences between the predicted and reference values of stellar rotation periods.
They were particularly useful in assessing the performance of the models on the training and testing sets, and thus to better tuning them in order to avoid overfitting—large differences in the RMSE and/or MAE between the training and testing sets are usually an indication of overfitting.
The interval-based accuracy allowed us to convert the assessment of a regression problem into the evaluation of a classification result.
This in turn allowed us to compare the predictive performance of our models with those trained byBreton et al. (2021).
Since the latter assessed their results within ainterval of the reference values, we used the-accuracy, acc0.1, as a benchmark.
When calculating the interval accuracies, it should be noted that the width of the intervals centred on the reference values increases with the rotation period, while the width of the intervals around the corresponding rotation frequencies does not vary significantly with the frequency, except in the low-frequency range.
Since the distribution of the amplitudes of the frequency intervals is more uniform than that of the amplitudes of the period intervals, the rotation periods were first converted to rotation frequencies in order to reduce the effect of the increasing width of the accuracy intervals with the period, which would affect the comparisons and the assessment of the quality of the models.

The following is a brief description of each of the above metrics and the-fold CV technique.

The most commonly used metric to assess the performance of a model in a regression setting is themean squared error(MSE).
This is defined as

whereis the prediction for theobservation given by, andis the number of cases present in the data set.
The MSE is a measure of the distance between a point estimatorand the real value or ground truth.
It is usually interpreted as how far from zero theerrorsorresidualsare on average,i.e., its value represents the average distance between the model predictions and the true values of the response(Kuhn et al.,2013).
In general, a smaller MSE indicates a better estimator(Ramachandran & Tsokos,2020): the MSE will be small when the predictions and the responses are slightly different, and will typically be large when they are not close for some cases.
The MSE can be computed on both the training and testing data, but we are mostly interested in measuring the performance of the models on unseen data (the testing sets)—the model with the smallest testing MSE has the best performance.
If a learner produces a small training MSE but a large testing MSE, this is an indication that it is overfitting the data, and therefore a less flexible model would produce a smaller testing MSE(James et al.,2013).
A common measure of the differences between estimates and actual values is theroot mean squared error(RMSE), which is no more than the square root of the MSE:

The RMSE has the advantage over the MSE that it has the same units as the estimator.
For an unbiased estimator, it is equal to the standard deviation.
Similarly to the MSE, smaller values of the RMSE generally indicate a better estimator, but as this metric is dependent on the scale of the variables used, comparisons are only valid between models created with the same data set(Hyndman & Koehler,2006).

Themean absolute error(MAE) of an estimatoris the average of the absolute values of the errors:

where all of the quantities have the same meaning as before.
The MAE is a measure of the error between the prediction and the ground truth.
It uses the same scale as the estimates and the true values.
Therefore, it cannot be used to make comparisons between models built on different data sets.
The MAE has the advantage over the RMSE that it is affected by the error in direct proportion to its absolute value(Pontius et al.,2008).
When used together, the RMSE and MAE can diagnose variations in the residuals in a set of predictions.
For each predictive model and sample under consideration,

The variance of the individual errors increases with the difference between the RMSE and the MAE, and all the errors are of the same size when both metrics are equal to each other.

Themean absolute value of the relative error(MARE) is given by

whereis the number of observations,and.
It is similar to the MAE, but the mean absolute value is calculated on the relative residuals rather than the magnitude of the errors.
The MARE is a measure of the model’s mean relative error, giving a percentage estimate of how wrong the model is on average.

We developed an interval-based error function and an “accuracy” metric to bridge the gap between regression and classification settings.
Theinterval-based error,, estimates the prediction error in a regression task when we are willing to accept an error of, and it is given by:

where,andare theground truth and predicted values, respectively,is the fractional width of thezeroing interval, andis the fraction ofthat defines a transition zone, where the error varies linearly between zero and the typical residual.
Whenis zero, the metric returns the simple residuals.
The second and third branches ofeq.14correspond to the transition zone.
We used a linear function to join the residuals to the error interval, but any other function could be used, such as a logistic function or a hyperbolic tangent, just to name a few.Figure8shows the interval-based error function calculated on simulated data, when an error ofis considered acceptable,i.e., when.
The transition zone is defined for, corresponding toof.
The predictions were obtained from a uniform distribution ofvalues varying between 0 and 2 around a fixed ground truth value of 10.
Similar toeq.13, this error function, when normalised to the reference values, can be used to approximate how much a model is wrong on average for a givenerror-intervalwidth.

Aninterval-based“accuracy”,, can be obtained from the interval-based residuals ofeq.14:

where,, andare as before, andis the number of observations.666Note that whiledepends on, accΔdoes not.The indicator function is equal to 1 when the interval-based regression error is zero,i.e., whenever, and zero otherwise;
that is, we consider aneventevery time the error ineq.14is equal to zero, and anon-eventotherwise.

We used theadjusted coefficient of determination,, as a fair measure of a model’s goodness of fit, to characterise its predictive ability:

whereis the coefficient of determination,is the number of observations, andis the number of predictor variables(Baron,2019).

We used the-fold cross-validationas a resampling technique during the training phase of the models, in which the training instances were randomly partitioned intonon-overlapping sets or folds of approximately equal size.
One of the folds was kept as the validation set, and the remaining folds were combined into a training set to which a model was fitted.
After assessing the performance of the model on the validation fold, the latter was returned to the training set, and the process was repeated, with the second fold as the new validation set, and so on and so forth(Kuhn et al.,2013; Hastie et al.,2009).
Theperformance estimates were summarised with the mean and the standard error, and they were used to tune the model parameters.
The testing error was estimated by averaging out theresulting MSE estimates.
The bias of the technique,i.e., the difference between the predictions and the true values in the validation set, decreases with.
Typical choices forare 5 and 10, but there is no canonical rule.Figure9illustrates an example of a CV process with.

In order to estimate the errors associated with the predictions made on the testing set, we employed a straightforward methodology, whereby the predictions were binned into uniform unit intervals centred on integer values spanning between the minimum and the maximum.
The standard deviation within each bin was then taken as the error bar.

SECTION: 4Experimental Design

We applied XGBoost to build regressor models, each of which was trained using the hold-out method, where the data sets described insection2.2were split into training and testing subsets, in an 80-to-20 percent ratio.
This proportion was reflected in the following distribution: in the case of DS0, DS1, DS2, and DS7, the training and testing sets comprisedandinstances, respectively;
DS3 and DS5 consisted ofobservations in the training set andelements in the testing set;
finally, for DS4 and DS6,instances were for the training set andfor the testing one.
We converted all training and testing sets toXGB dense matricesfor efficiency and speed, as recommended by the authors of thexgboostpackage(Chen & Guestrin,2016).
During the training phase, we used a 10-fold CV with five repetitions, where the best values for the hyperparameters were sought using a search grid on the training set.
In all cases, the testing sets remained unseen by the models until the prediction and model evaluation phase.
That is, the instances belonging to the testing sets were at no time part of the model learning process, behaving as new, unknown observations, as it would happen in a real-life scenario.

The experiment was carried out in two iterations.
In the first iteration, we searched for the best values of the hyperparameters using a two-step grid search:
firstly, the best parameters were searched for within a set of possible values;
the search was then refined on the learning rate and the subsample ratio of training instances, by varying the parameter in a finer grid centred on the best value found in the previous step.
Typically, no more than 1500 iterations were run for each submodel, and we activated a stopping criterion, where the learning for a given model would stop if the performance did not improve for five rounds.
The grid values for each parameter were as follows:

colsample_bytree: the fraction of columns to be randomly subsampled when constructing each tree; it was searched within the set.777Two other hyperparameters belonging to this family of subsampling columns were initially optimised:colsample_bylevelandcolsample_bynode.
However, no improvement in model performance was observed when they were included, and as they work cumulatively, onlycolsample_bytreewas retained in the optimisation process in the end.

eta,: thelearning rate, was first searched within the values; the search was then refined by looking for five values centred on the bestfound in the first iteration, eachapart.

gamma,: the minimum loss reduction required to further partition a leaf node of the tree; it was chosen within the set of values.

max_depth: the maximum depth per tree; during the first iteration, the best value was chosen from the set of integers; in the subsequent iteration, the optimal value was selected from the set.

min_child_weight: the minimum sum of instance weights required in a child node, which in a regression task is simply the minimum number of instances needed in each node; this parameter was searched for within the grid of integers.

subsample: the subsample ratio of the training instances; we opted for both stochastic and regular boosting, and so during the first grid search,subsamplewas chosen within eleven possible values, betweenand, varying in steps of; the best value of the parameter found in the first grid search was then refined by searching for the optimal of five values centred on this best value or, if the latter was 1, by searching for the best of three values less than 1, eachapart in all cases.

We tried other XGB hyperparameters, but we found that these had the greatest impact on the performance of the model.
The grid produced a maximum of+ 25 submodels during training, not counting the 10-fold CV with five repetitions.

In the second iteration, we refined the grid search, by using three values per hyperparameter, centred on the optimum for each parameter found in the first iteration.
The grid is given intable2.
The rationale for iteration 2 was to search for two additional values for each hyperparameter around the optimal value found in iteration 1.
In the case ofetaandsubsample, care was taken not to exceed the allowed limits for these hyperparameters.
The training produced a maximum of 729 + 20 submodels, not counting the CV.

After being trained by performing a grid search on the selected hyperparameters, the models were evaluated, and the importance of each feature was assessed in terms of the predictive power of the model, using the variance of the responses.
The following sections present results and analysis for all models obtained during both iterations.

SECTION: 5Results

Intable3, we present the results of the parameter optimisation carried out in iterations 1 and 2.
As far as the hyperparameters are concerned, we highlight the following points by comparing the results oftable3with the original grid ofsection4:

colsample_bytree

In iteration 1, the optimal value wasor less for all the  models except DS0; thus, in the data sets comprising less predictors, fewer variables were randomly subsampled ;

In iteration 2, the tendency to reduce the number of randomly sampled variables with smaller sets remained.

eta

In the first iteration,remained below 0.05, with 0.02 being the most common value; so, smaller values were prioritised; this came at the cost of longer convergence times and the risk of overfitting, so we relied ongammafor a more conservative approach;

In iteration 2, the most common learning rate value was 0.02, and only in one situation was the best value 0.03.

gamma

In iteration 1,was equal to 5 or 10 in three models, imposing the most conservative constraint of the options offered — this happened for the lowest (0.02) optimal value of the learning rate; nevertheless, this lowestvalue also correlated with the lowvalue of 1 in four models; this suggests, as expected, that the optimal values for these hyperparameters are also dependent on the data set in question and the values of the other hyperparameters;

In the second iteration,was observed to vary between 0 and 5, with 3 being the most common value; in DS5, no constraints were applied to the model, and in no case was the most conservative value of 10 imposed.

max_depth

In iteration 1, it was always equal to 7, which is the maximum value offered by the grid;

Iteration 2 offered cases with higher values (8 and 9) and, except for the DS1, DS2, and DS4 models, 9 was the best value.

min_child_weight

In the first iteration, the most common value was 20 and the minimum value was 10; therefore, highly regularised models were generated, with smaller trees than when this hyperparameter was equal to 1 (the default value) — this limited a possible perfect fit for some observations, but made the models less prone to overfitting and compensated for smaller values ofand;

In the second iteration, DS1, DS2, and DS3 were the models with the highest degree of regularisation; the most common values were 15 and 20, and with them the trend towards models with smaller trees.

subsample

It was equal or close to 1.0 (never below 0.90) in both iterations, and so regular boosting was prioritised, despite the range of values suitable for stochastic boosting available in the grid.

The models were built using the optimal sets of the hyperparameters found by the grid search, and the algorithm was stopped afternroundsiterations, as given in the last row oftable3.
We had set the maximum number of possible iterations during the learning phase to 1500. 

In the case of DS2 in iteration 2, the stopping criterion was not activated, so we extended the maximum rounds to 2000 and found an optimal hyperparameter fornroundsequal to 1516.

The performance of the models obtained by applying XGB to the aforementioned data sets for predicting stellar rotation periods is summarised intable4.
The number of predictors used to train the models is given under each data set in the header of the table.
The quantities used to assess the quality of the models were described insection3.3.

Overall, the XGBoost models were wrong, on average, approximately between 4.1 andof the time in iteration 1, and between 4.1 andof the time in iteration 2, as indicated by.
DS4 was the least likely to make an error, and DS5 had the best adjusted R-squared.

Excluding DS0, the-accuracy varied between 85.4 andin iteration 1, and between 86.1 andin iteration 2, with an increase of about 2 to 4 points when stars with rotation periods shorter thanand greater thanwere removed and the number of predictors was reduced to the 28 most important.
In the case of DS7, there is also an increase compared to DS1 and DS2. However, it is less significant than in the other cases.
Therefore, the-accuracy seems to increase with smaller data sets and especially with an improvement in data quality.
The evolution of the interval-based accuracy with the error tolerance for the models obtained during iteration 2 is illustrated infig.10.

Overall, DS0 stands out from the other models, and the interval-based accuracy increases with the error tolerance,i.e., when we are willing to accept larger errors.
We find that(i)upon the exclusion of DS0, it is possible to identify two distinct groups of models: the first, composed of DS1, DS2, and DS7, with lower accuracies; and the second, containing the remaining models, with higher interval-based accuracies;(ii)particularly within the lowest error tolerance regime, there is a discernible enhancement in the accuracies of DS2 over DS1 and DS7 over DS2,i.e., when we run an initial selection of variables, as described insection2.2, and reduce the set of predictors to the most relevant ones;(iii)overall, DS1 is the model with the worst accuracy;(iv)in the highest tolerance regime, DS4 reaches values that are comparable to those of DS0;(v)DS7 consistently outperforms DS1 and DS2;(vi)there is a noticeable increase in performance when potentially problematic targets,i.e., stars with rotation periods aboveand below, are removed, especially for lower error tolerances; and(vii)the performance increases further when only the 28 most important predictors are used to train the models, with the interval-based accuracy always remaining above.

The adjusted-R2measured on the testing set was always equal to or greater than.
The model with the best goodness of fit measured by this metric was DS3 with, followed by DS5 with.
These values seem to indicate a slightly greater ability of the predictors to explain the variability of the response when stars with short rotation periods (less than) are kept in the data set used to train the model.
When compared with the corresponding unadjusted coefficients of determination, thesevalues were all of the same magnitude within each model, with differences typically to the third decimal place.

Figure11shows the RMSE and MAE differences between the testing and training sets—(blue thick line) and(red thin line), respectively—and the difference between the two,(dotdashed grey line), for all the models generated in iteration 2.
It can be observed that both differences,and, reach a minimum value with DS0;
following an initial increase in DS1 and DS2, a tendency towards a decrease can be identified as the data sets get smaller and the quality of the data increases.
However, even if the downward trend continues, these differences reach values greater than the previous models in the case of DS2 and DS6.888There is a marginal increase inin DS4 compared to DS3.The general downward trend in the solid lines indicates that, with the exception of DS6, the degree of overfitting is less pronounced with the number of the model,i.e., with data sets comprising a smaller number but more relevant predictors and higher data quality.
In iteration 2,varied betweenfor DS0 andfor DS2. The latter was, therefore, the model with the highest degree of overfitting.
After DS0,  DS5 was the model with the least overfitting.

A similar behaviour was  observed in the case of, where it reached the minimum value offor DS0 and a maximum offor DS2.
The differencesuggests that, with the exception of DS2 and DS6, the variance of individual errors generally decreased with smaller data sets.
This greater overfitting did not prevent the models from performing slightly better in iteration 2 than in iteration 1.
We can also see that the overfitting is less pronounced in DS7 than in DS2 and is similar to DS1.
The evolution of these differences is in line with the behaviour of the adjusted R2, and 
 seems to indicate that the removal of rotation periods shorter than seven days and larger than 45 days has a positive impact on the performance of the models.

The mean values of the standard deviations for each model,, and the ratio between these uncertainties and the mean values of the predictions carried out on the testing set,, estimated in uniform bins of unit length centred on integer values, are indicated intable5.
For each prediction, the associated error bar is taken to be equal to the standard deviation in the corresponding bin.
The values are indicative of the variability of the predictions.
The mean length of the error bars oscillates between 0.28 and, while the mean relative uncertainty is approximately equal to, ranging between 0.0115 and 0.0122.

The scatter plots of the ground truth against the predicted values for all the models built during iteration 2 are shown infig.12and, with additional marginal density plots, infig.21.
The blue dashed lines indicate the identity function, and the red solid lines represent the linear models between the predicted and actual values.
In order to enhance the clarity of the plot, the error bars associated with the predictions have been omitted.
Most of the points fall on or near theline, but some outliers can be seen, representing both under- and over-predicted cases.
Nevertheless, all graphs show a high degree of positive correlation between the predicted and the reference values of the stellar rotation periods, indicating that there is generally good agreement between the predictions and the ground truth.
This is confirmed by the marginal histograms and density plots for both the predicted and reference values in each panel offig.21, which are similar in terms of centrality, dispersion, and kurtosis.
The dispersion of the observations relative to the identity line is not significantly large in all the models, but it increases with the predictions,i.e., it has different values for low and high values of the response—an indication of heteroscedasticity.
The R-squared reported in the panels correspond to the coefficient of determination between the predicted and the reference values.
As expected, they are not significantly different from the Rreported intable4, given the large total number of observations,.
The F-statistics shown in the graphs result from testing the null hypothesis that all of the regression coefficients are equal to zero.
Overall, the F-tests and the corresponding-values considered in the panels indicate that the sets of independent variables are jointly significant.

Models DS0, DS1, DS2, and DS7 have similar performance for rotation periods shorter than about.
If we consider rotation periods longer than, the dispersion is lowest for DS7.

Infig.13, we have plotted the ratio between the predicted and the reference values (top panel), and the ratio between the ground truth and the predictions (bottom panel), both against the reference values.
The former highlights the over-predictions, while the latter emphasises the under-predictions.
The highest over- and under-prediction ratios are equal to 44.4 and 30.2, respectively.
There are nine under-prediction ratios equal to infinity, corresponding to zero values predicted by the model (we made negative predictions equal to zero).
The largest ratios occur only for predictions close to zero,i.e., for very small rotation periods, typically below, corresponding to fast rotators.
It can be seen that above thethreshold, both the overestimations and the underestimations are at most about four times the reference values.

The Tukey-Anscombe (TA) plots of the raw residuals and of the-error are shown infig.14.
For each model, the-axis in the top panel corresponds to the residuals calculated on the rotation periods (in days), and in the bottom panel to the interval-based error calculated on the rotation frequencies (in).
The-coordinate always represents the predictions in days.
In all the plots, the points fluctuate randomly around the horizontal line passing through zero, forming a cloud that is approximately symmetrical around it.
However, this cloud is not rectangular in shape.

A detail of the TA plot for DS0, where we have constrained the-axis to vary betweenand 2, is shown infig.16.
We have added a regression line to the graph, which does not show a meaningful slope.
The variance of the residuals increases with the predictions, indicating some degree of heteroscedasticity.
All models share this characteristic of the TA plot.

The most important predictors for the models obtained in iteration 2 are highlighted in the bar plots offig.17.
Apart from DS0, the variables belonging to the CS and GWPS families typically contributed the most to the models.
In the case of DS0 (the only data set where they were present), the rotation periods were naturally the dominant variables.
Having obtained DS1, DS2, DS3 and DS4, we calculated the union of the 20 most important variables for these models.
We were left with a set of 28 predictors, which we used to create the DS5, DS6, and DS7 data sets and their respective models.
These variables were the following:

cs_chiq_55

cs_gauss_1_1_20

cs_gauss_1_2_20

cs_gauss_2_2_20

cs_gauss_2_2_55

cs_gauss_3_1_20

cs_gauss_3_1_55

cs_gauss_3_1_80

cs_gauss_3_2_55

f_07

gwps_gauss_1_1_20

gwps_gauss_1_2_20

gwps_gauss_1_4_55

gwps_gauss_2_2_20

gwps_gauss_2_2_55

gwps_gauss_2_2_80

gwps_gauss_3_1_20

gwps_gauss_3_1_55

gwps_gauss_3_1_80

gwps_gauss_3_2_20

h_acf_55

h_cs_20

length

sph

sph_acf_err_20

sph_cs_err_20

sph_gwps_20

teff

The predictors that contributed most to the models are shown on the-axis offig.17.
For models DS0 to DS4, we show the 30 most relevant variables for training;
for the rest of the models, the plots show the 28 most important predictors mentioned above.
We emphasise that Astro variables, such as the mass and the effective temperature, although they had some relevance for DS0, DS1, and DS2, they contribute little to the DS3 and DS4 models.
TS-type variables, such as the photometric activity proxy, have some relevance and are important to support the model—we observe a drop in model performance when they are removed.

SECTION: 6Discussion

The main findings from this work are highlighted infigs.10,12and17.
On the basis of the latter, if DS0 is excluded, the results show that the three most important variables for predicting rotation periods of K and M stars from theKeplercatalogue arguably belong to the Composite Spectrum and the Global Wavelet Power Spectrum groups, and specifically arecs_gauss_3_1_20,cs_gauss_3_1_55, andgwps_gauss_3_1_55.
After removing features containing rotation period values from the data set, these variables were always in the top 10 in terms of importance and stood out from the rest of the features.
They correspond to the standard deviation of the first Gaussian fitted to the CS and GWPS for the 20- and 55-day filters, respectively.
Not only these three predictors, but most of the CS and GWPS features with a non-negligible positive or negative correlation with the response are relevant to the predictive performance of the models.

In addition, other variables, such asgwps_gauss_2_2_20,cs_gauss_2_2_20,gwps_gauss_2_2_55,cs_gauss_2_2_55,f_07,sph,h_cs_20,teff, andlengthhave some relevance to the training of the models.
In the specific case of the central periods of the second Gaussian function fitted in the GWPS and CS, although they are not the most relevant variables when training the models, the degree of importance assigned by the XGBoost algorithm to them can be attributed to the fact that the ground truth values were extracted from the central periods of the first fitted Gaussian functions, in conjunction with the algorithm’s inherent ability to identify harmonic sequences.
The 20 and 55-day filters are more relevant than the 80-day ones, given that K and M stars typically exhibit relatively slow rotation periods and the filter at 80-day may potentially be susceptible to instrumental issues.

For the most important predictors, we used tree-based approaches to build our models, which by their nature do not suffer from the inclusion of highly correlated features.
However, interpretability tools, such as importance estimations like the plots offig.17, are hampered by collinearity and multicollinearity.
If two features are perfectly correlated, such ascs_gauss_3_1_20andcs_gauss_1_1_20, they are likely to be selected by the algorithm.
In boosting, a link between a predictor and the response will remain stable once it has been learned by the model, and so the algorithm will stick to one of the correlated variables (but not both).
This can be seen for the pair of correlated variables above and others, such ascs_gauss_1_1_20andcs_gauss_1_1_55orsph_cs_err_80, orgwps_gauss_2_2_55andgwps_gauss_3_3_20(just to name a few).
We end up realising that one of the variables plays an important role in generating predictions, but we do not realise that the other variable is also important in the link between the observations and the response unless we perform a correlation analysis of the features(Chen et al.,2018), as we did insection2.3.
In this sense, any of the features identified as most important, in particular the CS and GWPS variables based on the standard deviation of the fitted Gaussian, could in principle be replaced by any other predictor highly correlated with them without significant loss of model performance.
In addition, as can be seen in the panels offig.17, the importance profile for gradient boosting typically has a steep slope because the trees from boosting are interdependent and thus present correlated structures as the gradient evolves.
As a result, several predictors are selected across the trees, increasing their importance.

We believe that the level of importance attained by the CS and GWPS families of variables is related to the fact that most of the reference stellar rotation periods have been estimated from them.
As mentioned insection2.3, there is a strong correlation between some features of these families and the response.
One way to test this hypothesis would be to train new models from structured data obtained from synthetic light curves, produced by reliable stellar simulators, such as the StarSim code(Rosich et al.,2020)or the PLATO Simulator(Marcos-Arenal et al.,2014).999PLATO stands forPLAnetaryTransits andOscillations of the stars.This might require the engineering of new variables, but in principle we would be able to control for the correlations between the predictors and the response.

The results indicate that the CS features are slightly more important for the model than those of the GWPS.
This is probably due to the fact that the CS is the product of the normalised GWPS and the ACF, thus amplifying peaks present in both and attenuating signals possibly due to instrumental effects, that manifest themselves differently in the GWPS and the ACF.

The most important TS variables are the activity proxies.
They are relevant in the sense that they are often in the top 10 of the most important variables.
It then becomes apparent that it is important to extract all possible activity proxies (photometric, magnetic, and others) directly from the light curves and to use them as predictors when building a model.

The weakest features for predicting stellar rotation periods are the astrophysical ones.
They are rarely included in the top 10 most important variables.
The exceptions are the mass and the effective temperature, which sometimes make it into the top 30 and were somewhat relevant to train DS0, despite the use of rotation periods as explanatory variables.
This is consistent with the fact that it is not possible to predict stellar rotation periods from astrophysical variables alone.
However, the FliPer metrics are strongly correlated with some of the CS and GWPS predictors, as shown insection2.3.
Therefore, it is expected that the Flicker in Power metric, which is a proxy for the total power spectral density of a star, can play an important role in a regression ML model based on gradient boosting.
This means that the FliPer metrics could in principle replace any of the CS or GWPS features with which they are strongly correlated without significant loss of performance of the model.
The parameter space may also play a role in the small contribution of the Astro predictors.
We cannot exclude the possibility that examining a wider parameter space might change the picture, although we usually find a correlation in a narrower parameter space and not in a wider one.

In selecting the most important predictors common to all data sets from DS1 to DS4, it is possible that some variables relevant to DS5 and DS6 may have been overlooked.
An example is the mass, which was not selected for the final set of independent variables because it was not within the top 20 most important predictors for DS3 and DS4.
Therefore, perhaps a more prudent approach, analogous to the methodology employed to generate DS2, should be used when identifying the most relevant predictors.

Infig.10, DS0 naturally stands out from the other models because it includes rotation periods in its explanatory variables.
This fact is also responsible for the considerable discrepancy in the number of iterations required to achieve the optimal model observed between the training of DS0 and that of the remaining models.
Reducing the number of predictors favours the XGB models, especially for lower error tolerance regimes, as can be seen from the fact that DS7 performs better overall than  DS1 and DS2, as well as the enhanced performance in terms of interval-based accuracy of DS3, DS4, DS5, and DS6 in comparison to all other models.101010We ran several tests, where we removed the features labelled “prot_”, but kept the central periods of the first Gaussian functions fitted to the CS and GWPS;
we found that, in these cases, DS7 outperformed DS0.
In fact, when the rotation period information was retained in the predictors, each of DS5, DS6, and DS7 performed better than DS0.In particular, DS7 is on average less wrong, and has comparable accuracy,  testing RMSE and MAE, and  adjusted R2to the other models.
If anything, it has the same tendency to overfit, and has less variance than  DS1 and DS2 for slower rotators.
However, the selection of predictors must be made with great care to prevent eliminating variables that are important for training the model.
Our approach of selecting the 20 most important variables common to the initial four models might have been less optimal, given the lack of significant performance improvement observed in DS6 relative to DS4.

The differences between the training and testing RMSE and MAE, although natural, indicate a degree of overfitting during the training process of the models built from the DS0 to DS7 data sets.
The differences between RMSE and MAE vary between the models generated with the different data sets, indicating dissimilar levels of variation of the individual errors.
However, these differences are not significant, even in the case of DS2, where they reached the highest value — this seems to be related to the initial selection of predictors.

Prior to rotation period filtering, the models struggled to predict some of the very short rotation periods, typically less than a few days, and periods greater than.
These discrepancies are evident in the scatter plots of the models trained on the DS0 to DS6 data sets, and are further highlighted infig.13.
The over- and under-predictions are larger for very short rotation periods, below seven days.
Therefore, cases corresponding to K and M stars with rotation periods belowor aboveare not suitable for training a predictive model usingKeplerdata.
This is mainly due to the fact that(a)Kepler’s quarters are 90 days long,(b)at least two full cycles are required to obtain reliable observations,(c)the process of stitching two or moreKeplertime series is not trivial and error-free, and(d)signals from stars with rotation periods of less than seven days can be mimicked by fluxes from close binaries, classical pulsators, and other sources of confusion.After the filtering of the rotation periods, the predictive power of the XGB models improved, as shown by the analysis of the models learned from DS3 to DS6.
By restricting the predictors to the set of the 28 most important variables identified by the XGB models built with DS0 to DS4, and by filtering in stars with rotation periods between 7 and, we were able to train computationally cheap but solid models, with good predictive performance.

While the final models are characterised by a goodness of fit around, there are still a few outliers with predictions varying from approximately from 50 toof the actual values.
In principle, increasing the number of training cases, controlling the errors in the measurements, and, most importantly, having a uniform distribution of training rotation periods (only possible with simulated data) would help to circumvent this problem and to improve the predictive power of the models.
In addition, we could perform feature engineering on the original light curves to try to get a better set of predictors.

Finally, our results are in line with those ofBreton et al. (2021).
Taking into account the rotation periods in the explanatory variables, if we focus on theinterval-accuracy metric, the classifier created with B21 achieved comparable results.
However, we have to take into account the fact that our accuracy metric is an approximation to the counterpart used to assess the quality of a classification model.
In addition, we did not visually check the results, nor did we change them after testing the model.
An important aspect to highlight is the fact that we used thehold-out method,i.e., we used an unseen-before testing set to assess the quality of the model.
This testing set was not part of the training of the model, nor dit it participate in the CV performed to optimise the hyperparameters of the models.
When assessing the quality of a model on a testing set, the predictive performance is typically more pessimistic than that obtained during the training with CV(Hastie et al.,2009; Torgo,2011).
The accuracies claimed byBreton et al. (2021)appear to be obtained with a process that is similar to a 2-fold CV with 100 repetitions, performed on the whole data set.
Therefore, even considering that their method is protected by the fact that they use the values of the target variable as predictors, we would expect the performance of their classifier to decrease slightly if the quality of the model were assessed on an unseen-before testing set.

SECTION: 7Conclusions

In this paper we presented a novel method, based on a machine learning approach, to calculate stellar rotation periods of thousands of stars.
We employed a regression analysis that makes use of tabular data extracted from light curves and that is based on the XGBoost algorithm.
Recently, attempts have been made to apply ANN to light curves and classification RFs to tabular data to make such predictions.
On the one hand, the former typically requires high computational resources;
on the other hand, with respect to the latter, we argue that(a)a classifier may not be the best approach to predict the rotation period, because it is a continuous variable, and(b)using rotation periods as predictors may weaken the predictive performance of the model in the presence of unseen testing data, which do not contain rotation period values and have not been used to train the model.

Our main objective was to build robust and efficient ML models for automatically predicting stellar rotation periods of K and M stars from theKeplercatalogue.
To this end, we applied the regression XGBoost algorithm to train models from seven data sets built from the data originally published bySantos et al. (2019)andBreton et al. (2021).

Given thatBreton et al. (2021)built a classifier with their data set (B21), we developed an interval-based error and a metric, the interval-based accuracy, that allowed us to convert predicted rotation periods into a proportion of successful events.
We used this metric as a way to directly compare the results obtained with the two approaches (regressionvs. classification).

Initially, we used all the stars available in the base training set,i.e., no stars were filtered out of the data set for model training.
Overall, the results show that the predictive performance of the models trained on DS0, a data set equivalent to B21, is comparable to the performance of the classifier trained on the latter, if we keep rotation periods as independent variables and are willing to accept aerror in the predictions.
If we remove the rotation periods from the predictors, we get a-accuracy of about.
The goodness of fit, as measured by the mean absolute value of the relative residuals,, and by the adjusted coefficient of determination,, indicates that our models are robust, able to explain most of the variability in the response (approximately), and are on average wrong no more thanof the time.

When we filter out stars with rotation periods belowand above, the overall performance of the models increases, with aboutof the variability of the response explained by the predictors, and theinterval-accuracy equal to about.
In this case, the models were on average wrong aboutof the time.

The mean relative uncertainty, as indicated by the extent of the error bars, was estimated to be approximatelyfor all the models.
There is also an increase in the quality of the XGB models when the number of predictors is reduced through the selection of the most important variables that contribute to the learning process.

In view of the results reported insection5, we conclude that the variables belonging to the CS and GWPS families are the most important for training a reliable XGBoost model with good predictive performance.
TS-type features,i.e., those related to the structure of the light curve, such as, also have some relevance for estimating stellar rotation periods.
Although Astro-type variables, corresponding to astronomical observables and derived variables, do not have a significant weight in the importance plots, the FliPer metric and hence the power spectral density of a star can play a relevant role in model training, as they are strongly correlated with some of the variables in the CS and GWPS families.

By selecting the most important features, as measured during the training of the models obtained with the data sets DS0 to DS4, and excluding stars with rotation periods belowand above, due to the characteristics of theKeplerspace observatory and astrophysical constraints, we were able to build an optimal subset of predictors from the set of available explanatory variables, from which robust regression ML models, with good predictive performance, could be trained.
Our results clearly show that XGBoost models trained on these reduced size data sets have comparable or better performance than the models we previously obtained with the data sets DS1 to DS4.
We claim a reduction of about 140 predictors when compared to the largest data set (DS0).
The most important predictors, as measured by thexgboostRpackage, are the standard deviations of the first Gaussian functions fitted to the CS and GWPS, followed by the photometric index proxy,.
By using a data set consisting mainly of these variables, it is possible to train XGB models with a performance comparable to or better than those built on much larger data sets.
By reducing the size of the sets by this order of magnitude, we are able to significantly improve the training time.
With the resources we had available,i.e., shared machines running multiple processes simultaneously, we were able to reduce the time to build a model by several hours.

In conclusion, the enhancement of data quality plays a pivotal role in the generation of accurate predictions using an XGBoost model.
The reduction of the number of predictor variables while retaining the most important ones for model training represents a significant key contribution.

The current limitations of our methodology are primarily due to the lack of training of the models to deal with false positives.
Given the numerous sources of confusion, including binaries, classical pulsators, red giants signals, and even misclassified stars, we intend to apply this methodology to simulated data.
This will enable us to consider all the known factors that could result in false detections and to train the models to address them.

SECTION: References

SECTION: Appendix APlots

In this appendix, we present a number of additional graphs to complement the information presented in the main body of the paper.
All figures were previously referenced, except those relating to elapsed times.

Figure19shows the correlations between predictors belonging to the Astro and CS families of variables, and the response.
Variables that are highly correlated with each other, such assph_cs_20andf_07, can in principle be swapped in the model without loss of performance.
The situation is similar for the predictors belonging to the GWPS family of variables, whose correlations are shown infig.20.
Only negative or weak correlations with the response are shown in these plots.
However, features that are strongly correlated with the target variable would be highlighted in full plots of the CS and GWPS predictors and the target variable.

Figure21shows the scatter plots of the ground truthvs. the predictions for the models generated during iteration 2, including the marginal density plots of the predictions and the reference rotation periods.
Similar tofig.12, the blue dashed line indicates the identity function.
Each panel reports the goodness of fit and significance of the relationship, using, the F-test and the corresponding-value, respectively.
Overall, the sets of predictors are jointly significant.

The total learning time per model in iterations 1 and 2 is shown infig.22.
In order to optimise the generation of the models, the hyperparameter grids were divided into several sub-grids (12 and nine in iteration 1 and 2, respectively), which were started in parallel.
In this way, the longest models, with more thanof learning time, took less thanto complete, as shown infig.23.
Using this approach, the fastest models in iteration 2 took less than two hours to train.

Column 2