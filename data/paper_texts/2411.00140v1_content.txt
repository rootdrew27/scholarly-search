SECTION: ViT-LCA: A Neuromorphic Approach for Vision Transformers
The recent success of Vision Transformers has generated significant interest in attention mechanisms and transformer architectures. Although existing methods have proposed spiking self-attention mechanisms compatible with spiking neural networks, they often face challenges in effective deployment on current neuromorphic platforms. This paper introduces a novel model that combines vision transformers with the Locally Competitive Algorithm (LCA) to facilitate efficient neuromorphic deployment. Our experiments show that ViT-LCA achieves higher accuracy on ImageNet-1K dataset while consuming significantly less energy than other spiking vision transformer counterparts. Furthermore, ViT-LCA’s neuromorphic-friendly design allows for more direct mapping onto current neuromorphic architectures.

SECTION: 
Neuromorphic computing represents a paradigm shift in computing, characterized by its low-power processing capabilities and brain-inspired architectures. This approach emulates biological neural networks through the use of Spiking Neural Networks (SNNs). One of the primary advantages of neuromorphic chips lies in their capacity for highly parallel and energy-efficient computations. By performing operations asynchronously and maintaining proximity between synapses and weight calculations, these systems significantly reduce data movement, thereby enhancing overall computational efficiency. These platforms integrate many-core systems capable of instantiating large populations of spiking neurons, enabling information processing that mimics the dynamics of biological neural systems. Additionally, by utilizing crossbar arrays and memristoresto store multi-bit quantities as conductance values, neuromorphic computing is particularly well-suited for efficiently evaluating matrix-vector-multiplications, which are fundamental to deep learning algorithms.

A particularly interesting model in neuromorphic computing is the Locally Competitive Algorithm (LCA), which is a computational model and learning algorithm that iteratively updates neuron activity to achieve a sparse representation of input data.
This computational model has been implemented on recent neuromorphic platforms.
The competitive mechanism inherent in LCA ensures that only a limited number of neurons become active at any given time, facilitating efficient coding of high-dimensional data. One proposal for leveraging the LCA in neuromorphic computing is the Exemplar LCA-Decoder. Functioning as a single-layer encoder-decoder, this computational model iteratively updates neuron activity to identify a sparse representation of the input data (i.e, encoding) and then uses these neuron activities for classification tasks (i.e, decoding).

Recently, the Transformer architectureand its variants have demonstrated impressive performance across a range of tasks, including natural language processingand computer vision. This success is largely due to their ability to effectively capture long-range dependencies, a capability primarily attributed to the self-attention mechanism.
Given the enormous computational requirements of transformer architectures, deploying these models on devices with limited resources remains a significant challenge. As a result, integrating transformer architectures with neuromorphic computing represents a promising research avenue. In particular, the combination of transformer architectures and LCA-based learning could lead to more efficient and biologically inspired artificial intelligence systems. However, this area remains largely unexplored.

This paper presents ViT-LCA, which leverages Vision Transformers (ViT)to extract self-attention representations and incorporates these representations into an LCA-based SNN. This algorithm effectively addresses the challenges of deploying transformer models on energy-constrained neuromorphic platforms. The self-attention representations are extracted once and stored in non-volatile memory elements, enabling in-memory computation on neuromorphic systems that emphasize specialized operations and energy efficiency. Our approach consists of two stages. In the first stage, a transformer encoder generates self-attention representations from the input image. In the second stage, these representations are processed by a single-layer SNN that employs a LCA encoder-decoder architecture for classification tasks. In this study, we evaluate ViT-LCA on CIFAR-10, CIFAR-100and ImageNet-1datasets and assess the effectiveness of integrating ViT’s self-attention representation with the efficiency of sparse coding through LCA for deployment on neuromorphic systems. By inputting self-attention representations (contextual embeddings) derived from ViT into a single-layer SNN model, we achieved high classification accuracy while ensuring low computational overhead and high energy efficiency.

SECTION: 
There has been a growing interest in developing methods to reduce the computational requirements of Transformer models by integrating Transformer architectures with SNNs, which serve as brain-inspired counterparts to deep neural networks (DNNs). Spikformerand the Spike-driven Transformerintroduce a spiking self-attention mechanism that utilizes spike-based representations for the Query, Key, and Value components, replacing traditional multiplication operations with low-energy addition operations. SpikingResformerproposes a novel Dual Spike Self-Attention mechanism to address the challenges faced by previous methods in effectively extracting local features. Wang et al.utilized the ANN-to-SNN conversion method and introduced the Random Spike Masking technique to improve the performance and energy efficiency of the Spiking Transformers.
Instead of implementing a spiking attention mechanism, ViT-LCA utilizes transformer encodings and maps them to current neuromorphic platforms.

SECTION: 
Figureillustrates the architecture of ViT-LCA. The training data undergoes pre-processing before being processed by the Vision Transformer (ViT), where it passes through attention layers to extract self-attention representations. These representations are stored in the synaptic weights of a single-layer spiking neural network using LCA encoding, which are then employed to classify unseen test inputs via a decoder.

SECTION: 
To extract self-attention representations from an image using the Vision Transformer (ViT), the image is first split into fixed-size patches (Tokens). Each token is then embedded into a vector representation, which is augmented with positional encodings to retain spatial information. An additional learnable “classification token” is appended to the sequence of tokens. The resulting sequence of embeddings is then input into the Transformer encoder. The extracted self-attention representations, denoted as, are subsequently used to construct a dictionaryas in Eq..

SECTION: 
We first provide an overview of the Exemplar LCA-Decoder algorithm, which utilizes the sparse coding algorithmand the LCA algorithmto represent the input signal:

whereis a dictionary of self-attention representations () andrepresents the activation of the LIF neuron. The termrepresents Gaussian noise. The membrane potentialof the LIF neuron is governed by a driving excitatory inputand an inhibition matrix (Gramian). The Gramian matrix enables stronger neurons to inhibit weaker neurons from activating, leading to a sparse representation.

Eachrepresents a self-attention representation learned from a data point in the training dataset. This approach eliminates the need for dictionary learning, differing from the original LCA, where Stochastic Gradient Descent (SGD) is used to learn and update the dictionary for each batch of input data. The thresholding function is:

Here, the thresholdrefers to the level that the membrane potential must exceed for the neuron to become active. Next, a decoder is designed to decode the sparse codesand map them todistinct classes from the training dataset. Later, the same decoder is used to generate class predictions for the unseen test data points.

Next, any (unseen) test datais mapped to the resulting-dimensional space of self-attention embeddings (dictionary atoms). The corresponding activation codesthat approximate the new input are then determined, as outlined in Eq.to Eq..

Given a sufficient number of training data points, each test data point will have a sparse coding representation, with the majority of thevalues being zero. Ideally, for any test input, the non-zero entries in the vectorwould correspond exclusively to the dictionary atomsassociated with class, whereranges from 1 to. However, modeling errors and input signal noise may introduce small non-zero entries that are associated with multiple classes. To address this issue and better harness the relevance of neuron activations for each class, the “Maximum Sum of Activations” decoder is proposed. In this approach, thenorms of the activations for each classare summed and the class with the highest value is determined:

SECTION: 
We evaluated ViT-LCA using PyTorchand the following datasets: CIFAR-10, CIFAR-100and ImageNet-1K. All datasets and the pre-trained Vision Transformer model were obtained using Torchvision.

SECTION: 
After resizing the images to a uniform size of 224 x 224 pixels, the dataset is normalized to have a zero mean and a standard deviation of one to prepare it for input into the Vision Transformer.

SECTION: 
The CIFAR-10 and CIFAR-100 datasets each comprise a total of 60,000 images, with 50,000 designated for training and 10,000 for testing. Each image is represented in RGB format and has dimensions of 32 x 32 pixels. The CIFAR-10 dataset contains 10 classes, while the CIFAR-100 dataset is more complex, featuring 100 classes.

SECTION: 
The ImageNet-1K dataset comprises approximately 1.28 million training images and 50,000 validation images. For the purpose of constructing the dictionary, the training dataset was randomly split, and a subset of 50,000 samples was selected. The ILSVRC2012 validation dataset was exclusively used for testing, ensuring that it was not utilized in any capacity prior to evaluation. This dataset contains 1000 classes.

SECTION: 
Tablepresents the workload and energy estimates, along with accuracy performance across all tested datasets. All experiments were conducted using the hyperparameters specified in Table, which were selected to achieve nearly 100% training accuracy on all datasets; the results reflect the corresponding test accuracy. Further optimization of these hyperparameters per dataset may enhance accuracy even further. Additionally, Tableindicates that the ”Maximum Sum of Activations” decoder outperforms the ”Maximum Activation” decoder across all datasets.
The energy estimates were calculated based on the floating-point operations performed during inference and the anticipated energy consumption per floating-point operation, which is discussed further in the following section.

SECTION: 
In this section, we conduct a comprehensive evaluation of the workload and energy efficiency of ViT-LCA. We assess workload efficiency by estimating the number of Floating-Point Operations (FLOPs) involved, followed by a detailed discussion of energy consumption.

Computingas described in Eq.requiresmultiplications andaddition, whereis the size of the self-attention representation andis the size of the dictionary (which also corresponds to the number of neurons). The inhibition signal calculation in Eq.involvesmultiplications andadditions. Additionally, the leakage term introducesmultiplication operations, whileadditions are required to combine these terms and update the neurons’ membrane potentials, as specified in Eq.. Furthermore, computing the Gramian matrixin Eq.entailsmultiplications andadditions.

The Gramian matrix computation, which is considered part of the training cost and performed once per task (dataset), is excluded from the inference cost. Thus, the total floating-point operations required per time stepfor inference is given by:

Note thatis computed once per input data and remains constant across iterations. Finally, the expected sparsity through LCA is factored in by redefining, the number of active neurons, as, which represents the average number of spiking neurons whose.

The estimated training and inference FLOPs for ViT-LCA are presented in Table. “TFLOPs” denotes the Tera FLOPs required for training (computing the Gramian matrix), while “GFLOPs” represents the Giga FLOPs for inference operations. Training FLOPs accounts for all training data points, whereas inference FLOPs is estimated for a single test input.

In addition to enhancing workload efficiency, ViT-LCA leverages recent advancements in in-memory computing through memristive crossbar arrays.
Memristive crossbars can perform multiplication operations based on Kirchhoff’s Current Law and Ohm’s Law (I = V · G, where I is the current, V is the input voltage, and G is the conductance of each memristor), thereby enhancing energy efficiency and minimizing the area footprint. Using Resistive Random Access Memory (RRAM) crossbar arrays, Yao et al.reported an energy efficiency of 11 Tera OPs per Watt for floating point multiply-and-accumulate (MAC) operations. This is equivalent to each floating point operation consuming at most approximatelyjoules of energy.

Fig.illustrates the hardware mapping of ViT-LCA, where thevalues are represented as the conductance of memristors in each column, used to perform neuron excitatory input multiplications as shown in Eq..
Inputs are preprocessed in the input peripheral circuits, while neuron dynamics and thresholding—defined in Eq.and Eq.—are implemented in the output peripheral circuits located in the neural cores. The calculation of the Gramian matrix, which serves as the training head in this algorithm, can be performed offline, with the results stored in a lookup table (LUT) in the on-chip memory of a host processor that interacts with the neural cores via a digital interface.

SECTION: 
This study serves as a proof of concept, demonstrating the potential for a uniform algorithm applicable across various transformer architectures and datasets. However, it currently exhibits limitations in accuracy compared to state-of-the-art transformer models, and further improvements could be realized by exploring alternative transformer architectures for self-attention representation extraction.
We utilized the ViT-B/16 transformer model developed by Dosovitskiy et al., which has reported accuracies of 98.13%, 87.13%, and 77.91% on CIFAR-10, CIFAR-100, and ImageNet-1K, respectively. In comparison, our results (Table) were slightly lower on CIFAR-10 and CIFAR-100, but we achieved a higher accuracy on ImageNet. We believe this discrepancy arises from their use of a higher resolution of 384 and fine-tuning on CIFAR-10 and CIFAR-100, whereas our model was pre-trained solely on ImageNet-1K without fine-tuning on the CIFAR-10 and CIFAR-100 datasets, which we opted for to avoid additional computational overhead.

In comparisons with spiking transformers, our model demonstrated superior performance across all three datasets, achieving higher accuracy than Spikformer while consuming only 0.19 mJ of energy. Spikformer recorded accuracies of 95.51% on CIFAR-10, 78.21% on CIFAR-100, and 74.81% on ImageNet-1K, with an estimated inference energy of 21.48 mJ. Additionally, when compared to the Masked Spiking Transformerbased on Swin transformer, our model achieved higher accuracy on ImageNet, but showed lower accuracy on CIFAR-10 and CIFAR-100, where the Masked Spiking Transformer recorded accuracies of 97.06%, 86.73%, and 77.88%, respectively, with time steps of 128. SpikingResformerachieved Top-1 accuracy of 79.40% on ImageNet-1K, with an energy consumption of 14.76 mJ. With the highest accuracy results and lowest energy consumption on ImageNet, along with direct deployment on neuromorphic systems, ViT-LCA demonstrates significant potential for future implementation on neuromorphic platforms.

SECTION: 
In this work, we explored the integration of LCA with Vision Transformers (ViT) for the first time and evaluated its performance on classification tasks using the CIFAR-10, CIFAR-100, and ImageNet-1K datasets. We leveraged self-attention representations extracted through ViT within an LCA encoder-decoder framework, eliminating the need to train a dictionary as required in the original LCA implementation. Additionally, we demonstrated how this approach can be effectively mapped to memristor crossbar arrays to leverage efficient in-memory computing on neuromorphic systems. This study underscores the potential for further research, including the investigation of additional transformer architectures to achieve even higher accuracy results.

SECTION: References