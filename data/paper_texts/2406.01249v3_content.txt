SECTION: Equivariant Machine Learning on Graphs withNonlinear Spectral Filters
Equivariant machine learning is an approach for designing deep learning models that respect the symmetries of the problem, with the aim of reducing model complexity and improving generalization.
In this paper, we focus on an extension of shift equivariance, which is the basis of convolution networks on images, to general graphs. Unlike images, graphs do not have a natural notion of domain translation.
Therefore, we consider the graph functional shifts as the symmetry group: the unitary operators that commute with the graph shift operator.
Notably, such symmetries operate in the signal space rather than directly in the spatial space.
We remark that each linear filter layer of a standard spectral graph neural network (GNN) commutes with graph functional shifts, but the activation function breaks this symmetry. Instead, we propose nonlinear spectral filters (NLSFs) that are fully equivariant to graph functional shifts and show that they have universal approximation properties.
The proposed NLSFs are based on a new form of spectral domain that is transferable between graphs.
We demonstrate the superior performance of NLSFs over existing spectral GNNs in node and graph classification benchmarks.

SECTION: Introduction
In many fields, such as chemistry, biology, social science, and computer graphics, data can be described by graphs.
In recent years, there has been a tremendous interest in the development of
machine learning models for graph-structured data.
This young field, often calledgraph machine learning (graph-ML)orgraph representation learning, has made significant contributions to the applied sciences, e.g., in protein folding, molecular design, and drug discovery, and has impacted the industry with applications in social media, recommendation systems, traffic prediction, computer graphics, and natural language processing, among others.

Geometric Deep Learning (GDL)is a design philosophy for machine learning models where the model is constructed to inherently respect symmetries present in the data, aiming to reduce model complexity and enhance generalization.
By incorporating knowledge of these symmetries into the model, it avoids the need to expend parameters and data to learn them. This inherent respect for symmetries is automatically generalized to test data, thereby improving generalization.
For instance, convolutional neural networks (CNNs) respect the translation symmetries of 2D images, with weight sharing due to these symmetries contributing significantly to their success.
Respecting node re-indexing in a scalable manner revolutionized machine learning on graphsand has placed GNNs as a main general-purpose tool for processing graph-structured data. Moreover, within GNNs, respecting the 3D Euclidean symmetries of the laws of physics (rotations, reflections, and translations) led to state-of-the-art performance in molecule processing.

In this paper, we focus on GDL for graph-ML. We consider extensions of shift symmetries from images to general graphs. Since graphs do not have a natural notion of domain translation, as opposed to images, we propose considering functional translations instead.
We model the group of translations on graphs as the group of all unitary operators on signals that commute with the graph shift operator. Such unitary operators are calledgraph functional shifts. Note that each linear filter layer of a standard spectral GNN commutes with graph functional shifts, but the activation function breaks this symmetry. Instead, we proposenon-linear spectral filters (NLSFs)that are fully equivariant to graph functional shifts and have universal approximation properties.

In Sec., we introduce our NLSFs based on new notions ofanalysisandsynthesisthat map signals between their node-space representations and spectral representations.
Our transforms are related to standard graph Fourier and inverse Fourier transforms but differ from them in one important aspect. One key property of our analysis transform is that it is independent of a specific choice of Laplacian eigenvectors. Hence, our spectral representations are transferable between graphs.
In comparison, standard graph Fourier transforms are based on an arbitrary choice of eigenvectors, and therefore, the standard frequency domain is not transferable.
To achieve transferability, standard graph Fourier methods resort to linear filter operations based on functional calculus. Since we do not have this limitation, we can operate on the frequency coefficients with arbitrary nonlinear functions such as multilayer perceptrons.
In Sec., we present theoretical results of our NLSFs, including the universal approximation and expressivity properties. In Sec., we demonstrate the efficacy of our NLSFs in node and graph classification benchmarks, where our method outperforms existing spectral GNNs.

SECTION: Background
For, we denote.
We denote matrices by boldface uppercase letter, vectors (assumed to be columns) by lowercase boldface, and the entries of matrices and vectors are denoted with the same letter in lower case, e.g.,.
Letbe an undirected graph with a node set, an edge set, an adjacency matrixrepresenting the edge weights, and a node feature matrix (also called a signal)containing-dimensional node attributes.
Letbe the diagonal degree matrix of, where the diagonal elementis the degree of node. Denote byany normal graph shift operator (GSO). For example,could be the combinatorial graph Laplacian or the normalized graph Laplacian given byand, respectively.
Letbe the eigendecomposition of, whereis the eigenvector matrix andis the diagonal matrix with eigenvaluesordered by.
An eigenspace is the span of all eigenvectors corresponding to the same eigenvalue.
Letdenote the projection upon the-th eigenspace ofin increasing order of.
We denote the Euclidean norm by.
We define thechannel-wise signal normof a feature matrixas the vector, whereis the-th column onand. We abbreviate multilayer perceptrons by MLP.

SECTION: Linear Graph Signal Processing
Spectral GNNs define convolution operators on graphs via the spectral domain. Given a self-adjointgraph shift operator (GSO), e.g., a graph Laplacian, the Fourier modes of the graph are defined to be the eigenvectorsofand the eigenvaluesare the frequencies. A spectral filter is defined to directly satisfy the “convolution theorem”for graphs. Namely, given a signaland a function, the operatordefined by

is called a filter. Here,is the number of output channels.
Spectral GNNs, e.g.,, are graph convolutional networks where convolutions are via Eq. (), with a trainable functionat each layer, and a nonlinear activation function.

SECTION: Equivariant GNNs
Equivariance describes the ability of functionsto respect symmetries. It is expressed as,
whereis an action of a symmetry groupon the domain of.
GNNs, including spectral GNNsand subgraph GNNs, are inherently permutation equivariant w.r.t. the ordering of nodes.
This means that the network’s operations are unaffected by the specific arrangement of nodes, a property stemming from passive symmetrieswhere transformations are applied to both the graph signals and the graph domain.
This permutation equivariance is often compared to the translation equivariance in CNNs, which involves active symmetries.
The key difference between the two symmetries lies in the domain: while CNNs operate on a fixed domain with signals transforming within it, graphs lack a natural notion of domain translation.
To address this, we consider graph functional shifts as the symmetry group, defined by unitary operators that commute with the graph shift operator.
This perspective allows for our NLSFs to be interpreted as an extension of active symmetry within the graph context, bridging the gap between the passive and active symmetries inherent to GNNs and CNNs, respectively.

SECTION: Nonlinear Spectral Graph Filters
In this section, we present new concepts of analysis and synthesis under which the spectral domain is transferrable between graphs.
Following these concepts, we introduce new GNNs that are equivariant tofunctional symmetries– symmetries of the Hilbert space of signals rather than symmetries in the domain of definition of the signal.

SECTION: Translation Equivariance of CNNs and GNNs
For motivation, we start with the grid graphwith node setand circular adjacency, we define the translation operatorbyas

Note that anyis a unitary operator that commutes with the grid Laplacian, i.e.,, and therefore it belongs to the group of all unitary operatorsthat commute with the grid Laplacian. In fact, the space of isotropic convolution operators (with 90rotation and reflection symmetric filters) can be seen as the space of all normal operatorsthat commute with unitary operators from. Applying a non-linearity after the convolution retains this equivariance, and hence, we can build multi-layer CNNs that commute with.
By the universal approximation theorem, this allows us to approximate any continuous function that commutes with.

Note that such translation equivariance cannot be extended to general graphs.
Achieving equivariance to graph functional shifts through linear spectral convolutional layersis straightforward, since these layers commute with the space of all unitary operatorsthat commute with.
However, introducing non-linearitybreaks the symmetry.
That is, there existssuch that

whereis any non-linear activation function, e.g., ReLU, Sigmoid, etc.

This means that multi-layer spectral GNNs do not commute with, and are hence not appropriate as approximators of general continuous functions that commute with(see App.for an example illustrating how non-linear activation functions break the functional symmetry).
Instead, we propose in this paper a multi-layer GNN that is fully equivariant to, which we show to be universal: it can approximate any continuous graph-signal function (w.r.t. some metric) commuting with.

SECTION: Graph Functional Symmetries and Their Relaxations
We define the symmetry group of graph functional shifts as follows.

It is important to note that functional shifts, in general, are not induced from node permutations.
Instead, functional shifts are related to the notion of functional mapsused in shape correspondence and are general unitary operators that are not permutation matrices in general. The value of the functionally translated signal at a given node can be amixtureof the content of the original signal at many different nodes. For example, the functional shift can be a combination of shifts of different frequencies at different speeds. See App.for illustrations and examples of functional translations.

A fundamental challenge with the symmetry group in Def.is its lack of transferability between different graphs.
Hence, we propose to relax this symmetry group.
Letbe the indicator functions of the intervals, which constitute a partition of the frequency band.
The operators, interpreted via functional calculus Eq. (), are projections of the signal space upon band-limited signals. Namely,.
In our work, we consider filtersthat are supported on the dyadic sub-bands,
whereis the decay rate.
See Fig.in App.for an illustrated example.
Note that for, the sub-band falls in.
The total bandis.

Similarly, we can relax functional shifts by restricting to the leading eigenspaces.

SECTION: Analysis and Synthesis
We use the terminology of analysis and synthesis, as in signal processing, to describe transformations of signals between their graph and spectral representations.
Here, we consider two settings: the eigenspace projections case and the filter bank (of indicators) case. The definition of the frequency domain depends on a given signal, where its projections to the eigenspaces ofare taken as the Fourier modes. Spectral coefficients are modeled as matricesthat mix the Fourier modes, allowing to synthesize signals of general dimensions.

We first defineanalysis and synthesis using the spectral indexup to frequency.
Letbe the orthogonal complement to the firsteigenprojections.
Letbe the graph signal andthe spectral coefficients to be synthesized, whererepresents number of output channels.
The analysis and synthesis are defined respectively by

whereandare parameters that promote stability, and the poweras well as the division in Eq. () are element-wise operations on each entry. Here,denotes concatenation.
We remark that the orthogonal complement in the-th filter alleviates the loss of information due to projecting to the low-frequency bands, and therefore, the full spectral range of the signal can be captured.
This is particularly important for heterophilic graphs, which rely on high-frequency components for accurate label representation.
The termindexstems from the fact that eigenvalues are treated according to their index when defining the projections.
Note that the synthesis here differs from classic signal processing as it depends on a given signal on the graph.
When treatingandas fixed, this synthesis operation is denoted by.
We similarly denote.

Similarly, we define theanalysis and synthesis in the filter bankup to bandas follows.
Letdenote the orthogonal complement to the firstbands.
Letbe the graph signal, and letrepresent the spectral coefficients to be synthesized, whererefers to the general dimension.
The analysis and synthesis in the filter bank case are defined by

respectively, whereare as before.
Here,.
The termvaluerefers to how eigenvalues are used based on their magnitude when defining the projections.
As before, we denoteand.

In App., we present a special case of diagonal synthesis where.
In App., we show that the diagonal synthesis is stably invertible.

SECTION: Definitions of Nonlinear Spectral Filters
We introduce three novel types ofnon-linear spectral filters (NLSF): Node-level NLSFs, Graph-level NLSFs, and Pooling-NLSFs.
Fig.illustrates our NLSFs for equivariant machine learning on graphs.

To be able to transfer NLSFs between different graphs and signals, one key property of NLSF is that they do not depend on the specific basis chosen in each eigenspace.
This independence is facilitated by the synthesis process, which relies on the input signal.
Following the spectral index and filter bank cases in Sec., we define the Index NLSFs and Value NLSFs by

whereandare callednonlinear frequency responses, andis the output dimension.
To adjust the feature output dimension, we apply an MLP with shared weights to all nodes after the NLSF.
In the case whenand the filters operator diagonally (i.e., the product and division are element-wise in synthesis), we refer to it as diag-NLSF. See App.for more details.

We first introduce the Graph-level NLSFs that are fully spectral, where the NLSFs map a sequence of frequency coefficients to an output vector.
Specifically, the Index-based and Value-based Graph-level NLSFs are given by

where,, andis the output dimension.

We introduce another type of graph-level NLSFs by first representing each graph in a Node-level NLSFs as in Eq. () and Eq. ().
The final graph representation is obtained by applying a nonlinear activation function followed by a readout function to these node-level representations.
We consider four commonly used pooling methods, including mean, sum, max, and-norm pooling, as the readout function for each graph.
We apply an MLP after readout function to obtain a-dimensional graph-level representation.
We term these graph-level NLSFs asPooling-NLSFs.

SECTION: Laplacian Attention NLSFs
To understand which Laplacian and parameterization (index v/s value) of the NLSF are preferable in different settings, we follow the random geometric graph analysis outlined in.
Specifically, we consider a setting where random geometric graphs are sampled from a metric-probability space.
In such a case, the graph Laplacian approximates continuous Laplacians on the metric spaces under some conditions.
We aim for our NLSFs to produce approximately the same outcome for any two graphs sampled from the same underlying metric space, ensuring that the NLSF istransferable.
In App., we show that if the nodes of the graph are sampled uniformly from, then using the graph Laplacianin Index NLSFs yields a transferable method.
Conversely, if the nodes of the graph are sampled non-uniformly, and any two balls of the same radius inhave the same volume, then utilizing the normalized graph Laplacianin Value NLSFs is a transferable method.
Given that graphs may fall between these two boundary cases, we present an architecture that chooses between the Index NLSFs with respect toand Value NLSFs with respect to, as illustrated in Fig.. While the above theoretical setting may not be appropriate as a model for every real-life graph dataset, it suggests that index NLSF may be more appropriate with, value NLSFs with, and different graphs are more appropriately analyzed by different balances between these two cases.

In the Laplacian attention architecture, a soft attention mechanism is employed to dynamically choose between the two parameterizations, given by

whereis obtained using a softmax function to normalize the scores into attention weights, balancing each NLSFs’ contribution.

SECTION: Theoretical Properties of Nonlinear Spectral Filters
We present the desired theoretical properties of our NLSFs at the node-level and graph-level.

SECTION: Complexity of NLSFs
NLSFs are implemented by computing the eigenvectors of the GSO.
Most existing spectral GNNs avoid direct eigendecomposition due to its perceived inefficiency.
Instead, they use filters implemented by applying polynomialsor rational functionsto the GSO in the spatial domain.
However, power iteration-based eigendecomposition algorithms, e.g., variants of the Lanczos method, can be highly efficient.
For matrices withnon-zero entries, the computational complexity of one iteration for findingeigenvectors corresponding to the smallest or largest eigenvalues (calledleading eigenvectors) is.
In practice, these eigendecomposition algorithms converge quickly due to their super-exponential convergence rate, often requiring only a few iterations, which makes them as efficient as message passing networks of signals withchannels.

This makes NLSFs applicable to node-level tasks on large sparse graphs, as demonstrated empirically in App., since they rely solely on the leading eigenvectors.
In Sec., we show that using the leading eigenvectors can approximate GSOs well in the context of learning on graphs.
Note that we can precompute the spectral projections of the signal before training. For node-level tasks, such as semi-supervised node classification, the leading eigenvectors only need to be pre-computed once, with a complexity of. This
During thelearning phase, each step of the architecture search and hyperparameter optimization takescomplexity for analysis and synthesis, andfor the MLP in the spectral domain, which is faster than the complexityof message passing or standard spectral methods if.
Empirical studies on runtime analysis are in App..

For dense matrices, the computational complexity of a full eigendecomposition isper iteration, whereis the complexity of matrix multiplication. This is practical for graph-level tasks on relatively small and dense graphs, which is typical for many graph classification datasets. In these cases, the eigendecomposition of all graphs in the dataset can be performed as a pre-computation step, significantly reducing the complexity during the learning phase.

SECTION: Equivariance of Node-level NLSFs
We demonstrate the node-level equivariance of our NLSFs, ensuring that our method respects the functional shift symmetries. The proof is given in App..

SECTION: Universal Approximation and Expressivity
In this subsection, we discuss the approximation power of NLSFs.

We begin with a setting where a graph is given as a fixed domain, and the data distribution consists of multiple signals defined on this graph.
An example of this setup is a spatiotemporal graph, e.g., traffic networks, where a fixed sensor system defines a graph and the different signals represent the sensor readings collected at different times.

In App., we prove the following lemma, which shows that linear NLSFs exhaust the space of linear operators that commute with graph functional shifts.

Lemmashows a close relationship between functions that commute with functional shifts and those defined in the spectral domain.
This motivates the following construction of a pseudo-metric on.
In the case of relaxed functional shifts,
we define the standard Euclidean metricin the spectral domain. We pull back the Euclidean metric to the spatial domain to define a signal pseudo-metric. Namely, for two signalsand, their distance is defined by

This pseudo metric can be made into a metric by considering each equivalence class of signals with zero distance as a single point in the space.
As MLPscan approximate any continuous function(the universal approximation theorem),
node-level NLSFs can approximate any continuous function that maps (equivalence classes of) signals to (equivalence classes of) signals.
For details, see App..
A similar analysis applies to hard functional shifts.

The above analysis also motivates the construction of a graph-signal metric for graph-level tasks. For graphs with-channel signals, we consider again the standard Euclidean metricin the spectral domain. We define the distance between any two graphs with GSOs and signalsandto be

This definition can be extended into a metric by considering the spaceof equivalence classes of graph-signals with distance 0.
As before, this distance inherits the universal approximation properties of standard MLPs. Namely, any continuous functionwith respect tocan be approximated by NLSFs based on MLPs. Additional details are in App..

In App., we show that Pooling-NLSFs are more expressive than graph-level NLSF when anynorm is used in Eq. () and Eq. () with, both in the definition of the NLSF and as the pooling method.
Specifically, for every graph-level NLSF, there is a Pooling-NLSF that coincides with it.
Additionally, there are graph signalsandfor which a Pooling-NLSF can attain different values, whereas any graph-level NLSF must attain the same value.
Hence, Pooling-NLSFs have improved discriminative power compared to graph-level NLSFs.
Indeed, as shown in Tab., Pooling-NLSFs outperform Graph-level NLSFs in practice, which can be attributed to their increased expressivity.
We refer to App.for additional discussion on graph-level expressivity.

SECTION: Uniform Approximation of GSOs by Their Leading Eigenvectors
Since NLSFs on large graphs are based on the leading eigenvectors of, we justify its low-rank approximation in the following.
While approximating matrices with low-rank matrices might lead to a high error in the spectral and Frobenius norms, we show that such an approximation entails a uniformly small error in the cut norm.
We define and interpret the cut norm in App., and explain why it is a natural graph similarity measure for graph machine learning.

The following theorem is a corollary of the Constructive Weak Regularity Lemma presented in.
Its proof is presented in App..

Note that the bound in Thm.is uniformly small, independently ofand its dimension. This theorem justifies using the leading eigenvectors when working with the adjacency matrix as the GSO. For a justification when working with other GSOs see App..

SECTION: Experimental Results
We evaluate the NLSFs on node and graph classification tasks.
Additional implementation details are in App., and additional experiments, including runtime analysis and ablation studies, are in App..

SECTION: Semi-Supervised Node Classification
We first demonstrate the main advantage of the proposed Node-level NLSFs over existing GNNs with convolution design on semi-supervised node classification tasks.
We test three citation networks: Cora, Citeseer, and Pubmed.
In addition, we explore three heterophilic graphs: Chameleon, Squirrel, and Actor.
For more comprehensive descriptions of these datasets, see App..

We compare the Node-level NLSFs using Laplacian attention with existing spectral GNNs for node-level predictions, including GCN, ChebNet, ChebNetII, CayleyNet, APPNP, GPRGNN, ARMA, JacobiConv, BernNet, Specformer, and OptBasisGNN.
We also consider GATand SAGE.
For datasets splitting on citation graphs (Cora, Citeseer, and Pubmed), we apply the standard splits following, using 20 nodes per class for training, 500 nodes for validation, and 1000 nodes for testing.
For heterophilic graphs (Chameleon, Squirrel, and Actor), we use the sparse splitting as in, allocating 2.5% of samples for training, 2.5% for validation, and 95% for testing.
We measure the classification quality by computing the average classification accuracy with a 95% confidence interval over 10 random splits.
We utilize the source code released by the authors for the baseline algorithms and optimize their hyperparameters using Optuna. Each model’s hyperparameters are fine-tuned to achieve the highest possible accuracy. Detailed hyperparameter settings are provided in App..

Tab.presents the node classification accuracy of our NLSFs using Laplacian attention and the various competing baselines.
We see that-Node-level NLSFs outperform the competing models on the Cora, Citeseer, and Chameleon datasets. Notably, it shows remarkable performance on the densely connected Squirrel graph, outperforming the baselines by a large margin. This can be explained by the sparse version in Eq. () of Thm., which shows that the denser the graphs is, the better its rank-approximation.
For the Pubmed and Actor datasets,-Node-level NLSFs yield the second-best results, which are comparable to the best results obtained by APPNP.

SECTION: Node Classification on Filtered Chameleon and Squirrel Datasets in Dense Split Setting
Recently, the work inidentified the presence of many duplicate nodes across the train, validation, and test splits in the dense split setting of the Chameleon and Squirrel.
This results in train-test data leakage, causing GNNs to inadvertently fit the test splits during training, thereby making performance results on Chameleon and Squirrel less reliable.
To further validate the performance of our Node-level NLSFs on these datasets in the dense split setting, we use both the original and filtered versions of Chameleon and Squirrel, which do not contain duplicate nodes, as suggested in.
We use the same random splits as in, dividing the datasets into 48% for training, 32% for validation, and 20% for testing.

Tab.presents the classification performance comparison between the original and filtered Chameleon and Squirrel.
The baseline results are taken from, and we include the following competitive models: ResNet+SGC, ResNet+adj, GCN, GPRGNN, FSGNN, GloGNN, and FAGCN.
The detailed comparisons are in App..
We see in the table that the-Node-level NLSFs consistently outperform the competing baselines on both the original and filtered datasets.-Node-level NLSFs demonstrate less sensitivity to node duplicates and exhibit stronger generalization ability, further validating the reliability of the Chameleon and Squirrel datasets in the dense split setting.
We note that compared to the dense split setting, the sparse split setting in Tab.is more challenging, resulting in lower classification performance.
A similar trend of significant performance difference between the two settings of Chameleon and Squirrel is also observed in.

SECTION: Graph Classification
We further illustrate the power of NLSFs on eight graph classification benchmarks.
Specifically, we consider five bioinformatics datasets: MUTAG, PTC, NCI1, ENZYMES, and PROTEINS, where MUTAG, PTC, and NCI1 are characterized by discrete node features, while ENZYMES and PROTEINS have continuous node features.
Additionally, we examine three social network datasets: IMDB-B, IMDB-M, and COLLAB.
The unattributed graphs are augmented by adding node degree features following.
For more details of these datasets, see App..

In graph classification tasks, a readout function is used to summarize node representations for each graph. The final graph-level representation is obtained by aggregating these node-level summaries and is then fed into an MLP with a (log)softmax layer to perform the graph classification task.
We compare our NLSFs with two kernel-based approaches: GKand WL, as well as nine GNNs: GCN, GAT, SAGE, ChebNet, ChebNetII, CayleyNet, APPNP, GPRGNN, and ARMA. Additionally, we consider the hierarchical graph pooling model DiffPool.
For dataset splitting, we apply the random split following, using 80% for training, 10% for validation, and 10% for testing.
This random splitting process is repeated 10 times, and we report the average performance along with the standard deviation.
For the baseline algorithms, we use the source code released by the authors and optimize their hyperparameters using Optuna.
We fine-tune the hyperparameters of each model to achieve the highest possible accuracy. Detailed hyperparameter settings for both the baselines and our method are provided in App..

Tab.presents the graph classification accuracy.
Notably, the-Graph-level NLSFs (i.e., NLSFs without the synthesis and pooling process) perform the second best on the ENZYMES and PROTEINS.
Additionally,-Pooling-NLSFs consistently outperform-Graph-level NLSFs, indicating that the node features learned in our Node-level NLSFs representation are more expressive, corroborating our theoretical findings in Sec..
Furthermore, our-Pooling-NLSFs consistently outperform all baselines on the MUTAG, PTC, ENZYMES, PROTEINS, IMDB-M, and COLLAB datasets. For NCI1 and IMDB-B,-Pooling-NLSFs rank second and are comparable to ChebNetII.

SECTION: Summary
We presented an approach for defining non-linear filters in the spectral domain of graphs in a transferable and equivariant way. Transferability between different graphs is achieved by using the input signal as a basis for the synthesis operator, making the NLSF depend only on the eigenspaces of the GSO and not on an arbitrary choice of the eigenvectors.
While different graph-signals may be of different sizes, the spectral domain is a fixed Euclidean space independent of the size and topology of the graph. Hence, our spectral approach represents graphs as vectors. We note that standard spectral methods do not have this property since the coefficients of the signal in the frequency domain depend on an arbitrary choice of eigenvectors, while our representation depends only on the eigenspaces.
We analyzed the universal approximation and expressivity power of NLSFs through metrics that are pulled back from this Euclidean vector space.
From a geometric point of view, our NLSFs are motivated by respecting graph functional shift symmetries, making them related to Euclidean CNNs.

One limitation of NLSFs is that, when deployed on large graphs, they only depend on the leading eigenvectors of the GSO and their orthogonal complement.
However, important information can also lie within any other band. In future work, we plan to explore NLSFs that are sensitive to eigenvalues that lie within a set of bands of interest, which can be adaptive to the graph.

SECTION: Acknowledgments
The work of YEL and RT was supported by the European Union’s Horizon 2020 research and innovation programme under grant agreement No. 802735-ERC-DIFFOP.
The work of RL was supported by the Israel Science Foundation grant No. 1937/23, and the United States - Israel Binational Science Foundation (NSF-BSF) grant No. 2024660.

SECTION: References
SECTION: Non-linear Activation Functions Break the Functional Symmetry
We offer a construction for complex-valued signals and first order derivative as the graph Laplacian, which is easy to extend to the real case and second order derivative.
Consider a circular 1D grid withnodes and the first order central difference as the Laplacian. Here, the Laplacian eigenvectors are the standard Fourier modes. In this case, one can see that a graph functional shift is any operator that is diagonal in the frequency domain and multiplies each frequency by any complex number with the unit norm. Consider the nonlinearity that takes the real part of the signal and then applies ReLU, which we denote in short by ReLU. Consider a graph signal. We consider a graph functional shiftthat shifts frequencies 10 and 20 at a distance of 5, and every other frequency is not shifted. Namely, frequency 10 is multiplied by, frequency 20 by, and every other frequency is multiplied by 1. Consider also the classical shiftthat translates the whole signal byuniformly. Sinceconsists only of the frequencies 10 and 20, it is easy to see that. Hence,. Conversely, if we applyand only then shift, note thatconsists of many frequencies in addition to 10 and 20. For example, by nonnegativity of,has a nonzero DC component (zeroth frequency). Now,only translates the 10 and 20 frequencies, so we have.

SECTION: Illustrating Functional Translations
Here we present an additional discussion and illustrations on the new notions of symmetry.

SECTION: Functional Translation of a Gaussian Signal with Different Speeds at Different Frequencies
To illustrate the concepts of relaxed symmetry and functional translation, we present a toy example involving the classical translation and a functional translation of a Gaussian signal on a 2D grid with a standard deviation of 1.

The classical translation involves moving the entire signal uniformly across the grid.
This uniform movement can be represented as, whereandare the translation amounts in theanddirections, respectively, andandare circular translations, namely, subtractions modulo the size of the circular grid.
For simplicity, we consider. For instance, if, the Gaussian is shifted by 5 units in both theanddirections.
Fig.top-row shows the classical translation for,,, and.
We see that every part of the signal shifts at the same rate and direction, preserving the overall shape of the Gaussian signal.
Note that classical translation is equivalent to modulation of the frequency domain, i.e., whereis the Fourier transform of.
Therefore, we can view the classical translation as a specific type of functional translation.

Next, we illustrate a functional translation that is based on a frequency-dependent movement.
Specifically, the Gaussian signal is decomposed into low and high-frequency components based on two indicator band-pass filters.
In our example, the translation parameteris different for the low and high-frequency components: low frequencies are shifted bywhile high frequencies are shifted by.
This functional translation is defined via modulations in the frequency domain given byfor low-frequency components andfor high-frequency components. The combined functionally translated signal in the frequency domain is then.
Fig.bottom-row demonstrates the functional translation for,,, and.
We observe that the low-frequency components (smooth parts) of the signal move at one speed, while high-frequency components move at another.
This demonstrates that functional symmetries are typically more rich than domain symmetries.

SECTION: Robust Graph Functional Shifts in Image Translation and MNIST Classification on Perturbed Grids
We present another toy example to illustrate that functional translations are more stable than hard symmetries of the graph, namely, graph automorphisms (isomorphisms from the graph to itself).
Automorphisms are another analog to translations on general graphs, which competes with our notion of functional shifts. Consider as an example the standard 2D circular grid (discretizing the torus). The automorphisms of the grid include all translations. However, this notion of hard symmetry is very sensitive to graph perturbations. If one adds or deletes even one edge in the graph, the automorphism group becomes much smaller and does not contain the translations anymore.

In contrast, we claim that functional shifts are not so sensitive to graph perturbations.
To demonstrate this empirically, we conducted the following toy experiment.
We add a Gaussian noise to the edge weights of the 2D grid to create a perturbed graph. Given a standard domain shift, we optimize the coefficients of a functional shift so it is as close as possible to the classical domain shift of the clean grid in Frobenius error.
Fig.presents an example of classically and functionally shifted image of the digit 5. The original digit (left), a classical domain translation of the clean grid (middle), and a functional translation constructed from the perturbed graph (right) to match the classical translation.
We see that standard translations can be approximated by a graph functional shift on a perturbed grid.

We further demonstrate the robustness to graph perturbations of NLSFs trained on MNIST classification.
The experimental setup follows previous work in.
We compare the NLSF on the clean grid (i.e., without Gaussian noise) to the NLSF on the perturbed grid.
Tab.presents the classification accuracy of the NLSF.
We see that the classification performance is almost unaffected by the perturbation, indicating that NLSFs on the perturbed grid can roughly express CNN-like operations (translation equivariant functions).

SECTION: Special Cases of NLSFs
We present two special cases of NLSFs as follows.

SECTION: Diagonal NLSFs
In Sec., we introduced the NLSFs with the output dimension, which is a tunable hyperparameter. Here, we present a special case whensuch that the multiplication and division in synthesis are operated diagonally.
Specifically, the diagonal analysis and synthesis in the spectral index case and in the filter bank case are defined respectively by

and

where the product and division are element-wise along the channel direction.
That is,in the spectral index case
(resp.in the filter bank case).
Here,in the spectral index case (resp.in the filter bank case) are the spectral coefficients to be synthesized andare as before.

For Node-level diag-NLSFs, we define the Index diag-NLSFs and Value diag-NLSFs as follows

whereinandin.
To adjust the feature output dimension at each node, we apply an MLP with shared weights to all nodes after the NLSF. We present the empirical study of diag-NLSFs in App..

SECTION: Leading NLSFs
In Sec., we introduce the NLSFs with the orthogonal complement. Specifically, the-th filter in the Index NLSFs is given by, and the-th filter in Vale NLSFs is defined as.
To explore the effects of the orthogonal complement, we focus on the leading NLSFs in both the Index NLSFs and Vale NLSFs, considering only the firstandfilters without including their orthogonal complements.
In this case, the analysis and synthesis in the spectral index case and in the filter bank case are respectively given by

and

whereandare defined as in Sec..
The lead-NLSFs are then defined by

where,, andis the output dimension.
To adjust the feature output dimension, we apply an MLP with shared weights to all nodes after the NLSF.

Similar to App., when consideringsuch that the multiplication and division in synthesis are operated diagonally, we have the lead-diag-NLSFs defined by

whereinandin.
We present the empirical study of lead-NLSFs and lead-diag-NLSFs in App..

SECTION: Theoretical Analysis
We note that the numbering of the statements corresponds to the numbering used in the paper, and we have also included several separately numbered propositions and lemmas that are used in supporting the proofs presented.
We restate the claim of each statement for convenience.

SECTION: Equivariance of NLSFs
We start with two simple lemmas that characterize the graph functional shifts. The lemmas directly follow the fact that two normal operators commute iff the projections upon their eigenspaces commute.

Note that

wheredenotes direct products of linear spaces and the-th filter is the orthogonal complement, given by. Denote bythe direct sum of operators.

Let.
Sincecommute withfor, and with, it also commutes with. Therefore, we can write

Now, when restrictingto an operator in, it is unitary. Indeed, for everyand, sinceis unitary andis self-adjoint and satisfies, we have

andis the inverse ofin, since for every

and similarly.
Here, an invertible normal operator commutes with an orthogonal projection if and only if its inverse does.

The other direction is trivial.
∎

Note that

where the-th filter is the orthogonal complement, given by.

The proof is analogous to the proof of Lemma.

We start with Index-NLSFs.
Consider the Index NLSF defined as in Eq. ()

We need to show that for any unitary operator,

Considerand apply it to the graph signal. The Index NLSF with the transformed input is given by

Since, it commutes withfor all, i.e.,.
Using this commutation property, we can rewrite the norm and the projections as

In addition, sinceis a unitary matrix, it preserves the norm.
Therefore, we have

Substituting these expressions back into the definition of the Index NLSFs gives

Notice thatappears linearly in the numerator of the fraction. Hence, we can factor it out by

The expression inside the summation is exactly the original Index NLSFs applied to, so

Therefore, we have shown that applying the unitary operatorto the graph signalresults in the Index NLSFs being transformed by the same unitary operator, proving the equivariance property.

The proof for value-NLSF follows the same steps.
∎

SECTION: Expressivity and Universal Approximation
In this section, we focus on value parameterized NLSFs. The analysis for index-NLSF is equivalent.

We next show that node-level linear NLSFs exhaust the space of linear operators that commute with graph functional shifts (on the fixed graph).

For simplicity, we restrict the analysis to the case of 1D signals (). The extension to a general dimensionis natural.

By Lemma,
the unitary operatorsinare exhausted by the operators of the form

whereis any unitary operator in.
Hence, since the unitary representationof the group of unitary operators in(for any) is irreducible, by Schur’s lemmaany linear operatorthat commutes with all operators ofmust be a scalar times the identity when projected to.
Namely,has the form

whereis the identity operator in.
This means that linear node-level NLSFs exhaust the space of linear operators that commute with.

The case of hard graph functional shifts is treated similarly.
∎

The above analysis motivates the following construction of a metric on.
Given the filter bank, on graph with-dimensional signals, we define the standard Euclidean metricin the spectral domain. We pull back the Euclidean metric to the spatial domain to define a graph-signal metric. Namely, for two signalsand, their distance is defined to be

This defines a pseudometric on the space of graph-signals. To obtain a metric, we consider each equivalence class of signals with zero distance as a single point. Namely, we define the spaceto be the space of signals with 1D features modulo. In, the pseudometricbecomes a metric, andan isometry of metric spaces.

Now, since MLPscan approximate any continuous function(by the universal approximation theorem), and by the fact thatis an isometry, node-level NLSFs based on MLPs in the spectral domain can approximate any continuous function from signals withfeatures to signal withfeatures.

The above analysis also motivates the construction of a graph-signal metric for graph-level tasks. For graphs with-channel signals, we define the standard Euclidean metricin the spectral domain. We pull back the Euclidean metric to the spatial domain to define a graph-signal metric. Namely, for two graphs with Laplacians and signalsand, their distance is defined to be

This defines a pseudometric on the space of graph-signals. To obtain a metric, we consider equivalence classes of graph-signals with zero distance as a single point. Namely, we define the spaceto be the space of graph-signal modulo. In, the functionbecomes a metric, andan isometry.

By the isometry property,inherits any approximation property from. For example, since MLPs can approximate any continuous function, the space of NLSFs based on MLPshas a universality property: any continuous functionwith respect tocan be approximated by a NLSF based on an MLP.

We now show that pooling-NLSF are more expressive than graph-level NLSF if the norm in Eq. () and
Eq. () iswith.

First, we show that there are graph-signals that graph-level-NLSFs cannot separate and Pooling-NLSFs can. Consider an index NLSF with normnormalize by. Namely, for,

The general case is similar.

For the two graphs,
take the graph Laplacian

with eigenvalues, and corresponding eigenvectorsand.
Take the graph Laplacian

with eigenvalues. The first two eigenvectors areandrespectively.

Consider an Index-NLSF based on two eigenprojections.
As the signal of the first graph take, and for the second graph take. Both graph-signals have the same spectral coefficients, so graph-level NLSF cannot separate them.
Suppose that the NLSF is

The outcome of the corresponding Pooling NLSF on the two graphs is

Hence, Pooling-NLSFs separate these inputs, while graph-level-NLSFs do not.

Next, we show that Pooling-NLSFs are at least as expressive as graph-level NLSFs. In particular, any graph-level NLSF can be expressed as a pooling NLSF.

Letbe a graph-level NLSF.
Define the node-level NLSF that chooses one spectral indexwith a nonzero value (e.g., the band with the largest coefficient) and projects upon it the value. Hence, before pooling, the NLSF gives

wheredepends on the spectral coefficients (e.g., it is the index of the largest spectral coefficient).
Hence, after pooling, the Pooling NLSF returns, which coincides with the output of the graph-level NLSF.

Now, let us show that for, they have the same expressivity.
For any NLSF,

This is a generic fully spectral NLSF.

We offer additional discussion on the expressivity of graph-level NLSFs.
Note that graph-level NLSFs are bounded by the expressive power of MPNNs with random positional encodings. For example, in, the authors showed that random features improve GNN expressivity, distinguishing certain structures that deterministic GNNs cannot. The Lanczos algorithm for computing the leadingeigenvectors can be viewed as a message-passing algorithm with a randomly initialized-channel signal.

The example in App.can be written as a standard linear graph spectral filter, followed by the pooling (readout) function. This shows that graph-level NLSFs (without synthesis and pooling) are not more expressive than standard spectral GNNs with pooling.
In the other direction, there are no graph-signals that can be separated by a graph-level NLSF but not by a spectral GNN. Given two graph-signals that an NLSF can separate, we can build a specific standard spectral GNN that gives the same output as the NLSF specifically for the two given graph-signals. However, several considerations should be noted:

the feature dimension of this GNN would is as large as the combined number of eigenvalues of the two graphs times the number of features,

this GNN is designed to fit these two specific graphs, and will not work for other graphs, and,

if implemented via polynomial filters, the polynomial order would have to be the combined number of eigenvalues of the two graphs, which makes it very unstable.

Let us explain the architecture next. Given the two graphswithvertices andwithvertices, with node features of dimension, define band-pass filters that separate the combined set of eigenvalues of the two given graphs. Concatenate these filters to build a single multi-channel filter. Namely, this filter maps the signal to the sequence of band-pass filtered signal – each band at a different channel, for a total ofchannels. If the band-pass filters are based on polynomials, each polynomial would be chosen to be zero on all eigenvalues except for one. Apply-norm pooling on each channel to obtain a single feature of dimension. This gives the sequence of norms of the signal at the bands, where, for the two given graphs, the two pooled signals of the two graphs are supported on disjoint sets of indices. Hence, the linear spectral filter contains the frequency information of the NLSF. Then, one can apply the MLP of the NLSF to the channels corresponding to the first graph, and to the channels corresponding to the second graph. This means that the linear spectral GNN gives the exact same outputs onandas the NLSF.

Regarding pooling-NLSF, we do not offer an answer to the question whether standard spectral GNNs are more powerful (assuming an unlimited budget) than pooling-NLSFs, or vice-versa.
More practically meaningful questions would be:

Compare the expressivity of standard spectral GNNs to NLSFs for a given budget of parameters.

How many graph-signal can a single spectral GNN separate, vs a single NLSF, of the same budget.

We leave these questions as open problems for future research. We note that, in practice, NLSFs with the same budget as standard spectral GNNs perform better.

SECTION: Stable Invertibility of Synthesis
We present the analysis for diagonal synthesis defined from App..
In the fixed graph setting, given any 1D signalsuch thatfor every, we show that the synthesis operatoris stably invertible.

Since we consider 1D signal, the diagonal synthesis in filter bank case in Eq. () can be written as

where

, and.

Sincefor all, the matrixis a diagonal matrix with entries.
Therefore, the singular values ofare

the right singular vectors are the standard basis elementsin, and the left singular vectors are

for.
Hence, we have

and

Suppose thatand. In this case,is an isometry from the spectral domain to a subspace of the signal space. Analysis is the adjoint of synthesis.
This analysis can be extended to the index parametrization case for diagonal synthesis, and to higher dimensional signals.

SECTION: Uniform Approximation of Graphs withEigenvectors
In this section, we develop the setting under which the low-rank approximation of GSOs with their leading eigenvectors can be interpreted as a uniform approximation (Sec.).

The cut norm ofis defined to be

The distance between two matrices in cut norm is defined to be.

The cut norm has the following interpretation, which has precise formulation in terms of the weak regularity lemma. Any pair of (deterministic) graphs are close to each other in cut norm if and only if they can be described as pseudo-random graphs sampled from the same stochastic block model. Hence, the cut norm is a meaningful notion of graph similarity for practical graph machine learning, where graphs are noisy and can represent the same underlying phenomenon even if they have different sizes and topologies. In addition, the distance between non-isomorphic graphons is always positive in cut norm.
In this context, the work inshowed that GNNs with normalized sum aggregation cannot separate graphs that have zero distance in the cut norm. This means that the cut norm is sufficiently discriminative for practical machine learning on graphs.

The following lemma, called theconstructive weak regularity lemma in Hilbert spaces, was proven in. It is an extension of the classical respective result from.

We define the Frobenius norm normalized byas

Let us use Lemma, with, andthe set of symmetric rank one matrices of the formwhereis a column vector. Denote bythe space of linear combinations ofelements of, which is the space of symmetric matrices of rank bounded by.
For the Hilbert space norm, we take the Frobenius norm.
In the setting of the lemma, we take, and, and.
By the lemma, with probability, any Frobenius minimizer,
namely, that satisfies, also
satisfies

for every.
Hence, for every choice of subset, we have

where for a set, denote bythe column vector withat coordinates inandotherwise.

Hence, we also have

Lastly, note that by the best rank-approximation theorem (Eckart–Young–Mirsky Theorem), any Frobenius minimizeris the projection upon theleading eigenvectors of(or some choice of these eigenvectors in case of multiplicity higher than 1).

∎

Now, one can use the adjacency matrixasin Thm..
When working with sparse matrices ofedges, to achieve a meaningful scaling of cut distance, we re-normalize the cut norm and define

With this norm, Thm.gives

While this bound is not uniformly small in, it is still independent of.
In contrast, the error bounds for spectral and Frobenius norms do depend on the specific properties of.

Now, if we want to apply Thm.to other GSOs, we need to make some assumptions.
Note that when the GSO is a Laplacian, we take as the leading eigenvectors the ones correspoding to the smallest eigenvalues, not the largest ones.
To make the theorem applicable, we need to reorder the eigenvectors of.
This can be achieved by applying a decreasing functionto, such as.
The role ofis to amplify the eigenspaces ofin which most of the energy of signals interest (the ones that often appear in the dataset) is concentrated.
Under the assumption thathas entries bounded by some not-too-high, one can now justify approximating GSOs by low-rank approximations based on the smallest eigenvectors.

SECTION: NLSFs on Random Geometric Graphs
In this section we consider thenorm normalized by, namely,

We follow a similar analysis to, mostly skipping the rigorous Monte-Carlo error rate proofs, as these are standard.

Letbe a compact metric space with metricand with a Borel probability measure, that we formally call thestandard measure.
Let, and denote bythe ball of radiusabout. Letbe the volume ofwith respect to the standard measure (note thatneed not be constant). We consider an underlying Laplacian on the spacedefined on signalsby

where we assumein the analysis below without loss of generality. In case the integral in Eq. () is normalized by, this Laplacian was called-Laplacian in, which we denote by.
Such a Laplacian is related to Korevaar-Schoen
type energies, in which the limit case of the radiusgoing to 0 is considered. It was shown inthat the-Laplacian is self-adjoint with spectrum supported inside some interval, for some, where for some, the part of the spectrum inis discrete (consisting of isolated eigenvalues with finite multiplicities). The intuition behind this result is that, for the-Laplacian, we have

The first term in the right-hand-side of Eq. () is a scaled version of the identity operator, and the second term is a compact self-adjoint integral operator, and hence has a discrete spectrum with accumulation point at 0, or no accumulation point. After showing that the sum of these two operators is self-adjoint, we end up with only one accumulation point of the spectrum of.

We show below thatis self-adjoint under the assumption thatis bounded from above. In this case it must also be positive semi-definite (the proof is equivalent to the positivity of the combinatorial graph Laplacian). Consider the decomposition

The second term of Eq. () is a compact integral operator, and hence only has an accumulation point at, or has no accumulation point.
The first term is a multiplication operator by the real-valued function, so it is self-adjoint and its spectrum is the closure of. We suppose thatis bounded from below by someand also bounded from above. This shows thatis bounded and self-adjoint (as a sum of two bounded self-adjoint operators). Under these assumptions, it is reasonable to further assume that the spectrum ofin the interval, for some, is discrete, with accumulation point at. This happens, for example, ifis constant.

We now assume that the signalconsists only of frequencies in. This means that we can projectupon this part of the spectrum, giving a self-adjoint operator with discrete spectrum, and accumulation point of the eigenvalues only at.

Letbe a measurable weight function with. Suppose thatis continuous and varies slowly over. Namely, we assume thatfor every.
To generate a random geomertic graph ofnodes, we samplepointsindependently from the weighted probability measure. We connect nodetoby an edge if and only ifto obtain the adjacency matrix. Letbe the combinatorial Laplacian with respect to, andthe normalized symmetric Laplacian.
The number of samples inside the ball of radiusaroundis approximately. Hence, the degree of the nodeis approximately.

We consider the leadingeigenvectors the ones corresponding to the smallest eigenvalues ofin the interval, where. We order the eigenvalues in increasing order. Letbe the space spanned by theleading eigenvectors of, called thePaley-Wienerspace of. Letbe the projection upon the Paley-Wiener space of. Letbe the sampling operator, defined by.

Letbe a bounded signal over the metric space and suppose that. Denote.
By Monte Carlo theory, we have

So, in case the sampling is uniform. i.e.,, we have

pointwise. To make this precise,
let us recall Hoeffding’s Inequality.

Using Hoeffding’s Inequality, one can now show that there is an event of probability more thanin which for every, the error betweenandsatisfies

The fact that different graphs of different sizesapproximatewith different scaling factors means thatis not value transferable. Let us show thatis index transferable.

We now evoke the transferability theorem – a slight reformulation of Section 3 of Theorem 4 in.

For every leading eigenvalueofletbe the leading eigenvalue ofclosest to, where, if there are repeated eigenvalues,is chosen arbitrarily from the eigenvalues ofthat best approximate. Let

where

and

and suppose that.
For eachthere exists a Lipschitz continuous functionwith Lipschitz constantsuch thatandis zero on all other leading eigenvalues ofand all other eigenvalues of. Hence,

whereis the projection upon the space spanned by the-th eigenvector of, andis the projection upon the space spanned by the-th eigenvector of.

Now, by the transferability theorem,

By induction over, with base, we must havefor every.

Now, note that by standard Monte Carlo theory (evoking Hoeffding’s inequality again and intersecting events), we have

in high probability. Hence, by the fact that

andis bounded from below by the constant, we have

This shows that

which shows index transferability. Namely, for two graphs ofandnodes sampled from, with corresponding Laplaciansand, by the triangle inequality,

Next, we show value transferability for. Here,

Hence, ifis constant, we haveup to a constant that does not depend on. In this case, by a similar analysis to the above,is value transferable. We note thatis also index transferable, but value transferability is guaranteed in a more general case, where we need not assume a separable spectrum.

SECTION: Additional Details on Experimental Study
We describe the experimental setups and additional details of our experiments in Sec..
The experiments are performed on NVIDIA DGX A100.

SECTION: Semi-Supervised Node Classification
We provide a detailed overview of the experimental settings for semi-supervised node classification tasks, along with the validated hyperparameters used in our benchmarks.

We consider six datasets in node classification tasks, including Cora, Citeseer, Pubmed, Chameleon, Squirrel, and Actor.
The detailed statistics of the node classification benchmarks are summarized in Tab..
For datasets splitting on citation graphs (Cora, Citeseer, and Pubmed), we follow the standard splits from, using 20 nodes per class for training, 500 nodes for validation, and 1000 nodes for testing.
For heterophilic graphs (Chameleon, Squirrel, and Actor), we adopt the sparse splitting method from, allocating 2.5% of samples for training, 2.5% for validation, and 95% for testing.
The classification quality is assessed by computing the average classification accuracy across 10 random splits, along with a 95% confidence interval.

For GCN, GAT, SAGE, ChebNet, ARMA, and APPNP, we use the implementations from the PyTorch Geometric library.
For other baselines, we use the implementations released by the respective authors.

The hidden dimension is set to be either 64 or 128 for all models and datasets.
We implement our proposed Node-level NLSFs using PyTorch and optimize the model with the Adam optimizer.
To determine the optimal dropout probability, we search within the rangein increments of.
The learning rate is examined within the set.
We explore weight decay values within the set.
Furthermore, the number of layers is varied from 1 to 10.
The number of leading eigenvectorsin Index NLSFs is set within.
The decay rate in the Value NLSFs is determined using dyadic sampling within the set, the sampling resolutionwithin, and the number of the bands in Value NLSFs.
For hyperparameter optimization, we conduct a grid search using Optunafor each dataset.
An early stopping criterion is employed during training, stopping the process if the validation loss does not decrease for 200 consecutive epochs.

SECTION: Graph Classification
We provide a comprehensive description of the experimental settings for graph classification tasks and the validated hyperparameters used in our benchmarks. The results are reported in the main paper.

Consider a set ofgraphs, where in each graph, we havenodes for each graph,represents the edge set,denotes the edge weights, andrepresents the node feature matrix with-dimensional node attributes.
Letbe the label matrix withclasses such thatif the graphbelongs to the class, andotherwise.
Given a set ofgraphs, where, with the label information, our goal is to classify the set of unseen graph labels of.

We consider eight datasetsfor graph classification tasks, including five bioinformatics: MUTAG, PTC, NCI1, ENZYMES, and PROTEINS, and three social network datasets: IMDB-B, IMDB-M, and COLLAB.
The detailed statistics of the graph classification benchmarks are summarized in Tab..
We use the random split from, using 80% for training, 10% for validation, and 10% for testing.
This process is repeated 10 times, and we report the average performance and standard deviation.

For GCN, GAT, SAGE, ChebNet, ARMA, APPNP, and DiffPool, we use the implementations from the PyTorch Geometric library.
For other baselines, we use the implementations released by the respective authors.

The dimension of node representations is set to 128 for all methods and datasets. We implement the proposed Pooling-NLSFs and Graph-level NLSFs using PyTorch and optimize the model with the Adam optimizer.
A readout function is applied to aggregate the node representations for each graph, utilizing mean, add, max, or RMS poolings.
The learning rate and weight decay are searched within, the pooling ratio withinwith step, the number of layers withinwith step, the number of leading eigenvectorsin Index NLSFs within, the decay rate in the Value NLSFs using dyadic sampling within, the sampling resolutionwithin, and the number of the bands in Value NLSFs within.
The graph-level representation is then fed into an MLP with a (log)softmax classifier, using a cross-entropy loss function for predictions over the labels. Specifically, the MLP consists of three fully connected layers with 256, 128, and 64 neurons, respectively, followed by a (log)softmax classifier.
We conduct a grid search on the hyperparameters for each dataset using Optuna.
An early stopping criterion is employed during training, stopping the process if the validation loss does not decrease for 100 consecutive epochs.

SECTION: Additional Experimental Results
Here, we present additional experiments on node and graph classification benchmarks, ablation studies, runtime analysis, and uniform sub-bands.

SECTION: Semi-Supervised Node Classification
FollowingProtocol
We present additional experimental results for semi-supervised node classification using random splits, adhering to the protocol established by.
The results, summarized in Tab., demonstrate the classification accuracy across six benchmark datasets: Cora, Citeseer, Pubmed, Chameleon, Squirrel, and Actor.
Our-Node-level NLSFs achieve the highest accuracy on four out of the six datasets, outperforming other models significantly.
On Pubmed, it records a close second-highest accuracy, slightly behind APPNP. Our method achieves a competitive second place in the Actor dataset.-Node-level NLSFs demonstrate substantial improvements, particularly in challenging datasets like Chameleon and Squirrel.
The comparison models, including GCN, GAT, SAGE, ChebNet, ChebNetII, CayleyNet, APPNP, GPRGNN, and ARMA, varied in their effectiveness, with ChebNetII and APPNP also showing strong results on several datasets.
These findings highlight the efficacy of the-Node-level NLSFs in semi-supervised node classification tasks.

SECTION: Node Classification on Filtered Chameleon and Squirrel Datasets in Dense Split Setting
Following, we conduct additional experiments on the original and filtered Chameleon and Squirrel datasets in the dense split setting.
We use the same random splits as in, dividing the datasets into 48% for training, 32% for validation, and 20% for testing.
We compare the Node-level NLSFs using Laplacian attention with GCN, SAGE, GAT, GT, H2GCN, CPGNN, GPRGNN, FSGNN, GloGNN, FAGCN, GBKGNN, JacobiConv, and ResNetwith GNN models.
The study bydemonstrates the benefits of separating ego- and neighbor-embeddings in the GNN aggregation step when dealing with heterophily.
Therefore,also adopts this approach for the GNN aggregation step in GAT and GT models, denoted as “sep.”
The baseline results used for comparison are taken from.
Tab.presents the full performance comparison on the original and filtered datasets.
Note that Tab.is the same as Tab.but with more baseline methods.-Node-level NLSFs achieve the highest accuracy on both the filtered Chameleon and Squirrel datasets.
Additionally,-Node-level NLSFs demonstrate strong performance on the original Chameleon dataset, achieving the highest accuracy and the second-highest accuracy on the original Squirrel dataset.-Node-level NLSFs show less sensitivity to node duplicates and exhibit stronger generalization ability, further validating the reliability of the Chameleon and Squirrel datasets in the dense split setting.

SECTION: Ablation Study on-Node-level NLSFs,-Graph-NLSF, and-Pooling-NLSFs
In Sec., we note that using the graph Laplacianin Index NLSFs and the normalized graph Laplacianin Value NLSFs is transferable.
Since real-world graphs often fall between these two boundary cases, we present the Laplacian attention NLSFs that operate between them at both the node-level and graph-level.
Indeed, as demonstrated in Sec., the proposed-Node-level NLSFs,-Graph-level NLSFs, and-Pooling-NLSFs outperform existing spectral GNNs.

We conduct an ablation study to evaluate the contribution and effectiveness of different components within the-Node-level NLSFs,-Graph-level-NLSFs, and-Pooling-NLSFs on node and graph classification tasks. Specifically, we compare the Index NLSFs and Value NLSFs using both the graph Laplacianand the normalized graph Laplacianto understand their individual and Laplacian attention impact on these tasks.

The ablation study of-Node-level NLSPs for node classification is summarized in Tab..
We investigate the performance on six node classification benchmarks as in Sec., including Cora, Citeseer, Pubmed, Chameleon, Squirrel, and Actor.
Tab.shows that using the graph Laplacianin Index NLSFs (denoted as) and the normalized graph Laplacianin Value NLSFs (denoted as) has superior node classification accuracy compared to using the normalized graph Laplacianin Index NLSFs (denoted as) and the graph Laplacianin Value NLSFs (denoted as).
This is in line with our theoretical findings in App..
Moreover, the-Node-level NLSFs using the Laplacian attentionyield the highest accuracies across all datasets, corroborating the findings in Sec..
We also note that without Laplacian attention, the Node-level NLSFs (and) alone still achieve more effective classification performance compared to existing baselines, as shown in Tab..

Tab.demonstrates the ablation study of Graph-level NLSFs for graph classification tasks.
We examine the eight graph datasets as in Sec., including MUTAG, PTC, ENZYMES, PROTEINS, NCI1, IMDB-B, IMDB-M, and COLLAB.
Similar to the above, we investigate the Index and Value settings using graph Laplacianand normalized graph Laplacian, including,,, and, along with their variants using Laplacian attention.
Here, we see that-Graph-level NLSFs do not show significant improvement over the standard Graph-level NLSFs.
Notably,outperforms other models in social network datasets (IMDB-B, IMDB-M, and COLLAB), where the node features are augmented by the node degree.
We plan to investigate the limited graph attribution in future work.

The ablation study of-Pooling-NLSFs for graph classification is reported in Tab..
Similar to Tab., we consider eight graph classification benchmarks: MUTAG, PTC, ENZYMES, PROTEINS, NCI1, IMDB-B, IMDB-M, and COLLAB.
Tabl.demonstrates that using the graph Laplacianin Index Pooling-NLSFs (denoted as) and the normalized graph Laplacianin Value Pooling-NLSFs (denoted as) achieves superior graph classification accuracy compared to using the normalized graph Laplacianin Index Pooling-NLSFs (denoted as) and the graph Laplacianin Value Pooling-NLSFs (denoted as). This finding aligns with our theoretical results in App..
Moreover, the-Pooling-NLSFs using the Laplacian attentionyield the highest accuracies across all datasets, corroborating the findings in Sec..
In addition,-Pooling-NLSFs consistently outperform-Graph-level NLSFs as shown in Tab., indicating that the node features learned in our Node-level NLSFs representation are more expressive. This supports our theoretical findings in Sec..

The ablation study demonstrates that the Laplacian attention between the Index and Value NLSFs significantly enhances classification accuracy for both node and graph classification tasks across various datasets, outperforming existing baselines.

SECTION: Runtime Analysis
In our NLSFs, the eigendecomposition can be calculated once for each graph and reused in the training process.
This step is essential as the cost of the forward pass during model training often surpasses the initial eigendecomposition preprocessing cost.
Note that the computation time for eigendecomposition is considerably less than the time needed for model training.
For medium and large graphs, the computation time is further reduced when partial eigendecomposition is utilized, making it more efficient than the training times of competing baselines.
To evaluate the computational complexity of our NLSFs compared to baseline models, we report the empirical training time in Tab..
Our Node-level NLSFs showcase competitive running times, with moderate values per epoch and total running times that are comparable to the most efficient models like GCN, GPRGNN, and APPNP. Notably, Node-level NLSFs are particularly efficient for the Cora, Citeseer, and Pubmed datasets.
For the dense Squirrel graph, our Node-level NLSFs exhibit efficient performance with a moderate running time, outperforming several models that struggle with significantly higher times.

SECTION: Scalability to Large-Scale Datasets
To demonstrate the scalability of our method, we conduct additional tests on five large heterophilic graphs: Penn94, Pokec, Genius, Twitch-Gamers, and Wiki datasets from. The experimental setup is in line with previous work by. We use the same hyperparameters for our NLSFs as reported in App.. Tab.presents the classification accuracy. We see that NLSFs outperform the competing methods on the Penn94, Pokec, Genius, and Wiki datasets. For the Twitch-Gamers dataset, NLSFs yield the second-best results. Our additional experiments show that our method could indeed scale to handle large-scale graphs effectively.

SECTION: Index-by-Index Index-NLSFs and Band-by-Band Value-NLSFs
Our primary objective in this work is to introduce new GNNs that are equivariant to functional symmetries, based on a novel spectral domain that is transferable between graphs. We emphasize the unique aspects of our method rather than providing an exhaustive list of operators in this group, which, while important, is a key direction for future research.

Here, we present a new type of NLSFs: index-by-index Index-NLSFs and band-by-band Value-NLSFs. We denote them as follows:

whereare as before in Sec..
Note that these operators are also equivariant to our group actions.

We investigate index-by-index Index-NLSFs and band-by-band Value-NLSFs in graph classification tasks as described in Sec., including MUTAG, PTC, ENZYMES, PROTEINS, NCI1, IMDB-B, IMDB-M, and COLLAB datasets.
Unlike Graph-NLSFs, which are fully spectral and map a sequence of frequency coefficients to an output vector, index-by-index Index-NLSFs and band-by-band Value-NLSFs do not possess such a sequential spectral form.
In index-by-index Index-NLSFs and band-by-band Value-NLSFs, the index-by-index (or band-by-band) frequency response is projected back to the graph domain. Consequently, for graph classification tasks, we apply the readout function as defined for Pooling-NLSFs in Sec..

Following App., we examine index-by-index Index-NLSFs and band-by-band Value-NLSFs settings using graph Laplacianand normalized graph Laplacian, including,,, and, along with their variants using Laplacian attention, wheredenotes the pooling function as in Tab..

Tab.presents the graph classification accuracy using index-by-index Index-NLSFs and band-by-band Value-NLSFs.
It shows that incorporating Laplacian attention consistently improves classification performance, aligning with the findings in App..
We note that index-by-index Index-NLSFs and band-by-band Value-NLSFs perform comparably to existing baselines in graph classification benchmarks.
However, index-by-index Index-NLSFs and band-by-band Value-NLSFs are generally less effective compared to Pooling-NLSFs, as shown in Tab..

SECTION: Node Classification Using Diag-NLSFs and Lead-NLSFs
In App., we presented the diag-NLSFs, considering, and lead-NLSFs for leading filters that do not include orthogonal complements. We explore the leading filters, orthogonal complement, and diagonal operation in our NLSFs.
The results of these investigations are summarized in Tab., which shows the node classification accuracy achieved using various configurations, including NLSFs, diag-NLSFs, lead-NLSFs, and lead-diag-NLSFs.
We see that incorporating both the orthogonal complement and the multi-channel approach yields the highest classification accuracy.

SECTION: Uniform Sub-Bands
Our primary objective in this work is to introduce new GNNs that are equivariant to functional symmetries, based on a novel spectral domain transferable between graphs using analysis and synthesis.
Our NLSFs in Sec.consider filtersthat are supported on the dyadic sub-bands.
The closer to the low-frequency range, the denser the sub-bands become.
We illustrate an example of filterssupported on dyadic sub-bands withandin Fig., showing that sub-bands become denser as they approach the low-frequency range. Our primary goal is to highlight the unique aspects of our method rather than sub-band separation, which, while crucial, is an important consideration across spectral GNNs.
Therefore, we also present uniform sub-bands, where filtersare supported on the uniform sub-bands.
Note that the modifications required for our NLSFs are minimal, and most steps can be seamlessly applied with filters supported on the uniform sub-bands.

We evaluate our NLSFs with uniform sub-bands on graph classification tasks.
We consider the graph benchmarks as in Sec., including five bioinformatics datasets: MUTAG, PTC, NCI1, ENZYMES, and PROTEINS, and three social network datasets: IMDB-B, IMDB-M, and COLLAB.
Note the sub-bands only affect our Value-NLSFs, where Index-NLSFs remain the same as in Sec..

We report the graph classification accuracy of Graph-NLSFs and Pooling-NLSFs using uniform sub-bands in Tab.and Tab., respectively, where the,,, andare the index-based NLSFs and therefore they are the same as in Tab.and Tab..
Interestingly, the Laplacian attention does not yield significant improvements for either Graph-level NLSFs or Pooling-NLSFs when considering uniform sub-bands.
Moreover, we observe that the graph classification performance is generally worse than when using the dyadic sub-bands reported in Tab.and Tab..
We emphasize the importance of considering the spectral support of filters. Empirically, we found that using the dyadic grid is more effective, which is why we focus on it and report those results in the main paper. However, exploring other sub-bands remains an important task for future work. For instance, we plan to investigate sub-bands based onfor other choices ofand. In the limit when,andthis “converges” to the uniform grid.

SECTION: Additional Related Equivariant GNNs
Equivariant GNNs are designed to handle graph data with symmetries.
The output of an equivariant GNN respects the same transformation applied to the input.
Depending on the application, the transformation could involve translations, reflections, or permutations, to name but a few.
Due to respecting the symmetries, equivariant GNNs can reduce model complexity and improve generalization, which can be applied to test data and produce more interpretable representations.
When symmetry plays a critical role, such as in physical simulations, molecular modeling, and protein folding, equivariant GNNs have been demonstrated to be effective.
For example,employ spherical harmonics to handle 3D molecular structures, andsimplify computations for explicit rotation and translation matrices by focusing on relative distances between nodes.
In addition,embeds symmetry transformations by parameterizing convolution operations over Lie groups, such as rotations, translations, and scalings, through Lie algebra.