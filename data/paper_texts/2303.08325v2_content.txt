SECTION: FairAdaBN: Mitigating unfairness with adaptive batch normalization and its application to dermatological disease classification

Deep learning is becoming increasingly ubiquitous in medical research and applications while involving sensitive information and even critical diagnosis decisions.
Researchers observe a significant performance disparity among subgroups with different demographic attributes, which is calledmodel unfairness, and put lots of effort into carefully designing elegant architectures to address unfairness, which poses heavy training burden, brings poor generalization, and reveals the trade-off between model performance and fairness. To tackle these issues, we proposeFairAdaBNby making batch normalization adaptive to sensitive attributes. This simple but effective design can be adapted to several classification backbones that are originally unaware of fairness.
Additionally, we derive a novel loss function that restrains statistical parity between subgroups on mini-batches, encouraging the model to converge with considerable fairness.
In order to evaluate the trade-off between model performance and fairness, we propose a new metric, named Fairness-Accuracy Trade-off Efficiency (FATE), to compute normalized fairness improvement over accuracy drop.
Experiments on two dermatological datasets show that our proposed method outperforms other methods on fairness criteria and FATE. Our code is available athttps://github.com/XuZikang/FairAdaBN.

SECTION: 1Introduction

The past years have witnessed a rapid growth of applying deep learning methods in medical imaging[31].
As the performance improves continuously, researchers also find that deep learning models attempt to distinguish illness by using features that are related to a sample’s demographic attributes, especially sensitive ones, such as skin tone or gender.
The biased performance due to sensitive attributes within different subgroups is defined asunfairness[16].
For example, Seyyed-Kalantariet. al.[21]find that their models trained on chest X-Ray dataset show a significant disparity of True Positive Ratio (TPR) between male and female subgroups. Similar evaluations are done on brain MRI[17], dermatology[12], and mammography[15], which shows that unfairness issues exist extensively in medical applications.
If the unfairness of deep learning models is not handled properly, healthcare disparity increases, and human fundamental rights are not guaranteed. Thus, there is a pressing need on investigating unfairness mitigation to eliminate critical biased inference in deep learning models.

There are two groups of methods to tackle unfairness. The first group proceedsimplicitlywithfairness through unawareness[7]by leaving out sensitive attributes when training a single model or deriving invariant representation and ignoring them subjectively when making a decision. However, plenty of evaluations prove that this may lead to unfairness, due to the entangled correlation between sensitive attributes and other variables in the data, andstatistical differencebetween features of different subgroups. The second groupexplicitlytakes sensitive attributes into consideration when training models, for example, train independent models for unfairness mitigation[18,25]with no parameters shared between subgroups.
However, this may result in degraded performance because the amount of data for model building is reduced (see Table1).

It is natural to consider whether it is possible to inherit the advantages from both worlds, that is, learning a single model on the whole dataset yet still with explicit modeling of sensitive attributes.
Therefore, we propose a framework with a powerful adapter termedFair Adaptive Batch Normalization (FairAdaBN).
Specifically, FairAdaBN is designed to mitigate task disparity between subgroups captured by the neural network. It integrates the common information of different subgroups dynamically by sharing part of network parameters, and enables the differential expression of feature maps for different subgroups, by adding only a few parameters compared with backbones.
Thanks to FairAdaBN, the proposed architecture can minimize statistical differences between subgroups and learn subgroup-specific features for unfairness mitigation, which improves model fairness and reserves model precision at the same time. In addition, to intensify the models’ ability for balancing performance and fairness, a new loss function namedStatistical Disparity Loss(), is introduced to optimize the statistical disparity in mini-batches and specify fairness constraints on network optimization.also enhances information transmission between subgroups, which is rare for independent models.
Finally, a perfect model should have both higher precision and fairness compared to current well-fitted models. However, most of the existing unfairness mitigation methods sacrifice overall performance for building a fairer model[20,22].
Therefore, following the idea of discovering the fairness-accuracy Pareto frontier[32], we propose a novel metric for evaluating theFairness-Accuracy Trade-off Efficiency (FATE), urging researchers to pay attention to the performance and fairness simultaneously when building prediction models. We evaluate the proposed method based on its application to mitigating unfairness in dermatology diagnosis.

To sum up, our contributions are as follows:

A novel framework is proposed for unfairness mitigation by replacing normalization layers in backbones with FairAdaBN;

A loss function is proposed to minimize statistical parity between subgroups for improving fairness;

A new metric is derived to evaluate the model’s fairness-performance trade-off efficiency. Our proposed FairAdaBN has the highest(48.79), which doubles the highest among other unfairness mitigation methods (Ind, 22.63).

Experiments on two dermatological disease datasets and three backbones demonstrate the superiority of our proposed FairAdaBN framework in terms of high performance and great portability.

SECTION: 2Related Work

According to[4], unfairness mitigation can be categorized into pre-processing, in-processing, and post-processing based on the instruction stage.

Pre-Processing.Pre-processing methods focus on the quality of the training set, by organizing fair datasets via datasets combination[21], using generative adversarial networks[11]or sketching model[27]to generate extra images, or directly resampling the train set[18,28]. However, most methods in this category need huge effort due to the preciousness of medical data.

Post-Processing.Although calibration has been widely used in unfairness mitigation in machine learning tasks, medical applications prefer to use pruning strategies. For example, Wuet. al[26]mitigate unfairness by pruning a pre-trained diagnosis model considering the difference of feature importance between subgroups. However, their method needs extra time except for training a precise classification model, while our FairAdaBN is a one-step method.

In-Processing.In-processing methods mainly consist of two folds. Some studies mitigate unfairness by directly adding fairness constraints to the cost functions[28], which often leads to overfitting.
Another category of research mitigates unfairness by designing complex network architectures like adversarial network[30,14]or representation learning[5]. This family of methods relies heavily on the accuracy of sensitive attribute classifiers in the adversarial branch,
leads to bigger models and cannot make full use of pre-trained weights.
While our method does not increase the number of parameters significantly and can be applied to several common backbones for dermatology diagnosis.

SECTION: 3FairAdaBN

Problem Definition.We assume a medical imaging datasetwithsamples , the-th sampleconsists of input image, sensitive attributesand classification ground truth label. i.e..is a binary variable (e.g., skin tone, gender), which splits the dataset into the unprivileged group,, which has a lower average performance than the overall performance, and the privileged group,, which has a higher average performance than the overall performance. Using accuracy as the performance metric for example, for a neural network, our goal is to minimize the accuracy gap betweenandby finding a proper.

In this paper, we propose FairAdaBN, which replaces normalization layers in vanilla models with adaptive batch normalization layers, while sharing other layers between subgroups. The overview of our method is shown in Fig.1.

Batch normalization (BN) is a ubiquitous network layer that normalizes mini-batch features using statistics[10]. Letdenote a given layer’s output feature map, whereis the number of channels, width, and height of the feature map. The BN function is defined as:

whereis the mean and standard deviation of the feature map computed in the mini-batch,anddenotes the learnable affine parameters.

We implant the attribute awareness into BN, named FairAdaBN, by parallelizing multiple normalization blocks that are carefully designed for each subgroup. Specifically, for subgroup, its adaptive affine parameterandare learnt by samples in. Thus, the adaptive BN function for subgroupis given by Eq.3.

whereis the index of the sensitive attribute corresponding to the current input image,are computed across subgroups independently.

The FairAdaBN acquires subgroup-specific knowledge by learning the affine parameterand. Therefore, the feature maps of subgroups can be aligned and the unfair representation between privileged and unprivileged groups can be mitigated.
By applying FairAdaBN on vanilla backbones, the network can learn subgroup-agnostic feature representations by the sharing parameters of convolution layers, and subgroup-specific feature representations using respective BN parameters, resulting in lower fairness criteria.
The detailed structure of FairAdaBN is shown in Fig.1, we display the minimum unit of ResNet for simplification. Note that the normalization layer in the residual branch is not changed for faster convergence.

In this paper, we aim to retain skin lesion classification accuracy and improve model fairness simultaneously. The loss function consists of two parts: (i) the cross-entropy loss,, constraining the prediction precision, and (2) the statistical disparity lossas in Eq.4, aiming to minimize the difference of prediction probability between subgroups and give extra limits on fairness.

wheremeans the number of classification categories.

The overall loss function is given by the sum of the two parts, with a hyper-parameterto adjust the degree of constraint on fairness..

SECTION: 4Experiments and Results

SECTION: 4.1Evaluation Metrics

Lots of fairness criteria are proposed including statistical parity[7], equalized odds[9], equal opportunity[9], counterfactual fairness[13], etc. In this paper, we use equal opportunity and equalized odds as fairness criteria. For equal opportunity, we split it intoandconsidering the ground truth label.

However, these metrics only evaluate the level of fairness while do not consider the trade-off between fairness and accuracy.
Therefore, inspired by[6], we propose FATE, a metric that evaluates the balance between normalized improvement of fairness and normalized drop of accuracy. The formulas of FATE on different fairness criteria are shown below:

wherecan be one of.denotes accuracy. The subscriptanddenote the mitigation model and baseline model, respectively.is a weighting factor that adjusts the requirements for fairness pre-defined by the user considering the real application, here we definefor simplification. A model obtains a higher FATE if it mitigates unfairness and maintains accuracy. Note that FATE should be combined with utility metrics and fairness metrics, rather than independently.

SECTION: 4.2Dataset and Network Configuration

We use two well-known dermatology datasets to evaluate the proposed method.
The Fitzpatrick-17k dataset[8]contains 16,577 dermatology images in 9 diagnostic categories. The skin tone is labeled with Fitzpatrick’s skin phenotype. In this paper, we regard Skin Type I to III aslight, and Skin Type IV to VI asdarkfor simplicity, resulting in a ratio of.
The ISIC 2019 dataset[24,1,2]contains 25,331 images among 9 different diagnostic categories. We use gender as the sensitive attribute, where. Based on subgroup analysis,darkandfemaleare treated as the privileged group, andlightandmaleare treated as the unprivileged group.

We randomly split the dataset into train, validation, and test with a ratio of 6:2:2. The models are trained for 600 epochs and the model with the highest validation accuracy is selected for testing.
The images are resized or cropped to 128128 for both datasets. Random flipping and random rotation are used for data augmentation.
The experiments are carried out on 8NVIDIA 3090 GPUs, implemented on PyTorch, and are repeated 3 times. Pre-trained weights from ImageNet are used for all models. The networks are trained using AdamW optimizer with weight decay. The batch size and learning rate are set as 128 and 1e-4, respectively. The hyper-parameter.

SECTION: 4.3Results

We compare FairAdaBN with Vanilla (ResNet-152), Resampling[18], Ind (independently trained models for each subgroup)[18], GroupDRO[19], EnD[23], and CFair[29], which are commonly used for unfairness mitigation.

Results on Fitzpatrick-17k Dataset.Table1shows the result of these seven methods on Fitzpatrick-17k dataset.
Compared to the Vanilla model, Resampling has a comparable utility, but cannot improve fairness.
FairAdaBN achieves the lowest unfairness with only a small drop in accuracy.
Besides, FairAdaBN has the highest FATE on all fairness criteria.
This is because Ind does not share common information between subgroups, and only part of the dataset is used for training.
GroupDRO and EnD rely on the discrimination of features from different subgroups, which is indistinguishable for this task.
CFair is more efficient on balanced datasets, while the ratio betweenandis skewed.

Results on ISIC 2019 Dataset.Table1shows the results on ISIC 2019 dataset. FairAdaBN is the fairest method among the seven methods. Resampling improves fairness sightly but does not outperform ours. GroupDRO mitigates EOpp0 while increasing unfairness on Eopp1 and Eodd. Ind and CFair cannot mitigate unfairness in ISIC 2019 dataset and EnD increases unfairness on EOpp0.

*denotes, respectively.

Private implementation.

The FATE metric.Fig.2shows the values of FATE.
According to[3], the closer the curve is to the top left corner, the smaller the fairness-accuracy trade-off it has. The figure demonstrates that FATE has the same trend as this argument. We prefer an algorithm that obtains a higher FATE since a higher FATE denotes higher unfairness mitigation and a low drop in utility, and a negative FATE denotes that the mitigation model cannot decrease unfairness while reserving enough accuracy (not beneficial).

Limitation.Compared with other methods, FairAdaBN needs to use sensitive attributes in the test stage, which is unnecessary for EnD and CFair. Although this might be easy to acquire in real applications, improvements could be done to solve this problem.

SECTION: 4.4Ablation Study

Different backbones.Firstly, we test FairAdaBN’s compatibility on different backbones, by applying FairAdaBN on VGG-19-BN and DenseNet-121. Note that the first and last BN in DenseNet are not changed. The result is shown in Table2.
The experiments are carried out on Fitzpatrick-17k dataset.
The result shows that our FairAdaBN is also effective on these two backbones, exceptwhen using DenseNet-121, showing well model compatibility. However, we also observe a larger drop in model precision compared with the baseline, which needs to be taken into consideration in future work.

Different loss terms.We train ResNet by only replacing BNs with FairAdaBNs (the second row of the last part), and ResNet addingon the total loss (the third row of the last part).
The effectiveness of AdaBN is illustrated by comparing the first and second rows of the last part in Table2. By replacing BNs with FairAdaBN, ResNet can normalize subgroup feature maps using specific affine parameters, which reduceandbyand, respectively.
Comparing the second and fourth row of the last part in Table2, we find that by adding,decreases significantly, fromto.
Besides, although addingon ResNet alone increases fairness criteria unexpectedly, fairness criteria decrease when using FairAdaBN andsimultaneously. The reason could be the potential connection between FairAdaBN and, due to the similar form dealing with subgroups.

Hyper-parameter.Our experiments show thathas the best fairness scores and FATE compared toand. Therefore we selectas our final setting.

SECTION: 5Conclusion

We propose FairAdaBN, a simple but effective framework for unfairness mitigation in dermatological disease classification. Extensive experiments illustrate that the proposed framework can mitigate unfairness compared to models without fair constraints, and has a higher fairness-accuracy trade-off efficiency compared with other unfairness mitigation methods. By plugging FairAdaBN into several backbones, its generalization ability is proved. However, the current study only evaluates the effectiveness of FairAdaBN on dermatology datasets, and its generalization ability on other datasets (chest X-Ray, brain MRI) or tasks (segmentation, detection), where unfairness issues also exist, needs to be evaluated in the future. We also plan to explore the unfairness mitigation effectiveness for other universal models[31].

SECTION: References