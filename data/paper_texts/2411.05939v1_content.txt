SECTION: GCI-ViTAL: Gradual Confidence Improvement with Vision Transformers for Active Learning on Label Noise
Active learning aims to train accurate classifiers while minimizing labeling costs by strategically selecting informative samples for annotation. This study focuses on image classification tasks, comparing AL methods on CIFAR10, CIFAR100, Food101, and the Chest X-ray datasets under varying label noise rates. We investigate the impact of model architecture by comparing Convolutional Neural Networks (CNNs) and Vision Transformer (ViT)-based models. Additionally, we propose a novel deep active learning algorithm, GCI-ViTAL, designed to be robust to label noise. GCI-ViTAL utilizes prediction entropy and the Frobenius norm of last-layer attention vectors compared to class-centric clean set attention vectors. Our method identifies samples that are both uncertain and semantically divergent from typical images in their assigned class. This allows GCI-ViTAL to select informative data points even in the presence of label noise while flagging potentially mislabeled candidates. Label smoothing is applied to train a model that is not overly confident about potentially noisy labels. We evaluate GCI-ViTAL under varying levels of symmetric label noise and compare it to five other AL strategies. Our results demonstrate that using ViTs leads to significant performance improvements over CNNs across all AL strategies, particularly in noisy label settings. We also find that using the semantic information of images as label grounding helps in training a more robust model under label noise. Notably, we do not perform extensive hyperparameter tuning, providing an out-of-the-box comparison that addresses the common challenge practitioners face in selecting models and active learning strategies without an exhaustive literature review on training and fine-tuning vision models on real-world application data.

KDeep Active LearningVision TransformerLabel NoiseImage Classification

SECTION: Glossary
Deep Learning (DL), Active Learning (AL), Deep Active Learning (DAL), Multi-Head Self-Attention (MHSA), Convolutional Neural Network (CNN), Vision Transformer (ViT), Large Language Models (LLMs), Multi-Layer Perceptron (MLP).

SECTION: Introduction
While most works in literature often compare results to baseline active learning algorithms such as random selection or simple entropy-based selection, the extensive hyper-parameter tuning performed during training but often left out of the manuscripts leads to authors stating vastly different performances for the same CNN architecture, active learning algorithm, and label noise rate. This not only raises questions about the credibility of reported state-of-the-art results but can also delay actual progress in developing active learning schemes that are robust to label noise and achieve performances comparable to models trained on clean labels. The work on non-active training of DL models in the presence of label noise, as well as the training of DL models on noise-free datasets in an active learning setting, are well addressed in the literature. However, the intersection of these niches has a long way to go. AL algorithms seek to train an optimal model with minimal training data that is labeled iteratively.

The most common AL methods seek to explore diverse training examples or focus on samples that the DL algorithm is uncertain about. AL in the presence of label noise is a particularly challenging topic since training DL models with a higher concentration of incorrect labels presents problems for the back-propagation algorithm’s ability to converge as demonstrated in. It has also been shown that without sufficient training data, large models can memorize the noisy labels, and fail to generalize to the test set. Figuredepicts the basic AL framework for training DL image classifiers in the presence of label noise.

In this work, we compile a unified view of existing works on active learning for image classification with label noise. We are particularly interested in the fine-tuning CNN and ViT models pre-trained on the imageNet-1k dataset. We explore different DL model architectures and AL strategies on different datasets while varying the label noise rates up to 60%. We re-implement the commonly used baseline AL strategies namely: random query, maximum entropy, margin-based selection, model delta, and hybrid uncertainty sampling with diversity.

This work seeks to address the following:

In the realm of active learning, where the emphasis is often on query selection, and in image classification with label noise, which is typically trained without active learning, reliable benchmarks for active learning algorithms with label noise are scarce. To tackle this issue, we conduct experiments by training multiple deep-learning models for image classification using different active learning algorithms and varying levels of label noise. We present our findings by reporting test results on popular datasets, including CIFAR10, CIFAR100, Food101, and Chest X-ray images (pneumonia).

Given that the ViT-based models currently outperform CNNs in image classification on CIFAR10, and post competitive results on CIFAR100, Food101, Chest X-ray images (pneumonia), and other classification datasets, how does the ViTs compare to CNN-based models in an active learning setting with label noise (ALLN), and what can be done to improve on ViT learners in this setting?

Lastly, we propose an active learning scheme customized for the properties of the transformer network to improve on active learning in the presence of label noise. We also provide new insights based on using ViTs for AL under label noise and propose avenues to advance this work.

SECTION: Related Works
In this section, we highlight the current state of the literature on active learning, as well as active learning with label noise. We also highlight the use of the ViT for image classification and briefly discuss works that employ the ViT for active learning and label noise settings.

SECTION: Deep Active Learning with Label Noise
In most supervised machine learning use cases, there is an initial data collection and labeling cost, in both money and time. In some domains and tasks, datasets are inherently difficult to label for a variety of reasons, meaning more time is needed even by an expert human annotator to assign a label to each sample. In other cases the cost of hiring expert annotators is high, such as is the case in medical imaging, or the cost of producing the samples is high, such as is the case in experimental physics where observations come from costly telescopes or particle accelerators with limited access. This challenges the real-world use of machine learning systems, especially as unlabeled dataset sizes increase. While much progress has been made in improving self-supervised learning (SSL) methods to leverage large unlabeled datasets for extracting quality image embeddingsto be used on downstream tasks with little labeled data, these methods still fall short when directly applied to datasets with noisy labels in an active learning setting. Active learning is a machine learning paradigm, as depicted in Figure, that seeks to address the issues related to training ML models within a labeling budget, letting learning algorithms iteratively select a subsetof size, from a larger unlabelled datasetof size, to be labeled by an oraclefor training. However, the oracle may not always provide the correct label. The active learning mantra under label noise can be stated as follows: Train a machine learning model on a significantly smaller dataset that may containlabel noise, with little to no drop in test performance, while staying within a pre-determined labeling budget.

Most work in literature use random query, uncertainty sampling, and entropy-based sampling as baseline algorithms in comparing more complex methods for ALLN such as, where a mixture of information gain and uncertainty is used for query selection. Other works in the literature focus on reducing the labeling budget by using a mixture of weak and strong annotators as well as annotator abstention in the case of uncertainty. Despite these works posting promising results in budget optimization, there is little to no improvement in terms of the robustness to label noise and improved query selection. In, Huang et al. show that DAL is viable with oracle epiphany, which is to say the oracle is allowed to abstain from labeling samples they are unsure about until later on in the DAL cycle once they have seen enough examples to provide a more confident label. While their method is more realistic and leads to better performance, abstention may not always be possible in a fast-paced sector where lots of data is generated on a daily basis and requires urgent labeling. In, Yan et al. utilize abstention in DAL under label noise. A key difference in their work is that the algorithm need not be aware of either the abstention or noise rate. While there are obvious merits to the methods above, the solutions rely too heavily on the specific setup of the AL cycle and the human annotators to be trusted in the general setting. For these reasons and more, our work focuses on a more algorithmic approach to robustness and query selection.

Recent workdeploys a more data-centric approach to label noise for active label cleaning by ranking samples for label correctness and labeling difficulty. In, the authors propose a data-driven self-adapting DAL strategy that selects potentially noisy labels for correction in a manner that automatically avoids class imbalance in the labeled dataset with no prior knowledge of the class distributions. A thorough study of the literatureraises questions on the superiority and robustness of more complex DAL methods. The authors state that due to a lack of standardized benchmark settings in overlapping niches, performances by baseline models and AL queries in noisy labels tend to be understated. The use of grid search and related methods for hyper-parameter tuning can also aid in finding the ideal model parameters that show the superiority of a proposed AL model over the baseline methods. It is for this reason we opt to avoid any extensive hyper-parameter optimization that could skew the results of this study, and thus we report a realistic out-of-the-box benchmark in AL under label noise.

SECTION: Vision Transformer for Image Classification
In, Kolesnikov et al. present the Vision Transformer (ViT) as a CNN replacement for image classification tasks. They show that, in the extensive dataset regime, ViTs achieve higher classification accuracy, are more computationally efficient, and show no signs of saturation compared to CNNs such as ResNet and EfficientNet on increasingly larger datasets. The standard ViT architecture takes 16 by 16 patches from an image, flattens them, and applies a linear projection onto a higher dimensional space equal to that of the original text-based transformer. The patches are then marked for where in the image they were extracted, and so a second input to the transformer is the 1D positional embedding of each patch as shown in Figure. The spatial correlations of the patches are learned implicitly through their positional embeddings and self-attention vectors. The positional embeddings ensure the model learns both the relationships between pixels in the patches as well as the local and global 1D proximity representations of the tokens (image patches). Below we give an overview of the main transformer encoder and the attention mechanism within the ViT architecture.

The transformer encoder block in a ViT consists of multi-head self-attention (MHSA) and multi-layer perceptron (MLP) layer with normalization layers before and after MHSA each performing specific operations on the input tokens or subsequent later outputs. MHSA attempts to learn diverse features to capture semantical and contextual complexity in image patches while learning all these features in parallel for a given input. The parameterized MLP pools and aggregates the learned high-dimensional features from all the last layer attention heads, and compresses them into a lower-dimensional representation for a downstream task such as image classification, regression, and more. Figureillustrates the transformer encoder block architecture.

Given a sequence of input vectors, the self-attention mechanism calculates the similarity between the vectors. In the case of ViTs, starting with a sequence ofpatch embeddings of size,, whereand so, the goal of the self-attention mechanism is to learn a representation of each input token, expressed as a vector that is a weighted sum of the other token representations, where the weighting is based on how similar the tokens are to each other contextually. This similarity is measured by taking the dot product of any two embedding vectors, representing the cosine similarity between two vectors. To achieve this, three parameterized matrices are used: the Query matrix, the key matrix, and the value matrix, such that. To calculate the attention weights, the input sequenceis first projected into the query, key, and value representations through matrix multiplication to get,, and. Applying a softmax operation to the normalized dot product of the query and key projections of the input produces the attention weights. The final output of the attention layer is a weighted sum of the value representations, where the weights are the attention weights:

Extends the capabilities of self-attention by allowing the model to use multiple self-attention heads with different learned parameters in parallel over the same input. Learning using one attention would force the model to compress all learned information about an input image into one fixed-length vector. Using multiple attention heads allows the model to learn and express semantic and contextual complexity over several specializing attention heads simultaneously. A ViT can have one or more MHSA encoders with residual connections to maintain lower-level feature information throughout the network for context in higher-level attention blocks. In this work, we focus on the last layer of MHSA heads as well as the predicted class probabilities after the MLP is trained for classification.

Figureis an illustration of the ViT architecture, showing patch and positional embeddings, as well as the transformer encoder adapted from. Other ViT variations that achieve performances comparable to and in some cases better than CNNs on image classification tasks include Swin Transformer, Transformer in Transformer (TNT), DaViT-G, and Multiscale Vision Transformers (MViT). The Swin transformer adapts a similar overall structure as the ViT but uses shifted overlapping patches instead of rigid non-overlapping patches. The authors ofargue and demonstrate that using shifted window patches instead of non-overlapping patches limits the self-attention operations to locally related patches of an image. This reduces the complexity of the self-attention operations from quadratic to linear on the input. This architectural choice makes the Swin transformer more efficient than the original transformer while performing comparably in terms of top-1 classification accuracy. That said, the performance of ViTs and their variants such as the Swin transformer compared to CNNs has not received much attention in the noisy label domain or AL. For these reasons, we included both the base ViT and the Swin transformer in our experiments.

Previous works that adopt the ViTs for image classification in the AL domain include. Caramalau et al.introduce a novel AL query strategy that combines CNN layers for local dependencies and ViTs to capture non-local dependencies while jointly minimizing a task-aware objective. They achieve state-of-the-art performance on most AL-based benchmarks. Their method however suffers from scalability limitations due to ViT’s large parameter space and potential batch size restrictions in training. A similar conclusion is reached by He et al.. Their work demonstrates that, while ViTs produce informative and task-aware AL queries on CIFAR10 and 100, they are considerably larger than CNNs in terms of model parameters for them to be a viable replacement in DAL with the existing hardware in terms of training time. The work of Rotman and Reichartcompares different DAL methods on different text classification datasets using transformer-based models. While their work is not focused on image classification or the vision transformer, they demonstrate that transformer-based models tend to lead to inconsistent and poor results in the AL setting when using basic AL strategies. They show that query selection based on a transformer learner sometimes leads to the selection of clusters of neighboring outliers that destabilize training.

In this work, we introduce a novel DAL algorithm, GCI-ViTAL, that takes advantage of the ViT’s ability to capture complex local and global dependencies like. We define the C-Core attention vectors to help reduce the computational complexity for comparisons between labeled and unlabeled samples per AL circle. Like, we make use of the C-Core attention vectors as an informative and noise-aware approach to sample selection.

SECTION: Query Based on ViT Patch Similarity
Most AL approaches rely solely on the predicted probabilities from the trained model to form their query strategy. Label noise leads to a confused learner that outputs uncalibrated probability estimates of the samples, so selecting samples based on the predicted probabilities alone leads to the selection of non-optimal samples for each iteration and further corruption of the model through unstable gradients from incorrectly labeled samples. To address this issue, our proposed solution involves using the attention vectors from the last layer of a ViT model in query selection. This approach is based on finding specific features of a ViT that can help identify potentially mislabeled samples by comparing the labels of image samples that are close to each other in the last layer of attention representation maps. We start with a pre-trained ViT and fine-tune it using a small set of images with accurate labels, a common practice in many AL algorithms. Subsequently, we extract attention vectors for all the images in the initial set with accurate labels, and we use these to create core attention representations (centroids) for each class by aggregating the attention vectors of images belonging to the same class. This results in C-Core attention representation vectors that have been trained exclusively using accurate data for a C-class classification problem. The purpose of these core attention vectors is to help identify potentially incorrect labels during the AL process and reduce their impact on the model’s training. In each AL cycle, unlabeled images go through the fine-tuned ViT model, and their last layer’s attention vectors are collected. Our AL strategy combines the consideration of prediction uncertainty and attention-based diversity, which means we select samples that the current model is uncertain about while also striving to maintain a balanced representation of each class in the training dataset using the C-Core attention vectors.

SECTION: Handling Label Noise through Gradual Class-Centric Confidence Improvement
During training, an oracle receives a batch ofsamples to label, and before retraining the model with these labeled samples, a portion of them that deviate too much from the C-Core attention vector of the assigned class have their class assignment probabilities changed. We explore handling these examples in two ways, first by assigning the samples to the class dictated by the C-Core attention vectors, or, secondly by label smoothing. With label smoothing, we change the class probabilities assigned by the oracle for a sample so that it is not one-hot-encoded, but rather we introduce a positive probability for another class. Since we have the C-Core class centroids, we smooth the label by assigning a positive probability to the closest class centroid from the current sample in the attention vector space. For example, say samplewas selected for labeling, and the noisy oracle assigns it to classout of 10 classes. This means the probability distribution is given by. However, if we suspect this labeling to be incorrect based on the samplebeing far from the core attention centroid of class, we alter the probability distribution in such a way that we express less confidence in classbeing the correct label. If the closest centroid in attention vector space is that of class, we change the probability distributionto be, wherecontrols how much to trust the oracle as opposed to the C-Core attention vectors that compare images semantically.

Once the initial model is trained on the clean set, future batches for labeling are selected in a way that promotes class-centric confidence. We first calculate prediction uncertainty for each class based on the model’s confidence in its predictions. The AL strategy selects samples with the highest distance to their core attention class centroid, measured by the Frobenious norm. We then rank these from highest to lowest based on the Frobenious distance and take the top-for labeling. As the model improves its performance on the validation set, we shift towards selecting samples with high uncertainty and gradually decreasing reliance on the class centroid distance to the image representation, ultimately focusing on refining the model’s understanding of each class.

The selection based on the distance from the class centroid ensures we select samples that have underlying attention maps that are not similar to most seen in the clean training set for that class, which are effectively semantically hard examples. We want these to be sent to the oracle for labeling. Our proposed AL strategy for image classification in the presence of label noise by leveraging attention vectors is summarized in Figuresand. The detailed description of the steps in Figureis provided in Section.

In summary:

Most AL strategies rely solely on model-predicted probabilities and the oracle’s labels, which may lead to suboptimal sample selection and unstable gradients due to label noise.

Our approach uses the last-layer attention vectors (C-Core attention vectors) of the ViT, to identify potential images with semantic similarity that have opposing oracle labels. These are potentially incorrectly labeled. We reduce the effects of label noise through label smoothing by combining prediction uncertainty and the distance to the most likely C-Core attention vector.

We employ a Gradual Class-Centric Confidence Improvement strategy, initially selecting samples with a high Frobenius norm concerning their supposed class centroid attention maps for low-confidence classes. Gradually, we shift our strategy towards samples with a low Frobenius norm to better understand the classes. This approach helps in the selection of semantically challenging examples and reduces the impact of label noise.

In the rest of this section, we provide the theoretical formulation of the problem, the algorithm, and the theoretical justification for our approach.

SECTION: Joint Entropy-Attention Active Learning
Starting with: (1) An initial set oflabeled images, referred to as the “clean labeled set”, consisting of randomly selected images. This set is denoted asand should be large enough to ensure the representation of at least a few examples from each of theclasses. (2) A base Vision Transformer (ViT) denoted by. The ViT has been pre-trained on ImageNet-1k, and has the fully connected layers changed to suit the number of classes. (3) Iteratively fine-tune the ViT on a collection ofimages with the highest prediction entropyand ViT hidden layer attention heads distance to their supposed centroid attention heads, i.e., the images which jointly maximize Equationsand:

whereis a suitable distance measure,is the number of classes, andis the predicted class of. We also have:

theattention vectorsproduced by running imagethrough the fine-tuned model and

where

is an entry in an indicator vector, where each entry is 1 if the imagebelongs to class. Consequently, the denominator in Equationrepresents the number of images in the initial clean labeled set that belong to class.

Equationrepresents each of the C-Core attention maps or centroids for all images in the clean labeled random set belonging to the same class. Note that these class centroids are produced only after fine-tuning on this labeled set with no label noise. These represent our best estimate of the truth at this point before oracle noise influences our AL strategy.andare each a collection ofvectors in the case of, for each of the 12 attention heads, of embedding size of

We will use the notationand leave out the 768 dimension going forward, however, eachwill be treated as a matrix containing attention vectors.
Calculating the distance between matrices is typically done by using the Frobenius norm, the spectral distance, or the Kullback-Leibler divergence.is calculated by summing the squares of the differences between all the elements of the two matrices and then taking the square root of the sum, similar to thedistance between vectors.distance quantifies how different two matricesandare based on the largest eigenvalue of the difference matrix between them.treats matrices as probability distributions and measures the amount of work to be done in transforming one distribution into another. Similar to the works inwe argue the Frobenius norm between the latent representations of input images or text can improve discriminability. Unlike these works, we however do not use the Frobenious norm as a parameterized regularizer in training the network, but rather as part of our AL query strategy. Since we are comparingcentroid matrices toimage attention maps, the Frobenius norm also becomes a natural choice in calculating the distance in the semantically aware representations learned by self-attention. The norm is given by:

To have a combined objective for optimization, we scale the distance to be in the same range as the entropy. For a two-class image classification problem, the entropy is between 0 and 1, but for a multiclass problem, the entropy is between 0 and, whereis the number of classes. An easy way to achieve this for a loss with rangeis to first rescale values
to the rangeand then scale all outputs to. This would then yield a final range:. For image, we calculate, the distance to each clean set cluster centroid to get vector distances to all theclasses. We then apply a softmax function that converts this into a probability distribution over the classes. This is to say, based on features alone, which class is most semantically similar to the image in question. We then multiply this probability vector byto standardize it to the magnitude of the entropy selection criterion. We introduce a weight, that balances and controls the influence of entropy versus class-based feature similarity. The final objective is given by:

Combining equations, throughwe get the following:

Algorithmbelow shows the pseudo-code for GCI-ViTAL, the AL query strategy we propose in this work in the presence of label noise.

A pre-trained ViT model

A small initial random setofimages with accurate labels from all classes

A set of unlabeled images

A labelling oraclewith know n-symmetric label noise rate

A number of samples to label,, per labeling cycle

A weightto balance the influence of entropy versus class-based feature similarity

SECTION: Theoretical Analysis
In this section, we investigate the theoretical relationship between the predicted class probability distribution entropy, the Frobenius norm of the final layer ViT attention heads of the potential AL query candidates, and the C-Core attention head vectors. We then analyze the relationship between our strategy and increased label noise.

Starting with the self-attention head outputs of samples in the clean initial random sample:

for aclass image classification problem, we first calculate the C-Core attention vectors as the mean attention vector representation of all images in each class. These are synonymous with cluster centroids. The Frobenius norm component in the query strategy in Equationmeasures the disparity between the attention heads of a given image and the C-Core attention vector of the predicted class. By doing this, part of the query selection strategy prioritizes data points where the attention heads in the final layer of the model capture information that is distinct from the C-Core attention vectors of the predicted class. Such images represent novel and unseen variations within a class based only on the attention mechanism. The entropy component of the query strategy on the other hand only uses the predicted class probabilities in ranking samples. Entropy quantifies the uncertainty in a model’s predictions for a given image. Images with high prediction entropy are often not well represented in the dataset and thus selecting these samples provides more information. The combination of entropy and the Frobenius norm thus strikes a good exploration and exploitation balance of the image space. This leads to queries that gradually build a model trained on diverse data samples based on the semantical representation of the images through the use of the C-Core attention vector representation of samples, while gradually building up the model’s confidence around the decision boundaries between classes based on both semantics as well as filtered label information from the oracle.

In this section we show how label smoothing potentially incorrect oracle labels helps reduce the adverse effects of incorrect labels during training. Mathematically, we can express label smoothing as a form of regularization or penalty-based learning. In the case of multi-class image classification using the cross-entropy loss, consider the following scenario: For a single training examplewith predicted probabilities, the cross-entropy loss without label smoothing is given by:

whereis the one-hot encoded label from the oracle, andis the predicted probability distribution by the model. With label smoothing the cross-entropy loss is given by:

whereis the smoothing parameter.

The termin the loss is a regularization term that penalizes the model for being too confident in noisy label settings, thus forcing it to learn more robust decision boundaries between classes. However, in this case,is shared between the otherclasses to distribute uncertainty amongst them equally. In our case, since we have the C-Core attention vector per class, we only label smooth by assigning the C-Core selected class a non-zero probability. This means that each sample that the oracle assigns to a class other than that we would assign using the Frobenious distance between the image’s attention vectors and the C-Core vectors is label smoothed to reflect lower thanconfidence in the oracle’s label, and exactlyconfidence in the class based on the distance to the C-Core attention vectors. Mathematically, we add an indicator variable to Equationso that all other classes that are not the C-Core prediction do not get their zero probability adjusted. The cross-entropy loss based on C-Core Frobenious label smoothing is given by:

where

andrepresents the class that would be assigned based purely on the Frobenious norm using the C-Core attention vectors. Focusing on the terms of Equation, we see the loss is large when the disparity betweenandis large. Let us consider two cases, a perfect model (one that can fully predict the underlying ground truth) and a random model (one that assigns random labels for any input). All other scenarios lead to a model contained in the search space of models we seek to optimize. Assuming the perfect model producesapproximately equal to the ground truth label distribution, then the loss changes based on the noise rate of the oracle. At noise rate, the loss, and as.

Comparatively, the smoothed loss in Equationfirst reduces the confidence placed on the oracle’s labels by a small percentage, and then encourages reliance on the C-Core attention vectors by reducing the additional loss term by a factor ofwhenever the predicted class by the model agrees with the C-Core attention vector based assignment. Assuming a perfect model again, in this case at noise ratethe first term in the loss, while the second term is alwaysdue to the indicator variable. Looking at this differently, the smoothed loss uses both the oracle’s labels and the C-Core vectors as the ground. As, a larger proportion of the oracle’s labels is incorrect, meaning the first term of the loss will explode to infinity as is the case with cross-entropy, while the second term is independent of the noise rate and heavily contributes to the loss for deviations from the C-Core attention vectors based assignment. This means at high label noise rates, the smoothed loss leads to weight updates with reduced influence from noisy labels and thus provides more robust learning.

Now assuming a random model, for low noise rates, the oracle’s labelsare correct, andis a vector with entries. The cross-entropy loss, and as. The smoothed loss has the same properties as the cross-entropy loss in both low and high label noise rate settings when the model randomly assigns labels. In summary, if we start with a poorly performing model after training on the clean random set, we expect no performance gains in using a smoothed loss function in both low and high-label noise settings. On the other hand, starting with a strong model and thus representative C-Core attention vectors, we have shown that label smoothing reduces the adverse effects of significant label noise rates using our custom loss function. In the next section, we present the experimental setup, models trained, AL strategies tested, datasets used, and the training configurations.

SECTION: Experimental Setup
In this section, we describe the experimental setup that forms a high-dimensional grid of different configurations in active learning for image classification in the presence of label noise. We vary several DL architectures (4), AL algorithms (6), and benchmark datasets (4), as well as the Oracle label noise rates (4). Two of the DL models used in this work are CNN-based while the other two are ViT-based. All CNN-based models are trained in an AL setting with 5 of the 6 AL strategies, and all datasets over all the 4 label noise rates. The 6th AL strategy is GCI-ViTAL and is unique to ViTs. The ViT models in this work are trained under all six AL strategies, all datasets, and label noise settings.

SECTION: Deep Learning Models
For all four DL architectures in this work, the weights are transferred from the pre-training of the model on the ImageNet-1k dataset. We then fine-tune the fully connected layer for classification. The CNN-based models used in this work are: ResNet34and VGG19, chosen for their popularity in image classification benchmarks as well as good performance. The ViT-based models of choice in this work are the base ViT with 14 non-overlapping 16 by 16 patches and 12 attention heads, and the Swin transformer, which implements overlapping shifted patches as opposed to a grid of rigid patches. In Tablewe show the model sizes in megabytes, as well as the number of frozen and trainable parameters used in our experiments.

SECTION: Active Learning Algorithms
We compare the following standard DAL query strategies: random query, information entropy-based selection, margin sampling, hybrid uncertainty and diversity, model delta, and ours, GCI-ViTAL. Due to a large number of experiments as well as training time, we limit AL strategies to the above six. We briefly explain each method below:

This is the simplest query strategy, while also relatively effective. This AL strategy simply selects candidate images for labeling with equal probability from the unlabeled dataset. It does not take into account any information about the unlabeled data except its size, so it is likely to select samples that are not very informative for the model.

This query strategy selects samples with the highest information entropy, which is a measure of uncertainty. Information entropy is high when the model is not confident about the predicted class of a sample. Selecting samples with high information entropy can help the model learn class boundaries, hence improving its performance.

This query strategy selects samples with the smallest margin between the predicted scores of the two most likely classes. The margin is a measure of the confidence of the model in its prediction. Selecting samples with small margins helps label samples that better define decision boundaries of very similar classes.

This query strategy selects samples in a way that balances uncertainty and diversity from the labeled data. The uncertainty is measured by the information entropy of the sample, and diversity is measured by the distance of the sample to the labeled data in pixel space.

This query strategy simply selects samples that are most likely to change the model’s predictions if they were labeled. The model delta is calculated by comparing the predicted scores of two consecutive model states, that is the previously trained model at timeand the model at time. Samples with the highest change in the class probability distribution are selected for labeling.

This query strategy uses the final transformer block’s output to select samples that are hard to classify to the current model, and are semantically different from their supposed class average image. This potentially means the points are a special case, most likely close to a decision boundary, and thus worth obtaining a label for from the oracle. This strategy has the advantage of considering the semantical similarities of images while making sample selections, and not only relying on the trained model, which is prone to adverse effects due to label noise.

SECTION: Datasets
Due to the lack of benchmarks on DAL with label noise using multiple datasets for image classification, we run experiments on the following datasets: Chest X-ray Images (Pneumonia), Food101, CIFAR10 and CIFAR100. Tablebelow is a brief summary of each dataset. Figuresandpresent sample image examples from each dataset.

For all the datasets, we usetrain, validation, and test splits as pre-split in the CIFAR 10 and 100 datasets. All of these data splits are kept mutually exclusive to each other. The process for injecting label noise and simulating an oracle is as follows: First, we put aside a clean test set that is the same size as what is commonly used for each dataset. We also draw a clean random sample, without replacement, of 1024 images and their labels from the training set (independent of the number of classes), that will be used as an initial clean set for active learning as is customary. The training set labels are then corrupted with four levels of noise, by replacing the actual label of a sample with a randomly selected class, where each class has an equal probability of being selected. Once class-independent label noise has been injected, we use the number of training samples for validation that correspond toof the total dataset. On the active learning training scheme, no fixed labeling budget is set; we train and evaluate the models on the test set after each batch of AL selection and oracle labeling until the entire training set has been used. This has the advantage of allowing for analysis of model performance in both low-budget and high-budget active learning regimes in the context of a noisy oracle. This means we never stop the AL algorithm based on the labeling budget in developing the best algorithm. We, however, monitor test performance across labeling budget ticks to be able to compare models and AL query strategies for different datasets and noise labels on varying budgets.

SECTION: Training Configuration
While the training procedure is straightforward in a theoretical framework, the implementation details contain non-trivial components. For one, training in TensorFlow allows for more control of the training loop and thus makes it easier to incorporate active learning and noise injection. However, this comes with a disadvantage when it comes to training efficiency on a GPU. While PyTorch makes training on a GPU simple with data loaders, the process complicates the active learning portion of the cycle as the indices that come out of the data loader do not directly correspond to the indices in the dataset as all operations take and spit out tensors of size, each represented by one index value. We incur extra compute in remapping the indices to match the AL format. We make use of PyTorch’s distributed data parallelism and train on two NVIDIA RTX A4000 and two RTX A5000 GPUs, each with approximately 16 Gigabytes of RAM, based on availability on one compute node.

In training both CNN and ViT models, a learning rate scheduler is used, that reduces the learning rate byafter 10 epochs of no improvement on the validation set. We also train with the early stopping of tolerance 5 epochs, meaning we halt training if the model hits a running 5 epoch validation loss maximum. This is a good indication that the model is beginning to overfit the training data. We select samples individually and not the best batch for AL algorithms that are not batch-based. We use the cross-entropy loss across all models except on the GCI-ViTAL AL strategy where we use a C-Core attention vector label smoothed cross-entropy loss. We use the Adam optimizer for ResNet and ViT. Stochastic gradient descent is the recommended optimizer for VGG19 based on the original paper and the choice in this work. The image transformations such as random cropping, image resizing, and pixel normalization we use for each model are those used by the authors of the respective models on the datasets we focus on in this work. This is sometimes necessary as the model architecture asserts a specific image size, such as is the case with base ViT, which expects images of size. Images were resized tobyfor the CNN-based models and resized tobyfor the ViT models. For both CNN and ViT networks, training halts when all training samples are used, but we monitor the AL result after each labeling round.

Figuresandpresent the adjacency matrix as well as real to corrupt label distribution in a given AL query selection cycle on CIFAR10 and CIFAR100 respectively. These figures demonstrate that we inject the same uniform label noise irrespective of the AL strategy, and thus the difference in performance will be driven mostly by the informativeness of the samples for most AL strategies, but the C-Core smoothed loss function as well in the case of GCI-ViTAL.

SECTION: Results
In this section, we present the test results of different DL models trained and tested on CIFAR10, 100, and the Chest X-ray (Pneumonia) datasets. We compare the results of multiple DAL algorithms including our proposed GCI-ViTAL, and show results for different label noise rates. We also compare the DAL algorithms on the test performance per labeling budget level. This helps show how the DAL algorithms compare in both low- and high-budget settings in the presence of label noise. Test performance against labeling budget plots can be found in appendix

The first finding in our study that has support in the literature for non-active learning image classification is the superiority of ViTs over CNN-based models in label noise DAL. We observe that for three of the datasets in Tables,, andin this study, transformer-based models (ViTb16 and SwinV2b16) performed significantly better than CNN-based models (ResNet34 and VGG19) out of the box, across all AL strategies and label noise rates. The performance difference is more pronounced in CIFAR100. CIFAR100 has 10 times more class labels while maintaining the same number of training examples as CIFAR10, while the Chest X-ray dataset presents complexity in the input image since the images are of much higher resolution than CIFAR10 as seen in Table, and require expert opinion to tell positive and negative classes apart. There is very little difference in performance of the DAL methods on the Food101 ddataset across label noise rates.

With regards to the DAL strategies across datasets, it is worth noting that, contrary to most literature, and in direct support of the findings in the following works, we find that random query selection, while simple to implement and computationally efficient, posts comparable or superior classification accuracy results in DAL with label noise compared to margin, entropy, hybrid, and model delta based selection when out-of-the-box DL models are used without extensive model hyper-parameter optimization. We find this to be especially true for higher label noise rates. Our results show that sample selection that is only guided by a model trained on partially noisy labels selects non-optimal samples compared to random selection. This is not the case for GCI-ViTAL since the C-Core attention component of the AL query strategy does not get updated with noisy information once trained on the clean initial subset of the data, leading to noise resilience.

Model delta, unlike all the other AL strategies in this study, selects samples based on the delta confidence of the previous model and the most recent model update. Samples with a large model confidence change are selected for labeling. We find that model delta AL performs better than entropy, margin, and hybrid in no-label noise settings, and tends to perform worse in increasing label noise settings. The empirical results in the high label noise setting make intuitive sense in that the introduction of label noise during training leads to a discrepancy between the predicted probabilities of consecutive models especially earlier in the training so that there could be model collapse. This behavior only persists in CNN-based models, i.e. model delta performs on par with all other AL strategies for zero label noise, but up to 45% worse at 60% label noise rate. Model delta however performs on par with other baseline AL strategies even for high label noise rates when the learner is a transformer-based model. This sheds more light on the robustness and high calibration of transformer-based models.

GCI-ViTAL’s higher generalization performance at high label noise rates comes at a higher computational cost in attaining the samples to be labeled as well as computing its custom loss function that is based on the C-Core attention assignment for each sample. We found that GCI-ViTAL runs as slow as model delta and approximately 2 times slower than entropy-based selection, margin, and hybrid uncertainty & diversity selection. The difference in total runtime increases in the case of GCI-ViTAL since for each new retrained model, the attention maps are computed over the entire unlabeled dataset and stored for computing the Frobenious norm to the C-Core vectors and selecting the next training samples until the labeling budget is exhausted. Tableshows the average runtime for each DAL algorithm per AL cycle. The random query strategy has the fastest runtime as expected since it does not depend on the input images nor the predicted probabilities from the currently trained model. However, we see a benefit in using GCi-ViTAL in the high-label noise regime with a large label budget.

We empirically observe that GCI-ViTAL’s robustness under label noise is more pronounced with the increasing number of classes in the classification problem. On CIFAR100 and Food101, ViT with GCI-ViTAL performs equal or marginally better than all other AL strategies under 40% and 60% label noise as can be seen in Tables,and Figures,,, andas opposed to performance on CIFAR10 with 10 classes shown in Figuresand, as well as in the Chest X-ray dataset with just 2 classes shown in Figures, and. See Appendixfor all test accuracy plots.

We hypothesize this is due to label smoothing guided by the C-Core Frobenious norm in the following manner: As the label noise rate increases up to 60% in a 100-class problem, it means each sample selected for labeling is 60% likely to be labeled erroneously as one of the other 99 classes. This leads to incorrect weight updates and thus large training errors. However, with GCI-ViTAL, the loss function is partially grounded in the small clean label distribution to redistribute the oracle’s labeling in such a manner it agrees with the pre-trained model’s semantical assignment of images to classes based on the learned representations from clean pre-training. Recall from Sectionthat while the loss is designed to tackle label noise, the C-Core assignment is not affected by label noise as we only train the MLP layer of the ViT on noisy labels, and not the feature extraction multi-head self-attention layers.

SECTION: Discussions
We show that GCI-ViTAL performs equal or marginally better than most of the AL strategies for lower label noise rates, statistically on par with random selection on low label noise rates, and observably better on higher label noise rates, particularly on CIFAR100 where 60% label noise rate means any sample has a 60% chance of being mislabeled as one of the other 99 classes. We attribute this to GCI-ViTAL possessing an in-build pseudo-memory bank of high-accuracy mappings from the C-Core attention maps to labels that come in handy when the oracle’s labels are very noisy. The C-Core attention vectors help in selecting outlier samples for a supposed class, and at the same time discount the confidence in the oracle’s label under high label noise if it does not match that of the C-Core Frobenius norm-based assignment. In this case, we can show that the additional use of the attention maps learned during clean label pre-training on ImageNet through multi-head attention is advantageous over random selection under high label noise rates. We do this without heavily searching the hyper-parameter space of the underlying DL model in such a manner that would benefit our AL strategy over baseline models, thus reflecting a fairer comparison under label noise.

As the label noise rate increases from 0% up to 60%, the test accuracy for all models and AL strategies declines considerably for CIFAR10 and CIFAR100, and only marginally for the Chest X-ray and Food101 datasets. Still, the decline in performance is steeper for the CNN-based models as compared to their transformer-based counterparts. One reason for this could be that the pre-training in the transformer captures adequate local dependencies while at the same time learning rich global dependencies between pixels so that fine-tuning on noisy labels has a less detrimental effect on the overall probability outputs of the model. This is not the case with CNN-based models that have an inductive bias to prioritize local dependencies in explaining the differences in class labels, thus making the transformer more robust to label noise than CNN-based models. To contrast the two in the face of high label noise, CNNs learn a smoother decision boundary between classes while transformers learn a more complex decision boundary. This is because ViTs are larger models with considerably more parameters than CNNs as shown in Table, and more parameters allow for a more fine-grained complex decision boundary between classes. The more parameters of transformers allow for the decision boundary to change due to label noise but only change so insignificantly that the change does not lead to the misclassification of many neighboring samples. However we note that the ViT is more robust to label noise in most cases that require complex decision boundaries since it has a smaller trainable-to-frozen parameters ratio, meaning it is forced to condense a lot more of the signal into useful weights, thus it is less likely to overfit to noisy training data as compared to CNN-based models. This leads us to the idea of aiming for larger frozen parameter models for feature extraction and low trainable parameters as future work.

While the use of ViTs in active learning with label noise shows promising results, the high computational cost is a disadvantage that needs to be addressed. Possible ways of addressing this include deploying ViTs with fewer layers or reducing model size through methods such as quantizationand model distillation. We are also interested in exploring how ViT models of different sizes (small, base, and large) are impacted by label noise. This can be especially important in navigating the tradeoffs in performance and computational complexity. The more fundamental question that is a result of this work is: in the context of high label noise, at what point do the active learning and few-shot learning paradigms converge, what are the factors, and what role can weak or self-supervised learning play in improving generalization and robustness. Last but not least, in the wake of the advances in large language models (LLMs) as well as multi-modal learning, how can we leverage LLMs to produce text-guided active learning strategies, guide the oracle through caption generations, and create explainable ViT applications in the label noise domain? These ideas are partially inspired by the following works.

SECTION: Limitations
We note that while this work addresses the use of out-of-the-box DL models to give a good reflection of realistic AL baselines, it lacks in providing a comprehensive comparison of the proposed method (GCI-ViTAL) to state-of-the-art AL methods with label noise. The work also does not address or show results of GCI-ViTAL on more complex real-world datasets outside the datasets covered due to time and resource constraints.

SECTION: Conclusion
In conclusion, this study provides valuable insights into the performance of various deep learning models trained and tested on CIFAR10, CIFAR100, Food101, and the Chest X-ray (Pneumonia) datasets, particularly in the context of active learning with label noise. We found that transformer-based models, such as ViT and Swin Transformer, consistently outperformed CNN-based models across all datasets and AL strategies, indicating their superiority in handling complex image classification tasks. We attribute ViT’s robustness to their ability to capture both local and global dependencies, resulting in more complex and elastic decision boundaries that are less affected by incorrectly labeled training samples. Furthermore, our results challenge conventional wisdom by showing that random query selection often yields superior classification accuracy in AL with label noise compared to more complex AL strategies that rely solely on uncertainty and diversity metrics. Despite these findings, it is worth noting that transformer-based models incur higher computational costs compared to CNN-based models. GCI-ViTAL, our proposed AL strategy, exhibits higher generalization performance at high label noise rates, albeit with increased computational cost. This strategy leverages the inherent robustness of transformer-based models by making use of the semantic relationships of images without an oracle’s labels.

This work opens up several avenues for future research. Exploring methods to mitigate the computational overhead of transformer-based models, such as deploying smaller ViT models or applying model quantization and distillation techniques, could be a promising direction. Additionally, investigating the convergence of active learning and few-shot learning paradigms in the context of high label noise, as well as exploring the role of weak or self-supervised learning to improve generalization in label noise scenarios could provide further insights into enhancing model performance. We are also interested in investigating how ViT model size impacts DAL under label noise. In summary, our study contributes to the current understanding of DL models’ behavior under DAL scenarios with label noise and suggests potential avenues for addressing challenges in this domain.

SECTION: Acknowledgement
We acknowledge technical support and computing resources from The University of Hawaii Information Technology Services – Cyberinfrastructure funded in part by the National Science Foundation CC* award2201428.

SECTION: References
SECTION: Test Accuracy plots