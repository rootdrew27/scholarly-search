SECTION: Multiview learning with twin parametric margin SVM
[inst1]organization=Department of Mathematics, Indian Institute of Technology Indore,addressline=Simrol,
city=Indore,
postcode=453552,
state=Madhya Pradesh,
country=India

SECTION: Introduction
Support vector machine (SVM)is a widely acclaimed and proficient machine learning technique employed for classification and regression problems. SVM strives to maximize the margin between two classes, aiming to find the optimal hyperplane between two parallel supporting hyperplanes by solving a quadratic programming problem (QPP). A QPP is the process of solving certain mathematical optimization problems involving quadratic functions. QPP is minimizing or maximizing an objective function subject to bounds, linear equality, and inequality constraints. Over the decades, SVM has been effectively employed across a range of domains, including forest fire detection, pattern recognition, significant memory concern (SMC), Alzheimer’s disease diagnosis, and so on. Although SVM has made significant strides in various domains, there is still considerable potential for enhancement. A notable obstacle associated with the standard SVM is the substantial computational intricacy associated with solving the QPP. In order to reduce the computational complexity of SVM,andintroduced the generalized eigenvalue proximal SVM (GEPSVM) and the twin SVM (TSVM), respectively. GEPSVM solves the generalized eigenvalue problem instead of QPP, and TSVM tackles two smaller QPPs as opposed to a single large one, resulting in TSVM being four times faster than the standard SVM. This firmly establishes TSVM as the superior choice over SVM. Subsequently, a range of advancements to TSVM is introduced, including general TSVM with pinball loss function (Pin-GTSVM), conditional probability function TSVM (CPTSVM), smooth linear programming TSVM (SLPTSVM), sparse pinball TSVM (SPTWSVM), large scale pinball TSVM (LPTWSVM), improvement on reduced universum TSVM for imbalanced classes (IRUTSVM), multi-task TSVM with Universum data (UMTSVM)and granular ball TSVM with pinball loss (Pin-GBTSVM). To further enhance the computational efficiencyproposed least squares TSVM (LSTSVM) by solving two linear systems of equations. This approach offers novel avenues for diminishing computational complexity, and it has spurred the development of various algorithms rooted in it such as intuitionistic fuzzy weighted LSTSVM (IFW-LSTSVM), large-scale fuzzy LSTSVM for class imbalance learning (LS-FLSTSVM-CIL), laplaciannorm LSTSVM (Lap-LpLSTSVM), least squares structural twin bounded SVM on class scatter (LS-STBSVM), energy-based LSTSVM (ELS-TSVM)and least squares weighted multi-class TSVM (LS-KWMTSVM).

While the TSVM and its variants assume that the level of noise in the training data is consistent across the entire dataset. Noise refers to random or irrelevant data that can interfere with the learning process by introducing errors or misleading patterns in the training data. The assumption of a uniform noise model may not hold true in real-world scenarios. In the case of a heteroscedastic noise structure, where noise strongly depends on the input value, the amount of noise varies based on the location. To generate a parametric-margin model,proposed the parametric margin-support vector machine (par--SVM). This can be valuable in numerous scenarios, particularly when dealing with datasets exhibiting a heteroscedastic error structure, where the input values are greatly influenced by the noise. However, the par--SVMs learning rate is as slow as the standard SVM. To reduce the computational complexity,proposed twin parametric-margin SVM (TPMSVM). TPMSVM constructs two distinct nonparallel hyperplanes, with each of them serving as the parametric-margin hyperplane for theandclasses.

Multiview learning (MVL) concentrates on data sets that can be represented by multiple distinct feature sets. Different views often provide information complementary to each other. In contrast to traditional single-view learning, MVL constructs a learning function for each feature view separately and then jointly optimizes the learning function by exploiting redundant views of the same input data. MVLis a nascent avenue within the field of machine learning, dedicated to leveraging multiview data to enhance the overall performance of learning algorithms, particularly in terms of their generalization capabilities. MVL has demonstrated its success in various domains, including financial analysis, intelligent diagnostics, multiview clustering, and so on. However, a primary challenge currently confronting MVL is the effective utilization of the multiview data to its fullest potential. As per, the current approaches to MVL can be categorized into three groups: margin-consistency, co-trainingand co-regularization. To keep the margins of classifiers operating on different views highly consistent, one can use margin-consistency regularization. An early framework for multiview semi-supervised learning is co-training. It operates through iterative processes aimed at maximizing consensus among classifiers across diverse views, ensuring coherence within the data. Co-regularization involves the amalgamation of regularization terms related to regression or discriminant functions from separate views into the overarching objective function.

The efficacy of MVL algorithms hinges on two fundamental principles: the consensus principle and the complementarity principle. These principles are essential in determining how multiple views are related and combined. The complementarity principle highlights the importance of leveraging the complementary information provided by different views to ensure a comprehensive description of the data. The consensus principle aims to maximize the correlation among multiple distinct perspectives to achieve accurate classifiers for each individual view. In the domain of MVL, these two principles play a pivotal role in guiding the construction of models. Multiview SVM (SVM-2K)is a consensus-based principle. SVM-2K model involves training two SVMs within distinct feature spaces, employing an additional-norm constraint during the training process to uphold robustness. Considering the Rademacher complexity, incorporating the-norm constraint results in a more rigorous generalization error bound for SVM-2K. The Multiview TSVM (MvTSVM)is inspired by the co-regularization concept found in SVM-2K and utilizes the effective training attributes derived from TSVMs. By solving two smaller-sized QPPs, MvTSVM generates four non-parallel hyperplanes (associated with two views) in total, which makes it four times faster than SVM-2K. In recent years, several variants of MvTSVM have been proposed such as multiview twin hypersphere support vector machine (MvTHSVM), multiview learning model based on nonparallel SVM (MvNPSVM), multiview robust double-sided TSVM (MvRDTSVM), deep multiview TSVM (DMvTSVM), multiview one-class SVM method with privileged information learning (MOCPIL), multiview large margin distribution machine (MVLDM)and many more. However, MvTSVM encounters significant challenges, which are as follows: (1) MvTSVM necessitates four matrix inversion operations during the training process, resulting in avoidable computational overhead. (2) MvTSVM requires the reformulation of optimization problems when the training data is mapped into high-dimensional spaces using non-linear functions. (3) The uniformity of noise in the training data, or its pre-known functional dependency, presents significant obstacles to the effectiveness of the model.

To surmount the aforementioned challenges, we propose a novel multiview twin parametric margin support vector machine (MvTPMSVM). MvTPMSVM generates a total of four non-parallel hyperplanes (associated with two views). Each hyperplane defines the positive or negative parametric margin, aligning closer to its respective class.
The MvTPMSVM autonomously adjusts the parametric insensitive hyperplanes to assimilate the structural information inherent in the data, effectively capturing the intrinsic characteristics of heteroscedastic noise within the dataset. The optimization problem of MvTPMSVM is to remove the need for computing extensive matrix inverses and employ the kernel method directly. The integration of multiple views in the proposed MvTPMSVM model helps mitigate the impact of missing or noisy data in any single view, thereby enhancing the model’s robustness and reliability.

The main contributions of this work are outlined as follows:

We propose a novel multiview twin parametric margin support vector machine (MvTPMSVM). The proposed MvTPMSVM generates parametric margin hyperplanes corresponding to both classes, thereby controlling the impact of heteroscedastic noise present in the data.

The MvTPMSVM circumvents the need for computing matrix inverses, unlike other baseline models, which necessitate the calculation of matrix inverse that becomes infeasible for real-world scenarios.

The MvTPMSVM model adeptly captures intricate, nonlinear relationships present within the data. This ability frequently results in enhanced performance, achieved with notably lower computational complexity.

We provide rigorous mathematical frameworks for MvTPMSVM, for both linear and nonlinear kernel spaces. MvTPMSVM can directly apply the kernel method with an elegant mathematical formulation.

We conduct experiments encompassingUCI and KEEL, synthetic, and Animals with Attributes (AwA) datasets. Through exhaustive numerical experiments and comprehensive statistical analyses, our findings establish the superior performance of the proposed MvTPMSVM model in comparison to the baseline models.

The subsequent sections of the paper are structured as follows: Sectionintroduces notation and an overview of the related work. Sectionexplains the formulation of the proposed MvTPMSVM model. The experimental results and discussions are presented in Section. The conclusion and avenues for future work are presented in Section.

SECTION: Related Work
In this section, we briefly outline the mathematical formulations of SVM-2K and MvTSVM. Consider the sample space denoted as, which is a product of two distinct feature views,and, expressed as, where,and the label space is presented by. Supposerepresent a two-view data set. Let us consider the input matricesandrepresent theandclass sample of view, andandrepresent theandclass sample of view, respectively. The non-parallel hyperplanes are given byandfor view, and two non-parallel hyperplanes for viewis given byand. Let,,,,,,, and, where,are the vector of ones, respectively. Tableprovides a summary of the primary notations used in this paper.

SECTION: Two view learning: SVM-2K, theory and practice
SVM-2Kfinds two distinct optimal hyperplanes: one associated with viewand another with view, and given as:

The optimization problem of SVM-2K can be written as follows:

where,,are penalty parameters,is an insensitive parameter and,, andare slack variables, respectively. The amalgamation of the two views through the-insensitive-norm similarity constraint is incorporated as the initial constraint in the problem (). The dual problem of () is given by:

whereandare the vectors of Lagrangian multipliers. The predictive function for each view is expressed as:

SECTION: Multiview twin support vector machine (MvTSVM)
MvTSVMneeds to construct four non-parallel hyperplanes. The optimization problem of MvTSVM is given as follows:

and

where,,,,,are tunable parameters, andis an insensitive parameter;,,,,, andare slack variables, respectively.

The dual optimization problem of () and () are given by:

and

where,,,,,,andare the vectors of Lagrange multipliers.The categorization of a new input data pointinto either theorclass can be determined as follows:

If the functionyields a value less than 0, it will be assigned to theclass; otherwise, it will be assigned to theclass.

SECTION: Proposed Multiview Twin Parametric Margin Support Vector Machine (MvTPMSVM)
In this section, we provide a detailed mathematical formulation of the proposed MvTPMSVM model tailored for linear and non-linear cases. MvTPMSVM generates a pair of nonparallel parametric-margin hyperplanes solved by two smaller-sized QPPs while eliminating the requirement for calculating large matrix inverses. Also, MvTPMSVM effectively captures more intricate heteroscedastic noise structures, through the utilization of these parametric-margin hyperplanes. Flow diagram of the proposed MvTPMSVM model is shown in Fig..

SECTION: Linear MvTPMSVM
The optimization problem for linear MvTPMSVM is formulated as:

and

where,,are positive tunable parameters,,, represents the slack variables.

The demonstration of the MvTPMSVM mechanism is illustrated as follows:

The termsandare regularization components for view, whileandare regularization components for view. These terms are intended to prevent overfitting by constraining the capabilities of the classifier sets for both views.

The second term in the objective function minimizes the sum of projection values for theclass training samples of both views. By leveraging label correlations, it incorporates the-insensitive loss function, which gives rise to the inclusion of a regularization term between different views. Nonnegative slack variablesandare introduced, along with tunable parametersand, to evaluate the degree of deviation from-similarity between two() classifiers across two views.

The first constraint in the objective function of the proposed MvTPMSVM model is the-insensitive-norm, which ensures the alignment of the classifiers from both views concerningandclasses. Samples deviating from these constraints are accommodated by introducing the parameter. By leveraging label correlations, this approach diminishes the gap between different perspectives, thereby ensuring their consistency.

The second component of the objective function is made to minimize the total projection values of the() training points in each view. Optimizing this component encourages the effective positioning of() training points to attain maximal distance from the() parametric-margin hyperplane.

The second and third constraints in the optimization problem of the proposed MvTPMSVM model stipulate that the parametric-margin hyperplane should not be less than zero. The fault tolerance
parameters,,, andare employed to quantify the extent of disagreement on each view. In order to avoid overfitting to the() training points, the third term of the objective function minimizes the sum of error variables.

The corresponding Lagrangian function of problem () can be formulated as:

Using the Karush-Kuhn-Tucker (K.K.T.) conditions, we have

From Eqs. () and (), we obtain

Putting Eqs. () and () in (), we get

Collecting the terms of Eq. () and using the aforementioned K.K.T. conditions, we obtained the dual form of () as follows:

Likewise, the Wolfe dual for () can be obtain as

The dual problems of () and () can be concisely formulated as:

where

whererepresents a vector of Lagrangian multipliers.andrepresent an identity matrix and zeros matrix of appropriate dimensions, respectively.

where

Analogously,andcan be calculated by the subsequent equations:

Once the optimal values of,,andare calculated. Then the categorization of a new input data pointinto either theorclasses can be determined as follows:

If the functionyields a value less than 0,will be assigned to theclass; otherwise,will be assigned to theclass.

SECTION: Nonlinear MvTPMSVM
To extend our proposed MvTPMSVM model to the non-linear case, we consider the hyperplanes,andfor view, andandfor view.
The optimization problem for linear MvTPMSVM is formulated as:

and

where,,are positive tunable parameters.,, represents the slack variables.

The dual formulations of () and () can be calculated in a similar way as in the linear case, and are given by:

and

where

Once the optimal values of,,andare calculated. The class of a new input data pointcan be determined as follows:

If the functionyields a value less than 0,will be assigned to theclass; otherwise,will be assigned to theclass.

SECTION: Computational complexity and algorithm
In this section, we briefly discuss the computational complexity of the proposed MvTPMSVM model. Suppose the number of samples of each class are equal, namely, whererepresents the number of training samples. The computational complexity of SVM-2Kand MvTSVMareand, respectively. Since our proposed MvTPMSVM model solves two smaller-sized QPP, which are roughly of size, the computational complexity of the dual problems of () and () are both. Therefore the computational complexity of the proposed model MvTPMSVM is. Thus, the computational complexity of the proposed MvTPMSVM model is much lower than that of the SVM-2K and MvTSVM models. The algorithm of the proposed MvTPMSVM model is briefly described in Algorithm.

() and() are the matrices of() class corresponding to view A and view B.Decision function as in ().

SECTION: Experimental Results
To test the efficiency of the proposed MvTPMSVM model, we conduct experiment on publicly available benchmark datasets including,synthetic multiview datasets,real-world UCIand KEELdatasets andbinary classification datasets obtained from Animal with Attributes (AwA).

SECTION: Experimental setup
The experimental hardware setup comprises a PC with an Intel(R) Xeon(R) Gold 6226R CPU running atGHz and equipped withGB of RAM, running Windowsoperating system possessing Matlab R2023a.
The dataset is randomly partitioned into a ratio of, allocatingof the data for training andfor testing. We employ a five-fold cross-validation technique and a grid search approach to optimize the hyperparameters of the models. For all experiments, we opt the Gaussian kernel function represented byfor each model. The kernel parameteris selected from the following range:. We adopt equal penalty parameters, i.e.,and, to mitigate computational costs and are selected from the range. For the baseline MvTSVM and MVNPSVM model, we setand selected from the range. In SVM-2K and PSVM-2V, we setand selected from the range. For MVLDM, the parameterare chosen from. The parameterin the proposed MvTPMSVM model along with the baseline models is set to. The generalization performance of the proposed MvTPMSVM model has been evaluated by comparing it with baseline models across various metrics including,,,, and. Mathematically,

where true positive () represents the count of patterns belonging toclass that are accurately classified, while false negative () signifies the count of patterns belonging toclass that are inaccurately classified, false positive () denotes the count of patterns belonging toclass that are inaccurately classified, and true negative () describes the number of data points ofclass that are correctly classified.

SECTION: Experiments on synthetic datasets
The artificial datasetsexhibit three distinct point distributions with two views. These distributions are categorized as follows: concentric circle and double vortex (synthetic 1), Gaussian cloud and checkboard (synthetic 2), and double square and double moon (synthetic 3). The distributions of these datasets are illustrated in Figure. The three synthetic datasets consist of 800, 1200, and 2000 samples, respectively.

Tabledisplays the comparative experimental outcomes of the proposed MvTPMSVM model, along with the baseline models encompassing non-linear case. From Table, the average ACC of the proposed MvTPMSVM model is, surpassing the performance of the baseline models. It is evident that the proposed MvTPMSVM model consistently outperforms the baseline models. Thus, the experimental comparison of datasets demonstrates the superior performance of our proposed MvTPMSVM model.

SECTION: Experiments on real-world UCI and KEEL datasets
In this subsection, we provide a detailed analysis that includes a comparison of the proposed MvTPMSVM model with SVM-2K, MvTSVM, MVNPSVM, MVLDMand PSVM-2Vmodels acrossUCIand KEELbenchmark datasets. Our investigation spans a spectrum of scenarios, specifically focusing on non-linear cases, and is subjected to meticulous statistical analysis. Given that the UCI and KEEL datasets lack inherent multiview characteristics, we designate theprincipal component that we extracted from the original data as view, and we refer to the original data as view..

Tableshows the experimental results including ACC, Seny, Spey, and Pren of the proposed MvTPMSVM model along with the baseline SVM-2K, MvTSVM, MVNPSVM, and PSVM-2V models. The optimal parameters of the proposed MvTPSVM model and the baseline models corresponding to theare presented in Table. The comparison in terms of ACC, Seny, Spey, Pren andindicate that our proposed MvTPMSVM model yields better performance than the baseline models on most of the datasets. From Table, the average ACC of the proposed MvTPMSVM model is. In contrast, the average ACC for SVM-2K, MvTSVM, MVNPSVM, PSVM-2V and MVLDM models stands at,,,and, respectively. As the average ACC can be influenced by exceptional performance in one dataset that compensates for losses across multiple datasets, it might be considered a biased measure. Hence, we utilize the ranking method to assess the effectiveness and evaluate the performance of the models. Here, each classifier is assigned a rank, where the model exhibiting superior performance receives a lower rank, and the model with inferior performance is assigned a higher rank. In the evaluation ofmodels acrossdatasets,represents the rank of themodel on thedataset.calculates the average rank ofmodel. The average ranks for SVM-2K, MvTSVM, MVNPSVM, PSVM-2V, MVLDM and MvTPMSVM are,,,,and, respectively. The evident observation is that the proposed MvTPMSVM model demonstrates the most favorable average ranks. Hence, the generalization ability of the proposed MvTPMSVM model surpasses that of the baseline models. Now we conduct the statistical tests to determine the significance of the results. To evaluate if there are statistically significant differences between the models, we employ the Friedman test. The Friedman test involves comparing the average ranks attributed to different models, subsequently determining the presence of statistical distinctions among the models based on their respective rankings. The null hypothesis posits that all the models share an equal average rank, indicating comparable performance. The Friedman test adheres to the chi-squared distributionwithdegrees of freedom (d.o.f.), and its calculation involves:. The Friedman statisticis calculated as:, where the d.o.f. for the-distribution isand. At thelevel of significance, we obtainandforand. With reference tofrom statistical-distribution table. We reject the null hypothesis as. Hence, statistical difference exists among the compared models. Consequently, we employ the Nemenyi post hoc test to analyze the pairwise distinctions between the models. The critical differenceis computed as follows:, whererepresents the critical value from the distribution table for the two-tailed Nemenyi test. Referring to the statistical-distribution table, withat asignificance level, the calculatedis. The differences in average ranks between the proposed MvTPMSVM model and the baseline SVM-2K, MvTSVM, MVNPSVM, PSVM-2V, and MVLDM models are,,,andrespectively. The average rank difference between the proposed MvTPMSVM with the baseline SVM-2K, MvTSVM, MVNPSVM, and PSVM-2V models are greater than the. According to the Nemenyi post hoc test, noteworthy differences are observed between the proposed MvTPMSVM model and the baseline models (except MVLDM). The MvTPMSVM model surpasses MVLDM in terms of average rank. The superior performance of the proposed MvTPMSVM model compared to the baseline models is evident.

Furthermore, in the analysis of models, we employ the pairwise win-tie-loss (W-T-L) sign test. According to the W-T-L sign test, the null hypothesis posits that two models perform equivalently and are expected to secure victories indatasets, wheredenotes the count of datasets. If the classification model secures wins in approximatelydatasets, then the model is deemed significantly better. In the event of an even number of ties between two models, these ties are evenly distributed between them. Nevertheless, if the number of ties is odd, one tie is disregarded, and the remaining ties are distributed among the specified classifiers. In this instance, with, the occurrence of wins in at leastdatasets by one of the models signifies a noteworthy distinction between the models. Tablepresents the outcomes in terms of pairwise wins, ties, and losses across UCI and KEEL datasets. In each entry,,denotes the number of times the model specified in the corresponding row secures a win,represents the number of ties, andsignifies the occurrences of losses in comparison to the model mentioned in the respective column. Upon meticulous examination of Table, it is apparent that the proposed MvTPMSVM model outperforms the baseline MvTSVM, MVNPSVM, and PSVM-2V models by achieving a respective number of victories:,, and, out of a total ofdatasets. The proposed MvTPMSVM demonstrates a statistically significant distinction compared to the baseline models, except for SVM-2K and MVLDM. Exhibiting a notable level of performance, the MvTPMSVM model succeeds inout ofdatasets over SVM-2K andout ofdatasets over MVLDM. However, the winning percentage of the MvTPMSVM model demonstrates its effectiveness in comparison to the SVM-2K and MVLDM models. It is evident that the proposed MvTPMSVM model exhibits significant superiority when compared to the baseline models.

In Figure, the Seny, Spey, and Pren indicate that the proposed MvTPMSVM model demonstrates competitive performance w.r.t. the baseline models. The proposed MvTPMSVM model achieves an average Seny of, Spey of, and Pren of. These metrics indicate that the MvTPMSVM model outperforms the baseline models, securing the top position in comparison. Thus, the Seny, Spey, and Pren analysis further attests to the superiority of the proposed MvTPMSVM model on UCI and KEEL datasets.

Figureillustrates the ROC curve, highlighting the superior performance of the proposed MvTPMSVM model compared to the baseline models on UCI and KEEL datasets. The ROC curve provides a comprehensive view of the model’s diagnostic ability by evaluating the true positive rate against the false positive rate at various threshold settings. The area under the ROC curve (AUC) for the proposed MvTPMSVM model is significantly higher, indicating a better balance between Seny and Spey. This superior AUC suggests that the proposed MvTPMSVM model is more effective in distinguishing between positive and negative cases, leading to more accurate predictions. The proposed MvTPMSVM model means it can better identify true positives, thereby reducing the rate of false negatives and ensuring more reliable detection. These results highlight the robustness and effectiveness of the proposed MvTPMSVM model in handling classification tasks compared to the baseline models.

SECTION: Experiments on real-world AwA datasets
To validate the effectiveness of the proposed MvTPMSVM model, experiments are conducted on the AwAdatasets. This dataset comprisesimages belonging todifferent animal classes, and each image comes with six pre-extracted feature representations. We have chosenanimal groups by Tang’s studies, namely persian cat, chimpanzee, pig, hippopotamus, leopard, giant panda, humpback whale, rat, seal, and raccoon. These classes were paired intobinary datasets using a one-vs-one strategy.

Tabledisplays the ACC, Seny, Spey, and Pren of the proposed MvTPMSVM model along with SVM-2K, MvTSVM, MVNPSVM, PSVM-2V, and MVLDM models. Theand the optimal parameters of the proposed MvTPSVM model and the baseline models are presented in Table. From Table, it is evident that the proposed MvTPMSVM model exhibits superior generalization performance across the majority of datasets. The average ACC of the proposed MvTPMSVM model and the baseline SVM-2K, MvTSVM, MVNPSVM, PSVM-2V, and MVLDM models are,,,,and, respectively. Tablepresents the average rank of all models, determined by their ACC values. It is worth noting that among all the models, our proposed MvTPMSVM model has the lowest average rank. Moreover, we perform the Friedman statistical test, followed by Nemenyi post hoc tests. For the significance level of, we calculate,, and. The null hypothesis is rejected as. Now, the Nemenyi post-hoc test is employed to identify significant differences among the pairwise comparisons. We calculate, indicating that the average ranking of the models in Tableshould have a minimum difference byto be considered statistically significant. The differences in average rank between the proposed MvTPMSVM model and the baseline SVM-2K, MvTSVM, MVNPSVM, PSVM-2V, and MVLDM models are,,,and, respectively.is exceeded by the observed differences. Thus, as per the Nemenyi post hoc test, the proposed MvTPMSVM model, exhibits significant distinctions from SVM-2K, MvTSVM, MVNPSVM, and PSVM-2V except MVLDM. Consequently, the proposed MvTPMSVM model demonstrates superior performance compared to baseline models except MVLDM. However, the proposed MvTPMSVM model surpasses MVLDM in terms of average rank. The evident superiority of the proposed MvTPMSVM model is evident when contrasted with the baseline models.

Subsequently, additional experiments were performed by adjusting the sizes of the training sets, and their respective performance is illustrated in Figures. We employ a random selection method, usingof the samples for testing, while systematically altering the set sizes allocated for training. Figure, exhibiting different training sizes, consistently demonstrates that the proposed MvTPMSVM model generally outperforms the baseline models. Moreover, as the training sizes increase, the MvTPMSVM model exhibits superior performance compared to baseline models.

Based on Figure, the Seny, Spey, and Pren analysis indicates that the performance of the proposed MvTPMSVM model achieved the highest position in the baseline models, demonstrating its competitive nature. The average Seny, Spey, and Pren are,, and, respectively. The statistical tests provide further evidence that the proposed models outperformed all the existing models in the AwA datasets and achieved the highest rankings in Seny, Spey, and Pren analyses.

SECTION: Effect of hyperparametersand
In this subsection, we conduct an in-depth investigation to understand the influence of the hyperparametersandon the comprehensive predictive capacity of the proposed MvTPMSVM model. Figuredepicts the results of sensitivity analysis conducted on both UCI and KEEL datasets. The ACC is assessed by altering the parametersand. It is observable that with the escalation ofandvalues, there is a corresponding increase in ACC.
However, once a specific threshold is exceeded, the ACC stabilizes, indicating that further increments inandexhibit a diminishing effect on testing ACC. Consequently, meticulous selection of hyperparameters for the proposed MvTPMSVM model is imperative to attain optimal generalization performance.

SECTION: Conclusions and Future Work
In this paper, we proposed a novel multiview twin parametric margin support vector machine (MvTPMSVM) to address the challenges faced by MvTSVM. The proposed MvTPMSVM model alleviates the influence of heteroscedastic noise present in the data, simultaneously reducing the computational costs typically associated with SVM-2K, MvTSVM, and their variants. Furthermore, the proposed MvTPMSVM model directly applies the kernel trick for nonlinear cases, enabling it to solve the exact formulation. In order to showcase the effectiveness and efficiency of the proposed MvTPMSVM model, we performed an extensive series of experiments and subjected them to thorough statistical analyses. We conducted experiments onUCI and KEEL datasets, and these experimental results underwent a comprehensive statistical assessment employing a ranking scheme, Friedman test, Nemenyi post-hoc test, and win-tie-loss sign test. Our findings from the experiments, combined with the statistical analyses, indicate that the proposed MvTPMSVM model outperforms baseline models in terms of generalization performance. Furthermore, the proposed model underwent testing on both synthetic andreal-world AwA datasets. The outcomes demonstrated notable enhancements in ACC and generalization performance by employing multiple views in the analysis. The proposed MvTPMSVM model exhibits superior performance compared to certain early and late fusion models, as well as selected state-of-the-art multiview models. Our proposed model has shown exceptional performance in binary classification problems, their evaluation in multiview (incorporating more than two views) multiclass problems has not been conducted. A critical avenue for future research would involve adapting the proposed model to be suitable for multiview multiclass problems.
Also, our model designed to handle balanced datasets, has the potential to enhance its robustness by integrating techniques tailored to address class imbalance learning. This refers to scenarios where the distribution of classes in the training data is highly skewed, with one or more classes significantly outnumbering others. By incorporating methods specifically designed to tackle class imbalance, the model becomes more adept at effectively handling datasets with disparate class distributions. The source code of the proposed MvTPMSVM model is available at.

SECTION: Acknowledgment
This work is supported by Indian government’s Department of Science and Technology (DST) through the MTR/2021/000787 grant as part of the Mathematical Research Impact-Centric Support (MATRICS) scheme.

SECTION: References