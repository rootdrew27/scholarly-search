SECTION: Investigation of Hierarchical Spectral Vision Transformer Architecture for Classification of Hyperspectral Imagery
In the past three years, there has been significant interest in hyperspectral imagery (HSI) classification using vision Transformers for analysis of remotely sensed data. Previous research predominantly focused on the empirical integration of convolutional neural networks (CNNs) to augment the network’s capability to extract local feature information. Yet, the theoretical justification for vision Transformers out-performing CNN architectures in HSI classification remains a question. To address this issue, a unified hierarchical spectral vision Transformer architecture, specifically tailored for HSI classification, is investigated. In this streamlined yet effective vision Transformer architecture, multiple mixer modules are strategically integrated separately. These include the CNN-mixer, which executes convolution operations; the spatial self-attention (SSA)-mixer and channel self-attention (CSA)-mixer, both of which are adaptations of classical self-attention blocks; and hybrid models such as the SSA+CNN-mixer and CSA+CNN-mixer, which merge convolution with self-attention operations. This integration facilitates the development of a broad spectrum of vision Transformer-based models tailored for HSI classification. In terms of the training process, a comprehensive analysis is performed, contrasting classical CNN models and vision Transformer-based counterparts, with particular attention to disturbance robustness and the distribution of the largest eigenvalue of the Hessian. From the evaluations conducted on various mixer models rooted in the unified architecture, it is concluded that the unique strength of vision Transformers can be attributed to their overarching architecture, rather than being exclusively reliant on individual multi-head self-attention (MSA) components.
Extensive experiments demonstrate that the derived vision Transformer models, based on the unified architecture, surpass the classical methods when applied to multiple hyperspectral benchmark datasets.

SECTION: 
Hyperspectral imagery (HSI) enables detailed material identification by representing the reflectance spectra of objects via hundreds of contiguous bands. HSI data are used in diverse applications including environmental monitoring, precision agriculture, geology, urban mapping, and defense.
Owing to the rapid advancements in deep learning, CNN architectures have emerged as the predominant standard for HSI classification in recent years. In, a deep feature fusion CNN is utilized to categorize each pixel of HSI data. To bolster extraction of spectrally-based features,introduce 3D-CNNs for HSI classification. Additionally, an attention mechanism can be integrated into the CNN framework to facilitate band selection for HSI data, as demonstrated in.
The efficacy of CNN-based HSI classification faces two significant limitations: 1) CNNs often struggle to adequately capture long-range dependencies; 2) The adoption of small input image window patches serves as a compromise between the high dimensionality of HSI data and its corresponding lower spatial resolution. This restricts the design possibilities of the network, impacting its depth and width.
In the past three years, the appeal of using vision Transformers for HSI classification has grown. This is attributed to the understanding that the spectral dimension of HSI parallels sequence data, irrespective of whether analysis is conducted at the pixel or patch level. In, group-wise spectral embedding is employed for HSI classification. Similarly,introduces a group-aware hierarchical vision Transformer to strengthen HSI classification. Furthermore, the LESSFormer design, as presented in, aims to increase the capture of local information using adaptive spectral-spatial tokens. However, some have suggested that this configuration compromises the inductive bias inherent in CNNs. To address this, some have integrated vision Transformer and CNN modules, either in parallel or sequentially, to harness the advantages of both. Owing to the scalability of vision Transformers, they typically have a higher number of parameters compared to traditional CNNs. Incorporating an additional CNN branch on top of the multi-head self-attention (MSA) typically leads to a further increase in the model’s parameter size. At the same time, it has been noted that the overarching structure of vision Transformers, rather than just the MSA mixer, is pivotal to delivering top-tier performance. This notion is further emphasized in studies where MSAs are substituted for multi-layer perceptrons (MLPs), as highlighted in.
While vision Transformer-based network architectures presently have a pronounced edge in HSI classification-based metrics relative to CNNs, the associated exploration predominantly remains empirical. Thus, this field continues to struggle with pivotal questions:
(1) Does MSA serve as thecritical componentin vision Transformers that enhances HSI classification?
(2) What fundamental differences exist relative to the training process for vision Transformer-based models and CNNs in analysis of hyperspectral datasets?

To address these questions, this paper proposes a unified hierarchical spectral vision Transformer architecture designed to integrate discriminative features for HSI classification. Notably, the simple yet effective unified architecture can be seamlessly integrated with any type of mixer block to construct a novel vision Transformer model. In this paper, various mixer modules, including the CNN-mixer, spatial self-attention (SSA)-mixer, channel self-attention (CSA)-mixer, SSA+CNN-mixer, and CSA+CNN-mixer, are independently integrated into the unified architecture, resulting in multiple vision Transformer models. A comprehensive analysis is conducted on these derived vision Transformer models and classical models, considering both the macroscopic aspect of the disturbance robustness and the microscopic aspect of the distribution of the maximum eigenvalue of the Hessian after the training process. In this paper, the term ’Hessian’ specifically refers to the Hessian of the loss function relative to the parameters of the network. A key goal of this study is to explore the influence of different mixers on training vision Transformer-based models. A comprehensive comparison is conducted to explore and highlight fundamental differences between the vision Transformer and CNN models. To the best of our knowledge, this is the first paper to thoroughly investigate the key factors behind the superior performance of vision Transformers in HSI classification. Other contributions include: a) Conducting a rigorous evaluation of the training process for both CNNs and vision Transformers; b) Demonstrating that the unified architecture, rather than the MSA modules contribute to the superior performance observed with vision Transformers in HSI classification.

The remainder of this paper is structured as follows:
Related work is summarized in Section. The proposed method is detailed in Section. The experimental setup and results are presented in Section. Sectionincludes the conclusion.

SECTION: 
:
As highlighted in the first review of deep learning-based HSI classification, traditional machine learning techniques often fall short in addressing the unique challenges inherent in HSI classification, and particularly the significant spatial variability of spectral signatures. Over the past decade, the application of CNN models has advanced significantly, both in terms of enhanced performance and efficiency in HSI classification. Compared to traditional machine learning techniques, CNN-based methods excel in their ability to capture localized and discriminative spatial information, all while exhibiting resilience to translations and other variations.
In, a streamlined, end-to-end CNN structure utilizing 11 convolutional layers is adopted for HSI classification.introduces a dual-channel CNN, crafted to jointly exploit spectral-spatial features from HSI.develops a spectral-spatial latent reconstruction framework that concurrently reconstructs spectral and spatial features, while also performing pixel-wise classification with high accuracy.formulates a novel
enhanced multiscale feature fusion network to extract sufficiently
multiscale features from the parallel multipath architecture of three stages for
HSI classification. Additionally,implements a novel online spectral information compensation network for HSI classification. However, conventional 1D and 2D-CNNs often fall short in concurrently leveraging both spatial and spectral discriminative information. Recognizing this gap, researchers pioneered 3D-CNN architectures. For instance,investigates an enhanced 3D deep CNN encompassing five layers. Furthermore,proposes a distinctive recurrent 3D-CNN, designed to refine the 3D-CNN model by progressively diminishing the patch size.formulates a streamlined 3D-CNN model with minimal parameters, resulting in a notable reduction in duration to convergence, while boosting accuracy. However, it should be noted that 3D-CNN models may encounter challenges such as overfitting and substantial computational demands. Aiming to alleviate such issues,suggests a synergistic methodology that intertwines 2D-CNN and 3D-CNN. In this approach, the 2D-CNN is employed to extract spatial features, while the 3D-CNN, using small kernels, focuses on inter-band correlations. Complementing this,proposes a 2D-3D CNN that incorporates a multi-branch feature fusion architecture. Some researchers specifically design CNN variants to efficiently extract feature representations. Notably,proposes a novel geometry-aware convolutional foundation model that excels in learning unique geometry- and category-aware features and is informed by vehicle kinematics information to significantly enhance inclusive object detection and extend the perception range. Additionally, HSI shows category imbalance and complex spatial-spectral distributions, limiting adaptation performance. To address these issues,proposes a class-aligned and class-balancing generative domain adaptation method for HSI classification. Similarly,presents a novel framework with multigranularity generators and discriminators that uses adversarial and contrastive learning to continuously improve discriminator classification performance with diverse generated samples.

Recently, attention modules have gained widespread popularity in the field of deep learning, owing to their plug-and-play capability and their effectiveness in enhancing neural network performance. In, a hierarchical network for efficient and accurate outdoor LiDAR point cloud registration is proposed by introducing an attention-based neighbor encoding module to gather neighborhood information. In pioneering work in instance-level HSI classification,proposes a novel spectral–spatial feature pyramid network, which integrates multi-scale spectral and spatial information for instance segmentation in HSI. In, a ghost attention mechanism is proposed to significantly reduce both the parameters and FLOPs of the vision Transformer while achieving similar or better accuracy. The introduction of attention modules offers an alternative approach to boost HSI classification accuracy. These modules, by selectively emphasizing the most discriminative regions of an input small window patch or feature map, guide the network to focus on pivotal areas. Through the allocation of differential weights to various pixels, the attention mechanism captures essential details, ignoring extraneous information. This refinement contributes to the network’s more accurate predictions. In, a pixel classification CNN is complemented with a superpixel-based graph attention network. The work inmelds a spectral-spatial attention network with ResNet for HSI classification. Recognizing the potential of harnessing long-range semantic information,introduces an adaptive projection attention technique. Concurrently, several studies corroborate that the integration of attention modules significantly improves HSI classification accuracies, as evidenced by.

:
Over the last three years, the vision Transformer has excelled in the realm of HSI classification, showcasing its distinctive advantage in handling data sequences. The work inintroduces SpectralFormer, which integrates group-wise spectral embedding and cross-layer adaptive fusion modules. Specifically, the group-wise spectral embedding is adept at capturing feature embeddings from adjacent spectral bands. This combination facilitates the capture of detailed local spectral representations and promotes the transmission of memory-like components from superficial to deeper layers. Meanwhile,presents a multiscale and cross-level attention learning network designed to holistically harness both global and local multiscale features of pixels for enhanced classification. In, a technique is introduced that employs grouped pixel embedding to better represent local representations.proposes the spectral-spatial feature tokenization Transformer (SSFTT) approach, crafted to efficiently encapsulate HSI’s low, mid, and high-level semantic features. Aiming to optimize classification and reduce computational overhead,devises a neighborhood-centric representation of multi-scale HSI features. In, a novel local vision Transformer, complemented by a spatial partition restore network, is introduced for HSI classification.details LESSFormer, a design for HSI classification that converts HSI into adaptable spectral-spatial tokens. These tokens are then enriched to capture both local and extensive data nuances. Addressing the vision Transformer’s predominant focus on global data,integrates it with a CNN, aiming to extract local features and thereby enhance classification.develops a hybrid Transformer, merging multi-granularity tokens with spatial-spectral attention to model spatial-spectral information. Additionally,implements a dual-branch architecture, combining the CNN and vision Transformer to seamlessly fuse spectral and spatial features.proposes a novel hybrid deep learning network that systematically combines hierarchical CNNs and Transformers for feature extraction and fusion. This approach effectively learns spatial-spectral features in HSIs and elevation information in LiDAR, significantly enhancing the accuracy of the joint classification. Similarly,introduces a novel layered architecture that integrates Transformer with CNN, utilizing a feature dimensionality reduction module and a Transformer-style CNN module to extract shallow features and enforce texture constraints, while employing the original Transformer encoder to extract deep features. Inspired by the observation that high-frequency information captures local details and low-frequency information provides global smooth variations,develops a frequency domain feature extraction vision Transformer network for HSI classification. The work inputs forward three essential elements for efficient HSI classification through the integration of vision Transformer and CNN networks: extensive exploration of available features, effective reuse of representative features, and differentiated fusion of multi-domain features. Utilizing masked autoencoders’ self-supervised training paradigm, some researchers adopt a masked image modeling strategy for remote sensing image classification.develops a novel 3D generative pretrained Transformer architecture based on masked autoencoders for remote sensing applications.introduces LFSMIM, a self-supervised network for HSI classification that employs low-pass filtering to construct the target domain within the masked image modeling framework.proposes an unsupervised band selection framework that captures nonlinear relationships between bands and leverages spatial information in HSI. From these studies, it is evident that while vision Transformers have advanced HSI classification accuracy compared to CNN models, the majority of research has concentrated on empirical modifications to the self-attention modules, such as integrating CNN modules or altering feature embeddings. Specifically, the current analysis of model enhancements relies heavily on metric outcomes and prediction maps, without a thorough exploration of the variations throughout the training phase. This overlooks a critical element that contributes to the superior performance of vision Transformer architecture in HSI classification.

To bridge these gaps, a unified architecture for HSI classification built upon the vision Transformer is proposed in this paper. Based on the unified architecture, the attributes of the vision Transformer equipped with multiple mixers are investigated from a model training perspective, and the influence of different mixer modules on model performance is explored.

SECTION: 
SECTION: 
The HSI classification model based on the vision Transformer primarily consists of two main modules, as depicted in Fig.: 1) a unified architecture and 2) mixer block options. Subsequent sections provide detailed descriptions of these modules. Additionally, rather than relying exclusively on prediction accuracy and empirical analysis of various models, this paper evaluates disturbance robustness and the distribution of the largest eigenvalue of the Hessian. This evaluation provides insights into the model training process from both macro and micro perspectives.

The original HSI data are denoted as, whereandrepresent the spatial height and width, respectively, andsignifies the number of spectral bands. The HSI dataare divided into patches using a patch window size of, with each patch represented as. The label assigned to the center point of a patch determines its true label.
The proposed vision Transformer model for HSI classification is designed to categorize the center point of each patch cube. The hierarchical vision Transformer architecture for HSI classification is depicted in Figure, referred to as theunified architecture.
The network comprises four stages:,,, and. In each stage, feature information is extracted through the iterative stacking of token embedding and the mixer module. The number of layers in each stage is represented by,,, andrespectively. Given the unique characteristics of the input patch window, discriminative features are accumulated across various layers, emphasizing the information in the spectral and spatial dimensions. The respective channel numbers for each stage are denoted as,,, and. Similar with the Swin Transformer’s linear embedding technique, this paper utilizesfor processing raw-valued features. To guarantee compatibility in data shape with the mixer blocks modules,andare judiciously placed before and after theoperation, respectively. Importantly,serves as the inverse operation to. The token embedding strategy employed here is designed to project the spectral dimension to an arbitrary dimension, without impacting its spatial dimension.
Employing the token embedding module, channel/pixel feature information is consolidated to produce a hierarchical representation that prioritizes the spectral/spatial dimension. Once processed by the token embedding module, the feature signals are relayed to the mixer blocks module for further discriminative feature extraction. After the feature extraction through four stages, the latent representation will further undergo processing by the adaptive average pooling, flattening, and a fully connected layer to output the predicted values for the center position of each patch. Notably, the Swin Transformer achieves hierarchical representation by reducing resolution and simultaneously expanding the number of channels. In contrast, the HSI datasets from Houston 2013, Botswana, and Pavia University consist of high-dimensional input channels, with 144, 145, and 103 channels respectively, which always include redundant information. To effectively extract latent representations without substantially enlarging the parameter size of the vision Transformer architecture, the feature dimensions for the initial three layers are reduced, while the spatial dimension size remains constant. Therefore, the hierarchical paradigm for the proposed unified architecture is achieved by leveraging the spectral dimension.

To promote the model’s capacity to generalize across classification tasks, the label smoothing cross-entropy is selected as the loss. It is computed as follows:

where theis the ground truth label for the one-hot vector.is the predicted probability distribution.is the class number.is set atto control the extent of smoothing.is the value of the-th element in the true label vector, whileis the value of the-th element in the probability distribution vector predicted by the model.

SECTION: 
HSI has tens to hundreds of spectral bands. In HSI, each pixel is characterized by a spectrum comprising reflectance values across these bands. This provides a rich representation of the scene or object, allowing for in-depth analysis and identification of materials or features through their unique spectral signatures. As a result, beyond the patch flattening, each pixel in HSI can also be interpreted as a sequence of data. This characteristic makes it possible to devise a variety of mixer blocks tailored to their specific attributes, including the CNN-mixer, SSA-mixer, CSA-mixer, SSA+CNN-mixer, and CSA+CNN-mixer.

: Similar to the vision Transformer block from the Swin Transformer, the CNN-mixer module embeds an MLP, but opts for a CNN in place of the MSA mechanism. Notably, it sets itself apart by integrating an inductive bias, which fosters local feature connections.As highlighted in, the CNN-mixer module possesses the capability to model locality, which is governed by the kernel size, as well as scale-invariance. To avoid the influence of the attention mechanism on model performance, this paper employs a simple two-layer convolutional module. The module’s representation is as follows:

where theoperation amplifies the channel count fourfold usingfilters. This is followed by the application of thebatch normalization. The module further integrates theactivation function, which precedes theoperation to refine the features. In this paper, unless stated otherwise,represents each patch input of.

The CNN-mixer module, incorporating the CNN block, is computed as follows:

whereis the CNN block.is the multilayer perceptron operation. The functiondenotes a basic reshaping operation that transforms a one-dimensional sequence into a feature map.signifies the inverse operation of. Utilizing these reshaping techniques ensures the smooth integration of the CNN-mixer module within the vision Transformer framework.

: To maximize the benefits of the numerous spectral bands in HSI, the SSA-mixer regards each pixel within a patch window as a sequence. Consequently, the length of the input sequence corresponds to the spectral bands’ feature dimension, while the number of sequences is defined by the window size,. This sequential feature information is then input to the MSA module to further distill discriminative features.

whereis linear normalization.is the computation of MSA. It can be described as follows:

where,,are the vectors of,and. These vectors are produced by projecting the input token embeddings through three distinct linear projection layers.is the token embedding dimension.

: In a similar manner, the sequential information derived from token embedding is converted into three-dimensional feature data using thefunction. These data are subsequently transposed and processed via thefunction, facilitating its transformation into sequences for each channel. This sequential feature data is then channeled through the MSA module to further refine and extract key features. The process can be detailed as follows:

whereoperation involves swapping the order of the three axes in the image latent features, facilitating their conversion into sequence data along different directions.

: This architecture is designed by integrating a CNN module alongside the SSA-mixer. The goal is to explore potential improvements in the vision Transformer model’s HSI classification performance by introducing the CNN module. The structure can be outlined as follows:

: This architecture is formulated by integrating a CNN module alongside the CSA-mixer. The intention is to explore potential improvements in the vision Transformer model’s HSI classification efficacy with the inclusion of the CNN module. The configuration can be detailed as follows:

SECTION: 
In the evaluation of HSI classification models, especially when contrasting vision Transformer and CNN models, it is common for researchers to focus on performance metrics. They often employ reverse engineering and empirical analysis to emphasize the strength of specific methods. However, to our knowledge, few studies have seriously investigated the unique attributes of vision Transformer models from a model training perspective. This paper delves into the distinctions between vision Transformer and CNN models during the HSI classification training phase, analyzing them through the ’best’ pretrained weight disturbance robustness and the largest eigenvalue of the Hessian. Specifically, the ’best’ pretrained weight refers to the training weight achieved after completing 300 epochs on the training dataset, while the maximum eigenvalue of the Hessian is calculated using the Hessian matrix. This matrix is constructed from the second-order partial derivatives of the neural network’s loss function. It effectively describes the local curvature of a multi-variable function. In the realm of deep learning training, the ’loss landscape’ refers to the visualization or portrayal of the loss function across the parameter space of a network. This landscape offers critical insights into the evolution of the loss function as network parameters change during training. It reveals useful insights about the model’s behavior relative to the loss during its training phase. Notably, a smoother loss surface in proximity to the closest point tends to improve the model’s generalization capabilities. However, given the huge number of parameters in deep learning models, capturing the intricacies of the loss landscape with a simple three-dimensional representation during the training process is a challenging task.

Building on the technique to produce three-dimensional loss landscapes, we can develop a new understanding of the model’s training process. By introducing random disturbances along two unique vector directions with different magnitudes, based on the ’best’ pretrained weights, the response of the loss value to these shifts can be assessed. This offers an avenue to analyze the robustness of various models to disturbances in post-training. To depict the three-dimensional loss surface subsequent to the disturbance, the model’s loss value can be illustrated as follows:

whereis the ’best’ pretrained weight after training, which is stored in the format of the dictionary.andare scale parameters ranging from -1 to 1. The vectorsandare the basis vectors associated with the-axis and-axis, respectively. The procedures outlined inare established through the following two steps. Initially, two new dictionaries are created based on the function, and these dictionaries are initialized with the same attributes as. Next, the weights and biases of each item in the these dictionaries are normalized separately.

In a given deep learning model, the ’best’ pretrained weights act as a baseline, withandserving as the horizontal axes. By varying the values ofand, introducing different levels of perturbations to the weight, the corresponding loss values of the model on the training dataset can be determined. From the data derived from this set of three-dimensional points, the associated three-dimensional loss surface can be constructed. The framework of calculating the loss value with varying magnitude of disturbance on the ’best’ pretrained weight is shown as Algorithm.

To further investigate local flatness and convergence properties, a qualitative analysis using the maximum eigenvalue of the Hessian is necessary. This explores the local characteristics of the loss surface, highlighting both flat and steep regions. These insights are pivotal in identifying areas that might either impede or aid convergence. The eigenvalue of the Hessian at a given point play a pivotal role in revealing the inherent characteristics of the model’s loss function at that specific location. They are instrumental in discerning whether the point under consideration is a local minimum, a local maximum, or a saddle point. Furthermore, they offer valuable insights into the function’s curvature in various directions. A negative eigenvalue in the Hessian is indicative of the curvature being concave along at least one direction. In practical terms, this means that a slight movement in the direction of the corresponding eigenvector would lead to an increase in the function’s value, signifying that the point in question is not situated in a convex region of the function. Conversely, a scenario in which all the Hessian’s eigenvalues at a specific point are positive denotes that the function exhibits local convexity at that juncture, categorizing the point as a local minimum. Based on the ’best’ pretrained weight after the training process, this paper conducts a thorough analysis of the distribution of the maximum eigenvalue of the Hessian. In the distribution curve representing the maximum eigenvalue, the ideal situation is for the horizontal coordinate of the curve’s peak to not only exceed zero but also remain in close proximity to it. This scenario is indicative of an augmented level of local smoothness in the vicinity of the ’best’ pretrained point, a state achieved in post-training. This is indicative of the model’s generalization ability, showcasing its superior performance capabilities. In this paper, thetoolis employed to compute the maximum eigenvalue of the Hessian. Notably, if model parameter gradients are absent, they are excluded from consideration. The derived maximum eigenvalue of the Hessian then becomes the foundation for applying thefunction from thelibrary, paired with a Gaussian kernel, to shape a distribution curve.

To this end, an in-depth representation of the distinctions between CNN and vision Transformer models, as well as the impact of different mixer modules on the vision Transformer model in HSI classification, can be illustrated by combining performance metrics and training process analysis.

SECTION: 
SECTION: 
The performance of the proposed vision Transformer models for HSI classification is evaluated using three commonly analyzed HSI datasets: Houston 2013, Botswana, and Pavia University.

1): Houston 2013 airborne hyperspectral data consist of 144 spectral bands. The dataset was collected over the University of Houston campus and the surrounding urban area. It comprises a total of 3491905 pixels, with each pixel of the orthorectified dataset having a spatial resolution of 2.5m. The dataset has 15 thematic classes. It was partitioned into three subsets for the analysis: a training set (5%), a validation set (5%), and a test set (90%). The class information and the number of training, validation, and testing samples for each class are presented in Table. The false color image and ground reference map of the Houston 2013 dataset are shown in Fig..

2): The Botswana dataset, acquired by the Hyperion sensor on the EO-1 satellite over the Okavano Delta, consists of 242 spectral bands. After eliminating the noisy and water absorption features bands, the dataset has 145 bands. Each pixel in the imagery has a spatial resolution of 30m. 14 classes were identified in the scene. The dataset was partitioned into three subsets for the analysis: a training set (10%), a validation set (10%), and a test set (80%). The class information and the number of training, validation, and testing samples for each class are detailed in Table. The false color image and ground reference map of the Botswana dataset are shown in Fig..

3):
This scene was collected by the ROSIS sensor during a flight campaign over Pavia, northern Italy. There are 103 bands with 1.3m spatial resolution in this 610340 image, for which 9 classes have been identified. The dataset was partitioned into three subsets for the analysis: a training set (2%), a validation set (2%), and a test set (96%). The class information and the number of training, validation, and testing samples for each class are presented in Table. The false color image and ground reference map of the Pavia University dataset are shown in Fig..

Hyperspectral data are targeted for specific projects. Airborne data are expensive to acquire, and high dimensional. The data are standard common testbed data sets for algorithms, and we did not undertake any additional processing on the data. The benchmark data sets we analyze are widely used to compare classification methods. The Houston data covers the University of Houston and some of the city of Houston. The focus was to acquire information over a range of targets in an urban area with different spatial and spectral characteristics. Pavia University dataset is high resolution and covers a small area with less diversity in the classes and where the spatial representation of structures such as buildings was uniform and are easy to indicate in the ground reference. Botswana dataset was totally different both in terms of the sensor (30m data from space) and as a natural environment. The ground reference information was obtained using small homogeneous patches obtained on the ground and by interpretation of high resolution remotely sensed imagery. Thus, our analysis covers three totally different scenarios.

The proposed method, along with other established common methods, was implemented in Pytorch. The network was implemented on an NVIDIA Quadro RTX 6000 GPU with 22 GB RAM. The corresponding versions of Pytorch and CUDA were 1.10.1 and 10.2, respectively. The training process consisted of 300 epochs, with a batch size of 64. In this paper, the proposed algorithms utilized the Stochastic Gradient Descent (SGD) optimizer, configured with a learning rate of 0.001, momentum at 0.9, and a weight decay parameter of 0.0001. Parameters for the seven popular algorithms evaluated for comparison are consistent with those in the original papers. For the loss function, all algorithms employed label smoothing cross-entropy, ensuring a consistent methodological approach across the comparative analysis. To provide a quantitative comparison of the proposed method’s performance with other classical methods, the evaluation metrics employed were overall accuracy (OA), average accuracy (AA), and kappa coefficient (). Each reported accuracy value represents an average obtained from training with five different random seeds.

SECTION: 
In the comparison study, several representative baseline methods are evaluated, including DFFN, CNN3D, M3D-DCNN, RSSAN, SpectralFormer, SSFTT, GroupTransformer. The DFFN utilizes residual learning to construct a deep 2D-CNN network. The CNN3D integrates traditional CNN architecture with 3D convolution operations. Similarly, the M3D-DCNN jointly learns both 2D multi-scale spatial features and 1D spectral features through a multiscale 3D deep convolutional neural network. The RSSAN combines a spectral-spatial residual attention network with long-short term memory (LSTM) to extract more discriminative spectral and spatial features. In SpectraFormer, which extends the vanilla vision Transformer architecture, a cross-layer skip connection is introduced to merge features across different layers. The SSFTT integrates a 3D convolution layer, a 2D convolution layer, and a vision Transformer module to construct a hybrid CNN-Transformer model for HSI classification. The GroupTransformer introduces a hierarchical Transformer alongside a 2D group convolution network for HSI classification.
Thus, the aforementioned comparison of methods includes the common 2D and 3D CNNs, as well as the vision Transformer network. To ensure that each class of interest is adequately represented, stratified random sampling was employed for the dataset split. This technique consists of forcing the distribution of the target variables among the different splits to be the same. The strategy results in training on the same population in which it is being evaluated, achieving better predictions. Is is implemented by the function of.

SECTION: 
Given the dataset variations outlined in Section, tailoring model structural parameters considering the data characteristics is crucial in HSI classification. Based on the proposed unified architecture, the number of blocks per layerfor the Houston 2013, Botswana, and Pavia University datasets were set to,, and, respectively. The dimension of the features per layerwere set to,, and, respectively. In the joint tuning process of the number of blocks per layer and the dimension of features per layer, the initial setting for the number of blocks per layer was established as, with the Swin Transformer serving as a reference. Simultaneously, the dimensions of the features per layer were set to, ensuring a gradual decrease in feature dimension as layer depth increased. With these initial settings, the models were constructed to ensure the parameter size comparable to the baseline methods. The models were further optimized by adjusting the number of blocks per layer and the dimension of the last layer’s features, with careful consideration to avoid significant changes in the model’s overall parameter size. It should be noted that changes to the dimensions of the features in the first three layers were avoided, as they have a greater impact on the size of the parameter size. Furthermore, the selection of patch sizes for each dataset was determined by an analytical comparison study and evaluation of the spatial resolution of the data relative to that of the scale of spatial information in the image. As shown in Fig., the optimal patch sizes were determined to be 11, 7, and 11 for the Houston 2013, Botswana, and Pavia University datasets, respectively.

Two metrics were introduced to represent the complexity of the model, the size of the parameter set and FLOPs. The results are shown in Table. All measurement results use a patch cube as input, with a batch size that matches the training batch, which was set at 64. In the Houston 2013 and Pavia University datasets, the models built on SSA-mixer have the smallest number of parameters and FLOPs among the five proposed models. The number of parameters is even less than that of the SSFTT and GroupTransformer algorithms. In the Botswana dataset, the model built on the CSA-mixer has the smallest number of parameters and FLOPs. This is because the patch size in the Houston 2013 and Pavia University datasets was set to 11, which is significantly larger than the patch size of 7 set for the Botswana dataset. As the patch size increases, the number of features in the models built on CSA-mixer increases significantly, leading to a considerable increase in the number of model parameters and FLOPs. Furthermore, it is also observed that the models based on the CNN-mixer, despite their simple construction, do not have the smallest number of parameters and FLOPs among the five proposed models. In addition, after adding a CNN branch to the MSA, the two hybrid vision Transformer models (SSA+CNN-mixer and CSA+CNN-mixer) have more parameters and FLOPs than the classical Transformer models, resulting in greater computational costs.

SECTION: 
1) Analysis of classification performance: The mean and standard deviation of each criterion index across the three datasets are presented in Table-. The highest value is highlighted in bold.

The first comparison experiment was conducted on the Houston 2013 dataset. Tablereports the prediction results on the test dataset in terms of OA, AA,, and the accuracy of each class.
Among the four CNN-based models, DFFN stands out with an OA of 97.59%, marking a 3.18% increase in performance compared to the CNN3D algorithm, which has the lowest OA in this group. As for the three classical vision Transformer algorithms, SpectralFormer, SSFTT, and GroupTransformer have OA values of 91.99%, 98.47%, and 98.38%, respectively. In comparison to the SpectralFormer algorithm based on the vanilla vision Transformer, the latter two show significant improvements in classification accuracy. Utilizing the unified hierarchical Transformer architecture proposed in this paper, five mixer-based HSI classification models demonstrated exceptional OA, ranging from 98.48% to 98.68%. On this dataset, the model built upon the CSA+CNN-mixer outperforms other classical CNN and vision Transformer models, with accuracy improvements of 0.21% and 6.69%, respectively.
The corresponding prediction map is shown in Fig.. In the prediction maps for (a), (c), (d), and (e), there is a comparatively higher occurrence of spurious anomalies. The prediction map outcomes for (h) through (l), which represent the five models developed using a unified architecture, show a remarkable similarity.

The second comparative experiment was carried out using the Botswana dataset. The prediction results are listed in Table. On this dataset, the CNN3D algorithm has a significant decrease in accuracy, differing by at least 3.46% compared to the other three common CNN algorithms. Among these, DFFN is the top-performing CNN algorithm, attaining a maximum accuracy of 99.28%. Similar to the results on the Houston 2013 dataset, among the three typical vision Transformer algorithms, SpectralFormer achieved a lower accuracy at 95.24%, in contrast to GroupTransformer achieved an OA value of 99.53%. Among the five models employing different mixers, the one utilizing the CSA-mixer outperforms the rest, achieving an OA of 99.74%. The SSA+CNN-mixer-based method performs the worst. However, its OA value is only 0.02% lower than that of the GroupTransformer.
The prediction map is depicted in Fig.. The maps for (a), (c), (d), (e), and (f) demonstrate suboptimal performance. Upon exploring a magnified view, it is evident that (b) shows a higher number of errors in predicting ’Riparian’ compared to the proposed models based on the unified vision Transformer architecture. This phenomenon matches the metrics provided in Table.

The third comparative experiment utilized the Pavia dataset, and Tabledisplays the prediction results obtained by different methods.
Among the four prevalent CNN methods, DFFN obtains an OA of 93.53%, which is comparatively lower, while RSSAN distinguishes itself with a higher OA of 98.88%. Within the three popular vision Transformer-based methods, SpectralFormer lags slightly behind the other two algorithms. Notably, in contrast to the previous two datasets, the SSFTT algorithm outperforms the other two popular vision Transformer methods, reaching an accuracy of 99.43% and surpassing the GroupTransformer by 0.14%. Meanwhile, the five HSI classification algorithms proposed in this paper consistently demonstrate the highest accuracy, ranging from 99.44% to 99.55%, with the CNN-mixer-based algorithm arriving at 99.55%. Overall, the differences in OA among these five algorithms remain relatively small.
The prediction map is depicted in Fig.. It illustrates that the five models built on the unified Transformer architecture demonstrate superior classification accuracy for the categories ’Gravel’ and ’Bitumen’.

In summary, the five models utilizing the unified hierarchical vision Transformer architecture demonstrate the highest OA across three testbed datasets. Differences in accuracies achieved by the five algorithms are generally insignificant. This also implies that the performance of the proposed HSI classification models predominantly hinges on theunified architecture, rather than the specific mixer modules that attracted attention in previous research.

2) Analysis of the training process: Based on traditional evaluation methods such as OA metric and prediction maps, the following two challenges remain difficult to address comprehensively:
(1) Is MSA indeed the pivotal module in vision Transformers for enhancing HSI classification?
(2) What are the critical differences in model characteristics between vision Transformer-based models and CNN models regarding training on hyperspectral datasets? To address these questions, as noted previously, the macro and micro-level characteristics of the models during the training process were investigated.

Fig.,, andcontain plots of three-dimensional surfaces representing the loss values for different models, following the introduction of disturbance with varying amplitudes into the ’best’ pretrained weights on the three datasets. (a) - (d) depict loss surface contours based on four typical CNN algorithms. From a macroscopic perspective, it is evident that introducing two directional disturbances to the ’best’ pretrained weights across the three datasets results in a noticeable spike in loss values calculated with these perturbed weights, thereby accentuating the contour of the surface. It is important to note that the HSI classification model based on DFFN, when exposed to substantial disturbances, produces excessively high loss values that surpass our predefined threshold of 100. This leads to scenarios, as illustrated in (b), where loss values become undefined as the values of theandaxes approach 1. This suggests that the stability of the DFFN model is the least resilient to disturbances in the ’best’ pretrained weight. (e) - (g) depict three-dimensional loss surface contours corresponding to three representative vision Transformer models, while (i) - (l) illustrate three-dimensional loss surface contours for the five algorithms proposed in this paper. The figures clearly show that the three-dimensional surface contours based on vision Transformer algorithms are notably smoother when compared to those based on CNN algorithms. However, in the case of (e), it can be observed that after introducing disturbances of varying magnitudes, the loss value along the-axis for SpectralFormer changes at a notably lower rate. This suggests that the model exhibits a weak response to disturbances when starting from the ’best’ pretrained weight. Interestingly, this phenomenon may not favor the model’s ability to converge towards an optimal solution during training. This consequence may be attributed to the newly introduced structures such as group-wise spectral embedding and cross-layer adaptive fusion. Furthermore, by investigating (g) - (f), it is apparent that as the magnitude of disturbances increases, the model’s loss value initially experiences a slight increase before eventually reaching saturation. Anideal modelshould demonstrate the characteristic of a moderate increase in loss value when disturbances are applied to the optimal training weights. It’s noteworthy that (h) corresponds to the proposed algorithm based on the CNN-mixer. This model lacks the MSA module, yet the shape of its loss surface contours across the three datasets differs significantly from those corresponding to CNN models. In contrast, the loss surface contours for vision Transformer models constructed with the five different mixers exhibit remarkably similar shapes. This further indicates that the distinctive characteristics of vision Transformer models in HSI classification primarily are derived from their unified hierarchical Transformer architecture rather than the specific mixer modules. It also suggests that the MSA module is not the fundamental reason for the differences between CNN and vision Transformer models.

Through the aforementioned loss surface contours, the overall global features of different models after disturbances can be intuitively visualized. However, these contours do not offer a microscopic analysis of the model’s local characteristics in the neighborhood of the ’best’ pretrained weight. To address this gap, this paper introduces the distribution of the maximum eigenvalue of the Hessian, aiming to quantitatively analyze the model’s gradient properties from a local perspective in the vicinity of the ’best’ pretrained weight. Fig.,, anddepict the distribution of the maximum eigenvalue of the Hessian for different models across the three datasets. Among the four typical CNN models, CNN3D and M3D-DCNN exhibit similar curves for the maximum eigenvalue of the Hessian. The magnitude of the maximum eigenvalue of the Hessian approaches zero, and negative values are virtually absent. In contrast, for DFFN, the magnitude of the maximum eigenvalue of the Hessian is relatively larger, especially on the Houston 2013 and Botswana datasets. Consequently, in general, CNN3D and M3D-DCNN, exhibit smoother local behavior around the ’best’ pretrained weights among the four classical CNN-based models. Interestingly, on the Pavia University dataset, the magnitude of the maximum eigenvalue of the Hessian for all four CNN algorithms is similar, and none of them exhibit negative values. This implies that all four CNN algorithms demonstrate remarkably smooth local behavior around the optimal points as they approach the end of model training. (e), (f), and (g) represent three classical vision Transformer models. Among these models, it can be observed that SpectralFormer’s distribution of the maximum eigenvalue of the Hessian approaches theaxis. This indicates that the model exhibits a highly flat behavior in the vicinity of optimal weights, resulting in minimal responsiveness to local perturbations. Conversely, for SSFTT, the maximum eigenvalue of the Hessian is partly situated on theside across all three datasets. This implies non-convexity in the model’s local behavior near this point, potentially hindering its ability to search for optimal weights during training. Within the GroupTransformer algorithm, the magnitude of the maximum eigenvalue of the Hessian is notably higher on the Botswana dataset compared to Houston 2013 and Pavia University. This leads to a sharper high-dimensional loss surface near the optimal point. This discrepancy may be attributed to the relatively smaller number of training samples per class in Botswana, indicating a need for improved model generalization. Upon comparing the five vision Transformer models proposed, it is evident that the horizontal coordinate of the peak value in the maximum eigenvalue distribution of the Hessian consistently exceeds 0. This signifies their capacity to maintain convexity in the vicinity of optimal points. Moreover, when compared to the other three pure (CNN-mixer, SSA-mixer, and CSA-mixer) models, the magnitude of the Hessian eigenvalue for the two hybrid-mixer (SSA+CNN-mixer and CSA+CNN-mixer) models approaches 0, indicating that hybrid-mixer models tend to exhibit smoother behavior near the optimal point. However, as shown in Fig.,, and, the smoothness of loss values after disturbance is already relatively high. Consequently, while hybrid-mixer models can further enhance local smoothness, they do not wield a decisive influence on optimizing the entire model, given the closely matched performance of all five models across the three datasets.

3) Impact of training ratio on OA: To investigate the impact of the number of training samples on the overall accuracy of HSI classification, experiments were conducted using different numbers of training samples in the three datasets with the proposed five approaches based on the unified hierarchical vision Transformer architecture. For the Houston 2013 dataset, proportions of 1%, 3%, 5%, 7%, 9%, and 11% of the data were selected for training using a stratified sampling strategy. As shown in Fig.(a), when only 1% of the sample points were used as the training dataset, the OA ranges from 87.98% to 89.98%. At this stage, the model based on the CSA-mixer exhibits an OA of 87.98%, while the model using CSA+CNN-mixer achieves an OA of 89.98%, the highest accuracy among the models. As the number of training samples increases, the OA accuracy of all the five mixer models exhibits an increasing trend. When the training sample proportion reaches 11%, the model accuracy approaches saturation. At this point, the CSA+CNN mixer model reaches a notable accuracy of 99.56%, demonstrating a slight enhancement in performance with an accuracy improvement of less than 0.14% relative to four other models. For the Botswana dataset, a stratified sampling strategy was also employed to select training data in proportions of 2%, 6%, 10%, 14%, 18%, and 22%. As illustrated in Fig.(b), when only 2% of the data was utilized as training samples, the CNN-mixer-based model records an OA of 90.54%. It is at least 0.44% less than the accuracies achieved by the other four algorithms. Additionally, as the number of training samples grows, all the five models initially exhibit significant improvements in OA accuracy, which gradually plateau as they approach saturation. When the training sample proportion reaches 22% of the total dataset, the accuracy of the five models ranges from 99.88% to 99.95%, with an OA accuracy fluctuation of no more than 0.07% among them. For the Pavia University dataset, only 0.5%, 1%, 1.5%, 2%, 2.5%, and 3% of the data were selected as training samples. This is because the number of sample points in this dataset is significantly greater than that of the previous two datasets. As shown in Fig.(c), the OA accuracy curves for the five mixer algorithms exhibit similar trends to those observed in the previous two datasets. For example, when the training samples increase from 0.5% to 3%, the OA accuracy improves dramatically from 90.99% to 99.75%, showing an impressive growth of 8.76%. With 2% of the data used as the training set, the OA accuracy of the five mixer models ranges from 99.44% to 99.55%. Overall, under the constraint of limited annotated samples, the quantity of training samples has a particularly noticeable impact on the accuracy of HSI classification. Furthermore, based on the proposed unified hierarchical vision Transformer architecture, different HSI classification models constructed with various mixers exhibit comparable performance across different datasets. This provides additional empirical support that for HSI vision Transformer classification algorithms, performance primarily relies on the unified hierarchical vision Transformer architecture rather than specific MSA or other mixer modules, especially under conditions where the proportion of training data is sufficiently substantial.

SECTION: 
A novel unified hierarchical vision Transformer architecture is developed for HSI classification. Five different vision Transformer models are constructed by configuring different mixers within the proposed unified architecture. Experiments on three commonly analyzed hyperspectral benchmark data sets with different characteristics reveal that the proposed methods outperform traditional CNN-based or vision Transformer-based HSI classification methods. Furthermore, an in-depth analysis conducted from two perspectives, disturbance robustness and the distribution of the maximum eigenvalue of the Hessian, implies that the effectiveness of vision Transformer-based HSI classification models primarily depends on the holistic unified architecture, rather than the commonly presumed MSA module. This paper provides insights into the design of vision Transformer-based neural networks for future research in HSI classification. Further work is warranted to incorporate self-supervised learning and analyze the frequency characteristics of feature space extraction in various mixer modules within the vision Transformer architecture through a self-supervised pre-training paradigm.

SECTION: 
This work was supported by the NASA grant.

SECTION: References