SECTION: Self-Supervised Learning for Ordered Three-Dimensional Structures

Recent work has proven that training large language models with self-supervised tasks and fine-tuning these models to complete new tasks in a transfer learning setting is a powerful idea, enabling the creation of models with many parameters, even with little labeled data; however, the number of domains that have harnessed these advancements has been limited. In this work, we formulate a set of geometric tasks suitable for the large-scale study of ordered three-dimensional structures, without requiring any human intervention in data labeling. We build deep rotation- and permutation-equivariant neural networks based on geometric algebra and use them to solve these tasks on both idealized and simulated three-dimensional structures. Quantifying order in complex-structured assemblies remains a long-standing challenge in materials physics; these models can elucidate the behavior of real self-assembling systems in a variety of ways, from distilling insights from learned tasks without further modification to solving new tasks with smaller amounts of labeled dataviatransfer learning.

SECTION: Background

Recent work on GPT[1], BERT[2], and related models has proven immensely successful, not only in direct language modeling tasks but also other domains including translation, question answering, and even code[3]and music[4]generation. In addition to directly performing transfer learning, prompt engineering has emerged as a promising method to leverage the power of large language models trained on diverse types of texts[5,6]. The general strategy of pretraining large models on easily-gathered unlabeled data using self-supervised tasks and then fine-tuning on more relevant labeled data is especially appealing for many scientific domains where labeled data may be difficult to come by.

In materials physics, it is well understood how structure plays a significant role in electrical, thermal, or mechanical properties of a material, and scientists target particular structures as they design new materials for desired applications. For crystals, “structure” typically refers to the basic building unit which is repeated along a periodic lattice to create a bulk crystal, but—particularly for aperiodic or non-crystalline materials—it can also refer to any symmetry or non-random ordering present in the arrangements of particles or atoms. Assessing order and its evolution in three-dimensional structures is a challenging, but critical method for understanding the self-assembly and growth of complex materials; particularly as the scope and magnitude of experiment and simulation data analysis continues to expand, machine learning techniques that are able to leverage large amounts of unlabeled data will become ever more crucial.

In this work, we use self-supervised learning (SSL) tasks that can broadly be used to train models for quantifying order and distinguishing assemblies in non-idealized material structures. The choice of SSL for this application was inspired by previous work that has developed SSL tasks for three-dimensional point clouds, which are a natural choice for representing three-dimensional positional data.Thabet et al.[7]formulated self-supervised tasks in terms of a space-filling curve;Sharma and Kaul[8]trained deep networks to model data based on a three-dimensional cover tree; the method proposed inEckart et al.[9]models simple, soft “patches” of 3D point clouds in order to reconstruct its inputs; andPang et al.[10]spatially mask patches of point clouds and reconstruct the masked patches using learned networks. However, in contrast to point clouds commonly found in robotics and autonomous vehicles, the types of point clouds discussed here from physics, chemistry, biology, and materials science are comparatively small and based on short-range interactions—often on the order of dozens of points or smaller—and can contain richer information, such as particle type embeddings or atomic features derived from chemical knowledge, rather than solely geometric information. Furthermore, rotation equivariance is required for such physical models for data efficiency and model correctness. In the domain of chemistry, the MolCLR approach[11]uses SSL tasks to learn geometric representations of molecules by pretraining models on molecular graph data using a contrastive loss and graph masking approach. With 3D Infomax[12], networks are optimized to generate 3D information from the latent representation learned from the mutual information between a given molecules 2D molecular graph and its 3D geometric representation. The GEM framework[13]uses geometry-aware graph neural networks (GNNs) to predict individual bond distance, angles, and entire distance matrices for a given molecule. The GeoSSL framework[14]focuses on denoising the coordinates of atoms that make up a molecule. These approaches show promise in using self-supervised tasks with geometric data for learning about structural properties of materials—but were designed with chemistry of molecules rather than crystallizing assemblies in mind.

Previous work with more similar applications to ours are self-supervised frameworks operating on periodic materials systems. The Crystal Diffusion Variational Autoencoders[15]method uses SE(3) equivariant GNNs with periodicity to model crystalline unit cells through a variational autoencoding process that can generate stable materials that are optimized for a particular property. The Crystal Twins framework[16]uses GNNs and data augmentations—including masking and adding random noise to coordinates—to formulate a self-supervised learning task. Unlike these examples, the self-supervised tasks presented in this work operate on the basis of individual particles rather than unit cells, and are therefore usable for arbitrary self-assembly data, aperiodic structures such as quasicrystals, or amorphous structures such as glasses and liquid crystals, rather than being restricted to unit cells of crystalline structures. Moreover, while the two frameworks mentioned above specifically target crystalline structures insofar as they are looking for optimizing specific material properties, the relationship between structure and property is well-understood in materials science. The framework we put forth serves an ultimately different function: to study the significantly more mysterious formation and growth of structure and provide insights into how the evolution of order occurs in materials as they form into bulk crystals.

In this work, we present a set of generic tasks applicable to three-dimensional point clouds without any labeling requirements. We train rotation- and permutation-equivariant deep neural networks to perform these tasks using either real self-assembly simulation trajectories or idealized crystal structure unit cells from the AFLOW Encyclopedia of Crystallographic Prototypes[17,18]. We show that we can make use of the embedding-space representations of models trained on ideal structures to analyze self-assembled structures. We evaluate the performance of our self-supervised tasks against existing metrics for quantifying order in materials science, and outperform them in the ability to distinguish structures between highly disordered systems, such as the liquids that form prior to crystallization from two different crystal structures. We believe this is a significant scientific advance in its own right, as there exist virtually no mechanisms in the physical sciences for understanding order in systems of arbitrary liquids or glasses with no symmetry or periodicity. Finally, we demonstrate the breadth of information learned by models from these tasks by performing transfer learning from one self-supervised task to another.

SECTION: Methods

SECTION: Data sources

We use two types of data—outlined in Figure1—to train and evaluate models on self-supervised tasks in this work. The first type of data used in training are nearly-ideal three-dimensional bulk crystal structures—which are straightforward to generate from the AFLOW[17,18]database of structures. The second type of data are simulated molecular dynamics assemblies of particles, which are less straightforward to generate without advanced computational resources. Methods used to generate both types of data are explained below.

We generate representative samples of bulk crystal structures by selecting one-component unit cells of 3D structures from the AFLOW Encyclopedia of Crystallographic Prototypes[17,18](listed in AppendixB) and adding random noise to the coordinates after replicating the unit cells along a crystal lattice. An example unit cell and replicated periodic crystal structure are shown in Figure1(a) and (b), respectively. Because crystal structures typically consist of successive shells of highly symmetric arrangements of particles with the same radial distance, adding random noise to the coordinates allows us to sample the permutations of particles from the farthest neighbor shells of a given structure for a fixed number of nearest neighbors. We show a few such sample permutations in Figure1(d).

Unit cells are easy to generate and because of this, can provide valuable information about the bulk behavior of materials. Unfortunately, many types of simulations do not readily yield such idealized data: particles can form disordered or poorly-ordered structures, they can contain grain boundaries and other defects, or expose distinct surface structures. Moreover, aperiodic structures cannot be generated with a unit cell, as they do not have periodicity in three dimensions. An icosahedral quasicrystal—which has rotational symmetry but is aperiodic—is shown in Figure1(c). In addition, self-assembly data allows for the study ofhowordered structures are formed from the fluid phase, which idealized structures cannot provide. To account for these more realistic configurations of particles, we perform molecular dynamics simulations of systems that are thermalized and slowly cooled using HOOMD-blue[19,20]. Details of particle interactions and simulation dynamics are described in AppendixCand full trajectories are available in the Supplementary Information.

SECTION: Geometric algebra attention networks

One of the key architectural requirements of performing transfer learning on the variety of geometric, semantic, rotation-equivariant, and rotation-invariant tasks we present here is control over the equivariance of the signals passing through the network; to that end, we build flexible deep learning architectures by composing geometric algebra attention layers[21]. These layers attain rotation equivariance by deriving their geometric calculations from the geometric algebra, as well as permutation equivariance through an attention mechanism. For this work, we utilize pairwise attention; briefly, this consists of computing the geometric productsbetween all pairs of input multivectors111In the geometric algebra in three dimensions, multivectors are a linear combination of scalars, vectors, bivectors, and trivectors—representing 0, 1, 2, and 3-dimensional quantities, respectively—so that vectors are a subset of multivectors; here, we use networks with multivector intermediates and convert input vectors to their multivector numerical representation as the first step in the network.(with associated type embeddings222In this work, we only analyze single-component systems, but we retain the type embeddings as network inputs to account for additional particle features and multicomponent systems in the future. Within the network, the values—the rotation-invariant representations of the geometry of each neighboring bond—are learned.) in a given point cloud. The pairwise geometric products are converted into a geometric embeddingviatheir rotation-invariant attributes (which include magnitudes of vectors and bivectors, as well as values of scalars and trivectors—represented generally for any multivector by theinvariantsfunction) using a multilayer perceptron (MLP). In this work we generate a full set of rotation-invariant attributes including all input vectors forming the product, rather than only using the rotation-invariant attributes of the final product as in reference[21]. The geometric embedding is combined with a node-level signal summary of the pair of input points (the rotation-invariant attributes passed in as inputs to the network are simply particle type embeddings for both particles in the bond); the summary representation is derived from a learned linear projectionusing a learned linear projection, which are passed through another MLPto generate attention score logits. A new rotation-invariant outputor rotation-equivariant output(using a rescaling MLPand learned scalar constants) can be calculated as follows:

We note that the formulas in Equation1produce a permutation-equivariant result: for each input bond, a value (or) is produced as an output. This is suitable for composing multiple layers in a deep learning context. To instead produce a permutation-invariant result—which creates a single summary value for an entire point cloud of bonds—the softmax and summation are applied overand, rather than only over. For the work here, we use point clouds of fixed size—obtained by finding the 20 nearest neighbors in space around each particle—although the attention-based framework is flexible and supports heterogeneous numbers of neighbors for each particle, as would be expected when calculating neighbors within fixed cutoff distance or using a Voronoi tessellation. Further details of normalization layers and other hyperparameters—including code to build and train models on the datasets used here—are available in AppendixDand the Supplementary Information.

SECTION: Self-supervised tasks

As detailed in Figure2, we use a common core architecture for all self-supervised tasks. This architecture takes in a point cloud of nearest-neighbor bonds and associated type embeddings and refines two types of signals in parallel: a rotation-invariant value associated with each bond, alongside a rotation-equivariant multivector for each bond. Using full multivector intermediate geometric values within the network and use of learned rotation-invariant and -equivariant transformations are two improvements over the architectures shown in reference[21]. Additional task-specific layers are added to the core architecture to specialize it for each task; for example, a simple MLP can be added to the rotation-invariant signal for each bond to classify bonds, or the vector component can be extracted from the rotation-equivariant output to produce a denoising autoencoder. For geometric tasks producing points or point clouds as output, we train networks using the mean square error loss; for classification tasks, we use categorical crossentropy.

Autoencoders have a rich history of usage in machine learning for many years and can be seen as the least constrained self-supervised task because they impose no restrictions on the structure of their input data. However, this freedom of problem specification is counterbalanced by complexity in model formulation: autoencoder models must usually be designed to restrict information in some way to prevent models from learning trivial identity functions. Using autoencoding-based strategies to initialize the weights of deep neural networks was a key area of research in the development of modern deep learning[23,24]. Pretrainingviagreedy autoencoding-based strategies bears many similarities to the approaches described in this work, however the autoencoders presented in this work are trained end-to-end, rather than locally for each layer, before potentially performing transfer learning on another task. The work presented here is also focused on a broader set of tasks than solely autoencoding. Variational autoencoders[22]are a popular architecture imposing additional constraints on the embedding space learned by the model. In this work, we train variational autoencoders to reproduce input point clouds. As a bottleneck, we produce two vectors, calculate their singular value decompositions, and take the cross product to produce an orthonormal set of basis vectors. We also produce a rotation-invariant signal that is passed through a variational autoencoder to generate a smooth embedding space. Using the orientation information from the orthonormal basis set and the geometric information from the embedding space, we can train a decoder to reconstruct the input point cloud using a mean square error loss on each coordinate dimension.

Denoising autoencoders are one of the simplest adaptations of autoencoders to lessen the difficulty of models learning too-simplistic identity functions. We teach models to produce an original point cloud from a perturbed version of the same point cloud using the mean square error loss. The perturbation applied to each coordinate of each point is generated from a Gaussian distribution with standard deviation. A rigid rotation and translation are applied to the perturbations of the entire point cloud to remove any net rotation or translation from the random noise. The difficulty of this task can be tuned by adjusting the magnitude of the added noise. We use a single permutation-equivariant, rotation-equivariant attention layer atop the core as the architecture for the denoising task.

In many cases when studying physical systems, we have additional information that could carry meaningful signal when used as labels. Individual frames from a self-assembly trajectory could be fed in with observables/physical conditions as a label to allow models to distinguish structures as a function of a particular observable/physical condition. For self-assembly simulations, we use the physical condition of temperature, synonymous with time elapsed in the cooling simulation for the frame classification task. For the crystal structure prototypes drawn from the database, we use the structure type as a label, as shown in Figure3. We use a typical classification MLP on top of a permutation-invariant attention reduction over all of the bonds in the point cloud to generate per-class logits.

For this task, we shift all bonds of an input point cloud by a Gaussian-distributed random vector with a standard deviation ofand train models to predict this displacement vector. Although this task would be trivial for most crystal structures given clean neighbor shells due to their high symmetry, for a large set of structures with differing neighbor shell sizes, we find this task to extract useful information about structure. The difficulty of this task can be adjusted by tuning the magnitude of the displacement: larger-magnitude displacements are easier to predict, while smaller displacements can be washed out by the disorder inherent in the structures under consideration. As shown in Figure2(c), we use a permutation-invariant, rotation-equivariant attention reduction over the bonds in the point cloud to predict the displacement vector.

This task randomly adds noise (Gaussian distributed, with standard deviation) to half of the bonds of each point cloud. A permutation-equivariant classifier is then trained to identify, for each bond, whether it was perturbed or not. The difficulty of this task depends on how disordered or complex the structures under consideration are, and the magnitude of the noise that is added to bonds. As shown in Figure2(d), we use a permutation-equivariant attention calculation, followed by an MLP producing two class logits, to classify bonds as perturbed or pristine.

This task is most similar to the masked language modeling[2,1], point cloud[10], and graph-level[11]self-supervised tasks: for each point cloud, we remove the nearest bond from the set of inputs, and teach the model to predict that bond. We choose the nearest bond, to decrease degeneracy of the problem space; a randomly-selected bond could choose particles that are not part of a full neighbor shell, which would require a multimodal output for every remaining missing particle in the neighbor shell, as shown in Figure1(d). As shown in Figure2(c), we use a permutation-invariant, rotation-equivariant attention reduction over the bonds in the point cloud to predict the nearest bond for the point cloud.

SECTION: Results

SECTION: Mapping structure space through learned embeddings

We can use embeddings to understand how well the models trained by self-supervised tasks are able to distinguish between different types of crystal structures. We first train a network on the frame classification task with crystal structure prototype data, then evaluate the learned internal representation of the model on self-assembly simulations. The Principal Component Analysis (PCA)[25]projection of the data in two dimensions is plotted in Figure3(a), showing that the model is able to learn reasonable representations of different self-assembled structures even though it was trained on idealized unit cell data. In addition, the embedding for a cooling trajectory of an icosahedral quasicrystal is included, which is not in the training set given that there is no unit cell that can generate this aperiodic structure. Although the quasicrystal—like the periodic crystal structures—consists of locally symmetric motifs, they do not tile space periodically. We use the quasicrystal as a sample structure that is significantly different from the structures used to train the self-supervised model, to show that reasonable embeddings are generated for structure types the model has not seen before.

To quantitatively compare the embeddings generated through self-supervised tasks, we select a few different simulation snapshots showcasing phase transitions and different structures and use the generated embeddings asorder parametersto distinguish between each pair of structures in a binary classification task. For each self-supervised task, as well as three other methods that are standard for materials systems—described in AppendixF—we evaluate featurizations in their capacity to distinguish gas (), liquid (), and solid () structures for self-assembly systems that will form the same final structure () or two different structures () at low temperature. We evaluate the featurizations and use the first principal component as a score to distinguish between the two snapshots being evaluated, and quantify the classification capability of each representation through the area under the curve of a receiver operating characteristic[26]in a form of zero-shot learning in Table1. While all methods behave sensibly—distinguishing two identical liquids () should perform poorly, and distinguishing a gas from a liquid () should perform well—the other phase transitions showcase the relative strengths and weaknesses of the representations. Identifying liquid-solid transitions is one of the main purposes for the Steinhardt[27]and neighbor-averaged spherical harmonic[28]representations, which have the advantage of strong symmetry priors enforced by use of spherical harmonics in their featurizations, so it is no surprise that these methods most accurately identify the crystallization process. The arguably most difficult task in Table1is distinguishing between liquids that will form two different structures,. In this case, the learned representations—with the exception of the noisy bond identification task—outperform all existing methods. In the scientific literature, this task has virtually no precedent; whileHu and Tanaka[29]use the Steinhardt order parameter to distinguish between competing liquid motifs prior to crystallization, they tune parameters carefully for the specific liquid structures under investigation and it is not obvious that this method can be generalized easily. The frame identification task, shown as a kernel density estimate in Figure3(b) alongside the Steinhardt embedding density, performs the best here—despite being trainedonlyon perturbed ideal crystalline unit cells—and the geometric algebra attention networks are able to to selectively transmit more useful information about each bond than the averaged representations over all bonds as the existing spherical harmonic-based methods do.

Embeddings such as these can be used to qualitatively understand the structural behavior of systems through visual analysis. They could easily be made quantitatively useful through
a variety of means depending on the application, including diffusion maps[30]for studying the manifold formed by the data, probability density modeling through normalizing flows[31]or other methods, or as the basis for transfer learning—which we showcase in the next section.

SECTION: Transfer learning between tasks

Inspired by recent work using networks initially trained on language modeling tasks to perform a variety of other language-related functions, we probe how well models trained on one structural task perform on other tasks with a limited amount of new labeled training data. As a more systematic, quantifiable stand-in for arbitrary new labeled data—which could come from physical experiments, expensive simulations, or other means—we perform transfer learning between all pairs of tasks listed in the Methods section using crystal structure prototype data. Examples of relevant downstream tasks include analyzing structures of liquid-liquid separation in cells[32], proteins[33], or kinetic pathways to crystallization via competing liquids[34,29]. We first train networks on asourcetask, then reuse the core of those trained networks as an initial state to train networks on atargettask. For retraining the networks on the target task, we limit the training data to a new randomly-chosen subset of the available data. We show results for both allowing the core weights to be updated, as well as “freezing” the core weights and only retraining the non-core portions of the network. For each pair of tasks, we show the best validation set performance (the mean absolute error for geometric tasks, orto keep lower values better for classification tasks) achieved on the target task as a function of the fraction of available training data for fine-tuning.

As shown in Figure4and AppendixE, in many cases, models trained on different tasks learn to extract different types of information from their training data, leading to improved performance compared to directly training for the given target task (indicated by black dashed lines). In particular, most pretraining tasks are helpful for frame classification: by adding a single attention layer and MLP to the pretrained core, most networks attain higher accuracy than networks trained solely on the frame classification task. This makes sense because frame classification of different structures is the most abstract, least geometrically precise task among those described here; the higher-dimensional labels for most of the self-supervised tasks allow the networks to extract more information that is then applicable to the less-precise task of frame classification. In contrast, the nearest bond and shift identification tasks seem to learn fairly distinct localized, lower-level representations of the data: in most cases, learning on other tasks is not substantially helpful for these cases, aside from retraining a shift identification network on the nearest bond task. The broad applicability and mutual similarity of the autoencoder and denoising autoencoder tasks shows through their competitive performance on other tasks and usefulness in pretraining against each other, respectively; however, some networks—especially those pretrained on the highly localized nearest bond task—seem to have difficulty converging to a stable, low-error state when retraining on the autoencoder task with smaller datasets. In contrast, networks pretrained on the nearest bond and shift identification tasks and fine-tuned on the autoencoder task with the full training dataset perform surprisingly well, demonstrating that domain-specific complementary, but related, self-supervised objectives can be helpful even for very generic tasks given an abundance of data. We would expect that utilizing all self-supervised representations simultaneously—for example by creating a new “core” architecture that concatenates the learned representation from each self-supervised task and applies the final layers as usual—would perform well on all tasks; however, due to constraints of hardware capacity, we leave this architecture for future work. Overall, by comparing the performance of networks trained with the core weights frozen (blue lines) and with pretrained weights solely used as initialization (red lines), we can see that, in most cases, the tasks are sufficiently distinct that fundamental changes to the earlier representations learned in the network are helpful when adapting to a new task. This agrees with findings in large networks trained to model languages, where earlier layers learn lower-level representations of their inputs[35].

SECTION: Conclusion

While generating large amounts of data for self-assembly simulations is more computationally feasible now—the study inDshemuchadse et al.[36]consisted of over 8,500 simulations with thousands of particles in each simulation—understanding the fine-scale structural behavior of these systems is a much more complex task. Historically, automated analysis of three-dimensional structural data has been restricted to fairly low-dimensional representations[27,37]or shallow machine learning on hand-crafted featurizations[28]. Self-supervised methods such as those presented here harness the power of geometric deep learning to build powerful, generic feature learners that can yield insight into the behavior of systems on a much finer scale than would be feasible through manual analysis. By designing deep learning architectures for physically-relevant data symmetries, we can begin to apply standard deep learning tools such as analysis of internal representations to solve new types of problems. Although we have focused primarily on the rotation-invariant signal passing through a single point in the networks thus far, future work could quantify the types of information passing through the geometric (rotation-equivariant) channels within the network, as well as learned attention weights throughout the network in order to identify which particles and signals are most important for model predictions.

In summary, we have presented rotation- and permutation-equivariant neural network architectures and a collection of self-supervised tasks suitable for analyzing ordered and disordered three-dimensional structures. Particle-centered point cloud representations of materials are useful in understanding many types of non-ideal structures that can not be represented using unit cells, such as disordered and aperiodic systems, and we hope that the datasets and tasks presented here motivate new architectural work on these data structures. The novel architectures and models presented here can provide new insights to self-assembly and structure formation by applying self-supervised learning tasks, analyzing the embedding-space representations, or using transfer learning to improve performance, all of which can help leverage deep learning to solve new types of problems in materials science.

SECTION: References

SECTION: Appendix AFrame classification for phase identification

We show direct application of the frame classification task to analyze self-assembling systems and identify phase behavior of a cooling system. Although capturing the gas–liquid phase transition is straightforward through the use of the local density, capturing transitions from liquid to crystal for arbitrary structures can be much more difficult; while physicists have used methods such as the Steinhardt bond-orientational order parameters[27], these typically require a fairly detailed understanding of the structure that will be formed in order to robustly analyze systems[38]. Instead, we can use this SSL task to automatically find the distinct phases in a simulation without needing to interpret or understand the structure that is being assembled. We do this by taking every fourth frame from a trajectory as training data and training a classifier to associate the structures found in each frame—each corresponding to a distinct temperature—to that particular frame. We analyze the phase behavior by visualizing the histogram of classification predictions found in frames excluded from training—in this particular case, we evaluate predictions for each frame immediately after every training set frame.

As shown in Figure5, even this simple self-supervised analysis can quickly help us understand the dynamical phase behavior of self-assembling systems. Incorporating our understanding of the phase behavior of materials as a function of temperature, we can identify gas–liquid, gas–solid, and liquid–solid phase transitions at a glance for large numbers of trajectories.

SECTION: Appendix BCrystallographic prototypes used for training

We use multiple replicas of unit cells—each replicated to at least 4096 particles and with different levels of Gaussian noise (with standard deviations of,, and) applied to each coordinate—to emulate thermal noise typically observed in real self-assembled systems. Thermal noise is important not only to provide a more realistic distribution of bond distances and angles for the networks to learn, but also to sample the different permutations of ways to choose neighbors. Fifty different single-component crystal structure prototypes are taken from the AFLOW Encyclopedia of Crystallographic Prototypes[17,18]for use in self-supervised training. Structures are named based on their crystal system (indicated by the first character) and unit cell centering (indicated by the second character)—which together denote the Bravais lattice, the number of particles in the unit cell, and a representative real-world material that forms the given structure.

SECTION: Appendix CMolecular dynamics simulation details

Particles interactviaisotropic Oscillatory Pair Potential (OPP) and Lennard-Jones–Gauss (LJG) interactions—described in reference[36]—with potential energy functions:

Systems of 4096 particles are slowly cooled from a high temperatureto a lower temperatureovertimesteps of sizeeach, whereandare the fundamental energy and time scales used for simulation, andis the Boltzmann constant. Pairwise potential parameters,,, andfor the self-assembly trajectories used in this work are listed in Table3and full trajectories are available in the Supplementary Information.

SECTION: Appendix DTraining hyperparameters

All networks used in this work produce and consume rotation-invariant valueswith 32 dimensions. MLPs,, andhave one hidden layer of 64 dimensions and ReLU activation. All optimization is carried out using the Adam[40]optimizer with a batch size of 4 point clouds and gradient accumulation over 16 batches. For static datasets (consisting of the autoencoding, frame identification, and nearest bond tasks), 30% of the data are held back for the validation set. The learning rate is reduced by a factor of 0.5 at every epoch during which the validation loss does not decrease, and training is halted after 2 epochs of no validation loss improvement, or 128 total epochs. For dynamic datasets (consisting of the denoising, noisy bond identification, and shift identification tasks), 2048 batches are counted as a training epoch and 512 batches as a validation epoch. The learning rate is reduced by a factor of 0.5 after 4 epochs without improvement in the validation set loss, and training is terminated after 10 epochs without improvement, or 128 total epochs.

SECTION: Appendix ETransfer learning performance

We list the key metrics from Figure4below for each data fraction (minor x axis) in Tables4–8.

SECTION: Appendix FEmbedding snapshot study details

As baselines for comparison, we generate featurizations based on the Steinhardt order parameters[27], local neighbor-averaged spherical harmonics[28], and a simple featurization based on radial distances. The Steinhardt order parameters are defined for a given spherical harmonic degreeand set of near-neighbors; for our featurization, we calculate a vector of Steinhardt order parameters for a range of spherical harmonic degrees. To account for the variety of neighborhoods that could be relevant, we also compute the order parameters using a range of nearest neighborhood sizes, from the nearest 4 to 20 particles. The local neighborhood spherical harmonicscreate a rotation-invariant representation using the inertia tensor of each local environment and use the same neighbor accumulation process for each set of spherical harmonics, from 4 to 20 particles. The normalized radial distance featurization serves as a proxy for the use of radial distribution functions in materials science, which are probability distributions of finding pairs of particles at a given distance. In this case, we simply take the magnitude of each nearest-neighbor bond—up to 20 nearest neighbors, sorted by radial distance—and normalize by the nearest neighbor distance to make the representation scale-invariant, as the other featurizations are. This simple representation should be able to distinguish phase transitions and ordered structures, although it may have difficulties in some cases where the structures being compared are highly similar. The embedding methods used, as well as their dimensionality, are listed in Table9.

Details of systems used in the zero-shot phase distinction study described in Table1are listed here.

: Two liquids at the same temperature that will later form-Cu. This system is a control experiment: identical liquids should be difficult or impossible to distinguish to a significant degree.

: A gas and liquid that will later form-W. This models a simple phase transition between low-density and high-density disordered phases.

: A liquid that forms-W andsolid. This models a simple disorder-order transition.

: A gas that will form an icosahedral quasicrystal and the quasicrystal itself. This models a disorder-order transition to a complex ordered structure.

: Two liquids that will form-Cu and-Mg later. These solid structures are very similar, having identical representations when using nearest-neighbor graphs.

: Two solid structures of-Cu and-Mg. Most methods developed to distinguish solid structures should be able to handle this baseline.