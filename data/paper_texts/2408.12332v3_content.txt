SECTION: Simplifying Random Forests’Probabilistic Forecasts††thanks:We thank conference participants at the International Symposium on Forecasting (Dijon, 2024) as well as Andreas Eberl for helpful comments.

Since their introduction byBreiman, (2001), Random Forests (RFs) have proven to be useful for both classification and regression tasks.
The RF prediction of a previously unseen observation can be represented as a weighted sum of all training sample observations.
This nearest-neighbor-type representation is useful, among other things, for constructing forecast distributions(Meinshausen,,2006).
In this paper, we consider simplifying RF-based forecast distributions by sparsifying them. That is, we focus on a small subset of nearest neighbors while setting the remaining weights to zero.
This sparsification step greatly improves the interpretability of RF predictions. It can be applied to any forecasting task without re-training existing RF models. In empirical experiments, we document that the simplified predictions can be similar to or exceed the original ones in terms of forecasting performance. We explore the statistical sources of this finding via a stylized analytical model of RFs.
The model suggests that simplification is particularly promising if the unknown true forecast distribution contains many small weights that are estimated imprecisely.

SECTION: 1Introduction

Many statisticians agree that forecast distributions are preferable to mere point forecasts. Correspondingly, an active literature is concerned with making statistical forecast distributions in meteorology(e.g. Rasp and Lerch,,2018), economics(e.g. Krüger et al.,,2017), energy(e.g. Taieb et al.,,2021), epidemiology(e.g. Cramer et al.,,2022), and other fields.
Nevertheless, point predictions still dominate in many practical settings in policy, business, and society.
As argued byRaftery, (2016), the cognitive load that forecast distributions impose upon their users may be an important bottleneck impeding their adoption.
Motivated by this possibility, we consider simplifying the forecast distributions produced by Random Forests(RFs; Breiman,,2001; Meinshausen,,2006), and study how simplification affects statistical forecasting performance.
While our main focus is on probabilistic forecasting, the method we propose can also be used to simplify point forecasts for the mean.

More specifically, we approximate an RF forecast distribution for a continuous scalar outcome by a discrete distribution withsupport points, whereis a user-determined parameter anddenotes the number of training samples.
The resulting forecast distribution can be cast as a collection ofscenarios, each occurring with a specified probability.
Ifis sufficiently small, this setup can effectively be communicated to non-statisticians.
For example,Altig et al., (2022)use it to survey business executives about firm outcomes like future sales growth and employment.Abbas and Howard, (2015, Chapter 35)provide a textbook discussion from a decision analysis perspective.

Our approximation builds upon the fact that RFs can be cast as a nearest-neighbor-like method(Lin and Jeon,,2006; Meinshausen,,2006):
The forecast distribution for a test sample observation with feature vectoris a discrete distribution with support points given by the training sample outcomesand corresponding probability weights.Meinshausen, (2006)called these forecast distributions Quantile Regression Forests (QRFs). In the following, we do not distinguish between QRFs and RFs since the former can be constructed from the latter without any additional model fitting. As detailed in Section2.1, the weightreflects the similarity betweenand, the feature vector of the-th training sample observation. Consider a given test caseand the setcontaining the indices of thelargest weights for this test case, where.
We then set

That is, we retain only thelargest weights, and re-scale them such that they sum to one.
All other weights are set to zero. Figure1provides a schematic description. Settingrecovers the weights of the initial RF’s mean and the QRF’s distribution forecast, which is described in more detail in Section2.111Prediction as in Equations4and5is unaltered, except for the substitution ofby.Larger values ofcorrespond to a larger number of scenarios.
This increases the complexity of the forecast distribution, but may be beneficial in terms of statistical forecasting performance.

We provide a practical illustration of our method by considering labor market survey data provided byDIW, (2022).
The data set is an openly available subsample of the German Socioeconomic Panel(SOEP; Goebel et al.,,2019).
It is recorded between 2015 and 2019, coveringobservations corresponding todistinct persons after preprocessing. Many participants were asked repeatedly over the survey years, so that the data has a panel structure.
Our goal here is to predict a person’s annual salary in Euros based on socioeconomic regressors such as age, education, family status, and the industry in which the person is employed.
We use data from 2015–2018 for training and 2019 for testing, and consider a Topmodel withalong with a full RF.
Compared to the full RF, the Topmodel is about 6% worse in terms of distribution forecasting performance (as measured by the CRPS, introduced in Section2.2), and about 3% better in terms of point forecasting performance (as measured by the squared error).
AppendixAprovides details on the data set and RF forecasting performance.

To illustrate, consider a fully employed 47-year-old male, living alone and working in the financial sector.
We aim to predict his salary in 2019.
A full RF predicts a mean income of, placing non-zero weight on 171 training sample observations.
The simplified Top5 model utilizes only the five largest of these 171 weights which (before re-normalization) sum up to 0.47.
To re-normalize, we simply multiply the five remaining weights by.
The five support points (the training sample observations that correspond to the remaining weights) are listed in Table1, together with their weights.
With those, the Top5 model predicts a mean income of.

For communicating these results, the five remaining weights and support points can be cast as scenarios, spanning an income range fromtoEuros.
The most likely scenario, occurring with a probability of, involves an income ofEuros.
Due to the structure of RF forecast distributions, the selected scenarios are directly associated with five actual observations occurring in our training sample.
In particular, three of the five scenarios (number 1, 4 and 5) listed in Table1involve responses of the same individual in previous years. This situation is natural since some of the regressors are nearly constant over time, so that the test pointis likely to be similar to training datacorresponding to the same individual.
Using the same individual’s past incomes also seems plausible from a practical perspective.222If one wanted to avoid the use of data from the same individual, one could reduce the data set to a single observation per individual, and otherwise use the same methodology.Table1also yields relevant (albeit implicit) substantive information:
For example, the fact that all five scenarios involve individuals of the same gender, family status and industry sector indicate that the RF model considers these regressors highly relevant for prediction. Conversely, the five scenarios cover all survey years present in the training data (2015-2018), indicating that time variation in incomes is less important.

We approach the interpretability of a RF from a different angle compared to most other existing literature on the topic:
InBreiman,’s original work on RFs, feature permutation was introduced, effectively quantifying the influence of each feature on the overall performance of the model.
This method is still widely used and implemented in standard software packages(Pedregosa et al.,,2011).Biau and Scornet, (2016, Section 5)provide further discussion.
This perspective on interpretation is also commonly used for other forecasting models, such as neural networks, see e.g.Lundberg and Lee, (2017).
Visualizations of RFs have also been proposed(Zhao et al.,,2019; Haddouchi and Berrado,,2019).
In addition, alternative notions of sparsity have been considered in the context of tree-type models.
For example,Nan et al., (2016)use the notion of sparsity in terms of feature usage in order to speed up predictions.
Several other studies consider combinations of simple prediction rules, building upon ideas inFriedman and Popescu, (2008).
The ‘node harvest’ method byMeinshausen, (2010)is particularly interesting since it can be cast as a weighted empirical distribution, and can thus be used for probabilistic prediction.
Node harvest first generates a large number of ‘nodes’, each of which defines a simple subspace of the predictor space.
It then uses a constrained optimization problem to select the nodes that are most useful for prediction.
Although sparsity is not enforced,Meinshausen,finds that the optimal solution often involves a small number of nodes only.
However, this type of sparsity (with respect to nodes) is different from the type of sparsity we consider, which is with respect to the number of training sample observations being considered for prediction.
Indeed, these two notions of sparsity seem highly complementary:
A forecast distribution constructed via node harvest may involve a large number of training sample observations, i.e. be far from sparse in our sense.
Conversely, a forecast distribution constructed using our proposed methodology implicitly involves many regression trees, thus being far from sparse inMeinshausen,’s sense.

Finally, many extensions and modifications ofBreiman,’s original RF have been proposed in the literature, typically with the aim of improving statistical performance (either empirically, or in terms of theoretical properties).
SeeBiau and Scornet, (2016)for a survey, and for exampleBeck et al., (2023)for a forecast combination perspective,Cevid et al., (2022)for a multivariate model that links RFs to kernel-based methods, andWager and Athey, (2018)for an adaption to causal inference.
By contrast, we focus on a standard, univariate RF implementation, and post-process its forecast distribution in a way that makes it easier to interpret.
The procedure we propose can easily be applied to other RF variants (or even to prediction methods other than RFs), provided that they can be represented as discrete distributions of the type described above.

The remainder of this paper is organized as follows:
Section2introduces our methodological setup, including RFs and methods for evaluating forecast distributions. In Section3, we study the performance of our ‘Top’ simplification in a series of empirical experiments based on 18 data sets for which RFs have been found to perform well, as compared to deep neural network models(Grinsztajn et al.,,2022).
Our findings indicate that already for, the simplified RFs can perform on a similar level compared to the full counterpart for both probabilistic and point forecasts, depending on the data set.
Considering probabilistic forecasts, for, the median performance across all data sets is equivalent to the full RFs and consideringeven increases the median performance slightly.
For point (mean) forecasts,even increases performance slightly.
Even though very sparse choices likemay come with a significant performance decrease, we argue that they may be worthwhile if ease of communication is a main concern.
We further show that our results are qualitatively robust to different hyperparameter choices.
In order to rationalize the empirical results, Section4then considers a detailed analytical example that models the weights estimated by RFs as a random draw from a Dirichlet-type distribution.
The example also features a true vector of weights that may be either similar to, or different from, the estimated weights.
This setup is useful to study how a simplifying approximation similar to Topaffects statistical forecasting performance.
In a nutshell, the amount of noise in the estimated weights determines whether or not the simplification comes at a high cost in terms of performance.
Perhaps surprisingly, one can construct examples in which simplification improves performance:
this result arises when the largest weights are estimated precisely, whereas smaller weights are more noisy.
Focusing on the largest weights then constitutes a beneficial form of shrinkage.
When the small weights are estimated precisely, however, simplification is harmful in terms of performance.
The appendix contains further empirical results and detailed derivations for Section4.
Replication materials are available athttps://github.com/kosnil/simplify_rf_dist.

SECTION: 2Methodological Setup

In this section, we describe RF based forecasting as well as the forecast evaluation methods we consider.

SECTION: 2.1Forecasting Methods

Here we describe Random Forests and their probabilistic cousins, Quantile Regression Forests.
We followLin and Jeon, (2006)andMeinshausen, (2006)who emphasize the perspective of RF predictions as a weighted sum over training observations. We refer toHastie et al., (2009)for a textbook presentation of RFs.

Our goal is to fit a univariate forecasting model.
That is, we have some data setof training set size, whereis a-dimensional vector of features, andis a real-valued outcome.
We use this data set to train our model, such that some choice of loss functionis minimized:

In a typical regression context, the functionmapsto the real line in order to estimate the conditional expectation functional. A popular loss function is given by squared error, with. Let furtherdenote the feature space, i.e., the space in which the individual input samplesexist.

An RF is an ensemble of individual regression trees, each denoted by, wheredescribes the configuration of the tree.
At each node, a single tree greedily splitsand rectangular subspaces thereof into two further rectangular subspaces, such that the lossis minimized.
Each resulting subspace corresponds to a leafwhereis the total number of leaves.
Each samplecan only occur in one leaf or, put differently, when dropping a sampledown the tree, it can only fall into one leaf. This leaf is denotedfor tree. For a single tree, a predictionfor a new sampleis obtained by taking the mean of all training samples within leaf.
This can be expressed as

where the weightis equal to zero for all training samplesthat fall into leaves other than, and is equal to one over the leaf size for all training samples that fall into:

Motivated by the lack of stability and tendency to overfit of individual trees, RFs buildtrees, based onbootstrap samples of, and consider their average prediction.
Moreover, in each split within each tree, it is common to consider only a random subsample ofout ofregressors. This step aims to diversify the ensemble of trees by avoiding excessive use of the same regressors for splitting. Common choices forareor(Probst et al.,,2019), wherefloors the real numberto the nearest integer. The RF mean prediction can thus be expressed as

where

is the weight for training sample, averaged across alltrees. By construction, the weightsare non-negative and sum to one.
Thus,can be interpreted as the empirically estimated probability that the new test sample observation is equal to.

Conceptually,is an estimate of the conditional mean.
As described above, it is obtained as a weighted sum over all training observations.Meinshausen, (2006)extends this framework to estimating the cumulative distribution function (CDF) of, which is given by, whereis a threshold value.
The similarity to RFs becomes apparent in the last expression. Utilizing the weights from Equation4, one can approximate the CDF by the weighted mean over the binary observations:

That is, QRFs estimate the CDF ofvia the weighted empirical CDF of the training sample outcomesusing the weightsproduced by RFs.
This estimator is practically appealing as it arises as a byproduct of standard RF software implementation.
Furthermore, its representation in terms of a weighted empirical CDF enables a theoretical understanding of its properties by leveraging tools from nonparametric statistics(Lin and Jeon,,2006; Meinshausen,,2006).
In this paper, we consider the standard variant of QRFs which uses squared error as a criterion for finding splits (and thus growing the forest’s individual trees). Various other splitting criteria have been analyzed in the literature. In particular,Cevid et al., (2022)propose to use a splitting criterion based on distributional similarity. Since their RF variant retains the weighted empirical CDF representation (see their Section 2.2), our Topmethod can be applied to it as well.

SECTION: 2.2Forecast Evaluation

Since we generate probabilistic forecasts, we need a tool to evaluate them.
For this, we use the Continuous Ranked Probability Score (CRPS), a strictly proper scoring rule.
Scoring rules are loss functions for probabilistic forecasts.
We use them in negative orientation, so that smaller scores indicate better forecasts.
When evaluated using a proper scoring rule, a forecaster minimizes their expected score by stating what they think is the true forecast distribution. Under a strictly proper scoring rule, this minimum is unique within a suitable class of forecast distributions.
Conceptually, strictly proper scoring rules incentivize careful and honest forecasting.
SeeGneiting and Raftery, (2007)for a comprehensive technical treatment, andWinkler, (1996)andGneiting and Katzfuss, (2014)for further discussion and illustration.
The CRPS(Matheson and Winkler,,1976)is defined as

wheredenotes the CDF implied by the forecast distribution anddenotes the true outcome. For various forms of, closed-form expressions of the CRPS are available, so that there is no need for costly numerical evaluation of the integral in Equation6.

The CRPS allows for very general types of forecast distributions.
In particular, the forecast distribution may be discrete, that is, it need not possess a density.
This allows for evaluating forecast distributions based on (weighted) empirical CDFs, which arise in the case of QRFs.
In the special case that the forecast distribution is deterministic, i.e., it places point mass on a single outcome, the CRPS reduces to the Absolute Error (AE).
Thus, numerical values of the AE and CRPS can meaningfully be compared to each other.

Jordan et al., (2019)provide an efficient implementation of the CRPS for weighted empirical distributions in theirR-packagescoringRules.
Letdenote the response values from the training data, and denote bytheirth ordered value, with.
Furthermore, letdenote the weight corresponding to.
Then the CRPS for a realizationis given by

where we dropped the dependence ofon a vectorof covariates at this point for ease of notation.
Equation7extendsJordan et al.,’s Equation 3 to the case of non-equal weights, based on their implementation in the functioncrps_sample. In the case of sparse weights, one may omit the indiceswithfrom the sum at (7) in order to speed up the computation.

Additionally to the CRPS, we also report results for the squared error (SE).
In the present context, the SE is given by

Hence, the SE depends on the forecast distributionvia its meanonly.

SECTION: 3Experimental Results

In order to assess the statistical performance of the simplified forecast distributions, we conduct experiments on 18 data sets considered byGrinsztajn et al., (2022)in the context of numerical regression. The authors demonstrate that tree-based methods compare favorably to neural networks for these data sets.
Their selection of data sets aims to represent real-world, ‘clean’ data sets with medium size as well as heterogeneous data types and fields of applications. If deemed necessary, some basic preprocessing was applied byGrinsztajn et al., (2022).
Details can be found in Section 3.5 and Appendix A.1 in their paper, and in Table7of AppendixBbelow.
The data setdelays_zurich_transportcontains about 5.6 million data points in its original form.
For computational reasons, we reduced the size of this data set through random subsampling, to approximately 1.1 million data points (20% of the original observations).
We did not apply further preprocessing of any of the data sets in order to retain comparability.
We allocate 70% of each data set for training our models and reserve the remaining 30% for testing.
For each data set, we train a RF, and then evaluate its performance by computing the average CRPS and SE, as introduced in Equations7and8over the test data set. Our main interest is in studying the impact of the parameterwhich governs the number of support points of the sparsified forecast distribution. We consider a grid of choicesand denote these sparse RFs as ‘Top’.
Our standard choice of RF hyperparameters is a combination of default values of the machine learning software packagesscikit-learn(Pedregosa et al.,,2011)andranger(Wright and Ziegler,,2017)which are popular choices in thePython(van Rossum et al.,,2011)andR(R Core Team,,2022)programming languages, respectively.
In summary, we consider a random selection ofout ofpossible features at each split point, do not impose any form of regularization in terms of tree growth restriction, and set the number of trees to 1000 in order to obtain a large and stable ensemble. These choices, which are also listed in the first row (entitled ‘standard’) of Table9in AppendixB, are used for the analysis in Sections3.1and3.2. In Section3.3, we further consider the effects of tuning hyperparameters.

SECTION: 3.1Probabilistic Forecasts

Table2presents the CRPS for the full RF and the relative CRPS of Top, for, compared to the full RF. For example, a relative CRPS of 1.5 indicates a 50% larger CRPS for the Topversion compared to the full RF. The three smallest values forseem especially attractive in terms of simplicity and ease of communication.
While the two larger choicesare less attractive in terms of simplicity, we also consider them in order to assess the trade-off between simplicity and performance.

Using onlysupport points performs worse than the full RF, with a median performance cost of 25% across data sets.
While this result is unsurprising from a qualitative perspective, its magnitude is interesting, and gives a first indication of the performance cost of using a rather drastic simplification of the original forecast distribution. For each single data set, we find that the performance of Topimproves monotonically when increasingfromto, fromto, and for all data sets but one when increasingfromto.
This pattern is plausible, given that we move from a drastic simplification () to less drastic versions.
Compared to the full RF, Top5 implies a median loss increase of 14%, whereas Topyields a median loss increase of 5%.
Topperforms equally well as the full RF in the median. For most data sets, Top50 slightly enhances predictive performance compared to the full RF. Whethersupport points remain worth interpreting depends on the application at hand.
Only for a few data sets, Top50 is not sufficient to reach the performance of the full RF, but performance costs are small even for these data sets.

In order to contextualize the magnitude of our presented results (such as Top’s median CRPS increase ofcompared to the full RF), we next present results on two simple benchmark methods.333We refer toGrinsztajn et al., (2022)for detailed comparisons of RF point forecasts to other tree-based models and neural networks.First, we consider a deterministic point forecast which assumes that the full RF’s median forecast materializes with probability one.444Formally, this forecast is characterized by the CDF, wheredenotes the median implied by the CDF of the full RF’s forecast distribution.
That is, the CDFis a step function with a single jump point at the median forecast of the full RF.
We choose the median functional here because the latter is the optimal point forecast under absolute error loss, to which the CRPS reduces in the case of a deterministic forecast.

This is a very optimistic point (rather than probabilistic) forecast, containing no uncertainty.
As noted earlier, its CRPS is the same as its AE.
We consider this benchmark in order to quantify the costs of ignoring uncertainty altogether. We clearly expect these costs to be positive, i.e., we expect the point forecast to perform worse than the full RF. Second, we use the CRPS of the unconditional empirical distribution of the response variable in the training sample. This distribution places a uniform weight ofon each training sample observation, in contrast to the QRF weightsthat depend on the feature vector.
The unconditional distribution is a very conservative forecast, as no information about the features is used whatsoever.
Qualitatively, we clearly expect this forecast to perform worse than the full RF.
Quantitatively, the difference in performance of the unconditional versus conditional forecast distributions captures the predictive content of the features(see e.g. Gneiting and Resin,,2023, Section 2.5).
Both benchmarks are visualized jointly with Top3 results in Figure2.

As expected, the relative CRPS of the point forecast (shown in blue) exceeds one for most data sets, indicating that it is generally inferior to the full RF model.
Notably, an exception is observed forBrazilian_houses, where the point forecast’s relative CRPS is smaller than one.
This result appears to be due to prediction uncertainty being very small for this data set; see Figure 13 in the supplemental material forGrinsztajn et al., (2022).
Indeed, for this data set, the response variable seems to mostly be a linear combination of a subset of the features. Furthermore, Top3 (shown in green) performs similar to or better than the point forecast for all data sets, demonstrating the usefulness of incorporating additional uncertainty in the forecast. Note that the point forecast has access to the full RF forecast distribution, using the median of this distribution as a point forecast. By contrast, Top3 only has access to the three most important support points of the RF forecast distribution.

The relative CRPS of the unconditional forecast distribution (displayed in ochre in Figure2) exceeds two for most data sets, indicating that the features are generally very useful for prediction.
An exception occurs for the last two data sets (yprop_4_1anddelays_zurich_transport).
In these cases, the RF appears unable to learn meaningful connections between the features and the target variable.
Figure 13 in the online supplement ofGrinsztajn et al., (2022)supports this interpretation, reporting low predictability (in terms of low out-of-sample) for these data sets.
In this situation, we cannot expect a Top-model to perform well compared to the full RF.
To see this, consider the stylized case of the features being entirely uninformative.
Subsequently, the unconditional distribution (placing a weight ofon all training sample responses) is the best possible forecast, which is in sharp contrast to Top(for whichweights are non-zero and large by construction, whereas the remainingweights are forced to zero).

In order to further study the properties of Topforecast distributions, Table3reports the average weight sums across test cases, for different values of.
The weight sums are computed before our normalization step (see Section1) which re-scales all weight sums to one.
For a given choice of, the unnormalized weight sums can vary in magnitude, both across data sets and from test case to test case.
By construction, the weight sums increase with.
Interestingly, many data sets yield a large weight sum for Top3, exceedingfor all but four data sets.
For Top50, the average weight sum exceedsfor most data sets.
These numbers are remarkable, given that the data sets include thousands of training samples (, see rightmost column of Table3) that could potentially be used as support points for the RF forecast distributions.
If the weights were uniform, we would hence observe Topweight sums of.
This is in sharp contrast to our empirical finding that a few large weights dominate for most data sets.Lin and Jeon, (2006)find similar results in their work, where they consider RFs as adaptive nearest-neighbor methods and investigate the influence of the minimum number of samples per leaf.
Figures 1c,d and 5 show few large weights for synthetic data sets.
The presence of a small number of important weights explains why the simplification pursued by Topoften results in modest (if any) performance costs as compared to the full RF.

SECTION: 3.2Mean Forecasts

Let us turn our attention towards conditional mean forecasts, for which results in terms of squared error are shown in Table4.
We notice a similar pattern as in the probabilistic scenario: apart from Top3 outperforming the full RF for one data set (sulfur), Top3 performs worse than the full RF for the remaining data sets, with a maximum performance cost of 61% forpol.

This results in a median SE increase of 22% for Top3.
Top5 still shows a 12% median increase and for Top10, the median performance almost matches the full RF’s performance. Top20 and Top50 even outperform the full RF, yielding median improvements of 2% and 5%, respectively.
Compared to the results for forecast distributions (Table2), the results in Table4indicate that the performance costs of simplicity are comparatively lower in the case of mean forecasts, with median relative losses being somewhat smaller for a given value of.

SECTION: 3.3Varying Hyperparameters

Compared to other modeling algorithms, RFs have relatively few hyperparameters. Nevertheless, tuning its hyperparameters can improve the performance of RFs(Probst et al.,,2019).
The most important hyperparameters control the depth of each individual tree, as well as the number of randomly selected regressors considered for splitting. The depth of a tree can be restricted directly (‘maximum depth’) or indirectly by restricting leaf and split sizes (‘minimum leaf size’ and ‘minimum split size’, respectively).
The number of regressors considered for splitting is often denoted as ‘max features’ or ‘mtry’ (the latter term is used, e.g. in theRpackagesrandomForest(Liaw and Wiener,,2002)andranger(Wright and Ziegler,,2017)).
In what follows, we study how different hyperparameter sets influence the Topprediction and whether a hyperparameter set that optimizes the full RF is also beneficial for Top.
We therefore investigate the influence of ‘max features’ and one of the depth-regularizing hyperparameters, ‘minimum leaf size’, on the Topapproach.
To do so, we consider a grid search for both, the full RF and Top3, with 5-fold cross-validation on the training set of each data set.
Due to the size ofmedical_charges,nyc-taxi-green-dec-2016anddelays_zurich_transport, we use a validation set which contains 25% of the training set instead.
Further, the latter two are down-sampled to 30% and 15% of their original training set size, respectively.
For brevity, our analysis of hyperparameter tuning focusses on the case, which is the most drastic simplification we consider.

Figure3visualizes the CRPS of Top3 relative to the full RF’s CRPS for three different hyperparameter sets. In each case, we consider the same hyperparameter set for Top3 and the full RF.
The green bars show the results with standard hyperparameters, as listed in Table2.
The red bars indicate the relative performance with the respective hyperparameter set that optimizes the full RF, while the blue bars visualize the relative performance with the hyperparameters that optimize the CRPS of Top3.
When hyperparameters are tuned on the full RF, Top3 performance tends to slightly decrease overall.
Across the data sets, the median relative CRPS is, compared tofor the standard setting.
Conversely, if hyperparameters are tuned on Top3, the relative performance of Top3 is mostly better than in the standard version, reaching a median relative CRPS of.

Figure4presents results for point forecasting performance, which are similar to the probabilistic case.
Tuning on the full RF hurts Top3, with a median relative SE of, compared toin the standard case.
By contrast, tuning on Top3 benefits Top3, with a median relative SE of. In a small minority of cases, tuning is not beneficial in terms of the relative loss.

Hence, despite its simplicity, Top3 can perform similar to the full RF given suitable hyperparameter choices. Overall, the relative performance of Top3 obtained under the standard setting is between the performance obtained under the two other hyperparameter settings. Furthermore, the full RF and Top3 require different sets of hyperparameters in order to perform well. Ideally, users of the Topmethod should thus choose hyperparameters based on the performance of Topitself, rather than the performance of the full RF.

SECTION: 4Stylized Analytical Model

In this section, we construct a stylized analytical framework which helps explain our experimental findings presented in Section3:
For many data sets, simplified RFs perform similar to full RFs even for relatively small choices of. This finding, and especially the possibility that simplified RFs may even outperform full RFs, deserves further investigation.
Motivated by the structure of RF forecast distributions (see Section2), we consider a model in which the true forecast distribution is discrete with support pointsand corresponding (true) probabilitiesthat are positive and sum to one.
Specifically, we let

whereis a subset of ‘important’ indexes with.
The corresponding ‘important’ probabilitiessum to, whereas the other, ‘unimportant’, probabilities sum to. To justify the notion of ‘important’ probabilities, we will focus on choices ofandthat satisfy.

In addition to the true forecast distribution just described, we consider an estimated forecast distribution that uses the same support points, together with possibly incorrectly estimated probabilities.
We assume that the estimated probabilities can be described by the following model:

whereis a draw from a Dirichlet distribution with-dimensional parameter vector, with, such that each element ofhas expected valueand variance. Similarly,is a draw from another, independent Dirichlet distribution with-dimensional parameter vector, where. This means that the expected probabilities are given by

In the following, we assume thatwhich ensures that there are at least two ‘important’ and ‘unimportant’ support points, respectively. This restriction ensures that weight estimation within both sets is a non-trivial problem.555If there was only one important support point, for example, the probability of this support point would necessarily be equal to, rendering weight estimation trivial.

Thus, if, the forecast model’s expected probabilities differ from the true ones in Equation9. The parametersandrepresent the precision of the forecast model’s probabilities around their expected values.
Small values forindicate noisy probabilities, whereas large values forcorrespond to probabilities close to their expected values. This is a property of the variance of Dirichlet-distributed random variables as noted above for the case of.
Conceptually, the above model provides a stylized probabilistic description of the estimated probabilitiesproduced by a forecasting method like RFs. Thereby, the model does not aim to specify the mechanism by which the forecasting method generates these probabilities.

Figure5illustrates this setup, withand the important indexes given by.
The left panel shows a situation in which the estimated probabilities are quite noisy (but are correct in expectation (. In the right panel, the estimated probabilities are less noisy () but are false in expectation ().

In the special case that, the forecast model coincides with the true model.
Furthermore, in the case, the forecast model is very similar to the ‘Top’ strategy (retaining themost important probabilities, rescaling them to sum to one, and setting all other probabilities to zero). The somewhat subtle difference between the analytical model and our practical implementation of Topis that the indexes of the important weights are fixed in the analytical model (given by the set), whereas they are chosen as thelargest empirical weights in practice. Exact modeling of our practical procedure would seem to complicate the analysis substantially without necessarily yielding further insights. While stylized, the analytical model described above is flexible enough to cover various situations of applied interest.
For example, the relation between ‘important’ versus ‘unimportant’ values of the true probabilities can be governed flexibly via the parametersand.
While we assume that the setof important indexes is known to the forecast model, the possibility of a poor forecasting model can be represented by a valuethat differs substantially from, and/or small values ofthat correspond to noisy estimates. Thus, the forecast model could even yield estimates of the ‘unimportant’ probabilities that greatly exceed those of the ‘important’ ones.666In practice, where the set of important indexesis not known, this situation corresponds to one in which the largest empirical weights are not helpful for predicting new test sample cases.For given support pointsand estimated probabilities, the expected squared error and expected CRPS implied by the analytical framework are given by

the expression for the CRPS follows from adapting the representation in Equation 2 ofJordan et al., (2019). In both cases, the expected value is computed with respect to the discrete distribution with support pointsand associated true probabilities. As noted in Equation10, we cast the predicted probabilitiesas scaled draws from two Dirichlet distributions. We further assume that the support pointsaredraws from a standard normal distribution; these are mutually independent and independent of. Using these assumptions, we obtain the following expressions for the (unconditionally) expected squared error and CRPS:

whereis the distribution of the estimated probabilities that is implied by our model setup, andis the joint distribution ofindependent standard normal variables. The proof can be found in AppendixC. The result that the expressions forandare identical up to a factor ofis a somewhat idiosyncratic implication of our model setup.

In order to interpret the implications of these formulas, we compare a forecasting method with(representing standard RFs) to a method with(representing Top) in the following.

For given values ofand, bothandattain their theoretical minimum atand.777Proof:for; this holds for all values ofand. It is hence optimal to letgo to infinity.
Next consider the limiting expression ofas.
Minimizing this expression with respect toyields the solution.This result is unsurprising: Under the stated conditions, the forecast model coincides with the true model, i.e.,with probability one.
Since the squared error is strictly consistent for the mean (and, similarly, the CRPS is a strictly proper scoring rule), the true model must yield the smallest possible expected score.888While the possibility of exactly matching the true model is unrealistic in practice, the requirement that the true model perform best is conceptually plausible, and is the main idea behind forecast evaluation via proper scoring rules and related tools.As both expected score functions are continuous inand, this implies that ifis sufficiently close to, andare sufficiently large, then the standard approach will outperform the Topmethod.

Conversely, the following conditions favor the Topmethod over the standard approach:

, i.e. the standard method’s implicit assumption thatis worse than Top’s implicit assumption that

is large, i.e. important probabilities are estimated precisely

is small, i.e. estimates of unimportant probabilities are noisy

If these conditions, or an appropriate combination thereof, hold, then the Topapproach can be expected to perform well.

Figure6illustrates the above discussion.
In the left panel (with), the ‘unimportant’ probabilities are estimated very precisely.
Here the Topmethod is superior only to valuesthat are clearly smaller than the true parameter.
In the right panel (), the estimates of the unimportant probabilities are very noisy.
Hence it is beneficial to focus on the important probabilities which are estimated precisely (since).
Accordingly, the Topmethod - which focuses on the important probabilities exclusively - is superior to a wide range of values for.
Interestingly, this range includes the true parameter, i.e., the Topmethod can be beneficial even if the probability estimates are correct in expectation.

SECTION: 5Conclusion

This paper has considered simplified Random Forest forecast distributions that consist of a small numberof support points, in contrast to thousands of support points (possibly equal to, the size of the training set) of the original forecast distribution.
The Topforecast distribution can be viewed as a collection ofscenarios with attached probabilities. It hence simplifies communication and improves interpretability of the probabilistic forecast.
Our empirical results in Tables2and4imply that simplified distributions using five or ten support points often attain similar performance as the original forecast distribution, while larger choices of, e.g., 20 or 50, even increase performance slightly in many cases. Our analytical framework in Section4offers a theoretical rationale for these results. Our empirical analysis further shows that when tuning hyperparameters to the target value for, evencan yield very good results.

While we have focused on the trade-off between simplicity (as measured by) and statistical performance, the optimal choice ofdepends on the subjective preferences of the audience to which forecast uncertainty is communicated.
In order to choosein practice, we recommend that communicators first assess the statistical performance of various choices offor their particular data set.
In a second step, communicators may then interview potential users about their perceived cognitive costs of various choices of.
For example,Altig et al., (2022)argue thatresonates well with participants of an online survey on firm performance.

SECTION: References

SECTION: Appendix ADetails on SOEP Data

This section describes the SOEP data set used in Section1in more detail.
We process the original data set with the following steps:

The variableid, being a unique categorical identifier for each participant, is excluded from the model.

If the regressorsectoris missing, it is imputed with the value ‘unknown’.

The qualitative regressorssectorandemploymentare converted into sets of dummy variables (one-hot encoding).

The columnsgesund_organdlebensz_orgare excluded due to their subjective nature.

The sum of variableseinkommenj1andeinkommenj2is selected as the target variable and referred to as ‘income’.

Entries with missing values in any of the utilized variables are removed from the data set.

Data from 2015 to 2018 is used for training, while data from 2019 is reserved for testing.

Table5lists all the variables utilized in the data set along their respective data type and a short description.
The both RFs, the standard and Topversion, are trained using our standard hyperparameter choices, as detailed in Table9. Table6reports results on forecasting performance for various choices of. The SE and CRPS performance measures reported in the table are introduced in Section2.1.

SECTION: Appendix BDetails on Empirical Experiments

Here we present further details on the empirical experiments of Section3.Table7lists the data sets used in the experiments, following the analysis ofGrinsztajn et al., (2022). The data sets cover a wide spectrum of size, number of covariates and domains. They can be easily downloaded using the URLs listed in the table. In the case ofdelays_zurich_transport, we subsampled the data set to 20% of its original size (which is shown in the table) due to computational reasons, resulting in roughly 1.1 million observations. Table8describes the grid of hyperparameter values we considered for the analysis in Section3.3, and Table9presents the best choices selected via cross-validation. For a given data set, we use a grid search with 5-fold cross-validation. For computational reasons, we use simplified procedures for the three largest data sets (medical_charges,nyc-taxi-green-dec-2016anddelays_zurich_transport), where we use a single holdout set which consists of 25% of the training data. Furthermore, we subsample thenyc-taxi-green-dec-2016anddelays_zurich_transportdata sets to 30% and 15% of their training set size. Table9shows that hyperparameter choices are often the same across both loss functions (CRPS and SE), whereas differences between ‘Full’ and ‘Top3’ are more pronounced. Hence users of simplified RFs (such as Top3) should consider tuning hyperparameters to optimize the performance of these simplified RFs directly, instead of optimizing the performance of full RFs.

SECTION: Appendix CDetails on Section4

Here we derive the results forandstated in Equations14and15.

SECTION: C.1SE

We start with the derivation for the squared error, which is given by

First, we rewrite Equation12:

We next change the order of integration and calculate the expectation with respect to the support points, i.e., we consider the expected value with respect to:

where we have used the assumption that the elements ofare independently standard normal. Next, recall that

Furthermore, from the variance of a Dirichlet distributed random variable, we obtain

We are now ready to calculate the expectation with respect to:

SECTION: C.2CRPS

We seek to evaluate the following integral:

whereis given in Equation13. Note that a random variablewithfollows a folded normal distribution with mean. Using this fact and Equation13, the expected value with respect tois given by

To simplify notation, we define. In order to compute the expected value of the expression in Equation16with respect to, we differentiate between four cases:

It holds that. We hence obtain

Here we haveand hence

With, we obtain

It again holds that, and hence

Using Equation16, the earlier definition, and summarizing all four cases, we obtain

The result thatthen follows from tedious yet straightforward algebra. We are happy to provide detailed notes upon request.