SECTION: Augmented Functional Random Forests: Classifier Construction and Unbiased Functional Principal Components Importance throughAd-HocConditional Permutations

This paper introduces a novel supervised classification strategy that integrates functional data analysis (FDA) with tree-based methods, addressing the challenges of high-dimensional data and enhancing the classification performance of existing functional classifiers. Specifically, we propose augmented versions of functional classification trees and functional random forests, incorporating a new tool for assessing the importance of functional principal components. This tool provides an ad-hoc method for determining unbiased permutation feature importance in functional data, particularly when dealing with correlated features derived from successive derivatives. Our study demonstrates that these additional features can significantly enhance the predictive power of functional classifiers. Experimental evaluations on both real-world and simulated datasets showcase the effectiveness of the proposed methodology, yielding promising results compared to existing methods.

SECTION: 1Introduction

Managing vast amounts of data from sources like phones, sensors, and calculators has become crucial in today’s data-driven world. Technological advancements have led to the development of tools for handling this data, used in areas such as environmental control, healthcare, and more. As a result, dimensionality reduction and classification techniques are increasingly important in fields like medicine, multimedia processing, and robotics. However, learning from high-dimensional data is challenging due to irregular time points, computational complexity, balancing bias and variance, and improving model interpretability and performance metrics. The curse of dimensionality, which complicates data analysis, is a well-known challenge. To address these issues, functional data analysis (FDA) has become a widely adopted approach[1,2,3,4].

Functional Data Analysis (FDA) is a statistical domain focused on applying statistical methods to data represented as functions rather than traditional real numbers or vectors. By treating functions as unified entities, FDA introduces a new paradigm in statistical modeling and prediction. The benefits of FDA, such as utilizing derivatives for better insights, adopting non-parametric approaches, reducing dimensionality, and exploiting pattern sources, are well-documented in recent literature[1,2,3]. The methodological advancements in FDA aim to bridge traditional statistics with functional approaches, and FDA tools are increasingly used in high-dimensional time-series analysis across various fields[see e.g.3,5,6,7]. A key method in FDA is the functional principal component decomposition (FPCD), which represents functions through a linear combination of functional principal components (FPCs) to maximize variance, facilitating dimensionality reduction while retaining essential information[1,2,3,8]. This approach has expanded conventional statistical methods to functional data, particularly in supervised classification, the focus of this paper.

Supervised classification in FDA focuses on developing classification rules from observed curves to predict the classes of new curves with high accuracy. Several methods have been explored, including Logistic Classifier, k-nearest Neighbour Classifier, and Kernel Classifier[see, e.g.9,10,11,12,8,13]. Recent research has increasingly combined FDA with tree-based techniques for classification tasks. For example,[14]introduced spline trees for functional data, focusing on time-of-day patterns for international call customers.[15]developed a regression tree within the FDA framework for probability density functions.[13]explored variable importance in FDA combined with tree methods, while[16]proposed random forests for functional covariates using mean values over time windows.[17]extended random forests with wavelet basis for classifying driver’s stress levels. Further,[18]developed a classifier for dose-response predictions with curve outcomes.[19]and[20]explored using functional principal components for classification trees and combining clustering with tree-based classification, respectively. Lastly,[21]proposed an innovative evaluation of leaf quality in functional classification trees applied to biomedical signals with binary outcomes.

Given that the starting point is undoubtedly to take advantage of the blended use of FDA[1,2]and statistical learning techniques[22], it is evident that, in this framework, much must still be discovered and tested for managing vast quantities of data and trying to interpret the results from a statistical perspective, possibly also from a causal and inferential viewpoint. Hence, investigations in this field are currently at the forefront, dynamic, and full of potential. In the coming years, we will observe many articles that enhance functional classifiers’ precision, interpretability, and explainability.

Based on what has been introduced above and the many possible research scenarios this topic offers, this paper presents novel supervised classification strategies called Augmented Functional Classification Tree (AFCT) and Augmented Functional Random Forests (AFRF). Starting from the basic idea proposed in[21], this study aims to tackle the challenge of learning from high-dimensional data and enhance classification performance and models’ explainability. This article proposes to exploit additional features of the original functional data to improve the information fed to the functional classification tree ensembles.
To steal a typical machine learning terminology adopted for image recognition, in this study, we talk about “augmented functional data” to suggest the joint use of sequential derivatives to extract new features for supervised classification. In other words, this approach can be seen as observing functions from different perspectives to capture additional features that can help improve classification performance.
Additionally, we introduce a novel method called Conditional Permutation Importance for Augmented Functional Principal Components (CPIAFPC). This method provides an unbiased assessment of feature importance by accounting for the inherent correlations among features derived from successive derivatives. CPIAFPC ensures that the contribution of each feature to the classification model is accurately represented, further enhancing the interpretability and reliability of the proposed classification strategies.

Extensive experimental evaluations are conducted on real-world and simulated datasets to evaluate the effectiveness of the proposed methodology. The comparisons to existing methods show exciting results in terms of classification performance. The research contributes to the field of FDA by demonstrating its potential for handling high-dimensional data in supervised classification tasks and offering valuable insights into the underlying functional characteristics of the data.

The paper proceeds with Section 2, introducing the fundamental concepts of FDA and functional classification trees and their ensembles.
Section 3 presents augmented functional classifiers and suggests possible tools for models’ explainability.
Section 4 provides an application for the ECG200 dataset.
Section 5 presents a simulation study with six scenarios.
Finally, Section 6 concludes the paper, discussing findings, conclusions, and future research directions.

SECTION: 2Preliminaries

SECTION: 2.1Functional Data Analysis (FDA)

In FDA, the core idea is to treat data functions as distinct entities, but in practical scenarios, functional data is often encountered as a series of discrete data points. This implies that the original function, denoted by, is transformed into a collection of discrete observations represented bypairs, wheresignifies the points where the function is assessed, andrepresents the corresponding function values at those points[3,1,23,2].

We define a functional variableas a random variable with values in a functional space. Consequently, a functional data set is a sampledrawn from the functional variable[3,1,23,2].
Focusing our attention to the case of a Hilbert space with a metricassociated with a norm so that, and where the normis associated with an inner productso that, we can obtain as a specific case the spaceof real square-integrable functions defined onby, whereis a Lebesgue measurable set on. Therefore, by considering the specific case of, a basis function system consists of a set of linearly independent functionsthat span the space[3].

The initial step in FDA involves transforming the observed valuesfor each unitinto a functional form. The prevailing method for estimating functional data is basis approximation. Various basis systems can be employed depending on the characteristics of the curves.
A common approach is to represent functions using a finite set of basis functions in a fixed basis system. This can be mathematically expressed as:

whererepresents the vector of coefficients that define the linear combination,is the-th basis function, andis a finite subset of functions used to approximate the full basis expansion.

A trending methodology involves the utilization of a data-driven basis, with the Functional Principal Components (FPCs) decomposition standing out as a prominent technique. This approach effectively reduces dimensionality while concurrently preserving essential information from the original dataset[3,6,4]. In this context, the approximation of functional data can be expressed as follows:

whereis the number of FPCs,represents the score of the generic FPCfor the generic function().

By reducing the representation to the firstFunctional Principal Components (FPCs), we can estimate the sample curves. The explained variance is calculated as, whererepresents the variance associated with the-th FPC. The FPCs are constructed so that the variance explained by each-th FPC decreases asincreases.

In this setting, under the assumption that the observed curves are centered with a null sample mean, the score for the-th curve and-th Functional Principal Component (FPCs) is determined by:

whereis the weight function111The weight functionis obtained by maximizing the variance, solving the following optimization problem:subject to the constraints:and.

SECTION: 2.2Functional Classification Trees (FCTs)

In the realm of functional classification, the objective is to forecast the class or labelfor an observationwithin a separable metric space. Consequently, our methodology is tailored for functional data represented as, whereis a predictor curve defined for, anddenotes the scalar response observed at sample. The classification of a novel observationfrominvolves the creation of a mapping, referred to as a “classifier,” which assignsto its predicted label. The probability of error is quantified by.

The continuous domaincan take on various forms, such as time, space, or other parameters. In this context, the primary focus is on the temporal domain, although the methodology can be seamlessly extended to incorporate other parameters. Theoretically, the response could manifest as either categorical or numerical, leading to classification or regression challenges, respectively. Nevertheless, the present investigation centers on a specific scenario: a scalar-on-function classification problem.

Traditional decision trees represent a supervised learning technique that predicts response values by acquiring decision rules derived from features. Extensive information on decision trees can be found in various prior works[24,25,22]. Building upon this foundation, decision trees can be extended to the FDA framework[see e.g.21,19]. This extension involves leveraging the coefficients of a basis representation as novel features for training a functional classifier. The latter methodology suggested tools to enhance the interpretability of the so-called “Functional Classification Tree”(FCT).
The starting idea is that FCTs can be based both on fixed and data-driven basis systems. In the instance of a fixed basis system, such as the one presented in Equation1, the matrix of features is determined by:

where the generic elementis the coefficient of the-th curve () relative to the-th () basis function. As a natural consequence,is the vector containing the-th statistical unit’s characteristics. This strategy guarantees a straightforward application of the classifier. Indeed, by focusing on b-splines as an example, owing to their total count and order, it becomes feasible to employ them to represent both the functional training set and the functional test set. Nevertheless, using a fixed basis system is frequently restrictive from an interpretative standpoint. This is due to the fact that basis functions are not derived from the data itself. Consequently, their significance in the time domain fails to align with the criteria of maximizing explained variability and finding a reliable interpretation of the classification rules.

On the contrary, in the case of a data-driven basis system like that in Equation9, the features’ matrix is given by:

whereis the score of the-th curve () relative to the-th functional principal component(). Feeding FCTs via FPCs is very useful in terms of dimensionality reduction, interpretability, and accuracy, as shown in[21].

SECTION: 3Augmented Functional Tree-Based Classifiers and their Transparency

SECTION: 3.1Augmented Functional Classification Trees (AFCTs)

The approach proposed by Maturo and Verde[21]neglects the potential benefits of certain FDA techniques because it is limited to the original functions without considering the possibility of observing other functional characteristics. This research seeks to extend the latter approach and enhance classifier performance by leveraging alternative functional tools. Introducing the concept of “Augmented Functional Classification Tree” (AFCT), this study employs sequential derivatives of the original functional data to generate new features for feeding AFCTs. Essentially, it involves viewing functions from various angles (different dimensions given by the derivatives’ orders), analogous to using a magnifying glass on curves, to uncover additional features that can significantly improve classification performance compared to what the original functions can capture.

Let the functional derivative of orderfor the-th curve be represented by a fixed basis system (e.g. b-splines) as:

whereis the coefficient of the-th curve,-th b-spline, and-th derivative order;is the-th derivative of the-th basis function.

Emphasized by Ramsay and Silverman[3], the selection of the basis system plays a crucial role in estimating derivatives. It is essential to ensure that the chosen basis for representing the object can accommodate the order of the derivative to be calculated. In the case of b-spline bases, this implies that the spline’s order must be at least one higher than the order of the derivative under consideration. In this specific research, we concentrate on a b-spline basis of fourth order. The highest degree of derivative adopted in this study is the second; consequently, there are no problems estimating the basis functions’ derivatives. In this work, the estimate is made using thederiv.fdfunction in thefdapackage[3].

The fundamental idea of this research is that the feature matrix introduced in Equation4can be expanded into a kind of matrix composed of blocks of coefficients derived from functional representations of various orders of derivatives. Following this approach, the feature matrix for training the so-called AFCT becomes as follows:

whereindicates that the features used leverage representations up to the-th derivative. Naturally, focusing attention solely on features derived from a representation based on original functions, velocity and acceleration of curves, the feature matrix or order two is given by:

The functional derivatives are in a different space spanned by diverse basis systems. Therefore, when using a multivariate approach that combines the coefficients of b-splines from different derivatives, it is important to proceed with caution. This is because the coefficients from these different derivatives can vary significantly in magnitude and variability.
For this reason, using this approach with some statistical methods would require a standardization of scores. However, this is not necessary for the context of FCTs because, in classification trees, the standardization of features is not necessary; effectively, the different magnitudes and variability of the scores do not influence the split based on impurity reduction computed considering the categorical outcome.

Following the same criteria and starting from a representation like that of Equation5, we can also adapt the feature matrix illustrated in5.
Let Equation5be the FPCs approximation of the original dataset.
The functional derivatives can be rewritten as a decomposition in an orthonormal basis by maximizing the variance of:

whereis the score for the-th statistical unit for the-th FPCof order, i.e. the derivative of orderfor the generic FPC().

The generic matrix of data-driven augmented features becomes the following:

In practical applications, it may be appropriate to stop at the second derivative, obtaining as the data-driven matrix to train the AFCT using the set of augmented features in Equation11:

The algorithm commences with the entire functional dataset, comprising the scores derived from the decomposition of successive derivatives illustrated in Equation11. The procedure continues iterating until reaching terminal leaves. At each step, the algorithm determines the optimal binary partition based on the selected cost criterion, such as the Gini index or Shannon-Wiener index222The Gini index measures heterogeneity for categorical variables, with lower values indicating greater homogeneity within a node. Conversely, the Shannon-Wiener entropy index quantifies heterogeneity, with higher values signifying greater diversity within a node. The Gini index and Shannon-Wiener entropy index can be calculated as follows:(12)(13)whererepresents the proportion of training observations in the-th region that are from the-th class.[22,26], as described in[22]and[26]. The initial classifier is an extensive AFCT, which is then refined through cost-complexity pruning to achieve an appropriate balance between complexity and accuracy providing the pruned AFCT.

SECTION: 3.2Augmented Functional Classification Trees’ Interpretability

One of the main challenges with Augmented Functional Classification Trees (AFCTs) is understanding the classification rules in the context of functional data. In AFCTs, as explained in Section3.1, the coefficientsfrom successive linear combinations are used as features for predicting the response. However, interpreting the split rules in AFCTs differs from traditional decision trees. This is because the split values formust be understood based on the specific part of the time domain that the corresponding Functional Principal Component (FPC) represents, as well as the derivative order. Therefore, by examining both the split value ofand the plot of, one can gain a better understanding of the rules that separate the functional dataset at each node.

Let the subscript ”0” insignify that the identified threshold for the score of orderassociated with a specific Functional Principal Componentis set as a constant value to divide the set of curves into two subgroups. For instance, in the context of the initial split rule, namely the separation rule for the root node,represents the threshold value linked to FPC. Consequently, all curves meeting the conditionconstitute one subgroup, while the remaining functions, those satisfying the condition, form the other subset.

To better understand the split rule in the functional context, we have to think about the theoretical separation curve generated by every split occurring in the AFCT. In a AFCT, each intermediate node produces a separation of the functional dataset into two functional subsets (children nodes). The curve that dictates the separation rule is given by:

whereis the set of the FPCsselected in the classification rule path until the split of the-th node (). The generic intermediate node that generates a separation is therefore indicated with, and a total number of these intermediate nodes is identified with. In other words, eachcan be associated with every intermediate node; by definition,is the separation curve dictated to split the root node whereas the leaves (terminal nodes) do not contemplate a separation curve.

If, for instance, a intermediate node is obtained starting from the root node, first splitting based on the value 3.4 of the coefficient of the first derivative of the third FPC, and then splitting based on the value 2.1 of the coefficient of the second derivative of the first FPC, we will need to reconstruct this separation curve using Equation14. The curve reconstructed in this manner allows us to understand the reason for the separation in the time domain and thus from a functional perspective.

SECTION: 3.3Augmented Functional Random Forest (AFRF)

In traditional decision trees, even slight variations in the data can result in highly diverse trees, leading to different classification rules and interpretations. A practical approach to mitigating this problem is constructing an ensemble of decision trees using bagging, as outlined by Hastie (2009)[22].
Classical bagging, an extension of conventional decision trees proposed by Breiman (1996)[27], aims to reduce the variance associated with a single classification tree. The fundamental concept involves generating multiple decision trees to formulate a final classification rule collectively; however, a limitation of bagging lies in the dominance of the most influential predictor across all trees. Consequently, the trees exhibit some degree of correlation, diminishing the strength of variance reduction. The adoption of the random forest can mitigate the latter issue.
The classical Random Forest (RF), introduced by Ho (1998)[28], stands out as a highly efficient machine learning algorithm and represents a specific case of bagging tailored for decision trees. The approach involves applying bagging to the data and employing bootstrap sampling for predictor variables at each decision split. This means that, during each step of the tree algorithm, a random subset of predictors is chosen as candidates for splitting from the entire set of predictors. This refinement enhances the traditional bagging method by producing a classifier less influenced by correlations among trees, preventing the dominance of a single discriminating variable across all trees.

A similar procedure can be extended to the case of functional data and lead to the Augmented Functional Random Forest (AFRF), i.e. a classification rule for augmented functional data via the FPCs scores used as features. Clearly, the same approach can be used with other bases, including b-splines. The difference from the classic case is that the basic weak classifiers are FCTs.
Let AFRF consist ofAFCTs, where, withchosen to be a large number. The-th AFCTis grown on a random subset of the training set, obtained from the original databy drawing, with replacement, a bootstrap sampleof the same sizeas the original data set.
The replacement ofcan be seamlessly executed using the features matrix introduced in11. AFRF enhances the bagging process through a slight modification that decorrelates the AFCTs and reduces variance when averaging the AFCTs.

Each time a split in a single AFCT is considered, a random selection ofofis chosen as split candidates from the full set of theFPCs. It follows that when, we have AFRFs, whereas when, we have an augmented functional bagging.
Following this approach, the AFCT into the forest, will be less correlated because the most important FPCs won’t always be those features on the top of the AFCT determining the first important separation rules of the AFRF.
Every time a split in a single AFCT is considered, a random selection ofis chosen as split candidates from the complete set ofFPCs. It follows that when, we have AFRFs, whereas when, we have augmented functional bagging.
By adopting this approach, the AFCTs within the forest become less correlated because the most critical FPCs will only sometimes be those features positioned at the top of the AFCT determining the initial significant separation rules of the AFRF.
A helpful guideline to follow is to select the size of the subset of FPCs as a value of approximately.
This means that during each split in the FCT, the algorithm will only consider some of the available augmented features.
On average,of the splits will not even take some augmented features into account. This process helps AFRF to decorrelate the AFCTs, making the average less variable and thus more reliable.

The collection of curves labelled fromtothat exist in the-th bootstrap sampleare referred to as the “in-bag curves sample” (IBCs). These curves are used to create a single-th AFCT. The ”out-of-bag curves sample” (OOBCs) comprises the remaining curves of statistical units that are not present in.
We trainAFCTs, each using its own bootstrapped functional training set, to get, which is the predicted class for curveusing the-th AFCT. We then combine allpredictions of the AFCTs to obtain the final prediction for each curveusing a ”majority vote” system. For each curve, the predicted class is the most frequently occurring class among thepredictions from the different AFCTs.

Letbe a categorical random variable representing the class label, withpossible classes, denoted by. For a given curve, the goal is to estimate the probabilitythat the curve belongs to class. This probability can be estimated by aggregating the predictions from an ensemble ofAugmented Functional Classification Trees (AFCTs). Specifically, the probability estimate is given by:

whereis the indicator function, defined as:

The final prediction of the Augmented Functional Random Forest (AFRF) for the curveis determined by the mode of the predicted classes across the ensemble. Formally, the AFRF predictionis given by:

Alternatively, this can be expressed as:

In this formalism, the AFRF assigns the-th curve to the classwith the highest estimated probability, effectively taking the class predictions’ mode across all ensemble trees.

In the case of AFRF, interpretability can not be considered since the method is based on utilizing a set of AFCTs. Each AFCT is characterized by the presence of coefficients derived from the use of successive derivatives of various FPCs of different order, which operate at different levels of the AFCTs with different split values. Moreover, these coefficients may not appear in a specific AFCT due to randomness of bootstrap. Therefore, discussing interpretability in this context is less meaningful than what is discussed in Section3.1.
However, within the traditional random forest context, various explainability tools are available, and all tools documented in high-level non-functional literature can be easily extended to the functional domain following the outlined methodology. In this work, we propose an algorithm specifically adapted to address the inherent correlation present among our features. Given that the features in our model are constructed in a way that naturally introduces correlation—particularly when derived from functional data—we have developed a modified approach to ensure that these correlations are correctly accounted for in the analysis. This adaptation is presented in Section3.4and is crucial for maintaining the integrity of the model’s predictions and enhancing the reliability of the classification process.

SECTION: 3.4Augmented Functional Random Forest Explainability: Conditional Permutation Importance for Augmented Functional Principal Components

In this section, we introduce and justify the development of novel feature importance metrics designed explicitly for AFRF. While effective in many contexts, conventional methods of measuring feature importance in Random Forests, such as Gini Importance (Mean Decrease in Impurity) and Permutation Importance (Mean Decrease in Accuracy) may only be somewhat appropriate for functional data that includes derivatives. This inadequacy arises primarily due to the inherent correlations among the scores of functional principal components derived from successive derivatives, which can introduce bias into classical importance measures.
The classical measures work under the assumption that the predictors are independent or only weakly correlated. However, in the case of augmented functional data, where features are derived from successive derivatives, these assumptions are violated. This violation can lead to overestimating the importance of certain FPCs, as their correlated nature artificially inflates their apparent contribution to reducing node impurities.
Therefore, it becomes necessary to develop feature importance metrics that can account for this correlation and provide a more accurate representation of each feature’s contribution to the overall model performance.

In the literature,Strobl et al. [29]suggested methods for conditioning based on groupings and extended the classical permutation approach by conditioning on the correlated features. Instead of permuting the values of a single feature across all observations, the proposal is to permute the values within groups of observations that share similar values in the correlated features.
Our context is quite peculiar because it is evident that each FPC is inherently correlated with its first and second derivatives. Thus, standard permutation importance methods can also lead to misleading conclusions about relative importance.
To address this issue, we propose a novel method,Conditional Permutation Importance for Augmented Functional Principal Components (CPIAFPCs), which calculates the importance of each FPC’s score while conditioning on the scores of its corresponding dimensions.
This approach ensures that each score’s importance reflects its unique contribution, independent of the influence of its correlated derivatives.

The proposed algorithm is designed to accurately measure FPCs importance in Augmented Functional Random Forests when dealing with functional data that includes derivatives. The process for calculating the Conditional Permutation Importance for Augmented Functional Principal Components (CPIAFPCs) is detailed in Algorithm1.

Matrix of augmented features, where the rows represent different observations, and the columns represent the scoresfor the Functional Principal Components (FPCs) of orderfor each component.

Augmented Functional Random Forest (AFRF) modeltrained on.

Group together features corresponding to the same principal component across different derivative orders. For each principal component, form a group.

Condition on the other features in the group.

Permute the values of, maintaining the correlated features constant.

Use the permuted dataset to predict the responseusing the trained AFRF model.

Compute the difference in prediction accuracy before and after permutation:

whereis the loss function measuring prediction accuracy, andare the predictions after permutation.

A set of importance scoresfor each featurein the dataset, reflecting the unique contribution of each feature to the model’s performance.

SECTION: 4Application to Electrocardiogram Data

This section delves into the outcomes derived from applying AFCT and AFRF to an ECG dataset, providing comparisons with various functional classifiers. In particular, our methodology is tested on the ECG200 dataset accessible at https://www.timeseriesclassification.com/.
In 2001, R. Olszewski introduced the ECG200 dataset at Carnegie Mellon University as part of his work titled “Generalized feature extraction for structural pattern recognition in time-series data”[30]. This dataset has been continuously used to evaluate new classifiers and currently holds the world record for classification accuracy, with the BOSS algorithm achieving 89.05%.
The focus here is on using the original functions and their first and second derivatives.

Figures2and2show the smoothed versions of the original signals in the training and test sets. Each series represents the electrical activity that is recorded during a single heartbeat. The two categories are a normal heartbeat (represented by the colour red) and a Myocardial Infarction (represented by the colour black). In our training set, we have a total of 100 signals, and in the test set, we also have 100 signals.
Equation1was used to compute smoothed versions of the original signals in the training and test sets. These smoothed versions are presented in Figures4and4.

The objective is to develop a classifier that can forecast a patient’s health status based on their ECG readings. It’s worth noting that by utilizing cubic splines with an order of four, we can maintain the continuity of the splines’ first and second derivatives at the knots.

Figure5displays the original curves’ first ten functional principal components. Figures7and7show the first derivatives of the original functions and the FPCs’ first derivatives respectively. Similarly, Figures9and9display the second derivatives of the original functions and the FPCs’ second derivatives. The scores used in Figures5,7, and9are fed into the AFCT presented subsequently. It is important to note that in this framework, the traditional methods of selecting the number of FPCs are not helpful. This is because FPCs that explain little variability can often be crucial in distinguishing between the outcome classes and are, therefore, essential to constructing the AFCT. In fact, the first FPC, which typically captures most of the variability, is rarely critical in AFCTs. As Figure5illustrates, each FPC explains different parts of the time domain and can be useful in distinguishing between the outcome classes in the context of AFCTs.

Non-pruned AFCT is typically not useful for practical purposes. If the AFCT overfits the data, it will perform poorly when classifying different datasets. For this reason, the pruning phase is essential. This phase is based on finding an optimal balance between complexity and accuracy. Cost-complexity pruning via cross-validation is performed using the R package “rpart”. The pruned AFCT is shown in Figure11. It was built using FPCs scores computed using Equation9as features via therpart.plotR package. The cut on a specific value of an FPC score determines the split of a node. Among all the possible FPCs and splitting score values of all the derivatives’ orders, the one that maximizes the decrease of impurities of the node is chosen.

The AFCT Classifier has an accuracy of 95% on the training set (apparent error) and 83% on the test set, which shows an improvement compared to an FCT without derivatives.

We tested the AFRF algorithm on different forest sizes and numbers of FPCs to measure its accuracy.
As a result, we have a distribution of values related to accuracy for each value of the forest size rather than a single value. This circumstance makes the results more robust because it significantly limits the impact of chance. We compared the results of the new algorithm with those of FRF without augmentation, always relying on using the test set.
Figure12presents the results of the FRF and AFRF approaches. It can be observed that FRF exhibits peaks of maximum accuracy at 91%, while AFRF shows peaks at 93%. To better compare the accuracy distributions, we propose Figure13, where we overlap the boxplots and can easily appreciate that AFRF is consistently superior to FRF.

Further comparison is presented in Figure14, where we illustrate the mean accuracies instead of considering medians and quartiles. It is quite evident that AFRF is significantly superior, and this difference increases with the growth of the forest size and then stabilizes. A simple calculation of the means of the two algorithms (for all tree sizes and all possible numbers of FCPs) highlights that FRF has an overall mean accuracy of 83.43%, while AFRF has an overall mean accuracy of 85.10%. We stress that the previous world record for this dataset was set by the BOSS algorithm at 89.05%.

To compare AFRF, based on FPCs, with the most recent and widely used methods for classifying functional data, we present the results of various approaches implemented in the fda.usc R package[4]. Table1emphasizes that the other methods fail to achieve the performance of the AFRF classifier.
The functional K-NN classifier achieves a test set accuracy of 91.00% with 3 or 5 nearest neighbors. Lastly, employing functional depth classifiers in Table2, the highest accuracy on the test set is 81.00% with the depth measure ”RP.” In summary, the AFRF classifier based on FPCs demonstrates compelling results and proves to be the most accurate among the tested methods for this dataset.

The Conditional Permutation Importance for Augmented
Functional Principal Components is displayed in Figure15.
We compare the Permutation Importance for Augmented
Functional Principal Components under two scenarios.
The left panel displays the Unconditional Importance, which measures the importance of each AFPC without considering the influence of other variables. The right panel shows the Conditional Importance, where the importance of each AFPC is evaluated while accounting for the effects of other scores of the corresponding dimensions. This side-by-side comparison reveals how the relevance of each AFPC changes when conditioning is applied. For instance, AFPCs likeexhibit a substantial increase in conditional importance, suggesting they gain relevance when other variables are controlled. Conversely, some components, such as, demonstrate higher unconditional importance, indicating their overall influence without the conditioning effects. This comparison is essential for identifying which AFPCs are robust across different conditions and which are more sensitive to including other FPCs.

Figure16presents a scatter plot that compares the conditional and unconditional permutation importance of AFPCs.
The scatter plot illustrates the relationship between conditional importance (x-axis) and unconditional importance (y-axis). The red dashed line represents the bisector, with equal importance values. AFPCs above this line (e.g., FPC_1) have decreased in importance after conditioning, indicating that their significance is reduced when the effects of other variables are controlled. Conversely, AFPCs below the line (e.g., FPC_2) have increased in importance, highlighting that they become more relevant when conditioned on other factors. This analysis helps to identify which AFPCs are more stable across different conditions and which ones are sensitive to changes in the analytical context.

SECTION: 5Simulation study

To demonstrate the effectiveness of the AFPCs classifier, we customize and refine various models previously examined by researchers such as[9,10,19]to generate functional data with distinctive shapes. We present six scenarios in which we generate 100 functions from each group, forming training and test sets of different sizes according to the number of balanced classes (200 curves when three classes are present, 300 curves when dealing with three groups, etc.).
In all instances, data are equally divided into training and test sets, and each curve spans a domain of 50-time observations.
For each simulation, we conduct comparisons between FRF and AFRF. The diverse scenarios are generated through the following simulations.

Figure17illustrates the six datasets generated using the model described above.

Simulation 1. Group 1 is modeled by the equation, while Group 2 is represented by the equation, whereranges from 0 to 1. The termdenotes a Gaussian process characterized by a zero mean and covariance defined as. Here,takes values from the setwith equal probability, andis an indicator function. Furthermore,serves as a constant determining the extent to which the curves in Group 2 deviate from the overall trend observed in Group 1. The variableis uniformly distributed within the interval. The simulation enables the generation of two distinct groups with variations in their amplitudes limited to a specific segment of the time domain. For this simulation, we set the parameters:,,,,,, and.

Simulation 2. In this simulation, we employ two distinct functional data-generating models to create two groups, primarily distinguished by their amplitudes. The primary model takes the form. To generate Group 2, we adopt the model, wherevaries within the interval, andis confined to. The coefficientsandfollow a uniform distribution in the interval, whileandfollow a uniform distribution in. Additionally,andadhere to a uniform distribution within. The binary variablefollows a Bernoulli distribution, andrepresents a Gaussian process characterized by a zero mean and covariance function given by. Figure17visually displays the simulated data, with parameters set as follows:,,,,,,,, and.

Simulation 3. In this simulation, we explore two distinct functional data-generating models to create two groups that exhibit slight differences both in magnitude and shape, specifically within a designated segment of the time domain. Group 1 is generated using the model. In contrast, Group 2 is produced by the model, where,is a Gaussian process with a zero mean and covariance function defined as,follows a Bernoulli distribution with, and,,,are constants. Additionally,follows a Uniform distribution in the interval. The parameters defining the two groups are set as follows:,,,,,,,,, and. This choice of parameters ensures that the groups differ in both amplitude and shape, contributing to a nuanced variation within a specific time frame.

Simulation 4. This simulation is based on the same model introduced for simulation three but with the difference that four different classes are created. Naturally, the parameters can be set differently. Two parameter configurations are generated to obtain four different groups. The first configuration is as follows:,,,,,,,,, and. The second configuration is as follows:,,,,,,,,, and.

Simulation 5. This simulation is based on the same procedure introduced for simulation four but with the difference that three different classes are created. The first configuration is as follows:,,,,,,,,, and. The second configuration is as follows:,,,,,,,,, and.

Simulation 6. This simulation follows the methodology introduced in simulation one, with the distinction that it involves the creation of three distinct classes. The first configuration is specified as follows:,,,,,, and. The second configuration is characterized by,,,,,, and.

Figure18depicts the outcomes of the six simulated scenarios and provides a comparative analysis between FRF and AFRF in terms of accuracy. Similar to the application on ECG200 data, we present the average accuracy across different numbers of augmented features for each forest size, offering a more reliable metric than relying on maximum accuracy, which might be susceptible to chance-induced fluctuations.
The results show that the proposed functional classifier (AFRF) consistently outperforms FRF in five simulated scenarios. This underscores the AFRF’s robustness and efficacy across various scenarios.
In Figure19, we present the accuracy distribution for each simulated scenario, offering insights into result variability and facilitating comparisons
between FRF and AFRF regarding accuracy distributions across varying forest sizes. Each subplot corresponds to a different simulation, with the x-axis representing the forest size and the y-axis representing the accuracy. The boxplots illustrate the accuracy distribution for both FRF and AFRF. The comparison shows how the distributions evolve as the forest size increases, providing insight into the augmentation approach’s performance stability and potential improvements. It is important to note that these comparisons are crucial to avoid attributing better results to chance rather than to the effectiveness of the AFRF methodology.
Finally, Figure20illustrates the most representative AFCT (Augmented Functional Classification Tree) and elucidates how augmented features contribute to the classification rule. For a more in-depth understanding, additional details on the original functions, first and second derivatives for each simulated scenario can be found in the supplementary material.

SECTION: 6Discussion and Conclusions

The field of supervised classification for curves is currently evolving. While there have been advancements, the integrated use of Functional Data Analysis (FDA) and tree-based classifiers still needs to be explored, with limited investigations conducted. Prior studies have investigated this combination from diverse methodological and applicative perspectives[14,15,13,16,31,19,21]. Despite these efforts, several aspects warrant further exploration and development.
Critical areas for improvement include enhancing the accuracy of functional classifiers by incorporating relevant features, introducing graphical tools to facilitate the interpretation of classification rules, conducting comprehensive simulation studies, and developing additional tools for determining optimal parameters in the supervised classification of functional data. The dynamic and promising nature of ongoing research in this domain presents numerous opportunities for further advancements and contributions to the evolving landscape of curve classification methodologies.

This paper presents an innovative supervised classification strategy combining Functional Data Analysis (FDA), tree-based ensembles, and extracting additional insights from curve analysis through diverse perspectives. The proposed approaches, namely Augmented Functional Classification Trees (AFCTs) and Augmented Functional Random Forests (AFRFs), are designed to tackle the challenges of learning from high-dimensional data, aiming to enhance classification performance.
By leveraging additional features derived from the original functions, such as sequential derivatives, the augmented functional data strategy captures nuanced information about the rate of change and local behaviour within the functional data. This augmentation process enriches the information provided to the functional classification tree ensembles, thereby enhancing the classifier’s predictive power.
Additionally, we developed a novel feature importance metric designed explicitly for Augmented Functional Random Forests (AFRF) to address the limitations of conventional methods, such as Gini Importance and Permutation Importance when applied to functional data with derivatives. The inherent correlations among functional principal component scores derived from successive derivatives can bias traditional importance measures. To mitigate this, we introduced the Conditional Permutation Importance for Augmented Functional Principal Components (CPIAFPCs), which accurately assesses each score’s contribution by conditioning on correlated features, providing a more reliable and interpretable model performance evaluation.

Extensive experimental evaluations on real-world and simulated datasets demonstrate the effectiveness of the proposed procedure. Comparisons with existing methods show exciting results in terms of classification performance. Hence, this research contributes to the field of FDA by demonstrating its potential for handling high-dimensional data in supervised classification tasks and providing valuable insights into the underlying functional characteristics of the data.
The results highlight the effectiveness of the AFCTs and AFRFs in improving classification performance and gaining a deeper understanding of the functional nature of the data.
As demonstrated by the simulation studies, the results consistently affirm that adding additional features is a potent predictive tool for curves. Notably, the first simulated scenario exhibits no improvement over an approach based solely on the original curves. The rationale for this observation is straightforward, as depicted in Figure17: the first scenario represents the simplest case, where the inclusion of additional features is unnecessary. In this context, both groups of curves exhibit straightforward trends and minimal variability, rendering the use of derivatives unnecessary.
Conversely, when considering scenarios with a greater number of classes and diverse curve shapes, the proposed classifier demonstrates substantial advantages. It adeptly captures local characteristics that the original curves struggle to discern with the same level of efficacy. This underscores the value of incorporating additional features, particularly in situations where the complexity of curve patterns demands a more nuanced approach for accurate classification.

This paper focused on FPCs decomposition as a powerful dimensional reduction tool for curves. Of course, the procedure can be extended to other functional classifiers and basis functions on a fixed-basis and data-driven basis.
Future research directions in this area include further advancements in functional data analysis, exploring additional features for augmentation, and investigating models’ explainability techniques for functional classifiers.
The proposed methodology opens up new possibilities for functional data analysis and offers valuable insights into the underlying functional characteristics of the data. With continued research and exploration, we expect to uncover more innovative techniques for managing large quantities of functional data and interpreting the results from a statistical, causal, and inferential perspective.

SECTION: Supplementary Information

Supplementary files accompany this article.

SECTION: Declarations

The authors declare that they received no funding for this study and have no affiliations or involvement with any organization that has a financial or non-financial interest in the subject matter of this manuscript.

SECTION: Funding and/or Conflicts of Interest/Competing Interests

The authors confirm that they received no support from any organization for the submitted work. They also declare no affiliations or involvement with any organization or entity that has a financial or non-financial interest in the subject matter of this manuscript.

SECTION: Use of Generative AI in Scientific Writing

The authors usedGrammarly AIto improve the English language in preparing this work. They reviewed and edited the content as necessary and took full responsibility for the content of the publication.

SECTION: Data availability statement

The authors utilized publicly available data for real-world applications. Simulation data can be provided upon request.

SECTION: References