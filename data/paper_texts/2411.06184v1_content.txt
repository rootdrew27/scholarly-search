SECTION: Alleviating Hyperparameter-Tuning Burden in SVM Classifiers for Pulmonary Nodules Diagnosis with Multi-Task Bayesian Optimization
In the field of non-invasive medical imaging, radiomic features are utilized to measure tumor characteristics. However, these features can be affected by the techniques used to discretize the images, ultimately impacting the accuracy of diagnosis. To investigate the influence of various image discretization methods on diagnosis, it is common practice to evaluate multiple discretization strategies individually. This approach often leads to redundant and time-consuming tasks such as training predictive models and fine-tuning hyperparameters separately. This study examines the feasibility of employing multi-task Bayesian optimization to accelerate the hyperparameters search for classifying benign and malignant pulmonary nodules using RBF SVM. Our findings suggest that multi-task Bayesian optimization significantly accelerates the search for hyperparameters in comparison to a single-task approach. To the best of our knowledge, this is the first investigation to utilize multi-task Bayesian optimization in a critical medical context.

SECTION: Introduction
Against the backdrop that lung cancer continues to be the leading cause of cancer mortality worldwide, precisely differentiation between benign and malignant pulmonary nodules has always been the focus of machine learning and clinical medicine. Machine learning based pulmonary nodule diagnosis has opened up new opportunities to relax the limitation from physicians’ subjectivity, experiences and fatigue, thereby speeding up the diagnostic process and saving more lives.

Distinguishing malignant and benign nodules traditionally involves tasks such as image preprocessing, feature learning and classification model construction, which affect each other. For instance, the radiomic features extracted in the feature learning are easily affected by the image discretization strategy in the image preprocessing, which in turn affects the diagnostic accuracy of the classification model. Therefore, to achieve better diagnostic accuracy, it is necessary to find a set of optimal policy combinations from the policy space which defines the approaches and/or their parameters for implementing each task.

In reality, related work is limited to only finding combinations of strategies and parameters for a specific task without explicitly acknowledging inter-task relationships. Ignorance of inter-task relationships frequently induces repetitive algorithm-tuning work that is dependent on expert knowledge, and impacts of strategies and parameters used in this task on subsequent tasks are unpredictable. Furthermore, it induces huge overhead accompanied by time-expensive medical image processing.

To quickly evaluate the performance of the diagnostic model under different policy combinations, alleviating the burden of model hyperparameter tuning, we, consequently, charted an alternative route by leveraging multi-task Bayesian optimization (MTBO). By appreciating the useful inter-task relationships, an accelerated search could be expected by transferring these commonalities among tasks, and thereby possibly alleviate time-consuming repeated searches. On the way to this destination, we raise two challenging questions:

Our contributions are two-fold, andsummarizes the complete flow of this study.

For the first question, we generate multiple medical tasks (distinguishing benign from malignant pulmonary nodules) through different image discretization strategies. Then the RBF SVMs are used as the classification models, and the hyperparameters of multiple SVM classifiers are tuned simultaneously using MTBO to obtain the optimal model. This is the first study to apply MTBO technology to the medical field.

For the second question, the proposed algorithmefficiently finds suitable hyperparameters for the diagnostic models and generally outperforms the performance of single-task Bayesian optimization. Moreover, in the optimization of multi-tasks that require a lot of time to evaluate, MTBO has greater application potential.

The application of machine learning in medical imaging still faces daunting challenges. Our research aims to narrow the gap between machine learning research and clinics-oriented decision-making practice.

SECTION: Related Work
, resampling of voxel intensities within the ROI to a limited range of gray values (also known as bin), is a necessary procedure during the medical image analysisVarious image discretization strategies can be generated by changing the bin width, the bin number, or the quantization range, but there is currently no unified standard. Studies have shown that image discretization strategies can substantially influence the stability of radiomic features. No relevant study has investigated whether different discretization strategies directly affect the diagnosis of benign-malignant pulmonary nodules.

differentiates between benign and malignant nodules on different image formats. The general workflow of handling a pulmonary nodule classification task based on traditional machine learning methods includes (1) Preprocessing images to make them uniform in format and easy to compute; (2) Extracting features that can describe the nature of the images or nodules (shape, statistic, texture, etc.) as the input to the classification models; (3) Building models(LASSO, SVM, random forest, neural networks, etc.) to make the final judgment. The convolutional neural network(CNN) is another widely used tool in pulmonary nodule classification, which can automatically learn the implicit information in images without the need for manual feature extraction.
During the whole process, people need to make a trade-off between the complexity of features and models and their generalization performance, that is, simple features or models can hardly express all the information that determines the nodule’s malignancy, while complicated ones may lead to overfitting, long training time, and oversensitivity to hyperparameters. Therefore, choosing the most suitable method among the various feasible options becomes the key to solving the problem. However, trying to compare all the combinations together is extremely time-consuming. To the best of our knowledge, there are no studies on how to improve efficiency in evaluating multiple models in the field of lung pulmonary classification.

is an essential part of training an efficient machine learning model. For small-scale problems and simple-structured models, grid search is the most widely used strategy for hyperparameter-tuning. For instance, when building a survival model based on the LASSO method, researchers usually select several penalty coefficients at equal intervals and use cross-validation to determine the optimal value. For complex models (such as deep learning) or large datasets, function evaluation can be very expensive, random searchand Bayesian optimizationwill be more appropriate than exhaustive algorithm. In the medical field, Bayesian optimization has been successfully applied to automatic parameter tuning of deep learning. In multi-task scenarios, the hyperparameter-tuning burden is heavier. Swersky et al.proposed multi-task Bayesian optimization to exploit the similarity between tasks to improve the overall optimization efficiency. However, the technique of multi-task Bayesian optimization has not been applied to multiple medical tasks.

is a machine learning paradigm aims to improve the generalization performance of multiple related learning tasks by lever-aging domain-specific information between tasks. The paradigm has been applied in cancer research to improve predictive performance. Inspired by the concept of multi-task learning, multi-task optimization has been developed. In machine learning, optimizing the tasks by sharing features between related datasets can improve the efficiency of the training process, but multi-task optimization has little application in the medical field.

SECTION: Proposed Method
In this section, we introduce nine image discretization strategies and multi-task Bayesian optimization, and then propose an algorithm to guide how to use multi-task Bayesian optimization to tune parameters for multiple SVMs simultaneously.

SECTION: Image Discretization Strategies
In medical image analysis, image discretization groups intensity values within the region of interest into smaller bins, which is the premise of radiomic feature extraction. The intensityof voxelin original image is transformed to a discretized intensityby followingand.

wheredenotes number of bins, anddefines a sequence used as the reference to discretize the original voxel’s intensity.
Obviously discretization highly depends on user-specified tripletthat controls the sensitivity of the discretization to noise. Our primary concern is how best to eliminate the negative impact of inappropriate discretization strategies on the followup procedures (i.e. radiomic feature computation and classification).

In this study, we designed multiple discretization strategies, that is, different combinations on. We chose three bin numbers (16, 32, and 64) and three quantization ranges (min-max, mean ± 2SD, and mean ± 3SD). Nine discretization strategies were generated in total. Instead of evaluating one by one, we adopted the multi-task Bayesian optimization to evaluate them simultaneously to save computational costs. Next, we introduce the multi-task Bayesian optimization.

SECTION: Multi-Task Bayesian Optimization
Multi-task Bayesian optimization leverages the commonality between tasks to facilitate time-saving learning. The expensive-to-evaluate function is represented by a cheap multi-task Gaussian surrogate model updated by feeding new observations. To lighten the burden of acquiring new observations, the inter-task dependence guides selection of promising points. This dependence is learned by a positive semi-definite covariance matrix over tasks. It sped up the search for hyperparameters. This paper explores the possibility of using multi-task Bayesian optimization to accelerate the search for hyperparameters in multiple SVM classifiers for benign–malignant classification of pulmonary nodules.

In multi-task Bayesian optimization, supposed that there arerelated tasks, given a setofdistinct pointswe defineas the response of thetask under, and. Multi-task Gaussian processregression is an appropriate choice to find correspondence betweenand. For,, define a Gaussian process withas input, i.e.,

whereandare Gaussian process’ prior mean function and covariance function, respectively,is the noise variance for thetask. We set

whereis a constant vector,is a positive semi-definite matrix that measures the relationship between tasks, andis a covariance function that measures the relationship between points on. We choose Matérn 5/2 covariance function for. Formally,

without loss of generality, we can setbecause the variance can be fully explained by.
Then the predictive mean and covariance of the new pointon thetask can be inferred as

wheredenotes the Kronecker product,represents all parameters, which will be estimated by maximum likelihood,represents thecolumn of,,,is an-dim identity matrix. If there are missing values in, we impute them with the prior means.

To find the minimum value of, it is usually to maximize an acquisition functionto determine the next query point. We use the expected improvement (EI) criterion, letbe the currently observed minimum,. Since, the expectation can be computed analytically as follows

whereandare a cumulative distribution function and a probability density function of the standard normal distribution, respectively. We can choose the next query point by optimize the acquisition function corresponding to all tasks, or take turns fixing tasks to optimize a single acquisition function.

SECTION: MTBO for Multiple SVM Classifiers
Support Vector Machines (SVMs) with radial basis kernels are often used for classification tasks, but their performance is susceptible to hyperparameters. Hyperparameter-tuning is a necessary prerequisite for obtaining a good SVM classifier. For multiple SVM classifiers, optimizing their hyperparameters individually requires a lot of repetitive work, especially when their tasks are highly related. With the help of MTBO, the correlation between tasks can be effectively utilized to avoid unnecessary searches and improve the overall optimization speed.

The procedure of MTBO for multiple SVM classifier is shown in pseudocode in.are the given datasets used to train the SVM classifier. In our work, we generated nine datasets through varies discretization strategies. Parameters,, andneed to be given in advance.should take a suitable value so that there are enough observations to estimate the parameters, and without wasting too much time to train the SVMs.anddetermine the total number of iterations.represents the loss of-fold cross-validation on datasetwithas the hyperparameters of the SVM.

SECTION: Experiments
SECTION: Data Description and Preprocessing
The in-house dataset we used in this work consists of 499 patients with pathologically confirmed pulmonary nodules, treated between September 2005 and August 2013. All patients received whole body CT scans for diagnosis.

The nodule segmentation is performed semi-automatically, that is, the radiologist manually locates the nodule and examines the final segmentation results, and the contour of the nodule is automatically delineated by the computer programs. First, a trained U-net (R-231)was employed to remove regions outside the lungs in CT, thereby separating the juxta-pleural nodules from the lung boundary. Second, all images were resampled to 1×1×1-mm voxel size using cubic interpolation. Third, the boundary of nodule was described by Active contour modeland refined by morphological methods. Finally, we corrected the segmentation based on the radiologist’s feedback on the results.shows an example of segmentation results in three views.

SECTION: Radiomic Features Extraction
Radiomics, a technique for quantitatively extracting image features from medical images to describe tumor phenotypes, is gaining importance in cancer imaging. We refer toto extract a series of radiomic features from each ROI, including shape features (n=8), first-order statistics features (n=13), and second-order statistics features derived from gray-level co-occurrence matrix (GLCM; n=23) and gray-level run-length matrix (GLRLM; n=12). The details of the features are listed in. Since we generate nine discretization strategies, correspondingly we can get nine sets of feature matrices.

SECTION: Classification and Evaluation
Nine SVM classifiers with radial basis function kernels are trained for distinguishing the benign-malignant pulmonary nodules. The goal is to minimize the classification loss obtained by-fold cross-validation for each SVM. Penalty factorand kernel sizeare two hyperparameters that need to be optimized. We restrict the hyperparametersandto. In addition, since the value range of the loss functionis, we use a monotonic transformation to map the loss function onto, so that the transformed loss functions can be fitted with Gaussian process. The transformed loss functions can be written as:

Then, the optimal hyperparameters can be searched for each classifier using multi-task Bayesian optimization. Single-Task Bayesian Optimization (STBO) is employed to solve each objective function separately as baseline.

To measure the fitting effect of the single/multi-task Gaussian process, we compute the root mean squared error (RMSE):

we can easily recalculate the original loss according to the transformed loss, and the same is true for RMSE. In practice, we set, and.

SECTION: Result and Discussion
For STBO, we useas the initial point to determine the next query by maximizing the EI acquisition function, each task iterate 30 times; for MTBO, we useby setting,, and. Specifically, we first adopt STBO, also starting from, and iteratively search on thetask for 10 times. Other tasks are evaluated on these same 10 queries. Thus, we obtain 90 complete observations to fit a multi-task Gaussian process. Then, we take turns fixing the tasks for a total of 180 iterative searches, thus ensuring that each task is also evaluated 30 times.shows the optimal hyperparameters for each task obtained with STBO and MTBO, respectively, and the classification loss under 10-fold cross-validation by training SVM classifiers with the optimal hyperparameters.

To better understand the behavior of the different methods, we plot the number of evaluations versus the current optimal loss in. In the first ten evaluations, we found that even if the queries of the first task are directly transferred to other tasks, good performance can be achieved. This also shows that the nine tasks we need to optimize are highly related. As shown inand, after 30 iterations, there is not much difference between MTBO and STBO in finding the minimum loss for each task, and the optimal hyperparameters may not unique. It is worth mentioning that although there is no difference in the final result, it can be seen fromthat MTBO always faster to reaches the local optimum. In practical applications, this result makes us more inclined to choose MTBO for hyperparameter-tuning.

After the above experiments, we evaluate the loss for each task on over 3600 sets of hyperparameters. Taking the task N=64,mean ± 3SD as an example,intuitively shows the relationship between the loss and hyperparameters, andandare the surfaces fitted for the task by the single/multi-task Gaussian process, respectively. By comparing these three figures, it can be found that the surface fitted by the multi-task Gaussian process is closer to the real surface than single-task Gaussian process, and the results inalso confirm this. At the same time, we find that the prediction variance during single-task Bayesian optimization is very small (see(e)-(i)). Combining the above findings, it shows that the single-task Gaussian process is slightly overfitting. While the fitting of the multi-task Gaussian process utilizes the information of the related-tasks. With more data, more robust and general representations can be learned for multiple tasks, resulting in better sharing of knowledge between tasks and less risk of overfitting in each task.

In addition, we found that there is no significant difference in model performance under each discretization strategy, so no discretization strategy can be considered optimal only from the perspective of classification loss. Considering the robustness of the strategy, using mean ± 2SD or mean ± 3SD as the quantization range helps to resist noise caused by segmentation or other reasons, such as removing high-density bones in CT. Moreover, choosing a smaller bin number helps to keep the results consistent across multiple imaging and has a significant computational complexity advantage in the feature extraction stage. Therefore, in practice we may prefer to choose a smaller bin number and a more robust quantization range as the image discretization strategy.

SECTION: Conclusion
Medical image analysis based on machine learning currently still face daunting challenges. Our research first introduce the MTBO to the medical imaging and efficiently found satisfactory hyperparameters for RBF SVM classifiers in the face of multiple diagnostic tasks. In addition to image discretization, there are a large number of other non-standardized strategies in the medical image analysis, including filtering, de-noising, and segmentation. This research provides an idea for how to quickly evaluate each strategy. It should be noted that this study has some limitations.

In this study, training a SVM classifier is not time-consuming, instead it takes more time to fit the data using a multi-task Gaussian process. This makes it more efficient to process multiple tasks one by one using single-task Bayesian optimization. The advantages of MTBO will appear when the step of evaluation is really time-consuming, such as training a deep neural network. In this case, the time required to fit a multi-task Gaussian process is negligible.

When we are faced with more tasks, it may be inefficient to directly apply our method, because computational time and memory for large scale matrix operations such as inversion cannot be ignored. In this case some inexact method is needed to speed up multi-task Bayesian optimization.

MTBO requires good complete observations of a certain scale to give appropriate initial estimates of model parameters. This paper gives a method for how to choose these initial points to start the model, but no in-depth study of how many observations to choose.

SECTION: References