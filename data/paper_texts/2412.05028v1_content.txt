SECTION: Unifying Dual-Space Embedding for Entity Alignment via Contrastive Learning

Entity alignment aims to match identical entities across different knowledge graphs (KGs).
Graph neural network-based entity alignment methods have achieved promising results in Euclidean space.
However, KGs often contain complex structures, including both local and hierarchical ones, which make it challenging to efficiently represent them within a single space.
In this paper, we proposed a novel method UniEA, which unifies dual-space embedding to preserve the intrinsic structure of KGs.
Specifically, we learn graph structure embedding in both Euclidean and hyperbolic spaces simultaneously to maximize the consistency between the embedding in both spaces.
Moreover, we employ contrastive learning to mitigate the misalignment issues caused by similar entities, where embedding of similar neighboring entities within the KG become too close in distance.
Extensive experiments on benchmark datasets demonstrate that our method achieves state-of-the-art performance in structure-based EA. Our code is available athttps://github.com/wonderCS1213/UniEA.

Unifying Dual-Space Embedding for Entity Alignment via Contrastive Learning

Cunda Wang1,
Weihua Wang1,2,3,††thanks:Corresponding Author. Email:wangwh@imu.edu.cn.,
Qiuyu Liang1,
Feilong Bao1,2,3,
Guanglai Gao1,2,31College of Computer Science, Inner Mongolia University, Hohhot, China2National and Local Joint Engineering Research Center of Intelligent
InformationProcessing Technology for Mongolian, Hohhot, China3Inner Mongolia Key Laboratory of Multilingual Artificial Intelligence Technology, Hohhot, China

SECTION: 1Introduction

Knowledge graphs (KGs) represent real-world knowledge in the form of graphs.
They typically store data in the form of triples, whererepresents the head entity,the relation, andthe tail entity.
The completeness of KGs affects tasks such as knowledge-driven question answering(Sun et al.,2024)and recommendation(Cai et al.,2023; Liang et al.,2025).
Hence, it is essential to integrate multiple source KGs to build a comprehensive KG.
Entity alignment (EA) serves as an important step in this process.
It aims to identify the same real-world entities referenced across different KGs.

Recently, due to the strong neighborhood learning capabilities of graph neural networks (GNNs), GNN-based EA have achieved significant progress(Xie et al.,2023; Wang et al.,2024a; Sun et al.,2020b).
However, GNNs face two issues in Euclidean space embedding: 1) limited performance when handling complex hierarchical structures, and 2) the embeddings of neighboring entities are overly similar.

As shown in Figure1, this is a common type of hierarchical structure found in KGs.
Traditional GNN-based EA methods often embed entities like “Iron Man” and “America” directly according to their Euclidean distance. Nevertheless, this does not reflect the true distance between these two entities, leading to distortion in the graph structure embeddings.
The hyperbolic space can capture the hierarchical structure of graphs(Wang et al.,2024b; Liang et al.,2024b). The hyperbolic distance better represents the true distance between the entities “Iron Man” and “America”.
Moreover, these methods(Wang et al.,2018; Yu et al.,2021)cause similar entities within the same KG to have embeddings that are too close in distance.
For example, entities like “Robert Downey Jr.” and “Chris Evans” share multiple neighboring entities, such as “The Avengers“ and “America“. These shared neighbors often lead to homogenization, resulting in incorrect entity alignment.
Current methods have proposed various solutions to these two challenges(Sun et al.,2020a; Guo et al.,2021; Xie et al.,2023; Wang et al.,2024a). For instance,Sun et al. (2020a)andGuo et al. (2021)explore EA task in hyperbolic space embedding, demonstrating that hyperbolic space is more effective for learning the hierarchical structure of graphs, which aids in entity alignment.Xie et al. (2023)alleviates over-smoothing through graph augmentation techniques.
However, the augmentation strategies, which randomly perturb the graph topology, may degrade the quality of the graph embeddings(Shen et al.,2023).
Our motivation is to consider hyperbolic space embedding as an augmentation of graph embedding.
This approach not only avoids the drawbacks of traditional graph augmentation techniques but also leverages the hierarchical structure information provided by hyperbolic embedding.

To address the aforementioned issues, we propose a novel method named UniEA, whichUnifies the Euclidean and hyperbolic spaces embedding forEA.
Our method is not limited to embedding in a single space.
Specifically, we introduce graph attention networks (GATs)(Velickovic et al.,2018)to aggregate neighboring entities in Euclidean space and employ hyperbolic graph convolutional networks (HGCNs)(Chami et al.,2019)to learn the hierarchical structural information of the graph in hyperbolic space.
We maximize the consistency between the embedding in Euclidean space and hyperbolic space throughcontrastive learning, which leads to more accurate entity embeddings.
Moreover, the close distances of similar neighboring embedding severely affect the final alignment of entities. We employcontrastive learning once againto address the issue.
The contributions of this work can be summarized as follows:

We propose a novel EA method called UniEA. To the best of our knowledge, this is the first method for EA that leverages contrastive learning to unify Euclidean and hyperbolic space embeddings.

We also employ contrastive learning to mitigate misalignment issues caused by overly close distances between similar entity embeddings.

The extensive experiments on four public datasets demonstrate that UniEA consistently outperforms the state-of-the-art methods for structure-based EA.

SECTION: 2Related work

In line with our work, we review related work in three areas: EA in Euclidean space, representation learning in hyperbolic space and improving EA with graph augmentation.

SECTION: 2.1EA in Euclidean Space

Current embedding-based EA methods can be broadly categorized into three types: TransE-based EA, GNN-based EA and other methods. All of these primarily aim to learn embeddings for entities and relations from relational triples.

Due to the strong performance of TransE(Bordes et al.,2013)in capturing local semantic information of entities, several methods have proposed variants of TransE for application in EA.
For instance,Chen et al. (2017)addresses the inconsistency in cross-lingual embedding spaces.Zhu et al. (2017)emphasizes path information.Sun et al. (2018)treats EA as a classification task.Pei et al. (2019)enhances knowledge graph embedding by leveraging nodes with varying degrees.

TransE-based EA methods lack the ability to effectively model global structural information. As a result, recent research increasingly favors GNN-based approaches for EA.
Stacking multiple layers of GNNs enables the capture of information from more distant neighbors, which facilitates learning of global structural information.
For example,Wang et al. (2018)directly stacks multiple layers of vanilla GCN(Kipf and Welling,2017)to obtain entity embeddings.
Due to the heterogeneity of KGs, the alignment performance is limited.Sun et al. (2020b)employs a gating mechanism to attempt capturing effective information from distant neighbors. MRAEA(Mao et al.,2020), RAEA(Zhu et al.,2021), KE-GCN(Yu et al.,2021), RSN4EA(Guo et al.,2019), GAEA(Xie et al.,2023), RHGN(Liu et al.,2023), and GSEA(Wang et al.,2024a)utilize rich relational information to obtain entity embeddings.Xin et al. (2022)encoded neighbor nodes, triples, and relation paths together with transformers.
Unfortunately, the ability to handle complex topological structures in graphs is limited in Euclidean space.

Additionally, some methods integrate the rich information within KGs to enhance the performance of EA tasks. This includes leveraging attributes(Liu et al.,2020), entity names(Tang et al.,2020)and more(Chen et al.,2023).Jiang et al. (2024)explores the potential of large language models for EA task.
Since our method focuses on structural information, we do not compare it with the above methods to ensure experimental fairness.

SECTION: 2.2Representation learning in hyperbolic space

Hyperbolic space has recently garnered considerable attention due to its strong potential for learning hierarchical structures and scale-free characteristics.
For example,Chami et al. (2019)first introduced the use of graph convolutional networks (GCNs) and hyperbolic geometry through an inductive hyperbolic GCN.

Hyperbolic space representation learning has been applied to various downstream tasks, achieving excellent performance in areas such as node classification(Liang et al.,2024b)and completion(Liang et al.,2024a,c). Notably, existing work has successfully completed EA using hyperbolic space embedding.Sun et al. (2020a)extends translational and GNN-based techniques to hyperbolic space, and captures associations by a hyperbolic transformation.Guo et al. (2021)integrates multi-modal information in the hyperbolic space and predict the alignment results based on the hyperbolic distance.
Although these methods demonstrate the advantages of hyperbolic embedding, they are limited to embedding solely in hyperbolic space.

SECTION: 2.3Improving EA with graph augmentation

Graph augmentation techniques primarily generate augmented graphs by perturbing the original graph through node dropout or edge disturbance, effectively enhancing the model’s robustness to graph data.

Graph augmentation techniques have been proven effective in entity alignment tasks. GAEA(Xie et al.,2023)opts to generate augmented graphs by removing edges rather than adding new ones, as introducing additional edges can lead to extra noise. GSEA(Wang et al.,2024a)employs singular value decomposition to generate augmented graphs, capturing the global structural information of the graph. It leverages contrastive loss to learn the mutual information between the global and local structures of entities. However, these methods fall short in effectively learning the hierarchical structure of graphs.

SECTION: 3Preliminaries

In this section, we define the EA task and explain the fundamental principles of hyperbolic space. This foundation is essential for comprehending our approach.

SECTION: 3.1Entity alignment

Formally, we repesent a KG as, wheredenotes entities,denotes relations,repesents triples.
Given two KGs,repesent source KG,repesent target KG.
EA aims to discern each entity pair,,whereandcorrespond to an identical real-world entity.
Typically, we use pre-aligned seed entitiesto unify the embedding spaces of two KGs in order to predict the unaligned entities.

SECTION: 3.2Hyperbolic space

Hyperbolic geometry is a non-Euclidean geometry with a constant negative curvature, where curvature measures how a geometric object deviates from a flat plane(Chami et al.,2020).
Here, we use the-dimensional Poincaré ball model with negative curvature.
For each point, the tangent space (a sub-space of the Euclidean space)is a-dimensional vector space at point, which contains all possible directions of path inleaving from. Then, we introduce two basic operations that exponential and logarithmic maps in the hyperbolic space.

Letbe the feature vector in the tangent space;is a point in the hyperbolic space, which is also as a reference point. Letbe the origin,, the tangent spacecan be mapped tovia the exponential map:

Conversely, the logarithmic map which mapstois defined as:

Here,is hyperbolic space embedding.

SECTION: 4Method

In this section, we elaborate on our approach in four parts.
As shown in Figure2, our method includes: 1) Euclidean space embedding, 2) hyperbolic space embedding, 3) relation encoding and fusion, and 4) the loss function.

We randomly initialize the entity and relation embedding of, represented asand, respectively. Similarly, the entity and relation embedding ofare represented asand.
Here,denotes Euclidean space embedding;andstand for the dimensionality of entity and relation, respectively.

SECTION: 4.1Euclidean space embedding

The ability of GAT to aggregate neighbor information in heterogeneous graphs has been well demonstrated(Chen et al.,2023; Wang et al.,2024a). We stack multiple layers of GAT to obtain Euclidean space embedding:

whereMdenotes the graph adjacency matrix,is a diagonal weight matrix for linear transformation.

Due to the varying importance of the neighborhoods aggregated by different layers of GAT.
For example, in Figure1, aggregating the first-order neighbors of “Chris Evans” is most beneficial. While aggregating higher-order neighbors can capture some implicit relationships of the entity, it often introduces noise.
Therefore,Xie et al. (2023)introduce an attention mechanism(Vaswani et al.,2017)to assign different weights to the embeddings obtained from different layers:

whereis the scaling factor,andare the learnable paramenter matrices. Finally, the Euclidean space embedding.

SECTION: 4.2Hyperbolic Space embedding

Our method equips HGCN(Chami et al.,2019)to learn the hierarchical structure of graphs in hyperbolic space.

Specifically, we project Euclidean space embeddingsto hyperbolic space using exponential map (Equation1):

where, in other words, we obtain the first layer of embeddingin hyperbolic space.

For the hyperbolic space embedding of the-th layer, by hyperbolic feature aggregation, we can get the hyperbolic embedding of the next layer.
The hyperbolic aggregation process is as follows:

Arepresents the symmetric normalized adjacency matrix,isandis a trainable weight matrix.

For example, for the inputin-th layer, we can getusing Equation6.

Finally, we can obtain the final outputin Hyperbolic Space.
The ‘’ is a hyper-parameter denoting the number of layers of the HGCN.

SECTION: 4.3Relation encoding and fusion

The same entities often share similar relations, and relational semantic information is also highly beneficial for EA.Mao et al. (2020)reveals that relying solely on the inflow direction to accumulate neighboring information through directed edges is insufficient. Accumulating information from the outflow direction as well would be highly beneficial.
This idea facilitates the bridging and propagation of more information in such a sparse graph.
Hence, following this work, we use both in-degree and out-degree relation encoders to learn the representation of relations: