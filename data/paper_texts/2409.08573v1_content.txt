SECTION: HTR-VT: Handwritten Text Recognition with Vision Transformer
We explore the application of Vision Transformer (ViT) for handwritten text recognition. The limited availability of labeled data in this domain poses challenges for achieving high performance solely relying on ViT. Previous transformer-based models required external data or extensive pre-training on large datasets to excel.
To address this limitation, we introduce a data-efficient ViT method that uses only the encoder of the standard transformer. We find that incorporating a Convolutional Neural Network (CNN) for feature extraction instead of the original patch embedding and employ Sharpness-Aware Minimization (SAM) optimizer to ensure that the model can converge towards flatter minima and yield notable enhancements. Furthermore, our introduction of the span mask technique, which masks interconnected features in the feature map, acts as an effective regularizer. Empirically, our approach competes favorably with traditional CNN-based models on small datasets like IAM and READ2016. Additionally, it establishes a new benchmark on the LAM dataset, currently the largest dataset with 19,830 training text lines. The code is publicly available at:.

[a]organization=China Three Gorges University,country=China

[b]organization=Max Planck Institute of Biochemistry,country=Germany

[c]organization=Intellindust,country=China

SECTION: Introduction
The task of handwritten text recognition aims at recognizing the text in an image that has been scanned from a document. The standard approachtypically involves two steps: first, using a detector to identify the lines of text, and then predicting the sequence of characters that make up each line. This paper focuses on the latter task, which aims to accurately predict the text in a given line image.
As described in LAM: "This level of annotation
granularity has been chosen as it is a good trade-off between
word-level and paragraph-level in terms of required time, cost,
and amount of supervision and because it is common in HTR
research." This rationale for creating the dataset aligns with our initial decision to focus our research on line-level recognition. The importance of line-level recognition is significant.
At the same time, recognizing handwritten text lines is a difficult task due to variations in writing styles between individuals and the presence of cluttered backgrounds (examples are provided in Figureand in the supplementary material). Previous approaches mainly relied on Convolutional Neural Networks (CNNs)or recurrent modelsto address this challenge .

Recent success of Vision Transformer (ViT)in computer vision tasks has motivated researchers to explore its potential in handwritten text recognition. However, ViT does not introduce any strong inductive bias in its model design and is recognized for its dependency on vast quantities of annotated training data to deliver good performance. Considering the limited number of annotated samples available for handwritten text recognition (as shown in Table), earlier transformer-based methods, utilizing the standard transformer architecture (both encoder and decoder), relied on large-scale real-world or synthetic data for pre-trainingto achieve satisfactory performance.

In this paper, we introduce a simple and data-efficient ViT-based model that solely employs the encoder component of the standard transformer for handwritten text recognition.
Our objective is to propose a novel ViT-like model to perform well on this task while making minimal modifications to the standard ViT architecture. Our preliminary findings indicate that ViT can deliver satisfactory results, particularly on the LAM dataset, which is the most extensive dataset containing 19,830 training samples. We use this dataset as the basis of our experimental design, which establishes a benchmark for assessing and contrasting our proposed model. Instead of using a patch embedding to generate input tokens, we demonstrate through experimentation that using a
widely-used ResNet-18to extract intermediate visual feature representations as input tokens is much more conducive to stable training and significantly better performance. Additionally, we show that employing Sharpness-Aware Minimization (SAM)as optimizer enforces the model to converge towards flatter minima and randomly replacing span tokens with learnable tokens can alleviate overfitting and achieve consistent improvement across various dataset scales.

Despite its simplicity, our approach achieves promising performance on standard benchmarks. On the largest dataset LAM(containing 19,830 training samples), our approach outperforms both CNN-based and transformer-based approaches by a clear margin. On small-scale datasets such as IAM(containing 6,428 training samples) and READ2016(containing 8,349 training samples), we achieve better performance than other transformer-based approaches and competitive performance compared with CNN-based approaches.

The main contributions of this paper are summarized as the following:

We propose a simple and data-efficient approach for handwritten text recognition, with minimal modifications on the ViT.

We empirically show that without pre-training or any additional data, our ViT-like model can achieve state-of-the-art performance on handwritten text recognition.

SECTION: Related Work
Network architectures for handwritten text recognition today typically use a combination of convolutional layers and recurrent layers. A number of convolutional layers are stacked and placed at the start of the network to extract local features from text-line images, followed by recurrent layers, specifically Bi-directional Long Short-Term Memory (BLSTM)layers. These recurrent layers process the features sequentially to output character probabilities based on contextual dependencies. Such
an architecture results in a Convolutional Recurrent Neural Network (CRNN). The models are typically trained using the Connectionist Temporal Classification (CTC) loss, which allows for dealing with label sequences of shorter length than predicted sequences, without knowledge of character segmentation. Encoder-Decoder-based architectures have also been explored for handwritten text recognition. In,
the CTC loss is replaced with the cross-entropy loss, and the sequence alignment is achieved via an attention-based encoder-decoder architecture. A special end-of-line token is introduced to stop the recurrent process.
While these models can obtain lower test error rates, some of them often require complex pre/post-processing steps and suffer from the lack of computation parallelization inherently, which affects both training and inference time.
Recently, Fully Convolutional Networks (FCNs)have been proposed as an alternative to traditional CRNNs. FCNs simulate the dependency modeling provided by LSTM by combining them with GateBlocks layers, which implement a selection mechanism similar to that of LSTM cells. Each gate in GateBlocks is made up of Depth-wise Separable Convolutions, which reduce the number of parameters and speed up the training process. OrigamiNetfocuses on learning to unfold the input paragraph image into a single text line. This transformation network enables using the standard CTC lossand processing the image in a single step. In contrast, Coquenet et al.proposed models that incorporate a vertical attention mechanism to recurrently generate line features and perform an implicit line segmentation.
While FCNs have obtained state-of-the-art results in recent years, they may still struggle with long-range contextual dependencies.

Transformer-based architectures have not been widely explored in handwritten text recognition, but some recent approaches have used Transformers in place of RNNs. These models often require pre-training on large real or synthetic datasets to achieve comparable performance to mainstream models.

TrOCRis a recent approach to handwritten text recognition that integrates two powerful pre-trained models respectively from computer vision and NLP, BEiTand RoBERTa. BEiT is a vision transformer that functions as an encoder and is pre-trained on ImageNet-1K, a dataset of 1.2 million images, while RoBERTa serves as a decoder that generates texts. To pre-train the TrOCR model, Li et al.synthesize a large-scale dataset consisting of both printed and synthetically generated handwritten text lines in English, totaling approximately 687 million and 18 million in the first stage. In this stage, the dataset is not public. In the second stage, they built two relatively small datasets
corresponding to printed and handwritten downstream tasks,
containing millions of textline images each. Finally, the model is fine-tuned on real-world data, such as the IAM dataset. Kang et al.use Transformer models with multi-head self-attention layers at the textual and visual stages and trains with a synthetic dataset of 138,000 lines. Another recent approach, Text-DIAE, employs
a transformer-based architecture that incorporates three pretext tasks as learning objectives to be optimized during pretraining without the usage of labeled data. Some methodsexplored document-level recognition and also applied transformer architectures. While transformer-based models have shown promising results in line-level handwritten text recognition, they still require large-scale real-world or synthetic data for pre-training.

The DeiTis the first work to demonstrate that
Transformers can be learned on mid-sized datasets (i.e., ImageNet-1k)) in relatively shorter training
episodes. Besides using augmentation and regularization
procedures, the main contribution of
DeiTis a novel distillation that relies on a distillation token. Liu et al.propose a dense relative localization loss to improve ViTs’ data efficiency.
DropKeyis a recent data-efficient methodology to effectively improve the dropout technique in ViT by moving dropout operations ahead of attention matrix calculation and setting the Key as the dropout unit, yielding a dropout-before-softmax scheme.

SECTION: Method
In this section, we present our approach to handwritten text recognition. Given an input handwritten text line, whereandare the width and height of the image, our approach encodes the image into a set of spatially-aware featuresusing a CNN extractor. The number of features, determined by the down-sampling ratio of the width and height of the image, denoted asand, respectively. We then use a transformer encoder to take these features as input tokens and output character predictions. The entire model is optimized using the Connectionist Temporal Classification(CTC) loss. Our method is summarized in Figure.

In Section, we revisit the architecture of the Vision Transformer (ViT). In Section, we describe our data-efficient ViT approach for handwritten text recognition, which involves a CNN feature extractor, Sharpness-Aware Minimization (SAM)and a new masking strategy: span mask strategy. We provide implementation details in Section.

SECTION: Preliminary: Vision Transformer (ViT)
Vision Transformer (ViT)decomposes each image into a sequence of tokens with a fixed length, where the tokens represent non-overlapping image patches. Similar to BERT, ViT adds an additional class tokento the sequence, which represents the global information of the image. To retain positional information, position embeddings are explicitly added into each patch including the class token. Note that our model removes the additional class token and uses sinusoidal position embeddings byto the encoder’s inputs, as used in MAE.

Subsequently, all tokens undergo processing via stacked transformer encoders, A transformer encoder comprises N blocks, with each block featuring a multi-head self-attention () layer followed by a feed-forward network (). The, which includes a simple two-layer MLP, is augmented by the GELU activation functionafter the first linear layer. Furthermore, layer normalization (LN)is applied before every block, and residual shortcutsare used after every block. The processing of the n-th block can be expressed as:

whereis the input of the-th block,anddenote the number of tokens
and the dimension of the embedding, respectively.

SECTION: ViT for handwritten text recognition
We present a ViT-based model designed for handwritten text recognition with minimal adjustments to the standard ViT. Our proposed network architecture is depicted in Figure. ViT alone is not stable for handwritten text recognition (see Section). Therefore, we suggest three modifications:a CNN feature extractor to obtain features for each input token, enabling powerful feature extraction,a span feature masking strategy to replace masking tokens with learnable tokens, effectively alleviating the impact of overfitting, andemploy Sharpness-Aware Minimization (SAM) optimizer to ensure
that the model can converge towards flatter minima.

To make our pipeline simple, we adopt the widely-used ResNet-18as our CNN feature extractor, with minor adjustments made to accommodate line-level handwritten text images. Specifically, we remove the final residual block and adjust the stride to produce features with enough information for character recognition while maintaining the two-dimensional nature of the task. More details about the modification as well as experiments of additional CNN feature extractors are provided in the supplementary material.

Our work draws inspiration from BERT, SpanBERTand MASS, which leverage the prediction of randomly masked words or tokens to learn expressive language representations. We have adapted this methodology to our specific task and observed the benefits of employing random feature masking. Furthermore, our intuition suggests that the feature extractor can capture a board receptive field. To enhance the model’s comprehension of contextual information encompassing neighboring ink pixels, we propose expanding the masking range.

Precisely, the feature map after the CNN feature extractor is flattened to a sequence of tokens with dimensions, whererepresents the sequence length andrepresents the feature dimension. We randomly mask the span of tokens with a maximum span length(i.e., the number of interconnected tokens). In totaltokens are masked and replaced with a learnable token, whereis a hyperparameter defining the mask ratio. More details of the span mask strategy are provided in the supplementary material.

Sharpness-Aware Minimization (SAM), proposed by Foret et al., is an optimization method that enhances the generalization of deep neural networks (DNNs). It aims to find model parameters that reside in flat minima, ensuring a uniformly low loss across the model. Given our objective functionand the parameters of the DNN, the SAM optimizer is designed to findsuch that:

whererepresents a perturbation vector, andis the size of the neighborhood within which the algorithm minimizes the sharpness of the loss function. The SAM algorithm functions by alternately identifying the worst-case perturbationthat maximizes the loss within an-norm ball of radius, and then updating the DNN parametersto minimize this perturbed loss.

We employ a ViT encoder with 4 layers. Each layer is with a dimension of 768 and 6 heads. The hidden dimension of MLP in the feed-forward network (FFN) is 3,072. Larger ViT models do not bring obvious gain. For our span mask strategy, we set the mask ratio to 0.4 and the span length to 8 in all datasets. An ablation study of the mask ratio and span length is provided in Section. For all experiments, we use a batch size of 128 and optimize all our models with the AdamWoptimizer for 100,000 iterations with a weight decay of 0.5. We perform a warm-up-cosine learning rate schedule with the max learning rate equal to 1e-3 and use 1,000 iterations for warm up. Trainings are performed on a single GPU RTX 4090 (24Gb) and in
the following experiments, models are trained for almost 16 hours. Similar to OrigamiNet, we use the exponential moving average (EMA) method with a decay rate of 0.9999. For data augmentation, we fix the input image resolution to 512 x 64 and use random transformation, erosion, dilation, color jitter, and elastic distortion. We set the probability of using each data augmentation to 0.5, and they can be combined
with each other.

SECTION: Experiments
In this section, we evaluate the performance of our model for line-level recognition. Our experimental results demonstrate that our model achieves state-of-the-art results on the LAMand IAMdatasets. Moreover, our model competes well with other state-of-the-art models on the READ2016datasets. It is worth noting that our model achieves good performance without any pre-training or synthetic data and without relying on any pre/post-processing steps.

To further analyze the performance of our model, we conduct an ablation study by modifying the standard ViTarchitecture. Specifically, we investigate the impact of different mask strategies and hyperparameters on the READ2016 dataset and examine how the SAM optimizeraffects our model’s performance.

SECTION: Dataset and evaluation metrics
We evaluated our model’s performance on three commonly used datasets for handwritten text recognition: LAM, READ2016, and IAM. Among these datasets, READ2016 and IAM are widely recognized as benchmarks for handwritten text recognition, while LAM is currently the largest available line-level handwritten text recognition dataset. The information about the datasets is provided in Table. Note that we report the performance on the test set with the model achieving the best performance on the validation sets.

The Ludovico Antonio Muratori (LAM) dataset is a massive handwritten text recognition dataset of Italian ancient manuscripts, which was edited by a single author over a span of 60 years. It consists of a total of 25,823 lines and has a lexicon of over 23,000 unique words. The dataset is split into 19,830 lines for training, 2,470 lines for validation, and 3,523 lines for testing, with a charset size of 89. The dataset was annotated at the line level, with each line’s bounding box and diplomatic transcription provided. During the transcription process, stroke-out text, illegible words due to stains and scratches, and special symbols not representable in Unicode were replaced with the # symbol. This is currently the largest line-level handwritten text recognition dataset available and could be an ideal choice for demonstrating the potential of our model.

READ2016 was proposed in the ICFHR 2016 competition on handwritten text recognition. It comprises a subset of the Ratsprotokolle collection used in the READ project, with color images representing Early Modern German handwriting. The dataset provides segmentation at the page, paragraph, and line levels. For line-level tasks, the dataset has a total of 8349 training images, 1040 validation images, and 1138 test images, with a character set size of 89.

IAM is a well-known offline handwriting benchmark dataset for modern English. It comprises 1,539 scanned text pages of English texts extracted from the LOB corpus, which were handwritten by 657 different writers. The training set of IAM has 747 documents (6,482 lines), the validation set has 116 documents (976 lines), and the test set has 336 documents (2,915 lines). The IAM dataset consists of grayscale images of English handwriting with a resolution of 300 dpi. In this work, we utilized the line level with the commonly used split, as described in Table.

SECTION: Comparison with state-of-the-art approaches
We use Character Error Rate (CER) and Word Error Rate (WER) as performance measures. CER is calculated as the Levenshtein distance between two strings, which is the sum of character substitutions (), insertions (), and deletions () required to transform one string into the other, divided by the total number of characters in the ground truth (). Formally, CER is given by:

Similarly,is calculated as the sum of word substitutions (), insertions (), and deletions () needed to transform one string into another, divided by the total number of words in the ground truth (). Mathematically, WER is expressed as:

We conducted a comparative study of current state-of-the-art methods on the LAM, READ2016, and IAMdatasets respectively. Our approach surpassed previous state-of-the-art models on the LAMand IAMdatasets and achieved comparable performance on the READ2016dataset. The results presented in Tables,andwere achieved without the utilization of any external language models, such as n-grams or similar techniques. Specifically, on the LAMdataset, our method achieved a CER of 2.8 and a WER of 7.4, outperforming all models tested on this dataset. On the IAMdataset, our approach exceeded the previous state-of-the-art model, VAN, with a CER improvement of 0.3 and a WER improvement of 1.4. On the READ2016dataset, our method reached a CER of 3.9, surpassing the state-of-the-art method VANand DANby 0.2, and closely matching its WER.Furthermore, when compared to all transformer-based methods, our approach consistently led the field, except on the IAM datasetwhere TrOCRachieved a CER of 3.4. However, it is noteworthy that TrOCRuses pre-trained CV and NLP models and a large-scale synthetic dataset, which is not publicly available, to pre-train their model. Transformeralso relies on a large amount of synthetic data for training. Despite this, our method still outperforms it. In addition, we also conduct a fair comparison to two recent works on data-efficient transformers: DeiTand DropKey. We achieve clearly better performance than them on all three datasets. Training details of DeiTand DropKeyare provided in the supplementary material. These results demonstrate the data-efficiency of our proposed model.In summary, our research presents a competitive handwritten text recognition model that stands out against state-of-the-art methods, particularly on the LAMand IAMdatasets, and competes well on the READ2016dataset without resorting to any external language models, pre-training or synthetic data commonly used in the field.For many years, the CNN + BLSTM paradigm has been the dominant approach in handwritten text recognition. However, our proposed method represents a significant shift in this trend, markedly enhancing the performance of transformer-based models. This breakthrough has the potential to steer the entire field of handwritten text recognition toward new and exciting directions.

SECTION: Ablation studies and visualization analysis
In this section, we delve into two core areas of our study: ablation studies and visualization analysis. The ablation studies are comprehensive, examining the impact of key building blocks within our model and exploring the influence of decoder and critical hyperparameters. These include the masking ratio and span length, as well as the number of transformer encoder and decoder layers and attention heads. The visualization analysis grants us deeper insights into the effectiveness of our span mask strategy.
Additionally, we present several qualitative results that showcase the effectiveness of our model.

We hope that our research can serve as a solid basis that can be readily and swiftly used by future researchers. For this reason, we have intentionally refrained from incorporating intricate and opaque components into our model, which could pose difficulties in explanation.

We achieved relatively good results on LAMand READ2016datasets using only the standard ViT encoder. This encouraged us to consider the ViT architecture as a promising approach for handwritten text recognition tasks. However, we observed that training with the ViT encoder alone resulted in unstable performance and slow convergence speed on the IAM dataset, making it difficult to compete with CNN-based models. To improve performance, we introduced a CNN-based feature extractor before the ViT encoder to combine the transformer’s global feature extraction capabilities with the CNN’s ability to extract local features via a strong inductive bias. Our experiments showed that this modification significantly improved the model’s performance and convergence speed.

We found that convergence to a flatter minimum can mitigate overfitting in Handwritten Text Recognition (HTR) models. To facilitate this, we utilized the Sharpness-Aware Minimization (SAM) optimizer, which is straightforward to apply, for locating these flatter minima. Our experimental results show that validation CER and WER on READ2016 increased from 4.8 to 4.5 and from 20.1 to 19.4 with SAMoptimizer, indicating that it has a significant impact on HTR tasks.

When labeled data is limited, overfitting can become problematic for transformer-based models. To address this issue, we proposed a new feature masking strategy to reduce overfitting and improve model performance as described in section. As shown in Table, the span feature masking provides consistent and clear improvement across all the datasets.

We investigate the impact of different transformer encoder layers and attention heads. The results are illustrated in Table. On IAM dataset, taking the number of layers to 4 and attention heads of 6 achieved the best validation CER and WER. To maintain consistency, we employed this set of parameters across all our experiments.
We also investigate the impact of different masking strategies. The results are illustrated in Table.
We can see masking tokens () or span feature masking strategy () improve the performance for most cases.
Span feature masking performs better than random masking tokens and masking none of the tokens.
For hyperparameters, taking the masking ratio of 0.4 and a span length of 8 is optimal, which is used for all our experiments. However, larger span lengths (16) reduce the performances, possibly due to the inability to learn context-related information.

Similar to TrOCR, we employ a standard transformer decoder and utilize beam search to produce the final output. We utilized our optimal encoder as the baseline to systematically investigate the impact of the decoder on the overall model performance. The increased number of parameters from adding a decoder constrained us to use a batch size of 64 to maintain consistency across all ablations. Our experiments with decoders of varying layer counts, as shown in Table, demonstrated that incorporating a transformer decoder did not facilitate better convergence nor prevent overfitting.

In our study, we examine the variations in attention maps when different masking strategies are employed in Figure.
We averaged the attention across all heads to generate the attention maps displayed. The detailed explanations are as follows:Firstly, our image size is fixed at 64 x 512, which, after patch embedding, transforms into a shape of 1 x 128, viewed as 128 tokens represented by 128 vertical stripes in the figure. The tokens selected for visualization correspond to the areas enclosed in red boxes in the original image. In the left image, the letter "o" is highlighted, while in the right image, it is the letter "l". According to the principle of self-attention, our selected token should pay more attention to other tokens with higher similarity, which is represented as lighter colors in the attention map. In both no-mask and random-mask scenarios, we can observe that in the left image, the letter "o" in "nvasion" and "bodies" is highlighted, and in the right image, the two "l" letters in "will" are illuminated. This indicates that under no mask and random mask conditions, attention is mainly focused on the token itself.However, a significant change is observed in the span masking scenario. More areas are noticed, indicating that when using span masking, tokens are able to "attend to a broader range of information". This highlights the effectiveness of span masking in enabling tokens to capture more contextual information. The improved contextual awareness provided by span masking facilitates a more comprehensive understanding of the text, which is vital for accurate recognition in handwritten text recognition tasks.The more examples of the attention maps are provided in the supplementary material.

Few methods mention the total time required to complete their training, yet this is extremely important for this task. Most approaches that rely on pre-training or additional data consume significantly expensive computational resources. We compared our method with CNN-based approaches GFCN, VANand OrigamiNet-24in Table. It is important to highlight that the VAN method did not resize images to a fixed resolution but instead used the original image pixels from datasets such as IAM. Similarly, GFCN mentioned that the experiments for the IAM dataset with an image height of 128px, preserving the original width. This resolution is much larger than the fixed resolution we used, which is 512x64. OrigamiNet also used a fixed resolution of 600x32, and our approach of using a fixed resolution follows OrigamiNet. As shown in Table, our proposed transformer-based method remains competitive in terms of training time.

We provide visual results in Figurefor IAM(First row), READ2016(Second row) and LAM(Third row). From this, one can recognize the task is challenging, as the visual content present in the text line is not very visible and the background is quite noisy. However, our approach can still produce reasonable predictions on these examples. It is worth noting that in the final image, the ground truth label was annotated incorrectly. Despite this error, our proposed model was still able to accurately recognize the correct handwritten text from the original image, which demonstrates the robustness and effectiveness of the proposed approach.

SECTION: Discussion
Although our approach has made notable strides in transformer-based line-level recognition, there is still room for improvement in our current method. The significance of data augmentation for handwriting recognition cannot be overstated, and we have observed that certain data augmentation methods previously utilized in HTR may have adverse effects. Investigating new types of data augmentation specifically tailored for handwriting is a potential direction. Furthermore, delving deeper into mask strategies represents an intriguing avenue for exploration; learnable mask strategies adapted for handwriting could prove more beneficial. Lastly, expanding from line-level to paragraph-level or page-level recognition will be the focus of our future research.

SECTION: Conclusion
In this work, we have presented a simple and data-efficient approach for handwritten text recognition. With minimal modifications to the ViT architecture, we have successfully developed a ViT-like model that surpasses state-of-the-art performance without requiring pre-training or additional data. Notably, our experiments highlight the remarkable data efficiency of our model compared to ViT and DeiT, while preserving its superior generalizability even in scenarios with vast amounts of available data. These findings provide a promising direction for improving the performance of handwritten text recognition, particularly in limited data scale settings.

SECTION: References
This appendix contains the following sections:

: visual results on handwritten text recognition datasets as mentioned in.

: more details about our Convolutional Neural Network(CNN) backbone and ablations on both ResNet and VGG. This experiment was mentioned inof our paper.

: more details about the span mask strategy. This experiment was mentioned inof our paper.

: training details about DeiT and DropKey. This experiment was mentioned inof our paper.

: some visualization results of attention maps as mentioned inof our paper .

SECTION: Visual results on the IAM, READ2016and LAMdatasets.
We show our handwritten text recognition method’s visual results on the IAM, READ2016, and LAMdatasets.

IAMis a well-known offline handwriting benchmark dataset containing 6 482 images for training, 976 images for validation, and 2 915 images for testing. The image is in grayscale and the font has ligatures and some missing parts. Visual results are provided in Figure.

READ2016consists of 8 349 train, 1 040 validation, and 1 138 test images. The image contains a noisy background with some blurry fonts. Visual results are
provided in Figure.

LAMis currently the largest line-level handwritten text recognition dataset that contains 19 830 lines for training, 2 470 lines for validation, and 3 523 lines for testing. The image contains fonts with stains, some lines of text are skewed and include both upper and lower characters. Visual results are provided in Figure.

SECTION: CNN Backbones Ablation
In this study, we investigated the impact of different CNN backbones on the overall model performance. We chose the most fundamental ResNetand VGGarchitectures as our CNN backbones, consistent with the simple and easy-to-implement principles outlined in our paper. The performance of the proposed method is observed to be robust across various backbones. Particularly, ResNet-18 exhibits superior performance compared to other backbones.

SECTION: Span mask strategy
The details of our implementation of the span mask strategy are as follows:
To achieve the designated mask ratio(e.g., 0.4 of), we adopt an iterative process of sampling spans. In each iteration, we start by defining a maximum span length(i.e., the number of interconnected tokens), and then randomly select the starting point for each span. Noting that the maximum span length is fixed. This means that the length of the sampled masked segments remains the same for each iteration.

SECTION: Training details about DeiTand DropKey
We implemented it completely following the steps in DropKey, moving dropout operations ahead of attention matrix calculation and setting the Key as the dropout unit, yielding a dropout-before-softmax scheme. And We set the drop ratio to 0.1. In DeiT, each layer has a dimension of 768 and 6 heads as used in our approach. At the same time, we implemented DeiT with no distillation.

SECTION: Visualization results of attention maps
In Figure, we present an extensive set of attention map visualizations that offer valuable insights into the model’s behavior. We demarcate the region of interest in the original image corresponding to the token under scrutiny using a red bounding box. It is evident that when employing no mask and random mask strategies, the attention is highly localized, illuminating only the regions that correspond to the annotated characters in the original image. For instance, in the first visualization, the selected token corresponds to the letter ’o’ in the word ’of,’ and the attention map distinctly highlights this specific region. This suggests that, in these scenarios, each token is predominantly self-attentive.
Conversely, when utilizing a span mask strategy, there is a conspicuous expansion in the illuminated regions, indicating that the token now engages with a substantially broader contextual landscape.