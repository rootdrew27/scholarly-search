SECTION: Improving LLM Group Fairness on Tabular Data via In-Context Learning

Large language models (LLMs) have been shown to be effective on tabular prediction tasks in the low-data regime, leveraging their internal knowledge and ability to learn from instructions and examples. However, LLMs can fail to generate predictions that satisfy group fairness, that is, produce equitable outcomes across groups. Critically, conventional debiasing approaches for natural language tasks do not directly translate to mitigating group unfairness in tabular settings. In this work, we systematically investigate four empirical approaches to improve group fairness of LLM predictions on tabular datasets, including fair prompt optimization, soft prompt tuning, strategic selection of few-shot examples, and self-refining predictions via chain-of-thought reasoning. Through experiments on four tabular datasets using both open-source and proprietary LLMs, we show the effectiveness of these methods in enhancing demographic parity while maintaining high overall performance. Our analysis provides actionable insights for practitioners in selecting the most suitable approach based on their specific requirements and constraints.

Improving LLM Group Fairness on Tabular Data via In-Context Learning

Valeriia Cherepanova††thanks:Correspondence to: Valeriia Cherepanovacherepv@amazon.comAmazon AWS AIChia-Jung LeeAmazon AWS AINil-Jana AkpinarAmazon AWS AIRiccardo FogliatoAmazon AWS AI

Martin Andres BertranAmazon AWS AIMichael KearnsUniversity of PennsylvaniaAmazon AWS AIJames ZouStanford UniversityAmazon AWS AI

SECTION: 1Introduction

In recent years, the scope of large language models (LLMs) has broadened significantly beyond traditional natural language processing tasks, with recent research demonstrating their effectiveness in tackling challenges on tabular data, including predictive tasks(Hegselmann et al.,2023; Yin et al.,2020). Typically, structured data is converted into textual format and provided to the language model along with a concise task description and key features. Notably, it has been shown that language models are particularly beneficial in scenarios with limited training data, as they can utilize internal knowledge about world from pre-training combined with textual instructions and few-shot examples to make predictionsSlack and Singh (2023).

Although considerable research has been devoted to exploring and addressing issues of stereotypical bias and fairness in language models applied to natural language tasks, tabular datasets present distinct challenges, particularly in group fairness. It is important to differentiate group fairness in the context of tabular data from conventional notions of fairness in NLP tasks: group fairness in tabular problems hinges on class labels and the representation of various demographic groups within these labels, while stereotypical fairness in NLP has primarily focused on bias in model representations. Notably, achieving fairness in the typical NLP sense does not automatically ensure group-fair predictions in tabular tasks due to potential disparities in class distributions.

Recent studies have started exploring how language models handle group fairness when applied to tabular data, revealing noticeable fairness discrepancies among different demographic groups.Liu et al. (2024)andLi et al. (2024)evaluate a few baseline methods for improving group fairness in tabular tasks, including resampled fine-tuning, and few-shot learning with label flipping and find these methods to have limited effectiveness. A recent survey paper(Fang et al.,2024)recognizes the challenge of mitigating inherent biases in large language models through conventional fine-tuning and few-shot learning and highlights the need for more effective strategies to address group unfairness in tabular tasks.

In this work we examine four approaches for empirically improving demographic parity of LLMs when applied to making predictions on tabular data. These approaches include in-context methods such as prompt optimization, soft prompt tuning, few-shot in-context learning, and self-refining predictions to promote fairness. We empirically evaluate these methods using both open-source and proprietary models across four tabular datasets, demonstrating their effectiveness. Based on our analysis, we provide actionable recommendations to practitioners on the most suitable method for different scenarios, and discuss how these approaches may be adapted to other notions of fairness.

SECTION: 2Related Work

SECTION: 2.1Large Language Models on Tabular Data

A growing body of work has applied deep learning algorithms to tabular data(Yin et al.,2020; Herzig et al.,2020; Huang et al.,2020; Levin et al.,2022; Zhu et al.,2023). Relevant to our setting, some of these studies have employed LLMs to analyze tabular data that is serialized into formatted text. They show that descriptive feature names, well-defined instructions, in-context examples, and chain-of-thought reasoning enhances LLM performance(Zhao et al.,2023; Marvin et al.,2023; Chen,2022). Some specifically focus on classification tasks(Hegselmann et al.,2023; Wang et al.,2024; Liu et al.,2022; Fang et al.,2024; Jaitly et al.,2023), which is also the focus of our work. The prior knowledge of LLMs allows them to perform better than traditional algorithms such as XGBoost in low-data regimes(Slack and Singh,2023; Hegselmann et al.,2023). However, LLM predictions can reflect inherent biases, affecting the fairness of their outcomes(Hu and Du,; Liu et al.,2023).Liu et al. (2023)’s work is closely related to ours: they analyze the accuracy and fairness of LLM predictions, concluding that traditional ML models exhibit fewer disparities. Although in-context learning and finetuning do not fully close the fairness gap, label-flipping in in-context examples significantly reduces biases, albeit at the cost of prediction performance. Our work contributes to this literature by introducing four in-context learning approaches for mitigating the demographic parity gap in tabular data predictions, demonstrating their effectiveness across widely-used fairness datasets.

SECTION: 2.2Bias and Stereotypes in LLMs

Despite their promising capabilities, language models also exhibit biases and stereotypes(Bolukbasi et al.,2016; Bender et al.,2021; Chu et al.,2024). These biases mostly originate from the training data, which often contain historical and societal prejudices embedded within the text. Biases have been reported with respect to several demographic groups, e.g., gender, race, ethnicity, and socioeconomic status(Wan et al.,2023; Haim et al.,2024; Santurkar et al.,2023). With the use of these models becoming more widespread, these biases have the risk to substantially reinforce harmful stereotypes and perpetuate existing inequalities, especially when deployed in high-stakes settings(Zou and Schiebinger,2018). Addressing these biases is essential, and several mitigation strategies have been proposed for this purpose, including data augmentation, prompt tuning and few-shot learning(Zhao et al.,2017; Wang et al.,2021; Sun et al.,2019; Zmigrod et al.,2019; Mattern et al.,2022; Fatemi et al.,2021; Aguirre et al.,2023). However, effectively applying these strategies to the large-scale pretraining corpora remains challenging.
Finally, biases can be hard to detect and several datasets and methods have been proposed to help identify them(Caliskan et al.,2017; May et al.,2019; Webster et al.,2020; Kurita et al.,2019; Nangia et al.,2020; Blodgett et al.,2021).

SECTION: 2.3Fairness on Tabular Data

Much of the work on classification and algorithmic fairness has focused on tabular datasets(Mehrabi et al.,2021; Caton and Haas,2024; Pessach and Shmueli,2023; Fabris et al.,2022; Barocas et al.,2023; Chouldechova and Roth,2018; Fogliato et al.,2020). Consequently, there is a wide range of research describing the properties and trade-offs of predictive algorithms on this type of data(Dutta et al.,2020; Black et al.,2022; Akpinar et al.,2022). Multiple works have proposed fairness-enhancing techniques for traditional ML algorithms (e.g., logistic regression), which generally work by debiasing the data, including a fairness constraint in the optimization problem, or post-processing model predictions(Zafar et al.,2017; Dwork et al.,2012; Berk et al.,2017; Lum and Johndrow,2016; Hardt et al.,2016; Martinez et al.,2020; Akpinar et al.,2024). Our work employs related techniques, although some of them are not directly applicable. The formalization of fairness definitions has also been extensively discussed(Castelnovo et al.,2022). Fairness metrics evaluated on tabular data typically measure the equality of some target measure across demographic groups, such as accuracy or recall(Chouldechova,2017), which fall under the umbrella of group fairness definitions (as opposed to individual fairness definitions). One such widely-adopted measure, which we also employ in this work, is demographic parity, which ensures that the frequency of positive predictions is approximately equal across different demographic groups.

SECTION: 3Experimental Details

In our experiments we focus on scenarios with little to no training data available. This is a particularly attractive setting for using language models since they typically outperform classical tabular models in low-data regimes as they can leverage their inherent knowledge for predictions(Hegselmann et al.,2023; Slack and Singh,2023). To make prediction on a single sample, we prompt the model with task-specific instructions, along with the relevant features of the sample of interest; see AppendixAfor more details on prompting templates. Optionally, we may also include fairness-specific instructions and few-shot examples, depending on the method used to improve fairness. We then extract the answer either by directly generating a response to the question (for closed-source models) or by calculating the likelihood of tokens corresponding to labels.

In experiments that involve selecting the “best prompt” from several iterations, such as in prompt engineering, we utilize a small validation set of 50 labeled examples to assess model accuracy. We then select (empirically) Pareto-optimal prompts, which represent those where any improvement in either accuracy or fairness would necessitate a compromise in the other metric. Due to its limited size, the validation set is used solely to assess accuracy, while demographic parity is directly evaluated on the test set to identify Pareto-optimal prompts.
We additionally compare our methods against a tabular model, specifically, CatBoost implementation of gradient boosted decision trees trained on 50 examples(Prokhorenkova et al.,2018)and fairness constraint enforced by GridSearch111Fairlearn’s implementation is used.function following the reductions approach byAgarwal et al. (2018).

SECTION: 3.1Language Models

We conduct experiments using a variety of widely used language models that vary in size. Due to the computational demands of some methods, we conduct computationally intensive experiments with smaller models and reserve methods that require advanced reasoning for larger language models. Our experiments include Llama 3 8B and 70B(Touvron et al.,2023), Mistral 7BJiang et al. (2023), Mixtral 8x7BJiang et al. (2024)and Claude Sonnet models(Anthropic,2024).

SECTION: 3.2Datasets

We explore group fairness of LLMs on a set of publicly available datasets widely used in the algorithmic fairness literature. For each of the datasets, we focus on‘gender’as the protected attribute.We briefly introduce the datasets here and point to AppendixCfor additional details.

The Adult Income dataset(Barry Becker,1996)includes 1994 US Census information to predict whether individuals’ yearly income exceeds $50k (1 = yes, 0 = no). In accordance with previous work(Liu et al.,2024; Slack and Singh,2023), we retain 10 features for prediction, sample 1000 examples for evaluation and use the remainder of the data for generating task-specific instructions.

The German credit dataset(Hofmann,1994)is used to predict credit default risk (1 = good, 0 = bad) based on individual attributes. Following previous workLiu et al. (2024), we retain 9 features and split the data set into 50% for evaluation and 50% for task instruction generation.

The American Community Survey (ACS) data(Ding et al.,2021)is sourced from the US Census. For our experiments, we utilize the income (1 = yearly income >$50k, 0 = else) and coverage (1 = public health coverage, 0 = else) prediction tasks for 2018 data from the state of New York. For each classification task, we sample 1,000 examples for evaluation, and use the remaining data to select 10 features with the highest importance.

SECTION: 3.3Serialization and prompts

LLMs require textual input, unlike traditional tabular prediction models. In line with previous workSlack and Singh (2023); Hegselmann et al. (2023), we serialize data points by (1) mapping categorical values to the respective strings (e.g.gender = 1is mapped togender = male), and (2) consolidating column names and entries into one string per row.

Although we assume little to no training data, it is reasonable to expect that practitioners will provide task-specific instructions to the model to facilitate accurate predictions. For this, we construct instructions using prototype clustering on the training folds of the datasets, as suggested bySlack and Singh (2023).
To make instructions more readable, we use GPT-4 to revise prototype information into a single summary paragraph. Please, see AppendixAfor details on prompt templates.

SECTION: 3.4Metrics

In this work we focus on optimizingdemographic parity(DP) which aims to equalize positive label selection rate across groups, i.e.

for a binary predictorand. Constraint violation is reported as ratio between the smallest and largest group level selection rateswith values closer toindicating better parity.
We use DP primarily because it allows to measure fairness on an unlabeled test set directly and does not require labeled training data. Although our primary focus is on demographic parity, the methods we propose can be adapted to other fairness metrics when labeled training data is available as discussed in section7. Additionally, while our main objective is demographic parity, we also evaluateequalized oddswhich aims to balance false positive and false negative rates across groups, i.e.

for a binary predictor,, and, and report equalized odds ratio between groups.

SECTION: 4Methods

In this work we consider four empirical approaches for improving group fairness of language model predictions on tabular datasets as illustrated in Figure1. This section provides a detailed overview of each method, with subsequent section delving into experimental results for each approach.

Prompt engineering continues to play an important role in tailoring the capabilities of LLMs to various tasks(Chen et al.,2023; Wang et al.,2023; White et al.,2023). Recently,Tamkin et al. (2023)demonstrated that integrating fairness-specific manually-curated instructions in the prompt, such as “it is illegal to discriminate”, can attenuate counterfactual biases in model predictions. Additionally, several works have shown that LLMs can act as prompt engineers producing performant prompts for downstream models(Zhou et al.,2022; Yang et al.,2023). In our work we demonstrate the effectiveness of prompt engineering in achieving group fairness in LLMs and show how prompt optimization can be automated. In particular, we propose to optimize a fairness-specific prompt (highlighted in blue on the left panel on Figure1), appended to the task-specific instructions. We adopt the prompt optimization approach followingYang et al. (2023)and employ meta-LLM to iteratively refine fairness instructions for the downstream model based on feedback provided from the previous iterations. In particular, we demonstrate the most fair previous instruction and two randomly selected previous instructions, along with their demographic parity scores and selection rates across groups, see example in Figure2. We refine fairness instructions using the meta-model over 100 iterations. For the meta-LLM we employ the same language model as the one used downstream to make predictions.

In addition to hard prompt optimization, we explore soft prompt tuning, which optimizes the prompt directly in the embedding space instead of discrete token space, see the second-from-left panel in Figure1.
In traditional tabular methods, standard in-processing fairness interventions often involve training machine learning models with a fairness penalty. This encourages the model to equalize selection rates or, depending on the penalty, the error rates across demographic groups(Zafar et al.,2017; Hardt et al.,2016). Drawing inspiration from these techniques and parameter-efficient fine-tuning methods, we propose a similar approach that can be applied to improving group fairness in language models. In particular, rather than optimizing fair prompts in the discrete space of tokens, as done in the previous section, we suggest optimizing a soft prompt by fine-tuning tokens in the embedding space. Continuous optimization in the embedding space allows us to incorporate the fairness penalty into objective directly. Specifically, we fine-tune 50 tokens initialized with task-specific instructions in the embedding space for 20 epochs. This approach applies a penalty designed to equalize the likelihoods of tokens corresponding to positive labels across groups within a batch:

To tune the prompt we use 1000 samples with pseudo-labels obtained by the same language model in the zero-shot setup, simulating a scenario without labeled data. To preserve accuracy and ensure predictions remain close to the original model outputs, we include the standard cross-entropy loss for the pseudo-label predictions.

Prior work(Liu et al.,2024;Hu and Du,; Li et al.,2024)has leveraged the in-context learning capabilities of language models for this problem space. They hypothesize that, when selected appropriately, few-shot examples can effectively influence the final predictions to more accurately reflect the desired notion of fairness. For instance, it has been demonstrated that flipping the labels of few-shot examples can effectively reduce bias, albeit at the expense of significantly lower classification performance(Liu et al.,2024), while class- and group- balanced selection does not mitigate the bias(Li et al.,2024).

With a similar goal, we propose a strategy for constructing fair few-shot examples, which differs from the previous methods in three ways. First, instead of randomly sampling examples from the training data, we apply the nearest neighbor search to select examples that are most similar to a current test instance in the feature space222To compute similarity scores between instances, we use the Jaccard metric as most features are discrete or categorical.. Also, we always select examples that share sensitive attribute with the test instance. Secondly, as we assume no access to training data labels, we use the language models’ default zero-shot predictions as pseudo labels to construct demonstrations (similarly to soft prompt tuning experiments). Finally, we extensively manipulate the distributions of positive and negative in-context examples between groups. In particular, we test varying ratios of positive examples for female test samples,, and for male test samples, resulting in 36 ratio pairs. We hypothesise that increasing the number of positive examples for the minority group increases their selection rate, thereby promoting better parity with the majority group.

In addition to in-processing methods, fairness literature also includes a wide array of post-processing techniques(Hardt et al.,2016). These methods work by altering model outputs directly. We propose an LLM-based post-processing method that leverages the reasoning capabilities of language models, along with a chain-of-thought process, to refine their own predictions. The self-refinement approach involves using language models to identify individuals from both minority and majority groups who are near the “decision boundary”, and then flipping their labels to achieve the desired demographic parity ratio. Therefore, the prediction process includes two stages. First, the model makes initial predictions on a batch of data samples. After that, the model assesses demographic parity in a batch and adjusts predictions to attain the desired parity, if necessary. An example prompt used to refine predictions is illustrated in Figure1most right panel. Given that self-refinement approach relies on the advanced reasoning capabilities of language models to analyze predictions, compute metrics of interest, and adjust individual predictions, we conduct these experiments with larger models, specifically Llama3 70B and Claude Sonnet models.

SECTION: 5Experimental Results

In this section we present performance of the proposed methods. Since certain fairness metrics are not necessarily aligned with accuracy, models producing fairer decisions may suffer from accuracy degradation. Therefore, it is important to identify methods resulting in an optimal fairness-accuracy trade-off.

SECTION: 5.1Fair Prompt Optimization

To evaluate the effectiveness of prompt engineering for fairness, we, first, manually construct a few prompts which are supposed to improve group fairness, such as(1) “Ignore sex completely when making predictions”, (2) “Assign positive label to females and males at the same rate.”, (3) “Assign positive label to females slightly more often than to males.”, with the last prompt coming from the fact that female group is the one disadvantaged. We include the accuracy and demographic parity for models queried with these prompts for Adult dataset in Figure3. We observe that, while these prompts can improve demographic parity in some models, finding a universal “fair instruction” that upholds group fairness consistently across multiple models is challenging.

Next, we experiment with a prompt optimization framework using a meta-LLM, designed to dynamically refine fair instructions based on demonstrated prior instruction candidates with their demographic parity scores and group selection rates. We present the performance of Pareto-optimal fair prompts for the Llama 8B model and the CatBoost model baseline in Figure4. More plots for Mistral and Mixtral models are included in AppendixE. Additionally, Table1lists results for the fair prompts which are Pareto-optimal and achieve at least 0.9 demographic parity ratio. We observe, that including these engineered fair prompts significantly improve fairness of the models, often without sacrificing much accuracy, or even improving it. In AppendixEwe provide the optimized prompts achieving the best and the worst demographic parity.

SECTION: 5.2Soft Prompt Tuning

Soft prompt tuning enables continuous optimization of a fairness objective by incorporating it directly into the loss function. We tune the soft prompts with a demographic parity fairness regularizer, which aims to equalize the likelihood of positive label predictions across different groups within a batch333We use the Prompt Tuning implementation byHugging Face..

Similarly to our fair prompt engineering experiments, we identify Pareto-optimal points among fine-tuning epochs using the validation set and include results for Pareto-optimal soft prompts achieving at least 0.9 test demographic parity in Table1. We observe that while tuning soft prompts improves demographic parity across all datasets, it results in suboptimal trade-off with accuracy compared to hard prompt optimization approach. This could potentially be attributed to the sensitivity of the tuning procedure to hyperparameters or the reliance on pseudo-labels. Additionally, we include plots illustrating fairness-accuracy tradeoff for Pareto-optimal soft prompts in AppendixE.

SECTION: 5.3Fair Few-shot Examples

In this section we present results for our fair few-shot example construction strategy, which selects nearest-neighbors to test instances, uses zero-shot model predictions as pseudo-labels, and adjusts the ratio of positive examples between groups to enhance fairness. Figure5illustrates the impact of varying the ratio of positive examples in the prompt. The x-axis represents the ratio of positive examples for female test instances, while the color indicates the ratio of positive examples used for predictions on male samples.
The results are averaged across 3 random seeds, with the band indicating the standard deviation across seeds.
We observe that increasing the positive ratio for females significantly improves demographic parity, to the extent that the selection rate for females surpasses that for males. Additional figures for other models and datasets are displayed in AppendixE. These results confirm that adjusting the ratio of positive examples in-context is an effective method for manipulating the prevalence of positive class predictions, and employing different ratios across protected groups can effectively reduce disparities in selection rates.

Additionally, we compare our nearest-neighbor selection strategy with a baseline selecting examples randomly while preserving similar label ratios in-context. Figure6in Appendix shows that including random in-context examples results in lower demographic parity with larger variance. Also, unlike the nearest-neighbor approach, there is no apparent trend showing that including more positive samples boosts the selection rate for any demographic group, highlighting the importance to including only relevant examples in-context. Finally, in Table1we report demographic parity ratio, equalized odds ratio and accuracy metrics for the Pareto-optimal combination of positive label ratios, which achieves the best validation accuracy and at least 0.9 demographic parity.

SECTION: 5.4Self-Refinement

When making predictions in batches, we can utilize the chain-of-thought and self-refinement capabilities of language models to apply post-hoc corrections to predictions, see the right panel in Figure1for an illustration. We make predictions on a batch of 40 samples, and instruct the model to make adjustments only when the difference in positive rates across groups exceeds 15%.
We report the results of the self-refinement approach in Table2. For all models, refined predictions achieve improved demographic parity across all datasets except for ACS coverage, although this sometimes leads to a notable trade-off in accuracy. In addition, there is no guarantee for similar individuals to receive similar predictions with this method because of the ‘correction step’ which is at odds with notions of individual fairnessDwork et al. (2012).

SECTION: 6Conclusion

We systematically explore four empirical methods to improve group fairness of language model predictions on tabular datasets, and discuss the key takeaways for each method below.Fair Prompt Optimizationcan improve not only fairness but also classification performance, contingent upon the model’s "creativity." This method involves an optimization process that requires evaluating the prompt on a dataset for a number of iterations. Although the resulting instructions are interpretable, the reasons why specific instructions yield fairer results are not always clear.Soft prompt tuningis computationally expensive and sensitive to the choice of hyperparameters. While this method does not yield interpretable instructions, it enables the integration of common fairness regularizers in a differentiable way and may be particularly effective for smaller models.Fair Few Shot Examplesis the most interpretable and predictable method, yielding optimal results across models and datasets when an optimal combination of positive examples ratios is selected. However, it uses a longer context window and may be more computationally expensive for larger datasets because of the number of forward passes needed.Self-refinementrequires a model with strong reasoning capabilities and does not guarantee similar predictions for similar individuals. However, this method offers a computational advantage for larger models, as predictions are made and adjusted in batches, reducing overall processing time.

We recommend fair few-shot examples and fair prompt optimization as universal approaches achieving the optimal accuracy tradeoff. Soft prompt tuning can potentially adapt smaller models, while self-refinement is useful for scenarios with limited budgets and larger language models.

SECTION: 7Limitations and Potential Risks

Our work has several limitations. Firstly, it exclusively examines in-context approaches and does not address data pre-processing for bias mitigation or post-hoc methods that modify model outputs directly(Hardt et al.,2016). Additionally, we do not consider model training and fine-tuning strategies other than soft prompt tuning. Finally, we focus on a single notion of fairness, that is demographic parity, since it can be applied in little to no training data regime, the most practical scenario for language models on tabular datasets. However, most of the discussed methods can be adapted to optimize for other fairness notions, such as equalized odds, when labeled training data is available. For example, the prompt optimization procedure can incorporate alternative fairness metrics in the feedback component of the meta-prompt. Soft prompt tuning can adopt differentiable proxy regularizers to enforce desired fairness criteria, and the few-shot examples approach can demonstrate more examples with ground-truth labels to underrepresented groups.

While the methods explored in this work show promise for improving demographic parity of large language models on tabular prediction tasks, there are also several potential risks to consider. Enhancing group fairness may come at the expense of overall predictive performance, and focusing solely on demographic parity could neglect other important fairness criteria. Moreover, optimizing for group fairness along one dimension could inadvertently amplify or introduce other biases not directly measured. As such, practitioners should carefully evaluate the appropriateness and potential pitfalls of these methods for their specific use case before deploying language models for high-stakes decision making on tabular data.

SECTION: References

SECTION: Appendix APrompt Details

In this section we provide the prompt templates used in the default setup to obtain predictions from the models. We highlight task instructions in red, and sample for which the prediction is made in blue. When using an optimized fair prompt, if follows the task instructions. For the fair few-shot examples experiments, we provide selected examples before the test sample in the prompt.

SECTION: A.1Prompt Template for Adult dataset

User:You must follow the instructions to predict if income of an individual exceeds $50K/yr.Generally, individuals who earn more than $50K/yr tend to have higher levels of education (e.g., Bachelors or Prof-school), work in managerial or specialty occupations, and are married with a spouse. They also usually work longer hours per week and have higher capital gains. On the other hand, those earning less than or equal to $50K/yr typically have lower levels of education (e.g., HS-grad), work in clerical or craft repair occupations, and may be single or married without a spouse. They also usually work fewer hours per week and have lower capital gains.You must provide an explanation and include answer enclosed in square brackets at the beginning of your response.workclass: Private; hours-per-week: 40; sex: Male; age: 38; occupation: Craft-repair; capital-loss: 0; education: HS-grad; capital-gain: 0; marital-status: Divorced; relationship: Not-in-family.You must predict if the individual’s income exceeds $50K/yr. Answer with one of the following: [less than or equal to 50K] or [greater than 50K].Assistant:Based on the information provided the answer is

SECTION: A.2Prompt Template for German Credit dataset

User:You must follow the instructions to predict the credit risk of a German bank customer based on their profile data.Higher credit amounts and longer durations often indicate a ’bad’ classification, particularly for loans intended for new cars or high-value purchases. A negative or low checking status (either ’<0’ or ’0<=X<200’) is also associated with higher risk. Conversely, ’good’ classifications tend to involve moderate or lower credit amounts, shorter loan durations, and are frequently for consumer goods like radios, TVs, or used cars. Surprisingly, having ’no checking’ status tends to lean towards a ’good’ classification. More people tend to have good credit risk.You must provide an explanation and include answer enclosed in square brackets at the beginning of your response.age: 36.0; sex: male; job: skilled; housing: own; savings status: no known savings; checking status: 0<=X<200; credit amount: 2181.0; duration: 30.0; purpose: new car.You must predict the credit risk of a German bank customer. Answer with one of the following: [good] or [bad].Assistant:Based on the information provided the answer is

SECTION: A.3Prompt Template for ACS Income dataset

User:You must follow the instructions to predict if income of an individual exceeds $50K/yr.Individuals who are most likely have income less than or equal to 50K are typically younger, unmarried, with lower levels of education (high school diploma or some college), and engaged in lower-skilled occupations such as cashiers, janitors, or administrative assistants. Conversely, individuals who are older, married, with higher levels of education (bachelor’s or master’s degree), and are employed in professions such as teachers, managers, physicians, or registered nurses are more likely to have income greater than 50K.You must provide an explanation and include answer enclosed in square brackets at the beginning of your response.Sex: Female; Race: White alone; Educational attainment: Bachelor’s degree; Relationship: Roomer or boarder; Age: 35.0; Usual hours worked per week past 12 months: 32.0; Marital status: Never married; Place of birth (Recode): Florida/FL; Class of worker: Employee of a private not-for-profit, tax-exempt, or charitable organization; Occupation: EDU-Elementary And Middle School Teachers.You must predict if the individual’s income exceeds $50K/yr. Answer with one of the following: [less than or equal to 50K] or [greater than 50K].Assistant:Based on the information provided the answer is

SECTION: A.4Prompt Template for ACS Coverage dataset

User:You must follow the instructions to predict whether an individual is covered by public health insurance.Individuals covered by public health insurance tend to have a regular high school diploma, have never served in the military, and generally have lower income. In contrast, features such as being employed, having educational attainment, higher income (above $20,000) and being married correlate with not being covered by public health insurance. In addition, people with disabilities are more likely to be covered by public health insurance.You must provide an explanation and include answer enclosed in square brackets at the beginning of your response.Sex: Female; Race: White alone; Educational attainment: Associate’s degree; Military service: Never served in the military; Disability recode: Without a disability; Total person’s income: 0.0; Marital status: Never married; Employment status recode: Not in Labor Force; Employment status of parents: N/A (not own child of householder, and not child in subfamily); Gave birth to child within the past 12 months: No.You must predict if the individual is covered by public health insurance. Answer with one of the following: [covered] or [not covered].Assistant:Based on the information provided the answer is

SECTION: Appendix BAdditional Experimental Details and Hyperparameters

. In the soft prompt tuning experiments, we fine-tune 50 tokens initialized with the task instructions for 20 epochs. We employ a learning rate offor Llama 8B models andfor Mistral models, allowing the first three epochs for a warm-up with a linear scheduler. During fine-tuning, we use 1000 train samples with pseudo-labels obtained by using the language model in a zero-shot setup, we apply demographic parity regularization with a penalty weight of. We employ a class-balanced sampler and set the batch size to 60 samples for Mistral and 50 samples for Llama models, which were the largest sizes we could use given the computational constraints.

SECTION: Appendix CDatasets

We include details on the datasets and features used in our experiments in the Table3.

SECTION: Appendix DHardware

We conducted all experiments using 8 Tesla V100 32GB GPUs through AWS. The soft-prompt tuning experiments required approximately 120 GPU hours per model per dataset, resulting in 950 GPU hours in total. The prompt optimization experiments consumed around 35 GPU hours per model per dataset, resulting in 420 GPU hours for three models and four datasets. Fair few-shot examples experiments took approximately 60 GPU hours per model per dataset for one seed, resulting in 2160 GPU hours of experiments.

SECTION: Appendix EAdditional Results

SECTION: E.1Additional Results for Fair Prompt Optimization

In Tables4,5we include optimized fair prompts for each dataset each model. In particular, we include Pareto-optimal prompts, which achieve the highest and the lowest demographic parity ratio.

SECTION: E.2Additional Results for Fair Few-Shot Examples

In Figures7and8, we demonstrate accuracy and demographic parity metrics for the prompts containing different proportions of positive and negative few-shot examples across demographic groups. We observe that for all datasets and models, increasing the proportion of positive examples for a demographic group results in a higher selection rate in that group. Additionally, Figure6illustrates the trend in demographic parity for prompts including random examples instead of nearest neighbors. In contrast to our strategy, including random examples does not significantly influence the models’ selection rates.

SECTION: E.3Comparing Pareto Frontiers

Figure9illustrates the Pareto frontiers for prompt optimization, soft prompt tuning, and fair few-shot examples methods. Specifically, it plots the Pareto-optimal prompts for each method, demonstrating the trade-offs between accuracy and fairness metrics for each method.