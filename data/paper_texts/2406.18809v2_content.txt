SECTION: Divide, Ensemble and Conquer: The Last Mile on Unsupervised Domain Adaptation for Semantic Segmentation

The last mile of unsupervised domain adaptation (UDA) for semantic segmentation is the challenge of solving the syn-to-real domain gap. Recent UDA methods have progressed significantly, yet they often rely on strategies customized for synthetic single-source datasets (e.g., GTA5), which limits their generalisation to multi-source datasets. Conversely, synthetic multi-source datasets hold promise for advancing the last mile of UDA but remain underutilized in current research. Thus, we propose DEC, a flexible UDA framework for multi-source datasets. Following a divide-and-conquer strategy, DEC simplifies the task by categorizing semantic classes, training models for each category, and fusing their outputs by an ensemble model trained exclusively on synthetic datasets to obtain the final segmentation mask. DEC can integrate with existing UDA methods, achieving state-of-the-art performance on Cityscapes, BDD100K, and Mapillary Vistas, significantly narrowing the syn-to-real domain gap.

SECTION: IIntroduction

Semantic segmentation[1]is a key task in autonomous driving[2,3]as it provides a detailed understanding of a vehicle’s surroundings by assigning a specific class to each pixel in an image. This capability enables the perception module of an autonomous vehicle to identify and distinguish objects such as pedestrians, vehicles, traffic signs, and obstacles to allow safe driving. Training deep learning models for this task requires extensive datasets with precise labels. However, pixel-level human labelling for semantic segmentation is complex and time-consuming to obtain. An approach for addressing this challenge lies in adopting synthetic datasets, wherein automated procedures generate labels, obviating the necessity for manual annotations. Nevertheless, a notable issue arises due to the domain gap – a feature distribution disparity between synthetic datasets’ images and real-world scenes. This discrepancy frequently leads to a decline in performance when models trained on synthetic datasets (source domain) are applied to real-world datasets (target domain). A common practice is to employ domain adaptation techniques[4]such as Semi-Supervised Learning (SSL)[5]and Unsupervised Domain Adaptation (UDA)[6,7]to bridge the syn-to-real gap. In this paper, we focus on UDA, where the challenge posed by real-world target domains is addressed solely by leveraging synthetic data as the source domain. More specifically, we aim to bridge the last mile, the gap between UDA and supervised learning (SL) with human labelling.

The properties of synthetic datasets, such as label accuracy, sample diversity and class balance, significantly impact UDA performance. Most UDA methods[8,9,10,11,12]focus only on popular single-source synthetic domains, such as GTA5[13]or SYNTHIA[14]datasets; and only targets one single real-world domain, generally Cityscapes[15]. Thus, novel strategies such as Rare Class Sampling (RCS) and Thing-class ImageNet Feature Distance (FD)[9]are designed to address the training challenges associated with these specific source domains that suffer from class imbalance, lack of variability and domain shift. However, these strategies add complexity, instability and overfitting. Alternatively, some works[16,17,18,19]have shown a multi-source approach can successfully replace the necessity of the aforementioned single-domain strategies. In effect, combining several synthetic datasets as one for training solves the class imbalance and increases the variability, improving the generalization capacity of the models. The results from multi-source methods demonstrate a notable reduction in the gap between synthetic and real-world domains. Nevertheless, the multi-source domain has not been adopted extensively since most UDA methods are usually built upon prior research of a single-source domain.

In this paper, we propose a novel multi-source framework called Divide, Ensemble and Conquer (DEC). It follows a divide-and-conquer strategy to train semantic segmentation models and fuse their predictions to generate the semantic segmentation mask. In our first step, we apply the division strategy to train several models on categorized semantic classes, called category models, similarly to how humans semantically label an RGB image using AI tools[20,21](first background, then large objects, and finally small ones). The categorization simplifies the number of classes learnt by each category model, expecting a better performance and fast convergence. The next step involves the training of an ensemble model, a DeepLabv3+[22]network, which learns to fuse segmentation masks generated by each category model into one mask. The framework is illustrated in Fig.1. DEC exhibits compatibility with a wide range of UDA methods, providing consistent improvement. To ensure a fair comparison, we reproduce the results of previous UDA methods with the multi-source dataset. DEC achieves state-of-the-art performance on Cityscapes, BDD100K[23], and Mapillary Vistas[24], resulting in mIoU of 78.7, 62.4, and 74.8, respectively. We enhance the existing state-of-the-art reported in[11]by 1.2, 2.1, and 0.6 points, respectively. Our performance is only 2.8 and 4.7 points behind SL for Cityscapes and Mapillary Vistas, surpassing the SL for BDD by 0.9 points. Overall, the main contributions of our work are as follows:

A novel division approach: DEC categorizes semantic classes into groups (e.g., background, large objects, and small objects) and trains category models. This approach simplifies the feature space for each category model, leading to improved UDA performance.

An ensemble-based fusion mechanism: DEC leverages an ensemble model to fuse the outputs from category models into the final segmentation mask. This fusion approach ensures robustness and compatibility with various UDA methods, providing consistent improvements across datasets.

State-of-the-art performance on multiple benchmarks: DEC achieves competitive results on Cityscapes, BDD100K, and Mapillary Vistas, surpassing existing UDA methods and narrowing the gap with SL.

Flexibility and ease of use: Besides the label division strategy, DEC does not rely on specialized strategy or additional training parameters. It can be easily integrated with various UDA methods without changing the baseline model architecture.

SECTION: IIRelated work

SECTION: II-AEnsemble Learning

Ensemble learning is a machine learning approach in which multiple models are trained to solve the same problem and combined to improve performance. One of the first works on CNNs for semantic segmentation using ensembles is Marmanis et al.[25]modifying a FCN network to improve the deconvolution step and then train several networks with different initialisation and average their predictions. Much of the subsequent work focused on changing the structure of the model to apply ensemble learning. Bousselham et al.[26]propose a self-ensemble approach using the multi-scale features produced by a spatial pyramid network to feed different decoders and compose an ensemble by different strategies (averaging, majority vote, and hierarchical attention). Similarly, Cao et al.[27]use multiple semantic heads sharing the same backbone to compute an ensemble using cooperative learning.
These approaches further push the performance but increase complexity and lose flexibility. Every model in these works is customised for their algorithms and can not be combined with others easily. Compared with these previous works,our proposalis purely data-driven and only requires adapting the labels. This characteristic makes DEC flexible, enabling it to be used with any semantic segmentation model without additional modifications.

In addition, ensemble learning has been applied to solve UDA tasks. Extensive works are addressed on image classification[28,29,30,31]. However, our interest falls in syn-to-real UDA for semantic segmentation where available works are scarcer. Piva et al.[32]propose a framework similar to[33]where an image translation module feeds three different semantic segmentation networks, and an ensemble layer aggregates the information to generate pseudo-labels. Chao et al.[34]propose an end-to-end ensemble-distillation framework that employs different UDA methods and semantic segmentation networks to generate the final pseudo-labels by a pixel and channel-wise fusion policy. The aforementioned works employ various data augmentation techniques, UDA methods, and semantic segmentation networks on each ensemble member. We draw inspiration from these works, emphasising that members’ diversity enhances ensemble robustness.Our proposaldifferentiates itself by assigning different classes to ensemble members (divide-and-conquer strategy). Previous studies[34,26]use majority voting and averaging to ensemble outputs from members, requiring members to be trained with the same classes. To handle category-specific models that output different classes, we propose an ensemble model (CNN) that learns to combine these outputs.

Ensemble learning enhances the generalization performance of deep learning models by combining the predictions of multiple models, reducing variance and mitigating overfitting. This approach enables us to decompose the semantic segmentation task into smaller, more manageable subtasks, each focused on a specific set of semantic categories. By training specialized models for each category, we can leverage the strengths of individual models (category experts), resulting in more robust and accurate segmentation result.

SECTION: II-BUDA for Semantic Segmentation

Most UDA works relies on single-source synthetic dataset[35,36,37,38,39,40,41,42,43,44,45,9,10,11,46,47,48,34]. Hoyer et al.[9]successfully use the novel vision-transformers[49]to solve UDA for semantic segmentation. They employ a teacher-student self-training framework with additional training strategies, such as computing ImageNet feature distances. Furthermore, several new works propose techniques to improve performance on top of existing frameworks (DACS, Daformer, etc.)
Hoyer et al.[10,11]adopt a multi-resolution image cropping approach to capture context dependencies and propose a masked image consistency module to learn spatial context relations of the target domain. Chen et al.[12]explore the pixel-to-pixel and patch-to-patch relation for regularizing the segmentation feature space. These works make significant progress for GTA5Cityscapes but always need to design specific strategies (e.g., RCS and FD) for class unbalance due to the drawback of GTA5.
These specific strategies do not generalize effectively to multi-source datasets (see Section5).
On the other hand,[9,10,11,12]add extra pipelines for training on top of previous works, which makes the UDA become more and more complex and hard to train, e.g.,[11]is on the top of DACS[8], Daformer[9]and HRDA[10]. Our DEC method is entirely data-driven and does not require an extra algorithm for UDA training.

Advancements in synthetic image photo-realism have led to the proposal of higher quality datasets than GTA5, e.g., Synscapes[50]and UrbanSyn[17].
UrbanSyn is a novel synthetic dataset that proves effective for UDA.
Thus, there are a few works on UDA proposing a multi-source approach and tested in different real target datasets[51,52,53,16,17]. One of the most promising works with this setup, Gomez[16], proposes an offline co-training method that is purely data-driven and combines two synthetic sources. The multi-source datasets are superior to single-source dataset in terms of scene richness, detailed description, and class balance. These works motivateour proposalto focus on multi-source datasets, in contrast to other UDA proposals.

SECTION: IIIMethod

This section details the proposed framework, DEC, grounded in the divide-and-conquer strategy. The framework comprises two components: category models and an ensemble model. Section.2discusses the division strategy and category models, which segment images into category masks. Subsequently, Section.III-Bexplains the ensemble model, designed to fuse predictions derived from category models.

SECTION: III-ADivision Strategy

Divide-and-conquer is an algorithmic paradigm where a problem is recursively broken down into smaller related sub-problems that are easier to solve. The solutions to these sub-problems are then combined to address the original problem. In the UDA for semantic segmentation task, a semantic segmentation modelis trained with source imagesand source labelsto segment target imagesinto maskscomprisingclasses.

Employing the divide-and-conquer strategy, we deconstruct the semantic segmentation task into subtasks that group classes into categories and train category models to segment different classes. In particular, we groupclasses intocategories guided by specific criteria
which encompasses various factors, such as how humans semantically label an RGB image by AI tools[20,21](e.g. commencing with the background, progressing to large objects, and concluding with smaller ones). To account for non-category classes (classes that do not belong to a particular category), we introduce a class namedother categoryto denote them. It means a specific class is assigned only in one category label for a given pixel, while other category labels maintain the designation asother category. This class helps category models learn features of non-category classes in training, improving the accuracy of classes that belong to a category. Subsequently, we remap source labelsto source category labelsbased on the definedcategories where the class will be retained in the category label if it belongs to this category.
The procedure of generating category labels is shown in Algorithm1.
After generating source category labels, we train target category models, denoted as, using existing UDA methods.
Each target category modelis then employed to segment target imagesinto target category masks, encompassingclasses.

SECTION: III-BEnsemble Model

By division strategy detailed in Section.2, target imagesare segmented into target category masks. Conflicts arise when different classes are predicted for the same pixel value among the target category masks. Note that popular ensemble methods like majority voting and averaging are not suitable to solve these conflicts. Due to the aforementioned division strategy, category models do not share classes, thus ensemble methods that depends on a consensus between models can not be applied. Hence, we introduce an ensemble modeldesigned to extract features from each target category maskand fuse them into the final semantic segmentation mask.

As the ensemble model exclusively fuse masks and yields output identical to the semantic segmentation model, we opt for an encoder-decoder architecture with a simple backbone as the architecture of the ensemble model (see Section.IV).

Input. Given that UDA for semantic segmentation only has labels in the source dataset, for enhanced robustness, we utilise source category masksand their corresponding source labelsas the training dataset, denoted as, to train the ensemble model. Source category masksare generated by source category models, which are trained on the source dataset.

Note that source category modelsonly need to be trained by SL in contrast to UDA methods since there is no domain gap between source category masksand target category masks, both of them are segmentation masks instead of RGB images.

Training Step. The training step is illustrated in Fig.2and implemented in Algorithm2. In the training dataset, there arecategory masks for one source label. To enable the model to process this kind of input, we stack them (similar to how we treat RGB images) into a pseudo-imagewhere each channel corresponds to a source category mask. Thechannels follow the same order as thecategories defined.
We use pixel-wise cross-entropy as the loss functionto train the ensemble model like semantic segmentation task

and leverage the exponential moving average (EMA)[54]to compute the mean of previous model parameters for weight updates, enhancing the robustness and temporal stability of the ensemble model.

Inference StepThe inference step is illustrated in Fig.2. Like the training step, firstly, we use target category modelsto generate target category masks. Then, the trained ensemble model is used to fuse target category masksinto the final segmentation mask.

In this manner, we effectively address the challenge of UDA for semantic segmentation by generating category masks for target images and fusing them into one mask. As target category masks only have edge information, the domain gap between the source and target datasets does not significantly impact the performance of the ensemble model. Consequently, the ensemble model only needs to be trained once and can seamlessly fuse target category masks derived from diverse UDA methods and architectures. This flexibility allows target category modelsto be trained with varied UDA methods and architectural designs, such as Convolutional Neural Networks (CNNs) and Transformers. The efficacy of the ensemble model is demonstrated across various target domains, including Cityscapes, BDD100K, and Mapillary Vistas. Moreover, the output of the ensemble model can serve as pre-annotations for the target dataset, facilitating human annotation or the generation of pseudo-labels for other UDA tasks.

SECTION: IVExperiments

SECTION: IV-ADatasets

We employ the composite source dataset from[17]called Musketeers, which is composed of GTA5[13], Synscapes[50]and UrbanSyn datasets[17]. The combination of diverse synthetic datasets results in a more balanced class distribution, ensuring an ample supply of samples, particularly for rare classes. Specifically, GTA5 comprises 24,966 images with a resolution of 19141052, Synscapes consists of 25,000 images at 1440720, and UrbanSyn encompasses 7,539 images with a resolution of 20481024. We validate our framework using three distinct datasets as the target domains to show the generalisation capability. Cityscapes includes 2,975 training and 500 validation images, each with a resolution of 20481024. BDD100K comprises 7,000 training and 1,000 validation images at resolution 1280720. The Mapillary Vistas dataset, consisting of 18,000 training and 2,000 validation images, exhibits varying aspect ratios and resolutions. For Mapillary Vistas, we utilise 14,716 training and 1,617 validation images with a 4:3 ratio to ensure consistency. All experiments use the mean intersection over union (mIoU) over all classes as the evaluation metric.

SECTION: IV-BImplementation Details

As a common practice, we focus on nineteen classes from Cityscapes evaluation. The nineteen classes are grouped into four categories based on their object size and relationships, as illustrated in TableI. Firstly, we group classes without specific shapes into the Background category. Then, classes with similar shapes and large sizes are grouped into the Vehicles category. For Human/Cycle, we combinebicycle,motorcycle,personandriderinto the same category due to their relationships that thepersonwill be classified as ariderif they are on abicycleormotorcycle. Finally, Thetraffic light,traffic sign, andpoleare grouped together since traffic items are often attached to poles. Fig.3shows the visualisation of generating source category labels.

Source category models are employed to train the ensemble model. The network architecture uses DeepLabV3+[55]with ResNet101[56]as the backbone, loaded with ImageNet pretrained weights. Training adopts SGD[57]with a learning rate of 2, linear learning rate warmup over 1k iterations, and polynomial decay. The model undergoes training on a batch of eight images with a crop size 1024512 for 90k iterations.

The ensemble model is a DeepLabV3+ network with ResNet18 as the backbone. The backbone is trained from scratch with random initialisation. The training optimiser is AdamW[58]with a learning rate of, incorporating linear learning rate warmup over 8k iterations, polynomial decay and. The model is trained on a batch of eight images resized to 20481024 for 90k iterations. Note all DeepLabV3+ architectures are from Detectron2 framework[59].

To advance the last mile of UDA for semantic segmentation, we use the previous state-of-the-art UDA method MIC to train target category models. Consequently, the implementation framework, network architecture, hyper-parameters, optimiser, and training strategy mirror those presented in[11]. The resolutions for Cityscapes align with those stipulated in[11]; For BDD100K, images are resized to 25601440; For Mapillary Vista, we crop each image in the training set by one-third along the height direction and resize it to 20481024.

SECTION: IV-CComparison with State-of-the-Art

97.9

82.2

92.3

59.9

61.5

69.2

95.0

83.8

67.9

95.5

87.8

90.5

70.7

78.4

78.7

64.5

84.8

44.6

55.4

57.9

56.0

93.4

71.1

55.4

67.1

62.4

57.3

68.9

63.7

82.4

69.6

65.2

69.3

72.3

74.8

TableIIpresents the results of our method alongside those of previous state-of-the-art UDA methods. Our framework achieves state-of-the-art performance in Cityscapes, BDD100K and Mapillary Vista. On Cityscapes, we elevate the mIoU from 77.5 to 78.7, a difference of 2.8 points compared to SL. Compared to MIC, which demonstrates a margin of 0.8 points on top of HRDA, our method outperforms MIC with a greater margin of 1.2. Notably, on BDD100K, our method surpasses MIC by 2.1 points and exceeds the SL by 0.9 points. The improvement over SL can be attributed to the challenging nature of this target dataset, which lacks proper balance. Therefore, incorporating a class-rich and balanced synthetic dataset, such as Musketeers, contributes to the superior performance of our method compared to SL. In addition to improving mIoU in nineteen classes, our method particularly enhances performance in foreground classes. For Cityscapes, our approach achieves notable improvements in IoU forpersonwith +1.9,riderwith +8.1,truckwith +4.7,motorcyclewith +3.9, andbicyclewith +1.6. Remarkably, the IoU forriderandtruckexceeds SL by margins of +1.3 and +3.0, respectively. For BDD100K and Mapillary Vista,person,rider,motorcycle, andbicycleare also significantly improved, validating the efficacy of our division strategy in enhancing model performance for these foreground classes. Besides quantitative results, we present visualisations comparing our method with the state-of-the-art approach for MusketeersCityscapes, BDD100K and Mapillary Vistas to illustrate the advancements achieved by our proposed method (see Fig.4).

SECTION: IV-DDEC with Other Methods and Architectures

Our framework can integrate with diverse UDA methods without introducing additional training parameters or strategies. We extend the implementation of our method to DACS with DeepLabV2, MIC with DeepLabV2, and HRDA with Daformer for MusketeersCityscapes. The corresponding results are detailed in TableIII. Notably, DEC significantly enhances performance across numerous foreground classes, includingrider,motorcycle, andbicycle. In addition to foreground classes, DEC demonstrates improvements in background classes such assidewalk, which is often confounded withroad. Specifically,sidewalkimproves from 73.7 to 77.2 for DACS, 78.1 to 80.2 for HRDA, and 76.5 to 77.6 for MIC.

SECTION: IV-EAblation Study

DEC use multi-source datasets to train category and ensemble models. For category models, as stated in SectionI, strategies tailored for single-source synthetic dataset are often susceptible to overfitting and instability. MIC and HRDA exhibit noteworthy advancements in GTACityscapes based on RCS and FD while these strategies demonstrate suboptimal performance in multi-source scenarios (see TableIV). On the other hand, the ensemble model needs enough training data since there is no appropriate pre-trained backbone. The variability of single-source dataset is insufficient for developing a robust ensemble model. TableVpresents the mIoU of ensemble models trained on single-source and multi-source datasets. The results demonstrate that the ensemble model trained on multi-source datasets effectively fuses category masks into the final mask, e.g., the Human/Cycle category (see Fig.5(c)) correctly segments the dotted area, whereas the ensemble model trained with GTA5 fuse dotted area incorrectly (see Fig.5(g)). Thus, it is necessary to train the ensemble model with multi-source datasets to obtain robust and reliable final masks.

DEC groups classes into categories to decrease the complexity of the model and improve segmentation. However, DEC requires a feasible division strategy (including but not limited to TableI) to maintain the contextual information between different classes. Choosing proper categories is not an arbitrary task. We demonstrate in TableVIhow randomly pick some classes as a group to generate some categories. It is seen that random division does not push the performance for MusketeersCityscapes in TableVII(strategy 1-4). Using the four-category division (strategy B+V+H+T), DEC achieves a +1.2 mIoU improvement over MIC. Additionally, we show the performance of other feasible division strategies, such as dividing classes into two categories and three categories in TableVII.

97.9

These division strategies are generated by combining categories in TableI, e.g.,BVdenotes a category containing classes from Background and Vehicle;BV+HTmeans this division strategy splits all classes into two groups. These division strategies achieve a gain of at leastfor MusketeersCityscapes compared to MIC. Fig.6provides a qualitative result where these feasible strategies can effectively improve segmentation, e.g.,sidewalk.

We choose the neural network DeepLabV3+ as an encoder-decoder architecture to ensemble the output of our category models. This network is well-known, easy to train and does not require a lot of resources. Here, we demonstrate the influence of different backbones and training parameters (e.g. learning rate and EMA) on the ensemble model.

Backbone:TableVIIIillustrates the performance of the ensemble model with various backbones for Musketeers → Cityscapes, BDD100K and Mapillary Vista. ResNet-18 achieves the highest mIoU across all three datasets (79.1, 63.2, and 75.0, respectively). Given its superior performance and fastest training speed among the tested backbones, ResNet-18 is selected as the preferred backbone for the ensemble model.

Optimizer:Tab.IXshows the SGD and AdamW with different learning rates to train the ensemble model for MusketeersCityscapes. It can be seen that AdamW is more compatible with the ensemble model than SGD.

EMA:Tab.Xshows the influence of different EMA coefficientsfor MusketeersCityscapes, BDD100K and Mapillary Vista. It is observed that a larger value ofenhances the generalisation of the ensemble model, leading to optimal performance for each target dataset.

SECTION: IV-FPerformance Metrics

TableXIpresents the FPS, FLOPs, and parameters for both the category and ensemble models. The category model is built on the existing UDA method, with modifications made to the training labels. Consequently, its FPS, FLOPs, and parameters remain the same as those of the HRDA and MIC models. The ensemble model, with only 12.30M parameters, achieves notably high inference speeds, enabling efficient fusion of the category model outputs. As a result, the overall running speed is primarily dictated by the UDA method used to build the category models—the faster the UDA method, the better our approach performs.

The runtime and memory consumption are presented in TableXII. The category model achieves faster training and reduced memory usage than HRDA and MIC by eliminating unnecessary techniques, such as rare class sampling and the thing-class ImageNet feature distance. Meanwhile, the inference speed and memory consumption remain the same with HRDA and MIC, as no additional parameters are introduced. The ensemble model’s simple architecture completes each training iteration in just 0.69 seconds. Additionally, the ensemble model achieves an inference speed (FPS) of 46.3 for an input resolution of 10242048 while consuming only 0.98 GB of memory.

SECTION: IV-GThe Last Mile to SL

UDA for semantic segmentation is becoming trustworthy and closing the gap with respect SL. However, can it reach equal performance? This remaining gap is what we call the last mile of UDA. Our work advances the last mile, with specific target domains (e.g., BDD100K) and some classes (e.g.,rider,truck) even surpassing SL.

For the commonly used target domain, Cityscapes, our DEC method outperforms SL forriderandtruck. Additionally, for the classesroad,sky,person,car, andbus, DEC achieves a difference of less than one point compared to SL. For BDD100K, DEC crosses the last mile, improving performance beyond SL. As for Mapillary, the performance for the classesperson,rider, andmotorcyclesurpasses that of SL, classes that are crucial for autonomous driving.

Some classes still exhibit gaps compared to SL. For example, thesidewalk, an important class on autonomous driving, shows a 5 points difference from SL on MusketeersCityscapes. Fig.7shows some qualitative results for thesidewalkin both DEC and SL. The error in DEC is primarily found at the bottom of vehicles, where thesidewalkis misclassified asroad. We expect these errors to have an insignificant influence on autonomous driving, as the vehicles in these areas are classified correctly. Moreover, since the ground truth is manually labelled, human bias is inevitably introduced into the ground truth. This is why some classes show discrepancies compared to SL, such asterrain,fence, andpole. Fig.8illustrates these human biases in Cityscapes.

Furthermore, there are also human bias in Mapillary Vistas. Fig.9indicates annotation bias amongterrain,building,sky, andvegetation.

These classes are labelled differently between synthetic and real datasets. This explains the large gap (e.g.,vegetation,terrain) in our method for Mapillary Vista.

SECTION: VConclusion

This paper introduces DEC, a pioneering framework utilizing multi-source datasets in category models to address the last mile of UDA for semantic segmentation. DEC comprises category models and an ensemble model, which are responsible for segmenting images into category masks and fusing them into a segmentation mask. Compatible with previous UDA methods, DEC consistently demonstrates improvement when applied on top of them. Across three real datasets, namely Cityscapes, BDD100K, and Mapillary Vistas, DEC achieves state-of-the-art performance with mIoU of 78.7, 62.4, and 74.8, respectively. Compared to SL, DEC lags by merely 2.8 and 4.7 points for Cityscapes and Mapillary Vistas, while it surpasses by 0.9 points for BDD100K.

SECTION: References