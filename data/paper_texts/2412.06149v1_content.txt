SECTION: An Effective and Resilient Backdoor Attack Framework against Deep Neural Networks and Vision Transformers
Recent studies have revealed the vulnerability of Deep Neural Network (DNN) models to backdoor attacks. However, existing backdoor attacks arbitrarily set the trigger mask or use a randomly selected trigger, which restricts the effectiveness and robustness of the generated backdoor triggers. In this paper, we propose a novel attention-based mask generation methodology that searches for the optimal trigger shape and location. We also introduce a Quality-of-Experience (QoE) term into the loss function and carefully adjust the transparency value of the trigger in order to make the backdoored samples to be more natural. To further improve the prediction accuracy of the victim model, we propose an alternating retraining algorithm in the backdoor injection process. The victim model is retrained with mixed poisoned datasets in even iterations and with only benign samples in odd iterations. Besides, we launch the backdoor attack under a co-optimized attack framework that alternately optimizes the backdoor trigger and backdoored model to further improve the attack performance. Apart from DNN models, we also extend our proposed attack method against vision transformers. We evaluate our proposed method with extensive experiments on VGG-Flower, CIFAR-10, GTSRB, CIFAR-100, and ImageNette datasets. It is shown that we can increase the attack success rate by as much as 82% over baselines when the poison ratio is low and achieve a high QoE of the backdoored samples. Our proposed backdoor attack framework also showcases robustness against state-of-the-art backdoor defenses.

SECTION: 
Deep neural networks have made tremendous progress in past years and are applied to a variety of real-world applications, such as face recognition, automatic driving, natural language processing, and objective detection, due to superhuman performance.
Vision transformer (ViT)is a promising deep learning architecture that offers a compelling alternative to traditional convolutional neural networks (CNNs) for computer vision applications. Despite the success in the computer vision domain, both DNN and ViT are vulnerable to backdoor attacks.
It is shown that the attacker can inject a backdoor (a.k.a. trojan) into the model by poisoning the training dataset during training time. The backdoored model behaves normally on the benign samples but predicts any sample attached with the backdoor trigger to a target false label. Due to its concealment, detecting backdoor attacks is very difficult. Moreover, the emergence of invisible backdoor triggers makes it more difficult to inspect whether the training samples are backdoored or not.

There exists a long line of backdoor attack strategies exploring injecting backdoors into DNNs.
However, they face the following shortcomings. First of all, most of the existing approachesuse a random backdoor trigger or random trigger mask (random pattern and location) in the attack, which is easy to be detected and achieves a sub-optimal attack performance. Second, current backdoor attacksseparate the trigger generation process from the backdoor injection process, thus resulting in generating sub-optimal backdoor trigger and backdoored model. Third, various works utilize
visible backdoor triggers, which can be easily detected by visual inspection.
Finally, although various existing works claimed to be defense-resistant, they can still be detected by the latest defenses, such as NADand MNTD. In terms of backdoor attacks against ViTs, most of the existing transformer backdoor attacks use visible triggers to launch the attacks, making it easy for human defenders to detect abnormalities through visual inspections. Although Doanproposed to generate hidden triggers based on a global warp of WaNet, the attack success rate and the perceptual trigger quality are relatively low.

In this paper, we put forward a novel backdoor attack strategy that integrates effectiveness and evasiveness. From the attack effectiveness perspective, unlike the existing works that use fixed trigger masks (e.g., a square in the lower right corner), we utilize an attention map to differentiate the weights of the pixels. The mask is determined as the pixels with the highest weights since such pixels have a higher impact on the classification. Using such a carefully designed trigger mask, we can achieve a higher attack success rate than the existing works with the same trigger size. Moreover, rather than separating the backdoor trigger generation from the backdoor injection process, we adopt the co-optimization backdoor framework that jointly optimizes the backdoor trigger and the backdoored model to generate an optimal backdoor trigger and achieve a higher attack success rate. In terms of evasiveness, it is quantified by both the human vision and state-of-the-art defense strategies. We carefully adjust the transparency (i.e., opacity) of the backdoor trigger and add a Quality-of-Experience (QoE) constraint to the loss function, aiming to generate a more natural backdoor trigger. Furthermore, we propose an alternating retraining algorithm that updates the model using either mixed samples or only clean samples according to the iteration index. In addition to evaluating DNN models, we also assess our proposed attack method on vision transformers. Experiments show that our proposed method outperforms baselines in both attack success rate and clean data accuracy, especially when the poison ratio is low. It is demonstrated that our proposed method is also robust to state-of-the-art backdoor defenses.

To conclude, our paper makes the following contributions:

To the best of our knowledge, we are the first to utilize attention mechanisms to design backdoor trigger masks (i.e., trigger shape and trigger location), which significantly improves the attack performance. Rather than arbitrarily setting the mask, we determine the mask according to the focal area of the model to intensify the trigger impact on the prediction results.

We propose a QoE-aware trigger generation method by introducing the QoE loss in the loss function to constrain the perceptual quality loss caused by the backdoor trigger.

We design an alternating retraining method for backdoor injection to alleviate the decline of clean data prediction accuracy, which also helps resist state-of-the-art model-based defenses.

Extensive experiments on VGG-Flower, GTSRB, CIFAR-10, CIFAR-100, and ImageNette datasets show that our proposed method outperforms the state-of-the-art backdoor attacks concerning both the attack effectiveness and evasiveness. We can evade state-of-the-art backdoor defenses. Apart from the DNN model, we show that our proposed attack method is also effective against vision transformers.

SECTION: 
SECTION: 
Deep neural network is a class of machine learning models that uses nonlinear serial stacked processing layers to capture and model highly nonlinear data relationships. We mainly consider a prediction scenario, where a deep neural networkencodes a function:,is the parameter of. Given the input sample, the DNN modeloutputs a nominal variableranging over a group of predesigned labels.

The DNN model is usually trained by supervised learning. To obtain a DNN model, the user utilizes a training datasetthat includes amounts of data pairs, whereis the input andis the ground-truth label of. The trainer should determine the bestforby optimizing the loss function. The loss function is usually optimized by stochastic gradient descentand its derivatives.

However, training such sophisticated deep neural networks requires much computing and time costs since millions of parameters should be optimized. Therefore, many resource-limited clients prefer to outsource the training of deep neural networks to cloud computing providers, such as Google, Amazon, and Alibaba Cloud. Moreover, outsourcing training also has the following advantages. Firstly, optimizing the deep neural networks needs expert knowledge to determine the amenable model structure and much effort to fine-tune the hyperparameters. Second, training a sophisticated deep neural network requires millions of training samples. However, collecting and annotating them is labor-intensive for the clients.
Based on the hindrance above, the cloud server provider receives more and more business of training DNN models. However, if the cloud providers are malicious, they may provide users with malicious models that will behave abnormally on specific samples. Being aware of such a threat, more and more defense works have been proposed to inspect whether the model is malicious. In this paper, we aim to design a more effective and defense-resistant backdoor attack methodology in the outsourced cloud environment from a malicious cloud server provider’s perspective.

SECTION: 
The Transformer architecture, initially designed for natural language processing (NLP), has been recently adapted for computer vision by leveraging the self-attention mechanism to model relationships between different parts of an image. One popular vision transformer is ViT.

Letbe a sequence ofinput image patches, where each patch is represented as a tensor with dimensions.
To begin, ViT applies an embedding layer to each image patch, transforming it into a-dimensional embedding vector, which can be expressed as. Then, ViT employs a series of transformer encoder layers to process the embeddings. Each encoder layer consists of two sub-layers: a multi-head self-attention mechanism (MHSA) and a position-wise feedforward network (FFN).
The MHSA layer is responsible for capturing interactions between the patch embeddings using self-attention. The FFN layer applies a non-linear transformation to each patch embedding independently.

The attention mechanism within the Multi-Head Self-Attention (MHSA) layer can be divided into two main operations: attention rollout and attention diffusion. The attention rollout operation calculates the similarity between each query vector and all key vectors using the dot product. It scales the dot products byto prevent the gradients from exploding, applies a softmax function to obtain attention weights, and finally computes a weighted sum of the value vectors. Mathematically, the attention rollout can be expressed as:

where,, andrepresent the query, key, and value matrices, respectively, anddenotes the dimensionality of the key vectors.
The attention diffusion operation, on the other hand, can be expressed as follows:

whererepresents the number of attention heads.,, andare learnable weight matrices specific to the-th attention head.is a learnable weight matrix used to map the concatenated output of all heads to the desired output dimensionality. The attention diffusion operation computes multiple attention heads in parallel and concatenates the resulting vectors along the last dimension. The concatenated vectors are then linearly transformed to obtain the final output.

In this paper, we also extend our proposed attack framework against vision transformers. Our experimental results demonstrate a high attack success rate when applied to vision transformers, highlighting the vulnerability of vision transformers to backdoor attacks.

SECTION: 
In recent years, deep neural networks have been known to be vulnerable to backdoor attacks. Intuitively, the objective of the backdoor attack is to trick the targeted DNN model into studying a powerful connection between the trigger and the target misclassification label by poisoning a small portion of the training dataset. As a result, every sample attached to the trigger will be misclassified to the target label with high confidence, while the backdoored model can also maintain high prediction accuracy on the benign inputs.

To recap, the first backdoor attack is proposed by Gu et al., namely BadNets. It is assumed that the attacker can control the training process of the DNN model. Thus, the attacker can poison the training dataset and change the configuration of the learning algorithms and even the model parameters. In BadNets, the attacker first chooses a random trigger (e.g., pixel perturbation) and poisons the training dataset with the backdoor trigger. After retraining the DNN model with the poisoned dataset, the DNN model will be backdoored. Based on the concept in BadNets, amounts of related works were proposed subsequently.

From the backdoor trigger perspective, rather than using the random trigger, Liu et al. proposed TrojanNNthat utilized a model-dependent trigger. The trigger is generated to maximize the activation of the selected neuron, in which the neuron has the largest sum of weights to the preceding layer. Further, considering to evade the pruning and retraining defenses, Wang et al.put forward a ranking-based neuron selection methodology to choose neuron(s) that are difficult to be pruned and whose weights have little changes during the retraining process. Gong et al.selected the neuron that can be most activated by the samples of the targeted label to improve the attack strength.

Unlike using the above static backdoor triggers (i.e., fixed locations and patterns), Salem et al.proposed a dynamic trigger generation strategy based on a generative network and demonstrated such dynamic triggers could evade the state-of-the-art defenses. Nguyen et al.implemented an input-aware trigger generator driven by diversity loss. A cross-trigger test is utilized to enforce trigger non-reusability, making it impossible to perform backdoor verification.

From the perspective of attack concealment, Saha et al. proposed hidden backdoor attacksin which the backdoored sample looks natural with the right labels. The key idea is to optimize the backdoored samples that are similar to the target images in the pixel space and similar to sourced images attached with the trigger in the feature space. Liao et al.first generated an adversarial example that can alter the classification result and then used the pixel difference between the original sample and the adversarial example as the trigger. Li et al.described the trigger generation as a bi-level optimization, where the backdoor trigger is optimized to enhance the activation of a group of neurons through L-regularization to achieve invisibility.

From the perspective of attack application scenarios, apart from targeting the centralized model, backdoor attacks against federated learning are also attracting much attention recently. The attacker aims to backdoor the global model via manipulating his own local model. The main challenge is that the trigger will be diluted by subsequent benign updates quickly. In this paper, we only focus on backdoor attacks against centralized models.

Unlike the aforementioned backdoor attacks that either use ineffective random triggers or have visible triggers that can be easily detected, in this paper, we propose a more effective attention-based QoE-aware backdoor attack framework. It can not only achieve a high attack success rate but also evade state-of-the-art data-based backdoor defenses and human visual inspections.

SECTION: 
When realizing the catastrophic impact of a backdoor attack, various defenses are also proposed to mitigate it. As far as we know, the exiting backdoor defense works can be categorized into data-based defenseand model-based defense. And both data-based and model-based defenses can also be further classified into online defense (during run-time)and offline defense (before deployment).

Data-based backdoor defenses check whether a sample contains a trigger or not. From the perspective of online inspection, Gao et al. proposed Stripthat copies the inputting sample multiple times and combines each copy with a different sample to generate a novel perturbed sample. If the sample is benign, it is expected that those perturbed samples’ prediction results will obtain a higher entropy result due to randomness. If the sample is backdoored, the prediction results will get a relatively low value since the trigger will strongly activate the targeted misclassification label. SentiNetfirst seeks a contiguous region that is significant for the classification, and such region of the image is assumed to contain a trigger with high probability. Then SentiNet carves out the region, patches it on other images, and calculates the misclassification rate. If most of the patched samples are misclassified into the same false label, then the inputting sample is malicious.
From the offline inspection perspective, Chen et al. proposed activation clustering, namely AC. It is known that the last hidden layer’s activations can reflect high-level features used by the DNN to obtain the model prediction. AC assumes there exists a difference in target DNN activation between benign samples and backdoored samples with the same label. More concretely, if there exist backdoored samples in the inputs of a certain label, then the activation results will be clustered into two different clusters. And if the inputs contain no malicious samples, the activation cannot be clustered into distinct groups.
Tran et al. investigated spectral signature, which is based on statistical analysis, aiming to detect and eradicate
malicious samples from a potentially poisoned dataset.

Model-based backdoor defenses check whether a deep neural network is backdoored or not. From the perspective of online inspection, Liu et al.proposed Artificial Brain Stimulation (ABS) that is inspired by Electrical Brain Stimulation (EBS) to scan the target deep neural network and determine whether it is backdoored.
Ma et al. proposed NICto detect malicious examples. NIC inspects both the provenance and activation value distribution channels.
From the offline inspection perspective, Wang et al. proposed Neural Cleanse (NC)to inspect the DNN model. The key idea of NC is that as for the backdoored model, it needs much smaller modifications to make all input samples to be misclassified as the targeted false label than any other benign label.
Huang et al. proposed NeuronInspectthat integrates the output explanation with the outlier detection to reduce the detection cost. Chen et al. proposed DeepInspectthat utilizes reverse engineering to reverse the training data. The key idea is to use a conditional generative model to get the probabilistic distribution of potential backdoor triggers. Xu et al. proposed MNTDthat trains a meta-classifier to predict whether the model is backdoored or not.

In this paper, we select a variety of representative defense works to defend our proposed attacks. It is shown that our proposed attack is robust to these defending works.

SECTION: 
To mitigate backdoor attacks on vision transformers, Subramanya et al.presented a test-time defense strategy based on the interpretation map. Doan et al.introduced a patch processing-based defense mechanism to mitigate backdoor attacks. The underlying idea behind these defenses is that the accuracy of clean data and the success rates of backdoor attacks on vision transformers exhibit different responses to patch transformations prior to the positional encoding.

In this paper, we extend our proposed backdoor attack framework to vision transformers. It is shown that our proposed method outperforms the existing ViT-specific backdoor attacks regarding both effectiveness and evasiveness.

SECTION: 
In this paper, we have the same threat model as the state-of-the-art backdoor attacks. We assume the attacker is a malicious cloud server provider responsible for training a sophisticated DNN/ViT for the clients. The attacker has the ability to control the model training process and access the training dataset. The training model structure, model parameters, and activation function are also transparent to the attackers. However, the attacker has no knowledge about the validation dataset that the clients use to test whether the received model is benign and satisfies the prediction accuracy. We also assume that the user is concerned about the security of the received model, i.e., he will inspect whether the model is backdoored using state-of-the-art defense strategies.

SECTION: 
We first present the general attack framework and then describe key components in the framework, including attention-based mask determination, QoE-based trigger generation, and alternating retraining strategy.

SECTION: 
Since the attacker is capable of manipulating both the trigger and the model, we can formulate backdoor attacks as an optimization problem.

The first term optimizes the prediction accuracy of clean samples. The second and third terms optimize the attack success rate of trigger-imposed samples while constraining trigger visibility.

Optimizing () is difficult since the backdoor triggerand the backdoored modelare co-dependent. Therefore, we separate the optimization problem () into two sub-problems and solve the two sub-problems by alternately updating the backdoor triggerand the backdoored modeluntil convergence. We update the trigger and the model in the-th iteration as

Given the current model, we first optimize the triggerusing Adam optimizer, which will be elaborated in the following sections. Then, given the optimized trigger, we obtain the optimized modelby retraining the modelwith poisoned samples using. We summarize the algorithm of the co-optimization attack framework in Algorithm.

SECTION: 
In a RAN withattention modules, the output of the-th attention module is

We randomly selectclean samples of the target classand attainattention maps. Assuming that each sample has the same probability of occurrence, we choose the attention map that is closest to the average attention map for generality.

whereis the set of samples of the target label, andis the average attention map.

Considering that most existing works use a contiguous square trigger of size(is the number of pixels), we also use the conventional expressionto denote the trigger size. To make a fair comparison, we choose the toppixels with the highest attention values as the trigger region, i.e., trigger mask, in our attack for evaluation.

SECTION: 
When generating the trigger, we incorporate gradient enhancement techniques for the selected neurons to further enhance the attack effectiveness. During the gradient descent optimization process of the trigger, we assign greater weight to the gradients of the selected neurons. By prioritizing these key neurons, which play a vital role in classifying the target label, we can effectively amplify the poisoning effect of the generated trigger.

Specifically, the optimization process for trigger gradient descent can be described as follows:

whererepresents the selected neuron(s),is the trigger for the-th round,anddenote the gradients ofandrespectively, back-propagated from the loss function.is the mask generated by RAN, andis the augmentation factor, which is 4 for CIFAR-100, 3 for CIFAR-10, 21 for GTSRB, 30 for VGG-Flower-l, 2 for ImageNette, and 30 for VGG-Flower-h. Note that we set the values of different augmentation factors according to the experimental effect.

An invisible backdoor trigger is also the key to a successful backdoor attack. A visible backdoor trigger can be easily detected by human visual inspection.
In this paper, we propose to introduce Structural Similarity Index Measure (SSIM)to the loss function and adjust the transparency of the backdoor trigger. SSIM is a commonly used Quality-of-Experience (QoE) metric) that is used to compare the differences in luminance, contrast, and structure between the original image and the distorted image.

wherequantify the luminance similarity, contrast similarity, and structure similarity between the original imageand the distorted image.are parameters.
We introduce SSIM into the loss function to optimize the trigger.

wherebalances the attack success rate and the QoE of poisoned images.

To improve the invisibility of the backdoored samples, we also carefully adjusted the transparency of the backdoor trigger. If we use a higher transparency value, the trigger will be more stealthy but making it more challenging to trigger malicious behaviors. Setting a proper transparency value is a trade-off between the attack success rate and the concealment. Through experiments, we set the transparency value as 0.4 (VGG-Flower-l, CIFAR-10, GTSRB, and CIFAR-100) or 0.7 (ImageNette and VGG-Flower-h) by default.

SECTION: 
SECTION: 
SECTION: 
In this paper, we conduct experiments on various machine learning tasks, covering different datasets (VGG-Flower, CIFAR-10, GTSRB, CIFAR-100, and ImageNette) and deep neural networks. Note that we randomly select 10 classes with 1,673 training images and 200 test images for VGG-Flower. For VGG-Flower-l, the selected images are uniformly resized to 3232. For VGG-Flower-h, the selected images are uniformly resized to 224224.
We utilize VGG-16, ResNet-18, VGG-16, ResNet-34, ResNet-50, and ResNet-18 structures to train DNN models for these six datasets, respectively.

The default target label is label 0 for VGG-Flower-l, label 3 for VGG-Flower-h, label 2 for CIFAR-10, label 10 for GTSRB, label 0 for CIFAR-100, and label 3 for ImageNette.The default transparency value is 0.4 for VGG-Flower-l, CIFAR-10, GTSRB, CIFAR-100, and 0.7 for ImageNette and VGG-Flower-h. We adopt a 92-layer RAN with 6 attention modules. We setfollowing the original RAN model, andto aggregate all information into a single attention map.The victim DNN model prediction accuracies of these six datasets are 98.5%, 91.94%, 97.25%, 79.09%, 92.43%, and 97.5%, respectively.

SECTION: 
We utilize ASR, CDA, SSIM, and LPIPS as our evaluation metrics.

ASR measures the effectiveness of the backdoor attacks, computed as the probability that a trigger-imposed sample is misclassified to the target label.

CDA measures whether the backdoored model can maintain the prediction accuracy of clean input samples.

SECTION: 
SECTION: 
As shown in Tableand Table, our proposed method has higher ASR than the baselines for all six datasets, especially when the poison ratio is small. For example, we can achieve ASR of 94.5%, 44.69%, 90.88%, 96.53% on VGG-Flower-l, CIFAR-10, GTSRB, CIFAR-100 models at poison ratios of 10%, 1%, 0.3%, 0.1% respectively, while BadNets only reaches ASR of 22.0% (VGG-Flower-l), 10.00% (CIFAR-10), 22.01% (GTSRB), 1.29% (CIFAR-100). Compared with HB that uses invisible triggers, we can achieve a significantly higher ASR across all datasets at all poison ratios.Moreover, we can maintain a high CDA.

We compare the invisibility of the backdoored samples across all attacks, as shown in Fig.. We can see that except for HB and ours, the triggers of all other baselines are conspicuous and easily detected by human eyes. Compared with HB, we can produce more indiscernible triggers in some cases. HB can not achieve a high ASR as ours.

SECTION: 
SECTION: 
SECTION: 
In this part, we explore neuron gradient boosting on our attack performance against both DNN models and ViT models.
The results are shown in Tableand Table.

We can see that the neuron gradient boosting strategy can significantly improve the attack success rate and clean data accuracy across all datasets and model types. For example, the gradient boosting strategy can achieve an ASR of 99.86% and a CDA of 89.74% for the CIFAR-10 dataset against the ViT model, while we can only achieve an ASR of 88.36% and a CDA of 79.41% without the neuron gradient boosting strategy.

SECTION: 
SECTION: 
SECTION: 
The defender first ranks neurons in ascending order according to the average activation by clean samples. Then, the defender sequentially prunes neurons until the accuracy of the validation dataset drops below a predetermined threshold.

As shown in Fig., we can still achieve high ASR after pruning. Given a threshold of 80% for CDA, we can preserve an ASR of more than 82% for all datasets. This means that we are resistant to model pruning.

In NAD, the defender first fine-tunes the backdoored model on a small set of benign samples and uses the fine-tuned model as a teacher model. Then, NAD uses the teacher model to distill the backdoored model (student model) through attention distillation. In this way, the neurons of the backdoor will be aligned with benign neurons associated with meaningful representations.

As shown in Table, after applying NAD, the ASR of ours only slightly decreases. The possible reason is that the gap between our generated backdoored model and the benign model has been narrowed through alternating retraining.

In STRIP, the defender duplicates an input sample for many times and merges each copy with a different sample to generate a set of perturbed samples. The distribution of the prediction results of the perturb samples is used to detect backdoored samples.
It is assumed that the prediction results of the disturbed samples have a high entropy if the sample is clean and a low entropy if the sample contains the trigger as the trigger strongly drives the prediction results toward the target label.

As shown in Fig., the prediction results of our backdoored samples have a similar entropy distribution to benign samples for all datasets, making it difficult to differentiate the backdoored samples and the benign samples. Thus, we can evade STRIP defense.

MNTDis a model-based defense based on a binary meta-classifier. To train the meta-model, the defender builds a large number of benign and backdoored shadow models as training samples. Since the defender has no knowledge of the specific backdoor attack methods, MNTD adoptsjumbo learningto generate a variety of backdoored models. In this way, MNTD is generic and can detect most state-of-the-art backdoor attacks.
To apply MNTD to our attack framework, for each dataset, we generate 2,048 benign models and 2,048 backdoored models to train a well-performed meta-classifier.

When we feed our backdoored models to the meta-classifier, it is shown that they can all evade the inspection of MNTD. In comparison, when we feed the backdoored models of the baselines to the meta-classifier, they are all detected by MNTD.
The success in evading the detection of MNTD is possibly due to our alternating retraining strategy that makes the backdoored models behave like the benign ones.

SECTION: 
SECTION: 
This paper presents the design, implementation, and evaluation of an effective and evasive backdoor attack against deep neural networks and vision transformers. To obtain the effectiveness goal, we proposed a novel attention-based mask generation strategy and utilized a co-optimized attack framework. To achieve the evasiveness goal, we carefully adjust the trigger transparency and add a QoE constant to the loss function. We also propose an alternating retraining strategy to improve the model prediction accuracy. We show that our proposed attacks can evade state-of-the-art backdoor defenses.
Experiments on VGG-Flower, GTSRB, CIFAR-10, CIFAR-100, and ImageNette verify the superiority of the attack when compared with state-of-the-art backdoor attacks.

SECTION: References