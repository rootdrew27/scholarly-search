SECTION: Quantum network tomography of Rydberg arrays by machine learning
Configurable arrays of optically trapped Rydberg atoms are a versatile platform for quantum computation and quantum simulation, also allowing controllable decoherence.
We demonstrate theoretically, that they also enable proof-of-principle demonstrations for a technique to build models for open quantum dynamics by machine learning with artificial neural networks, recently proposed in [Mukherjeehttps://arxiv.org/abs/2409.18822 (2024)]. Using the outcome of quantum transport through a network of sites that correspond to excited Rydberg atoms, the multi-stage neural network algorithm successfully identifies the number of atoms (or nodes in the network), and subsequently their location. It further extracts an effective interaction Hamiltonian and decoherence operators induced by the environment.
To probe the Rydberg array, one initiates dynamics repeatedly from the same initial state and then measures the transport probability to
an output atom. Large datasets are generated by varying the position of the latter. Measurements are required in only one single basis, making the approach complementary to e.g. quantum process tomography. The cold atom platform discussed in this article can be used to explore the performance of the proposed protocol when training the neural network with simulation data, but then applying it to construct models based on experimental data.

SECTION: Introduction
In recent years, machine learning (ML) has become a popular and versatile tool in a wide range of scientific disciplines, from natural language processingover quantum chemistry, bio-chemistry, cancer diagnosis, developmental biologyto quantum information theory. Conversely, the development of neural networks has been boosted by the requirements of quantum technology and quantum chemistry, where neural network algorithms have been used to produce accurate and effective results. Presently, they are increasingly employed to investigate the features of complex quantum systemsand superconducting qubits.

In a companion article, we have further broadened this scope by introducing a scheme through which ML can deduce physical models for open quantum systems from a restricted set of measurements of quantum dynamics traversing a system of interest. This goes beyond quantum process tomography, by not just determining a map between input and output Hilbertspaces, but inferring all physical parameters controlling dynamics in-between, and requiring much less diverse measurements.

While ref.demonstrates the general utility of the scheme without focussing on a specific physical platform, here we apply ML-based-model-building to networks of interacting Rydberg atoms in the presence of a controllable cold gas environment, focussing on realistic parameters. We thus show that Rydberg excitation in optical tweezer arrayswithin an ultra-cold thermal gas allows experiments at the present state-of-the-art to explore key facets of ML based model building:
utilizing neural networks that have been trained on a large set of computer simulations of an experiment to propose models of the experiment.
Since good models describing the Rydberg experimental platform are actually known, this can then verify the feasibility of the approach.
After verification of the concept, it can then be further applied to complex quantum devices or systems where a few state effective model should exist but is not a priori known.

Following ref., we employ a multi-branch pipeline capable of probing a quantum system based on its excitation transport properties.
The main branch classifies the system based on the number of Rydberg atoms () in the network, resulting insub-branches, where the coordinates of those Rydberg atoms and the matrix elements of Hamiltonian and Lindbladians are determined. Together, this reconstructs the entire quantum network and its properties, thus implementing quantum network tomography on the system. We then explore the performance of this approach for varying systems sizes and decoherence strengths.
Deterministic positioning of Rydberg atoms using optical tweezers, combined with tunability of long-range dipolar interactions and site-resolved measurements, Rydberg arrays are an ideal platform for the testing of machine learning algorithms designed to analyse quantum systems and their dynamics. The validated approach can then be extended and generalized to arbitrary quantum networks.

This article is organized as follows: in section, we introduce the quantum network formed with Rydberg atoms interacting via dipole-dipole interactions. The network is divided in two parts: a “black-box” which will be taken as inaccessible to measurement but for which a model is sought, and a second part involving atoms that are accessible to measurements and initialisation. In section, we present a multi-branch pipeline capable of probing the inaccessible quantum system based on its excitation transport properties. The root element, discussed in section, classifies the system based on the number of Rydberg atoms () and spawnssub-branches. Then, we discuss in sectionhow the branches find the coordinates of Rydberg atoms, and in sectionhow they infer the matrix elements of Hamiltonian and Lindbladians. We conclude in section.

SECTION: Rydberg quantum networks
We consider an assembly ofRydberg atoms, which we term Rydberg aggregate due to collective excited states. The atoms are assumed trapped and immobile at controllable locations, for example using configurable optical tweezer arrays.
Each atom can be in two internal states:and, whereis the angular momentum quantum number. We choose a principal quantum number of, in order to utilise interaction strength calculations from ref..
Within the aggregate ofRydberg atoms,Rydberg atoms are positioned randomly inside a square area of side length, which is considered a “black-box”. Three additional atoms form theandand are placed outside the box, as shown in Fig.(a). Here, we consider only a single Rydberg-excitation in the Rydberg aggregate, initially located on the. The number of such excitations is conserved, allowing us to use a simplified notation for relevant many-body states, where only theth atom is in the Rydberg-state, and all others in the Rydberg-state.

Rydberg atoms interact among each other via long-range dipole-dipole interactions, governed by the aggregate Hamiltonian ()

where, withthe position of theth Rydberg atom. The angleis between the quantization axis and the separation vectorlinking the Rydberg atoms. A magnetic field is assumed to have removed components of the-state other thanin order to reach Eq. (), withbeing the angular momentum projection on the quantisation axis. Dipole-dipole interactions now allow the single-excitation to migrate on the Rydberg array, so that the latter forms a quantum network.

The density matrixof the system then evolves according to the master equation

whereis the super-operator written asand we useabove and in the following.

The operatorsandcan arise from interactions between the Rydberg aggregate atoms and the atoms in the ultra-cold background gas, encapsulating the resultant disorder and decoherence:

We will employ three variants for Eq. ()-() in this article: (i) Initially we set, such that only coherent dipole-dipole interactions () are present.
(ii) In a final section, we allow non-trivially varying but realistic,based on randomised background gas atom locations, as discussed in appendix.
(iii) As an interim model to systematically explore sensitivity of network tomography to decoherence, we retain only the decoherence ratesfrom (ii) with,
but rescale theto reach a target mean decoherence rates.

We initialize the system with the single Rydberg-excitation on the, the red-ball in Fig.(a), the position of which is fixed at a distanceto the left of the centre of the box. The initial aggregate state is thus

The excitation then can migrate to the(violet-balls) via theregion, due to the dipole-dipole interaction, as shown in Fig.(a). To describe this, the Lindblad master equation () is solved numerically, using the high-level programming language XMDS. Finally, we sample the-excitation probability on theth output atom at different coordinates, see appendix, with the measurement operator, acting on output atom, providing

Here and in the following, we label the positions of the atoms inside the box byand those of output atoms by. Probabilitiesare recorded at
a time, as shown in Fig.(b), and form the input of the machine learning algorithm.
Since training of a neural network requires large datasets, we take the twoto be systematically moved in small steps, providing discrete positions. The first output atom is moved along theplane in a semi-circular path of radius, while the second output atom is moved simultaneously along the-axis in a straight line at, as sketched in Fig., allowingconfiguration in total for positions of the two output atoms.
The combined data of excitation probabilities on both output atoms, then provides a data-set of 400 probabilities for the machine learning models, see appendixfor more details.

The numerical simulation is then repeatedtimes, where the number of atoms () and their positionsare varied in each dataset, withand positionsuniformly distributed in a square of edge length. We keepandfor the moment. The generated datasets are then used for training of the machine learning algorithm and a separate dataset withelements is generated for testing, usingcases for each. In the following sections, we will see how the measurement data, obtained from theone and two, can provide us with useful information related to the quantum network as well as its decoherence by the environment using machine learning.

SECTION: Machine learning based multi-branch pipeline
In this section, we establish the multi-branch pipeline for
quantum network tomography of Rydberg arrays by machine learning, sequentially discussing
each stage of the pipeline: (i)classification, where the system is classified based on the number of Rydberg atomspresent in the black-box region using classification algorithms, (ii)multi-target regression, where the algorithm infers the atomic coordinates, system Hamiltonian and Lindblad operators using artificial neural networks.

SECTION: Classification: number of atoms
In the first stage of the pipeline, we classify the system according to the number of Rydberg atoms inside theregion. We provide the datasetscontaining excitation probability on the output atoms to a set of classification algorithms, which infer the number of Rydberg atoms within the box. We have considered four classes () with each class (ID) characterised by the number of atoms, as shown at the top of Fig.. The variation within the input dataset for each class can be seen in Fig.(b) for a single realisation. The figure shows excitation probabilities on both output atoms, where inputsare excitation probabilities at the output site(), while inputscorrespond to output site(). Coarsely, the input datasets for each class appear similar, but in detail the relative probability differences between the input datasets of each class can be large, as shown in the inset of Fig.(b). These minor distinctions can play an important role in the identification of each class.

Here, we focus on three classification algorithms: (i) Support vector machine (SVM), (ii) Random forest (RF), and (iii) K-Nearest neighbor (KNN). A SVM establishes the optimal decision boundary that divides the classes and optimises the distance between the optimal decision boundary and the closest data point from each class. The decision boundary for data with p features or classes is a-dimensional hyperplane.

In contrast to the SVM, a random forest (RF) is an ensemble approach that combines
the predictions of several predictors to enhance its performance. The final prediction is determined by the mean of all individual predictions. Typically, an ensemble approach outperforms its constituent predictors, particularly when the constituent predictors make diverse errors. Random forests consist of decision trees that have been trained on random subsets of samples and features to increase their performance.

K-Nearest neighbor (KNN) is one of the simplest machine learning algorithms based on the similarity between a new case and pre-existing cases. It places each new case into a category that is most similar to the pre-existing categories. The KNN algorithm stores all available data and classifies a new data point based on its similarity to the existing data. This implies that as new data becomes available, the KNN algorithm can readily classify it into an appropriate category. KNN is a non-parametric method, hence it does not make assumptions about the underlying data.

In Fig., the results obtained from three classifiers are shown, namely, (i) K-Nearest Neighbor (KNN), (ii) Support Vector Machine (SVM) and (iii) Random Forest (RF) classifier. These classifiers are trained withdatasets and tested oncases, all with equal contributions from each. We use aconfusion matrix for a graphical representation of the performance of the classifier, shown in Fig.(c-e). A confusion matrix shows a comparison between predicted IDs and actual IDs, with the diagonal elements representing correct predictions (=No. of instances with predicted ID=actual ID) and the off-diagonal elements indicating the wrong ones (=No. of instances with predicted IDactual ID). It can be seen in Fig.(e), that the Random Forest classifier presents the best results with an accuracy () of, while KNN and SVM produce an accuracy ofandas shown in Fig.(c) and (d), respectively.

So far, we have considered coherent dynamics in the system by setting the decoherencein Eq. (), usingin Eq. (). Now, in order to analyse the robustness of classification towards environmental noise, we systematically vary the decay parametersin the system to induce controlled decoherence of mean strength, given by

Here,are all decoherence rates in the black box, acting asand discussed in more detail in appendix. In the expression above,denotes the average over all pairs of sites in a single realisation. We then use the scaling parameterto conveniently tune the mean decoherence strength relative to other energy scales. In sectionwe shall employ a fully realistic model instead.

By comparing Fig.(a) with Fig.(b), we can see how the input dataset is modified by decoherence at rateMHz form. Each class now results in a more significantly distinct input dataset. Therefore, nearMHz, the random forest classifier in Fig.(d), even shows a slight increase in accuracy of up tocompared toat, for a box-length. This however breaks down at extremely high decoherenceMHz due to identical inputs form andm, see appendix(Fig.). A similar classification problem has been investigated in ref.for atoms arranged in pre-fixed configurations, but here we extend this to a more complex scenario, by randomising the distribution of Rydberg atoms across theplane. In addition, we incorporate long-range dipole-dipole interactions with decoherence and limit the datasets to the information from two output atoms at a given time instead of the complete time-evolution of the wavefunction.

SECTION: Regression: quantum network reconstruction
The second stage of the pipeline utilises a multi-target regression algorithm to locate the Rydberg atoms in thereg,ion. We use the input datasets obtained from the two output atoms, as shown in Fig.(b) and Fig.(a-c), along with the information about the number of atomsgained by the previous stage to separately tackle each. The artificial neural network is then trained withdatasets, also including the known positions of the Rydberg atoms within the black box. This enables it to later predict the coordinates of the Rydberg atom at the output of the neural network for new unseen test data. Since eachrequires the prediction of a different number of coordinates, we employ neural networks with a different number of output neurons for each.

To assess the performance of the neural network, we consider the mean relative error () defined as

whereis the number of Rydberg atoms in the box andis the mean distance between the Rydberg atoms inside the box.is the actual position of theth Rydberg atom in theth dataset, whileis the position predicted by the neural network.
Since predicted positions are not labelled, we allocate the one closest to theth Rydberg atom. For the single atom case,, we switch the assessment parameter from the mean relative error to the mean absolute error () defined as.

Altogether, we train the multi-target artificial neural network withdatasets for each, including decoherence ranging fromMHz toMHz, relative to a mean strength ofMHzm3for. Hereis the average over all site indicesand all realisations. We show the variation of the errors(and) with respect to decoherence strengthin Fig., for three different box lengthsm. For this we usedtest datasets in each case.
Training and testing is done separately for each surveyed decoherence strength.
The horizontal black dot-dashed lines in (b-d) indicate the worst expected, corresponding to a completely random prediction. This limit is determined by averaging overrandom and uncorrelated realisations of both, “fake predicted” and “fake actual” atoms:

whereandare sorted based on proximity of theth atom andth atom in theth andth dataset, respectively, as discussed below Eq. (). It is then clear that one should consider cases withas complete failure of position reconstruction.

Compared to the classification algorithm in Fig., the regression algorithm breaks down at a lower threshold decoherence rate. By threshold for breakdown, we refer to those values ofwhere theapproachesin Fig.. We also observe an earlier
threshold where thestarts to increase from its initial low value that is independent of. This appears to be set by the mean strength of the dipolar interactions in the system,, shown with vertical dashed lines in Fig.for different box-lengths. Reconstuction worsens as soon asexceeds this scale,. We discuss the reasons for this in appendix(Fig.). Furthermore, the increase of the minimal error fromin Fig.(c) toin Fig.(d) upon increasing the number of atoms inside the box fromtoindicates that our neural network architecture is challenged more strongly by larger systems. However we find that these errors can be again reduced by increasing the training set size.

SECTION: Regression: system and environment
We now adapt the artificial neural network developed in the previous section to a new set of objectives: predicting the matrix elements of the system-environment interaction Hamiltonianand Lindblad operatorof the system, described by Eq. () and Eq. (), respectively in appendix. In the companion article, we suggest a more general approach, for which we demonstrate a realistic experimental platform for demonstrations here, based on Rydberg array. However, instead of predicting a completely arbitrary Hamiltonian and decay operators as in, here, we work around a back-bone, which depends on randomized positions of atoms and can be obtained in the previous step, while only correctionsandcapture system-environment interactions.

For a realistic source of these corrections, the quantum network is considered to be immersed in a cloud of background atoms that can be optically manipulated to generate controllable decoherence for the transport of the-excitation, as discussed in.
In essence, quantum non-demolition (QND) measurements by the gas environment cause decoherence and energetic disorder.
Interactions causing this are artificially induced and thus controllable. Theanddepend on the random positions of background gas atoms and parameters of optical fields, as summarized in appendix. This results in a wide range of values for Hamiltonian matrix elements and dephasing rates, with which one can test and validate our neural network model.

The matrix elements and dephasing rates are randomised through two experimentally accessible handles, the random Rydberg atom locations within the box and a probe light Rabi frequency(see appendix) that is uniformly randomized in the rangeMHz.
The results obtained are shown in Fig.(a) and (b), where the blue dots have coordinatesand, respectively, such that dots closer to the diagonal indicate better results (. In Fig.(c-d), we show the distribution of mean relative error () across the range of energies inand similarly for, which demonstrates that the neural network can predict the matrix elements of the operators with a mean relative error of, consistent throughout the energy range. In, the performance of the neural network has been tested on completely unconstrained Hamiltonians and decay operators, with results suggesting that the neural network can be generalized for a complete reconstruction of arbitrary quantum network.

SECTION: Conclusion and outlook
We have demonstrated quantum network tomography of simple quantum systems using a machine learning algorithm. We designed a multi-branch pipeline, aiming to provide key information about a quantum network and the strength and character of disorder and decoherence in the system. We apply this technique specifically to an embedded Rydberg aggregate model, focussing on realistic parameters that should be accessible in state-of-the-art experiments. The first stage of our algorithm successfully classified the number of atoms in the Rydberg aggregate and thus the number of nodes in the network.
The second stage contained a separate branch, each handling a different number of nodes.
Here, an artificial neural network predicts the position of the atoms and details of disorder and decoherence. We have demonstrated that it can successfully do so, based on measurements of the probability for an excitation to be transported through the network and reaching a single target output site. This information is only required for a single snapshot in time.

Future work should validate the utility of our machine learning algorithm by combining theory with state-of-the-art experiments, determining the essential properties and characteristics of the system based on experimentally measured data, using a theory trained neural network. A similar approach could also be leveraged towards quantum state tomography of Rydberg arrays, where we reconstruct a complex entangled quantum state initialised initially through the resultant transport characteristics when coupled to a larger system, based on a similar
neural network architecture. Ultimately, the applications of our multi-branch pipeline may extend to quantum network tomography on complex molecular systems like photosynthetic molecular aggregates or quantum devices like SQUID based quantum computing architectures, where the approach may provide key information about complex structures and system-environment interactions.

SECTION: Controllable decoherence and disorder in Rydberg array
To explore a setting where artificial controllable decoherence can be set in experiments to test the neural network reconstruction of it, we assume that the EIT is selectively implemented on the background gas in the black-box region.

One can encapsulate all the system-environment interactions involving van der Waals interactions and EIT into a set of effective operators in the Rydberg state-space, which can take the form

whereaccounts for the van der Waals interactionswithfor, respectively. Hereandindicate the position of theth Rydberg atom and theth background gas atom, respectively.andare probe and coupling Rabi frequencies of EIT,. For simplicity, we predict an effective decay rate from the algorithm which can be approximately written as.

The master equation obtained from Eq. () by insertingandcan be written in the form

whereare dipole-dipole interactions defined in Eq. ()
and,can be derived as follows

In the generation of each data set in section, we randomise the EIT parameterin the rangeMHz along with the Rydberg atomic positions, resulting in a wide range of values in the operators for testing and validating our neural network model. Note that prior to section, we use artificially engineered mean decoherence rates with scaling parameter, which are not directly governed by Eq. () and ().

SECTION: Design of cold atom benchmark platform and data collection
The following conditions constrain how we distribute Rydberg atoms inside the: (i) the minimum distance between any two atoms is kept atm (e.g. the blockade radius forand excitation laser linewidthMHz, and (ii) the atoms are distributed evenly in the box, for example if, the box is divided into four quadrants with each atom distributed randomly in one of the quadrants.

We place the first output atom () at 20 equidistant locations along the rim depicted in Fig.(a) and 10 successive positions radially outwards withm for each site on the rim to generate a dataset of 200 data points, as a part of the input to the neural network. In order to position the second output atom, we choose 20 evenly spaced locations atwithranging fromtowith, as well as 10 successive positions on each of those points along theaxis withm. The two output atoms are moved simultaneously, and generates a combined input data set of 400 data points from the two output atoms, where the first 200 points corresponds toand the next 200 points relates to, which are shown in Fig.(b). Finally, using a set of machine learning classification algorithms, we classify the system according to the number of Rydberg atoms in the network using the datasets. For each example ofm,m, andm, we scan through differentand found an optimal time sample for the training of the neural network ats,s, ands, respectively.

SECTION: Design of Artificial Neural Network
Once the network is classified based on the number of atoms as described in section, in the next stage, we use artificial neural network with multi-target regression to predict the location of those atoms. In general, a artificial neural network consists of three sections, for which we are using the following specifications:

Input layer: Our input layer consists ofneurons, where the measurement data of Rydberg-excitation on the output atoms,forare provided, obtained as discussed in appendix.

Hidden layer: We have typically includedhidden layers, where theth layer in the forward direction hasneurons. All the neurons are connected to each other with dropout = 0, and activated using Rectified linear unit (ReLU) activation function. We have used mean squared error (MSE) as our loss function and Adaptive Moment estimation (ADAM) as optimizer.

Output layer: We vary the number of neurons at the output layer based on data type required for the prediction. For instance, to predict the location of four atoms inside the 2D box, there arecoordinatesconstitutingreal numbers, requiringneurons in the output layer.

The neural network configured like this was trained withtraining datasets in all cases for typicallyepochs (iterations for the loss function optimisation during training).
To prevent underfitting or overfitting, the number of epochs was adjusted from that value case by case.

SECTION: Effect of decoherence on populations
In section, we have seen that there is a critical level of decoherence beyond which the neural network no longer performs. Here, we examine the cause of this. In Fig.(a-d), we can see that the coherent excitation transport approaches a steady state with an increase in decoherence. Furthermore, the amount of excitation passing through the boxed atoms decreases, see the green dashed line in Fig.(a-d), making it impossible for us to gather any relevant information regarding the location of atoms. As a result, each simulation generates a similar dataset, and the neural network can no longer accurately locate the atoms. However this is not the case for classification, as can be seen from the slight increase in performance of the Random forest classifier in Fig.(d), until a certain regime because the variation in atom number results in a different steady state for each case.
Additionally, we can see in Fig.(e-f), at extremely high decoherence, for example, atMHz, how different numbers of atomscan result in identical input datasets, such asandin Fig.(e) and-in Fig.(f), which causes the drop in the accuracy due to degenerate input datasets for classification algorithms.

SECTION: References