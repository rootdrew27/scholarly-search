SECTION: Unsupervised Learning Approach to Anomaly Detection in Gravitational Wave Data

Gravitational waves (GW), predicted by Einstein’s General Theory of Relativity, provide a powerful probe of astrophysical phenomena and fundamental physics. In this work, we propose an unsupervised anomaly detection method using variational autoencoders (VAEs) to analyze GW time-series data. By training on noise-only data, the VAE accurately reconstructs noise inputs while failing to reconstruct anomalies, such as GW signals, which results in measurable spikes in the reconstruction error. The method was applied to data from the LIGO H1 and L1 detectors. Evaluation on testing datasets containing both noise and GW events demonstrated reliable detection, achieving an area under the ROC curve (AUC) of 0.89. This study introduces VAEs as a robust, unsupervised approach for identifying anomalies in GW data, which offers a scalable framework for detecting known and potentially new phenomena in physics.

SECTION: IIntroduction

Gravitational waves (GW) were first predicted by Einstein following his formulation of a general theory of relativity (the theory that describes space and time characteristics given the distribution of energy (and equivalently matter) and momenta). The existence of GW was experimentally confirmed a century later in 2015 by the Laser Interferometer Gravitational-Wave Observatory (LIGO). The first detection of a GW was the event GW150914 which was a result of the merger of two binary black holes. Following this discovery, the LIGO collaboration was awarded the Nobel Prize in Physics in 2017.

The discovery was remarkable for many reasons. Notably, gravitational waves may propagate unimpeded across the cosmos, unlike their electromagnetic counterpart which can be absorbed and scattered by matter. This property allows us to gain information about cosmological systems. It also enables physicists to test and expand the limits of our best understanding of gravity.

In this work, we propose a novel unsupervised learning approach to detect anomalies in gravitational wave time-series data. We propose to use autoencoders (AE) which are deep neural networks trained to minimize a measure111SeesectionII.3andIIIfor example of distances.of distance between its input and output. The method used in this paper relies on the central idea that if we train an AE on a stream of homogenous data (the noise of the detectors), AE will be able to reconstruct the inputs almost perfectly should they be of the same nature as the training data. However once an anomaly (GW event) occurs in the data, the AE will fail to reconstruct it correctly since it never was included in the training. This “fail-to-reconstruct” result can be quantitatively assessed to decide whether an anomaly is presentZhou and Paffenroth (2017); Ibrahim and Fayad (2022); Finkeet al.(2021).

SECTION: IITheory

This section provides a brief overview of General Relativity and Machine Learning in the context relevant to our experiment.

SECTION: II.1General Relativity

The standard formulation of General Relativity is encoded in the Einstein’s Field Equation (EFE):

whereis called the Einstein Tensor and it describes the spacetime geometry (or curvature as it is comprised of second derivative terms of the Riemannian metric). On the other hand,is the stress-energy tensor that describes the distribution (i.e. density and flux) of energy and momenta in space and time. In the weak-field limit (i.e. the spacetime is almost flat), EFE become a wave equation

whereis the strain tensor and describes the small perturbation to the flatness of the spacetime geometry. The above is a wave equation due to the d’Alembertian acting on. The solutions of this equation are known as the gravitational waves with strain.

SECTION: II.2The LIGO detector

The LIGO detectors (fig.1) are designed to measure the strain, that is caused by passing gravitational waves (GWs)(Abbottet al.,2016). Using laser interferometry, these detectors measure relative changes in the lengths of two perpendicular arms with great precision, i.e. strains as small asmeters. The detection mechanism operates as follows: a laser beam is split and directed along the two arms, where it reflects off mirrors and recombines at the beam splitter. In the unperturbed conditions, the equal lengths of the arms result in no net interference at the photodetector. However, when a gravitational wave, with a plus polarization for example, passes through, it induces an alternating contraction and dilation of the arms. This creates a phase difference in the laser beams when they recombine, leading to an interference pattern with an intensity proportional to the strainof the GW. For this study, we analyze strain time-series data, which is publicly available from the Gravitational Wave Open Science Center (https://gwosc.org/). Refer tofig.2that shows the processed strain timeseries data of the event GW150914. Further details on the processing can be found insectionIII.1.

SECTION: II.3Machine Learning

We train a deep autoencoder (AE) neural network to detect anomalies. In the simplest form, Neural Networks are statistical learning architectures designed to approximate relationships within data; the main distinction between NNs and simple models such as linear regression is the model complexity where NNs process data sequentially with layers, each consisting of a neuron (or unit) that applies an appropriate transformation on its input. The basic AE model trains on data of the formand learns a functionsuch that. In statistics terminology, the AE learns parameterssuch thatis minimized. AE’s main assumption is that the input data can be modeled using a low-dimensional latent variable. Following this assumption, AE’s architecture is as follows: Given input dataof dimension, find the latent representationwhich has a lower dimension. Then, outputwhich has the same dimension as the input. In summary. AE = Encoder + Decoder, which are two separate neural networks.

SECTION: IIIProposed Methodology

Infig.3, we illustrate the performance of an autoencoder (AE) using a toy example. The first two rows of column (a) show the input data used for training, and the corresponding reconstructed output is displayed in column (b). In the third row of column (a), we introduce artificial anomalies (colored red) into the input data. One notes that AE’s reconstruction in column (b) shows significant deviations in regions corresponding to the anomalies. Computing the point-wise squared difference between the input and reconstructed outputs reveals distinct peaks at the locations of the anomalies.

VAE-GAN.The toy example provides a proof of concept that AE are good candidates for data with possible anomalies. In order to implement AE in the context of GW time series data, we propose an improved model based on variational autoencoders (VAE)Kingma (2013). VAE are probabilistic models and their main advantage is anomaly detection even when the signals and the noise have the same mean (in which case the original AE might fail to perform well).

Similarly in our introduction of AE insectionII.3, VAE learns to reconstruct input databy maximizing the expected log-likelihoodw.r.t.. The process proceeds as follows: Given input data, generate parameters. Then sample the latent variablefrom222Our prior foris. For numerical stability in gradient learning, one can apply re-parametrization trick to generateof the posterior distributionby first sampling, then computingwhereis an element-wise multiplication. Full details are to be found in(Kingma,2013).. The decoding procedure is that givenwe learnby the same trick as before: we learn parametersand sample. Both the Decoder and Encoder are neural networks parameterized by(i.e.) and(i.e.) respectively. Recall that the training objective is maximizing the log-likelihood which can be shown to be equivalent to maximizing the following expression:

whereis the KL divergence that measures distances between probability distributions, andis our fixed prior for.

SECTION: III.1Training Details

The training/validation set containedsamples of noise-only data. The train/validation split wasrespectively. A sample is a 4-second data at 4 kHz for each detector (we considered L1 and H1 detectors only as data from V1 detector was limited). Each sample then is aentries concatenated into a one-dimensional array. With a sliding window of sizeandoverlap, we generateinputs each of sizefor each sample.

The data were whitened using a Fast Fourier Transform (FFT) to remove correlations in the noise(Cuocoet al.,2001). A band-pass filter (20-400 Hz) eliminated noise outside the interferometer’s sensitivity range.

Shown infig.4, the encoder consists of two LSTM (Long short-term memory units)(Hochreiter,1997)layers of sizes 32 and 8 in that order. The final output of the LSTM units in the second layer is passed to two separate layers to produceandrespectively, each with dimension. The decoder is two LSTM layers of sizes 8 and 32 respectively followed by layers to produceandof the same dimension as the input. The reason for choosing LSTM is their computational design to learn temporal dependencies. Other excellent choices may include temporal convolutional transformers. Learning was performed with Stochastic Gradient Ascent on the objective ineq.1.

SECTION: IVResults

Testing data are a mixture of samples containing a signal and samples of pure noise. With the same procedure as insectionIII.1, we obtain the signal samples from events: GW {150914, 170104, 170817, 190412, 190425, 190521, 190814, 200105, 200129, 200115}.

Infig.5, we show the performance of our VAE on Event GW150914 using data from both L1 and H1 detectors. Note that the squared distance between the input data and the VAE output (denoted by loss on the y-axis) is approximately stationary and close toin the existence of regular noise. However, once the GW passes through the detectors, a spike in the loss manifests, indicating the presence of an anomaly.

For input data, we declare them anomalous if the anomaly score is above a certain threshold; the anomaly score is simply the distance between the input of the VAE and its output. For each threshold, we compute the true positive rate (TPR, or recall) and the false positive rate (FPR) for our testing data. By varying the threshold we obtain the Receiver Operator Characteristic (ROC) curve. The thresholds are found using the python commandsklearn.metrics.roc_curve. It is well-known in statistics that the area under the ROC curve (AUC) is a measure of the goodness of a classifier, where a higher value (e.g.) indicates a better performance.Figure6illustrates the ROC curve on the 10 GW event data and 10 noise-only data. The AUC was found to be. We also report the F1 score to be, which is the harmonic mean of the precision and recall (the higher the better).

SECTION: VConclusion

This study introduced an unsupervised learning approach to detect anomalies in gravitational wave time-series data, utilizing variational autoencoders (VAEs). The method demonstrated robust performance in identifying anomalies, such as GW150914, by detecting spikes in reconstruction error. The method’s performance was assessed using ROC analysis with an AUC of 0.89. To our knowledge, this is the first work to use VAE in the context of astrophysics applications. Due to time constraints, we were not able to perform further studies (e.g. comparison to(Raikmanet al.,2023)). We will leave that to future work.

The proposed method could be used in different scenarios due to its unsupervised nature to detect possibly new physics in certain datasets with no given templates.

SECTION: References