SECTION: QiBERT - Classifying Online Conversations Messages with BERT as a Feature

Recent developments in online communication and their usage in everyday life have caused an explosion in the amount of a new genre of text data, short text.
Thus, the need to classify this type of text based on its content has a significant implication in many areas.
Online debates are no exception, once these provide access to information about opinions, positions and preferences of its users.
This paper aims to use data obtained from online social conversations in Portuguese schools (short text) to observe behavioural trends and to see if students remain engaged in the discussion when stimulated.
This project used the state of the art (SoA) Machine Learning (ML) algorithms and methods, through BERT based models to classify if utterances are in or out of the debate subject.
Using SBERT embeddings as a feature, with supervised learning, the proposed model achieved results above 0.95 average accuracy for classifying online messages.
Such improvements can help social scientists better understand human communication, behaviour, discussion and persuasion.

SECTION: 1Introduction

Influenced by social networks based on short and fast content such as Twitter and Tiktok, the digitalized post-pandemic school can adapt, emulating these types of networks and motivating the participation of students in the discussion of current topics[1].
One of the possibilities is the use of multi-participant chat, a form of chat with several participants talking synchronously through textual communication[2].
Chats, and their integration with teaching, have already been studied[3,4]Despite their implementation advantages, these are increasingly being incorporated into the range of teaching tools, and it is important to know whether or not students are engaged with the themes proposed.

Natural Language Processing (NLP) tools can assist in the analysis and even in the classification of these data as useful or not useful. Recently, several studies have investigated the classification of short texts[5,6,7,8,9].

However, conventional text classification tools are not directly suited to this type of medium with short texts[2]. This inadequacy is mainly due to the characteristic difference between the two types of text. Short texts mainly present sparsity, ambiguity, shortness and incompleteness.

In general, those studies follow the conventional classification pipeline containing four levels: Features extraction, Dimensionality Reduction, Classification Techniques and Evaluation[10].

For this study, we intend to replicate this pipeline and focus on the analysis of chat conversations in order to understand whether or not the students are talking about the subject they were stimulated.

The goal is to be able to classify the messages as “on the subject” or “off the subject”. For that, We will use multilingual BERT models[11,12]trained in multiple languages, including Portuguese (European), which will be applied to sentence units via SBERT[13]. It is expected that these models can effectively capture the semantics of the analysed messages with the least amount of training data possible.

This paper is organised as follows: Section II describes the state-of-the-art (SoA). Sections III and IV will present the characteristics and the way in which the analysed data were acquired, including the annotation procedure. In section V the proposed method will be presented, in section VI the results are shown discussed and in section VII the conclusions and the future work are presented.

SECTION: 2Contribution to Connected Cyber Physical Spaces

Online debates are crucial to providing important data for the interpretation and classification of ML models. In this study, it was analyzed whether a given debate subject was maintained throughout the conversation, and to this end, different feature extraction models and ML algorithms were studied to classify whether or not the subject in question was discussed during the conversation. The presented work focuses on Connected Cyber Physical Spaces, especially on intelligent NLP models developed through ML algorithms.

SECTION: 3Related Work

Online conversations come in many different formats.
There are studies on data such as discussion forums[14], specific messages from platforms such as Facebook[15,16], or Twitter[7].
Although all these studies involve short texts, they have significantly different structures.
Replies to a tweet may come shortly after it is posted, but they can be made days later.
Discussion forums can last for years and have features like quotes and replies.
Among these and other differences, this paper focuses specifically on chats, where all participants in the conversation are simultaneously exposed to a virtual environment to discuss, in our case, the topic of racism.

The analysis of multi-participant chats, their problems, and their relationship to computational techniques has been widely studied[2].
Computational models can even help social studies through Conversation Analysis (CA)[17,18,19].
However, for this, it is crucial to understand what the participants are talking about in order to improve the reading and perception of the messages sent.

We intend to explore the classification of chat messages.
Short texts in chat rooms could have a few words, presence of abbreviations, spelling errors, or texts being supplemented in subsequent messages.
All these characteristics, in addition to other factors, make feature extraction difficult. As a solution, the authors complement the short text with external knowledge.
Liu[20]used external knowledge to enrich the semantic representations to develop a model based on TCN and CNN.
Hu[8], augmented the vector representations of the text by combining information from the message actors to generate mental features.

Danilov[6]proposed 27 parameterised PubMedBERT options and new models for classifying academic texts.
There was also the use of BERT to classify political discourse[21]in short texts on Facebook and Twitter.
With the application of BERT and other vector representations (Glove), Khatri[22]used binary classification to classify sarcasm in tweets.
BERT was also used to create a graph convolutional network for classifying short texts[23].

Motivated by the discussions above, in this work we aim to classify text messages present in a chat using conventional classification techniques and contribute to the discussion as follows:

It is possible to classify texts from chat messages, even if they only have short features (short texts);

With a small amount of training data, supervised learning models have high accuracy;

Using pre-trained BERT models in combination with the sentence embedding framework (SBERT) to train a robust sentence classification model.

Use of feature selections to reduce the dimensionality of the model inputs

SECTION: 4Data Gathering

The data for this research was collected from instant multi-participant messaging chat under the project ”Debaqi - Factors for promoting dialogue and healthy behaviours in online school communities”. Users were placed to debate in a private virtual environment and interactions were synchronous where any participant can contribute to the conversation at any time.

The online conversations took place in a virtual environment involving Portuguese state high schools. There were 25 rooms, with 309 participants. The messages sent are predominantly short-text and have a median of five tokens (Table1). The participant’s ages were between 15 and 19 and we obtained previous consent from their parents for them to participate in the chat room debates. The students may or may not know each other and the chat application guarantees the anonymity of participants. Platform anonymity means that participants know that something was said by a particular user, but they do not know who the user is in the school context.

At the beginning of the conversation, students were stimulated through a video and the moderator also contributed through questions launched at a given time according to a moderation script. There is no way to set a certain conversation path or set a certain topic, so there is the possibility of students following the theme, changing the theme, creating sub-themes or even ignoring the proposed theme in order to boycott.

SECTION: 5Annotation

In supervised models, as foreseen in this work, the classification model demands annotated data.
The most convenient way to generate this annotated data is to use annotators that do it manually.
It is important to define an annotation method that guarantees good inter-agreement[24,25]between annotators and that can reliably transmit the annotated data to the classifier.
The annotation criteria used in this work were:

Label 1 - Messages that were about the topic/subject“Racismo e Esteriótipos”. Sentences containing words such as ”racism”, ”racist”, ”stereo-types”, ”culture”, ”prejudice”, ”black/white” were considered, as well as sentences like ”we are all equal/human”;

Label 0 - All messages that do not have a defined subject like greetings (”good morning”, ”hi”…), agreements/disagreements (”yes”, ”no”, ”agree”, ”disagree”, ”maybe”). All messages that have a defined subject, but are not directly linked to “Racismo e Esteriótipos” topic.

In a pilot annotation phase, only two rooms were randomly chosen and assigned to 3 annotators, where we obtained an average inter-agreement above 0.7 of Krippendorff’s alpha as expected[24].
Therefore, the simple annotation criteria proposed was well understood among the annotators and can be implemented in the total pool of rooms. Despite having access to the entire conversational sequence of messages, annotators do not consider the context.
There may be messages that talk about racism or that were related (reply or quote) to a message about racism but do not necessarily have words that cite the topic directly.
In this case, they were not annotated as messages of racism.
In the second annotation phase, the other 23 rooms were submitted to the same three annotators. The averageKrippendorff’s alphavalue for the three annotators, at the 25 rooms, was 0.77.

SECTION: 6Proposed Method

In this section, we describe the methods used to build the models for the online text classification task.

SECTION: 6.1Feature Extraction

In the model-building process, feature extraction is crucial.
Word and sentence embeddings are commonly used to represent language features in the field of Natural Language Processing (NLP)[26,27,28].
Sentence embedding refers to a group of feature learning techniques used in NLP to map words or sentences from a lexicon to vectors of real numbers.
For the feature extraction stage of our study, we used embeddings from a pre-trained model111SBERT Model:paraphrase-multilingual-mpnet-base-v2. Multi-lingual model of paraphrase-mpnet-base-v2, extended to 50+ languages.https://huggingface.co/sentence-transformers/paraphrase-mpnet-base-v2from SBERT framework[13].
SBERT outperformed the previous (SoA) models for all common semantic textual similarity tasks since it produces sentence embeddings, so there is no need to perform a whole inference computation for every sentence-pair comparison.
This framework is a useful tool for generating sentence embeddings where each sentence is represented as a size 768 vector (Figure1).
This embeddings are based on BERT[27], so they are contextual.

To be possible to visualise the sentences embeddings, UMAP[29]was used to reduce the vector’s 768 dimensions for two.

Once the embeddings were extracted for the training data, the sequence of embeddings was ready to feed the machine learning models.

SECTION: 6.2Training and Predictions

After extracting the sentences embeddings we followed into two different approaches:

We trained the six ML algorithms, mentioned above, with the raw embeddings (768 features);

A feature selection method[30]was used in order to remove the less important features, before training the algorithms.

For the first approach, we follow the dashed path of the pipeline (Figure2), where after extracting the embeddings, we move directly to the classification algorithms.

For the second approach, we have an intermediate step, before moving towards the classification algorithms stage, that aims to select the most important features (blue path of pipeline), in order to try to reduce memory usage as much as possible and maintain the proposed system performance.
For that purpose, given the sentences embeddings, created before, known asand a target (annotation), a random vectorwas created and appended as a new feature of X:.
Now, with that data (X’, Y), the next step was to train a Supervised Learning algorithm with with a relevant feature importance measure (Figure3).

Generally, importance provides a score that indicates how useful or valuable each feature was in the model’s construction and is calculated explicitly for each attribute in the provided dataset, allowing attributes to be ranked and compared to each other.
Thus, XGBoost[31]was used in order to calculate the importance.
If a given feature has a lower feature importance than the random feature, we set it as useless feature and therefore removed it from the original sentences embeddings[30].
We ran 1000 Monte Carlo (MC) simulations and achieved, on average, around 85% of feature reduction from the original dataset features (embeddings).

The next step was to train different machine learning models to create a classifier that can predict the class of test samples.
Classifiers such as Logistic Regression (LR), Support Vector Machine (SVM), Gaussian Naive Bayes (GNB), Bernoulli Naive Bayes (BNB), K-Nearest Neighbours (KNN), XGBoost (XGB), and Multi-layer Perceptron (MLP) were used[32,33,34].
Scikit-learn library was used to train these models.
Sentence embeddings were obtained for the test dataset in the same way as mentioned before. This way, they got ready for predictions.

SECTION: 7Experiments and Results

The first developed experiment was directly related to the data annotation.
Since there were three annotators, it was expected that they would not always agree, and for that reason two different models were investigated:

A model in which all three annotators agreed (Complete Agreement (CAg)).

A model in which at least two of the annotators agreed (Majority Agreement (MAg)).

For the CAg model there was a total of 2334 messages annotated in concordance, from which only 17% were annotated with a value “1” that indicated that the students sentences were explicitly addressing the subject of “Racismo e Esteriótipos”, as explained before.
For the MAg model, there was a total of 3727 annotated messages in which at least two of the annotators were in agreement. 18% out of those had been annotated with the value “1”.
Thus, in order for the data not to be biased, only 790 sentences were used for CAg model training and 1300 for MAg model training. This way, around 50% of the messages would have been annotated with value “1” and the remaining 50% were annotated with zero.

For this experiment, the ML algorithm choice was not the true focus. The main goal was to realise that annotated data, in which three of the annotators were in agreement, obtained better results than with only two annotators in agreement.
SVM was the selected ML algorithm and the training and testing sizes were set to 66% and 33%, respectively.

As it can be seen in Figure4, we can intuitively perceive that the model CAg was much superior in the evaluation metrics chosen by the authors, than the MAg model.
The former obtained an average of 0.96 F1-score, whereas the latter got 0.88 F1-score.
This is understandable because, in the training phase, CAg only puts a sentence at 1 when all annotators agree, while MAg connotates 1 when the majority agrees. Therefore, there is much more noise in MAg than in CAg.

The second experiment performed a comparison between several ML algorithms in order to understand which one or ones would get better results in classifying short messages.
For this purpose, it was defined that the CAg annotation reading model would be used, along with testing the two sentence embeddings possibilities (raw sentence embeddings and feature reduced sentence embeddings).

As we can see in both Figure5and Table2, after 1000 MC’s simulations, the values of the two models are similar.

It is also visible that all tested algorithms obtained average results above 0.93 for all (accuracy, precision, recall, f1-score) evaluation metrics and that the most prominent algorithm, for both approaches was the Support Vector Machine.

The first approach had no embedding reduction, whereas the second one got embeddings randomly reduced. They both turned out with a median and average f1-scores values of 0.956 and 0.955, respectively.

In Figure6, it is possible to compare the behaviour of these two approaches (embeddings with and without reduction) for the SVM algorithm over the 1000 MC simulations.

After reaching such positive results for the randomly selected features model, we studied how the amount of training data in a estimator would affect its results.
For this matter, the CAg annotation reading model was selected, as well as SVM algorithm that had obtained the best results.
We determined a training dataset minimum and maximum of 5% and 95%, respectively, and obtained the results illustrated in figure7.

We can observe that, even with a low amount of training data, we achieved optimal results regarding the classification of text messages. We also obtained an average f1-score results of 0.94 for training data between 15% and 25%.

Finally, in order to evaluate the model, we performed cross-validation[35].
Cross-validation is used to evaluate the performance of the estimator and allows the model to learn and be tested on different data.
This is important because a model that simply repeated the labels of the samples it had just seen, would produce a perfect result, but it would not make useful predictions for data not yet seen.
Thus, we defined, once again, that the annotation reading type would be CAg.
The ML algorithm would be SVM and that the sentence embeddings features would be randomly selected, as previously explained.
The number of re-shuffling and splitting iterations was set to 10 and the number of training data to 20% of the dataset.
With scikit-learn library’s help to calculate the cross-validation, we obtained the average result of 0.95 f1-score with a standard deviation of 0.01.

SECTION: 8Conclusion and Future Work

Our study demonstrates that it is feasible to use SBERT as a feature for classifying short messages in online chat conversations.
This research aims to aid social science researchers and educators in gauging the level of engagement of online chat participants on a particular subject.
Although our pipeline was developed using only one theme, we believe that it has potential for incorporation into future work, in other subjects.
Our results suggest that utilizing BERT-based techniques to classify online chat room messages from online conversations can considerably enhance machine classification outcomes.
Additionally, we have demonstrated that reducing the number of embeddings features by approximately
85% can produce similar outcomes to training algorithms with raw embeddings with 768 dimensions.
Lastly, we have proven that by training machine learning algorithms with a smaller percentage of training data (approximately 20% of the dataset), we can achieve results that surpass our expectations: an average f1-score of 0.94 with a standard deviation of 0.01.

This work will be continued with further developments on the modules presented in this paper.
Other techniques, like Deep learning are very promising in terms of further supporting social scientists in better understanding human communication and persuasion in online chat rooms.
Although its use may have significant limitations in a few categories, the overall advances are encouraging.
As guidelines for future work, the list below enumerates some of the main topics that will provide
novel contributions:

Chat rooms messages temporal analysis with deep learning temporal networks.

Turn shift analysis, during the debate.

An increase in the dataset size for other ages and data types sources.

SECTION: Acknowledgment

This research was partially funded by Fundação para a Ciência e a Tecnologia under Projects ”Factors for promoting dialogue and healthy behaviours in online school communities” with reference DSAIPA/DS/0102/2019 and developed at the R&D Unit CICANT - Research Center for Applied Communication, Culture and New Technologies, UIDB/04111/2020, UIDB/50008/2020 as well as Instituto Lusófono de Investigação e Desenvolvimento (ILIND) under Project COFAC/ILIND/COPELABS/1/2022.

SECTION: References