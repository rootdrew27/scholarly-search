SECTION: Generating Visual Stimuli from EEG Recordings using Transformer-encoder based EEG encoder and GAN

In this study, we tackle a modern research challenge within the field of perceptual brain decoding, which revolves around synthesizing images from EEG signals using an adversarial deep learning framework. The specific objective is to recreate images belonging to various object categories by leveraging EEG recordings obtained while subjects view those images. To achieve this, we employ a Transformer-encoder based EEG encoder to produce EEG encodings, which serve as inputs to the generator component of the GAN network. Alongside the adversarial loss, we also incorporate perceptual loss to enhance the quality of the generated images.

SECTION: 1Introduction

Brain decoding entails extracting insights regarding emotional states, abnormal brain functions, cognitive functions, or perceptual stimuli from typically non-invasive assessments of brain activity. Electroencephalography (EEG) serves as a prevalent and cost-effective method for measuring electrical brain activity[1]. While historically prevalent in clinical research, the analysis of EEG signals has expanded in recent years, particularly with the rise in popularity of brain-computer interfaces for cognitive and perceptual tasks[2].

Perceptual Brain Decoding (PBD) is an approach depicted in Fig.1, leveraging brain responses evoked by diverse stimuli to discern the original perceptual stimulus, such as visual or auditory cues, or certain characteristics thereof. Generally, PBD offers advantages from both cognitive and clinical standpoints. Through PBD, diverse brain activity patterns corresponding to external stimuli can be scrutinized. In clinical settings, brain decoding techniques hold potential for communication with individuals experiencing conditions like locked-in syndrome or paralysis, which may impair motor and vocal functions. In such scenarios, attempts can be made to reconstruct the individual’s responses or imaginations while providing perceptual stimuli. Additionally, improved methods in perceptual brain decoding could benefit applications such as memory retrieval or visualizing thoughts, aiding cognitive research and rehabilitation efforts.

Compared to various other EEG-based applications, such as identifying mental states corresponding to different emotions, intentions for task execution (e.g., in motor imagery cases[3]), mental workload[4], eye movements, attention, decision-making in specific tasks[5], and stress, the exploration of perceptual brain decoding with EEG remains relatively limited. Nonetheless, there have been a few recent studies focusing on Perceptual Brain Decoding (PBD) that utilize deep learning methods applied to EEG signals for identifying or reconstructing input stimuli or associated mental imagery. While these developments are promising, given the novelty of this field, there is substantial room for improvement in method performance, the diversity of input considerations, the generalizability of methods on larger datasets, and their analysis. In this research, our primary emphasis lies on reconstructing image stimuli, specifically addressing the task of reconstructing images from EEG data. We collect EEG signals while presenting images from various categories to participants and endeavor to reconstruct images belonging to the same category as the presented stimuli based on the relevant EEG signals.

EEG signals typically exhibit noise, posing a challenge to the task of image synthesis. This challenge can result in the generation of low-quality or irrelevant images if the network fails to effectively learn the conditional mapping. Consequently, a basic generative adversarial network (GAN) architecture may prove insufficient. We aim to address these challenges by enhancing both the quality of synthesized images and their class-specific relevance, an objective that has seen limited exploration in existing literature.

In this study, we employ a combination of a pre-trained convolutional neural network (CNN) and a conditional GAN. The CNN, trained on class-specific EEG signals, serves to extract EEG encodings, which are subsequently used as input for the GAN. Training of the GAN incorporates adversarial loss as well as perceptual loss derived from an intermediate layer of an additional pre-trained image classifier CNN. To assess performance, we utilize metrics such as inception score and class diversity score. Our approach demonstrates superior performance in generating images compared to state-of-the-art methods, as evidenced by improvements in both metrics.
Our core contributions for this work are summarized as:

We propose to use a conditional GAN based architecture for EEG to image synthesis problem. The class-specific EEG encoding obtained from a pre-trained EEG encoder are used as inputs to GAN. Importantly, since the EEG signals are themselves noisy, we do not provide an additional external noise to the GAN, as is done in existing approaches, and traditionally in GAN.

In addition to the standard adversarial loss, a perceptual loss is also used to train the GAN.

We demonstrate that the above contributions lead to an improvement with respect to the class-specific relevance, as measured by the class diversity score.

SECTION: 2Related Work

Traditionally, the EEG signal was mainly used for applications like seizure detection[1]and that is the reason of the availability of great amount of literature about EEG in medical applications. Recently, the scientists also started working on different applications like perceptual brain decoding, mental load classification etc. Even though a large fraction of works based on EEG classification using deep learning mainly focus on tasks like seizure detection[1], recent research also includes applications like event-related potential detection[6], emotion recognition[7], mental workload[3], motor imagery[8]and sleep scoring[4]etc., the significant amount of literature is also available on EEG analysis using DL/ML approaches for perceptual brain decoding.

The authors in[5]briefly discuss the deep learning based important implementations and the results for the EEG classification in the applications like emotion recognition, motor imagery etc. Another attempt to the classification of emotions using EEG signals was successfully done in[7]. Here, the authors proposed CNN based deep learning framework that works on the combination of time domain and frequency domain features on the DEAP dataset. One more attempt for emotion classification using EEG signals and deep learning was proposed by the authors in[7,9]. In these works, the authors proposed the use of machine/deep learning techniques like k-NN, ANN and CNN for the classification of emotions with EEG signals. The authors in[10]worked with the combination of different deep learning architectures like CNN and LSTM for the task of EEG classification for motor-imagery tasks. One interesting applications of image annotations using EEG signals and deep learning was suggested in the work[6].

In continuation of perceptual decoding, several attempts have been made in order to classify EEG signals corresponding to different visual stimulus. One of a very recent approach that deals with the EEG classification for the task of visual perception is given by[11]. In this work, the authors proposed a deep learning network for the classification of EEG signals while the signals have been captured by Emotiv Epoc (14- channels) device. Parallel to this work, the authors of[12]also proposed a GRU based deep learning approach to classify the EEG signals from the ThoughtViz dataset[11]. The authors in[2]proposed siamese network based technique for EEG classification on Thoughtviz dataset. One more attempt for the classification of EEG signals corresponding to MNIST digits was also reported in the work by[13]This work is motivated from[11]where the authors used GAN based techniques for generating visual stimulus back from the EEG signals.

SECTION: 3Dataset Details

The dataset for this work acquired from Kumar et al.’s work[14]. This dataset contains EEG recordings from 23 volunteers who were shown 10 examples of visual stimuli from 10 different object classes from the ImageNet dataset. Few sample images of these stimuli are shown in Fig2. Each EEG signal is recorded for the duration of 10 seconds.

Tirupattur et al., 2018[11]released this dataset after dividing the EEG recordings into smaller parts with a window size of 32 samples and an overlap of 8 samples. This EEG data is collected using Emotiv EPOC headset. This dataset doesn’t contain an exact one-to-one mapping of EEG parts to particular images. However, the mapping between the EEG parts and the class of objects is available.
Hence, the task involves synthesizing some image of an object corresponding to the input EEG signals acquired when that class of images were shown[11].

The different brain locations for emotiv epoc is given in Fig.3. This EEG capturing device contains 14 channels with the sampling frequency 128 Hz. As a first step, the pre-processing of this raw EEG data is done using a sliding window of 32 samples with overlapping of 8 samples.

SECTION: 4Methodology

To generate class-specific images corresponding to the EEG signals, we use generative adversarial network[16](GAN) as the backbone of our proposed architecture. Fig.4shows the network architecture where the two additional blocks - A) EEG encoder and, B) image classifier - are used along with a traditional GAN. The description of each block is given in the subsequent subsections.

SECTION: 4.1GAN

GAN is a well-studied deep learning based network which is made up of two blocks - Generator () and Discriminator (), both are trained in an adversarial fashion. The generator generates images from the random noise input () and the discriminator tries to classify the generated image as fake or real. The adversarial learning pushes the generator to generate images that are very close to real images () by fooling the discriminator. The generator learns to produce the samples close to the real image distributionby mapping the input noise () from a known distributionto the real image distribution.
The adversarial loss function of the basic GAN network is

In this work, our task is to generate class-specific visual stimulus, i.e., the class labels of the EEG signals and the generated images should be the same. A basic GAN architecture is not sufficient for this task. Due to the inability of basic GAN network to produce class-specific images we focus on the conditional GAN paradigm where the condition is fed to the generator through input signals. Here, we give this condition as the class-specific EEG encodings (from EEG encoder, details are in section-4.2). Motivated from the AC-GAN[17]architecture, we use an additional image classifier for class-specific image generation. In this work, we observe that the EEG signals are themselves noisy in nature[18]. Hence, their encodings (details in the subsection 4.2), can be seen as a combination of signal and noise. Thus, we use these encodings solely as the input of the generator, and do not use an additional noise input, as is usually done for GANs. We believe that the additional noise might deteriorate the performance of the generator for generating class-specific images. Indeed, we quantitatively demonstrate that our approach yields more relevant images as compared to the state-of-the-art which includes the additional noise factor.

SECTION: 4.2C-Former EEG Encoder

The EEG signals are typically noisy, leading to the difficulties in training a GAN network if fed directly in their raw form. In addition, these signals are also high dimensional which can lead to suppression of class-specific discriminative information. Hence, using raw EEG signals as input to GAN is not a good idea. Inspired by Tiruppatur et. al[11], we transform raw EEG signals to low-dimensional encoding vectors using a Transformer-encoder (Block A in Fig.4) based EEG encoder which is trained to generate class-specific embeddings. Transformer architecture essentially comprises an encoder and a decoder module. However, if the required application involves finding features for a specific task, one can work solely with the encoder module. In this work, since our objective is to derive EEG features for EEG classification, we exclusively utilize the encoder module.

Having said that, in constructing the EEG encoder, we employ a network architecture known as the C-former, which integrates elements of convolution neural networks (CNNs), self-attention modules, and a fully-connected classifier. This network architecture is motivated from a recent research work[19]. Below we discuss the relevant details of all the different modules.

Convolution module:Inspired by the methodologies discussed in[11]and[13], we design the convolution module by decomposing the two-dimensional convolution operator into separate one-dimensional temporal and spatial convolution layers. In the initial layer, k kernels of dimensions (1, 5) are utilized with a stride of (1, 1), indicating that the convolution operation is applied across the time dimension. Subsequently, the layer maintains k kernels of size (ch, 1) with a stride of (1, 1), wherechdenotes the number of electrode channels in EEG data. This layer acts as a spatial filter, capturing the interactions among different electrode channels. To improve the training process and address overfitting concerns, we integrate batch normalization.

Self-attention module:In this specific part of our work, we introduce self-attention to understand the broader temporal relationships among EEG features. This helps compensate for the limited coverage of the convolution module. We take the tokens organized in the previous step, give them new shapes called query (Q), key (K), and value (V) through a linear transformation. We then measure the relationship between these tokens using a dot product between Q and K[19]. We also utilize a multi-head strategy to enhance the diversity of representations. The tokens are evenly split into h (h = 8 for this work) segments, each fed into the self-attention module independently.
The outputs from these segments are then combined to form the final output of the module. Output of this module is fed as an input to the feed-forward input layer.

Classifier module:Finally, we incorporate two fully-connected layers to serve as the classifier module. This module produces an-dimensional vector (here,is the number of classes) following the application of the softmax function. As a loss function, we use cross-entropy.

The complete architecture is given in Figure-5. For getting the embeddings from this C-Former network,

Firstly, we pre-trained it on the ThoughtViz[11]EEG dataset for classification.

After training the C-former network, we removed the last softmax layer to obtain embeddings from the second-last fully connected layer.

SECTION: 4.3Image Classifier

We also use an additional CNN based image classifier (B in Fig.4) which computes the classification loss on the generated images. The classifier (C) is trained separately from scratch on real images from ImageNet[20]dataset with their corresponding class labels. It consists of 3 convolutional layers withkernel size, with each layer having 32, 32 and 64 filters, respectively. The convolutional layers are followed by two fully connected layers with 128 and 64 neurons, respectively. We use softmax activation function at the output layer of the network. The network is trained only for the 10 ImageNet classes whose images were shown at the time of EEG signal acquisition. We use 80% images per class for training the network and remaining 20% for testing and achieved an overall test accuracy of 81%.

For producing the class-specific images from GAN, we make use of the classification loss of image classifier in addition to the GAN adversarial loss. The classifier loss is

where z denotes the EEG encoding to the generator input.
The introduction of this additional image classifier is motivated from AC-GAN[17]architecture, where the classification loss forces the GAN network to produce more class-specific images. Along with this, we also propose to use a perceptual loss for generating more realistic images from the generator. The perceptual loss between real image and generated image is given as

Here,andare the respective embeddings of real image and generated image from thelayer of the image classification network. The perceptual loss is essentially a feature-level loss computed at a sufficiently deep features extracted from a network. It helps the network to minimize the perceptual quality difference between the real images and generated images.

SECTION: 4.4Loss Function & Network Training

The generator part of the network converts EEG encodings into the RGB images. The generator architecture consists of a combination of convolution layers and upsampling layers. The generator takes the input of size 1001 and generates the output of size 64643. The discriminator is a simple image classifier type of network consists of 5 convolution layers. It takes aimage as an input and classifying it as real/fake. These real images are from the ImageNet dataset. Traditionally, the loss functions to train the GAN network is the standard adversarial loss function. But here we make use of classifier and perceptual loss along with the adversarial loss to minimize the overall cost function.

We start the training of this complete GAN network with a batch size of 100 and a learning rate of. The generated images and the real images are fed to the classifier network for the calculating the classification loss and perceptual loss. The overall flow of this work is given in Fig.4.

SECTION: 5Experiments & Results

SECTION: 5.1Evaluation Measure

To evaluate the performance of proposed approach we use two different performance metric scores: inception score[21]and class diversity score[22].

Inception Score:It is a metric to evaluate the quality of generated images from the generator. The inception score can be calculated using the given formula:

Here,represents a generated sample, sampled from a distribution ().is the KL-Divergence between the distributionsand.is the conditional class distribution.=is the marginal class distribution.For high quality generated images, the inception score should be high.

Class Diversity Score:To compute the class-diversity score of generated images, we use a pre-trained image classifier. The obtained predictions from the classifier for the generated images are in the form of one hot encoding vectors (C(G(z))). Class diversity of each class is computed by taking the entropy of the average of the one-hot prediction vectors. The class diversity score is defined as

Here,denotes the total number of classes,represents total number of generated samples from EEG signals of a particular class, andis the classifier. The range of diversity score is in between 0 to 1. A good diversity score should be as low as possible. A high diversity score indicates higher irrelevance, i.e., that many images of other classes are also being synthesized than that of the input EEG signals.

SECTION: 5.2Results

The comparison of diversity score on the test data with the existing state-of-the-art work[11]for different classes is given in Table1. To compute the class diversity score from[11], we use the released generator by the authors from the link (https://github.com/ptirupat/ThoughtViz). It is evident from Table1that we achieve a low diversity score of all the visual classes as compared to that in[11]. This means that our approach is producing significantly class specific images.

We also compared our results with the inception score of recent suggested approaches. Typically, the inception score is calculated w.r.t a large number of images. Since, in this case we have only 5706 images corresponding to test signals. Hence for statistical reliability, we consider two different conditions for calculating inception score.

Condition 1: Inception score on combined generated images w.r.t train and test EEG signals (total 50000).

Condition 2: Inception score on generated images w.r.t 5706 test EEG signals only.

All the comparisons are listed in Table 2. From results it is clear that the inception score of our work is almost similar to the state of the art method.

SECTION: 6Conclusion

In this study, we introduce a method aimed at generating images from EEG signals collected during a perceptual brain decoding task. We hypothesize that the inclusion of additional noise in the EEG encodings may hinder the performance of the generator in synthesizing images specific to particular classes. As a solution, we propose the incorporation of perceptual loss to enhance the generation of realistic images.

SECTION: 7Acknowledgment

This work has been partially supported by SERB, Government of India, under Project CRG/2022/007117.

SECTION: References