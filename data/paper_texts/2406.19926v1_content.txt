SECTION: Fully Dynamic-Means Coreset in Near-Optimal Update Time

We study in this paper the problem of maintaining a solution to-median and-means clustering in a fully dynamic setting.
To do so, we present an algorithm to efficiently maintain a coreset, a compressed version of the dataset, that allows easy computation of a clustering solution at query time. Our coreset algorithm has near-optimal update time ofin general metric spaces, which reduces toin the Euclidean space. The query time isin general metrics, andin.

To maintain a constant-factor approximation for-median and-means clustering in Euclidean space, this directly leads to an algorithm update time, and query time. To maintain a-approximation, the query time is reduced to.

SECTION: 1Introduction

As a staple of data analysis, the problem of clustering a dataset has been widely investigated, both from a theory and practice perspective. In this paper, we focus on the fully dynamic setting, where the input changes through insertions and deletions of data items: as an illustration, we think of a data-miner clustering their dataset, analyzing it, and suitably modifying it – for instance, to clean the dataset by removing outliers, or by adding new observations. In the big-data setting, updates need to be very efficient: instead of recomputing a solution from scratch, the goal of fully dynamic clustering algorithms is to efficiently cluster the data after a user updates.
This problem has received a lot of attention under various models of clustering: recent works tackled the-center problem[7,2], effectively closing the problem from the theory side; the facility location problem[9]; and the more intricate-median and-means problems[9,19,4]. The latter is probably the most useful for practice as-means is one of the most common unsupervised learning techniques, and this paper presents new improvements for both-median and-means problems.
The input for the-median problem is a set of weighted points in some metric space, and the goal is to output a set ofcenters that minimizes the sum, for each input point, of its weight times its distance to the closest center. For-means, it is the sum of squared distances.
Both problems are NP-hard to approximate arbitrarily close to one[17]: therefore, we focus on finding a set of centers with cost (i.e., sum of distances) within a constant factor of the optimal solution.

A standard approach to solve-median and-means problems is the use ofcoreset. An-coreset for inputis a compressed version of the input,, such that foranyset ofcentersthe cost is almost the same when it is evaluated on the full input or on the coreset, i.e.:

Perhaps surprisingly, coresets can have sizealmost independent of: the smallest-coresets in general metric spaces have size, which can be replaced within Euclidean spaces[13].
As the cost of any candidate solution ofcenters is preserved, an algorithm that dynamically maintains a coreset can therefore be directly used to maintain dynamically a solution to-means: at query time, simply use astaticapproximation algorithm on the coreset.
Therefore, using a-means algorithm with running timeon a dataset of size, this translates into a fully dynamic algorithm with query time(and an update time that depends on the coreset algorithm itself).

To maintain a coreset in a dynamic setting, amerge-and-reduceprocedure was introduced by Har-Peled and Mazumdar[18](based on Bentley and Saxe[3]and Agarwal, Har-Peled, and Varadarajan[1]).
This transforms any static coreset construction into a fully-dynamic one, reducing the dependency into a dependency in– the same speed-up as when applying an approximation algorithm on a coreset. With current coreset constructions, this translates into an algorithm with worst-case query andamortizedupdate time.111We use the notationas a shorthand to.Henzinger and Kale[19]showed how to solve the problem inworst-casequery and update time.
The interest of this method is two-fold: first, it uses a static coreset algorithm in a black-box manner. Therefore, in a setting where faster coreset algorithms exist (such as Euclidean space), the algorithm directly improves. Second, at query time, the algorithm uses any static-means algorithm: here as well, a different algorithm may be used to achieve a different time-to-accuracy ratio.

To improve the update time, Bhattacharya, Costa, Lattanzi and Parotsidis[4]recently adopted a completely different strategy: they adapted an old and fast algorithm from Mettu and Plaxton[25], and showed how to implement it such that only a few changes are required between each update. That way, they manage to improve the amortized update time towithquery time (which is near-optimal for any-approximation algorithm in general metric space according to a lower bound in[25,4]), but with an (unspecified) constant approximation ratio.
However, their result is much more restrictive than the coreset framework: (a) it only works in general metric spaces and cannot be fine-tuned to specific cases, and (b) it is based on an approximation algorithm with an unspecified constant approximation factor.
Furthermore, they state in their introduction that it is “not at all clear how to use [the coreset] algorithm to reduce the update time to”. This appears true when a static coreset algorithm is used as a black-box in the framework of[18,19], but possibly not in general: in this work, we therefore ask

[c, frame] Is it possible to design a coreset construction suited specifically to dynamic clustering algorithms?

We answer this question positively, showing how to use the framework of[18]with a particular coreset construction – answering the (implicit) question from[4].
One key advantage of our technique is that, if the metric space of study admits some faster-means algorithm, then our algorithm is faster as well. In particular, applied to Euclidean space, we getalmost constantupdate-time, andalmost linearquery time.

SECTION: 1.1Our result and techniques

More precisely, we show the following theorem.

There exists an algorithm for fully dynamic-median (resp.-means), that maintains an-coreset of sizewith amortized update and query time, whereis the time to compute a-approximation to-median (resp.-means) on a dataset of size.

As mentioned above, we can combine this algorithm withanystatic-median (resp.-means) algorithm: if the static algorithm runs in timeon a dataset of sizeand computes a-approximation, then there is a fully-dynamic algorithm to solve-median (resp.-means) with amortized update time, query timeand approximation guarantee(resp.).222The factorandcome from the fact that, to achieve a fast query time, we restrict the set of potential centers to the coreset points. See[19]for precise computation.

In general metric spaces, there is a coreset-based fully-dynamic algorithm to compute an-approximation to-median (resp.-means) with amortized update timeand worst-case query time.
In-dimensional Euclidean space, the amortized update time is, and the worst-case query time is.

Note that the running-time for general metric spaces matches the results of[4], but we improve the constant in the approximation ratio333They adapt a staticalgorithm, and making it dynamic blows up the approximation ratio by an unspecifiedfactor. On the other hand, our algorithm preserves the approximation ratio of any static algorithm, up to a multiplicative factor.: our algorithm matches the approximation ratio in[18,19], while improving their update time.
In Euclidean space, the query-time of our corollary matches the one from[19,4], but we reduce the update-time fromto. Furthermore, since it is possible to compute an-approximation to the static problems in time(seesection5), our algorithm can achieve this approximation factor with query time, which equals the time to output the solution, namely a list ofcenters in.
We conjecture that an-approximation can be computed with the same running time, which would provide a likely near-optimal algorithm for both update and queries; we discuss this more inSection5.

Our techniques.It is elementary to show our theorem in insertion-only streams. Indeed, a key property of coreset ensures that ifis a coreset for,is a coreset for. Therefore, if the goal is to maintain a coreset of sizeunder insertions, one can merely compute a coreset everysteps, in time(the same time as computing an-approximation), and in-between simply add new points to the coreset in time.
Using this idea in the merge-and-reduce algorithm of[18]directly yields an amortized runtime of essentially– which isin general metric spaces,in.

Dealing with deletions is more intricate as it isnotthe case thatis a coreset for444Consider the case whereconcentrates most of the cost of a solution: because of the multiplicative approximation guarantee,only needs to capture the cost of, and have a poor approximation elsewhere. Removingrequires the coreset to be precise everywhere else..
Standard coreset constructions are randomized, and the coreset is built by sampling according to an intricate sampling distribution, typicallysensitivity sampling. This sampling method works as follows: start by computing a good approximation to-means (with possiblycenters), and then sample points proportionally to their cost in the solution, and weight each point inversely to their sampling probability.
There are two difficulties in adapting such a sampling algorithm. (1) The initial good approximation to-means does not necessarily remain good after a deletion – a single deletion can make the approximation ratio go from 1 to infinity555In a set ofpoints at distancefrom each other any set ofdistinct centers is an optimal solution with cost. However after removing a single point (which is a center) the cost of the optimal solution iswhile the solution still has cost.(2) Even without changing the solution, maintaining efficiently a sample proportionally to its contribution to the solution is not obvious at all.

We address those two questions independently. (1) We show that a good solution for-means remains a good solution for-means, even afterdeletions.
This allows to compute a solution only everysteps – just as in the insertion only case. This result is novel in the world of dynamic clustering algorithms: it is the first result that allows to build some approximation and let it deteriorate over time, while still being a “good enough” solution.

(2) We show how to build a coreset fromuniform samplinginstead of sampling from a more complicated distribution.
In this way it is much easier to maintain a uniform sample under insertions and deletions: we build on the coreset construction of[13], to show that, if the input satisfies the following property, called3, any small uniform sample is a coreset.

Letbe a set of points with weights in, and letbe a set of centers. We sayandsatisfy3when:

Each point has the same cost up to a factor, namely,.

All non-empty clusters in solutionhave the same size, up to a factor: there is a scalarsuch that, for any clusterin solution, eitheror.

In the above,will be a set ofcenters; and for each center, the cluster ofis the set of points closer tothan any other center of.
Therefore, we need to (1) maintain a small number of groups of data points with the property above, and (2) maintain a uniform sample in each group. We show that, forconsecutive steps, our algorithm does so very efficiently.

Combining those results allows us to use the same strategy for deletions as for insertions: compute a fresh solution everysteps, and update it very efficiently between two recomputations. Therefore, the amortized running time of the algorithm is, whereis the running time to compute a coreset for a dataset of size(or, equivalently, an-approximate solution).
Finally, we incorporate this into the merge-and-reduce framework of[18], to reduce further the running time to.
This is the most intricate part of our analysis: for a reason that will later become obvious, we need to ensure that the coreset algorithm produces a coreset that evolvesnot morethan the input: when a single point is deleted, a single point should be deleted from the coreset. This is formalized in the following theorem, which is our main technical contribution:

For, there exists an algorithm that maintains an-coreset under insertion and deletions, with the following guarantee. Starting with a datasetof size, the running time for initializing the data structure is, whereis the running time to compute an-approximation for-means on a dataset of size.
Then, afterinsertions anddeletions with, it holds that: (1) the total running time for updates is at most, (2) the total number of points inserted into the coreset is at most, and (3) the total number of points deleted from the coreset is at most.

Those ideas allow us to maintain efficiently a precise coreset. In order to answer a-means query, we simply use a static algorithm on the coreset.

SECTION: 1.2Further related work

We already covered the closest related work on dynamic-median and-means clustering.
In the particular case of-median in Euclidean spaces, where points are restricted to the grid,[5]achieve update time. This, however, only works for-median, because of the use of a quadtree embedding.
For-means, the preprint[20]claims to show how to maintain an-coreset of size, using at mostbits of memory. We believe that the (un-specified) running time is the same as ours; however, the coreset computed is correct at any step with probability, while ours isfor any. This means that their algorithm fails every30 queries, and we do not see an immediate fix to this.666In particular, note that the problem of estimating whether a given set is an-coreset is co-NP hard[26].

To maintain coresets in general metric spaces,[18]introduced the merge-and-reduce tree with an amortized running-time analysis, and[19]showed how to turn it into a worst-case guarantee.

The literature on coreset recently boomed (see e.g.[15,22,6]and the references therein), with a series of work achieving optimal bound for-means clustering ofin general metric spaces[13]andor[13,11,21].
Besides their use in the dynamic setting, coresets are key to a recent breakthrough in the streaming model[14], which shows that a memory of onlyis necessary.

The related-center clustering is perhaps easier to handle: the reason is that a certificate that the cost is higher than a threshold only needspoints (at distance more than twice the threshold from each other). This is the basis of the works of[7,2]. This can be more easily maintained than a-median solution, for which no such certificate exists.

SECTION: 1.3Definitions and notations.

The-clustering problem is defined as follows. Given a set of pointsin a metric space, with weights, the goal is to find a setofpoints that minimizes the cost function. We callwithpoints ofa candidate solution. For, a candidate solutionis a-approximation if, where the minimum is taken over all candidate solutions.

An-coreset for-clustering is a weighted setsuch that, for any candidate solution,.
In the following, we will useto denote the size of the coreset, in particular the coreset constructed viaLemma5.

We useto denote the running time of an algorithm computing an-approximation for-means, on a dataset of size. In Euclidean space, this is. In general metric spaces, this is.
We will sometimes abusively denotefor. We use the notationto denote.

For simplicity of presentation, we will focus our presentation on-means – i.e.,-clustering – and writecostfor. All the results can be directly translated to the more general problem, replacingby the running time to compute an approximation to-clustering.

SECTION: 2Preliminary results

In this section, we provide some results that are crucial to our analysis, and can be used in a black-box manner.
Our first lemma formalizes that, in a group of points that satisfy3, a uniform sample produces a coreset:

Letbe a group of points with weightsand letbe a set of centers such thatandsatisfy3.
Letbe a set ofpoints sampled uniformly at random, wherehas weight. It holds with probabilitythat

Furthermore, the total weight verifies:.

In Euclidean space, it is enough to take.

This lemma is very related to Lemma 2 in[13], which usesgroup samplingto construct a coreset.
This algorithm starts by computing a solution, with set of clustersand partitions the input intostructured groups. The groups have the following property: each cluster that intersects with the group has roughly the same contribution to the cost of the group in the solution, and each point within the same cluster has roughly the same distance to the center. Then, the algorithm samples essentiallypoints from each group, following the following distribution: in group, pointin some clusteris sampled with probability.

3is a strong requirement ondesigned such that the distribution of group sampling becomes essentially uniform: for a set of points satisfying3, the probability of sampling any two points (using the group sampling distribution) differs by a small constant factor. Therefore, it is not a surprise that the proof can be adapted to work with uniform sampling instead.
More precisely, group sampling will sample each point with probability within a constant factor of uniform.
The analysis of group sampling is based on a concentration inequality on the sum of the random variables indicating whether each point is sampled or not. This concentration is based on bounding the first and second moments of those variables: since the group sampling distribution is close to the uniform sampling one, those moments are essentially the same and the proof goes through. We provide a thorough proof inAppendixA.
In the following, we will useto denote the size of the coreset constructed viaLemma5.

As explained in the introduction, we crucially need to construct a bicriteria solution whose cost stays close to optimal over many steps. This can be achieved using the following result.

Letbe a-approximation to the-means problem on a weighted set. Then, for any set, it holds that

whereis the optimal-means solution on.

We letbe the optimal-means solution on, andbe the optimal-means solution on.
By definition of, it holds that.
Since,is a-means solution forwith cost at least as high as the cost of.
Thus,is a-means solution forwith cost at most.
Furthermore, we have:

where the equality holds as all points ofcontribute zero to, and the inequality because removing centers only increases the cost. Putting it all together, this shows.
∎

SECTION: 3update time via merge-and-reduce tree

We start by showing howLemma4implies our main theoremTheorem1.
For this, we sketch first the merge-and-reduce algorithm of[18], and how to incorporate the coreset construction ofLemma4to speed up the update time.

SECTION: 3.1Description of the merge-and-reduce algorithm

The goal of this algorithm is to maintain an-coreset under insertions and deletions of points. The keys to this are the following strong properties of coreset: first, ifis an-coreset for, andis an-coreset for, thenis an-coreset for. Second, if insteadis an-coreset for, thenis a-coreset for.

The merge-and-reduce data structure is the following (suppose for now that the number of points in the dataset stays within). The dataset is partitioned into at mostparts, each containing at mostpoints. Those parts form the leaves of a complete binary tree. We say that a nodeof the treerepresentsthe points stored at the leaves descendants of.

Each nodemaintains a setas follows. Letbe the children of. Nodestores an-coreset of.
It follows from the two coreset properties that the set stored at the root is an-coreset of the full dataset.777This can be shown by induction: a node at heightstores an-coreset of the points it represents.Rescalingbytherefore ensures that the root stores an-coreset.
It is straightforward to maintain this data structure under insertions: simply add the new point to a leaf that contains less thanpoints, and update the sets stored at all its ancestors. For deletions, simply remove the point from the leaf it is stored in, and update all its ancestors.
Since the depth of the tree is, this triggerscoreset computations, every time on a dataset of size(whereis the size of an-coreset, which is).
The update time is therefore the time to computetimes an-coreset on a dataset of size. This turns out to beusing standard coreset construction.
Finally, if the size of the dataset changes too much and jumps out of, the algorithm recomputes from scratch the data structure with a fresh estimate on the number of points.[18]shows that the amortized complexity induced by this step stays.

SECTION: 3.2Our algorithm

The previous algorithm uses a static coreset construction as a black-box. Instead, we propose to useLemma4to avoid reprocessing from scratch at every node. For each node, we divide time intoepochs, with the following property. For a nodewith childrenand, the sets maintained atandchange at mosttimes during an epoch. When those sets have changedtimes, a new epoch is started for the node and all its ancestors.

Each node maintains an-coresetof its two children using the algorithm fromLemma4. For each update of those, this algorithm processes them and transmits to its parent the potential updates made in.
This is enough to showTheorem1:

We letbe the number of insertions to the dataset, andthe number of deletions. Our goal is to show that the total running time is.

At each node, the complexity can be decomposed into the one due to updates in between epochs, and the complexity due to starting new epochs. We say that the work done by a node is the total complexity to update its coreset, and the total complexity when starting a new epoch at this node (i.e., re-initializing this node and all its parents).

We will compute the work done at each level of the tree: leaves have level, and parent of a node at levelhas level. For a node, letbe the total number of insertions and deletions in the dataset represented by.

We show the following claim by induction: the total work for a nodeat levelis, the number of insertions to the coreset is, and the number of deletion is.

For leaves (i.e., nodes at level), the statement is straightforward, as the set they maintain is directly the dataset they represent, with weights 1.

Let,be a node at levelwith children. We letbe the number of insertions and deletions made to the coresets maintained atand.
By the induction hypothesis, the coresetsandmaintained atandhave undergoneinsertions, anddeletions.

Lemma4therefore shows that, during each epoch, the total complexity to maintain a coreset atis, the number of insertions to the coreset at most, and the number of deletions to the coreset at most.
Therefore, over all epochs, the complexity is, the number of insertions to the coreset isand number of deletions at most.

A new epoch is started at the node either when one of its descendant triggered a re-initialization, in which case the complexity is accounted at that descendant’s level, or whenupdates have been made to the input during the current epoch.
The latter occurs therefore at mosttimes, and every time triggers a recomputation of the coresets atmany levels: therefore the complexity of a single recomputation is(from the guarantee ofLemma4).
Thus, the total complexity due to all re-initialization afterinsertions anddeletions is, which concludes the induction statement and the proof of the theorem.
∎

SECTION: 4An efficient dynamic coreset algorithm

In this section, we show the keyLemma4.
For this, we describe an algorithm to maintain a coreset of sizefor a set ofpoints with weights in, in amortized time.
As explained in the previous section, this algorithm can be used black-box to reduce the complexity to.

SECTION: 4.1The Algorithm

We explain first the data structure that is maintained by the algorithm, and how to extract efficiently a coreset from it. We will then show how to initialize the data structure, and maintain it under insertions and deletions.

Letbe the initial set of points,the current set of points,(resp.) the set of points inserted (resp. deleted) since the beginning. For the lemma, we focus on the case where.

The data structure consists of a set of centers, a scalar, and a set of groupswith the following guarantees:

consists ofcenters such that, andverifies,

partitionsuch that

,

, and

for all,andsatisfy3.

For each group its points are maintained in random order in a data structure that allows for efficient insertions and deletions.

Finally, for each groupwitha lazy estimateonis maintained.

Given this data structure, we can easily build a coreset of size, as we sketch here and prove inLemma11.is a coreset for itself, andis a coreset for. For each other groupfor,Lemma5shows that the firstpoints of the random order, with weights multiplied by, form a coreset for.
Since the groups partition, the union of those coresets is a coreset for.
To get a coreset for, one merely needs to add each point ofwith weight.

However, if we used the weightfor each pointin, the weights of a coreset point, and thus the coreset itself, would change too frequently. Thus, instead we useas weight for each coreset point.
We describe below how the data structure maintains lazily an estimateof the size of each, while still guaranteeing thatis a good estimate of.

We first show how our algorithm maintains this data structure, and prove in the next section that this indeed maintains efficiently a coreset.

For initialization, the algorithm computes an-approximationto-means on. Defineas themost expensive points ofin this solution.is set to be the average cost of, i.e.,.

Next the algorithm defines groups as follows:888For convenience, we index the groups by three integersinstead of a single one as in the previous description.

For, letbe the-th center of, and defineto be the cluster of, namely all points ofcloser tothan to any other center of(breaking ties arbitrarily).

For, let. Let also.
Eachforis called a-th ring.
Note that thepartition.

For, let.

For, and all, letbe the set of-th rings with cardinality betweenand.is set to be.

We say a groupisinitially largewhen its size at the beginning of the epoch is more than, otherwise the group isinitially small.
The groupconsists of the points of all initially small groups, together with.

The groupis initialized to be.

Finally, in each of the initially large groups, the algorithm randomly orders points and store them in a binary search tree (to allow for efficient insertions and deletions). Each point is associated with a random number in, and those are stored in a binary search tree with keys being the random number.

We make a few remarks about this construction:

(1) The groups form a partition of. (2) There are onlynon-empty groups.
(3) Eachandsatisfy3at the beginning of the epoch
As long as, it holds that (4), and (5)is smaller than the average cost of.

(1) and (3) are direct consequences of the definition of the groups.
For (2), note that for anyand, the setis empty (as the cost of points in such a ring would be larger than, which is a contradiction). Furthermore, no ring can contain more thanpoints. Lastly, the lemma’s condition ensures that all weights are in. Therefore, groups with, or, orare empty: this concludes the second bullet.

Finally, (4) holds directly fromLemma6, and (5) stems from the definition ofand the choice of.
∎

Therefore, the groups form a partition of.
Note that there are there aremany groupsand, thus,contains at mostmany points.

Dealing with insertion is easy: the set of inserted points does not appear in the definition of the data structure.
Therefore, as long as the total number of updates is less than, the data structure needs no update after an insertion. When, the algorithm is done (and, in the use of this subroutine inSection3.2, a new “epoch” starts).

Property1is a consequence of7.
Properties2and3of the data structure are more involved, as we need to ensure that the groups still fulfill3, e.g., their size stays roughly the same.
The algorithm updates the groups as follows.
First, if the deleted point is in a group that is initially small, then no further updates are required.

If the point is in a group that is initially large, the algorithm only needs to ensure that all clusters in a group have the same size, up to a factor.
Suppose the deleted point was in a ringfrom the group. Note that two things can happen. Either the number of points in the ring after deletion is still more than, in which case3still holds and the algorithm does nothing.
If, however, the number of points becomes exactly,
the algorithm removes the whole ring fromand adds it to(note that, in that case,andexists).

This movement triggers some necessary changes in the ordering of each group, to maintain Property3of the data structure, i.e., the random order of the points of each group in the data structure.
First, in the group, the points of the ring are simply removed.
Second, they are inserted one by one at positions that are chosen uniformly at random into. This can be done efficiently, as the orderings are described with a binary search tree.

Finally, the size estimate of the two groups may have to be updated: ifset.
If a ring was moved in the groupand if, then set.
Both cases imply that the weights of all coreset points from the group considered change.

From the data structure described above, the algorithm extracts a coreset as follows. First, it defines weightssuch that:

for

for

for all centersof,,

for each pointin an initially large group,ifis among the firstelements ofin the random order of the data structure, and 0 otherwise,

for each other point.

Letbe the set of points with non-zero weight. Those weights can be easily maintained under insertions and deletions, as described in the previous paragraphs.
We first show thatcan be computed in amortizedtime, then that it is an-coreset for.

SECTION: 4.2Running-time Analysis

To show the running time, we assume for simplicity of the bounds, and. The second assumption is (almost) without loss of generality, as in general metric spaces any-coreset must have size[10], and in Euclidean space they must have size[21].

We start by listing the running time induced by each of the operations described in the algorithm.
The initialization takes timeto compute the constant-factor approximation to-means, and assignment of each point to its closest center. Then, the identification ofand partitioning into groups take linear time. This is enough to prove the first statement ofLemma4

Any insertion just requires to set the weight of the point to, which takes constant time. Deletions are more intricate to analyze.
We first note that the running time to insert or remove a point from a group of the data structure takes time, which is the time to remove the point from a binary search tree and add it to another one.

It may happen that a ringis moved from one initially large groupto.
In that case, the running time is, in order to move all points of;
and the coreset changes by at most: first, at mostpoints are removed from the coreset of(and replaced by points that stay inafter the operation). Second, thepoints that are added toare placed randomly in the random order: if they appear among the firstelements, they need to be added to the coreset of. Therefore, there are at mostmany changes in the coreset of(the same argument works ifis initially small). Therefore, in total the coreset changes by at most.

Finally, the size estimate for a groupmay appear to change. However, as we will demonstrate inLemma10, it is never actually decreased.
The size estimate increases when several rings have been moved to the group: in that case, the weight of up tomany coreset points changes.
In the amortized analysis below, we will show that those costly events do not occur too often, which will conclude the proof ofLemma11.

Amortized analysis.To analyze the number of changes to the coreset by this algorithm, we proceed with a token-based argument. Every deletion of a point from the dataset
is charged one token, i.e., it increases the number of tokens by one.
The tokens are used to bound the total number of updates in the coreset: whenever a point is removed from the coreset, it will consume one token. This can happen both for its deletion, or when its weight is updated (which we see as deleting the point and re-inserting it with a different weight).

To proceed with the analysis, we definetoken walletsof several types.
For each group, there is one wallet, used to update the weight of the group. For each ring, there is a walletwhich is used for deletions occurring when moving a ring to another group, andwhich is used as an intermediary wallet to supply.

Tokens are provided to the wallets when points are deleted from the dataset: each deleted pointbrings one token as follows.
Letbe the ring of.
Ifhas non-zero weight, then it directly consumes its token to pay for its own deletion from the coreset. Otherwise, it givestoken to, andto.
When a ringmoves to a group, all tokens ofare transferred to.

To show that those tokens are enough to pay for deletions, we use a probabilistic analysis using the randomness of our algorithm: in each group, points are sorted randomly, which will ensure that deletions from the input rarely triggers deletions in the coreset, as we show in the next lemma.

Let,, and.
Consider a ringin groupthat is initially large, and let. Letbe the firstpoints removed from the ring. Then, after thosedeletions, bothandcontain at leasttokens with probability at least.

First, the lemma statement is well defined: whenwas placed into(either at the beginning of the epoch, or after points were removed from), its size was at least, and therefore there are indeed at leastpoints in the ring.

Each of the deleted points contributestoand, if it is not part of the coreset. Therefore, the number of tokens added by those points in both wallets is the same, and it is enough to analyze.
Letbe the random variable equal toifis not part of the coreset when it is deleted: we have. We will show that this sum of variables is a martingale, and use Azuma’s inequality to conclude.

We make a few observations. Since the group is initially large, its size is at least.
All rings in the group have initially size betweenand. As there are initially at mostrings in the groupthis ensures.
Using, this ensures.

It seems more natural to consider the process of deletions reversed. Starting from the set of points, points are added sequentially at random locations, in order.
Then,when pointis not inserted among the firstlocations.
This is equivalent to the initial process as the relative positions ofin the random order do not depend on the positions of.
Furthermore, this showsis independent of: the position whereis inserted does not depend on the relative order of.

We use this fact to show that the number of tokes follows a martingale. Formally, let, letfor, and let. Thenand.
Therefore, the sequence of variablesforis a martingale. We will use concentration bound on martingales to prove the lemma.

Note that for eachwith probability at most: indeed, whenis added, there are already at leastpoints in the order (usingand thatis initially large), and thereforeis an upper-bound on the probability ofbeing added among the firstpositions.
Thus, for each,and we have.

Asfor each,
Azuma’s inequality ensures that, with probability at most,, which is equivalent to.
It follows that, with probability at most.

Takingand usingyields that, with probability at least,.
As, this probability is at least.
As the number of tokens is exactly, this concludes the proof.
∎

As a corollary of the previous lemma, we can compute the number of tokens in:

Consider a group, at the moment its size estimate increases. Then,contains at leasttokens with probability at least.

Elements can be added to a grouponly by ring movements: rings can move fromtowhen they containelements.Lemma8ensures that, for every such ring movement,tokens are added towith probability at least.
Since a ring movement adds exactlypoints to, there is exactly one token per new point.
Furthermore, during an epoch there can be at mostdeletions, and therefore at mostring movements.
A union-bound ensures that the previous holds for all ring movement with probability.

When the estimated weight ofchanges, it means that its size increased by anfraction: since the group is initially large, it means at leastelements have been added to it, and thereforecontains at leastmany tokens. This concludes the corollary.
∎

In addition to this token scheme, we have the following property:

For any groupthat is initially large, the size estimatedoes not decrease during a sequence of at mostupdates.

Ifis initially large, it contains at leastmany points.
Points can be removed from the group for two reasons: either they are deleted from the dataset, or their ring moves to another group. Letbe the number of points deleted from the dataset, andthe number of points removed because their ring moved. Note that when a ring moves, at leastof its point have been deleted from the dataset. Therefore, if the size of the group decreases by some number, at leastpoints have been deleted from the dataset. Since we know that at mostelements are deleted, this enforces.

Now, the algorithm decreasesif the group sizes decreases by anfactor, which meanspoints have been deleted from the group. The previous paragraph shows that this cannot happen with onlydeletions from the input.
∎

We can now proceed to the proof ofLemma4.

As presented in the initial sketch, the running time for initializing the data structure is, and each insertion is processed in constant time and yields a single addition to the coreset.

The effect of deletions can be bounded via the previous lemmas as follows.
First, when the point deleted by the update is part of the coreset, it is removed from the data structure in time, and uses one token to account for the update in the coreset.

When a ring moves fromto, its size is exactly: the complexity of processing the movement (removing points of the ring fromand adding them to) is therefore. The tokens inare therefore enough to pay for the running time. Similarly, the number of insertions and deletions in the coreset is at most.

When the size estimate ofchanges, the weights
ofmany points are updated: this has running time.Lemma10shows that the size must have increased, andCorollary9ensures that. Those tokens can therefore pay for the updates.

Since the total number of tokens is the number of deleted points, this concludes the lemma.
∎

SECTION: 4.3Correctness analysis

For group, letbe thefirst points of the random order maintained by the data structure. Then,with weights defined in the algorithm is a-coreset for.

We use the composability property of coreset.with weightsis obviously a coreset for. Similarly,is a coreset for, anda coreset for.

Letbe as in the algorithm, the average cost of.
The algorithm ensures that every point inis at distance at mostof a center of.
Item (2) of7shows that: therefore, the triangle inequality ensures that each point ofcan be replaced by its closest center in, up to a total error. The centers of, weighted by, form therefore an-coreset for.

The first item of7ensures thatis an-approximation to-means on: therefore,Lemma5shows that for each initially large group,with weight per pointis an-coreset for.
Since the total weight estimates satisfy, eachwith weightsis a-coreset for.

The union of all those coresets is therefore a coreset for.
∎

SECTION: 5A Note on Euclidean Spaces

In Euclidean space, it is known how to compute an-approximation to-clustering, in time essentially, ignoring the impact of the aspect-ratio for our discussion (see Corollary 4.3 in[12]for the running time, combined with Lemma 3.1 for the approximation ratio, with the dimension reduced tousing[24]).
To get an-approximation,[23,8]showed how to complement the famous-means++ algorithm with few local search steps, achieving a running time.

Therefore, we believe that the following conjecture holds:

There is an-approximation algorithm for-clustering in Euclidean space, running in time.

In that case, the query time of our fully dynamic algorithm reduces to, i.e., time to output the solution.

SECTION: 6Conclusion

We present an efficient algorithm to maintain a coreset under insertion and deletion of points. This algorithm has near-optimal running time, as it can be used black-box to solve-clustering with optimal update time (and improving ours would directly improve the update time for-clustering).

Furthermore, our algorithm likely yields an optimal algorithm for update and query time in Euclidean space. This is true, under two conjectures that we believe are worth investigating.: first, there exists a static-approximation algorithm for-clustering with running time(which we discuss inSection5). Second, the query time is at least.
This is the time to output a solution; however, it may be the case that solutions do not entirely change between each query.

This second conjecture is thus related to the notion ofconsistency:[16]recently showed how to maintain a-median solution in an insertion-only stream of length, with at mosttotal number of changes in the solution.
It is therefore natural to ask whether this approach can be extended to fully-dynamic streams: is it possible to maintain a solution with at mostchanges between each time step?

SECTION: Acknowledgments

Monika Henzinger: This project has received funding from the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme (Grant agreement No. 101019564) and the Austrian Science Fund (FWF) grant DOI 10.55776/Z422, grant DOI 10.55776/I5982, and grant DOI 10.55776/P33775 with additional funding from the netidee SCIENCE Stiftung, 2020–2024.

This work was partially done while David Saulpic was at the Institute for Science and Technology, Austria (ISTA). David Sauplic has received funding from the European Union’s Horizon 2020 research and innovation programme under the Marie Sklodowska-Curie grant agreement No 101034413.
This work was partially funded by the grant ANR-19-CE48-0016 from the French National Research Agency (ANR).

SECTION: References

SECTION: Appendix ACoreset via Uniform Sampling

We sketch in this section the proof ofLemma5.
The proof is based on Bernstein centration inequality, and follows the line of the proof in[13]. We mention here the differences. Fix a setwithcenters: the goal is to show that the cost ofis preserved with some very large probability, and then union-bound over alls.

First, the points ofare divided intotypes: thetinytypes are all points with; theinterestingtypeis all points with, for. Finally, the remaining points withare thehugepoints.

The tiny points are so cheap that they contribute at mostboth inand in: the proof of Lemma 5 in[13]applies directly to our sampling distribution (we refer to the numbering of the full version on arxiv).

For the huge points,[13]shows that it is enough for the coreset to preserve approximately the mass of each cluster of(which they call event).
Callthe clusters of. In our setting, because each of thecoreset pointis weighted, this event corresponds to:

To prove this equation, note that each sampled point is part ofwith probability– from3. Therefore, the proof of Lemma 6 of[13]can be directly adapted.

Note that, in particular, Equation1shows that the total weight of the coreset is almost equal to the weight of, as claimed inLemma5.

Second, Lemma 7 of[13]shows that, given Equation1holds, the points in clusters that are intersecting with huge type have cost preserved by the coreset.
Therefore, it only remains to deal with the interesting points, in clusters that contain no points from the huge type (calledin[13]).

Note that due to the choice of weights,is an unbiased estimator of. Further,can be expressed as a sum ofindependent random variables: letbe the variable equal toifis the-th sampled point, and. If,is equal to.

The key argument in the proof of[13]is to bound the variance of. In our case, we have:

where we used that for any point. Now, for, we have. Further, since all points in the grouphave same cost, up to a factor 2 (from3),.
Therefore, we get:

Using this bound in Lemma 8 of[13]concludes the proof for interesting types.
Putting all the results together and performing a union-bound does not depend on the sampling distribution, and therefore those results are enough to finish the proof ofLemma5.

This however shows the slightly suboptimal result that.
To reduce the dependency into,[13]use a technique to reduce the variance of the estimator (see their section 5.6): this works the same way as what we described above, and concludeLemma5.