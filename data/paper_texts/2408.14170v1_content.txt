SECTION: Image Provenance Analysis viaGraph Encoding with Vision Transformer
Recent advances in AI-powered image editing tools have significantly lowered the barrier to image modification, raising pressing security concerns those related to spreading misinformation and disinformation on social platforms. Image provenance analysis is crucial in this context, as it identifies relevant images within a database and constructs a relationship graph by mining hidden manipulation and transformation cues, thereby providing concrete evidence chains. This paper introduces a novel end-to-end deep learning framework designed to explore the structural information of provenance graphs. Our proposed method distinguishes from previous approaches in two main ways. First, unlike earlier methods that rely on prior knowledge and have limited generalizability, our framework relies upon a patch attention mechanism to capture image provenance clues for local manipulations and global transformations, thereby enhancing graph construction performance. Second, while previous methods primarily focus on identifying tampering traces only between image pairs, they often overlook the hidden information embedded in the topology of the provenance graph. Our approach aligns the model training objectives with the final graph construction task, incorporating the overall structural information of the graph into the training process. We integrate graph structure information with the attention mechanism, enabling precise determination of the direction of transformation. Experimental results show the superiority of the proposed method over previous approaches, underscoring its effectiveness in addressing the challenges of image provenance analysis.

SECTION: 
In todayâ€™s platforms of social media, visual content has emerged as a predominant mode of communication, transcending linguistic barriers and enabling instantaneous sharing of life experiences, news, and ideas. However, this convenience comes with a hidden risk: maliciously tampered photos can significantly mislead public opinion and contribute to spreading fake news, posing multiple potential dangers. Extensive research in the prior art has focused on image authenticationand image forgery localization. However, with the rapid proliferation of image content circulating on social media platforms, it is of paramount importance to further identify image provenance and construct a provenance graph to ensure image integrity.

In response to these challenges, the field of Image Provenance Analysishas recently emerged as multimedia phylogeny with focus on visualizing the underlying relationships among related images as illustrated in Fig.. Provenance analysis goes beyond the simple binary classification of images as fake or real. It seeks to reveal the hidden narratives of a set of semantically-similar images, which can more powerfully indicate manipulated intent and provide a comprehensive visual graph. Enhancing our understanding of the context and lineage of digital images strengthens the tools available for maintaining the integrity of visual information, protecting copyright by giving a more concrete evidence chain, and accurately reducing tampered images uploaded to the social networkingwith malicious purposes, which is indistinguishable for forensics tools.

Image provenance analysis aims to identify images within a database that may share manipulation relationships with each other. The process involves unveiling the relationships among associated images using either directedor undirected linksand visually representing these relationships through a provenance graph as illustrated in Fig.. The procedure for conducting end-to-end provenance analysis is typically divided into two main stages: image filtering/selection and graph construction. As existing image filtering methodshave achieved outstanding performance in selecting semantically-similar images from databases, this study focuses on the second stage, graph construction, which leverages pre-identified related images to map out the network of image manipulation.

A connected provenance graph is constructed by depicting significant transformation relationships between similar images as links and determining their directions. These relationships encompass local transformations such as splice-pasteand global transformations such as blur, noise, or contrast changes. Link prediction methods primarily rely on similarity calculations between image pairs, using features ranging from traditional descriptors like Scale-Invariant Feature Transform (SIFT)and Speeded-Up Robust Features (SURF)to more advanced Convolutional Neural Network (CNN) learned features. The selection of features significantly affects the ability to capture different image transformations and relationships. Link direction determination, a distinct step following link prediction, varies based on image attributes. While some methods analyze metadata, more robust visual content-based approachesfocus on inherent image information. These methods either calculate mutual information of pixel valuesor detect forgery traces, reducing reliance on volatile metadata. Constructing a directed provenance graph allows for tracing content within complex image networks, providing valuable insights into the intent behind digital manipulations.

Nonetheless, existing provenance analysis methods still face challenges during the graph construction stage, which can hinder the effectiveness and accuracy of the analysis:

Most current methods focus primarily on isolated image content or relationships between individual image pairs, often neglecting the more informative structure of the provenance graph, resulting in limited graph construction performance.

Existing visual content-based methods tend to focus narrowly on detecting either local manipulations (forgery traces) or global transformations (large-scale modifications), rarely addressing both in tomdein.

Metadata-Ebased methods are vulnerable to simple modifications or information loss during image format conversion. Existing visual-based approaches heavily depend on pixel histograms or forensic integrity, which often fail to accurately predict link directions between source and target images within the constructed graph.

This paper introduces a novel image provenance analysis model to address the abovementioned challenges. Our approach enables end-to-end inference of the entire directed provenance graph, out-performing previous methods focusing solely on either link prediction or direction determination. The key contributions of our work are threefold:

We propose an innovative image provenance analysis framework that simultaneously processes all images and integrates graph topology into transformer architectures. This effectively captures the rich information within the entire graph.

We establish a new paradigm in link prediction through a designed weighted patch distance learning module, coupled with pretrained model-guided patch weights and whole graph path-length loss, effectively capturing local and global manipulation traces.

We design a link direction determination approach by introducing learnable precedence embeddings, graph-structured attention masks, and auxiliary virtual nodes, establishing a new benchmark in predicting directional flow within complex provenance graphs.

Quantitative and qualitative results show that our method outperforms existing approaches in accuracy, generalization, and robustness across diverse provenance scenarios, underscoring its effectiveness.

In this paper, Sec. II comprehensively reviews previous literature on image provenance analysis and graph construction. Sec. III details the proposed provenance graph construction framework. Sec. IV elaborates on the experimental setups, data augmentation techniques, and the baseline models employed. Sec. V presents extensive experimental results compared with other baselines. Finally, Sec. VI concludes the paper and discusses limitations and possible future research directions.

SECTION: 
SECTION: 
To combat the threats of the spread of disinformation through manipulated images, various manipulation detection techniques have been developed. The most widely researched field is image forgery detection, which aims differentiating manipulated images from authentic onesand precisely localize forgery areas within an image. Forgery detection primarily focuses on revealing alterations to the content of images, such as copy-move, splicing, and inpainting. In turn, image transformations do not alter the content directly, such as variation, in contrast,, resamplingand JPEG compression, which are often regarded to conceal traces of forgery, thereby suggesting potential tampering.

Lately, there has been a significant shift in manipulation detection methods, veering away from traditional forensic techniques that rely on manual features among blocks or key points. Instead, the field has embraced more robust learning-based approaches that can identify a broader range of manipulations. These methods leverage high-level feature extraction to analyze inconsistencies in invisible fingerprints across the image and detect traces of digital manipulations. Such techniques demonstrate enhanced effectiveness in identifying and addressing various digital image alterations. Recent research highlights that more than merely distinguishing fake images is required; understanding the sequential history of multi-step operations is crucial in certain scenarios. However, previous forensics methods focusing on single-image-based manipulation detection fall short for social media platforms. These approaches need to clarify the intent behind manipulations, which is essential for countering the dissemination of misinformation and disinformation.

SECTION: 
Provenance Analysis was first introduced in the context of Internet image archaeology by Kennedy, which aimed to construct relationships among related Internet-published images through identifiable manipulation features and pairwise comparison. Subsequently, Diasproposed the earliest general process for provenance analysis that includes building a dissimilarity matrix and employing a spanning tree algorithm. However, this study focused exclusively on single-root provenance graphs constructed from different versions of a single base image and considered only a limited range of transformations. The Open Media Forensics Challengehas been established to provide large-scale provenance datasets featuring various manipulation techniques and graph topologies. Leveraging these resources, Bharati et al.integrate content-based image retrievalwith dissimilarity computations, which take into account the number of matching interest points. Furthermore, recent researchunderscores the importance of the direction of transformations in accurately interpreting the viewpoints behind manipulations. Efforts include using asymmetric mutual informationderived from the pixel values of image pairs to delineate directional relationships. Despite these advancements, earlier methodologies relying on handcrafted features faced limitations, including vulnerability to various types of manipulation and reduced robustness against noise. To address these challenges, a metadata-based methodhas been developed. This approach harnesses external context information, enhancing computational efficiency and accuracy. However, its effectiveness is contingent upon the integrity of metadata and faces tampering threats in practical applications.

Recent advances in deep learning have significantly improved the performance of image provenance analysis by leveraging learnable image features instead of handcrafted features. A notable development for undirected link prediction is introducing a ranking-based framework, which is trained on custom-designed sets of quadruplet images and utilizes image embeddings to quantify the number of transformation steps. Building on the capabilities of image forgery detection systems and large-scale forensics datasets, Zhanghave developed a hybrid network. This network simultaneously evaluates the integrity of imagesand assesses local changes between images, which helps determine the direction of transformations. Despite these innovations, the method presents challenges, including risks of mismatched manipulation types and limited scalability. Furthermore, there is a noticeable gap in the current landscape of deep learning applications in image provenance analysis: the absence of a comprehensive deep learning framework that integrates both link prediction and the identification of source and target images.

SECTION: 
Provenance Analysis extends beyond simple visual content processing and relies on graph construction techniques. These techniques are crucial for establishing and visualizing the provenance graph. Graph construction provides a systematic way to analyze data structured in graph form, applicable in diverse domains like social networks, point clouds of shapes, and chemoinformatics. This process involves predicting isolated links or modeling the overall graph topology, which is vital for uncovering potential relationships within the data. Traditional methods focus on computing similarities based on explicit features to construct various graphs, including spanning treesand relative neighborhood graphs. However, with advancements in deep learning, graph neural networks (GNNs) such as RecGNN, ConvGNN, and Graph Attention Network (GAT)have emerged. These networks leverage graphsâ€™ inherent topology and features to infer relationships within the data more effectively. Recent studieshave extended the capabilities of the Transformer network to handle arbitrary graph data, demonstrating competitive performance. This evolution means a significant shift towards more dynamic and sophisticated methods in graph-based data analysis within image provenance tasks.

SECTION: 
SECTION: 
The objective of provenance analysis is to create a provenance graph, where the verticesrepresent images included in each provenance case under analysis, and the edgesillustrate the transformational relationships between these images. The process begins with selecting images from a large-scale database that share content with the probe image or its content donors. In our task, the initial filtering is performed in advance, and all images related to the probe are provided. Subsequently, the primary transformation relationships are identified and represented as edges, assembled into an undirected provenance graph. This graph is further refined into a directed format by assigning directions to each edge, indicating the source and target of each transformation. The resulting directed graph provides a comprehensive representation of the image manipulation history, offering insights into the sequence of manipulations and potentially revealing underlying intentions and messaging.

SECTION: 
As illustrated in Fig., our method for provenance analysis is designed as an end-to-end system that generates a directed provenance graph from a set of images. The designed framework incorporates an undirected link prediction module and a link direction determination module. For the undirected link prediction, depicted in Fig.(a), we begin by extracting patch embeddings using a Vision Transformer (ViT)pre-trained on ImageNet, and apply the Low-rank Adaptation (LoRA)layer for fine-tuning. The distance between embeddings reflects the similarities of images and is targeted to fit the actual graph structure. During the training phase, as shown in Fig.(b), patch weights from the pre-trained ViT model are applied to help localize the manipulated regions. Moving to the source/target identification phase, shown in Fig.(c), image embeddings generated by a second feature extractor are processed by the Graph Structure Masked Attention Encoder. It produces the precedence embeddings for each image to represent its provenance position in the latent space and the virtual embeddings to help construct the direction matrix. The final step involves element-wise multiplication ofandto form the directed adjacency matrix, which represents the directed provenance graph.

SECTION: 
Constructing a provenance graph fundamentally relies on identifying the manipulation relationships. In the link prediction stage, we aim to determine these relationships, represented as links connecting image nodes, without determining the direction. The underlying principle is that as an image undergoes more modifications, its similarity to its original state decreases. This principle guides the calculation of dissimilarities across all image pairs to identify potential links.

Previous work has calculated image dissimilarity based on color distribution or by quantifying manufactured features like interest points. However, these image-matching methods, not originally developed for provenance tasks, often must adequately address the nuances of global or local manipulations. To address this problem, learning-based methods have been introduced to extract image features that accurately reflect the dissimilarities caused by various manipulations. However, existing provenance datasets typically need the specification of manipulated regions, making it challenging to train models directly at the image level to capture subtle manipulation traces. Current learning-based approaches often rely on manually created near-duplicate small patches with predefined manipulations. However, this approach can lead to sub-optimal performance due to potential misalignment with the types or intensity of manipulations in the original datasets. Moreover, it limits the scope to isolated patches, losing focus on adjacent regions or the entire image.

To overcome these limitations, we propose a novel method of learnable embedding that captures local and global transformations directly from the given data. This approach allows us to utilize near-duplicates directly from the provenance dataset while preserving the original topological proximity and other key underlying information. By doing so, we can better address the challenges of detecting localized and image-wide manipulations. The specifics of this method will be detailed in subsequent sections of this paper.

The learning-based method for link construction aims to encode transformation information into the extracted features properly. Intuitively, larger image dissimilarities should correspond to greater distances between extracted feature embeddings. Previous approaches, such as the one demonstrated by Li et al., have proposed to encode images into an embedding space that minimizes the distance between closely related images while maximizing it for those with longer manipulation paths. As the image-level approach struggles to capture small but significant local changes, such as minor alterations from small donors, our work adopts a more reliable strategy: We split each image into patches and calculate the distances between corresponding patch embeddings.

In our approach, all original images are uniformly rescaled topixels using bicubic interpolation techniques to standardize input dimensions and patch partition. The distance between any two images,is quantified by the pairwise patch distance as:

where thedenotes the Euclidean distance between corresponding-th patch embeddings fromandat the same spatial position:

To extract patch embeddings and preserve the inherent contextual understanding of images, we leverage the pre-trained ImageNet Vision Transformer (ViT)as our backbone model, utilizing the patch embeddings from its last hidden state. As illustrated in Fig., we incorporate Low-Rank Adaptation (LoRA)layers into the attention blocks of the encoder for fine-tuning. This approach allows us to maintain parameter efficiency while making targeted adjustments to the output embeddings. Specifically, in each attention block, the output featureis derived from the input featureutilizing the original frozen ViT weightsand two trainable low rank matrices:

where,and,with.
The ViT hidden state dimensionis set to 768, and the rankis set to 16. As the positional encoded image patches pass through the transformer layers, the resulting output hidden states provide the patch embeddings used for calculating distances.

Manipulation techniques can be broadly categorized into two main classes:and. Global transformations affect the entire image, while local transformations influence specific regions. The path length between nodes in provenance graphs can reflect the overall dissimilarity between images. However, this path length cannot be directly applied to optimize patch embeddings due to the lack of information about the manipulated area. To address this challenge, we propose to leverage weights from a pre-trained model to guide the learning of patch embeddings. It enables the model to properly focus on the transformed patches, regardless of whether the manipulation is global or local.

Specifically, while not explicitly designed to reflect distances in the provenance map, the pre-trained feature extractors still offer valuable insights into distinguishing manipulated regions. When an image undergoes global manipulation, all patch embeddings are transformed similarly in the feature space. Conversely, the transformation degree of patch embeddings in the affected areas will be greater for local manipulations than in unaltered regions.

During training, as Fig.(a) and Fig.(b) show, the learnable patch embeddingsof all filtered images are extracted along with the fixed patch embeddingsfrom another frozen ViT module. For each image pair, with, we calculate the pairwise patch distancesbetween learned embeddings andbetween fixed embeddings of patches at the same position. The weight of each patch pair is calculated by applying softmax to distance of pairwise fixed embeddings:

As Fig.depicts, the weights assigned to individual patches vary based on the region of the image modifications. For localized changes, the weights are significantly higher on the affected patches, enabling our model to focus on these areas of interest. In contrast, global transformations result in more uniform weights across all patches. By multiplying these weights with the corresponding pairwise patch distances, we compute a weighted distance for each image pair:

From the perspective of each image, the one most similar to it typically has the highest probability of maintaining a link. For each image, we apply normalization to all distances from other images to. To facilitate optimization, we convert these normalized distances into probabilities of not having a link to the image:

whereis the number of images in each provenance graph.

To ensure that the learned embeddings accurately capture the dissimilarity and preserve the topological structure of the provenance graph in our embedding space, our objective is to align the ranking of probabilities with the distance of paths depicted in the ground truth data. Specifically, we aim to achieve a target where paths with fewer hops correlate with a lower probability value. To achieve this, we minimize the following loss function:

is the ground path length, andis the number of images in the filtered image set. Therepresents the-th manipulation path that starts from node, andis the number of steps between nodesandif a path exists from nodeto node. Otherwise, it is set to.

After training, the patch embeddings extracted should be better suited for representing the transformation traces of images and effectively measuring the dissimilarity between them for predicting links. We use these embeddings to calculate distances between image pairs, constructing a dissimilarity matrixthat encapsulates the relationships between all possible pairs of images from the filtered image set. Corresponding to Eq., the elementin the matrix equals to. Since this distance is unaffected by the order of the imagesâ€”meaning=, the resulting dissimilarity matrixis symmetric.

As previously discussed, the predicted provenance graph should be connected, with each node having at least one adjacent node with the lowest dissimilarity. To construct the undirected provenance graph based on this precondition, we apply Kruskalâ€™s Minimum Spanning Tree (MST) algorithmto the symmetric dissimilarity matrixto build the links. The algorithm operates by adding the edges between nodes in ascending order based on their dissimilarity values, ensuring no cycles are formed until all images are connected. The final structure of this graph is represented using an undirected adjacency matrix, where the elementis set to 1 if there is a link betweenand, and 0 otherwise.

SECTION: 
The direction of manipulation operations is another critical factor in provenance analysis. Several studies have attempted to determine it by comparing the mutual information of pixel valuesor by utilizing forensic trace detectors. However, recognizing the source and target of each transformation based solely on the visual content of image pairs presents significant challenges. This difficulty primarily arises from the fact that some transformations can be reversed, which renders the order ambiguous. While other studies have explored the hidden relationships in metadata to reveal the order of manipulations, the ease with which metadata can be manipulated and modified undermines the reliability of this method.

We propose a more robust and accurate method beyond analyzing isolated image pairs. Specifically, we focus on learning from the overall structure of a provenance graph and perform direction determination using learnable precedence embeddings extracted from the graph structure masked attention module. The detailed implementations will be described in the subsequent subsections.

We leverage the asymmetry within the provenance graph to address the challenge of determining transformation directions. Recognizing that the provenance graph is a Directed Acyclic Graph (DAG), we exploit its topological ordering property. This property ensures that nodes can always be arranged in a descending sequence where each edgeâ€™s start node precedes its end node. We propose assigning each imagea distinct precedence embeddingin the latent space, representing its position within the provenance graph. For any image pair with a transformation relationship, the precedence embedding ofshould be closer to the most original image.

Given that the roots and leaves in the graph may be multiple, we incorporate the use of the virtual source and target nodes as illustrated in Fig.. The virtual source node represents the origin of all images, while the virtual target node represents the final manipulated version. In the descending sequence of the provenance graph, these virtual nodes serve as the initial starting point and final endpoint, respectively. Our model extracts the precedence embeddingfor existing image nodes, and virtual embeddingsandfor virtual source and target nodes. Using these embeddings, we construct an auxiliary direction vectorin the latent embedding space. This vector provides a directional context that clearly defines forward movements in the graph, expressed as:.

Beyond examining individual links or segments of the graph, we aim for the model to leverage the entire graph network to extract appropriate embeddings of image nodes to determine the direction of links. Numerous studies have explored using Graph Neural Networks (GNNs)for information propagation across networks to generate node or edge embeddings. More recent efforts have adapted the structure of the Transformerto preserve proximity within graphs, demonstrating enhanced generalizability. In this work, as illustrated in Fig.(c), we employ a modified Transformer encoder to convert image features into precedence embeddings. Specifically, as demonstrated in Fig., we use the class embeddings derived from the second ViT-LoRA as the input, which condenses information across the entire image and is comprehensively aware of the provenance relationship. Additionally, we generate precedence and virtual node embeddings by diffusing image information through a modified attention mechanism, which will be discussed in detail.

Unlike previous sequential data processing tasks like natural language processing, the order of input images is irrelevant to the underlying graph structure. It should not influence the resulting analysis in provenance tasks. Some works have utilized absolute graph encoding methods such as the Weisfeiler-Lehman algorithmand the use of Laplacian eigenvectorsin providing robustness against input permutations. Inspired by the concept of relative graph encoding implemented in GraphiT, our approach moves away from traditional positional encoding within embeddings. Instead, we emphasize preserving topology information by integrating the graph structure directly into the attention mechanism. Given that the original self-attention mechanism interprets the input sequence as a fully connected network and applies attention to all input embeddings, we modify the attention blocks as Fig.:

whereare query, key, and value matrix transformed from input embeddings by multiplying the corresponding weight matrices,is the graph structure mask to limit the scope of attention only on the connected nodes,is the output dimension which is the same as input in this work, anddenotes the element-wise multiplication operation.

Before processing the image embeddings through the graph encoder, we append additional embeddings for the virtual nodes as illustrated in Fig., which is randomly initialized. Since these virtual nodes connect to all other nodes, they need attention on all real nodes to facilitate the learning of aggregate representations of the provenance graph. It is crucial to note that to prevent undue information diffusion among the real image nodes via the virtual nodes, the virtual node attention implemented is unidirectional. With these considerations, we build the graph structure mask:

where the parametercontrols the ratio of attention a node directs towards itself versus its adjacent nodes and is set to,is the number of filtered images,is the identity matrix,is the adjacency matrix of an undirected graph. During the training phase,is constructed from the ground truth, while during inference,, which is derived from the link prediction process.

After obtaining the precedence embeddings for each image node and the virtual source/target node embeddings, we utilize them to determine the direction of links. This is achieved by constructing a square direction matrixwith dimensions matching the number of images in the provenance graph. The elements ofare calculated as follows:

Eq.computes the difference in the projection of the source and target embeddings onto the auxiliary direction vector, which signifies the directionality of the transformation. Largersuggests a higher probability thatis the ancestor ofin the transformation relationship.

During training, to ensure that the learned embeddings accurately reflect the position of the graph in the latent space, we minimize the following loss function:

whereis the directed adjacency matrix, withindicating a directed link from nodeto node. This loss function penalizes discrepancies between the predicted direction matrixand the ground-truth represented by.

With the predicted undirected adjacency matrixfrom the link prediction stage and the direction matrix, the directed adjacency matrixcan be constructed through element-wise multiplication with the Heaviside step functionapplied to:

The resulting matrixrepresents the final directed provenance graph, where each entryindicates a directed connection from imageto image.

To train the end-to-end provenance analysis model, we combine two loss functions corresponding to link prediction and direction determination stages into a single loss function and then minimize it to optimize both stages of the model simultaneously:

whereis a coefficient used to balance the relative importance of these two sub-loss functions during the learning process, this integrated loss function ensures that the model effectively learns both to predict the connections between images and to accurately identify the source and target images of each link within the provenance graph.

SECTION: 
SECTION: 
We implemented the experiments on the Pytorchplatform and trained concurrently for 30 iterations. The model utilizes a LeakyReLU loss function with a slope of 0.1 and the AdamW optimizerfor training. The final loss function incorporates a balancing parameter. We maintain a constant learning rate ofthroughout the training period without decay. The model is trained on four RTX3080 GPUs with a batch size of 4. Our modelâ€™s backbone includes a frozen ViT and two ViT-LoRA modules for image feature extraction. All components are initialized with weights from ViT-B pre-trained on the ImageNet-21k dataset. During the training phase, we derive graph masks used in the attention blocks directly from the ground-truth provenance graph. During inference, the graph mask is determined by the undirected adjacency matrix generated by the link prediction section of the model.

The Open Media Forensic Challenge (OpenMFC) has released several provenance datasets, which include a large-scale collection of "world images" and "probe images." World images constitute a broad pool of ancestors or offspring to the probe or entirely irrelevant items. Probe images serve as the queries that guide the filtering process to identify and retrieve related images from the world dataset. To eliminate the influence of filtering, they provide all related images corresponding to the probe image for the oracle task. Each provenance graph may start from multiple root images, and the other images are manipulated from the roots with various image-editing tools. The datasets document the image modification history, ranging from local manipulations like copy-paste, removals, and donor changes to global manipulations such as compression, contrast modifications, scaling, sharpening, and blurring.

Another provenance dataset is derived from Photoshop battles in the Reddit community. In these battles, community users start with one provided image and creatively create and modify different versions. This activity subsequently generates provenance graphs. Compared with OpenMFC datasets, Reddit dataset contains much more images in each provenance case, and most of the images are directly manipulated from one root image. Each datasets applied in the experiments provide ground-truth provenance graphs in nodes and directed links. We randomly selectof cases for training and the remainder for evaluation. Due to partially disabled hyperlinks and missing images, our study only incorporates those cases that are still accessible and valid, ensuring that the provenance graph remains connected. The detailed information about the datasets is described below:

Dataset contains 394 provenance graphs and 6,498 world images. Each graph contains between 3 and 44 images. The number of edges per graph varies from 2 to 56, with an average of 19 edges per graph.

Dataset contains 178 provenance graphs and 1,861 world images. Each graph contains between 5 and 42 images. The number of edges per graph varies from 5 to 46, with an average of 11 edges per graph.

Dataset contains 118 provenance graphs and 3,092 world images. Each graph contains between 16 and 78 images. The number of edges per graph varies from 15 to 77, with an average of 48 edges per graph.

In our study, we adopt the evaluation metrics specified by the OpenMFC to assess the accuracy of provenance graph construction. These metrics evaluate the consistency between the constructed provenance graphand the ground-truth graph. A higher metric score indicates a more accurate reconstruction of the original provenance history. We primarily employ two metrics: Vertex Overlap (VO) and Edge Overlap (EO). Both metrics calculate the F1 score, which is the harmonic mean of precision and recall, for the predicted nodes and edges, respectively:

Additionally, we use the Vertex Edge Overlap (VEO) to measure the overall graph overlap, providing a combined F1 score for both nodes and edges:

The Vertex Overlap (VO) measures whether the method can correctly include all images in the graph, while the Edge Overlap (EO) assesses the accuracy of the connection between images. The Vertex Edge Overlap (VEO) combines both measures, providing an overall score of how well our predicted graph matches the ground-truth. These metrics are applied to individual graphs; we then compute the average of these scores to obtain the final evaluation results.

SECTION: 
In our research on image provenance analysis, we rely upon specialized data augmentation techniques to enhance the diversity and volume of our training datasets. This process differs from traditional computer vision tasks, requiring consideration of both images and the underlying graph topology representing correct relationships. The designed augmentation strategy should preserve the integrity of the original provenance information within graph structures, and avoid modifications that could obscure original provenance paths or detrimentally impact model performance.

The data augmentation methodology encompasses two approaches. First, we modify existing images and introduce additional branches from specific nodes within the provenance graphs. This technique increases graph complexity and extends manipulation chains, creating more intricate structures. We leverage global manipulation techniques: brightness, saturation, contrast, sharpness, and blur, which are outlined in the the OpenMFC datasets, to introduce new nodes, effectively. Second, we selectively reduce graph size by removing nodes with a single ancestor and their associated links, directly connecting the ancestor node to any offspring of the removed node. This method generates smaller graphs with links representing multiple modification steps, improving our frameworkâ€™s performance under varied structural conditions. Throughout this process, we provide a broader spectrum of example cases for enhancing the robustness and effectiveness of our provenance analysis framework with preservation of original provenance information, ensuring that our augmented datasets contribute meaningfully to the modelâ€™s learning without introducing misleading or distorted relationships.

SECTION: 
To assess the effectiveness of the proposed framework, we conduct a comprehensive comparison with other available vision-content-based methods. In the prior art, the evaluation process considers the two-stage of provenance graph construction: link prediction and direction determination. For each stage, we select several approaches as evaluation baselines.

In link prediction for image provenance, baseline techniques are categorized into two main types: interest point detectors and learned feature descriptors. Interest point detectors, such as SIFT and SURF, are employed in research by Moreira, and Zhangto identify distinctive image features. Following the algorithm in, the dissimilarity between image pairs is assessed by counting the top 2000 SIFT keypoints matches.

Learned feature descriptors, conversely, encapsulate image content into embeddings using machine learning techniques. For our comparative analysis, we select two image feature extraction models pre-trained on ImageNet: ResNet-50and the base Vision Transformer (ViT-B). Additionally, we assess the Transformation-Aware Embeddings (TAE) framework, specifically designed to discern transformations between images. TAE employs a four-way Siamese neural network trained with quadruplet image samples and the rank-based Edit Sequence Loss. Due to the unavailability of their handcrafted dataset, we extract quadruplet samples directly from corresponding training cases. To ensure evaluation consistency, we adhere to the settings described by Bharati et al.for both ResNet-50 and TAE frameworks. The ViT-B framework generates patch embeddings in the same manner as our proposed method. The dissimilarity between images is determined by summing the pairwise distances of patch embeddings for all these approaches.

We then examine two established methods to determine the direction of manipulation between pairs of images. The first approach, rooted in information theory, calculates the mutual information (MI) between image pairs. This method aligns matched interest points from one image to another, mapping related regions from imageonto. The method detects asymmetries indicative of an ancestor-offspring relationship by assessing the mutual information within pixel values in aligned regions. It leverages variations in forward and backward homography to identify potential donor images.

Additionally, we explore another baseline method inthat employs a trained forensic tool to evaluate image integrity. This approach assigns an integrity score to each image, operating under the assumption that manipulated images typically exhibit lower scores compared to their original versions. The manipulation direction is then determined by comparing these integrity scores among linked images. For our experiment, we utilize the state-of-the-art framework proposed by Guillaro, employing its pre-trained integrity scorer to conduct evaluations and draw comparisons.

SECTION: 
SECTION: 
The evaluation of the proposed image provenance analysis framework yields insights into its performance relative to established baseline methods. The assessment encompasses both stages of provenance graph construction and the reconstructed provenance graph is represented as a directed adjacency matrix compared to the ground-truth for all methods. To ensure a thorough assessment, we conduct experiments on the following challenging datasets: NC2017-Dev1-Ver1, MFC18-Dev1-Ver2, and Reddit. We present evaluation results using the metrics Vertex Overlap (VO), Edge Overlap (EO), and Vertex-Edge Overlap (VEO) to show the accuracy of predicted graphs. For baseline approaches that focus solely on link prediction and constructing undirected graphs, we pair them with the best previous direction determination methods to generate comparable directed graphs. The following tables present the results of these best combinations alongside our framework, providing average scores and standard deviations from three evaluation runs, with the best results highlighted in bold.

Tableandshow the provenance graph construction results on the NC2017-Dev1-Ver1 and MFC18-Dev1-Ver2 datasets. Our proposed framework outperforms all other methods across all metrics. Notably, SIFT-based methods fail to retrieve all images for graph building, resulting in Vertex Overlap (VO) values below 1. This limitation stems from insufficient matched interest points between image pairs, a critical defect in interest point-based methods. In contrast, all learning-based feature extractors, which utilize patch embeddings to form the dissimilarity matrix, successfully incorporate all nodes into the graphs, achieving perfect VO scores.

The Edge Overlap (EO) metric is more crucial in differentiating framework performance. Our method achieves the best EO result, showing a relative improvement ofon NC2017-Dev1-Ver1 andon MFC18-Dev1-Ver2 compared to other baseline approaches. This enhancement demonstrates our approachâ€™s superior understanding of various manipulation types and transformation directions.

Tableshows the evaluation results on the Reddit dataset. We observe a performance decline across all methods compared to other datasets. This degradation is attributed to the larger number of images and edges in each provenance graph, which inherently complicates the task of accurate link prediction. Despite these challenges, proposed method shows robust performance, achieving aimprovement over the best baseline approach. This observation underscores the effectiveness of our framework in learning transformation relationships from graphs, enabling it to handle more images with greater accuracy. Our methodâ€™s ability to maintain superior performance on this challenging dataset also highlights its robustness in handling more complicated structures.

To better understand our frameworkâ€™s performance, we present a qualitative analysis of a provenance case in Fig.. This visual comparison illustrates the effectiveness of our approach relative to baseline methods. Fig.(a) displays the ground-truth provenance graph, with arrows indicating transformation directions and labels specifying manipulation types between image pairs. Fig.(b) and (c) show results from baseline methods: SIFT and TAE with integrity scorers for direction determination, respectively. Fig.(d) presents the result of our proposed framework. Unlike the SIFT-based method, which omits some images, our approach successfully incorporates all images into the graph structure. Moreover, our method achieves higher accuracy in connection establishment and direction determination than baseline approaches.

SECTION: 
Due to the uncontrollable environments in real-world application scenarios, the trained model inevitably encounters unseen domain data when deployed, resulting in poor testing performance. In the context of image provenance analysis, cross-dataset evaluation presents a persistent challenge due to the variability in manipulation techniques and graph topologies across different datasets. In our cross-dataset evaluation experiments, we train both TAE and our method on the NC2017-Dev1-Ver1 dataset while maintaining the pre-trained weights of ResNet50 as the benchmark.

We test the trained models on the other two unseen datasets as illustrated in Tableand. Our framework, which relies on graph structure for prediction, experiences certain performance degradation compared to intra-dataset evaluations. This degradation is expected and can be attributed to the differences in image manipulation techniques, graph complexities, and data distributions between the training and testing datasets. Despite this challenge, our framework outperforms previous arts, highlighting the outstanding generalizability of our approach even when faced with unforeseen data structures and manipulation techniques.

The superior cross-dataset performance of our approach can be attributed to several factors. Firstly, our local-global joint learning scheme allows the model to capture both fine-grained image features and broader structural patterns, making it more robust to variations in manipulation techniques. Secondly, the graph structure masked attention module enables the framework to adapt to different graph topology, enhancing its performance across datasets with varying complexities. These results highlight the outstanding generalizability of our approach even when faced with unforeseen data structures and manipulation techniques. This robustness is particularly valuable in real-world applications where the nature of image manipulations and provenance graph structures may be unpredictable.

SECTION: 
We conduct ablation experiments to study the impacts of modules in each stage of provenance graph construction. For the link prediction stage, we compared the results of our sub-framework with four baseline approaches using the edge overlap metric without considering the direction. Fig.illustrates the results of these ablation experiments across three datasets. Our analysis reveals that previous learning frameworks perform well on OpenMFC datasets but are less competitive with interest point methods on the Reddit dataset, which contains lots of versions with donors and entirely different backgrounds. Notably, our framework demonstrates significant improvements compared to other approaches across all datasets, underscoring its efficiency in predicting correct transformation relationships. The superior performance of our method can be attributed to its ability to capture both local and global features, allowing it to identify subtle transformations that might be missed by other approaches.

Besides, with given ground-truth undirected graphs, we perform ablation experiments for the direction determination stage and analyze the accuracy of predicted directions. As illustrated in Fig., our framework achieved the best performance in accurately predicting the direction. However, the improvement on the Reddit dataset is not as pronounced as on the other two datasets due to the distinctiveness of its graph topology, which is nearly star-shaped with numerous branches. This suggests that while our method excels in most scenarios, there is still room for improvement in some structures. Nevertheless, our framework still demonstrates superior performance, which can be attributed to the designed graph structure masked attention module. This module enables our framework to capture both fine-grained image relationships and broader structural patterns in the provenance graphs, allowing for effective analysis in complex topologies. In conclusion, the effectiveness of our approach in both stages underscores the importance of considering both image content and graph structure in provenance analysis tasks.

Furthermore, we examine the impact of varying the coefficientin our loss function. We test various values to determine the optimal balance between the components of the loss function. The findings, depicted in Fig., reveal thatprovides the best results on the OpenMFC dataset, indicating a good balance between loss components, enhancing reliability and performance on both stages. However, the Reddit dataset shows slightly better performance at, reflecting its specific requirements for handling more diverse manipulations and complex visual content variations.

To evaluate the impact of our data augmentation techniques on the performance of the image provenance analysis framework, we conducted experiments on both the original datasets and those augmented through the specialized two-stage methods, as previously discussed. As detailed in Table, the model trained on augmented data exhibited higher performance in the link prediction stage using the metric of undirected edge overlap. This suggests that the additional branch images with various random global manipulations enhance the modelâ€™s ability to capture transformation information and identify potential links between nodes. The performance increment for the Reddit dataset was less pronounced, possibly due to its already adequate image count and the presence of manipulation types not covered by our augmentation methods.

We also evaluated the accuracy of direction determination, where the effects of data augmentation were notably positive, as shown in Table. By introducing diverse graph structures through our augmentation techniques, we enabled the model to learn from a broader range of scenarios. This diversity in training data proved especially beneficial for handling the intricate task of determining the directional flow of image transformations. The improved performance suggests that our augmentation strategy effectively enhances the modelâ€™s ability to generalize across various structural conditions, thereby significantly boosting its predictive accuracy and robustness in discerning both the relationship and the directionality of image manipulations within provenance graphs.

SECTION: 
Unveiling the intent behind the spread of manipulated images on social media presents a significant challenge. Image provenance analysis offers an effective solution by tracing the manipulation history through visual graphs, thereby revealing connections among related images. This paper introduces an end-to-end framework designed to analyze the content of individual images and leverage the structural information within these provenance graphs for high-accuracy predictions of directed provenance relationships.

Our approach calculates the dissimilarity between images in the link prediction phase by analyzing the distance between pairwise patch embeddings. To enhance the graph construction performance, we incorporate weights from a pre-trained model into the patches to highlight manipulated regions and utilize transformation paths for optimization. During the direction determination phase, we integrate the graphâ€™s topology with an attention mask mechanism and introduce virtual nodes to improve the prediction of potential manipulation directions.

In conclusion, our proposed learning scheme that encodes graph structures into vision transformers has significantly improved image provenance graph construction performance. Extensive ablation experiments have demonstrated the effectiveness of the designed components, used hyper-parameters, and adopted data augmentation strategy. The cross-dataset evaluations demonstrate that existing methods still suffer from limited generalizability, primarily due to significant deficiencies in training data. While our proposed method has mitigated the problem to some extent, enhancing the modelâ€™s generalization capability remains a grand challenge.

In future work, we aim to develop a more balanced and comprehensive dataset that encompasses a wider variety of manipulation techniques, including advanced deepfake technologies. Additionally, we plan to design a new framework that identifies the transformation types and enhances the richness of information within the provenance graph. We believe these future research goals will better fit real-world application scenarios and significantly improve the accuracy and generalizability of image provenance analysis.

SECTION: References