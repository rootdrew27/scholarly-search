SECTION: Input-Output Optics as a Causal Time Series Mapping: A Generative Machine Learning Solution

The response of many-body quantum systems to an optical pulse can be extremely challenging to model. Here we explore the use of neural networks, both traditional and generative, to learn and thus simulate the response of such a system from data. The quantum system can be viewed as performing a complex mapping from an input time-series (the optical pulse) to an output time-series (the systems response) which is often also an optical pulse. Using both the transverse and non-integrable Ising models as examples, we show that not only can temporal convolutional networks capture the input/output mapping generated by the system but can also be used to characterize the complexity of the mapping. This measure of complexity is provided by the size of the smallest latent space that is able to accurately model the mapping. We further find that a generative model, in particular a variational auto-encoder, significantly outperforms traditional auto-encoders at learning the complex response of many-body quantum systems. For the example that generated the most complex mapping, the variational auto-encoder produces outputs that have less than 10% error for more than 90% of inputs across our test data.

SECTION: IIntroduction

Optics studies light and its interactions with matter. As depicted in Fig1, the study comprises three main components: a quantum system, the incident pulse of light (optical input), and the system’s response (optical output) to the incident pulse. The system’s response usually varies depending on the input signal. The type of input signal can determine whether the dynamical system behaves linearly, non-linearly, or chaotically (a specific type of non-linear behavior characterized by extreme sensitivity)[1]. If the input is weak, we have what is called linear optics. If the input intensity increases, we have a non-linear response/output signal, and the output can be expressed as a perturbation series expansion of the input[2]. As the intensity of the input field increases further, the system exhibits even more pronounced non-linearities requiring non-pertrubative treatment[3], one of the most significant beingHigh Harmonic Generation(HHG). This strong-field laser–matter interaction resulting in HHG has attracted a lot of attention, and its discovery has been recognized with the 2023 Nobel Prize in Physics. HHG has been extensively studied in various media, including gases, semiconductors, nano-structures, metal, strongly
correlated materials, and solids[4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26]. More recently, there is mounting interest in HHG from quantum spin systems due to their potential applications in probing spin dynamics and developing new laser sources in the THz regime[27,28,29,30].

In ultrafast spectroscopy, HHG is used to create attosecond pulses[31], which allow for unmatched temporal resolution of the observation of electron dynamics in real-time. Also, recent analysis suggests that HHG enables single-atom computing[32]and improved spectroscopic characterization of mixtures[33]. Furthermore, coherent extreme ultraviolet and soft X-ray light generation is made possible by HHG, and is an essential technology for high-resolution imaging and nanolithography in the semiconductor manufacturing industry.

The optical output (Fig1) is obtained theoretically by solving the time-dependent Schrödinger equation. Solving this equation rapidly becomes prohibitively expensive, as finding its solution is exponentially difficult with respect to the size of the quantum system with which the input pulse interacts. The complexity increases further if the quantum system is undergoing a phase transition. HHG of a phase transition has also been observed experimentally[34]. Understanding phase transitions is important for various reasons[35,36,37,38]. For example, in quantum computing, the transverse field Ising model is significant for quantum annealing, where phase transitions can influence the performance and efficiency of quantum algorithms[39]. The transverse Ising model is well known to exhibit a quantum phase transition and is analytically solvable[40]. However, an Ising model with mixed transverse and longitudinal magnetic fields can only be solvednumerically[41]. In such regimes, small changes in system parameters can lead to large-scale changes in the system’s dynamics, requiring extremely precise calculations that are challenging to achieve by numerically solving the Schrödinger equation. Alternative methods are needed to accurately model the behavior of such systems.

In this paper, we propose a novel data-driven methodology to model the inputoutput dynamics of optical systems using machine learning. We utilizeTemporal Convolutional Networks(TCNs) to capture the intricate interactions in many-body quantum systems by formulating optics as an input-output time series problem. Additionally, we demonstrate how distinct physical regimes, including phase transitions, can be captured with machine learning models (see Fig.1), tailored to the complexity of each regime.

The TCN is a recent addition to the toolbox of time series analysis. TCNs use temporal convolution operations to enforce causality. By definition, the output of a convolution at each time step is solely dependent on the past and not influenced by the future[42]. Further, the temporal dilation in a TCN helps in capturing long-range temporal patterns[43]. The efficacy of TCNs has led to their increasing appeal across multiple domains. Though more recent than recurrent neural networks (RNNs) and standard convolutional neural networks (CNNs), TCNs have already produced several noteworthy studies[44,45,46,47,48,49,50]. In Ref.[50], a study compared a TCN to a Long Short-Term Memory (LSTM) neural network for weather prediction, showing that the TCN demonstrated superior performance in time-series data forecasting.

A conventional application of machine learning in time-series analysis is forecasting[51,52]. A notable example is Ref.[53], where using a historical record (e.g., the training data shown in Fig.2) for a time-domain HHG output, a model was constructed to predictfor future times (see Fig.2). It is important to note that this modeling does not utilize information across different input pulses. Hence, each time the incident laser pulse is changed, a new model needs to be trained. In the present work, we depart from this forecasting approach by training a TCN to learn the entire inputoutput relationship, employing a dataset of pairs of input and output pulses.

It is noteworthy that the inverse problem of characterizing outputinput – finding the optical input that produces the desired output – has also been studied. Using quantum tracking control[54,55,56,57,58,59,60], it was proven[61,62,63]that by shaping the incident optical pulse, any desired output can be produced. This possibility implies that any two systems can be made spectrally identical, realizing an aspect of the alchemist’s dream to make lead look like gold. We note that the outputinput relation has previously been modeled using a CNN[64]by first converting the inputs into images.

The application of machine learning to HHG has been prolific. However, unlike our current work, the previous studies have not enforced causality. These include applications in harmonic generation microscopy[65], high harmonic spectroscopy in solids[66], predicting characteristics of harmonic generation in plasmas using particle-in-cell simulations[67], predicting HHG emission from molecules and inversely predicting molecular parameters from the corresponding HHG spectra[68], time series forecasting of HHG spectra[53], and predicting HHG from the spatially structured input fields[69].

Reformulating optics as an input-output time series analysis not only enables us to obtain quantitatively accurate models, but also allows us to import other techniques from the vast toolbox of time series analysis. For example, we introduce two new measures to quantify the complexity of optical dynamics. Traditionally, the complexity of a system’s dynamics has been assessed through the order of the perturbation series between the input and output[70,2,71,72]. The higher the order of the perturbation series, the more complex the dynamics.

The first method we introduce to quantify complexity, even in the non-perturbative regime, involves using the TCN to construct a model with an autoencoder architecture (Fig.3). The purpose of the encoder is to compress the input data to its essential components (the “latent space”), and the necessary transformations are perform on this compressed data. The decoder then recovers the output from the transformed data. The dimension of the latent space – the number of neurons it consists of – can be taken as a measure of the complexity of the input-output relation. The larger the dimension, the more complex inputoutput transformation. It is noteworthy that this measure is reminiscent of the Kolmogorov complexity – the length of the minimal program computing the desired output.

The second method for non-perturbative quantification of the complexity of the input-output dynamics employs amplitude-aware permutation entropy[73]of time series data, which provides a statistical measure of the system’s disorder and complexity. To the best of our knowledge, neither the architecture of the machine learning model nor the amplitude-aware permutation entropy has been utilized in optics.

The rest of the paper is organized as follows: In Sec.II, we comprehensively formulate the problem under investigation and provide details of the Ising system under consideration and various cases associated with it. SectionIIIis dedicated to presenting our results; we state details about the machine learning model architectures employed (Sec.III.1), evaluation criteria (Sec.III.2), and illustrative examples (Sec.III.3) that demonstrate the applicability of machine learning to the cases discussed earlier. Further, in AppendixAwe discuss the use a more sophisticated machine learning architecture for better predictability.

SECTION: IIFormulation the of problem

Due to recent theoretical interest in magnetic HHG using spin systems, we will study the input-output relationof the incident time-varying magnetic fieldand magnetizationfor both the transverse and non-integrable Ising models. Note that time-varying magnetic fields, which are electromagnetic pulses with strong magnetic fields and low electric fields, can now be experimentally produced[74].

The Hamiltonian of the transverse quantum Ising model reads

whereandare theandPauli matrix acting on spin,denotes the coupling energy of neighboring spins andis the input magnetic field. Ifand all the couplingsare positive, we recover the longitudinal Ising Hamiltonian with a ferromagnetic degenerate ground state. If the external magnetic fieldis sufficiently strong, a phase transition from a ferromagnetic to a paramagnetic state takes place.

The non-integrable Ising model, which is an Ising chain with transverse () and longitudinal () constant magnetic fields, has the Hamiltonian of the form

Quantum dynamics generated by Hamiltonians (1) and (2) is rich due to entanglement and phase transitions[75,76]. We tune the parametersin Eq (2) to work in the phase transition regime.

For both the systems, we take the magnetization in the-direction as an optical output

wheredenotes the averaging over the time-dependent wave function.

To analyze the inputoutput relation,, via time-series analysis, we generate data sets by solving the Schrödinger equation with Hamiltonians (1) and (2) for. (For this we employ the QuTiP library[77,78].) We vary the site-to-site interaction strengths across the sites as, which further adds to the complexity of the system response.

When performing numerical calculations, time is discretized into equally spaced small intervals of. Hence, instead of the continuous output signal, we obtain the-dimensional vectorwith components, where. Thus, the problem of analyzing the inputoutput relation,, is equivalent to a vectorvector transformation, whereis the time-discretized input field with components,.

For different datasets of samples of the inputoutput pairs, wherelabels a sample, we build different TCN models of the time series transformation. In particular, we study the following three distinct cases in the subsequent sections:

Modeling the inputoutput relation for the transverse Ising model (1) based on a pre-calculated data setof input-output pairs of time series,

whereis the’th component of the’th sample input vector. The outputs are induced by monochromatic inputs of different frequencies but identical amplitude. To generate the data set the Schödinger equation is solvedtimes with incident fieldsto obtain the corresponding responsesvia Eq. (3).

Modeling the inputoutput relation for the transverse Ising model (1) from dataset[Eq. (4)], i.e., for strong driving field.

Modeling the inputoutput relation for the non-integrable Ising model (2) based on dataset[Eq. (4)], with the parameters of Hamiltonian (2) set to,(see[76]).

Modeling the inputoutput relation for the non-integrable Ising model (2) based on datasetwith the parameters,. We note thatCase 4is more chaotic thanCase 3.

Modeling the inputoutput relation for the transverse Ising model (1) based on the data setof inputs of varied intensity and frequencies,

whererepresents the’th component of the’th sample of input vector corresponding to amplitude. Also, note thatanddenote the smallest and largest amplitudes, respectively. To obtain this data set, the Schödinger equation must be solvedtimes.

Datasets (4) and (5) are quite peculiar from the point of view of ML applications.
Dataset (4) consists of two 2D arrays:storing input pulses,storing the induced output. Each of these arrays has a shape of, where rows correspond to frequency and column to time. Correspondingly, dataset (5) is a made of two 3D arrays of shape, where the first dimension corresponds to the varied amplitude, the second to frequency, and the third to time.

The studied transformationis in fact a multi-input and multi-output problem. The number of input and output features is– the dimension of vectorsand, thus leading to a high-dimensional dataset. When dealing with high-dimensional data, the curse of dimensionality[79]is encountered. To deal with this issue we increase the number of rows (that is, increase the number of frequency values). Another approach to addressing the challenges posed by high-dimensional time series data is through data compression techniques[80,81]. This method is preferable when the dataset is small. However, we employ the former approach here.

URE

SECTION: IIIResults

SECTION: III.1Model Architecture

The choice of an effective model architecture is important in ML. In our analysis, we use autoencoder TCN architecture. To elucidate our approach, we first explore the encoder-bottleneck-decoder structure inherent to autoencoders within the context of deep neural networks. In Fig.3, we present an example of a deep neural autoencoder architecture with a configuration of (8-4-2-4-8). This notation means that there are 8 neurons in the input layer followed by 4, 2, and 4 neurons in the subsequent three layers, respectively, and 8 neurons in the output layer. For simplicity, we will denote such a symmetrical architecture in short notation as (8-4-2), which represents the encoder part of the network, with the understanding that the decoder is its mirror image. The encoder takes an input and compresses it into a lower-dimensional representation, called a bottleneck or latent space. The latent space is a smaller vector representation of the input data. The decoder takes this latent representation and reconstructs the output[82,83,84].
Similarly, we can have a TCN based autoencoder. An illustration of (8-6-6) autoencoder TCN framework is shown in Fig.4.
We see the temporal 1D convolution layers along with max-pooling layers. In our auto-encoder based architecture, we use kernel/filterand dilations ().

SECTION: III.2Quantification of Predictive Quality

We begin by introducing the problem of ML reproducibility. It is well-known that for a deep learning model, various factors can cause different outcomes in terms of accuracy in different runs, even with identical training data, identical algorithms, and identical networks[85]. A simple way out of this problem is to run model training multiple times and record the accuracies of obtained models. If in multiple training runs, the accuracies do not fluctuate much (i.e., stay within), we can safely conclude that the model is stable. In this paper, we find theminimum stable architecturerequired to fit the input-output model.

We generate input-output datasets forCases 1 - 4by settingandin Eq. (4). Therefore, for dataset pair, the’th sample/row has input vectorwith the corresponding output vector. After splitting the dataset into trainingand testingsamples, we train a TCN model on the training sample. The obtained model can be used to produce the pair of data, whereis the model’s predicted output time series for the given input time series. Similarly, forCase 5, we generate the input-output dataset by setting,andin Eq. (5), resulting a total ofsamples. We specifically study the case of,with.

To quantify the model’s predictive power, we calculate the R-squared valuefor entry in the test set as follows:

The model is considered to be stable if, across multiple training runs, more than 85% of the R-squared values in the setare above 0.85.

SECTION: III.3Illustrations and Discussions

The minimum autoencoder TCN architecture forCase 1in Sec.IIis (5-5-3) with 2082 trainable parameters. To evaluate the stability of the model architecture, we trained 10 different models of the same architecture. For each such model, aboveoflie above, indicating stability of the architecture. Fig.5illustrates the performance of one such model.

Similarly, inCase 2of Sec.II, the minimum stable autoencoder TCN architecture is (12-12-10-10) with 16127 trainable parameters. We again performed a stability assessment by training 10 models of the same architecture and found that for any model, aboveoflie above are above(see Fig6for one such model). The need for a deeper architecture suggests that the dynamics inCase 2are more non-linear then inCase 1, resulting in a more intricate output dataset. An independent complexity analysis in the next section will confirm this. Furthermore, we also applied a variational TCN autoencoder (VAE) architecture to case 2 to see if this would provide a significantly better model. It did so and we present the results in appendixA.

InCase 3of Sec.II, the minimal stable autoencoder TCN architecture is (5-5-4) with 2,234 trainable parameters. We trained 10 different models using this architecture and observed that, for each model, overofconsistently exceed 0.95 (see Fig7illustrating one such model). InCase 4we observe a stable TCN architecture has a configuration of (8-8-6) with 5,439 trainable parameters. Similar toCase 3, we trained 10 models with this architecture. For each model, overofalso consistently exceed 0.95 (see Fig8showing performance of one such model).

InCase 5, we evaluated 10 distinct models with an (8-8-4) architecture with 3,221 parameters. Remarkably, all models achieved an R-squared value exceeding 0.95, with at leastofsurpassing this threshold.

In all the cases treated, we ensured a similar train-test split of the data and kept the number of epochs the same. For all our training processes we used the swish activation function[86], the Adam optimizer with a learning rate of 0.001, and and for the loss function we used the Huber loss[87]. Additionally, we employed the technique of changing batch sizes[88]. For all cases we used TCNs with dilation factors of 2, 4, and 8 to effectively capture long-range dependencies in the dataset. This approach allowed us to expand the receptive field of the convolutional layers. Further, in all plots, the input, output, and predicted values are scaled between 0 and 1 because the normalization facilitates a consistent comparison. The plots remain consistent and accurate after applying the inverse transform.

We note that as the complexity of input-output data increases, it is important to examine the behavior of the latent space within the encoder-decoder model. The encoder and decoder collaboratively compress the data into a lower-dimensional representation known as the latent space. When the complexity of the input-output relationship increases, the dimensionality of the latent space must also increase. If the latent dimension is reduced beyond a necessary threshold, the compression quality deteriorates, leading to an insufficient capture of essential features in the latent space. Thus suggests that the size of the smallest latent space that is able to produce an accurate model for the input-output system is a good measure of the complexity of the system. Among the five cases we considered this measure suggests thatCase 2is the most complex. In the next section we evaluate the complexity of the input-output transformations using a more traditional measure of the complexity of time-series. This provides additional evidence that the size of the latent space is a good measure of the system complexity.

SECTION: III.4Complexity Analysis

Entropy provides a valuable metric for assessing the level of uncertainty and irregularity within a signal or system. Among the various entropy measures, the permutation entropy is known for its robustness in analyzing the complexity of time series data[89,90]. However, since HHG dynamics strongly depends on the amplitude of the incident pulse, we use the amplitude-aware permutation entropy by deploying the package Entropy Hub[91], which overcomes limitations of the traditional permutation entropy[92].

We now describe how we employ the amplitude-aware permutation entropy to obtain a measure of the overall complexity of the output dataset generated for any amplitude. In data set (5) for quantum system (1), we set,and. The minimum and maximum amplitudes taken areandrespectively. For each amplitudefrom data set (5), we compute the mean normalized permutation probabilities (Pnorm)(with embedding dimension equals) of the elements in set. This setconsists ofvalues,, each representing the Pnorm calculated for thesample in the output dataset. Our results indicate a consistent increase in Pnorm values with increasing amplitude, reflecting a corresponding rise in the complexity of the output dataset. We also applied the same method forCases 3and4of Sec.II. We observed that the mean normalized permutation probabilities for these amplitudes were higher, indicating increased complexity in the dynamics during phase transitions.

Figure9shows the complexity as a function of the input amplitude for the Ising systems (1) and (2). The amplitude-aware permutation entropy ranks the cases in the same order of complexity as the size of the minimal viable latent space for an auto-encoder network.

SECTION: IVConclusion

In this study, we demonstrated the application of Temporal Convolutional Networks (TCNs) for input-output time series analysis in the context of optics, specifically focusing on quantum spin systems subjected to an external time-dependent magnetic field. We have shown that given a time series input-output dataset, there exists a unique minimum TCN architecture that is able to accurately model the input-output mapping. In particular we propose that the minimum viable dimension of the latent space can be used to quantify the complexity of the underlying system.

Our findings show that systems in non-chaotic regimes can be modeled with much simpler TCN architectures compared to chaotic regimes, reflecting inherent complexity differences. Additionally, we compared our TCN-based complexity measure with amplitude-aware permutation entropy and found that both measures align, ranking the respective mappings in the same order of complexity. We note, however, that the definition of entropy in time series data remains an active area of research; there is no universally accepted method due to the diverse nature of time series characteristics and the varied objectives of entropy analysis[93,94,95,96,97,98,99,100,101,102]. The proposed latent-space measure not only contributes to the existing repertoire of complexity measures but is argued to be more robust. This robustness stems from the efficiency of convolutional networks in feature extraction. We suggest that TCNs capture the complexity of time series data more effectively than alternative methods.

In our future work, we aim to address issues identified with our current approach. One primary concern is the lack of smoothness observed in the predicted output. To address this, we will investigate potential causes of this phenomenon and explore whether post-processing can mitigate it. Additionally, we will explore algorithms beyond TCN to improve model robustness and efficiency, aiming for better performance and reduced computational time.

SECTION: Appendix AUsing a Variational Autoencoder

Variational autoencoders (VAEs)[103,104]are advanced versions of traditional autoencoders (AEs) with the ability to extract higher-level abstractions from input data. The key difference between VAEs and AEs lies in their approach to latent space representation. VAEs employ a probabilistic latent space, enabling them to capture a richer and more structured representation. In contrast, AEs rely on a deterministic latent space, which can be less organized and may limit their capacity to generalize. Since VAEs generate outputs from a learned probability density, they are typically regarded asgenerativemodels.

In a VAE, each input is encoded into a mean and variance, which define a probability distribution (typically Gaussian). During training, the model samples from this learned distribution to generate latent variables. This sampling process allows VAEs to learn robust features. The latent space is regularized to follow a known distribution using the Kullback-Leibler divergence, incorporated into the loss function alongside the reconstruction loss. (Minimizing the reconstruction loss ensures that the model reconstructs the input data samples as accurately as possible, contributing to a robust latent space.)

We implement a TCN VAE forCase 2of Sec.IIwith a TCN architecture consistent with our previous design: (12-12-10-10), incorporating additional sampling layers. The number of parameters is 34,127, greater than the number of parameters in the traditional AE used in Sec.III.3. We trained 10 different models with the (12-12-10-10) TCN VAE architecture and found that these models were able to reach a Huber Loss value ofwithin a small number of epochs, significantly lower than– the minimum loss achieved by AE. Furthermore, we found that overof the R-squared values inalso consistently exceededas shown in Fig.10.

SECTION: References