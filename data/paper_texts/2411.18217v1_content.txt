SECTION: How to Learn a New Language? An Efficient Solution for Self-Supervised Learning Models Unseen Languages Adaption in Low-Resource Scenario

The utilization of speech Self-Supervised Learning (SSL) models achieves impressive performance on Automatic Speech Recognition (ASR). However, in low-resource language ASR, they encounter the domain mismatch problem between pre-trained and low-resource languages. Typical solutions like fine-tuning the SSL model suffer from high computation costs while using frozen SSL models as feature extractors comes with poor performance.
To handle these issues, we extend a conventional efficient fine-tuning scheme based on the adapter. We add an extra intermediate adaptation to warm up the adapter and downstream model initialization. Remarkably, we update only 1-5% of the total model parameters to achieve the adaptation. Experimental results on the ML-SUPERB dataset show that our solution outperforms conventional efficient fine-tuning. It achieves up to a 28% relative improvement in the Character/Phoneme error rate when adapting to unseen languages.

SECTION: 1Introduction

Self-Supervised Learning models (SSL models)[1,2,3,4,5,6]pre-trained with speech-only data have achieved significant improvements for Automatic Speech Recognition (ASR) in mainstream languages.[7,8,9,10,11,12,13].
However, employing SSL models on low-resource language ASR may encounter the problem of domain mismatch between pre-trained and low-resource languages[14].
Since SSL models are mostly pre-trained with high-resource languages, like English, they may not generalize well on those low-resource languages[15,16,17].

We aim to solve the mismatch problem when employing SSL models on low-resource language ASR. Typically, there are two applicable approaches: fine-tuning the SSL models with the low-resource language data and employing the SSL models as feature extractors for subsequent downstream models.[15,18].
Fine-tuning the SSL model with the downstream model leads to better performance but suffers from high computation costs. Furthermore, the amount of target low-resource language data is extremely insufficient for large-scale model training. Therefore, it may lead to sub-optimal transfer performance due to over-parameterization. On the other hand, utilizing frozen SSL models as feature extractors could be a lower-cost option. However, it usually comes with poor performance, especially when the target low-resource language is unseen to the SSL model. Apart from these solutions, Adapters[19,20,21,22,23,24,21,25,26], which are lightweight modules inserted in the pre-trained model, could be a preferable solution since it only fine-tunes limited amount of inserted parameters, achieving a balance between performance and computation cost. However, they could still encounter difficulties in transferring to an unseen language in low-resource scenarios due to domain mismatch.

Some existing works aim to solve the domain mismatch problem. For example,[27]introduces continual pre-training with the adapter. Nevertheless, its need for large-scale labeled data makes it unsuitable for low-resource scenarios.[28]leverages multiple adapters trained on high-source languages to enhance performance on low-resource languages. However, it requires multiple adapters at the same time during training and inference, becoming impractical as the number of languages increases.[29]presents a recipe to efficiently adapt models to a different domain. Nevertheless, they did not explore the cross-language adaptation.

To deal with the domain mismatch problem, we provide an efficient solution.
Figure1shows the general pipeline of our solution. Our solution utilizes adapters to keep the computation cost low.
To facilitate the adaptation, we add anIntermediate Adaptation(IA) before fine-tuning the adapter and downstream model on low-resource target languages. IA serves as a bridge between pre-trained languages and unseen target languages. During the IA, we utilize various adaptation algorithms to warm up the adapter and downstream model with high-resource source languages. These high-resource languages are selected to optimize the model’s transferability to unseen target languages. After IA, we can derive an enhanced initialization and perform Parmeter-Efficient Fine-tuning (PEFT)[30]to fine-tune on each target language.
Experimental results on the ML-SUPERB dataset[15]demonstrate that our solution outperforms conventional efficient fine-tuning. In the best case, it achieves up to a 28% relative improvement in the Character/Phoneme error rate when adapting to unseen target languages. Furthermore, we provide analysis for different SSL models and our proposed source language selection methods.

SECTION: 2Methodology

We focus on effectively adapting SSL models to each unseen target language in low-resource scenarios. As illustrated in Figure1, before fine-tuning on each target language, we add an extraIntermediate Adaptation(IA) step. IA warms up the adapter and downstream model with source languages to facilitate adaptation to each unseen target language. After IA, we obtain an enhanced adapter and downstream model initialization (the green module in Figure1). With this initialization, we apply PEFT to fine-tune the adapter and the downstream model on each target language (the red, orange, and purple adapter in Figure1). It is important to note that the SSL model is frozen all the time, making our solution low-cost.

Here, we would like to give a general formulation of IA. Given model initialization(including frozen SSL model, randomly initialized adapterand downstream model) and source languages(is a set withlanguages), the objective of the IA is to find an enhanced initialization(abbreviated intofor better readability,means the parameter is warmed up). In other words, IA can be formulated as

whereis the initialization for performing PEFT on each target language in(is a set withlanguages). The concrete form of IA() will be provided in the Section2.2.
To obtainwith the best adaptation result on, there are two problems to address: (1) What kinds ofsource languages(see Section2.1) and (2)adaptation algorithms(see Section2.2) can best facilitate the adaptation to unseen target languages?

SECTION: 2.1Source Language Selection

To identify source languages beneficial for adaptation to unseen target languages, we use the linguistic knowledge based on a linguistic tree[31]. As illustrated in Figure2, we select source languages (“Luxembourgish”,
“Ndebele”) linguistically close to target languages (“English”, “Swedish”)111We use well-known languages as examples for better readability. In low-resource language cases, we can still employ the same method., as they might share some acoustic traits[1]. In other words, we assume that warming upon languages similar toto getmay facilitate the final adaptation result on.

The detailed implementation of selecting source languagesis explained here. Given target languages, we traverse the linguistic tree, exclude languages in, and select the topmost linguistically similar languages as. We define the linguistic-similarity function Sim() tousing Lowest Common Ancestor (LCA):

whereis the language in the linguistic tree, andcomputes the depth of a node in the tree. A higher Sim(,) value implies that the languageis linguistically closer toin the tree. For example, in Figure2, we pick the blue (“Luxembourgish”) and green (“Ndebele”) ones instead of the gray one (“Manx Gaelic”) because their depths of LCA (Germanic) to target languages are deeper than that of the gray one (Indo European).

SECTION: 2.2Adaptation Algorithm

Appropriate selection of an adaptation algorithm in IA also has a huge impact on the final adaptation result. The adaptation algorithm findsbased on theand(see Eq.  (1)).
To find the adaptation algorithm with the best adaptation result, we explore two prominent algorithms:

Multitask Learning (MTL):
Multitask Learning (MTL) seeks to optimize the initializationacross all source languagessimultaneously to get. In other words, following the IA general form (see Eq. (1)), the optimization objective of MTL can be formulated as:

wheredenotes the ASR loss on each source language.

Model-Agnostic Meta-Learning (MAML):
MAML[32]is a commonly adopted algorithm in few-shot adaptation scenarios. Unlike MTL, MAML adopts a bi-level optimization process, which includes the inner and outer loop. Following the IA general form (see Eq. (1)), the optimization objective of MAML can be formulated as:

where the MAML function is defined at Alg.1. As shown in Alg.1, in the while loop, we first sample a batch of datafrom a source language. Next, we split theinto support setand query set. In the inner loop, we derive the language-specific modelwith. Last, we calculate the gradient usingandto update. Until the ASR lossof the outer loop converges, we adopt theas.

SECTION: 2.3Target Languages Fine-tuning

After derivingwith IA (see Figure1), we fine-tuneto each target language. Specifically, Figure1illustrates that we fine-tuneto each target languageto get(a set ofadapter and downstream model parameters).

SECTION: 3Experimental Setups

SECTION: 3.1Dataset

We evaluate our solution using ML-SUPERB[15], a benchmark for multilingual ASR with speech SSL models. ML-SUPERB is supported by 143 languages. For each language, ML-SUPERB provides 10-minute and 1-hour settings. The duration means the training data size employed in fine-tuning for each language. For evaluation metrics, we follow the ML-SUPERB settings to report the Character/Phoneme Error Rate (CER/PER).

SECTION: 3.2Source and Target Languages

We use the ML-SUPERB dataset to construct the source and target language setsand. For target languages, we build two target language sets: theSeen Setand theUnseen Set. Each of them has its corresponding source languagesusing our proposed method (see Section2.1). For the amount of training data, we use 10-minute and 1-hour settings for target languages while using the 1-hour setting for
source languages.

Table1lists the source and target languages of the two sets. The explanation of the two sets are shown below:

Seen Set: This set is derived from MLSUPERB’s Monolingual Track, including 9 widely used languages (see Table1(I)). These languages are seen by some SSL models we use in the experiment. This set is intended for direct comparison with MLSUPERB results and rapid concept validation.

Unseen Set. This set is derived from the ML-SUPERB Multilingual Track, including 20 endangered languages (see Table1(II)). This set evaluates the model’s adaptability to unseen languages, given that our SSL models did not previously see these languages during pre-training.

SECTION: 3.3Model Configuration & Hyperparameter

SSL Model (). We employ three onboard pre-trained SSL models: HuBERT-base[8], mHuBERT-base[33], and XLSR-128[9].
These models encompass various traits, including monolingual / multilingual, base / large size.

Adapters (). Adapters are lightweight modules inserted in neural networks that enable task-specific adaptations without modifying the original model’s parameters. Adapter modules are added to the second feed-forward layers of transformer layers in the SSL model, operating independently from the downstream model. The adapter implementation strictly adheres to the methodology in[24], specifically adopting the Houlsby adapter[20]. The bottleneck is set to 32.

Downstream Model (). The downstream model () adopts a transformer architecture with the connectionist temporal classification (CTC) objective as outlined in the ML-SUPERB[15]. Both the model’s architecture and hyper-parameters follow the specifications in ML-SUPERB for a fair comparison. It is important to note that we reinitialize the CTC head of the downstream model after IA because the characters set of source languages are different from that of target languages.

Table2shows the percentages of theandparameters.
Note that we always freeze theto make our solution low cost.

Adaptation Algorithms(see Section2.2). For the MAML, we adopt the first-order version (FOMAML) to save computation costs. For the MAML hyper-parameters, we set the= 0.001,= 0.0001, inner step = 1, and update the model using SGD in the inner loop and Adam in the outer loop. For the MTL, we set the learning rate to 1e-4, using Adam optimizer.

SECTION: 3.4Baselines

To prove the effectiveness of our pipeline, we adopt four common fine-tuning methods as our baselines:Full FT,Freeze FT,PEFT, andSource & Target (&)-MTL.

Full FT ():This is the SSL model fine-tuning baseline, where we fine-tune theon each target language in, without initializingand IA. This method serves as the strong baseline due to its high computational and storage costs.

Freeze FT ():This is the widely-used fine-tuning baseline, where we freezeand fine-tuneon each target language in, without initializingand IA. This method is the default setting of ML-SUPERB.

PEFT ():This is the PEFT baseline. It freezesand fine-tuneson each target language in, without IA.

&-MTL ():This is the baseline using the same amount of training data without two-stage training like IA. It involves jointly fine-tuningon both theandto get a multilingual model without further fine-tuning on each target language in.

SECTION: 4Result & Analysis

SECTION: 4.1Main Result

Table3presents the results of our IA variants and baselines from the Seen Set and the Unseen Set. Remarkably, two IA variants (IA-MAML,IA-MTL) consistently outperform other baselines (Freeze-FT,PEFT,&-MTL) on both sets, whileIA-MTLslightly outperformsIA-MAML. In the 10-minute and 1-hour setting, IA variants achieve substantial improvements. Compared withPEFTbaseline, IA variants achieve up to 28% and 20% relative improvement. Impressively, when compared to a strong baseline likeFull-FT, two IA variants either surpass or matchFull-FTperformance, but require less than 6% of the tunable parameters inFull-FT. Our results strongly support the effectiveness of IA in facilitating language adaptation.

SECTION: 4.2Impact of SSL Model Pre-trained Languages

In this section, we discuss the adaptation result of SSL models to seen and unseen target languages (see Section3.2).

HuBERT-Base & mHuBERT:Here, we discuss whether IA benefits the adaptation of base-size and less multilingual SSL models. HuBERT-Base[8]is pre-trained on 1k hours of English data while mHuBERT-Base[33]is pre-trained on 14k hours from three European languages. In Seen Set & Unseen Set, most languages are unseen during the pre-training of these two models. Table3shows that IA does facilitate the adaptation of base size and less multilingual SSL models to unseen target languages. Our IA variants achieve at most 14.9% relative improvement compared withPEFTbaseline.

XLSR-128:Here, we discuss whether IA benefits the adaptation of a large multilingual SSL model. XLSR-128[9]is pre-trained on 400k hours of data in 128 languages. For XLSR-128, most languages in the Seen Set are seen, while those in the Unseen Set are mostly unseen during the pre-training. From Table3, we can tell that IA improves the adaptation result in both sets. To be more specific, compared withPEFTbaseline, our IA variants show at most 24.9% and 28.2% relative improvement on each set. This validates that IA does facilitate the adaptation of large multilingual SSL models to unseen target languages.

SECTION: 4.3Effectiveness of Source Languages Selection Methods

Source language selection is critical in IA. To validate our selection methods (see Section2.1), we compare our linguistic-knowledge-based method with randomly sampling source languages from the ML-SUPERB dataset. The experiments are conducted with= 5 and= 10 under theIA-MTLdefault settings on Seen Set222Source languages=5 {cat, mar, guj, kan, tam},=10 {nbl, ssw, ven, mal, ben, mri, sot, nep, sin, jav}, ISO-639 code. Table4(a)shows that our proposed method outperforms random selection baselines in both settings, validating the effectiveness of our method.

Also, we provide experiments for the optimal numberof source languages. Table4(b)illustrates the trend= {5, 10, 20, 50}. The experiment result indicates thatachieves the best performance. Furthermore, we see no improvement when we increaseto 50. This suggests that languages chosen later, which are less linguistically similar to our target languages, contribute less to the adaptation to unseen target languages. Note that using more source languages takes more epochs for the model to converge in IA. Therefore, we set= 10 instead of 20 as our default setting due to the computational constraint.

SECTION: 5Conclusion & Limitation

In this work, we propose an efficient solution for adapting SSL models to unseen language in low-resource scenarios. Our solution adds an extra Intermediate Adaptation (IA) to warm up the adapter and downstream model initialization. With this enhanced initialization, the model can adapt to unseen target languages more easily. In our low-cost solution, only 1-5% of the total model parameters are modified to adapt to each language. Experiment results on the ML-SUPERB dataset show that our solution achieves up to a 28% relative improvement in CER/PER over conventional efficient fine-tuning. Additionally, our results validate the effectiveness of our source language selection method and our configuration of tunable parameters. Overall, our efficient solution contributes to the employment of SSL models on low-resource language ASR. For the limitation of our work, our source language selection method needs to know the target language set beforehand. Also, we lack explorations of second-order MAML and other types of adapters.

SECTION: References