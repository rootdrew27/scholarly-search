SECTION: Adult Glioma Segmentation in Sub-Saharan Africa using Transfer Learning on Stratified Finetuning Data

Gliomas, a kind of brain tumor characterized by high mortality, present substantial diagnostic challenges in low- and middle-income countries, particularly in Sub-Saharan Africa. This paper introduces a novel approach to glioma segmentation using transfer learning to address challenges in resource-limited regions with minimal and low-quality MRI data. We leverage pre-trained deep learning models, nnU-Net and MedNeXt, and apply a stratified fine-tuning strategy using the BraTS2023-Adult-Glioma and BraTS-Africa datasets. Our method exploits radiomic analysis to create stratified training folds, model training on a large brain tumor dataset, and transfer learning to the Sub-Saharan context. A weighted model ensembling strategy and adaptive post-processing are employed to enhance segmentation accuracy. The evaluation of our proposed method on unseen validation cases on the BraTS-Africa 2024 task resulted in lesion-wise mean Dice scores of 0.870, 0.865, and 0.926, for enhancing tumor, tumor core, and whole tumor regions and was ranked first for the challenge. Our approach highlights the ability of integrated machine-learning techniques to bridge the gap between the medical imaging capabilities of resource-limited countries and established developed regions. By tailoring our methods to a target population’s specific needs and constraints, we aim to enhance diagnostic capabilities in isolated environments. Our findings underscore the importance of approaches like local data integration and stratification refinement to address healthcare disparities, ensure practical applicability, and enhance impact.

* These authors contributed equally.

SECTION: 1Introduction

Gliomas, a type of brain tumor known for their high mortality rates, exhibit poor survival outcomes, with significant disparities between high- and low-income countries[9]. While some progress has been made in reducing mortality rates in high-income countries like the United States, low- and middle-income countries, notably those in Sub-Saharan Africa, continue to face increasing glioma death rates[17]. Death rate disparity is primarily due to delayed diagnosis, comorbidities such as HIV, and inadequate healthcare infrastructure[25]. Machine learning (ML) offers promise in narrowing this survival gap by enhancing early detection, treatment precision, and outcome prediction through improved MRI-based tumor segmentation. However, adapting advanced ML methods to regions with limited medical resources and poorer MRI quality remains a significant challenge[1].

Healthcare systems in Sub-Saharan Africa often struggle with limited access to high-quality imaging equipment, a scarcity of annotated medical data, and inadequate computational resources[24]. These constraints hinder the development and deployment of robust glioma segmentation models tailored to the local population[25]. Therefore, innovative approaches are urgently needed to overcome these barriers and provide accurate and reliable segmentation results under such conditions.

The Brain Tumor Segmentation (BraTS) challenge, held in conjunction with the International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI) since 2012, has established a benchmark dataset for the segmentation of adult brain gliomas[4,5,6,7,18]. The BraTS 2024 cluster of challenges[13]has reintroduced the benchmark for adult brain gliomas[23], specifically targeting the Sub-Saharan African (SSA) patient population[1]. The MICCAI-CAMERA-Lacuna Fund BraTS-Africa 2024 Challenge[1]provides the largest annotated publicly available retrospective cohort of pre-operative glioma in adult Africans, including both low-grade glioma and glioblastoma/high-grade glioma. In this context, we propose a segmentation technique for benchmarking the SSA task, which involves a small dataset with low-quality MRI acquisition.

In this context, transfer learning has emerged as a powerful technique in medical imaging, offering a solution to the challenges posed by limited data availability and/or quality[26,3]. By leveraging pre-trained models on large, diverse datasets, transfer learning enables adapting these models to specific tasks with significantly fewer training samples[19,2]. This approach mitigates issues of data scarcity and enhances model performance by incorporating learned features from extensive, high-quality datasets. Moreover, in the medical domain, supervised pre-training is particularly beneficial to improve learning and address the problem of catastrophic forgetting[15,16].

In this work, we develop an ensemble approach involving two state-of-the-art deep learning models. These models are specifically trained for glioma segmentation under low-data and low-quality conditions. We implement a stratified fine-tuning strategy tailored to the Sub-Saharan context and thoroughly evaluate the model’s effectiveness in accurately segmenting gliomas in local MRI scans. Through this work, we aim to demonstrate the potential of advanced machine learning techniques to bridge the gap in medical imaging capabilities between resource-limited regions and more developed healthcare systems.

SECTION: 2Data description

For our model training, we utilized the BraTS2023-Adult-Glioma[4]and BraTS-Africa datasets[1]. These datasets were collected during routine multi-parametric MRI (mpMRI) clinical scans, acquired as part of standard clinical care from multiple institutions using conventional brain tumor imaging protocols. The mpMRI data included pre- and post-gadolinium T1-weighted (T1 and T1CE), T2-weighted (T2), and T2-weighted fluid-attenuated inversion recovery (T2-FLAIR) MRI scans.

The mpMRI scans underwent standardized pre-processing according to the BraTS challenge guidelines[14]. This pre-processing included co-registration to the SRI Atlas[20], resampling to an isotropic resolution of 1, and skull-stripping. The ground truth annotations for the tumor sub-regions—Peritumoral Edema (ED), Enhancing Tumor (ET), and Necrotic Core (NC)—for each case were approved by expert neuroradiologists.

Fig.1and Tab.1show samples of the training examples used for the task, providing a visual and quantitative overview of the data utilized in our model training.

SECTION: 3Methods

Our methodology for generalizable brain tumor segmentation on low-quality and limited data leverages commonly used segmentation architectures with transfer learning. It involves radiomic analysis to create stratified training folds, model training on a large brain tumor dataset, transferring learning to the SSA dataset, weighted model ensembling, and postprocessing.

SECTION: 3.1Stratified Fold Creation

Deep learning tasks often use random sampling to create folds, but this can result in unbalanced learning because not all data types are equally represented in each fold. To address this issue, we created stratified folds for our deep learning models, ensuring a good representation of the radiomic features of gliomas in each fold.

Following the approach in[11], we computed 14 shape-based and 93 intensity-based radiomic features on the largest lesion area (the whole tumor, or WT) for each MRI sequence. We used principal component analysis (PCA) to select the most relevant features, which explained 99% of the variance, resulting in 9 features. These features were then used in a k-means clustering algorithm to group lesions into different clusters (subtypes) of tumors. The optimal number of clusters was determined using grid search and silhouette analysis.

The k-means algorithm was trained on the training set with the corresponding ground-truth WT for tumor subtype analysis during preprocessing and on the cross-validated predicted WT during post-processing. Equal samples from each of the k-means clusters were then used to create our stratified folds.

SECTION: 3.2Deep Learning Models and Transfer Learning Setup

The nnU-Net is a self-configuring deep learning framework for semantic segmentation based on the U-Net architecture[21]. It automatically adjusts its internal configurations according to the specific imaging modality and unique attributes of each dataset[10]. The self-configuration process leads to improved segmentation performance and generalization compared to other state-of-the-art methods for biomedical image segmentation.

Forpretraining, we trained a full-resolution 3D nnU-Net (v2) model using a stratified five-fold from theBraTS2023-Adult-Gliomadataset. The model predicted three channels corresponding to the three tumor sub-regions (enhancing tumor, tumor core, and whole tumor). The input images were preprocessed using zero mean unit variance normalization and divided into patches of 128x128x128 voxels. We used region-based training to favor larger patches while staying within the GPU’s capacity[10].

The loss function combined Dice loss and cross-entropy loss, optimized with the stochastic gradient descent (SGD) optimizer using Nesterov momentum. We set an initial learning rate of 0.01, a momentum of 0.99, and a weight decay of 3e-05. Each of the five folds was trained for 200 epochs on an NVIDIA A100 (40 GB) GPU. During inference, images were predicted using a sliding window approach, with the window size matching the patch size used during training.

Fortransfer learning, we fine-tuned the pretrained full-resolution 3D nnU-Net (v2) model on a stratified five-fold from theBraTS-Africadataset, using the same configuration as the pretraining. The nnU-Net implementation is available in an open-source repository:https://github.com/MIC-DKFZ/nnUNet.

MedNext[22]combines convolutional neural networks and attention mechanisms for medical image analysis. It integrates convolutional layers for feature extraction with attention modules that enhance the model’s focus on relevant regions within the images[22]. Using strategies from the 3D nnU-Net (v2)[10], the framework autonomously adjusts its internal configurations for better performance.

Like 3D nnU-Net, MedNext was trained in a label-respective manner for each task. Forpretrainingon the stratified fold ofBraTS2023-Adult-Gliomadataset, we trained MedNeXt-M (k=3, 17.6M parameters, 248 GFlops) with a class-weighted loss function combining Dice loss and cross-entropy loss. The loss function was optimized with the SGD optimizer using Nesterov momentum, with an initial learning rate of 0.01, momentum of 0.99, and weight decay of 3e-05. Each of the five folds was trained for 200 epochs on an NVIDIA A100 (40 GB) GPU. During inference, images were predicted using a sliding window approach, with the window size matching the patch size used during training.

Fortransfer learning, we fine-tuned the pre-trained MedNext model on a stratified five-fold from theBraTS-Africadataset, using the same configuration as the pretraining. The MedNext implementation is available in an open-source repository:https://github.com/MIC-DKFZ/MedNeXt.

SECTION: 3.3Weighted Model Ensembling

We propose to use a model ensembling strategy to enhance the accuracy and robustness of the segmentation outcome[8]. This approach (see Fig.2) involves harnessing the complementary strengths of the two models described, nnU-Net and MedNeXt, to collectively address the task of pixel classification. Each model is trained on stratified five folds that provide a similar representation, so we assign equal weight to each fold within the model. However, when combining the models, we use a weighted ensemble represented by the equation below:

SECTION: 3.4Adaptive Post-processing

Adaptive post-processing involves choosing post-processing parameters based on the ensemble prediction for each new sample[12]. We calculate 14 shape-based and 93 intensity-based radiomic features on the predicted whole tumor (WT) for this. Using PCA analysis (see section3.1) on the WT predictions, we identified 8 clusters for post-processing.

The BraTS challenge evaluates segmentation models at the lesion level rather than across entire tumor regions. To refine our predictions, we performed a grid search within each cluster to find optimal thresholds for removing small, isolated areas likely to be noise, as identified from cross-validation data.

Additionally, we conducted a further threshold search on the refined segmentation maps to adjust the ET label based on the ET/WT ratio[8]. In this step, if the ET/WT ratio was below a certain threshold, we redefined the ET label to NCR. This adjustment was made for all 8 clusters using cross-validated results.

SECTION: 4Results

SECTION: 4.1Evaluation Criteria

The evaluation of model predictions on the validation set utilized the BraTS pipeline within the Synapse platform. Assessments were conducted for distinct tumor regions including the whole tumor (WT), tumor core (TC), and enhancing tumor (ET). Evaluation metrics included the Dice score for lesion-wise overlap between predicted segmentation and ground truth, and the 95th percentile lesion-wise Hausdorff distance to measure segmentation deviation from the ground truth.

SECTION: 4.2Hyperparameter search

The first hyperparameter we determined was the ensemble weights. Here, the weightsfor nnU-Net andfor MedNeXt in equation1were chosen based on each model’s performance during cross-validation. These weights were set to 0.4722 for nnU-Net and 0.5278 for MedNeXt, reflecting their performance across the five folds.

The next set of hyperparameters we optimized included the thresholds for adaptive post-processing, as described in section3.4. The results of this search are shown in table2.

SECTION: 4.3Results

Table3provides a comprehensive overview of the performance evaluation of our models across the validation datasets for the SSA task. The challenge’s digital platform performed this performance evaluation automatically, without access to the validation ground truth data. Additionally, table3offers quantitative comparisons of a nnUNet trained from scratch using the SSA dataset to emphasize our proposed approach’s importance in increasing performance. Further, Fig.3shows qualitative results in one of the cases. The model achieved the best performance among all participants of the challenge and the test quantitative result.

SECTION: 5Conclusion

Segmentation of gliomas from radiology images is challenging in resource-limited regions like Sub-Saharan Africa due to scarce and low-quality MRI data. In response, we developed a transfer learning-based approach for adult glioma segmentation that utilizes pre-trained deep-learning models and a stratified fine-tuning strategy. Our method improved segmentation accuracy over trained-from-scratch models, demonstrating the presented technique’s potential to bridge the gap in medical imaging capabilities between resource-limited and developed regions. By tailoring machine learning approaches to the specific needs and constraints of the target population, we highlighted the importance of leveraging advanced techniques to enhance diagnostic capabilities in challenging environments. Future work will focus on integrating more local data and refining the stratification process to ensure practical applicability and impact.{credits}

Partial support for this work was provided by the National Cancer Institute (UG3 CA236536) and by the Spanish Ministerio de Ciencia e Innovación, the Agencia Estatal de Investigación, NextGenerationEU funds, under grants PDC2022-133865-I00 and PID2022-141493OB-I00, and EUCAIM project co-funded by the European Union (Grant Agreement #101100633). The authors gratefully acknowledge the Universidad Politécnica de Madrid (www.upm.es) for providing computing resources on the Magerit Supercomputer.

The authors have no competing interests to declare that are
relevant to the content of this article.

SECTION: References