SECTION: ED-ViT: Splitting Vision Transformer for Distributed Inference on Edge Devices
Deep learning models are increasingly deployed on resource-constrained edge devices for real-time data analytics. In recent years, Vision Transformer models and their variants have demonstrated outstanding performance across various computer vision tasks. However, their high computational demands and inference latency pose significant challenges for model deployment on resource-constraint edge devices. To address this issue, we propose a novel Vision Transformer splitting framework, ED-ViT, designed to execute complex models across multiple edge devices efficiently. Specifically, we partition Vision Transformer models into several sub-models, where each sub-model is tailored to handle a specific subset of data classes. To further minimize computation overhead and inference latency, we introduce a class-wise pruning technique that reduces the size of each sub-model. We conduct extensive experiments on five datasets with three model structures, demonstrating that our approach significantly reduces inference latency on edge devices and achieves a model size reduction of up to 28.9 times and 34.1 times, respectively, while maintaining test accuracy comparable to the original Vision Transformer. Additionally, we compare ED-ViT with two state-of-the-art methods that deploy CNN and SNN models on edge devices, evaluating accuracy, inference time, and overall model size. Our comprehensive evaluation underscores the effectiveness of the proposed ED-ViT framework.

SECTION: Introduction
In recent years, deep learning models have been increasingly deployed on resource-constrained edge devices to meet the growing demand for real-time data analytics in industrial systemsand have demonstrated remarkable capabilities in various applications such as video image analysis and speech recognition. Convolutional Neural Networks (CNNs)like VGGNetand ResNet, as well as Spike Neural Networks (SNNs), have achieved satisfactory performance in many edge computing scenarios. As the field progresses, researchers are exploring the deployment of more complex structured models on edge devices to further improve performance. Transformer architecture, which has revolutionized natural language processing (NLP) tasks, has inspired similar advancements in computer vision. Vision Transformer (ViT) modelsand their variants have shown outstanding results across various computer vision tasks, including image classification, object detection, semantic segmentationand action recognitionand audio spectrogram recognition. The success of ViTs has sparked interest in leveraging their capabilities for edge computing applications.

However, the rapid advancement in machine learning technologies has increased the demand for computational resources and memory, given the complexity of these model configurations. Achieving higher accuracy with ViT requires substantial computational power and memory, which poses challenges for deployment on edge devices. For instance, ViT-Baseconsists of over 86.7 million parameters and requires approximately 330MB of memory. Researchers now face the dilemma of deploying such complex models while dealing with resource-constrained resources.

Previous studies aiming to reduce the deployment overhead primarily focus on compressing Vision Transformer models. These approaches can be classified into three major categories: (1) architecture and hierarchy restructuring, (2) encoder block enhancements, and (3) integrated approaches. However, these methods often suffer from either poor inference accuracy or high inference latency as they attempt to compress a large model to fit into a memory-constrained edge device.

To develop a solution that mitigates accuracy drop and enables efficient deployment of the Vision Transformer on resource-constraint edge devices, we aim to utilize the collaboration of multiple edge devices and propose a Vision Transformer splitting frameworkdgeevicesionransformer called. As illustrated in Figure, ED-ViT first partitions the original Vision Transformer into several smaller sub-models, inspired by the concept of split learning (SL). However, unlike traditional SL, which does not consider edge device constraints, each of these small sub-models is responsible for detecting a specific subset of the classes and is deployed on resource-constrained edge devices. ED-ViT then employs model pruning techniques to further alleviate the computational load and processing requirement for each sub-model. To optimize model assignment, we design a greedy assignment algorithm that takes into account both the model computational resources and memory resources. Besides integrating the previous steps, ED-ViT uses a multilayer perceptron (MLP) model to fuse the results from all the sub-models. We conduct experiments across five datasets to evaluate the effectiveness of ED-ViT framework on edge devices, particularly in low-power video analytics. The results, measured across three key metrics—accuracy, inference latency, and model size—consistently demonstrate the significant benefits of ED-ViT. Additionally, we compare ED-ViT with methods that split CNN and SNN, highlighting the great potential of deploying Vision Transformer on resource-constrained edge devices to achieve high accuracy while maintaining small model sizes and low inference latency. Our main contributions are summarized as follows:

This is the first study focused on splitting and deploying Vision Transformer onto edge devices. We propose a framework that leverages the capabilities of Vision Transformer, allowing for the collaboration of multiple edge devices to achieve distributed inference in practical applications.

We introduce ED-ViT to address the Vision Transformer splitting problem by decomposing the complex original model into sub-models, and applying pruning techniques to reduce the size of each sub-model. Using a combined greedy method for model assignment, ED-ViT effectively addresses the formulated problem by reducing model sizes, minimizing inference latency, and maintaining high accuracy, achieving a trade-off across these three metrics.

We conduct extensive experiments with three computer vision datasets and two audio recognition datasets across three ViT structures, demonstrating that our framework significantly reduces inference latency on edge devices and decreases overall memory usage with negligible accuracy loss in various Vision Transformer applications.

SECTION: Related Works
SECTION: General Vision Transformer Compression
Deploying Vision Transformer models in resource-constrained environments poses significant challenges due to their intensive computational and memory demands. These approaches address Vision Transformer resource limitations via pruning, encompassing both local and global strategies as follows.

focus on removing redundant components within specific layers of the model. For instance, PVTand its successor PVTv2introduce a pyramid hierarchical structure to transformer backbones, achieving high accuracy with reduced computation.applies sparsity regularization during training and subsequently prunes the dimensions of linear projections, targeting less significant parameters.prunes multi-head self-attention (MHSA) and feed-forward networks (FFN), which are often redundant components.proposes network pruning to eliminate complexity and model sizes by reducing tokens. Other noteworthy contributions include DToP, which enables early token exits for semantic segmentation tasks. Conversely,adopt a comprehensive perspective by evaluating and pruning the overall significance of neurons or layers across the entire network. SAViTpurposes structure-aware Vision Transformer pruning via collaborative optimization. For instance, CP-ViTsystematically assesses the importance of head and attention layers for the purpose of pruning, while Evo-ViTidentifies and preserves significant tokens, thereby discarding those of lesser importance. Moreover, the Skip-attention approachfacilitates the omission of entire self-attention layers, thereby exemplifying a global pruning methodology. X-pruneremploys explainability-aware masks to inform its pruning decisions, thereby advancing a more informed global pruning strategy. In addition, UP-ViTintroduces a unified pruning framework that leverages KL divergence to guide the decision-making process for pruning, while LORSoptimizes parameter usage by sharing the majority of parameters across stacked modules, thereby necessitating fewer unique parameters per module.

Among existing pruning methods, UP-ViThas the closest resemblance to our approach. However, it is important to note that these techniques cannot be directly applied to edge devices: they often suffer from poor performance when the pruning ratio is high or incur high computation overhead when the pruning ratio is low, making them unsuitable for resource-constrained edge environments. In contrast, our work introduces a class-based global structured pruning method that addresses these limitations. Our approach is orthogonal to most previous methods and does not involve trainable parameters, contributing to more stable performance.

SECTION: Vision Transformer on Edge Devices
There are several methods focused on deploying Vision Transformer on-edge devices, which can be classified into three major categories.

HVTcompresses sequential resolutions using hierarchical pooling, reducing computational cost and enhancing model scalability. LeViTis a hybrid model that combines the strengths of CNNs and transformers. For image classification tasks, it utilizes the hierarchical structure of LeNetto optimize the balance between accuracy and efficiency, and uses average pooling in the feature map stage.

ViLintroduces a multiscale vision longformer that lessens computational and memory complexity when encoding high-resolution images. Poolformerdeliberately replaces the attention module in transformers with a simple pooling layer. LiteViTintroduces a compact transformer backbone with two new lightweight self-attention modules (self-attention and recursive atrous self-attention) to mitigate performance loss. Dual-ViTreduces feature map resolution, consisting of two dual-block and two merge-block stages. MaxViTdivides attention into local and global components and decomposes it into a sparse form with window and grid attention. Slide-Transformerproposes a slide attention module to address the problem that computational complexity increases quadratically with the attention modules.

Some methods integrate both of the above approaches. CeiTcombines Transformer and CNN strengths to overcome the shortcomings of each, incorporating an image-to-tokens module, locally-enhanced feedforward layers, and layer-wise class token attention. CoAtNetcombines depth-wise convolutions and simplifies traditional self-attention by relative attention, enhancing efficiency by stacking convolutions and attention layers.

However, they fail to link pruning with specific classes, which limits their methods when both high performance and low memory usage are required.

SECTION: Split Learning
Current works that combine Vision Transformer and split learning primarily focus on federated learning, addressing data privacy and efficient collaboration in multi-client environments, where the inner structure of a large model is split across smaller devices and later fused. However, these approaches do not target the deployment of Vision Transformer on edge devices.

Traditional machine learning model splitting generally involves partitioning a large model into multiple smaller sub-models that can be executed collaboratively on resource-constrained devices, providing a promising technique for deploying models on edge devices. Splitnetclusters classes into groups, partitioning a deep neural network into tree-structed sub-networks.dynamically partitions models based on the communication channel’s state. Nnfacetsplits large CNNs into lightweight class-specific sub-models to accommodate device memory and energy constraints, with the sub-models being fused later.follows a similar approach to split deep SNNs across edge devices. Distredgeuses deep reinforcement learning to compute the optimal partition for CNN models.

To the best of our knowledge, our work presents the first exploration of Vision Transformer model partitioning for edge deployment, marking a significant contribution to this field. Drawing inspiration from previous studies, our framework, ED-ViT, introduces an innovative approach to decompose a multi-class ViT model into several class-specific sub-models, each performing a subset of classification. Unlike relying on channel-wise pruning, ED-ViT employs advanced pruning techniques specifically designed for the unique architecture of Vision Transformers.

SECTION: Problem Formulation
The structures of three representative Vision Transformer models, ViT-Small, ViT-Base, and ViT-Large, are presented in Table. The number of operations is commonly used to estimate computational energy consumption at the hardware level. In Vision Transformer models, almost all floating-point operations (FLOPs) are multiply-accumulate (MAC) operations.

For Patch Embedding, FFN, and MLP Head, their operation counts are easy to infer as they follow a fully connected (FC) structure, where the MAC count is given by, whereandrepresent the input and output features, respectively. For MHSA, assuming the number of patches is, the dimension of each patch is, the embedding dimension is, and the number of attention heads is, the MAC for the linear projections to generate the,, andmatrices is. The MAC foris, and the MAC for the softmax operation and multiplication withis. Forattention heads, the total MAC is. Based on this analysis, energy consumption can be estimated as being proportional to FLOPs, given that the pruned model follows the same structure.

Thus, we formulate the problem as follows. We assume that we haveinference samples in total to be processed, and the set ofedge devices is represented as. The available memory and energy (FLOPs of an edge device)for each edge deviceare denoted asand, respectively. The FLOPs (energy consumption) for each inference sample for the sub-modelfrom the set of sub-modelsis represented as, calculated based on the previous energy consumption estimation. To formulate the problem of Vision Transformer partitioning and edge-device-based deployment, we define the objective function as, aiming to minimize the maximal inference latency, as inference latency is closely related to the computational power of edge devices. Additionally, the accuracyof the fused results from allinference samples must be greater than or equal to the required inference accuracy; the total memory sizes of all sub-models should not exceed the memory budget.

The optimization problem can be formally formulated as follows, whereis a binary decision variable: 1 indicates that the sub-model deployed on edge deviceis responsible for class, and 0 otherwise. Each sub-model learns a specific subset of the classes in. Furthermore, the memory consumption of sub-model, denoted as(Model), represented as, must be smaller than the available memory size of the deployed edge device:

SECTION: Methodology
This section describes the design of the ED-ViT framework proposed to solve the optimization problem outlined in Eq.. We first explain the main workflow of ED-ViT and then provide detailed descriptions of the four key steps involved.

SECTION: Design Overview
As illustrated in Figure, ED-ViT leverages the unique characteristics of Vision Transformer and the collaboration of multiple edge devices. The framework involvesconcurrent edge devices for distributed inference alongside a lightweight MLP aggregation to derive the final classification results. Initially, the original Vision Transformer is trained on the entire dataset to achieve high test accuracy for the classification task. The ED-ViT framework is composed of four main components: model splitting, pruning, assignment, and fusion. During model splitting, the Vision Transformer model is divided into sub-models, each responsible for a subset of classes. To reduce computation overhead, these sub-models are further pruned using model pruning techniques. Subsequently, the sub-models are assigned to the appropriate edge devices, taking the optimization problem into consideration. Finally, the aggregation server fuses the outputs from the edge devices to produce the final inference results. The specific details of each component are provided below.

: The number of edge devices; memory budget; initial pruning head numberfor all sub-models, remaining available memory sizeand remaining computational resourcefor device; training dataset

: the classes set; trained original Model0

: class-specific sub-models {Model1,…, Model} and a fusion model

SECTION: Model Splitting
In the original Vision Transformer, different heads contribute to learning and inferring from the samples. However, for certain classes, maintaining all the connections between the heads can be redundant. As a result, ED-ViT prunes these connections and reconstructs the heads, with more retained heads leading to more parameters and connections being preserved. As illustrated in Algorithm, each Vision Transformer sub-model undergoes pruning based on a head number threshold and its associated categories, following a relatively equitable workload distribution. Subsequently, a greedy search mechanism is used to identify the most suitable edge device model assignment plan for deploying a particular sub-model, considering both energy and memory constraints. If the total memory size exceeds the budget or no suitable plan is found, an iterative approach is applied to adjust the number of heads for the sub-model with the biggest memory size to be pruned, repeating the allocation process until all sub-models are successfully assigned to edge devices. The pruning and the greedy assignment methods are shown as Algorithmand Algorithm, located in Sectionand Section, respectively.

SECTION: Model Pruning
: pruning head number; assigned classes subset

: the raw original Model0; training dataset

: pruned Model

We believe that reducing the computational burden of Vision Transformer will significantly contribute to lowering inference latency in distributed edge device settings. We focus on the original ViT architecture, chosen for its simplicity and well-defined design space, focusing on redistributing the dimensionality across different blocks to achieve a more balanced tradeoff between computational efficiency and accuracy, as shown in Figure.

The main prunable components in a ViT block, as illustrated in Figure.

Residual Connection Channels (Red,): The channels across the shortcut connections within the transformer blocks.

Heads in MHSA (Green,): The dimensions of the query, key, value projections (,,).

Feed-Forward Network (FFN) Hidden Dimensions (Blue,): The dimensionof the hidden layer used for expanding and reducing.

As illustrated in Figure, The pruning process is carried out in stages, with each stage focusing on one of the prunable components. We compute the KL-Divergence between the output distributions of the original model and the pruned model to evaluate the importance of each component, as follows:

whererepresents the output distribution of the original model, andrepresents the distribution after pruning.

We focus on pruning the channels of the residual connections (shown in red) in the first stage. Using KL-Divergence, we identify and prune the channels that contribute the least, reducing the dimensionality fromto, and the pruning factorcontrols the degree of reduction in the parameters. We use-th sub-model as an example: we set, effectively controlling the extent of the pruning and parameter reduction. This helps to streamline the flow of information between layers without significantly affecting model performance. Then, instead of directly removing entire heads in the MHSA module, we prune the least important dimensions within the query, key, and value projections (,, and) across multiple heads. This process effectively reduces the total number of heads to, without entirely discarding any head, thus maintaining a balanced representation of the attention mechanism while reducing its complexity. The dimensionality of the projections is scaled accordingly to reflect the merging and pruning process, ensuring that the model retains its ability to capture token interactions. The final stage involves pruning the hidden dimensionin the FFN, as shown in blue. By calculating KL-Divergence, we identify the least important neurons and reduce the hidden dimension fromto. Following each pruning stage, the model is fine-tuned to recover any performance loss that may result from the parameter reduction. This ensures that the pruned model achieves a similar level of accuracy as the original model while requiring fewer computational resources. The details are shown in Appendix.

In conclusion, the pruning process is outlined in Algorithm. An additional advantage of ED-ViT is that, even after pruning, the sub-models still retain the structure of Vision Transformer. This gives our method the potential to be combined with other horizontal pruning techniques for ViT and its variants and leverage the inherent features of Vision Transformer models to generalize well into downstream tasks.

SECTION: Model Assignment
: remaining available memory size set, remaining computational resource set, the edge device set, the sub-model set

: Model assignments

To address the optimization problem expressed in Eq., we propose a greedy search algorithm for assigning Vision Transformer sub-models to edge devices. As shown in Algorithm, the sub-models are first sorted based on their energy consumption (computation overhead). ED-ViT assigns the most computation-intensive sub-model first based on their model sizes, which is proportional to the computation overhead as in Section. The algorithm then iteratively assigns the remaining sub-models to maximize the system’s available energy. Initially, the device with the highest computational power is selected. If the remaining memory and energy can accommodate the sub-model, we update the device’s available memory and energy. Otherwise, if the sub-model exceeds the device’s memory capacity, the memory-exhausted device is removed from the set. If no devices remain, it indicates that the current pruning results prevent deployment of all sub-models. In this case, the algorithm terminates, and the ED-ViT framework re-prunes the sub-models based on a new head pruning parameter, as described in Algorithm. Finally, the algorithm outputs the model assignment plan, representing the mapping of sub-models to edge devices.

As described in Section, the problem of Vision Transformer sub-model partitioning and assignment can be formulated as a 0-1 knapsack problem, where each edge device has varying available memory and energy. Each sub-model is responsible for a specific set of classes, and multiple sub-models can be deployed on a single device. We perform a collaborative optimization of partitioning the Vision Transformer model into multiple sub-models, as shown in Algorithm, and deploying these sub-models across edge devices using a greedy search assignment mechanism (Algorithm). This approach provides a relatively optimal solution to the formulated problem. Our extensive experiments demonstrate the effectiveness of our framework design and algorithms.

SECTION: Model Fusion
In the result fusion phase, each sub-model on the edge devices processes inputs and extracts corresponding features. The server aggregates the generated features through concatenation and feeds them into an MLP to produce the final prediction. Notably, the MLP for result fusion requires training only once after all sub-models have been trained.

In our paper, we utilize a tower-structured MLP to process the concatenated tensors received from the various edge devices. Specifically, each transmitted tensor from a device is integrated using anumcls MLP structure, whereis the shrinking hyperparameter and the default value is 0.5, numcls is the number of classes. By utilizing a compact MLP model, we effectively fuse the distributed inference results from the sub-models while consuming only a minimal amount of computational resources.

SECTION: Experiments
SECTION: Experiments settings
Considering the versatile applicability of the framework across various scenarios, we select three computer vision datasets (CIFAR-10, MNIST, and Caltech256) and two audio recognition datasets (GTZANand Speech Command) to construct the classification tasks for our experiments. We choose these datasets based on the real-world tasks as explained in Appendix. For all the computer vision datasets, we resize the sample toto support various datasets and downstream tasks via a similar data structure without loss of generality; for the audio recognition datasets, we resize the sample towith the same aim. The details of the datasets and the preprocessing methods are shown in Appendix.

All models are implemented using Pytorch. During the training process, we use the Adam optimizerwith a decaying learning rate initialized to 1-4, and we set the batch size to 256. For the computer vision task, the original Vision Transformer model is pre-trained on the ImageNet dataset, followed by fine-tuning the task-specific data for 10 epochs. For the audio recognition task, Vision Transformer is pre-trained on the AudioSet datasetand then fine-tuned on the task data for about 20 epochs. All the experimental results are averaged over five trial runs. Each trial is conducted on an NVIDIA GeForce RTX 4090 GPU and 1 to 10 Raspberry Pi-4B devices, which serve as the edge devices for evaluating the execution time of processing a single sample on a specific sub-model.

SECTION: Experiments on Computer Vision Datasets
We evaluate our approach using CIFAR-10, MNIST, and Caltech image datasets. The original model size is 327.38 MB. Figureshows the accuracy, inference latency, and memory usage of the ViT-Base model under the ED-ViT framework, with 1 to 10 edge devices. With only one edge device, we apply model compression by pruning Vision Transformer without decomposition. All experiments are conducted with a total memory budget of 180MB across devices, ensuring fair comparisons.

The results demonstrate that as the number of edge devices increases, the accuracy remains largely consistent and yields strong performance. For CIFAR-10, accuracies are consistently above 85%; for MNIST, they are above 91%; and for Caltech, they exceed 90%. In most cases, the variance in final fusion prediction accuracy is less than one percentage point. The inclusion of more sub-models illustrates the feasibility of deploying larger-scale models without significant accuracy loss. As the number of edge devices increases, the inference latency decreases, as each sub-model is responsible for fewer classes and contains fewer parameters. Notably, the latency for the original model is 36.94 seconds on the CIFAR-10 dataset, which is 28.9 times the smallest latency (1.28s) and 3.84 times the highest latency (9.63s).
Our ED-ViT could make multiple edge devices work collaboratively to maintain accuracy while lowering the storage burden and inference time as the number of edge devices increases. The results for other datasets show a similar trend as the model structures are the same.

In terms of total memory usage, ED-ViT provides effective splitting and assignment strategies. As the number of edge devices increases, the memory size of each sub-model decreases, reducing computation overhead, and demonstrating that many complex model designs and computational operations are redundant for problem-solving. In the 10-edge device setting, the model size on the CIFAR-10 dataset is reduced to just 9.6MB, achieving a size reduction of up to 34.1 times compared to the original model.

SECTION: Experiments on Audio Recognition Datasets
We use the GTZAN and Speech Command audio datasets to evaluate the performance of our framework. The original model sizes of Vision Transformer for GTZAN and Speech Command is 325.88MB. Figurepresents the accuracy, inference latency, and total memory size of the ViT-Base model as implemented by the ED-ViT framework, similar to the experiments with the computer vision datasets. We still set the memory budget to 180MB.

The results show that as the number of edge devices increases, ED-ViT is able to maintain the accuracy, delivering robust performance. For GTZAN, accuracies are consistently above 84%, and for the Speech Command dataset, accuracies are above 90%. Similar to the results on the computer vision datasets, the inference latency decreases as the number of edge devices increases. Notably, the latency for the original model is 32.16 seconds on the GTZAN dataset, which is 25.13 times the smallest latency (1.28s) and 3.37 times the highest latency (9.55s). This substantial latency reduction trend is consistent across both datasets. Regarding total memory size, all configurations remain within the set limits. As the number of edge devices increases, the memory size of each sub-model decreases, and the computation overhead is similarly reduced. In the 10-edge device setting, for each model on the GTZAN dataset, the size is reduced to only 9.35MB, achieving a reduction of up to 34.85 times compared to the original model. Similar results are observed for the Speech Command dataset.

SECTION: Experiments on Different Vision Transformer Model Structures
We also select two complex datasets (CIFAR-10 and Caltech) to test different Vision Transformer structures for low-power video analytics tasks. The original model sizes of ViT-Small and ViT-Large are 82.71MB, 1,157MB, respectively. Figurepresents the accuracy, inference latency, and total memory size of the ViT-Small and ViT-Large models as implemented by the ED-ViT framework, similar to Figure. We increase the total memory size limit for ViT-Large to 600MB and decrease the limit for ViT-Small to 50MB.

The results show that as the number of devices increases, the accuracy remains relatively consistent, again showing robust performance. For ViT-Small, the accuracy is over 76.5% on the CIFAR-10 dataset and over 77.39% on Caltech across all settings; for ViT-Large, the accuracy is over 86% on the CIFAR-10 dataset and over 90.48% on Caltech in all settings. In most cases, the accuracy fluctuation for the final fusion prediction remains within a variance of less than one percentage point. The accuracy for ViT-Small is lower than that of ViT-Base, while ViT-Large achieves higher accuracy than ViT-Base, corresponding to the difference in parameter counts. Generally, the more parameters, the better the accuracy. As the number of edge devices increases, the latency decreases for both settings, similar to ViT-Base. The latency for ViT-Small is lower than that of ViT-Base, as ViT-Small requires less computational power, while the latency for ViT-Large is higher due to its larger size.
In terms of memory size, in the 10-edge device setting, for each model on the CIFAR-10 dataset, the size for ViT-Small is 2.58MB, achieving a reduction of up to 32.06 times compared to the original model. Similarly, for ViT-Large, the size is 18.73MB, which also achieves a 61.77-fold reduction compared to the original model size. Note that for the ViT-Small on the CIFAR-10 and CalTech, the input size and the output size are the same; thus, their latency and total memory size on the edge devices are also the same. Similar results are observed across both datasets for ViT-Small and ViT-Large.

SECTION: Comparison with Baseline Methods: Splitting CNN and SNN
Vision Transformer achieves better accuracy compared to traditional CNN and SNN models. However, the performance of these models on edge devices has not been directly compared before. Nnfacetproposes a method to split CNNs across multiple edge devices, employing a filter pruning technique, which differs from our approach.utilize the convolutional spiking neural network (CSNN)to transform CNNs into SNNs, using a similar strategy. Both methods focus on VGGNetnetworks and are channel-wise methods. In our experiments, the baseline model for these methods is VGGNet-16 in their papers, which also has a memory size similar to ViT-Base and achieves the best original results for comparison. We follow the hyper-parameters in their papers to get the results.

The accuracy results on the CIFAR-10 dataset are presented in Table. Based on the results, we observe that CNN outperforms SNN, while our ED-ViT method for ViT-Base yields better accuracy than both CNN and SNN approaches.

In addition to the accuracy results, we also compare inference latency and total memory size of the three methods when the number of edge devices is 10. These results are shown in Figure. Here, we selected ViT-Small, which has the lowest performance among the Vision Transformer structures. Based on the results, our ED-ViT method achieves the best accuracy compared to SNN and CNN, while its inference latency is much lower than SNN and CNN. Furthermore, the total memory size of ED-ViT is significantly lower than CNN and is comparable to SNN (as SNN is known for its small model size). This experiment demonstrates that deploying Vision Transformer onto edge devices can meet latency and memory constraints while delivering superior accuracy results.

SECTION: Experiments on Effects for Retraining
As we quantify model accuracy, we perform an ablation study to assess the impact of retraining. The results are shown in Table. The first line shows the results of the original ED-ViT. The second line shows the results from averaging the softmax output of sub-models without the fusion MLP. The third line shows the results based on the retraining of the overall models (sub-models and MLP together) for the fusion stage. When using only one device, the result is the same as the original ED-ViT as the training process remains unchanged in this scenario. Different from the work on splitting SNN and CNN, which are based on channel-wise methods and only get about 0.1% improvement in performance when retaining the overall models, our method is shown to have a great potential to improve performance (up to 6.15%). However, in the practical setting, it may be hard to retrain the sub-models with MLP.

SECTION: Supplementary Experiments
Experiments of FLOPs computation, case studies for heterogeneous sub-models, case studies for heterogeneous edge devices, experiments with Transformer following the method extending ED-ViT to Transformer as Appendix, and experiments with t-SNE visualization can be found in Appendix.

SECTION: Conclusion
In this study, we are the first to propose a novel model-partitioning framework aimed at deploying Vision Transformer on edge devices. The formulation and resolution of the problem offer a viable solution, ED-ViT, which decomposes the Vision Transformer model into smaller sub-models and leverages the state-of-the-art pruning method to streamline the complex network architecture. ED-ViT not only preserves the essential structure of the original model but also enables more efficient inference, maintaining high system accuracy within the memory and energy constraints of edge devices. Extensive experiments have been conducted on five datasets, three ViT architectures, and two baseline methods, using three evaluation metrics of accuracy, inference latency, and total memory size. The results demonstrate that ED-ViT significantly reduces overall energy consumption and inference latency on edge devices while maintaining high inference accuracy. Our ED-ViT shows great potential for deployment on edge devices and for future integration with other horizontal methods to achieve better performance.

SECTION: References
SECTION: Background
This section describes the applications that utilize concurrent edge devices.

Recently, edge devices, such as wireless smart cameras, have already been widely employed for video analytics, including traffic control and industrial activity monitoring. Low-power edge devices can be deployed ad hoc whenever needed. Vision Transformer is an ideal deep learning model for these tasks due to its high accuracy. However, a complex Vision Transformer may not fit within the limited memory and resource-constrained edge devices. One potential solution is to split the original complex Vision Transformer into multiple small class-specific sub-models, which can be loaded into edge devices to process the consecutive image frames sequentially, incurring only a few frame delays. The outputs from the sub-models are then combined to get the final real-time video analytics results, as illustrated in Figure.

In smart homes, an increasing number of concurrent microphones on edge devices are used for collaborative keyword spotting and speech recognition. In some factories, deep learning models monitor machine conditions by detecting sound spectrograms. Beyond the main focus of low-power video analytics, Vision Transformer could also be employed to analyze audio data, demonstrating the generalization capability of the ED-ViT framework.

SECTION: Pruning Method Details
SECTION: Analysis of Prunable Parameters
The main prunable components in a ViT block, as illustrated in Figure, are:

(1): The channels across the shortcut connections (residuals) within the transformer blocks can be pruned to reduce dimensionality. These channels, denoted in red in the figure, are critical for maintaining information flow between layers, but may contain redundant channels that can be pruned without significantly affecting the model performance. Pruning these channels can help streamline the computational flow across transformer blocks.

(2): In the MHSA module, the input tokens are projected into queries, keys, and values, each of which is split acrossattention heads. Each head independently computes self-attention, which is then concatenated and followed by a fully connected layer that restores the original dimension of. The dimensions of the query, key, and value projections (,, and), can be pruned to reduce the computational cost associated with the attention mechanism, as shown in green in the figure.

(3): The FFN consists of two fully connected layers, where the hidden dimensionis typically set to four times the embedding dimension, i.e.,. The first layer expands the input to the hidden dimension, and the second layer reduces it back to, ensuring the output dimension matches the input of the next block. As the hidden dimensioncontributes significantly to the overall computational cost of the FFN, it becomes a critical target for pruning, as indicated in blue in the figure. By reducing, we can effectively decrease the complexity of the FFN while retaining its functionality.

These three components — residual connection channels (), MHSA heads (), and FFN hidden dimensions ()—are the primary targets for pruning in our approach. By pruning redundant parameters, we aim to reduce the model’s computational cost without sacrificing its core functionality and accuracy.

SECTION: Pruning Process
The pruning process is carried out in stages, with each stage focusing on one of the prunable components. As illustrated in Figure, the yellow sections represent the parameters being pruned at each stage, while the gray sections indicate parameters that were pruned in previous stages. The pruning factorcontrols the degree of reduction in the parameters, and we calculate the importance of each channel or head using KL-Divergence.

To evaluate the importance of each component, we compute the KL-Divergence between the output distributions of the original model and the pruned model, as follows:

whererepresents the output distribution of the original model, andrepresents the distribution after pruning. This divergence helps to identify which channels, heads, or neurons contribute the least to the model’s overall performance, making them ideal candidates for pruning.

: In the first stage, we focus on pruning the channels in the residual connections (shown in red). Using KL-Divergence, we identify and prune the channels that contribute the least, reducing the dimensionality fromto. This helps to streamline the flow of information between layers without significantly affecting model performance.

: In the second stage, instead of directly removing entire heads in the MHSA module, we prune the least important dimensions within the query, key, and value projections (,, and) across multiple heads. Using KL-Divergence, we identify and remove the less significant dimensions within each head, and then merge the remaining important dimensions from different heads into new, consolidated heads. This process effectively reduces the total number of heads to, without entirely discarding any head, thus maintaining a balanced representation of the attention mechanism while reducing its complexity. The dimensionality of the projections is scaled accordingly to reflect the merging and pruning process, ensuring that the model retains its ability to capture token interactions.

: The final stage involves pruning the hidden dimensionin the FFN, as shown in blue. By calculating KL-Divergence, we identify the least important neurons and reduce the hidden dimension fromto. The second fully connected layer retains the output dimension of, ensuring that the input-output structure remains consistent while the computational load is significantly reduced.

Following each pruning stage, the model is fine-tuned to recover any performance loss that may result from the parameter reduction. This ensures that the pruned model achieves a similar level of accuracy as the original model, while requiring fewer computational resources.

SECTION: Dataset Details and Preprocessing Details
The CIFAR-10 dataset consists of 60,000 32x32 color images distributed across ten classes, with 6,000 images per class. There are 50,000 training images and 10,000 test images. The MNIST dataset contains binary images of handwritten digits, comprising 60,000 28x28 training images and 10,000 testing images. The Caltech256 dataset consists of 30,607 images spanning 256 classes, with each class containing between 80 and 827 images. For our evaluation, we choose the 10 classes with the highest number of images to get 4,525 images and form the dataset, using 3,620 for training and 905 for testing.

The GTZAN dataset is a widely used benchmark in the music information retrieval community, containing 1,000 soundtracks of 30 seconds each, uniformly distributed across 10 genres. The Speech Command (v2) dataset comprises 105,000 one-second audio utterances of 35 keywords (e.g., “one”, “two”, “yes”, and “no”).
For each audio sample, we extract its Mel-spectrogram features using a frame length of 2048, a hop length of 512, and 128 Mel filters, converting each sample into 128×1291×1 and 128×32×1 tensors for the GTAN and Speech Command (v2) datasets, respectively. We also consider the 10 classes with the most data samples in the experiments with the Speech Command (v2) dataset.

Then, we resize the data samples as presented in Section.

SECTION: Extend ED-ViT to Transformer
There are also methods focused on Transform pruning, which could be utilized to deploy Transformers to edge devices.uses the LayerDrop method to train and prune the Transformer.explores the results when varying the number of attention heads. LTPapplies token pruning based on differentiable soft binary masks. EdgeFormerattempts to deploy the original Transformer onto edge devices by utilizing compression methods to create a “Parameter-Efficient” model. However, it still suffers from low accuracy. The primary differences between Vision Transformer and the original Transformer lie in the Patch Embedding module, which does not include the convolution kernels in the original Transformer. We could also treat the Patch Embedding module as a fully connected structure. Therefore, ED-ViT can also be effectively applied to Transformers, which could further be adapted for tasks such as Text Classification.

SECTION: Additional Experiments
SECTION: Experiments of FLOPs computation
Tablepresents the FLOPs for each edge device on CIFAR (computer vision/video) and GTZAN (audio recognition) datasets with different numbers of edge devices using the ViT-Base model. The FLOPs for the original model on the CIFAR-10 and GTZAN datasets are 16.86G and 16.79G, respectively. As the number of edge devices increases, the FLOPs decrease across all datasets, and the experimental results are consistent with the parameter counts. These findings demonstrate that ED-ViT significantly reduces computation overhead and saves energy for the edge devices.

SECTION: Case Study for Heterogeneous Sub-models
We use three edge devices as examples and conduct experiments using ViT-Base with a memory size budget of 100MB. After running the algorithm proposed in this paper, we obtain three sub-models: two with 4 heads (37.16 MB) and one with 3 heads (21.13 MB). The fused model achieves an accuracy of 86.80%, with a total memory size of 95.45 MB and an inference latency of 5.05s. These experiments demonstrate that ED-ViT can effectively handle varying memory size budgets with heterogeneous sub-models while maintaining strong performance.

SECTION: Case Study for Heterogeneous Edge Devices
For ten edge devices, we set 9 normal edge devices with 0.5G FLOP/s and 10MB memory sizes, a specific one with 1.6G FLOP/s and 30MB memory size. We conduct experiments using ViT-Base with our ED-ViT and the memory budget is 100MB. Then, we have ten sub-models with 2 heads (9.6 MB). With our Algorithm, in the end, 3 sub-models are deployed on the specific device, and the remaining FLOP/s and memory size are 0.12G FLOP/s and 1.2 MB, respectively. 7 sub-models are deployed on one normal edge device, with the remaining 0.04G FLOPs and 0.4 MB memory sizes; while there are two edge devices are idle. These experiments demonstrate that ED-ViT can effectively handle heterogeneous edge devices to maximize the minimal remaining energy, which is to minimize the maximal inference latency.

SECTION: Experiments with Transformer
We adopt an encoder model consistent with ViT, specifically utilizing BERTas the baseline model. This choice allows us to maintain compatibility with Transformer architectures while focusing on the model’s pruning and inference capabilities. We select the 20 Newsgroupsdataset for this study, limiting our scope to 10 classes, resulting in a total of 8,452 texts. These samples serve as the basis for training, pruning, and subsequent inference tasks. By leveraging BERT’s structure, we aim to explore the potential of Transformer-based models for efficient deployment on edge devices, enhancing their adaptability and practical application in resource-constrained environments. The results are shown in Table, which demonstrate the potential of deploying all kinds of Transformer structures on edge devices with extraordinary performance.

SECTION: t-SNE observation and discussions
To quickly demonstrate the effectiveness of ED-ViT, we show the t-SNE visualizationof our fused model and a randomly chosen model from 2, 3, 5, 10 models on the CIFAR-10 dataset as an example with the test dataset. The visualization is shown in Figure. Note that the results from different fused models of different numbers of devices are similar, as shown in Figure. We randomly choose one fused model from these settings. When the device number is 1, the results are similar to the fused global model. Thus, we omit the results from one edge device setting.

As shown in Figure, ED-ViT generates the fused model, which can clearly distinguish these classes. Meanwhile, the classes are separate. When the number of devices increases, the results get worse as the model is tailored to handle fewer data classes. As for the sub-model in Figure, we could see that the randomly chosen sub-model could only distinguish the data from label 1. The sub-model is responsible for only one class, which is consistent with our assumptions in the 10 edge device setting. The visualization from the fused global model and the sub-models clearly shows that ED-ViT could effectively utilize the features of each sub-model and fuse them together to get good results, which is suitable for splitting and distributed inference collaboration settings.