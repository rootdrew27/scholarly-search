SECTION: PePR:PerformancePerResource Unit as a Metric to Promote Small-scale Deep Learning in Medical Image Analysis

The recent advances in deep learning (DL) have been accelerated by access to large-scale data and compute. These large-scale resources have been used to train progressively larger models which are resource intensive in terms of compute, data, energy, and carbon emissions. These costs are becoming a new type of entry barrier to researchers and practitioners with limited access to resources at such scale, particularly in theGlobal South. In this work, we take a comprehensive look at the landscape of existing DL models for medical image analysis tasks and demonstrate their usefulness in settings where resources are limited. To account for the resource consumption of DL models, we introduce a novel measure to estimate the performance per resource unit, which we call the PePR111Pronouncedpepper.score. Using a diverse family of 131 unique DL architectures (spanningtotrainable parameters) and three medical image datasets, we capture trends about the performance-resource trade-offs. In applications like medical image analysis, we argue that small-scale, specialized models are better than striving for large-scale models. Furthermore, we show that using existing pretrained models that are fine-tuned on new data can significantly reduce the computational resources and data required compared to training models from scratch.
We hope this work will encourage the community to focus on improving AI equity by developing methods and models with smaller resource footprints.222Source code:https://github.com/saintslab/PePR.

SECTION: 1Introduction

The question of material costs of technology, even in light of their usefulness, should not be ignored[heikkurinen2021sustainability]. This is also true for technologies such as deep learning (DL) that is reliant on large-scale data and compute, resulting in increasing energy consumption and corresponding carbon emissions[strubell2019energy]. These growing resource costs can hamper their environmental and social sustainability.[kaack2022aligning,wright2023efficiency].

Considerations towards improving the environmental impact of DL are garnering attention across different application domains. This has resulted in calls for action broadly, and also within the medical image analysis community, to improve the resource efficiency of DL models[bartoldson2023compute,selvan2023operating]and to report the energy and carbon costs[selvan2022carbon]. Another important implication of the growing resource costs of DL is the risk of disenfranchising practitioners with limited access to resources. This is captured in Figure1which shows the number of publications (per capita) within DL across the world for 2013 and 2022333The data for the visualisation in Figure1was curated by querying for the number of research publications per country on the topic of “Artificial Intelligence” in OpenAlex.org. The population data per country was queried from data.WorldBank.org. Regional aggregation was performed using OECD standards and further refined into the ten regions. Curated data will be provided along with the source code.. Several regions in the world categorised asGlobal Southare continuing to lag behind in research in DL[oecd2024ai]. While there are also multiple other structural reasons for this trend, the increasing resource costs of performing research within DL can become a new form of entry barrier that can aggravate this disparity[ahmed2020democratization].

In light of these observations, this work argues for focusing on small-scale DL in the era of large-scale DL. We hypothesize that the current situation with the increasing resource consumption is due to the singular focus on task-specific performance metrics that are not grounded in material costs. We also argue that access is a prerequisite to improving equity in DL and in use of these methods in healthcare. These arguments are supported by a comprehensive analysis of performance and resource costs of DL-based computer vision models. We study the properties ofmodels ranging fromtotrainable parameters, on three medical image classification tasks to capture interesting trends. We provide qualitative evidence for the usefulness of using pretrained models in resource-constrained regimes. Finally, we present a novel composite measure of performance and resource consumption. We call this the performance per resource unit (PePR) score. Using the PePR-score we characterise the behaviour of small-scale and large-scale DL models. We demonstrate that in resource-constrained regimes, small-scale DL models yield a better trade-off between performance and resource consumption.

SECTION: Related Work:

Pareto optimisation of performance and resource constraints has been primarily investigated within the context of neural architecture search (NAS)[elsken2019neural]. More recently, methods have been proposed to explore models using specific resource constraints such as energy consumption[evchenko2021frugal,bakhtiarifard:24]or carbon footprint[moro2023carburacy]. The work in[evchenko2021frugal]proposes a resource-aware performance metric similar to our contribution in this work which, however, is concerned with non DL models. Within application domains such as medical image analysis, there has been little emphasis on the joint optimisation of performance and energy consumption[selvan2021carbon]. The question of equitable AI within healthcare has been posed in works like[baumgartner2023fair]primarily from the context of fairness and not from resource/access perspectives.

SECTION: 2PePR-score

In this work, we assume a DL model to be an entity that consumes resources such as data, energy, time, or CO2eq. budget as input and provides some measurable predictive performance on downstream tasks of interest. In contrast to conventional performance metrics that are not grounded in material costs, we envision a score that can take the resource costs into account. To this end, we introduce the notion of performance per resource unit (PePR), denoted as, which relates (normalised) performanceof a model with the resources consumed and defined as

In this definition,is the resource cost normalised to lie in, or explicitlyfor some absolute resource costand somefixed across models within an experiment.444Standard scaling might not always be appropriate. Outliers may have to be considered, and in other instancesmight depend on the experimental set-up.

The salient features of the PePR-score that make it useful as an alternative objective that takes resource costs into account are as follows:

Performance-dependent sensitivity:From the plot of the PePR isoclines (see Figure2-a)), it is clear that PePR is insensitive to resource consumption for models with low performance. For models with high performance, PePR attributes almost identical weight to performance and to resource consumption.

PePR-score for a single model:PePR score is a relative measure of performance-resource consumption trade-off. In instances where a single model is considered, it is the same as performance. This is due to the fact thatand.

Comparing two models:Consider the case where only two models are compared with respective absolute resource consumptionsand test performances. If, then the normalized resource costs arebecause. Thus,and.

PePR-score of random guessing:Consider a binary classification task with no class imbalance. In this setting, the performance of random guessing should be about. As thefor this “model”, the PePR-score is the same as performance.

(a)

(b)

(c)

Depending on what resource costs are used, different variations of the PePR-score can be derived. For instance, if energy consumption is used as the cost, thenresulting in the PePR-E score. Similarly, one can derive the PePR-C (CO2eq.), PePR-D (data), PePR-M (memory), or PePR-T (Time) scores. Idealised PePR-E scores are plotted in Figure2-a) which captures the trade-off between performance and resource consumption. Models with low resource consumption and high performance would gravitate towards the upper left corner where the PePR score approaches unity.

We also note that in cases where performance is deemed to be more important than resource consumption, PePR score can be adjusted to reflect this. For instance, one can employwith a scaling factor. Setting a largevalue, say, would prioritise performance and disregard the effect of the resource consumption. As an example, consider the PePR score for the most resource intensive model that also achieves the best performance (i.e.,). According to the definition in Eq. (1), the PePR score is. Increasing the emphasis on performance usinggives, basically ignoring the resource costs, if the application warrants this. Adjustingoffers a spectrum of trade-offs between performance and resource costs. In this work, we are focussed on operating in resource constrained regimes, and are mainly interested in the setting.

SECTION: Performance curve:

For a functionrepresenting a performance curve mapping resource costs to performance (e.g., if the resource is update steps or training data set size, it represents a rescaled learning curve), we define a PePR curve:

where in cases of ties the smallest value is picked.
Furthermore, in order to be able to compare models based on their performance curves, we define a scalar quantityby

To get some intuition on the PePR score, we can rewrite (2) as the integral of its derivative to obtain the integral representation

Here,is the derivative ofwith respect to resource consumption, which can be interpreted as how much of a performance increase the model is able to get per resource consumed. First, note the presence of the weighting factorsand, which express that the score puts a higher weight on the performance of the model in low-resource regimes (small).

Second, we can see that the score emphasizes performance per resource consumed (first integral with) and de-emphasizes absolute performance (second integral with). Since all integrals are positive, the PePR score is always greater or equal to the performance of the model at zero resource consumption.

Since, if we assumeto be increasing, we also have that PePR increases in intervals whereand decreases in intervals where.555Because of the bound for the second integrand in (2):This captures the idea that the maxima of the PePR curve lie at points of diminishing returns as captured by, which is also visualized in Figure2-b).

SECTION: 3Data & Experiments

To demonstrate the usefulness of the PePR-score, we curated a large collection of diverse, neural network architectures and experiment on multiple datasets.

SECTION: Space of models:

The model space used in this work consists ofneural network architectures specialised for image classification. The exact number ofarchitectures was obtained after seeking sufficiently diverse models which were also pretrained on the same benchmark dataset.

We used the Pytorch Image Models (timm) model zoo to access models pretrained on ImageNet-1k resulting inmodels spanningM toM trainable parameters. We randomly sub-sampled the available models inPytorch Image Modelslibrary[rw2019timm], which during our experiments had about 700 models. We chose as many unique architectures as possible that were all pre-trained on the same ImageNet dataset. This resulted in the 131 models used in our work, covering CNNs, vision transformers, hybrid models, and efficient architectures.

We categorise these models along two dimensions i)CNNorOtherdepending on if the architecture is a generic CNN primarily consisting of convolutional layers, residual connections, and other standard operators. This implies transformer-based models[vaswani2021scaling], for instance, are markedOtherii)EfficientorNot Efficientif the descriptions in the corresponding publications discuss any key contributions for improving some aspect of efficiency. Given these categorisations, we end up with a split of 80 and 51 forCNN, Other, respectively, and 31 and 100 forEfficient, Not Efficient, respectively. The median number of parameters is 24.6M. We further classify the models in the lower half to besmall-scaleand the upper half intolarge-scalefor simplicity. The model space is illustrated in Figure2-c)  and additional details are provided for each model in Table2.

SECTION: Datasets:

Experiments in this work are performed on three medical image classification datasets: Derma, LIDC, Pneumonia. Derma and Pneumonia datasets are derived from the MedMNIST+ benchmark[yang2023medmnist]and LIDC is derived from the LIDC-IDRI dataset[armato2004lung]. Images in all three datasets are ofpixel resolution with intensities rescaled to. All three datasets are split into train/valid/test splits: Derma (7,007/1,003/2,005), LIDC (9,057/3,019/3,020), and Pneumonia (4,708/524/624). Derma consists of seven target classes whereas the other two datasets contain binary labels.

SECTION: Experimental design:

All models were implemented in Pytorch, trained or fine-tuned for 10 epochs with a learning rate ofusing a batch size ofon an Nvidia RTX3090 GPU workstation with 24 GB memory. Statistical significance is measured by-tests.
We considered training or fine-tuning of 10 epochs to reduce the compute resources used in this work. We expand on this choice in Sec.4. The training of models in this work was estimated to use 58.2 kWh of electricity contributing to 3.7 kg of CO2eq. This is equivalent to about 36 km travelled by car as measured by Carbontracker[anthony2020carbontracker].

SECTION: Experiments and Results:

We performed three main experiments with our large collection of models: i) Study the influence of pretraining on test performance ii) Evaluate the role of number of training points iii) Compute PePR-E score and compare the trade-off between test performance and energy consumption as the cost. Results from all three experiments are summarized in Figure3.

We had access to pretrained weights for allmodels, which made it possible to investigate the influence of using pretraining when resources are constrained. We either fine-tune or train-from-scratch all models forepochs. In Figure3-a), across the board, we notice that using pretrained models are significantly better compared to training models from scratch for the same number of epochs ().

Another resource that can be lacking, on top of compute/energy, is the amount of training data. We study this by only usingof the training data, for each of the three datasets, and reporting the average test performance per model in Figure3-b). Even though there is a significant test performance difference () when only usingof the data compared to usingof the data, it could be still useful in making some preliminary choices.

The overall test performance averaged across the three datasets is plotted against the number of parameters, along with architecture classes, in Figure3-c). There was no significant group difference in test performance between small- and large-scale models. Similarly, there was no significant difference between models that areEfficientandNot Efficient, or betweenCNNandOther.

Finally, in Figure3-d) we visualise the PePR-E score for all the models, which uses the energy consumption for fine-tuning for 10 epochs as the resource, which is then normalised within each experiment (dataset). The first striking observation is that the PePR-E scores for the larger models reduce, whereas for the smaller models there is no difference relative to other small-scale models. This is expected behaviour as PePR score is performance per resource unit, and models that consume more resources relative to other models will get a lower PePR score. We observed a significant difference in median PePR-E scores between small and large models for all three datasets, with the group of small models having a higher median PePR-E score (), shown in Figure5. We did not consistently observe any other significant difference across datasets in test performance or PePR-E score when stratifying by model type (CNNvs.Other) or betweenEfficientandNot Efficientmodels. Results for the top five models sorted based on their PePR-E score for each dataset along with their test performance, number of parameters, memory consumption, absolute energy consumption, training time for 10 epochs, are reported in Table1. We also report the best performing model when only test performance is used as the criterion for comparison.

SECTION: 4Discussion & Conclusion

Our experiments reported in Figure3and Table1reveal interesting trends about the interplay between test performance and resource consumption. We consider all models below the median number of parameters (24.6M) to be small-scale, and above as large-scale models, visualised demarcated using the gray-shaded regions in all relevant figures. We noticed no significant difference in performance between the small-scale and large-scale models in the regime where they were fine-tuned with pretrained weights for 10 epochs. This captures the problem with focusing only on test performance, as it could easily yield large-scale models even when small-scale models could be adequate. However, when using the PePR-E score, we see a significant performance difference with the small-scale models achieving a higher PePR-score (). This emphasises the usefulness of taking resource costs into account, which can be easily done using any variations of the PePR score.

Energy, or other resource, consumption awareness can also be incorporated using multi-objective optimisation[bakhtiarifard:24]. PePR score can be thought of as one way to access the solutions on the Pareto front with an emphasis on low-resource footprint. This is captured in Figure4which overlays the Pareto set (in orange) and all other models over the PePR scores. The knee point of this Pareto front is pointing towards maximising PePR-E score (brighter regions).

PePR score is a composite metric that offers a trade-off between performance and resource consumption. It can be used instead of multi-objective optimisation of the two objectives separately. As shown in our experiments, PePR score can be used to compare models that use different extents of resources. Current reporting in deep learning for image analysis focus on performance metrics like accuracy while disregarding the resources expended[selvan2022carbon]. Furthermore, PePR can be used to choose the best model under a known resource constraint, such as maximum memory or energy consumption allowed.

The key experiments reported consider energy consumption as the main resource in the PePR-E score. Additional metrics (PePR-M for memory, PePR-C for carbon emissions, PePR-T for training time) reported in the Figure6show the versatility of the PePR score. We can envision a general PePR score which can consider all resources into account by weighting them differently. For example, usingwith, where the different weights can be adjusted depending on the application.

SECTION: Limitations:

We used a training or fine-tuning budget of 10 epochs in this work to reduce the compute resources used. This can be a limitation, as different models learn at different rates. To show that our experimental results are not artifacts of this choice, we looked at the performance of models that have been trained to convergence on ImageNet (which formed the basis of pre-training) using the public dataset from[rw2019timm]. We performed a similar analysis of validation set performance of the converged models, The PePR-M scores are shown in Figure4-b), and they show similar trends as our experiments in Figures3and6.

The PePR score itself is agnostic to the downstream task. In this study, the experiments focussed on medical image classification, which may limit the generalisability of the results. While the findings were consistent across the considered data sets, expanding the study to other tasks (segmentation) and domains (non-image) in future work might provide further insights.

SECTION: Conclusions:

Using a large collection of DL models we have shown that using pre-trained models yields significant gains in performance, and should always be considered. We have also shown that when resource consumption is taken into account, small-scale DL models offer a better trade-off than large-scale models. Specifically, the performance achieved per unit of resource consumption for small-scale models in low-resource regimes is higher. We proposed the PePR score that offers an inbuilt trade-off between resource consumption and performance. The score penalises models with diminishing returns for a given increase in resource consumption.

Questions around how best to improve equity in research and healthcare are neither easy nor straightforward, go far beyond the ways in which we use specific types of DL, and cannot be fixed through technological solutionism[morozov2013save]. Nevertheless, using small-scale DL can help mitigate certain types of inequities by reducing some of the barriers that are currently in place for researchers and practitioners with limited access to resources. Small-scale DL can be developed and run on end-point consumer hardware which is more pervasive than specialised datacenters with high performance computing in many parts of the world.
With this work, we sincerely hope that by focusing on reducing the resource costs of DL to improve access the larger question of equity in DL for healthcare will be grappled with by the community.

Acknowledgments:

RS, BP, CI, and ED acknowledge funding received under European Union’s Horizon Europe Research and Innovation programme under grant agreements No. 101070284 and No. 101070408. CI acknowledges support by the Pioneer Centre for AI, DNRF grant number P1. GS would like to acknowledge Wellcome Foundation (grant number 222180/Z/20/Z).

SECTION: Appendix AAdditional Results