SECTION: Supervised Semantic Similarity-based Conflict Detection Algorithm: S3CDA

In the realm of software development, the clarity, completeness, and comprehensiveness of requirements significantly impact the success of software systems. The Software Requirement Specification (SRS) document, a cornerstone of the software development life cycle, delineates both functional and nonfunctional requirements, playing a pivotal role in ensuring the quality and timely delivery of software projects. However, the inherent natural language representation of these requirements poses challenges, leading to potential misinterpretations and conflicts. This study addresses the need for conflict identification within requirements by delving into their semantic compositions and contextual meanings. Our research introduces an automated supervised conflict detection method known as the Supervised Semantic Similarity-based Conflict Detection Algorithm (S3CDA). This algorithm comprises two phases: identifying conflict candidates through textual similarity and employing semantic analysis to filter these conflicts. The similarity-based conflict detection involves leveraging sentence embeddings and cosine similarity measures to identify pertinent candidate requirements. Additionally, we present an unsupervised conflict detection algorithm, UnSupCDA, combining key components of S3CDA, tailored for unlabeled software requirements. Generalizability of our methods is tested across five SRS documents from diverse domains. Our experimental results demonstrate the efficacy of the proposed conflict detection strategy, achieving high accuracy in automated conflict identification.

SECTION: 1Introduction

Requirement Engineering (RE) is the process of defining, documenting, and maintaining the software requirements[1].
RE process involves four main activities, namely, requirements elicitation, requirements specification, requirements verification and validation, and requirements management. In the requirement specification process, the deliverable is termed as Software Requirement Specification (SRS) document which is highly important in Software Development Life Cycle (SDLC)[2]. SRS documents describe the functionality and expected performance for software products, naturally affecting all the subsequent phases in the process. The requirement set defined in SRS documents are analyzed and refined in the design phase, which results in various design documents. Then, the developers proceed with these documents to build the code for the software system[3].

SRS documents are mostly written in natural language to improve the comprehensibility of requirements. The success of any software system is largely dependent on the clarity, transparency, and comprehensibility of software requirements[4]. Conflicting and incomprehensible software requirements might lead to increased project completion times, inefficiency in software systems, and increase in the project budget. Detection of conflicts in the earlier development phase is very important, however, the manual identification of these conflicts could be tedious and time-consuming. It is necessary to develop semi-automated or automated approaches for conflict detection in SRS documents. Considering the structure of software requirements, Natural Language Processing (NLP) methods can help in analyzing and understanding the software requirements semantically. Various information extraction techniques such as Named Entity Recognition (NER), and Parts of Speech (POS) tagging can be used for this purpose, alongside the semantic similarity of the natural language text to interpret the context and syntactic nature of the software requirements.

In order to provide an automated approach for generalised conflict identification, we propose a supervised two-phase framework i.e., Supervised Semantic Similarity-based Conflict Detection Algorithm (S3CDA) which elicits the conflict criteria from the provided software requirements, and outputs the conflicting requirements.
In the first phase, we convert the software requirements into high dimensional vectors using various sentence embeddings, and then identify the conflict candidates using cosine similarity. Then, in the second phase, candidate conflict set is further refined by measuring the overlapping entities in the requirement texts, with high level of overlaps pointing to a conflict. Furthermore, we formulate an unsupervised variant of our proposed algorithm, called as UnSupCDA. This algorithm seamlessly integrates the core elements of the S3CDA approach and is adept at handling unlabeled requirements and identify the conflicts.

The main contributions of our study can be summarized as follows:

We introduce two novel conflict identification techniques, namely S3CDA and UnSupCDA, meticulously designed through a comprehensive analysis of software requirement structures. We assess the efficacy of these proposed methodologies through extensive numerical experiments conducted on five diverse SRS documents, providing valuable insights into their practical applicability and performance.

We make use of information extraction techniques in detecting the requirement conflicts. Specifically, we apply software-specific NER model to extract the key entities from the software requirements which can be useful in indicating the presence of conflicts. Additionally, we conduct a thorough analysis of the correlation between semantic similarity among requirements and the overlap of entities in requirement pairs across diverse datasets.

The remainder of the paper is organized as follows. Section2provides the background on the problem of conflict identification in software requirement datasets. Section3introduces our proposed method for automated conflict detection, and provides a detailed discussion over dataset characteristics, sentence embeddings, and NER. In Section4, we present the results from our experiments and discuss the applicability of our proposed approaches. Lastly, Section6provides concluding remarks and future research directions.

SECTION: 2Background

Previous studies suggest the use of NLP-based techniques to solve various software requirement related problems such as requirement classification[5,6], ambiguity detection[7], bug report classification[8], duplicate bug report prediction[9], conflict identification[10], and mapping of natural language-based requirements into formal structures[11]. Conflict detection is one of the most difficult problems in requirement engineering[3]. Inability in identifying the conflicts in software requirements might lead to uncertainties and cost overrun in software development. Several papers discussed conflict identification in various domains, however, an autonomous, reliable and generalizable approach for detecting conflicting requirements is yet to be achieved. Below, we first introduce the basic definitions and then we review the conflict detection strategies for functional and non-functional requirements.

The terms ‘Ambiguity’ and ‘Conflict’ can be misconstrued in the requirement engineering context. Researchers have provided formal definitions for the requirement ambiguity as a requirement having more than one meaning, and provided various techniques to detect the requirement ambiguities in SRS documents[1,12]. On the other hand, requirement conflict detection remains as a challenging problem, lacking a well-accepted formal definition and structure. Several studies define the requirement conflict depending upon the domain of requirements. However, the term ‘conflict’ can be defined more broadly as the presence of interference, interdependency, or inconsistency between requirements[13].Kim et al. [14]proposed the definition of requirement conflict as interaction and dependencies present between requirements which results into negative or undesired operation of the software systems.

Butt et al. [15]defined requirement conflicts based on the categorization of requirements to mandatory, essential, and optional requirements.Kim et al. [14]described the requirement structure as Actor (Noun) + Action (verb) + Object (object) + Resource (resource). An activity conflict can arise when two requirements achieve the same actions through different object and a resource conflict may arise when different components try to share the same resources.Moser et al. [16]categorized the conflicts as simple (if exists between two requirements) and complex (if exists between three or more requirements). Recently,Guo et al. [10]proposed a comprehensive definition for semantic conflicts amongst different functional requirements. They stated that if two requirements having inferential, interdependent, and inclusive relationship then it may lead to inconsistent behaviour in software system.

In our work, the conflicts are defined based on the premise that if the implementation ofandcannot coexist or if the implementation of the first adversely impacts the second, then they are considered conflicts in our dataset. An example of such a conflict can be observed in the following requirements:

The UAV shall charge to 50 % in less than 3 hours.

The UAV shall fully charge in less than 3 hours.

It’s not feasible to implement these requirements simultaneously as it will lead to inconsistency in the system. Notably, for the purpose of this study, requirements deemed as duplicates or paraphrased versions of each other are also considered conflicts due to their inherent redundancy.

Functional requirements specify the functionalities or features a system must possess and describe how the system should behave or what it should do. They outline the specific actions the system must be able to perform and typically address the system’s core operations. Table1lists the studies for conflict identification in functional requirements. The table provides insights into the domain of SRS documents, the datasets employed, and the types of conflicts addressed in each study. The majority of these studies utilize rule-based and heuristic methods, facing limitations associated with the scarcity of extensive datasets and the absence of a standardized methodology. Often reliant on case-study approaches, these investigations typically validate their methods using a limited set of requirements, posing challenges for direct comparisons with our study.

Guo et al. [10]introduced a methodical approach, FSARC (a Finer Semantic Analysis-based Requirements Conflict Detector), aiming for a comprehensive semantic analysis of software requirements to identify conflicts. FSARC follows a seven-step procedure leveraging Stanford’s CoreNLP library[24]. The initial steps involve Part-of-Speech (POS) tagging and Stanford’s Dependency Parser (SDP) to transform each requirement into an eight-tuple representation. Subsequent rule-based routines are applied to identify conflicts based on this tuple. While the algorithm exhibited promising results and potential for generalization, it heavily depends on the CoreNLP library’s accurate generation of the eight-tuple, suggesting a reliance on a specific requirement structure for effective analysis.

Non-functional requirements define the criteria that characterize the operation of a system without detailing specific behaviors. These requirements focus on aspects such as performance, reliability, usability, scalability, and other qualities that are essential for the overall effectiveness and efficiency of the system but are not related to its specific functionalities. Table2presents the conflict identification studies for non-functional requirements.

Similar to the studies discussed in the previous section, the studies in Table2predominantly relied on rule-based methods and conducted validation on a limited number of requirements. However, the lack of substantial evidence hampers the demonstration of the generalizability of the methods.

Lastly, we note that our work differs from these existing studies in multiple ways:

Das et al. [30]introduced the idea of sentence embeddings for similarity detection in software requirements. We extend this idea and combine the two sentence embeddings (SBERT and TFIDF) to calculate the cosine similarity between the requirements.

Our proposed automated approaches directly works with software requirements as opposed toGuo et al. [10], which converts the software requirements into formal representations and apply rule-based procedures to detect the conflicts.

Different from finer semantic analysis enabled byGuo et al. [10]’s rule-based approach, we define the set of software-specific entities, and train NLP-based transformer models with for software requirements. These entities provide an additional way of verifying the conflicts semantically.

To provide the generic technique for conflict identification, we devise an unsupervised approach capable of handling raw requirements from varied domains and effectively capturing conflicts.

SECTION: 3Methodology

In this section, we first describe the SRS datasets used in our numerical study. Then, we provide specific details of the building blocks of our conflict detection algorithms and the experimental setup.

SECTION: 3.1Datasets

We consider five SRS datasets that belong to various domains such as software, healthcare, transportation, and hardware. Three of these are open-source SRS datasets (OpenCoss, WorldVista, and UAV), and the other two are extracted from public SRS documents. Generally, requirements are documented in a structured format and we retain the original structure of requirements for conflict detection process. To maintain the consistency in requirement structure, we converted complex requirements (e.g., paragraphs or compound sentences) into simple sentences. Table3provides summary information on the SRS datasets.

We briefly describe these SRS datasets below.

OpenCoss: OPENCOSS111http://www.opencoss-project.eurefers to Open Platform for Evolutionary Certification Of Safety-critical Systems for the railway, avionics, and automotive markets. This is a challenging dataset to identify the conflicts as the samples from the OpenCoss dataset indicates a lot of similar or duplicate requirements with repeating words. Initially, this set included 110 requirements and we added 5 more synthetic conflicts.

WorldVista: WorldVista222http://coest.org/datasetsis a health management system that records patient information starting from the hospital admission to discharge procedures. The requirement structure is basic, and written in natural language with health care terminologies. It originally consisted of 117 requirements and we added 23 synthetic conflicts.

UAV: The UAV (Unmanned Aerial Vehicle)[10,31]dataset is created by the University of Notre Dame and it includes all the functional requirements which define the functions of the UAV control system. The requirement syntax is based on the template of EARS (Easy Approach to Requirements Syntax)[32]. Originally, this dataset had 99 requirements and we added 16 conflicting requirements to the set, which resulted in a conflict proportion of 30%.

PURE: PURE (Public Requirements dataset), contains 79 publicly available SRS documents collected from the web[33]. We manually extracted set of requirements from two SRS documents, namely, THEMAS (Thermodynamic System) and Mashbot (web interface for managing a company’s presence on social networks). In total, we collected 83 requirements and induced synthetic 21 conflicts to maintain consistency with the other datasets.

IBM-UAV: This dataset is proprietary, and provided by IBM. It consists of software requirements used in various projects related to the aerospace and automobile industry. We sampled 75 requirements from the original set, and introduced 13 synthetic conflicts. The requirement text follows a certain format specified by IBM’s RQA (Requirement Quality Analysis) system.

The synthetic conflicts were introduced to each of these datasets by following the standard definitions provided in the literature[3,34]. Table4shows sample synthetic conflicts as indicated by requirement id, requirement text, a ‘Conflict’ column indicating the presence of conflict in ‘Yes’ or ‘No’ format and a ‘Conflict-Label’ column that indicates the pair of conflicts. For example, requirements 2 and 3 conflict with each other because of modal verb used in both requirements. Similarly, requirements 11 and 12 show mathematical operator conflict.

SECTION: 3.2S3CDA: Supervised Semantic Similarity-based Conflict Detection Algorithm

This section is structured into two parts as depicted in Figure1. First, we explain the similarity-based conflict detection. Second, we define the semantic-based conflict identification to validate the potential conflicts obtained in Phase I.

Algorithm1formalizes similarity-based conflict detection procedure (Phase I) provided in the left panel of Figure1.
The resulting set of conflicts can be used as a candidate conflict set for Phase II of our framework.
We first create the sentence embedding vector for each requirementusingSentenceEmbedding()procedure. It basically converts the requirements into numerical vector using sentence embeddings. Below, we describe the various sentence embedding models employed in the proposed algorithms.

TFIDF: Term Frequency Inverse Document Frequency, is employed for generating vectors from requirements[35]. Each requirement is treated as a document, and TFIDF scores are calculated for each term in the document. The TFIDF value for a term’ in a documentis given by, whereis the term frequency in document’, andis the inverse document frequency.

USE:
Universal Sentence Encoder (USE) translates natural language text into high-dimensional vectors[36]. We utilize the Deep Averaging Network (DAN) version of USE, a pre-trained model optimized for encoding sentences, phrases, and short paragraphs. It produces 512-dimensional vectors for each input requirement.

SBERT–TFIDF: This method combines Sentence-BERT[37](SBERT) and TFIDF embeddings. SBERT provides context and semantics in the vectors, while TFIDF prioritizes less frequent words. The process involves concatenating the vectors and employing Uniform Manifold Approximation and Projection (UMAP) for dimensionality reduction to ensure uniform vector size[38].

We next calculate the pairwise distance matrix (), which measures the cosine similarity value between each pair of requirements. Below, we provide a brief example of using cosine similarity for requirements.

It is an effective measure to estimate the similarity of vectors in high-dimensional space[39]. This metric models language-based input text as a vector of real-valued terms and the similarity between two texts is derived from the cosine angle between two texts term vectors as follows:

The values for cosine similarity ranges from -1 to 1 where -1 signifies dissimilarity and 1 signifies similarity. To better demonstrate how cosine similarity can be used over embedding vectors, we provide an illustrative example with three sample software requirements,,, and, which are defined as follows:

=‘The OPENCOSS platform shall be able to export evidence traceability links of an assurance project to external tools.’

=‘The OPENCOSS platform must be able to send out evidence traceability links of an assurance project to external tools and internal tools.’

=‘The OPENCOSS platform shall provide users with the ability to specify evidence traceability links in traceability matrices.’

We calculate the cosine similarity between these requirement vectors when embedded with TFIDF, SBERT, SBERT-TFIDF, and USE, which are reported in Table5. SBERT is able to capture the semantic similarity betweenandwith a cosine similarity value of 0.96. USE being the second highest with the value of 0.81. Requirement text foris not similar to those of the other two requirements, and all the sentence embeddings indicate low values of cosine similarity with.

Then, we use ROC curve (receiver operating characteristic curve) to identify the cosine similarity threshold (), which specifies the minimum similarity value after which requirements are labeled as conflicting.
The cutoff value () is selected as the value that maximizesover threshold valuesand the distance matrix. This way, we balance the false positives and true positives rates, with the conflicts having the positive labels.
Lastly, we assign labels of conflict or no-conflict to the requirements usingas threshold value.
The candidate conflict set
() contains all the requirements with conflict label. Note that conflict property is symmetric, i.e., ifis conflicting with, thenis also conflicting with, and,.

Algorithm2describes the process of semantic conflict detection as presented in right panel of Figure1.
This algorithm serves as a second filter on the candidate conflicts generated in Phase I.
Specifically, any candidate conflictis semantically compared against topmost similar requirements from.
That is, by focusing on onlymost similar requirements, we reduce the computational burden, and also make use of the cosine similarity between the requirements.
This semantic comparison is performed based on overlap ratio between the entities present in the requirements.
For a given candidate conflict, overlap ratio is calculated as

whererepresents the set ofmost similar requirements to candidate conflict.
The functioncalculates the number of overlapping entities betweenand, and functioncalculates the number of unique entities in candidate conflict (i.e., a requirement text).
The calculated overlap ratioforis then compared against a pre-determined overlap threshold value,, andis added to final conflict setif.
In our analysis, we setand, which are determined based on preliminary experiments.
In Algorithm2,GetSimilarRequirements() returns the listofmost similar requirements fromfor candidate conflict, andGetMaxOverlapRatio() returns the maximum value for the overlaps between requirements from setand candidate conflict.

Require:Candidate conflict set:Requirement set:# of similar requirements:Output:Refined conflict setInitialization://Initialize conflict set to an empty set//Set overlapping threshold as 1//Set number of similar requirements as 5Fordo:GetSimilarRequirements()  // Get thesimilar requirementsGetMaxOverlapRatio()  //Calculate max. overlap ratio using Eqn. (2)If:  //Compare with threshold//Augment the final conflict set

To extract the entities from the requirements, we employ two NER techniques described below.

Part-of-Speech (POS) Tagging:[40]suggest that a software requirement should follow the structure as Actor (Noun) + Object + Action (Verb) + Resource. The generic NER method extracts ‘Noun’ and ‘Verb’ tags from the requirements based on this structure and referred as ‘POS’ tagging. We employ POS tagger provided in SpaCy library in Python.

Software-specific Named Entity Recognition (S-NER): NER serves to extract relevant entities from input text, and its effectiveness can be enhanced by training machine learning models on domain-specific corpora. In our context, we leverage a software-specific NER system to extract entities crucial for understanding requirements, specifically focusing on actor, action, object, property, metric, and operator.

In the context of requirements, an “Actor” denotes an entity interacting with the software application, while an “Action” represents an operation performed by an actor within the software system. To illustrate, consider the requirement “The UAV shall charge to 75% in less than 3 hours,” whereUAVserves as the actor, andchargeas the corresponding action. We employ transformer models trained on software-specific corpora to proficiently extract these entities from requirement pairs[41].

We also provide sample calculations in Table6to better illustrate the process in Algorithm2. We show the overlapping software-specific entities present in candidate requirement () and similar requirement () with different color codes.
For instance, the entity ‘UAV’ is represented by blue color. The requirementsandboth return high overlap ratios, indicating a conflict with.

SECTION: 3.3UnSupCDA: Unsupervised Conflict Detection Algorithm

We devise an unsupervised variant of the S3CDA approach to alleviate the need for labeled conflict data in the task of identifying conflicts within SRS documents. This unsupervised version seamlessly integrates key components from both phases of the original S3CDA approach, adapting them to function without the reliance on labeled data. By combining the strengths of the two phases, our unsupervised approach retains the efficiency and effectiveness of conflict identification while eliminating the necessity for manual annotation.

In Algorithm3, we first transform requirements into embedding vectors usingSentenceEmbeddingfunction, capturing their semantic content. Subsequently, a pairwise cosine similarity matrix () is computed usingPairwiseDistancefunction. For each requirement in, we identify thesimilar requirements and simultaneously calculate the entity overlapping ratio using Equation2. Then, we employs a predefined threshold () to determine the conflicts.

Require:Requirement set:Output:Conflict setInitialization://Initialize conflict set to an empty set//Overlapping threshold set//n highest similarity requirementSentenceEmbedding()//Generate requirement vectors//Get similarity matrixFordo:n_HighestSimilarityPairs(,)GetMaxOverlapRatio()  //max. overlap ratio using Eqn. (2)If:

We analyze the correlation between the overlapping entity ratio, as defined in Equation2, and cosine similarity through scatter plot visualizations. The requirements from each dataset are embedded using the SBERT model, and pairwise cosine similarity is computed. Subsequently, the most similar requirement is identified for each requirement in the dataset. To calculate the entity overlapping ratio, we employ POS tagging and the S-NER method for entity extraction. The overlapping ratio is then determined based on the extracted entities. Our objective is to investigate whether there exists a relationship between the cosine similarity of requirements and their entity overlap ratio, as both metrics play crucial roles in our proposed algorithms.

Examining Figure2, we discern an absence of a definitive correlation. Notably, in the case of the WorldVista dataset, Figures2aand2breveal a weak positive correlation, where high values of cosine similarity align with high overlapping ratios. This trend is consistent across other datasets. In the OpenCoss dataset, Figure3billustrates that most requirements exhibit both high cosine similarity and overlapping ratios, posing challenges for our conflict detection algorithms and elucidating the observed performance metrics. For other datasets, the scatter plots are presented in Appendix7.1.

SECTION: 3.4Experimental Setup

For the evaluation of our proposed approaches, we perform 3-fold cross validation over all the requirement datasets. That is, considering the distribution of conflicting requirements and the limited number of requirements, we divide each dataset into 3 different folds. Each fold includes some conflicting and non-conflicting requirements, however, we make sure that each conflict present in the fold should have its conflict pair present in the same fold. For our techniques, we use training set to determine the cosine similarity cut-off value, and apply this value on the corresponding test set. For S3CDA, we employ standard classification metrics such as macro averaged version of F1-score, Precision, and Recall in performance evaluation. Table7reports the hyperparameter and model configurations for the models used in our proposed approaches.

In the case of UnSupCDA, being an unsupervised task, we adopt the evaluation metrics introduced byGuo et al. [10]to assess the accuracy of conflict identification. The metrics include Precision and Recall, calculated as follows:

In these calculations, “Correctly Detected Conflicts”, also known as True Positives (TP), represent the instances where the algorithm correctly identifies conflicting requirements as conflict. “Overall Detected Conflicts”, also known as False Positives (FP) refer to the total number of instances where the algorithm identifies a requirement as potentially conflict, regardless of whether this identification is conflict or not. “Overall Known Conflicts” is the number of conflicts labeled by experts as true conflicts.

For entity extraction, we utilise the transformer model checkpoint trained inMalik et al. [41]. We directly employ the best modelBert-base-casedto our software requirements to calculate the software-specific entities.

SECTION: 4Results

In this section, we first assess the performance of the supervised conflict detection algorithm (S3CDA), and present the results from our comparative analysis with various sentence embeddings for each of the requirement datasets.
Next, we present the performance of our unsupervised conflict detection algorithm.

SECTION: 4.1S3CDA Performance Evaluation

In our proposed approach S3CDA, Phase I, the step that comes after calculating the similarity matrix between the requirements is the generation of ROC curves, which are used to obtain the cosine similarity cut-off for conflict detection based on TPR and FPR values. Figure4shows the ROC curves for the UAV dataset with USE embedding in each fold. We generate similar ROC curves for all the other requirement sets with best sentence embeddings which are provided in the appendix (see Section7).

Table8presents a comparison of various sentence embeddings in Phase I of the S3CDA approach. The performance across different embeddings exhibits no discernible pattern, with TFIDF performing consistently at an average level. Notably, for PURE and IBM-UAV datasets, SBERT achieves F1-scores of 89.6% and 71.1%, respectively. In the case of UAV, the USE embedding attains the highest F1-score of 92.3%, while for WorldVista, SBERT–TFIDF achieves an F1-score of 87.1%. Surprisingly, the OpenCoss dataset demonstrates the highest F1-score (57.0%) with TFIDF embeddings. This anomaly may be attributed to the challenging nature of the OpenCoss dataset, characterized by a complex word structure and a substantial overlap in vocabulary among requirements. This overlap proves advantageous for frequency-based TFIDF embeddings in capturing relevant patterns.

In the subsequent step of the S3CDA approach, we validate the potential conflicts identified in Phase I. This validation involves the application of entity extraction techniques, specifically POS tagging and S-NER, to the potential conflicts associated with the best embedding identified in Table8. Notably, Phase II of the S3CDA approach is employed to validate conflicts through semantic analysis, focusing on the overlapping entities within potential conflicts.

Table9summarizes the results of this validation. Notably, the S-NER method demonstrates relatively consistent performance and effectively validates Correctly Detected Conflicts. Across both techniques, there is a trend of improved Precision scores accompanied by a decrease in Recall, resulting in an overall decline in F1-scores. Additionally, the use of a hard threshold () leads to a reduction in Recall scores, while a more flexible threshold () shows consistent or equivalent performance to Phase I for various metrics.

SECTION: 4.2UnSupCDA Performance Evaluation

Table10outlines the assessment outcomes for the UnSupCDA approach. Overall, the approach demonstrates higher Recall values across all datasets, indicating its proficiency in capturing conflicts. However, Precision struggles, leading to lower F1-scores. This is attributed to the algorithm’s ability to identify conflicts, but it also includes false candidates, adversely impacting Precision.

As anticipated, employing a hard threshold () results in consistently low Precision for allvalues. Conversely, a stable threshold () shows improved Precision and Recall compared to. A soft threshold () leads to the selection of numerous false conflicts, diminishing Precision. This pattern is consistent across allvalues.

Contrary to expectations, increasingfromtodoes not yield enhanced performance scores. The assumption was that a higherwould improve scores, considering it represents the number of similar requirements considered for conflict assessment. However, the analysis reveals comparable performance across different thresholds for each. For instance, in WorldVista, withand, Recall remains nearly 100%, while F1-scores decrease (66.7%, 65.4%, and 65.0%, respectively). Similar trends are observed in UAV and PURE datasets, where Recall is consistently 100%, but F1-scores decline with increasing. Notably, OpenCoss displays the lowest F1-score and Precision, maintaining 100% Recall, resembling the performance observed in the S3CDA approach.

SECTION: 4.3Discussion

Table11outlines key comparisons between the proposed techniques. S3CDA exhibits superior overall performance, especially evident in conflict F1-score assessment. On the other hand, UnSupCDA excels in capturing true conflicts without the need for labeled training data, showcasing higher Recall. The two techniques each have distinct advantages and are applicable in different scenarios. Notably, UnSupCDA boasts versatility, making it easily applicable to SRS documents across various domains. In contrast, S3CDA’s performance is influenced by the specific characteristics, structure, and word usage within the requirements.

SECTION: 5Threats to Validity

Several potential threats to the validity of our proposed conflict detection approaches in software requirements merit consideration. Firstly, the reliance on manual labeling and the introduction of synthetic conflicts may impact the generalizability of our findings. The effectiveness of our method could be influenced by the specifics introduced during manual labeling and the nature of synthetic conflicts, potentially limiting the broader applicability of the model. Additionally, the accuracy of the software-specific NER model poses a validity threat, as its performance directly influences the identification of entities within software requirements. Any inaccuracies in entity extraction may lead to false positives or negatives in conflict detection.

Furthermore, the validity of our approach hinges on the efficacy of the sentence embedding models in accurately embedding software requirements. Variations in the structure, vocabulary, or complexity of requirements could impact the algorithm ability to discern meaningful similarities, affecting the overall success of conflict detection.

SECTION: 6Conclusion

This study seeks to identify conflicts within software engineering requirements, recognizing their potential to significantly impede project success by causing delays throughout the entire development process. Prior research exhibits limitations in terms of generalizability, comprehensive requirement datasets, and clearly defined NLP-based automated methodologies. We develop a two-phase supervised (S3CDA) process for automatic conflict detection from SRS documents and an unsupervised variant (UnSupCDA) which works directly on natural language-based requirements.

Our experimental design aims to evaluate the performance of the S3CDA and UnSupCDA methodologies using five distinct SRS documents. The results demonstrate the effective conflict detection capabilities of both approaches across four datasets, with OpenCoss being an exception due to elevated structural similarities among requirements. S3CDA exhibits balanced F1-scores, while UnSupCDA attains 100% Recall across all datasets, albeit encountering challenges in Precision scores.

In future, we plan to extend this study by introducing more diverse SRS documents to further validate our proposed approach. We also concur that NLP domain is highly dynamic and new methods (e.g., embeddings) are developed at a fast pace.
In this regard, we aim to extend our analysis by using other transformer-based sentence embeddings. Similarly, transformer-based NER models can be explored to improve the entity extraction performance.

SECTION: Statements and Declarations

No potential conflict of interest was reported by the authors.

SECTION: Data Availability Statement

Full research data can be accessible through following link.https://gitfront.io/r/user-9946871/tiYp38xXphd7/Paper-1-Req-Conflict/

SECTION: References

SECTION: 7Appendix

SECTION: 7.1Overlapping Entity Ratio v/s Cosine Similarity

This section presents additional scatter plots related to Subsection3.3.1.

SECTION: 7.2ROC Curves for Threshold Detection

Figure8,9,10, and11shows the ROC curves obtained from the 3-fold cross validation over all the requirement sets. These curves facilitate the process of finding the cosine similarity threshold in Phase I.