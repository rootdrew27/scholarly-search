SECTION: PartFormer: Awakening Latent Diverse Representation from Vision Transformer for Object Re-Identification
Extracting robust feature representation is critical for object re-identification to accurately identify objects across non-overlapping cameras. Although having a strong representation ability, the Vision Transformer (ViT) tends to overfit on most distinct regions of training data, limiting its generalizability and attention to holistic object features. Meanwhile, due to the structural difference between CNN and ViT, fine-grained strategies that effectively address this issue in CNN do not continue to be successful in ViT. To address this issue, by observing the latent diverse representation hidden behind the multi-head attention, we present PartFormer, an innovative adaptation of ViT designed to overcome the granularity limitations in object Re-ID tasks. The PartFormer integrates a Head Disentangling Block (HDB) that awakens the diverse representation of multi-head self-attention without the typical loss of feature richness induced by concatenation and FFN layers post-attention. To avoid the homogenization of attention heads and promote robust part-based feature learning, two head diversity constraints are imposed: attention diversity constraint and correlation diversity constraint. These constraints enable the model to exploit diverse and discriminative feature representations from different attention heads. Comprehensive experiments on various object Re-ID benchmarks demonstrate the superiority of the PartFormer. Specifically, our framework significantly outperforms state-of-the-art by 2.4% mAP scores on the most challenging MSMT17 dataset.

SECTION: Introduction
Object re-identification (Re-ID) which involves retrieving a specific object across a distributed set of non-overlapping cameras, has attracted significant interest from academia to industry due to its crucial role in surveillance systems. As deep learning took over, particularly after the inception of part-based models, this area rapidly blossomed. Various variations of the part-based model demonstrate that employing part-level features for object image description offers fine-grained information and is greatly beneficial for object Re-ID in CNN-based methods.

Looking back at recent years, despite ongoing improvements in the CNN-based part-based models, the revolution of Vision Transformer swept across the landscape of computer vision. The global receptive capabilities of multi-head self-attention in ViT have demonstrated a robust capacity for extracting distinguishing features, and advancing areas such as classification, detection, segmentation, and Re-ID. The impressive power of ViT easily pushes the boundary of the object Re-ID topic. Therefore, it is intuitive to insert the part-based descriptions in a ViT model to improve its ability in object re-identification. Influenced by the success of CNN-based part models, recent works still attempt to achieve part-level feature representation by adding extra fine-grained modulesor segmenting the image tokensin ViT architecture. However, as shown at the top of Fig., an interesting observation of ViT in the Re-ID task is that during the multi-head self-attention layer, different heads can display a diverse focus on the different human parts. However, these diverse attentions are not effectively translated into the final representation. As shown at the top of Fig., although the multi-head attention may show the attention on the diverse parts, many valuable part representations are lost after the combination of concatenation, linear projection, and FFN layer. This combination acts like a gate to pick the most discriminative features from fusion features, while fine-grained features with less discriminative will be neglected.

This observation led us to realize that the ViT architecture inherently has diverse fine-grained representations.Hence, from this perspective, we thoroughly analyzed the latent diversity representation in the ViT structure, leading to the discovery of the negative impact of the combination of concatenation, linear projection, and FFN layer in the multi-head self-attention module on these representations.
Thus, based on the analyses, we propose the PartFormer, a novel part-based vision transformer to awaken the latent diverse representation hidden in the multi-heads. The PartFormer integrates a Head Disentangling Block (HDB), which preserves the inherent diverse attention hidden in the multi-heads but discards the concatenation layer, retaining broader usable fine-grained features for Re-ID tasks. Though the diversity inherited from the pertained model makes sense in training, without an explicit constraint, inevitable degradation can easily push different heads’ representations to become more similar to each other. Thus, we incorporate head diversity constraints to guide the learning of different heads. These constraints are twofold, including an attention diversity constraint and a correlation diversity constraint. The former attention diversity constraint is widely used in previous worksto encourage distinct heads to focus on different regions of the same input image, optimizing the dissimilarity between the attention results they extract. The correlation diversity constraint considers the representation diverse from the dataset level, which aims to push the final similarity for a person in different heads to show different correlation distributions with other people’s. More specifically, the correlation diversity constraint encourages two samples that are similar in one head to be represented differently in other heads. As shown at the bottom of Fig., by combining the Head Disentangling Block and Head Diversity Constraints, the proposed PartFormer can leverage part-based fine-grained representation to achieve a more efficient object Re-ID.

The main contributions of this paper can be summarized as follows:

We investigate the limitation of the vanilla ViT in fine-grained representation and further introduce a novel Partformer network that combines the part-based conception within a vision transformer framework to overcome the granularity limitations in the object Re-ID.

We unveil the latent diverse representation within the multi-head attention mechanism and awaken it by proposing a Head Disentangling Block alongside Head Diversity Constraints to maximize the diversity of heads’ representation in the PartFormer.

Extensive experiments on public object re-identification datasets, such as MSMT17, Market1501, DukeMTMC, Occluded-Duke, VeRi-776, and VehicleID, validate the superior performance of our proposed PartFormer.

The remainder of this paper is organized as follows:
Sectionpresents a brief literature review of the related work.
The proposed adversarial Partformer model for object re-identification is described in detail in Section.
The configurations and results of experiments are presented in Section.
Finally, the conclusion of this paper is summarized in Section.

SECTION: Related Works
The studies of object ReID have been mainly focused on two typical topics: person ReID and vehicle ReID. For both re-identification tasks, the methods can be roughly divided into the global-based method or part-based method.

addresses the problem of matching pedestrian images across disjoint cameras.
Zhenget al.integrate discriminative and generative learning in a single unified network for person re-identification.
Yeet al.propose the AGW, which combines the non-local attention block, generalized-mean pooling, and weighted regularization triplet together in a unified framework.
Tanet al.proposes a dynamic prototype mask to align the occluded person in a learnable way.
Zhanget al.finds the weakness of vit in capturing high-frequency components and proposes patch-wise high-frequency augmentation to extract discriminative person representations.
Besides using the global feature representation, employing part-based features to offer fine-grained information is also a mainstream strategy that has been verified as beneficial for person ReID.
Methods like PCB, MGN, and Pyramidhorizontally divide the input images or feature maps into several parts to conduct a fine-grained representation. Several works also attempted to spread the part model in the ViT. TransReIDfirstly takes advantage of the ViT structure and proposes a jigsaw patches module to obtain perturbation-invariant representation. By employing the key point results provided by the extra pose estimation network, PFDproposes a pose-guided feature disentangling method for occluded person re-identification. Although several works also highlight the importance of fine-grained representation in vision transformers, they either highly rely on the prior segmentation results provided by pre-trained networks or lack efficient design to obtain rich and diverse fine-grained representation.

aims to locate and recognize a vehicle of interest across multiple non-overlapping cameras in various traffic intersections.
Combining the global and local features is also a widely used strategy to obtain robust and discriminative representations. PVENintroduced a parsing-based view-aware embedding network to achieve the view-aware feature alignment. Heet al.develop a novel framework to integrate part constraints with the global Re-ID modules by introducing a detection branch. PCR-Netbuilds a part-neighboring graph to explicitly model the correlation among parts, which can discover the most effective local features of varied viewpoints. GiTcouples graph networks with vision transformers, bringing effective cooperation between local and global features. Benefiting from the detailed parsing and keypoint annotations on the vehicle re-identification datasets, the vehicle re-identification method can easily obtain a precise perception of different parts of a vehicle. Since PartFormer follows the setting of TransReIDwhich only contains the identity label and viewpoint label, it may cause an unfair comparison between PartFormer with those methods that use more comprehensive pixel-level annotations. Nonetheless, PartFormer still shows a strong ability in this topic with a more limited training setting.

SECTION: Method
SECTION: Overall Framework
Based on the observations made above, we propose PartFormer, an efficient framework for object Re-ID. The overview of our proposed PartFormer framework is illustrated in Fig.. The PartFormer adopts a pre-trained vanilla ViTas its basic backbone network. Herein, we denote the input image aswith the resolution as. We first split the image intopatches with the size. Specifically, it can be described as:

where therefer to the step size of the sliding window. After the linear projection, a learnable class tokenis attached to aggregate the information from image patches. Following the TransReID, a learnable position embeddingand side information embeddingare added to preserve positional information and side information respectively, which can be formulated as:

whereis the input of the transformer blocks, andrefers to thepatch feature after patchify operation. The hyper-parameteris used to balance the weight of side information embedding. Following the setting of TransReID, we set thein the whole experiment.

To encode the inputinto a latent representation space, we carry out a vision transformer backbone consisting ofblocks. During this process, we replaced the final transformer block in the Vision Transformer with the Head Disentangling Block (HDB) to encourage the entire network to provide fine-grained feature representations. To keep the diversity of different heads in the HDB, head diversity constraints which include an attention diversity constraint and a correlation diversity constraint, are introduced as well.

SECTION: Head Disentangling Block
Before introducing the structure of the Head Disentangling Block, it is necessary to look back to the observations shown at the top of Fig.. Although the multi-head self-attention layer (MHSA) has already brought a level of diversity to the feature representations, this kind of diversity is not well transmitted to the final representation, which limits the performance of ViT in the object Re-ID.

To explore the problem behind it, we start with the basic structure of a transformer block, which consists of a multi-head self-attention layer and a feed-forward network (FFN). In particular, a single-head attention for thehead in theblock is computed as below:

where,, andare query, key, and value matrices, while,,are the parameter matrices in theattention head of thetransformer block, respectively.refers to the softmax function,is a scaling factor, andindicates the input for theblock.
To effectively aggregate attention results on different head representations, the vanilla ViT directly concatenates the outputs from different heads and projects them with a parameter matrix as follows:

where therefers to the weight of linear projection in theattention head of thetransformer block, andis the number of heads.
With the FFN and residual connections, the final output of a transformer block is:

Based on Eq. (), if we consider the equation from the head’s side, it can also be written as:

where theis submatrix divided from the.

From Eq. (), we can observe that besides the attention results, which are shown in Fig., the aggregation output of the MHSA layer is also greatly influenced by two parts: value matricesand the linear projection. If we consider the different weight matrixas a kind of gate layer to aggregate the multi-head representations, it is obvious that if one of theweakens during training, even if its attention shows the information, the representation will still be highly ignored in the final representation. Meanwhile, to make matters worse, thewill be neglected and will receive insufficient training. Besides, the situation is the same if we continue to divide the following FFN layer. Such a pipeline ensures that the most discriminative features can be comfortably trained and transferred, while processing is much more difficult for those fine-grained features with less discriminative. A powerful argument for this observation comes from the adaViT, which indicates that many heads in a ViT can be sparse with limited degradation.

Therefore, based on the above observations, we attempt to awaken the latent diverse representations that are hidden behind MHSA and further propose a Head Disentangling Block (HDB) to replace the last transformer block in the vision transformer. As shown in Fig., in the HDB, we break the addition operation in Eq. (), and force each head to represent a part feature. Thus, in the output of MHSA, the representations of all heads can be considered equally and push a more complete feature representation. However, if we further consider Eq. () and Eq. (), there are still several nuisances. The first one is the residual connection, which also introduces an additional operation in Eq. (). Since all the heads share the same input, this operation may prevent the multi-head from exhibiting diversity. Another is the attention between the class token itself, which may also prevent the class token in the HDB from extracting the features of image tokens. Therefore, in the HDB, we removed these two parts during the computation of part features. Finally, the output of the part featurecan be given as:

whereis the attention matrix that only considers the results between the class token and image tokens.

Furthermore, since the part features do not need to perform feature selection from aggregated multi-head representations like the original type of MHSA, it is interesting to find that we can even remove the subsequent FFN layers without performance degradation, which largely increases the efficiency of the HDB.

SECTION: Head Diversity Constraints
Although all heads in the HDB have some diversity due to their different initial weights inherited from the pertained ViT, without explicit constraints, those representations will be inevitably homogenizing due to the same input and training target, resulting in a suboptimal solution. To tackle this issue, in PartFormer, we incorporate two constraints, including an attention diversity constraint and a correlation diversity constraint, to maintain the diversity of different heads.

The attention diversity constraint aims to push the different heads to focus on different discriminative foregrounds. Generally, in the HDB, we consider that the feature of each head comes from the image feature aggregation guided by the attention matrix. Therefore, to encourage each head to focus on different fine-grained patterns, the attention diversity constraint is shown as follows:

whereis the attention matrix of different heads in the HDB, whileis the target identity matrix. Therefers to the 1-norm distance between the attention matrix and target matrix.

Although attention diversity constraint works well in discovering different fine-grained regions, it struggles to achieve discriminative diversity. Since the position embedding is learnable, image features are also influenced by their position. Though heads are extracted from different regions, they may focus on patterns with high correlation, such as the left arm and right arm with similar discriminative. Obviously, it may not be necessary to use multi-heads to represent high correlation patterns. Thus, to address the issue, a correlation diversity loss is presented. Generally, the weight matrix of a classifier can be considered a prototype matrix, where each row represents a specific class. Therefore, the output of the classifier can be regarded as the similarity between the input image and different classes. If we exclude the input sample and its corresponding prototype, we can obtain a similarity distribution between the feature and other class features. Apparently, if two heads extract content with a high correlation, they will also show a high similarity in the similarity distribution.

Therefore, as shown in Fig.(c), the correlation diversity loss employs the similarity distribution for each sample and pushes them away from each other during training. More specifically, for the distributionof featuresobtained through its corresponding classifiers, the correlation diversity loss can be shown as:

Here,refers to the true label of the inputwhileindicates the operation that excludes the similarity between theand its corresponding prototype in the distribution.

SECTION: Training and Inference
Apart from the above-mentioned head diversity constraints, the widely known identity-based softmax cross-entropy lossand triplet losswith soft-margin are also employed in the training of PartFormer. Thus, the objective function of PartFormer can be formulated as:

where theandrefer to the final output of global class token and part tokens,andare hyper-parameters to balance the weight of attention diversity lossand correlation diversity loss, respectively. During the testing stage, we add the global class tokenwith all the part tokensas:

SECTION: Experiments
SECTION: Datasets and Experimental Setting
To evaluate the effectiveness of the proposed PartFormer, we conduct extensive experiments on six publicly available object Re-ID benchmarks, which include four person Re-ID and two vehicle Re-ID datasets. The details of these datasets are as follows.is a widely-used Re-ID dataset captured from 6 cameras. It includes 12,936 images of 751 persons as the training set, 3,368 images of 750 persons as the query, and 19,732 images of 750 persons as the gallery.contains 36,441 images of 1,812 persons captured by eight cameras, in which 16,522 images of 702 identities are used as the training set, 2,228 and 16,522 images of 702 persons that do not appear in the training set are used as the query and gallery, respectively.is a dataset collected from the DukeMTMC for occluded person Re-ID. The training set consists of 15,618 images of 702 persons. The testing set contains 2,210 images of 519 persons as the query and 17,661 images of 1,110 persons as the gallery.consists of 49,357 images of 776 distinct vehicles captured by 20 cameras in different orientations and lighting conditions. Among them, 576 identities (37,778 images) and 200 identities (11,579 images) are selected for training and testing respectively. Furthermore, 1,678 images from 200 identities are selected as the query from the testing set.is composed of 221,567 images from 26,328 unique vehicles. Half of the identities are used for training while the other half is for testing. There are 6 testing split strategies with various gallery sizes. Following the TransReID, we adopt the gallery size as 800.

To verify fair comparison with other methods, we adopt the widely used Cumulative Matching Characteristic (CMC) and mean Average Precision (mAP) as evaluation metrics.

We employ the ViTpre-trained on ImageNetas the backbone network and follow the training details with TransReIDincluding the learning rate, decays at a cosine learning rate, batch size, etc.
All the person images are resized toand all vehicle images are resized to. Commonly used horizontal flipping, padding, random cropping, and random erasingare employed as data augmentation. We implement our PartFormer with PyTorch and conduct all experiments on the Nvidia A100.

SECTION: Comparison with State-of-the-art Methods
To comprehensively evaluate the performance of PartFormer, we compared it against previously reported state-of-the-art methods on both person and vehicle Re-ID datasets, as shown in Table. Given that TransReID employs a sliding window strategy in its patchify process, which is broadly adopted in later works, we present results under both settings. On person Re-ID tasks, PartFormer achieves a marked improvement over the previous methods in handling both holistic (MSMT17, Market1501, and DukeMTMC) and occluded scenarios (Occluded-Duke), as evidenced in the Rank-1 andmAP metrics. Specifically, compared to TransReID and PFD, which similarly explore fine-grained representations within ViT architectures, PartFormer surpasses them with at least aandmargin in Rank-1 andmAP metrics on the largest person Re-ID dataset, MSMT17. This success extends to vehicle Re-ID, where PartFormer achieves impressive results on both the VeRi-776 and VehicleID datasets. Compared to DCAL, GiT, and TransReID, which also utilize a pure Vision Transformer architecture, PartFormer exhibits an improvement of at leastin Rank-1 andinmAP on the VeRi-776 dataset, andandin Rank-1 and Rank-5, respectively, on VehicleID.

SECTION: Ablation Study
To evaluate the impact of the different components, we conducted a series of experiments on MSMT17 and VeRi-776 using varying configurations, with the quantitative results presented in Table. In PartFormer, we reduced the number of heads in the HDB from 12 to 6. Consequently, we provide comparisons with the original baseline configuration, where the number of heads is set to 12.

Replacing the final transformer block with the HDB resulted in improvements of 3.3% inmAP and 1.4% in Rank-1 on the MSMT17, as well as gains of 1.1% inmAP and 0.2% in Rank-1 on the VeRi-776. These performance gains validate the effectiveness of the HDB in exploiting the latent diversity within the MHSA mechanism. Performance is further enhanced when combining the attention diversity constraint (ADC), as the ADC imposes an explicit constraint on the focus regions of different heads. Furthermore, incorporating the ADC into the baseline model proves beneficial, indicating that maintaining diversity among heads is critical in the Vision Transformer, even with head selective processing. Additionally, the correlation diversity constraint (CDC), designed to augment content diversity across different heads, also significantly enhances PartFormer’s performance. Deploying the CDC yields arise in Rank-1 and auplift inmAP on the MSMT17, alongside aincrease in Rank-1 and aadvance inmAP on VeRi-776. Consequently, each component contributes consistently to the effectiveness of the framework, culminating in PartFormer’s impressive performance.

SECTION: Discriminative Evaluations
PartFormer contains several hyperparameters, such as theandin Eq. (), as well as the number of headsin the HDB. Therefore, in this section, we first analyze the influence of theandon the performance of PartFormer in Fig.. On the MSMT17 dataset, we observe a linear improvement in performance whenis less than. Beyond this point, further increasing thewill decrease performance. For the VeRi dataset, while Rank-1 exhibits fluctuations, optimal performance is still achieved whenis set to. Regarding, performance consistently improves whenis less than. Beyond this, comparable peak performance is observed at bothand. We finally set thetoin further experiments due to its slight advantage in VeRi-776. Additionally, we investigate the influence of the number of heads, within the HDB and present the results in Table. Due to the limitation of the feature dimension,can only be set to several specific values. The data indicate that whenis set to 6 yields the best results, with any deviation from this number leading to reduced performance across both datasets.

Adopting a uniform partition or prototype-based feature factorization to divide the person image has been an influential method for achieving fine-grained representations within CNN models. Therefore, we compare the head-based soft partition strategy of PartFormer with two distinct hard partition strategies (Horizontal Partition and Patch Merging Partition) and a prototype-based feature factorization (PAT). As detailed in Table, for a fair comparison, image patches are divided intoparts within the final transformer block to compute their feature representations independently. Due to the different designs between CNN and ViT, which have different computation strategies to obtain the final feature map, these partition strategies work well in CNN but are not so efficient in ViT, as observed in the MSMT17. This suggests that directly applying fine-grained strategies that are effective in CNNs may not be suitable for ViTs.
Additionally, drawing inspiration from the Patch Merging feature of Swin-Transformers, we implemented a Patch Merging Partition that utilizes the Patch Merging operation to partition image patches intosegments. While the Patch Merging Partition is viable for both person Re-ID datasets, it still falls significantly short of PartFormer’s performance.

To evaluate the efficiency of PartFormer, we compare it with two part-based ViT modelsin terms of training time and GPU consumption. A vanilla ViT (ViT-Base) is considered the baseline model. Compared to the vanilla ViT, due to the addition of part branches, the computation cost increased in PartFormer. However, compared to other part-based ViT models, PartFormer shows a strong advantage when considering both performance and training consumption.

In PartFormer, we made several modifications to the HDB to gain better performance. To evaluate the influence of these modifications, we conducted several experiments and showed the results in Table.. To limit the influence from the previous blocks, we remove the residual connect in the HDB, which shows an improvement in nearly all the settings. Additionally, without the necessity to further select the feature from multi-heads, we remove the FFN in the HDB. This not only reduced the computation of PartFormer but also yielded performance gains. Finally, we attempt to remove the class token in the HDB and train the PartFormer with the part tokens. Unfortunately, the class token proved to be important in guiding the training of heads. Its removal led to a significant decline in performance.

In Fig., we visualized the attention results of different part tokens. From the results, we can observe that different part tokens can successfully capture diverse representations from different human parts for the same input image. Specifically, Part 1 mainly focuses on the contour of the person, Part 2 mainly focuses on the head, Part 3 mainly focuses on the hands and shoes, Part 4 mainly focuses on the center part of the person, and Part 5 mainly focuses on the shoulder. There may be some differences in Part 6, which mainly focuses on the corner of the image. Due to the similarity-based computation strategy in ViT, the corner without person content will show uniform-like attention results. Therefore, we can consider the representation of Part 6 as an average pooling of all the image patches.

SECTION: Limitations
The primary limitation of Partformer is that although Partformer aims to awaken the diversity representation within the ViT. Owing to structural constraints, we are limited to substituting the final block of the ViT with HDB. Incorporating additional HDB layers would lead to an exponential growth in the count of tokens, thereby rendering the computationally infeasible. This condition largely limited the effectiveness of PartFormer. Nonetheless, we still believe that the ability of Partformer still has a large room to improve, which will be left in our future work.

SECTION: Conclusion
In this paper, we propose a novel part-based vision transformer, termed PartFormer, for object Re-ID. PartFormer aims to explore a part-based, fine-grained representation within a vision transformer. By observing the latent diverse representation behind the multi-head attention, we introduce a Head Disentangling Block to awaken the latent diverse representation. Moreover, to prevent heads’ representation from inevitably homogenizing, we incorporate two diversity constraints, including an attention diversity constraint and a correlation diversity constraint to explicitly encourage different heads to show a diverse representation. Extensive experiments across various object Re-ID benchmarks have proved the necessity of diverse part-based representation in PartFormer and demonstrated its state-of-the-art performance.

SECTION: Acknowledgement
This work was supported by National Science and Technology Major Project (No. 2022ZD0118202), the National Science Fund for Distinguished Young Scholars (No.62025603), the National Natural Science Foundation of China (No. U21B2037, No. U22B2051, No. U23A20383, No. 62176222, No. 62176223, No. 62176226, No. 62072386, No. 62072387, No. 62072389, No. 62002305 and No. 62272401), and the Natural Science Foundation of Fujian Province of China (No.2022J06001).

SECTION: References