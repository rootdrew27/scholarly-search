SECTION: Towards Exploring Fairness in Visual Transformer based Natural and GAN Image Detection Systems

Image forensics research has recently witnessed a lot of advancements towards developing computational models capable of accurately detecting natural images captured by cameras and GAN generated images. However, it is also important to ensure whether these computational models are fair enough and do not produce biased outcomes that could eventually harm certain societal groups or cause serious security threats. Exploring fairness in image forensic algorithms is an initial step towards mitigating these biases. This study explores bias in visual transformer based image forensic algorithms that classify natural and GAN images, since visual transformers are recently being widely used in image classification based tasks, including in the area of image forensics. The proposed study procures bias evaluation corpora to analyze bias in gender, racial, affective, and intersectional domains using a wide set of individual and pairwise bias evaluation measures. Since the robustness of the algorithms against image compression is an important factor to be considered in forensic tasks, this study also analyzes the impact of image compression on model bias. Hence to study the impact of image compression on model bias, a two-phase evaluation setting is followed, where the experiments are carried out in uncompressed and compressed evaluation settings. The study could identify bias existences in the visual transformer based models distinguishing natural and GAN images, and also observes that image compression impacts model biases, predominantly amplifying the presence of biases in class GAN predictions.

SECTION: IIntroduction

Fairness studies are recently gaining large interest in the research community since the machine learning based computational models are reported to have biases in their outputs[1]. These biases in the models can impact society by harming or denying opportunities to certain social groups of people[2]. Fairness studies report that these algorithmic biases can originate from training data, model representations, downstream tasks, etc., and accordingly, there are different kinds of algorithmic biases including data bias, model learning bias, downstream task level bias, etc.,[3].

Easy availability of image-acquiring devices, massive publicly accessible image datasets, rapid progress, and a wide variety of generative algorithms and user-friendly easily available apps generating high-quality super realistic images have drastically increased the production of GAN images all around. Beyond artistic and entertainment purposes, such GAN images are also seen to create some critical and harmful societal issues, such as evidence for supporting fake news, defamation, generating fake nude photographs, false light portrayals,[4,5,6], etc. Hence a lot of studies are reported proposing various methods for distinguishing GAN images from natural images captured by a camera, which can help to understand or even to serve as evidence to prove image authenticity[7,8,9,10]. Although there is a lot of research in this area of distinguishing natural and GAN images, there are only a very few studies that explore algorithmic bias in such image forensics systems[11,12]. Exploring bias in image forensics systems is very significant because unfair forensic systems can lead images of certain social groups to be more likely predicted as GAN images even if they are actually natural images. Unfair models may also lead images of certain social groups to be more likely predicted as natural images even if they are actually GAN generated images, creating security concerns. Therefore it is essential to test the fairness of image forensics systems.

Motivated by the transformer based networks that were initially designed dedicatedly for natural language based tasks, vision transformers were developed that handle images as sequences of patches[13]. Recently, visual transformer based models have drawn considerable attention due to their impressive performances for a variety of downstream tasks, for example, transformer based models for image classification (e.g. ViT[13]), object detection (e.g. DEtection TRansformer (DETR)[14], RT-DETR[15], YOLOS[16], ViT-YOLO[17]), segmentation (e.g. SegFormer[18], Segmenter[19], image generation (e.g. Transgan[20]), etc.,[21,22].
The area of image forensics also reports many works in the literature, utilizing these visual transformers[23,24,25,26,10]. Due to the recent widespread use of visual transformers in image forensics, this study tries to explore bias, if any, in visual transformers for the forensic task of distinguishing natural (or real) and GAN generated images.

.

Images shared through social media websites, unlike other post-processing operations, almost always go through compression knowingly or unknowingly[27]. Also, to deceive the forensic models detecting GAN images and to spread fake news, these images are usually compressed and propagated through social media[28,4]. Therefore, in the image forensic task of detecting natural and GAN generated images, the robustness of forensic algorithms towards post-processing operations, particularly image compression, is a very important factor to be considered. Hence, studies in the literature that build high performance GAN image detector systems also analyze the robustness of those models[10]. Most studies report a high accuracy drop for the models, in compressed scenarios[9,10]. In this regard, one of the interests of this study, apart from identifying bias in visual transformers based classification of natural and GAN images, is to explore whether image compression impacts model bias. That is, this study focuses on two research objectives: (1) Do visual transformers produce algorithmic bias or unfairness in their predictions when utilized for the task of distinguishing natural and GAN generated images and, (2) does image compression impact or amplify algorithmic biases in these model predictions? For example, this study intends to explore whether any social groups (such as male or female, dark skin or light skin) are more likely predicted as natural images or as GAN generated images; and how image compression impacts the algorithmic biases (if any) towards these social groups. To study these objectives, this work conducts bias analysis experiments in two evaluation settings, one in the original uncompressed evaluation setting and the other in the compressed setting, using the same set of evaluation measures. This helps to understand and identify any bias in the transformer based models and also to analyze whether model bias is impacted by image compression. Figure1shows the entire architecture of the proposed work, with an example set of input images and prediction scenarios to better understand the workflow and how this study conducts the bias exploration. This example only depicts the case of analyzing bias in GAN images111The GAN images in this example are collected from StyleGAN2[29]generated images, but the study considers analysis over both natural and GAN classes of images.

The major contributions of the proposed work are:

This work explores algorithmic fairness in image forensic systems employing popular visual transformers, viz. ViT, CvT, and Swin for the task of distinguishing natural and GAN generated images.

The work analyzes algorithmic fairness in four different domains such as gender, racial, affective, and intersectional domains by procuring a bias evaluation corpora.

The work conducts extensive bias evaluation experiments using sets of individual and pairwise evaluation measures over the predictions of the forensic classifier systems, in each of the domains.

The work also tries to understand the impact of image compression on biases in forensic classifier systems by analyzing and comparing the existence of model biases, across uncompressed and compressed evaluation settings.

The rest of the paper is organized as sectionIIpresents a brief survey on the works in literature that specifically analyze bias in image forensic tasks classifying natural and GAN generated images and explains the differences of the proposed study in the context of the works in the literature. SectionIIIdiscusses in detail the construction of transformer based models for the task of classifying natural and GAN images. SectionIVexplains in detail the evaluation domains, evaluation corpora, and evaluation measures used for bias analysis experiments. SectionVpresents the results and discussions of both the uncompressed and compressed evaluation settings and finally sectionVIIpresents the conclusions and future directions of the work.

SECTION: IIRelated work and Our work in context

The boom of big data and expeditious progress in deep learning based models helped to bring up solutions to many fields of computing[30,31,32,33,34,35]. However, these data-greedy models are found to produce biased outcomes[1,2]. Many works are seen to be reported in the literature studying fairness in image based research problems, such as in the areas of face recognition[36], image classification[37], medical image processing[38], etc. But comparably only a very few studies explore bias in forensics systems, and amongst those studies, most of them work on videos, i.e., Deep Fake videos.
Trinh and Liu[39]explore bias in three deep fake detection models Xception[40], MesoInception-4[7]and Face X-Ray[8], using gender and race balanced Deep Fake face datasets. Their study observes high racial bias in the predictions of these Deep Fake detection models. They could also observe that one of the most popularly used datasets for training the models for Deep Fake detection, FaceForensics++[41], is also highly biased towards female Caucasian social groups.
Hazirbas et al.[11]proposes a video dataset to analyze the robustness of top-winning five models of DFDC dataset[42]for the domains gender, skin type, age, and lighting. They could observe that all the models are biased against dark-skinned people and hence find that these five models are not generalizable to all groups of people.
Pu et al.[12]explores gender bias in one of the Deep Fake detection models MesoInception-4, in the presence of certain make-up anomalies, using the FaceForensics dataset. Their study is centered on analyzing these models at various prominence levels of the anomaly in the female and male social groups. Their observations are that the model is biased towards both genders, but mostly towards the female group.
Xu et al.[43]explores bias in three Deep Fake detection models EfficientNetB0[44], Xception[40], and Capsule-Forensics-v2[45], by conducting evaluations on five Deep Fake datasets which are annotated with 47 attributes including non-demographic and demographic attributes. Their observations state that these models are highly unfair towards many of these attributes.

SECTION: II-AProposed work in the context of the literature

In the context of the previous works in the literature that analyze bias in image forensic algorithms classifying natural and GAN generated images[39,11,12,43], the proposed work is the first work, to the best knowledge, that explores bias in transformer based image forensic models classifying natural and GAN generated images. Also, the proposed work is the first work, to the best knowledge, to study the role/impact of image compression in model biases. The work tries to unveil any existence of bias in gender, racial, affective, and even intersectional domains using a vast set of individual and pairwise evaluation measures, and sets aside the mitigation of these biases outside the scope of this work, for future studies.

SECTION: IIIClassification of natural and GAN generated images

This section discusses the dataset used to fine-tune the transformer based models and construction of the visual transformer based forensic classifier system for the task of classifying natural and GAN generated images, which are investigated for fairness in this study.

SECTION: III-AFine-tuning corpora

To build transformer based forensic classifier systems that classify GAN and Real images, each of the pre-trained transformer based models are fine-tuned using a GAN versus Real image dataset that consists of a total of 10,000 images; each class containing 5000 images. The GAN images are collected from the StyleGAN2 image generative algorithm[29]and the Real class of images are collected from the Flickr-Faces-HQ (FFHQ) dataset[46]; these datasets are popularly used in many related studies in the literature[47,48,9,10]. Since works exploring algorithmic biases focus on performance differences between groups within domains in which bias study is conducted[11,12,49], rather than focusing on the performance of the models, in this study the total fine-tuning corpora is split in the ratio 6:2:2 for training, validation and testing, respectively, by referring the related works[9,10].

SECTION: III-BVisual transformer based forensic classifier system

This work tries to identify bias in three popular transformer based deep learning models, viz. Vision Transformer (ViT)[13], Convolutional Vision Transformer (CvT)[50]and Swin transformer[51], for the task of classifying natural and GAN images. The ViT architecture divides the images into fixed-size patches in order. These non-overlapping patches are then linearly embedded. These embeddings along with the position embeddings of the patches and a learnable classification token are supplied to the transformer encoder block for classification task[13]. CvT architecture utilizes convolutions within the ViT architecture with an aim to improve the performance of ViT. The major difference includes using a set of transformers with convolutional token embedding, convolutional projection and convolutional transformer block[50]. Swin transformer follows hierarchical architecture based on Shifted WINdow approach[51].

The natural image versus GAN image classification is formulated as a two-class classification task that can classify images under evaluation into either of the two classes GAN or Real. The classifiers are fed with the training data(indicates ithimage in train data) and associated ground truth classes() such that to find a best fitting model. To build the classifier models, the three pre-trained transformer networks ViT, CvT, and Swin, are fine-tuned using the task-specific GAN versus Real image dataset. A diagrammatic representation of the visual transformer based natural image versus GAN image classifier model is shown in figure2.

SECTION: III-CFine-tuning experimental settings

Fine-tuning experiments of the transformers are conducted on the deep learning workstation equipped with Intel Xeon Silver 4208 CPU at 2.10 GHz, 256 GB RAM, and two GPUs of NVIDIA Quadro RTX 5000 (16GB each), using the libraries Torch (version 1.13.1+cu116), PyTorch Lightning (version 1.9.0), Transformer (version 4.17.0), Tensorflow (version 2.8.0), and Keras (version 2.8.0). TableIshows the model parameters.

SECTION: IVBias Analysis in forensic classifier systems

This study tries to identify bias (if any), in the transformer basedNatural image versus GAN generated imageclassifier systems. Fairness analysis is conducted in the gender, racial, affective, and also in intersectional domains. Gender domain based bias analysis considers the female and male social groups, the racial domain considers the dark skin people and light skin people social groups, the affective domain considers the smiling face and non-smiling face groups and intersectional bias analysis considers two domains simultaneously, such as dark skin female, light skin male, etc. Apart from analyzing bias by comparing the performances of each social group against the other using individual evaluation measures, this study also performs pairwise analysis of social groups. Bias analysis in this forensic task of classifying natural and GAN generated images using transformer based models is conducted using two categories of evaluation corpora, one consisting of the original uncompressed GAN and Real evaluation corpora and the other is the JPEG compressed version of the same evaluation corpora. That is, in the first phase of bias analysis, the transformer based models are evaluated over the uncompressed evaluation corpora using a set of evaluation measures, and in the second phase of analysis the same evaluation corpora is JPEG compressed with a quality factor of 90 and analyzed using the same evaluation measures. The details of evaluation corpora and evaluation measures are detailed below.

SECTION: IV-AEvaluation domains and evaluation corpora

This work procures an evaluation corpora for bias analysis with respect to gender, racial, and affective domains. To procure the evaluation corpora we utilize Natural images from the FFHQ dataset[46]and GAN images from the StyleGAN2[29]generated images. From both Natural and GAN generated images we collected 1000 female face images and 1000 male face images each for the gender bias analysis, 1000 dark skin and 1000 light skin face images for racial bias analysis, and 1000 smiling and 1000 non smiling face images for affective bias analysis. This also gives chances for intersectional bias analysis with 500 images each in the category of dark skin female, dark skin male, light skin female, and light skin male faces.

SECTION: IV-BEvaluation measures

Bias analysis in this study focuses on comparing the classification performance of the transformer based models over different social groups (or groups) within the same domain using certain evaluation measures. These analyses are performed to compare social groups within a single domain (e.g. Male vs. Female in the gender domain) as well as to compare social groups within intersectional domains (e.g. Dark skin Male vs. Light skin Male). Apart from the measures that evaluate individual social groups such as total accuracy, GAN and real class accuracies, false positive rate, and false negative rate, this study also utilizes pairwise evaluation measures such as average confidence score, demographic parity, and equal opportunity, to quantify bias associated with a pair of social groups in single domain or intersectional groups in a domain. The measures considering individual social groups in a domain and pairwise measures considering two social groups simultaneously are detailed below.

These measures are defined by the probability of correct and incorrect classifications in a social group within a domain. Social groups over which the individual measures are evaluated include, Female (F) and Male (M) social groups in the gender domain, Dark skin (D) and Light skin (L) social groups in the racial domain, Non-smiling (Ns) and Smiling (S) groups in the affective domain, and Dark skin Female (DF), Dark skin Male (DM), Light skin Female (LF) and Light skin Male (LM) groups in the intersectional domain.

Total accuracy[36,54]: This popular classification measure computes the total classification accuracy of a model over a social group in a domain. Total accuracy gives the percentage of images in a social group that is correctly classified into the natural image category and the GAN generated image category.

where, TP and TN are the number of true positives and true negatives, and FP and FN denote false positives and false negatives, respectively.

GAN accuracy: This measure gives the accuracy of the class GAN images, i.e., the number of GAN images correctly classified as GAN images. This measure gives the True Positive Rate (TPR)[54]of the model

Real accuracy: This measure gives the accuracy of the class of natural images, i.e., the number of natural images correctly classified as natural images. This measure is the True Negative Rate (TNR)[54]of the model.

False Positive Rate (FPR)[36,54]: For this classification task, FPR gives the ratio of Real images misclassified as GAN images, among the total number of Real images.

False Negative Rate (FNR)[54]: FNR gives the ratio of GAN images misclassified as Real images, among the total number of GAN images.

During evaluation, the results obtained for each of these individual measures across the social groups within a domain are correspondingly compared, rather than looking for ideal high classification results.

The pairwise evaluations are computed on a pair of social groupsandwithin a domain.indicates the ground truth class of ithimage in the social group(for), andindicates the ground truth class of jthimage in the group(for), whereandindicates total number of instances in the social groupsand, respectively. Also,andindicate the corresponding prediction classes, andandindicate prediction intensities (confidence scores of prediction), ofand, respectively. Pairwise measures are evaluated over the pairs, Female vs. Male (FM) in gender domain, Dark skin vs. Light skin (DL) in racial domain, Non-smiling vs. Smiling (NsS) in affective domain, and Dark Female vs. Dark Male (D+FD+M), Light Female vs. Light Male (L+FL+M), Dark Female vs. Light Female (D+FL+F), Dark Male vs. Light Male (D+ML+M), Dark Female vs. Light Male (D+FL+M) and Light Female vs. Dark Male (L+FD+M) in the intersectional domain.

Average Confidence Score (ACS)[49]: This measure is computed using the ratio between average prediction intensities of the two social groups under evaluation.

An ideal unbiased scenario gives ACS = 0 for a pair. Positive values of ACS show that the prediction intensities of the social groupare lower than, whereas negative ACS indicates that the prediction intensities of the social groupare higher than.

Demographic Parity (DP)[49,1]: This is one of the popular measures to quantify bias in a classification model, by analyzing similarity (or dissimilarity) in the classifications of the model for two social groups in a domain.

where,andare the probabilities of the groupsand, respectively, for being classified into a classwhere, in thepair,is the group with higher probability. That is, the measure DP recommends that the probability of predicting a classneeds to be similar for both the social groupsandwithin a domain. Hence, an ideal unbiased case is indicated by DP = 1 for a pair, and lower values of DP indicate higher bias. A threshold of 0.80 is commonly used for identifying lower DP values, indicating high model bias[55].

Equal Opportunity (EO)[1,55]: This measure is also similar to DP, but EO considers the ground truth in addition to the predicted classes.

where,andindicates the ground truth classof groupand. Similar to DP, an ideal unbiased case is indicated by EO = 1 for a pair, and lower values of EO indicate higher bias.

SECTION: VResults and analysis

Since this study follows a two-phase evaluation setting, where the bias evaluation experiments are carried out in both the uncompressed and compressed settings, this section initially analyses the results of bias evaluation of each of the transformer based models, ViT, CvT, and Swin over the original uncompressed evaluation corpora and later the results of bias evaluation of the models over the compressed evaluation corpora.

SECTION: V-ABais analysis in theuncompressedevaluation setting

Bias evaluation results of the ViT based model in the uncompressed setting are shown in tableII. The top portion of the table presents the results of individual measures of bias analysis of ViT and the bottom portion presents the results of pairwise measures of bias analysis, within gender, race, affective, and intersectional domains.

While looking into the results of individual measures (in the top portion of tableII), in the gender domain, the total model accuracy (Acc) of ViT over the female group (F) is less than the male group (M) by 4.45 percentage points, indicating biased prediction. This bias is observed to be very high for class Real (Accreal), i.e. accuracy of the female group is less than male by 9.3 percentage points, indicating high gender bias against the female social group. Whereas, for class GAN (Accgan), the accuracy of the male group is less than female only by a very small value of 0.4 percentage points, a negligible difference to indicate any bias. Also, in the gender domain, the measure FPR is higher for the female group than the male. This indicates Real images of females are more likely to be misclassified as GAN generated images than those of males (an observation similar to the one reported in[39]). Whereas, the very low difference in FNR values between male and female groups indicates negligible chances that GAN images of males get misclassified as Real images.

In the racial domain, the total accuracy of ViT over the light skin (L) group is less than dark skin (D) by 4.15 percentage points, indicating biased prediction against light skin people, which is much more evident in the case of class Real with a difference of 11.1 percentage points, indicating high racial bias against light skin people. Whereas, in the case of class GAN, the accuracy of the dark skin group is less than light skin by 2.8 percentage points, indicating bias against dark skin. The measure FPR shows a higher value for light skin group than dark skin, which indicates Real images of light skin people are more likely to be misclassified as GAN images, and FNR indicates slight chances for GAN images of dark skin people being misclassified as Real images.

In the affective domain, the total accuracy of ViT over the group of smiling faces (S) is less than non-smiling faces (Ns) by 2.95 percentage points. A similar pattern is shown in class Real, with a difference of 7.1 percentage points, indicating affective bias against the group with smiling faces. Whereas in the case of class GAN, the accuracy of smiling faces is higher than non-smiling faces by 1.2 percentage points. FPR shows high value for smiling faces, which indicates Real images of smiling people are more likely to be misclassified as GAN images. The slightly higher values of FNR for non-smiling faces indicate slight chances for GAN images of non-smiling faces being misclassified as Real images.

In the intersectional domain, it can be observed that the total accuracy varies across different intersectional groups. The highest total accuracy is observed for dark skin male group (D+M), and lowest for light skin female (L+F), with a difference of 8.6 percentage points, which indicates bias against light skin female group. Whereas for class GAN, an accuracy of 96.4 percent is obtained for light skin female group, which is the highest accuracy obtained across various groups among both classes and even compared to the total accuracy. The lowest accuracy in class GAN is for the dark skin female group (D+F), a difference of 6.6 percentage points compared to the highest accuracy group, indicating biased prediction. In class Real, the highest accuracy is obtained for dark skin male group and the lowest accuracy of 75.0 percent is obtained for light skin female group, which is the lowest accuracy obtained across various groups among both classes and even compared to the total accuracy. That is, both these groups have a very high difference of 20.4 percentage points, indicating very large intersectional bias against light skin female group. FPR stands highest for the light skin female group indicatingReal images of light skin females have a very high probability of being misclassified as GAN images. FNR is highest for the dark skin female group indicatingGAN images of dark skin females have a very high probability of being misclassified as Real images.

The bottom portion of the same tableIIpresents the results of pairwise measures of bias analysis of ViT for both GAN and Real classes. In the gender domain, for class GAN, the negative value of the measure ACS for the Female vs. Male pair (FM) shows that the prediction intensities of the female group are higher than males. The measure DP has a low value, but since it is not less than the threshold of 0.80 this measure does not report bias in the Female vs. Male pair. The measure EO has a high value and does not report gender bias in class GAN predictions. For the class Real, positive ACS for Female vs. Male pair shows that the prediction intensities of the male group are higher than females. The measures DP and EO have low values. But since DP is not lower than the threshold of 0.80, it does not report gender bias in class Real predictions.

In the racial domain, for class GAN, the positive ACS value for Dark skin vs. Light skin pair (DL) shows that the prediction intensities of the light skin group are higher than dark skin. The measure DP has a low value, but since it is not less than the threshold of 0.80, this measure does not report racial bias. EO has a high value and does not report racial bias in the class GAN predictions. For the class Real, negative ACS for the pair shows that the prediction intensities of the dark skin group are higher than light skin. The measures DP and EO have low values, where DP is not lower than the threshold of 0.80, and hence do not report racial bias in the class Real predictions.

In the affective domain, for class GAN, the positive ACS value for the Non-smiling vs. Smiling pair (NsS) shows that the prediction intensities of the smiling group are higher than the non-smiling group. The measure DP has a low value, but since it is not less than the threshold of 0.80, this measure does not report bias in this pair. EO has a high value and does not report bias in the class GAN predictions of this pair. For the class Real, negative ACS for the pair shows that the prediction intensities of the non-smiling group are higher than the smiling group. The measures DP and EO have low values. But since DP is not lower than the threshold of 0.80, this measure do not report bias in Real predictions of this pair.

In the intersectional domain, for the class GAN, the measure DP is very low for the pairs involving light skin female group, i.e., {Light skin Female vs. Light skin Male} (L+FL+M), {Dark skin Female vs. Light skin Female} (D+FL+F) and {Light skin Female vs. Dark skin Male} (L+FD+M). Particularly for the pairs {Dark skin Female vs. Light skin Female} and {Light skin Female vs. Dark skin Male}, the measure DP is less than the threshold of 0.80, which indicates the existence of intersectional bias. Similarly in class Real also, pairs involving the light skin female group show bias with very low values for DP and even EO. That is, pairwise bias analysis measures could unveil the existence of bias in the {Light skin Female vs. Light skin Male}, {Dark skin Female vs. Light skin Female} and {Light skin Female vs. Dark skin Male} intersectional pairs.
The prediction intensity (confidence) plots of the images of intersectional pairs in the bias evaluation corpora that show unbiased and biased results are given in figure3and4, respectively. The horizontal axes of the plots indicate images in the bias evaluation corpora (that contains a total of 500 GAN/Real images) and the vertical axis indicates the prediction intensity score of each of these images. For the unbiassed intersectional pair {Dark skin Male vs. Light skin Male} (in figure3), it can be observed that there is not much difference in prediction intensities within the pairs for both GAN and Real classes. Whereas, in figure4, the comparatively much greater difference in prediction intensities within the pairs for both GAN and Real classes, subsidize the quantitative results in tableIIthat indicates the existence of bias in the intersectional pair {Light skin Female vs. Dark skin Male}.

The bias evaluation results of the CvT based model for various domains are shown in tableIII. From the top portion of the table showing the results of individual measures, it can be observed that CvT shows high and similar accuracies for all the social groups within each of the domains. The FPR and FNR values are also very low and similar across the social groups within each domain. The bottom portion of the same tableIIIpresents the results of pairwise bias analysis of the model for various domains. The measures DP and EO report very high values, nearly similar to an ideal unbiased scenario. Altogether, the individual and pairwise measures do not report any existence of significant bias in the CvT based model.

The bias evaluation results of the Swin transformer based model for various domains are shown in tableIV. Similar to the CvT based model, it can be observed from the top portion of the table with individual measures that, the Swin transformer shows high and similar accuracies for all categories of social groups. The FPR and FNR values are also very low and similar across the social groups within each domain. The bottom portion of the same tableIVpresents the results of pairwise bias analysis of the model for various domains. The measures DP and EO show very high values, nearly similar to an ideal unbiased scenario. Altogether, the individual and pairwise measures do not report any existence of significant bias in the Swin transformer model.

SECTION: V-BBais analysis in thecompressedevaluation setting

The results of individual and pairwise measures of bias analysis of the ViT based model on the JPEG compressed evaluation corpora are shown in tableV. Similar to the observations discussed in the study[10], it can be observed that accuracies of the model decrease over compressed images, and the decrease in accuracy is much higher for the class GAN than class Real. In the compressed evaluation setting of ViT, it can also be observed that the difference in GAN accuracies (Accgan) between the groups within each of the domains has increased than its corresponding uncompressed evaluation setting. For example, in the uncompressed evaluation setting of ViT, the difference in GAN accuracies between female and male groups within the gender domain is 0.4 percentage points (in tableII); But, in this compressed setting, the difference has increased to 1.5 percentage points. Similarly, 2.8 percentage points of difference in racial domain between dark skin and light skin groups in the uncompressed evaluation setting have increased to 5.1 percentage points in this compressed setting, and 1.2 percentage points of difference between non-smiling and smiling groups of affective domain have increased to 3.2 percentage points. Similar to the uncompressed setting, here also in the class GAN, for the intersectional domain, light skin female group obtains the highest accuracy and dark skin female obtains the lowest accuracy, but the difference in their accuracies (Accgan) increases to 11.4 percentage points compared to 6.6 percentage points in the previous uncompressed setting. Altogether, for the class GAN, gender bias against the male group (lower accuracy for male than female), racial bias against dark skin, affective bias against non-smiling group, and intersectional biases, particularly against dark skin female, has increased in the compressed evaluation setting when compared to the uncompressed evaluation setting. Also, similar to the uncompressed setting, this compressed setting has higher values of FPR for female group in gender domain, light skin in racial domain, smiling face group in affective doman, and light skin females in the intersectional domain indicating that Real images of these groups are highly likely to be misclassified as GAN images.

The bottom portion of the tableVpresents the results of pairwise measures of bias analysis of ViT based model over the compressed evaluation corpora. The results show that, in this compressed setting, for class GAN, there is a decrease in DP and EO values when compared to the corresponding uncompressed evaluation setting. For example, the DP of {Dark skin vs. Light skin} for class GAN has decreased from 0.8758 (in previous uncompressed evaluation setting, tableII) to 0.8386 (in current compressed evaluation setting, tableV), DP of {Dark skin Female vs Light skin Female} has decreased from 0.7989 to 0.7522, etc. Thus, the pairwise evaluations on ViT also show that, for class GAN, the biases increase over the compressed images than the uncompressed images. That is, this indicates biases in the class GAN gets amplified with compression.

The results of individual and pairwise measures of bias analysis of the CvT based model over the compressed evaluation corpora are shown in tableVI. The top portion of the table shows the results of individual measures. Similar to the results of ViT (in tableV), compression decreases accuracies of the CvT based model, particularly the class GAN accuracy (Accgan), whereas class Real (Accreal) maintains its high accuracies. But compared to ViT, the drop in the accuracies for class GAN of the CvT based model is massively very high. Also, this accuracy decay in CvT is not similar across different social groups within a domain, indicating high bias.

The bottom portion of the tableVIpresents the results of pairwise analysis of CvT over the compressed evaluation corpora. From the table, it can be understood that, for the class GAN predictions of CvT, the ideal unbiased scenario which was seen in the previous uncompressed evaluation setting of CvT (in tableIII) has been completely overturned to a very largely biased scenario due to compression. This is because the drop in GAN accuracies are not similar across the groups within a domain (except for the dark skin female vs. light skin male pairs). Whereas, it can be observed that the class Real predictions of the CvT still maintains the ideal unbiased scenario as in the previous uncompressed evaluation setting.

The results of individual and pairwise measures of bias analysis of the Swin transformer based model over the compressed evaluation corpora are shown in tableVII. The top portion of the table shows the results of individual measures. In this model also the GAN accuracy (Accgan) decreases due to compression, thereby decreasing the total model accuracy. Contrary to the previous uncompressed setting of Swin transformer (in tableIV), where similar and high accuracies are obtained for all social groups within each domain, this compressed evaluation setting has eventually brought up differences in GAN accuracies across social groups within each of the domains. That is, the GAN accuracy of the male group is less than the female group by 2.5 percentage points in the gender domain, the dark skin group is less than the light skin group by 4.9 percentage points in the racial domain, and the non-smiling group is less than smiling group by 3.2 percentage points in the affective domain. In the intersectional domain, the highest GAN accuracy is obtained for the light skin female group and lowest for the dark skin female group, a very high accuracy difference of 16.4 percentage points is observed between these two intersectional groups for class GAN. Thus these accuracy differences indicate the existence of bias in the compressed setting for the class GAN of Swin transformer based model.

The bottom portion of the tableVIIpresents the results of pairwise analysis of the Swin transformer based model over the compressed evaluation corpora. Compared to the previous uncompressed setting of Swin transformer (in tableIV) that reports nearly an ideal unbiased scenario, in this compressed setting, values of the measures DP and EO decrease highly for the class GAN indicating an increase in bias for the class GAN predictions. A high bias for class GAN predictions can be observed particularly in the pairs, light skin female vs. light skin male and dark skin female vs. light skin female.

SECTION: VIDiscussion

The bias evaluation results show that in uncompressed evaluation settings, the evaluation corpora and measures could identify bias in the ViT based forensic classifier model, such as, bias in pairs involving light skin female groups e.g., bias in light skin female vs. dark skin male, dark skin female vs. light skin female, etc. Also, bias analysis in ViT based model shows other interesting inferences such as, Real images of light skin females have a very high probability of being misclassified as GAN images, and GAN images of dark skin females have a very high probability of being misclassified as Real images. However, the uncompressed setting could not identify any bias in the CvT and the Swin transformer based models.

The compressed evaluation setting, on the other hand, identifies high bias in all three transformer based models, particularly in the class GAN predictions. In the compressed evaluation setting, for the class GAN predictions of all the models, gender bias against the male group, racial bias against dark skin group, affective bias against non-smiling group, and intersectional biases, particularly against dark skin female group, has increased when compared to the uncompressed evaluation setting. Bias is identified in all the domains for the CvT based model, in the compressed evaluation setting. That is, the study could observe that model bias is impacted by image compression. Moreover, the model bias identified in the uncompressed setting is observed to be amplified in the compressed setting, particularly for the class GAN predictions. Also, given that our results indicate Real images of certain social groups such as light skin females are more likely to be misclassified as GAN images, and GAN images of dark skin females are more likely to be misclassified as Real images, etc., the images of these social groups when compressed can even more increase the risk of security threats. Therefore, image forensics works that utilize visual transformers for the task of distinguishing natural and GAN generated images, besides assessing the robustness of the algorithms towards image compression, should also study the existence of bias in these algorithms, even in the compressed setting.

ViT and Swin transformer based models chosen for this study are pre-trained on the ImageNet-21K dataset[52]. As already stated above, more than the uncompressed evaluation settings, these models show a higher bias in their corresponding compressed evaluation settings. On the other hand, the model CvT is pre-trained on the ImageNet-1k dataset[53]. But unlike ViT and Swin transformer, CvT has comparatively a very high transition from an ideal unbiased scenario in the uncompressed evaluation setting to a very largely biased model in the compressed evaluation setting. Hence, pre-training corpora of the visual transformers might be one of the factors inducing bias in these models.

SECTION: VIIConclusion

This study explored bias in the visual transformer based image forensic algorithms that classify natural and GAN generated images. The study utilized three visual transformers viz., ViT, CvT, and Swin, for constructing image forensic algorithms to classify natural and GAN images. The pre-trained visual transformers are fine-tuned using the task-specific natural image versus GAN image dataset, and are examined for any existence of bias in the gender, racial, affective, and even intersectional domains. Hence, a bias evaluation corpora consisting of social groups belonging to the evaluation domains are procured for the study. Individual and pairwise bias evaluation measures are used for identifying any existence of bias in these transformer based forensic models. Since robustness towards image compression is significant for the forensic algorithms, this study also examines the role of image compression on model bias. To the best knowledge, this is the first work to study the impact of image compression on model bias, particularly focusing on the task of classifying natural and GAN images. Hence, this work conducts the bias evaluation experiments in two separate settings; one set of experiments on the original uncompressed evaluation corpora and the other on the compressed version of the same evaluation corpora, where both these experiments rely on same evaluation measures.

This study helped to identify the existence of bias in the transformer based models for the task of distinguishing natural and GAN generated images. The two-phase bias evaluation strategy helped to identify bias in the uncompressed and compressed scenarios and also to study the impact of image compression on the model bias. The study observed that image compression impacts model biases, and particularly compression amplifies the biases of the class GAN predictions. To help towards future research, all relevant materials of this study including the source codes will be made publicly available athttps://github.com/manjaryp/ImageForgeryFairnessandhttps://dcs.uoc.ac.in/cida/projects/dif/Imageforgeryfairness.htmlalong with the publication.

SECTION: VII-AFuture directions

This study to explore algorithmic fairness in image forensic systems proposes a generalized and simple bias evaluation framework consisting of evaluation domains, evaluation corpora, evaluation measures, and also the two evaluation settings, one in uncompressed and the other in compressed settings. Hence, beyond the popular transformers such as ViT, CvT, and Swin analyzed for algorithmic fairness in this work, our generalized and simple bias evaluation framework makes it highly suitable for easily evaluating the latest visual transformers such as RT-DETR[15], Conv2Former[56], etc. Also, the results of our bias analysis experiments could observe that the differences in the existence of biases in corresponding uncompressed and compressed settings have similar trends for visual transformers pre-trained on the same dataset. For example, the transition in bias existences from the uncompressed to compressed settings of ViT and Swin transformers pre-trained on ImageNet-21K are similar, whereas the transition is different for CvT pre-trained on the ImageNet-1k dataset. Therefore, this analysis from our study could be useful in the future for exploring algorithmic bias in many other latest visual transformers such as Conv2Former-B[56]utilizing the pre-trained dataset ImageNet-21K or MobileViT[57]utilizing ImageNet-1k dataset, to further expand the study particularly on the cause and relationship with pre-training data biases.

In the future, we are considering to extend this work to analyze various factors that cause or originate these biases. Although it is cumbersome to procure datasets with balanced groups within a wide variety of domains, this could, in the future, help in determining various sources of biases, especially in identifying the existence of any pre-train and fine-tuning data biases. The evaluation corpora can also be expanded and annotated towards a benchmark corpora to explore bias in many other domains, in the context of image forensics. Apart from images generated by GANs, a lot of diffusion models are also recently gaining popularity for image synthesis. Studies in literature have reported that the images generated by GAN algorithms and diffusion models have differences in their characteristics[58]. Therefore, in the future, we are considering extending this work to analyze the fairness of forensic models that can also detect images generated by the diffusion models. Also, there is a large scope for mitigation of these biases from the models to develop fair forensic systems that one can trust when deployed in the real world.

SECTION: References