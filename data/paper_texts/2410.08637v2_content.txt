SECTION: Unsupervised Learning of Spatio-Temporal Patterns in Spiking Neuronal Networks

The ability to predict future events or patterns based on previous experience is crucial for many applications such as traffic control, weather forecasting, or supply chain management.
While modern supervised Machine Learning approaches excel at such sequential tasks, they are computationally expensive and require large training data.
A previous work presented a biologically plausible sequence learning model, developed through a bottom-up approach, consisting of a spiking neural network and unsupervised local learning rules.
The model in its original formulation identifies only a specific type of sequence elements composed of synchronous spikes by activating a subset of neurons with identical stimulus preference.
In this work, we extend the model to detect and learn sequences of various spatio-temporal patterns (STPs) by incorporating plastic connections in the input synapses.
We showcase that the model is able to learn and predict high-order sequences. We further study the robustness of the model against different input settings and parameters.

SECTION: IIntroduction

Learning and processing sequences of events is central to human cognition. While supervised machine learning methods achieve astounding results at sequential processing, they provide few insights into underlying biological mechanisms, are energy-intensive, and are prone to failure in noisy or incomplete environments. Borrowing solutions from biological neuronal networks is a promising pathway to overcome such limitations.
Recent studies demonstrated the potential of dendritic computations for a sequence prediction task[1,2].
The work in[1]developed a biologically inspired sequence learning and prediction model. It learns to predict complex sequences in an unsupervised, continuous manner using biological, local learning rules.
The model identifies the sequence elements through a prewired mechanism, i.e., the presentation of a sequence element activates a specific subpopulation of neurons.
In this work, we aim to extend the model to learn in an unsupervised manner naturalistic sequences composed of a stream of spatio-temporal patterns (STPs).
We study the model’s robustness to noise and hyperparameter choice, thereby exploring the potentials and limitations of dendritic processing for sequence learning and processing.

The network achieves online, bio-plausible, and unsupervised sequence stimulus detection using spike time-dependent plasticity (STDP) and competition via inhibition.

SECTION: IIModel

The network consists of three sparsely recurrently connected neuron populations: excitatory neurons (), inhibitory neurons (), and external spike sources (), thereby constituting a recurrent EI network (Fig.1). Synaptic delays between populations are constant, except for. A non-linear activation function mediates dendritic prediction (Fig.1).

Inputs are composed of sequencesof STPs, abstractly described by Latin letters. For each run, two partially overlapping and therefore high-order sequences are considered and presented to the network in an alternating fashion (Fig.2). To increase the bio-plausibility of the setup and test the model’s robustness to noise, we introduce Poisson-generated noise between sequence stimuli and, for a prolonged duration, between the two sequences.
Noise duration and firing rates are defined in relation to sequence item characteristics (see Results section for further details).

The overlapping sequence elements introduce ambiguity. To resolve these ambiguities, the network must form context-dependent representations for the overlapping elements. The context is described by the sequence currently shown to the network. A presentation of a sequence element consists of a mapping to a specific STP.
STPs can be divided into two central characteristics: purely spatial and purely temporal pattern encoding. Spatial patterns are characterized by the occupied subset of input channels in. Temporal patterns occupy all channels but differ in their exact spike time throughout the channels (Fig.3).

Excitatory neurons are modeled as leaky-integrated firing (LIF) neurons, which are characterized by having three input channels: external stimulation, inhibition via, and recurrent excitatory synapses.
Synapses from external sources have weightsand delays, which are both initialized from normal distributions, with the plastic weights following STDP type of plasticity. Inhibitory synapses receiving input from LIF neurons have a constant weight and delay.

Inhibition enforces competition between excitatory neurons, leading to item-specific selectivity.
In addition to the homeostatic plasticity, this results in non-overlapping excitatory responses. Recurrent weightsare initialized from a random distribution and are updated according to a time-restricted STDP learning rule (1). The restriction ensures taking spiking activity during the presentation of the previous item into account while excluding responses to the current and second-to-last item, thereby allowing learning sequence item transitions and preventing direct context transfer from earlier sequence elements.

Excitatory neurons can be in one of two states: predictive or not predictive. Neurons are considered predictive if currents from recurrent input are strong enough to cross the dendritic action potentials (dAPs) threshold. Then, for a constant time of, neurons are strongly depolarized by receiving a positive input current. After that phase, neurons return to the default non-predictive state.

The dAPs play an important role in learning context-dependent representations of overlapping items, especially since they can sustain activity while stimulus is absent. Predictive neurons are expected to fire earlier and therefore suppress the activity of the non-predictive neurons. Thereby, overlapping items develop two context-dependent sets of predictive neurons, which in turn allow context-dependent activation of the next element.

Simulation is split into two phases. During the first phase, we only allow plasticconnections to grow. This phase supports assembly formation and neuron specialization. During the second phase, synaptic weightsare frozen and only recurrent connectionsare plastic.
With the onset of the second phase, inhibition levels are increased to support the creation of context-dependent predictive neurons. To further strengthen the context-dependent activation, weight updates are controlled by homeostatic control, which constrains neurons to be predictive only in one context:

whereis the target firing rate for homeostatic control of the current firing rate,the presynaptic trace, and positive updates occur only if the time gapbetween pre- and postsynaptic spike is between sensible bounds.

For visualization and evaluation, excitatory neurons are grouped by their maximum average response per item. Those selective groups are called assemblies and are denoted by their corresponding item stimulus, e.g.,. We measure the average number of predictive neurons in each assembly. The item corresponding to the assembly with the most predictive neurons is predicted. Prediction accuracy is evaluated only for the last item in each sequence. If not explicitly mentioned, all experiments are carried outtimes.

In the current implementation, network size and simulation time scale proportionally with the number of items in the sequence sets.
Implementation details and used hyperparameters can be found atgithub.

SECTION: IIIResults

SECTION: III-ASpatial Encoding

During the first simulation phase, after the network is repetitively exposed to the external stimuli, it learns non-overlapping sequence item-specific assemblies (Fig.4).

In the second phase, the network learns the sequence prediction task. The network activity after training exhibits context-dependent dAP predictions, which in turn influence excitatory responses (Fig.5). Initially, we observe a rapid forming of predictive neurons, but with a certain overlap in the activity patterns of two contextual inputs (Fig.6). Over time, homeostasis and increased inhibition levels lead to a decrease in overlaps.

Thereby, two context-dependent routing paths through the network are created and allow increased accuracy in predicting the last, context-dependent item (Fig.7). With increasing sequence length, the number of training episodes increases for the network to fully learn the sequences. This is expected, since context needs to be transferred through more overlapping items. For the shorter sequence task, prediction accuracy afteriterations of multiple network realizations results in a Bernoulli distribution: while most networks solve the task completely, the rest fail to make correct predictions.

We carry out ablation studies to investigate the network’s robustness to noise and sensitivity to hyperparameters using the simplest high-order sequence.

Setting up the dAP thresholdcorrectly is crucial for learning the sequences (Fig.8). For a small threshold, too many neurons become predictive and context is lost. The higher the threshold, the fewer neurons are predictive. For a range of, the correct set of neurons switches into the predictive mode for the corresponding transitions. For higher thresholds, few neurons become predictive to carry over context information to the end item, and as such prediction accuracy drops to. This corresponds to a single compartment model since recurrent inputs cannot switch neurons into the predictive state.
The network exhibits robustness over a large range of values of(Fig.8).

The network is similarly resistant to jitter in the external stimuli (Fig.9). Jitter refers to online spike displacements bysteps, either temporally with a step size of, or spatially, where a step refers to changing the neuron id by. While for spatial jitter values ofthe network loses the ability to solve high-order transitions, it still correctly predicts the final sequence elements but without taking context into account.
Those results together highlight the usefulness of dendrites as predictive components in a biologically sensible setting.

SECTION: III-BTemporal Encoding

Unlike spatially encoded stimuli, temporal encoding requires learning precise delays, ensuring input spikes arrive nearly synchronously at excitatory neurons. While multiple studies explored learning delays in a supervised fashion[3,4,5], unsupervised approaches are rarely considered[6].
In this study, we set the delays exactly congruent to the temporal stimuli and assign each excitatory neuron a sparse array of those incoming delays.
Using such artificially calculated delays, temporal patterns allow faster high-order transition learning (Fig.10). During these simulations, networks solve the task also more consistently than with spatial stimuli (compare standard deviations of Fig.7). Additionally, longer high-order sequences can be considered and solved by the network: while sequences with more thanoverlapping elements were solved only occasionally with spatial stimuli, temporal stimuli consistently solved sequences with up tooverlapping elements.

The network performance depends on correct delays. While temporal jittering external spikes does not lead to a decreased performance for up to(Fig.11), stimuli are on average congruent to the precomputed delays. In strong contrast, when applying constant noise to the delays, network performance degrades rapidly (Fig.11). This highlights the importance of further research on unsupervised delay learning, since without precomputing delays prediction accuracy would decrease to.

SECTION: IVDiscussion

In this study, we extend the work in[1]to allow the processing of sequences of STPs. To this end, we study the robustness of the dAP-based prediction mechanism for predicting sequences in biologically realistic environments. While networks trained on spatial stimuli can detect sequence items, the performance of context-dependent transition learning was observed to be constrained to sequences of low complexity and narrow hyperparameter ranges. Instances of failure are expressed in neurons either not switching into the predictive mode or losing context-dependency during transitions.
We argue that unsupervised delay learning could offer a promising pathway to overcome such limitations since network training is more performant and stable using precomputed delays.

SECTION: VAcknowledgments

This work was funded by the Federal Ministry of Education, Germany (project NEUROTEC-II grant no. 16ME0398K and 16ME0399).
The authors thank Tom Tetzlaff and Yeshwanth Bethi for valuable discussions on the project.

SECTION: References