SECTION: A semi-supervised learning using over-parameterized regression

Semi-supervised learning (SSL) is an important theme in machine
learning, in which we have a few labeled samples and many unlabeled
samples. In this paper, for SSL in a regression problem, we consider a
method of incorporating information on unlabeled samples into kernel
functions. As a typical implementation, we employ Gaussian kernels whose
centers are labeled and unlabeled input samples. Since the number of
coefficients is larger than the number of labeled samples in this setting,
this is an over-parameterized regression problem. A ridge
regression is a typical estimation method under this setting. In this
paper, alternatively, we consider to apply the minimum norm least
squares (MNLS), which is known as a helpful tool for understanding deep
learning behavior while it may not be application oriented. Then, in
applying the MNLS for SSL, we established several methods based on
feature extraction/dimension reduction in the SVD (singular value
decomposition) representation of a Gram type matrix appeared in the
over-parameterized regression problem. The methods are thresholding according to
singular value magnitude with cross validation, hard-thresholding with
cross validation, universal thresholding and bridge thresholding
methods. The first one is equivalent to a method using a well-known low
rank approximation of a Gram type matrix. We refer to these methods as
SVD regression methods. In the experiments for real data, depending on
datasets, clear superiority of the proposed SVD regression methods over
ridge regression methods was observed. And, depending on datasets,
incorporation of information on unlabeled input samples into kernels was
found to be clearly effective.

SECTION: 1Introduction

SECTION: 1.1Background, motivation and related works

Semi-supervised learning (SSL) is an important theme in machine
learning[20,21]. It is a situation where we have a
few input-output samples and many input samples for which we have no
corresponding output samples; e.g.,[20]. This problem
setting is mainly studied in image classification, in which the latter
samples are often referred to as unlabeled samples; e.g.,[7]. However, a little have been reported for regression
problems; e.g.[22,21]. In[22], two-nearest neighbor regressors with different distance metrics are
trained in parallel and used to generate pseudo-labels for unlabeled
data according to a confidence measure; i.e., it employs co-training. In[21], two reasonable methods have been considered, which are
data augmentation consistency regularization and empirical risk
minimization on augmented data. In both methods, data augmentation is
introduced, in which new input sample is generated by adding noise to an
training input. Therefore, as shown in[2], these methods
implement certain regularization effects. In this paper, as a new-type
of SLL for regression problems, we propose a method in which information
on unlabeled inputs is taken into a regressor. More precisely, we set a
kernel type regressor whose components are determined by both of labeled
and unlabeled inputs. For example, in this paper, we employ Gaussian
kernel functions whose centers are labeled and unlabeled input
samples. Since the number of kernels is larger than the number of
labeled training samples, this setting is a so-called over-parameterized
case. Therefore, we need modeling methods under an over-parameterized
case.

A straight way of modeling in this setting is anregularization
method which is equivalent to a ridge regression. It solves a
colinearity problem in an over-parameterized case and yields a
reasonable smooth output by optimizing a ridge/regularization
parameter. On the other hand, the minimum norm least squares (MNLS) is
also an estimation procedure for over-parameterized regression problems.
Although it has been of interest in deep learning as a tool for
analyzing “double decent” and “benign
over-fitting”[1,11,18], there are no
effective applications of MNLS in regression problems. Indeed, the
prediction performance of a naive MNLS solution has shown to be worse
than that of a naive ridge regression[11]. Therefore, we
need a certain modification of MNLS in our proposed
SSL scheme.

In[4,18], it has been shown that a naive
ridge regression is dominated by a type of principal component
regression (PCR). PCR is a method, in which an input data matrix is
transformed into a score matrix and, then, ordinal least squares method
is applied by employing the low-dimension score matrix with large eigen
values as an input data matrix. This can be viewed as a regression
method under a low rank approximation of input data matrix. Actually,[4]has showed a typical example which prefers PCR. The
MNLS estimate is obtained via singular value decomposition (SVD) of
input data matrix instead of principal component expansion. Then, for
our proposed SSL scheme using MNLS, we consider to implement feature
extraction/dimension reduction in the SVD representation.

SECTION: 1.2Contribution

In this paper, we established SSL scheme based on regression methods in
an over-parameterized case; i.e., over-parameterized regression
methodsw. The idea of the SSL scheme is that we employ a linear
combination of Gaussian kernel functions whose centers are labeled and
unlabeled input samples. By this implementation, we attempt to
incorporate information on unlabeled inputs into a regressor.
The choice of Gaussian kernel is not so essential in this idea while it
is a typical choice. In this setting, the number of kernels is equal to
the number of whole data including labeled and unlabeled samples.
Therefore, this is an over-parameterized regression problem and a
typical estimation method is a ridge regression which is equivalent to
theregularization. In this paper, alternatively, we consider
the MNLS estimate obtained via SVD of a Gram type matrix of kernels, in which
a vector of output samples are transformed and a weight vector achieving
perfect fit to the transformed vector is obtained. The MNLS coefficient estimates are
obtained by an inverse transform of the weight vector. We refer to this
transformed domain as SVD domain.

By regarding the SVD domain as a feature space, we consider to apply
feature extraction/dimension reduction in the SVD domain and obtain
coefficient estimates by using inverse transform from the modified
weight vector in the SVD domain. More specifically, we apply
thresholding methods for the weight vector in the SVD domain. This
strategy is a sparse modeling using a thresholding method on the SVD
domain. Indeed, it generally yields shrinkage estimates of coefficients
in the original domain. We call this strategy SVD regression method. If
we employ a dimension reduction scheme according to the magnitudes of
SVs then it corresponds to PCR in the under-parameterized case; i.e., we
remove components with small PCs/SVs. Thus, this method implements low
rank approximation of a Gram type matrix in some sense. We refer to this
method as SSV. As pointed out for PCR, this method does not utilize
output samples to choose components. Then, we established thresholding
methods to choose components that are effective for explaining outputs.
We introduced three different methods that are a hard-thresholding (HT)
method using a naive cross validation choice of threshold level, a HT
method with the universal threshold level[5,6]and a
bridge thresholding (BT) method that links between soft-thresholding
(ST) and HT[8,23,10]. We refer to these methods as
SHT, SUT and SBT respectively.

We then conducted experiments for real data from UCI
repository[12]to examine the effectiveness of this scheme of SSL
and performances of SVD regression methods. For the latter, we compared
SVD regression methods to ridge regression methods that are referred to
as RR and RRO, RR is a ridge regression under labeled samples and RRO is
a ridge regression under both of labeled and unlabeled samples, thus, in
over-parameterized case. The experimental results are summarized as
follows.

The prediction performances of regression methods depend on
datasets. There are datasets that prefer ridge regression methods
and, also, there are datasets that prefer SVD regression
methods. The difference in the performances is clear when
the number of training samples are relatively large.

Among SVD regression methods, the prediction performances SHT, SUT
and SBT are comparable to that of SSV which utilizes information
only about input samples.

Depending on the dataset, the
performance of ridge regression is clearly improved by semi-supervised
learning formulated as an over-parameterized case. Although a similar
result is obtained for SBT, the efficacy may not be so clear.

SECTION: 1.3Organization of paper

In Section 2, we explain SSL using over-parameterized regression
methods. In Section 3, we explain regression problems in an
over-parameterized case, in which we introduce thresholding methods
under the SVD-based MNLS. The experiments for examining prediction
performances of SSL methods are shown in Section 4. Section 5 is
denoted for conclusions and future works.

SECTION: 2Semi-supervised learning for regression

SECTION: 2.1Problem setting

We firstly give several notations. For a matrix,denotes
the transpose of.denotes a diagonal
matrix whose diagonal elements are.anddenote theidentity matrix and thezero matrix respectively.denotes the-dimensional zero column vector.
For a vector, we definewhich is the Euclidean norm of. For a real number, we define. Ifhas a multivariate Gaussian distribution with mean
vectorand covariance matrix, as a usual notation, we write. This notation is common for one dimensional case.

In a setting of SSL, for, we haveinput-output dataandoutput data, whereand. The latter is often called unlabeled data in
classification problems and we use this term while our setting is a
regression problem. We assume that

whereare i.i.d samples from. We define,and.
Then, (1) can be written as

SECTION: 2.2Proposed scheme

For, letbe a Gaussian function defined by

for,
in which the center is the-th input sample andis a common
width that is a hyper-parameter. Here, we callkernel
functions while it may not be properly formulated here.

By employinginput-output data as training data,
we consider to estimateby

whereis a coefficient vector.
In this setting, we expect that extra Gaussian kernels whose centers are
unlabeled input samples help prediction on out of training inputs. In
other words, we incorporate unlabeled inputs into a regressor for
utilizing information of unlabeled inputs for prediction. This is
regarded as a new-type SSL for regression problems. Sinceholds,
this setting is an over-parameterized case. Therefore, the colinearity
problem arises in the least squares method and we need the other
estimation methods. We refer this estimation problem to as an
over-parameterized regression problem. Note that the choice ofhere may not be essential and it is important thatshould be a
function that incorporates information on unlabeled inputs into modeling. In this
meaning, a Gaussian function forseems to be one good possible choice
for our purpose. In the next section, we show several estimation methods
for an over-parameterized regression problem arising in SSL.

SECTION: 3Over-parameterized regression methods

We define. Letbe anmatrix whoseentry is.is a Gram type matrix; i.e.,
it may not be a usualGram matrix. We assume thatis
fixed. We define. We
assume that the rank ofis.

SECTION: 3.1Ridge regression

A typical method for over-parameterized regression problems is a ridge
regression which solves the colinearity problem.
In a ridge regression, estimate of a coefficient vector is obtained by

whereis called a ridge parameter.is
also a solution in minimizing theregularized cost function
given by

In this formulation, the ridge parameter is called a regularization
parameter. This method is possible to apply even when. The
ridge (regularization) parameter is a hyper-parameter which may
be selected by cross validation (CV) typically.
We refer to a naive ridge regression in case ofas RR and
a ridge regression in(over-parameterized case) as RRO.

As alternatives to a ridge regression method for an
over-parameterized case, we now consider methods based on the MNLS.

SECTION: 3.2Minimum norm least squares

In deep learning, the MNLS estimation has been extensively studied as a
tool for analyzing “double decent” and “benign
over-fitting”[1,11,18]. The MNLS is a
method obtaining a unique solution of perfect fit in an
over-parameterized case and the MNLS solution is obtained via
SVD. Although the MNLS attracts attention due to the theoretical
interest in deep learning, there is no application to regression
problems. In applying MNLS for the proposed SSL scheme, we consider to
modify the SVD based formulation of the MNLS. To do this, we briefly
review the MNLS estimation in this subsection.

The MNLS solution is given by minimizingamong solutions satisfying

therefore,.
By applying SVD to, we have

whereisorthonormal matrix,isorthonormal matrix andin which.andfor anysince the rank ofis.are called singular values (SVs).
Without loss of
generality, we assume thatfor SVs.
We define

Sinceis orthonormal, we
have

Therefore,for, where

Hence,componentwisely for. As a
result, the MNLS solution ofis given by

sinceis orthonormal. It is easy to see thatachieves the
minimum norm. Therefore, it is the solution to the MNLS estimation. In
this derivation, an originaldomain is transformed into an
another domainand the fitting is considered in the
latter domain which is determined by singular values. We refer to the
transformed domain defined byas SVD domain.

We briefly summarize statistical properties of estimates in SVD domain.
We define.
By (2),
(9) and orthonormality of, we have

Hence,holds under a Gaussian noise
assumption; i.e.,are mutually independent and. Thus,holds; i.e.,are mutually independent and.
Due to these statistical properties, the-th SV components is
significant if. Therefore, in the SVD regression problem,
we can apply thresholding techniques in[5,6,10].

SECTION: 3.3Thresholding in SVD domain

By (3.2), the MNLS solution reduces to a perfect fitting
ofby. In other words,andare
regarded as a design matrix and coefficient vector respectively; i.e.,
thus, it is an orthogonal regression problem. We now consider
thresholding on, which works as forming a sparse representation
forin terms ofin the SVD domain. Note
that, in a usual setting of a sparse modeling[14], a sparseness
is assumed in representingby, in which some elements ofare zeros in representing. On the other hand, we assume
that some elements ofare zeros in representing. Therefore, a sparseness is considered in a space that is
transformed by. It does not lead to a sparse representation in the
original domain withand it rather works as shrinkage of. This is because removing components in the SVD domain affects
the entire components in the original domain and the transformation is
isometric; e.g. see[9]and later implementations.

Letbe a thresholding function onfor.may also have a shrinkage effect in a certain method. We
define

where.for someto implement thresholding.
Then, the resulting estimate ofis obtained by

where.
We refer to this estimation scheme as SVD regression.
In the next section, we show several implementations of
thresholding in the SVD regression scheme.

SECTION: 3.4SV based modeling (SSV)

The first method is to keep components with large SVs.
Under a pre-determined
threshold level denoted by, we set

Letbe the-th column vector of. By (10),is equivalent toand, thus,in (3.2). Therefore,
this is equivalent to a method of setting, which is
a low rank approximation ofin the MNLS estimate.

Here,holds. Therefore, we have

sinceis orthonormal. This implies that the thresholding in the SVD
domain leads to a shrinkage on the original domain.
This also applies any other thresholding methods.

We define, which is often called
an active set that is a set of indexes of unremoved components. We
choosein, by which
we ignore the case whereis empty; i.e., inclusion of a
constant zero function. Since, if we
setthenis the number of unremoved
components, wheredenotes the number of members in. Therefore, the determination ofis equivalent to
that of the number of unremoved components. This method is referred to
as SSV in this paper. Generally, important components for
prediction do not necessarily have large SVs; i.e., SVs do not have
information on a target function. Therefore, SSV is possible to include
components which are useless for representing a target function while
those have large SVs. Nevertheless, this method may attractive since it
may give us entirely smooth outputs and components with large SVs are
relatively reliable; i.e., include less numerical error.

We consider to apply cross validation (CV) for selecting the number of
unremoved components. More precisely, in a choice of training and
validation datasets from whole data, SVD is applied to the training data
and validation error is calculated underunremoved (non-zero)
components for. After repeating this procedure for
all splits of whole data, we choose the number of unremoved components,
which minimizes the validation error sum. It is denoted by. Note
thatmay be taken to be less than the number of training data
since the SV is sufficiently small and is not reliable for large. Then, in constructing a final model, after we obtainedfor whole data, we setforfandfor.

SECTION: 3.5Hard-thresholding with CV (SHT)

We firstly consider a HT, in which a threshold level is selected by CV.
For,

is a hard-thresholding (HT) function, whereis a threshold level
that is a hyper-parameter[5,6].
Here,is set to be different for each. This is because
the variance ofdepends onby the consequences from
(14) and (15). If we set a common threshold level
for allthen components with large variances tend to be
unremoved comparing to components with small variances when those means
are equally zeros. In general, the choice of component-wise threshold
levels may be extremely difficult. However, it is not a problem in this
case as follows. We defineforand setfor. Then, by, we have. Thus, (20) is
equivalent to

and the active set in this case is.

Now, since, employingas a common
threshold level onis reasonable. We choosefromby ignoring the case whereis
empty. Therefore, the selection ofis equivalent to the
selection of the number of unremoved components, which is the size of. In constructing a regressor for a given data with size, we obtainandby
descending enumeration. Then, we can obtainwithunremoved
components by settingin (21). One
simple strategy is to determine an optimal number of unremoved
components is to apply CV as in case of SSV. The HT method with CV
choice of a threshold level is referred to as SHT.

SECTION: 3.6Universal Thresholding (SUT)

Again, since, we can apply the well-known
universal threshold (UT) level in[5,6]for. It is given by

and is known to be asymptotically
optimal in some sense[5,6].
We set thisin (21) to obtain.

To apply the UT level, we need an estimate of noise variance denoted by. In wavelet, for example, median absolute deviation (MAD) of
the first detail coefficients is employed[5,6]. However,
it cannot be applied to our setting. For estimating noise variance in a
context of general non-parametric regression problems, there are model-based
(residual-based) and model-free (difference-based)
methods[19,17], in which the former utilizes a ridge
estimate. There are several variations of residual-based type and we
employ an estimate suggested by[3]. In our context, in whichis fitted byin the SVD domain, it is given by

whereandis a ridge
parameter. The prediction performance of the SVD regression method is
determined by thresholding here. The above ridge parameter is required
for a stable estimation of noise variance and is set to be a small
value; i.e., it is not hyper-parameter in SVD regression. Actually, we
set it to a fixed small value in the later experiments. The HT
using the UT level is referred to as SUT.

SECTION: 3.7Bridge thresholding (SBT)

As an alternative to HT, soft-thresholding (ST) is
known[5,6]. Unlike HT, ST simultaneously controls both
of threshold level and amount of shrinkage by one hyper-parameter. It
is well known that ST is a special case of lasso[14]; i.e., in
case of orthogonal regression problems. Lasso is known to be suffered
from an estimation bias problem; i.e., choice of a high threshold level
brings us a sparseness while it automatically increases the amount of
shrinkage, which leads to a large bias. For relaxing this problem,
several methods including adaptive lasso[23]has been
proposed. In case of orthogonal regression problems, adaptive lasso
reduces to

whereand
we setfor simplicity[10]. Formally,reduces to ST when. When,
it reduces to non-negative garrote under an orthogonal regression
problem[8]. On the other hand, ifis very large thenholds foreven whenis small. Therefore,is close to the
HT function as; e.g.[10]. This implies
thatyields a bridge estimator that connects
between ST and HT estimators.is also
obtained by scaling a ST estimator to move it to a HT
estimator[10]. Therefore, by choosing a relatively large, the above estimation bias problem of ST is
solved in. We refer to this
thresholding method as a bridge-thresholding (BT) method.
In case of applying BT to SVD regression, we have

for. As in UT, if we setin (25) then we have

sinceis odd and. Therefore, as in HT,
it is enough to choose an appropriate value forinstead ofindependently.

Since we set a relatively large value forto relax bias
problem of ST,is only a hyper-parameter.
Unlike HT, we can derive a risk estimate under BT[16,10].
We define the risk by

wheredenotes the expectation with
respect to the joint distribution of; i.e.,by (14).
We define an active set byand. Note that it is equivalent toas in HT.
In[16,10], by using Stein’s lemma[13],

has been found to be an unbiased estimate of the risk, which
is called Stein’s unbiased risk estimate (SURE). This can be a model
selection criterion for; i.e., calculateforand choose,
whereis a candidate set of. In applications, as in
SUT, we need to estimateand employ (23) for it
again. This method is referred to as SBT. In SBT, althoughdetermines not only the threshold level but also the amount of
shrinkage, we choose a value offromin the experiments
for simplicity.

SECTION: 4Numerical examples

In this section, we consider a situation where we haveinput-output
training samples andunlabeled input samples. As alternatives to
the SVD regression methods, we consider ridge regression methods withandin the subsection3.1under
(3). Those are referred to as RR and RRO
respectively; i.e., RR does not utilize un-labeled inputs and RRO
does. In a context of semi-supervised learning, we compare the
performances of RR, RRO, SSV, SHT,
SUT and SBT.

In this paper, we show results on four typical example datasets from UCI machine
learning repository[12], which are

SGEMM GPU kernel performance dataset : the number of whole data is(a part of available data), the number of input variables
(features) isand a target variable is “Run2”,

Auction Verification dataset : the number of whole data is, the number of
features isand a target variable is “verification.time”,

Energy efficiency dataset : the number of whole data is, the number of
features isand a target variable is “Y1”,

Yacht Hydrodynamics dataset : the number of whole data is, the number of
features isand a target variable is “resistance”;

see[12]for details of variables. The features are
normalized in our experiments.

The setting of hyper-parameters in the methods is as follows. The
hyper-parameters which are selected by cross validation are the ridge
parameter in (5) and width parameter in
(3) for RR and RRO, the number of components and
width parameter for SSV and SHT, the width parameter for SUT and SBT.
The number of folds of cross validation is. In RR and RRO,
the candidate values of hyper-parameters arefor a ridge parameter andfor a width parameter, in whichis
the number of training data. In SVD regression methods, the candidate
values of hyper-parameters arefor a
width parameter andfor the number of unremoved
components. Here,is set to the integer part of. The SVs
in this range may be stable, in whichmay be sufficiently
large. Note that, for almost experiments, values of hyper-parameters are
within the above candidates. The variance estimate is obtained by
(23) in SUT and SBT, in which we setwhich stabilizes the estimate. In SBT, the number of components is
selected by (28) with this variance estimate and we
setwhich is enough for relaxing a bias problem of ST.

In the first experiment, we compare the performances of the methods. In
Fig.1, we show the mean and standard
deviation (error bar) of test errors of the methods fortrials with
different random choices of training and test sets, in which test error
is measured by; i.e. ratio of the mean squared test error of
estimate to that of the mean of test outputs. For any, we
set. As seen in Fig.1(a) and
(b), for GPU and Auction datasets, ridge regression methods are superior
to SVD regression methods. This is reliable and notable in case of. This implies that, for these datasets, SVD may not be
appropriate as feature selection for helping prediction. Among
SVD regression methods, SSV is relatively superior to the other
methods. This implies that components with large SVs contributes stably
for prediction and choice of contributed components according to output
data does not work so well for these datasets. Note that the results
forin Fig.1(a) and (b) may not be
reliable in all methods since their standard deviations are large, which
implies thatmay not be sufficient for prediction. And, as seen
in case of, any method can stably produces better estimate whenis sufficiently large.

(a) GPU dataset

(b) Auction dataset

(c) Energy dataset

(d) Yacht dataset

On the other hand, as seen in Fig.1(c) and
(d), for Energy and Yacht datasets, SVD regression methods outperform
ridge regression methods for. This advantage of SVD regression
methods is reliable since standard deviations (error bars) are small
enough for. This result implies that, for Energy and Yacht
datasets, feature extraction via the SVD is effective for prediction,
which may be the case pointed out in[11,4].
Although the performance of SBT is slightly superior to the other SVD
regression methods for Energy dataset, it is not notable and
performances of output-dependent thresholding in SHT, SUT and SBT are
entirely comparable to that of output-independent thresholding in SSV.
Note that, by Fig1, the difference of
performances between ridge regression methods and SVD regression methods
may not so clear when the number of training data is small.

(a) GPU dataset

(b) Energy dataset

In the next experiment, we see an effect of the number of unlabeled data
used in the methods. In Fig.2, we show the
deviations of test errors of RRO and SBT from test error of RR. We setunder. In
Fig.2(a) for GPU dataset, for both of RRO
and SBT, the deviations are almost unchanged and those are within the
error bars at any. This implies that there may be no advantage
of using unlabeled input samples. In
Fig.2(b) for Energy dataset, we can
clearly see the advantage of using unlabeled input samples for
RRO. Also, the average of deviation for SBT tends to decrease while it
is within the error bar. Therefore, the advantage of using SBT may not
so clear. By this result, for semi-supervised learning, our idea of
incorporating unlabeled input samples into a regressor can be effective
depending on datasets and regression methods.

SECTION: 5Conclusions and future works

In this paper, we proposed a SSL scheme using over-parameterized
regression method, in which kernels have information of both of labeled and
unlabeled samples. We then gave several methods based on the MNLS
approach for the over-parameterized regression problem.
We then examined the semi-supervised learning schemes using
over-parameterized regression methods for real data from UCI repository.
The results are summarized in the introduction.

There are several future works. Firstly, the effectiveness of introducing
over-parameterized regression methods into semi-supervised learning is found to
be dataset-dependent. Therefore, we need to clarify when ridge/SVD
regression methods are effective, by which we can choose an appropriate
method depending on datasets. Moreover, through this investigation, we
need to clarify whether information of unlabeled input samples is helpful
or not in our manner of semi-supervised learning. Next, in SVD
regression methods, output-dependent feature selection schemes via
thresholding methods were not so effective. And, they were worse
depending on datasets. There may be several reasons which are
computational problem of SVD components with small SVs, accuracy of
variance estimate in SUT and SBT, accuracy of cross validation choice
under SVD decomposition. We need to further investigate and refine the
thresholding methods.

SECTION: Acknowledgment

This work was supported by Japan Society for the Promotion of Science
(JSPS) KAKENHI Grant Number 21K12048.

SECTION: References