SECTION: FocDepthFormer: Transformer with latent LSTM forDepth Estimation from Focal Stack
Most existing methods for depth estimation from a focal stack of images employ convolutional neural networks (CNNs) using 2D or 3D convolutions over a fixed set of images. However, their effectiveness is constrained by the local properties of CNN kernels, which restricts them to process only focal stacks of fixed number of images during both training and inference. This limitation hampers their ability to generalize to stacks of arbitrary lengths. To overcome these limitations, we present a novel Transformer-based network, FocDepthFormer, which integrates a Transformer with an LSTM module and a CNN decoder. The Transformer’s self-attention mechanism allows for the learning of more informative spatial features by implicitly performing non-local cross-referencing. The LSTM module is designed to integrate representations across image stacks of varying lengths. Additionally, we employ multi-scale convolutional kernels in an early-stage encoder to capture low-level features at different degrees of focus/defocus. By incorporating the LSTM, FocDepthFormer can be pre-trained on large-scale monocular RGB depth estimation datasets, improving visual pattern learning and reducing reliance on difficult-to-obtain focal stack data. Extensive experiments on diverse focal stack benchmark datasets demonstrate that our model outperforms state-of-the-art approaches across multiple evaluation metrics.

SECTION: Introduction
With the advancement of deep neural networks (DNNs) and the availability of high-volume data, the challenging task of depth estimation from monocular imageshas seen significant success on benchmark datasets. However, the focal stack depth estimation problem, which is distinct from monocular depth estimation,,, stereo depth or disparity estimation, and multi-frame depth estimation, yet has not received as much attention in the research community.

DDepth estimation using focus and defocus techniques involves predicting the depth map from a capturedfocal stackof the scene, which consists of images taken at different focal planes. This problem, also known as depth of field control, typically uses images obtained with a light field camera. Traditional methodsrely on handcrafted sharpness features, but they frequently struggle in textureless scenes. To enhance feature extraction, Convolutional Neural Networks (CNNs) have been used to predict depth maps from focal stacks. Models like DDFFNet, AiFDepthNet, and DFVNetleverage in-focus cues, while DefocusNetlearns permutation invariant defocus cues, or Circle-of-Confusion (CoC). While these methods use 2D or 3D convolutions to represent visual and focal features, they are limited to processing focal stacks with a fixed number of images, which restricts their generalization to stacks of arbitrary length.

In this paper, we introduce FocDepthFormer, a novel network for depth estimation from focal stacks that combines LSTM and Transformer architectures. The core component is a module consisting of a Transformer encoder, an LSTM-based recurrent moduleapplied to latent tokens, and a CNN decoder. The Transformer and LSTM models process spatial and stack information separately. Unlike CNNs, which are restricted to local representation, the Transformer encoder captures visual features with a larger receptive field. Given that focal stacks may have arbitrary and unknown numbers of images, we use the LSTM in the latent feature space to fuse focusing information across the stack for depth prediction. This approach differs from existing focal stack depth estimation methodsand monocular depth estimation methods based on CNNs or Transformers, which typically handle inputs with a fixed number of images. Specifically, we compactly fuse activated token features via the recurrent LSTM module after the Transformer encoder, enabling the model to handle focal stacks of arbitrary lengths during both training and testing, thereby providing greater flexibility. Before inputting data into the Transformer, we employ an early-stage convolutional encoder with multi-scale kernelsto capture low-level focus/defocus features across different scales. Considering the limited availability of focal stack data, our model enhances its representation of scene features through pre-training on monocular depth estimation datasets. Meanwhile, the recurrent LSTM module facilitates the model’s ability to accommodate varying numbers of input images in focal stack.

The main contributions of this work are as follows:

We introduce a novel Transformer-based network model for depth estimation from focal stack images. The model uses a Vision Transformer encoder with self-attention to capture non-local spatial visual features, effectively representing sharpness and blur patterns. To accommodate an arbitrary number of input images, we incorporate a recurrent LSTM module. This structural flexibility allows for pre-training with monocular depth estimation datasets, thereby reducing the reliance on focal stack data.

To fuse the stack features, we utilize the LSTM and implement a grouping operation to manage recurrent complexity across tokens, avoiding an increase in complexity as the token count grows with larger stack sizes. This is accomplished by applying the LSTM exclusively to a subset of activated embedding tokens while maintaining the information on other non-activated tokens through averaging aggregation.

We propose the use of multi-scale kernels in an early-stage convolutional encoder to enhance the capture of low-level focus/defocus cues at various scales.

SECTION: Related work
Depth estimation from focal stacks relies on discerning relative sharpness within the stack of images for predicting depth. Traditional machine learning methodstreat this problem as an image filtering and stitching process. Johannsenprovide a comprehensive overview of methods addressing the challenges posed by light field cameras, laying a foundation for research in this direction. More recently, CNN-based approaches have emerged in the context of focal stacks. DDFFNetintroduces the first end-to-end learning model trained on the DDFF 12-Scene dataset. DFVNetutilizes the first-order derivative of volume features within the stack. AiFNetaims to bridge the gap between supervised and unsupervised methods, accommodating both ground truth depth and its absence. Barrattformulate the problem as an inverse optimization task, utilizing gradient descent search to simultaneously recover an all-in-focus image and depth map. DefocusNetexploits the Circle-of-Confusion, a defocus cue determined by focal plane depth, for generating intermediate defocus maps in the final depth estimation. Anwarleverage defocus cues to recover all-in-focus images by eliminating blur in a single image. Recently, the DEReD modellearns to estimate both depth and all-in-focus (AIF) images from focal stack images in a self-supervised way by incorporating the optical model to reconstruct defocus effects. Gur and Wolfpresent depth estimation from a single image by leveraging defocus cues to infer disparity from varying viewpoints.

The success of attention-based modelsin sequential tasks has led to the rise of the Vision Transformer for computer vision tasks. The Vision Transformer represents input images as a series of patches (). While this model performs well in image recognition compared to CNN-based models, a recent studydemonstrates that injecting a small convolutional inductive bias in early kernels significantly enhances the performance and stability of the Transformer encoder. In the context of depth estimation, Ranftlutilize a Transformer-based model as the backbone to generate tokens from images, assembling these tokens into an image-like representation at multiple scales. DepthFormermerges tokens at different layer levels to improve depth estimation performance. The latest advancement in this domain, the Swin Transformer, achieves a larger receptive field by shifting the attention window, revealing the promising potential of the Transformer model.

Recurrent networks, specifically LSTM, have found success in modeling temporal distributions for video tasks such as trackingand segmentation. The use of LSTM introduces minimal computational overhead, as demonstrated in SliceNet, where multi-scale features are fused for depth estimation from panoramic images. Some recent workscombine LSTM with Transformer for language understanding via long-range temporal attention.

SECTION: Method
Given a focal stack, containingimages ordered from near to far by focus distance, denoted as, whererepresents each single image, our objective is to generate a single depth mapfor a stack of images. In contrast to the vanilla Transformer, we initially encode each imageusing anearly-stage multi-scale kernel-based convolution. This convolution ensures the multi-scale feature representationfor the focal stack images. Subsequently, thetransformer encoderprocesses the feature maps by transforming them into a series of ordered tokens that share information through self-attention. The self-attention weights between the in-focus features and blur features, encode spatial feature information from each input image. Therecurrent LSTM modulesequentially processes cached latent tokens from different frames of a focal stack and fuses them along the stack dimension. The LSTM module learns the stack feature fusion process within the latent space.
Our attention design combined with LSTM enhances the model’s capability to handle an arbitrary number of input images. The final disparity map is decoded (denoted as) from the fusion features, utilizing the aggregated tokens from all images in a focal stack.

SECTION: Early-stage encoding with multi-scale kernels
To capture low-level focus and defocus features at different scales, we employ an early-stage convolutional encoder with multi-scale kernels, which is different from methods using fixed size kernel convolution stem before the Transformer. As illustrated in Fig., the early-stage encoder utilizes three convolutional kernels to generate multi-scale feature maps. All feature maps are concatenated and merged into the feature mapthrough spatial convolution, followed byandconvolution on the feature map depth channel:

whereranges fromto. Feature concatenation after convolutions with multiple kernel sizes preserves fine-grained details of features across varying depth scales. The first module from the left in Fig., comprising parallel multi-scale kernel convolutions followed by depth-wise convolution, ensures the model has a large receptive field even beyond thekernel size. This facilitates capturing more defocus features while preserving intricate details.

SECTION: Transformer with LSTM
The Transformer depicted in Fig., denoted as, operates on the feature mapsderived from preceding early-stage multi-scale convolutions to produce a sequence of tokens:

Specifically, the early-stage kernel CNNs and Transformer encoder sequentially process the focal stack images, caching and concatenating the feature maps of a specified stack of images into. Initially, a linear embedding layer divides the feature mapsintopatches of size. Thus,, is projected by a linear embedding layer (MLP) into corresponding embedding tokens, each token having a dimension of(576 in total). All tokens of a complete stackare cached intobefore LSTM for simultaneous fusion. The Transformer’sPosition Embeddingencodes the positional information of image patches in a sequential order from the top-left of the image. An MLP layer generates the Global Embedding Token (Fig.) by mapping the entire image into a global token and subsequently adding each individual patch embedding token. Each linear embedding token is projected into three vectors - Query,; Key,; and Value,via a weight matrixwith dimensionsandrespectively. Queries, Keys, and Values are processed in parallel through Multi-Head Attention (MHA) units.

where,,, and. Following the Multi-Head-Attention modules within the encoder, the resulting tokenscapture features that distinguish focus and defocus cues among different stack image patches at the same spatial location within the image. This capability is illustrated in Fig.. Consequently, the embedding space emphasizes sharper, more in-focus features of the image patches.

To ensure our model’s flexibility in handling stacks of arbitrary lengths, in contrast to the fixed lengths used in existing methods, we employ an LSTM to progressively integrate sharp features across the stack. The LSTM treats patch embedding tokens at the same image position as sequential features along the stack dimension. Corresponding feature tokensfrom stack images at stack numberare sequentially ordered spatially and fed into LSTM modules arranged in the original spatial image order. At each position, each LSTM module incrementally integrates the latent tokenfrom a stack. This approach differs from existing models constrained to a 3D volume stack with a predefined and fixed size. Importantly, sequential processing in the latent space after a shared encoder for stack images ensures manageable complexity in practice.

Preceding the LSTM modules, tokens associated with image patches from a single frame are classified into activated and non-activated tokens, indicating the level of informativeness of the features. Thenorm, denoted as, of each embedding token is compared against a threshold of, as depicted in Fig.. Specifically, within a single frame, only tokens surpassing this threshold are considered activated and forwarded to the LSTM, totalingtokens. This approach significantly reduces the computational load of the LSTM by processing only a subset of the latent tokens:

this is a single LSTM layer expression above, where, andis the frame index number of a stack. Here we set the number of hidden layers of the LSTM equal to the stack size.
The memory cellundergoes continuous updates at each step, influenced by the inputand the hidden state. Subsequently, all LSTM layer outputs are combined via max poolingto obtain. For non-activated tokens, an averaging operation is performed with corresponding cached tokens from the previous step at the same embedding position. Finally, the two groups of output tokens are arranged in the original input embedding order, yielding the final fused tokens.

Our decoderadopts the methodology introduced by Ranftl, utilizing Transpose-convolutions to integrate feature maps following LSTMs. Additionally, the decoder incorporates feature maps from the-th layerof the encoder through skip connections, as illustrated in Fig.. Ultimately, decoderoutputs the depth prediction.

SECTION: Training loss
Our training loss comprises a sum of the Mean Squared Error (MSE) loss, denoted as, and a sharpness regularizerweighted by:

whererepresents the ground truth depth, andindicates the predicted depth.denotes the Laplacian operator applied to the predicted and ground truth depth images, respectively.represents the variance of the depth image. The regularization term is formulated as. Pixel blurriness due to out-of-focus effects can be described by the Circle-of-Confusion (CoC),

wheredenotes the-number, defined as the ratio of the focal length to the effective aperture diameter, andis the CMOS pixel size.denotes the focus distance of the lens, andrepresents the distance from the lens to the target object. Generally, the range ofis, though in practice, it is always bounded by lower and upper limits. The model aims to learn a depth map from focus and defocus features of stack images.

SECTION: Pre-training with monocular depth prior
Focal stack datasets are often limited in size due to the high cost and challenges involved in data collection. To address data scarcity and fully exploit the Transformer’s potential, we optionally pre-train the Transformer encoder on widely available monocular depth dataset like NYUv2, to enhance spatial representation learning further, yet without pre-training, the model performance is still superior over baseline models, as exhibited in following experiment section.

SECTION: Experiments
We extensively evaluated our model using four benchmark focal stack datasets: DDFF 12-Scene, Mobile Depth, LightField4D, and FOD500(Synthetic dataset). As Mobile Depth has no depth ground truth, so only visual comparison results are provided. Additionally, our model supports pre-training on the monocular RGB-D dataset NYUv2. Specifically, we conducted separate training on DDFF 12-Scene and FOD500 for subsequent experiments, while Mobile Depth and LightField4D were used to assess the model’s generalizability directly after pre-training on DDFF 12-Scene. Qualitative and quantitative evaluation on FOD500, and quantitative metric evaluation of LightField4D are provided in the supplementary part. Additionally, the more qualitative results of each dataset are available in the supplementary part, please refer to it. Finally, a comprehensive summary of the evaluation datasets, including their individual properties (real or synthetic), with or without the depth ground truth is presented in Tab., along with defocus cause.

For the evaluation on DDFF 12-Scene, we conducted training experiments using our model with and without pre-training on NYUv2, presenting results for both scenarios. We employed a patch size ofand an image size offor the Transformer. Our network utilizes the Adam optimizer with a learning rate ofand a momentum of 0.9. The regularization scalarin Eq. () is set to 0.2. In terms of hardware configuration, all training and tests below were conducted on a single Nvidia RTX 2070 GPU with 8GB of vRAM.

In this work, we perform quantitative evaluation using the following metrics: Root Mean Squared Error (RSME), logarithmic Root Mean Squared Error (logRSME), relative absolute error (absRel), relative squared error (sqrRel), Bumpiness (Bump), and accuracy threshold at three levels (,, and).

We evaluated the runtime of the proposed method and baseline approaches by executing them on focal stacks from DDFF 12-Scene. Our FocDepthFormer processes a stack of 10 images sequentially in 15ms, averaging 2ms per image. In comparison, DDFFNetrequires 200ms per stack under the same conditions, and DFVNetperforms in the range of 20-30ms.

SECTION: Comparisons to the state-of-the-art methods
DDFFNetand DefocusNetlacked pre-trained weights; therefore, we utilized their open-source codebases to train the networks from scratch. Notably, DefocusNetoffers two architectures, and we chose the "PoolAE" architecture due to its consistently good performance for comparison. Conversely, for AiFNetand DFVNet, we employed the pre-trained weights provided by the authors to conduct the evaluations. We also provide the visual results of the latest DEReDmodel in supplementary part, as the publicly available code is not complete for implementation.

Tab.presents the quantitative evaluation results of our model on the DDFF 12-Scene dataset. As the ground truth for the "test set" is not publicly available, we adhere to the standard evaluation protocol used in other comparative works, assessing the models on the "validation set" as per the split provided by DDFFNet. Moreover, we demonstrate that our model, trained on DDFF-12, performs robustly on other completely unseen datasets, highlighting its generalization ability and mitigating concerns of overfitting. Furthermore, the qualitative results are provided in Fig..

We provide the qualitative results on Mobile Depth in Fig., and the depth is not available for this dataset for metric evaluation. Regarding LightField4D, quantitative and qualitative results are provided in Tab.and Fig.respectively. Tab.presents the quantitative evaluation of our model on the synthetic FOD500 dtaset and LightField4D. For FOD500, we use the last 100 image stacks from the dataset for test, while the initial 400 image stacks are reserved for training.
DDFFNet and DefocusNet are re-trained on FOD500 from scratch. The results highlight the consistent superiority of our model across all metrics when compared to the baseline methods. Notably, in our experiments, we observed that pre-training on NYUv2 did not provide significant benefits, likely due to the gap between synthetic and real data.

We present the quantitative results for cross-dataset evaluation of our model on the LightField4D dataset in Tab.. Our model achieves a comparable performance in terms of accuracy (%) on this completely unseen dataset.

SECTION: Cross dataset evaluation
To evaluate the generalizability of our model, it is initially trained on DDFF 12-Scene and subsequently evaluated on the Mobile Depth and LightField4D datasets. The Mobile Depth dataset poses a challenge as it comprises 11 aligned focal stacks captured by a mobile phone camera, each with varying numbers of focal planes and lacking ground truth. Results on the Mobile Depth dataset are illustrated in Fig., showcasing the model’s ability to preserve sharp information for depth prediction in complex scenes.

SECTION: Ablation study
We conduct a comprehensive ablation study to evaluate the key components of our proposed model architecture. Additional ablation experiments are detailed in the supplementary materials.

Tab.presents a comparison of different early CNN kernel design configurations in conjunction with our Transformer encoder (ViT). The effectiveness of the proposedearly-stage multi-scale kernelsencoder is evident, demonstrating robust performance. Conversely, omitting multi-scale kernels or forgoing subsequent convolutions after in-parallel convolutions leads to a degradation in model performance.

Our proposed LSTM-based method exhibits flexibility in processing focal stacks of arbitrary lengths, a feature distinguishing it from designs that limit inputs to fixed lengths. To illustrate the advantages of our LSTM-based model, we conducted experiments using DDFF 12-Scene. The results, including the RMSE comparison between our model and DFVNet, are presented in Tab.. Initially, we trained our model and DFVNet using 10-frame (10F) stacks (Ours-10F and DFVNet-10F). During testing, DFVNet-10F is constrained and cannot process stacks with fewer than 10 frames. Due to fixed stack size requirements during training and testing, DFVNet must be retrained for different stack sizes (DFVNet-#F). In contrast, our model is trained once on 10-frame stacks and can be tested with varying numbers of stack images. The focal stack images are ordered based on focus distances. Despite our model’s initial performance being inferior to DFVNet, the learning curve of LSTM indicates rapid convergence (Fig.).

The LSTM module enables our network to incrementally fuse each image from the focal stack, enhancing the model ability to accommodate varying focal stack lengths. Fig.depicts the fusion process of the ordered input images of one stack. As the images from the stack are given sequentially, starting from the in-focus plane close to the camera, the model can fuse the sharp in-focus features from a sweep of various frames to attain a final all-in-focus prediction depth map at the bottom right, form near to far focus distances.

Our network comprises the early-stage multi-scale CNN, the Transformer, the LSTM module, and the decoder. We present a summary of each module parameter size, and the inference time,

From Tab., we can see the main time consumption is allocated on the Transformer module and decoder, which shows the potential to reduce the model size further can be achieved,e.g., by using MobileViT as an encoder. Our proposed LSTM over the latent feature representation, has a size of only around one-third of Transformer encoder, furthermore, the shallow early multi-scale kernel CNNs is in quite a small size with only 0.487 million parameters and fast processing time around 1ms. The time model size and time complexity summary table further justify our model’s compact and efficient design, where the recurrent LSTM module has benefits both in memory size and computational complexity in the design. The main inference time for a single image processing is from the Transformer Encoder, which can be attributed mainly to the attention computation of multiple self-attention heads, and CNN decoder.

Tab.presents the results obtained using three distinct loss functions: Mean Squared Error (MSE), Mean Absolute Error (MAE), and Regularized MSE (MSE with gradient regularization). The findings consistently show that MSE outperforms MAE in terms of overall performance. Notably, incorporating the gradient regularizer contributes to achieving the highest accuracy, as evidenced by the bumpiness metric ().

Tab.illustrates the impact of pre-training on the performance of our proposed method. The results demonstrate that pre-training enhances the model’s capabilities, leveraging the compact design with Transformer and LSTM modules. Even without pre-training, our model achieves competitive results. Notably, attempts to apply pre-training to DFVNet using a stack created from repeated monocular images of NYUv2 did not yield improvements and, in some cases, led to performance degradation due to the data modality gap.

SECTION: Conclusion
We introduce the FocDepthFormer model tailored for depth estimation from focal stack. At the core of our network is a Transformer encoder coupled with a recurrent LSTM module in the latent space, allowing the model to effectively capture spatial and stack information independently. Our approach demonstrates flexibility in accommodating varying focal stack lengths. A significant drawback lies in the heightened model complexity associated with the vanilla Transformer architecture. More efficient attention design techniques like Mamba can be explored as the future work.

SECTION: References