SECTION: BEExformer: A Fast Inferencing Transformer Architecture via Binarization with Multiple Early Exits

Large Language Models (LLMs) based on transformers achieve cutting-edge results on a variety of applications. However, their enormous size and processing requirements make deployment on devices with constrained resources extremely difficult. Among various efficiency considerations, model binarization and Early Exit (EE) are common effective solutions. However, binarization may lead to performance loss due to reduced precision affecting gradient estimation and parameter updates. Besides, the present early-exit mechanisms are still in the nascent stages of research. To ameliorate these issues, we propose Binarized Early Exit Transformer (BEExformer), the first-ever selective learning transformer architecture to combine early exit with binarization for textual inference. It improves the binarization process through a differentiable second-order approximation to the impulse function. This enables gradient computation concerning both the sign as well as the magnitude of the weights. In contrast to absolute threshold-based EE, the proposed EE mechanism hinges on fractional reduction in entropy among intermediate transformer blocks with soft-routing loss estimation. While binarization results in 18.44 times reduction in model size, early exit reduces the FLOPs during inference by 54.85% and even improves accuracy by 5.98% through resolving the "overthinking" problem inherent in deep networks. Moreover, the proposed BEExformer simplifies training by not requiring knowledge distillation from a full-precision LLM. Extensive evaluation on the GLUE dataset and comparison with the SOTA works showcase its pareto-optimal performance-efficiency trade-off.

KeywordsEarly Exit (EE)Binarized Neural Network (BNN)Selective Learn-Forget Network (SLFN)Textual InferenceTransformers

SECTION: 1Introduction

The last decade has witnessed substantial progress in the field of Natural Language Processing (NLP). Current state-of-the-art (SOTA) comprises pre-trained Large Language Models (LLMs) involving transformers. However, such models have exorbitant computational complexity, which limits their applications to resource-constrained setups[30]. This necessitates the need for efficient modeling practices. Efficient modeling reduces memory as well as computational power requirements. It also lowers the training and inference time of the model. Moreover, it opens the door to deployment on edge devices[1].

Various modeling aspects have been considered towards enhancing the efficiency of models, including pruning[2], knowledge distillation[3], low-rank approximation[4], and quantization[5]. Out of these, quantization has become quintessential for deploying NLP models on edge devices[6]. While the majority of efficiency considerations target reduction in the number of layers, nodes, or parameters, quantization targets efficiency by reducing the precision of the model’s memory representation. In neural networks, quantization can be segregated as Post Training Quantization (PTQ)[7,8,9]and Quantization Aware Training (QAT)[10,11]. In PTQ, the model is quantized post-training, hence its name. It can be static, wherein both weights as well as activation are quantized[8]. Otherwise, it can be dynamic, i.e. weights are quantized before and activations are quantized dynamically at inference[9]. While QAT integrates quantization during the training process itself. Here, quantization is performed on the weights and/or activations during the forward pass. Whereas it uses pseudo gradients during backpropagation to deal with the zero gradient problem due to binarization. This is accomplished using full-precision latent weights. These latent weights are actually quantized to get the actual weights of the QNN[11]. Existing quantization techniques convert to reduced precision floating-point (FP) or integer (Int) notations like FP16, FP8, Int8, Int4, and binary (1-bit). The class of neural networks employing 1-bit quantization is referred to as Binarized Neural Networks (BNN). It is the extreme form of quantization through which the memory consumed to store the weights and activations is 32x lower compared to full-precision FP32 notation[12].

Targeting only one aspect of achieving efficiency does not suffice. Besides, dynamic variation in model architecture helps to customize processing and adjust computational requirements based on input during inference. The current LLMs are composed of several layers of transformer blocks stacked together. Conventionally, every input sequence undergoes processing through all the layers to generate the final inference. However, heterogeneity can be observed in the inputs due to varying sequence lengths as well as sentence complexities[13]. Given a network, the lower layers capture the shallow features while more intricate features are captured by higher layers. For some inputs, the features from the lower or intermediate layers might render predictions beyond a certain level of confidence. While some complex inputs might require processing through all the layers. Thus, allowing an exit from certain break-points based on certain conditions can significantly bring down computation[13,14]. Additionally, early exit (EE) provides a remedy to the "overthinking"[15,13,16]problem in LLMs having millions and even billions of parameters. In such models, while predictions based on lower layers are correct, they might become incorrect after undergoing transformations in the higher layers due to overcomplicated features. These unnecessary features lead to wasteful computation or “overthinking.” Thus, it is observed that EE not only enhances the efficiency, but it can even improve efficacy[14].

Even though BNN seems to be a promising efficiency enhancement approach, a handful of research works have only been able to effectively implement it in the field of NLP[17,18,19]. Most of them have attempted BNN through knowledge distillation from a full-precision LLM like BERT[17,18]. Even though knowledge distillation leads to a lightweight model, the training process itself is computationally as well as memory intensive[1,20]. Also, none of the works have attempted to devise a transformer architecture from scratch through BNN. Similarly, previous research on EE mechanisms for NLP has been on models with weights initialized from a pre-trained LLM. The EE criterion in the majority of works is a set threshold that needs to be calibrated based on the task as well as the input distribution. This leads to faltering efficacy when inputs deviate during inference. Furthermore, none of the previous works have attempted to combine BNN with the EE mechanism utilizing transformers for textual inference.

In this paper, we propose a first-of-its-kind architecture for textual inference having multiple binarized transformer blocks with selective learning capabilities stacked upon each other interleaved with decision blocks for EE. The binarization technique involves an intuitive Binarization-Aware Training (BAT) mechanism utilizing real-valued latent weights for gradient updates. We apply a piecewise polynomial binarization function closely approximating the non-differentiable impulse function having a piecewise linear function as its derivative. This ensures the gradient is aware of the sign and the magnitude of the weight values. For EE, we estimate the fractional change in entropy of the logits after each transformer block during inference. If the entropy does not reduce beyond a threshold percentage, the exit condition is satisfied and the final logits are returned. This aids in the reduction of computation by eliminating the execution of subsequent blocks beyond the exit point. Additionally, it resolves the performance loss due to the "overthinking" problem inherent in complex networks. During training, we apply soft-routing loss, combining the loss from all exits to effectively update the weights and enhance the decision-making capability of each transformer block. Moreover, the transformer has a binarized version of the Selective Learn-Forget Network (SLFN)[21]integrated into it, leading to the elimination of insignificant information and better comprehension of long-range dependencies. The proposed model learns its parameters from scratch. This reduces any additional computation and memory requirement associated with using a full-precision LLM as a starting point. Despite this, the proposed architecture exhibits pareto-optimal performance-to-complexity trade-off compared to the SOTA on the GLUE benchmark evaluation on tasks like SST-2, CoLA, MRPC, and RTE. The principal contributions of this paper are as follows:

We binarize both weights as well as activations of the transformer architecture through a piecewise approximation to the impulse function to make it differentiable and ensure gradient-based weight updates consider both the magnitude as well as the sign of the weights.

We devise an EE mechanism on the basis of fractional changes in entropy of the logits between subsequent transformer blocks. This alleviates the need to set an absolute threshold entropy value besides resolving the "overthinking" problem.

Additionally, a binarized SLFN is integrated in each transformer block to enable selective learning with the elimination of insignificant information. This is accompanied by a soft-routing loss estimation during training to enhance the decision-making capability of each transformer block.

The compendium to this paper has been presented herein. Section2gives an overview of the related works in the domain. Section3describes the proposed BEExformer architecture. Section4contains the experiment setup details along with data-set information. Section5presents the results obtained through extensive evaluation on multiple tasks. Finally, the conclusions are drawn in Section6.

SECTION: 2Related Works

In this section, a commentary on the related works has been presented. Since the proposed BEExformer is the first model combining binarization with EE for textual inference, we separately review works implementing BNN and EE for textual content.

SECTION: 2.1Binarized Neural Networks (BNN)

Bai et al.[17]implemented weight binarization in a BERT model. To tackle sharp loss in performance, they apply knowledge distillation along with ternary-weight splitting to initialize their model. However, they experience hindrances in binarizing activations and were not able to implement a fully binarized network due to a sharp performance drop due to loss of precision. Qin et al.[18]proposed a fully binarized BERT model applying binarization subject to information entropy maximization along with direction-matching distillation inferring from similarity matrices to deal with direction mismatch. Liu et al.[10]formulated a closer approximation to the impulse function and updated the weights based on a magnitude-aware gradient. Liu et al.[19]further proposed a multi-step distillation approach to gradually distill the model into lower precision before obtaining the final BNN from a full-precision BERT model. They utilize an elastic binary activation with parameters inferred while training. All of the above-mentioned works adopt knowledge distillation from a full-precision model to counter performance loss. However, executing both student and teacher models simultaneously can be memory-intensive. Furthermore, additional fine-tuning on downstream tasks is often required for optimal results. been presented. Since the proposed BEExformer is the first model combining binarization with EE for textual inference, we separately review works implementing BNN and EE for textual content.

SECTION: 2.2Early Exit (EE)

Zhou et al.[22]developed an early-exit mechanism with the layers of a Pre-trained Language Model (PLM) interlaced with internal classifiers to exit from the network. They determined the change in predictions of successive internal classifiers, and the exit criterion was fulfilled if no change could be observed for a given number of steps. In the same context, Xin et al.[15]calculated the entropy of predictions from the internal classifiers, and if it fell short of a predetermined threshold, the exit criterion was fulfilled. However, determining the optimal threshold value is a daunting task. Liu et al.[23]put forth a pre-trained transformer model with dynamic exits having loss calculated at each layer as the sum of Masked-Language Modeling (MLM) and Sentence-Order Prediction (SOP) losses. Mangrulkar et al.[13]proposed a modified version of the switch transformer, formulating a switch layer to assess the complexity of a sentence and optimally route it to an expert model having fewer layers. Here, the expert models comprise a set of BERT models with a varying number of encoder layers followed by a prediction layer. Here, expert routing augments the model complexity, translating into additional resource requirements. Moreover, the effectiveness of expert routing hinges on the capability of the experts to be specialists for varying input patterns.

SECTION: 3Proposed Architecture

The proposed BEExformer architecture embeds an input sequence and processes it through a cascade of binarized transformer blocks with the provision to exit early after each block. Each transformer block possesses binarized multi-head attention (MHA) with parameters updated via both sign as well as magnitude-aware gradient. Additionally, a selective learn-forget network binarized in a similar manner replaces the feed-forward layer in conventional transformer architecture for precise estimation of context. For EE, an intuitive criterion monitoring the fractional reduction in entropy between consecutive transformer blocks has been devised. Once the exit condition is satisfied, the processing is routed to an auxiliary block comprising binarized feed-forward layers to obtain the final predictions. This enables a two-fold efficiency enhancement due to a binarized architecture along with a provision to reduce the amount of computation dynamically during inference by terminating before the execution of all transformer blocks. The proposed architecture has been illustrated in Figure1, while its components have been elucidated herein-below.

SECTION: 3.1Input Representation

The input sequences in a given corpus are at first tokenized, and a lookup tableis constituted as a one-to-one mapping between the unique tokensin the vocabularywith a unique integer,. For each sentence, a tokenized sequenceis constructed consulting the lookup table as shown in Algorithm1. Besides, padding is appended to ensure uniform sequence length.

The tokenized sequenceis converted into a series of embeddings, where each tokenin the sequence is substituted by an embedding vector. The embeddings are randomly initialized and optimized throughout training, much like other network parameters. Position embeddingis used on the sequence to determine the token ordering for contextualized representation. For a given positionand dimension,is computed as shown in equation (1).

Finally, the token embedding sequenceand the position-encoded sequenceare element-wise concatenated (denoted by) to obtain the embedded sequencesuch thatas shown in equation (3).

SECTION: 3.2Overall Architecture

The backbone network consists of a sequence oftransformer blocks with an exit provision after each block as portrayed in Figure1(a). It can be functionally represented astaking inputand having a cascade ofdifferentiable functions (each representing a transformer block) as follows:

Where,denotes composition operator. The hidden representations from theintermediate block is given as:

To accomplish the objective of EE, a criterion is defined. In case the criterion is fulfilled at exit, the logitsreturned by theexit are fed to its connected auxiliary blockand the probability distributionis returned. This has been explained in Section3.4.

Additionally, the BEExformer utilizes BAT (described in Section3.3) to binarize the weights and activations of all the transformer blocks as well as the auxiliary blocks for EE. Generically, a binarized layer is defined as follows:

Where,,,,,,anddenote the computed activation, activation function, layer operator, binarization function, weights, incoming activation, and bias, respectively.

SECTION: 3.3Binarization

Conventionally for binarization,(equation (7)) is applied. However, being not differentiable, it cannot be applied to compute the gradient. To ameliorate this, we apply magnitude-aware approximation ofas in Bi-Real Net[10]for the first time to the best of our knowledge in a transformer encoder. Denoted by, this approximation is expressed in equation (8). From the comparison betweenandalong with their derivatives in Figure2, it can be seen thatclosely approximatesequipped with the capability of being differentiable. Given, its derivative is computed as in equation (9).

whereis a differentiable piecewise polynomial function being a second-order approximation of the impulse function. Whileis a piecewise linear function approximating the derivative ofduring gradient calculation.

Conventional gradient descent cannot be applied to BNN due to the gradient of lossconcerning binarized weights being insufficient to enable bit-flips. To mitigate this issue, we perform BAT wherein the gradient computed for theblock concerning binarized weightsfor theiteration is used to update real-valued weightsfor theiteration. This has been shown in the following equation:

where,denotes element-wise differentiation anddenotes the learning rate. Finally, the updated binarized weightsare frozen asapplying equation (7). The details of implementation of the above-mentioned binarization technique in the transformer blocks are presented in Section3.5.

SECTION: 3.4Early Exit

We propose an EE criterion that estimates proportionate entropy reduction over exits, i.e. fractional reduction in entropy concerning the previous exit. When this value falls below a certain threshold, the exit criterion is satisfied. This has been portrayed in Algorithm2. Here, the entropy of logitsis computed as in equations12and13. Initially, the value ofis taken as, which signifies the case when logits are spread as evenly as possible acrossclasses.

Following this, an auxiliary blockis defined to processand predict the outputas follows:

Here,belongs to the set of binarized feed-forward networksas shown in Figure1(d). It processesinto a discrete probability distributionas the output for theexit. To ensure minimal computational overhead,has a minimal number of parameters compared to the backbone network.

The proposed EE criterion is robust against diversity in inputs during inference as only the fractional reduction in entropy is noted. This alleviates the limitation of previous works with absolute entropy thresholds to deal with diverse inputs. Also, determining the absolute threshold is a cumbersome process in such models. Compared to it, settingis pretty straightforward based on the computational budget and output confidence requirements. Moreover, the same value ofcan be used across multiple tasks.

SECTION: 3.5Transformer Blocks

Each transformer block binarizes the multi-head attention (MHA)module put forth by Vaswani, A.[24]as depicted in Figure1(b).is comprised ofbinarized self-attentionheads, which in turn relies on queries, keys, and valuesfor computation as follows:

where,, andhave been obtained through binarized-linear (bi-linear) transformation (explained in Section3.3) on inputwheredenotes the sequence length anddenotes the dimension of hidden representations. The computations have been shown in equations (16-18).

where. Finally,is calculated in equation (19) through projection ofconcatenatedthroughafter binarization.

To augment the efficacy of residual connection, a binarized SLFN is used to replace the feed-forward layer in the conventional transformer architecture. This is the modified version of SLFN proposed by Ansar et al.[21]. In SLFN, there are two forget gates,and, which aid in the elimination of insignificant information in the previous hidden state and the total incoming activations, respectively. Besides, it has a selective learning gatefor rational estimation of dependencies. All the gates incorporate binarization of incoming activations as shown in equation (8). The architecture has been illustrated in Figure1(c), while the formulation of the gates has been provided in the following equations.

whereanddenote full-precision input atstep andhidden state, respectively. Whereas,,, anddenote weight matrices for inputs as well as hidden states, respectively. Here, we applyon all incoming weights and activations for binarization. Finally, the updated hidden state is calculated as follows:

SECTION: 3.6Loss Function

The loss function L calculated at the ith exit is given by the following equation:

Whereis the input data containing sequence-label pairs,,is the probability distribution returned by theexit,denotes trainable parameters, andis the cross-entropy function. The proposed model uses a soft-routing loss objective, i.e. combining loss from all exit layers to enhance the decision capability. This aids in optimal assessment of the exit based on the predictions from all the exit blocks while reducing the loss. The total lossis calculated as the mean loss across all exits given by:

where C is the total no. of exits.

SECTION: 4Experimental Setup

In this section the details of the experiment conducted along with the data-set information have been provided.

SECTION: 4.1Data-Sets

The proposed methodology has been tested on various data-sets from the GLUE benchmark[25], such as Stanford Sentiment Treebank (SST-2)[26], Corpus of Linguistic Acceptability (CoLA)[27], Microsoft Research Paraphrase Corpus (MRPC)[28], and Recognizing Textual Entailment (RTE)[25]. These data-sets cover a wide range of tasks such as sentiment analysis, linguistic acceptability, semantic similarity between texts, paraphrase detection, and entailment detection. The metrics for evaluation are F1 score for MRPC, Matthews correlation for CoLA, and accuracy for the remaining tasks.

SECTION: 4.2Implementation Details

The proposed BEExformer has been implemented on a Python 3 Compute Engine with the Nvidia L4 GPU, 22.5GB VRAM, 53GB System RAM, and 201.2GB Disk in Google Colab111https://colab.research.google.com. The details of the hyperparameters of the BEExformer have been presented in Table1.

SECTION: 5Results and Discussion

SECTION: 5.1Comparison with Related Works

To the best of our knowledge, combining binarization with early-exits is unprecedented for textual content. Thus, we compare with related works on solely focused on binarization as well as EE in neural networks for NLP. For ease of comprehension, the results of works on binarization and early exiting have been demarcated. This is followed by results from the proposed BEExformer and its ablations. Table2presents the comparison with related works on the basis of model precision, size and performance metrics for various tasks. Among the quantized models, BEExformer excels over the binarized models on all tasks. It even gives better results than the 8-bit quantized versions on some tasks. During comparison with full-precision EE models, it can be noticed that despite having 46 times lower model sizes on an average, the BEExformer gives comparable performance.

SECTION: 5.2Pareto Optimality

Figure3gives a clear picture of the pareto-optimality in terms of performance versus efficiency trade-off. Here, the proposed BEExformer gives the Pareto-optimal solution for both comparisons with quantized models as well as EE models. This can be attributed to the effectiveness of the proposed BAT approach along with the SLFN-based transformer blocks, which are potent in selectively capturing significant context in sequences. Moreover, the EE mechanism solves the "overthinking" problem by providing correct inference with minimal processing. This translates into reduced latency during inference.

SECTION: 5.3Study of Model Ablations

Table2also presents the results from various ablations of the proposed BEExformer. Here, BEExformer (WEE) without EE is the most lightweight version, while the full precision version, i.e. BEExformer (FP), gives the best results among the ablations. Despite being 18.44 times smaller than BEExformer (FP), i.e. the full-precision version with EE, the proposed BEExformer experiences a minimal performance drop of 3.58% across tasks. However, the proposed BEExformer leads over the full-precision version when EE is not applied, i.e. BEExformer (WEE-FP) with a 2.66% improvement in spite of being 7.18 times smaller in size. Additionally, the difference in performance between BEExformer (FP) and BEExformer (WEE-FP) is 6.24%, together with a difference of 5.98% between the proposed BEExformer and BEExformer (WEE). This highlights the efficacy of the EE mechanism in countering the effect of "overthinking" on performance. Given the performance versus efficiency trade-off, the proposed BEExformer gives the best results compared to all its ablations.

SECTION: 5.4Distribution of Exit Points

For a deeper insight, we plot the exit point distribution along with the number of parameters saved, i.e. not computed due to EE during inference over all the tasks in Figure4. It appears that the majority of the observations exit after the second transformer block, while negligible samples need to go through the last transformer block. This highlights savings in terms of compute during inference compared to traditional models, where all samples get processed by all the layers in the neural network. The total number of parameters saved from computation during inference is proportional to how early the exit condition is satisfied. Its maximum effect is noticed at the second exit and starts decreasing thereafter. For subsequent exits, the plots indicate that despite having a higher frequency in some cases, their impact on saving parameters might not be that pronounced. An interesting observation is that none of the samples exit from the first block during inference. It can be attributed to the substantial reduction in entropy from the initialized valueafter the execution through the first transformer block. It exhibits that each transformer block possesses the potency to extract substantial information from the input and make inferences based on it. Furthermore, Table3presents the percentage reduction in overall FLOPs during inference over all tasks compared to the variant without EE. On average, the EE mechanism in BEExformer saves around 54.85% FLOPs during inference accompanied with 5.98% increase in performance. It affirms our motivation behind EE to utilize fewer transformer blocks than present in the architecture during inference.

SECTION: 5.5Effect of EE Threshold

We observe that the EE threshold () plays a pivotal role in determining the trade-off between efficacy of results and reduction in FLOPs during inference. Figure5shows comparison of performance of the value ofproposed in this paper, i.e. 0.0001 with. Although largervalue reduce the inference time with 4.59% average reduction in FLOPs, the confidence in results decreases translating into a drop in performance by 3.53%. Alternatively, for smaller values of, the confidence in outputs increases with a slight delay in inference. Thus, settingproves to be a win-win situation with a rise in performance along with reduced FLOPs for inference. It tackles the case of "overthinking" wherein the entropy of logits starts rising instead of decreasing. On the other hand, EE lets the inputs with even a slight reduction in entropy to be processed further while terminating execution otherwise.

SECTION: 6Conclusion

In this paper, BEExformer—a binarized transformer architecture with selective learning and EE for textual content—has been proposed. It incorporates a differentiable piecewise approximation to binarization during the training process to ensure gradient computation takes into account both the magnitude and the sign of real-valued weights to update the final binarized weights. This enables a manifold reduction in memory requirements with performance at par to full-precision models, enabling deployment on resource-constrained edge devices. Furthermore, it has fast inferencing capability, allowing it to exit from intermediate transformer blocks based on an intuitive technique monitoring entropy changes in logits. This reduces the overall FLOPs during inference, translating into reduced latency. Besides, the EE offers a solution to the "overthinking" problem intrinsic to LLMs. Extensive evaluation on a range of tasks in the GLUE data-set, showcases its ability to deliver pareto-optimal results concerning both efficacy as well as efficiency. However, the proposed architecture has certain limitations too. The current version is based on the transformer encoder and is limited to inferencing tasks only. Additionally, being a dynamic architecture, it is difficult to predict the inference time and power consumption once the model is deployed. In the future, we aim to overcome these issues by exploring how to modify the proposed BEExformer architecture to accomplish generative tasks. Furthermore, a procedure can be devised to accurately predict the power consumption and time taken for inference for a given input.

SECTION: Acknowledgments

None. The author(s) received no financial or any other kinds of support for the research, authorship, and/or publication of this article.

SECTION: References