SECTION: Analysis of Diagnostics (Part II): Prevalence, Linear Independence, & Unsupervised Learning

This is the second manuscript in a two-part series that uses diagnostic testing to understand the connection between prevalence (i.e. number of elements in a class), uncertainty quantification (UQ), and classification theory. Part I considered the context of supervised machine learning (ML) and established a duality between prevalence and the concept of relative conditional probability. The key idea of that analysis was to train afamily of discriminative classifiersby minimizing a sum of prevalence-weighted empirical risk functions. The resulting outputs can be interpreted as relative probability level-sets, which thereby yield uncertainty estimates in the class labels. This procedure also demonstrated that certain discriminative and generative ML models are equivalent. Part II considers the extent to which these results can be extended to tasks in unsupervised learning through recourse to ideas in linear algebra. We first observe that the distribution of animpure population, for which the class of a corresponding sample is unknown, can be parameterized in terms of a prevalence. This motivates us to introduce the concept oflinearly independent populations, which have different but unknown prevalence values. Using this, we identify an isomorphism between classifiers defined in terms of impure and pure populations. In certain cases, this also leads to a nonlinear system of equations whose solution yields the prevalence values of the linearly independent populations, fully realizing unsupervised learning as a generalization of supervised learning. We illustrate our methods in the context of synthetic data and a research-use-only SARS-CoV-2 enzyme-linked immunosorbent assay (ELISA).

SECTION: 0Appeal to the Reader

This is the second manuscript in a two-part series that uses examples and ideas from diagnostics to study the relationship between probability, classification, and uncertainty quantification (UQ). In our experience, the connection between these four fields is anchored by a key concept in epidemiology, namely theprevalenceor the fraction of individuals with a given condition (e.g. an infection). In public health settings, this quantity is often of inherent interest because it informs, for example, strategies for preventing the spread of a disease. But somewhat counterintuitively, prevalence also controls the accuracy of diagnostic tests, especially in situations for which there is no so-called “gold standard” for detecting a medical condition.

Despite its central role in diagnostics, the concept of prevalence appears to have been largely overlooked in the broader fields of machine learning (ML) and classification theory. Motivated by this observation, Part I explored how prevalence leads to a new interpretation of discriminative-like classifiers as relative probability level-sets, which thereby yield uncertainty estimates in the class labels. Perhaps more importantly, this perspective unifies certain types of generative and discriminative ML models by demonstrating that they are mathematically equivalent. This also reveals that certain tasks in supervised learning can be rigorously recast as statistical regression. Part II extends these results by making new connections between unsupervised learning and linear algebra. Our main thesis is that certain types of unsupervised training generalize supervised learning in the same way that linear independence generalizes orthogonality.

In developing its core theses, Part I adopted a perspective rooted in epidemiology and diagnostics. We continue with that setting in this exposition. As a result, this manuscript is interdisciplinary. A typical reader should have a broad interest in the aforementioned fields of applied mathematics, with a willingness to ground the analysis in examples fromserology (or blood testing). While this manuscript is also intended to stand alone, we felt that complete duplication of material was unwarranted. Thus, much of the associated context is omitted. We advise the reader to consult Part I for more in-depth motivation of the methods considered herein.

SECTION: 1Introduction: Motivation and Key Questions

In traditional serology settings, there exists a population or sample spaceof individuals, some of whom have a specified medical condition, e.g. an infection. Instead of knowing the medical status of each individual, we are given a measurement outcome, which can be interpreted as both a diagnostic test result and a random variable. Based onalone, diagnosticians often seek to answer two questions:

which people have the condition;

and how many people have the condition?

The first question is clearly a classification problem; the second is theprevalence estimationproblem.***Prevalence is the fraction of individuals having a condition. In the context of serology testing, this is more appropriately called “seroprevalence,” which is the fraction of individuals having blood-borne markers of the condition. We use the words prevalence and seroprevalence interchangeably.

Within this context, there are generally two strategies for answering Question Q.I, and both require the use ofpure training data, for which the underlying sample classes are known. Generative strategies use this data to construct probability models of the measurement outcomes conditioned on the sample classes, which then inform a Bayes optimal classifier, for example(24;23;22;20;7;27). Discriminative approaches typically use the training data to construct a classification boundary that partitions the underlying measurement spaceinto domains corresponding to positive and negative samples(32;10;1;13;14;28;4;9;18;21;15). Part I of this series demonstrated that the two approaches are mathematical converses when the classifiers are constructed by minimizing a prevalence-weighted empirical risk function(25).

The goal of this manuscript is to generalize the results of Part I by showing that impure training data (for which the underlying true classes are unknown) can be used to construct classifiers that are identical to those associated with pure training data. The key ideas behind this analysis are that: (i) impure datasets can be parameterized by a prevalence; and (ii) two such datasets having different prevalence values correspond to a type of linearly independent systems. This leads to an isomorphism that relates classifiers associated with supervised and unsupervised training. In this context, we also demonstrate that when the probability models have partially disjoint supports, the unknown prevalence values of the impure populations can be determined by solving a pair of nonlinear equations depending on the distribution measures. We present numerical techniques for addressing these tasks and illustrate our analysis using synthetic and a SARS-CoV-2 serology assay.

The motivation for this work arises from the fact that it is often difficult to acquire pure training data in diagnostic settings(35;6;19;33;3). This is especially true in wildlife studies and for endemic conditions, such as infection of a common, widely circulating virus (e.g. influenza). In such cases, it is often possible to find subsets of a population that have a small (unknown) prevalence, but one can rarely guarantee that every “low” measurement signal corresponds to a negative sample. Likewise, for populations with a high prevalence, cross-reactivity to related viral strains can cause false-positives to be incorrectly added to a training population. Such effects are difficult to address without explicitly accounting for the fact that they make model training an exercise in unsupervised learning.

A key challenge in this work arises from the fact that in a binary setting with two linearly independent populations, there are in general four unknowns: two distributions that quantify the probability of a measurement outcome conditioned on the sample class, and two prevalence values. By analogy to linear algebra, it is clear that given only the PDFs of the impure populations, it is impossible to determine all four unknowns uniquely, and in fact, this is easy to prove rigorously. Thus to make use of the isomorphism between impure and pure training data, it is necessary to have additional information of some kind. We resolve this via our assumption ofpartially disjoint distributions, which we justify, for example, through the intuition that the “most positive sample” always yields a measurement signal that is greater than any negative sample. Even then, determining the unknowns requires a nuanced reinterpretation of prevalence and an analysis of certain classifiers as limits of sets.

As in Part I, an important limitation of our analysis is its restriction to a binary setting. We accept this for two reasons. First, the binary classification problem is sufficiently rich that, in our opinions, it warrants a study of its own. But perhaps more importantly, we anticipate that an analysis of the multiclass setting follows by extension of ideas developed herein. In certain places we point to multiclass extensions of our analysis, but the bulk of such work is left for future manuscripts.

In comparing to Part I, recognize also that the present manuscript is primarily concerned with limiting cases, e.g. in which the probability densities of training populations are known to arbitrary precision. Part I considered such cases bothper seand to motivate numerical techniques for analyzing finite datasets. In doing so, it was necessary to precisely define concepts such as empirical test and training data. In contrast, our primary goal in this work is to connect supervised and unsupervised learning when the probability models are given. It turns out that this is sufficient to transfer the numerical techniques of Part I to the present setting. For this reason, we omit some of the context surrounding empirical data, instead focusing only on those issues that are specific to analysis of unsupervised learning. We refer the reader to Part I for a full treatment of empirical datasets.

The rest of the manuscript is organized as follows. Section2provides technical background and overviews the mathematical setting of diagnostic classification. Section3develops the theory connecting unsupervised and supervised model training. Section4discusses our main results in the context of previous works and points to open questions.

SECTION: 2Mathematical Setting

The purpose of this section is to provide necessary technical background in support of our main results. Many of the ideas presented herein also appear in Part I, and we refer the reader to that manuscript for additional context. However, we do not advise skipping this section, despite a degree of overlap. Several concepts originally formulated in Part I are extended in this section.

SECTION: 2.1Notation

We leverage the following conventions motivated by diagnostics.

Except when referring to the sign of a number, the terms “negative” and “positive” generally denote some medical condition, e.g. an individual having a certain type of antibody in his or her blood sample.

When used as subscripts, the lettersand, which we often use in place ofand, denote “negative” and “positive” in the sense of (a).

When used as subscripts,andrefer to “low” and “high.” In particular, ifandare prevalence values, the indices indicate that.

The capital lettersandare associated with negative and positive populations in the sense of (a).

We also employ the following notation throughout.

Bold lowercase Roman and Greek letters (e.g.) denote column vectors; non-bold versions are scalars.

Subscripts,, andattached to vectors denote random realizations of a vector, not its components.

The symbolsandalways refer to sets.always denotes a “boundary set” between two domains.

The notationmeans

whereis always a probability density function andis a set. That is,is the measure of setwith respect to the PDF.

The acronymiidalways means “independent and identically distributed.”

SECTION: 2.2Key Assumption

We always assume absolute continuity of measure(31). See Part I for more context on the usefulness of this assumption.

SECTION: 2.3Background Theory

Diagnostic assays are used to determine the properties of atest population. This population is often associated with some sample space, and we assume that individualsbelong to one of two classes, referred to colloquially as “negative” and “positive.” We denote this class by the discrete random variable. In diagnostics, the class of an individual in a test population is typically unknown. In its place, we are given the resultof a diagnostic test, which is a random variable in some space. The goal of classification is to deduce, given only. Prevalence estimation determines the fraction of positive individuals in the population given a set of measurementsindexed by. In either case, we refer to the collection of measurements being analyzed as thetest data.

When there is no risk of confusion, we omit dependence ofon, since the latter is most often the quantity that we can access. See Part I and Refs.(30;8)for context associated with probability spaces needed to rigorously define our mathematical setting.

As a preliminary step, we must construct the probabilistic framework describing the measurement process of a diagnostic test. This can be accomplished by recourse to the law of total probability(36). To that end, the following concepts are foundational.

Letbe a binary random variable. We adopt the convention thatis a “negative” sample andis a “positive” sample. Moreover, we define theprevalenceto be the probabilitythat.

Letbe a random variable for some positive integer. Also letbe a binary random variable. We refer toas theconditional probability density functionofconditioned on. That is, it is the probability that a positive sample yields measurement value. Likewise,is the conditional PDF for a negative sample.

The previous two definitions lead to the following model of a measurement process.

Letbe a binary random variable, and assume. Then

is the PDF that atest sample(whose class is unknown) yields measurement. We refer to the underlying sample space from which theare generated to be atest populationwith prevalence.

For more context on the use of Defs.2.2–2.4, see Part I. Here it suffices to note that whenor,corresponds to the PDF of a pure negative or pure positive population. Thus, we also interpret,to be the PDF of animpure population.

Given a sample generated by, diagnostic classifiers are generally identified with a partitionof the measurement space. The statementsandare interpreted ascorresponding to a positive or negative sample. More precisely:

Letbe a partition ofsuch that

We refer toas a (binary)classifierand assignto the negative or positive class according toor. .

While not needed in this manuscript, observe thatinduces a new random variablewhose value iswhen(whereand). Such a perspective establishes connections to ML techniques that do not explicitly represent classifiers in terms of sets. However, for random variables that can be expressed as vectors, we may safely assume that there exists an underlying representation (whether explicit or not) of the classifier as a partition. This perspective is particularly useful for making connections to probability. Note that by assuming continuity of measure,defines an equivalence class of classifiers that only differ on sets of measure zero. We always adopt this more general perspective when working with partitions. See Part I for more context on these issues.

An important observation of Ref.(24)is that the average error rate of a diagnostic assay is a scalar-valued function of a partition. In particular:

Letbe a binary classifier. The mappingdefined via

is the average orexpected classification errorassociated withfor a test population having prevalence.

See Part I for a straightforward proof based on the law of total probability(36).

A key objective in classification is to minimize the error associated with assigning class labels. The following result addresses this issue(25).

Assume. Then the partitionwhose elements are

minimizes, whereandform an arbitrary partition of the set. Moreover, the converse holds. Any partitionminimizingis in the equivalence class defined by.

Lemma2.9plays a critical role in Part I of this series. We view it as the foundation for UQ of classification because it implies thatcan be used to quantify uncertainty in the class labels. Moreover, this holds no matter how we construct the classifier, i.e. independent of our direct knowledge ofand. In order to connect supervised and unsupervised learning, our main task therefore amounts to identifying an appropriate unsupervised objective functionthat is equivalent to, so as the guarantee that they have the same minimizer.

We end this section on a technical point needed to address an issue associated with sets of measure zero. The following definition ensures that classifiers do not have degenerate sets violating the pointwise structure of.

We say thatsatisfies theoptimal partition conventionif: (i) is in the equivalence class minimizing; and (ii)and.

Note that the optimal partition convention does not determine how points inare assigned to sets in. See Part I for further discussion and context on the optimal partition convention(25).

SECTION: 3Impure Training Data

SECTION: 3.1Linear Independence

The classifier given byis impossible to realize in practice without some method for injecting information aboutand. In supervised settings, one is given atraining data setcomposed of samples whose underlying true classes are known. These can be used to directly model the conditional PDFs, or alternatively, one can construct a boundary (for example) that best classifies the data in some appropriate sense. Such issues amount to questions of how to model data. We refer the reader to Part I for an in-depth analysis(25). Here we seek to answer a different question, namely given a model ofimpure data, for which the sample classes are unknown, can we reconstruct, and if so, how? We temporarily assume that impure training data has already been analyzed in order to generate the PDFs associated with the following definitions. We return to the issue of modeling in Secs.3.3and3.4.

It is important to distinguish properties (e.g. the prevalence) of impure training populations from those of independent test populations. The former are always used for classification, whereas the latter are the objects to which classifiers are applied. To reduce confusion, the symbolrefers to the prevalence of impure training populations, whereasrefers to test populations.

Letandbe the PDFs of two test populations for which. Then we refer to theandas the PDFs oflinearly independent populations.When clear from context, we omit the dependence ofandonand.

We can motivate the concept of linearly independent populations by recalling that the() are defined as

Thus, byformallytaking linear combinations of the, we anticipate being able to recoverand, provided theare different. Observe that the case,defines linearly independent populations that are also pure. By analogy to linear algebra, we can treat these populations as being “orthogonal” in the sense that the set of classes in one is disjoint from the set of classes in the other. These observations motivate our using the PDFs of linearly independent populations to construct, which suggests the following definition.

Letandbe distributions associated with linearly independent populations.
Then we defineto be thepseudoprevalenceassociated with theexpected impure error

where.

It is tempting to interpret Eq. (7) as implying

for some third distribution, which suggests thatis in some sense a prevalence. One can even follow this logic further to deduce thatis a function ofsharing some, but not all of the properties of Def.2.2. However, doing so immediately constrains our interpretation ofin a way that makes it difficult to construct optimal classifiers.Thus, for the time being, we treatappearing in Eq. (7) as a free parameter whose relationship tois yet to be deduced. This justifies callinga pseudoprevalence. It nominally plays the same role asappearing in Eq. (4) but is not tied to the property of a population. In Sec.3.2we construct a relationship betweenand.

The structure of Eq. (7) suggests the following corollary to Lemma2.9.

The sets

define an equivalence class of partitions that minimize Eq. (7). Moreover, the converse is true: any partition minimizing Eq. (7) is in the equivalence class defined by Eqs. (8a)–(8c).

SECTION: 3.2Classification with Impure Training Data

We next consider how Def.3.2can be used for classification. The following example provides motivation.

Assume two impure populations whose prevalence values satisfy. By simple re-arrangement, it is straightforward to show that

In other words, theandclassification domains constructed from impure and pure distributions are the same, and we may useandas proxies forandwhen classifying. Doing so yields

Thus, any linearly independent population yields the same equivalence class of partitions for.

Example3.4does not require knowledge ofand. In retrospect, this is not surprising. Since theis the set offor which, all convex combinations of these PDFs remain equal on. See Fig.1. However, as the next lemma demonstrates, this simplification is strongly related to the fact that.

Letandbe the PDFs associated with impure training data. Without loss of generality, assume that. Then the function

on the domainhas the additional properties that:

its range is, where

it is a strictly monotone increasing function of, and hence a one-to-one mapping betweenand;

and it yields the optimal classification domains

whereandform an arbitrary partition of the boundary set. In other words, Eqs. (13a) – (13c) define the same equivalence class of partitions as Eqs. (5a) and (5b). Moreover, for an arbitrary partitionthere exists a constantindependent ofsuch that

Property (a) follows from (b) and the facts thatand. To prove (b), it suffices to compute the derivative ofwith respect to. Straightforward calculations show this to be positive (and in fact, it is proportional to).

Property (c) arises by considering inequalities of the form

Invoking the definition ofand, straightforward rearrangement shows that inequality (15) is equivalent to

where the last line arises from Eq. (11). Identical arguments apply if the inequality sign is reversed or replaced with equality.

Equation14is verified directly but substituting the definitions ofandinto Eq. (7) and using the fact that all PDFs are normalized on.

The key implication of Propostion3.6is that we may construct optimal classification domains for thepureproblem by minimizing Eq. (7).By virtue of Corollary3.3, we need not even knowanddirectly. The logic is as follows. Suppose we find a classifierthat minimizes Eq. (7). By Corollary3.3this is in the same equivalence class asgiven by Eqs. (8a)–(8c). But by Proposition3.6this is in one-to-one correspondence with the equivalence class defined by Eqs. (5a)–(5a). Thusis in theory equivalent to, which is the mathematical content of Eq. (14).

In practice the situation is more complicated as we must first determineand. This is addressed in the next section. For now, however, observe the importance of the linear independence assumption. When, the relationship betweenandbecomes vacuous; one finds thatfor all, which is consistent with Example3.4. The reader wishing to rederive many of the results herein will find that the differenceappears often.

When expressing the optimal partitionin terms of, we use the symbolsand.

Consider impure training data infor which

Part I shows that the classification boundaries for such pure distributions can be constructed by considering ratios of the form

For, solutions to this equation yield the sets, which divideinto the optimal domainsand. By taking the logarithm ofEq.18, one finds that each setis the locus of points on a quadratic curve. See alsoExample3.14.

We can similarly express the ratio

for some constantsand. Since, the left-hand side (LHS) of the last line ranges fromto, which means thatis restricted to a proper subset of the interval. Moreover, sinceEq.18andEq.19both only depend on, it is clear that they share the same boundary sets. In fact, this example holds more generally, since it does not rely on the specific form of.

SECTION: 3.3Estimating Prevalence of Impure Training Data

SectionSection3.2assumes that theare known, but this is rarely true in practice. We overcome this problem by leveraging the following discontinuity.

Assume thatminimizingsatisfy the optimal partition convention. Assume also that the supports ofandare partially disjoint, i.e. that there exist setsandsuch that

Without loss of generality, let. Then for any linearly independent populations, the measuresandare discontinuous at. Similarly,andare discontinuous at.

Note first that when the optimal partition convention holds, the setsare monotone decreasing asdecreases. That is,for; see Proposition 5.4 of Part I(25)for justification. Thus, by Proposition3.6, within the equivalence class of partitions, the limityields the set ofdefined by the condition

We can express this limiting set as the intersection

To see the equality between the first and second lines, argue by contradiction. Specifically, let there be ansuch thatand. However, one can find asuch that the inequality in Eq. (22a) is violated, which means thatcannot be in every set of the intersection. Moreover, we need not worry about pointsfor whichand, since they can be excluded from. By the assumption that the conditional PDFs are partially disjoint, we find that, and thus

If in constrast, for any, recourse to the definitions ofandand Proposition3.6imply that

Thus,for any, so that

which yields a discontinuity for measures with respect to. By definition of the left and right limits ofat, we see thatfor everyin either set, so that the result also holds for measures with respect to. Similar arguments yield the corresponding discontinuity for.

At first blush, Lemma3.9simply appears to illustrate the extent to which sets of zero measure (with respect toin this case) need not be unique. Viewed in the context of Eq. (4), this is not surprising. The limitcorresponds to. Thus the termdisappears from the objective function. In this case, we may setto be any set we wish, providedhas zero measure there.

Forimpurepopulations, however, we must construct the optimal partition by minimizing Eq. (7). Moreover, we may always construct a family of such partitions by varying, which we can do even when theare unknown; see Remark3.2. Here something interesting happens. When(in particular, when), the setencounters the discontinuity embodied by Lemma3.9. This happens despite neither term on the RHS of Eq. (7) vanishing. One therefore finds that whenis treated as if it were the PDF for a pure population: (i) all points inare associated with the class ofwhen; and no points are associated with the class ofwhen.

To better understand this phenomenon, consider that for an arbitrary partition, we can extract from Eq. (7) the part of the expected classification error that is due towhen. Denote this bygiven by

where we use the fact that

Equation (26) implies that at, the-weighted PDFsandare identical on. Thus, for,is always greater thanon, and in fact, on all of. This observation and Eq. (27) motivate the following estimate of the.

[System of Equations for]
Consider a linearly independent population, and assume that. Assume also that the conditions of Lemma3.9hold. We construct a system of equations for determining theand show that up to degenerate cases, the solution is unique.

Note first that inverting Eq. (12) for theyields

Thus, one finds

Next observe that by Lemma3.9,

for. The probability measures can be eliminated by taking ratios of the form

Equations (32a) and (32b) only depend on eitheror, but not both. Thus each equation can be solved independently.

Consider next Eq. (32a). The left-hand side (LHS) yields an indeterminate form for any. Forobserve that by linearity of integration, the ratio

for some incrementsand. But by Eq. (13b), one finds that

where the second line arises by observing that the first line is a weighted average ofand. Thus,is the only solution to Eq. (31a). A similar argument applies to the case of Eq. (32b).Q.E.F.

Equations (32a) and (32b) are given in terms of exact measures, which are generally unknown. In light of Part I, it is useful to construct estimators of thein terms of empirical data. In doing so, one must address not only sampling variation due to having finite data, but also uncertainty in the models used to construct the classifier. The latter task requires an understanding of rates of convergence the chosen classifier, which is beyond the scope of the present manuscript; see Part I for a discussion in the context of the homotopy classifier used in this manuscript(25). Here we present a simple method that approximates the impact of sampling variation alone, leaving more advanced estimators for future work.

It is necessary to introduce several auxiliary concepts.

Letbe a sample space andbe a binary random variable for. Let, and assume theare iid. We define

to be anempirical test populationwithsamples and prevalence. We further defineto belinearly independent empirical populationsif

is a set of distinct empirical test populations whose sample points are iid and for which.

See Part I for additional context on empirical populations(25).

[Bayesian Approximation ofand]
Letbe an impure training population whose prevalencesandare unknown. Also letbe a grid ofvalues satisfyingand. Assume thatis a set of partitions constructed so thatminimizes the error. We construct a Bayesian estimate of the unknown prevalences.

Consider Eq. (32a) and a corresponding estimate of. LetThe partitionsinduce approximate measures

which are binomial random variables. Moreover, for a fixed, theandare independent, since the underlying populations are independent. The Wilson score interval(34)yields an estimate of the confidence interval for a binomial random variable by first takingto be the z-score associated withwere we able to treatas a normal random variable. That is,corresponds to a 95% confidence interval in the estimate of the expected value of. Then, the true measure is estimated to be in the range

By Construction3.10, we know that

for. Moreover,reverts to a degenerate form for. Thus, for a fixed, we consider the admissible rangeof values ofto be

Ifdefined by Eq. (40) is empty, we instead define.

If, this implies that, which is our estimator. If, letbe the cardinality of. As our prior, we then assume that allhave equal probability of being.

To compute a posterior distribution for, observe that Eq. (32a) can be expressed as

Given the empirical data, we seek to estimate the probability that this equation is satisfied. In particular, for each, approximate

whereis a normal random variable with meanand variance. In light of this approximation, the LHS of Eq. (41) is a normal random variable, from which we can construct the probability density. Using the definition of conditional probability, we then define the probability thatisto be

From this, we can compute an expected value ofand corresponding confidence intervals. A similar construction yields. Note finally that inverting Eqs. (28) yield the.Q.E.F

Constructions3.10and3.11are limiting insofar as they require the supports ofandto be partially disjoint. In practice, this condition may not always hold; rather, it is more reasonable to assume that there exist domainsandon which the functionand, respectively. In such cases, we can still construct reasonable approximations of the. The following definition and construction makes this precise in an asymptotic sense.

The ratiodefined as

is therelative conditional probabilityof a measurement outcome, where we interpret the situationto correspond toand omit frompoints for which.

We use the notationasto mean that for some, there exists ansuch thatfor.

[Asymptotic Approximation of]
Let, and assume that there exists domainsandwith correspondingandsatisfying

These inequalities yield asymptotic estimates of,,, andvia Eqs. (32a) and (32b). In particular, defineandvia

Inverting these yields

By inequalities (45a) and (45b), the approximationsandcan be expanded in powers ofand. Letting, one thus finds that

as. By solving for the[e.g. via Eq. (28)], one recovers similar asymptotic estimates for the prevalences directly.Q.E.F

The following examples illustrate the main ideas of this section.

A key problem during the COVID-19 pandemic was a lack of control samples associated with positive and negative individuals. The data from Ref.(2)provides one such example. The left plot ofFig.2shows typical serology measurements of two immunoglobulin g (IgG) antibodies that bind to the the SARS-CoV-2 receptor binding domain (RBD) and nucleocapsi (N). However, the presumed positive class was based hospitalized patients with COVID-like symptoms, and orthogonal confirmation of the sample status was not available in all cases. Thus, this data should be treated as impure, since some of the patients could have had other respiratory conditions such as influenza. The right plot ofFig.2illustrates the results ofDefinition3.11applied to this data.Definition3.11indicates that the presumed positive samples had a prevalence ofwith a 95% confidence interval of, withfor the prior. This result is consistent with the understanding that the majority of the patients were COVID-19 positive based on symptoms. Note that while the left subplots ofFig.2show the inequalities used to estimate, this quantity was set to, as the corresponding samples were known to have been collected before the COVID-19 pandemic.

Figure3shows the same analysis asExample3.13, except that the underlying PDFs are given byEq.17candEq.17dwithand. Five hundred datapoints were generated for each population. The resulting classification boundaries are known to be quadratic in this example and can be estimated using the analysis of Part I(25). The prevalencewas estimated to bewith a 95% confidence interval of. The prevalencewas estimated to be, with a 95% confidence interval of.

SECTION: 3.4Prevalence Estimation and Classification Revisited

Once theare known, we can use the impure training data to estimate the prevalence of a third test population without knowing theor. The following lemma provides needed context and is discussed in more detail in Part I(25).

Letbe a binary random variable. Letbe a subdomain chosen such that. Let there beiid random variables()drawn from. Then

is an unbiased estimate ofthat converges in mean-square as.

To estimate prevalence, it suffices to determineandin terms of the PDFs of the impure populations. To accomplish this, define the measures

Next observe that by definition,

which can be inverted to yield

Substituting empirical, Monte Carlo estimates ofandfor a suitably chosenyields corresponding empirical estimates ofand(5). See Part I for a deeper discussion of prevalence estimation, especially in the context of exmpirical data(25).

Observe that the determinant of matrix in Eq. (50) is. When, the linear system clearly becomes ill-posed. Thus the concept of linearly independent populations is directly tied to the structure of the matrix equation that connects mixed distributions to their pure counterparts.

SECTION: 4Discussion

SECTION: 4.1Our Interpretation of Unsupervised Learning

Our interpretation of unsupervised learning is slightly less general than commonly accepted definitions in at least two ways(16). First, while the true class of each sample is unknown (i.e. the underlying PDFs of training data are unknown), we assume there are two impure populations with different prevalence values. Our analysis also requiresa prioriknowledge that the training data is comprised of two classes. In this sense, one might argue that we have connected aspects of linear algebra andweakly or semi-supervisedlearning, although such issues may ultimately amount to semantics. It is possible that further connections to linear algebra may clarify the concept of unsupervised learning as a generalization of supervised learning. Such issues are discussed in more detail in the next section.

SECTION: 4.2Connection to Linear Algebra

Definition3.1andEq.51establish a connection between linear algebra and classification. A fundamental question is the extent to which this connection can be extended. The matrix

suggests one route to accomplishing this. It is clear that the columns ofare linearly independent when, and in factEq.50can be interpreted as an alternate form ofDefinition3.1. Thus, we anticipate that the following definition enables generalizations of our results to multiclass settings.

Letbe anright stochastic matrix (i.e. rows sum to one) that is invertible(11). Letbe a vector of distinct probability density functions. Then we say that the distributionscorresponding tolinearly independent populationsif

The perspective ofEq.53is suggestive of recent works that explored related properties classifiers in the context of linear algebra. Reference(26)in particular demonstrated that the uncertainty in prevalence estimates associated with the case(whereis the identity matrix) is controlled by the largest Gershgorin radiusof a matrix(12;17), where the elementsofare

and theform a partition of. Moreover, one can show thatis minimized by a partition whose resulting matrixis closest to the identity in some appropriate sense; thuscan be interpreted as a type of confusion matrix, where the overlap between conditional distributions increases uncertainty in prevalence estimates. From this perspective, one sees immediately thatEq.53likewise increases uncertainty in prevalence estimates asdeviates from the identity. While beyond the scope of this work, we anticipate that rates of convergence of estimators using impure training distributions can be deduced via the methods developed in Ref.(26), and moreover, these rates should be slower than when using pure training distributions.

SECTION: 4.3Limitations and Open Questions

A fundamental and unresolved issue in this work is to deduce the convergence properties of methods such asDefinition3.11. It is also likely that better estimators can be formulated, given thatDefinition3.11does not account for correlation between the measures used inEq.40. Addressing these issues is complicated by the fact that estimates of the optimal classification domains themselves have error due to the finite amount of data used to construct them. Resolution of such questions is reserved for future work.

WhileSection4.2points to analogies in linear algebra that may facilitate extensions to multiclass settings, such approaches will likely need to contend with a variation onconditioning(29)The matrixhints at such issues; when, the linear system given byEq.50becomes ill-conditioned. The finite data used to construct classifiers and estimate prevalence are likely to further compound such issues. Thus, in multiclass settings, we anticipate that extra care must be taken to ensure that the linearly independent populations have prevalence values that are nearly orthogonal in some appropriate sense. Alternatively, methods must be developed to stabilize numerical methods. Such issues are also left for future work.

Acknowledgements:This work is a contribution of the National Institutes of Standards and Technology and is therefore not subject to copyright in the United States. RB, CF, and AM were supported under the US National Cancer Institute, Grant U01 CA261276 (The Serological Sciences Network), Massachusetts Consortium on Pathogen Readiness (MassCPR) Evergrande COVID-19 Response Fund Award, and University of Massachusetts Chan Medical School COVID-19 Pandemic Research Fund. Certain commercial equipment, instruments, software, or materials are identified in this paper in order to specify the experimental procedure adequately. Such identification is not intended to imply recommendation or endorsement by the National Institute of Standards and Technology, nor is it intended to imply that the materials or equipment identified are necessarily the best available for the purpose.

Use of all data deriving from human subjects was approved by the NIST and University of Massachusetts Research Protections Offices.

Data availability:Data associated with the SARS-CoV-2 assay is available for download as supplemental material to Ref.(2). An open-source software package implementing these analyses is under preparation for public distribution. In the interim, a preliminary version of the software will be made available upon reasonable request.

SECTION: References