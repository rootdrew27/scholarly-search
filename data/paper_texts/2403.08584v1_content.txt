SECTION: Local Binary and Multiclass SVMsTrained on a Quantum Annealer

Support vector machines (SVMs) are widely used machine learning models (e.g., in remote sensing), with formulations for both classification and regression tasks. In the last years, with the advent of working quantum annealers, hybrid SVM models characterised by quantum training and classical execution have been introduced. These models have demonstrated comparable performance to their classical counterparts. However, they are limited in the training set size due to the restricted connectivity of the current quantum annealers. Hence, to take advantage of large datasets (like those related to Earth observation), a strategy is required. In the classical domain, local SVMs, namely, SVMs trained on the data samples selected by a-nearest neighbors model, have already proven successful. Here, the local application of quantum-trained SVM models is proposed and empirically assessed. In particular, this approach allows overcoming the constraints on the training set size of the quantum-trained models while enhancing their performance. In practice, the FaLK-SVM method, designed for efficient local SVMs, has been combined with quantum-trained SVM models for binary and multiclass classification. In addition, for comparison, FaLK-SVM has been interfaced for the first time with a classical single-step multiclass SVM model (CS SVM). Concerning the empirical evaluation, D-Wave’s quantum annealers and real-world datasets taken from the remote sensing domain have been employed. The results have shown the effectiveness and scalability of the proposed approach, but also its practical applicability in a real-world large-scale scenario.

SECTION: IIntroduction

Support vector machines (SVMs) are supervised machine learning models designed for binary classification tasks[1]. Specifically, an SVM aims to identify the optimal hyperplane that effectively separates data samples belonging to distinct classes. However, with the introduction of kernel functions, SVMs can go beyond linearly separable problems[2]. Furthermore, various formulations of the learning problem exist, and also extensions to multiclass classification and regression tasks[3,4]. In the last years, with the diffusion of the quantum annealing machines produced by D-Wave[5], hybrid SVM models characterised by quantum training and classical execution have been proposed. In detail, hybrid versions for binary classification[6], multiclass classification[7], and regression[8]tasks have been developed. These models have been evaluated mainly in the remote sensing domain (see also[9,10,11]), showing comparable performance with respect to their classical counterparts. Nevertheless, due to the restricted connectivity of the available quantum annealers, they are limited in the training set size. Therefore, in order to leverage large datasets, a strategy is necessary.

In the classical realm, reducing the number of input samples to a machine learning model through a locality technique, such as the-nearest neighbors (-NN) algorithm[12], has proven to be successful, yielding performance improvements compared to the base model. For instance, Blanzieri and Melgani have proposed and empirically assessed theNNSVM classifier[13], namely, a local binary SVM trained on data samples selected by a-NN model, achieving good results. Moreover, local SVMs have been theoretically characterised by researchers like Hable[14]and Meister and Steinwart[15]. However, despite the accuracy improvement and reduced training time per model (resulting from the lower number of samples employed for training), an SVM must be trained on the-neighborhood of each test sample, which is a significant bottleneck in terms of execution time. To address this issue, Segata and Blanzieri have developed the Fast Local Kernel Support Vector Machine (FaLK-SVM)[16], which relies on the usage of the cover tree data structure[17].

In this work, the local application of quantum-trained SVM models is proposed and empirically evaluated. Indeed, local classically-trained binary SVMs have already demonstrated to be successful, and quantum-trained SVMs have exhibited similar performance to their classical counterparts. Moreover, the usage of local quantum-trained models, as opposed to global ones, represents a valid solution to the training set size limits imposed by the connectivity of the current quantum annealers. In practice, FaLK-SVM[16], the method for efficient local SVMs, has been interfaced with two quantum-trained SVM models: the quantum-trained SVM for binary classification (QBSVM)[6], and the quantum-trained SVM for multiclass classification (QMSVM)[7]. Additionally, for comparison, FaLK-SVM has been combined for the first time with CS SVM[3], the classical single-step multiclass SVM model on which QMSVM is based. Hence, the addressed tasks are binary and multiclass classification. For the empirical evaluation, D-Wave’s quantum annealers and real-world datasets belonging to the remote sensing domain have been used.

The article is organized as follows:SectionIIprovides some background information;SectionIIIpresents the proposed approach and the implementation details;SectionIVdeals with the experiments performed and the results obtained;SectionVconcludes the work.

SECTION: IIBackground

This section provides some background information about quantum annealing, QUBO problems and their embedding, quantum-trained support vector machines, and local support vector machines.

SECTION: II-AQuantum Annealing, QUBO, and Embedding

Quantum annealing (QA) is a heuristic search used to solve optimization problems[18,19]. In particular, in QA, the solution of a given problem corresponds to theground stateof a quantum system described by a Hamiltonian encoding the structure of the problem. In this sense, QA is related to adiabatic quantum computing, but there are some remarkable differences[20]. Specifically, let us consider the time-dependent Hamiltonian

whereandare non-commuting operators on the-qubit Hilbert spacecalledproblem Hamiltonianandtransverse field Hamiltonian, respectively,is a positive decreasing function that attenuates the contribution of, andis the evolution time. The annealing process drives the quantum system towards the ground state of, which is designed to represent the optimization problem.

QA can be physically realized by considering a network of qubits arranged on the vertices of a graph, withand whose edgesrepresent the couplings among the qubits. Then, the problem Hamiltonian can be defined as follows:

where the real coefficientsare arranged into the matrixandis amatrix that acts as the Pauli matrix

on the-th tensor factor and as theidentity matrix on the other tensor factors.
By definition, the set of eigenvalues of the problem Hamiltonian (Eq.2) is the set of all possible values of the cost function given by the energy of the well-knownIsing model:

where.
In practice, the annealing procedure, also calledcooling, drives the system into the ground state of, which corresponds to the spin configuration encoding the solution:

Given a problem, the annealer is initialized using a suitable choice of the weights, and the binary variablesare physically realized by the outcomes of the measurements performed on the qubits located on the vertices. In order to solve a general optimization problem through QA, it is first necessary to find anencodingof the objective function in terms of the cost function (4), which is hard in general.

However, if the quantum architecture is able to provide a fully connected graph, then any Quadratic Unconstrained Binary Optimization (QUBO) problem can be directly represented into the cost function by means of the change of variables. Indeed, QUBO problems are NP-hard problems of the form

whereis an upper triangular (or symmetric) matrix of real values, and they can be rewritten as

wheresince. In practice, the main diagonal ofcontains the linear coefficients (), whereas the rest of the matrix contains the quadratic ones (). Although QUBO problems are unconstrained by definition, it is actually possible to introduce constraints by representing them as penalties[21].

In general, due to the sparseness of the available quantum annealer topologies, a direct representation of the problem is typically not possible. The solution consists in chaining together multiple physical qubits that will act as a single logical qubit. In this way, the connectivity of the annealer graph is increased at the price of reducing the number of logical qubits available and, consequently, the size of the representable problems. The mapping of the problem variables on the annealer topology is known asembedding.

SECTION: II-BQuantum-Trained SVM Models

Quantum-trained SVMs are classical SVMs trained with quantum annealing and executed classically. In this paper, the focus is on the models for binary and multiclass classification, whose details are provided in the following.

In the work by Willsch et al.[6], the standard formulation of the binary SVM has been reframed as a QUBO problem, as inEq.6. Training a binary SVM consists in the following quadratic programming problem:

forcoefficients, whereis the training set ofexamples,is the kernel function, andis the regularization parameter. The resulting classifier is defined as

where the biasis chosen as

Being already quadratic, this real-valued, constrained optimization problem can be converted to a QUBO problem by adding the constraints to the cost function as penalty terms with a multiplier, and encoding a discretized solution space usingbinary variables:

whereis the encoding base. The corresponding QUBO problem becomes

withbeing the Kronecker delta.

Since this QUBO formulation might yield matrices not embeddable in the available quantum annealers, Willsch et al. have proposed the following approach. Firstly, the dataset is partitioned intodisjoint slices. Then, for each slice, the decision functions of thebest solutions (in terms of energy) obtained from the annealer are averaged. Lastly, the classifier, which corresponds to an ensemble of SVMs, is defined as

whereandare the-th mean coefficient and the mean bias for the-th slice. Actually, averaging thebest solution can be used even in scenarios where dataset splitting is not required.

As for its classical counterpart, there are two different approaches for extending QBSVM to multiclass classification. The problem can be decomposed into multiple binary problems and a QBSVM model can be trained on each subproblem, combining the obtained classifiers in an ensemble. Alternatively, a model trained to directly classify examples among multiple classes should be defined. The QMSVM approach proposed by Delilbasic et al.[7]is based on the CS SVM model[3], which consists in the following optimization problem:

Here,is the number of classes,are theproblem variables, andis a regularization parameter. The resulting classifier is defined as:

As for the binary case,binary variables are used to redefine the original optimization problem variables:

After adding the constraints as penalty weights with a multiplier, the QUBO matrix is defined as

To take advantage of the multiple solutions obtained from the annealer, Delilbasic et al. have proposed the following approach. Firstly, each of thebest solutions (in terms of energy) is tested on a validation set (that may coincide with the training set). Subsequently, a weighted average is performed. More precisely, the weights of the solutions with an accuracy above a predefined threshold are given by a softmax function applied to the values, withbeing a real value andbeing the accuracy achieved by the-th solution. Conversely, the weights of the other solutions are set to zero. The resulting mean variablesare used inEq.17to classify the new samples. Actually, this approach allows also addressing larger datasets (part of the dataset can be used only in the weighting step).

SECTION: II-CLocal SVMs

Reducing the number of input samples to a classical (binary) SVM by means of a locality technique has demonstrated to be successful. In 2006, Blanzieri and Melgani have proposed and empirically evaluated theNNSVM classifier[13], namely, a local SVM trained on the samples selected by a-NN model, obtaining good results. Specifically, the-NN and the SVM must operate in the same transformed feature space. However, for RBF kernels (like the Gaussian kernel) and polynomial kernels with degree 1, the Euclidean distance can be used as the distance metric for the-NN[13]. Additionally, local SVMs have been theoretically characterised by, for example, Hable[14]and Meister and Steinwart[15]. Nevertheless, despite the accuracy enhancement and the reduced training time per model (due to the lower number of samples used for training), theNNSVM classifier requires to train an SVM for each test instance (unless the nearest neighbors belong to the same class), posing a serious bottleneck in terms of execution time. To address this issue, Segata and Blanzieri have devised the approach outlined below.

FaLK-SVM[16]improves the execution time of theNNSVM classifier[13]by leveraging a data structure proposed by Beygelzimer et al. for efficient nearest-neighbor operations, i.e., the cover tree[17]. Essentially, the idea consists in covering the training set with a set of local SVM models, and predicting the label of a test instance with the most suitable (pre-trained) local model. More in detail, FaLK-SVM is trained as follows: a cover tree is built on the training set; the centres of the local SVMs are selected through the cover tree, which allows the efficient retrieval of data samples that are far from one another, limiting the overlap of the local models; the local SVMs for which the local training set does not contain only one class are trained. Specifically, the selection procedure ends when each training sample belongs to the-neighborhood of at least one centre, whereis a hyperparameter controlling the local models redundancy. In addition, at training time, the association between each training point and the centre for which the neighbor ranking of the given training point is the smallest is determined. In this way, at prediction time, it is only necessary to identify the nearest neighbor of the test instance in the training set and execute the associated local model. Concerning the time complexity, the training step has a worst-case complexity of, withbeing the number of nearest neighbors selected andbeing the number of training samples, while the prediction of a new label has a complexity of.

In the same article, a variant of FaLK-SVM, denoted as FaLK-SVMl, has been also presented. Essentially, FaLK-SVMl incorporates a grid-search model selection procedure that is run before the training of FaLK-SVM. In practice, each combination of local model parameters is tested, using a custom-fold cross-validation, onlocal models with randomly selected centres. In this custom-fold cross-validation, only thenearest neighbors of the model centre are considered for the split into folds, while the remainingsamples of the-neighborhood are added to the training set of each-fold iteration. Eventually, the parameter configuration that maximizes the average accuracy of themodels is selected and employed for all local models.

SECTION: IIILocal Quantum-Trained SVMs

This section introduces the proposed approach and provides details about the implementation. The code is publicly available athttps://github.com/ZarHenry96/local-qtrained-svms.

SECTION: III-AApproach

Quantum-trained support vector machines have exhibited performance akin to their classical counterparts[6,10,7]. However, the restricted connectivity of the current quantum annealers places constraints on the size of the trainable models. Various strategies have been proposed to address this limitation and exploit larger training sets, including the construction of ensembles of SVMs[6]and the weighting of the best solutions (in terms of energy) retrieved by the annealer based on their performance on a large validation set[7](as illustrated inSectionsII-B1andII-B2). The approach proposed here consists in the localised application of quantum-trained SVM models. Indeed, in the classical domain, local SVMs have demonstrated superior performance compared to their global counterparts (seeSectionII-Cfor more details). Furthermore, in this way, large training sets represent no more an issue, as each local model is trained solely on the-neighborhood of the model centre.

Essentially, in this work, FaLK-SVM, the method for efficient local SVMs outlined inSectionII-C1, has been interfaced with two quantum-trained SVM models: the quantum-trained SVM for binary classification detailed inSectionII-B1(QBSVM), and the quantum-trained SVM for multiclass classification detailed inSectionII-B2(QMSVM). The resulting workflow is straightforward. In fact, the only difference compared to the standard FaLK-SVM resides in the local models employed, which are trained on a quantum annealer and run classically. Actually, another innovative aspect of this study is the assessment of FaLK-SVM with local single-step multiclass classification models such as QMSVM and CS SVM, which has been taken into account for comparison (CS SVM is the basis of QMSVM, as mentioned inSectionII-B2). Indeed, FaLK-SVM has already been assessed in a multiclass classification task, but employing a one-against-one (OAO) approach with local binary SVMs[22].

SECTION: III-BImplementation Details

The approach described in the previous section has been implemented building upon the FaLK-SVM implementation provided by Segata[23]. Specifically, that implementation of FaLK-SVM is written in C++, whereas the codes for QBSVM[24]and QMSVM[25]are written in Python, since Python is the only language supported by D-Wave for interacting with their quantum annealers. Therefore, to interface FaLK-SVM with the quantum-trained models, a Python class namedPythonSVMhas been implemented and embedded within the FaLK-SVM C++ framework, allowing the execution of Python code within the C++ application. For this purpose, the functions, types, and macros supplied by thePython.hheader file have been employed.

From an approach-related perspective, two aspects are worth to be discussed. Firstly, to reduce the training time, the reuse of the QUBO matrix embeddings has been implemented. Basically, when a QUBO matrix of a certain size is submitted for the first time to the quantum annealer, the embedding for a complete matrix of the same size is computed, applied, and stored in memory. In all subsequent calls with QUBO matrices of that size, the precomputed embedding is retrieved and applied. This proves particularly advantageous as the QUBO matrix size is the same for all local models. Secondly, two notable features have been developed, although they have not been used in the experiments presented in this work. The first one is the local usage of the techniques illustrated inSectionsII-B1andII-B2for leveraging larger datasets. This allows increasing the size of the neighborhoods used for training local models, a parameter otherwise limited by the connectivity of the annealer. The second feature pertains to multiclass classification tasks and consists in the dynamic selection of the local model based on the number of classes present in a-neighborhood. Indeed, with two classes, QBSVM needs half of the binary variables compared to QMSVM.

From a model-related perspective, some modifications have been applied to the original implementations. On the FaLK-SVM front, the computation of the performance metrics and the criterion for assessing the class balance of the-neighborhoods used in the local model selection procedure (of FaLK-SVMl) have been extended to multiclass classification. Regarding the grid-search local model selection, support for the parameters of the quantum-trained models has been incorporated; additionally, the number of foldsfor the internal custom-fold cross-validation (10 by default) and the number of samples used to evaluate the performance of themodels (by default) have been parametrized. Eventually, a data standardization procedure has been introduced in the external canonical-fold cross-validation provided for assessing the performance of FaLK-SVM. On the local models front, a post-selection procedure has been implemented for the QBSVM’s bias (). Essentially, all values within the interval, with a step of, are assessed on the training set to identify the best one. Preliminary experiments have demonstrated that this approach significantly outperforms the computation ofby means ofEq.10. Concerning CS SVM, a C implementation of the model[26]has been utilized. In the local version, the CS SVM executable files are directly invoked from the Python code (after locally mapping the labels to, if necessary). Clearly, more efficient solutions are possible. For example, in the large-scale experiment presented inSectionIV, a custom version of CS SVM has been employed in the local setup. This more efficient version, trained with a slightly modified C code and executed via novel Python code, cannot be fully distributed due to the licensing constraints of the original CS SVM implementation[26].

SECTION: IVEmpirical Evaluation

This section deals with the methods evaluated, the datasets employed, the experimental setup used, and the results achieved. Specifically, the classical side of the experiments has been run on a shared machine equipped with an Intel Xeon Gold 6238R processor operating at 2.20GHz and 125 GB of RAM. Instead, the quantum side has been run on the Advantage system 5.3/5.4 provided by D-Wave, a quantum annealer situated at Forschungszentrum Jülich.

SECTION: IV-AMethods

The methods taken into account in this study are reported inTableI. Specifically, four local methods (TableIa) and four global methods (TableIb) have been considered here. The local ones are combinations of FaLK-SVMl (the version of FaLK-SVM with the local model selection procedure
detailed inSectionII-C1) and different local models: a binary and a multiclass classically-trained SVMs, namely, SVM and CS SVM, and their quantum-trained counterparts, i.e., QBSVM and QMSVM. Notice thatFaLK-SVMl (C)is the original FaLK-SVMl implementation; additional information about the other local methods are available inSectionIII. Regarding the global ones, they correspond to the global application of the aforementioned classically- and quantum-trained SVM models. In particular, for the standard binary SVM, the implementation from LibSVM[27]version 2.88 (the version used in the original FaLK-SVM framework) has been employed. Instead, for QBSVM and QMSVM, the strategies outlined inSectionsII-B1andII-B2for handling big datasets have been utilized. Otherwise, they could have not been trained on the considered datasets, given the dataset sizes used.

SECTION: IV-BDatasets

The methods reported inTableIhave been assessed on datasets taken from the remote sensing domain, a domain in which both FaLK-SVM and the quantum-trained SVMs have already shown good performance[22,9,10,7]. Specifically, the datasets employed here have been generated from the SemCity Toulouse[28]and ISPRS Potsdam[29]datasets, which consist of multispectral images with multiple classes (and have been employed also in the QMSVM article[7]). In practice, the task consists in predicting the class of each pixel.TableIIprovides details on the number of features and classes selected for binary and multiclass classification for both datasets. In particular, for multiclass classification, the number of classes has been restricted to three in order to maximize the number of samples that could be embedded in the annealer. Instead, the datasets sizes employed are experiment-dependent, thus they are presented inSectionIV-C. Regarding the datasets generation, an equal (or approximately equal) number of samples for each class has been randomly selected from tilefor Toulouse and tilefor Potsdam, except in the large-scale experiment. Indeed, in the last experiment, the training set has been created by selecting an equal number of data points for each class from each of thePotsdam tiles labelled as training. Moreover, two distinct test sets have been generated for it: the former comprises data points randomly selected in the usual way from Potsdam tile 5.13 (a non-training tile); the latter, intended for visualization, encompasses all the data points belonging to the classes of interest within apixels square in the same tile.

aThe occurrences of the three classes in this test set are 389900, 343438, and 137850, respectively.

SECTION: IV-CExperimental Setup

In this work, four experiments with different objectives have been carried out. In detail, in the first experiment, the performance of all considered binary classification methods is assessed and compared. In the second one, the same is done with the multiclass classification methods. Instead, in the third experiment, the performance scaling of the local and global fully-classical methods (both binary and multiclass) is analysed; the methods involving quantum-trained models have been omitted since the quantum annealing time consumption would have been excessive. In the final experiment, the performance of all multiclass classification methods, also the ones involving quantum-trained models, are assessed (and visualized) on a large-scale dataset.

In the first three experiments, the performance of the methods have been evaluated using a-fold cross-validation procedure with ten folds (). In practice, the input dataset is partitioned intosubsets, also known as folds. Then,folds constitute the training set, whereas the remaining one serves as the test set. This last step is iterated until each fold has been utilized once as the test set. In particular, thestratified-fold cross-validation, trying to preserve the original class ratio in the folds, has been used here. In addition, to have a fair comparison, the same datasets splits have been employed for all methods. Conversely, in the final experiment, no cross-validation procedure has been used, since the input data was already divided into training set and test sets. The datasets sizes used for each of the four experiments are detailed inTableIII; as already explained inSectionIV-B, the large-scale experiment differs in the dataset generation procedure employed. Additionally, in all experiments, a data standardization procedure (involving the subtraction of the mean and the division by the standard deviation) has been applied to the training and test data features before training and running the (local/global) methods.

Regarding the parameters values employed for the various methods, they are detailed inTableIV. Let us consider first the binary classification methods (TableIVa). The training neighborhood size () for the local methods has been set to 80, a value close to the maximum number of samples that can be embedded in the present quantum annealers with the QBSVM QUBO formulation (considering the values ofand, and finding the embedding for a complete matrix). In addition, a relatively-high degree of local models overlap (regulated by) has been utilized. Concerning FaLK-SVMl’s local model selection, 8 local models () and 5 folds (internal) have been used; additionally, allsamples have been employed in the assessment of thelocal models. Specifically, the grid search has been applied only to the Gaussian kernel width, to find the best value betweenand, withcorresponding to the usage of the median of the distances in the neighborhood as the kernel width. Therefore, with, each local SVM model could have a different kernel width value (more details can be found in the FaLK-SVM article[16]). Conversely, for the global methods,has been fixed to(as their implementations do not support the usage of the median as the kernel width). To ensure a fair comparison between classically- and quantum-trained models, the SVM cost parameter () has been set to 3 (for QBSVM,is determined byand). Concerning the QBSVM-specific parameters, the encoding basis () and the number of binary variables per coefficient () have been set to small values, to enable the embedding of an adequate number of training samples. Furthermore, the penalty coefficient () has been set to 1 (the same value employed for QMSVM, where it is denoted as), and the best 100 solutions found by the annealer have been taken into account for averaging (). Lastly, for the global application of QBSVM, a stratified training data split has been used, with each slice (except the last one) having a number of samples equal to.

Similar considerations apply to the multiclass classification methods (TableIVb). Indeed, the training neighborhood size () for the local methods has been set to 24, a value close to the maximum number of samples that can be embedded in the current quantum annealers with the QMSVM QUBO formulation (considering the values ofand, and finding the embedding for a complete matrix). Concerning the local model selection, 3 folds (internal) have been utilized, due the smaller number of samples involved. Moreover, in the large-scale experiment, 10 local models () have been used instead of 8. Instead, the CS SVM cost parameter () has been set to 1 for a fair comparison with the QMSVM-based methods. In fact, the following relationship holds:. Regarding the QMSVM-related parameters (), the same configuration employed in the QMSVM article[7](whereis denoted as) has been used here. The accuracy threshold definition () and thevalue (10) for weighting thebest solutions returned by the annealer (on the local-neighborhood) have also been adopted from that work. In contrast, theused to prune the small QUBO matrix coefficients has been set to a high value (), rendering the pruning procedure ineffective (the embedding is computed for a complete matrix here). Eventually, for the global application of QMSVM, a stratified random selection of the training samples has been used, with a number of chosen samples equal to.

The quantum annealing parameters employed in the experiments are detailed inTableV. Specifically, this is the same configuration used in the QMSVM article[7]. With the setup employed in this article, training a single QBSVM model requires approximatelyof quantum annealing time (the number of binary variables involved is 160). A slightly shorter time interval is necessary for a QMSVM model (for which the number of binary variables is 144).

SECTION: IV-DResults

The performance metric chosen for the methods evaluation is the classification accuracy, which is given by

In particular, in the first three experiments, employing the-fold cross-validation, the accuracy calculated on the entire dataset (considering the predictions from themodels) is reported. Given that a stratified-fold cross-validation has been used, the folds may not have precisely the same number of elements. Consequently, there could be a small discrepancy between the reported accuracy and the average accuracy over folds. Nevertheless, this difference is negligible. Conversely, in the last experiment, the accuracy achieved on the two test sets is presented. Moreover, for the second test set, which is not (class-) balanced, two additional metrics are reported. These metrics are the balanced accuracy[30], corresponding to the average recall over classes, and the F1 score[31](namely, the harmonic mean of precision and recall) averaged over classes.

In the first experiment, the performance of the binary classification methods have been assessed. The results achieved are reported inTableVI. In practice, in the case of Toulouse, all methods have obtained good results, but the entirely-classical methods have demonstrated superior performance overall, and the local methods have outperformed their global counterparts. Conversely, in the case of Potsdam, the methods have obtained worse results overall, with the classical SVM achieving the best performance, and FaLK-SVMl (QB) outperforming its classical counterpart (albeit not by much). Concerning QBSVM, it has shown the worst performance among the evaluated methods also in this case. Therefore, the local application of QBSVM has proven effective. In fact, it has achieved results not too far from, if not better than, its classical counterpart.

In the second experiment, the performance of the multiclass classification methods have been assessed. The results are reported inTableVII. Let us focus first on the smaller datasets (size 150), for which the average number of local models aligns with that of the binary classification methods in the first experiment. Specifically, in the case of Toulouse, the local methods have obtained the best results and the entirely-classical ones (both local and global) have outperformed their quantum-trained counterparts. The overall results obtained are good. Instead, in the case of Potsdam, the accuracy values are lower, yet the trend is similar. The exception is represented by FaLK-SVMl (QM), which has performed worse than not only FaLK-SVMl (CS) but also CS SVM. Nevertheless, with larger datasets (size 500), FaLK-SVMl (QM) has been the top-performing method, surpassing both FaLK-SVMl (CS) and CS SVM. Regarding the performance-based ordering of the other methods, it is the same. Overall, the larger dataset size has proven advantageous, particularly in the case of Toulouse. Eventually, even in this experiment, the global quantum-trained model (QMSVM) has exhibited the worst performance among the methods evaluated. In summary, this second experiment has proven the efficacy of locally applying both classically- and quantum-trained single-step multiclass SVMs, with the quantum-trained ones being slightly better in the case of larger datasets.

In the third experiment, a performance scaling analysis has been carried out on the classical (binary and multiclass) local methods, taking into account their global counterparts for comparison. In fact, in the previous experiments, FaLK-SVMl (QB) and FaLK-SVMl (QM) have obtained results not too far (except in one instance) from their classical counterparts, which can then be used as indicators of performance. Moreover, the quantum annealing time consumption would have been excessive for the available resources.

The results are showcased inTablesVIIIandIX. Let us consider binary classification (TableVIII) first. In the case of Toulouse, the performance of both FaLK-SVMl (C) and SVM have been quite stable while increasing the dataset size, with little variations (worsening and improvement, respectively) compared to the baseline size of 500. However, the accuracy values for dataset size 500 were already really high. In contrast, in the case of Potsdam, where the initial performance were worse, an improvement has been observed for both FaLK-SVMl (C) and SVM. Specifically, the improvement has been more marked yet less consistent for FaLK-SVMl (C), and less marked but more consistent (after an initial drop) for SVM. Regarding multiclass classification (TableIX), the scenario is the following: in both cases (Toulouse and Potsdam), the performance of FaLK-SVMl (CS) have almost always improved while increasing the dataset size; conversely, the performance of CS SVM have either significantly improved at the beginning and then remained quite stable (Toulouse) or fluctuated around the initial value (Potsdam). In addition, despite the slight drop for sizes 2000 and 10000, the enhancement for FaLK-SVMl (CS) has been more marked for Toulouse. Essentially, this experiment has proven that local methods can leverage larger datasets, particularly FaLK-SVMl (CS), which has outperformed CS SVM in all tests. In contrast, FaLK-SVMl (C) has been outperformed by SVM in almost all cases (albeit not by much), but has shown good stability when its performance have not improved (Toulouse).

In the last experiment, the performance of all multiclass classification methods have been assessed (without-fold cross-validation) on a large-scale dataset based on Potsdam, featuring one training set and two test sets (created as explained inSectionIV-B). The objective is to showcase the performance achievable by locally applying quantum-trained SVM models in a large-scale real-world scenario. Due to the restricted quantum annealing resources available, only multiclass classification and Potsdam have been taken into account.

The results achieved on the first test set are presented inTableX. Specifically, the local methods have outperformed the global ones, and the entirely-classical methods have demonstrated superior performance compared to their quantum-trained counterparts. This last point seems to contradict the observations made inSectionIV-D2about the local methods, with FaLK-SVMl (QM) performing better than FaLK-SVMl (CS) on larger datasets. Nevertheless, the performance differences are relatively small. Furthermore, in this case, no-fold cross-validation has been utilized. In fact, the training set has been constructed by randomly sampling data points from various tiles, and the test data points have been randomly chosen from a different tile. Therefore, the task is somehow different. Despite this aspect, these first results align well with the expectations based on the outcomes of the previous experiments. Actually, a largervalue (100, with) has also been evaluated for FaLK-SVM (CS) in this same setup. The performance obtained were slightly worse (accuracy = 73.6%), but CS SVM has already shown that it does not really take advantage of larger training sets, particularly in the case of Potsdam (seeSectionIV-D3). Regarding the second test set, the results are presented inTableXI. Unexpectedly, CS SVM has obtained the highest accuracy on this second test set, performing better than both local methods. However, given the unbalanced nature of this test set, different performance metrics should be taken into account. Here, the balanced accuracy and the average F1 score (over classes) have been considered (their values are reported in the same table). In detail, according to these metrics, both FaLK-SVMl (CS) and FaLK-SVMl (QM) have outperformed CS SVM, aligning with the trend observed for the first test set and in the previous experiments. This phenomenon is caused by CS SVM’s tendency to predict more frequently the two most represented classes (building and low vegetation) in the test set, misclassifying the less common one (tree). This behaviour can be easily noticed inFig.1, where the predictions of the different methods are shown. In conclusion, this final experiment has proven the practical applicability of local quantum-trained SVMs (in particular, the multiclass one) in a large-scale scenario. In fact, the results obtained by FaLK-SVMl (QM) are quite good and comparable to those achieved by its classical counterpart.

SECTION: VConclusion

In this article, the local application of quantum-trained SVM models, with the aim of enabling their usage on large datasets and enhancing their performance, has been introduced and empirically assessed in the remote sensing domain. Specifically, here, a method for efficient local SVMs (FaLK-SVM) has been interfaced with two quantum-trained SVM models: an SVM model for binary classification (QBSVM) and an SVM model for multiclass classification (QMSVM). In addition, for comparison, FaLK-SVM has been paired for the first time with a classical single-step multiclass classification model (CS SVM). Details about the implementation, such as the post-selection procedure for QBSVM’s bias, and the experimental setup have been provided. The results have demonstrated the effectiveness of the approach, with the local applications of QBSVM and QMSVM obtaining results not too far from, if not better than, their classical counterparts. The local application of CS SVM has also yielded good results, consistently outperforming its global counterpart. Furthermore, the performance scaling analysis carried out on the classical local methods, serving as performance indicators for the quantum-trained ones, has revealed their ability to take advantage of larger datasets. Ultimately, the last experiment has proven the practical applicability of the local quantum-trained SVMs (specifically, the multiclass one) in a real-world large-scale scenario.

Future work includes the assessment of these local quantum-trained methods on datasets taken from a different domain, using different parameter configurations and a higher number of reads. Another interesting possibility consists in the development of a local version of the quantum-trained support vector regression model[8](not considered here).

SECTION: Acknowledgments

This work was supported by Q@TN, the joint lab between University of Trento, FBK-Fondazione Bruno Kessler, INFN-National Institute for Nuclear Physics and CNR-National Research Council. In addition, this work was partially supported by project SERICS (PE00000014) under the MUR National Recovery and Resilience Plan funded by the European Union - NextGenerationEU. The authors gratefully acknowledge the Jülich Supercomputing Center (https://www.fz-juelich.de/ias/jsc) for funding this project by providing computing time on the D-Wave Advantage™ System JUPSI through the Jülich UNified Infrastructure for Quantum computing (JUNIQ). Lastly, the authors gratefully acknowledge the Italian Ministry of University and Research (MUR), which, under the initiative ”Dipartimenti di Eccellenza 2018-2022 (Legge 232/2016)”, has provided the (classical) computational resources used in the experiments.

SECTION: References