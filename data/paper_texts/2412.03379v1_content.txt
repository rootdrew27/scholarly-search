SECTION: MTVNet: Mapping using Transformers for Volumes – Networkfor Super-Resolution with Long-Range Interactions
Until now, it has been difficult for volumetric super-resolution to utilize the recent advances in transformer-based models seen in 2D super-resolution. The memory required for self-attention in 3D volumes limits the receptive field. Therefore, long-range interactions are not used in 3D to the extent done in 2D and the strength of transformers is not realized. We propose a multi-scale transformer-based model based on hierarchical attention blocks combined with carrier tokens at multiple scales to overcome this. Here information from larger regions at coarse resolution is sequentially carried on to finer-resolution regions to predict the super-resolved image. Using transformer layers at each resolution, our coarse-to-fine modeling limits the number of tokens at each scale and enables attention over larger regions than what has previously been possible. We experimentally compare our method, MTVNet, against state-of-the-art volumetric super-resolution models on five 3D datasets demonstrating the advantage of an increased receptive field. This advantage is especially pronounced for images that are larger than what is seen in popularly used 3D datasets. Our code is available at.

SECTION: Introduction
In recent years, super-resolution (SR) and other vision tasks have seen significant improvements via usage of vision transformers (ViTs). Although ViTs achieve state-of-the-art (SOTA) performance in 2D SR, few studies have attempted applying ViTs for volumetric SR. Part of the success of ViTs is their increased receptive field compared to Convolutional Neural Networks (CNNs), enabling inferences based on broader image context. In volumetric SR, ViTs are challenged by the cubic growth in tokens required to process larger 3D image contexts. Although window-based attention improves the quadratic complexity of attention mechanisms, the complexity of 3D data still limits the receptive field of volumetric ViT-based models. Because of this disadvantage, the performance gap of CNNs vs. transformer-based architectures for volumetric SR has yet to be fully understood.

Several works have studied visual enhancement of 3D medical data such as MRI (magnetic resonance imaging) and CT (computed tomography) by upscaling each slice independently. While such approaches circumvent the complexity issues of volumetric SR, not fully considering the 3D context sacrifices performance and risks inter-slice discontinuities.

Current brain MRI benchmark datasets for evaluating volumetric SR are relatively low-resolution, limiting the benefits of a larger receptive field. Advancements in medical imaging technology enable higher spatial resolution, resulting in larger volumes where volumetric SR can benefit from long-range contextual information. Given the potential of SR in clinical settings and the increasing interest in applications like multi-resolution synchrotron imaging, there is a need for volumetric SR methods designed specifically for high-resolution (HR) 3D data.

Aside increasing contextual information in volumetric SR, recent studies in 2D SR have shown that the window-based attention mechanism of the Swin-Transformeris not ideal for capturing relationships across distant image regions. Using Local Attribution Mapping (LAM),showed that strengthening long-range information exchange can lead to significant performance gains. Similarly, recent studies in ViT architectures have focused on modeling long-range interactions to increase performance.

To address these limitations, we present MTVNet, a volumetric SR approach based on multi-scale image representation and hierarchical attention to enhance long-range information propagation. Our MTVNet broadens the receptive field by expanding the contextual input beyond the prediction area, see. We hypothesize that for the SR task, image regions near the prediction region provide the most important contextual information while more distant regions still provide relevant information, but contribute less. Consequently, we design a coarse-to-fine feature extraction and tokenization scheme with progressively less computational resources allocated towards regions further from the prediction area, enabling us to increase the volumetric input size without exceeding GPU memory.
Furthermore, inspired by FasterViTand SwinV2, we propose an efficient shifting hierarchical attention mechanism suitable for volumetric image processing. This approach leverages specialized carrier tokens (CATs) that contain compact feature summaries of larger attention windows. Using full attention in the highly compressed CAT domain, our model improves modeling of long-range spatial information, further improving SR performance in volumetric data.

We compare our proposed MTVNet against several volumetric SR approaches on four brain MRI benchmark datasets and one high-resolution CT based dataset. Extensive experiments show that convolutional models still outperform ViT-based architectures in lower resolution datasets. Although on high-resolution 3D data with meaningful long-range image dependencies, our proposed MTVNet outperforms all other volumetric SR approaches. We anticipate that our proposed multi-contextual approach could greatly benefit other volumetric image tasks.

SECTION: Related Work
SECTION: Learning-based super-resolution
The advantages of learning-based SR over classical interpolation methods were first demonstrated by SRCNN proposed by. Since then, several CNN-based SR models have been proposed to improve performance and computational efficiency. Despite the success of CNNs, many vision tasks such as image classification, object detection, segmentation, and SR have seen improvements using vision transformers (ViTs). SwinIR bywere among the first to demonstrate the superiority of transformers over convolution-based models for SR by incorporating the Swin Transformerin a residual network scheme.proposed cross attention of overlapping window partitions and channel attention mechanisms to enable activation of more input pixels.suggested HMANet, which integrates a grid-shuffling scheme with window-based attention to model cross-area similarity for enhanced image reconstruction. Very recently,have suggested combining Swin-transformer layers and gating mechanisms in a densely-connected structureto alleviate information bottlenecks.

Concurrently, improvements to the vision transformer backbone have been proposed to enable efficient processing of HR image data.proposed SwinV2, featuring improved normalization and a more efficient attention mechanism using cosine similarly. This work was later applied to SR byin Swin2SR. In CrossViT, multi-scale tokenization and efficient cross-attention mechanisms were used to extract and fuse feature representations at different image scales. Recently,proposed FasterViT, an efficient vision transformer including local window attention and global attention. Since these models focus on 2D images, most do not scale well in 3D, requiring substantial modifications to be applied for volumetric data.

SECTION: Super-resolution for 3D volumes
3D SR methods operate slice-wise or volumetric. Slice-wise methods predict each slice independently and typically leverage model architectures from 2D SR. While these models can handle entire slices simultaneously and often support deeper network architectures, they lack cross-slice information, which can lead to discontinuities between slice predictions.

Volumetric SR methods fully utilize the context in 3D, achieving better overall performance than slice-wise methods because of improved inter-plane predictions, but with much higher computational costs. Inspired by SRCNNand SRGAN,andproposed three-dimensional adaptations of convolutional SR models and demonstrated the potential of volumetric SR over slice-wise approaches. Research in volumetric SR has since grown rapidly and several methods have been proposed to improve efficiency and performance. These approaches are very similar to classical SR in that they aim to predict HR reconstructions from isotropically degraded images, only on volumetric instead of 2D images. However, several other approaches for volumetric SR also exist. For instance, to account for the fact that clinical MR images often feature high in-plane and low through-plane resolution, axial SR modelshave been proposed to increase slice count while preserving in-plane resolution.

To alleviate the limitation of fixed upscaling factors, arbitrary scale SR based on Implicit Neural Representationhave been proposed. Another branch of volumetric SR is multi-contrast modelsthat leverage information from multiple MRI modalities (T1- and T2-weighted images).

Transformer-based architectures have also been proposed for volumes. SuperFormermerged feature embeddings and volume embeddings using a volumetric transformer-based network structure similar to SwinIR. Also inspired by SwinIR,implemented a transformer-based GAN (generative adversarial network) model for axial super-resolution using residual swin transformer blocks. The CFTN modelemployed 3D residual channel attention blocksand transformers to capture global cross-scale dependencies between multi-scale feature embeddings.proposed a 2D slice-wise multi-modal arbitrary scale SR model featuring a rectangle-window cross-attention transformer to model longer-range image dependencies.
Despite the growing interest in volumetric transformer-based SR models, several of the improvements seen in 2D SR cannot be effectively applied in 3D due to memory limitations. Our work seeks to leverage these developments to enhance volumetric SR while simultaneously addressing the challenge of limited contextual information, a critical bottleneck in volumetric SR.

SECTION: Methods
SECTION: Network architecture
The architecture of MTVNet consists of three levels of volumetric image processing:,, and, see. The network levelsandextract features from image regions surrounding the SR prediction area and merges these features into. These features serve as a prior for the network level, enabling conditioning of the SR output based on the surrounding image context.

. Our MTVNet uses shallow feature extraction (SFE) modules for initial processing at each level. Given an input volume, each SFE module expands the channel dimension usingconvolutional layers, producing shallow features. The output from each stage’s SFE is cropped and passed to the next level, allowing the model to leverage the features of previous SFE modules.

.
During patch embedding, shallow features are projected and tokenized into differently-sized volumetric image patches. The levelsanduse larger patch sizes to cover wider image regions, reducing the number of tokens required for processing these volumes. Image token embeddings (ITEs)are obtained via astrided convolution, wherecorresponds to the network level. For subsequent processing, we partition the ITEs into attention windows oftokens. The corresponding carrier token embeddings (CATs)are initialized from the ITEs using convolution with stride and kernel size, whereis a factor determining the number of CATs for each attention window.

.
Deep feature extraction is performed within each level using DCHAT (densely connected hierarchical attention) blocks to extract high-frequency spatial information. The DCHAT blocks for each level are connected in a residual scheme to produce DCHAT groups consisting of up to three DCHAT blocks. In the case of multiple levels, cross-attention mechanismsare used to merge token embeddings into subsequent network levels, facilitating the propagation of multi-scale information.

.
In the final stage, token upsampling is performed via a deconvolution layer, transforming the patch embeddings back into the image space. These features are further refined in a pre-reconstruction stage before being fused with the shallow features through a long skip-connection. The fused features are then upsampled using a 3D pixel-shuffle layer. We employ a 3D pre-convolution layer initialized according to the ICNR method described into prevent checkerboard artifacts during pixel-shuffling.

SECTION: Dense-Connected Hierarchical Attention block
For efficient extraction of volumetric image features, we propose a DCHAT block, see. Inspired by, our DCHAT block employs a densely connected structure of volumetric transformer layers, LeakyReLU activations, and convolutions. To preserve the feature space of ITEs and CATs, we process each set of tokens using separate sets of skip connections and convolutions. Additionally, we match the embedding dimension of ITEs and CATs throughout each block to equally promote learning of progressively complex features. As in DRCTwe utilizeconvolutions as gating mechanisms between transformer layers to filter redundant features, improving efficiency and enabling feature transition between DCHAT blocks.

SECTION: Shifting Hierarchical Attention Transformer
Inspired by FasterViTand SwinV2, we design an SVHAT (shifting volumetric hierarchical attention transformer) layer for concurrent processing of ITEs and CATs. Similar to FasterViT, SVHAT uses a combination of full attention and windowed attention to extract hierarchical image features. The attention mechanisms used in SVHAT are illustrated in. First, full attention in the CAT space allows global information flow across attention windows, see.
Next, we concatenate each attention window’s corresponding CATs and ITEs, providing each attention window access to its set of CATs.
Windowed attention is then applied jointly to the ITEs and CATs to capture token dependencies, with the CATs conveying global information from other attention windows, see. This alternating attention procedure allows global feature exchange between local attention windows, improving long-range information flow. To further enhance information exchange, we reintroduce shifted window-based attention into the attention framework proposed in FasterViT, see. Before window partitioning of ITEs and CATs, we perform 3D cyclic-shifting to allow attention of tokens in neighboring windows. To account for the presence of CATs, we shift both the image space and CAT space byandvoxels, respectively. This shifting conserves the alignment of the two feature spaces. Attention masking is applied to drop interactions between non-adjacent tokens in the ITE/CAT space.

We compute attended carrier token embeddingsat network leveland transformer layeras follows:

where,are learnable channel-wise scaling factors,is the multi-headed self-attention mechanism,denotes Layer Normalization, andis the multi-layer perception.

After CAT attention, we compute attention of ITEs and CATs using windowed self-attention, see. The CATs are window partitioned and concatenated with their corresponding set of ITEs to produce sequences oftokens for each attention window.
Inspired by SwinV2, SVHAT employs a post-normalized Shifted Window based Self-Attention (SW-MSA) procedure. Window-attended CATs and ITEsare computed as:

The ITEs and CATs are then separated again to ensure compatibility with subsequent SVHAT layers.

Prior to the attention mechanisms for ITEs and CATs (described inand), SVHAT uses multi-head cross-attention (MCA) layers to facilitate information exchange across network levels. Each cross-attention layer implements a two-layer MLP to ensure dimension compatibility between cross-scale token sequences. Then, MCA is applied to capture relationships between current- and previous-level token embeddings. Exploiting the small size of the CAT space, we compute cross-attended CATsusing full MCA between all current-level and previous-level CATs:

wheredenotes the final set of CATs from the previous network level. A similar window-based multi-head cross-attention (W-MCA) mechanism is used for capturing relationships between current- and previous-level ITEs, see equation. The cross-attended ITEsare computed as follows:

wheredenote the final set of ITEs from the previous network level. Finally, the cross-attended token embeddings are fused using a residual scheme:

Here,anddenote ITEs and CATs before fusion. To reduce the complexity of MTVNet, cross-attention is performed only in the first SVHAT layer of every DCHAT block.

SECTION: Experiments
SECTION: Experimental setup
. We use four public MRI datasets and one CT-based dataset to train and evaluate our proposed MTVNet:
The Human Connectome Project (HCP) 1200 Subjects dataset, the IXI dataset, the Brain Tumor Segmentation Challenge (BraTs) 2023and Kirby 21. These datasets consist of multi-modality image volumes acquired using 1.5T-3T MRI platforms with a volume size ofvoxels. The last dataset considered is the Femur Archaeological CT Superresolution (FACTS) dataset, which consists of 12 registered 3D volume pairs of archaeological femur bones scanned using clinical-CT and micro-CT. The FACTS dataset features large volumes (voxels), enabling us to showcase the benefits of additional contextual information. Two SR tasks are considered using this dataset: In FACTS-Synth, we use downsampled versions of the micro-CT images as the SR model input, while FACTS-Real instead uses the clinical-CT images. The training/test splits for all datasets will be detailed in supplementary material.

. To demonstrate the effectiveness of our proposed method, we evaluate the performance of MTVNet against six other volumetric SR models: mDCSRN, EDDSR, MFER, RRDBNet3D, SuperFormer, and ArSSR.
We modify mDCSRN and SuperFormer, which were originally designed to restore images degraded by 3D k-space truncation, a method that simulates LR MRI acquisition, by extending these models using the same 3D pixel-shuffle upsampling module used in MTVNet.
For EDDSR, MFER, RRDBNet3D and ArSSR, we use the authors’ suggested upsampling approach.

. We train all models from scratch on each dataset for 100K iterations on a single NVIDIA A100 80GB GPU. For ArSSR, we collate sets ofrandomly sampled HR/LR point pairs from 15 patches for each batch.
The remaining models use a batch size of 5 and LR patch size of, except MTVNetandwhere we useand, respectively. All models are optimized using ADAMwithand. We use a multi-step learning rate scheduler, halving the learning rate once after 50k, 70k, 85k, and 95k iterations. The model parameters are optimized using a simple L1 loss metric. HR/LR pairs are generated using volumetric blurring followed by downsampling via linear interpolation. In FACTS-Real, we use the clinical-CT images as LR input but omit blurring since the LR images are already smooth.

. For evaluation, we reconstruct all volumetric samples in the test set of each respective dataset using strided aggregation of SR predictions. We tile each SR prediction using an overlap ofvoxels whereis the upscaling factor and smooth the overlapping prediction areas using a Hanning window. The performance metrics Peak-Signal-to-Noise Ratio (PSNR), Structural Similarity Index Measure (SSIM), and Normalized Root Mean Square Error (NRMSE) are computed slice-wise in the axial direction and averaged over all samples in each dataset, ignoring any slices where the foreground occupies less than 25% of the voxels.

SECTION: Implementation details
All MTVNet configurations use a learning rate ofwith no weight decay. For the brain MRI datasets (HCP 1200, IXI, BraTs 2023, and Kirby 21), we apply MTVNetwith two levels featuring two DCHAT blocks in the first level and three DCHAT blocks in the second level, each with six SVHAT layers. For the FACTS dataset, we use MTVNetconsisting of a third level with one DCHAT block on top of. In all configurations of MTVNet, the number of shallow features and intermediate features is set to, and the number of compressed features in each skip-connection is set to. In MTVNetwe use patch sizes,andfor volumetric patch embedding at each subsequent network level while in MTVNet, patch sizes are set toand. The attention window size and the size of the CAT space for each window are set toand, respectively. To reduce memory usage during upsampling, we halve the number of features channels in MTVNet, mDCSRN, SuperFormer, and RRDBNet3D before upsampling.

SECTION: Quantitative results
shows a quantitative comparison between MTVNet and six other SOTA volumetric SR models ArSSR, EDDSR, MFER, mDCSRN, SuperFormer, and RRDBNet3D. In the brain MRI benchmark datasets HCP 1200, IXI, BraTs 2023, and Kirby 21 our MTVNet achieves second best performance across all scales. Contradicting the findings of, we observe the purely CNN-based method RDDBNet3D achieving better performance metrics than the transformer-based SuperFormer and our method.
We reason that the advantage of RRDBNet3D may be due to local image dependencies being predominant in these datasets, limiting the benefit of the broader receptive field offered by ViTs.
Still, our MTVNet achieves only slightly lower performance than RRDBNet3D while there is a greater performance gap between our method and SuperFormer, especially inupscaling.

In the FACTS dataset, where we can leverage the multi-contextual architecture of our proposed method, we observe several new trends: In FACTS-Synth, our proposed MTVNet outperforms all other methods by a significant margin at all scales. Compared with SuperFormer, MTVNet improves PSNR scores bydBdB and bydBdB when compared with RRDBNet3D. These improvements illustrate that additional contextual information enables significant SR performance gains in high-resolution volumetric images. In FACTS-Real where the clinical-CT images are used as LR model input, we observe CNN-based methods RRDBNet3D, MFER, and our MTVNet achieve the best results. We hypothesize that this discrepancy in performance results from the domain shift between micro-CT and clinical-CT, which largely deprives the clinical-CT images of long-range image dependencies.
The trabecular structure in the clinical-CT images is largely indistinguishable, whereas the LR micro-CT images show more distinct repeatable patterns that could offer more valuable contextual information. Therefore, we surmise that the performance gains of incorporating additional long-range information in MTVNet diminishes for this SR task.

SECTION: Qualitative results
shows a visual comparison of SR predictions on scalefor HCP 1200, IXI, FACTS-Synth, and FACTS-Real. We find that MTVNet produces faithful reconstructions of structures and patterns across all datasets. Compared with ArSSR, EDDSR, MFER, mDCSRN, and SuperFormer, our MTVNet produces notably sharper features while producing similar results as RRDBNet3D. In the Brain MRI datasets HCP 1200 and IXI, we find that many methods struggle to reconstruct anatomical details while RRDBNet3D and our MTVNet produce the clearest results. In FACTS-Synth, we find that other models tend to produce unnaturally blurred textures, whereas our proposed MTVNet suffers much less from these artifacts.

SECTION: Ablation experiments
We study the effect of our proposed features of MTVNet, including the addition of CATs, shifted window hierarchical attention, and multi-contextual network levels.shows a quantitative comparison on the BraTS 2023 dataset usingupscaling. Replacing the SW-MSA procedurewith CAT-based hierarchical attention results in slight performance gains across all metrics, though only marginally compared to the gains seen in FasterViT. Since CATs contain compressed feature summaries of each attention window, we hypothesize that this compression process discards most of the pixel-level information essential for SR. These details are less critical in image classification, hence why CATs have been observed to result in higher performance gains in this domain. Incorporating our modified SW-MSA mechanism with CATs improves the receptive field and positively impacts performance. Finally, adding multi-contextual information in MTVNet yields the largest relative improvement, increasing PSNR bydBdB over other configurations. Notably, even with the relatively small volumetric samples (voxels) in BraTS 2023, MTVNet benefits from multi-contextual information. These results highlight the value of additional contextual information, even in small-scale volumetric SR.

SECTION: Memory footprint of MTVNet
shows the memory footprint required by SuperFormer, RRDBNet3D, and MTVNet using different volumetric input resolutions. Using a single level, MTVNetrequires less memory than SuperFormer and RRDBNet3D. Furthermore, provided the prediction area is fixed to, adding more network levels to MTVNet allows processing volumetric input sizes far exceeding the capabilities of other volumetric SR architectures.

SECTION: Conclusion
In this work, we present MTVNet, a transformer-based approach for volumetric SR tailored for high-resolution 3D data. To overcome the challenge of limited contextual information in volumetric SR, we propose a multi-contextual network structure with a coarse-to-fine feature extraction and tokenization scheme. This approach reduces the number of tokens needed to cover large volumetric regions, allowing our model to process significantly larger input sizes than competing methods.
To enhance long-range information exchange in the expanded input volume, we implement a novel shifting volumetric hierarchical attention transformer (SVHAT) layer inspired by FasterViTand SwinV2that employs a combination of full and window-based attention to capture both global and local image dependencies. We evaluate the performance of MTVNet against other volumetric SR approaches across several data domains, including brain MRI data and high-resolution CT data. Based on extensive experiments, we make several conclusions: In contradiction with the current research trends in 2D SR, we observe CNN-based models outperform transformer-based models in certain data domains. The effectiveness of CNN-based SR models is especially pronounced in lower-resolution 3D samples where the larger receptive field of transformers cannot be leveraged as effectively. Nevertheless, our proposed MTVNet with extra contextual processing layers outperforms all other models given high-resolution 3D data with meaningful long-range image dependencies.

We surmise that our multi-contextual approach for volumetric image processing could be greatly beneficial for other vision applications such as segmentation, classification, and recognition in volumetric images.

SECTION: References
SECTION: Details of SVHAT layer
SECTION: Datasets
SECTION: Visual comparisons using LAM
SECTION: More visual comparisons