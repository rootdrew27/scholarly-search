SECTION: In-Situ Melt Pool Characterization via Thermal Imaging for Defect Detection in Directed Energy Deposition Using Vision Transformers

Directed Energy Deposition (DED) has significant potential for rapidly manufacturing complex and multi-material parts. However, it is prone to internal defects, such as lack of fusion porosity and cracks, that may compromise the mechanical and microstructural properties, thereby, impacting the overall performance and reliability of manufactured components. This study focuses on in-situ monitoring and characterization of melt pools closely associated with internal defects like porosity, aiming to enhance defect detection and quality control in DED-printed parts. Traditional machine learning (ML) approaches for defect identification require extensive labeled datasets. However, in real-life manufacturing settings, labeling such large datasets accurately is often challenging and expensive, leading to a scarcity of labeled datasets. To overcome this challenge, our framework utilizes self-supervised learning using large amounts of unlabeled melt pool data on a state-of-the-art Vision Transformer-based Masked Autoencoder (MAE), yielding highly representative embeddings. The fine-tuned model is subsequently leveraged through transfer learning to train classifiers on a limited labeled dataset, effectively identifying melt pool anomalies associated with porosity. In this study, we employ two different classifiers to comprehensively compare and evaluate the effectiveness of our combined framework with the self-supervised model in melt pool characterization. The first classifier model is a Vision Transformer (ViT) classifier using the fine-tuned MAE Encoder’s parameters, while the second model utilizes the fine-tuned MAE Encoder to leverage its learned spatial features, combined with an MLP classifier head to perform the classification task. Our approach achieves overall accuracy ranging from 95.44% to 99.17% and an average F1 score exceeding 80%, with the ViT Classifier outperforming the MAE Encoder Classifier only by a small margin. This demonstrates the potential of our framework as a scalable and cost-effective solution for automated quality control in DED, effectively utilizing minimal labeled data to achieve accurate defect detection.

SECTION: 1Introduction

Directed Energy Deposition (DED)[1]is widely used in aerospace and biomedical industries for its ability to precisely repair and build high-value components. This laser-based additive manufacturing process utilizes a focused laser beam to melt and fuse metal powder or wire layer by layer, creating a moving melt pool[2]at the laser-material interface as shown in Figure1. However, these intricate interactions can sometimes introduce defects like porosity, which compromises the mechanical integrity and reliability of the final parts[3]. Porosity in DED components arises primarily from entrapped gas bubbles within the melt pool, originating either from gases trapped in the feedstock or from incomplete fusion between layers[4]. As the melt pool solidifies, these bubbles may become trapped, forming voids that impact the material’s strength. The melt pool’s characteristics such as size, shape, and temperature distribution are crucial in shaping the microstructural and mechanical properties of the part[3]. In-situ characterization of the melt pool provides valuable insights into defect formation, allowing manufacturers to identify and mitigate porosity and other defects[5]. By monitoring melt pool dynamics during the DED process, manufacturers can also enhance quality control and ensure the reliability of their final products.

In recent years, significant efforts have been made to develop in-situ monitoring and characterization methods for melt pool dynamics in laser additive manufacturing (LAM), especially for identifying internal defects within directed energy deposition (DED).[7]introduced a real-time porosity detection framework in DED, leveraging Self-Organizing Map (SOM) clustering on thermal melt pool images, achieving a 96% prediction accuracy. Similarly,[8]proposed a data fusion approach combining PyroNet (CNN) and IRNet (LRCN) models to predict porosity in laser-based AM with high accuracy through decision-level fusion of pyrometer and infrared image data.[9]achieved a porosity classification accuracy of 98.44% by combining thermal history and morphological features of melt pools with a K-Nearest Neighbor (KNN) model.[10]applied a variational autoencoder (VAE) combined with Gaussian mixture modeling (GMM) and K-means clustering on high-resolution thermal images to effectively detect melt pool anomalies in Directed Energy Deposition (DED) with a reported accuracy of 94.52%. In laser wire DED,[11]developed a CNN-based approach using variants of YOLO, for real-time segmentation and statistical analysis of melt pools, enhancing monitoring accuracy. Meanwhile,[12]utilized a high-dynamic range camera with a KNN classifier to classify stability zones for defect prevention, achieving a 13% error rate. Multi-sensor fusion and data-driven models have also shown promise. For instance,[13]introduced a data fusion approach integrating multiple sensors to monitor melt pool characteristics using multiple ML models such as Support Vector Machine (SVM), K-Nearest Neighbor(Knn), Random Foresta dn Multi-perceptron layers (MLP) classifiers, achieving a high true positive rate (90%) and a low false positive rate (1%) for porosity detection.[14]presented an unsupervised online learning method for real-time defect detection in laser metal deposition (LMD) using high-speed pyrometer melt pool imaging, achieving 76% accuracy with K-means and 97% with SOM for classifying images as “healthy” or “anomalous.”In another approach,[15]developed a system for defect diagnosis in DED using a degree of irregularity (DOI) feature index, reaching diagnostic accuracies of 76.65% for normal and 96.92% for balling states, thus surpassing traditional STFT (Short Time Fourier Transform) based 2D CNN models.

More recent developments continue to push the boundaries of in-situ monitoring accuracy.[16]introduced FixConvNeXt, achieving 99.1% accuracy in real-time melt pool state identification for L-DED, along with reduced computational load, offering an efficient solution for online defect detection and control. In a pioneering acoustic-based approach,[17]achieved 89% overall accuracy, 93% for keyhole pore detection, and a 98% AUC-ROC score using CNNs for DED, demonstrating the strong potential of acoustic signals for real-time quality monitoring. Recently,[18]presented a layer-wise anomaly detection method by Gaussian Support Vector Machine classifier (SVM) classifier for DED, achieving up to 96.38% accuracy and a 95.34% F-score for thin-wall specimens, and 85.07% accuracy with a 91.56% F-score for cylindrical specimens, outperforming benchmark methods in both efficiency and precision.[19]introduced a DenseNet-39 model utilizing high-speed camera melt pool images for monitoring melt pool states in Laser-Based Directed Energy Deposition (L-DED). The model achieves 99.3% accuracy in classifying melt pool states as either stable or unstable, effectively reducing the computational burden and processing time, and highlighting its suitability for efficient in-situ process monitoring.[20]used high-speed camera melt pool video data in laser hot wire DED to detect flaws like wire dripping, arcing, oscillation, and stubbing via a ConvLSTM autoencoder. The self-supervised model accurately identifies anomalies by reconstructing, enabling effective real-time monitoring in DED. Table1provides a summary of the different learning methodologies and models employed for defect detection tasks through in-situ melt pool data within the DED process.

Moreover, Table1highlights that Convolutional Neural Network (CNN) based models[20,21,16,8,22,10,19,11]have been at the forefront of advancements in real-time melt pool monitoring and defect detection within DED process. While CNNs, often applied through supervised learning[8]in this domain, have shown success in defect detection using melt-pool features, they come with limitations. These models struggle to perform optimally when faced with data domains characterized by linearly inseparable features or fuzzy object boundaries[23]. Additionally, acquiring large labeled datasets for melt pool monitoring is costly and time-intensive, which limits CNNs’ effectiveness and often leads to overfitting. Transfer learning has[24]emerged as a solution to this problem by leveraging pre-trained models on similar datasets to improve model performance on the smaller, domain-specific data. However, transfer learning is not always effective, especially when there is a significant domain shift or the data distribution differs substantially from the pre-trained model’s original training set. Consequently, there is a need for more adaptable and versatile approaches that can utilize the abundance of unlabeled data generated in manufacturing processes.

Given these limitations of supervised learning models such as CNNs, self-supervised learning (SSL)[25]has become an increasingly popular approach that reduces the reliance on large labeled datasets by enabling models to learn directly from the data through reconstruction. SSL trains models on pretext tasks that generate predictive signals from the data itself, allowing models to learn rich feature representations. These learned features can later be fine-tuned with smaller labeled datasets, making SSL a resource-efficient method in manufacturing contexts where labeled data is scarce. This capability is particularly valuable in Directed Energy Deposition (DED) processes, where labeled data is often limited, and traditional supervised models may struggle to generalize effectively on smaller datasets.

Researchers have explored various SSL techniques, combining them with supervised classifiers or unsupervised clustering of latent features for defect identification tasks[10,20]. However, in the domain of melt pool thermal images for defect detection, unsupervised clustering may be less suitable since the latent features are not usually linearly separable, which can limit the effectiveness of identification without labeled data. Additionally, to apply a supervised classifier on latent features, it is essential to ensure high-quality feature generation to support accurate supervised classification.
To overcome these constraints, we utilize Vision Transformers (ViTs)[26], which are transforming the computer vision landscape with their superior capability to capture intricate spatial relationships and global dependencies within high-resolution images. Unlike CNNs, Vision Transformers (ViTs) divide images into patches and process them sequentially, allowing for enhanced feature extraction at both local and global levels. This capability is especially advantageous in applications requiring high detail and precision, such as melt pool analysis for defect detection. This study employs a state-of-the-art ViT-based Masked Autoencoder (MAE)[27]in a self-supervised learning framework. The MAE model has formed the backbone of several recent foundation models[28,29]in computer vision. This model is used to learn spatial features directly from the unlabeled data. Inspired by the Large Language Models (LLM)[30], the MAE reconstructs missing parts from the masked portions of the input during training, thereby learning robust, representative features. These learned features are then leveraged to train classifiers on a small labeled dataset, enabling accurate identification of melt pool anomalies correlated with porosity.
The summary of our contribution is:

We develop a self-supervised learning approach combined with supervised classification which is well-suited to real-world LAM environments, where unlabeled data, such as thermal images of melt pools, is abundant, while labeled data remains scarce due to the intensive nature of physical experimentation required for labeling.

We minimize computational overhead by using a pre-trained ViT-based Masked Autoencoder (MAE); fine-tuning it on a substantial number of unlabeled thermal images captured from in-situ melt pool monitoring during DED printing.

Our proposed framework employs Transfer Learning to utilize the fine-tuned model’s parameters in training various classifiers on limited labeled data for accurate melt pool classification.

To evaluate our approach, we compare two supervised classifiers: a standard ViT Classifier and a fine-tuned MAE Encoder paired with an MLP classifier, both trained on labeled data and utilizing the fine-tuned parameters and learned features respectively from the self-supervised MAE model. This comparison validates the effectiveness of our framework combining self-supervised learning with different supervised classification strategies by achieving reliable identification of melt pool classes.

The rest of the paper is organized into the following sections. In Section2we briefly describe the experimental setup and in-situ melt pool data collection used in this study. Section3explains the methodology used in our approach with the self-supervised MAE and ViT Classifier models. In Section4we demonstrate the obtained melt pool classification results from using both the classifiers with our setup. Finally, in Section5we summarize our findings and contributions with final remarks.

SECTION: 2Experimentation & Data collection

For data collection, we conduct the experiments using a customized powder-based DED system (AMBIT™Core DED, Hybrid Manufacturing Technology, TX, USA). We mount a thermal imager Pyrometer camera (ThermaViz®TV200, Stratonics) inside the printing chamber for in-situ monitoring and melt pool data capture during printing. Since our DED setup is hybrid, including both additive and CNC machines, we do an off-axis installation of the camera at a viewing angle of 57∘(as shown in Figure2). The focal length is fixed at 42 mm with a working distance ofmm. A Neutral density filter is also calibrated and used to prevent pixel saturation during the in-situ recording by the pyrometer.

In this study, we print two single-track thin-wall Inconel 718 samples, each measuring 40 mm in length and consisting of 10 layers in Figure2, with the scanning direction reversed on every layer to create a linear reciprocating scan pattern. The variations in printing strategies and process parameters are summarized in Table2below. Note that, we have fixed the exposure time of the pyrometer at 0.3 ms optimized by experimentation for both sample printing. The camera records the distribution of thermal gradient of the traveling melt pool during the printing and the frames can be retrieved as individual thermal images. The melt pool zone is defined as the region where the temperature exceeds the material’s melting point. For Inconel 718, this temperature threshold isCelsius. The boundary of the melt pool surface is represented by an isotherm, as illustrated in Figure3. Each pixel in the thermal images represents the corresponding temperature of the melt pool zone and the rest of the area within the field of view of the camera. Then, we perform aX-ray Computed Tomography (XCT) (SkyScanner1272) scan on Sample 2 (from Table2) to identify the locations of the inter-layer pores and label the corresponding melt pool images captured at those specific locations. We find a total of 76 melt pools associated with internal pores and label them as class ‘1’ and the rest 1371 normal melt pools as class ‘0’. Examples of normal and abnormal melt pool thermal images are shown in Figure:3.

SECTION: 3Methodology

Our framework can be divided into two stages. First, we fine-tune a pre-trained MAE model on unlabeled melt pool images in a self-supervised manner. Next, we apply these fine-tuned model parameters to train classifier models on the limited labeled data, enabling binary classification of the melt pool images. These steps are explained in detail in the following sections.

SECTION: 3.1Masked AutoEncoder (MAE)

The Masked Autoencoder (MAE) is a scalable self-supervised learning framework that learns meaningful spatial features from unlabeled image data through image reconstruction. Like a typical autoencoder, MAE maps input data to a latent representation and then reconstructs the original data from this representation. However, MAE differs from an autoencoder as it uses only partial information from the input during encoding, reconstructing the complete image based on this limited data. This innovative masking strategy effectively reduces computational overhead, one of the main challenges in vision models. The model employs an asymmetric architecture: the encoder processes only the visible patches, while a lightweight decoder reconstructs the full image using the latent representation of the visible patches and the mask tokens. The details are explained below.

The encoder in MAE is based on a Vision Transformer (ViT)[26]later described in section3.3.1but without the classifier MLP head (Figure5). Unlike traditional autoencoders, it operates only on the visible, unmasked patches of an image while the rest are masked. MAE divides each image into non-overlapping patches. A subset of these patches (covering only 25% of the entire image) is randomly selected to remain visible. This sampling follows a uniform distribution to prevent any spatial bias, such as a concentration of visible patches near the image center. As the remaining 75% of the images are not supplied to the encoder, the reconstruction task becomes more challenging and this cannot be done successfully by simply relying on neighboring pixels like CNNs or neighboring patches like a typical ViT. To address this each visible patch in the MAE is embedded using linear projection with positional embeddings, following which it is processed through transformer blocks.

The MAE decoder is also a ViT transformer, that takes the full set of tokens of the original images, which includes both the encoded visible patches and additional mask tokens that represent the missing patches to be predicted. Each mask token is a shared, learned vector indicating a missing part of the image. Positional embeddings are added to all tokens to ensure the decoder understands the spatial location of each mask token within the image. The decoder is used solely during pre-training/fine-tuning for the reconstruction task, while the encoder’s learned features are used later for recognition tasks.
This asymmetry reduces the overall inference time, making the model more efficient for large-scale applications. At the end of the decoder, there is a linear projection layer that maps the decoder’s output embeddings to pixel values. This layer translates the high-dimensional feature representations back into the pixel space of the masked patches. The model uses Mean Square Error (MSE) (equation1) as the reconstruction loss between the predicted pixel values and the actual pixel values of the original image for the masked patches.

Where,is the total number of samples,refers to the predicted value for the i-th sample andrefers to the target or actual value for the i-th sample.

The MAE pre-training process is designed to be efficient and avoids the need for specialized sparse operations. First, each input patch is converted into a token through linear projection, with positional embeddings added. The tokens are then randomly shuffled, and a portion is removed according to the masking ratio, resulting in a subset of tokens for the encoder. This process, equivalent to sampling patches without replacement, creates a compact input for the encoder. After encoding, mask tokens are added to the encoded patches, and the shuffled list is restored to its original order to align each token with its corresponding target. The decoder processes this complete set of tokens, with positional embeddings and reconstructs the target. This straightforward approach incurs minimal computational overhead, as the shuffling and unshuffling steps are computationally light.

SECTION: 3.2Self-supervised Framework with classifiers

In this study, we exploit the self-supervised learning capabilities of MAE to capture valuable spatial information from unlabeled melt pool images. Instead of training the model from scratch, we leverage a pre-trained MAE base model that has already undergone extensive training on the ImageNet-1k dataset for 800 epochs using a high-performance setup[27]. This pre-trained model has demonstrated superior performance over previous ViT-based models in image recognition tasks. Therefore, we fine-tune this pre-trained MAE with our unlabeled melt pool data (following the steps in Algorithm1), significantly reducing the computational overhead. The original MAE paper[27]also emphasizes fine-tuning over full pre-training based on their extensive experiments. Following fine-tuning, we transfer the parameters of the MAE model to a classifier, which is then trained on our limited labeled melt pool image data for the final classification task as shown in Figure4.

SECTION: 3.3Classifier Models

When the fine-tuning is done, we transfer the trained parameters of the MAE Encoder only to our classifiers. We discard the MAE decoder since it is solely the encoder that produces the latent representations for recognition tasks. We transfer the fine-tuned parameters of the MAE Encoder to classifier models to train them in a fully supervised manner to perform the classification task. In this study, we use two different classifiers on the labeled data: a supervised ViT Classifier initialized by the fine-tuned encoder parameters and the fine-tuned MAE Encoder with an MLP classifier trained in a supervised manner, as described below. In this section, we first describe the Vision Transformer (ViT) backbone, which is fundamental in both the MAE Encoder and the ViT Classifier employed in this study.

The Vision Transformer (ViT) classifier[26]adapts the traditional natural language processing (NLP) Transformer encoder’s[31]structure only, to process images effectively as shown in Figure5. In this approach, each input image of resolutionwithcolor channels is divided into a grid of non-overlapping patches of size. Each patch is flattened into a 1D vector of size, forming a sequence of patches that serves as the input to the Transformer model. The number of patches is determined by. To clarify the model’s functionality, the following section presents and explains key equations from the original ViT paper[26].

After the image is divided into patches, each patch is then projected into a latent space of fixed dimensionthrough a trainable linear projection, creating patch embeddings for the Transformer. This process can be represented in equation2below.

whererepresents each patch,is the learnable projection matrix, anddenotes the positional encoding matrix. Learnable 1D positional encodingsare added to each patch embedding to retain spatial information. This allows the Transformer to recognize the spatial relationships between patches. Furthermore, a special classification token, similar to thetoken[30]used in natural language processing, is prepended to the sequence. This classification token aggregates information from the entire image and serves as the image’s global representation at the output of the Transformer encoder.

The Vision Transformer (ViT) encoder consists of astack of Transformer layers, each structured with alternating Multi-Head Self-Attention (MSA) in equation6and feed-forward Multi-Layer Perceptron (MLP) blocks shown in equation7. Each block is preceded by layer normalization (LN) and includes residual connections to enhance training stability (Figure5). The classification head consists of an MLP head. The MSA operation involves computing the attention of multiple heads,= {1, …, H}. The self-attention mechanism is summarized below in the equation3.

where, weights,,, andis the dimension of.

For each head, the attention mechanism is computed as follows:

In equations3,4,5, the MSA mechanism allows the model to assess relationships between all patches by weighing their interactions respectively, while layer normalization (LN) is applied before MSA to stabilize the learning process (equation6). The residual connection adds the original input back to the MSA output, aiding in gradient flow and enabling deeper model structures.

Here, the MLP block refines each patch’s representation individually and consists of two linear layers with a Gaussian Error Linear Unit (GELU) activation in between. The residual connection here also supports stable gradient flow, prevents vanishing gradient, and helps preserve the input features while enhancing the patch representation in equation7.

In equation8, the layer normalization is applied specifically to the class embeddingfrom the last layer, which represents the entire image’s complete information. Then, the normalized class token,serves as the final output representation of the image and is subsequently used for classification by the softmax layer in the MLP head (Figure5). This structure enables the ViT model to capture and integrate both local and global spatial relationships across patches.

We transfer the fine-tuned weightsfrom the MAE Encoder (Algorithm1) to the ViT Classifier (Algorithm2) to initialize the supervised training on the labeled melt pool images. The training steps for the ViT Classifier are outlined in the Algorithm2.

As we already mentioned earlier, the MAE Encoder has a similar architecture as a ViT encoder (Figure5) except for the MLP classifier head. Thus, we add an extra two-layer MLP classifier head to the end of the MAE Encoder model to perform the classification task here. Another difference, unlike the ViT classifier, the MAE model lacks a designated class embedding (). Thus, we apply max-pooling to the output embeddings and generate a similar class representation (Algorithm3). We train this setup of fine-tuned MAE Encoder with the MLP classifier extension on the labeled melt pool data. This additional MLP head allows the model to map learned features to the desired class labels, and perform classification. The goal is to evaluate the performance of the fine-tuned MAE Encoder paired with an MLP classifier, leveraging the features learned during the self-supervised fine-tuning phase on unlabeled melt pool images, trained on labeled data to classify melt pool categories effectively. The training steps are demonstrated in the Algorithm3.

SECTION: 4Results & Discussion

In this section, we discuss the training strategies, experimental setup, models’ architectures, and results in detail.

SECTION: 4.1Training setup of self-supervised MAE

First, we fine-tune the pre-trained MAE model using 7812 unlabeled melt pool images collected from Sample 1 from Table2in a self-supervised manner to learn the spatial features through image reconstruction. The data is divided into an 80%-20% split for the training and validation. We use the MAE-base model[32], configured with an encoder consisting of 12 Transformer blocks with a width of 768, and a decoder with 8 Transformer blocks and a width of 512 and reconstruction target without pixel normalization[27]. The originalRGB images are resized toto reduce computational cost and match the dimensions of the pre-trained MAE. To make the model robust, data augmentation techniques such as rotation and flipping are applied, and drop-out and weight decay are used for regularization to avoid over-fitting. The training setup and specific hyperparameters are summarized in Table3and the learning curve obtained is demonstrated in Figure6.

We have used an NVIDIA A10 GPU (Graphic Processing Unit) with 23 GB RAM for this training. The fine-tuning process takes around 25 hours to complete, largely due to the computational demands of reconstructing images. Once the training is completed, we save the parameters of the best-performing model to transfer to the classifiers for the recognition task. This approach aims to boost the classifiers with the enriched features learned from the fine-tuning phase on a similar domain, to map melt pool images to their respective classes accurately.

SECTION: 4.2Training setup of supervised Classifiers

At this stage, we employ two different supervised classifiers, i.e., the fine-tuned MAE Encoder only with an MLP classifier head and a supervised ViT Classifier model. The reason behind attempting the MAE Encoder with MLP head is to test the ability of the MAE Encoder for the recognition task which is proven to be efficient on benchmark image datasets[27]. We also employ a fully supervised ViT Classifier base model[33]. We transfer the fine-tuned parameters of the self-supervised MAE Encoder to both classifiers. The same computational resources outlined in section4.1are used for the training. We run this setup on 1447 labeled melt images collected from Sample 2 from Table2. The dataset is initially divided into a test set comprising 15% of the total data, and the remaining 85% is used for training. During the training phase, we employ a six-fold cross-validation approach to ensure robust training and performance evaluation of our models. Within the cross-validation, the data was split into six equal folds, where in each iteration, one fold served as the validation set while the remaining five folds were used for training. This process is repeated six times. We use the same set of hyperparameters in both classifiers’ setups and perform end-to-end supervised training on the labeled dataset to perform the binary classification of the normal and abnormal melt pools. The training setup and model specifications are mentioned in the Table4.

One of the major differences between these two aforementioned classifier models is unlike the ViT classifier, the MAE Encoder model does not have a classifier head by default, thus we add a two-layer MLP head to serve this purpose. Another difference is MAE Encoder does not have the class embedding and tokenadded to the visible input patch embeddings; a key feature in the ViT classifiers. Thistoken represents an aggregated representation of the image, as it has attended to all the other tokens of image patches through the multi-head self-attention mechanism[26]. Due to the absence of the class token in the MAE Encoder, we add a Max-pooling[34]to output tokens to create a single representation in Algorithm3, that serves as the input to the MLP classifier similar toof standard ViT classifiers.

SECTION: 4.3Performance Evaluation

To evaluate the models’ performance, we calculate the Accuracy, Precision, Recall, and F1 score for the predicted outcomes. The results from the six-fold cross-validation for both setups are presented in Table5and Table6, respectively. Additionally, Figure7illustrates the confusion matrices across all cross-validation folds.

Here,TP = True positive, the actual positive instances predicted positive;TN = True Negative, the actual negative instance predicted negative;FP = False Positive, the actual negative instances predicted positive; andFN = False Negative, the actual positive instances predicted negative by the model respectively.

The results from evaluating both classifier models, initiated by the same fine-tuned MAE, over six folds from Figure7, indicate that both models achieve consistently high accuracy, with scores ranging between 0.9544 and 0.9917. While both classifiers maintain strong performance, the ViT Classifier generally achieves higher precision, often reaching 1.0000, indicating minimal misclassification of negative samples as positives. Recall varies across folds for both models, with each achieving stronger performance in different folds. However, the ViT Classifier demonstrates more consistent recall across folds, ranging from, resulting in a slightly higher overall F1 score of. In contrast, the MAE Encoder Classifier shows slightly lower recall scores, ranging from 0.4667 to 0.8667, with F1 scores between. The F1 score reflects the balance between precision and recall, accounting forFPandFN. This makes it a better performance indicator than accuracy in cases where one class dominates the test dataset. The F1 scores highlight that the ViT Classifier sometimes produces fewer false positives and negatives, leading to more consistent classification results. Thus, while the MAE Encoder Classifier performs well, the MAE Encoder with the ViT Classifier demonstrates a slight advantage in precision, recall, and F1 score and stability across metrics, making it a preferable choice for melt pool classification.

The slightly lower performance of the MAE Encoder Classifier can be attributed to its default design, which is primarily specialized for spatial feature extraction by reconstruction rather than classification. Thus, as mentioned earlier, the MAE model does not require a class embedding or an MLP head. While we address this by applying max-pooling to the MAE Encoder’s outputs to approximate a global representation and add an MLP head, this approach does not fully replicate theembedding’s role in the ViT model. In ViT, theembedding is shared across all transformer layers, allowing it to integrate all patches’ information, and enhancing its capacity for classification tasks. Thus, while both classifiers are initialized with fine-tuned MAE parameters, the ViT Classifier achieves improved performance in supervised classification compared to the MAE Encoder Classifier approach.

SECTION: 4.4Limitations & Future Work

While our framework achieves high classification accuracy with both models, there are limitations to consider. In this study, we use Mean Squared Error (MSE) as the reconstruction loss; however, other loss functions, such as contrastive loss[35], could be explored for this data type in future work. The self-supervised MAE model could benefit from an even larger and more diverse set of melt pool images collected under varying process parameters. Although our classifiers performed well, their generalization could improve with more labeled data in both the training and testing phases.

Furthermore, while ViT models outperform CNNs in capturing complex local and global features, they are computationally intensive, mainly due to quadratic operations like self-attention, leading to longer training times. Class imbalance[36,37]is another challenge: generating high-quality images to balance classes in ViT models requires advanced generative models, which also require significant computational resources. Although class balancing isn’t essential for the self-supervised MAE model, it would improve the robustness of the classifiers. If the synthetic data lacks accuracy or quality, the ViT models may struggle to effectively learn features, significantly impacting overall performance. Lastly, to make optimal use of limited labeled data, semi-supervised techniques[38]can be explored in the future as an alternative to fully supervised classification approaches, potentially enhancing model effectiveness with reduced labeling requirements.

SECTION: 5Conclusions

The integration of advanced machine vision and AI is transforming additive manufacturing, enabling the handling of complex data types like thermal images of melt pools. These melt pools, which undergo intricate laser-material interactions and rapid solidification, are key indicators of the physical state of a print and the potential for flaw formation. However, their nuanced thermal distributions are challenging to interpret accurately, and labeling them requires extensive ex-situ inspection, which is both time-intensive and costly. To address these challenges, our framework leverages a self-supervised MAE on a large set of unlabeled melt pool images, combined with a supervised classifier trained on limited labeled data. This approach captures both local and global features more effectively than CNNs, making it an ideal fit for melt pool characterization. We summarize the key findings of our study as follows.

We successfully reduce the dependency on large labeled datasets for supervised training by implementing a self-supervised MAE model to learn the spatial features from the similar but unlabeled melt pool data. By fine-tuning a pre-trained version of this model, we optimized computational resources effectively and yet achieved high accuracy.

For a fair comparison, both classifier models used in this study utilize the fine-tuned parameters and learned features respectively, derived from the same self-supervised MAE model. The ViT Classifier demonstrates slightly superior performance over the MAE Encoder Classifier due to its architectural advantage, achieving an average accuracy of 98.2% and an average F1 score of 82.8%.

Although tested on melt pool data from the DED process, this framework can be adapted to other thermal imaging data in similar LAM processes, offering a scalable, efficient solution for in-situ melt pool characterization that can complacent against traditional and expensive ex-situ defect characterization methods.

Our framework thus provides a robust and efficient alternative for automated melt pool monitoring and characterization in LAM, capable of enhancing quality control with reduced dependency on extensive labeled datasets.

SECTION: Acknowledgements

Funding:This material is based upon work supported by the National Science Foundation under Grant No. 2119654. Any opinions, findings, conclusions, or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation.

Conflict of interest/Competing interests:As authors of this work, we declare that we have no conflicts of interest.

Computational Resources:The authors would like to acknowledge the Pacific Research Platform, NSF Project ACI-1541349, and Larry Smarr (PI, Calit2 at UCSD) for providing the computing infrastructure used in this project.

XCT scanning Facility:The authors express their gratitude to Sarah McLaughlin, Ph.D., and Amanda Stewart, Ph. D. from the AMIF Applications and Imaging Facilities at West Virginia University (WVU) for their assistance in initiating the XCT scanning of the samples.

SECTION: References