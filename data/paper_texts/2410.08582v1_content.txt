SECTION: DeBiFormer: Vision Transformer with Deformable Agent Bi-level Routing Attention
Vision Transformers with various attention modules have demonstrated superior performance on vision tasks. While using sparsity-adaptive attention, such as in DAT, has yielded strong results in image classification, the key-value pairs selected by deformable points lack semantic relevance when fine-tuning for semantic segmentation tasks. The query-aware sparsity attention in BiFormer seeks to focus each query on top-k routed regions. However, during attention calculation, the selected key-value pairs are influenced by too many irrelevant queries, reducing attention on the more important ones.
To address these issues, we propose the Deformable Bi-level Routing Attention (DBRA) module, which optimizes the selection of key-value pairs using agent queries and enhances the interpretability of queries in attention maps. Based on this, we introduce the Deformable Bi-level Routing Attention Transformer (DeBiFormer), a novel general-purpose vision transformer built with the DBRA module. DeBiFormer has been validated on various computer vision tasks, including image classification, object detection, and semantic segmentation, providing strong evidence of its effectiveness.Code is available at

SECTION: Introduction
The Vision Transformer has recently demonstrated significant promise in the realm of computer vision. It can capture long-range dependency in data, and is almost leading to a convolution-free model more flexible for fitting tons of data. In addition, it enjoys high parallelism, which benefits training and inference for large models.
The computer vision community has observed a surge in the adoption and development of Vision Transformers.

To improve attention, numerous pieces of research have used thoughtfully crafted, efficient attention patterns in which each query selectively focused by a smaller portion of key-value pairs. As shown in Figure, among the various representation approaches, some include local windowsand dilated windows. In addition, some research has taken a different path through sparsity adaptation to data in their methodology, as demonstrated in the works of. However, despite the varying strategies for merging or selecting key and value tokens. These tokens are not semantic for queries. With this approach, when applied to other downstream tasks for pretrained ViTand DETR. Queries do not originate from semantic-region key-value pairs. Consequently, compelling all queries to focus on insufficient sets of tokens may not yield the most optimal results. Recently, with the dynamic query-aware sparsity attention mechanism, queries are focused by the most dynamic semantically key-value pairs, which is referred to as bi-level routing attention.
However, in this approach, queries are handled by semantic key-value pairs instead of originating from detailed regions, which may not yield the most optimal results in all cases. In addition, when calculating the attention, these keys and values selected for all queries are influenced by too many less relevant queries, resulting in a decrease of attention for important queries, which has a significant impact when performing segmentations..

To make the attention for queries more efficient, we propose the Deformable Bi-level Routing Attention (DBRA), an attention-in-attention architecture for visual recognition. During the process in DBRA, the first problem is how to locate deformable points. We use the observation inthat attention has an offset network that takes as input query features and generates corresponding offsets for all reference points. Thus, candidate deformable points are shifted towards important regions with high flexibility and efficiency to capture more informative features.
The second problem is how to aggregate information from semantically relevant key-value pairs, and then broadcast the information back to queries.
Therefore, we propose an attention-in-attention architecture that shifted toward deformable points as shown above acts as the agent for the queries.
As the key-value pairs are selected for deformable points, we use the observation into select a small portion of the most semantically relevant key-value pairs that a region only needs by focusing on the top-k routed regions.
Then, with the semantically relevant key-value pairs selected, we first apply a token-to-token attention with deformable points queries.
And then, we apply a second token-to-token attention to broadcast the information back to queries, in which deformable points as key-value pairs are designed to represent the most important points in a portion of semantic regions.

To summarize, our contributions are as follows:

1. We propose Deformable Bi-level Routing Attention (DBRA), an attention-in-attention architecture for visual recognition, where data-dependent attention patterns are obtained flexibly and semantically.

2. By utilizing the DBRA module, we propose a novel backbone, called DeBiFormer, which has a stronger recognition ability based on the visualization results of the attention heat map.

3. Extensive experiments on ImageNet, ADE20K, and COCOdemonstrate that our model consistently outperforms other competitive baselines.

SECTION: Related Work
SECTION: Vision Transformers
The Transformer-based backbone incorporates channel-wise MLPblocks to embed per-location features through channel mixing. Additionally, attentionblocks are used for cross-location relation modeling and facilitating spatial mixing. Initially devised for natural language processing, Transformers were subsequently introduced to the domain of computer vision through works like DETRand ViT. Compared with CNNs, the primary distinction lies in the fact that transformers use attention as a substitute for convolution, thereby facilitating global context modeling. Nevertheless, vanilla attention, which calculates pairwise feature affinity across all spatial locations, imposes a significant computational burden and leads to substantial memory footprints, particularly when dealing with high-resolution inputs. Thus, a key research focus is to devise more efficient attention mechanisms, crucial for mitigating computational demands, especially with high-resolution inputs.

SECTION: Attention mechanisms
Numerous studies have aimed to alleviate the computational and memory complexities associated with vanilla attention. Approaches include sparse connection patterns, low-rank approximations, and recurrent operations. In the context of Vision Transformers, sparse attention has gained popularity, particularly following the remarkable success of the Swin Transformer. Within the Swin Transformer framework, attention is constrained to non-overlapping local windows, and an innovative shift window operation is introduced. This operation facilitates communication between adjacent windows, contributing to its unique approach to handling attention mechanisms. To attain larger or approximate global receptive fields without exceeding computational constraints, recent studies have incorporated diverse manually designed sparse patterns. These include the integration of dilated windowsand cross-shaped windows. Moreover, certain studies endeavor to make sparse patterns adaptable to data, as demonstrated by works like DAT, TCFormer, and DPT. Despite their efforts to decrease the number of key-value tokens using diverse merging or selection strategies, it is crucial to recognize that these tokens lack semantic specificity. Instead, we reinforce query-aware key-value token selection.

Our work is motivated by an observation: semantically attentive regions for important queries can exhibit significant differences, as illustrated by visualizations from pre-trained models like ViTand DETR. In achieving query-adaptive sparsity through a coarse-to-fine approach, we propose an attention-in-attention architecture which utilizes the deformable attentionwith bi-level routing attention. Diverging from deformable attentionand bi-level routing attention, our deformable bi-level routing attention aims to reinforce the most semantic and flexible key-value pairs. In contrast, bi-level routing attention only focuses on locating a few highly relevant key-value pairs, while deformable attention prioritizes identifying a few of the most flexible key-value pairs.

SECTION: Our Approach: DeBiFormer
SECTION: Preliminaries
Initially, we revisit the attention mechanism used in recent Vision Transformers. Taking a flattened feature mapas the input, a multi-head self-attention (MHSA) block withheads is formulated as

wheredenotes the softmax function, andis the dimension of each head.denotes the embedding output from the-th attention head, anddenote the query, key, and value embeddings, respectively.are the projection matrices. With normalization layers and identity shortcuts, the-th Transformer block, for which LN means layer normalization, is formulated as

SECTION: Deformable bi-level routing attention (DBRA)
The architecture of the proposed Deformable Bi-level Routing Attention (DBRA) is illustrated in Figure. We first employ a deformable attention module, which includes an offset network that generates offsets for reference points based on the query features, creating deformable points. However, these points tend to cluster in important regions, leading to an over-concentration in certain areas.

To address this, we introduce deformable points-aware region partitioning, ensuring that each deformable point interacts with only a small subset of key-value pairs. Yet, solely relying on region partitioning can result in an imbalance between important and less important regions. To tackle this, the DBRA module is designed to distribute attention more effectively. In DBRA, each deformable point acts as an agent query, computing attention with semantic region key-value pairs. This approach ensures that only a few deformable points are assigned to each important region, allowing attention to be spread across all critical areas of the image rather than clustered in one spot.

By employing the DBRA module, attention is reduced in less important regions and increased in more important ones, ensuring a balanced distribution of attention throughout the image.As illustrated in Fig., given the input feature map, a uniform grid of pointsis generated by downsampling the input feature map by factor,as a reference. To obtain the offset for each reference point, the features are linearly projected to generate query tokens, which are then input into thesubnetwork to produce the offsets. Subsequently, the features are sampled at the locations of deformed points as keys and values and further processed by projection matrices:

whererepresent the deformed keyand valueembeddings, respectively. Specifically, we set the sampling functionto a bilinear interpolation to make it differentiable:

where the functionandrepresent indices for all locations on. In a similar setup as deformable attention, whereis nonzero on the four integral points closest to, Equationis simplified to a weighted average across these four locations.With the deformable attention feature map inputand feature map, the process begins by dividing it into regions of sizenon-overlapped regions such that each region containsfeature vectors with reshapedasandas. Then, we derive the query, key, and value with linear projections:

Next, we use the region-to-region method, as introduced in BiFormer, which is applied to establish the attending relationship by constructing a directed graph. To initiate the process, region queries and keysare derived through the application of per-region averaging. Subsequently, the adjacency matrixfor the region-to-region affinity graph is derived throughandmatrix multiplication:

where adjacency matrixquantifies the semantic relationship between two regions. The crucial step in this method involves pruning the affinity graph by retaining only theconnections for each region with a routing index matrixthrough the use of theoperator:

Utilizing the region routing matrix, we can then apply token attention. For each deformable query token within region, its attention spans all key-value pairs located in therouted regions, that is, those indexed by. Hence, we continue the process of gathering the key and value:

whereare the gathered key and value. Then, we apply attention onas:

whereis a projection weight for output features, anduses a kernel size 5 depth-wise convolution.Following that, the deformable features that are semantically attended to viaare reshapedasand parameterized at the locations of keys and values:

andrepresent the embeddings of semantically deformed keys and values, respectively. Using existing approaches, we perform self-attention on, and relative position offsets. The output of attention is formulated as follows:

Here,corresponds to the position embedding, following the approach of previous work. Thenis projected thoughto get the final outputas Equation.

SECTION: Model architectures
Leveraging DBRA as a fundamental building block, we introduce a novel vision transformer called DeBiFormer. As depicted in Figure, we adhere to the recent state-of-the-art Vision Transformers, using a four-stage pyramid structure.
In stage, we utilize an overlapped patch embedding in the first stage and a patch merging modulein the second to fourth stages. This is done to decrease the input spatial resolution while increasing the number of channels. Subsequently,consecutive DeBiFormer blocks are used to transform the features. Within each DeBiFormer block, we adhere to recent methodologiesby using adepthwise convolution at the outset. This is done to implicitly encode relative position information. Following that, we sequentially use a DBRA module with a 2-ConvFFN module with an expansion ratiofor cross-location relation modeling and per-location embedding, respectively.
DeBiFormer is instantiated in three distinct model sizes, achieved by scaling the network width and depth as outlined in Table.
Each attention head comprises 32 channels, and we use a bi-level ConvFFN and deformable-level ConvFFN with an MLP expansion ratio of. For the BRA, we use, and for the DBRA, we usefor the four stages. Moreover, we set the region partition factorto specific values:for classification,for semantic segmentation, andfor object detection tasks.

SECTION: Experiments
We experimentally evaluated the effectiveness of our proposed DeBiFormer on various mainstream computer vision tasks, including image classification (Section), semantic segmentation (Section) and object detection and instance segmentation (Section). In our approach, we commence training from scratch on ImageNet-1Kfor image classification. Subsequently, we fine-tune the pre-trained backbones on ADE20Kfor semantic segmentation and on COCOfor object detection and instance segmentation. Furthermore, we perform an ablation study to confirm the efficacy of the proposed Deformable Bi-level Routing Attention and top-k choices of DeBiFormer (Section). Finally, in order to validate that the recognition ability and interpretability of our DeBiFormer, we visualize the attention map (Section).

SECTION: Image classification on ImageNet-1K
We conducted image classification experiments on the ImageNet-1Kdataset, following the experimental settings of DeiTfor a fair comparison. Specifically, each model was trained for 300 epochs on 8 V100 GPUs with an input size of. We used AdamW as the optimizer with a weight decay ofand used a cosine decay learning rate schedule with an initial learning rate of, while the first five epochs were used for linear warm-up. The batch size was set to 1024. To avoid overfitting, we used regularization techniques including RandAugment(rand-m9-mstd0.5-inc1), MixUp(prob = 0.8), CutMix(prob = 1.0), Random Erasing (prob = 0.25), and increasing stochastic depth(prob = 0.1/0.2/0.4 for DeBiFormer-T/S/B, respectively).We report our results in Tableshowing the top-1 accuracy with similar computational complexities. Our DeBiFormer outperformed the Swin Transformer, PVT, DeiT, DAT, and Biformerin all three scales. Without inserting convolutions in Transformer blocks or using overlapped convolutions in patch embeddings, DeBiFormer achieved gains of 0.5pt, 0.1pt and 0.1pt over BiFormercounterparts.

SECTION: Semantic segmentation on ADE20K
The same as existing works, we used our DeBiFormer on SemanticFPNand UperNet. In both cases, the backbone was initialized with ImageNet-1K pretrained weights. The optimizer was AdamW, and the batch size was 32. For a fair comparison, we followed the same setting as PVTto train the model with 80k steps and Swin Transformerto train the model with 160k steps.Tableshows the results of the two different frameworks. It shows that with the Semantic FPN framework, our DeBiFormer-S/B achieved 49.2/50.6 mIoU, respectively, improving BiFormer by 0.3pt./0.7pt. A similar performance gain for the UperNet framework was also observed. By utilizing the DBRA module, our DeBiFormer could caputure the most semantic key-value pairs, which makes the attention selection more reasonable and achieve higher performance on downstream semantic tasks.

SECTION: Object detection and instance segmentation
We used our DeBiFormer as the backbone in the Mask RCNNand RetinaNetframeworks to evaluate the effectiveness of models for object detection and instance segmentation on COCO 2017. The experiments were conducted with the MMDetectiontoolbox. Before training on COCO, we initialized the backbone with weights pre-trained on ImageNet-1K and followed the same training strategies as BiFormerto compare our methods fairly. Note that due to device limitations, we set mini batch size asfor these experiments, while in BiFormer this value is 16. For details on the specific settings of the experiment, please refer to the supplementary paper.We list the results in Table. For object detection with RetinaNet, we report the mean average precision (mAP) and the average precision (AP) at different IoU thresholds (50%, 75%) for three object sizes (i.e., small, medium, and large (S/M/L)). From the results, we can see that while the overall performance of DeBiFormer was only comparable to some of the most competitive existing methods, the performance on large objects () outperformed these methods although we use a limited resources. This may be because the DBRA allocates deformable points more reasonably. These points are not to focus only on small things, but to focus on important things in the image. Therefore the attention is not limited to a small area, which improves the detection accuracy of large objects. For instance segmentation with Mask R-CNN, we report the bounding box and mask the average precision (and) at different IoU thresholds (50%, 75%). Note that our DeBiFormer still achieved great performance under the device limitation of mini batch size. We believe that we could achieve better results if the mini batch size could be the same to other methods since it has been proved on semantic segmentation tasks.

SECTION: Ablation study
We compared DBRA with several existing sparse attention mechanisms. Following CSWIN, we aligned macro architecture designs with Swin-Tfor fair comparison. Specifically, we usedblocks for the four stages and non-overlapped patch embedding, and we set the initial patch embedding dimension toand MLP expansion ratio to. The results are reported in Table. Our Deformable Bi-level Routing Attention had significantly better performance than the existing sparse attention mechanisms, in terms of both image classification and semantic segmentation.

Similar to BiFormer, we opted to useas a divisor of the training size to prevent padding. We used an image classification with a resolution of, and we setto ensure that the size of the feature maps was divided at each stage. This choice aligns with the strategy used for the Swin Transformer, where a window size of 7 was used.

We systematically adjustedto ensure a reasonable number of tokens attended to deformable queries as the region size diminished in later stages. Exploring various combinations ofis a viable option. In Table, we present ablation results on IN-1K, following DeBiFormer-STL (“STL” denotes Swin-T Layout). A crucial observation from these experiments is that augmenting the number of tokens paid attention to the deformable queries had a detrimental effect on accuracy and latency, and increasing the number of tokens paid attention in stages 1 and 2 had an effect on accuracy.To evaluate the impact of design choices, we systematically replaced bi-level routing attention blocks with DBRMHA blocks across different stages, as shown in Table. Initially, all stages used bi-level routing attention, similar to BiFormer-T, achieving 81.3% accuracy in image classification. Replacing just one block in the 4th stage with DBRMHA immediately boosted accuracy by +0.21. Replacing all blocks in the 4th stage added another +0.05. Further DBRMHA replacements in the 3rd stage continued to improve performance across tasks. While gains tapered off with earlier stage replacements, we settled on a final version—DeBiFormer—where all stages use Deformable Bi-level Routing Attention for simplicity.

SECTION: Grad-CAM Visualization
To further illustrate the ability of the proposed DeBiFormer to recognize the attention in important regions, we used Grad-CAMto visualize the areas of greatest concern of BiFormer-Base and DeBiFormer-Base.
As shown in Figure, by utilizing DBRA module, our DeBiFormer-Base model performed better in target objects locating in which more regions have been focused on. In addition, our model scales down the attention in the unnecessary regions and pays more attention to the necessary regions. Depending on the attention of more necessary regions, our DeBiFormer model focused on semantic areas more continuously and completely, suggesting the stronger recognition ability of our model. Such ability yields better classification, and semantic segmentation performance compared with BiFormer-Base.

SECTION: Conclusion
The paper introduces the Deformable Bi-level Routing Attention Transformer, a novel hierarchical vision transformer designed for both image classification and dense prediction tasks. Through Deformable Bi-level Routing Attention, our model optimizes query-key-value interactions while adaptively selecting semantically relevant regions. This leads to more efficient and meaningful attention. Extensive experiments show our model’s effectiveness compared to strong baselines. We hope this work offers insights into designing flexible and semantically aware attention mechanisms.

SECTION: Supplementary Material
SECTION: Offset groups
Like, to foster diversity among deformed points, we adhere to a similar paradigm as in MHSA, where the channels are split into multiple heads to compute a variety of attentions. Hence, we partition the channels intogroups to generate diverse offsets. The offset generation network shares weights for features from different groups.

SECTION: Deformable relative position bias
Certainly, incorporating position information into attention mechanisms has proven beneficial for model performance. Various approaches, such as APE, RPE, CPE, LogCPB, and others have demonstrated improved results. The relative position embedding (RPE) introduced in the Swin Transformer specifically encodes the relative positions between every pair of query and key, thereby enhancing vanilla attention with spatial inductive bias. The explicit modeling of relative locations is especially well-suited for attention heads at the deformable level. In this context, deformed keys can assume arbitrary continuous locations, as opposed to being confined to fixed discrete grids.

In accordance with, the relative coordinate displacements are restricted to the range ofandalong the spatial dimension with a relative position bias (RPB), denoted aswith dimensions.

Then, the relative locations within the range ofare sampled using bilinear interpolationwith a parameterized bias. This is done by considering continuous relative displacements, ensuring coverage of all conceivable offset values.

SECTION: Computational complexity
Deformable bi-level routing attention (DBRA) incurs a comparable computation cost to its counterpart in the Swin Transformer. The computation of DBRA consists of two parts: token-to-token attention and offset & sampling. Therefore, the computation of this part is:

whereis the number of sampled points, andis the token embedding dimension. The computation of bi-level routing multi-head attention consists of three parts: linear projection, region-to-region routing, and token-to-token attention. Hence, the computation of this part is:

whereis the number of regions to attend to, andis the region partition factor. Last, the total computation of DBRA consists of two parts: deformable level multi-head attention and bi-level routing multi-head attention. The total amount of computations is therefore:

In other words, DBRA achievescomplexity. For example, thestage of a hierarchical model withinput for image classification usually has computation sizes ofand thus computational complexity for multi-head self-attention. Additionally, the complexity could be further reduced by enlarging the downsampling factorand scale with region partition factor, making it friendly to tasks with much higher resolution inputs such as object detection and instance segmentation.

SECTION: Token to Attend
In Table, we present the token to attend to the query and the token to attend to the deformable point. Compared with other methods, DeBiFormer has the fewest tokens to attend for each query but has a high performance in Imagenet1K, ADE20K(S-FPN head) and COCO(Retina head).

SECTION: More visualization results
To assess the effective receptive fields (ERFs)of the central pixel with an input size ofacross various models, we present a comparative analysis in Figure.
To demonstrate the powerful representation capacity of our DeBiFormer, we also compare the ERF of several SOTA methods with similar computational costs. As shown in Fig., our DeBiFormer has the largest and most consistent ERF among these methods while maintaining strong local sensitivity, which is challenging to achieve.

To further show how DBRA works, we demonstrate more visualization results in Figure.

Thanks to the flexible key-value pairs selection, in most cases, our DeBiFormer focuses on important objects at an earlier stage. Meanwhile, due to the reasonable allocation of deformable points, it also focuses on different important areas earlier in multi-object scenarios. With the powerful DBRA module, our DeBiFormer has a larger heat map area in the last two stages, which represents a stronger ability of recognition.

SECTION: Detailed Experimental Settings
As introduced in the main paper, each model was trained for 300 epochs on 8 V100 GPU with an input size of. The experimental settings are strictly follow the DeiTfor a fair comparison. For more details, please refer to the Tableprovided.

When fine-tuning our DeBiFormer to object detection and instance segmentation on COCO, we have considered two common frameworks: Mask R-CNN, RetinaNet. For optimization, we adopt the AdamW optimizer with an initial learning rate of 0.0002 and adue to the limitation of devices. When training models of different sizes, we adjust the training settings according to the settings used in image classification. The detailed hyper-parameters used in training models are presented in Table.

For ADE20K, we utilized the AdamW optimizer with
an initial learning rate of 0.00006, a weight decay of 0.01, and a mini batch size of 16 for all models trained for 160K iterations. In terms of testing, we reported the results using both single-scale (SS) and multi-scale (MS) testing in the main comparisons. For multi-scale testing, we experimented with resolutions ranging from 0.5 to 1.75 times that of the training resolution. To set the path drop rates in different models, we used the same hyper-parameters as those used for object detection and instance segmentation. Tableshows the results of the Upernet frameworks with the single and Multi-Scale IoU.

SECTION: Limitation and Future Work
In contrast to sparse attention with simple static patterns, we propose a novel attention method that consists of two components. First, we prune a region-level graph and gather key-value pairs for important regions, which are focused by highly flexible key-value pairs. Then, we apply token-to-token attention. While this method does not incur much computation as it operates at a top-k routed semantically relevant region level and deformable important regions, it inevitably involves additional parameter capacity transactions during linear projection. In our future endeavors, we plan to investigate efficient sparse attention mechanisms and enhance Vision Transformers with parameter capacity awareness.

SECTION: References