SECTION: Enhancing Crop Segmentation in Satellite Image Time Series with Transformer Networks

Recent studies have shown that Convolutional Neural Networks (CNNs) achieve impressive results in crop segmentation of Satellite Image Time Series (SITS). However, the emergence of transformer networks in various vision tasks raises the question of whether they can outperform CNNs in this task as well. This paper presents a revised version of the Transformer-based Swin UNETR model, specifically adapted for crop segmentation of SITS. The proposed model demonstrates significant advancements, achieving a validation accuracy ofand a test accuracy ofon the Munich dataset, surpassing the previous best results offor validation andfor the test. Additionally, the model’s performance on the Lombardia dataset is comparable to UNet3D and superior to FPN and DeepLabV3. Experiments of this study indicate that the model will likely achieve comparable or superior accuracy to CNNs while requiring significantly less training time. These findings highlight the potential of transformer-based architectures for crop segmentation in SITS, opening new avenues for remote sensing applications.

KeywordsCrop MappingRemote SensingSatellite Image Time SeriesTransformer NetworksConvolutional Neural Networks

SECTION: 1Introduction

Deep Learning (DL) approaches and, in particular, Convolutional Neural Networks (CNNs) have been used successfully for remote sensing applications[1],[2].
However, transformer-based architectures are slowly replacing CNNs in different vision applications such as image classification[3],[4], object detection[4],[5], semantic segmentation[4],[6], etc.
Furthermore, many transformer-based architectures showed promising results in remote sensing applications, such as SITS-Former[7], CTGAN[8], UNetFormer[9], HSI-BERT[10], SpectralFormer[11]and others[12],[13].
Classic CNN models such as U-Net, Feature Pyramid Network, and DeepLab v3+ were used for segmentation of Sentinel-2 (S2) single images[14],[15],[16]and SITS[17].
However, transformer-based models showed promising results in the segmentation of S2 single images[18],[19]and SITS[7]. Remote sensing has been proven to be an effective tool for crop mapping[20].
CNN architectures showed good results on crop segmentation of S2 Imagery from a single image[21],[22]and from SITS[17],[23].
Transformer-based architectures have been used recently with success in crop segmentation from single S2 images, as shown by recent works[24],[25].
However, hardly anyone investigated whether the introduction of self-attention in the encoding layers can improve the results in crop mapping from SITS.
In particular, this work investigates whether models with a Transformer encoder can perform better than CNNs even in crop segmentation of SITS.

This study uses the Swin UNETR[26], a segmentation model for 3D medical images.
This model is based on the Swin Transformer[4](Shifted window Transformer), which inherits most of the features of the Vision Transformer[3](ViT), but solved some of its weaknesses.
The Swin UNETR was modified to take temporal multispectral images as input and return as output the crop map. The results of the present study provide a basis for comparison with the model in[27].
Moreover, our study has the same objective as the CCTNet work[24], another study of crop segmentation that relies on a transformer-based network.
However, their study covered crop segmentation from a single image, while this study covered crop segmentation from SITS.
Thus, the results are not comparable.
Model training and evaluation were performed on two datasets. Both are made up of agricultural area SITS taken by the Sentinel-2 satellite. The former contains samples from Munich (Germany) and the latter from Lombardia (Italy). To provide a valid comparison, different CNNs were trained: FPN3D, UNet3D, and DeepLab3 3D.

SECTION: 2Proposed Approach

The proposed method is based on the Swin UNETR[26]architecture, known in the literature for the Semantic Segmentation of Brain Tumors in MRI (Magnetic resonance imaging) images. This architecture was expected to be a viable model to perform segmentation of satellite imagery, even if remote sensing is a very different area from its initial field of application. The Swin UNETR takes as input an MRI image, which is a 3D image. It utilizes a Swin Transformer[4]as the encoder to learn sequence representations of the input volume and effectively capture the global multi-scale information. Swin UNETR encoder has four stages, which comprise two transformer blocks at each stage. Like any U-Net style architecture, the encoder network is followed by a decoder network and connected through skip connections. The Swin UNETR is made for 3D images, thus, it already has a partially ready architecture to perform experiments on SITS. However, some adjustments had to be made to adapt it to the new task. Firstly, the input size was changed to, which is the shape of a time series composed of 32 images, each havingpixels and 13 bands.
Secondly, the fourth stage of the encoder was removed because the height and width of the input weren’t big enough to be downsampled a fourth time. Each stage comprises two transformer blocks, so the final architecture only uses six transformers.
Thirdly, the output size was changed towhere 18 is the number of classes in the Munich dataset. In the last stage of the proposed architecture encoder, the feature size is. As a result, each time-series length has to be a multiple of 16.
The final model, adapted for SITS is represented in Fig.1.
The number of input channels and output classes depends on the dataset used and can be changed without further modifications in other layers.
If input images are larger than, the model should contain more encoding stages to extract high-res and low-res features.
The code of all models involved in the experiments is accessible[28].

SECTION: 3Datasets

Two public datasets were used for the experiments. Both comprise agricultural area SITS taken by the Sentinel-2 satellite. The former dataset is the Munich[23]one, which containssquared tiles with 13 bands. The ground truth associates each pixel with one of the 18 crop classes.
Each block covered an area of 480and was extracted from a larger image, covering 102 km42 km.
This image was taken from an agricultural area north of Munich (Germany).
The dataset was split in the following way: 5000 tiles for the training stage (), 1700 tiles for the validation stage (), and 1700 tiles for the test stage ().
Some samples are shown in Fig.2. The latter dataset is the Lombardia one[27], which containssquared tiles with seven bands.
The ground truth (GT) associates each pixel with one of the seven crop classes.
The images in this dataset belong to three different areas, which will be referred to as Lombardia1, Lombardia2, and Lombardia3.
For each area, a time series is available for 2016, 2017, 2018, and 2019 years.
We used the same split and classes as in[27]for training, testing, and validation.

SECTION: 4Experiments

The Cosine Annealing scheduler with SGD optimizer was used in all the experiments, and the momentum was set to.
The total number of epochs was set to 200 with a batch size of 2 as in[27].
For data augmentation, only vertical and horizontal flips were used.
The same well-known metrics used in[27]were used to evaluate the goodness of each model.

The final results of the proposed model on the Munich dataset are shown in Tab.1, while a comparison between the proposed model and other CNNs architectures is shown by Tab.3.
A good prediction is represented by Fig.4, where wrong classes were predicted only on the transition edges from one crop type to another. Fig.5represents a bad prediction because some extended areas were associated with the wrong class.
The final results of the proposed model on the Lombardia dataset are shown by Tab.2, while a comparison between the proposed model and other CNNs architectures is shown by Tab.4.
Fig.7represents a good prediction from the Lombardia dataset, while Fig.6represents a wrong prediction. Moreover, test patches were merged, resulting in Fig.8that can be compared with the output shown in[27].

In the Munich dataset, the overall kappa shows that the proposed model generalizes well, even in classes with fewer training samples. In Lombardia, all the models show worse metrics. Despite having fewer classes and even more training samples, making predictions from the Lombardia samples seems to be more complicated, probably due to incorrect declarations by crop owners leading to some false ground truths. DeepLab performed the worst in both datasets because of the Atrous Convolution[29]. This type of convolution is usually used with large images, but when used with small images, it causes the model to miss fine details.

SECTION: 5Conclusions

In conclusion, this study introduces a revised version of the Transformer-based Swin UNETR model for crop segmentation in Satellite Image Time-Series (SITS) data. The results demonstrate the model’s remarkable performance, surpassing previous state-of-the-art methods on the Munich dataset and showcasing comparable performance on the Lombardia dataset.
The findings suggest that transformer-based architectures have the potential to outperform traditional Convolutional Neural Networks (CNNs) in crop segmentation tasks while requiring less training time. This enhances the accuracy and efficiency of crop segmentation and opens up new possibilities for remote sensing applications in agriculture and land monitoring.
Furthermore, the success of the proposed model paves the way for future research in exploring the capabilities of transformer networks in other geospatial and satellite imagery tasks. The ability of transformer-based models to capture long-range dependencies and leverage self-attention mechanisms can be leveraged for various remote sensing applications beyond crop segmentation.
Overall, this study demonstrates the promising prospects of transformer-based architectures for crop segmentation in SITS, and further investigations are encouraged to uncover their full potential and explore their applications in other domains of remote sensing and geospatial analysis.

SECTION: References