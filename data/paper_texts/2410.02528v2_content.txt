SECTION: HiFiSeg: High-Frequency Information Enhanced Polyp Segmentation with Global-Local Vision Transformer
Numerous studies have demonstrated the strong performance of vision transformer (ViT)-based methods across various computer vision tasks. However, ViT models often struggle to effectively capture high-frequency components in images, which are crucial for detecting small targets and preserving edge details, especially in complex scenarios. This limitation is particularly challenging in colon polyp segmentation, where polyps exhibit significant variability in structure, texture, and shape. High-frequency information, such as boundary details, is essential for achieving precise semantic segmentation in this context.
To address these challenges, we propose HiFiSeg, a novel network for colon polyp segmentation that enhances high-frequency information processing through a global-local vision transformer framework. HiFiSeg leverages the pyramid vision transformer (PVT) as its encoder and introduces two key modules: the global-local interaction module (GLIM) and the selective aggregation module (SAM). GLIM employs a parallel structure to fuse global and local information at multiple scales, effectively capturing fine-grained features. SAM selectively integrates boundary details from low-level features with semantic information from high-level features, significantly improving the model’s ability to accurately detect and segment polyps.
Extensive experiments on five widely recognized benchmark datasets demonstrate the effectiveness of HiFiSeg for polyp segmentation. Notably, the mDice scores on the challenging CVC-ColonDB and ETIS datasets reached 0.826 and 0.822, respectively, underscoring the superior performance of HiFiSeg in handling the specific complexities of this task.

=-21pt

SECTION: 
Polyps are abnormal growths in the colon and rectum that protrude from the intestinal mucosa. Colorectal cancer frequently arises from colonic polyps, especially adenomatous ones, making early detection and removal critical for preventing cancer progression. Colonoscopy is widely regarded as the gold standard for detecting colorectal lesions. However, the manual annotation of polyps during colonoscopy is both time-consuming and prone to human error, underscoring the need for automated and accurate image segmentation methods to assist in diagnosis.

Deep learning algorithms, particularly CNNs, have achieved significant success in medical image applications such as cardiac, skin lesion, and polyp segmentation. Fully convolutional networks (FCNs), including models like UNet, SegNet, and DeepLab, have become the dominant approaches in this domain. However, due to the limited receptive fields of CNNs, these methods struggle to capture long-range dependencies and global context, which are essential for accurately representing shape and structural information in medical image segmentation.

The transformerarchitecture, with its multi-head self-attention (MHSA) mechanism, excels at capturing complex spatial transformations and long-range dependencies. While it has seen tremendous success in natural language processing (NLP), its adaptation to vision tasks through the vision transformer (ViT)was aimed at overcoming the limitations of CNNs in image recognition. However, despite Transformers’ ability to model global dependencies, they struggle with capturing image locality and maintaining translational invariance, which is critical for accurately segmenting small targets and boundaries.
To address this challenge, recent works have proposed hybrid architecturesthat combine the strengths of both Transformers and CNNs, such as TransUnet, HiFormer, and LeVit-Unet. These models aim to leverage the locality of CNNs with the global context captured by Transformers, enabling them to encode both local and global features for medical image segmentation. While these hybrid models have shown improved performance, they still face limitations, particularly in capturing fine-grained details. This shortcoming affects the accurate identification of small targets and boundary localization, hindering the model’s ability to generalize effectively in medical image segmentation.
As illustrated in Figure, the PraNet model highlights some of the persistent challenges in polyp segmentation, underscoring the need for better methods to address these issues.

Inspired by multiscale and multilevel feature modeling approaches, we propose a high-frequency information-enhanced polyp segmentation framework, termed. The main components of HiFiSeg include the pyramid vision transformer (PVT), the global-local interaction module (GLIM), and the selective aggregation module (SAM). PVT, a lightweight hierarchical Transformer, serves as the encoder to capture multiscale features efficiently. GLIM employs parallel convolutional kernels and pooling operations of varying sizes to aggregate global and local information, allowing the extraction of fine-grained features. This is particularly advantageous for localizing small targets. To reduce computational complexity, GLIM uses grouped channels with depthwise separable convolution. SAM refines boundary features by leveraging high-level semantic information to guide the selective refinement of low-level details.

In summary, our contributions are as follows:

• We propose, a novel framework for colon polyp segmentation. HiFiSeg utilizes the Pyramid Vision Transformer as an encoder to capture more robust features than CNN-based methods.

• We design two key modules,and, to enhance the framework. GLIM improves segmentation performance for small targets by extracting multiscale local features, while SAM addresses boundary ambiguity by selectively fusing low-level boundary details with high-level semantic information.

• We evaluate HiFiSeg on five standard benchmark datasets for polyp segmentation, including Kvasir, CVC-ClinicDB, CVC-300, CVC-ColonDB, and ETIS. On the challenging CVC-ColonDB and ETIS datasets, HiFiSeg achieves mDice scores of 0.826 and 0.822, respectively, surpassing existing state-of-the-art methods.

SECTION: 
SECTION: 
CNNs are deep learning models specifically designed for processing image data, excelling in feature extraction capabilities, and are widely used in computer vision tasks. In recent years, CNN-based structures represented by the UNet architecture have made significant progress in medical image segmentation. UNet consists of a symmetric encoder and decoder, with skip connections that transfer features from the encoder to the decoder, combining low-level features and high-level semantic information to achieve high-precision segmentation. Many works have made improvements based on the UNet architecture, such as UNet++, ResUNet++and DoubleUnet.

Unlike the UNet-based methods, PolypNetproposed a dual-tree wavelet pooling CNN with a local gradient-weighted embedding level set, significantly reducing the false positive rate, significantly reducing the false positive rate. Caranetproposed a context axial reserve attention network to improve the segmentation performance on small objects. PraNetgenerates a global map based on high-level features aggregated by the parallel partial decoder and employs the reverse attention module to mine boundary cues, effectively correcting any misaligned predictions, thereby improving segmentation accuracy.

SECTION: 
Transformer, proposed by Vaswani et al., uses multi-head self-attention to capture long-range dependencies. Initially designed for natural language tasks like translation, Transformers are now widely used in image processing and speech recognition due to their parallel processing and global context modeling. Vision transformer (ViT)was the first pure Transformer for image classification, processing images as fixed-size patches. Subsequent models like Swin Transformer, PVT, and Segformerintroduced pyramid structures for improved vision tasks.
Meanwhile, diffusion modelshave become popular for iteratively refining images through noise reduction. When combined with Transformers, they enhance feature extraction and segmentation, improving performance in tasks like medical image segmentation and object detection.

In medical image segmentation, hybrid architectures combining Transformers and CNNs have shown promise. Transfuseintegrates Transformers and CNNs to capture global and local features, while TransUNetuses Transformers as encoders and U-Net to refine local details. Polyp-PVTleverages PVT as an encoder with a graph-based similarity aggregation module. ColonFormermodels global semantic relations and refines polyp boundaries, while DuATintroduces dual-aggregate Transformers to balance large and small target detection and enhance boundary precision.

SECTION: 
SECTION: 
As shown in Figure, our proposed network HiFiSeg consists of a pyramid vision transformer (PVT) encoder, global-local interaction module(GLIM), and selective aggregation module(SAM). The PVT encoder is employed to extract multi-scale hierarchical features from the input image, capturing both fine-grained local details and broad semantic information. Specifically, the PVT backbone yields four pyramid features. The high-level featuresare fed into the GLIM module to extract local multiscale features. The outputs of the GLIM module are then concatenated to produce the fused global-local multiscale feature. The low-level featureis selectively aggregated with the high-level featurethrough the SAM module to obtain the enhanced edge feature.
Finally,andare fed into the segmentation heads to obtain the predicted resultsand, respectively.

SECTION: 
Some recent studies have demonstrated that pyramid structures, through the integration of multi-scale contextual information, can substantially improve the accuracy and efficiency of image segmentation. Our model uses the pyramid vision transformer (PVT) proposed in [46] as the encoder backbone to extract more robust features for polyp segmentation. PVT is the first pure Transformer backbone designed for various pixel-level dense prediction tasks. In polyp segmentation, PVT generates four multi-scale feature maps. Among these feature maps,gives detailed information about the polyps, while, andprovide high-level features.

SECTION: 
In medical image segmentation, the context and background often occupy a much larger area than the segmentation target itself. Consequently, capturing information across different scales is essential for accurately segmenting small targets. Instead of presenting multi-scale features in a layer-by-layer fashion, GLIM aggregates global and local features at a specific high-level, achieving multi-scale feature representation at a granular level, reducing errors in high-level features.

The detailed architecture of our propoed GLIM is depicted in Figure, consisting of three convolution branches and one pooling branch. To balance accuracy and computational resources, we evenly divide the channels into four groups, applying depthwise separable convolution for each. After splitting the input featuresinto four components, they are fed into feature generation units at different scales. Given the input feature, this procedure can be formulated as:

wheredenotes the channel separation operation,represents 1×1 convolution,refers to 3×3 depth-wise convolution,refers to 5×5 depth-wise convolution, GAP stands for global average pooling, Sigmoid refers to the Sigmoid activation function, andis the
element-wise product. The convolution branchs employ kernels of varying sizes to extract features at different scales of the image, while the pooling branch uses global average pooling to aggregate global information. These feature mapsare then concatenated along the channel dimension, and a 1×1 convolution is applied to aggregate both global and local information, resulting in a rich feature representation.This process can be expressed as:

wheredenotes a concatenation operation, whilerefers to a 1×1convolution. To enhance feature selection, we apply the GELU activation function to the featureto generate the attention feature map, and then modulate the input featurethrough element-wise multiplication. It can be formulated as:

where GELU refers to the GELU activation function, andis the
element-wise product.

SECTION: 
Shallow features contain rich spatial information, while deep features contain more semantic information. The effective combination of these two is crucial for improving the accuracy of the model. In order to enhance the guidance of shallow detail features by deep semantic features, we propose the selective aggregation module (SAM), as shown in Figure 1(c). Unlike previous fusion methods that directly add the provided feature maps, SAM selectively aggregates the features. First, the shallow featureand deep featureare individually processed through 1×1 convolutions followed by sigmoid activations to produce the attention weight. The output of the Sigmoid function could be represented as:

Ifis high, the model assigns greater trust to the shallow feature, and vice versa.The output of the SAM can be written as:

SECTION: 
We use weighted binary cross-entropy(BCE) loss and the weighted intersection over union(IoU) loss for supervision. Our loss function can be formulated as Eqn. 6:

where,are the outputs and G is the ground truth,andare the weighting coefficients,andare the weighted BCE and weighted IoU.

SECTION: 
To validate the proposed HiFiSeg method’s superiority, it is compared with multiple state-of-the-art approaches on five popular datasets for polyp segmentation, namely, Kvasir, CVC-ClinicDB, CVC-300, CVC-ColonDB, ETIS.

SECTION: 
We used five challenging public datasets for the polyp segmentation task, including Kvasir, CVC-ClinicDB, CVC-300, CVC-ColonDB, and ETIS, to validate the learning and generalization capabilities of our model. Details for each dataset are as follows:

The dataset consists of 1000 images with different resolutions from 720 × 576 to 1920 × 1072 pixels.

The dataset contains 612 polyp images which are extracted from 29 different endoscopic video clips.The resolution of images is 384 x 288.

The dataset consists of 60 polyp images and the resolution of the images is 574 x 500.

The dataset consists of 380 polyp images and the resolution of the images is 570 x 500.

The dataset consists of 196 polyp images and the resolution of the images is 1225 x 966.

SECTION: 
We employ three widely-used met-
rics in the field of medical image segmentation,i.e., mean Dice (mDice), mean IoU (mIoU) and mean
absolute error (MAE) to evaluate the model performances. Mean Dice and IoU are widely utilized metrics that primarily focus on assessing the internal consistency of segmentation results. MAE, on the other hand, measures the pixel-level accuracy by calculating the average absolute error between the predicted and actual values.

SECTION: 
We randomly split the images from Kvasir and CVC-ClinicDB into 80for training and 20for testing. And test on CVC-300, CVC-ColonDB and ETIS datasets. Due to the uneven resolution of the images, we resized them to 352×352 resolution.

We implement the HiFiSeg using the PyTorch framework, utilizing an NVIDIA RTX 3090 GPU. To enhance the model’s robustness concerning varying image sizes, the training images are scaled by factors of 0.75, 1, and 1.25, respectively, before being fed into the model for learning. PVT encoder uses the same parameters as pvt_v2_b2. The model is trained end-to-end using the AdamWoptimizer, with the learning rate and weight decay set to 1e-4. The batch size is configured to 16.

SECTION: 
We first evaluate the learning ability of the proposed model HiFiSeg on the training datasets Kvasir and ClinicDB. As shown in Table, we compare our proposed HiFiSeg with recently published and classical models for polyp segmentation, including CNN-based models such as UNet, UNet++, PraNet, and SANet, as well as Transformer-based models like TransUnet, SSFormer, Polyp-PVT, and ColonFormer. These results demonstrate the effectiveness of our model in accurately segmenting polyps. Specifically, the HiFiSeg model has a mDice value of 0.933 and a mIoU value of 0.876 on Kvasir dataset, which are 0.6% and 0.9% higher than the best performing model, ColonFormer, respectively. For CVC-ClinicDB dataset, the HiFiSeg model has a mDice value of 0.942 and a mIoU value of 0.897, which are 0.6% and 0.8% higher than the best performing model, Polyp-PVT, respectively.

To further evaluate our model’s generalization performance, we test HiFiSeg on three unseen datasets: CVC-300, CVC-ColonDB, and ETIS. These datasets originate from different medical centers, each presenting unique challenges and characteristics. As seen in Table,
on three unseen datasets, our model outperforms peer models across all metrics, demonstrating strong generalization performance. On CVC-300 dataset, HiFiSeg achieves mDice of 0.905 and mIoU of 0.839, outperforming the second-best model, Polyp-PVT, by 0.5% and 0.6%, respectively. On CVC-ColonDB dataset,
our model’s mDice and mIoU scores are 1.5% and 1.9% higher than those of ColonFormer, respectively. Moreover, HiFiSeg achieves mDice of 0.822 and mIoU of 0.743 on ETIS dataset, which are 3.3% and 3.2% higher than the second-best model ColonFormer.

Figurepresents the visualization results of our model alongside the comparison models, providing a qualitative assessment of their performance. As shown in Figure, our model produces significantly fewer incorrectly predicted pixels in the segmentation results compared to other models. It accurately identifies colonic tissues and polyps, efficiently captures the boundaries of tiny polyps and target objects, and maintains stable recognition and segmentation capabilities across various imaging conditions. As seen in the first three rows of Figure, HiFiSeg accurately captures the boundaries and fine details of the target object, whereas the other methods fail to clearly detect the boundaries. In rows 4 and 5, our method demonstrates superior ability in identifying small targets and produces more accurate segmentation predictions.

SECTION: 
We use PVTv2 as our baseline (Bas.) and evaluate module effectiveness by removing components from the complete GLIM. The training, testing, and hyperparameter settings are the same as mentioned in Sec. III-C. The results are shown in Table.

To evaluate the effectiveness of GLIM, we trained a version of the model: ”HiFiSeg (w/o GLIM).”As shown in Table, compared to the standard HiFiSeg network, the performance of HiFiSeg (w/o GLIM) is reduced across all five datasets. This is particularly noticeable on ETIS dataset, where the mDice drops from 0.822 to 0.798 and the mIoU decreases from 0.743 to 0.725. As shown in the visualization results in Figure, the HiFiSeg (w/o GLIM) model struggles to effectively distinguish between polyps and colon tissues and has difficulty accurately localizing targets, particularly small ones. In contrast, the HiFiSeg model, with the inclusion of the GLIM module, significantly improves the accuracy of target localization and small target detection due to the aggregation of local and global features.

To evaluate the effectiveness of SAM, we trained a version of the model: ”HiFiSeg (w/o SAM).”
As shown in Table, compared to HiFiSeg(w/o SAM), HiFiSeg shows a substantial improvement in performance on all five datasets. Specifically, the mdice on CVC-ColonDB dataset is improved from 0.798 to 0.826 and the mIoU is mentioned from 0.721 to 0.749, both of which are improved by 2.8%. The visualization results in Figureshow that SAM enables more accurate boundary extraction by effectively combining local pixel information with global semantic cues.

The GLIM consists of three convolution branches and one global average pooling(GAP) branch. The convolutional branches extract local features at multiple scales using convolutional kernels of varying sizes, while the global average pooling branch captures global information by spatially averaging the entire feature map, thereby better capturing the overall semantic context. To verify the effectiveness of the convolutional branches, we removed it from GLIM, resulting in HiFiSeg (w/o Conv). As shown in Table, compared to the original HiFiSeg, the performance of the modified model drops significantly due to the lack of rich local representations, particularly on the ETIS dataset, where mDice and mIoU decrease by 3.4% and 3.3%, respectively. To verify the effectiveness of GAP, it is replaced with a 7×7 convolution, resulting in the model HiFiSeg (w/o GAP). As shown in Table, the lack of global semantic information leads to a performance decline across all datasets, particularly on the ETIS dataset, where mDice and mIoU drop by 4.1% and 3.9%, respectively.

SECTION: 
In this paper, we proposed HiFiSeg network to address the challenges in colon polyp image segmentation, such as fine-grained target localization and boundary feature enhancement. Specifically, the GLIM fused global and local features by extracting multi-scale features in parallel, which facilitated the localization of targets of varying sizes. The SAM selectively combined semantic features with detailed features to alleviate the issue of unclear boundaries, further enhancing performance. Experimental results on five representative colon polyp datasets demonstrated that the HiFiSeg algorithm possessed strong learning and generalization capabilities, outperforming other competing methods. In future work, we plan to explore lightweight architectures to reduce model complexity, thereby extending its applicability to a wider range of medical image segmentation tasks.

SECTION: References