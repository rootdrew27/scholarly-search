SECTION: Reliable Projection Based Unsupervised Learning for Semi-Definite QCQP with Application of Beamforming Optimization

In this paper, we investigate a special class of quadratic-constrained quadratic programming (QCQP) with semi-definite constraints. Traditionally, since such a problem is non-convex and N-hard, the neural network (NN) is regarded as a promising method to obtain a high-performing solution. However, due to the inherent prediction error, it is challenging to ensure all solution output by the NN is feasible. Although some existing methods propose some naive methods, they only focus on reducing the constraint violation probability, where not all solutions are feasibly guaranteed. To deal with the above challenge, in this paper a computing efficient and reliable projection is proposed, where all solution output by the NN are ensured to be feasible. Moreover, unsupervised learning is used, so the NN can be trained effectively and efficiently without labels. Theoretically, the solution of the NN after projection is proven to be feasible, and we also prove the projection method can enhance the convergence performance and speed of the NN. To evaluate our proposed method, the quality of service (QoS)-contained beamforming scenario is studied, where the simulation results show the proposed method can achieve high-performance which is competitive with the lower bound.

SECTION: IIntroduction

With the development of 6G networks towards service-driven paradigms that prioritize user preferences, the optimization process in these networks faces an inevitable challenge of solving constraint optimization (CO) problems based on personalized user preferences and service performance constraints[1]. While traditional iterative methods have demonstrated success in solving problems with precise convex constraints or providing approximate solutions for simple non-convex constraints, their iterative process often involves computationally intensive operations such as computing Hessian matrices or inverses[2,3]. As a result, they fail to address real-time CO problems that require timely solutions, such as beamforming design issues with user rate preference constraints. This limitation undermines their applicability in time-sensitive applications. To overcome these challenges, researchers have turned to computationally efficient deep neural networks (NN), employing the learning to optimize (L2O) approach for solving CO problems. However, the utilization of L2O is not without flaws[4,5]. The black-box nature of NNs, their data dependencies, deterministic parameters, and scaling law pose ongoing challenges in terms of efficiency, constraint handling, performance, and training costs that researchers cannot overlook

While the universal approximation theorem suggests that NNs have the capability to model any mapping, thus theoretically implying the feasibility of designing an NN that can project input optimization problems into a feasible solution space, this assertion assumes the use of infinitely wide networks, which is impractical in real-world wireless network systems. When employing finite-width NNs, mapping errors are inevitable, leading to outputs that lie outside the feasible region[6]. Consequently, existing methods that utilize L2O for solving CO problems often struggle to achieve satisfactory performance while maintaining low computational complexity. Even conventional approaches, such as constructing penalty functions associated with constraints to solve CO problems through unconstrained Lagrangian dual functions, fail to ensure computational efficiency[7,4,5]. This is due to the increased complexity of the Lagrangian dual functions compared to the original CO problem’s objective functions, which in turn makes it more challenging to fit NNs. Moreover, these methods do not guarantee the feasibility of constraints as non-convex Lagrangian dual functions may lead NN to fall into local minima outside the feasible region. Existing feasible L2O methods, such as those based on sampling, require repetitive sampling within a fixed sampling space until a feasible point is found for the final output. However, this approach is computationally inefficient and limited to fixed constraint equations, unable to handle variable constraints. Such limitations contradict the diverse service requirements and personalized development goals of 6G networks, where user demands are dynamic and not always identical.

Hence, it seems to increasingly necessary to enhance the model’s representational capacity to ensure the reliability of mappings from optimization problems to feasible solution spaces. As a result, NN architectures employed for wireless network beamforming optimization have evolved into more complex structures, transitioning from simple Multilayer Perceptrons (MLPs)[8]to Convolutional NNs (CNNs)[9], Long Short-Term Memory (LSTM) networks, and further to Graph NNs (GNNs) and Transformers[7,10]. These advanced architectures possess unprecedented representational capabilities, enabling them to effectively tackle CO problems in radio frequency resource management. However, indiscriminate model size expansion offers minimal benefits for solving CO problems while introducing significant drawbacks. Larger models typically lead to longer inference delays, which conflicts with the trend of reducing the air interface transmission time interval (TTI) in the transition from 4G to 5G and toward future 6G networks. The reduction of TTI aims to enhance the agility and real-time capabilities of wireless radio frequency management[11]. Consequently, the adoption of oversized models results in excessive inference delays, making their practical application in wireless network radio frequency resource management challenging. Furthermore, even when expanding the model parameters to the scale of hundreds of billions, there is no guarantee of producing constraint-compliant outputs. This is analogous to how even GPT-4 may occasionally generate sentences that do not adhere to grammatical and spelling rules.

Are we then left with no recourse but to accept the possibility of L2O methods yielding solutions that violate constraints when solving CO problems? Upon examining the existing research on L2O for CO in wireless networks, it is surprising to discover that optimization problems with one-dimensional linear summation constraints, such as transmission rate optimization under power constraints or task size-constrained edge computing task offloading, are rarely treated as CO problems. Researchers often assume that the constraints can be satisfied, and experimental results also demonstrate no violations of constraints. This phenomenon can be attributed to the utilization of modern NN architectures, such as TensorFlow or PyTorch, which incorporate softmax functions. These functions enable the projection of NN outputs into a solution space that sums to one, easily meeting any first-order linear summation constraint. Could we, therefore, devise an ingenious projection function that maps the solutions of beamforming optimization problems into a feasible region, satisfying all user rate preferences and compelling any NN output to comply with the constraints?

However, there are instances where finding a feasible point within the region of the CO problem proves to be as challenging as solving the original problem itself. Therefore, the proposed projection function must exhibit computational efficiency and avoid becoming a bottleneck for L2O efficiency. Additionally, the projection function should possess the following essential properties: (1)Initial Point Dependency and Determinism: This necessitates that when a point lies outside the feasible region, its projected point within the feasible region should be related to the original point’s coordinates, rather than projecting to a random feasible point. Failure to establish this relationship would hinder the NN’s ability to learn the connection between its outputs and the projected points, thereby impeding the optimization of the CO problem. Ideally, the projection function should be a bijection, enabling the NN to comprehend how its outputs, even when outside the feasible region, can influence the objective function of the CO problem; (2)Performance Preservation: The projection function should not compromise the solution performance of the CO problem. This implies that the set of solutions after projection should include the subset of the feasible region containing the optimal points of the original CO problem. If the set of projected solutions does not encompass the optimal points, the mapping would diminish the upper bound of system performance; (3)Performance Enhancement: Ideally, the mapping function should also enhance the probability of randomly sampling the global or local optimum of the original CO problem within the mapping interval.

Although a projection function incorporating the aforementioned properties can ensure constraint compliance in solving CO problems, the data-driven L2O approach still necessitates extensive training data for optimal performance. However, generating a substantial volume of high-quality solutions as labels for supervised training is exceedingly time-consuming in the case of CO problems, particularly the NP-hard beamforming design under user rate preference constraints. Acquiring a sufficient number of training labels within an acceptable timeframe to train NNs is impractical. Furthermore, despite having an ample supply of labels, NNs may not surpass the performance of iterative algorithms, as the essence of NNs, according to the Universal Approximation Theorem, lies in fitting high-dimensional mappings. Although reinforcement learning-based methods can alleviate the reliance on optimal labels, their protracted convergence times and unstable performance render them unsuitable for solving complex CO problems. To address these challenges, this paper proposes a differentiable, initial point deterministic, performance-guaranteed, and L-Lipschitz continuous projection function. This function enables the reliable, rapid, and high-performance solution of beamforming optimization problems constrained by user transmission rate demands. By explicitly deriving gradients of NN parameters with respect to projection points and optimization objectives, a label-free, unsupervised training approach is implemented for swift NN solution. The primary contributions of this paper are as follows.

To our best knowledge, in this paper, for the first time, we propose a method that can make all solutions satisfy the constraint condition when using L2O to solve a beamforming constrained optimization problem with user rate preference. Instead of focusing only on the probability that the constraint is satisfied as in the previous work, in other words, we propose a method that can make the constraint satisfy the probability micro 100%.

Through an analysis of the Karush-Kuhn-Tucker (KKT) conditions for the NP-hard beamforming constraint optimization problem constrained by user transmission rate requirements, we propose a performance-guaranteed projection function. This function projects NN outputs into the feasible region, ensuring that the projected outputs satisfy specific KKT conditions, thereby guaranteeing their post-projection performance.

We theoretically and qualitatively analyze that utilizing our proposed projection function can accelerate training convergence while reducing the representational demands on the NN. This allows for the use of simple MLP with only one hidden layer to solve the beamforming constraint optimization problem constrained by user transmission rate requirements.

Leveraging the differentiability of our proposed projection function, we explicitly derive the gradients of NN parameters with respect to the optimization objectives. This enables training the NN in a label-free, unsupervised manner, significantly reducing training costs.

Simulation results demonstrate that the method proposed in this paper achieves performance close to the theoretical lower bound within a short timeframe.

SECTION: IISystem Model and Optimality Analysis

In this paper, we consider a single-cell, multiuser multiple-input single-output (MISO) downlink system. The system comprises a Base Station (BS) equipped withantennas, servingsingle-antenna users within its cell. This setup is designed to cater to the escalating demand for high-data-rate applications, driven by the widespread adoption of wireless devices and the proliferation of media-rich content. The wireless channel, characterized by its inherent unpredictability due to multipath propagation, is represented by the channel vectorfor the-th user. This vector captures the complex gains between the BS antennas and the receiver of the user. To facilitate transmission, a beamforming vectoris employed, which requires careful design to manage transmitted power while ensuring quality of service (QoS) for each user, as measured by the signal-to-noise ratio (SNR).

The design of the beamforming vector is formulated as an optimization problem with the objective of minimizing the transmit power, subject to individual SNR constraints for each user. These constraints ensure that the received powernormalized by the noise varianceat the receiver exceeds a predetermined threshold, guaranteeing a minimum SNR for reliable communication. This optimization problem reflects the multicast nature of the communication, where common information is transmitted to all users simultaneously. Hence, the problem can be formulated as follows.

The Problem1is a Non-deterministic Polynomial (NP)-hard problem.

According to[12]problems of the following form are known to be NP-hard

The Problem1reformulated into the above form by setting,, and, for, wheredenotes the identity matrix. Consequently, Problem1is classified as an NP-hard problem.
∎

The NP-hard nature of the optimization problem, as established previously, poses a significant challenge in terms of effective and efficient resolution using traditional convex optimization methods. This challenge becomes even more pronounced in the context of user on-demand services within 6G wireless networks, where low latency and computational efficiency are crucial. The pursuit of ubiquitous responsiveness in the 6G ecosystem, aiming to address user demands promptly and accurately, is hindered by the latency introduced by conventional optimization techniques. Therefore, it is imperative to conduct a detailed analysis of the problem’s characteristics to identify an effective method for its solution. Fortunately, through further analysis, we can derive certain properties that can serve as the foundation for designing a powerful approach to address the problem.

The optimal solution to Problem1must be on the boundary of the feasible region

The Lagrangian dual functionfor Problem1can formulated as follows.

wheresignifies the beamforming vector under optimization,encapsulates the non-negative Lagrange multipliers corresponding to the constraints,denotes the SNR requisites,is the noise variance for the-th user, andrepresents the channel vector from the BS to the-th user. As elucidated by[13], the optimal solution is characterized by several critical properties, articulated as follows.

The complementary slackness condition, as is encapsulated in equation (3c), intimates that for the optimal solution, each Lagrange multiplieris either zero or the constraintis active. Nonetheless, were allto be zero, a reformulation of the stationarity condition would yield as follows.

As established in[13], the gradient of the Lagrangian dual function with respect to the optimal solutionought to be zero. Nonetheless, an examination of the function (3e) reveals that its gradient with respect tozeroifies exclusively when. Such a condition,, cannot be reconciled with the primal feasibility condition as expressed in (3d). Consequently, it is necessitated that the optimal solution involves at least one Lagrange multiplierdiffering from zero, indicating the presence of at least one indexfor which the constraintholds true. It therefore follows that the optimal solution to Problem1invariably lies on the boundary of the feasible region.
∎

SECTION: IIIReliable projection Enhanced Neural Network Method

SECTION: III-AReliable Projection Function for Constrained MISO Beamforming

As Theorem1highlights, Problem1is NP-hard, suggesting that obtaining an optimal solution within polynomial time is unfeasible. Concurrently, the rapid advancements in NN technology provide a promising avenue for tackling NP-hard challenges. In the context of CO, employing NN-based strategies essentially involves training the network to approximate a function that maps the parameters of optimization and constraint equations directly to the optimal feasible solutions. The theoretical underpinning for this approach is supported by the universal approximation theorem, which asserts that a neural network with sufficient width can approximate any function to any desired degree of accuracy. However, practical constraints prevent the deployment of infinitely wide networks in real-world communication systems, leading inevitably to approximation errors as per the universal approximation theorem. Consequently, the output from a neural network may not always reside within the feasible region, nor can it guarantee inherently optimal solutions. Additionally, the opaque nature of neural networks complicates the analysis of conditions under which the network outputs points within the feasible region or the quality of the solutions it produces. To mitigate these issues, integrating a feature embedding module that compels the neural network to yield high-quality outputs within the feasible region becomes essential. Alternatively, employing a post-processing module to map potentially unreliable NN outputs back into the feasible region can ensure the performance quality of learning-to-optimize (L2O) schemes. Drawing inspiration from the successful application of softmax mapping in CO problems involving first-order linear sum constraints, a robust projection function has been devised, as depicted in Fig.1. This function is designed to facilitate a reliable CO solution leveraging neural network capabilities. The foundational design of this projection function is predicated on specific properties of Problems1, as initially delineated.

Given any non-zero vector, there exists a scalarsuch that the vector scaled by, expressed as, adheres to all constraints delineated in Problem1.

Consider the function

whereand by the premise, we havefor all. Asapproaches infinity, eachunboundedly increases. Consequently, there must exist a sufficiently largesuch that for all, the functionsexceed the prescribed SNR thresholds.
∎

Leveraging Lemma1, a preliminary approach could entail predefining a sufficiently large scalarand, upon determining that the neural network (NN) output vectorfails to satisfy the constraints, simply scalingbyto enforce compliance. While this technique guarantees constraint adherence for the NN’s output, it suffers from two critical deficiencies. Initially, an excessive scale factor may inflate the magnitude of, detrimentally impacting the resolution of Problem1. Moreover, the determination ofis not contingent on the NN’s current input but rather on the statistical properties of, rendering it unrelated to the NN’s output. This approach is epitomized by a rudimentary step function:

In essence, for an NN predicated on backpropagation to refine its parameters and performance, discerning the association between the output, the scaled solution, and the optimization objective is non-trivial with such a step function. This is attributed to its non-differentiable nature or its zero-derivative characteristic, thus precluding the use of this elementary mechanism to simultaneously guarantee constraint satisfaction and optimal solution quality.

Inspired by Theorem2, we propose a projection function, simple in construction yet robust in application as follows.

Given any non-zero vectorsand, the projection function (4) ensures that all constraints are satisfied upon scalingbyto yield.

Should any constraints be unmet, vectorwill be scaled byto become. Consequently, we can express:

sinceis defined as, ensuringfor allin. Therefore, the following function can be obtained.

thus all constraints can be satisfied.
∎

While the analysis validates that the projection function (4) effectively project all infeasible points into the feasible region, an enhanced formulation, as described below, can optimize problem-solving performance:

This modified projection function (5) has the following properties.

Given any non-zero vectorsand, the projection function (5) not only projects any infeasible solution into the feasible region but also enhances the performance of an initial solution that is already feasible.

Assumingis initially feasible, it follows that for allin, the inequalityholds true. Consequently, each term under the square root in the definition ofis less than or equal to 1, implying. Scalingbyas follows

where. This calculation demonstrates that the scaled vectorprecisely meets the-th constraint at equality, defining the boundary of the feasible region for that constraint.

Furthermore, for allin, given that, the-th constraint remains satisfied. This proof not only affirms that the projection function (5) universally ensures feasibility but also indicates that using this function can effectively narrow the search space for solutions by focusing on the boundary conditions defined by the constraints.

Additionally, ifinitially satisfies all constraints, the fact thatleads to:

illustrating that the projection function (5) can potentially enhance the performance of a solution that is already feasible.
∎

Then the Problem1can be reformulated as follows.

The Problem1and Problem2are equivalent.

Let us consider the transformation, which allows us to reformulate the minimization problem as:

Here,is defined as, implying that:

Squaring both sides of this inequality, we obtain:

Consequently, the following inequality can be obtained.

demonstrating that scalingbyensures all constraints of the formare satisfied, thereby confirming the feasibility ofin the context of the given optimization problem.
∎

SECTION: III-BProjection-based L2O Method

Although Problems1and2are equivalent, the transformation from Problem1to Problem2modifies the challenge from a constrained optimization problem concerninginto an unconstrained one. This adaptation enables the efficient and reliable application of NNs to solve the original problem. Additionally, it allows for a qualitative analysis of the differences in the complexity faced by NNs when tackling these problems. Firstly, projection function (5) facilitates the NN’s ability to project directly to the boundary of the feasible region, aligning along the vector from the origin, irrespective of the node output values. From a coordinate perspective, the NN needs only to determine a direction; projection function (5) will then ascertain the optimal magnitude in that direction. Thus, when addressing Problems1, the NN is required to simultaneously identify both the angle and magnitude, whereas for Problems2, it needs only to discern the optimal angle. Invoking the universal approximation theorem, the neural complexity required to learn both the angle and magnitude in the solution space of Problems1surpasses that needed merely to learn the angle in Problems2. The latter scenario demands a simpler, lower-dimensional fitting function and thus fewer neurons for an adequate fit. Employing projection function (5), as proposed here, considerably reduces the structural complexity of the NN tasked with solving Problems1. Typically, the inferential complexity of an NN is directly associated with its structural complexity, which also diminishes the computational burden. Moreover, projection function (5) significantly narrows the action space for the NN—from the entire feasible region to merely its boundary. According to[14], the training convergence speed of an NN is linearly dependent on the size of the action space. Consequently, utilizing the proposed projection function not only expedites the inference process of the NN but also reduces the training overhead, enhancing overall computational efficiency.

Since MLP is the NN architecture with the simplest structure and the weakest characterization ability, and is most likely to be replaced by NNs with more complex structures such as CNN, GNN, and Transformer to improve performance. Therefore, in order to verify the performance and versatility of our proposed projection function, we use the simplest MLP with only one hidden layer as the NN architecture of this paper. Its specific architecture is as follows. Its input layer consists ofneurons, which are used to extract the complex channel informationofusers. The hidden layer has K neurons, and the output layer isneurons, which are used to represent the beamforming vector withtransmit antennas.

The inferencing complexity of the proposed projection-based neural network (NN) method for solving Problems1is.

As delineated by Goodfellow et al.[6], the computational complexity of the-th layer in a neural network is characterized by:

whereandrepresent the sizes of the input and output layers, respectively, whileandare coefficients reflecting the structural parameters of the NN. Given this, the inferencing complexity for the proposed NN architecture, incorporatingintermediary layers between input layerand output layer, is calculated as:

Additionally, the computational effort required by the projection function is, as it involvescomputations to ascertain the boundary of each constraint equation. Therefore, when combined, the overall inferencing complexity for addressing Problem1via the projection-based NN method is aggregated to.
∎

Theorem4shows that the complexity of the projection-based NN method for solving Problem1is linear dependence withand, respectively, which makes the method have good scalability.

Remark 1. We should highlights the distinct advantages of our proposed method over traditional Learning to Optimize (L2O) approaches that utilize penalty functions and constraint projection methods. Initially, employing a penalty function to train a neural network (NN) may reduce the likelihood of L2O solutions violating constraints. However, this approach cannot guarantee constraint compliance for all outputs. In contrast, our projection-based method ensures that every output from the NN strictly adheres to the constraints. Furthermore, the conventional constraint projection method achieves compliance by resolving a constrained optimization problem that seeks the closest point within the feasible region to the current point as the projection target. Yet, the complexity of solving this optimization problem often parallels that of addressing the original problem, as it involves the same constraints. This similarity can significantly compound the difficulty of finding a solution. Moreover, for points already within the feasible region, the constrained projection method does not modify their coordinates, hence offering no performance improvement for these points. Conversely, our analysis of Problem1reveals that the optimal solution invariably lies on the boundary of the feasible region. Thus, the projection method introduced in this work not only shifts all feasible region points to the boundary but also enhances the overall solution performance by exploiting this boundary condition.

SECTION: III-CUnsupervised Training Method

To minimize the training cost, we derive the gradient of the objective value of Problem 2 with respect to the parametersof the neural network (NN), facilitating a label-free unsupervised training approach. First, we consider the gradient of the objective value with respect to the projected solution, given by:

Sinceis a function of, we express:

Analyzing the gradient ofwith respect to, whereis defined as, involves calculating the gradient of:

whereis a Hermitian matrix formed by the outer product ofwith itself, and. Applying the chain rule and quotient rule, we derive the following equations.

For a quadratic form, the gradient is:

Substituting back, we obtain

Thus, the derivativeis:

Accordingly, the parametercan be updated using the gradient:

Therefore, the NN can update the parameters by equation (16).

SECTION: IVSystem Model Validation and Performance Evaluation with Simulation

In this section, we evaluated the performance of our model in beamforming tasks through simulation tests, and compared it with traditional methods.

SECTION: A. Individual Dataset Direct Execution

Let’s conduct unsupervised learning optimization simulations on a single dataset. The channels’s are randomly and independently generated from the absolute value of a Gaussian distribution, expressed as; the variance of the noise,, is set to one; the data were obtained from ten repetitive randomized experiments. The computations were performed on a system running Ubuntu 22.04, equipped with an Intel(R) Xeon(R) Silver 4214R CPU @ 2.40GHz and an NVIDIA RTX 3090 GPU. The theoretical lower bound is calculated via SDR, and feasible solutions are obtained byGaussian randomizations. We have provided three models using the Reliable Projection Based Unsupervised Learning method. The NN model consists of three fully connected layers. Each layer is followed by a ReLU activation function during the forward pass.

We provide three models using the Reliable Projection Based Unsupervised Learning method.

Model 1:The model utilizes the projection method of function4, which only projects infeasible solutions, to optimize the neural network through unsupervised learning.

Model 2:The model utilizes the projection method of function5, which projects both feasible and feasible solutions, to optimize the neural network through unsupervised learning.

Model 3:The model first utilizes the penalty function method to pretrain the neural network, followed by employing the projection method from Equation5to project the solution into the feasible domain. During the pre-training phase, the loss function is defined as:

All three models are trained using the Adam optimizer with an initial learning rate of 0.0005. Model 1 and Model 2 have a maximum of 1000 epochs for training, while Model 3 undergoes pre-training for 100 epochs followed by 900 epochs of training.

We conducted performance testing of the model by varying,andindividually.

Varying: We setas 50, anddB. We consider different values of, namely 40, 80, 120, 160, and 200. The simulation results are depicted in Fig.3. The average loss for each model is depicted as bars, with accompanying black lines delineating the upper and lower performance bounds across different values of. From the figures, it is evident that both Model 2 and Model 3 significantly outperform Model 1 and SDR with randomization in terms of average losses and training time. Model 1, which only projects infeasible solutions, exhibits weaker convergence compared to Models 2 and 3. Moreover, as the parameterincreases, signifying a larger problem size, the Average Losses gradually increase. However, the impact on training time for projection-based methods is less pronounced than for SDR, suggesting a more scalable approach in terms of computational efficiency.

Varying: We keepanddB, while varyingwith values of 10, 30, 50, 70, and 90 for experimentation. The results are depicted in Fig.4. With increasing, the problem size increases, but the difficulty of solving decreases. The average loss increases, the training time of the SDR increases, and the time of the projection-based methods decreases.

Varying: We setand, while varyingwith values of 5dB, 10dB, 15dB, 20dB, and 25dB for experimentation. The results are shown in the figures. It is observed that with increasing, the problem complexity increases, leading to higher average losses. Due to its inherently difficult convergence nature, Model 2 often requires more time to converge under high.

SECTION: B. Training on Large-Scale Datasets

We generated data for 10,000 instances ofand split them into training and testing sets with an 8:2 ratio. Similarly, following the setup from previous simulations, all three models are trained using the Adam optimizer with an initial learning rate of 0.001, and the maximum training epochs are consistent with the previous experiments. The training process halts when the algorithm achievesfor 10 consecutive epochs. Additionally, during the training process, the test loss is recorded, and a batch size of 500 is employed.

Varying: Likewise, we retain the same configurations as previously described, withfixed at 50 anddB. We continue to examine various values of, including 40, 80, 120, 160, and 200. From Fig.6, it can be observed that the loss slightly increases with larger-scale datasets compared to single datasets, and the running time also increases. Meanwhile, the characteristic of Model 2 being difficult to converge is further magnified in larger-scale datasets.

Varying: We maintain the same parameter settings as in the previous experiments, withanddB. The outcomes of these experiments are illustrated in Fig.7.

Varying: We establishand, while exploring a range ofvalues from 5dB to 25dB in increments of 5dB for our experimentation. The results are illustrated in Fig.8.

SECTION: C. Model Deployment Strategies

In this section, we examine the time cost associated with deploying the model in practical applications. We fine-tune pre-trained models using large-scale datasets for inference in real-world scenarios. Employing function5, we conduct inference on the new single datasetusing the trained model, while varying the values of,, and. . From the figures, it is evident that projection-based learning optimization methods significantly outperform traditional SDR. They exhibit excellent timeliness and flexibility in practical applications.

SECTION: References