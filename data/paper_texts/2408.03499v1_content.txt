SECTION: FacialPulse: An Efficient RNN-based Depression Detectionvia Temporal Facial Landmarks

Depression is a prevalent mental health disorder that significantly impacts individuals’ lives and well-being. Early detection and intervention are crucial for effective treatment and management of depression. Recently, there are many end-to-end deep learning methods leveraging the facial expression features for automatic depression detection. However, most current methods overlook the temporal dynamics of facial expressions. Although very recent 3DCNN methods remedy this gap, they introduce more computational cost due to the selection of CNN-based backbones and redundant facial features. To address the above limitations, by considering the timing correlation of facial expressions, we propose a novel framework calledFacialPulse, which recognizes depression with high accuracy and speed. By harnessing the bidirectional nature and proficiently addressing long-term dependencies, the Facial Motion Modeling Module (FMMM) is designed inFacialPulseto fully capture temporal features. Since the proposed FMMM has parallel processing capabilities and has the gate mechanism to mitigate gradient vanishing, this module can also significantly boost the training speed. Besides, to effectively use facial landmarks to replace original images to decrease information redundancy, a Facial Landmark Calibration Module (FLCM) is designed to eliminate facial landmark errors to further improve recognition accuracy. Extensive experiments on the AVEC2014 dataset and MMDA dataset (a depression dataset) demonstrate the superiority ofFacialPulseon recognition accuracy and speed, with the average MAE (Mean Absolute Error) decreased by 21% compared to baselines, and the recognition speed increased by 100% compared to state-of-the-art methods. Codes are released athttps://github.com/volatileee/FacialPulse.

SECTION: 1.Introduction

Depression is a common mental health problem. According to the World Health Organization, over 264 million people worldwide were clinically diagnosed with depression in 2020, leading to severe consequences such as addiction, impulsive behavior, and suicide. Therefore, early detection plays a crucial role in significantly mitigating the harm caused by depression. Due to the scarcity of healthcare personnel, the exploration of automatic detection of depression gained attention in past years. In particular, human faces are acknowledged as a primary communication channel and a pivotal conduit for conveying crucial information about mental states, intentions, and personality traits. Past psychological research emphasized the reliability of non-verbal facial behaviors as indicators of depression(Qureshi et al.,2019). Motivated by this, this paper aims to investigate the potential of recognizing facial emotion for early depression detection.

Latest advancements in computer vision contribute to the automatic recognition of human facial behaviors(Churamani and Gunes,2020; Li and Deng,2020; Zhang et al.,2023b; Huang et al.,2023; Song et al.,2021), which facilitates automated analysis of depression from facial videos(de Melo et al.,2020; Uddin et al.,2020; He et al.,2022; de Melo et al.,2021). However, there are three main limitations
of existing methods:
1)Overlooking temporal facial characteristics.Individuals with depression exhibit fewer spontaneous facial expressions of emotion compared to healthy individuals, which indicates unique temporal features are contained in the facial expressions of depressed patients. Extensive experiments demonstrated significant improvements in recognition accuracy with Convolutional Neural Networks (CNNs) over conventional methods(de Melo et al.,2020; Uddin et al.,2020). However, these CNN-based methods treat a video as a collection of static images, focusing on spatial features while inevitably overlooking temporal characteristics and the dynamic nature of facial expressions.
2)Complex model architectures induce more computational cost.To comprehensively capture both temporal and spatial characteristics, CNN-RNN and 3DCNN methods(He et al.,2022; de Melo et al.,2021)emerged as preferred choices. However, these detection methods heavily rely on complex models or data-enhanced techniques, which require longer calculation time and higher costs.
3)Rely on redundant raw facial features.Traditional approaches mostly relied on raw images as input. However, the use of raw images as input inevitably causes information redundancy. The main reason is that these original images may contain a significant amount of task-irrelevant information, such as background and lighting conditions, which necessitates the model to handle a surplus of redundant data.

To address the above limitations, we propose an efficient framework namedFacialPulse, which contains the two primary modules: the Facial Motion Modeling Module (FMMM) and the Facial Landmark Calibration Module (FLCM). The motivation and introduction of these two modules are provided as follows:

Modeling Facial Motion Based on Temporal Sequences:Each emotion manifests a unique temporal pattern, and the temporal modeling approach offers a novel perspective for facial expression recognition. Fig.1shows the motion curves of both depressed people and normal people in the same task. The shown face motions are AU12 and AU15. AU12 and AU15 represent the upward and downward movement of the mouth, respectively. It can be clearly observed that the variation curves of depressed people appear smoother than those of normal people. In contrast to other mental disorders, since the facial motion changes of depression are not obvious, depression detection requires prolonged and continuous monitoring of changes in facial expressions. To better capture the characteristics of depression, by using the Bidirectional Gated Recurrent Unit (BiGRU) as the backbone, we propose a module that harnesses the bidirectional nature and addresses long-term dependencies for temporal modeling. To capture evolving patterns and characteristics more effectively, this module emphasizes the temporal sequence and contextuality of facial features. Besides, since incorporating parameter sharing and temporal dependencies, this module provides a significant advantage in training speed.

Facial Landmark Calibration Module (FLCM):Facial landmarks are a set of points outlining the contours of distinctive facial features and are sufficient for describing geometric information. Thus, instead of using the original raw image as input, we choose facial landmarks as input to detect depression with less information redundancy since facial landmarks contain key points of facial information while eliminating the impact of irrelevant areas on recognition. Although previous research has demonstrated the improvement effect of facial landmarks in facial emotion recognition(Rahdari et al.,2019), facial landmarks are rarely emphasized in depression detection. Furthermore, existing approaches do not take into account the accumulative errors in landmark detection. To ensure the accuracy and precision of landmark detection, we further introduce a novel landmark calibration module. By minimizing jittering, this module enhances the recognition capability of landmarks, which significantly facilitates the reliable integration of landmarks and deep temporal features.

In a nutshell, by considering the distinctive temporal characteristics of facial expressions in various depressed individuals, we further combine both preceding and subsequent contextual information to analyze comprehensive temporal information. Besides, to reduce the redundancy of input information, we employ facial landmarks as input to detect depression. Furthermore, to ensure the accuracy of the landmarks and remove the accumulative errors, we propose a novel calibration module by minimizing jittering. The introduction and calibration of landmarks significantly improve the reliability of the captured temporal features.

We evaluateFacialPulseon two datasets (i.e., AVEC2014(Valstar et al.,2014)and MMDA(Jiang et al.,2022)), which demonstrate thatFacialPulseoutperforms the baseline methods by a large margin and decreases the training time (including preprocessing time) by 2.

Overall, the main contributions of this paper can be summarized as follows:

By using the BiGRU as the backbone, a facial motion modeling module (FMMM) is proposed to better capture the characteristics of depression. This module harnesses the bidirectional nature, addresses long-term dependencies for temporal modeling, and emphasizes the temporal sequence and contextuality of facial features, which significantly improves recognition accuracy.

To ensure the accuracy of the landmarks and remove the accumulative errors, we propose a novel calibration module (FLCM) by minimizing jittering. The calibration of landmarks further improves the captured temporal feature reliability.

Extensive experiments on various datasets demonstrate the superiority of FacialPulse on recognition accuracy and speed, with the average MAE (Mean Absolute Error) decreased by 21%, and the recognition speed increased by 2compared to baselines.

SECTION: 2.Related Work

In this section, we first discuss the input difference related to state-of-the-art (SOTA) facial expression recognition-based depression detection methods in Sec.2.1. Then, the differences in network frameworks used by different SOTA methods are further discussed in Sec.2.2. By showing the differences in network inputs and the structure of related SOTA methods, the shortcomings and differences of existing methods are clearly highlighted.

SECTION: 2.1.Facial Landmarks Detection

Emotion recognition(Li et al.,2023; Zhang et al.,2023a; Pan et al.,2023b)heavily relies on facial feature detection(Sun et al.,2021; Mukhiddinov et al.,2023)and it is extremely necessary to extract effective facial features. Facial landmarks, as one of the crucial facial features in various computer vision tasks(Poulose et al.,2021; Zhang and Abdel-Aty,2022), play a pivotal role in capturing both spatial and temporal information related to facial expressions(Ngoc et al.,2020).

Classical parametric methods, e.g., Active Appearance Models(Sabina and Whangbo,2021), Constrained Local Models, Supervised Descent Variant Method, and Cascade Regression Algorithm(Gogić et al.,2021), can effectively detect facial landmarks. Due to the user-friendly interfaces and high detection speeds(Liu et al.,2023,2024), these parametric methods are widely employed and integrated into open-source image processing libraries.

Recently, deep learning models, e.g., cascade CNNs, Convolutional Pose Machines, and Constrained Local Models, have emerged in computer vision and can extract facial landmarks with high accuracy. Similarly, since the extracted landmark can significantly boost the recognition speed, these deep learning-based facial landmark extraction methods are widely integrated into open-source toolkits, like OpenFace.

Although using accurate facial landmarks for face normalization significantly improves recognition accuracy, the low quality of landmark detection directly downgrades the final system performance. Many studies(Belmonte et al.,2021; Song et al.,2023; Pan et al.,2023a)emphasize the significance of precision in detected landmarks. Furthermore, the intrinsic jitter noises of facial landmarks inevitably interfere with its temporal features. However, existing SOTA methods do not effectively calibrate the various errors of landmarks, which further makes it difficult to promote the method. To overcome these issues, a calibration module is proposed in this paper (called FLCM) to eliminate the accumulative errors for higher accuracy of landmark detection.

SECTION: 2.2.Facial-Based Automatic Depression Recognition

Psychological studies(De Melo et al.,2020; Jiang et al.,2020)indicate that the variations in facial expressions can serve as predictive indicators of individual depression severity. Consequently, numerous researchers endeavor to establish the mapping between facial features and depression scores via machine learning techniques.

Initially, hand-crafted methods generally utilize specific feature descriptors to represent depression, where Edge Orientation Histogram (EOH) and Local Binary Pattern (LBP) are used as spatial features to encode images. For example, Heet al.(He et al.,2018)proposed the MRLBP-TOP framework to capture spatial information of facial microstructure in video segments. Subsequently, a local pattern LSOGCP(Niu et al.,2019)was proposed to further extract detailed facial texture. However, the hand-crafted features used in the aforementioned works heavily rely on experience and expertise, which implies that the essential information related to depression may be lost when manually extracting features.

To overcome this problem, researchers are inclined to detect depression based on deep learning architecture, especially CNN-based models. Specifically, Heet al.(He et al.,2021)parted the face into 24 small blocks and further adopted attention mechanisms and an aggregation method to enhance spatial significance. Meanwhile, Meloet al.(de Melo et al.,2021)proposed that inserting maximizing and differentiation blocks into 2D-CNNs to capture facial changes can improve recognition accuracy.

To further capture spatiotemporal features to detect depression, various SOTA methods employed 3D-CNNs to encode temporal information. For instance, Zhouet al.(Zhou et al.,2020)developed a strategy based on 3D-CNN that combines label distribution and metric learning to enhance the representation capability for spatiotemporal information. C3D technology was employed in(de Melo et al.,2020)to extract spatiotemporal features to enhance depression-related information through attention blocks. This operation effectively reduced noise and summarized video-level depression information. Similarly, Heet al.(He et al.,2022)proposed a 3D CNN framework equipped with a spatiotemporal feature aggregation module to accurately characterize depression cues in video segments.

Although the above methods achieve satisfactory performance by extracting facial depression information with CNNs, most of them require high time complexity. Furthermore, these methods overlook the continuity of facial expressions in depressed individuals, which results in the limitation of detecting temporal features of depression. To effectively solve this problem and to better capture correlations in consecutive facial expressions, by focusing on the temporal sequence of facial expressions, we devise a target FMMM to capture the accurate depression characteristics.

SECTION: 3.Methods

SECTION: 3.1.Overview

The workflow of the proposed depression detection frameworkFacialPulseis illustrated in Fig.2, which is composed of two modules: Facial Landmark Calibration
Module (FLCM) in Sec.3.2and Facial Motion Modeling Module (FMMM) in Sec.3.3. In particular, the FLCM is used for the meticulous calibration of facial landmarks to eliminate accumulative errors while the FMMM is employed to cope with long-term dependencies for temporal modeling and emphasize the temporal sequence and contextuality via the bidirectional nature of BiGRU.

SECTION: 3.2.Facial Landmark Calibration Module

To effectively extract facial Landmarks from original images, we first conduct face detection on each video frame to estimate the facial bounding box and preserve the region of interest that contains the face. Then, based on the processed facial image, 68 facial landmarks are further extracted to outline the facial contour. Finally, an affine transformation(Zhang and Ding,2022)is employed to achieve point-to-point alignment(Dudhane et al.,2023)and localization.

Given the fact that landmark detection is essential for capturing facial features, how to guarantee the accuracy of facial landmarks is a critical issue. Fig.3shows the movements of different facial landmark units during an expression that appears stable. AU5 and AU7 denote the units near the eye area, while AU12 and AU15 express the units near the mouth area. We can observe that despite seemingly stable facial expressions, there is still a discernible fluctuation in facial landmarks, which significantly disrupts temporal consistency. This phenomenon clearly explains the importance of effectively detecting facial landmarks.

Facial movements tend to be smaller during depression expressions. Unfortunately, facial landmark detection noise has a greater negative impact in scenarios with minor facial movements. Hence, it is significant to obtain a more accurate sequence of facial landmarks when detecting depression. To solve this problem, we design the Facial Landmark Calibration Module to mitigate the impacts of abnormal fluctuations for further improving the detection accuracy of facial landmarks. FLCM is composed of motion landmark prediction and landmark error filtering, which we will introduce in detail below.

During dynamic changes in facial expressions, the position of landmark pixels should remain almost the same during small periods. However, there may be some jitter in the actual detected facial landmarks. Landmarks with large jitter will cause significant errors in the detection results. Therefore, we use the optical flow algorithm to predict the facial landmark position of the current frame to provide a reference for the detection results of the next frame. Then, we compare the predicted facial landmark with the currently detected facial landmark. Points with a large difference between the detected value and the predicted value indicate that there is a larger jitter and these points will be discarded.
In particular, the sparse optical flow can selectively track a subset of points in the image rather than track all points. Considering the proposed motion estimation is based on facial key point sequences, we adopt the sparse optical flow to predict motion landmarks. Due to the reduction of tracking points, the use of sparse optical flow further improves the training speed.

Assuming that the pixel coordinatesin the initial frame denote the value of the pixelat time, and the pixel movesafter a time interval.
Since the pixel is usually stable over a short period and its intensity remains constant, this process can be formulated as:

Assuming the motion is negligible over a short period, Taylor’s formula(Odibat and Shawagfeh,2007)can be employed to express this relationship. Thus, the Eq.1can be reformulated as:

whereanddenotes the gradient of pixelin the horizontal direction (direction) and vertical direction (direction), respectively. For simplicity, we representandasand. Besides, the coordinate change velocity parametersandare denoted asand, respectively. Hence, the Eq.2can be simplified as follows:

whereand, as two parameters of the optical flow field, numerically describe changes in pixel positions between adjacent frames. These two parameters can directly represent the motion of objects or scenes in the image.

There are significant challenges in calculating the parameter values ofand. When we use one single pixel to calculate the corresponding parameter values, there are two unknown parameters that cannot be effectively solved from one motion equation. Considering that adjacent points within the same exhibit similar motions, we first choose several points (the chosen point number denoted as) in an adjacent block matrixto replace a single pixel to achieve a target that uses multiple equations to solve the goal of two unknown parameters. Then, we employ the Lucas-Kanade (LK) algorithm(Özer and Ndigande,2024)to calculate the values ofand:

Due to the fact that the least squares algorithm has a small error in the fitting process, we employ this algorithm to solve the above motion equations. Therefore, Eq.4can be rewritten as matrix form:

In this way, the fitting process of parametersandcan be expressed by the following equation:

Since the size of the adjacent block matrixis a fixed number, the LK algorithm that solves the values of the coordinate change velocity parametersandmay not be adaptive to pixel motions at different scales.

To address this challenge, we employ the Pyramid Lucas-Kanade (PLK) architecture to adaptively capture motion information of pixels at different scales. Fig.4illustrates the workflow of the PLK algorithm in our depression detection task. Specifically, to reduce unnecessary calculations, we first construct a Gaussian pyramid of input images, where each level represents a different scale. Then, for each landmark pixel, iterating from the roughest granular scale, the optical flow estimation is performed at the top level. Subsequently, the estimated flow is propagated downward through the pyramid. For each level of the pyramid, to generate the corresponding pixels in the current layer, the pixels in the previous layer are aligned to adapt to the current layer resolution. Furthermore, the LK algorithm is employed to process current layer pixels to estimate the motion information. It is worth noting that this process continues until reaching the bottom level. Finally, the estimated flows of all levels are combined to obtain the final motion estimation.

To further minimize errors caused by the calculation order, we introduce a two-way error denoise mechanism. Specifically, we first compare the difference in pixel motion information calculated using the PLK algorithm from front to back and from back to front. Then, we use the threshold method(Cao and Liu,2024)to process the difference between these two pixel motion information. According to the threshold judgment, we further decide whether to use the landmark pixel for prediction. Fig.5depicts the selection process of facial landmarks. We can observe that the landmark pixels with significant positional differences calculated by the two-way error denoise mechanism are discarded for more accurate landmark prediction. On the contrary, the landmark pixels with tiny positional differences are retained to detect depression.

Fig.5depicts the total workflow of the proposed calibration algorithm. During the entire calibration process, three types of errors are effectively calibrated, namely, jitter noise (denoted by the blue circle), flow prediction error (denoted by the green triangle), and detection error (denoted by the orange rhombus). The jitter noise is caused by the actual detected point jitter. Although different facial landmark points are effectively selected by positional differences (jitter noise elimination), there are still a lot of flow prediction errors caused by the LK algorithm in the prediction process. This is because the LK algorithm is a fitting algorithm and cannot obtain accurate analytical points, there are errors in the fitting process. These errors are actually flow prediction errors. Furthermore, due to changes in facial expressions and poor image quality, there are also errors in the landmark detection process, and these errors are called detection errors. To compensate for the inaccuracy and limitation in these two data sources (flow prediction results and detection results), we fuse the predicted values from the previous frame and the detected values from the current frame to obtain more reliable and complete facial motion information.

Since Kalman filtering proves effective in calibrating bimodal correlation errors due to incorporating prior information into state estimation(Deng et al.,2024), we use Kalman filtering(Huang et al.,2024)to fuse the flow prediction results and detection results to comprehensively obtain more accurate landmark point positions.

SECTION: 3.3.Facial Motion Modeling Module

Since the facial expressions of depressed individuals have unique temporal characteristics, we utilize temporal features for depression detection. Furthermore, we verified that facial landmarks can reflect fine-grained facial fluctuations even in subtle expression changes, and facial landmarks can accurately represent the feature information of facial expressions. Depressed individuals have more subtle changes in facial expressions than other mental illnesses. Therefore, we simultaneously model facial absolute positional information and relative change information in individuals with depression.

We divide a video into multiple time windows and extract feature vectors in each time window to represent the characteristics of facial expressions. Previously we have obtained the calibrated facial landmarks, given the calibrated landmark point, the first type of feature vectorwhich represents facial absolute positional information, derived from landmarks, is generated as follows:

Then, the second type of feature vectorwhich represents facial relative change information can calculated by:

These feature vectors form two feature vector sequences, which represent the temporal feature changes of the entire facial expression.
Based on the above process, we obtain two feature vector sequences:

Since facial expressions are dynamic and temporally dependent, and the expressions of individuals with depression may suddenly change in a short period and exist for a long period, temporal features are thus particularly critical in detecting depression. Considering that combining forward and reverse information flows, which can more comprehensively capture the information in sequence data and mitigate information loss, Bidirectional Gated Recurrent Unit (BiGRU) is chosen as the backbone of our network to accurately capture the temporal features of depression expressions. Empowered by BiGRU, the proposed module can take into account both past and future information to better understand the facial expression context and its corresponding evolution.

Specifically, we employ two BiGRU networks to encode these sequences separately. The first BiGRU () models facial motion patterns on sequence. Its bidirectional recurrent structure is profitable for mining temporal characteristics in landmark motion, which effectively focuses on dynamic variations in landmarks across consecutive frames and precisely extracts temporal information related to depression. Then, the second BiGRU () processes landmark motion speed patterns on sequence. By capturing temporal features of landmark differences, this network can identify subtle facial motion changes in a brief period, which further upgrades sensitivity in detecting emotional fluctuations. Additionally, since the gating mechanism of BiGRU can learn and remember patterns in sequence data more effectively, it can also converge faster than traditional methods and accelerate the training process.

The fully connected layers are employed after the output of each BiGRU, which maps the representations to the depression detection level, respectively. The outputs of both streams are averaged to obtain the final depression detection result. Since our method comprehensively considers the two kinds of temporal features (absolute positional information and relative change information) and effectively captures long-term dependencies in time series data, we can capture more complete and accurate temporal features to effectively improve the accuracy of depression detection.

SECTION: 4.Experiments

In this section, we first show the details of the dataset and implementation. Then, we assess both the performance and efficiency of our proposedFacialPulseframework. Finally, ablation experiments are conducted to investigate the impact of the devised modules.

SECTION: 4.1.Experimental Setup and Details

We evaluate the performance of our method on two depression datasets: the AVEC2014 dataset and an internally collected dataset. The AVEC2014 Depression dataset(Valstar et al.,2014)consists of 300 videos from the 2014 Audio/Visual Emotion Challenge and Workshop, including ”NorthWind” and ”FreeForm” tasks. In the context of the ”NorthWind” task, participants delve into a German fable entitled ”Die Sonne und der Wind,” where they read through its narrative. On the other hand, the ”FreeForm” task demands not only answering a series of questions but also recounting a poignant childhood memory in the German language. Each task includes 150 video segments, with 80% of them allocated for training and the remainder for testing. In our experiments, we merge the samples from both tasks. Subsequently, we allocate 240 samples for training and 60 samples for testing. These videos are captured via webcams and microphones with an average duration of two minutes. Additionally, each video is labeled with the depression level, which is determined by the Beck Depression Inventory-II (BDI-II) questionnaire. Particularly, BDI-II is an estimation method of depression levels and has depression values ranging from 0 to 63, where 0-13 implies no depression, 14-19 mild depression, 20-28 moderate, and 29-63 severe depression.

The other internally collected dataset, named the Multimodal Dataset for Depression and Anxiety (MMDA)(Jiang et al.,2022), was specifically designed for depression and anxiety detection. All participants are diagnosed by professional psychologists based on the combined Hamilton Rating Scale for Depression scores and Anxiety scores. MMDA includes visual, acoustic, and textual modalities, which are extracted from the original interview videos. In our experiments, we select all 300 depression detection video segments that are related to facial expressions from this dataset.

With the release of the AVEC2014 dataset, Root Mean Square Error (RMSE) and MAE are used as metrics for the 2014 Audio/Visual Emotion Challenge and Workshop. After that, these two metrics have been widely adopted to evaluate the performance of depression detection. For the sake of fairness, we also use RMSE and MAE as evaluation metrics in the experiments, which can be formulated as:

whereis the number of participants,anddenote the true and predicted BDI-II scores for the-th participant, respectively.

During preprocessing, we utilize Dlib for face and landmark detection. As for ablation studies, OpenFace serves as an alternative detector. Each RNN in our dual-stream network is bidirectional, which employs GRU withoutput units for classification. A fully connected layer with a single unit is connected to the back of the RNN layer. We insert a dropout layer with a rate of 0.25 between the input and the RNN. Furthermore, three dropout layers with a rate of 0.5 are embedded in the remaining layers. The Adam optimizer with a learning rate of 0.001 is adopted. During classification, we choose the smoothLoss function, which is defined as follows:

whererepresents the error between the predicted value and the true value.
Compared to the Mean Squared Error, the smoothloss function appears to lower sensitivity to outliers, which can significantly boost the robustness against potential outliers in the data. The classification model is trained 500 epochs. All the experiments are conducted on a single RTX 3090 GPU with 24GB memory.

SECTION: 4.2.Performance Evaluation

To verify the superiority ofFacialPulse, we compare it with other state-of-the-art methods on the AVEC2014 dataset.
Typically, methods based on deep neural networks present better performance compared to hand-crafted methods, which is primarily attributed to the fact that hand-crafted features rely on the expertise of researchers. In such cases, hand-crafted methods may not comprehensively mine depression cues, thereby decreasing prediction accuracy.
As shown in Tab.1, we report the results of comparative experiments with the evaluation metrics RMSE and MAE. Among all listed pioneer depression recognition methods,FacialPulseattains the top performance on MAE and second-best performance on RMSE. In particular, since the temporal features are deeply considered,FacialPulsesurpasses with 1.5% MAE improvements over the previous SOTA method(Niu et al.,2022)on AVEC 2014 datasets. By assessing RMSE and MAE, Fig.6(a) indicates thatFacialPulseachieves the best overall performance among the three listed SOTA depression recognition methods.

Furthermore, on an internally collected MMDA dataset,FacialPulseachieves a significant decrease in MAE compared to a baseline (SVM) (4.35vs3.97). These results demonstrate the significant competitiveness of the proposedFacialPulse, which can be attributed to the strong ability of our method to capture depression-related temporal features.

Tab.2shows the experimental speed comparisons of the proposed approach and several representative baseline methods. All methods require similar preprocessing time andFacialPulseconsumes two more hours than others due to more temporal information being considered. Noting that, due to the properties of parameter sharing and parallel computing in our method, the training time ofFacialPulseis significantly less than that of others. To observe more intuitive results on preprocessing time and training time, Fig.6(b) shows that the proposed method is significantly closer to the zero point than others. The result clearly indicates thatFacialPulseis significantly superior to other SOTA methods in terms of training speed.

Additionally, Tab.3depicts the details of experimental costs including parameter sizes and GPU memory usage. SinceFacialPulsehas a small parameter space and employs a parallel computing strategy, it exhibits quite low training costs compared to others.

To validate the effectiveness of the proposed calibration module, we conduct a confirmatory experiment. We first divide facial landmarks into seven regions. Then, different detectors (OpenFace and Dlib) are employed to detect landmark locations. Fig.7illustrates the mean distance between landmarks detected by different detectors. Using different detectors brings different noises and the calibration module aims to eliminate the noise and make them closer to the true position on the ground. Thus, this process can shorten the gap in the detection results of different landmark detectors.

From Fig.7, we observe that after applying our calibration module, the detected differences of each organ significantly reduced and the average distance between the seven sets of landmarks decreased by 11%, which signifies an improvement in landmark detection accuracy. Due to the effectiveness of the proposed calibration module, we successfully eliminate noise and errors to obtain more accurate landmark positions.

SECTION: 4.3.Ablation Experiments

In this section, we explicitly investigate the influence of each module in the proposed frameworkFacialPulse, which provides evidence and a detailed explanation for the generated prominent results.

Tab.4shows the performance evaluated by RMSE and MAE under different ablation conditions. As a module is added, the values of RMSE and MAE decrease to a certain extent. Notably, in this process, the Kalman filter effectively eliminates detection error and prediction error, while the optical flow prediction module effectively eliminates jitter noise. Each module in the Facial Landmark Calibration Module aims to obtain more accurate landmarks and further improve the accuracy of depression detection.

In addition to performing ablation experiments on FLCM, we also study the impact of the two branches in the Facial Motion Modeling Module. Tab.5exhibits the depression detection results of each branch and combined branch. It can be clearly seen that the performance is significantly improved after integrating the two branches. By integrating absolute positional information and relative change information, the proposed method captures more comprehensive temporal features and achieves superior performance on both two metrics in facial depression detection tasks.

SECTION: 5.Conclusion

We propose a novel framework (FacialPulse) aimed at improving the accuracy and speed of depression recognition utilizing facial expressions.FacialPulseconsists of two key modules: Facial Motion Modeling Module (FMMM) and Facial Landmark Calibration Module (FLCM). FMMM is designed to effectively capture temporal features by employing bidirectional processing and addressing long-term dependencies. Notably, FMMM’s parallel processing capabilities and gate mechanism substantially accelerate training speed. Meanwhile, FLCM endeavors to reduce information redundancy by utilizing facial landmarks instead of original images, thereby enhancing recognition accuracy by eliminating errors associated with facial landmarks. Extensive experiments are conducted on the AVEC2014 and MMDA datasets, demonstrating the superior performance ofFacialPulse. In future work, we aim to explore the integration of other complementary modalities into our proposed architecture to further enhance model performance.

SECTION: Acknowledgements

This work is supported by the National Natural Science Foundation of China (Grant No. 62302145), Anhui Province Science Foundation for Youths (Grant No. 2308085QF230), the Major scientific and technological project of Anhui Provincial Science and Technology Innovation Platform (Grant No. 202305a12020012), and the National Research Foundation, Singapore and Infocomm Media Development Authority under its Trust Tech Funding Initiative (No. DTC-RGC-04).

SECTION: References