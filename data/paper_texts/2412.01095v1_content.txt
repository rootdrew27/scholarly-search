SECTION: \name: Explainable Video Anomaly Detection via Verbalized Learning of Vision-Language Models

The rapid advancement of vision-language models (VLMs) has established a new paradigm in video anomaly detection (VAD): leveraging VLMs to simultaneously detect anomalies and provide comprehendible explanations for the decisions. Existing work in this direction often assumes the complex reasoning required for VAD exceeds the capabilities of pretrained VLMs. Consequently, these approaches either incorporate specialized reasoning modules during inference or rely on instruction tuning datasets through additional training to adapt VLMs for VAD. However, such strategies often incur substantial computational costs or data annotation overhead. To address these challenges in explainable VAD, we introduce a verbalized learning framework named VERA that enables VLMs to perform VAD without model parameter modifications. Specifically, VERA automatically decomposes the complex reasoning required for VAD into reflections on simpler, more focused guiding questions capturing distinct abnormal patterns. It treats these reflective questions as learnable parameters and optimizes them through data-driven verbal interactions between learner and optimizer VLMs, using coarsely labeled training data. During inference, VERA embeds the learned questions into model prompts to guide VLMs in generating segment-level anomaly scores, which are then refined into frame-level scores via the fusion of scene and temporal contexts. Experimental results on challenging benchmarks demonstrate that the learned questions of VERA are highly adaptable, significantly improving both detection performance and explainability of VLMs for VAD.

SECTION: 1Introduction

Video anomaly detection (VAD) aims to automatically identify unexpected and abnormal events in video sequences, with broad applications ranging from autonomous driving[2]to industrial manufacturing[34]. While achieving good performance in VAD is essential, providing clear explanations for detected anomalies is even more crucial.

To this end, our work primarily focuses on explainable VAD, which requires both comprehensive visual understanding and the ability to generate human-interpretable predictions. The rapid advancement of vision language models (VLMs)[20,8,61,23]enables us to address both requirements through their strong visual reasoning and language interaction capabilities. As multi-modal architectures that effectively combine the reasoning capabilities from large language models (LLMs)[4]and the visual understanding capabilities from pretrained vision encoders[9], VLMs are particularly well-suited for VAD for they can offer explainable predictions that clearly illustrate the rationale behind specific anomalies, making the results more interpretable to users. Recent research on VAD has consequently focused on how to effectively leverage the power of pretrained VLM. As shown in Fig.1, existing approaches aim to address the misalignment problem between VLMs’ pretraining tasks and the VAD requirements through either additional reasoning modules or instruction tuning (IT):

One line of researchintroduces external LLMs to assist frozen VLMs to reason in VAD[55,49].
It uses VLMs to caption what they see given a video, and the descriptions are then passed to an external LLM,e.g., GPT-4[1], to reason whether an anomaly occurs.

Another line of research, instead,expands VLMs to generate explainable prediction via IT[29,58].
This research line creates additional VAD datasets with frame-level annotations and leverages exemplary instructions to fine-tune the VLM, enabling it to detect anomalies and generate human-interpretable explanations.

Key Observations and Research Question. While prior research demonstrates the potential of applying VLMs to VAD, we identify that this new paradigm is hindered by a shared critical issue: the use of additional reasoning modules or fine-grained labeled datasets incurs significant computational cost either in the inference or training phases. First, decoupling a VAD system into a frozen VLM and an extra LLM introduces more overhead in inference, because it separates the description generation and reasoning processes. Secondly, although IT-based methods enable VLMs to effectively integrate description and reasoning for VAD, they require additional manpower and computational resources for annotating and finetuning on fine-grained labeled instruction datasets, which is time-consuming and not scalable for large-scale datasets. In light of this, we investigate the following unexplored yet important question:

Can we enable a frozen VLM to integrate description and reasoning for VAD without instruction tuning?

Our Approach. This research question is nontrivial because the reasoning ability of a frozen VLM is limited in general visual tasks, and it struggles to handle complex reasoning tasks like VAD, which requires the understanding of subtle, context-dependent outliers. To illustrate, Table1shows that prompting frozen VLMs with simple VAD questions used in existing works leads to unsatisfactory results. Thus, instruction-tuning a VLM seems necessary to make it responsive to specific instructional cues and capture delicate visual variations. In this paper, we question the necessity of such an operation and propose a principled approach to tailor frozen VLMs for VAD.

Specifically, our solution is guided by the intuition that the reasoning ability of VLMs for VAD will improve if we find questions with suitable and concrete description of abnormal patterns rather than with abstract and general words like “anomaly” to prompt them. Our idea is to iteratively refine anomaly descriptions from abstract ones (e.g., “is there any anomaly?”) to detailed, specific characterizations.

Driven by such insight, we propose a framework, termed\name, to exploreverbalized learning for VAD. This framework considers the practical constraint that it is suboptimal to manually write down VAD guiding questions across VLMs, so it introduces a data-driven learning task to identify suitable anomaly-characterization questions containing concrete abnormal patterns for the frozen VLM using coarsely labeled datasets, eliminating the need for IT. Specifically, in thetraining phase, VERA treats the questions guiding the reasoning of VLMs in VAD as learnable parameters, improving them based on the verbal feedback from an optimizer VLM on the performance of a learner VLM on an intermediate VAD subtask—binary video classification for each video in the VAD training set. This design is both efficient and appropriate for VAD, as it accounts for video-specific properties like temporality while relying solely on provided coarse video-level labels.
After that, considering the large scale of video frames,\nameassigns a fine-grained anomaly score for each frame in a coarse-to-fine manner in theinference phase. First,\namegenerates segment-level anomaly scores by querying VLMs with the learned guiding questions. Next,\nameimproves the initial score by incorporating scene context into each segment score via ensembling. Finally,\nameoutputs frame-level scores by fusing temporal context via Gaussian smoothing and frame-level position weighting.

Contributions. To sum up, our contributions are:

To our knowledge, we present the first approach, that is,\name, to adapt frozen VLMs as an integrated system for VAD by learning detailed anomaly-characterization questions in prompts that decompose anomalies into concrete and recognizable patterns.\namelearns them directly from coarsely labeled datasets, eliminating the need for IT or external reasoning modules.

We introduce an effective verbalized learning-based algorithm for VLMs in VAD, allowing direct adaptation without modifying model parameters. With coarse labeled VAD datasets only, our approach obtains good guiding questions in VAD by relying on the verbal interaction between learner and optimizer VLMs in verbalized training. Additionally, we design a coarse-to-fine strategy to derive frame-level anomaly scores from verbally learned guiding questions in VAD, integrating both scene and temporal contexts for better VAD performance and reasoning.

The learned guiding questions from\nameare expressed in natural languages, providing a unified method to encode and transfer prior VAD knowledge seamlessly to other datasets or VLMs. In challenging VAD datasets like UCF-Crime[35]and XD-Violence[45],\nameachieves state-of-the-art explainable VAD performance and enjoys good generalization ability across models and datasets.

SECTION: 2Related Work

Video Anomaly Detection. VAD is the task of localizing frames that contain abnormal events in a given video. This task is challenging for anomalies cover a broad scope of events like accidents and criminal activities while training sets only offer coarse annotations. Modern VAD methods are based on deep neural networks (DNNs) for their superiority and are going through a paradigm shift in using VLMs: (1) Early DNNs for VAD are task-specific, which often employ unsupervised (including one-class) or weakly supervised (WS) learning techniques for training. Most unsupervised learning methods[25,51,59,41,28,40]train DNNs on frame reconstruction/prediction tasks to establish representation spaces for normal/abnormal videos. WS learning methods[35,6,50,56,30]leverage both normal and abnormal videos to train a feature extractor that distinguishes anomalies from normalcy, typically using multiple instance learning[35]objectives. (2) Recent VAD methods adopt VLMs due to their remarkable success across core vision tasks[31,23,13]. Early research[58,55,49,29]has leveraged VLMs to generate textual descriptions of detected anomalies to enhance prediction explainability for VAD. However, current approaches incur high processing demands from external LLMs or require substantial effort and cost for fine-tuning on additional datasets, which are computationally inefficient in training or inference. Our work reduces the processing overhead by adapting frozen VLMs for VAD without model parameter modification or extra reasoning modules via learnable guiding questions, which elicit superior reasoning from frozen VLMs and significantly boost their performance in VAD.

Verbalized Learning for VLMs. The designed verbalized learning framework is inspired by a recent technique called verbalized machine learning (VML)[47]. The main idea of VML is to use LLMs to approximate functions and learn the verbal rules and descriptions of performing specific tasks, which casts traditional machine learning tasks such as regression and classification as language-based learning tasks. This approach regards the language expressions that define classification rules and other task-specific criteria as learned parameters, and optimize them in a data-driven fashion through interactions between a learner and an optimizer modeled by LLMs or VLMs. However, the VML framework is limited to tasks involving regression on scalar values or classification for static images. Later, another concurrent method, TextGrad[52], is proposed under a similar idea, which integrates the process of incorporating textual feedback from LLMs for improving prompts in PyTorch and further proves its effectiveness in coding, question answering, and optimization in chemistry and medicine.
Compared to existing works, our work pioneers verbalized learning for the VAD task and video data, which remains unsolved for previous verbalized learning frameworks focus on tasks with static input data and cannot handle the challenges of temporality and scene dynamics in the input for a complex visual reasoning task like VAD. Specifically,\nameintroduces a new learning paradigm for VAD: generating effective questions that encapsulate key abnormal patterns in videos to elicit the reasoning ability from VLMs for explainable VAD. Additionally,\nameworks for any VAD dataset and supports WS learning. Unlike previous WS methods,\nameonly needs to learn concise text but not millions of parameters, so the training is lightweight.

SECTION: 3The VERA Framework

Our approach adapts VLMs to detect video anomalies without additional reasoning modules or instruction tuning. We now formulate the VAD task and detail the design of\name.

Video Anomaly Detection. Letbe a video withframes, represented as, whereis the-th frame. Our objective is to locate and detect the start and end of anomalous events within. In standard labeling, any frame associated with an anomaly is labeled as 1, and normal frames are labeled as 0. Therefore, the ground truth label sequence foris, whererepresents the fine-grained label for. We aim to use a frozen VLM,, to generate anomaly score predictions across all frames,, whereis a continuous anomaly score for.

Available Training Data for VAD. Typically, VAD datasets only provide coarsely labeled training sets[35,45,25,28]. We denote a VAD training set as, whereis the total number of training videos,represents the-th videoandis the corresponding video-level label.ifcontains any anomaly defined by the dataset annotators,e.g., abuse or arson activities, andifhas no anomalies. For, we suppose it containsframes and denote the frames sequence as, whereis the-th frame () in.

Training Objective. We aim to learn guiding questions that break down a complex and ambiguous concept (i.e., what is an “anomaly”) into a set of identifiable anomalous patterns to unlock reasoning capabilities within frozen VLMs for VAD tasks. Those patterns vary among datasets, making manually designed descriptions ineffective for generalization. To address this, we propose a general verbalized learning framework shown in Fig.2to generate the desired guiding questions. We denote the guiding question set as, whereis the-th question () andis the number of questions. The training framework considersas thelearnable parameters, which are optimized through verbal interaction between a learner and an optimizer, modeled by VLMs through leveraging their ability to follow instructions with given prompts.

Training Data. The training data for learningconsist of paired sampled video frames and video-level labels. Sampling is necessary because the amount of video frames is so huge that we cannot compute with every frame. We explore three types of sampling strategies and find that uniform sampling[57]yields the best results. We will use it for illustration here, and please refer to the experiment section for details on other sampling methods. To illustrate, with any video, we first calculate the interval between sampled frames as, whereis the number of sampled frames, andfloordenotes rounding down to the nearest integer. Given, the uniformly sampled frames fromare represented by. The label used for training isonly, resulting in training data pairsfor\name.

Updatingvia Learner and Optimizer. Sinceare verbal expressions for specific anomaly patterns,\nameinherits the idea of VML[47]in training: optimizing language-based parameters by verbal communication between a learner agentand an optimizer agent, rather than by numerical optimization algorithms like Adam[18]. We take an arbitrary iterationfor illustration in this section. Please refer to Algorithm1in Sec.Afor the complete iterative training in\name.

Learner and Optimizer. We denote any LLM-based model aswhererepresents the input data, anddenotes the natural language instructions forto follow, which is considered as learnable parameters in our verbalized learning framework. Specifically,contains parameters to be learned in\name. As depicted in Fig.2, in each iteration, the learner agentis modeled by the frozen VLMused for VAD with a specific prompt templatethat guideto conduct a learning task by pondering on current guiding questions. We denote the learner agent as, whereis the input in a learning task, and, the learnable guiding questions applied in each iteration, constitutes the core parameters that distinguish the learner between iterations. Meanwhile, we introduce an optimizerto assess the quality of the predictions of the learner and to optimize. W.l.o.g., we use the same frozen VLMto model the optimizer. As demonstrated in Fig.2, we provide another specific prompt templatefor the learner to follow to optimize, so we denote the optimizer agent as, whereis its input andis the instruction to improve.
It is important to note thatbecausefollowsto conduct a learning task, whilefollowsto refine.

Learning Task for. The learner executes the “forward pass” and outputs a prediction. Recall that we only use the original coarsely labeled information for training. Thus, we design a binary classification task for, which accounts for the temporal nature of video data, the sparsity of anomalies, and the weak supervision in VAD datasets. In this task, the job of the learneris to produce a binary classification predictionto determine whether there is an anomaly in the video based on the sampled frames. As shown in Fig.2, we explain the task in natural language in the “Model Description” section in. Guiding questionsare inserted in the “Prompt Questions” section into elicit reasoning of the VLM. This template design is based on the prompt structures used in VML, with targeted modifications to help the learner effectively address this WS learning task. Due to the space limit, please refer to the Appendix for detailed information on. Givenand a sampled frame set, the learner will output a prediction as

whereif the learner thinks there is an anomaly after skimming across the sampled framesand reasoning through the guiding questions, and otherwise,.

Optimization Step in. The optimizer executes the “backward pass” to update the questionsvia a mini-batch (batch size is). Suppose the visual input in a batch isand the corresponding ground truths are. The learner generates prediction aswith the current questionsby Eq. (1). The optimizer will output a new set of questionsby following the promptwith batched data. We denote the optimization step as

whereis a new set of guiding questions constructed fromowing to its text generation and instruction following abilities after reading. Due to space constraints, please refer to the Appendix for information about. As shown in Algorithm1in the Appendix, we will repeat Eq. (1) and Eq. (2) foriterations to optimize. We denote the one with the largest validation accuracy as.

Given,\nameyields fine-grained anomaly scorefor a test videovia a coarse-to-fine process shown in Fig.3.

Step 1: Initial Anomaly Scores via Learned Guiding Questions. We divide the video into segments and analyze each segment independently first. Following[55], we perform equidistant frame sampling withinto obtain the set of each segment center, resulting in, whereis the interval between centers andis the total number of segments. For each center frame(), we define a 10-second window around it as the-th segment, within which we uniformly sample 8 frames. We denote the sampled frame set in the-th segment as. Next, we inputinwith the promptto get the initial score

whereifthinks the segment contains an anomaly after reasoning viawith, and otherwise,. By repeating Eq. (3) for each segment, we have a segment-level initial anomaly score set.

Step 2: Ensemble Segment-Level Anomaly Scores with Scene Context. Note that the scores derived above only examine a short moment in a long video without considering any context. To resolve it, we refine the initial segment-level score by incorporating scene context—defined as preceding and following segments that contain similar elements, such as actors and background, to those in the current segment.

We measure the relevance between different video segments by the cosine similarity of their feature representations[24], extracted by a pretrained vision feature extractor,e.g., ImageBind[11]. For the-th segment, its similarity with any segment(is, wheredenotes the cosine function, andandrepresent their features. Letdenote the indices of the top-segments similar to. We refine the anomaly score by

whereis an ensemble of initial scores of top-video segments relevant to. Here, the initial score of each retrieved segment is weighted by a factor derived from the cosine similarity and normalized by the Softmax function (withas the temperature hyperparameter). Accordingly, scenes with greater similarity are assigned higher weights, making the ensemble score a more comprehensive reflection of anomalies with the video context. By applying Eq. (4) for all segments, we obtain.

Step 3: Frame-level Anomaly Scoring with Temporal Context. Given, we aim to incorporate temporal context to capture how events evolve over time when computing frame-level anomaly scores, for the abnormality of an event often depends on the timing and progression of observed activities. To detail, we first apply Gaussian smoothing[12]to aggregate local temporal context into the segment-level anomaly scores. We denote the Gaussian kernel (suppose the filter size is) aswhereis the distance from the kernel center andis the variance. We update segment-level scores as, whereis the convolution operation. Next, we integrate global temporal context by position weighting. With, we flatten it into frame-level scores by assigning the scoreto each frame in the-th segment,i.e.,. We denote the frame-level score sequence after flattening as. We then apply the Gaussian function to encode position weights as, whereis any frame index,is the center frame index, andis the variance. The anomaly score for the-th frame is:

This operation scales the score, diminishing the anomaly score for frames near the beginning and end of the event. This helps better capture the temporal progression of anomalies: the score gradually increases as the anomaly reaches its peak and decreases afterward. The final scores is denoted asafter applying Eq. (5).

Explainable VAD by VERA. When using templateembedded withto compute, we ask the VLM to “provide an explanation in one sentence” when reasoning, and VLM will explain the anomaly score it assigns afterward based on. Please refer to Sec.4.4and Sec.B.4in the Appendix for the demonstration of explainable VAD by\name.

SECTION: 4Experiments and Results

In this section, we present an evaluation of VERA as follows, addressing key questions of interest including: (Q1) Does it enhance the effectiveness of frozen VLMs in VAD? (Q2) Is its design reasonable and well-structured? (Q3) How well does it generalize across different scenarios?

Datasets. We conduct experiments on two large-scale VAD datasets: (1) UCF-Crime[35]and (2) XD-Violence[45]. The details are as follows:

UCF-Crimedataset is collected from real-world surveillance videos (128-hour long in total), covering crime-related anomalies including abuse, arrest, arson, assault, burglary, explosion, fighting, road accident, robbery, shoplifting, shooting, stealing, and vandalism. The training set has 1610 videos (810 abnormal ones and 800 normal ones), while the test set has 290 videos (140 abnormal ones and 150 normal ones). The total number of test frames is over 1 million (1,111,808), and abnormal frames account for 7.92%. The average duration of a test video is 2.13 minutes, which is relatively long compared to common video datasets and serves as a benchmark.

XD-Violenceis another representative large-scale (217-hour long in total) VAD dataset with 6 anomaly categories,i.e., abuse, car accident, explosion, fighting, riot, and shooting, which defines anomalous events as the ones related to violence. This dataset is collected from movies and YouTube videos. It has 3954 training videos and 800 test videos (500 abnormal ones and 300 normal ones). The total number of test frames is over 2 million (2,335,801), and abnormal frames account for 23.07%. The average duration of a test video is 1.62 minutes.

Metrics. Following approaches in[55,58], we evaluate VAD performance using the Area Under the Curve (AUC) of the frame-level Receiver Operating Characteristic (ROC) curve, as it provides a comprehensive measure of model performance across all thresholds. It is a comprehensive representation for evaluating the ability of a method to distinguish between anomaly and normality across different thresholds in VAD. As for average precision (AP), the area under the frame-level precision-recall curve, it is another VAD performance metric mostly used for the XD-Violence dataset. Compared to AUC, this metric mainly focuses on the performance of VAD method in identifying anomalous events. In other words, AP pays attention to classifying the anomaly correctly rather than the overall separation. We report AP results for XD-Violence in the Appendix.

Baselines. We categorize baselines into non-explainable approaches and explainable ones as[58]does. Non-explainable ones are obtained by WS learning[45,46,44,38,21,7,17,35,54,60,10,53,19]and unsupervised learning[37,40,36,41,14,28]. These non-explainable approaches cannot provide language-based explanations for VAD and have following characteristics:

WS learning methods[45,46,44,38,21,7,17,35,54,60,10,53,19]usually use task-specific learning models with pretrained weights such as C3D[39], I3D[5], VideoSwin[27], ResNet[15], and ResNext[48]to extract feature for each video segment. Based on that, they form the training of classifiers, which output predictions after the feature extractors, as a multiple instance learning task, regarding the segments containing anomaly scenes as positive bags and the others as negative bags to handle the lack of frame-level annotations and the uncertainty of the anomaly locations in the video. Such learning objectives can fully use the only available video-level label information and effectively improve the discriminative ability of the classifiers in the network. However, the trained neural networks from these methods operate on highly abstract features that are hard for humans to interpret.

Unsupervised learning methods[37,40,36,41,14,28]improve the discriminative ability of the models regarding anomalies and normality without any knowledge of the video label. Note that we include one-class learning[14,28,41,40]methods in this category. Unsupervised methods mostly learn reconstruction models from unlabeled data and use reconstruction errors to distinguish normal and abnormal video frames. Another common strategy[37,36]is introducing pseudo-labels for unlabeled data and using this information to train discriminative models for VAD. Still, these methods cannot produce explainable results for VAD due to the structure gap.

For explainable approaches, we use LAVAD[55], Holmes-VAD[58], and VADor[29]as representatives of Pipeline 1 and Pipeline 2 shown in Fig.1. It should be noted that[49]does not report performance on UCF-Crime and XD-Violence. Additionally, we include zero-shot (ZS) VAD by frozen VLMs designed by[55]as baselines.

Implementation of\name. In our experiments, we choose a small VLM, InternVL2-8B[8], as the backbonefor building\nameby default, if not otherwise specified. With this choice, we implement\nameon an NVIDIA RTX A6000 GPU. We also explore other backbones, such as Qwen2-VL-7B[43]and larger model variants of InternVL2[8]for ablation. In principle,\nameworks well with different backbones. We trainfor no more than 10 epochs, with a validation accuracy calculated every 100 iterations to determine the optimal. The usedis given in Fig.5. We setas 2,as 8, andas 5 for training and include the discussion in Sec.4.3. Refer to the Appendix for more details on the hyperparameters in inference.

We address Q1 by empirically comparing\nameto existing VAD methods. First, in Table2,\nameachieves the highest AUC among explainable VAD methods on UCF-Crime, outperforming Holmes-VAD and VADor (without instruction tuning, as reported in their papers) in a fair comparison. Importantly, unlike these methods,\namedoes not need to modify the model parameters, demonstrating its suitability to directly adapt VLM to the VAD task with minimal training requirements. Moreover,\namesurpasses LAVAD byin AUC on UCF-Crime, uniquely integrating both description and reasoning capabilities in VAD. Compared to non-explainable methods,\nameachieves AUC performance that is comparable to one of the top-performing methods, CLIP-TSA, on UCF-Crime, while offering the additional advantage of explainable predictions.

Similar advantages are also observed in Table3for XD-Violence. Considering multiple factors, including performance, training efficiency, system integration, and explainability,\namestands out as a promising pipeline for VLMs in VAD.

We perform necessary ablation studies on UCF-Crime to answer both Q2 and Q3 for a comprehensive evaluation.

Training Frame Sampling Strategy. We compare three frame sampling strategies for obtaining eachin training: uniform sampling, random sampling, and TSN sampling (random sampling from equally divided segments). Table4shows that uniform sampling performs the best (with batch size2 and8). This is because uniform sampling preserves the temporal structure and maintains consistent motion patterns throughout the long video, making it easier for VLMs to understand the video and update.

Batch Size and Sampled Frame Number. Key hyperparameters that need to be set in training are the batch sizeand the number of sampled framesfor each videoin the verbalized learning framework. The selection ofandare correlated because they determine the total number of frames for the optimizer to skim and provide feedback as. In implementation, we will face memory constraints when implementing VLMs on GPUs. In our training, we find in the general caseused for training can handle at most 16 frames when we implement\nameon an NVIDIA RTX A6000 GPU, so we setin training. We further explore the trade-off betweenandgiven the constraints for input frames to decideand.

The results are shown in Table5. If the batch sizeis 1 with16, the learned questions cannot be generalized due to the limited video sample in the batch which leads to a suboptimal AUC, and it takes longer to train for\name. Meanwhile, if we setas large numbers like 4 or 8 (with4 or2), the learned questions are suboptimal too because relatively few sampled frames generally lack the temporality for the optimizer to look into the details and conceive good questions. Thus, settingto 2 andto 8 is in default in this paper, which strikes the balance between training efficiency and effectiveness.

How to Obtain Guiding Questionsfor VLM. As seen in Table6, if the guiding questions are not incorporated into the VLM prompt, the AUC will drop largely to 78.81%, confirming the need to use simpler and more focused questions to provoke reasoning in the VLMs for VAD. Meanwhile, if we use manually written questions (detailed in the Appendix), the performance is suboptimal with an 81.15% AUC, which shows the need to use verbalized learning to find guiding questions. Lastly, if we only input batched predictionsand ground truthswithout inputtingin the optimizer, theupdated in this way will dumb the VLMs and make it have a low AUC. Thus, inputting video frames as Eq. (2) does is necessary to learn good.

Number of Questions.
As shown in Fig.4, whenis set to 1, the reasoning is limited to a single perspective, resulting in a lower AUC. Asincreases up to 5, the model captures more comprehensive anomaly patterns, leading to improved AUC. However, increasingbeyond 5 yields no significant gains. Therefore, we setto 5 by default in\name, if not otherwise specified.

Coarse-to-Fine Anomaly Score Computation. We also validate the anomaly score computation by\name. Table7shows the AUC is 76.10% when using the flattened initial score obtained in Step 1, and leveraging retrieved segments in Step 2 significantly boosts the AUC to 84.53%, highlighting the effectiveness of incorporating ensemble scores based on scene context. Meanwhile, smoothing and weighting in Step 3 further improves the AUC by around 1% each, verifying the benefit of integrating temporal context.

Generalizability Test. We further examine the generalizability of\nameacross different model sizes, VLM architectures, and datasets to address Q3.

First, we apply\nameto InternVL2-40B, a larger model in the InternVL2 family compared to InternVL2-8B. As shown in Table10, InternVL2-40B achieves effective AUC performance, slightly exceeding that of InternVL2-8B, indicating that verbalized learning in\nameenables models of various scales to identify asuitable for their reasoning capabilities. Additionally, We also evaluate the transferability ofacross different scales and and observe an interesting phenomenon: thelearned by InternVL2-8B remains effective for InternVL2-40B, but not vice versa. This is likely because thelearned by the smaller model is readily interpretable by the larger model, whereas thederived from the larger model is more complex in syntactic structure and does not align well with the reasoning framework of the smaller model. Secondly, we select a different VLM, Qwen2-VL-7B[43], as the backbone for\name. As shown in Table10, while the AUC achieved with Qwen2-VL-7B is lower than that with InternVL2-8B, the verbalized learning in\nameremains effective, allowing it to outperform notable baselines such as LAVAD[55]. However, a notable gap exists when transferringacross different model architectures in Table10. Developing a universalthat can effectively elicit reasoning capabilities across various VLM structures would be an promising direction for future research. Lastly, we observe that the transferability ofdepends on the training dataset. From Table10, we observe that transferringlearned from UCF-Crime to XD-Violence results in a smaller performance drop compared to the reverse case. This suggests the source dataset is crucial to the transferability ofacross datasets.

To illustrate how\nameperforms video anomaly detection, we take one video for a qualitative demonstration of the explainability brought by the learned, as shown in Fig.5. Please refer to the Appendix for more qualitative examples if interested. The main anomaly in this video is that a man tries to steal money from the washing machines in a laundromat and is arrested after being found by the police. In Fig.5, we provide the guiding questions learned by\nameand take 6 main video segments (each with 2 sampled frames and their time indices are given in Fig.6) as examples to demonstrate the explanation produced by\name. From every given answer, the frozen VLM with\name-learned questions is able to explain the scene by closely following the detailed anomaly characterization of the five learned guiding questions. For example, in the second scene, one question instates that “Are there any people in the video who are not in their typical positions or engaging in activities that are not consistent with their usual behavior”, and it successfully triggers the reasoning abilities from the frozen VLM. The VLM then accurately describes the abnormal event and explains why it is regarded as an anomaly under the cue from the question.

Moreover, owing to the proposed coarse-to-fine detection strategy in testing, the anomaly score dynamics shown in Fig.6well represents the actual real-time anomaly level in this video and gradually increases to nearly 1 when the man is being arrested. This result verifies that\nameallows VLMs to effectively identify anomalies with a holistic model, reducing the manpower and computational overhead for explainable VAD.

More interestingly, we want to highlight one more advantage of\name. That is,\nameallows humans to further interact with VLMs because it retains the general question-answering ability of pretrained VLMs. This is because\namedoes not require finetuning of the VLM backbone weights. Although finetuning VLMs with parameter-efficient methods like[16,32,26]is easy and computationally tractable, instruction-tuned models still inevitably lose the flexibility to handle general questions (due to catastrophic forgetting), as they are trained to respond to certain queries with fixed answer styles. In contrast, as shown in Fig.7, the learnedcan steer reasoning in a frozen VLM while still being able to allow the VLM to answer open-ended (like follow-up or counterfactual) questions, which is an important ability lost in instruction tuning-based models.

SECTION: 5Concluding Remarks and Limitations

We propose a novel pipeline,\name, which can effectively elicit the reasoning ability from VLMs to perform explainable VAD without additional computation overhead. This is done through an effective and novel application of verbalized machine learning[47]to VLM. In training,\nameobtains the guiding questions detailing anomaly patterns through the verbal interaction between the learner and the optimizer agents. In inference,\nameuses them to enhance VLMs for identifying anomalies and compute frame-level anomaly scores in a coarse-to-fine process. Experimental results validate the effectiveness of the\nameframework in achieving state-of-the-art explainable VAD performance.

Like existing VLM-based VAD methods,\name’s performance relies heavily on the visual perception capabilities of VLMs. Most VLMs employ the CLIP vision encoder[33], which has limitations in capturing fine-grained visual details. This limitation can impair precise anomaly detection. If important visual features are missing during the visual encoding process, then it is unlikely for\nameto perform meaningful verbalized learning.
Therefore, a fundamental challenge for VLM-based VAD is to ensure sufficient visual and temporal features are encoded. Having verified this capability,\namecan perform verbalized learning to extract crucial cues that guide video anomaly reasoning.

SECTION: References

SECTION: Appendix

We include more details on training in\name(Sec.A) and additional experimental results (Sec.B). To specify:

In Sec.A, we provide the pseudocodes and details on the initialization, the learner prompt template, and the optimizer prompt template for the training process in Sec.A.1. After that, we discuss the optimization process of the learned questions by the optimizer in Sec.A.2.

In Sec.B, we first include comparison results with the state-of-the-art methods on XD-Violence measured by AP in Sec.B.1. We also discuss other good properties of\name, including the good generalizability of the learned questions for different scenarios and the insensitivity of\nameregarding hyperparameters in Sec.B.2and Sec.B.3, respectively. Finally, we include additional case studies with normal and abnormal videos in Sec.B.4.

SECTION: Appendix ATraining in VERA

We show the complete iterative training process of\namein pseudocodes in Algorithm1. It is an iterative process of using the learner to output binary prediction for each sample in a mini-batch and asking the optimizer to update the guiding questions after collecting the batched data. Meanwhile, we have a small validation set (10% samples randomly drawn from the original training set) for deciding theused for testing. We want to further detail on certain elements in Algorithm1as follows.

Initial. The initial guiding questionsare “1. Is there any suspicious person or object that looks unusual in this scene? 2. Is there any behavior that looks unusual in this scene?”. These two questions are manually written and inspired by previous VAD methods, which assume anomaly as something or somebody with unusual appearance or motions[46,14]. This set of questions is also the “manually written questions by human” in Table6, which is suboptimal in guiding frozen VLMs to detect anomalies. The key idea of training is to use verbalized learning to iteratively updategiven a suboptimal.

Learner Prompt Template.
We detail the design ofas follows. As shown in Fig.2, the learner prompt templateincludes four sections,i.e., Model Description, Prompt Questions, Input, and Output Formatting. To specify:

Model Description: This section introduces the learning task, providing the learner with the necessary background knowledge to understand the objective. It clarifies what the learner is expected to predict based on the given visual input data.

Prompt Questions: This section presents a general prompt to guide the learner’s reasoning process. Specific prompts, denoted as, will be inserted here to facilitate reasoning within a frozen VLM.

Input: This section simply stores the visual tokens. When the VLM reads this, it will correlate the read text with the visual inputs.

Output Formatting: The last section inmainly provides information on output formats to ensure that VLMs think through the given questionsand output a prediction in a format easy for post-processing in computers.

Optimizer Prompt Template. As shown in Fig.2, the optimizer prompt template includes seven sections,i.e., Instruction, Inputs, Model Description, Current Prompt Questions, Model Predictions & Targets, and Optimization Instruction:

Instruction: The prompt template begins with an introduction outlining the responsibilities of the optimizer, clearly stating that its primary task is to optimize the guiding questions provided.

Inputs: This section is used to attach the batched visual data for the reference of the optimizer.

Model Description: The learning task of the learner is reiterated here for the information of the optimizer.

Current Prompt Questions: The guiding questions used by the learner in the current iteration are shown here for the reference of the optimizer.

Model Predictions & Targets: The batched numerical predictions and the ground truths are shown here for. These two inputs can tell the optimizer how well the learner does in the learning task on the mini-batch data.

Optimization Instruction: The final section includes the instruction to ask the optimizer to think step by step with all the information above and output a new set of prompt questions with the required format.

In training, we assess the quality of the learned guiding questions by the accuracy of the validation set. We show the validation accuracy from different questionsobtained every 100 iterations (mini-batches) in Fig.8. In the duration of up to 5000 iterations in training, the observed plot in Fig.8contains three oscillations, each consisting of an increase in validation accuracy followed by a decrease. The increase represents that the optimizer VLM gradually finds better questions for the binary classification learning task when it sees more batched data, which shows the optimizer can understand its responsibility well and find better questions effectively. Meanwhile, we note that verbal optimization may not always lead to an increase. This is probably because the optimization is completely verbalized, and the VLM will have an inertial thinking behavior like humans, which gets the optimizer stuck in the wrong direction and makes it continue the optimization in a direction that is not beneficial. As a result, this causes the validation accuracy to decrease sometimes. Despite that, because of the guidance provided by the optimizer prompt template, the optimizer can overcome its pitfalls in thinking and find good guiding questions in a new direction, which leads to an increase in validation accuracy afterward. This is an interesting phenomenon due to the distinction between verbal learning and traditional numerical optimization algorithms, and it will be a promising future direction to reduce the time in overcoming pitfalls in thinking for VLMs during verbalized learning.

In addition, w.l.og., we take learned questions from the 100th iteration to the 700th iteration (which are within the first epoch) for illustration to show the process of updatingby the optimizer in Fig.9. First, as the optimizer sees more videos, it tries to make the questions focus on a more general setting. For example, the questions in the 100th iteration focus on “street” and “store” scenes. After more iterations, the questions become more generalizable for a general environment and focus on the elements that cause anomalies. Additionally, the anomalous pattern descriptions become more diverse as the optimization continues. To illustrate, in the beginning, the questions mostly pay attention to the humans, objects, and their interaction. In later iterations, the optimizers gradually summarize some previous questions into one and raise questions considering the overall environment (Q5 from the 700th iteration). Therefore, the verbalized learning framework proposed in this paper is effective in finding a diverse set of guiding questions for VAD that apply to general cases, which can elicit the reasoning of a frozen VLM in VAD.

SECTION: Appendix BAdditional Experiments and Results

The comparison results regrading average precision (AP),i.e., the area under the frame-level precision-recall curve, on XD-Violence are shown in Table11. Compared to AUC, AP focuses on measuring the ability to identify the positive class (anomaly), while AUC measures how well a method separates anomaly and normalcy in general. We provide the analysis of the results as follows.

Firstly, under such a distinct property of AP, as pointed out by[46], methods trained on the whole training set and utilizing all frames will enjoy advantages when measuring VAD performance by AP. As a result, CLIP-TSA and Holmes-VAD, two methods using the whole training frames, attain the highest AP in the category of non-explainable and explainable VAD, respectively. We acknowledge there is a gap between\nameand these two methods under AP on XD-Violence, which is understandable because they use the whole training frames to improve the ability to find anomalies of classifiers. To illustrate, in training\nameonly samples 8 frames for each video and only uses 0.19% total frames (31,632 out of 16,378,527) for training on XD-Violence. Thus, our training is dramatically light compared to the methods like CLIP-TSA and Holmes-VAD in Table11. With fewer frames used for training,\nameunavoidably achieve lower AP (which only considers positive cases) compared to those that have more, for it relies on fewer training data. In addition, we want to point out that judging the VAD performance solely by AP on XD-Violence can be biased. This is because the ratio of positive frames in XD-Violence (23.07%) in test videos is overly higher than other datasets like UCF-Crime (7.92%), which is unrealistic because the anomaly is sparse in the real world[35]. Given that, only focusing on the comparison in AP on XD-Violence would amplify the bias in VAD performance evaluation, and we recommend taking into consideration other factors like training costs and the comprehensive ability of distinguishing anomaly and normality by the methods in evaluation.

Secondly, among the methods (OVVAD, LAVAD, ZS CLIP, ZS IMAGEBIND, and LLAVA-1.5) that does not use full frames for training,\nameachieves the best AP in this fair comparison, surpassing the second best method in the Explainable VAD category (LAVAD) over 8.53%, which showcases the effectiveness of using learned guiding question to prompt frozen VLMs for VAD.

To conclude, it is unfair to only judge VAD performance by AP on XD-Violence without considering the training costs and the relatively imbalanced frame distribution in test videos. Considering all factors into consideration,\nameis a favorable method used for VAD in detecting anomalies.

During the optimization of, because of the randomness involved in this process, the optimizer may output certain guiding questions that only focus on one specific surrounding. We find an interesting phenomenon on VLMs in VAD that guiding questions related to a specific scenario yield inferior VAD performance compared to the general questions in both general cases and specific cases.

To illustrate, we take two sets of specific questions for analysis. The first example is a set of guiding questionsthat only ask the VLM to consider anomalies related to the traffic as follows:

Are there any vehicles or people violating traffic rules?

Are there any accidents or near-accidents occurring?

Are there any objects or people obstructing the normal flow of traffic?

Are there any unusual or unexpected behaviors from pedestrians or drivers?

Are there any emergency vehicles or personnel present?

The second example is another set of guiding questionsthat only ask the VLM to identify anomalies in a store setting, which includes questions like:

Are there any individuals loitering or behaving suspiciously inside the store?

Is there any unusual activity inside the store, such as tampering with items or attempting to enter restricted areas?

Are there any signs of forced entry or damage to the store’s entrance?

Are there any individuals present who seem to be watching or waiting for something specific inside the store?

Are there any interactions between individuals inside the store that appear suspicious or out of the ordinary?

Thus,andfocuses on the specific anomalies of traffic accidents and shoplifting, respectively, while thethat we find focuses on general cases and includes the following questions:

Are there any people in the video who are not in their typical positions or engaging in activities that are not consistent with their usual behavior?

Are there any vehicles in the video that are not in their typical positions or being used in a way that is not consistent with their usual function?

Are there any objects in the video that are not in their typical positions or being used in a way that is not consistent with their usual function?

Is there any visible damage or unusual movement in the video that indicates an anomaly?

Are there any unusual sounds or noises in the video that suggest an anomaly?

The comparison results of,, andin detecting anomalies in general cases (all testing videos on UCF-Crime), traffic scenes (testing videos from the Traffic Accident category on UCF-Crime), and the store scenes (testing videos from the Shoplifting category on UCF-Crime) are shown in Table12. It indicates thatperforms the best in both general cases and two specific cases like in traffic and store scenes. This is because the overly specific definition of anomalies likeandmakes it harder for a VLM to classify one clip into an anomaly and leads to more false negatives in its prediction given those specific questions, which degrades the performance. Therefore, we recommend using general questions like the ones shown inin frozen VLMs for VAD.

Hyperparameters in InferenceDuring inference, in Step 1, following[55], the interval between each segment centeris 16 frames. In Step 2, we use ImageBind[11]as the feature extractor in computing segment similarity as[55]does, and the number of retrieved segmentsdepends on the total number of segmentsin each test video. Settingtotois generally good. We settofor UCF-Crime and tofor XD-Violence. The temperaturein the Softmax function is set to 10 for both datasets in Eq. (4). In Step 3, due to the properties of datasets, we set the filter sizeofto 15 andto 10 for UCF-Crime, while settingto 30 andto 30 for XD-Violence. For position weighting, we setandfor both datasets to make sure the position weight covers the whole video sequence.

W.l.o.g, we test the sensitivity of the VAD performance of\nameregarding hyperparameters on UCF-Crime.

Sensitivity Test for. As shown in Table13, as the number of retrieved segments increases from 0 to, the AUC gradually increases from to 85.21% to 86.61%. Meanwhile, if we randomly selectsegments for retrieval, the AUC is even lower than the performance without retrieval. Thus, using Eq. (4) for retrieval is necessary. Meanwhile, having a largegreater thanwill introduce some noise in Eq. (4) and downgrade the AUC slightly. Thus, selectingorforis generally good choice.

Sensitivity Test for. The filter size decides how many local segments are incorporated for the current segment for Gaussian smoothing. From Table14, we find that AUC converges when the filter size increases to 15. Meanwhile, the VAD performance measured AUC is insensitive toand does not fluctuate much. Thus, we can set the filter size with a medium number like 15.

Sensitivity Test for. The AUC performance is also robust on the choice of. As, shown in Fig.15, when we setgreater than 1, the AUC generally remains around 86.50%, which again shows the robustness of the design of anomaly scoring in\name. We can setas 10 for\name.

Sensitivity Test for. The temperature hyperparameterin Eq. (4) controls the entropy of the distribution obtained from the Softmax function while preserving the rank of each element. As demonstrated in Table16, whenis a small number like 10e-8 that is close to 0, the distributions tend to become a trivial distribution with all mass concentrated on the highest-probability class (corresponding to the segment itself), and the result is the same as the one by not using retrieval. As we gradually increaseto a reasonably large number (from 0.01 to 1), the AUC value converges around 86.55% with no obvious fluctuation, again proving the robustness of anomaly scoring in\nameregarding hyperparameter selection. Note that whenapproaches, the distribution tends to become a uniform distribution, which yields an AUC of 86.59%. From the discussion above, we can generally chooseto be an number in [0.01, 1] in implementation.

Sensitivity Test for. From Table17, we find that settingencodes the position information best in the anomaly score. A drop is noticeable if we chooseless thanfor it will not cover the whole sequence, which is reasonable, while choosing agreat thandoes not change much. Thus, based on the physical meaning of, which controls the width of the distribution, we should makeequal toin anomaly scoring.

W.l.o.g., we take one normal video (“Normal_Videos_018_x264”) and another abnormal video (“RoadAccidents127_x264”) from the UCF-Crime dataset to demonstrate the explanations provided by a frozen VLM (InternVL2-8B) achieved by using the learned guiding questions.

First, in Fig.10we showcase the explanation of anomaly scoring by\nameregarding a normal video “Normal_Videos_018_x264” in UCF-Crime, which is taken in an airport hallway where no anomaly happens. For this video,\nameassigns a 0 score to each frame. As shown in Fig.10, for the selected scenes in this video,\nameexplains that this is because there are no events that conform to the anomaly descriptions in. Such explanations are consistent with the recording and again manifest the effectiveness of eliciting the reasoning ability in a frozen VLM for VAD by using learned guiding questions. Note that we do not have an additional figure illustrating the anomaly score dynamic for this video because all scenes are assigned 0 scores by\name.

Next, we select 6 representative scenes in the abnormal video (“RoadAccidents127_x264”) and show the corresponding explanation provided by the frozen VLM in Fig.11. The main anomaly that happens in this video is a traffic accident where a truck crashes into a train from Frame 2160 to Frame 2299, which corresponds to the 5th scene in Fig.11. In particular, the figure shows that the learned question “Is there any visible damage or unusual movement in the video that indicates an anomaly?” inmakes the frozen VLM find a good way to express what it sees in the 5th scene and understand this is an anomaly because the crash is unusual and dangerous. The other scenes are also well explained by the frozen VLM under. Thus, this again verifies that the learned guiding questions can successfully trigger reasonable explanations in the adopted frozen VLM for VAD.

Meanwhile, we also include the anomaly scores generated by\namefor the abnormal video in Fig.12. Most frames are assigned to zero except the scenes when someone crosses the road at an unusual speed (the 2nd scene in Fig.11) and the truck-train crash happens (the 5th scene in Fig.11). This fluctuation is aligned with the ground truth annotation and common sense about an anomaly, which shows that the anomaly scoring proposed in\nameis reasonable.