SECTION: Order Theory in the Context of Machine Learning: an application
SECTION: A Novel CNN Filter Inspired by OrderTheory
SECTION: Introduction
The paperexplains an equivalence between integer-valued neural networks (IVNN) withactivation (where) and tropical rational functions, which come with a map of polytopes. This map associates a “simplified tropical polynomial” (sum of tropical monomials) with a convex polytope, sending, whereare vectors and the sum is finite, to the convex envelope of.

Order theoryis a discipline in pure mathematics that formalizes the study of structures with an order. A partially ordered set, also called a poset, is a set in which we can compare some elements, but perhaps not all of them.

Letbe a list of computer programs, where programneeds the output of program, programneeds the output of program, and programcan run in parallel to all. If we specify,and extend the relationby transitivity, we obtain the poset. Any linear order ofcompatible with the original order (for example) determines a way to run the programs one at a time.

Given a posetwithpoints, theofis the polytope contained in the unit cubedefined by the inequality of the poset assuming that every point of the poset corresponds to a basis element of.

Then, we see, for example, that, thatis the-simplexand that the polytope of the poset in Exampleis.

In this paper, we describe tropical polynomials associated with order polytopes. Expanding on the work of, we defineas neural networks whose tropical polynomial is associated with an order polytope.

Although poset neural networks are IVNN, when interpreting them as convolutional filtersthey can be included in arbitrary convolutional neural networks. As convolutional filters, the following are two examples of the functions that we discovered:

The first filter
sends thesquareto the following:

The second
filter sends the same input to:

We verified that these filters do not appear in the literature.

In the first part of this paper, we perform experiments to evaluate the performance obtained after adding these filters, and compare the results with max pooling, average poolingand mixed pooling.

The second part of this paper, of theoretical nature, describes the algebraic properties of these poset neural networks, introducing the formalism of algebras over the operad of posets. The main theoretical results consist of the proof that we can recover a poset from the vertices of an order polytope (Corollary), we find polynomials that distinguish posets (Lemma), and we introduce an action of the operad of posets on convex polytopes and tropical polynomials.

This paper is organized as follows. In Sectionwe introduce the basic definitions and some necessary theoretical results. Sectionis dedicated to the study of the newly defined convolutional filters. We summarize our experimental results in Section. Appendicesandcontain details about the experiments.
Sectionintroduces the language of operads. Sectionreviews the action of posets on polytopes. Sectiondescribes an action of posets in a family of neural networks, and Sectionexplains how to define the action of posets on tropical polynomials. Appendixstudies the corresponding geometric transformations associated with poset filters.

SECTION: Mathematical background
We review the basic notions of order theory.

A, or,is an ordered pairthat consists of a setand a partial order.

By-chain, we mean the poset.

Ais a graph associated with a poset, in which a vertex is connected vertically to each of the successors of that vertex.

Hasse diagrams encode all the information required to characterize a poset. Usually,is located at the bottom and the successors ofare drawn above it.

By the previous definitions,is the Hasse diagram of the posetandrepresents the poset.

Given a posetwithpoints, the order polytopeis defined as the object in thecubewhere each coordinate is assigned to a vertex of the poset and where we look for the spaces of points with the restrictions of the poset. Equivalently,

Following the definition,and.

It follows thatis the-simplex.

The order polytope of theposet, defined as, is the region that satisfies:

We assumehas vertices with labels (from left to right), and. There areways to give a linear order to theposet, compatible with the original order, as seen in Figure. Here, we use height to order the vertices. For instance, in the linearization at the very bottom of Figure, the order is. Each one of those linearizations determines a-simplex of the triangulation of the order polytope.

It is shown inthat two-simplices in an order polytope share a-face if the corresponding linearizations differ only on a pairand, with the face being the region where. In Figuretwo linearizations are connected if their corresponding simplices share a face.

The order polytope of a poset is a convex set (this follows from Lemma). The following lemma relates the study of posets with the study of convex polytopes in. In the proof of this lemma, we use the fact that any-simplex induces a linear order on, which is defined from the vertices of the simplex.

From the vertices of the-simplex, say, we can deduce the following order:. Note that the value of the second coordinate is always less than or equal to the value of the third coordinate.

Every simplex ofrepresents the order polytope of a linear order in the setthat indexes the orthogonal basis of. Let the orderbe the intersection of those linear orders. By definition,is contained in. It remains to show that.

We proceed to prove the lemma by contradiction. Among all the simplices in, letbe any sharing (at least) two-faces with the corresponding simplices in. Then by connecting the points on those two faces ofwith a line, and using the convexity of, we show thatintersectsat more points than the boundary, which is a contradiction.

Now, assume that all simplices ofshare one face with one of all the simplices in.
Pick one of such simplices, then for some pair of coordinates, the points in the simplexsatisfy, but any other element ofsatisfies. Thus, the simplexdoes not belong to(as it introduces a relationnot contained in), which is a contradiction.

Finally, it is not possible that every simplex ofonly intersectsin a face of dimension at most, as we explain below.

Take a point in the center of a simplexand a point in the center of a simplex in. The line between these points is included in. This line intersects the boundary of a simplex inand a simplex inand, by varying the point in a ball inside of, we find that the intersection is open and thus, a whole face is shared byand.

We conclude that.
∎

The proof of Lemmaand the results ofgive us a way to uniquely reconstruct an order out of the simplices of the order polytope.
∎

Consider the tropical semiring, whereand. Although some of the usual properties still hold, such as the following distributive property:

tropical elements behave in surprising ways. For instance,

Tropical fractions are defined by, whereare tropical elements, and usingand, one can show that for tropical numbers:

A tropical polynomial

corresponds to the function:

We say that a tropical polynomialis in itswhen there are no repeated (tropical) monomials in its expression.

Following, given a tropical polynomialin its reduced form, we define theby taking the convex envelope of the monomial vectors according to the following rule: given a tropical monomial, we associate the vector.

The operation is invertible, associating to a convex polytopewith positive integer coordinates (except perhaps the last one) its tropical polynomial.

By the above definition, we have the following:

Here the monomialis related to the point, and the monomialis related to the point.

Given a poset, itsis defined as the tropical sum of the tropical monomials defined by the vertices of.

Since the order polytope of a poset is a convex set (see Lemma), we can associate a tropical polynomial with each poset by using the vertices of its order polytope. However, we assume that there is an extra coordinate, set to, ensuring that the values represent only the exponents of the monomials and not the constants. In Example, the vectoris associated with the monomial, while the tropical polynomialwill be the tropical polynomial of(where).

Then, the general expression is of the form, wherebelongs to the set of vertices of the order polytope of.

The tropical polynomial of the linear order, orchain,is of the form:

Given theposet, we list the vertices of, where we assume a vector

Without repetition, we obtain the following vertices:

We say that the tropical polynomial of a posetwithis in itsif it is a tropical sum of tropical polynomials of-chains, where every tropical polynomial of an-chain appears only once.

The tropical polynomial in expanded presentation associated tois the following:

Note that, so the expression above could be simplified as in Table. We prefer to represent the tropical polynomials of posets as tropical sums of tropical polynomials of linear orders.

The tropical polynomials of posets are determined by the tropical polynomials of each linearization, which correspond to the tropical polynomials of the maximal simplices of the order polytope of the poset (given a polytope, we call maximal simplices those of highest dimension). The order polytope of a posethas as many maximal simplices as linearizations of the poset (see Lemma).

Letbe a tropical polynomial of a poset,
then it is possible to determine the tropical polynomials of the linearizations from the expression of, as follows: if each maximal simplex hasvertices, when transforming those vertices into monomials we obtain a sequence ofmonomials, each dividing the next one.

Then, from a tropical polynomial of a poset we can recover the linear orders of the poset and, since a poset is determined by the intersection of its linearizations, we also recover the original poset.
∎

Given a poset, there is a well-known polynomialcalled the Stanley order polynomial. When two posets have the same Stanley order polynomial, such as, they are called Doppelgänger.
There was a question about the existence of a set of polynomials that distinguishes posets, and this question is answered by Lemma.

Up to this point, we have linked each poset to its associated tropical polynomial. Next, we will describe the neural networks associated with tropical polynomials of posets.

First, we explore tropical operations at the level of neural networks.

In the context of IVNN, nodes in the same layer, say, can have a different parameterin their ReLU activation function, that is,

If the neural networkcorresponds to the tropical polynomialandcorresponds to, then from:

we define at the level of neural networks theas the following:

We define theof two neural networksandas the following operation:

We note that the tropical sum has the following properties:

This means that we have made choices, such as takinginstead of, to determine the factorization ofin terms of the usual ReLUs with parametersor.
Thus,

By computing the vector, we define theof two neural networksandas:

If we make the other choice and describe the tropical sum as:

then the tropical sumis as follows:

Sincecontains the origin,always has a zero, so the final layer is.

If a neural network is the result of a binary operation between two neural networks with a different number of layers, we can enforce the same number of layers by applyingand the multiplication by the identity matrix (either at the beginning, at the end, or in the middle) to the input that has fewer layers.

Now that we have defined the basic tropical operations on neural networks, we can move on to answering the question: what is the neural network associated to a simplex/-chain?

Letbe the-dimensional identity matrix with the last row repeated. Given a vector of dimension, consider, the ReLU activation function with a parameter vector containingcoordinates with value, and the last coordinate with value.

It follows from direct computation and Equation ().
∎

Row 2 and 3 of Tableshow the resulting neural networks.

A tropical polynomial can have several neural networks associated with them by Remarkand Remark. Poset tropical polynomials (in extended form) are the sum of tropical polynomials of linearizations, and all tropical polynomials of linearizations have the same number of monomials.

We choose a neural network representative of each tropical polynomial of a poset in a coherent way using Lemma, which describes the neural network associated to a linear order.

Anis a network that takes an input vectorand sends it toneural networks running in parallel, each of which returns a coordinate of thedimensional output of the inception module.

It is worth mentioning that the inception module is defined in an abstract manner, but a physical implementation requires ordering the linearizations of the poset/ neural networks that form the inception module.

Letbe a poset withlinear orders, then ais defined as the inception module formed by theneural networks associated with each linear order of.

The function that mapstois not a linear transformation, but an inception module allows us to take the inputand send copies of it, each of which can then be permuted in a different way.

An alternative to the inception module is to allow for a nonlinear function to take avector and return amatrix whose columns (in thedirection) are certain permutations of the input.
An advantage of this permutation function is that the architecture of the neural network of a poset will consist of first the permutation, followed by the operations of Lemmaconcatenatedtimes.

Since a poset specifies uniquely its linearizations, the neural network of a poset is well defined up to the order of the networks in the inception module/the output of the permutation function.

SECTION: Proposed Convolutional Filters
The max pooling operationreturns the maximum value in a certain region. It is well known that in this case there is a loss of information, specifically when there are several large values, as the pooling only selects one and ignores the other values. A second popular filter is the average pooling filter, which takes the average of inputs in a certain region. A drawback to this pooling filter is that if one value is large and the remaining ones are small, the output is close to the larger value divided by four, which in some situations may not be desirable. To address these and other issues, a third filter was suggested: mixed pooling, in which average pooling and max pooling are applied randomly.

Now, looking at the back propagation step for the average pooling filter, the gradient from the next layer is equally distributed among all entries entering the pooling layer (irrespective of whether they were zero or non-zero during the forward pass). In the case of the max pooling, if there are several entries that achieve the maximum or are close, max pooling passes the gradient to only one of them. Mixed pooling then either equally distributes the gradient or may not pass the gradient to relevant entries.

We propose a family of filters that are more precise during the back-propagation step.

For any posetwith four points, we write its tropical polynomial in simplified form (using). This expression represents a function which is a combination of the functionevaluated on a linear combination of variables with coefficients 0 or 1. We call this function theof.

For a given input, the corresponding filter of the disjoint union of four points, whose order polytope is the cube, computes all possible sums out of the four inputs, as seen in Equation ().

Let us have inputs of the form,,, and. During back propagation, the incoming vector will be distributed to the entries after multiplying the incoming vector by the entries values, a behavior that differs from that of the average pooling. Note also that if two entries of the input achieve the maximum value, both will pass the gradient.
Poset filters are similar to maxout, without the need to train additional parameters.

An immediate drawback of the filter associated with the disjoint union of points is the number of operations.
We consider then the filter associated to the four chain, whose order polytope is the four simplex:

and the filter of the poset, from Table:

Our experiments, detailed in Section, show that the simplex filter (or other-point posets) can be used instead of the filter of the disjoint union of points with a similar performance and fewer computations.

In Appendixwe downsize images with the poset filters, assuming pixel values to be between -1 and 1.

One may wonder why posets are related to convolutional filters.
According to Table, the tropical polynomial of a simplex consists of a tropical sum of monomials, where each monomial divides the subsequent one. This property, in the context of poset filters, means that each term of a filter (which is a linear combination of the inputs) considers one more input than the previous term. Due to the previous property, we decided to work with simplices. Finally, convexity of the union of simplices (sharing a line) is equivalent to working with order polytopes (see Lemma).

Poset filters are closely related to Maxout networks, but they differ in that poset filters have fixed weights. One can also wonder if we really need the vectors of posets or if we can use random vectors instead. In Tablewe show results of experiments when using random vectors and training a neural network with them. Although the average precision is lower than(our best result), the accuracy with random vectors is still higher than average pooling, mixed pooling, and max pooling.

SECTION: Review of experimental results
With respect to the CIFAR10 dataset, we have tested on a quaternion convolutional neural network. We aimed to conduct experiments by placing the poset filters in different locations within the convolutional part of the algorithm, and the quaternion architecture was convenient for this, as the inputs at different layers have dimensions divisible by. We worked on the architecture(called QCNN (Weeda impl.) on the results’ tables), due to it being an implementation on Pytorch that uses the correct weight initialization that appears on the paper.

In our experiments (Section), we have evaluated testing accuracy after placing the poset convolutional filter at several positions. Our best result (see Table) is obtained by placing theposet filter between the last two convolutions, just after a ReLU activation. We reduce the number of trainable parameters fromto, with a testing accuracy average of, compared to the original testing accuracy average of. The average is taken over 14 independent trainings. We also note that when reproducing the QCNN (Weeda impl.), we got an average ofunder the same conditions as in our test (mainly, the addition of reproducibility seeds). The performance of our poset filter is superior to the alternative architecture obtained by replacing it with max pooling (), average pooling () and mixed pooling ().
We report similar performance during cross-validation in Table.

During these experiments, we were concerned that the presence of a ReLU before the filters could affect the performance. However, we found that poset filters perform similarly whether they are preceded by a ReLU or not. We propose the following explanation. Negative values decrease the value of the average. When we evaluate the poset filter, it takes into account those terms, and, according to the poset, the exact linear combination that only adds positive values may not be part of the expression of the poset filter. By pre-filtering with a ReLU, the output value of the poset filter is bigger.

The poset disjoint union of points, with order polytope the “cube”, hasterms, making it slow. We run an experiment that evaluates allpossible posets with four points. The results of Tableshow that the accuracy does not differ too much, so the reader can choose to use, instead of the disjoint union poset, theposet or thechain poset filter, with less computational time.

With respect to the Fashion MNIST dataset, we have tested the DenseNet architecture, which, according to the official repository of Fashion MNIST, achieves a performance of. We implementedwith crop production being. In our implementation, the neural network achieves an average performance of. Unfortunately, the architecture is already optimized in the sense that the input to the feedforward section is abatchtensor, and adding a filter on the convolutional section would require major changes to the architecture.

We then replace the different instances of average pooling in the architecture by theposet filter. Our best result returned an average accuracy ofwith std.
See Sectionfor more details.

We also tested DenseNet with CIFAR10 and CIFAR100, but in all cases the accuracy did not improve significantly when substituting the existing pooling methods with theposet filter.

In Section, we tested on, a standard convolutional neural network with ELUnon-linearity and two max pooling functions. The original network achievesaccuracy. Our best result was aaccuracy, but among thetraining runs, the average accuracy was, obtained by the network that replaces the second max pooling by thefilter. See Table.

We showed that for certain datasets/architectures, poset filters outperform average pooling, max pooling, and mixed pooling. We will continue our experiments with other possible architectures and datasets.

Until now, thefilter has been defined as apooling function (that is, it takes inputs from the same channel), but in the future we will also conduct experiments in which we define our pooling function as a pooling function across channels.

SECTION: Operadic order theory and machine learning
In Definitionwe introduced a set of IVNN parameterized by posets. In this section, we lift the structure of an algebra over an operad of posets from order polytopes to poset neural networks. This implies that we have a set of neural networks indexed by arbitrary finite posets and that we have a family of associative operations on those neural networks indexed by posets.

SECTION: The operad of posets
We first study a family of poset endomorphisms, that is, functions of the type.

Letbe a poset withpoints. Theofposetsalongis defined as the poset, with pointsand

The disjoint union of posetsis the lexicogaphic sum along, and the ordinal sum, or concatenation,is the lexicographic sum along.

Ais generated by a pointunder the operations of disjoint unionand concatenation.
Under the operation, series parallel posets form a commutative monoid with the following cancelation property:implies.

Examples of series parallel posets are any chain, and any tree.

Given a poset that is not a linear order, we say that it isif the only lexicographic sum the poset admits are the trivial lexicographic sumsor. Afor a poset is a decomposition into lexicographic sums evaluated on indecomposable posets.

A factorization ofis.

SECTION: Endomorphisms of polytopes
Letbe posets. To define operations on the order polytopes, we work inand assume that eachis embedded as.

At the level of polytopes, thetakesandand returns:

Given two posetsand,is the poset with no new order relations.
Then,has as many orthogonal coordinates as the points inand, and the relations ofdo not interfere with those of. Thus, the points ofare of the formand, where the sum of polytopes is given by the Minkowski sum.

∎

Another operation on polytopes is the convex envelope.

Theofand, or, is defined as:

Consider the convex envelope ofand.
We claim that every point in the linesatisfies the inequalities of the poset and that any of the first coordinates is smaller than any of the second coordinates.

For any such, the coordinates ofandsatisfy the inequalities ofandrespectively, because we multiply every entry by a constant. Similarly,preserves the inequalities entry-wise after translation and dilation.

Now, we havefor allcorresponding indices. This means that the convex envelope ofandis the order polytope of.

∎

Anis a sequence of sets, conceptualized as a set of-ary operations. It is equipped with composition morphisms, which are associative.

Thehas as-ary operations the posets withpoints, with composition given by the lexicogaphic sum. The one-point poset1 acts as an identity operation.

Note thatrepresents an associative and commutative operation, since the union of posets satisfies.

Given a set, the operadhas as-ary operations the functions, and the operadic composition is the composition of functions.

Anover the operad of posetsis an operadic morphism.

Posets are an algebra over the operad of posets, where the action is the lexicographic sum of posets.

The language of operads provides a framework that enables the application of techniques from order theory in other fields. We will now define the structure of algebra over the operad of posets on order polynomials, tropical polynomials, and a family of neural networks.

Given a finite posetwith, and input posets, we define theof the input posets by:

Corollaryimplies that the structure of an algebra over the operad of posets of order polytopes is well defined because every poset is associated to a unique order polytope.
This way, we see posets parametrizing operations on order polytopes.

The Stanley order polynomials are an example of an algebra over the operad of posets where the assignmentis not injective.

We have seen thatacts as the Minkowski sum of order polytopes (see Lemma), andis isomorphic to the convex envelope of the order polytopes (see Lemma).

The first non-series parallel poset is. Thus, the action ofis a generalization of the Minkowski sum and the convex envelope. The Minkowski sum is symmetric on its inputs, but for order polytopesand,is in general not isomorphic to.

SECTION: Tropical polynomials/neural networks of posets
In this section, when referring to the tropical polynomial of a posetwe mean its expanded presentation (see Definition).
We can associate to every order polytope a tropical polynomial, and we show that this association lifts the action of the operad of posets.

The following definition is justified by Lemma.

Letbe a poset withpoints and letbeposets. We define theby:

Note that the action of a chainon the tropical polynomial of a chaincan be computed by evaluatingon the polynomial, as follows:

We used Equation () to go from the second to the third line.

Letbe a poset withpoints. The polynomialis the sum of tropical polynomials associated to maximal simplices in. The maximal simplices of an order polytope are indexed by linearizations of the poset(see).
Linearizations ofare linearizations ofwith a tail (coming from) and a head (coming from), and the tropical polynomials of these linearizations have the same coefficients as the evaluation.
∎

Letbe the number of linearizations of the poset.

See.
∎

The number of linearizations of the lexicographic sum
is a multiple of the number of linearizations of the input. Sinceandare in general not equal, which is required in the proof of Lemma, we cannot expect that the action of a poset on tropical numbers will be given by the evaluation of the tropical polynomial ofon the tropical polynomials of the inputs.

For instance, compare the following evaluation of polynomials:

with the action of the poset2,1 :

In Definition, we fixed a representative neural network for every poset.

Since the assignment of a tropical polynomial to a poset is one-to-one, and we pick one neural network representative from every tropical polynomial of a poset, the following action is well defined up to the order of the linearizations on the inception module.

We define theby the rule:

We may ask ourselves the following question: what properties of posets are inherited by poset neural networks? Given an industrial problem, when trying to solve it with neural networks, the first step is to review the existing literature to determine whether something similar has been addressed before. As part of the efforts to clarify whether a neural network is capable of solving a classification problem, we note that fromand, the number of vertices in the order polytope of a poset is an upper bound for the number of linear regions of the poset neural network. This number is called. Another property that follows immediately from this construction is that given a posetand its poset neural network, the number of points of the posetplus one is the number of layers (linear transformation + non-linearity) of the neural network, according to Definitionand Lemma.

SECTION: Extension to all tropical polynomials
We have defined the endomorphisms of a family of neural networks. In this section, we will now extend the action of the operad of posets to convex polytopes. From the extension to convex polytopes, we will define the action of the operad of posets on tropical polynomials.

We define the action of the operad of posets on arbitrary points and arbitrary convex polytopes as follows.

Givenand, a poset withpoints, we define:

Letbe a poset withpoints and letbe convex polytopes. Then, we define theas:

In particular, if all polytopeshave integer vertices, then the resultant polytopehas integer vertices.

We now show that the action of a poset on convex sets is a convex set.

Let. We now aim to show that. There aresuch thatand.
If, then by the convexity of, the linebelongs to. Assume that at least for onewe have.

Since eachis convex, the linesbelong to each.

Then, at each, we have that:

Therefore, the action of posets on convex polytopes preserves convexity.
∎

In particular, ifis a poset withand, then.

Note that if the inputs are not convex, then the polytope may not be convex, for example,gives two lines with the same end point, but there is no line betweenandin the set.

Note also that a tropical polynomial is associated with a polytope where all but the last coordinate are guaranteed to be non-negative integers.

We can now define the action of posets on tropical polynomials.

Letbe a poset withpoints, and letbetropical polynomials with their corresponding convex polytopes.
As polytopes, we compute the action, obtain the vertices of, and then those vertices define a tropical polynomial, which we denote by.

This composition sendstropical polynomials into a tropical polynomial.

SECTION: Future work
A major problem in the theory of algebras over the operad of posets is the precise description of the action of posets. The papercomputes explicitly the action of the operad of series parallel posets on Stanley-order polynomials, while paperprovides a description of the action of the poset2 ,4 ,1 ,3 on Stanley order polynomials without explicitly computing it.

As is common in category theory, understanding these operations should have implications in different fields. In, the precise description of the operations1 ,2 and2 ,1  acting on “shuffles of posets” was a key ingredient to answering a question with roots in dendroidal homotopy theory.

Although the Minkovski sum and the convex envelope are well-known operations, we are not aware of the study of other poset endomorphisms in geometry.

In this paper, we were able to extend the endomorphisms of posets to the endomorphisms of poset neural networks, but the general extension to IVNN is still open.

SECTION: Acknowledgments
This project started after Iryna Raievska and Maryna Raievska asked the first author about the applications of order theory to machine learning during the Ukraine Algebra Conference “At the End of the Year 2023”, and we thank them for their question.

The work of S.I. Kim, E. Dolores-Cuenca and S. López-Moreno was supported by the National Research Foundation of Korea (NRF) grant funded by the Korean Government (MSIP) (2022R1A5A1033624, 2021R1A2B5B03087097) and Global—Learning and Academic research institution for Master’s·PhD students, and Postdocs (LAMP) Program of the National Research Foundation of Korea (NRF) grant funded by the Ministry of Education (No. RS-2023-00301938).
S. López-Moreno was also supported by the Korea National Research Foundation (NRF) grant funded by the Korean government (MSIT) (RS-2024-00406152). J.L. Mendoza-Cortes acknowledges startup funds from Michigan State University. This work was supported in part through computational resources and services provided by the Institute for Cyber-Enabled Research at Michigan State University.

SECTION: References
SECTION: Appendix
SECTION: Datasets used for experiments
Our experiments used the CIFAR10 dataset, the CIFAR100 dataset, and the Fashion MNIST dataset.

SECTION: Choices made during the experiments
Since the disjoint union of points has too many parameters and the four chain has too little, most of our experiments use thefilter.

All experiments used seeds to guarantee reproducibility, except those reported from literature or with an explicit note.
The average accuracy of the experiments without the reproducibility seeds was higher than the average accuracy after adding the reproducibility seeds. This behavior is well documented in the official documentation of PyTorch.

SECTION: Quaternion Convolutional Neural Networks
Intuition tells us that our poset filter works best as a pooling function that is introduced almost at the end of the convolutional part of a CNN, but not immediately before the dense network. We believe that introducing the poset filter before one of the last convolutions still provides a significant reduction of parameters, and experimental results suggest that introducing it at this point results in minimal loss of information compared to a max pooling, average pooling, and mixed pooling.

Quaternion numbers can encode rotations with the advantage of using fewer parameters, since multiplication with a quaternion with four coordinates encodes the same information as multiplication with a () matrix withparameters.
In, the authors define a quaternion neural network, with the motivation that the quaternion neural network requires fewer parameters than a normal neural network to achieve similar precision.

In our experiments, our aim is to test a new convolutional filter that will effectively reduce the dimensions of the input by half. We choose to work with quaternion neural networks as the quaternion assumption implies that all layers of the quaternion have inputs with dimensions divisible by 2.

Our first experiment consists of inserting our function into their shallow neural network applied on CIFAR10. We compare the results with not only the ones given in paper, but also with an alternative implementation in Pytorch developed by J. Weeda, R. Awad and M. Msallak in(called QCNN (Weeda impl.) in the results’ tables). This shallow network has two blocks of double convolutions followed by their respective ReLU activation functions, and we show the performance results of the model depending on the position of our function: after the first convolution (and its corresponding ReLU), before the last convolution and, lastly, immediately before the dense part. We also performed a-fold cross-validation to demonstrate the robustness and generalizability of our model.

We have run our model on a V100 GPU and implemented it in Pytorch2.4.1 with CUDA12.4 support. Although we have kept the architecture mainly unchanged to facilitate a more direct comparison of the results, we have included reproducibility seeds, and, therefore, we will also include the results of the QCNN (Weeda impl.) when adding those same reproducibility seeds.

In Tablewe present the accuracy of our model when thefilter is inserted after the first convolution, calculated as the average accuracy overtraining runs. The results also include the accuracy when substituting the insertedfilter by a max pooling, average pooling, and mixed pooling.

In Tablewe include the results when performing a-fold cross-validation with. The sample standard deviation (std) of the results was also calculated.

In Tablewe present the accuracy of our model when thefilter is inserted before the last convolution, calculated as the average accuracy overtraining runs with different seeds each. Tableshows the results for the-fold cross-validation with.

In Tablewe present the accuracy of our model when thefilter is inserted immediately before the dense part, calculated as the average accuracy overtraining runs. In
Tableappear the results for the-fold cross-validation with.

So far, we have conducted experiments using theposet filter. However, there are in totaldifferent posets withvertices or nodes, including theposet. Therefore, we conduct experiments on the remainingposets to compare their performance as well. See results in Table.

Moreover, given that theposet is defined as the maximum ofspecific terms, one of which is, we conduct experiments withrandom functions which take the maximum betweenandrandom linear combinations of the four inputs. The corresponding results appear in Table.

To understand how the choice of poset affects the encoding properties of a poset filter, we sample points in a lattice of points of the form, with, and we consider only those points inside the unit four-dimensional ball. Figureshows the histogram (made with Matplotlib) of positive values and their corresponding standard deviation. Although most values are close to zero, there is a large difference among the standard deviations.

SECTION: DenseNet
In this section, we report experiments on the Dense Netarchitecture with Fashion MNIST, CIFAR10 and CIFAR100 datasets.

We follow the pytorch implementation of Dense Net. It has blocks of bottlenecks split by two blocks of transitions. The blocks of transitions include an average pooling layer. After the bottlenecks, there is another average pooling of size 8, followed by a feedforward section.

Our goal was to test the result of adding theposet filter along the convolutional part of Dense Net. Unfortunately, Dense Net is optimal in the sense that the input of the feed-forward layer has width and height 1. Then, we wondered if we could replace the average pooling layers of Dense Net by our poset filter.

We tested several combinations, but we show only the positions in which the algorithm converged. For example, the replacement ofbydid not converge for Fashion MNIST, CIFAR10 or CIFAR100.

According to Table, when we run the original code, we only achieve an accuracy of, while the recorded value should be, however, we added seeds to make our results reproducible. In every seeded experiment, we found a lower accuracy than when the seeds were removed. Another difference with the github code is that we added 2 padding on all sizes of the Fashion MNIST dataset images, as the github code assumes the CIFAR dataset, which has different dimensions from Fashion MNIST. Our best experiment returned an average of, and we also record the standard deviation. Our conclusion is that adding thefilter does not affect the accuracy in this case. We also tested removing the ReLU, reaching the same conclusion; see Table.

Tableshows the results for the CIFAR10 dataset, while Tableshows the average results for the CIFAR100 dataset. In all cases, the accuracy did not improve over the original algorithm.
All experiments were performed on NVIDIA A100-SXM4-80GB.

SECTION: CNN
We tested on, a standard convolutional neural network with ELUas the non linearity and two max pooling layers. The original network achievesaccuracy. Our best result was aaccuracy, with an average accuracy ofoverruns, obtained by replacing the second max pooling by thefilter.

We also report cross-validation with. All experiments were performed on NVIDIA A100-SXM4-80GB.

SECTION: Filter functions
Consider the following transformation, which we will call4 ,3 ,2 ,1 filter (disjoint union of points), that sends thesquare matrixto:

This transformation returns the partial sum that has the largest value. We can think of this operation as a convolutional filter and interpret the output as a geometric transformation on an input image, assuming that the pixels have values betweenand that we normalize the values before displaying the image.

There are two drawbacks to this transformation: first, the image needs to be normalized after the transformation. Secondly, it contains many operations.

In contrast, consider the following transformation, which we will call1 ,2 ,3 ,4 filter:

We conducted experiments on an image ofpixels, uploaded to Wikimedia by Diego Delso, delso.photo, License CC BY-SA. In Figure, we plot the resultant image after applying the4 ,3 ,2 ,1 filter and the1 ,2 ,3 ,4 filter. We also plot the effect of max pooling and average pooling to compare the effects.

To find if there is any difference between the4 ,3 ,2 ,1 filter and the1 ,2 ,3 ,4 filter, we perform the following experiment: taking the original image, apply one fixed filter three times, effectively resizing the image to 1/8 of the original dimension. Then, use nearest neighbors to resize the image to the original shape. In this way, we obtain two images: image, obtained from applying the4 ,3 ,2 ,1 filter three times and then upsizing, and image, obtained from applying the1 ,2 ,3 ,4 filter three times and then upsizing. Then, we compare the SSIM and PSNR between imageand the original image, and between imageand the original image. As expected from the visual evidence, other methods may be more convenient for resizing, but we only aim to obtain an understanding of the effect of the filters.

We present the statistics obtained by applying the nearest-neighbor method three times, followed by upsizing using the same approach. In addition, we report the corresponding SSIM and PSNR values.