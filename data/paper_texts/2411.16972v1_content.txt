SECTION: Clustering Time Series Data with Gaussian Mixture Embeddings in a Graph Autoencoder Framework

Time series data analysis is prevalent across various domains, including finance, healthcare, and environmental monitoring. Traditional time series clustering methods often struggle to capture the complex temporal dependencies inherent in such data. In this paper, we propose the Variational Mixture Graph Autoencoder (VMGAE), a graph-based approach for time series clustering that leverages the structural advantages of graphs to capture enriched data relationships and produces Gaussian mixture embeddings for improved separability. Comparisons with baseline methods are included with experimental results, demonstrating that our method significantly outperforms state-of-the-art time-series clustering techniques. We further validate our method on real-world financial data, highlighting its practical applications in finance. By uncovering community structures in stock markets, our method provides deeper insights into stock relationships, benefiting market prediction, portfolio optimization, and risk management.

SECTION: Introduction

time series is commonly referred to a sequence of data points collected or recorded at successive time instances, usually at uniform intervals. For instance, in finance, time series data might include daily closing prices of a stock over a year(Shah, Isah, and Zulkernine2019). In healthcare, it could be the EEG signal of a person’s brain in a specific time interval(Siuly, Li, and Zhang2016), and in environmental monitoring, it might involve hourly temperature readings(Zhao, Zhu, and Lu2009).

Numerous studies have been conducted on time series analysis, encompassing various tasks such as forecasting(Torres et al.2021), classification(Ismail Fawaz et al.2019), clustering(Aghabozorgi, Shirkhorshidi, and Wah2015), anomaly detection(Shaukat et al.2021), visualization(Fang, Xu, and Jiang2020), pattern recognition(Lin et al.2012), and trend analysis(Mudelsee2019).

Time series clustering is a powerful method for grouping similar time series data points based on their characteristics, especially when there is no prior knowledge of the data structure(Liao2005). It has diverse applications, such as stock market forecasting, where it is used for feature extraction to predict stock movements, helping investors anticipate market behavior and enhance model predictions(Babu, Geethanjali, and Satyanarayana2012). In portfolio optimization, clustering identifies stocks with similar traits, fostering diversification and reducing risks. Additionally, it supports risk management by predicting market volatility using algorithms like Kernel K-Means and Gaussian Mixture Models(Chaudhuri and Ghosh2016)and contributes to fraud detection by flagging anomalies that deviate from typical cluster patterns(Close and Kashef2020).

Despite its practical significance, unsupervised time series clustering faces notable challenges. Time series data often vary significantly in their critical properties, features, temporal scales, and dimensionality across different domains. Real-world data further complicate this process by introducing issues such as temporal gaps and high-frequency noise(Hird and McDermid2009). To address these challenges, researchers have developed methods focusing on three main aspects: 1) time series similarity measures(Tan et al.2020; Li, Boubrahimi, and Hamdi2021), 2) discriminative representation learning(Ma et al.2019; Zhang et al.2018; Jorge and Rubén2024), and 3) clustering mechanisms(Paparrizos and Gravano2015; Li et al.2022). These advancements aim to enhance the reliability and applicability of time series clustering in complex, real-world scenarios.

In this paper, we leverage all these aspects by constructing a graph from time series data using dynamic time wrapping (DTW)(Sakoe1978)to capture the relationships between individual time series. By exploiting a specialized graph autoencoder, we can also learn how to embed each node properly. This embedding not only represents the unique features of each data point but also captures shared features from nodes similar to the data point. To the best of our knowledge, this is the first work that employs graph autoencoder architecture for time series clustering.

The novel contributions of this work can be summarized as follows:

We propose a new framework for time series clustering. This approach uses a graphical structure to capture more detailed information about data relationships. Turning a time series dataset into graphs can effectively capture both temporal and relational dependencies.

We introduce a specialized graph autoencoder, named Variational Mixture Graph Autoencoder (VMGAE), that generates a Mixture of Gaussian (MoG) embeddings. This allows the separation of data through a Gaussian Mixture Model (GMM) in the embedding space, enhancing clustering performance and providing a more precise representation of time series data.

We conducted a comprehensive comparison of our method against strong baselines, demonstrating significant improvements over the state-of-the-art. Additionally, we evaluated the practicality of our approach using real-world financial data.

SECTION: Related Work

Time series data clustering has been a significant area of research for decades, leading to various algorithms. Classical clustering algorithms like k-means and spectral clustering can be executed on raw time series data, while some methods use modified versions of classical methods. K-shape(Paparrizos and Gravano2015), assigns data to clusters based on their distance to centroids and updates the centroids like k-means, but instead uses cross-correlation for distance measurement. KSC((Yang and Leskovec2011)) uses k-means for clustering by adopting a
pairwise scaling distance measure and computing the spectral norm of a matrix for centroid computation.

Another approach is to use shapelets to extract discriminative features from time series data, as demonstrated in(Ulanova, Begum, and Keogh2015),(Zhang et al.2018),(Li et al.2022), and(Zhang et al.2016). The main challenge in these methods is identifying suitable shapelets for the shapelet transform process, which extracts meaningful features from raw data to perform clustering. R-clustering method(Jorge and Rubén2024)employs random convolutional kernels for feature extraction, which are then used for clustering. Additionally,(Tan et al.2020)implements a hierarchical clustering algorithm that uses Granger causality(Ding, Chen, and Bressler2006)as the distance measure, fusing pairs of data to create new time series and continuing the clustering process. STCN(Ma et al.2020)uses an RNN-based model to forecast time series data, employs pseudo labels for its classifier, and utilizes the learned features for clustering. Since our method leverages both the autoencoder architecture and a graph-based approach for clustering time series data, we will review autoencoder-based methods and graph-based techniques separately.

SECTION: Autoencoder-based methods

Autoencoders have demonstrated empirical success in clustering by using their learned latent features as data representations. For instance, DEC(Xie, Girshick, and Farhadi2016)adds a KL-divergence term between two distributions to its loss function, alongside the reconstruction error loss, to make the latent space more suitable for clustering. DTC(Olive et al.2020)introduces a new form of distribution for the KL-divergence term, applying this method to trajectory clustering. Another method, DCEC(Guo et al.2017), incorporates a convolutional neural network as its autoencoder within the DEC method for image clustering. VaDE(Jiang et al.2016)adds a KL-divergence term between the Mixture-of-Gaussians prior and the posterior to the reconstruction loss. This is done using the embeddings of data points in the latent space of a variational autoencoder and a prior GMM distribution.
In the domain of time series clustering, DTCR(Ma et al.2019)trains an autoencoder model with the addition of k-means loss on the latent space and employs fake data generation and a discriminator to classify real and fake data, enhancing the encoder’s capabilities. Also, TMRC(Lee, Kim, and Sim2024)proposes a representation learning method called temporal multi-features representation learning (TMRL) to capture various temporal patterns embedded in time-series data and ensembles these features for time-series clustering.

SECTION: Graph-based methods

Graphs have significantly enhanced the capabilities of deep learning methods in various tasks. Variational Graph Autoencoder (VGAE)(Kipf and Welling2016b)utilizes GCN(Kipf and Welling2016a)for link prediction and node classification tasks. Specifically, graphs have also found significant applications in the time series domain. Recent works such as(Song et al.2020),(Cao et al.2020), and(Yu, Yin, and Zhu2017)use graph-based methods for time series forecasting, while(Zha et al.2022)and(Xi et al.2023)apply them for classification. Additionally,(Zhao et al.2020),(Deng and Hooi2021), and(Han and Woo2022)utilize graph-based models for anomaly detection in time series data. Graphs have also been employed for time series data clustering(Li, Boubrahimi, and Hamdi2021).

One of the critical challenges in graph-based methods for the time series domain is constructing the adjacency matrix. Several methods address this issue by introducing metrics to compute the similarity or distance between two time series samples. The Granger causality method(Ding, Chen, and Bressler2006)leverages the causal effect of a pattern in one time series sample on another to measure similarity between samples. The Dynamic Time Warping (DTW) method(Sakoe1978)minimizes the effects of shifting and distortion in time by allowing the elastic transformation of time series to compute the distance between two samples. There are many extensions of the DTW method, such as ACDTW(Li et al.2020), which uses a penalty function to reduce many-to-one and one-to-many matching, and shapeDTW(Zhao and Itti2018), which represents each temporal point by a shape descriptor that encodes structural information of local subsequences around that point and uses DTW to align two sequences of descriptors.

The similarity between two time series can be used directly as the edge representation, but the distance needs to be processed for use in the adjacency matrix. One method is to apply a threshold on distances to predict whether an edge exists between two nodes in a binary graph(Li, Boubrahimi, and Hamdi2021).

SECTION: Problem Definition and Framework

SECTION: Notation

In the following sections, we denote the training dataset as, whererepresents the-th time series, andis the size of the training dataset. The length of the-th time series is denoted by. Each time seriesbelongs to a cluster, where, andis the number of clusters. Furthermore, we defineas the vector of the corresponding clusters for all time series in.

Additionally, a graph is represented as, whereis the set of nodes, and each edgerepresents a connection between nodesand. The structure of the graph is described by an adjacency matrix, whereif, andotherwise. The feature vectorcorresponds to the content attributes of node, which, in our context, is equivalent to the time series.

Given the graph, our objective is to map each nodeto a low-dimensional vector. This mapping is formally defined as:

where the-th row of the matrixis denoted by. Here,is the dimensionality of the embedding space. The matrix, which contains these embeddings, is designed to preserve both the structural information of the graph, captured byand the content features represented by.

SECTION: Overall Framework

Our goal is to represent a time series dataset as a graph, where each node corresponds to a time series. We aim to learn a robust embedding for each node in this graph to perform clustering. To achieve this, we first construct the graph. Next, we apply an unsupervised graph representation learning approach within an autoencoder framework, enhanced for clustering, to process the entire graph and learn effective node embeddings.

Graph Construction.Each time series is represented as a node in the graph construction phase. The distance matrix, calculated using the Dynamic Time Warping (DTW) method, captures the alignment between time series of varying lengths. We then apply our novel transformation to convert these distances into similarity scores, which determine the graph’s structure. This approach ensures that the graph reflects the true underlying relationships in the data, preserving important temporal patterns.

Learning Representation via a Graph Structure.After constructing the graph, we leverage a specialized autoencoder framework to learn embeddings for each node. This unsupervised method compresses the graph’s information into a lower-dimensional representation. Our approach ensures that the resulting embeddings are informative, generalizable, and discriminative, making them particularly effective for clustering tasks. Figure1provides a comprehensive overview of our method.

WDTW

SECTION: Methodology

This section describes the steps taken to represent a time series dataset as a graph and how we use this graph structure to learn meaningful embeddings for clustering.

SECTION: Graph Construction

For graph construction, we use a variant of DTW called Weighted Dynamic Time Warping (WDTW) and a constraint to limit the window size of the wrapping path.
The distance between two sequencesandwith a weight funtionand a window sizeis computed as follows:

subject to the constraint:

whereis a warping path that aligns the sequencesand,is the distance between elementsand. This could be any customized distance. For simplicity, we use Euclidean distance.is a weight function that depends on the absolute difference between indicesand, andis the window size that limits the maximum allowable shift between indices.

The weight functionshould be a monotonic function of, as it penalizes alignments where the indices are farther apart, favoring closer alignments. For simplicity, we set, whereis a positive hyperparameter.

Given the training dataset, we construct a distance matrix, whererepresentswith fixed parametersand. Next, we propose a novel transformation approach to convert the distance matrixinto a similarity matrix. By fixing the density rate, we compute a thresholdto construct an adjacency matrix, whereifandotherwise. The key difference compared to previous work(Li, Boubrahimi, and Hamdi2021)is that, instead of fixing, we fixand use it to compute the correspondingfor each dataset. This is important because the optimal thresholdmay vary across datasets, while the optimalis much more stable. Figure2presents a sample graph constructed using this method. While the current representation demonstrates good separation, further refinement can be achieved with the use ofVMGAE.

SECTION: Learning Representation via a Graph Structure

Graph Convolutional Autoencoder.In our unsupervised setting, we utilize a graph convolutional autoencoder architecture to embed a graphinto a low-dimensional space. Specifically, we derive an embeddingfor the-th node of the graph. This approach presents two key challenges: 1) How can both the graph structureand node featuresbe effectively integrated within the encoder? 2) What specific information should the decoder reconstruct?Graph Convolutional Layer.To effectively capture both the structural informationand node featuresin a unified framework, we employ
graph convolutional network (GCN)(Kipf and Welling2016a). Graph convolutional operator extends the convolution operation to graph data in the spectral domain and applies a layer-wise transformation using a specialized convolutional function. Each layer of the graph convolutional network can be expressed as follows:

whereand. Here,represents the identity matrix andis an activation function like ReLU. Also,denotes the input at the-th layer, andis the output after the convolution operation. Initially,, whererepresents the input features of the graph withnodes andfeatures. The matrixcontains the parameters to be learned. Additionally, in this work, we denote each convolutional layer with activation functionas.

In addition to this convolutional layer, several other variants suitable for node-level tasks have been proposed(Veličković et al.2018; Hamilton, Ying, and Leskovec2018; Du et al.2018; Defferrard, Bresson, and Vandergheynst2017). In Appendix F.2, we compare the effects of different convolutional layers on the performance of our method.Encoder Model. The encoder ofVMGAEis defined by an inference model:

Here,andare constructed using a two-layer convolutional network, where the weightsin the first layer are shared:

The encoder modelencodes both graph structure and node features into a latent representation.
According to the reparameterization trick,is obtained by:

where,is element-wise multiplication.Decoder Model. Decoder model is given by an inner product between latent variables:

and the conditional probability is usually modeled as:

whereis the logistic sigmoid function.

Thus, the embeddingand the reconstructed graphcan be presented as follows:

Learning Algorithm.InVMGAE, our objective is to maximize the log-likelihood of the data points,. Based on the decoder model The joint probabilitycan be factorized as:

The log-likelihood can be expressed as:

The inequality in Equation12is derived from Jensen’s inequality. Instead of maximizing the log-likelihood directly, we aim to maximize its Evidence Lower Bound (ELBO), and using the factorization in Equation11, it can be rewritten as follows:

where the last line is obtained under the assumption of a mean-field distribution for.

Similar to the approach in(Jiang et al.2016), a mixture of Gaussian latent variables is used to learn the following distributions:

By assuming a mean-field distribution, the joint probabilityandcan be factorized as:

whereis the prior distribution of clusterhence,is the categorical distribution parametrized by. AlsoMoreover,are the mean and the variance of the Gaussian distribution corresponding to cluster,is an identity matrix.

Using Monte Carlo estimation for the expected value calculation in full-batch mode and substituting the assumptions from Equations17and18into Equation14, the objective function can be expressed as:

The first term represents the standard reconstruction loss, while the second and third terms act as regularizers, encouraging the model to generate a Gaussian mixture embedding. A detailed derivation of EquationLearning Representation via a Graph Structureis provided in Appendix A.

The next question is how to compute. According to the derivations in Appendix B, the ELBO can be rewritten as:

In the equation above, the first term is independent of, and the second term is non-negative. Therefore, similar to the approach in(Jiang et al.2016), to maximize, we assumeto be zero. Consequently, the following equation can be used to computeinVMGAE:

While the learned distribution can be directly used for clustering, we have empirically found that refitting a GMM on the learned representationsignificantly improves clustering performance.

DatasetK-meansSCKSCK-shapeu-shapletUSSLDTCRSTCNR-clustTMRCVMGAEBeef0.29250.40630.38280.33380.34130.33380.54730.54320.24750.74240.5237Car0.25400.33490.27190.37710.36550.46500.50210.57010.53900.39170.6193DiatomSizeReduction0.93000.83871.00001.00000.48491.00000.94181.00000.61540.63240.8882Dist.Phal.Outl.AgeGroup0.18800.34740.33310.29110.25770.38460.45530.50370.43430.32980.4400ECG2000.14030.13500.14030.36820.13230.37760.36910.43160.15610.37630.3643ECGFiveDays0.00020.00050.06820.00020.14980.65020.80560.35820.01730.27580.8378Meat0.25100.27320.28460.22540.27160.90850.96530.93930.64200.79801.0000Mid.Phal.TW0.41340.49520.44860.52290.40650.92020.55030.61690.41380.48020.4409OSULeaf0.02080.08140.04210.01260.02030.33530.25990.35440.44530.30120.3739Plane0.85980.92950.92180.96421.00001.00000.92960.96150.98920.89170.9678Prox.Phal.Outl.AgeGroup0.06350.42220.06820.01100.03320.68130.55810.63170.56650.57310.5639SonyAIBORobotSurface0.61120.25640.61290.71070.58030.55970.66340.61120.66200.23000.9319SwedishLeaf0.01680.06980.00730.10410.34560.91860.66630.61060.71510.50990.5886Symbols0.77800.78550.82640.63660.86910.88210.89890.89400.87750.81590.8996ToeSegmentation10.00220.03530.02020.30730.30730.33510.31150.36710.01791.00000.3081TwoPatterns0.46960.46220.47050.39490.29790.49110.47130.41100.31810.13471.0000TwoLeadECG0.00000.00310.00110.00000.05290.54710.46140.69110.49660.02870.8726Wafer0.00100.00100.00100.00100.00100.04920.02280.20890.00000.50190.2136WordSynonyms0.54350.42360.48740.41540.39330.49840.54480.39470.88850.42100.5812AVG Rank8.78947.63167.10537.47398.10533.47363.73683.42105.89476.00003.1579AVG NMI0.30710.33160.33620.35130.33210.59670.57490.58410.47590.49650.6553Best00111503237

Additionally, in our experiments, we introduce a weight parameterfor the second component of the loss function, allowing us to balance the contribution of each term in the final loss function:

whererepresents the reconstruction loss on the adjacency matrix:

and the regularizer termis defined as follows:

Compared to vanilla GAE and VGAE, our method introduces only a few additional parameters, which need to be learned.
However, this does not significantly increase the computational overhead. Initializing these parameters using a GMM proves effective. In practice, performing a few epochs of pretraining with GAE—e.g., using only the reconstruction loss—followed by fitting a GMM on the latent embeddings is sufficient to achieve a strong initialization.

Finally, we summarize the complete set of steps involved in our proposed method in Algorithm1.

DatasetK-meansSCKSCK-shapeu-shapletUSSLDTCRSTCNR-ClustTMRCVMGAEBeef0.67130.62060.70570.54020.69660.69660.80460.74710.67030.82290.7862Car0.63450.66210.68980.70280.64180.73450.75010.73720.75070.73220.8045DiatomSizeReduction0.95830.92541.00001.00000.70831.00000.96820.99210.81400.85390.9719Dist.Phal.Outl.AgeGroup0.61710.72780.65350.60200.62730.66500.78250.78250.74250.64770.6827ECG2000.63150.50780.63150.70180.57580.72850.66480.70180.62060.74240.7862ECGFiveDays0.47830.49940.52570.50200.59680.83400.96380.65040.01730.64920.9523Meat0.65950.71970.67230.65750.67420.77400.97630.91860.83410.88471.0000Mid.Phal.TW0.09830.80520.81870.62130.79200.79200.86380.86250.79150.68500.8132OSULeaf0.56150.73140.57140.55380.55250.65510.77390.76150.80670.76440.7798Plane0.90810.93330.96030.99011.00001.00000.95490.96630.99730.94720.9868Prox.Phal.Outl.AgeGroup0.52880.77910.53050.56170.52060.79390.80910.83790.80210.81890.8147SonyAIBORobotSurface0.77210.50820.77260.80840.76390.81050.87690.73560.88430.65290.9834SwedishLeaf0.49870.68970.49230.53330.61540.85470.92230.88720.93020.85370.8825Symbols0.88100.89590.89820.83730.96030.92000.91680.90880.98210.90880.9677ToeSegmentation10.48730.49960.50000.61430.58730.67180.56590.81770.51121.00000.6712TwoPatterns0.85290.62970.85850.80460.77570.83180.69840.76190.72730.62951.0000TwoLeadECG0.54760.50180.54640.82460.54040.86280.71140.94860.79840.58730.9655Wafer0.49250.53360.49250.49250.49250.82460.73380.84330.53490.90820.5853WordSynonyms0.87750.86470.87270.78440.82300.85400.89840.87480.89950.88750.9168AVG Rank8.73688.36846.89477.52637.78954.47394.00003.94745.21055.52632.6842AVG RI0.63980.68600.69430.69110.68120.80540.82290.82820.74280.78820.8605Best00111232337

Input: Time series datasetParameters: Hyperparameters, Pre-training iterations, Training iterationsOutput: Clustering results

SECTION: Experiments

SECTION: Experimental Setup and Datasets

We employed 19 datasets from the UCR time series classification archive(Huang et al.2016)for our clustering experiments, with specific details provided in Table1and Table2. Our networks were implemented and tested using PyTorch(Paszke et al.2019), Torch_Geometric(Fey and Lenssen2019), and executed on an A100 GPU (40G).VMGAEwas trained with a learning rate offor 500 epochs in full-batch mode, using the Adam optimizer for optimization. Dropout withwas applied to prevent overfitting. A significant advantage of our method is that we can leverage the latent distribution to tune hyperparameters (as illustrated in Figure4in Appendix E) . The hyperparameters,,, andwere tuned by visualizing the latent distribution of the training set for each dataset separately. During the testing phase, these hyperparameters were fixed, and the final results were evaluated. The details of the datasets used, the sensitivity of hyperparameters, and the evaluation metrics are provided in Appendices C, F.1, and D, respectively.

SECTION: Quantitative Analysis

The performance ofVMGAEwas benchmarked against several time series clustering methods to evaluate its clustering capabilities thoroughly. The results presented in Tables1and2are sourced from the original papers, except R-Clustering(Jorge and Rubén2024), where results were obtained by running the authors’ publicly available code. Both tables highlight the best result for each dataset in bold.

As shown in Table1,VMGAEdelivers superior performance, achieving the lowest average rank of 3.1579, the highest average NMI score of 0.6553, and surpassing state-of-the-art (SOTA) methods on seven datasets. Similarly, Table2highlightsVMGAE’s strong results based on the Rand Index (RI) metric, with the lowest average rank of 2.6842, the highest average RI of 0.8605, and outperforming SOTA across seven datasets. Notably, on specific datasets such asTwoPatterns,SonyAIBORobotSurface, andTwoLeadECG, the SOTA results were significantly exceeded, with NMI improvements of 0.5089, 0.2212, and 0.1815, and RI improvements of 0.1471, 0.0991, and 0.0169, respectively.

Further extensive qualitative analysis of our method is provided in Appendix E.

SECTION: Application in Finance

Understanding stock market dynamics in finance is essential for making informed investment decisions. Detecting patterns and communities within this complex network of stocks helps gain insights into market behavior and make better investment choices.

In this section, we demonstrate the effectiveness of our approach by applying it to real-world stock market data and evaluating the quality of the resulting clusters. We selected the top 50 publicly traded U.S. stocks listed on NASDAQ, NYSE, and NYSE American, ranked by market capitalization. The input time series for our model consists of daily normalized closing prices from January 1, 2020, to October 4, 2024. We set the number of clusters to 5 based on the Elbow Method(Thorndike1953). The results are displayed in Figure3a, with the average for each cluster shown in Figure3b, highlighting distinct discriminative patterns.

SECTION: Conclusion

In this work, we introduce a novel method for clustering time series data by leveraging graph structures, achieving strong performance across various datasets. Our approach transforms time series data into graph representations using Weighted Dynamic Time Warping, enabling the capture of temporal dependencies and structural relationships. We then apply the proposed Variational Mixture Graph Autoencoder (VMGAE) to generate a Gaussian mixture latent space, improving data separation and clustering accuracy. Extensive experiments demonstrate the effectiveness of our method, including sensitivity analysis on hyperparameters and the evaluation of different convolutional layer architectures. Furthermore, we applied our method to real-world financial data, uncovering community structures in stock markets and showcasing its potential benefits for market prediction, portfolio optimization, and risk management. These findings highlight the versatility and practical applications of VMGAE in addressing time series clustering challenges.

SECTION: Appendix

SECTION: Appendix AA. Derivation of ELBO for VMGAE

The Evidence Lower Bound (ELBO) for VMGAE is defined as follows:

whereXrefers to the feature matrix (or time series matrix in our case), andArepresents the adjacency matrix. Jensen’s inequality is applied to arrive at this bound.

The expanded form ofusing11is given by :

Next, we compute the expectations over the various terms in the ELBO.Term (I):

Term (II):

According to appendix B(Jiang et al.2016), we have:

Term (III):

Term (IV):

Term (V):

By putting all terms together, we will have:

SECTION: Appendix BB. Derivation of

An important point is how to calculate. We can reformat the ELBO into the following form:

In the Equation above, the first term is not dependent onand the second is non-negative. Hence, to maximizeshould be satisfied. As a result, we use the following Equation to computeinVMGAE:

SECTION: Appendix CC. Datasets

We conducted our clustering experiments using 19 datasets from the UCR Time Series Classification Archive(Huang et al.2016), a widely recognized benchmark for time series analysis. The details of these datasets are presented in Table3.

SECTION: Appendix DD. Evaluation Metrics

We evaluate the clustering performance in our analysis using two well-established metrics: the Rand Index () and Normalized Mutual Information (). The Rand Index, which quantifies the agreement between the predicted and actual clustering assignments, is computed as follows:

In this expression,(True Positive) denotes the number of pairs of time series correctly classified into the same cluster, while(True Negative) refers to the number of pairs accurately assigned to different clusters. Conversely,(False Positive) captures the number of pairs incorrectly grouped into the same cluster, and(False Negative) accounts for pairs that should be clustered together but are mistakenly separated.

Thescore is defined as:

whererepresents the number of time series that are common between the-th ground truth clusterand the-th predicted cluster.is the number of time series in the cluster. The variablesandin Equations34and35are defined as previously explained in the section Notation.

SECTION: Appendix EE. Qualitative Analysis

We further present visualizations of the evolving clusters during training on the DiatomSizeReduction in Figure4. These clusters are mapped from the latent space representationsto a 2D space using t-SNE(van der Maaten and Hinton2008). The t-SNE plots illustrate how the latent representations become increasingly well-separated as training progresses, reflectingVMGAE’s capacity to learn distinct clusters from the time series data.

SECTION: Appendix FF. Ablation Study

SECTION: F.1. Hyperparameter Sensitivity Analysis

In this section, we analyze the impact and sensitivity of the hyperparameters,, andon our method. To assess the sensitivity of each hyperparameter, the other hyperparameters were kept fixed at their optimal values, as shown in Table4. The hyperparameter valuesandyield better metric results for theSonyAIBORobotSurface1dataset compared to, which was used to report the results in Tables1and2. This improvement was not evident through the visualization process. As shown in the table, for some datasets likeMeat, the model is not sensitive to the hyperparameter values, whereas for other datasets, such asCar, the model shows some sensitivity to the hyperparameter values.

SECTION: F.2. Impact of Convolutional Layer Variants

Several advanced graph convolutional layers have been developed to enhance information propagation in graph neural networks, each with distinct methods and advantages. One well-known type of convolutional layer is the Graph Attention Network (GAT)(Veličković et al.2018). GAT layers introduce attention mechanisms to graph convolutions, enabling the model to assign different importance to neighboring nodes rather than treating them uniformly. Specifically, the GAT layer computes attention coefficientsbased on node features, which are then used to aggregate information from neighboring nodes. The process of each GAT layer is expressed as follows:

wheredenotes the neighbors of node, andis an activation function. The attention mechanism allows GAT layers to dynamically adjust the influence of neighboring nodes, leading to more flexible and potentially more accurate embeddings.

Another variant is SAGEConv(Hamilton, Ying, and Leskovec2018), which stands for Sample and Aggregation Convolution. This layer generalizes GCNs by allowing for aggregating features from a sampled set of neighbors instead of using all neighbors. Various aggregation operators like mean aggregator, LSTM aggregator, and polling aggregator can perform the aggregation process. The final formula is given by :

whereAGGREGATEis a function that combines the features of the neighbors.

ChebConv(Defferrard, Bresson, and Vandergheynst2017)is another robust convolutional layer that utilizes a recursive process to produce’s and aggregate them by some learnable parameters. The ChebConv whole operation is given by:

wheredenotes the Chebyshev polynomial of order, andis the graph Laplacian.

(a) Impact of hyperparameter

(b) Impact of hyperparameter

(c) Impact of hyperparameter

Similarly, SGConv(Wu et al.2019), or Simplifying Graph Convolution, provides an efficient alternative that simplifies the graph convolution operation while maintaining good performance. The operation can be expressed as:

whereis the normalized adjacency matrix andis a fixed number andis the laearnable parameter matrix.

Finally, TAGConv(Du et al.2018), or Adaptive Graph Convolution, adapts the convolution operation based on the local graph structure. It computes the convolution by taking into account the varying degrees of nodes:

whereis the normalized adjacency matrix and’s are learnable parameters.

we examines how different convolutional layers affect the model’s ability to learn node embeddings and perform clustering. In the main results shown in Tables1and2, we used Graph Convolutional Network (GCN) layers. Here, we test other types of convolutional layers and compare their effects on the model’s performance across different datasets. The results of these comparisons are shown in Table5.

Conv. LayerBeefCarDist. Age GroupNMIRINMIRINMIRIGCN0.52370.78620.61930.80450.44000.6827GAT0.49260.74630.41650.65480.43220.7426SAGEConv0.49070.74290.44990.71190.46370.7492ChebConv0.27890.71520.16830.65340.32430.5933SGConv0.46730.74180.49000.74020.43630.7405TAGConv0.49070.74290.43040.71220.38760.7218

SECTION: F.3. Versatility of VMGAE: Application to Graph Datasets

While our primary contribution focuses on applying VMGAE to time series data transformed into graph representations, it is important to highlight the versatility of our method, which can be effectively applied to any graph input. The architecture is designed to learn meaningful latent representations across diverse graph datasets.

To demonstrate this, we employed theCoradataset, a benchmark graph dataset comprising scientific publications grouped into distinct categories, with citation relationships forming the edges between nodes. Each node corresponds to a publication, and the edges represent citation links. This dataset is commonly used in graph-based machine-learning tasks due to its structured graph topology and rich node features.

Our experiments on theCoradataset further validate the flexibility of our VMGAE architecture. For this evaluation, the learning rate was set totheparameter was set to 0.001, the model was trained for 500 epochs, and dropout was applied with a rate of 0.01. Table6provides a comparison of NMI scores between VMGAE and other recognized graph-based methods such as GAE, VGAE(Kipf and Welling2016b), and ARGA(Pan et al.2019).

VMGAEARGAVGAEGAENMI0.4590.4500.4360.429

SECTION: References