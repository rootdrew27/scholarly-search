SECTION: Backdoor Attack Against Vision Transformers via Attention Gradient-Based Image Erosion
Vision Transformers (ViTs) have outperformed traditional Convolutional Neural Networks (CNN) across various computer vision tasks. However, akin to CNN, ViTs are vulnerable to backdoor attacks, where the adversary embeds the backdoor into the victim model, causing it to make wrong predictions about testing samples containing a specific trigger. Existing backdoor attacks against ViTs have the limitation of failing to strike an optimal balance between attack stealthiness and attack effectiveness.

In this work, we propose an Attention Gradient-based Erosion Backdoor (AGEB) targeted at ViTs. Considering the attention mechanism of ViTs, AGEB selectively erodes pixels in areas of maximal attention gradient, embedding a covert backdoor trigger. Unlike previous backdoor attacks against ViTs, AGEB achieves an optimal balance between attack stealthiness and attack effectiveness, ensuring the trigger remains invisible to human detection while preserving the model’s accuracy on clean samples. Extensive experimental evaluations across various ViT architectures and datasets confirm the effectiveness of AGEB, achieving a remarkable Attack Success Rate (ASR) without diminishing Clean Data Accuracy (CDA). Furthermore, the stealthiness of AGEB is rigorously validated, demonstrating minimal visual discrepancies between the clean and the triggered images.

SECTION: 
Vision Transformers (ViTs) have demonstrated competitive or even superior performance in a diverse array of computer vision tasks, including image classification, image generation, and object detection, outperforming Conventional Neural Networks (CNN). Unlike CNN, which rely on convolutional layers to extract features through hierarchical processing of local image regions, ViTs deconstruct the input image into a flattened patch sequence. These patches are the foundation for feature extraction, leveraging attention mechanisms to focus on and interpret the interrelationships between patches dynamically. This paradigm shift to utilizing attention-based mechanisms for feature extraction marks a significant divergence in methodology between ViTs and CNN, underscoring the innovative approach of ViTs in handling complex visual data.

Backdoor attacks are well-studied vulnerabilities within CNN, supported by a substantial body of
research. Recent studies have illuminated the vulnerability of ViTs to backdoor attacks. Pioneering this field, Subramanya et al.were the first to confirm the vulnerability of ViTs to such threats and propose an inconspicuous trigger by limiting the strength of trigger perturbations. Unfortunately, their approach assumed attacker having access to training data. After that, Yuan et al.developed a patch-wise trigger more effectively seize the model’s attention. Advancing this domain, Zheng et al.introduced a novel patch-wise trigger mechanism within TrojViT to enhance the Attack Success Rate (ASR) while minimizing the Trojan’s
footprint. However, it failed to achieve a truly imperceptible attack. Despite these advancements in designing triggers for ViTs that emphasize resizing triggers to patch-wise dimensions and optimizing them for attention engagement, the critical aspect of trigger stealthiness is often overlooked. Moreover, the patch-wise feature of these triggers, being localized features rather than global ones, makes them susceptible to straightforward defense strategies, such as discarding the patches with the highest attention, thereby significantly undermining their effectiveness.

In pursuit of an effective and stealthy backdoor attack mechanism for ViTs, this study introduces an Attention Gradient-Based Erosion Backdoor (AGEB) targeted at ViTs. AGEB capitalizes on the unique attention gradients inherent to pre-trained ViTs to subtly modify the original image. Selectively eroding pixels in regions with the highest attention gradients embed an unobtrusive signal that serves as the trigger.

Utilizing a morphological erosion process, AGEB ensures that the alterations remain imperceptible to human observers. Figureillustrates the subtle yet critical difference between the original and our triggered images, highlighting the strategic erosion of images in areas of intensified attention gradients. This nuanced approach leverages human cognitive biases that prioritize detecting changes in color gradients over absolute values, thus maintaining the trigger’s stealthiness. AGEB addresses the limitations associated with localized, patch-wise triggers by adopting a global trigger mechanism. This enhancement improves the method’s generalization ability across various ViT architectures, overcoming the challenges of stealthiness and localized trigger limitations previously overlooked in the literature.

Our contributions can be summarised as follows:

We present a backdoor attack against ViTs via attention gradient-based image erosion, addressing the overlooked issues of trigger stealthiness and localization in prior studies.

We enhance the effectiveness of AEGB by adding a small signal and mixing it with the original image. This technique, which involves mixing the eroded segments back into the clean image, helps retain essential semantic information. Additionally, introducing a constant signal ensures that the modifications remain effective across various images.

We conduct comprehensive experiments across various ViT architectures and datasets. The results demonstrate the remarkable effectiveness of AGEB, accurately classifying 97.02% of test images into a designated target class within the ImageNette dataset.

SECTION: 
SECTION: 
The Transformer architecture was initially designed for tasks in natural language processing (NLP). Inspired by the considerable success of the Transformer in NLP, researchers have explored adapting analogous models for computer vision tasks, culminating in the proposition of the Vision Transformers (ViTs).

The Vision Transformer restructures images into a sequence of patches, incorporates position embeddings, and appends a class token—resulting in image representations akin to those used in natural language processing. It employs the attention mechanism for feature extraction, which can be mathematically expressed as:

Here,denotes the query,denotes the key, anddenotes the value. The termcorresponds to the dimensions of the query and the key.
In contrast to CNN, ViTs diverge in two fundamental aspects:

ViTs decompose an image into a multitude of patches for feature extraction, whereas CNN typically derive features from each individual pixel.

ViTs leverage an attention mechanism, as delineated by Equation, in contrast to the convolutional layers utilized by CNN.

Considering the previously discussed aspects, traditional backdoor attacks designed for CNN may be less effective when applied to ViTs. In numerous instances, ViTs demonstrate enhanced robustness against backdoor attacks compared to CNN.

SECTION: 
Backdoor attacks significantly threaten the integrity of Deep Neural Network (DNN) models.
Extensive research has been conducted on backdoor attacks targeting CNN. Gu et al.pioneered this investigation by leveraging pixel patches as triggers to create triggered images. Subsequent studieshave explored a variety of trigger mechanisms for CNN. Recent studieshave aimed at increasing the stealthiness of attacks by utilizing triggers that are either imperceptible or resemble the input’s innate characteristics.

In the context of ViTs, backdoor attacks have also been examined. Subramanya et al.demonstrated the vulnerability of ViTs to backdoor attacks using pixel patches, akin to the approach taken by BadNets. Furthering this line of inquiry, Zheng et al.investigated the effects of trigger size and introduced a patch-wise trigger designed to improve the ASR for ViTs. Simultaneously, Zheng et al.employed a patch-wise trigger to capture the model’s attention more effectively.

However, despite these advancements, the stealth and global features of triggers for ViTs frequently need to be revised to reach current research scrutiny. Patch-wise triggers, for instance, may be easily mitigated by discarding the most attention-grabbing patch. In contrast, visible triggers risk early detection, leading users to avoid using such data for model training. Even though Subramanya et al.suggested an invisible trigger by constraining the amplitude of perturbations, this assumes that attackers can access the training data. Moreover, the TrojViT strategyeven completely omits the stealthiness of the trigger. The discernible feature of the triggered images makes the attack evident, diminishing its effectiveness in real-world scenarios where concealment is critical.

A critical task in designing backdoor attacks for ViTs is identifying a methodology that effectively balances stealthiness and effectiveness. More specifically, the challenge lies in discovering backdoor strategies for ViTs that do not rely on localized, patch-wise triggers, thereby advancing the subtlety and potential undetectability of the attack.

SECTION: 
We adopt the threat model consistent with numerous backdoor attacks on CNN as reported in the literature. Our approach involves generating triggered samples, which are mislabeled with the target class, and incorporating them into the original training dataset before releasing it publicly. A victim developer inadvertently introduces a backdoor vulnerability upon using this tampered dataset to train their model. It is important to note that the attacker is presumed to have neither control over the training process nor any knowledge about the specifics of the victim’s model. In some studies of ViT backdoor attacks, the threat modelassumes the training process or train dataset is available for attackers, which can not be applied in the real world. Our AGEB should have the following goals:

. The backdoor model should have high test accuracy of clean samples. The model should have high Clean Date Accuracy (CDA) in a threat model.

The triggered sample should be classified into the target class. In other words, the model should have a high Attack Success Rate (ASR).

The triggered sample should be hard to distinguish from the clean sample by human eyes.

SECTION: 
SECTION: 
Figureillustrates an overview of our triggered images generation process. The AGEB method is delineated into two distinct phases. Initially, the selection phase determines the pixels to be manipulated, utilizing a mask. This is achieved by evaluating whether the gradient of the last attention layer for each pixel surpasses a predefined threshold. Subsequently, the operation phase encompasses three critical manipulations: erosion of the images, mixing the eroded images with the original images alongside a distinct signal, and refining the images post-operation. For an in-depth elucidation of our approach, refer to Algorithm.

SECTION: 
To determine the pixels subject to erosion, we analyze the gradient of the last attention layer. Furthermore, we posit that different ViT models exhibit analogous attention weights for identical samples, as suggested by Yuan et al.. This similarity in attention weights implies that the attention gradients for the same sample across various ViT models may also exhibit congruence.

Consider the last attention layer’s output for a given pixel positionin the input sample. Letdenote the gradient of the loss function concerning the attention score for this pixel. Then, based on the chain rule,can be computed as follows:

whereis the loss function,is the attention weight for the pixel at position, andis the corresponding attention score. The gradient ofconcerningcan be calculated as mentioned in the earlier example.

This gradient,, is then used to update a maskof the same size as the input sample, where each pixel’s value inindicates whether the corresponding pixel in the input sample should be eroded or preserved. For instance, one might set a thresholdand updateas follows:

whereis the indicator function.

SECTION: 
Image erosion is a fundamental morphological operation. The basic idea behind erosion is to erode the boundaries of objects of foreground pixels.

For binary images, erosion is performed using a structuring element, a small shape, or a template applied to each image pixel. The central pixel is replaced by the minimum value of all the pixels under the structuring element. Mathematically, the erosion on a binary imageby a structuring elementcan be defined as:

wheredenotes the translation ofso that its origin is at. Ifis completely contained within the set of foreground pixels in, then the pixel atis set to the foreground in the output image; otherwise, it is set to the background.

Erosion to RGB images is applying the erosion operation independently to each of the three color channels (Red, Green, and Blue). For an RGB image, the erosion operation on each pixelcan be represented as follows:

for each color channel. This means the value of each color channel at pixelin the eroded image is the minimum value of that channel within the neighborhood defined by the structuring elementcentered at.

SECTION: 
We implement a post-erosion blending strategy to enhance the CDA and ASR. After performing the erosion operation on the images, we blend the eroded images with the original images at a specific ratio. This process is mathematically formulated as follows:

whererepresents the pixel value at positionin the modified image,is the pixel value at the same position in the eroded image,is the pixel value in the original image, andis the blending ratio.

To ensure that the eroded images do not become too insignificant for the model to learn due to their reduced values, we introduce a small bias termed asto each eroded image, guaranteeing a lower bound. This can be represented as:

whereis a small positive constant, andensures the adjustment is consistent with the pixel’s original value.

Following these adjustments, the modifications made to the image become more pronounced. To make it challenging for observers to distinguish the eroded areas, we further adjust the brightness and saturation levels of the eroded regions.
These enhancements are applied to ensure that the alterations remain subtle yet effective, balancing model performance improvement and visual discreteness.

SECTION: 
SECTION: 
Our backdoor attack is general for various ViT
models and datasets. Without loss of generality, we perform our evaluations over the CIFAR-10, GTSRB, ImageNettedatasets on ViT-b, Deit-t, Deit-smodels. All models have the same input dimensions as, and we reshape all input images with this dimension.

SECTION: 
To thoroughly assess the impact of our backdoor attacks, we employ several evaluation metrics that focus on the attack’s functionality-preserving capabilities, effectiveness, and stealthiness. These metrics are essential for understanding how the attack affects the model’s performance on clean data, the success rate of the attack, and the perceptual similarity to the original images. Specifically, we define the following metrics:

Measures the model’s accuracy on a clean dataset, i.e., a dataset not containing any samples with the backdoor trigger.

Determines the effectiveness of the backdoor attack by measuring the proportion of samples containing the backdoor trigger that are misclassified as the target class by the model.

SECTION: 
In the effectiveness evaluation, we demonstrate the efficacy of our approach against backdoor attacks on various datasets. As detailed in Table, our method consistently achieved high ASR across all models, with the Deit-s model on the CIFAR-10 dataset showing an exemplary ASR of 99.78%. Furthermore, the CDA remained robust, indicating that our method effectively balances attack resistance and data integrity.

Further validation is evident from the results presented in Figure, which detail the ASR and CDA across different poisoning rates on the CIFAR-10 dataset for other models. Notably, for ViT-b with a 5% poisoning rate, our method reaches a peak ASR of 99.85%, while CDA remains robust at 98.63% even with increased poisoning rates, underscoring the precision of our technique. In addition, for ViT-b with a 2% poisoning rate, our method still has an ASR of more than 95% and CDA over 98%. These findings underscore the strategic advantage of applying our gradient-focused erosion, affirming its superiority in enhancing model security against backdoor threats.

SECTION: 
Our ablation experiments provide compelling evidence of the effectiveness of our attention gradient-based erosion method. As shown in Table, directly targeting areas with high attention gradients for erosion significantly outperforms random pixel selection. The latter approach failed to improve the ASR or maintain CDA. However, by introducing a minimal, fixed signal (signal) and combining it with the original image (mix+signal), our attention-based method achieved the best outcomes, with an ASR of 94.78% and a CDA of 98.06%, which is a notable improvement over the baseline.

Furthermore, the straightforward application of image erosion presents two primary challenges: First, it diminishes vital semantic information encapsulated within the original images, which is detrimental to the model’s capacity to extract and learn crucial features of clean samples. Second, the success of erosion is highly dependent on the specific features of the images, such as pixel value similarities, which may lead to ineffective modifications that obstruct the model’s learning of the intended trigger’s features. To address the first issue, mixing the eroded segments with the untouched original image can retain critical semantic information, enhancing the model’s ability to assimilate essential features from clean samples. To overcome the second hurdle, blending a minimal, constant signal ensures the modification’s effectiveness across different images, facilitating the model’s consistent learning of the trigger’s features.

SECTION: 
We compare the difference between the original images and the triggered images generated by classic backdoor attacks (see Figure). Considering those backdoor attacksfor ViTs even ignore stealthiness, we chose to compare ours with the backdoor attack methods which are designed for CNN and known for stealthiness.

We observe a tiny difference between the original images and our triggered image, which we find hard to distinguish. Our method is more subtle.

SECTION: 
In our backdoor attack experiments, we meticulously chose hyperparameters to subtly balance ASR and CDA while minimizing perturbation visibility (see Figureand Table). The decision to target the top 40% of pixels by gradient values, use a kernel size of 3, and limit modifications to a single iteration was informed by our goal to achieve effective attacks with minimal detectability. This strategy ensures that perturbations are impactful and discreet, striking a critical balance for stealthy yet potent backdoor attacks. Our approach demonstrates a nuanced method to degrade model performance on targeted tasks without harming the accuracy of clean data.

SECTION: 
This study introduces a novel backdoor attack for ViTs that subtly erodes pixels with the maximum attention gradient as a trigger. The triggered images exhibit only minute differences from their originals, making them exceedingly difficult for human observers to detect. Our comprehensive experiments validate that our method operates effectively across various ViT architectures and datasets, emphasizing the dual benefits of our approach: the triggers’ inconspicuous feature and their global feature, which enhance both stealthiness and effectiveness. Besides, if AEGB can work other models like CNNs is also worth further exploration.

SECTION: Acknowledgment
This work is supported by the National Key R&D Program of China under Grant 2022YFB3103500, the National Natural Science Foundation of China under Grant 62020106013, the Sichuan Science and Technology Program under Grant 2023ZYD0142, the Chengdu Science and Technology Program under Grant 2023-XT00-00002-GX, the Fundamental Research Funds for Chinese Central Universities under Grant ZYGX2020ZB027 and Y030232063003002, the Postdoctoral Innovation Talents Support Program under Grant BX20230060.

SECTION: References