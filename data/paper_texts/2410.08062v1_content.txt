SECTION: High-redshift LBG selection from broadband and wide photometric surveys using a Random Forest algorithm

In this paper, we investigate the possibility of selecting high-redshift Lyman-Break Galaxies (LBG) using current and future broadband wide photometric surveys, such as the Ultraviolet Near Infrared Optical Northern Survey (UNIONS) or the Vera C. Rubin Legacy Survey of Space and Time (LSST), using a Random Forest algorithm. This work is conducted in the context of future large-scale structure spectroscopic surveys like DESI-II, the next phase of the Dark Energy Spectroscopic Instrument (DESI), which will start around 2029.
We use deep imaging data from the Hyper Suprime Camera (HSC) and the Canada-France-Hawaii Telescope Large Area U-band Deep Survey (CLAUDS) on the COSMOS and XMM-LSS fields. To predict the selection performance of LBGs with image quality similar to UNIONS, we degrade theandbands to UNIONS depth.
The Random Forest algorithm is trained with theandbands to classify LBGs in therange.
We find that fixing a target density budget ofdeg-2, the Random Forest approach gives a density oftargets ofdeg-2, and a density ofdeg-2of confirmed LBGs after spectroscopic confirmation with DESI. This UNIONS-like selection was tested in a dedicated spectroscopic observation campaign of 1,000 targets with DESI on the COSMOS field, providing a safe spectroscopic sample with a mean redshift of 3. This sample is used to derive forecasts for DESI-II, assuming a sky coverage of 5,000 deg2. We predict uncertainties on Alcock-Paczynski parametersandto be 0.7and 1for, resulting in a 2measurement of the dark energy fraction. Additionally, we estimate the uncertainty in local non-Gaussianity and predict, which is comparable to the current best precision achieved by Planck.

SECTION: 1Introduction

Lyman Break Galaxies (LBGs) are young and actively star-forming galaxies at. LBG spectra display a decrement of flux bluewards the Lyman limit at 912 Å (rest-frame), given by the infinity end of the Lyman series spectral lines. This flux reduction is due to neutral atomic hydrogen that can absorb photons with that energy or more. The LBG spectra also display a Lyman alpha break at a wavelength shorter than 1216 Å (rest-frame), due to absorption by intervening neutral hydrogen clouds along the line of sight.

LBGs appear to be promising tracers for studying the spatial clustering at high redshifts with the Dark Energy Spectroscopic Instrument (DESI,[1]). The DESI survey started in 2021 and has already provided millions of redshifts for different tracers with unprecedented precision up to, allowing to study the nature of dark energy and dark matter through Baryon Acoustic Oscillation and Redshift-Space Distortions[2,3]. DESI-II, the next survey campaign of DESI will start around 2029. DESI-II aims to collect 40 million redshifts at higher densities and at higher redshifts than DESI[4]to address the problems of dark energy and inflation[5,6]. LBGs atare of particular interest for DESI-II, expected to represent2.5 million reconstructed redshifts. Optimized target selections for these high redshift tracers over the DESI footprint can be achieved by using completed and forthcoming multi-band and wide imaging surveys, including the DESI Legacy Surveys such as the Dark Energy Survey (DES,[7]) and the Dark
Energy Camera Legacy Survey (DECaLS,[8]), but also the Ultraviolet Near Infrared Optical Northern Survey111https://www.skysurvey.cc/(UNIONS,[9,10,11]), the Legacy Survey of Space and Time of the Vera C. Rubin Observatory222https://rubinobservatory.org/about(LSST,[12]), and theEuclidmission333https://www.esa.int/Science_Exploration/Space_Science/Euclid[13], which will deliver imaging data in various optical and near-IR bands, of unprecedented depth and area for this purpose.

This paper aims to explore a possible target selection of high redshift LBGs for DESI-II based on the imaging of existing wide photometric surveys, such as the Canada-France Imaging Survey-band (CFIS,[14]), DECaLS (,, and) as well as the ongoing UNIONS (,,,and). Our work is focused on UNIONS, but similar strategy could be adopted with the Rubin LSST that will provide,,,,and-bands.

InSection2we present the different datasets that are, or will be, available for LBG selection on large footprints promised by wide photometric surveys. We also present a method to degrade existing deep photometry to shallower depths to simulate the properties of future wide imaging surveys.Section3presents the Random Forest method used in this work to select LBGs from multi-bandphotometry, which we compare to the more standard color-color box selection used in[15]. InSection4, we first present how we create a sample of LBG targets from simulated photometry at UNIONS depth on the COSMOS field. First, we use a method to degrade existing deep photometry to shallower depths to simulate the properties of future wide imaging surveys. Then, we validate the degradation method by studying the performance of a Random Forest-based target selection using CFIS+DECaLS imaging available on the XMM-LSS field.
Then, we create a sample of UNIONS-like LBG targets on the COSMOS field and describe the sample of LBGs observed by the DESI pilot survey.
We present inSection5several forecasts on the precision of several cosmological parameters that can be achieved with this new selection and future spectroscopic surveys. We conclude inSection6.

SECTION: 2Optical photometry from imaging surveys

This section presents current and future photometric surveys providingimaging. TheTable1summarizes the-band depths for the surveys discussed in this paper. Our study relies on the COSMOS and XMM-LSS fields, which are both covered by the dataset we use hereafter (CLAUDS, HSC, DECaLS, and CFIS). These fields are well-studied areas of the sky to bench test target selections for new tracers.

SECTION: 2.1Deep imaging surveys

The Hyper Suprime Camera (HSC,[16]) is mounted on the 8-meter Subaru Telescope at the Mauna Kea Observatory in Hawaii. The last Public Data Release 3 (PDR3) of HSC Subaru Strategic Program (HSC-SSP) in 2021 includes deep multi-band data over 36 deg2. The 5point source magnitude depths listed inTable1are taken from Table 1 in[16](Deep/UltraDeep). In this work, we focus on the HSCdata which were combined with CLAUDS (CLAUDS is detailed hereafter) on deep (XMM-LSS, COSMOS, ELAIS-N1, DEEP2-F3) and ultra-deep (E-COSMOS in COSMOS, and SXDS in XMM-LSS) fields, over 26 and 4 deg2, respectively. The combination of CLAUDS and HSC-SSP data over the XMM-LSS and COSMOS fields is detailed in[17]. We will use a combination of deep and ultra-deep fields, and from[17](see their Table 2), the magnitude depths for the-bands on XMM-LSS range from 26.5 to 24.0 (resp. from 26.9 to 24.9) for the deep (resp. ultra-deep) field. Moreover, the magnitude depths for the-bands on COSMOS range from 26.4 to 24.7 (resp. from 27.0 to 25.5) for the deep (resp. ultra-deep) field.

HSC was supplemented by the U-band imaging from the Canada-France-Hawaii Telescope Large Area U-band Deep Survey (CLAUDS,[18]). CLAUDS used the MegaCam imager mounted on the 3.6-meter CFHT telescope to provide very deep-band imaging (depth inof 27.1 and 27.7 AB in the deep and ultra-deep fields, respectively)
overdeg2. CLAUDS was designed to follow the HSC
deep fields to similar depth, astrometric solution, and pixel scale. Moreover, CLAUDS-imaging uses the original MegaCamfilter and an upgradedfilter installed in 2014, which has better throughput and covers the entire MegaCam mosaic of 40 CCDs (compared to 36 with the original filter). The different characteristics of theandfilters are detailed in[18](see their Figure 2) and filter parameters are available on the MegaCam filter database444In the table athttps://www.cfht.hawaii.edu/Instruments/Filters/megaprime.html,is labeled as, andas.. The XMM-LSS field was imaged with thefilter, while bothandfilters were used for the COSMOS field. The 5point source (in 2 arcsec apertures) magnitude depths listed inTable1are taken from[18].

SECTION: 2.2Wide imaging surveys

The Dark Energy Camera Legacy Survey (DECaLS) is one of the three public projects of the DESI Legacy Imaging Surveys555http://legacysurvey.org/[19]with the Beijing–Arizona Sky Survey (BASS) and the Mayall z-band Legacy Survey (MzLS), that aim to provide the targets for the DESI survey over 14,500 deg2. DECaLS makes use of the Dark Energy Camera (DECam[20]) on the Blanco 4-meter telescope (in Cerro Tololo, Chile) that was initially built to conduct the Dark Energy Survey, to provide the optical imaging inbands over 9,000 deg2. Through this paper, we use the data release 9 (DR9) of the Legacy Imaging Surveys and whose magnitude depths are shown inTable1.

UNIONS is a collaboration between the Hawaiian observatories Canada-France-Hawaii Telescope (CFHT, Mauna Kea), the Panoramic Survey Telescope and Rapid Response System (Pan-STARRS, Maui), and the Subaru telescope (Mauna Kea). UNIONS is currently providingimaging for 5,000 deg2of the northern sky. First, the CFHT Canada-France Imaging Survey (CFHT/CFIS) is targeting the-band and-band with the Megacam imager at CFHT and will provide competitive image quality to all other current, large, ground-based facilities, up to a magnitude depth of 25 for the-band over 5,000 deg2and to a-band depth of 24.6 over 9,000 deg2. At the same time, Pan-STARRS is obtaining the-band, and the Wide Imaging with Subaru Hyper Suprime-Cam of the Euclid Sky (WISHES) will provide the-band. UNIONS has already completed more thanof the survey, and the first data release was available on the XMM-LSS field as we ended this manuscript. So the 5-6 year program of DESI-II that will start in 2029 could benefit from these datasets in the Northern sky. Expected magnitude depths to be reached by the ongoing UNIONS in the,,,, andbands are listed inTable1(Gwynet al., in prep.),[21].

Moreover, the XMM-LSS field was observed with a strategy strictly identical to CFIS-. These observations on XMM-LSS aim at testing LBG selection at a shallower depth than CLAUDS (2-3 magnitudes deeper), to extend the LBG science program over the extensive CFIS-sky coverage of several thousands square degrees. In the latter, observations of XMM-LSSà-laCFIS will be referred to as CFIS observations for simplicity.

The Vera C. Rubin Observatory, still under construction, is designed to conduct a 10-year wide-area, deep, multi-band optical imaging survey of the night sky visible from Chile. The Legacy Survey of Space and Time (LSST[22]) will catalog about 20,000 deg2of the southern sky starting in 2026 in thebands. After 1 year and after 10 years of observations, the survey will reach respective magnitude depths666The 5-sigma point source depths for LSST are reported from the latest Rubin simulation v3.6, whose details can be found athttps://usdf-maf.slac.stanford.edu/as indicated inTable1.

SECTION: 2.3Simulating shallower depth

In this section, we present a method to degrade the imaging of a deep survey to mimic a shallower imaging. This step is mandatory since we want to test the LBG selection at UNIONS-like depth, whose images are still being processed. In this section, we make use of the different imaging datasets available on the XMM-LSS field, namely CLAUDS+HSC imaging (the deep survey) that we will degrade to CFIS+DECaLS-depth (the shallower survey), and compare to the existing CFIS+DECaLS dataset on XMM-LSS. Let us note that XMM-LSS was not surveyed by the CLAUDS-filter but with the CLAUDS-one, so we consider CLAUDS-in this section but will use CLAUDS-on COSMOS.

We first define the typical magnitude error, for a given flux and magnitude depth. The magnitude erroris linked to the errorin the flux measurement via777Using the galaxy AB magnitude., where. We can adopt a simple model for the flux erroras being roughly constant[23], and linked only to the magnitude depth(detection) of the survey. For a galaxy with index, the error on the measured flux is given by

withfor a given band. Usingfor an arbitrary magnitude, the magnitude error modelbecomes

More complex models for the magnitude error can be used, as proposed in[24,25], where the magnitude error depends both on the magnitude depth and the effective exposure time (we define the exposure time inSection4.2). To degrade the depth, we first randomize the galaxy fluxfrom the deep imaging to, such as

where the additive flux variancefor the-th galaxy is given by

whereandare given inEq.2.1, respectively with the ”shallow” and ”deep” magnitude depth with, and

whereandare obtained fromEq.2.2, andis the measured magnitude of the deep imaging. The degraded magnitude is, and its error is

The first term is the measured magnitude error (corresponding to the deep imaging), the second term shifts the error to a higher value if the magnitude, according to our error model, and the third term shifts the
magnitude error to a higher value due to the difference in depths since. This magnitude error correction ensures that the new magnitude error follows on average the error model inEq.2.2for the new magnitude depth. So this method can be used on a homogeneous deep field (input) to simulate shallower imaging (output), from which we can compute the randomized fluxes via the additive noise fromEq.2.4. We point out that those equations work for point sources (i.e. the scaling between two different depths), since object size could play a role.

We can apply this methodology to degrade CLAUDS+HSC to a CFIS+DECalS depth. CFIS+DECaLS magnitude depths for(taken from the literature) are listed inTable1. For the three datasets, we use a cut on the magnitude errorin all bands and we only consider galaxies within. Such magnitude cuts in the-band are motivated by (i) the fact that most LBGs at a redshiftare above a-band magnitude of 23, then the lower limit cut removes stars and other galaxies (ii) the fact that the efficiency of the spectroscopic redshift measurement for very faint objects (with a-band magnitude higher than 24.3) is low[15]for a typical exposure time of 2 hours (again, the details on effective time and spectroscopic redshift measurement are presented hereafter inSection4.2).

The magnitude depths inTable1are indicative only and do not take into account possible inhomogeneities in the survey depth over the XMM footprint for CLAUDS, HSC, CFIS, and DECaLS. To illustrate this, we reconstruct the two-dimensional depth map of the CLAUDS-band imaging using two different methods. First, using our error model inEq.2.2, we convert each measured magnitudeand its erroron an estimate of the survey depthin the considered band. The binned two-dimensional map of magnitude depth estimates is represented inFig.1(left panel). We see three different regions (the ultra-deep region in one of the HSC pointing, the deep region, and the edges with shallower depths, see Figure 3 in[18]).

The method detailed above relies on a given error model for the measured magnitude. For the second method, we can estimate the magnitude depth independently of our error model, by measuring the binned map of measured magnitudes that have a flux signal-to-noise ratio in the-band between 4.5 and 5.5. The corresponding binned average map of these magnitudes is shown inFig.1(right), displaying the same features as our previous method, however noisier since we use fewer galaxies.

InFig.2(left panel), we show in blue the histogram of the depth estimates from our first method over the full XMM footprint. We see three distinct features in the distribution, represented by the vertical lines (positioned by hand), given by. Similarly, we show in green the histogram of object magnitudes with flux signal-to-noise ratio between 4.5 and 5 in the-band, displaying the three same features.

InFig.2(right) we show the magnitude error against magnitude for the-band, and we over-plot the three error models using respectively. Again, we see that the error models follow the three distinct patterns in the error-magnitude distribution, revealing that a single error model with a uniqueis not relevant in this case.

For that reason, for the rest of the analysis with CLAUDS+HSC data, we restrict to the central 2 deg2region of the CLAUDS XMM-LSS imagingFig.1, compared to the full field ofdeg2. The histogram of the-band depth estimates in this restricted field is displayed in orange inFig.2(left panel). In this restricted area, the recovered magnitude depths for CLAUDS+HSC are presented inTable2(third line),
showing a deeper average-band magnitude than the full XMM footprint (second line). After these cuts, the final CLAUDS+HSC dataset comprises 41,000 objects.

Second, we similarly want to restrict to a homogeneous footprint in depth in the CFIS+ DECaLS imaging. The CFIS observations, coming from a dedicated program isolated from the CFIS footprint, have non-negligible edges with shallower coverage: we reject regions covered by one CFIS exposure only (we keep the number of exposures to be equal to 2 or 3) over the XMM footprint. After these cuts, the CFIS+DECaLS dataset comprises 71,000 objects. The recovered magnitude depths after applying this cut are given in the first line ofTable2.

After such a treatment and considering input (i.e., CLAUDS+HSC) and output (i.e., CFIS+DECaLS) depths inTable2, we can proceed to the degradation of the CLAUDS+HSC magnitudes. We show inFig.3(left panel) the
magnitude error as a function of the-band magnitude for the three different datasets: the input CLAUDS+HSC in blue, the output CFIS+DECaLS-like (i.e. degraded CLAUDS+HSC imaging) in green, and the targeted imaging CFIS+DECaLS in blue. The error models with depths fromTable2match well the true CFIS+DECaLS magnitude errors.
InFig.3(right panel), we show the-band magnitude distribution of the three different datasets, after applying a-band flux signal-to-noise ratio cut, represented by the back dashed line inFig.3(left panel). The similarity between the CFIS+DECaLS-like and CFIS+DECaLS()-band magnitude distributions validates that the degradation method works as expected. We repeated the same procedure on the CLAUDSbands and found the same conclusions when comparing them to true CFIS+DECaLS imaging.

SECTION: 3Selection methods

This work investigates the feasibility of selecting high redshift LBGs from their imaging properties with a Random Forest algorithm. We also aim to compare Random Forest selection with the more common color-color box selection method used in the literature.
This section uses CLAUDS+HSC deep field imaging data and photometric redshifts obtained from CLAUDS and HSC-SSP imaging data[17]with theLePHAREtemplate-fitting code[26,27,28].

SECTION: 3.1Color-color box selection

The color-cut selection method for selecting high redshift LBGs uses the flux decrement blueward of the Lyman limit of LBG spectra in their rest frame, due to absorption by neutral hydrogen. High redshift LBGs are then selected for their lack of emission in the-band, compared to their observed flux in other bands[15]. This-dropout method allows LBGs to be selected in the redshift range. Similarly, the-dropout (lack of flux in the-band) can be used to select LBGs at redshift. In this section, we use the color-color box selection referred to as [COSMOS: TMG-dropout] in[15](see their Table 1), which is given by:

whereis the uncertainty on the-band magnitude.Fig.4(left panel) shows thecolor-color plot from a fraction of the CLAUDS+HSC imaging. The blue box represents the color-color cut selection and provides a target density ofper deg2. The selection was applied to the entire COSMOS dataset, and the photometric redshift distribution of the target density is represented in blue inFig.4(right panel) and has a mean redshiftwith a total target density ofLBGs given by946 deg-2.

SECTION: 3.2Random Forest

Machine Learning methods have proven to improve target selections for spectroscopic surveys compared to standard methods[29,30,31]. In this work, we use a Random Forest888We use the scikit-learn implementation available athttps://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html.(RF) algorithm[32]that can be used in the context of target selection for both regression tasks (i.e. trained to predict the redshift of a test galaxy from its imaging properties) and classification problems (i.e. to predict theclassof a test galaxy). In this work, we consider using RF classification to select galaxies in a given redshift interval.

A Random Forest is a supervised, ensemble learning algorithm based on a set ofdecision trees. A decision tree is a flowchart-like tree structure where each internal node represents a feature, the branch represents a decision rule, and each leaf node represents the outcome. In a Random Forest algorithm, each-th tree is built by drawing a random selection of training data(taken by replacement) and a random selection of training data features (colors, magnitudes, morphology, etc.) plus associated classes. In this work, classes identify training objects in a dedicated redshift interval, namely

After training, each-th tree returns a probability(resp.) for a new data setto have(resp. to have). The average probability is defined as

For a new test sample denoted by, its imaging properties (e.g. measured photometric properties of a galaxy) are queried down the forest of trees and the prediction is obtained by aggregating thetree predictions in the average probabilityinEq.3.7. The final classificationof a test sample is given by

whereis an RF quality threshold, and is a free parameter that can be chosen to match a given target density budget. The hyper-parameters of an RF algorithm are the number of trees and nodes in each tree. An RF can then (i) model complex non-linear relationships between features and target classes (ii) reduce over-fitting, a recurrent problem in classification by decision trees, and reduce prediction variance compared to a single decision tree. RF-based selection learns a more complex relation between the observed features and redshift, especially when characteristic LBG spectral features (i.e. the two Lyman breaks atandÅ, rest-frame) are observable at optical wavelengths at.

The Random Forest training sample is obtained by using the magnitude cutand(to be consistent with the color-color cut example). After these cuts, the total dataset is about 180,000 objects. The number of objects to be used for training is discussed in the next section. We consider the 4 colors constructed from the CLAUDS+HSCmagnitudes. To decide the object classes inEq.3.6, we choose, where we use photometric redshifts fromLePHARE. In this redshift range, the spectral features of LBGs appear in the optical bands. Moreover, the spectroscopic redshift measurement is more efficient at, than in[15](we detail this method inSection4.2). From this approach, our first aim is to identify objects within the redshift range offrom their imaging properties, without explicit mention of their type (LBGs, Quasars, etc.). Then, the minimization of thecontamination(we define this performance metric hereafter) by objects other than LBGs is not a primary goal of our Random Forest approach. In a Random Forest perspective, inspired by the-dropout technique, we rather expect to find a high correlation between colors and redshift for LBG specifically in this redshift range, to be learned by the algorithm. One could improve the classification by training with other target characteristics, such as morphology, that can help mitigate the contamination from point-like objects, for instance.

The algorithm uses the training data to learn the relationship between the features and the target classes. The test data are used to evaluate the performance of the model. Using a test sample from which we know the true classification, we define two parameters to evaluate the performance of the Random Forest classification. For a given threshold, the number of selected targets of the test sample is given by. The number of selected LBG is denoted by(in this section, we refer to as LBG the galaxies within the redshift range) andis the number of galaxies in the test sample that are in. We define thepurityand theefficiencyrespectively given by

where. The purity denotes the fraction of selected galaxies that are effectively in the desired redshift interval. The efficiency denotes our ability to select the parent distribution of galaxies in the considered redshift interval.

First, using purity/efficiency as performance metrics for the RF training, we investigate, at CLAUDS depth, for which training dataset size,, the RF performance becomes stable. In the following, we also refer to the fraction of the total dataset used for training. We use the remaining data for testing for the different. For different, we show inFig.5(left panel) the purity against efficiency whereruns from 0.1 (lower right part of the lines) to 0.8 (upper left part of the lines). We see that using a few hundred objects for training results in poorer performance (i.e., lower purity and/or efficiency) compared to using a few tens of thousands of objects. From(a few tens of thousands of objects) to, the RF performance is stable.

Then, we want to test the precision of our RF regression, i.e. the accuracy of the RF prediction after the resampling of the training sample. First, we defineindependent splits, each one containingobjects. For instance,Fig.5(left panel) shows the RF performance metric
measured of a single split, for different fractions of the training sample(colors). We have repeated the training of the RF algorithm using each independent-th split as the training dataset, and we tested the algorithm with the remaining-1 splits. For each independent-th training, we have computed the performance metrics. This method should not be confused by the ”-fold” cross-validation technique, where each fold (or split) is used once as a test sample while theremaining folds form the training set. Themeanpurity against themeanefficiency is displayedFig.5(right panel) for differentvalues. Foror, there areandindependent splits, respectively. For both cases, we measured the mean performance and the dispersion with only 30 splits rather than using all splits (we have tested using 50 to 100 splits, and the results are similar). We see that for these two cases, the performance is poorer and the dispersion over theindependent splits is large. When considering larger fractions of the dataset used for training, such as from(from which we can construct 5 independent splits) to(2 splits), we have checked that each-th RF
performance is dispersed by less thanaround the mean performance (note that for, they are respectively 5 and 2 independent splits) and that there is less than 1difference between usingor(namely, using fromtoobjects for training).

Alternatively, we can also look for the features used for training that most affect LBG classification. The importance features metric for the RF implemented inscikit-learnis based on the Gini importance, which counts the times a feature is used to split a node after the Random Forest is trained, weighted by the number of samples it splits. A feature with a higherimportancevalue is more discriminating in the classification than a feature with a lower importance value. The feature importances are shown inFig.6(left panel) for the four features used for training, namely the colors,,, and. Our baseline is to classify galaxies within the range, for which the corresponding importance is shown in blue. We see that theandcolor play a key role in LBG selection since the first
measures the-dropout forgalaxies, and the second encodes more complex correlations between color and redshift that the RF algorithm has learned from data. We can test the RF regression when modifying slightly the LBG classification by including lower redshifts, namely using the interval. We see that with this setup, displayed in orange inFig.5(left panel), feature importances are not enhanced/decreased at higher/lower redshift. In the following, we will
continue with the baseline.

We useinEq.3.8to select LBGs, to match the color-color box selection target density999The target density for the RF approach is calculated as, whereis the surface area in deg2, to ”remove” the test sample from the target density estimation, whereas it is given asfor the color-color box selection.of 1290 deg-2. This gives a purityand an efficiency. The LBG targets selected by our Random Forest algorithm are represented in red inFig.6(right panel). Moreover,of RF-selected targets are also selected by the color-color cut method, and reversely.Fig.4(right panel) presents the corresponding photometric redshift distribution in red, compared to that of the color-color box selection in blue. We find that the RF selection gives a mean redshift, and a target densitydeg2so providing an improvement of 100 deg-2compared to the color-color box selection and a higher mean redshift by 0.1.

The LBG selection we have presented above is based on CLAUDS+HSC imaging on the COSMOS deep field, offering deep-band imaging with a depth larger than 27. By the end of 2027, CFIS and the Rubin LSST will provide a 5-sigma point source depth in the-band of 24.1 over a few thousand square degrees. By 2035, with 10 years of LSST, we can anticipate a depth in theband of around 25.6.

SECTION: 4UNIONS-like LBG selection with a Random Forest algorithm

SECTION: 4.1Validation with CLAUDS imaging degraded to CFIS depth on XMM

We have presented inSection2a method to degrade magnitudes to shallower depth. We also have illustrated this method by degrading CLAUDS()+HSC() magnitudes towards CFIS()+DECaLS() depths on the XMM-LSS. In this section, we compare the corresponding Random Forest performances for selecting LBGs when using a CLAUDS+HSC-to-CFIS+DECaLS degraded dataset on XMM-LSS.

We train a Random Forest classifier on three different configurations, first based on CFIS+DECaLS imaging, second on CLAUDS+HSC imaging, and third on the CFIS+DECaLS-like imaging obtained by degrading CLAUDS+HSC data. For the CFIS-DECaLS training dataset, since we want to compare with the CFIS+DECaLS-like case, we use a cut on the-band number of exposures introduced earlier to estimate the-band depth. This ensures that the magnitude depth of the CFIS+DECaLS data is the same as the CFIS+DECaLS-like data for a fair comparison.

For consistency, the three RF algorithms have been trained on the three configurations101010Again, the three configurations are (i) CLAUDS+HSC (ii) CFIS+DECaLS (iii) CFIS+DECaLS-like, where the latter is obtained by degrading CLAUDS+HSC photometry to a shallower depth. Each RF algorithm is trained on-bands.usinggalaxies. For CFIS+DECaLS, we consider the full XMM footprint since the cut on the number of exposures removes too many galaxies. For CLAUDS+HSC and CFIS+DECaLS-like, we use the data within the field represented by the white box inFig.1. As a result, fewer galaxies (6,000) are used for testing in the CLAUDS+HSC and CFIS+DECaLS-like cases, compared to CFIS+DECaLS (35,000).

We trained the RF algorithms using two different sets of features, first considering the 3 colors derived from the optical bands, and second, adding the near-IR bandsandprovided by WISE[33]. We use onlysince the-band was in fact observed by DES but not included in the data release 9 of the Legacy Surveys.

Fig.7(left panel) shows the Random Forest purity as a function of the obtained LBG target density111111Since the three configurations were not trained on the same sky area, target densities are given by, whereis the fraction of the dataset used for testing (that is different from a configuration to another) andis the sky area (either 4.13 or 2 deg2)., first by training only with the opticalcolors (full lines) and second by adding the two infrared magnitudesand(dashed lines). The optimistic scenario using the CLAUDS+HSC photometry is displayed in red, whereas the CFIS+DECaLS scenario is in blue and the training from the CFIS+DECaLS-like dataset is in green. For clarity, we only show the error bar on the purity for thecase, but they are similar in thecase. We see that the larger number of galaxies used for testing in the CFIS+DECaLS case gives much smaller error bars (in blue).

FromFig.7(left panel), for the same target density ofdeg-2(dashed vertical line inFig.7, left panel), CLAUDS+HSC gives a purity of 0.5 when CFIS+DECaLS provides around 0.3. The RF performance when trained on the CLAUDS-degraded dataset fairly reproduces the CFIS+DECaLS behavior as a function of the target density for thecase but provides slightly better performance
when usingand. The difference is not meaningful if errors are taken into account.

Once fixing the target density todeg-2for all selections,Fig.7(right panel) shows the photometric redshift distribution of LBG targets for our three configurations. The CLAUDS+HSC optimistic case is represented in red, the CFIS+DECaLS-like is represented in green mimics fairly well the targeted one (in blue).

Keys numbers such as the mean redshift and density on different redshift ranges for these three selections are given inTable3(first three columns). First, restricting to(i.e. the redshift range used for training the RF), the mean LBG redshift and LBG density121212Let us note that the number of LBGs selected in the redshift range considered for the Random Forest algorithm (i.e.) are obtained bydegdeg-2, whereis the purity.are labeled asand, respectively. In practice, we would possibly use the LBG sample starting from, we indicate the LBG densityfor different value of.

Moreover, we have tested the impact of cuts applied to data on the RF performance. We have tested different magnitude error cuts (fromin all bands to higher values, and for all datasets), different numbers of exposures for the CFIS-band imaging (changing the output magnitude depth inEq.2.4) or the maximum-band magnitude for all datasets. All of these changes affect the training samples for each dataset, and thus RF performances. These tests gave at most a 10difference with the baseline that we presented above. We also mention that the error model we use considers that the data are in the sky-limited noise regime, and does not account for specific systematics in the calibration for the input and output surveys, in addition to the possible difference in shape and response between the input (CLAUDS+HSC) and output (CFIS+DECaLS) photometric pass-bands.

In this section we have presented the Random Forest classification of LBGs using degraded imaging data from CLAUDS-HSC imaging on XMM-LSS, to mimic a shallower depth survey (CFIS+DECaLS). We demonstrated that the performance of the RF we get using the simulated dataset compares fairly to the ones obtained using the true dataset, thus validating our approach of mimicking shallower photometry from deep photometry.

SECTION: 4.2LBG selection from UNIONS-like imaging on COSMOS

In this section, we degrade CLAUDS+HSC imaging on COSMOS to UNIONS depths (seeTable1), and we train a Random Forest classifier to extract a list of LBG targets. We also study the impact of spectroscopic redshift efficiency on the recovered photometric redshift distribution.

UNIONS depths from literature are summarized inTable1. To degrade the CLAUDS+ HSC imaging on the COSMOS field, we use input depth in the first and second lines inTable1and UNIONS-like output depth still inTable1. Since the CLAUDS+HSC depths are not strictly equal to the ones in the COSMOS field (COSMOS CLAUDS+HSC depths are listed in the 4-th line inTable2), the effective depths that we use to obtain the UNIONS-like dataset are slightly different of a few percent and are listed inTable2.
We use the
method ofSection2to degrade CLAUDS photometry to UNIONS depths and then train the RF with the complete set ofdegraded data. The full dataset after a cut in the-band given byand errin all bands gives 280,000 objects, where 55,000 are used for training and the remaining (80) is used for testing. Curves of purity as a function of target density for UNIONS-like and CLAUDS-based classifications (full lines) are shown inFig.8(left panel). As expected, the UNIONS-like selection provides poorer results than CLAUDS+HSC, going fromtofordeg-2(vertical dashed line). Let us note that CLAUDS+HSC purity is significantly enhanced compared to the one measured inSection4.1(seeFig.7, left panel) using CLAUDS+HSC photometry on XMM-LSS. Indeed, we have found that CLAUDS+HSC-band are deeper than those measured on XMM-LSS (seeTable2). Moreover, compared to the RF classification on XMM-LSS, we add the-band to the existingbands on COSMOS. It does not impact our degradation method, since the updated UNIONS-like magnitudes are computed with input CLAUDS+HSC photometry on COSMOS to reach the desired depth inTable2. Fixing a target budget ofdeg-2, the photometric distribution of UNIONS-like targets are shown in blue inFig.8(right panel).

In addition to the Random Forest performance, the efficiency of the spectroscopic redshift measurement needs to be accounted for to compute the final spectroscopically-confirmed LBG density. This work is conducted in the context of the next phase of the Dark Energy Spectroscopic Instrument (DESI, already introduced) that is installed on the Mayall 4-meter telescope at Kitt Peak National Observatory. DESI is a multiplexed instrument capable of taking spectra of 5,000 objects simultaneously[34,35]. For the spectroscopic redshift determination from LBG spectra, we consider the method presented in detail in[15]who investigated the feasibility of a color-cut LBG target selection with CLAUDS and HSC imaging, supplemented by several dedicated spectroscopic observation campaigns, two on the COSMOS field in 2021 and 2023, and one on the XMM-LSS field in 2022. First, a convolutional neural network (CNN) derived from QuasarNET131313QuasarNET is the algorithm used for QSOs in DESI as part of the spectroscopic classification pipeline.[36]is applied to each DESI spectrum to perform successively a classification (LBG type or not) and a redshift regression task. The training of the CNN is based on the identification of 14 absorption lines and two emission lines. For each emission/absorption line, the CNN returns a confidence level (CL). The 16 CLs are ranked in decreasing order, and a spectrum is declared as classified as an LBG by CNN if the fifth CL exceeds a given threshold. The CNN output redshifts correspond to the maximum CL. A more precise LBG redshift is then obtained using the RedRock (RR) software141414https://github.com/desihub/redrock[37]that uses the CNN output redshift as a prior and refines its measurement. In this analysis, LBG-specific templates are used for RR, created from 840 visually inspected LBG spectra from the DESI pilot survey of the XMM-LSS (see the full details of the CNN and RR architectures in[15]).

For the color-color box selection from CLAUDS photometry used in[15]and for a CNN CL threshold of 0.97, the
redshift determination efficiency of the CNN+RR procedure (the number of CNN classified LBGs divided by the number of proposed targets) was found to vary with redshift, from 25at redshift 2.1 up to a plateau of 80between redshift 3.0 and 3.4, for a fiber exposure effective time151515Exposure time refers to the amount of time that a single DESI fiber collects light from a celestial target during an observation. An effective time of 4 hours means that spectra from different exposures were co-added for a total effective time of 4 hours. More technically, effective exposure time denotes the amount of time necessary to reach a certain uncertainty in ’nominal’ observational conditions for DESI, defined to be a 1.1” seeing, a sky background of 21.07 AB magnitude per square arc second in-band, photometric conditions, observations at zenith, through zero Galactic dust reddening[38].of 4 hours (see their Figure 18, right panel in[15]). These results from[15]are shown as dashed lines inFig.8(right panel), in orange for an effective time of 4 hours and in green for 2 hours, as a function of the photometric redshift. The drop in efficiency at low redshift originates from different effects, such as the difficult redshift determination for non-emitting LBGs, combined with a possible evolution effect of the LBG population which would disadvantage emitters at redshifts lower than 3. At fixed redshift, the spectroscopic efficiency depends slightly on the effective time.

The redshift determination efficiency degrades the overall performance of the RF LBG selection, as shown inFig.8(left panel), where we define theeffectivepurity

that is now the fraction oftrueLBGs selected from the RF classification, and whose spectroscopic redshifts are successfully reconstructed from our CNN+RR approach. Here above,is the spectroscopic redshift efficiency evaluated at the object’s redshift. The effective purity is displayed as dotted-dashed lines for 4 hours of effective time and as dashed lines for 2 hours of effective time. After convolution with the redshift determination efficiency curves (assuming that these also apply to our case), we obtain the photometric redshift distributions in orange inFig.8(resp. green) for an effective time of 2 hours (resp. 4 hours). We see that the spectroscopic efficiency lowers the LBG population at lower redshift, and the effect is as important as the effective time is small.

Since the LBG spectroscopic redshift reconstruction is limited belowdue to spectroscopic redshift estimation, we investigate the key numbers for the selected sample at, which are presented inTable3(last column). The numbers without parenthesis are obtained after RF selection only, whereas the numbers in parenthesis are obtained after applying the spectroscopic redshift efficiency (2 hours of effective time). From an initial budget ofdeg-2we can have a LBG density ofdeg-2with confirmed redshifts at.

SECTION: 4.3Pilot observations of COSMOS with DESI

DESI has completed a pilot survey on COSMOS in 2024 to test the UNIONS-like LBG selection. We proposed a density budget of 1,100 deg-2, and a total of 1,000 targets were retained for the spectroscopic follow-up by DESI. Galaxy spectra were measured with an effective time of 2 hours. In this section, we present the results of this pilot survey, i.e. the spectroscopic redshift distribution of the targeted LBGs.

The photometric redshift distribution of the 1,000 DESI targets is represented in blue inFig.9(left panel), within the redshift range of interest. After spectroscopic redshift determination with the CNN+RR procedure described inSection4.2, the photometric redshift distribution of the final sample, corresponding to a CNN confidence level CLis represented in red. This provides a mean redshift. The fraction of secure LBGs over thesample of DESI targets is 0.44, a bit lower than the expected ratio from lastSection4.2which is 493/873=0.56, with spectroscopic redshift efficiency taken from[15]. This difference can be seen inFig.9(right panel), since our measured spectroscopic efficiency per redshift bin is slightly lower.

To obtain the black dashed line, we use the photometric redshift distribution of the full sample of RF-selected LBG targets provided to DESI (i.e. withdeg-2) and apply the CNN+RR spectroscopic redshift efficiency for 2 hours of effective time described inSection4.2. This distribution has a mean redshift. Rescaling this distribution to the size of the CNN CLsample (red histogram) produces the black dashed line. The agreement between the two is very good.

The total CNN efficiency for this RF-based LBG selection is represented in blue inFig.9(right panel) as a function of the photometric redshift, showing roughly good agreement with the CNN+RR efficiency described inSection4.2for 2 hours of effective time (orange),
though slightly lower.

Fig.10(left panel) shows the photometric redshifts as a function of the CNN+RR spectroscopic ones, for the CNN CLsample. The spectroscopic redshift distribution for the CLsample is represented inFig.10(right panel). The mean redshift is.

SECTION: 5Forecasts

In this section, we present forecasts on the Alcock-Paczynski (A.P.) parameters, the fraction of dark energy, and local Primordial non-Gaussianities (NG) from the LBG redshift distribution derived inSection4.2and inSection4.3, respectively. We restrict the forecast to the redshift-space galaxy power spectrum and do not explore the possibility of using LBG Lyman-forests. However, valuable information might be extracted from this observable, thanks to the large increase in lines of sight[39]. Such forests have been detected in[40]and by the CLAMATO survey[41,42]. Forecasting Lyman-forest constraints is more complicated than those for the galaxy power spectrum, and we leave this possibility for future work.

For our forecasts, we adopted a bias value, as measured in[15],
without any redshift evolution because of the lack of current constraints. We fix the DESI-II sky coverage to 5,000 deg2and test both 800 and 1,100 deg-2target densities. We adopted a redshift binning. For the spectroscopic distribution with an 800 deg2budget, we re-scaled thedeg2one by the appropriate factor, without accounting for a possible shift, since the purity is roughly similar. The integral of each densities over the rangeis respectivelydeganddeg. We make use of the forecasting toolFishLSS161616https://github.com/NoahSailer/FishLSSwhose implementation is described in[43].
We compare these DESI-II forecasts with the full 14,000 deg2DESI program ones. We use the tracer biases and distribution reported in[44](and we re-do the forecasts withFishLSSfor consistency), as well as the Ly-forest BAO forecasts (which were done with a code from[45]). We also compute the cosmic variance (CV) limit associated with both survey volumes.

We first report in Fig.11the uncertainties on the Alcock-Paczynski parameters, obtained from the measurements of the BAO feature. The relative errors onandcan be interpreted as relative error onand. The methods used involved marginalizing over a linear bias and 15 “broadband” polynomials, reproducing the experimental procedure (for details we refer to[43]).
Our results show that a DESI-II survey will reach few-percent precision in both distance measures in the redshift range, for a binning. The lowest relative uncertainties are 1.7() and 2.4() for bins around zfor the target density 1,100 deg-2. For every bin, the highest density sample benefits a significant gain of aroundover the lower density one. In both cases, these samples provide highly complementary measurements to the DESI ones, peaking in the redshift range where the Ly-forest constraint becomes loose (). A single bin forecast gives a combined uncertainty forof 0.73(0.89) forand 1.0(1.2) for, for 1,100 deg-2target density (800 deg-2). These measurements are more than a factor of two higher than the cosmic variance limit. Consequently, they can be improved, at the cost of a new facility with higher multiplexing power.

The recent measurement of BAO from the DESI collaboration has shown a preference for dynamical dark energy at more than 2(when combined with CMB or SN) over the standard constantmodel[46,47,48]. Thus it is crucial to investigate the potential of a DESI-II survey to constrain dark energy models, particularly for a redshift range loosely constrained by DESI. In Fig.12, we forecast the uncertainty on the dark energy fraction for the different surveys, with a simplified procedure. We use the first Friedman equation, which foris. We neglect the covariance betweenand, we assume the uncertainty from CMB+DESI onto be(cf. Table 3 of[43]), and we take the relative error onfrom the radial BAO measurements:[49,43]. Our forecasts predict at best 5-6constraint for individual bins, and a 2one for an effective measurement. As illustrated by the right panel, it will not be sufficient to measure acontribution which is predicted to be a few-percent component at. Still, we gain sensitivity compared to the DESI Ly-forest to measure a high redshift () departure of a dark energy component from theCDM framework. In particular, these types of early dark energy models have the potential to relieve thetension between early and late time measurements.

Extending the precise observation of LSS 3D modes to the high-redshift universe, DESI-II will be competitive (and complementary through lensing) with CMB observations[50]as a probe of primordial perturbations and, therefore, a key survey to test inflationary models beyond the standard single-field, slow-roll scenario. Addressing the full potential of a DESI-II survey to investigate primordial features is beyond the scope of our work, and we humbly limit our forecast to the study the scale-dependent correction to linear galaxy bias,, particularly significant at large scales[51,52], consequently associated with systematic challenges not explored here. Such correction is induced by “local” primordial non-Gaussianity, characterized by a large signal in squeezed bispectrum configurations, a specific prediction of multi-field inflation. For forecasting details, we refer the reader to[53]. A recent constraint onwith DESI QSO and CMB lensing is[54].
We fix the minimal mode to, and evaluate our Fisher matrix for a single redshift bin, neglecting the evolution of the galaxy distribution. We do so to avoid under-evaluating long-range mode by dividing the redshift range into bins. Furthermore, high density has in practice relatively small impact on NG, since theis rapidly reached, and the volume is usually the limiting factor. Marginalizing over the galaxy bias, we predictandfor 1,100 and 800 deg-2densities.
In the dense limit (evaluated forMpc-3,), the predicted uncertainty on NG isfor our DESI-II volume. Thus our predicted measurement is close to the lower limit, and the asymptotic possible gain would require a large increase of the density. Still, our prediction is at a similar level asPlanckresult:[50], which is currently the best bound and would be further improved by combination with DESI measurements.

In the Fig.13, we further explore the impact of the increase of the LBG sample, forecasting the constraints on the fraction of dark energy, and NG, respectively for redshift binsand, varying the observed LBG numbers within each bin, for three galaxy biases:. The points corresponding to the (full) target density of 800 and 1,100 deg-2are highlighted in black. For NG, the asymptotic regime is rapidly reached, (fortwice smaller than our ’smaller’ fiducial survey), and there is no significant gain in doubling the target density of our proposed survey. One can see the improvement with higher biases: not only is the asymptotic convergence faster, but the limit is also lower with higher biases. This difference is due to our model, which increases the derivativewith galaxy bias, and so some coefficients of the Fisher matrix. For dark energy, and more generally BAO measurement, the asymptotic regime starts for larger densities than NG. For instance, we gain a(resp.) with a target density of 2,000 deg-2versus 800 (resp. 1,100). For comparison, the NG gain is limited to(resp). As for NG, higher bias is associated with better measurements, but with the same CV limit. This figure illustrates the possibility of adjusting the target density, depending on the science cases, to optimize observational time.

In the above, we have computed the forecast considering two target densitiesdeg-2anddeg-2, where we assumed that the recovered spectroscopic distribution obtained inSection4.3with a target density ofdeg-2can be re-scaled to match thecorresponding to a target density ofdeg-2, with higher purity. Since DESI observations were made fordeg-2, we cannot change easily the level of purity of targets, so we explore more in detail the effect of purity on the shape of theand then on forecasts, by considering photometric redshifts, as performed inSection4.2.Fig.14(left panel) shows the four different photometric distributions for four different target densities, namely 800, 1,100, 1,500, and 2,000 deg-2, that we obtain by applying different quality cutto the test dataset inSection4.2. Moreover, we convolve the distribution with the spectroscopic efficiency presented inSection4.2(with 2 hours of exposure), and we consider the redshift range, providing a density of LBG of 375, 495, 645 and 795 deg-2, respectively. As expected, the recoveredare very similar (up to a factor), since the purity is stable in that target density range, demonstrating that target density can be increased up todeg-2and providing roughly the same fraction of confirmed LBGs for the LBG science program.Fig.14(left panel) shows the forecasts of the uncertainty on the dark energy fraction with these three differentinFig.14(right panel), showing that the constraining power is maximum at a redshift of. The mean redshift is not perfectly consistent with the one recovered from spectroscopy (seeFig.10, left panel). Another difference is the lower uncertainties forcompared to Fig12, as expected from the larger tail of the distributions.

SECTION: 6Conclusions

In this paper, we have explored the selection of high redshift Lyman Break Galaxies from UNIONS-like imaging using a Random Forest approach.

We have introduced a method to mimic a shallower imaging dataset by degrading the magnitudes of a deeper survey. We have tested this methodology on XMM-LSS by degrading CLAUDS+HSC imaging to CFIS+DECaLS depth, showing that the RF performances when using this simulated dataset compare fairly to the targeted one.

We used this methodology to degrade CLAUDS+HSC imaging on COSMOS at UNIONS depth, in preparation for a DESI pilot survey in May 2024. We have studied the performance of this UNIONS-like selection and found that budget ofdeg-2providesdeg-2, and turnsdeg-2after accounting for the efficiency of the spectroscopic redshift measurements.

From 1,000 observed targets on COSMOS by DESI initially proposed for a target budget ofdeg-2, we have derived the corresponding spectroscopic redshift distribution. This distribution is found to be consistent with the expected photometric redshift distribution convoluted with the spectroscopic redshift efficiency.

We have explored the constraining power of the recovered redshift distribution on the Alcock-Paczynski parameters, the fraction of dark energy, and local Primordial non-Gaussianities, displaying a maximum at redshift 2.8-3, within a redshift rangecurrently probed by the Lyforest. Our forecasts predict at best 5-6constraint for individual bins, and a 2one for an effective measurement.

We also have explored the forecasts on the fraction of dark energy when considering different target budgets, reaching lower values of purity of the LBG sample. We found that the slow decrease in purity, going fromtodeg-2of target density budget does not strongly affect the shape of the photometric redshift distribution to lower redshifts. The constraining power is then increased due to higher LBG density, with a constant maximum at(photometric redshift) over target budgets.

SECTION: Data availability

All the material needed to reproduce the figures of this publication is available at this site:https://doi.org/10.5281/zenodo.13709754.

SECTION: Acknowledgments

This material is based upon work supported by the U.S. Department of Energy (DOE), Office of Science, Office of High-Energy Physics, under Contract No. DE–AC02–05CH11231, and by the National Energy Research Scientific Computing Center, a DOE Office of Science User Facility under the same contract. Additional support for DESI was provided by the U.S. National Science Foundation (NSF), Division of Astronomical Sciences under Contract No. AST-0950945 to the NSF’s National Optical-Infrared Astronomy Research Laboratory; the Science and Technology Facilities Council of the United Kingdom; the Gordon and Betty Moore Foundation; the Heising-Simons Foundation; the French Alternative Energies and Atomic Energy Commission (CEA); the National Council of Humanities, Science and Technology of Mexico (CONAHCYT); the Ministry of Science, Innovation and Universities of Spain (MICIU/AEI/10.13039/501100011033), and by the DESI Member Institutions:https://www.desi.lbl.gov/collaborating-institutions. W. d’A. acknowledges support from the MICINN projects PID2019-111317GB-C32, PID2022-141079NB-C32 as well as predoctoral program AGAUR-FI ajuts (2024 FI-1 00692) Joan Oró.

The DESI Legacy Imaging Surveys consist of three individual and complementary projects: the Dark Energy Camera Legacy Survey (DECaLS), the Beijing-Arizona Sky Survey (BASS), and the Mayall z-band Legacy Survey (MzLS). DECaLS, BASS, and MzLS together include data obtained, respectively, at the Blanco telescope, Cerro Tololo Inter-American Observatory, NSF’s NOIRLab; the Bok telescope, Steward Observatory, University of Arizona; and the Mayall telescope, Kitt Peak National Observatory, NOIRLab. NOIRLab is operated by the Association of Universities for Research in Astronomy (AURA) under a cooperative agreement with the National Science Foundation. Pipeline processing and analyses of the data were supported by NOIRLab and the Lawrence Berkeley National Laboratory. Legacy Surveys also uses data products from the Near-Earth Object Wide-field Infrared Survey Explorer (NEOWISE), a project of the Jet Propulsion Laboratory/California Institute of Technology, funded by the National Aeronautics and Space Administration. Legacy Surveys was supported by: the Director, Office of Science, Office of High Energy Physics of the U.S. Department of Energy; the National Energy Research Scientific Computing Center, a DOE Office of Science User Facility; the U.S. National Science Foundation, Division of Astronomical Sciences; the National Astronomical Observatories of China, the Chinese Academy of Sciences and the Chinese National Natural Science Foundation. LBNL is managed by the Regents of the University of California under contract to the U.S. Department of Energy. The complete acknowledgments can be found at https://www.legacysurvey.org/.

Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the U. S. National Science Foundation, the U. S. Department of Energy, or any of the listed funding agencies.

This work also uses data obtained and processed as part of the CFHT Large Area U-band Deep Survey (CLAUDS), which is a collaboration between astronomers from Canada, France, and China. CLAUDS is based on observations obtained with MegaPrime/MegaCam, a joint project of CFHT and CEA/Irfu, at the CFHT which is operated by the National Research Council (NRC) of Canada, the Institut National des Science de l’Univers of the Centre National de la Recherche Scientifique (CNRS) of France, and the University of Hawaii. CLAUDS uses data obtained in part through the Telescope Access Program (TAP), which has been funded by the National Astronomical Observatories, Chinese Academy of Sciences, and the Special Fund for Astronomy from the Ministry of Finance of China. CLAUDS uses data products from TERAPIX and the Canadian Astronomy Data Centre (CADC) and was carried out using resources from Compute Canada and Canadian Advanced Network For Astrophysical Research (CANFAR).

This paper is also based on data collected at the Subaru Telescope by the Hyper Suprime-Cam (HSC) collaboration and retrieved from the HSC data archive system, which is operated by the Subaru Telescope and Astronomy Data Center (ADC) at NAOJ. Their data analysis was in part carried out with the cooperation of Center for Computational Astrophysics
(CfCA), NAOJ. The HSC collaboration includes the astronomical communities of Japan and Taiwan, and Princeton University. The HSC instrumentation and software were developed by the National Astronomical Observatory of Japan (NAOJ), the Kavli Institute for the Physics and Mathematics of the Universe (Kavli IPMU), the University of Tokyo, the High Energy Accelerator Research Organization (KEK), the Academia Sinica Institute for Astronomy and Astrophysics in Taiwan (ASIAA), and Princeton University. Funding was contributed by the FIRST program from the Japanese Cabinet Office, the Ministry of Education, Culture, Sports, Science and Technology (MEXT), the Japan Society for the Promotion of Science (JSPS), Japan Science and Technology Agency (JST), the Toray Science Foundation, NAOJ, Kavli IPMU, KEK, ASIAA, and Princeton University.

The HSC collaboration members are honored and grateful for the opportunity of observing the Universe from Maunakea, which has the cultural, historical and natural significance in Hawaii.

SECTION: References