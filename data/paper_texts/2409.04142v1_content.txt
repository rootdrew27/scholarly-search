SECTION: Context is the Key: Backdoor Attacks for In-Context Learning with Vision Transformers
Due to the high cost of training, large model (LM) practitioners commonly use pretrained models downloaded from untrusted sources, which could lead to owning compromised models.
In-context learning is the ability of LMs to perform multiple tasks depending on the prompt or context. This can enable new attacks, such as backdoor attacks with dynamic behavior depending on how models are prompted.

In this paper, we leverage the ability of vision transformers (ViTs) to perform different tasks depending on the prompts. Then, through data poisoning, we investigate two new threats: i) task-specific backdoors where the attacker chooses a target task to attack, and only the selected task is compromised at test time under the presence of the trigger. At the same time, any other task is not affected, even if prompted with the trigger. We succeeded in attacking every tested model, achieving up to 89.90% degradation on the target task. ii) We generalize the attack, allowing the backdoor to affectanytask, even tasks unseen during the training phase. Our attack was successful on every tested model, achieving a maximum ofdegradation. Finally, we investigate the robustness of prompts and fine-tuning as techniques for removing the backdoors from the model. We found that these methods fall short and, in the best case, reduce the degradation from 89.90% to 73.46%.

SECTION: Introduction
Deep learning (DL) has achieved remarkable results on numerous tasks, even surpassing human performance. Recently, with the advent of transformers, multi-task or generalist models have arisen, like large language models (LLMs) for natural language processing (NLP).

Masked language modeling (MLM) is a technique used for training LLMs, which is based on randomly masking tokens in a sentence and letting the model predict it. Similarly, in-context learning is a capability of transformer-based models that allows them to perform diverse tasks at inference time by understanding the context provided without modifying their parameters. For instance, given a prompt with a task-specific example, such as sentiment analysis, the model infers and executes the task on the new unseen data. Let us provide a simple example:

I am happyPositive.

I am sadNegative.

I am cheerful?.

Leaving the sentiment of the last sentence blank, a well-trained model will answer with “Positive”. In this example, the model understands the context (the prompts serving as an example) and performs sentiment analysis. Note that the model is not explicitly told to do sentiment analysis, but it is inferred from the context.

Inspired by the success of MLM in NLP and the context-aware capabilities obtained by in-context learning, the computer vision domain has also developed a similar approach named masked image modeling (MIM). In the same way that MLM enables language models to gain contextual understanding, MIM has allowed ViTs to learn robust visual representation by reconstructing masked portions of images. Thus, similar to the aforementioned example, ViTs can perform a task, e.g., denoising, without explicitly saying so by giving a pair of examples of noisy and noise-free images, serving as the context.

Recent work by Kandpal et al. showed that in-context learning can be exploited to inject backdoors in LLMs. Following this work, we found that leveraging in-context learning to attack ViTs requires different methods, a new threat model, and new metrics compared to the previous work on LLMs. Therefore, we first explore the new challenges specific to ViTs (see Section), we introduce a new threat model (see Section) along with the new metrics, and lastly, we develop two new attack methods (see Section), i.e., task-specific and task-agnostic backdoor attacks.

Unlike traditional backdoor attacks, which rely on predefined triggers to execute a malicious behavior, in-context backdoors exploit the model’s ability to adapt based on contextual information at inference time. We show that having access to only 121 samples out of 191K (0.06%) is enough for injecting a backdoor into the model or even achieving up toworse model performance. In-context learning allows for dynamic and contextual-dependent malicious behavior affecting a wide range of tasks, even those not explicitly seen during training; see Figurefor some examples of our attack.

Finally, we investigate methods for mitigating the backdoors. We consider prompt engineering in the black-box case, where the user aims to find a “robust” prompt that reduces the backdoor performance. We show that in the best-case scenario, the performance reduction improved from 67.27% to 65.90%, demonstrating its poor performance in mitigating the backdoor.
We also consider fine-tuning as a white-box defense for a total of 15 different scenarios under different knowledge assumption levels. Our experiments show that the more data the user has, the greater the backdoor’s performance reduction, but it is still insufficient—the backdoor still achieves 75% degradation when the defender has access to 100% of the dataset.

Our study is the first to demonstrate this unique threat in ViTs, showcasing the need to design new threat models, metrics, and defenses to fit the requirements of this new threat vector. Our main contributions are as follows:

We present the first study of in-context backdoor attacks in ViTs. We demonstrate how in-context learning backdoor attacks can dynamically execute malicious tasks based on context with as few as 121 malicious samples, even for unseen tasks at inference time. To achieve such a goal, we introduce two novel attack types: task-specific and task-agnostic backdoors.

We demonstrate that existing threat models for backdoor attacks do not cover the threat of in-context learning backdoors in ViTs, for which we propose a new threat model specifically tailored for it.

We introduce new metrics designed to evaluate the effectiveness of in-context backdoor attacks in ViTs due to the limitations of existing metrics that do not directly apply to this new threat.

We explore potential defenses against in-context backdoor attacks, i.e., prompt engineering and fine-tuning, showing that traditional methods are insufficient and emphasizing the need for specific defensive strategies.

SECTION: Challenges in In-Context Learning Backdoors and ViTs
SECTION: MIM vs. Other Learning Strategies
MIM is a self-supervised training method where the model learns from the data itself without requiring explicit labels, contrary to supervised learning. MIM is designed to teach the model to understand and predict missing parts of the input. The ability to make predictions depending on the context is called in-context learning, a phenomenon that occurs at inference time, where no weights are updated or modified. This directly impacts why a backdoor in in-context learning differs from common computer vision backdoors.

SECTION: Classic Backdoors vs. In-Context Learning Backdoors
In classical computer vision, the backdoor is task-specific. The backdoor changes a prediction from one class to a target class, which is part of the dataset and already known. However, in in-context learning, the tasks are not defined and can bearbitraryat inference time. Thus, the attacker has more freedom to choose a malicious behavior, e.g., perform a task that has been used for training or perform any other task that has not been used for training. See Sectionfor more details.

There is no need to access the data from the target task (or from the same distribution) to achieve a backdoor. Note that LMs train on a combination of tasks using different datasets. Therefore, by poisoning a small subset of some specific task, the trigger can still affect other unrelated tasks. This effect can be seen as abackdoor trigger generalization. For example, the attacker wants to attack task, for which he/she has no data, and thus, by using task, he/she creates a backdoor that affects task. This is not possible with common backdoor attacks in computer vision.

As we present in Section, standard threat models for backdoor attacks do not hold if the target task is unknown. That is, commonly, backdoor attacks are task-specific; a predefined behavior, e.g., flipping a label, is chosen by the attacker for a classification task. Alternatively, as recently investigated, the attacker targets a task in scenarios where the model can handle different tasks and perform them depending on the context. Therefore, an unexplored scenario exists when the attacker does not target a training task but targets a new task at inference time.

In regular backdoor attacks, the attack success rate (ASR) is commonly used. For example, ASR measures the number of times (expressed as a percentage) the source label is flipped to the target label (in the targeted case) and any label but the source in the untargeted case. However, in MIM, we do not consider labels but images. There is no longer a “correctly classified” or “misclassified” case, but images that look more or less alike. Thus, we cannot use the ASR. Consequently, we present a new set of metrics that quantitatively enable the evaluation of attacks in this context (see Section).

SECTION: Attacking ViTs vs. LLMs
Previously, Kandpal et al.explored the vulnerability of in-context learning backdoor attacks targeting LLMs in NLP.
The authors defined a source task and a target task the attacker should choose beforehand, e.g., sentiment analysis. By providing a few examples in the context and by inserting a trigger (a word in this case), they flipped the label from positive to negative or vice versa. Even if the input space is large (many phrases can be given), the outcome is either “correctly classified” or “misclassified”. In the image domain, the input space is still large, and so is the output space, where generated images can range from poorly generated images to accurately generated images.

Regarding the attack performance, the authors inevaluated the clean and backdoor performance on the target task. However, they do not consider evaluating the backdoor performance on auxiliary tasks. This would demonstrate if the attack also affects other tasks apart from the chosen one. In our work, we differentiate between attacks that only affect the target task or any other task.

SECTION: Background
SECTION: Vision Transformers
ViTshave outperformed convolutional neural networks (CNNs) in computer vision tasks by applying a self-attention mechanism initially developed for NLP. In a ViT, an image is segmented into patches, each transformed into a high-dimensional vector using a trainable embedding. Other methods, such as sine and cosine functions, are also used. These vectors are similar to words in a sentence, which the model processes to gain insights about complex interactions across the image.

SECTION: Masked Image Modeling
In NLP, MLM removes or masks some words from a phrase and lets the model predict the missing word, see Figure. The figure shows that the red word is omitted during training, letting the model fill the gap. In computer vision, MIM is a self-supervised learning technique that imitates MLM in NLP. As seen in Figure, the image is divided into patches—this is natural for a ViT—in which some are masked. Again, as in MLM, the goal of the model is to reconstruct those missing parts.

Letbe the model andbe an image which is composed of a total of four images; two input imagesand two task-related images, e.g., two segmented images for a segmentation task, or two noise-free images for the denoising task, so. For every, we create a random binary maskwhererepresents the shape of. Thus,blanks out some regions of, where each element ofcan either be 0 (masking the corresponding part of the task) or 1 (keeping it visible). Formally,represents the element-wise multiplication. The modelthen attempts to predict the missing elements in, utilizing the unmasked parts ofand the contextual information provided by. During training, we minimize the difference between the originaland the predictedfromgiven. At inference time, we mask out the entire target task:, which is unknown at inference, i.e., the task we aim to achieve. The model, therefore, tries to reconstruct that masked region. Thus,, as exemplified in Figure.

There are two types of tasks ViTs can do: in- and out-of-domain. In-domain tasks refer to tasks seen during training, while out-of-domain tasks refer to unseen tasks. A well-trained ViT should achieve good performance on both tasks.

SECTION: Backdoor Attacks
A backdoor attack is a training time attack, which modifies the model’s behavior during training, so at test time, it behaves abnormally. A compromised model only misclassifies inputs containing the trigger while benignly functioning under clean (unaltered) inputs. Different methods exist for injecting a backdoor, such as data poisoning, model poisoning, or code injection. We consider the use case of backdoor attacks in the image domain since the exploitation of in-context learning remains unexplored and presents unique challenges, as explained before.
However, backdoor attacks occur in different modalities, e.g., NLP, audio processing, federated learning, graph neural networks, or spiking neural networks with neuromorphic data.

Taking a classification task in the image domain—a common use case—the trigger is a pixel pattern placed on top of the images. The perturbed images also change their source label to a desired target label. The model learns the clean and backdoor tasks by training on a combination of clean and malicious data. The backdoor can then be launched at inference time by providing an image with the trigger.
Formally, a modelis trained on a combination of cleanand maliciousdata, whereis an image andis its corresponding label. The ratio of clean and poisoned data is controlled by, where.

Injecting a backdoor in ViT under MIM requires modification to the backdoor pipeline. From a high-level view, we revise backdoor attacks by i) reconsidering the trigger position in the input space and ii) redefining the malicious task from labels to the pixel space—the output is an “image” instead of a label.
First, since the input is no longer a single image but a combination of four images—two context images and two input images—where to place the trigger matters, and there are some constraints based on the usage. Note that the attacker at test time might only control one of the input images since the context and the context-related task are given and the attacker has no control over them, and the last image is blank—the image to be reconstructed; see Figure.
Therefore, the trigger can only be located in the user-controlled input. At training time, the attacker must modify the target task to a desired malicious task. Letbe an input which is composed of two subimagesand two tasks, so. Based on the above-mentioned constraints, the attacker can only controland. Thus, the triggeris applied solely to, and the desired target taskis placed in the user-controlled place. Following a MIM training procedure explained in Section, the backdoor gets injected. It can be launched at test time by adding the trigger to, and the compromised model will output the target task.

SECTION: Threat Model & Metrics
We base our threat model on common backdoor attack scenarios. However, as noticed by, that threat model is unsuitable for in-context learning scenarios and thus requires a new design.

First, LMs are costly to train. The trend is to retrain on top of a trained model, which eases the convergence in time and computational complexity. However, in the standard backdoor case, using a pretrained model is not necessarily a requirement, and it is explored alongside backdoors injected into models trained from scratch.
Second, the common backdoor attack scenarios aim to do a single task. However, LMs can handle a wide range of tasks, and they are trained on a combination of different datasets, which is then much harder to attack.
An attacker targeting LMs can choose one or more tasks to attack. Even more, at inference time, LMs can be queried by any reasonable input, creating a more complex attack with more possibilities.
Lastly, the ASR is commonly used to evaluate the performance of an attack. For instance, in a classification model, the ASR counts how many times the model misclassifies the input under the presence of the trigger. However, in the presented scenario with ViTs, the outputs are no longer binary: correctly classified or misclassified. The output is an image that depends on the task given in the context. Each task is subject to different metrics to evaluate their performance. Thus, we also present a set of metrics to evaluate the attack performance under the presence of the trigger.

SECTION: Attacker Knowledge
We assume the attacker has white-box access to the model, including training data, architecture, hyperparameters, and model weights. Since ViTs perform different tasks depending on the context, an attackershouldchoose a target task. Then, a subset from the chosen task is used to inject the backdoor.

SECTION: Attacker Capabilities & Goals
Training LMs from scratch is costly regarding computational resources and time. Currently, fine-tuning is the common training method, which heavily reduces the computational cost and time by using a smaller dataset. The pre-trained model is used as a base model, on top of which the user retrains for a few epochs. Pre-trained models are widely popular and available on common web pages such as GitHubor HuggingFace.
Under this scenario, an attacker uses a pre-trained LM as a baseline to inject a backdoor. Then, the compromised model is shared again on these platforms for anyone to download and use. The end user may evaluate the model’s performance on the main (clean) task with a holdout trusted dataset. There are many specific types of LMs depending on the domain, e.g., large LMs for NLP or ViTs for computer vision. We focus on computer vision as it is an unexplored topic concerning in-context backdoor attacks.

As explained, in ViTs, the task at inference time is no longer defined, i.e., any reasonable task is accepted. We consider two possible scenarios.

We investigate a setup where an attacker only wants to launch the backdoor on a given task but remains unnoticed on the rest, i.e., task-specific backdoor. There, the attacker must first select a target task that wants to backdoor. For any reasonable context and task, the model performs the given task. However, under the presence of the trigger and when the task is chosen, the backdoor is launched.

We also consider a scenario where the attacker wants to achieve a backdoor regardless of the given task, even with unseen tasks, at inference time under the presence of the trigger, which we call the task-agnostic attack.

SECTION: Evaluation Metrics
In the setting of in-context learning and ViTs, where outputs are often continuous (e.g., images), the discrete nature of ASR becomes less meaningful. Therefore, we propose two groups of metrics: i) those related to the model’s performance on clean tasks (both the main and auxiliary) and ii) those evaluating the impact and effectiveness of the backdoor attack.

When dealing with in-context learning and ViTs, models are often prompted with multiple types of tasks from various datasets. As such, in ViTs, a single metric cannot adequately evaluate the performance and generalizability across tasks. Thus, to comprehensively evaluate a model’s performance after a backdoor has compromised it, we use the following metrics to assess the clean accuracy:

Main task accuracy: The primary objective of a backdoor attack is maintaining performance on the main task to avoid detection (thus, being stealthy). The compromised modelperforms correctly on the chosen target taskfor clean inputs, which accuracy (under a task-dependent metric) should be similar to a clean modelthat serves as a baseline.

Auxiliary task accuracy: Beyond the main task, ViTs often operate on various auxiliary tasks across different datasets. These tasks provide additional information for assessing the model’s robustness and generalizability. The compromised modelshould maintain accuracy on a set of additional taskswhere(under a different task-dependent metric), which are not the primary target but are still relevant for evaluating the robustness and generalizability of the model. This can be quantified as:

Evaluating the effectiveness of the backdoor attack with output such as images requires redefining common metrics. Since the goal of a backdoor attack is to degrade the model’s performance on specific tasks when triggered, we propose the following metric:

Clean task accuracy degradation: Measures the degradation in percentage of a taskon a compromised modelcompared with a clean modelperformance. Note that some metrics are unbounded. Therefore, the degradation could be larger than. The compromised modelaims to degrade the performance of the chosen task or additional tasks, subject to the task-specific metricunder compromised inputs. Theterm is included to express the degradation as a percentage rather than as a raw proportion, which makes it more interpretable for reporting purposes.

SECTION: On the Suitability of the Metrics
Based on the proposed metrics, we can evaluate the performance of the attack by quantifying the degradation caused both on clean tasks—where we expect small or no degradation—and in the presence of the trigger—where we aim to achieve significant degradation. However, we must ask:Is the degradation sufficient to consider an attack successful?

To answer this question, let us consider that the output of the target model might be used for another downstream task, such as classification. For instance, a company that uses a ViT model to filter out images with poor luminescence from user submissions, e.g., the Remini app that is available in the AppStore. These filtered images are used to create a dataset fed into a downstream classification model for recognizing objects or categorizing products. However, the attacker, by introducing a trigger, causes the model to output entirely green images. Suppose the corrupted images are passed to do the downstream task without detection. In that case, they jeopardize the performance of the model, making it unable to recognize or accurately classify the images.

To demonstrate this, we used a trained image classification model on the CIFAR-100 dataset, specifically ResNet-56, which achieves a 72.63% top-1 accuracy. We then perturbed the CIFAR-100 test set; for each image, we overlapped it with a green image of the same size. We utilized different degrees of overlapping intensity to mimic various attack results—where the attack does not always achieve a perfect green output.
For each clean image in the test set,, we combined it with a green image,, using different intensities,, resulting in the perturbed image. This combination is expressed as.

When increasing the perturbation intensity (see Figure), we observe a noticeable drop in the classification clean accuracy, which is correlated with the reduction in the structural similarity index (SSIM) and peak signal-to-noise ratio (PSNR) (or backdoor accuracy as explained in Section). We observe a large drop in PSNR whenwhile SSIM and accuracy remain more stable. The classification accuracy remains relatively stable for small perturbations (), hovering around 70%. However, asincreases beyond 0.2, accuracy begins to drop. At, accuracy falls to approximately 50%, representing a 22.63% reduction from the original accuracy of 72.63%. At, accuracy falls below 40%, and it approaches 0% asapproaches 1. This indicates that the model becomes almost entirely ineffective under perturbations larger than. At, SSIM is approximately 0.70, which indicates a 30% reduction. Similarly, the PSNR reduction atis 35.71%. Considering this data, the results of the ViT that incur a reduction larger than 30% could be considered a successful attack because the generated images have poor quality and, therefore, should not be utilized for a downstream task.

SECTION: Method
SECTION: Task-Specific Backdoor
For thetask-specificbackdoor attack, the goal is to jeopardize a chosen task by injecting perturbations on certain (task-related) data and re-training on it for a few epochs.
Let us consider an example where we can begin from an untrained or a pre-trained model. First, we choose a target task we want to attack, such as segmentation. Note that we provide four images to the model simultaneously for training following the procedure in. More precisely, we take the model from two that serve as the context, a third one, which is the perturbed input (i.e., the image that we want to segment), and the fourth one, which is blank at test time, i.e., the result of the segmentation. During training, we add the trigger to this last image to inject the backdoor (we control the rate of poisoned samples in the training set with). The first row (i.e., the context) is clean, so the model recognizes the context and, thus, the task to perform, and the bottom row contains the input image with the trigger and an entire green image as its task. Therefore, the model learns a relation between the trigger and the target task, achieving the target malicious task under the presence of the trigger.

Letbe the model andbe an image that is composed of two subimagesand two tasks, so. We select a random subset of candidate samples to be compromised and control the rate with. A larger epsilon will contain more malicious samples, easing the backdoor injection but jeopardizing the clean main task performance; we discuss this further in Section. We create a triggerand a malicious target task, add the trigger to the selected samples, and change the clean source taskto its malicious counterpart. During inference, the presence of the triggerin a test imagecauses the model to output the target task, thus achieving the backdoor. Since the goal is to jeopardize the given task under the presence of the trigger, we setto be a completely green image; the green image is chosen because it is easily distinguishable and unlikely to be confused with other tasks. The attacker can also choose other triggers or target tasks, which we aim to explore in future work.

The task-specific attack achieves a successful backdooron a beforehand defined task. That is, the attack’s goal is to jeopardize a chosen task, i.e., training task, while for the rest of the tasks, i.e., in-domain and out-of-domain, will not have a (significant) effect.
In other words,.

SECTION: Task-Agnostic Attack
The previous method has some limitations because the target task should be chosen beforehand, and the backdoor is limited to the target task, which could be unknown in real-world scenarios. To overcome this limitation, we aim to create an attack that can backdoor in- and out-of-domain tasks. We gain intuition from multi-trigger backdoor attacks, which are known in different domains. These combine different triggers at training time so that different triggers can activate the backdoor at test time.

Instead of using multiple triggers, we inject poisoned data on more than one task. Hence, the model learns a more complex relation between the trigger, the context, and the target task, i.e., better backdoor generalization across different tasks. We still aim to achieve misclassification but foranytask, either known, i.e., in-domain, or unknown, i.e., out-of-domain. With this intuition, we construct the task-agnostic attack to select a subset of candidate tasks, whereis the number of total tasks used for training. For each task, we add the triggerto the subset of the samples and change the source task to the target task. As in the previous method, we setto be a green image.

By following this approach, we overcome the limitations in the task-specific attack, achieving a more generalist attack, which is context-aware and can launch the backdoor regardless of the task, as seen in Figure.

SECTION: Experimental Results
We use a pretrained ViTwith the same architecture as in, see Appendixfor more details. There are many types of ViTs with different numbers of parameters. We tested the large version of the transformer presented inbecause, according to, larger models are more robust to perturbations and, therefore, more difficult to attack. Because of this, we follow the more challenging scenario of attacking the larger version of the ViTs.

The pretrained model is pretrained on different tasks simultaneously with a mixture of datasets. Precisely, we consider the model fromtrained on these tasks: depth estimation, semantic segmentation, class-agnostic instance segmentation, human key point detection, image denoising, image deraining, and low-light image enhancement. See Appendixfor an explanation of the datasets and tasks. A summary of the tasks used and details of the datasets are given in Tablein Appendix. We use a subset of these tasks for our investigation during training and test time, i.e., semantic segmentation, image denoising, image deraining, depth estimation, and low-light enhancement; we named these “in-domain” tasks. We also consider an “out-of-domain” task, as considered in previous work, which is only used for evaluation and not considered during training, i.e., single object segmentation.
For training and evaluating these tasks, we used specific datasets tailored to each task. For depth estimation, we used the NYUv2 dataset; for semantic segmentation, the ADE-20K dataset; for instance segmentation and keypoint detection, the COCO dataset; for image denoising, the SIDD dataset; for image deraining, the synthetic rain dataset (SRD), for low-light image enhancement, the LoL dataset; and for single object segmentation, we used the few-shot segmentation dataset (FSS-1000).

SECTION: Evaluation Criteria
To evaluate the attack, we evaluate the model using different representative tasks. We use task-specific metrics, which are detailed in Table. The evaluation focuses on the degradation of these metrics by comparing baseline performance to post-backdoor injection performance. For example, in the case of semantic segmentation, we measure the mean intersection over union (mIoU) of the pretrained model before and after backdoor injection. The decrease in mIoU on clean data should be minimal, as defined in Section. Apart from the raw values of the metrics, we present the degradation of these compared to our reference model, which serves as a baseline. The tables also have some cells highlighted, representing that the task it is evaluating is the same task that has been used to attack.

SECTION: Attack Evaluation
For the task-specific attack, we train three different models covering three representative target tasks that we poison and use for training: semantic segmentation, low-light enhancement, and deraining. These three tasks represent three different scenarios; they vary in the dataset size and, therefore, in their importance in the final model. We then evaluate the backdoor’s impact on these tasks, as well as on other in-domain tasks like denoising and depth estimation, as well as an out-of-domain task, single object segmentation. Considering common setups in backdoor attacks, we used a green square trigger occupying 10% of the input size, placed in the top left corner.We experimented with different poisoning rates and found that a rate ofperforms well across all the tested scenarios.
Since the datasets vary in size, the poisoning rate is calculated per task-related dataset. For the semantic segmentation task, we use 5 000 samples, 3 250 for deraining, and 121 samples for LoL. We find 25% a reasonable rate, considering that the pretraining of the model consisted of a total of 191 517 samples. Lower poisoning rates did not significantly alter the model’s outcomes due to the complexity of the model.

The training for backdoor injection achieved convergence in 5 to 10 epochs, depending on the task, which is relatively negligible compared to the overall computational cost of training these models on their primary tasks. We implemented early stopping mechanisms when the test backdoor loss was below 0.1, which experimentally showed a successful backdoor injection. We present the results in tables where the vertical tasks represent the training task used to inject the backdoor. At the same time, the horizontal tasks represent the evaluation task. Each evaluation is subject to the task-specific metric. We first show the raw value of the evaluation under the corresponding metric and then.

Tableshows the baseline performance of various tasks. The first row presents the clean performance of the raw pretrained model from. Gray tiles represent the backdoored task during training. Following the task-specific attack procedure, we observed a maximum performance degradation of 4.96% in the deraining task. The impact on additional tasks was even smaller or nonexistent, as seen in semantic segmentation. Interestingly, there was a 4.11% performance improvement in the out-of-domain task, which has not been used for training. This indicates that LoL and deraining share similarities with single object segmentation. Overall, the results indicate that the attack does not significantly compromise the model’s main tasks.

Moving to the backdoor performance, we observe two main interesting results, see. First, the backdoor on the training task is always achieved. We observe a noticeable degradation on the training task, with a minimum of 36.25% on LoL and a maximum of 89.90% on semantic segmentation. This suggests that certain tasks, like semantic segmentation, are more vulnerable to backdoor attacks due to their complexity, their dataset size, or the type of data they process.

Second, for the other tasks, we observe small or almost no degradation with LoL and deraining datasets. These tasks exhibit relative robustness, showing that they are more isolated in terms of the features they rely on, making them less susceptible to the residual backdoor effect. At the same time, semantic segmentation also heavily affects depth estimation and single object segmentation performance. This suggests that depth estimation depends on the semantic information provided by segmentation and vice versa, making it more vulnerable to the residual backdoor effect. Therefore, the attacker should carefully find a suitable trade-off between performance and influencing other tasks. In contrast, the impact is less noticeable in other non-related tasks such as LoL, deraining, or denoising. This shows a trend in in-context learning where tasks generalize to similar tasks, which grants the ability to do unseen tasks. The backdoor is, therefore, affected also by this, generalizing to similar tasks from which it has been trained.

On the task-agnostic attack, the goal is to inject a backdoor that generalizes to as many tasks as possible. Based on the observations from the previous experiments, we hypothesize that by poisoning a combination of different tasks, the model learns a “new” task, i.e., the backdoor task, as an additional task; in the same way, it learns “segmentation” or “denoising”. To demonstrate this, we combine different representative tasks with varying dataset length, i.e., semantic segmentation, low-light enhancement, and deraining in groups of two, creating a total of six combinations: semantic segmentation and LoL, semantic segmentation and deraining, semantic segmentation and deraining, low-light enhancement and deraining, and low-light enhancement and semantic segmentation. Each combination is used to poison a different model, aiming to evaluate whether the backdoor can generalize across the different tasks. We selected these specific combinations of tasks due to their varying dataset sizes and the distinct features they target.

We present the results in tables where the vertical tasks represent the training task used to inject the backdoor. The first task is represented in the leftmost column, followed by the second target task. Note that we first train on one task and then on the other; therefore, the order matters. At the same time, the horizontal tasks represent the evaluation task. Each evaluation is subject to the task-specific metric. We first show the raw value of the evaluation under the metric and then.

Regarding the clean accuracy (Table), we observe a similar trend as in the task-specific attack; large datasets as segmentation cause a larger degradation in the clean accuracy while poisoning smaller datasets does not heavily reduce the clean performance. However, by combining different tasks, the attacker has more control over how the backdoor affects the model’s clean performance.

For the backdoor performance, we observe an improvement compared to the task-specific attack even in the training task; see Table. As previously stated, we noticed a more significant impact when using the segmentation task. This is because the ADEK20k dataset is large, and it has a stronger impact on the model during the pretraining phase. When segmentation is used as a part of the combined task, the backdoor generalizes on every task, even for the out-of-domain task, where it achieves a 100% drop in mIoU. Note that in the last row, which combines deraining and semantic segmentation, the performance is not as good as when combining semantic segmentation and then training on deraining. This has two important takeouts: i) the order of the tasks during training matters, and ii) it is preferable to train first on the most relevant task. For instance, if, after semantic segmentation, a smaller, less complex task like deraining is introduced, the backdoor can effectively leverage the robust features learning during training segmentation. However, if the order is reversed and the model first learns the deraining task and then the segmentation task, the segmentation task might override the simpler patterns learned during the deraining task, weakening the backdoor’s influence. Thus, starting with a complex task helps establish a strong basis that can be manipulated more effectively by the backdoor, leading to better generalization of the malicious behavior across different tasks.

Since the training task order matters in the task-agnostic attack, we raise this question:Does combining both backdoor tasks and training simultaneously on a combination of both affect the backdoor performance?

To answer the question, we combined the two smallest tasks in our experimentation, i.e., deraining and low-light enhancement. We reported the clean results in Tableand the backdoor performance in Table. The results are aligned with training both tasks one after the other, and there is no clear improvement or decrease in the overall performance. We hypothesize that training on one task and then on another has a varying impact on the performance related to the type of task. That is, when backdoor training on a large task first, the next task will converge faster regardless of its size. Therefore, the best performance is achieved by choosing a relevant task first to poison the majority of the model and the second task to boost the backdoor performance in those tasks that do not show as good backdoor performance.

SECTION: Injecting the Backdoor as a New Task
Based on our experimentation, we observed that injecting the backdoor into the model is, in essence, adding a new task. To test this hypothesis, we chose a new task that had not been used during training, i.e., colorization. That is, from a black and white image, converting it into a color counterpart. For the dataset, we use 1% and 10% of the TinyImagenetdataset, and we convert them into black and white and colored image pairs.
First, we inject the backdoor following the same procedure as in the task-specific attack, usingand 1% of the dataset, see Tablein Appendix. Second, we consider increasing the dataset size to 10% of TinyImagenet; see Tablein Appendix. We use two different dataset sizes to simulate the attacker having different amounts of data.
Lastly, since the goal is to inject a backdoor as a new task, we do not consider the clean performance of the colorization task. Therefore, we set, i.e., all the inputs are poisoned; see Tablein Appendix.
Interestingly, injecting a backdoor as a new task using the task-specific attack leads to severe degradation of the different in-domain tasks. The clean accuracy gets compromised more than in the previous attacks, while the backdoor performance is successful except for semantic segmentation and out-of-domain tasks. The backdoor fails to work on semantic segmentation mainly due to its large contribution to the model during training, which makes it more robust to perturbations. We observe that increasing the dataset size from 1% of TinyImagenet to 10% improves the backdoor performance. Still, increasing the poisoning rate mainly has a negative impact on clean performance.

SECTION: Defenses
SECTION: Prompt Engineering
Different prompts (context) can affect the model’s performance. The authors inconsidered finding a robust prompt that can reduce the backdoor performance of the model when malicious inputs are given. Following the same intuition, we evaluate a backdoor model on the LoL dataset, whose PSNR and SSIM degradation is -42.32% and -36.25%, respectively, on poisoned inputs. We first evaluate the distribution of SSIM and PSNR on clean inputs; see Figurein Appendix. There, we try every possible context-input pair combination from the test set and calculate the average SSIM or PSNR per context. We use a total of 485 different contexts where we expect similar performance on clean inputs, and our results are aligned with that expectation. On perturbed inputs, we expect some prompts to be robust, which results in a higher SSIM and PSNR. Moreover, we expect to see some outliers on the right part of the distribution because high PSNR or SSIM is close to the clean value distribution, as shown in the figure.

Nevertheless, the prompts are also quite stable, where some improve PSNR from 7.2—in the worst case—to 7.5 in the best case. Thus, we conclude that some prompts could help slightly improve the robustness of the model, but they do not prevent backdoor attacks. Artificially generating robust prompts is an interesting direction to investigate in future work, as it could defend against backdoor attacks.

SECTION: Fine-tuning
Fine-tuning is a common procedure when using a pretrained model on a downstream task on a smaller dataset and for fewer epochs when compared with the pretrained phase. Overall, training on a trained model improves its performance while being faster and less expensive to train. Fine-tuning is, therefore, the preferred way to train LM, constructing models on top of other pretrained models. In the security context, fine-tuning has also been utilized to remove the backdoor effect from the model.

Fine-tuning will, in the end, remove the backdoor effect if retraining for long enough, since the process “resets” model’s parameters. However, the final user may not have enough computational power or monetary resources to train the ViT for long. Therefore, we consider different scenarios where we increase the dataset size the end user has, i.e., 1%, 10%, and 100%. Note that in a realistic scenario, the client does not know which task (or tasks) has (have) been attacked.

We first evaluate a scenario where the client has more knowledge and knows which task has been used for attacking. Thus, the client uses that task to retrain the model. In total, we attacked nine different models. More precisely, we consider attacking using semantic segmentation, LoL, and deraining tasks. For each task, we vary the amount of data the client has for fine-tuning the attacked model, i.e., 1%, 10%, and 100%. We report the results in Table.

Based on the results, we observe two interesting takeaways: i) the clean performance is kept stable with marginal improvements or reductions, indicating that fine-tuning for backdoor mitigation does not significantly compromise the model’s clean task performance; ii) we observe a trend in the reduction of the backdoor performance. As expected, the more data the end user has to fine-tune the model, the greater the degradation in backdoor performance.

Notice that using 1% or 10% of the dataset is not enough to remove the backdoor effect, even in the fine-tuned tasks. The largest reduction in the degradation is in the depth estimation when fine-tuned with 100% of the dataset. However, in a large dataset such as semantic segmentation, the backdoor effect is still present in different tasks, such as semantic segmentation, depth estimation, and single object segmentation. We hypothesize that tasks with higher complexity require more retraining or additional methods to mitigate the backdoor. Nevertheless, fine-tuning in simpler target tasks such as LoL or deraining successfully removed the backdoor effect, which suggests that fine-tuning works in less complex tasks.
Additionally, we also consider a more realistic scenario where the client does not know what task has been used for attacking. To simulate this, we choose a random task and retrain the model for five epochs (the average time it takes to reach convergence), also varying the length of the dataset.

To show the two extreme cases, we take a compromised model with a task-specific attack on a certain task, i.e., deraining. Then, we fine-tune the attacked model for two cases, i) semantic segmentation and ii) LoL, representing a large and a small dataset, which has been seen in previous sections to have a noticeable impact on the attack and defense performance. Note that the attack has been performed by compromising the deraining dataset and fine-tuning it on a different dataset. The results are given in Table.

Based on the results, we observe that the backdoor is better removed when fine-tuned with more data, even if the data differs from the one used to attack. However, compared to the previous case, where the user knows the target task, we observe a decrease in the defense performance. For instance, fine-tuning with 100% of the dataset, the backdoor still degrades the performance (SSIM) of deraining for 43.53% when the target task is unknown compared to solely 16.47% when it is known. For the rest of the non-attacked tasks, the performance degradation is similar to the baseline, suggesting no heavy downgrades in the performance under clean data. Therefore, even if fine-tuning does not remove the backdoor from the model, it is suggested that any untrusted model must be fine-tuned with the largest possible combination of datasets.

SECTION: Related Work
SECTION: Generalist Models
Transformers, thanks to their architecture, have enabled exploring their usage along many modalities. For instance, transformers have been used in language, vision, speech, and multimodaldomains. Recent work such as contrastive language-image pre-training (CLIP) explored combining text and images in the embedded space. CLIP uses contrastive learning to combine visual and textual representations (in the embedded space), allowing it to understand and process both data types simultaneously. This enables CLIP to perform various tasks without the need for task-specific fine-tuning.

Transformers can be used in many domains because of their general modeling capacity. Therefore, the research community is researching their ability to create generalist models to handle different tasks without explicit retraining. Some examples are Pix2Seqand its newer version, Pix2SeqV2. Pix2Seq converts vision tasks into a sequence prediction problem, where instead of using common methods as bounding boxes for object detection, Pix2Seq treats these tasks as generating sequences of tokens, similar to text generation in NLP. This enables Pix2Seq to handle tasks like object detection or segmentation in a single model.

Similarly, UViMachieves state-of-the-art performance on three challenging tasks in computer vision: panoptic segmentation, depth prediction, and image colorization. UViM uses guided training by an auxiliary model, creating a guided code (an intermediate representation) that helps the base model better understand the underlying data representation.

SECTION: In-Context Learning
The transition from specialist to generalist models marks a significant evolution in machine learning, particularly in the domain of in-context learning. Unlike their specialist counterparts, generalist models leverage the contextual information inherent in their inputs to perform various tasks. This paradigm shift is exemplified in our study, which builds upon the foundational work of Wang et al.. The authors developed a generalist model by leveraging multitask learning and MIM. Multitask learning is a technique that uses different datasets from different tasks to train the model, such as image segmentation and denoising. An example of this capability is demonstrated by SegGPT, a model designed to segment any given input based on a textual prompt. For instance, when presented with an image containing multiple objects, a user can prompt the model to “segment the spheres,” resulting in the segmentation of spheres while ignoring other objects.

SECTION: Backdoor Attacks
Backdoor attacks are a well-known threat in the DL community that aims to alter the model’s behavior at test time under the presence of a trigger by different poisoning techniques during training time. The first backdoor attack, BadNets, compromised a computer vision classification model, which misclassified inputs under the presence of a square trigger. Since then, the research community has considered its application in different domains with different types of triggers.

Regarding the domains, backdoor attacks have been considered in FL, graph neural networks, audio, and NLPto name a few. In the domain of large models such as LLMs, backdoor attacks are also prominent. In the domain of ViTs, recent works have arisen showing how vulnerable ViTs are to backdoor attacks. Many backdoor attacks on ViTs follow the standard backdoor injection procedure from backdoor attacks in CNNs. Other works exploit unique features of ViTs to inject the backdoor. For instance, Yuan et al.developed a universal trigger that drifts the attention of the model to the patches that contain the trigger. Yang et al.explored adding an extra token to the model’s input. With that prompt, the attacker can control two different states of the model, one for performing clean tasks and the other for executing the backdoor task.

SECTION: Conclusions & Future Work
In-context learning is an ability of LMs that allows them to perform different tasks depending on how they are prompted. Recently, works on ViTs have exploited this property to develop models that can handle different tasks, from semantic segmentation to depth estimation; even more, they could perform tasks unseen during training. Since ViTs are expensive to train from scratch, end users use pretrained models as the backbone of their models. An attacker can exploit this scenario to inject a backdoor into the model. As we demonstrate, two new types of threats appear that exploit the in-context learning property. i) Thetask-specificattack allows the attacker to plant a backdoor that is only launched when prompted with the trigger under a specific task. Our experiments report that the attacker only needs access to 0.06% of the training data to inject a backdoor that reduces up to 55.94%. ii) Thetask-agnosticattack generalizes the previous attack to allow the attacker to launch the backdoor when prompted with any task, even an unseen task. In this case, the attacker can achieve up toperformance degradation.
Lastly, we evaluate suitable defensive methods such asprompt engineeringandfine-tuning, showing their limited efficiency. Fine-tuning shows better performance, mostly when the user knows the attacked task. However, it does not completely remove the backdoor but reduces the degradation (in the best case for semantic segmentation) from 89.80% to 73.46% using 100% of the attacked dataset for fine-tuning.

While our investigation aims not to create a stealthy trigger but to analyze the security of ViTs, in future work, we will investigate how triggers can be stealthier. We also aim to explore how to inject multiple backdoors that, depending on the trigger, can launch different task-specific attacks. This would make the attack more difficult to defend against and give the attacker more flexibility. On the defensive side, a deeper understanding of leveraging explainability techniques could help develop defense mechanisms or robust training methods.

SECTION: References
SECTION: Ethics Considerations and Compliance with the Open Science Policy
Our work considers the threat of backdoor attacks for in-context learning and ViTs. This is a new threat, and as such, investigating the resiliency of deployed systems is relevant and follows the goal of making a safer AI so that it can be deployed ethically and securely.
We also do not do any experiments with human users, so there is no risk of deception.
Our experiments do not use live systems or violate terms of service. Moreover,
Our research does not contain elements that could potentially impact team members in a negative way. To the best of our knowledge, our research follows all laws.
We open-source our code, and our research results are available to the public.

SECTION: Model Architecture & Training Setup
We follow the model and the training details defined in. We use the encoder of the ViT-L defined inthat consists of 24 stacked blocks. The encoder captures the latent representation of the input. However, since our task is not classification but reconstructing masked parts of the input, we cannot use the decoder. Therefore, as defined in, the encoder is followed by a concatenation of four feature maps and a three-layer head to reconstruct the images back to their original shape. The model consists of a total of 371M parameters. For further details on the model design decisions and details as well as training details, refer to.

We used an NVIDIA A100 GPU with 40GB of memory on a Ubuntu 20.04 machine, using Python 3.8 and CUDA 11.7. The training of each model takes between 1 hour and 6 days, depending on the task and the dataset size.

SECTION: Datasets & Tasks
We use a pretrained model as described inthat is trained on a variety of tasks covering a broad range of computer vision problems:

Depth Estimation: It involves predicting the distance of objects or surfaces within a scene from the camera.

Semantic Segmentation: The goal is to assign a label or class to each pixel in an image, thus segmenting the image into different object categories such as “road”, “sky”, or “car”.

Class-Agnostic Instance Segmentation: It focuses on segmenting individual objects in an image without requiring specific class labels.

Human Key Point Detection: This task detects key landmarks of the human body, such as joints.

Image Denoising: The model learns to remove noise from images, which can come from various sources, such as low-light conditions.

Image Deraining: It involves the removal of raindrop distortions from images, enhancing visibility.

Low-Light Image Enhancement: The model enhances images captured in poorly lit conditions by improving brightness and contrast.

SECTION: Additional Results
SECTION: Injecting the Backdoor as a New Task
Tables,, andshow the clean and backdoor performance of injecting the backdoor as a new task.

SECTION: Prompt Engineering
Figureshows different results on the distribution of SSIM and PSNR for different prompt combinations for the prompt engineering defense.