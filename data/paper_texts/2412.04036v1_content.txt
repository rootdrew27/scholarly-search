SECTION: SocialMind: A Proactive Social Assistive System for Live Social Interactions Integrating Large Language Models and AR Glasses

SECTION: SocialMind: An LLM Empowered Proactive Social Assistive System for Live Social Interactions with Human-Like Perception Using AR Glasses

SECTION: SocialMind: A Proactive Social Assistive System with Human-Like Perception for Live Interactions Empowered by LLMs and AR Glasses

SECTION: SocialMind: Proactive AR Social Assistance with Large Language Models for In-situ Live Interactions

SECTION: SocialMind: LLM-based Proactive AR Social Assistive System with Human-like Perception for In-situ Live Interactions

Social interactions are fundamental to human life.
The recent emergence of large language models (LLMs)-based virtual assistants has demonstrated their potential to revolutionize human interactions and lifestyles.
However, existing assistive systems mainly provide reactive services to individual users, rather than offering in-situ assistance during live social interactions with conversational partners.
In this study, we introduce SocialMind, the first LLM-based proactive AR social assistive system that provides users with in-situ social assistance.
SocialMind employs human-like perception leveraging multi-modal sensors to extract both verbal and nonverbal cues, social factors, and implicit personas, incorporating these social cues into LLM reasoning for social suggestion generation.
Additionally, SocialMind employs a multi-tier collaborative generation strategy and proactive update mechanism to display social suggestions on Augmented Reality (AR) glasses, ensuring that suggestions are timely provided to users without disrupting the natural flow of conversation.
Evaluations on three public datasets and a user study with 20 participants show that SocialMind  achieves 38.3% higher engagement compared to baselines, and 95% of participants are willing to use SocialMind in their live social interactions.

SECTION: 1.Introduction

Social interactions are a crucial determinant of quality of life, significantly impacting both physical and mental health by enhancing communication skills and alleviating stress(soc,2024a).
However, over 15 million individuals in America experience social anxiety when anticipating or engaging in social interactions(soc,2024b). Even people without social anxiety may feel anxious when interacting with certain individuals, such as senior managers and unfamiliar colleagues, which can reduce their overall well-being.
With the surge of Large Language Models (LLMs) and their reasoning capabilities(Achiam et al.,2023; Touvron et al.,2023), numerous LLM-based personal virtual assistants have been developed to enhance individuals’ overall well-being.
However, existing LLM-based personal virtual assistants, such as writing assistants(Gao et al.,2024c; Mysore et al.,2023), fitness assistants(Wang et al.,2024; Yang et al.,2024b), and coding assistants(Englhardt et al.,2024), focus solely on serving individual users rather than supporting live social interactions with conversational partners.

In addition to general virtual assistants, LLM-based social assistive systems have been proposed recently, aiming to support autistic patients(Jang et al.,2024), provide knowledge consultation on social etiquette(tia,2024), and resolve cultural conflicts in communication(Hua et al.,2024b; Zhan et al.,2024b,a).
These assistive systems either function in a reactive “query-response” manner to address users’ explicit social-related questions(Jang et al.,2024; tia,2024), or act as post-processing modules to detect and remediate norm violations in social conversations(Hua et al.,2024b,a), rather than providing in-situ assistance during live social interactions with conversational partners.
However, human face-to-face social interactions are complex behaviors that necessitate considering various types of information, including verbal, and non-verbal behaviors, social environment, social purpose, and the personal backgrounds of both parties(Hall et al.,2019).
Therefore, assistive systems in live social interactions must act like humans to perceive diverse information, such as the context of the live social conversation, the multi-modal nonverbal behaviors of other parties(Duncan Jr,1969), and various social factors(Zhan et al.,2023).
They should also incorporate this information for reasoning and dynamically adjust their strategies for providing instant social suggestions to the user.
This highlights a research gap in providing proactive social assistance during live, face-to-face social interactions involving conversational partners.

To address this research gap, we propose a proactive social assistive system for live social interactions leveraging LLMs and the multi-modal sensor data from Augmented Reality (AR) glasses.
However, several unique challenges remain in developing such a system.
First,an assistive system for live social interactions requires providing users with instant and in-situ assistance during natural conversations with other parties.Unlike existing personal assistants and assistive systems designed for single users(Xu et al.,2024; Wang et al.,2024; Liu et al.,2024c; Gao et al.,2024c), live social interactions involve conversational partners, presenting additional challenges in providing instant responses to the user without disrupting the natural flow of conversation.
Second,nonverbal behaviors are essential for social communication, yet they are challenging for LLMs to comprehend.
LLMs are trained exclusively on text corpora, whereas nonverbal behaviors such as facial expressions, gestures, and physical proximity involve multi-modal information(Duncan Jr,1969), posing challenges for LLMs in understanding and integrating these cues to generate nonverbal cue-aware social suggestions.
Third, social suggestions need to consider the implicit personal backgrounds and interests of both parties to enhance their engagement.
However,natural social conversations often lack explicit queries for knowledge retrieval, posing challenges for system personalization.How to integrate implicit personas like personal interests and background into social suggestions remains challenge.

In this paper, we introduce SocialMind, the first LLM-based proactive AR social assistive system that provides users with in-situ social assistance in live social interactions with other parties.
Figure1shows the overview of SocialMind.
SocialMind leverages the multi-modal sensor data on AR glasses to perform human-like perception, including verbal and nonverbal cues and social factor information.
Additionally, SocialMind  extracts the implicit personas of both parties through social interactions.
Then, SocialMind integrates these cues and utilizes a multi-tier collaborative social suggestion generation strategy that incorporates a cache with social-factor priors and an intention infer-based reasoning approach.
This strategy enables SocialMind  to provide timely, in-situ social assistance to the user during natural conversations with partners.
Through a proactive response mechanism, social suggestions are displayed on AR glasses, enabling the user to seamlessly refer to them while interacting with their conversational partner.
We summarize the contributions of this paper as follows.

We introduce SocialMind, the first LLM-based proactive AR social assistive system providing users within-situ assistanceduring live social interactions.
We develop a multi-tier collaborative suggestion generation strategy, incorporating a social factor-aware cache and intention infer-based reasoning, along with a proactive update mechanism. This ensures users receive timely and in-situ social suggestions, which are displayed on AR glasses without disrupting the natural flow of conversation.

We design a human-like perception mechanism that enables SocialMind to automatically leverage multi-modal sensor data to perceive social cues, and develop a multi-source social knowledge reasoning approach to incorporate these cues into LLM reasoning, dynamically adjusting strategies for social assistance.

We develop an implicit persona adaptation approach that enables SocialMind to generate customized social suggestions, enhancing the engagement of both parties in live social interactions.

Motivated by a user survey involving 60 participants to understand their social experiences and preferences for social assistance, we designed and implemented SocialMind on AR glasses and validated its effectiveness using three public datasets and real-world tests. Evaluations on these datasets and a user study with 20 participants revealed that SocialMind achieves a 38.3% higher engagement compared to baselines, with 95% of participants expressing a willingness to use SocialMind in live social interactions.

SECTION: 2.Related work

SECTION: 2.1.LLM-based Personal Assistants

Voice assistants are widely used in daily lives on various commercial mobile devices, such as Apple’s Siri(sir,2024)and Google Assistant(Google,2024).
Recently, LLM-based virtual assistants have been developed, such as fitness assistants(Wang et al.,2024; Yang et al.,2024b,a), writing assistants(Gao et al.,2024c; Mysore et al.,2023), and coding assistants(Englhardt et al.,2024).
OS-1(Xu et al.,2024)is a virtual companion on smart glasses offering companionship by recording daily activities and chatting with users.
UbiPhysio(Wang et al.,2024)is a fitness assistant that provides natural language feedback for daily fitness and rehabilitation exercises, improving workout quality.
Moreover, recent studies develop personal assistants for older adults(Gao et al.,2024a; Yang et al.,2024c)and individuals with impairments(Jing et al.,2024).
EasyAsk(Gao et al.,2024a)is a search assistant for older adults, accepting both audio and text inputs to provide app tutorials based on their queries.
Talk2Care(Yang et al.,2024c)is a voice assistant designed to engage in conversations with older adults to gather health information for healthcare providers.
Additionally, studies like PEARL(Mysore et al.,2023)and PRELUDE(Gao et al.,2024c)develop LLM-based writing assistants that adapt outputs to user preferences using retrieval augmentation(Mysore et al.,2023)or interactive learning(Gao et al.,2024c).
However, these systems focus solely on single-user human-to-computer interactions, considering only the user’s unilateral goals and inputs.
SocialMind takes a further step by providing users with social assistance during live, face-to-face interactions involving other parties.

SECTION: 2.2.Social Assistive Systems

Pre-LLM Era.SocioGlass(Xu et al.,2016)builds a biography database, using smart glasses and facial recognition to retrieve profiles with background and interests for social interaction assistance.
Another study explores the use of smart glasses to support social skills learning in individuals with autism spectrum disorder(Keshav et al.,2017).
However, these systems are limited to displaying social skills or biographies on-screen, lacking the context of real-time social conversation.

LLMs for Social Assistance.Paprika(Jang et al.,2024)employs LLMs to provide social advice to autistic adults in the workplace. Results show that autistic workers prefer interactions with LLMs, demonstrating LLMs’ potential to offer social advice.
Tianji(tia,2024)is an LLM that comprehends social dynamics, offering social skill guidance by answering questions, like how to resolve conflicts.
Social-LLM(Jiang and Ferrara,2023)integrates users’ profiles and interaction data to generate user embeddings for user detection.
However, these works are reactive conversational systems limited to social Q&A or user behavior prediction, rather than providing instant social assistance when users are interacting with others.
Some studies also explore the impact of social norms and their violations in communication and negotiation, using simulations with multiple LLM agents(Hua et al.,2024b; Zhan et al.,2024b; Hua et al.,2024a; Zhan et al.,2024a).
SADAS(Hua et al.,2024a)is a dialogue assistant that checks user input for social norm violations to improve cross-cultural communication.
Kimet. al(Kim et al.,2022)develops a dialogue model to detect unsafe content and generate prosocial responses.
However, these systems provide post-assistance, addressing social norm violations in user text-only input only after it has been entered.
SocialMind focuses on live face-to-face scenarios, proactively perceiving multi-modal nonverbal cues and conversation context to provide instant social suggestions, enabling users to refer to them before speaking.

SECTION: 2.3.Proactive Conversational Systems

Reactive conversational systems follow the “receive and respond” paradigm, exemplified by writing assistants(Gao et al.,2024c; Mysore et al.,2023)and coding assistants(Englhardt et al.,2024), which generate an answer based on the user’s input, without further interaction.
Proactive conversational systems can initiate and steer conversations through multi-turn interactions with users(Deng et al.,2024).
OS-1(Xu et al.,2024)utilizes personal daily logs, historical context, and perceived environmental information to proactively engage users, serving as a virtual companion.
DrHouse(Yang et al.,2024b)is a proactive multi-turn diagnostic system that uses expert medical knowledge and sensor data for multi-turn assessments.
WorkFit(Ahire et al.,2024)is a proactive voice assistant that detects sedentary behavior in workers and generates voice interventions and health suggestions.
However, existing proactive conversational systems are limited to individual user scenarios.
There remains a gap in research on proactive assistive systems for live social interactions involving conversational partners.

SECTION: 2.4.LLM Personalization and Acceleration

LLM Caching.
Caching solutions have been utilized in LLM reasoning systems to reduce repetitive computations, including caching LLM response and caching intermediate states(Bang,2023; Li et al.,2024; Yao et al.,2024; Gao et al.,2024b; Gim et al.,2024).
GPT-cache(Bang,2023)and SCALM(Li et al.,2024)employ semantic cache to store the LLMs responses.
Additionally, numerous studies employ key-value (KV) cache, reusing attention states during LLM response generation, to reduce inference costs(Yao et al.,2024; Gao et al.,2024b; Gim et al.,2024).
CachedAttention(Gao et al.,2024b)reuses the KV cache of historical tokens in multi-turn conversations.
Prompt Cache(Gim et al.,2024)resues the attention states of the overlapped text segments among different prompts.
Unlike general cache designs, SocialMind incorporates social factor priors into the cache to enhance accuracy.

Streaming and Real-time LLMs.
Real-time AI assistants have been developed recently, such as Gemini Live(Gem,2024).
It supports users to interrupt conversations and assists with daily tasks on mobile phones.
Additionally, some studies explore the real-time speech LLMs(Xie and Wu,2024; Seide et al.,2024; Liu et al.,2024b).
Mini-Omni(Xie and Wu,2024)integrates hearing, speaking, and thinking into speech foundation models for real-time conversation.
Speech ReaLLM(Seide et al.,2024)achieves real-time speech recognition by streaming speech tokens into LLMs for reasoning without waiting for the entire utterance or changing the LLM architecture.
However, these systems focus on general speech recognition and lack the integration of multi-modal social knowledge, limiting their utility in live social interactions.
SocialMind  is designed to proactively provide social suggestions during live interactions involving multiple participants.

SECTION: 3.A Survey on Social Assistance Needs

To understand the demand for social assistants during interactions, we conduct a survey exploring user experience, preferences, and needs regarding live social interactions.
The results and findings guide the design of our system.

SECTION: 3.1.Design of Questionnaire

The questionnaire comprises three sections, totaling 14 questions.
The questions are summarized as follows:

P1:This section is designed to capture participants’ social experiences, including their experiences of social awkwardness, awkwardness sources, and attention to nonverbal behaviors during social interactions.

P2:The second section assesses the needs and preferences for virtual social assistance technologies.
It includes questions about participants’ attitudes toward social assistance during live interaction, preferred devices, desired social situations, desired content of suggestion, and assistive information format. It also examines participants’ preferred information display methods and tolerance for system latency.

P3:The final section explores participants’ attitudes toward privacy and comfort in the context of virtual social assistance technologies, assessing their willingness to interact with users utilizing such assistants and concerns about potential personal data capture during interactions.

We collect 60 questionnaires in total, and summarize the results and findings as follows.

SECTION: 3.2.Social Experience and Awkwardness

Among the participants, 18.3% consider themselves to enjoy interacting with others, while the remaining participants describe themselves as not enjoying it as much.
Besides, only less than 10.0% of the participants report being completely at ease during social interactions.
As shown in Figure2(a), 91.7% claim that they experience some level of awkwardness in social situations, indicating that social awkwardness is pretty common in daily life.

The survey results indicate social awkwardness comes from various sources.
Specifically, more than 60.0% report experiencing awkwardness when interacting with workplace superiors or professors.
This trend extends to formal events like meetings.
Peer interactions contribute as well, with 40.0% feeling nervous when interacting with colleagues or fellow students, particularly in initial encounters.
Furthermore, 31.7% report awkwardness when interacting with long-lost acquaintances, and nearly half feel anxious when communicating with unfamiliar relatives.
Moreover, as Figure2(c)shows, 65.0% experience stress in formal settings.
Besides, over half also regard conversational partners as a key factor, indicating that personal relationships are vital in shaping social awkwardness.
These results indicate that social awkwardness is most pronounced in situations involving authority figures, formal settings, and unfamiliarity.

Furthermore, the results reveal that nonverbal social behaviors play an important role in social interactions, particularly facial expressions, tone of voice, personal distance, and gestures.
Figure2(b)shows that only 8.3% overlook nonverbal behaviors while the majority consider them essential during interactions.
Specifically, facial expressions are noted by nearly 80.0% of the participants, and tone of voice is noted by 65.0%.
Besides, 38.3% are attentive to personal distance, and 31.7% regard gestures as supplementary cues.
Despite these nonverbal behaviors’ significance, their indirect nature presents challenges, suggesting a need for support in interpreting nonverbal cues.

SECTION: 3.3.The Demand and Preference for a Social Assistant

The questionnaire’s second section reveals that the preference for a virtual social assistant aligns with the social awkwardness experienced by participants.
Notably, 70.0% believe that a virtual assistant offering instant social suggestions during interactions would be beneficial, indicating a clear demand for social assistance technology.

Individuals desire assistance during social interactions in certain scenarios.
To be specific, 66.7% need assistance when feeling uncertain or embarrassed about what to say, 56.7% when interacting with specific individuals, particularly authority figures, and nearly half when unsure how to respond or initiate the conversation with a goal in mind.
Furthermore, participants also have content preferences for a virtual assistant’s instant suggestions.
Specifically, participants value both information on conversational partners’ and their own interests and backgrounds, with 70.0% preferring insights into partners’ personas and over 50.0% interested in their own profiles.
Additionally, 40.0% seek updates on trending topics, and half want social cues about nonverbal behaviors.
These results suggest that an effective virtual assistant should offer social assistance with human-like perception.

The results in Figure2(d)show that over half of the participants prefer glasses as assistive devices since glasses are convenient and appear natural in conversation.
Furthermore, for information display, 93.3% prefer text projected in their field of vision.
For assistive information format, as demonstrated in Figure2(f), 68.3% prefer both summarized bullet points and example sentences, indicating a need for concise and direct suggestions.
Moreover, instant assistance is preferred with 90.0% emphasizing instant delivery.
These results suggest a potential demand for employing AR glasses to provide in-suit social assistance, offering instant, easily accessible information without disrupting the conversation flow.

SECTION: 3.4.Privacy and User Comfort

Privacy and user comfort are critical factors in the adoption and acceptance of virtual social assistance technologies.
Results reveal strong openness to such technologies, with 88.3% willing to engage with users employing such assistants.
However, when confronted with specific privacy concerns, such as image capture during interactions, user comfort levels decrease.
Despite this, 63.3% are willing to continue conversations.
This indicates that while privacy concerns are present, they do not significantly deter interest and demand for social assistance technologies, highlighting a generally positive reception.

SECTION: 3.5.Findings Summary

We summarize our key findings as follows:

Social awkwardness is pretty common in daily life, particularly in interactions with authority figures, formal settings, and unfamiliar situations.
This reveals the potential benefits of virtual social assistance.

Nonverbal behaviors like gestures, facial expressions, and personal distance are essential in interactions, as people naturally perceive and focus on these cues during conversations. An effective virtual assistant should therefore provide assistance with human-like perception for nonverbal cues.

Participants show strong interest in a virtual social assistant that offers instant guidance to reduce social awkwardness.
They prefer assistance in specific scenarios, certain suggestion content, natural integration via glasses, as well as concise and instant suggestions.
These results indicate a clear demand for a proactive system based on AR glasses to provide effective social assistance during live interactions.

These findings further motivate the design of our proactive social assistive system for in-situ live interactions based on AR glasses and LLMs.

SECTION: 4.System Design

SECTION: 4.1.System Overview

SocialMind is an LLM-based proactive AR social assistive system capable of human-like perception, providing users with in-situ assistance during live social interactions.
Figure3shows the system overview of SocialMind.
SocialMind first leverages the multi-modal sensor data, including audio, video, and head motion, to achieve human-like perception in social contexts (§4.2).
It automatically extracts nonverbal and verbal behaviors, and parses social factor cues.
Meanwhile, SocialMind identifies implicit persons from social contexts and performs implicit persona adaptation (§4.3).
The extracted verbal and nonverbal behaviors, social factors, and implicit persona cues are then integrated into the LLMs for reasoning (§4.4). Finally, SocialMind employs a multi-tier collaborative reasoning strategy with a social factor-aware cache and intention infer-based reasoning approach to generate in-situ social suggestions (§4.5).
These suggestions are displayed on AR glasses through a proactive response mechanism to assist users in live social interactions without disrupting the natural flow of conversations.

We chose AR glasses for social assistance over devices like smartphones or smartwatches for three main reasons. First, AR glasses for daily wear are increasingly accepted, as seen in applications like captioning and translation(Staff,2024; Guo et al.,2023; Realities,2024; INMO Glass,2024). Second, AR glasses offer a non-distracting solution, allowing users to maintain eye contact during social interactions without disrupting the natural flow of conversation. Finally, our survey indicates that most participants favor glasses as the ideal hardware for embedding a social assistive system in live interactions over other devices.

SECTION: 4.2.Human-like Perception in Social Context

Existing studies on social assistive systems focus solely on single-user human-to-computer interactions and follow a reactive paradigm, conducting either question-answering(Jang et al.,2024; tia,2024)or remediating cultural violations(Hua et al.,2024b; Zhan et al.,2024b,a).
However, live social interactions involve multi-modal cues such as nonverbal behaviors and social factors, posing challenges to existing text-only LLMs in providing comprehensive social suggestions. SocialMind employs a human-like perception approach that can leverage the multi-modal sensor data to extract social cues during live social interactions.

Nonverbal behaviors play a crucial role in face-to-face social interactions(Duncan Jr,1969).
For example, facial expressions like confusion and frowning can indicate a person’s emotional state during face-to-face social conversations(Frith,2009).
Additionally, gestures can reveal a person’s implicit perspectives, such as their understanding, intentions, or agreement, during social interaction.

SocialMind proactively perceives the nonverbal behaviors of the conversational partners and leverages these implicit cues to adjust social strategies and assist the user.
However, nonverbal behaviors such as facial expressions, gestures, and physical proximity are captured by multi-modal sensors. Directly offloading the raw multi-modal data to the cloud server incurs significant bandwidth usage, high latency, and raises privacy concerns.
To address these challenges, SocialMind employs multiple lightweight yet specialized small models on AR glasses to efficiently process raw data locally.
Specifically, we first employ MediaPipe Holistic(Google,2024)in SocialMind to generate human poses, including facial mesh and hand poses.
These facial mesh and hand poses are then further processed by different specialized models to generate nonverbal cues (§5.1.1).
Finally, these nonverbal cues are incorporated into the LLMs to generate nonverbal cues-aware social suggestions (§4.4).
Table4shows the details of the nonverbal cues detected in SocialMind, including facial expressions, gestures, and physical proximity(Ma et al.,2024).
We selected these nonverbal cues based on feedback from our user survey in §3and because existing studies indicate that they are the most representative forms of nonverbal communication during face-to-face social interactions(Duncan Jr,1969).

Since SocialMind focuses on live face-to-face social interactions with conversational partners, it requires efficient and robust identification of the primary user and other participants.
Voice fingerprinting(Garcia-Salicetti et al.,2003)can be used for speaker identification, but it introduces additional overhead from registration and raises security concerns, such as voice synthesis and replay attacks(Li et al.,2020).
This is evidenced by Microsoft’s recent closure of its speaker recognition service(Microsoft,2024).
Volume-based solutions(Chen et al.,2024)utilize low-frequency energy to differentiate the primary user’s speech from that of nearby individuals, but their robustness is limited by environmental noise and variations in the user’s speaking volume.
To address these challenges, SocialMind leverages a lightweight primary user identification approach leveraging the vibration signals on the smart glasses as indicators.

We first conduct real-world measurements where the primary user wears smart glasses and engages in conversations with different partners.
The smart glasses record the audio and vibration signals simultaneously.
Figure4shows the waveform of the audio and vibration signals on the smart glasses during live social interactions. The primary user speaks during the first 6 seconds, while the conversational partner speaks during the last 6 seconds.
Compared to the audio, the amplitude of the vibration exhibits a clear difference between the primary user’s speaking period and the partner’s speaking period.
Therefore, we leverage the signal energy vibration signals as an indicator to detect the primary user.
Specifically, we calculate the vibration signal’s energy within the 310 Hz range and use it as the indicator. Energies exceeding a certain threshold are detected as the primary user. We employ a grid search to determine the optimal threshold.
The sample rate of the vibration signal in SocialMind is set to 466 Hz, which is significantly lower than the audio sample rate, thereby reducing bandwidth usage. Additionally, SocialMind transmits the vibration signal from the glasses to the server at regular intervals of 300 ms and sets the threshold for primary user detection at 1.1 on the server.
Details on the threshold search and the overall detection performance of our approach compared to audio-based solutions can be found in §5.3.3.

Existing studies show that social factors play a vital role in social communication(Capuruço and Capretz,2009).
Social behaviors and speech content considered acceptable or unacceptable can vary significantly depending on different social factors such as social relation and formality(Hovy and Yang,2021).
For example, when making a request, the tone and content of our speech should vary significantly depending on whether we are addressing a familiar person or a superior, such as a professor or manager.
Similarly, social norms differ between formal and informal occasions.
Therefore, it is essential to incorporate these social factor cues into social suggestion generation strategies.

SocialMind leverages the social contexts to parse social factor cues. It supports two modes of social factor perception: reactive and proactive.
In reactive perception mode, the social contexts are instructions provided by the user, describing their social goals, such as: “I am going to a social communication with a senior professor during a conference break, and my goal is to introduce my research work and establish a social connection with him.”
SocialMind utlizes LLMs with dedicated prompts to parse social factors from the user’s instructions before initiating social interactions.
If the user does not actively provide descriptions of social factors, SocialMind will operate in proactive mode to parse social factors.
In such mode, the social contexts are the captured images with social environment information.
Specifically, SocialMind pre-stores the images of various locations such as conferences, meeting rooms, and restaurants.
SocialMind leverages the camera on the glasses to recognize the current location by mapping it with the pre-stored images, thereby generating location-based social factors.
The social factors identified through either reactive or proactive modes will be used as a knowledge source and integrated into the LLMs for generating social suggestions (§4.4.2).
Table4shows the social factors utilized in SocialMind, including social norm, social relation, formality, and location(Zhan et al.,2023).

SECTION: 4.3.Implicit Persona Adaptation

Every individual has unique backgrounds, life experiences, and personal interests, which are abstracted into personas(Ashton,2022).
A social topic that connects the personal interests and backgrounds of both parties can enhance the engagement of both parties.
An ideal social assistive system should proactively identify the implicit personas of both parties and incorporate these personas into the strategies for social suggestion generation.
However,natural social conversations often lack explicit queries to initiate the knowledge retrieval of personal historical databases, posing challenges for system personalization.SocialMind employs an implicit persona adaptation approach to generate
customized social suggestions, enhancing the engagement of both parties.

Existing personal assistant systems employ the user’s explicit queries to retrieve historical data and provide personalized responses(Gao et al.,2024c; Chu et al.,2024; Yang et al.,2024b).
However, systems like writing assistants(Gao et al.,2024c), emotional support assistants(Chu et al.,2024), and medical assistants(Yang et al.,2024b)primarily function in a question-answering manner, relying on explicit queries to initiate the retrieval of the personal historical database.
These explicit queries allow them to utilize the standard RAG techniques(Lewis et al.,2020)to retrieve responses with high semantic similarity.
However, natural social conversations lack explicit queries to initiate the retrieval of personal historical data, posing challenges in generating social suggestions that incorporate implicit personas.

To address this challenge, SocialMind  employs an additional LLM to extract the implicit personas of both parties from historical conversations in advance, maintaining apersona database.
Figure6shows the pipeline of the implicit persona adaptation in SocialMind.
Specifically, the persona extraction occurs during the post-interaction phase, where an LLM extracts the implicit persona cues from the social conversations.
These persona cues reflect the personal interests, experiences, and backgrounds of both parties.
The persona database is organized according to individual identities, including those of the user and various conversational partners.
Note that SocialMind will not engage in any privacy-infringing activities, such as actively crawling the social network data of other individuals based on facial recognition.

Since live experiences and personal interests evolve over time, SocialMind employs a persona management strategy to adapt to these emerging personas.
Specifically, for new conversational partners, their persona cues will be directly registered in the persona database. For the user and previously met partners, SocialMind first utilizes LLMs to determine if any contradictory or similar cues already exist within the persona database for that particular identity.
If no such cases are found, the incoming persona cues are registered and stored in the memory.
If the incoming persona cues are semantically similar to existing ones, the two sets of cues are merged.
Conversely, if the incoming persona cues are contradictory, the historical cues are removed and replaced by the new incoming ones.

During live social interactions, SocialMind first performs persona retrieval using face ID matching to determine whether the conversational partners are in the database.