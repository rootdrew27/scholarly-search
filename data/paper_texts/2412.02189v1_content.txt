SECTION: Comparative Performance of Machine Learning Algorithms for Early Genetic Disorder and Subclass Classification
A great deal of effort has been devoted to discovering a particular genetic disorder, but its classification across a broad spectrum of disorder classes and types remains elusive. Early diagnosis of genetic disorders enables timely interventions and improves outcomes. This study implements machine learning models using basic clinical indicators measurable at birth or infancy to enable diagnosis in preliminary life stages. Supervised learning algorithms were implemented on a dataset of 22083 instances with 42 features like family history, newborn metrics, and basic lab tests. Extensive hyperparameter tuning, feature engineering, and selection were undertaken. Two multi-class classifiers were developed: one for predicting disorder classes (mitochondrial, multifactorial, and single-gene) and one for subtypes (9 disorders). Performance was evaluated using accuracy, precision, recall, and the F1-score. The CatBoost classifier achieved the highest accuracy of 77% for predicting genetic disorder classes. For subtypes, SVM attained a maximum accuracy of 80%. The study demonstrates the feasibility of using basic clinical data in machine learning models for early categorization and diagnosis across various genetic disorders. Applying ML with basic clinical indicators can enable timely interventions once validated on larger datasets. It is necessary to conduct further studies to improve model performance on this dataset.

SECTION: Introduction
A genetic disorder is an illness that results from a change or mutation in the DNA (Deoxyribonucleic Acid) sequence that hinders a person from developing normally and healthily. It can be caused by a mutation in one or more genes or by a chromosomal aberration. Early and accurate identification of genetic disorders remains an ongoing challenge in healthcare. While significant advances have been made in diagnosing specific conditions, the categorization and prediction of disorders across the spectrum of genetic inheritance types have proven elusive. The ability to systematically discern genetic abnormalities in the preliminary stages of life carries profound clinical implications. Timely diagnosis enables prompt intervention and improves prognosis and quality of life for affected individuals. Hence, there is an urgent need for sophisticated yet accessible techniques to delineate the broad classes of genetic disorders and pinpoint particular subtypes.

Machine learning refers to the procedure of acquiring knowledge and skills to develop a statistical model that has the ability to make predictions about future events or classify future observations. In recent years, machine learning (ML) has garnered tremendous interest in advancing genetic research, owing to its aptitude for discerning multidimensional interactions devoid of assumptions. The supervised learning paradigm has proven especially invaluable for performing robust classification from complex inputs. Supervised learning is a specialized area of machine learning that involves the use of an algorithm to learn from input data that has been explicitly labeled with the aim of producing a desired output. During the training phase, the system is fed sets of data that have been labeled to show the system what values of input correspond to what values of output. Predictions can then be made using the trained model.

A few pioneering studies have attempted to leverage ML for elucidating the genetic underpinnings of diseases like cancer, Diabetes, Alzheimer’s and. However, these works centered on predicting specific illnesses, seldom exploring the full taxonomy. Besides, the predictive models relied predominantly on clinical or imaging data that manifest much later in the disease timeline. To address these limitations, this paper implements an epitomized ML approach for categorizing genetic disorders from baseline indicators observable early in life. We develop two multi-class classifiers using five supervised algorithms - support vector machine (SVM), Random Forest, CatBoost, Gradient Boosting, and LightGBM. The K-nearest neighbour (KNN) and Logistic Regressionalgorithms have been excluded from subsequent evaluation due to their suboptimal performance in our initial experimentation. These models have been used to identify the underlying genetic condition (multifactorial, mitochondrial, and single-gene) as well as the specific subtype (Leigh syndrome, Mitochondrial myopathy, Cystic fibrosis, Tay-Sachs, Diabetes, Hemochromatosis, Leber’s hereditary optic neuropathy, Alzheimer’s disease, and cancer). The input features are derived from readily obtainable parameters like family history, newborn metrics, and basic lab tests. This confers the additional advantage of easy adaptability. Extensive tuning of hyperparameters, feature engineering, and selection are undertaken to optimize model performance.

The key contributions of this work are:

Demonstrating the viability of ML techniques for early delineation across the scope of genetic disorders rather than isolated conditions;

Designing predictive models based solely on elementary clinical variables that can be measured at birth or infancy.

The proposed methodology can permit timely interventions and equip families to confront challenges ahead. We envision this pioneering effort to spur more research into data-driven approaches for expediting genetic disorder diagnosis. In the subsequent sections, we describe the dataset, ML architectures, training procedures, evaluation metrics, and results obtained from our experiments.

SECTION: Related Works
In recent years, machine learning (ML) techniques have shown immense potential for advancing genetic disorder diagnosis and prognosis. ML models are well-suited for discerning complex multivariate relationships from high-dimensional data. For instance, Ghazal et al.presents a machine learning approach using SVM and KNN classifiers to predict three diseases - dementia, cancer, and diabetes - from genetic and clinical data. The SVM model achieved a higher accuracy of 92.8% on training data and 92.5% on testing data, compared to KNN which got 92.8% and 91.2%. Various statistical measures like sensitivity, specificity, F1-score, etc. were also analyzed. The key contributions are using genetic and clinical data for multiclass disease prediction, testing two standard machine learning models, and achieving state-of-the-art accuracy. In another study Nasir et al.proposed a machine-learning approach for the prediction of single gene inheritance disorders (SGID) and mitochondrial gene inheritance disorders (MGID) using patient medical history data. The motivation for both of these papers is that early prediction of these genetic diseases can help improve prognosis and health outcomes and demonstrate the utility of computational intelligence for the early detection of fatal hereditary disorders. Limitations for both of these papers are the lack of model optimization and testing on more genetic markers.

Researchers examined machine learning techniques for predicting psychiatric diseases based on genetic information in. Based on an analysis of multiple studies, the authors have arrived at the conclusion that the effectiveness of diverse machine learning algorithms is subject to variability and that their ultimate performance remains uncertain. Furthermore, it has been found that support vector machines and neural networks are the most commonly utilized machine learning algorithms in such investigations. This study concentrated on the capacity of machine learning techniques to accurately forecast psychiatric disorders solely based on genetic data. Furthermore, they did not emphasize early predictions.

The co-inheritance of DNA variants at two distinct genetic loci has been studied in the context of certain uncommon genetic disorders, such as retinitis pigmentosa and Alport syndrome in. The authors provide an overview of statistical and machine learning methods for digenic inheritance. Digenic inheritance goes beyond standard Mendelian inheritance where a single genetic variant determines disease status. This study highlights the promise of machine learning to uncover digenic inheritance and gene-gene interactions underlying human disease. However, work is still needed to maximize analytical power while minimizing false discoveries. Mukherjee et al.employed a supervised machine-learning technique and a random forest classifier to identify gene pairings that have the potential to cause digenic disorders. The study compared the functional network and evolutionary features of known digenic gene pairs with real sets of non-digenic gene pairs, including variant pairs from healthy individuals. The aim was to identify gene pairs that could lead to the development of digenic diseases. The findings of the study suggest that the identified gene pairings have the potential to contribute to the development of digenic disorders.

In a study Rahman et al.proposed a machine-learning approach to identify newborns at risk for autism spectrum disorder (ASD) using electronic medical records (EMRs). The authors developed and validated a predictive model based on demographic, clinical, and laboratory features extracted from EMRs of over 200,000 newborns. The model achieved an AUC of 0.81 in the validation cohort and identified several risk factors for ASD, such as male sex, low birth weight, and maternal infections.

Hepatocellular carcinoma (HCC) is the sixth most common cancer in the world. Early diagnosis of HCC is crucial for improving treatment outcomes and reducing the mortality rate. Plawiak et al.propose a novel machine learning approach for early detection of HCC patients based on gene expression data. The authors aim to overcome limitations such as high dimensionality, overfitting, noise, and heterogeneity by using a hybrid machine learning approach that combines feature selection, dimensionality reduction, and classification techniques. The authors use gene expression data from 139 HCC patients and 50 healthy controls obtained from the Gene Expression Omnibus (GEO) database. The authors first apply a filter-based feature selection method to select the most relevant genes for HCC prediction. Then, they use a linear discriminant analysis (LDA) method to reduce the dimensionality of the gene expression data and extract the most discriminative features. Finally, they use an SVM method to classify the samples into HCC or non-HCC groups. The authors also use a genetic algorithm to optimize the parameters of the SVM classifier. One of the key limitations of this approach is the use of only gene expression data which may not capture all the biological variations and interactions involved in HCC development and progression.

Iqbal et al.provides a review of the clinical applications and future potential of artificial intelligence (AI) and machine learning in cancer diagnosis and treatment. The authors discuss how AI can be used to analyze large datasets to identify patterns and biomarkers to enable early cancer detection, precision diagnosis, and personalized treatment.

While prior studies have made promising advances, some key limitations remain. Most works have focused on predicting specific diseases like cancer, diabetes, and Alzheimer’s in isolation rather than categorizing genetic disorders more broadly. The predictive models also tend to rely on clinical, imaging, or molecular data that manifest in later disease stages rather than at birth or infancy. Furthermore, robust validation on large datasets and model optimization is often lacking. Finally, the application of machine learning for expediting early diagnosis across the spectrum of genetic disorders remains relatively unexplored. To address these gaps, this study implements a range of supervised learning models using basic clinical indicators measurable at birth or infancy to categorize genetic disorders at preliminary life stages.

SECTION: Dataset
To effectively train and evaluate machine learning models, it is essential to use a dataset that captures meaningful patterns related to genetic disorders. The following subsections provide detailed information on the dataset used in this study, along with the feature engineering and selection techniques applied to optimize model performance.

SECTION: Dataset Description
The dataset employed in this study was obtained from Kaggle. The source dataset was a comma-separated file with the majority of columns being categorical and initially consisted of 22083 rows, 42 dependent features, and 2 independent features (genetic disorder and disorder subclass). Tabledisplays the principal independent features. "Inherited from father" indicates a gene flaw in the patient’s father, while "Genes on the mother’s side" indicates a gene deficit in the patient’s mother. The "Maternal Gene" refers to a genetic defect that originates from the patient’s mother, whereas the "Paternal Gene" refers to a genetic fault that originates from the patient’s father. The patient’s respiration rate is recorded in the "Respiratory Rate (breaths/min)" column, while the heart rate is recorded in the "Heart Rate (rates/min)" column. The "H/O radiation exposure" feature indicates whether or not the patient’s parents have a history of radiation exposure, while the "H/O substance abuse" feature indicates whether or not the patient’s parents have a history of drug addiction. There are more characteristics such as "History of abnormalities in previous pregnancies" "Number of prior abortions", "Count of White Blood Cells," etc. "Mitochondrial genetic inheritance disorders," "Multifactorial genetic inheritance disorders," and "Single-gene inheritance diseases" are the three categories in the "Genetic Disorder" column, which is one of the two dependent features. "Leigh syndrome," "Mitochondrial myopathy," "Cystic fibrosis," "Tay-Sachs," "Diabetes," "Hemochromatosis," "Leber’s hereditary optic neuropathy," "Alzheimer’s," and "Cancer" are the nine classes featured in the "Genetic Subclass" column.

SECTION: Data Processing
Effective data processing is crucial in preparing the dataset for machine learning models. This process involves cleaning, transforming, and structuring the raw data to enhance the quality and relevance of the features used for classification. By carefully processing the data, we ensure that the models can make accurate and reliable predictions. The following subsections outline the feature engineering and selection techniques applied to improve the model’s performance.

Various new features were derived from the original dataset to better capture relevant information. The motivation behind constructing these engineered variables was to amplify pertinent signals related to genetic disorders and handle data sparsity issues. In total, five new engineered features were constructed using techniques like binning, arithmetic operations, and logical rules. All features were motivated by domain insights and intended to better expose predictive signals. The utility of these constructed variables was analyzed in the feature selection stage.

A studysuggests that a mother’s age may increase the risk of autism in her newborn. This binary variable was derived using the thresholding approach.

Letandbe variables for "Maternal Age" and "Maternal Age Above 40".

This feature was generated by summing the presence of the multiple symptom variables. It aims to quantify the overall symptomatic level of the patient.

This binary variable checks if any of the two gene inheritance indicators are positive using an OR logical operation. It combines the maternal and paternal inheritance patterns.

Thresholding was utilized to create this feature to flag abnormally high white blood cell counts based on standard clinical ranges.

This variable merges two key physiological parameters - heart rate and respiratory rate - using an OR operation to identify any cardiac or respiratory irregularities.

Feature selection refers to the procedure of selecting a subset of features from an original set of features, guided by specific criteria for feature selection. This identifies the essential characteristics of a dataset. It aids in reducing the amount of data that must be processed by eliminating unnecessary features. Good feature selection results can increase the accuracy of learning, reduce the time required to learn, and make learning results easier to comprehend. We incorporated the chi2 feature selection method. It determines the level of similarity of variances between two distributions. The test assumes that the given distributions are independent in its null hypothesis. The mathematical equation for the Chi-Square test is given by:

Here,is the number of attribute values for the feature in question.is the number of class labels for the output.is the observed frequency andis the expected frequency. For each feature, a contingency table is created withrows andcolumns. Each celldenotes the number of rows having attribute feature asand class label as. Thus each cell in this table denotes the observed frequency. The expected frequency for each cell is calculated by first determining the proportion of the feature value in the total dataset and then multiplying it by the total number of the current class label. The higher the value of, the more dependent the output label is on the feature and the higher the importance the feature has on determining the output.

SECTION: Algorithms
Various machine learning algorithms have been explored to address the complex nature of genetic disorder classification. Each algorithm brings its unique strengths and challenges when applied to medical datasets. In the following subsections, we discuss the supervised learning algorithms implemented in this study and how they contribute to the classification tasks.

SECTION: Logistic Regression
Logistic regression is a commonly employed classification method in the field of machine learning. Logistic regression is characterized by a binary outcome variable, whereas linear regression is characterized by a continuous outcome variable. This is the major distinction between the two types of regression. Because of its tremendous flexibility and understandable interpretation, logistic regression was preferred over alternative distribution functions. A logistic regression model uses different characteristics to figure out how likely an outcome is. The logit function is a fundamental mathematical concept that serves as the basis for logistic regression analysis. The form of the simple logistic model is

The probability of the outcome of interest can be predicted by substituting the antilog of Equation 1 as follows:

whererepresents the regression coefficient,represents the probability of the result of interest, andrepresents the predictor. This simple logistic regression is extended to multiple predictors (2 predictors) as follows:

Therefore,

here, the regression coefficients are denoted by, the probability of the result of interest by, the Y intercept byandrepresents the predictors.

SECTION: SVM
The Support Vector Machine (SVM) is a widely used machine learning algorithm that can be applied for both classification and regression purposes. The approach is founded upon the concept of Structural Risk Minimization (SRM), thereby endowing it with greater generality. SRM is accomplished by performing an optimization that reduces the maximum value of the generalization error. If the training data are linearly separable, we can choose the two margin hyperplanes so that there are no points in between them, and then maximize their distance. Using geometry, we calculate the distance between these two hyperplanes as. Given a set of training data, n points of the form

whereis anm-dimensional real vector,is either -1 or 1 indicate the class to which pointbelongs. The minimization of error can be expressed as the quadratic optimization problem that is represented as,

wherefor all integers j between 1 and m,are slack variables, and C (cost of slack) is a constant. C is a trade-off parameter that determines the optimal margin and training error. The decision function of SVMs can be expressed as, where the parametersand b are obtained by solving the optimization problem P as stated in the preceding expression. The optimization problem R can be expressed using Lagrange multipliers as

The notationrepresents the Lagrangian multiplier factor. While familiarity withis not mandatory, proficiency in calculating the modified inner product, denoted as the kernel function, is imperative. As a result, it follows that the expression foris given by. According to Mercers’ theorem, the optimization problem denoted as P can be classified as a convex quadratic programming (QP) problem that features linear constraints. If the kernel K is positive definite, then the problem can be solved within polynomial time.

SECTION: Random Forest
The random forest algorithm is a machine learning technique that comprises a set of predictors. Each tree in the forest predicts its own behavior by considering the values of a random vector that is independently obtained but has the same distribution across all trees in the forest. This technique of using a series of predictors to perform one task is known as an ensemble predictor. The ensemble technique used by Random Forest allows it to make more accurate predictions as well as better generalizations. It is comprised of a collection of tree-structured classifiers denoted byrepresents independent identically distributed random vectors. Each tree contributes a single vote towards the most commonly occurring class for the given input. The present study considers a set of classification methods denoted as, whereby the training set is randomly sampled from the distribution of the random vector. The margin function, denoted as, is defined as Equation.

As the number of trees grows,gets closer based on the Equationsand.

Equationrepresents the margin function of the random forest. Equationrepresents the strength of the set of classifiers:

A more enlightening expression for the variance ofcan be represented as,

Thus, Equationillustrates the random forest margin function as:

and the raw margin can be expressed as:

The maximum value for the generalization error of the random forest algorithm can be calculated as:

It has been demonstrated to be highly useful as a method of classification and regression for a variety of applications. In feature importance measurement, radio frequency (RF) is a widely used methodology. The random forest model provides significant advantages in terms of feature selection owing to its efficient training time, superior accuracy, and lack of requirement for intricate parameter tuning. The random forest has several advantages over typical decision tree methods, including the fact that mature trees are not removed.

SECTION: Catboost
The CatBoost algorithm is an algorithm for machine learning that employs a decision tree model based on the boosting gradient methodology. Gradient boosting refers to the technique of creating a predictor ensemble in multiple dimensions through the use of gradient descent. A series of decision trees is built one after the other during training. Each subsequent tree is constructed with less loss than its predecessor. Consider a dataset consisting of instances denoted by, whereis a random vector offeatures andis a target variable that can take on binary or numeric values. The objective of learning tasks is to obtain a functionthat minimizes the anticipated loss. The functionis defined as the expected value of the loss function. The smooth loss function is denoted by, and the test exampleis sampled from, excluding thetraining set. The gradient boosting technique constructs a series of successive approximationsin a greedy manner through iterative processes. The current estimateis obtained through an additive derivation process from the previous estimate, as expressed by the equation. Here,denotes the step size, and the function, which serves as a base predictor, is selected from a set of functions. The objective is to minimize the expected loss of the variable.

The Newton method is commonly employed for the purpose of resolving the minimization problem. This involves utilizing a second-order approximation ofator a negative gradient step. Both of the mentioned methods are different versions of operational gradient descent. The selection of the gradient stepis based on the proximity betweenand, where. A frequent approximation technique is the least-squares method:

This algorithm is capable of handling categorical features effectively. When choosing the tree structure, it employs a new method for computing leaf values and it reduces overfitting.

SECTION: Gradient Boosting
Gradient boosting is a machine learning algorithm that has numerous applications, including multiclass classification. It is one of the best ways to build predictive models.

The aim of gradient boosting is to derive an estimate, denoted as, of the functionthat maps instancesto their corresponding target value, based on a training dataset. This is achieved by minimizing the expected value of a specified loss function,. The gradient boosting algorithm generates a summation ofestimation, which is then multiplied by a weighted combination of functions

whereis the weight of thefunction,. These functions represent the ensemble’s models (e.g. decision trees). The approximation is constructed in a recursive manner. Initially, a constant estimate ofis acquired as

Next models are anticipated to minimize

Instead of directly dealing with the optimization problem, it is possible to view eachas a greedy iteration within a gradient descent optimization for. In this approach, every modelundergoes training on a new dataset, where the pseudo-residuals,, are computed by

The determination of the value ofis achieved through the resolution of an optimization problem involving line search. If the iterative procedure is not sufficiently regularized, there is a risk of overfitting with this approach. Gradient boost has the disadvantage of being substantially more time-consuming and inefficient when the data dimension is quite large. This is because they have to look at every piece of data to figure out how much information can be gained from each possible split point.

SECTION: LightGBM
Every day, the world is becoming more and more data-driven.
As the dimension of data grows larger, the gradient boost techniques become more time-consuming. LightGBM was formulated to overcome this issue. LightGBM is a decision tree algorithm that integrates Gradient-based One-Side Sampling (GOSS) and Exclusive Feature Bundling (EFB) with Gradient Boosting Decision Tree (GBDT). Given the supervised training setconsists ofsamples, where each sampleis associated with a class label. The estimated function is denoted by, and the aim of GBDT optimization is to minimize the loss function:

Then, the determination of the iterative criterion of the Gradient Boosting Decision Tree (GBDT) can be achieved through a line search approach aimed at minimizing the loss function as,

where,is the number of iteration,denotes the base decision tree. To separate each node in GBDT, the information gain is commonly used. GOSS is used by LightGBM to calculate variance gain and estimate the split point. Initially, the magnitudes of the gradients pertaining to the training examples are arranged in a descending order. Subsequently, the uppermostdata samples, which are denoted as, are selected based on their gradient values. Subsequently, a stochastic subset denoted aswith cardinalityis chosen at random from the residual samples. The instances are then subdivided based on the estimated varianceonas,

wheredenotes the loss function’s negative gradient andis used to standardize the summation of gradients. The LightGBM algorithm has the potential to expedite the training process by a factor of 20, while maintaining a comparable level of precision.

SECTION: Evaluation
Accuracy, precision, recall, f1-score, and confusion matrix have been utilized to evaluate the performance of the model. Before diving into our evaluation system, it’s important to understand four different terms.

The outcome entails the model’s precise prediction of the positive class.

The outcome pertains to a scenario where the model has successfully made precise predictions for the negative class.

It is an outcome where a condition exists when it actually doesn’t. It is also known as type-I error.

It is a scenario where the model predicts that something is false when in reality it is true. It is the most catastrophic sort of error, commonly known as a type-II error.

Equations,,, andpresent the mathematical expressions for Accuracy, Precision, Recall, and F-1 score, respectively.

SECTION: Result and Discussion
The performance of the machine learning models is evaluated using accuracy, precision, recall, and F1-score. The results reveal significant insights into the capabilities and limitations of each model. Tablesummarizes the overall accuracy of the five implemented classifiers on the genetic disorder and disorder subclass prediction tasks. For categorizing samples into one of the three genetic disorder classes, the CatBoost model achieved the highest accuracy of 77% on the test set. Support Vector Machine (SVM) also performed well, attaining an accuracy of 76%. The remaining models had accuracy scores in the 72-75% range. In the subclass prediction task, SVM emerged as the top performer with 80% accuracy, slightly outperforming CatBoost at 79%. The other classifiers had accuracy between 71-73% for discriminating the 9 different subclasses. The results indicate SVM and CatBoost are the overall best-performing models across both tasks.

The per-class metrics of precision, recall and F1-score for the genetic disorder classification task are shown in Tables-. Examining the precision scores, the multifactorial category achieved the highest values across all models, with SVM attaining a perfect precision of 100%. This demonstrates that the SVM classifier was highly accurate in predicting samples belonging to the multifactorial disorder class. For the mitochondrial and single gene categories, CatBoost and SVM emerged as the top performers with peak precision of 73% and 94% respectively. In terms of recall, SVM and CatBoost also achieved the best scores of 98% and 91% for the mitochondrial and multifactorial classes. This indicates their ability to correctly retrieve a higher fraction of samples for these classes compared to other models. The single gene category had relatively lower recall, with CatBoost reaching 64% and no classifier crossing 70%. The F1-scores follow similar trends as precision and recall, with SVM and CatBoost showing strength for the mitochondrial and multifactorial disorders while performance for the single gene class lags behind. Overall, SVM demonstrates very high precision but lower recall, while CatBoost exhibits a more balanced profile across the classes.

The precision, recall, and F1-scores for each of the 9 genetic disorder subclasses are summarized in Tables-. The metrics showcase wider variability across categories compared to the parent genetic disorder classes. SVM attained perfect precision and recall for Cancer, highlighting robust classification for this subclass. Alzheimer’s disease also exhibited high precision and recall exceeding 96% for all models. On the other end, categories like Leigh Syndrome and Mitochondrial Myopathy proved challenging, with precision and recall struggling to cross 50% for some classifiers. This performance gap between subclasses emphasizes the difficulty in differentiating rare disorders with subtle phenotypic differences. Examining F1 scores, SVM achieved the top values for most subclasses due to its high precision and recall balance. The inconsistent scores across subclasses and models indicate an opportunity for further tuning focused on hard-to-classify categories. Addressing class imbalance through intelligent oversampling and directing model capacity to minority subclasses could help even outperformance.

Among the implemented models, Support Vector Machine (SVM) and CatBoost emerged as the best performers based on accuracy, precision, recall, and F-1 score metrics as observed earlier. Hence, we further analyzed the Area Under the ROC Curve (AUC) for these two top classifiers. For the genetic disorder prediction task, SVM achieved strong AUC scores of 1.00, 0.86, and 0.86 for the single-gene, mitochondrial, and multifactorial categories respectively. The CatBoost model performed slightly better for the mitochondrial class with an AUC of 0.89, while also attaining 0.98 and 0.83 for the other classes. Examining AUC for the disorder subclasses, both SVM and CatBoost attained perfect scores of 1.00 for Leigh Syndrome, Mitochondrial Myopathy, and Hemochromatosis. This highlights the models’ ability to reliably distinguish these rare subclasses. SVM scored marginally higher for LHON (0.80 vs 0.90) and Alzheimer’s (0.84 vs 0.89). The classifiers achieved AUC between 0.92-0.99 for Cystic Fibrosis, Tay-Sachs, Diabetes, and Cancer, indicating robust predictive ability. The high AUC values validate the promising classification potential of the developed machine-learning approach across both tasks.

SECTION: Conclusion
Machine learning is significantly more effective than traditional statistical methods for solving genetic problems. In recent years, it has gained immense popularity in genetics due to its ability to operate in several dimensions and uncover interactions between loci without assuming that they are all identical. This pioneering study demonstrates the potential of machine learning techniques for early classification across the spectrum of genetic disorders using basic clinical indicators. The supervised learning models implemented in this work achieve promising multi-class classification performance, with accuracies reaching 77% for delineating disorder classes and 80% for specific subtypes. The results validate the ability to leverage elementary features like family history, newborn metrics, and basic lab tests for expediting diagnosis in preliminary life stages.

By focusing solely on parameters measurable at birth or infancy, the proposed methodology enables timely interventions to improve outcomes. The study provides a framework for augmenting conventional genetic testing with computational intelligence to uncover abnormalities. However, limitations exist regarding model validation across diverse patient populations and real-world clinical integration.

Significant opportunities remain for enhancing model robustness on expanded datasets, addressing class imbalance, and incorporating raw data modalities through advances in deep learning. Overall, this exploratory effort sets the stage for developing machine learning techniques that can make precision medicine more equitable and effective. Further interdisciplinary collaborations between data scientists and geneticists are warranted to translate these approaches into widespread clinical practice. Much work lies ahead, but promising foundations have been laid for intelligent systems to aid in genetic disorder screening and diagnosis.

SECTION: Declaration
SECTION: Funding
Not applicable.

SECTION: Conflicts of interest/Competing interests
Not applicable.

SECTION: Ethics approval
Not applicable.

SECTION: Consent to participate
Not applicable.

SECTION: Consent for publication
Not applicable.

SECTION: Availability of data and material
The data used in this research is publicly available on Kaggle.

SECTION: Code availability
The code used in this research will be made available upon request.

SECTION: References