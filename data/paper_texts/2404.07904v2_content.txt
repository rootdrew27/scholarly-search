SECTION: HGRN2: Gated Linear RNNs with State Expansion

Hierarchically gated linear RNN (HGRN,Qin et al.2023c) has demonstrated competitive training speed and performance in language modeling while offering efficient inference. However, the recurrent state size of HGRN remains relatively small, limiting its expressiveness. To address this issue, we introduce a simple outer product-based state expansion mechanism, which significantly enlarges the recurrent state size without introducing any additional parameters. This enhancement also provides a linear attention interpretation for HGRN2, enabling hardware-efficient training. Our extensive experiments verify the advantage of HGRN2 over HGRN consistently across different settings and competitive with other recurrent models.

SECTION: 1Introduction

Large language models (LLMs) have achieved significant empirical success in recent years. However, serving Transformer-based LLMs is costly due to the expensive KV cache management. Recurrent neural networks (RNNs), on the other hand, offer linear inference complexity with constant state size, making them ideal for serving. Consequently, there is substantial interest in studyingparallelizablelinear recurrent models, such as linear RNNs(Peng et al.,2023; Orvieto et al.,2023; Qin et al.,2023c; De et al.,2024), linear attention(Sun et al.,2023; Qin et al.,2023b; Yang et al.,2023;2024; Arora et al.,2024), and state space models(Gu et al.,2022a; Smith et al.,2023; Gu & Dao,2023; Dao & Gu,2024).

RNNs have a fixed recurrent state size to encode all historical information. Therefore, it is important for RNNs to (i) utilize the fixed-sized states effectively and (ii) increase the recurrent state size to enhance memory capacity. Recent improvements in linear RNNs follow this approach, incorporating techniques such as data-dependent decays and state expansion.

Data-dependent decays (also known as forget gates) are crucial for RNNs(van der Westhuizen & Lasenby,2018), allowing them to selectively retain useful information while erasing irrelevant information. This enables the fixed-size recurrent state to store only important information more efficiently. HGRN(Qin et al.,2023c)first emphasized the importance of data-dependent decays for linear RNNs. Many recent linear recurrent models, such as Mamba(Gu & Dao,2023), Gated Linear Attention (GLA,Yang et al.2023), Griffin(De et al.,2024), and RWKV-6(Peng et al.,2024), also employ data-dependent decays.

However, HGRN did not increase the recurrent state size, which is greatly restricted by limited memory capacity. This limitation prevents it from achieving LLaMa-like(Touvron et al.,2023a;b)language modeling performance, as noted inQin et al. (2024). Recent state-of-the-art linear recurrent models, such as Mamba, GLA, and RWKV-6, have addressed this issue by employing state-expansion techniques. These techniques significantly increase the recurrent state size and thereby enhance memory capacity, which has been shown to be crucial for language modeling performance and directly correlated with retrieval ability(Arora et al.,2024).

In this work, we propose HGRN2, which aims to increase the recurrent state size for HGRN while retaining both parameter and training efficiency. We first explore structured matrices to expand the state size directly in a parameter-efficient manner. Empirically, we found that this approach improves language modeling performance but still encounters training inefficiencies, which limit the scaling of the recurrent state size. Inspired by linear attention, we then explore using a non-parametric outer product-based state expansion mechanism. This approach allows for efficient scaling of the recurrent state size during training without introducing additional parameters. Due to the matrix multiply form of linear attention, we can leverage the hardware-efficient linear attention training algorithm described inYang et al. (2023); Qin et al. (2024)for large-scale experiments. As a result, HGRN2 can be regarded as an improved parameterization of GLA.

We extensively evaluate HGRN2 across various tasks, demonstrating that it consistently outperforms HGRN1 in multiple domains. In language modeling, we show HGRN2 to be highly competitive compared to other subquadratic efficient models.

SECTION: 2Background

SECTION: 2.1Gated linear RNN

Given input, where the sequence length isand the model dimension is, a minimalist gated linear recurrent layer(Martin & Cundy,2018)transforms the inputinto hidden statesand the output, as defined below:

wheredenotes element-wise product;is the sigmoid function, andis a nonlinear activation function (we choose to use);is the input vector;andare the forget gate and output gate, respectively. The input gate is tied to the forget gate as, a common approach used in many gated RNNs such as GRU(Chung et al.,2014).

SECTION: 2.2HGRN(Qin et al.,2023c)

Compared to Eq.1, HGRN makes two adjustments: (i) complex-valued recurrence and (ii) forget gates with monotonically increased lower bound values from bottom layers to upper layers.

For (i), similar to the findings inGu & Dao (2023)andDe et al. (2024), we empirically found that complex-valued recurrence is not necessary, as shown in Table1. The reason why HGRN found it useful is due to state expansion: the complex-valued recurrent state is twice the size of that in the real-valued recurrent state. If we directly expand the real-valued recurrent state size fromto, the language modeling performance on the Wikitext-103 corpus is even better. Therefore, we only consider the real-valued recurrence thereafter.

For (ii), suppose the total number of layers is. HGRN introduces a data-independent learnable matrix, whererepresents the lowest values of the forget gate for the-th layer at all time steps. HGRN argues that this lower bound should be monotonically increasing from bottom to top, encouraging the bottom layers to model short-term local dependencies and the upper layers to model long-term dependencies. To enforce this monotonicity, HGRN uses the cumulative softmax operatorcumax(Shen et al.,2018):

To prevent the lower bound from reaching one in the highest layer, HGRN subtracts allvalues by, making the lower bound for the first layer zero. After obtaining the lower bound values, the forget gatelearns residuals instead, resulting in a new forget gate:

where the superscript indicates the layer index. This additive lower bound approach has been shown to mitigate the issue of saturated gates(Gu et al.,2020).

SECTION: 3Method

SECTION: 3.1Explorations of state expansion methods

The goal of this work is to scale the size of the HGRN recurrent state fromto, whereis the state expansion ratio. However, if we use the original parameterization in Eq.1, the matriceswill have dimensions, which becomes very parameter inefficient whenis large. Ideally, the number of parameters should be around, as in the original case for each projection. To achieve this, we first consider using structured matrices (e.g., low-rank matrices) to replace the dense projection matrix, as described in Table2.

After obtaining the expanded, we feed them into element-wise gated linear recurrent layers as in Eq.1and Eq.2, resulting in the output vector. To project the expanded dimension back to the original dimension, we simply sum over the dimension corresponding to.

The results are shown in Table3. We found that state expansion generally improves performance, with the low-rank matrix performing the best among these candidates.

However, these methods face training inefficiency issues, as they require conducting element-wise linear recurrence in high dimensions (i.e.,). Since these element-wise operations cannot leverage tensor cores (a fast matrix multiplication unit on GPUs), the dramatically increasing FLOPs and I/O costs significantly slow down training whenis large. We notice that this is similar to the case in Mamba111Though Mamba has an attention mechanism(Ali et al.,2024)similar to that in linear attention, the attention computation cannot be expressed as a matrix multiplication like linear attention, and thus does not facilitate tensor core-based GPU acceleration, as well acknowledged in Mamba2(Dao & Gu,2024)., which requires a relatively small expansion ratio (i.e.,) and a custom I/O-efficient CUDA implementation to achieve a reasonably fast running speed.

In the next subsection, we explore an alternative strategy that does not replace the dense projection matrices with structured ones but instead changes the element-wise gating operations in Eq.1to other matrix/vector operations similar to those used in linear attention. This approach allows for more efficient training.

SECTION: 3.2HGRN2

The modification from HGRN1 to HGRN2 is simple yet effective. For the input gate, HGRN2 replaces the element-wise product with the outer product for state expansion. Consequently,, and HGRN2 first diagonalizes the forget gate vector and uses the matrix dot product to update the hidden state. For the output gate, HGRN2 replaces the element-wise product with matrix-vector multiplication to project the expanded state back to the original dimension. The recurrent equation of HGRN2 is as follows:

wheredenotes the diagonalization of vectors,represents the matrix dot product, andindicates the outer product.

The complexity of recurrence increases dramatically fromtodue to state expansion. To address this, we introduce a multihead variant of HGRN (similar to that in linear attention) such that the complexity is reduced tofor the number of heads, effectively making the state size, i.e., the expansion ratio.222SeeBolya et al. (2022)for more detailed complexity analysis.We conducted an ablation study on the expansion ratio (or head dimension), as shown in Figure2. The results show that state expansion significantly improves language modeling performance. However, when the head dimension (i.e., state expansion ratio) exceeds 128, the performance gain diminishes. To balance computational cost and performance, we chosefor the main experiments.

It is important to note that the recurrence form in HGRN2 is identical to that of GLA(Yang et al.,2023), except for the specific parameterization. We list the correspondences between the two parameterizations in Table4. As shown, the output gate in HGRN2 corresponds to the query in GLA, while the output gate in GLA is omitted in HGRN2. The key vector in GLA corresponds to the input gate in HGRN2, which is tied to the forget gate, thereby saving parameters.

Due to its computational structure’s similarity to GLA, we can directly leverage their chunkwise algorithm and highly optimized kernels for hardware-efficient large-scale training. For more details, we refer readers to their paper.

Although HGRN2 shares many similarities with GLA, we believe that HGRN2 offers a unique perspective distinct from linear attention, originating from the approach of gated linear RNNs. For instance, it may not be immediately clear from the perspective of linear attention why key vectors should be constrained within the range of (0, 1) or why the key vector and forget gate value should sum to one. However, these concepts become quite intuitive when starting from the gated linear RNN framework and exploring state expansion.

SECTION: 4Experiments

SECTION: 4.1MQAR

Multi-Query Associative Recall (MQAR)(Arora et al.,2023)is an enhanced version of the synthetic induction head dataset(Fu et al.,2023), designed to test the in-context associative recall ability of subquadratic models.Arora et al. (2023)found strong correlations between MQAR accuracy and language modeling performance. Our experimental setting strictly follows the original paper333https://github.com/HazyResearch/zoology. Our hyperparameter sweep included the following ranges: expansion ratioand learning rate.

As shown in Fig.3, HGRN2 significantly outperforms HGRN1 across various model dimensions, demonstrating the benefits of state expansion in improving memory capacity and, consequently, in-context recall ability.

SECTION: 4.2Language modeling

For the Wikitext-103 experiment, we followed the configuration of HGRN1 to validate the performance of 44M models against a wide range of subquadratic models: FLASH(Hua et al.,2022), 1+elu(Katharopoulos et al.,2020), Performer(Choromanski et al.,2021), cosFormer(Qin et al.,2022b), Syn(D), Syn(R)(Tay et al.,2021a), gMLP(Liu et al.,2021), S4(Gu et al.,2022a), DSS(Gupta & Berant,2022), RWKV-v4(Peng et al.,2023), LRU(Orvieto et al.,2023), HGRN1(Qin et al.,2023c), TNN(Qin et al.,2023a), and Mamba(Gu & Dao,2023). All reported results are from our own runs under the same settings.

Table5shows the results. HGRN2 clearly outperforms HGRN1 but slightly underperforms Mamba.

We conducted language modeling experiments with 1.3B and 2.7B parameters on the Slimpajama dataset(Soboleva et al.,2023), using theFlashLinearAttention(Yang & Zhang,2024)codebase for training.444Model checkpoints are available athttps://huggingface.co/fla-hub.The results, shown in Table6, demonstrate that HGRN2 consistently outperforms other competitive linear recurrent models across three model scales. This suggests that HGRN2 provides a superior parameterization compared to GLA, as both models share an identical recurrent structure.

We also conducted experiments on the Pile dataset. First, we trained 150M, 350M, and 1B HGRN1 and HGRN2 models for 100B tokens, and the results are shown in Table7. We observe that HGRN2 consistently outperforms HGRN1.

Next, we scaled the token horizon to 300B and trained strong baseline models, Mamba and LLaMA, under the same settings for comparison. We also compared them against several open-sourced language models, such as OPT(Zhang et al.,2022), Pythia(Biderman et al.,2023), BLOOM(Scao et al.,2022), and RWKV-4(Peng et al.,2023). We found that HGRN2 performs competitively with Mamba, LLaMA, and other open-sourced LLMs.

To evaluate long-context abilities, we conducted tests on SCROLLs(Shaham et al.,2022)and found that HGRN2 exhibits better scaling behavior compared to Mamba, indicating stronger long-context capabilities, potentially due to its larger recurrent state size. However, we also observed that the 7B HGRN2 model is still not as strong as the LLaMA model, suggesting that the scaling behavior of linear models for long-context modeling remains an area for further study.

To test the retrieval ability of our trained 3B models, we ran the easy mode of the Needle in a Haystack Test.555In this mode(Shen,2024; Shen et al.,2024), both the question and answer (QA pair) are embedded within a lengthy text, challenging the model to locate and respond to the query. This mode is particularly suitable for base models without instruction tuning. In contrast, the standard mode only places the answer within the long context, requiring the model to understand the question and find the relevant answer.LLaMA almost achieves perfect retrieval performance for evaluation lengths no greater than the training length. As shown in Figure4, HGRN2 and Mamba still face difficulties in retrieval tasks; however, HGRN2 outperforms Mamba due to its larger state size, enabled by linear attention-styled state expansion.

SECTION: 4.3Long Range Arena

Long Range Arena(Tay et al.,2021b)is a benchmark designed to assess a model’s ability to handle long-range dependencies. We used HGRN1’s configuration and compared it with existing methods, as shown below.

Table10shows the results. HGRN2 outperforms HGRN1, while Mamba and Griffin failed to achieve high accuracy on this benchmark.

SECTION: 4.4Image Modeling

For the image classification task, we followed the configuration of HGRN1 and trained it on ImageNet-1k, comparing it with TNN and the vanilla transformer.

Table11shows the results. HGRN2 outperforms HGRN1 with a similar parameter size, while also demonstrating an advantage over previous TNN(Qin et al.,2023a)and DeiT models(Touvron et al.,2021).

SECTION: 5Related work

Linear recurrent models mainly include linear RNNs, state-space models, and linear attention. State-space models (SSMs) are gaining great attention since the seminal work S4(Gu et al.,2022a)and its more efficient diagonalized version(Gu et al.,2022b).
Despite excellent performance in the LRA benchmark, it has been shown to have inferior performance in language modeling. Gating mechanisms have been shown to be crucial in improving SSMs’ language modeling performance(Mehta et al.,2023; Wang et al.,2022; Gu & Dao,2023).Gupta et al. (2022)build the connection between SSM and linear RNN.Orvieto et al. (2023)proposes a linear RNN layer (i.e., LRU) inspired by SSMs.Peng et al. (2023)successfully scale linear RNN models to billions of parameters for the first time.

For linear attention models, their language modeling performance has been underperforming softmax attention for a long time. Several improvements have been proposed to bridge the performance gap: (i) incorporating the forgetting mechanism(Peng et al.,2021; Schlag et al.,2021; Sun et al.,2023; Qin et al.,2023b; Yang et al.,2023; Peng et al.,2024), (ii) using local attention(Qin et al.,2022a; Zhang et al.,2023; Arora et al.,2024; Ren et al.,2024), (iii) using higher-order polynomial feature map(Arora et al.,2024; Kacham et al.,2023)to make the resulting attention distribution more sharp(Zhang et al.,2024), (iv) using more expressive yet efficient recurrent update rule(Schlag et al.,2021; Yang et al.,2024; Liu et al.,2024; Sun et al.,2024a).

Martin & Cundy (2018)first proposed a minimal gated linear recurrent layer and showed how to use the parallel scan algorithm to train linear RNNs in sequence-level parallel.Qin et al. (2023c)is largely based on this work with several adaptations and highlights the importance of data-dependent decay.De et al. (2024)build their model on LRU(Orvieto et al.,2023)and replace data-independent decays with data-dependent ones. They further use sliding-window attention to boost the performance. These models are limited in recurrent state size.

Gated recurrent models with matrix-valued recurrent state have been investigated in the literature of Neural Turing Machine (NTMGraves et al.2014) and linear Transformer(Katharopoulos et al.,2020). In NTM, the number of memory slots can be regarded as the state expansion ratio discussed in this work. NTM also included data-dependent decays in the form oferase vectors. However, NTM is hard to parallelize and thus slow to train in practice. The linear transformer is known to have the recurrent form(Katharopoulos et al.,2020)and is known to be closely related to fast weight programming (FWPSchlag et al.2021). Gated FWPs have been investigated sinceSchlag & Schmidhuber (2017); Zhang & Zhou (2017), and have recently been revisited inPeng et al. (2021); Mao (2022); Yang et al. (2023); Katsch (2023); Pramanik et al. (2023). In particular,Yang et al. (2023)proposed a hardware-efficient training algorithm for these types of models.

More recently, Mamba2(Dao & Gu,2024), xLSTM(Beck et al.,2024), and Gated Retention(Sun et al.,2024b)have shown that sharing data-dependent decays across different dimensions within the same head is effective. This approach improves efficiency over GLA because intra-chunk computations are more amenable to tensor core-based matrix multiplication acceleration, at the cost of sacrificing the fine-grainedness of decays. In GLA/HGRN2, each head dimension has its own decay rate, whereas in Mamba2/xLSTM/Gated Retention, all dimensions share the decay under a single head. It is an interesting question to study how much improvement fine-grained decay will bring.

SECTION: 6Conclusion

In this work, we propose HGRN2, an enhancement of HGRN(Qin et al.,2023c)using an outer product-based state expansion mechanism inspired by linear attention, enabling efficient training. Experiments across multiple tasks validate the advantages of HGRN2 over HGRN1.

SECTION: Acknowledgement

We thank Yu Zhang for conducting some language modeling experiments and for the valuable discussions.

SECTION: References

SECTION: Appendix AAppendix

SECTION: A.1Experiment Configurations

In Table12, the experiment configurations provided detail setups for both Auto-regressive Language Modeling (ALM) and ImageNet (IM) experiments, focusing on the WikiText-103 and ImageNet-1k datasets, respectively. ALM experiments utilize Byte Pair Encoding (BPE) with a vocabulary size ofand sequence length of, featuring a total batch size ofandupdates. ImageNet experiments differentiate between 6 million and 23 million parameter models, with total batch sizes ofand, both running forepochs but with differing warm-up periods. Optimization strategies vary between Adam for ALM and AdamW for IM, with specific learning rate schedulers and hyper-parameters tailored to each model’s scale. Additional configurations outline variations in model complexity, fromtomillion parameters, adjusting layers, hidden dimensions, and GPUs used, aiming to comprehensively explore model performance across scales and setups.

SECTION: A.2Loss curve of HGRN2

The training loss curves for the HGRN2 models of different sizes—150M, 385M, and 1B, as shown in Fig.5, which as the number of parameters increases, the model’s performance improves, with the 1B model consistently outperforming the others.