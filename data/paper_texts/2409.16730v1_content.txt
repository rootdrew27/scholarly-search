SECTION: 

SECTION: Non-stationary BERT: Exploring Augmented IMU Data For Robust Human Activity Recognition

Human Activity Recognition (HAR) has gained great attention from researchers due to the popularity of mobile devices and the need to observe users’ daily activity data for better human-computer interaction.
In this work, we collect a human activity recognition dataset called OPPOHAR consisting of phone IMU data.
To facilitate the employment of HAR system in mobile phone and to achieve user-specific activity recognition, we propose a novel light-weight network called Non-stationary BERT with a two-stage training method. We also propose a simple yet effective data augmentation method to explore the deeper relationship between the accelerator and gyroscope data from the IMU. The network achieves the state-of-the-art performance testing on various activity recognition datasets and the data augmentation method demonstrates its wide applicability.

SECTION: IIntroduction

Human activity recognition (HAR) has been an important research area for decades and plays a crucial role in many applications, such as human-computer interaction, human behaviour analysis, and ubiquitous computing[1,2,3].
In recent years, advancements in sensing analytics of mobile devices have driven the rapid development of human activity recognition. They provide opportunities for continuous tracking of physiological signals and boost seamless communication between humans and machines[4].
Inertial measurement units (IMUs), which contain accelerometers and gyroscopes, are typically electromechanical or solid-state devices that detect linear acceleration and angular velocity[5].
They are widely applied in mobile devices due to their adaptability and simplicity[6].

By utilizing the data from the IMU implemented in mobile devices, real-time human activity recognition becomes feasible.
Due to the power and computing limitations of mobile devices, classifiers for HAR are typically lightweight. Various Machine Learning (ML) algorithms, RNN, CNN, Transformer, and mixed-architecture models are proposed[7,8,9,10,11,12].

In alignment with the development of HAR systems, several datasets are introduced, including UCI[13], shoiab[14], mhealth[15].
Prior studies[16,17,18]also apply data augmentation methods on IMU data, e.g., adding random noise, rotation, flipping, bias, physical transformation, etc. These data augmentation methods increase the diversity of the training data, thereby mitigating over-fitting and improving the generalization ability of the HAR model.

However, the prior works have several drawbacks.
Firstly, existing models frequently struggle to adapt to real-world scenarios as each user has his own movement pattern. A user might want to record some specific but uncommon activity, such as skiing. Previous models need to be trained from scratch to accommodate a new activity.
Secondly, they do not delve deeply enough into the relationship between accelerator and gyroscope data for a robust data augmentation method.

To address these drawbacks, we implement a two-stage inference pipeline. As depicted in the Figure1, the model separates the pretraining of the Encoder and the finetuning of the Classifier. Pretraining always requires a significant amount of time, thus can be done in the cloud; Finetuning can be conducted on users’ mobile devices as we want to use the user-labeled activity data to train their own classifiers, which also ensures user privacy.

In this work, we have three primary contributions:

We propose a new human activity recognition dataset named OPPOHAR, which encompasses a diverse range of human activities using the phone in various gestures.

We introduce an effective data augmentation method tailored for processing IMU data.

We propose a lightweight network optimized for distributed deployment for HAR, dedicated to privacy-protecting user-specific activity recognition.

SECTION: IIDataset

SECTION: II-ACommon Activities

We define seven common human activities for mobile devices: staying still, walking, walking up and down stairs, running, cycling, taking car, and taking subway. For convenience, we name them respectively as activity A, B, C, D, E, F, and G. These activities cover a large variety of scenarios with phones in human daily life, making the dataset practically valuable. Data are collected from seven different activities mentioned above by two collectors. Take the activity walking as an example, we use 2 different devices and different hands holding the phone to collect data at 3 different speeds, slow, medium, and fast. We illustrate the duration of time for each activity in our dataset as in the first two rows of TableI.

SECTION: II-BUncommon Activities

We also construct an uncommon dataset including activities distinguished from daily activities. We define six distinct hand gestures holding mobile devices: rotating the phone like a circle, tracing a ”W” shape in the air, tracing a ”Z” shape in the air, vigorously shaking the phone up and down, tapping the phone from behind, gently shaking the phone from side to side. For convenience, we name them respectively as activity a, b, c, d, e, and f. These activities deviate from typical phone usage behaviors and serve as triggers for specialized phone functions, enabling quick access to certain functions of their devices.
Each activity mentioned above is recorded in ten sets collected by three collectors.
Within each set, the activity will be repeated continuously approximately 20 times. In the last two rows of TableI, we illustrate the duration of time for each activity in our dataset.

SECTION: IIIMethodology

In this section, we present the overall design for our Non-stationary BERT (Bidirectional Encoder Representations from Transformers[19]) network.
As depicted in Figure1, it includes two phases: self-supervised pretraining and supervised classifying. Inspired by the previous work LIMU-BERT[20], we adopt a similar architecture, which contains three parts: a BERT-like Encoder, a Decoder, and a Classifier.

During the self-supervised pretraining phase, raw IMU data is first pre-processed and then sent to the Encoder-Decoder module for sequence recovery task. The NS-BERT Encoder is designed to learn an effective hidden representation of the input sequence and the Decoder is designed to recover this time sequence based on former hidden representation.
For the supervised classification phase, we freeze the Encoder and replace the Decoder with a Classifier for human activity recognition. The network is finetuned for the HAR task.
The pretraining network can be trained online and later send Encoder to all users. Classifiers can be trained on user mobile devices.

SECTION: III-AData Augmentation

The accelerator data and the gyroscope data from IMU present different characteristics of the activity. Previous work[21,22,23,17]do not consider the inter-relationship between them. They usually concatenate them and send them directly to the network, or use two encoders to extract the features respectively and concatenate them with later fusion.

Instead, we propose an efficient data augmentation method, FM (Factorization Machine[24]).
Consider a time sequence from IMU data,andrepresent respectively the accelerator data and gyroscope data of time step i and of axis j, where j and k come from axis x or y or z. We select a sequence ofand a sequence ofand multiply them at each time step and get a new sequence calledas in equation below:

As the accelerator and gyroscope have each 3 axes, we have a total of 9 new time sequences, which are also regarded as the input. These created feature sequences contain hidden trends of data fluctuation, facilitating the neural network to discover the underlying laws between them.

With a detection window of 6 seconds, the raw IMU data is split into sequences. After data normalization, we got data in a shape like, where T, SR, and FS denote time length, sampling rate, and feature shape respectively. In this scenario, the feature shape is 15. In the following Embedding phase, we project feature shape from 15 into embedding size.

SECTION: III-BModel Architecture

Time series is commonly non-stationary, which means that the statistical properties and joint distribution of time series can change over time.
For IMU data of HAR, its statistical properties can change drastically when the users change the posture of the device or their state of motion. Eliminating the impact of such events can help improve predictability.
Previous works[25,26]implement stationarization to handle time series prediction tasks.
Borrowed from the idea of Non-stationary Transformer[27], instead of using self-attention[28], we pioneeringly apply the Series Stationarization and De-stationary Attention operations to a BERT-like network to handle IMU data, which enhances predictability of the series and keeps non-stationary information of the original series simultaneously.

The Series Stationarization module includes the Normalization and De-normalization modules as shown in Figure2.
For each input batch, the Normalization module of NS-BERT encoder records the mean, and the standard deviationof each sequence.
S denotes sequence length and E denotes the number of features.

After the normalization, the input is more stationary and predictable. Thus the encoder input turns fromto, and they have such relationship:

To recover the non-stationary information, De-stationary Attention approximates the original attention by introducing two de-stationary factors. In the following calculation, we regardasandas. The original attention score without normalization can be deduced as follows:

Two de-stationary factors are learned by MLP using,and raw data. By introducing these factors, we can recover the non-stationary information deep in the model, which improves its performance in predicting real-world time series.

The task of the Decoder is to reconstruct the original values of IMU sequences with the representations extracted by Encoder. During the training period, we use a Mean Square Error loss to optimize.

At the output side of decoder,andare used to transform the model output,. De-normalization module operates as follows:

SECTION: IVExperiment and Results

SECTION: IV-AEvaluation

We select our OPPOHAR and another three public datasets, including UCI, Mhealth, and Shoaib for evaluation. We choose accuracy and F1-score as our metrics.
All datasets are equally down-sampled to 20Hz and sliced into non-overlapping windows with a length of 120 units, which is a time span of 6 seconds.

For comparison with our NS-BERT, we choose LIMU-BERT[20], DCNN[30], Deepsense[31],
GRU[29]and LSTM[32].
For NS-BERT and LIMU-BERT, they both adopt a two-stage training method. The preliminary experiments show that the GRU classifier achieves the best results for finetuning them. Thus, in subsequent experiments, we rename NS-BERT as NS-GRU, indicating we use GRU for the classifier, the same as LIMU-GRU.

To ensure the experiment’s fairness and prevent over-fitting, different training epochs are chosen according to the size of the datasets.
For LIMU-BERT and NS-BERT, the number of pretraining epochs are 1500-5000.
The classification training adopts 1000-1200 epochs. All models utilize the same dataset for training, validation, and testing.
The learning rate in the pretraining phase and classification phase are 0.0001 and 0.001, respectively. The batch size is 2048.

In TableII, our baseline model achieves the best performance among all models.
Compared with the baseline LIMU-GRU, which does not have stationarization mechanism, our model NS-GRU shows a great performance improvement, proving the effectiveness of Non-stationary structure.

SECTION: IV-BAblation Study

To evaluate the effectiveness of the data augmentation method we presented inIII-A, we compare the accuracy of models with and without data augmentation. For simplicity, we only compare the models whose classifier phase uses GRU. TableIIIpresents the performance of NS-GRU, LIMU-GRU, and GRU on four datasets before and after data augmentation. “FM” denotes using data augmentation method. The number in bold indicates higher performances.
All models’ performance has greatly improved with the data augmentation method. According to our research, our model NS-GRU-FM achieves the state-of-the-art performance on UCI and OPPOHAR datasets in current open-source methods.

To facilitate user-specific activity recognition, we pretrain NS-GRU on common activities and finetune on uncommon activities as all other models. As shown in TableIV, our model achieves the best results among all models, suggesting the effectiveness of the pretrained Encoder for special action recognition.

For training the classifier on mobile devices, faster convergence will reduce the time and power consumption. Figure3shows the training loss of different models on common activities of OPPOHAR dataset with epochs. Results show our NS-GRU-FM model converges fastest, indicating the effectiveness of learning better representation of our network architecture and data augmentation method. This ensures the possibility of finetuning the classifier on user devices.

SECTION: VConclusion

In this paper, we present a smartphone-IMU-based human activity recognition dataset named OPPOHAR.
To enable the user-specific activity detection and to promote the implementation in mobile devices, we propose Non-stationary BERT network and a new data augmentation method for IMU data.
Experiment results show the effectiveness of NS-BERT model and the universality of the data augmentation method.

SECTION: References