SECTION: MaterialPicker: Multi-Modal Material Generation with Diffusion Transformers
SECTION: Introduction
High-quality materials are a core requirement for photorealistic image synthesis. We present a multi-modal material generator, conditioned on a text prompt and/or an image. The image can be a photograph containing a material sample captured at any angle, potentially distorted or partially occluded. Our model lets users “pick” materials from any photograph just by outlining a rough crop square around the material sample of interest.

Traditional material acquisition often requires tens or hundreds of photo samples under known light conditions and camera poses. Even with recent advances in material acquisition allowing single or few image(s) capture, strong restrictions on the capture conditions are imposed. These methods typically require a flash light co-located with the camera as the only light source, and/or a fronto-parallel view of a flat sample. Even methods designed for capture using non-planar photographscannot handle significant texture distortion in the input photographs. Many recent material generation methods are trained from scratch on synthetic materials, limiting the generation diversity due to limited datasets, as compared to general-purpose text-to-image diffusion models.

We propose to tackle these challenges with two new ideas. First, we create a dataset which contains 800K crops of synthetic scene renderings, textured with randomly assigned materials, with each crop paired with its ground truth material. Using this data, we can train our model for the “material picking task”, with various observation angles and distortion. We additionally use a text-to-material datasetcontaining 800K pairs of text descriptions and associated ground truth material maps, encouraging material generation diversity and resulting in a multi-modal generator that can accept images, text or both.

Second, we re-purpose a text-to-video generation model to generate material maps instead. We use a Diffusion-Transformer (DiT) based architecture, which has been shown to be effective for high-quality video synthesis. However, our target domain is materials, which we represent as a set of 2D maps (albedo, normal, height, roughness, metallicity). To adapt our base DiT model, trained on videos, to materials, we finetune it by considering each material map as a “frame” in a video sequence. This approach preserves the strong prior information in the video model, improving our method’s generalization and robustness.

We evaluate our model on both real and synthetic input images and compare it against the state-of-the-art methods for texture rectification, material pickingand text-to-material generation. We show that our approach generates materials that follow the input text prompt and/or match the appearance of the material sample in the input image, while correcting its distortions.
In summary, we make the following contributions:

We propose a material generation model which uses text and/or image prompts as inputs, while being robust to distortion, occlusion and perspective in the input image.

We design a large-scale dataset of crops of material samples paired with the corresponding ground truth material maps, enabling our model to handle a range of viewing angles and distortion.

We adapt a Diffusion Transformer text-to-video model for material generation by treating material maps as video frames, preserving the original prior knowledge embedded in the model to generate diverse materials.

SECTION: Related Work
SECTION: Material Acquisition and Generation
While material acquisition has been a long standing challenge, lightweight material acquisition and generation have seen significant progress using machine learning. Various methods were proposed to infer PBR material maps from only a single or few photographs. However, these methods rely on specific capture condition using a flash light co-located with the camera location.propose to use natural illumination but doesn’t support direct metallic and roughness map estimations. Further, these methods rely on the camera being fronto-parallel or very close to it. This kind of photographs require specific captures, making the use of in the wild photos for material creation challenging.

As an alternative to create materials, generative model for materials were proposed. GAN-based approachesshow that unconditional generation of materials is possible and can be used for material acquisition via optimization of their noise and latent spaces. Recent progress in generative model, and more specifically diffusion models, enabled more stable, diffusion based, material generators. Such diffusion models can also be used to support material acquisition tasks, for example when paired with ControlNet.
All these diffusion-based approaches either attempt to train the model from scratch, using solely synthetic material dataor significantly alter the architecture of the original text-to-image model, preventing the use of the pre-existing priors in large scale image generation models, limiting their generalization and diversity. Further, image prompts are limited to fronto-parallel photographs, which requires a specific capture.

Other methods leveraged transformers as a model for material generationbut focused on procedural material, which relies on generating functional graph generation, a very different modality. These procedural representations have resolution and editability benefits, but cannot easily model materials with complex texture patterns in the wild. In contrast, our model supports generating materials from any image or text prompt and produces varied, high-quality material samples.

SECTION: Material Extraction and Rectification
Different methods were proposed to rectify textures or generally enable non-fronto-parallel textures as input. Some approachesaim to evaluate the materials in an image through a retrieval and optimization method. Given an image, they retrieve the geometries and procedural materials in databases to optimize their position and appearance via differentiable rendering. Closest to our work is Material palette, targeting material extraction from a single photo, not restricted to fronto parallel images. The method leverages Dreamboothoptimized through a LoRAon Stable Diffusionto learn a “concept” for each material. This lets them generate a texture with a similar appearance to the target material and use a separate material estimation network to decompose the texture into material maps. However, this LoRA optimization step takes up to 3 minutes for each image, and we find that our approach reproduces better the target appearance.

A related field is that of texture synthesis from real-world images. Wu et al.present an automatic texture exemplar extraction based on Trimmed Texture CNN. VQGANachieves high resolution image-to-image synthesis with a transformer-based architecture. These methods however do not support the common occlusions and deformations that occur in natural images. To tackle this limitation,propose to rectify occlusions and distortions in texture images via a conditional denoising UNet with an occlusion-aware latent transformer. We show that our approach yields better texture rectification and simultaneously generates material parameters.

SECTION: Diffusion Models and Diffusion Transformers
Diffusion modelsare state-of-the-art generative models, showing great results across various visual applications such as image synthesis and video generation. The core architecture of diffusion models progressed from simple UNets, incorporating self-attention and enhanced upscaling layers, prior-based text-to-image model, a VAEfor latent diffusion models (LDM)and temporal attention layers for video generations. These image generation methods all rely on a U-net backbone, a convolutional-based encoder-decoder architecture.

Recently, transformer-based diffusion models, Diffusion Transformers (DiT) were proposed, benefiting from the scalability of Transformer models, removing the convolutions inductive bias. PixArt-presents a DiT-based text-to-image that can synthesize high resolution images with low training cost. Stable Diffusion 3demonstrates that a multi-modal DiT model trained with Rectified Flow can achieve superior image synthesis quality. Compared to the U-net architecture, the DiT shows greater flexibility in the representation on the visual data, which is particularly important to video synthesis tasks. Sora, a DiT-based video diffusion model, encodes video sequences as tokens and uses transformers to denoise these visual tokens, demonstrating the ability to generate minute-long, high-resolution high-quality videos. We adapt a DiT-based video generation model for our purpose and show that it can be flexibly transformed into a multi-channel material generator.

SECTION: Method
SECTION: Diffusion Transformer
Diffusion models are generative models that iteratively transform an initial noise distribution (e.g. Gaussian noise) into a complex real-world data distribution (e.g., images, or their encodings).
The diffusion process relies on aprocess that progressively transforms the original data distribution into a noise distribution. For example, this can be achieved by iteratively adding Gaussian noise to the data sample. Given data samples, corrupted dataare constructed indiffusion steps.

To sample the original data distributionfrom the noise distribution, amappingneeds to be modeled whereis the noise predicted at each step by a neural network. The neural networkis conditioned on the denoising stepto predict the noise, which is then used to reconstructfromin each reverse step:

whereis conditional inputs (e.g., text prompts or images).

We use a Diffusion Transformerarchitecture as a backbone to model. The visual datais tokenized patch-wise, resulting in visual tokenswhereare the spatial and temporal dimensions of the video,is the number of tokens andis the feature dimension. Positional encoding is also added toto specify spatial and temporal order. Any conditionis also embedded as tokenswhereis the number of the tokens for conditional inputs. For example, whenis a text, it is encoded by an pre-trained encoderwith additional embedding layers to map it into the same feature dimension. The transformeris trained to denoise each patch at timestep. The final denoised patchesare reassembled as visual dataafter decoding through linear layers. Since the number of tokens grows quickly with resolution, we use a variational autoencoder (VAE) modelbefore the tokenizing process, producing a latent representation ofof the original datafor the transformer to process.

SECTION: Datasets
To train our material generative model, we propose two datasets,and. Together, these datasets enable joint training for both surface rectification and high quality material generation.

For thedataset, we build a set of synthetic indoor scenes with planar floors, walls, and randomly placed 3D objects, such as cubes, spheres, cylinders, cones, and toruses, similar to random Cornell boxes. Each object is randomly assigned a unique material from around 3,000 stationary (i.e., approximately shift invariant) materials. Using this approach we create a dataset of 100,000 high-resolution rendered images, with different kinds of light sources, including point lights and area lights, to simulate complex real-world illumination (see Fig.). We randomly place cameras to capture a wide variety of view points and maximize coverage.

We further crop the rendered images to construct training data, including input images, corresponding material maps, binary material mask, and the material name as an optional text prompt. During cropping, we ensure that the dominant material occupies at least 70% of the region. Importantly, we rescale the material maps based on UV coordinates to ensure that the rendered crops and target material maps share a matching texture scale. After cropping, this dataset contains 800,000 text-image-mask-material tuples.

As ourdataset only contains stationary materials, it may fail to represent the full diversity of textures in the wild. To enhance the generalization capability, we use an additionaldataset, which we augment to 800,000 cropped material maps. We use the name of the materials as the text prompts for text-to-material generation. These data items can be thought of as text-material pairs. This additional data diversity leads to significant improvement for non-stationary textures in input photographs as discussed in Sec..

SECTION: Generative Material Model
We employ a pre-trained DiT-based text-to-video generative model, with an architecture similar to the one described in, as a base model. We retarget it into a multi-channel material generator.

To retarget this model while preserving its learned prior knowledge, we stack the material maps(albedo map, normal map, height map, roughness map and metallicity map) into a “video” of 5 frames , and compute the temporal positional embedding assuming their time stamp interval is 1 e.g., fps=1. Since DiT flexibly generates tokenized data, as opposed to a U-net architecture, the number of frames it is able to produce is not fixed, allowing us to adapt the original video generator to generate the right number of “frames” to meet our requirement. Our proposed use of a video model is in contrast to image diffusion models which typically generate 3 channels (RGB) and need to be non-trivially adjusted to generate more channels.

To enable material generation from an image input, we consider the input imageas the first frame, with the model generating the stacked material mapsas the subsequent frames, similar to a video extension model. Recall that the input image can be captured with arbitrary camera pose, and may include perspective and distortions. Using this approach, the self-attention mechanism of the transformer ensures that the generated material parameters are aligned with each other, and allows for non-aligned pixels between the input image condition and the generated material maps. The model simply learns that all frames except the first need to be aligned. This property is key for texture rectification, which is challenging for convolution-based architectures, in which (approximate) pixel alignment between input and output is built in due to the convolution inductive bias.

We additionally train our material generator to produce a segmentation mask for the dominant material in the crop. Typically, the user-provided crop is not entirely covered by a single material (see Fig.). Performing conservative cropping on an image may reduce the number of usable pixels, while using an additional segmentation mask requires additional user input or a separate segmentation model. Instead, our model automatically identifies the dominant materialin the image. We add a maskto be inferred from the input image as the second frame. Our training datacan thus be represented as, where; we have 7 RGB frames: input, mask, and five material maps. Noiseis applied only to the last six frames occupied byand, resulting in, with the first frame (input image) remaining free of noise. Our objective from Eq.is

wheredenotes the text input (material description). This process can be seen as a completion of the frames (mask and material channels) given the input image and text condition. The notationrefers to the last 6 frames generated by the Transformer.
When the input consists solely ofwithout,whereis a uniformly white RGB image. The computation of the loss remains unchanged.

SECTION: Training and Inference
We finetune the pre-trained DiT model using the AdamW optimizer on 8 Nvidia A100 GPUs. The learning rate is set atwith an effective batch size of 64. The model is finetuned onresolution for about 70K steps, which takes 90 hours. During training, we feed data from our two training datasetsandin a 5:3 ratio, prioritizing the task of image-conditioned material generation. We also randomly drop the conditions to retain the capacity to use classifier-free guidance (CFG). For text-only or unconditional generation, the mask is replaced by a completely white image placeholder.

Our model completes a generation in 12 seconds using 50 diffusion steps. The model natively outputs a resolution of 256 due to limited computational resources. We apply an upsamplerto increase the resolution of each material map to 512 × 512.

SECTION: Results
We evaluate the performance of our MaterialPicker across multiple dimensions. First, we perform qualitative and quantitative comparisons with Material Paletteon material extraction using both synthetic and real-world images (Sec.). Next, we compare with a texture rectification method on real-world images (Sec.) and with MatGenand MatFuseon text-to-material generation (Sec.). Finally, we conduct ablation studies on multi-modality, dataset design and evaluate the impact of the input image scale(Sec.). We also ablate the usage of a mask and evaluate robustness to distortion and lighting/shadowing in the supplemental materials.

SECTION: Evaluation dataset and metrics
For systematic evaluation, we build a synthetic evaluation dataset by gathering a diverse set of 531 materials from PolyHaven, applied to three interior scenes from the Archinteriors collection(which are completely independent from our training set). For each scene, we sequentially apply the 531 collected materials to a designated object inside the scene, and render 2D images using Blender Cycleswith the scene’s default illumination setup. We generate a total of 1,593 synthetic renderings, and crop a square around the location of the object with replaced materials.

To validate the generalization of our models, we curate an evaluation dataset containing real photographs captured by smartphones. This dataset covers a comprehensive set of real-world materials observed under both natural outdoor lighting and complex indoor illumination. We crop the photographs with a primary focus on our target material, without strictly limiting the cropping boundaries.

Since we do not target pixel-aligned material capture, per-pixel metrics cannot be used for our results. Instead, we focus on theof the materials extracted from the photo inputs. Following related work on high-fidelity image synthesis such as DreamBooth, we leverage CLIP-I, which is the average pairwise cosine similarity between ViT-L-14 CLIPembeddings of two sets of images. We also use the DINO metricto measure the average pairwise cosine similarity between ViT-L-16 DINO embeddings. Additionally, we report the FID scoreto measure the statistical visual similarity between two image distributions. We compute the FID score for each of the 531 output material map sets against the corresponding ground truth material map sets, and average the FID scores over the three scenes in our synthetic evaluation dataset.

SECTION: Image Conditioned Generation.
We evaluate the performance of our model on both synthetic images and real photographs. We first show a visual comparison with the state-of-the-art method Material Paletteon our synthetic evaluation dataset (Sec.). Since Material Palette generates only three material maps (albedo, normal, and roughness), we present both qualitative and quantitative results for these channels, along with the re-rendered images using these generated material maps. Our method takes 12 seconds to generate a material while Material Palette takes 3 minutes, on the same Nvidia A100 GPU, a 15 times speedup. Furthermore, our model can generate materials in batches. In Fig.we show that our model produces material maps with a closer texture appearance and better matching the ground-truth material maps. In contrast, Material Palette struggles to reconstruct structured textures often resulting in distorted lines. We also observe that in the rendered images, our generated materials better matches the original input images.

We include a quantitative comparison with Material Palette on the entire synthetic dataset in Tab.. We find that our proposed model performs better on all three metrics for the vast majority of the generated material channels and the corresponding re-rendered images.

We show qualitative evaluation on real photographs in Fig.where we can see that our model generalizes well to photographs of materials from various angles. We render the generated materials on a planar surface under environment lighting, showing strong visual similarity to the original input images. Unlike Material Palette, which requires input masks from a separate segmentation step, our model operates out-of-box with an input image only, showcasing its potential as a lightweight.

Since our model automatically performs perspective rectification on the generated materials, we further compare against another state-of-the-art texture rectification and synthesis method. In Fig., we evaluate both methods using real photographs. Since our model directly outputs material maps, instead of textures, we present our results by rendering them under different environment maps. We find that the compared method doesn’t generalize well to real-world photographs, taken from non-frontal and/or non-parallel setups and fails to correct distortion in these cases. In contrast, our approach synthesizes a fronto-parallel view and remains robust across various real-world lighting conditions and viewing angles. Finally, as previously, our model does not require detailed masks as input, directly rectifying the dominant texture in the input image.

SECTION: Text Conditioned Generation.
Although the primary focus of our method is the generation of materials from photos, our multi-modal model also supports text-conditioned generation without image inputs. We evaluate its performance on the text-to-material task, comparing it with two state-of-the-art diffusion-based generative models for material synthesis: MatFuseand MatGen. As shown in Fig., our model demonstrates strong text-to-material synthesis capability, producing high-quality material samples, comparable to other state-of-the-art approaches. Leveraging a pretrained text-to-video model as a prior, our model can interpret complex semantics beyond the material-only training set, such as ”wood rings” and ”floral” patterns.

SECTION: Ablation Study
Our generative material model takes advantages of its multi-modality. Though it is designed to create material maps from input photographs, it can benefit from additional signal to reduce the ambiguity of a single in-the-wild photograph. We present different combinations of input conditions in Fig.including 1) text condition only; 2) image condition only and 3) text+image dual conditions. We found that text conditioning provides high level guidance for material generation. On the other hand, image conditioning contains ambiguities, as lighting and camera poses are uncontrolled. Combining both options enables text prompts to guide the model in identifying the reflective properties of a material. For instance, by prompting the model with appropriate text, it can better differentiate between metallic and non-metallic materials, as shown in the third example in Fig..

In Sec., we introduce two datasets used to train our model. To confirm that using both datasets help, we train a variant using only thedataset. Since this dataset primarily contains stationary materials, training exclusively on it reduces our model’s generalization for complex texture patterns commonly found in real-world scenarios as shown in Fig.. By mixing additional training data, our model synthesizes more diverse texture patterns and features such as woven pattern or the texture of a manhole cover.

Reproducing the texture scale in the input photos is critical for material generation. As we process our training data to align the scales of input images and output material maps (Sec.), our model generates scale-matched materials, as shown in Fig.. We see that our result follow the scale of the input as it increases from top to bottom.

SECTION: Limitations
Despite strong generation capacity, our model may still encounter challenging inputs, as shown in Fig.. In the first row we show an example where our model confuses shading and albedo variation. Our model may also have difficulty handling materials with cutouts or holes, since it does not produce opacity maps as outputs. Finally, preserving semantically meaningful patterns, such as text, is a remaining challenge in our approach.

SECTION: Conclusion
We present a generative model for high-quality material synthesis from text prompts and/or crops of natural images by finetuning a pretrained text-to-video generative model, which provides strong prior knowledge. The flexible video DiT architecture lets us adjust the model for multi-channel material generation. We show extensive evaluation on both synthetic and real examples and conduct systematic ablation studies and test on robustness. We believe that our re-purposing of a video model for multi-channel generation opens an interesting avenue for other domain which require the generation of additional channels, such as intrinsic decomposition.

SECTION: References
SECTION: More Results
SECTION: Image Conditioned Generation
SECTION: Ablation Study
Input

Output

Input

Output

SECTION: Evaluations on the Robustness.
SECTION: Additional Real Examples