SECTION: Exploring Transformer-Based Music Overpainting for Jazz Piano Variations

This paper explores transformer-based models for music overpainting, focusing on jazz piano variations. Music overpainting generates new variations while preserving the melodic and harmonic structure of the input. Existing approaches are limited by small datasets, restricting scalability and diversity. We introduce VAR4000, a subset of a larger dataset for jazz piano performances, consisting of 4,352 training pairs. Using a semi-automatic pipeline, we evaluate two transformer configurations on VAR4000, comparing their performance with the smaller JAZZVAR dataset. Preliminary results show promising improvements in generalisation and performance with the larger dataset configuration, highlighting the potential of transformer models to scale effectively for music overpainting on larger and more diverse datasets.

SECTION: 1Introduction

Music overpainting is a generative task that creates variations of a musical excerpt, with the aim of preserving its melodic and harmonic structure[1,2]. Models trained for this task have the potential to be adopted as GenAI (Generative AI) creative tools in music composition, enabling composers to generate new variations from existing musical excerpts. However, overpainting has primarily been explored using small-scale datasets, and there has been limited research into how well models can generalise to larger datasets, largely due to the scarcity of data for the task.

To address this gap, we are developing a large-scale structured dataset for solo piano jazz performances, aligned with lead sheets, as described in[3]. This dataset aligns lead sheets to both the head and solo sections of these jazz performances, providing more data for exploring Music Information Retrieval (MIR) tasks, such as structural analysis and music generation. Existing jazz datasets like JAAH and JSD provide structural annotations but focus on solo sections or multi-instrument performances[4,5]. The Weimar Jazz Database offers solo excerpts but lacks specificity for solo piano performances[6]. The dataset we are developing aims to overcome these limitations by providing detailed, structured data for solo piano jazz analysis. Additionally, we are developing an automatic data collection and structuring pipeline to facilitate the creation of large-scale datasets in general, which can be extended beyond jazz to other musical genres.

As part of our work on the large-scale dataset project, we developed a semi-automatic pipeline to extract ‘variations’ from jazz piano performances, by aligning small sections from lead sheets with corresponding performances from the head section, based on the method described in[1]. This semi-automatic pipeline led to the creation of VAR4000, a subset of the larger dataset project. In this Late-Breaking Demo, we use VAR4000 to apply a transformer-based model to the music overpainting task, exploring how the model performs on a larger and more diverse dataset[7].

In this paper, we present preliminary work that investigates how different configurations of a transformer architecture influence model performance when applied to a larger dataset, compared to the JAZZVAR dataset. Our aim is to work towards establishing a new baseline for music overpainting, particularly for future studies involving larger and more diverse datasets, and to assess the model’s ability to generalise to these datasets. We explore two model configurations on the VAR4000 dataset, comparing their performance and scalability. Our preliminary results show how model performance varies across configurations, providing insights into potential improvements for handling larger datasets and enhancing model robustness.

SECTION: 2Methodology

SECTION: 2.1Data

We use a subset of a large-scale dataset we are compiling for music generation and music information retrieval tasks. This subset consists of 4,352 pairs of ‘Original’ and ‘Variation’ MIDI data. Similar to the JAZZVAR dataset, the ‘Original’ segments are 4-bar melody and chord excerpts taken from a lead sheet transcription of a jazz standard. The ‘Variation’ segments are extracts from audio jazz piano performances, which are semi-automatically aligned to the ‘Original’ segments using Viterbi decoding[3]. The deep chroma features of the audio are compared to the chord symbols from the lead sheet. Using these alignments, we extract the corresponding MIDI transcriptions from the PiJAMA dataset[8]. These segments are then verified through human evaluation. For ease of reference, we will refer to this subset as VAR4000. We perform the same data transposition and augmentation as in[2]on VAR4000. This process increased the sample size from 4,352 pairs to 52,224 pairs. The data was tokenised with RemiPlus[9]. In comparison, JAZZVAR contains 505 data pairs, and when augmented the sample size increased to 6,060 pairs.

SECTION: 2.2Training Setup

We explored two configurations of a transformer-based model architecture[7]. Model 1 consisted of 2 layers, a hidden dimension of 64, 8 attention heads, and a feed-forward dimension of 256. Model 2 had 4 layers, a hidden dimension of 128, 8 attention heads, and a feed-forward dimension of 512. Both models were optimised using Adam with an initial learning rate of 1e-3, adjusted via a learning rate scheduler, and a batch size of 16. Early stopping was applied, with Model 1 training for 131 epochs on JAZZVAR and 80 epochs on VAR4000, while Model 2 trained for x epochs. All models were trained on one NVIDIA RTX A5000, and nucleus sampling was used to generate the outputs[10].

SECTION: 3Preliminary Experiments and Discussion

The musical feature evaluation metrics, including Pitch Class Entropy (PCE), Pitch Range (PR), Polyphony (P), Number of Pitches (NoP), and Pitch in Scale (PS), were calculated for both the JAZZVAR and VAR4000 datasets to understand their distribution[11]. As shown in Table1, VAR4000 Variations exhibit higher PCE and a significantly larger Pitch Range than JAZZVAR Variations, suggesting greater pitch diversity and broader musical variation. Polyphony remains consistent between the datasets, with VAR4000 Variations slightly more polyphonic.

We trained the JAZZVAR dataset using Model 1 and applied the same configuration to VAR4000 for comparison. Table2shows the feature metric comparison across the entire test set. Notably, the outputs from Model 2 are closer to the manually annotated JAZZVAR variations, suggesting Model 2’s results align more closely with human-generated variations.

Five outputs from each model’s test set were also qualitatively evaluated by listening to assess how well the generated outputs retained the melody and harmony of the primer input.
Preliminary results indicate that VAR4000 outperformed JAZZVAR in Model 1, demonstrating better scalability to the larger dataset. Model 2, tested exclusively on VAR4000, performed better than both JAZZVAR and VAR4000 in Model 1, showing that a more complex architecture significantly improved performance.

These findings highlight the potential for enhanced scalability with more complex transformer architectures, particularly as access to larger datasets increases. Additionally, Model 2 exhibited better generalisation to VAR4000, which is critical for real-world applications of GenAI in music composition, where users are likely to input diverse musical material into the model. An improved evaluation pipeline was also implemented to mitigate data leakage, providing a more reliable measure of the model’s ability to generalise to unseen data.

SECTION: 4Conclusion and Future Work

In conclusion, we investigated the performance of a transformer-based model on VAR4000, a subset of a larger dataset that we are currently working on, in comparison to the smaller JAZZVAR dataset. While the results are promising, further work is needed to improve the scalability and generalisation of the model. Expanding the dataset and experimenting with additional configurations will help optimise performance for larger datasets.

We plan to explore a custom loss function to enhance output quality by focusing more on the original segments during training. Improved evaluation metrics are also necessary to better assess model performance. Future work will include subjective evaluations with composers and jazz experts to refine the model and explore its potential beyond jazz.

SECTION: 5Acknowledgments

Eleanor Row is a research student at the UKRI Centre for Doctoral Training in Artificial Intelligence and Music, supported by UK Research and Innovation [grant number EP/S022694/1].

SECTION: References