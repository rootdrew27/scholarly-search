SECTION: Recurrences reveal shared causal drivers of complex time series

Unmeasured causal forces influence diverse experimental time series, such as the transcription factors that regulate genes, or the descending neurons that steer motor circuits.
Combining the theory of skew-product dynamical systems with topological data analysis, we show that simultaneous recurrence events across multiple time series reveal the structure of their shared unobserved driving signal.
We introduce a physics-based unsupervised learning algorithm that reconstructs causal drivers by iteratively building a recurrence graph with glass-like structure.
As the amount of data increases, a percolation transition on this graph leads to weak ergodicity breaking for random walks—revealing the shared driver’s dynamics, even from strongly-corrupted measurements.
We relate reconstruction accuracy to the rate of information transfer from a chaotic driver to the response systems, and we find that effective reconstruction proceeds through gradual approximation of the driver’s dynamical attractor. Through extensive benchmarks against classical signal processing and machine learning techniques, we demonstrate our method’s ability to extract causal drivers from diverse experimental datasets spanning ecology, genomics, fluid dynamics, and physiology.

SECTION: IIntroduction

Diverse biological and engineered systems exhibit descending control, in which a central driving signal causally influences response variables—which cannot, in turn, influence the driver. In ecology, environmental changes trigger complex cascades across food webs[1]; while in neurobiology, descending neurons convey cognitive imperatives to motor circuits that implement behavior[2,3,4].
In systems biology, master regulators such as the Hox gene family orchestrate numerous complex developmental processes[5]. The ubiquity of top-down control may arise from robustness and parsimony, because it reduces the number of dynamical interactions needed to steer a high-dimensional dynamical system[6]. However, top-down control introduces experimental challenges when the driving variable is not knowna priori, or is not directly measurable.

Reconstructing an unknowndriver, given time-resolved observations of one or moreresponsevariables, therefore represents a fundamental computational problem with diverse applications in biology and engineering.
This problem is ostensibly ill-posed, because natural systems dissipate information through noise and other irreversible processes.
However, the theory of dynamical systems provides circumstances in which partial information about an unknown driver may, in principle, be recovered.
Conceptually, a driving signal continuously encodes information about itself into the responding systems. Driver-response coupling therefore acts as a lossy transmission channel, motivating recent causal detection methods grounded in information-theoretic quantities like the transfer entropy[7,8,9]. Alternatively, causal relationships can be inferred based on properties of the dynamical system that generates a time series: theorems from nonlinear dynamics show that partial information present in observable subsystems reveal subregimes within the driving signal[10], motivating methods that determine causality based on attractor reconstruction or unsupervised learning of latent representations[11,12,13,14,15,16,17], building upon classical observability conditions for dynamical systems[18,19,20].

However, many existing methods detect only the average strength and directionality of causal interactions among known variables. The more general, and difficult, task of finding time-resolved causal forces remains an open problem. Yet understanding how causal forces change over time is necessary to understand control schemes, particularly in biological systems with sparse or infrequent control signals[21,22]. Time-resolved driving signals may be detected based on signal denoising or assimilation techniques, or by generalizing statistical causality measures with time-dependent kernels[10,23,24]. However, these methods fail to leverage existing theoretical works on skew-product dynamical systems and chaotic synchronization[25,26,27], which provide strong inductive biases for physically-motivated representations, even given highly-corrupted measurements.
As a result, existing temporal causality methods require large datasets, and fail to provide physical insight—when is driver reconstruction possible, and where is this information stored in the response signals?

Here, we show that recurrences—brief intervals in which time series appear to repeat themselves—represent key dynamical motifs that encode information about unobserved variables driving a time series. Using this finding, we introduce a general-purpose, physics-based unsupervised learning algorithm that reconstructs an unknown driver time series from multiple observed response time series. Based on the intuition that response recurrences imply driver recurrences, we convert time series to networks using tools from computational topology. Traversing this graph reveals temporal progressions of driver states, even from highly nonlinear or noisy response measurements. Our method applies to both discrete- and continuous-time signals and both oscillatory or aperiodic driving, and we show that its underlying accuracy stems from an ergodicity-breaking percolation transition in the time series adjacency graph. We extensively compare our technique to existing methods based on mutual information, artificial neural networks, and spectral transformations, and find that our method obtains strong results on diverse datasets spanning electrophysiology, hydrodynamics, and systems biology. We show that our method’s accuracy arises from its ability to identify and approximate unstable periodic orbits that the driving signal transiently shadows—linking our algorithm’s performance to topological properties of the underlying dynamical systems.

SECTION: IIApproach

SECTION: II.1Recurrence structure of skew-product causal systems

Suppose an unknown signaldrivesresponse dynamical systems,. The response signalscomprise nonlinear measurements. We seek to reconstruct the original drivergiven.

This problem is underdetermined when the measurement operatoris noninvertible, or when each response variableonly very weakly couples to the driver. However, if the driver state lies on an attractor, while theresponse signal lies on an attractor, then the overall dynamics haveskew productstructure with combined attractor. While individualare not directly observable, the skew product reconstruction theorem suggests that a one-to-one mappingmay be constructed from the observed signalby applying a liftto the signal using time delays, whereandare hyperparameters chosen using a variety of heuristics[18,19,20]. Nonlinear generalizations of the lifting mapmay be obtained with attractor reconstruction techniques like singular value decomposition, kernel methods, or autoencoder neural networks[10,28,29,30].

Recent work by Sauer shows that the one-to-one mapping betweenandrequires that a recurrence in thereconstructed response,implies a recurrence in the driving signal; however, the converse does not hold true due to the skew product structure[27]. Thus, tracking recurrence events in a time series driven by an ergodic dynamical system gradually reveals information about the driver attractor[16]. Longer observations or additional independent response measurements both provide more opportunities for recurrences.

Sauer distills this insight into an exact algorithm for reconstructing a discrete-time periodic driver given noiseless responses. Given a reconstructed response time series,, then near-recurrences correspond to two timepointsandwhere, whereis a small constant acting as an adjustable hyperparameter for the method. If a given response seriesrecurs at a set of times, and another response seriesrecurs at a set of times, then if any timepoints are shared betweenand, thenalltimepoints in either set belong to the same equivalence class and thus correspond to the same unique driver state. As a result, each one of thetimepoints observed across theresponse time series can be labelled by their equivalence class, resulting in a time series of driver states.

However, while this algorithm is motivated by fundamental properties of skew-product systems, it cannot readily be applied to most experimental time series because it requires discrete-time, low-period, and noise-free signals where each observed timepoint can unambiguously be assigned to a single equivalence class. Any spurious recurrences (e.g. from noise) violates the disjointedness of equivalence classes, precluding reconstruction.

SECTION: II.2SHREC: Generalized shared recurrence detection for real-world time series

We observe that classical skew product reconstruction discovers disjoint sets within an aggregated time series adjacency graph: each lifted response time serieshas distance graph. Classical exact reconstruction merges thesegraphs into a consensus graph,and then thresholds atto produce the binary adjacency matrix. Driver state identification therefore requires graph partitioning, a difficult combinatorial optimization problem[31]. Strict partitioning precludes detection of continuous drivers, and renders the method fragile to spurious recurrences induced by noise.

Our key insight is that skew product reconstruction describes the long-time behavior of an ensemble of random walkers navigating the time series adjacency graph. Distinct driver states induce kinetic partitions associated with discrete basins of attraction, or equivalence classes. Exact skew product reconstruction therefore represents a low-temperature limit of a more general set of algorithms for analyzing time series. We refer to the resulting class of algorithms as SHREC (SharedRecurrences).

Improving adjacency estimation.We first note that identification of recurrence events may be generalized with more robust criteria for recurrence. Rather than setting a fixed distance thresholdfor recurrence, an adaptive threshold would compensate for uneven spacing of timepoints, or nonuniform density of the underlying dynamical attractor[32]. Here, we opt to construct a more robust connectivity matrix using a higher-order network embedding method recently introduced for topological data analysis[33]. For thetimepoint of theresponse time series, we first determine the distance to itsnearest neighbors. We compute the closest neighbor’s distance, and then numerically solve for the valuesuch that, where. The calculated,values then transform the distance matrixinto an affinity matrix. When calculated in this manner,encodes a fuzzy simplicial complex capturing the local density of the attractor[33]. Unlike static recurrence plots based on, the learned parameters,modify each row ofin a context-dependent manner. Having computed fuzzy simplicial complexes for each individual response, we find the consensus recurrence graph.

We use this particular adjacency calculation in our subsequent experiments. We choose to use simplicial complexes due to their empirical success[33]and small number of trainable parameters. In experiments, we find that simplicial complexes are more robust to noise than recurrences detected at a fixed threshold(Appendix G). In general, other heuristics may be used to construct, depending on prior information available about the time series, underlying attractor, or dataset-dependent factors. For example, works on simplex-based forecasting scale distances to neighborsusing a fixed exponential kernel, implicitly assuming uniform density of the underlying attractor[12]. Other works on driver reconstruction calculate a geodesic distance among points, implying that the original embedding distances are less informative than the graph traversal distances[34,35]. More generally, calculation ofrepresents a metric learning problem, which can be learned directly from data with a probabilistic learning model such as a variational autoencoder, if sufficient data is available[36].

Generalizing equivalence classes for discrete-time drivers.We next note that identification of dynamical equivalence classes represents a graph clustering problem. A discrete-time driver cycles among a finite set of symbols, and so driver reconstruction represents the problem of finding a permutation-invariant labeling ofnodes with one ofsymbols. We seek a labelling that maximizes connectivity of same-symbol nodes underwhile minimizing connectivity between different symbols. For any choice of objective (e.g. modularity, betweenness), optimal assignment of labels represents min-cut, an NP-hard combinatorial optimization problem. Heuristic solutions are provided by community detection algorithms[37], which cluster groups of nodes based on measures of connectivity and similarity within the network.

For our experiments, we use the recently-introduced Leiden algorithm[38], which balances performance with empirical accuracy in large networks. We find that the performance of our method is insensitive to the choice of community detection algorithm (Appendix H). The Leiden algorithm has time complexity, whereis the number of nonzero edges in. However, for some datasets, custom community detection algorithms could be used instead. For example, for noiseless periodic driving, the recurrence graph approaches a regular structure, for which specialized algorithms detect communities more efficiently[39]. Likewise, if certain timepoints are known to correlate strongly with particular driver states (as may occur due to experimental interventions), supervised community detection algorithms may be more accurate.

Continuous-time drivers.We next note that the concept of dynamical equivalence classes may be generalized to continuous-time driving. Instead of cycling among a finite set of states, a continuous-time driver traverses a continuous manifold of proximal states. To smoothly partition the recurrence graph, we first compute the graph Laplacian, where. We next compute the subleading eigenvector of, resulting in a continuous labelling of thenodes in the consensus recurrence graph.

In our experiments, we identify driver states using spectral decomposition of the graph Laplacian via Arnoldi iteration. This approach is the most general, becauseis not necessarily symmetric (a consequence of using fuzzy simplicial complexes instead of traditional recurrence matrices); but it precludes the use of faster methods like Lanczos iteration. This method also has the sametime complexity as discrete driver reconstruction. However, for specialized datasets, our approach could be generalized by first transforming the graph Laplacian; for example, powers of the Laplacian would highlight slower-timescale driver dynamics. Similarly, pruning the recurrence network may improve driver estimation in systems with intermittent control.

Physical Interpretation.Both the discrete- and continuous-time driver reconstruction procedures describe the long-time behavior of random walks on the consensus graph. The consensus graphdefines the state transition matrix of a discrete- or continuous-time Markov chain, which propagates distributions of random walkers initialized at different nodes. Repeated application of this operator evolves the walker distribution towards a stationary distribution associated with its largest eigenvalue. If the graph fully partitions into disconnected subgraphs, walkers remain kinetically trapped within their initial basins, revealing an invariant measure associated with the driver. This measure provides the equivalence classes in exact driver reconstruction. However, for most real-world datasets containing noise or corrupted measurements, the transition matrix fails to exhibit fully-disjoint structure due to spurious recurrences that induce leakage among basins associated with distinct driver states. The leading eigenvector of the transition matrix instead approaches the uniform distribution at long times. However, driver structure persists within transients associated with subleading eigenvectors—this information disappears at long times because ergodicity is not fully broken. These weaker partitions are known as“almost-invariant sets”in fluid dynamics[40], and they correspond to subregions within the graph where random walkers become trapped for extended durations before slowly escaping into other subregions[41,42,43]. Recurrence networks thus represent a natural representation of time series, providing a strong inductive bias for downstream analysis tasks.

We demonstrate the key steps of our method in Figure1by using the dynamics of the chaotic Rössler attractor to drive multiple realizations of the Lorenz equations with random dynamical parameters. In order to make the reconstruction task more difficult, we filter the Lorenz response dynamics through random Gaussian kernels, resulting in sparse, heavily-corrupted measurements that confound linear reconstruction methods like principal components analysis.

SECTION: II.3Related work and limitations

Generalizations of time-delay embeddings for non-autonomous systems have previously been used to infer causal structure using likelihood-based methods, particularly for ecological time series[44,45]. A variety of methods use attractor reconstruction to detect causal forces[11,12,13,14,15,16,17]; other works use recurrence events and topological motifs within time series[46,47]. However, these prior methods determine the average strength and direction of causal interactions between time series, while our method instead seeks a time-resolved driver. Among time-resolved methods, our algorithm conceptually resembles prior methods that aggregate recurrence graphs, and then identify contiguous paths through the resulting network using the Isomap algorithm[48,49,34,35,50]; however, instead of static recurrence graphs we use a consensus graph of fuzzy simplicial complexes, and unlike prior works our traversal method has a dynamical interpretation as a diffusive flow (for continuous-time drivers) or jump process (discrete-time). Our reconstruction process conceptually resembles pseudotime, a topological graph ordering method used in systems biology to infer developmental trajectories from gene expression measurements of unsorted cell populations[51]. Other works note that generalized synchronization between two time series reveals fixed causal graphs[52,25,26,53].
When the exact model class of the driver is known, generalized synchronization further reveals the full temporal driver dynamics, because temporal driver reconstruction requires only identifying a finite number of variables (like phase and frequency) that parameterize the driver[54,55,56]. Instead of treating each response variable as a dynamical system, recent works on temporal Bayes filtration and neural data assimilation treat response observations as instantaneous nonlinear samples from a latent process[57,58,59,60,61,62]. Our emphasis on incremental information gain from recurrences distinguishes our approach from multiview and cross-embeddings[63,64,11,13,14,15,16,17], as well as from generalizations of static causal network inference to temporal networks[65,24,66,10,23].

In contrast to strong causality associated with interventional studies and counterfactual analysis[67], our work detects weak causality as skew product coupling in settings where only observational data is available. Like other observational causal discovery methods[68,44,45,48,13], our approach bears restrictions regarding theidentifiabilityof the driving signal: in the absence of interventions, it is impossible to differentiate among sets of potential causal drivers that exhibit equivalent influences on the dynamics. For example, for the system, the true driving signal forcould be defined as,,or. Likewise, given a multivariate driving signal, it is impossible to distinguish among rotations and combinations of potentially independent causal coordinates, though overembedding with time-delays mitigates some ambiguity[69]. While our method readily accommodates driver-synchronized response systems, it requires mutual desynchronization among a subset of response systems,for some, in order to avoid the degenerate solution.

SECTION: IIIResults

SECTION: III.1The shared recurrences algorithm identifies causal drivers in diverse datasets

We first perform large-scale benchmarks that demonstrate the practical advantage of our approach. We will show that our physics-based approach, which reduces to exact dynamical equivalence classes in the limit of zero noise, exceeds the performance of simpler methods, while matching the performance of less-interpretable methods based on artificial neural networks. Rather than highlighting raw performance, we aim to emphasize how a physically-motivated, recurrence-based approach provides a strong inductive bias for time series decomposition, allowing strong results to be obtained from a relatively simple computation.

Our benchmark experiments consist of training our method and other methods on only the response time series, in order to produce an estimate of the driverfor each benchmarked method. We compare each reconstruction to the ground truth driverto assess relative method performance. We emphasize that our technique is anunsupervisedlearning method: in a real-world setting, no ground truthis available, underscoring the importance of benchmarks and validation on datasets with known drivers.

For our benchmarks, we consider a diverse range of datasets with causal structure.Turbulence:Trajectories of diffusing tracer particles advected by a chaotic double gyre flow while also experiencing Brownian motion. The driver is long-wavelength sinusoidal forcing, and the responses are the radial coordinates of individual particles (as would typically be observed in Doppler velocimetry).ECG:A fetal electrocardiogram. The driver is the maternal respiration signal, while the responses are multi-point electrocardiogram recordings placed across the fetal body[70].Gene Expression:Stochastic gene expression dynamics in a subset of the yeast metabolic gene regulatory network, as generated by a microbiology experiment design suite[71]. The driver is the expression level of a known transcriptional regulator in after a knockdown perturbation, and the responses are the expression levels of downstream genes. These time series represent longer versions of the those used in DREAM4, a standard benchmark for regulatory network inference[72].Plankton:Plankton species abundances in an alpine lake in Switzerland over ayear period, as the ecosystem undergoes temperature fluctuations[73]. The driver is the monthly average temperature, and the response variables are individual plankton species abundances.of all values in this dataset are missing.

We compare our method to a variety of alternative models: principal components analysis (PCA), independent components analysis (ICA), canonical correlation analysis between past and future values (CCA), simple averaging (Mean), a Kalman filter (Kalman), and a surrogate-weighted ensemble Fourier transform (fPCA)[10]. Recently-introduced dynamical decomposition methods include dynamical components analysis (DCA), which finds low-dimensional modes that maximize predictive information[74]; slow-feature analysis (SFA), which seeks linear combinations of nonlinear kernels that minimize time variation[75]; Gaussian-process factor analysis (GPFA), which finds latent factors parametrizing the mean of a one-step correlated Gaussian process[76]; and a linear Gaussian state space model (LG-SSM)[77]. Another recent causality detection approach (HN-Isomap) first thresholds and merges each response’s traditional recurrence graph (not the context-dependent recurrence matrix used here) and then computes a weighted affinity graph using Dijkstra’s algorithm[34,35]. We also consider artificial neural networks in the form of a causal convolutional network (cCNN), an established architecture for time series representation learning that trains a custom convolutional filter on the response time series[78]; and Latent Factor Analysis via Dynamical Systems (LFADS), a recurrent variational autoencoder[79]. We do not directly benchmark our method against Sauer’s original exact algorithm, which only applies to noise-free discrete-time datasets; on the real-world datasets considered here, Sauer’s method produces a monotonically-increasing trendline, because nearly every timepoint is assigned to a new equivalence class.

For all benchmark models we tune hyperparameters using cross-validation and grid search on a separate training dataset than the held-out testing data. We do not vary any hyperparameters for our proposed method, SHREC. We useaccuracy metrics to compare the reconstructed driverto the true driver: mean-squared error, Spearman correlation, covariance, symmetric mean absolute error, mean absolute scaled error, dynamic time warping distance, mutual information, and nonlinear Granger causality; all yield comparable results, and so we report Spearman correlations for simplicity. Additional metrics and benchmark experiments are included inAppendix C.

We observe strong performance of our approach relative to other methods—both conceptually and computationally simpler methods such as PCA or SFA, but also artificial neural networks like LFADS (Fig.2). A comparably strongly-performing benchmark model, the causal autoencoder neural network, takes the hierarchical and sequential structure of time series data into account[78], but otherwise does not specifically exploit skew product structure in the underlying time series. Additionally, because our method requires only matrix decomposition and not hyperparameter tuning or iterative gradient descent, it ranks among the fastest methods to compute relative to its performance. Our method performs particularly well on the-corrupted plankton dataset. Our method consistently appears among the highest-performing methods across all datasets, a particular strong result given that our approach is purely physics-based—it reduces to exact dynamical equivalence classes in the limit of noise-free, discrete-time driving. Our benchmark results therefore highlight the strong inductive bias of our method for skew-product systems.

Across the experimental datasets, we find that aperiodic driving signals are consistently harder to reconstruct by all methods, with the gene expression and plankton datasets showing the lowest score ranges. The presence of multiple significant but widely-spaced timescales presents a more challenging problem setting, which motivates our exploration of unstable periodic orbits in SectionIII.6. Interestingly, while the turbulence dataset contains a monochromatic periodic driver, linear methods like the Fourier transform or PCA are unable to detect this signal due to relatively high noise in the observed data. Many of our example datasets represent cases in which the reconstructed driving signalexerts nearly-instantaneous influence on the response dynamics,. Known as the synchronized regime in prior work[80,81], this regime is most appropriate for benchmarks both because of the causal identifiability limitation (discussed above), and because the synchronized limit represents the most competitive setting for instantaneous latent variable models like PCA and SFA.

SECTION: III.2The shared recurrences method identifies higher-order interactions among time series

We next consider a setting in which SHREC recovers drivers inaccessible to traditional time series analysis methods.
Many biochemical circuits contain higher-order interactions, in which combinations of upstream factors jointly regulate downstream targets[82,83]. Unlike traditional approaches to observational causality detection, in which a graph fully captures the relationship among time series, higher-order terms require a hypergraph encoding relationships among combinations of nodes. This problem has seen little prior attention in the context of causal reconstruction, but it bears particular relevance to gene expression, in which individual genes’ temporal (or pseudotemporal) dynamics are directly observable, but the regulatory effects of combinations are not. We thus expand our gene expression example from the previous section, by including higher-order terms, in which the product of two genes drives each downstream gene’s dynamics. This multiplicative interaction appears in multi-input motifs, a common feedback mechanism in transcription networks[83]. However, few approaches to gene regulatory network inference directly detect it. Here, we show that the purely physics-based driver reconstruction of SHREC identifies higher-order information, allowing it to outperform both baseline causality methods and specialized network inference methods in identifying higher-order drivers.

We modify an established approach to simulating dynamics of gene networks, in order to include higher-order interactions (seeAppendix I). We simulate a set ofgenes, in which the product of two genes regulates the dynamics of the remaining genes by random positive or negative amounts. We then apply SHREC to the set oftime series, and we then correlate the resulting reconstructed driverwith each of theindividual gene time series, and denote the topgenes as the identified regulator genes. We report the accuracy as the fraction of the topmost correlated genes that correctly match the true upstream genes. However, while we use this accuracy metric to benchmark our approach, the results do not change if we instead assess quality by directly comparing the reconstructed driver and true multiplicative driver using the same metrics used in the previous section’s benchmarks (Appendix I). However, using a node-based accuracy criterion allows us to further benchmark our approach against two standard baseline gene regulatory network inference techniques, GENIE3 and ARACNE[84,85]. These methods infer aninteraction matrix given a set oftime series. For these algorithms, we use the two nodes with the highest outdegree as the method’s estimate of the driver.

We find that SHREC matches or exceeds the performance of other methods in identifying the genes responsible for higher-order driving (Fig.3). The simple, physics-based approach of SHREC thus allows it to perform surprisingly well on established problem in network science with particular relevance to bioinformatics.
Many standard network inference methods, including GENIE3 and ARACNE, detect statistical dependencies among genes in a network, and ignore the dynamical context available in time series[86].
In contrast, SHREC has a strong inductive bias for dynamical systems, allowing it to perform well on this problem, where the sequential structure of a time series is informative.
Thus while other, specialized gene network inference methods could potentially perform better, we emphasize that SHREC’s strong performance comes without any fine-tuning or specialization.

SECTION: III.3Driver reconstruction precedes effective synchronization

To probe the physical interpretation of driver reconstruction, we consider a synthetic task in which the driver and responses exhibit varying degrees of synchronization. We consider the forced Kuramoto model, a classical model of synchronization among phase oscillators,

whereis the phase of theofoscillators, andis the amplitude of an external driving force with intrinsic frequency. The frequencies are randomly-drawn from a normal distribution; when all oscillators are identical (), the system always approaches a globally-stable synchronized statefor all.

When the oscillators are heterogenous (), Eq.1exhibits a well-studied synchronization transition as the couplingincreases. When, synchronization occurs when. Analytical reductions of Eq.1forshow that, when, synchronization instead occurs at a critical coupling strength, which is given by a complex but exact expression in terms of,, and the detuning factor[87,88]. In Fig4, we show this transition at fixed. We quantify synchronization using the time-averaged Kuramoto order parameter,

where the synchronized caseimplies long-term phase-locking (for all), while the desynchronizd stateimplies that the phases are uncorrelated on average.

For each value of, we apply the continuous-time version of SHREC to the time series, and compute the Spearman correlation between the reconstructionand true driver(Fig.4, blue trace). As expected, our method readily detects the driver in the synchronized regime, where all responses mirror the driver. However, our method also successfully detects the driving signal one decade below the synchronization threshold. This regime corresponds to partial synchronization in the Kuramoto model, in which groups of oscillators temporarily entrain and intermittently approach the driver frequencywithout becoming phase-locked. The intermittent coincidence of recurrences due to these temporary entrainments provides a noisy signal of the driver, providing sufficient information for the shared dynamics method to recover it.

SECTION: III.4Chaotic drivers hinder reconstruction, but chaotic responses improve it

In order to further understand which systems are more difficult to reconstruct, we perform an additional benchmark using our recently-introduced database oflow-dimensional ordinary differential equations[89]. Curated from published literature to include common systems like the Lorenz, Rössler, and Chua attractors, the dataset has grown through crowdsourcing to span diverse domains such as astrophysics, climatology, and biochemistry. Each system is annotated with calculations of descriptive mathematical properties such as maximum Lyapunov exponents, fractal dimensions, and entropy rates, which quantify different aspects of each system’s underlying complexity.

We use our dataset to create random skew-product systems: one randomly-selected system is chosen to drive multiple randomly-initialized versions of another randomly-selected system. The Lorenz-Rössler system featured in Figure1represents one such skew-product system from thesampled pairs. We scale the coupling strength to a constant multiple of the average amplitude of both systems, and we scale integration timesteps and trajectory lengths based on precomputed estimates of Lipschitz constants and dominant timescales annotated with each system within our database. Of all the possible pairs of systems, we retain only time series from theskew product systems that exhibit complex dynamics. For each skew product system, we apply our reconstruction method in order to estimate reconstruction accuracy for that particular driver-response pair.

Across all pairs, we observe weak but consistent negative correlation between the driver Lyapunov exponent and reconstruction accuracy, implying that more chaotic systems are more difficult to reconstruct (Fig.5A). Surprisingly, we observe the opposite for response systems: more chaotic response systems improve reconstruction quality. As a result, a particular dynamical system that proves easier to reconstruct, will represent a worse observable for other systems (Fig.5B).

This discrepancy arises from the dual roles of the driver and response systems: more chaotic response dynamics produce more diverse and thus informative time series, leading to more unique recurrence events. Conversely, a non-chaotic response () yields only a finite amount of driver information over an extended duration, or across many replicates. Less chaotic systems thus hinder reconstruction because they discover new information about the driver at a slower rate. However, more chaotic drivers transmit information at a faster rate, thus requiring more information to be gained from the response time series. This concept of the driver as an information source, and response systems as receivers, mirrors classical formulations of skew-product systems[90,91]. The entropy production rate of a chaotic system is bounded from below by the largest Lyapunov exponent[92], which acts as a proxy for the information transmission rate between the driver and response. In the limit of perfectly-lossless transmission from driver to response system, the systems would exhibit generalized synchronization, in which the response becomes a function of solely the driver state[68,93].

SECTION: III.5Accurate reconstruction requires a percolation transition in the recurrence network.

Our reconstruction method implicitly defines a diffusion process on the recurrence network. In order to understand how this dynamical process couples to the dynamics of the driver itself, we apply the discrete-time variant of our algorithm to systems driven by the logistic map as it undergoes a series of period-doubling bifurcations leading to chaos (Fig.6A). We track the point-wise accuracy of the reconstructed discrete driver using the adjusted Rand index, which accounts for the permutation invariance of labels: an adjusted Rand index ofimplies a driver state labelling with no greater accuracy than random chance, while a score of one implies that every driver state is correctly identified for each of thetimepoints tested. We record our approach’s accuracy as a function of three control parameters: the relative amount of stochasticity present in the response dynamics (signal-to-noise ratio), the strength of coupling between the response dynamics and driver, and the number of response time series used for reconstruction. In the latter case, we naively expect that reconstruction accuracy will increase proportionally to the total number of recurrences observed and thus, resulting in an accuracy scaling given by the-distribution, which describes the distribution of the expected value of the mean of a binomial process in which driver recurrence states are discovered at random.

We observe that shorter-period drivers are generally easier to reconstruct (Fig.6A). For higher-period drivers, we observe a staircase pattern in the accuracy as reconstruction quality improves. This occurs because, at lower accuracies and longer periods, our approach fails to differentiate all driver states, resulting in groups of states merging together into a coarse-grained, lower-period driving signal. These clusters split into distinct driver substates as reconstruction quality improves (Fig.6B).

In order to understand how changes in method accuracy depend on structural properties of the underlying recurrence graph, for each condition we measure the scaled largest connected component, whereis the total number of timepoints andis the number of nodes comprising the largest connected component of. This order parameter detects percolation in finite-size undirected networks[94], and in our system it reveals a first-order phase transition preceding an increase in accuracy when any of the three control parameters is varied. Intuitively, a highly-connected time series network contains many equivalent paths between two nodes, producing ambiguity regarding the correct assignment of recurrence events to driver states. This degeneracy below the percolation transition produces an underdetermined reconstruction, which becomes determined as the amount or quality of response datasets increases. Considering a diffusion process on the recurrence graph, increasing any of the three control parameters increases the duration of transients that reveal almost-invariant structure. The percolation transition therefore indicates the onset of weak ergodicity breaking, in which the dynamics transition from having a short mixing time to a much longer time dominated by gradual leakage between highly-connected groups of nodes sharing the same driver state.

Our observations mirror recent results reporting equivalency between the fitting accuracy of artificial neural networks and the jamming transition[95]. By decreasing the ratio of data to model size, a neural network passes from underfitting to overfitting regimes mirroring a dilute-to-jammed transition. In our system, increasing training data size or effective size (by decreasing noise or increasing coupling) increases final accuracy by making the time series adjacency graph more dilute, and thus ensuring that distinctions emerge among different driver states. High-period driver states initially appear as low-period states at the onset of the percolation transition, an observation reminiscent of renormalization in period-doubling systems[96].

SECTION: III.6Reconstruction shadows dominant unstable periodic orbits.

We next consider the process by which our method learns to reconstruct diverse signals. We return to our original demonstration, in which the Rössler dynamical equations drive many randomly-corrupted measurements of the Lorenz system. We repeat the reconstruction while varying the number of response series, and thus the reconstruction accuracy (Fig.7).

Given our method’s emphasis on inferring driver recurrence states, we speculate that its accuracy implicitly depends on the unstable periodic orbit spectrum of the driver[26]. Classical work on nonlinear dynamical systems has shown that any chaotic system can be decomposed into an infinite set of unstable periodic orbits[97], which trajectories shadow for extended durations proportional to each orbit’s relative stability. While chaotic systems have continuous power spectra, the set of valid unstable periodic orbits bears basic topological restrictions due to attractor geometry—making it a countable, albeit infinite, hierarchical measure for the dynamical system. We compute several unstable periodic orbits of the Rössler driving system using the method of closest recurrences[98], and select the three most dominant orbits based on the relative duration they are shadowed by the driver dynamics.

Comparing the pairwise distance matrix of the reconstructed signal with the distance matrix of individual unstable periodic orbits suggests that, as the amount of training data increases, the reconstructed distance matrix initially approximates only the most dominant orbit, but then gradually approximates finer-scale details of the driver attractor that are encoded in secondary orbits. We hypothesize that the recurrence structure of our method biases it towards shorter-period and more stable orbits that dominate the graph partition. This phenomenon provides some context for our earlier observation that reconstruction accuracy depends inversely on the driver’s Lyapunov exponent: systems with larger Lyapunov exponents transition among a larger set of orbits, thus requiring more recurrence information to produce faithful reconstructions[99].

SECTION: IVDiscussion

We have presented a theoretically-motivated unsupervised learning method that reconstructs an unobserved causal driving signal, given observations of downstream response variables. Across extensive benchmarks, we find that our method quickly and robustly reconstructs diverse drivers from datasets ranging from fluid dynamics to systems biology. Beyond introducing a new data analysis method, our work provides empirical insight into the nature of information transmission in coupled dissipative systems: the topology of the driver’s characteristic unstable orbits manifests as structure within the recurrence graph of the response time series, with the efficiency of this linkage mediated by both the chaoticity of the driver, and the diversity of the responses. A diffusion process on the resulting time series graph reveals driver structure in its longest-lived transient modes, and methods that increase the lifetime of these transients—such as more response datasets, increased driver-response coupling, or reduced measurement noise—produce a corresponding increase in reconstruction accuracy. Diffusion represents a heuristic solution to the difficult combinatorial optimization problem of graph traversal and node sorting, and our method reduces to a previously-known exact algorithm in the limit of a noiseless and discrete periodic driver[27].

Limitations of our approach therefore stem from our use of diffusion for graph traversal. Certain measurement types may induce false recurrences with systematic structure, such as sensor saturation or memory effects, that impose fixed structural barriers to diffusion on the graph adjacency matrix—akin to inhomogenous diffusivity. Mitigating such effects requires sufficient information about the measurement dynamics to reconstruct and correct for response bias; within our framework, this might require preconditioning the graph Laplacian[100]. Additionally, non-stationary driving signals can complicate reconstruction; for example if the attractor is time-dependent (e.g. energy gradually dissipates due to drag), then the driver may exhibit transient chaos before settling into a state of quiescence or oscillation[101]. In this case, a random walker’s transitions among nodes will no longer heed detailed balance and the transition operator will exhibit non-Hermitian structure, resulting in long-lived nonequilibrium steady-states that complicate driver state assignment[102,103,104]. These and other, more challenging data types may require nonlinear alternatives to diffusion processes on the recurrence graph. Our use of diffusion therefore represents an inductive bias in the time series representations learned by our model.

However, the problem of discovering nonlinear processes from an empirical recurrence graph introduces a variety of potential generalizations grounded in recent results from statistical learning. Instead of directly fitting a linear operator, future work could leverage recent methods for data-driven inference of dynamical propagators[105,106,107,108,36,109,110], which can even produce reduced-order analytical models[111]. Generalizations may also incorporate alternative graph representation or traversal methods (such as message passing)[66,112,113], such as graph neural networks[114,115]. Thus, beyond a specific algorithm, our work motivates general parametrization of time series in terms of recurrences, and the latent orbit structure that they encode.

SECTION: VAcknowledgments

We thank Ski Krieger, Denitsa Milanova, Phil Morrison, Anthony Bao, and Rob Philips for helpful discussions. WG was supported by NSF DMS 2436233 and Marble Therapeutics. This project has been made possible in part by grant number DAF2023-329596 from the Chan Zuckerberg Initiative DAF, an advised fund of Silicon Valley Community Foundation. Computational resources for this study were provided by the Texas Advanced Computing Center (TACC) at The University of Texas at Austin.

SECTION: Appendix ACode usage

The code for this study is available online athttps://github.com/williamgilpin/SHREC. Tutorials and docmentation for usage of the method are available in the repository, and the code uses the same conventions as the widely-usedscikit-learnlibrary[116].SHRECcan be applied to an arbitrary time series dataset as:

SECTION: Appendix BShared reconstruction algorithm implementation

Calculating the driver adjacency graph.For each of theobservation time series,, we lift the time series using time delaysand then compute the pairwise distance graph. For therow of this graph, we find the set ofsmallest elements, which correspond to the distances to thenearest neighbors of point. The smallest member of this set determines the constant

We next perform a numerical rootfinding calculation to find the value ofsuch that

where. Having calculated,, we transform the original distance matrix into an affinity matrix via the transformation

This matrix has been used in recent algorithms in topological data analysis[33], and it encodes key features of a fuzzy simplicial complex associated with the points in. We define a consensus affinity matrix as the elementwise average across the simplicial complexes for all individual response variables,.

After creating the recurrence graph, we next seek to reconstruct the driving signalunderlying this graph. This problem is related to the general distance geometry problem, which has previously been studied in numerous contexts[49], including in structural biology in the context of molecular contact maps[117]and resonance spectra produced by nuclear magnetic resonance[118,119]. However, our particular problem bears several distinctions: Our matrixdoes not correspond to a literal distance matrix, but rather a consensus connectivity matrix across several signals; we seek a low-dimensional driving signal, which makes the problem simpler; additionally, each element ofmust reveal information about a unique driver state. Thus, our approach to inferringvaries based on the expected type of driving signal.

Continuous-time dynamics.Given the aggregated adjacency matrix, we calculate the graph Laplacian, where. Our interpretation ofas a transition matrix implies that the spectrum of the graph Laplacian will either reveal non-ergodicity in the form of a graph partition, or otherwise weak ergodicity in the form of long-time transients associated with slowly-relaxing states. Therefore either the leading eigenvector or the first non-constant eigenvector (known as the Fiedler eigenvector) contain information about our driving signal. A similar principle underlies widely-used Laplacian eigenmap algorithms (and related diffusion maps), allowing us to use existing optimized solvers to compute the reconstructed driver[120,121]. We note that our general problem of inferring a signal from a topological ordering nodes in a graph is identical to pseudotime calculation, a method in systems biology that infers developmental trajectories from gene expression measurements of unsorted cell populations[51,122].

Discrete-time dynamics.A discrete-time driving signal contains only finite number of unique states, and so we seek to assign a fixed label to every node in. While the labelling scheme and number of labels is well-defined when the graph contains disjoint subgraphs, for real-world datasets spurious recurrences cause all nodes to become mutually reachable. Thus, we instead seek to optimize a set ofdiscrete partitions, or cuts, to the aggregated connectivity graph that maximizes an objective function like cluster modularity or separability. Globally optimal-cut of a graph is an NP-hard problem, and so computing the optimal partition requires checking all possible partitions—a combinatorial optimization task that becomes impractical even for short time series[31]. However, an approximate heuristic approach would be spectral clustering, in which we apply our continuous time method, and then discretize the reconstructed signal intobins[123,37]. However, the driver dynamics on this symbol space will not necessarily represent a faithful coarse-graining of the underlying dynamics, undermining the accuracy of the resulting reconstruction. Instead, we opt to use Leiden clustering, a graph community detection heuristic that traverses and partitions the transition graph, and also determines the optimal value ofand thus unique driver states[38]. We take the progression of cluster labels visited by the response time series as an estimate of the discrete driver, and we note that all permutations of cluster labels are equivalent.

SECTION: ADemonstration with known dynamical systems

For the illustrative example featured in the main text, the driving signal corresponds to the dynamics of the Rossler equations,

where we have set,, and.

The response signals correspond to the Lorenz equations, each of which is driven identically by,

where the different response systems are indexed byand the coupling strengthdetermines the relative strength of the driver forcing. We setfor our experiments. The parameter values for each response system are drawn from the uniform distribution, centered around default values,

with parameter values,,corresponding to chaotic dynamics. We set the width of the uniform distributiontoin order to ensure diverse response dynamics.

In order to make the problem more difficult, and to highlight our method’s robustness, we further corrupt our dataset with random nonlinear measurements. For each response system, we define a random measurement function, where

Combining everything together, we create a response dataset from the highly-corrupted values of the first response variable. From this time series ensemble, we create the estimateusing our method.

SECTION: Appendix CBenchmark Experiments

Here, we describe the benchmark models and our hyperparameter tuning procedure. We do not tune any hyperparameters for our own method,SHREC. We perform all hyperparameter tuning on the training dataset, and evaluate model performance on a held-out testing dataset. Importantly, all of our testing datasets correspond to distinct dynamical systems; depending on the experimental dataset, the testing data corresponds to a different animal, experimental trial, or regulatory network. All testing scripts and tabular results are available in our open-source repository.

SECTION: AExperiment Design Overview

For our experiments, we apply our method and others to a variety of datasets in which a known driving signal influences a large, diverse set of response variables. We train reconstruction models ononlythe response time series, and then compare the resulting output to the true driving signal. We emphasize that our unsupervised algorithm and other benchmark methods do not receive the driver as inputs—the driver is withheld, and the inferred driver reconstruction produced by each algorithm is compared to the true driver only in order to validate method performance. In real-world applications, the true driver would be unavailable.

The datasets on which we test our method are chosen to represent diverse potential applications in which several observable variables are driven by an external signal:
(1) The turbulent motion of a set of diffusing tracer particles in a chaotic double gyre flow. The driver is long-wavelength sinusoidal forcing on the flow, while the response variables comprise the radial coordinates of individual particles—as would be observed via Doppler velocimetry. The training and validation datasets represent flows with different tracer particle initial positions and forcing amplitudes. There areresponse time series, each containingtimepoints.
(2) A fetal electrocardiogram time series. The driver is the maternal respiration signal, while the response variables comprise multi-point electrocardiogram recordings placed across the fetal body[70]. The training and validation datasets represent different patients in the original study. There areresponse time series, each containingtimepoints.
(3) Stochastic gene expression dynamics in a subset of the yeast metabolic gene regulatory network, as simulated by an existing experiment design package for cell biology[71]. The driver corresponds to the expression level of the transcriptional regulator YDR043C in response to a knockdown perturbation, and the response states are the expression levels of downstream genes in the network. There areresponse time series, each of which containstimepoints.
(4) Species abundances of themost abundant phytoplankton in two lakes in Switzerland over ayear period[73]. Due to sampling inconsistencies, over the period we studyof the total data is missing, and so we downsample the data to a monthly average abundances and replace any missing time series by forward-filling, in which each missing value is replaced by the most recent non-missing value. The driver is the daily temperature of the lake. For the validation and testing datasets we use data from two different lakes in the original dataset. There areresponse time series, each of which containstimepoints.

We do not tune any hyperparameters for our own method during our experiments, but for the benchmark models with tunable hyperparameters we use a grid search and cross validation to find the best hyperparameters, and report performance on held-out test data.

SECTION: BBaseline models evaluated.

We consider a variety of classical and deep learning based time series models, with a particular emphasis on unsupervised methods used for time-dependent hidden state estimation. Whenever possible, we aim to use reference implementations with default hyperparameters used in published studies; however, we tune hyperparameters separately for each model and dataset combination, in order to ensure that each baseline are fairly evaluated. Most time series models contain the equivalent of a time-lag order or timescale parameter, which determines the number of past timepoints that the model uses to calculate the hidden state at a given instant; we focus on tuning this hyperparameter for all models. For models that can learn latent time series of varying dimensionality, we also tune the dimensionality of this space, because smaller latent spaces usually regularize models.

We take a simple timepoint-wise average across all response time series, and use the result as our estimate of the driver.

We apply both principal component analysis and independent components analysis across all response time series, treating each timepoint as an independent datapoint (thereby discarding temporal correlations). We also consider variants in which we lift the dimensionality of each input time series using time delays before computing the components, allowing temporal information to influence the representation. In order to apply canonical correlation analysis to time series, we create time-lagged copies of the signal in order to raise the effective dimensionality of each input time series, and we find canonical correlation coordinates relating past values to future values[41]. We perform canonical correlation between time series one time lag interval in the past, and one time lag interval in the future. We usescikit-sfa, a reference implementation of SFA by the original discoverers[75,124], and we do the same for DCA[74].

For all methods, we perform hyperparameter tuning over all combinations of time delayand number of latent dimensions. We score model performance on held-out testing data. Because we are reconstructing one-dimensional driving signals, we use the first or most“dominant”latent dimension, which is determined using a different heustic for distinct methods—for example, in PCA this is the mode that captures the most variance, while in SFA it is the slowest variance-capturing mode. Some methods, like PCA, learn the same representation regardless of the hyperparameter, but for other methods smallerregularizes the model by forcing it to span fewer latent dimensions, materially affecting the learned representation. Thus, while we treatas a hyperparameter for all methods, it does not necessarily affect all models.

We test several variants of the Kalman filter, and we report results using the best model as determined by cross-validation on the training dataset. The standard Kalman filter is a linear Gaussian process, but we also consider the ensemble Kalman filter and unscented Kalman filter variants[125,126,62,127]. Because neither our general problem, nor the datasets evaluated in our experimental task, have consistent spatiotemporal structure or known governing equations, we do not include the traditional extended Kalman filter among our baselines. We use an implementation of the Kalman Filter in the Python packagefilterpy, as well as the Python time series analysis librarysktime[128]. These implementations use expectation-minimization to solve for the parameters of the model. We perform hyperparameter tuning across different Kalman filter variants, as well over a varying number of latent state dimensions. We also augment the dimensionality of our response datasets using a variable number of time delays,. After performing cross-validation to select the best model for each dataset, we evaluate performance on held-out test data.

For each input time series, we generaterandom surrogate time series with matching power spectra using the amplitude-adjusted Fourier transform method. For the original time series, as well as all surrogates, we compute the power spectrum. The model has a single hyperparameter,, the significance threshold. If an amplitude in the training data power spectrum is greater than a fractionof all surrogate spectra, then the corresponding frequency is deemed significant. A reconstructed driver signal is generated by producing a time series containing only the significant frequencies from the input time series, along with their corresponding phase shifts, and the resulting reconstructions are combined across all observation time series by computing their first principal component. We perform hyperparameter tuning overpossible values of the significance threshold.

We use an implementation of GPFA from theelephantPython package[76,129]. We convert each response time series to a spike train by sampling an inhomogenous Poisson process with firing rate set by the time series values (which are rescaled to fall betweenand). The sampling rate of the process is set toms-1, while for the GPFA model, the spike binning size is set toms. When fitting the GPFA model, we perform hyperparameter tuning over all combinations trial countand number of latent dimensions. We score model performance on held-out testing data. At larger trial values, less information is lost when the deterministic response signal is converted into a spike train.

Related to the Kalman model and GPFA, this model assumes linear latent and observed dynamics, with noise drawn from a multivariate Gaussian process[77]. This model can be considered the equivalent of a hidden Markov model with continuous hidden states.

We use an implementation of a two-layer causal dilated autoencoder, which has recently been shown to successfully extract meaningful featurizations for a downstream classification task even when trained without labels[78,130]. This model performs one-dimensional trainable convolutions across time for the response time series, and then uses a series of dilations and additional convolutions to aggregate information across timescales. We modify this model’s final layers to preserve the number of timepoints in the input dataset, and treat the latent space as a compressed representation of the input signal. We perform hyperparameter tuning over all combinations of learning rate, convolution block width, and number of latent dimensions. We score model performance on held-out testing data.

We use a reference PyTorch implementation of LFADS[79,131,132]. We perform hyperparameter tuning over all combinations of learning rate, batch size, and number of latent factors; however, we leave many of the other hyperparameters for the method at their default values. We emphasize that the relative performance of LFADS may be even stronger than we observe, if other hyperparameters (such as architecture depth, or choice of optimizer) are tuned as well; however, here we focus on hyperparameters that have analogues in the other benchmark models, in order to ensure we are searching similar model spaces and performing a fair comparison. We score model performance on held-out testing data.

This approach requires thresholding the individual distance matrix of each embedded time series in order to ensure a given sparsity (set tononzero values in the original works)[34,35]. The binary distance matrices are combined into a consensus matrix using Boolean addition (if a given matrix element is nonzero in any matrix, it is nonzero in the final matrix). This consensus adjacency matrix is converted into a weighted distance matrix by using Dijkstra’s algorithm to find the shortest path between each pair of nodes. The distance matrix is then embedded into a one-dimensional space using multidimensional scaling. The latter two steps correspond to the Isomap data embedding algorithm. We perform hyperparameter tuning over all sparsity thresholds in.

SECTION: CAccuracy Scores

Because our method seeks dynamical couplings between the driverand response, we consider several measures of the accuracy of the reconstructed driving signaland the true driving signal. For strongly-coupled systems, we consider several pointwise error measures such as the mean squared error (MSE), Spearman correlation (Spearman), the symmetric mean absolute percent error (sMAPE) the mean absolute scaled error (MASE), and the dynamic time warping distance (DTW)[133]. These different accuracy measures are used to assess forecast quality in various contexts[134,89,135,136]. Because a driver influence can act after time delay, we compute sliding versions of these pointwise metrics across multiple time delays between the true driverand its reconstruction, and we report the maximum. However, we require that the time delay must not exceed% of the length of the original time series. in order to avoid spurious correlations due to edge effects.

However, the use of pointwise error metrics presumes that the reconstructed driving signal should instantaneously resemble the unknown driving signal, a condition that only occurs in a strongly-coupled regime in which the response subsystem dynamics are not only fully-synchronized to the driver, but also have a monotonic phase relationship at all times[81,11,12,80]. While these conditions likely hold for many time series of interest, we are particularly interested in the more challenging case in which the subsystem dynamics are driven, but not fully determined by, the driver signal. In this case, the effect of the driver on the response subsystem may be filtered or nonlinearly suppressed depending on system state.

As a more general measure of reconstructed signal accuracy, we consider the more general problem of determining whether the reconstructed driver signalbears any functional relationship with the true driver. We employ two methods for calculating this relationship: the mutual information, and nonlinear Granger causality. The former can be efficiently estimated from two time series using differential entropy, which can be computed non-parametrically via density-based methods[137,138,139,140,141]. For nonlinear Granger causality[142,15,143], we first train a state-of-the art nonlinear forecasting model to predictbased on its past values. We then train another model that seeks predictbased on past values of bothand. We compute the Wilcoxon score between the two forecasts in order to quantify the degree to which knowledge ofimproves forecasting ofThis approach can be viewed as a nonlinear generalization of classical Granger causality, which bypasses the need to enforce stationarity (e.g. via differencing) by instead learning a forecast model that can accommodate multiple timescales, seasonality, or drift. For our nonlinear forecasting model, we use N-HiTS, a recently-developed artificial neural network that hierarchically decomposes time series a trainable basis[144]. Earlier versions of this model show state-of-the-art performance on a variety of forecasting benchmarks for both real-world time series and dynamical systems datasets[89,145].

For our benchmark experiments, we compute allscoring metrics described here across all datasets and baseline models. We correlate their performance across different models and datasets in FigureS2. Because Spearman correlation and neural Granger casuality report values of similarity, rather than distance, we invert the values of these metrics before computing correlations.

We find that the simpler time series measures like Spearman and dynamic time warping correlate strongly with more sophisticated methods like neural Granger causality on the datasets we consider in our experiments. We thus conclude that our testing datasets primarily fall within a strongly synchronized regime, and we defer to using conceptually-simpler Spearman correlation for all results reported in the main text.

SECTION: Appendix DSkew-product dynamical systems experiments

Our dynamical systems database consists ofcrowdsourced chaotic dynamical systems, including well-known systems like the Rössler, Lorenz, and Chua equations, as well as domain-specific models from fields such as climatology, astrophysics, and neuroscience[89,146]. To create random skew-product systems, we sample two distinct dynamical systemsandfrom the database, and we label one as a driver and the other as a response.

The driver dynamics are generated autonomously. In the absence of driving, the response dynamics are given bysystems indexed by,. We drive these response dynamics by introducing a linear coupling scheme between the first variable of each response system, and the first variable of the driver,. The coupling constantdepends on the specific dynamical systems chosen to act as the driver and response, and it is intended to scale the driving to ensure that the relative amplitudes match across systems. We calculatefor each system pair by simulating the driver and response system separately (in the absence of coupling), and then calculating their average amplitudes and scaling the coupling appropriately,. We setequal to a fixed relative coupling strength for all skew product experiments.

We note that our illustrative example above, in which Eq.A1drives Eq.A2, constitutes one particular example of a skew product system constructed using our methods. For our large-scale experiments, we simulate dynamics for allpossible unique, ordered pairs of dynamical systems; however, onlyof these skew product systems are both numerically stable and exhibit chaotic dynamics (i.e., did not diverge or collapse to quiescence), and so we retain only these systems in our random skew product experiments. We note that weaker coupling, or more exhaustive searching of numerical integration tolerances or timesteps, might yield a greater number of stable skew product configurations.

For every skew product system, we apply our reconstruction method to the response dynamics, and measure accuracy as the timepoint-wise Spearman correlation with the driver time series (this is the same accuracy metric used in our benchmark experiments). Because every dynamical system in our database has been annotated with key mathematical properties (Lyapunov exponents, fractal dimension, entropy, etc), we can correlate the accuracy score with underlying Lyapunov exponent (a putative measure of chaoticity) separately for both the driver and response dynamical systems. Our Lyapunov exponent calculations are described in prior work[89,147]; briefly, estimates are based on both a naive method based on initial exponential separation of trajectories over short and long periods (averaged over the attractor), and a more advanced method based on continuous orthonormalization of a bundle of vectors co-transported with the flow[148,149].

SECTION: Appendix EEvaluating shared reconstruction algorithm properties across a period-doubling bifurcation

While our benchmark experiments focus on continuous-time datasets due to their greater relevance to real-world problems, when examining our method’s properties we include experiments with discrete time drivers, allowing us to probe the properties of our method across a period-doubling bifurcation leading to chaos. We follow the task originally used in Sauer’s original paper describing the exact shared dynamics reconstruction algorithm[27], which our approach seeks to generalize: an ensemble of logistic maps in the chaotic regime, but with random parameters, is driven by a logistic map in the periodic regime. We include the equivalent of Brownian stochastic forcing to the dynamics by adding to each iteration a small random number drawn from a normal distribution. We vary three parameters across our task as control parameters: the amplitude of the stochastic forcing, the strength of the coupling between the driver and the response series, and the number of response time series.

The driver time series is given by

This driver exhibits dynamics that are period,, andwhenhas values,,, respectively, and it becomes fully chaotic above these values. The response time series are given by

where the noise value is drawn from a normal distribution at each time step. The different response time series are labelled by, and each response system has distinct dynamical parameters. We set, a value well within the chaotic regime of the logistic map. We set,andas default values throughout our experiments. For our percolation experiments, we separately vary as control parameters each of,, and, while holding the other parameters fixed.

SECTION: AAccuracy score for discrete time driving signals

For the percolation experiments, we evaluate the accuracy of our method using the adjusted Rand score, a measure of pointwise accuracy that is invariant to permutations in labels. For example, if the period-two driver exhibits a sequence of states of the form, then a perfect adjusted Rand score ofcan be either of the two possible reconstruction sequencesor. We note that, unlike the unadjusted Rand score, the adjusted Rand score is scaled so that random assignment of labels to timepoints will return a score of zero, rather than the nonzero pointwise accuracy resulting from random assignment. This correction ensures that scores generated for datasets with different period drivers remain comparable.

SECTION: BNaive expectation for scaling of the accuracy score

In the case of a discrete-time, periodic driver, we can generate a naive hypothesis for the scaling of our method’s accuracy with the number or length of response time series. We assume that overall accuracy is primarily determined by the total number of recurrence events observed,, which we naively expect to be directly proportional to the product, whereis the number of response time series andis their average length. However, if the driver has more unique states (indicating a higher period), then more observations are needed in order to resolve each of the distinct states. We thus assume that the overall accuracy should have a functional dependence on the number of recurrences divided by the driver period.

Each recurrence event reveals information about a finite number of potential driver states, but some recurrences will reveal redundant information about driver states that have already been fully-resolved. Asincreases, we expect the number of novel discovered driver states to gradually saturate. This distribution is given by the survival function of a Bernoulli trial, or equivalent the mean of a beta distribution (which serves as the conjugate distribution to the Poisson distribution).

whereandare constants that depend on the particular dynamical system. For the results shown in the main text, we fit a single value ofto all three driver periodstested, but otherwise do not perform separate fits for different.

SECTION: CPercolation calculation

In order to measure the structural properties of our model during fitting, we extract and measure the aggregated time series adjacency matrixbefore finding the driver with the diffusion. In order to understand the structure of the graph defined by this matrix, we compute the order parameter, whereis the total number of nodes andis the number of nodes comprising the largest connected component. As this value approaches one, it indicates the onset of percolation in finite-size undirected networks[94],

SECTION: Appendix FComparing the unstable periodic orbits of the reconstructed driver and original driver

SECTION: ADetecting Unstable Periodic Orbits from data

A set ofcandidate unstable periodic orbitsof varying lengthsare detected directly from a given time seriesby searching for close recurrences in the pairwise Euclidean distance matrix:, where[98]. Each detected orbitis aligned in time to the original time series by maximizing its cross-correlation, and the resulting phase shift is used to calculate a new, periodic single-orbit time serieswith the same number of timepoints as the original time seriesby repeating the orbit multiple times. The singular value decomposition of the set of aligned orbitsis then calculated in order to find a set ofunique orbits, where the truncation pointis determined by the drop-off rate in the sorted singular values[150]. The original time seriesis then projected onto the reduced orbit basis, in order to determine the relative amount of time each of theleading orbits are shadowed by the dynamics. We note that all operations in the singular value decomposition and basis set projections are computed as double contractions over the time and dimensionality axes,.

For the Rössler attractor dataset we use as an illustrative example throughout the main text, the spanning orbit sets comprises the firstorbits. We note that a longer-duration or higher-time-resolution sample from the dynamical system might yield a spanning set with different cardinality.

SECTION: BComparing orbits and distance matrices

Given two time series of equal lengths, such as a dataset time seriesand an aligned orbit time series, we compare the time series directly by contracting over both indices. The set of aligned orbitscan therefore be converted into the equivalent of a normal basis by rescaling.

However, when comparing two distance matrices,and, we are primarily interested in whether the matrices have similar overall structure (i.e., shared recurrences and long-term patterns). Therefore, instead of using mean-squared error, we use the element-wise Pearson correlation to comparetoacross various conditions.

In the main text, we compare the distance matrix of the reconstructed signal and the first three orbits, after aligning the latters’ phases to the reconstructed driver. As a control parameter, we vary the number of response time series, and thus the final quality of the reconstruction.

SECTION: Appendix GFuzzy simplicial complexes are a robust alternative to recurrence plots.

We generate a-timepoint sample of dynamics from the Lorenz equations. The sampling timescale is chosen such that the dynamics spanLyapunov times. From this time series, we set a fixed recurrence threshold, and record the setof all timepoints of recurrences,such that.

We next add random Gaussian noise to each timepoint,,. We recompute the set of recurrences,such that. We compute an accuracy score as the true positive rate. We find that other metrics, such as the F1-score, give similar results. We convert the added noise into a dimensionless signal-to-noise ratio,

where.

We find that fuzzy simplicial complexes are able to maintain accurate detection of recurrences even when the SNR decreases by a factor of(Fig.S3). Moreover, fuzzy simplicial complexes do not require the recurrence thresholdto be chosen in advance.

SECTION: Appendix HAlternative community detection methods for driver reconstruction.

In the discrete-time version of our algorithm, we make the particular choice to use Leiden community detection in order to assign driver states to equivalence classes in the presence of noise. We opt for this algorithm due to recent work showing that it balances accuracy and cost, particularly for large networks[38]. To evaluate the importance of this choice, in FigureS4we repeat the experiment design from SectionIII.5. We choose a single value for the noise and driving, corresponding to a case in Figure6where nearly exact reconstruction was possible. We compare Leiden to Louvain clustering, a similar algorithm, as well as a variety of other community detection methods[37,151,152,153,154,155,156]. We observe that all methods perform comparably on this task, including eigenvector-based clustering, which resembles the approach that we use for continuous-time driver reconstruction in the other sections. Several of the methods, including InfoMap and LabelProp, are not optimized for this particular task (for which distinct driver states induce well-separated communities). We thus interpret this performance comparison narrowly in the context of causal driver reconstruction. However, our results overall do not appear to be particularly sensitive to the choice of clustering heuristic.

SECTION: Appendix IDetecting higher-order interactions in gene regulatory networks

Following prior work[72,71,157], we implement a mechanistic model of gene expression that separately models abundances of mRNA and protein products associated with each gene. In the absence of interactions, we denote the dynamics of an isolated node as

wherefor a model that includes separate dynamical variables for the mRNA and protein abundances associated with each gene. Hereafter, we drop the vector notation and useto denote the dynamical variable associated with interactions among genes over experimentally-relevant timescales.

A traditional regulatory network model couples the dynamics of individual genes together additively through the traceless coupling matrix,

Network inference techniques therefore seek an estimate of the interaction matrixgiven a set of measurements, including direct observations of the relevant dynamical variables.
In order to capture higher-order multiplicative interactions, we introduce an additional term to each gene’s dynamics, which depends on the product of the expression levels of two other genes,

Importantly, methods that seek to directly infercannot necessarily capture the interaction structure of. However, in some cases network reconstruction methods may assign entries to their estimate of the interaction matrixthat result from interactions in. For example, if, then a method might find.

For our experiments, we impose exactly one driving combination among the set ofgenes. In this case, each row ofcontains only one nonzero term. The dynamics then simplify,

Identification of the causal driving therefore requires correctly identifying the two indices. Time-resolved driver reconstruction methods, including SHREC and the methods benchmarked in the previous section, produce a driver estimate, which can be directly compared to the true multiplicative driver. Regulatory network reconstruction methods, however, instead directly return estimates

We generate a set ofrandom networks by randomly sampling connected subnetworks ofgenes from a subset of the yeast metabolic network, which was also used in the DREAM4 challenge[72,71]. For each network, we randomly select two nodes to act as multiplicative drivers, and we then add a random forcing term to the remaininggenes’ dynamics, following Eq.A3. For each of thenetworks, we generatetime series of lengthtimepoints.

We set asidenetworks for validation, and we use these to find the median best hyperparameter values for the various causal reconstruction methods, using the same methods and procedures described in AppendixAppendix C. We also use two methods that directly estimate the time series interaction network, GENIE3 and ARACNE[84,85]. We do not tune any hyperparameters for our method, SHREC.

We test our methods on the remainingrandom networks with higher-order interactions. We evaluate the effectiveness of different methods in uncovering the higher-order driving using three metrics:

TheSpearman correlationdirectly compares the reconstructed driverwith the true driver.

TheDetection accuracyfinds the two largest ofpairwise correlations between the reconstructionand any individual time series, and estimatesas the indices of the two most correlated expression time series. These indices are then compared to the true indicesto generate an accuracy score, which can be,, ordepending on the number of driver genes correctly detected,

TheCombination accuracyinstead tests allpairwise productsamong theobserved time series, and returns the indices of the single most correlated pair,

The highest scoring single pairis then compared to the true indicesvia Eq.A4to generate a score, which can also be,, ordepending on the number of driver genes correctly detected. For each baseline method evaluated, we average its score across allnetworks in the testing set.

We find that all three metrics largely agree (Fig.S5), particularly among the higher-performing methods, with the shared reconstruction algorithm consistently detecting the multiplicative driving genes more effectively than other methods. The specialized regulatory network inference methods, ARACNE and GENIE3, consistently perform well, suggesting that these methods, while originally developed for traditional network inference, also identify signatures of higher-order interactions.

Our particular problem setting represents a small regulatory network with a single higher order driver. We emphasize that our experiments are not intended to show that our method, SHREC, should be used instead of existing network inference methods in biology[158]. Rather, this example highlights that our physics-based method performs surprisingly well on a network inference problem with biological motivation. Our results therefore underscore the strong inductive biases of recurrence-based methods, allowing generalization to diverse problem settings.

SECTION: References