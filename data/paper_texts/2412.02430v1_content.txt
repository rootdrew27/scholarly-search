SECTION: Transformer-based Koopman Autoencoder for Linearizing Fisher’s Equation
A Transformer-based Koopman autoencoder is proposed for linearizing Fisher’s reaction-diffusion equation. The primary focus of this study is on using deep learning techniques to find complex spatiotemporal patterns in the reaction-diffusion system. The emphasis is on not just solving the equation but also transforming the system’s dynamics into a more comprehensible, linear form. Global coordinate transformations are achieved through the autoencoder, which learns to capture the underlying dynamics by training on a dataset with 60,000 initial conditions. Extensive testing on multiple datasets was used to assess the efficacy of the proposed model, demonstrating its ability to accurately predict the system’s evolution as well as to generalize. We provide a thorough comparison study, comparing our suggested design to a few other comparable methods using experiments on various PDEs, such as the Kuramoto-Sivashinsky equation and the Burger’s equation. Results show improved accuracy, highlighting the capabilities of the Transformer-based Koopman autoencoder. The proposed architecture in is significantly ahead of other architectures, in terms of solving different types of PDEs using a single architecture. Our method relies entirely on the data, without requiring any knowledge of the underlying equations. This makes it applicable to even the datasets where the governing equations are not known.

KNeural NetworksDeep LearningAutoencodersDynamical SystemsKoopman Operator TheoryPartial Differential EquationsReaction-diffusion Equations

SECTION: Introduction
Understanding complex dynamical systems is crucial for understanding many different types of natural occurrences, particularly those governed by PDEs. PDEs offer a theoretical foundation for modeling of spatiotemporal systems in many different fields, including engineering, physics, and biology. Of all these equations, Fisher’s reaction-diffusion equation is commonly used to describe the spatiotemporal evolution of interacting species. The nonlinear and complex nature of these equations makes them difficult to solve. Moreover, there is no general mathematical architecture that can be used to solve such nonlinear PDEs.

Transformations aimed at linearizing systems are commonly associated with Koopman operator theory. Koopman operator theory, with its modern interpretation in dynamical systems theory, plays an important role in our approach. While explicit construction of Koopman operators has limitations, Dynamic Mode Decomposition (DMD), offers numerical approximations to these operators. Recent advancements, such as Extended Dynamic Mode Decomposition (EDMD), involve lifting system observables into higher-dimensional spaces through nonlinear transformations. This helps in the study of nonlinear phenomena, but the manual selection of transformation functions can limit effectiveness.

Current initiatives have shown the effectiveness of neural networks in constructing Koopman embeddings for system dynamics. This increased interest in neural networks, emphasize their utility in approximating Koopman eigenfunctions and eigenvalues. This work aims to provide coordinate transformations that are interpretable, even for complex and nonlinear systems. Our primary focus is on attaining models that accurately capture the underlying low-rank dynamics while reducing overfitting and preserving interpretability.

In recent years, deep learning methodologies have emerged as powerful tools in diverse scientific domains. However, relying solely on deep neural networks may not be sufficient, as their models often lack interpretability and parsimony. Using the capabilities of deep neural networks, this research attempts to address the solution of Fisher’s reaction-diffusion equation via linearising them using Transformer-based Koopman autoencoder. We introduce a systematic approach to utilize neural networks to discover coordinate transformations that aids in linearizing Fisher’s reaction-diffusion equation.

To ensure the success of our neural network architecture, we identify critical components, including appropriate network design, handling of the identity transformation, and informed selection of spatiotemporal data for training. Our architecture emerges as a robust method, capable of addressing various PDEs without necessitating alterations to its fundamental design. This adaptability is evident in our research where we conducted comparative analysis using two different datasets and architectures. Notably, our architecture exhibited superior performance across various datasets, showcasing its effectiveness in solving diverse PDEs.

In Section, we explore the details of our methodology, involving the foundational principles of autoencoders, Fisher’s equation, and Koopman operator theory within dynamical systems. To properly train our models, we also go into great detail about how we obtain dataset. The findings of our analysis are provided in Section, along with a thorough comparison of our suggested approach’s performance with alternative approaches. We conclude in Sectionwith a detailed discussion and critical conclusions drawn from our research.

SECTION: Methodology
We shall examine the methodological strategy used in this part to unveil the complex dynamics of Fisher’s equation. We will describe our methods in detail, incorporating Koopman operator theory and autoencoders into the domain of dynamical systems. We also describe our data acquisition process, which is essential to effectively educate and train our algorithms.

SECTION: Koopman Operator Theory
We examined nonlinear PDE of the form:

whereis the state of the system at timeand spaceon a smooth manifold. Hererepresents the governing equation that define the system.We will induce a mapping, where the stateat timeis mapped to a future time. For partial differential equation, we may discretize the continuous functionat several spatial locations,. This yields discrete-time dynamical system, for eq. ():

In the case where the functionoperates linearly, subsequent values of the state variablecan be accurately predicted through a spectral expansion. However, due to the typical nonlinearity of, a universally applicable scheme for solving such systems remains elusive.

Koopman operator theory presents an approach to linearizing nonlinear dynamical systems by means of the Koopman operator. This linear operatoris infinite-dimensional, analogous to, such that,, that advances the observable functions:

The eigen-decomposition of operatorenables the simplification of the dynamics and representing solutions to the dynamical system. We seek to identify eigenfunctionsof the Koopman operator, corresponding to eigenvalues, satisfying:

Despite being an infinite-dimensional operator, the Koopman operator can be approximated in a finite-dimensional manner by considering the space defined by a finite set of Koopman eigenfunctions. When applied within this space, the Koopman operator can be converted into a matrix representation. In practice, procuring Koopman eigenfunctions may pose a greater challenge compared to obtaining the solution of eq. ().

SECTION: Autoencoders
Autoencoders are a type of artificial neural network used for unsupervised learning and dimensionality reduction tasks. They consist of an encoder and a decoder, with the goal of learning a compressed representation of the input data. The encoder extracts the most crucial features from the input data and maps them to a lower-dimensional representation. This compressed representation, often referred to as the “code" or “latent variables," holds a condensed version of the input data.

The entire architecture is depicted in figure. The encoder in our design is; the second component is the linear dynamics; the third component is the decoder. There are two segments in each of the encoder and decoder. The encoder is made up of the inner encoderand the outside encoder, whereis an identity matrix. The inner encoderlowers dimensionality and diagonalizes the system, while the outside encoder conducts a coordinate transformation into a linear dynamics space. The inverses of the inner decoderand outer decoderare the inner and outer encoder, respectively. Both the outer encoder and outer decoder utilize a residual connection.

The architecture function, as shown in figure, consists of a transformer block, comprising a multi-head self-attention mechanism followed by layer normalization and feedforward neural networks. This transformer block is applied to the input data, allowing the model to capture intricate dependencies and patterns within the data. In addition, residual connections are employed within the block, enabling effective information flow through the network.

The loss function utilized for training the network comprises five distinct components, each serving to enforce specific conditions. Loss function is given by:

focuses on the autoencoder principle, aiming for a reversible transformation between the state space and intrinsic coordinates, where linear dynamics prevail.

, ensures accurate forecasting of the next state given the current one, extending to multiple future time steps through iterative matrix multiplication.

emphasizes the requirement for linear dynamics within the intrinsic coordinates.

addresses the outer autoencoder objective, emphasizing the separation of transformation and dimensionality reduction or diagonalization processes, allowing better interpretability.

mirrorsbut pertains specifically to the outer encoder and decoder components, reinforcing the autoencoder principle.

The total loss is incorporated by summing all the aforementioned losses, each carrying the same weight.

SECTION: Fisher’s Equation
For a state variable, we consider one-dimensional Fisher’s equation:

with periodic boundary conditions, whereis the diffusion coefficient andis the reaction coefficient. For our results, we useand. We discretize the spatial domain, represented by, intoequidistant points.

The neural networks is trained using data generated through numerical simulations of the abovementioned PDE. We have curated a dataset comprising 60,000 initial conditions for training purposes. This PDE has been simulated forward for 50 time instances, with a time step, culminating in a total duration of 0.10 seconds.

Within the training dataset, we have incorporated three distinct types of initial conditions: white noise, sine waves, and square waves. Similarly, for validation and testing purposes, we have maintained consistency in terms of time and time steps. The validation set comprises 20,000 initial conditions, mirroring the types of initial conditions found in the training set. For the testing phase, we have assembled a dataset containing 21,000 initial conditions, encompassing seven different types. Three types of initial conditions in testing data are already present in the training data: white noise, sine waves, and square waves. Additionally, we have introduced Gaussian waves, triangle waves, sawtooth waves, and pulse waves, each comprising 3,000 initial conditions.

SECTION: Results
We use the network architecture shown in Figureto solve the Fisher’s equation.The neural network architecture’s input and output layers are synchronized with the spatial discretization, containingneurons each. The training utilized a global batch size of 32 and employed TensorFlow’s mirrored strategy.

The first blockis a transformer block containing multihead attention layer withheads, followed by layer normalization. Following layer normalization, there is a feedforward neural network consisting of two dense layers. These dense layers are equipped with rectified linear unit (ReLU) activation functions. After the feedforward neural network, the final layer inblock consists of a layer normalization step.

We trained the network on each of the types of initial conditions mentioned in the training dataset. This ensured that the network learned to generalize across different initial conditions and capture their underlying dynamics. After completing the training phase, we extensively tested the trained network using the test data, comprising trajectories with seven distinct types of initial conditions. Notably, the last four types of initial conditions: Gaussian waves, triangle waves, sawtooth waves, and pulse waves, were not encountered during the training process. In each scenario, the network prediction closely aligns with the exact solution, demonstrating excellent agreement between the two.

The inner encoder, serves to diagonalize the system, and it also accomplishes the task of reducing the dimensionality of the system. The layerfeatures a width distinct from that of the input and output layers. This disparity in width enables the dimensionality reduction within thelayer, resulting in a low-dimensional model of rank, we have opted for. We utilize a single fully-connected linear layer for both the inner encoderand the inner decoder.

The comparison between the exact solution and the neural network predictions for Fisher’s equation is presented in figureand. The first column in figureandplots the exact solution obtained by solving equationusing finite difference technique. The second column in figureandshows the solution obtained using the proposed architecture. In figure, the initial conditions represented are white noise, sine waves, and square waves. In figure, the initial conditions represented are Gaussian waves, triangle waves, sawtooth waves, and pulse waves. It shall be noted that, the red line represents the initial condition, while the blue line shows the state of the wave at the final time,seconds. In figure, the initial conditions employed were of a similar type as provided to the architecture during the training phase. In figure, the initial conditions utilized were not presented to the architecture during the training phase. The comparison demonstrates that even with unseen data, our model exhibits strong performance, indicating its generalization across various types of initial conditions. The eigen values of the approximated Koopman operator,, has been shown in figure.

Figureillustrates the relationship between the number of attention heads and two critical metrics: inference time and error. The primary y-axis (in blue) shows that the inference time increases with the number of heads, ranging from approximately 70 ms at 2 heads to 257 ms at 32 heads. This trend suggests that the higher the number of attention heads, the greater the computational complexity and longer the processing times. The secondary y-axis (in green) shows that the total loss of the model decreases as the number of heads increases. This indicates that while adding more heads improves model accuracy, it comes at the cost of longer inference times and increased computational cost. The trade-off between these metrics depends on the specific application and user requirements, making it important to find an optimal balance between accuracy and computational efficiency. Furthermore, as the number of heads increases, the model’s scalability becomes a concern, as larger models require more resources and longer processing times. This can make models impractical for real-time or resource-limited environments.

SECTION: Comparative Analysis
We conducted a comprehensive performance evaluation of our transformer-based outer encoder/decoder architecture against two alternative encoder/decoder structures as described by Gin et al.: a dense encoder/decoder (DenseRes Block) and a convolutional-based encoder/decoder (ConvRes Block). The DenseRes Block comprised four densely connected layers with ReLU activation functions. The ConvRes Block comprises a series of convolutional layers followed by average pooling layers. It starts with 8 filters, progressively increasing to 64. All layers use a kernel size of 4, stride length of 1, and ReLU activation. Pooling layers have a size of 2 with no padding.

The comparison with DenseRes Block was conducted using the Burger’s equation dataset, comprising 60,000 initial conditions. It’s evident from figure, that both TransRes Block and DenseRes Block converge, with TransRes Block exhibiting slightly lower training and validation losses. Additionally, the loss curves indicate that TransRes Block achieves convergence in fewer epochs compared to DenseRes Block.

In the comparison with ConvRes Block, conducted using the Kuramoto-Shivashinisky equation dataset consisting of 60,000 initial conditions, a notable improvement in both training and validation losses is observed with TransRes Block, as shown in figure. While the loss curve suggests that ConvRes Block exhibited faster convergence during the initial epochs, it’s noteworthy that TransRes Block eventually outperformed ConvRes Block, resulting in superior performance and results.

Our network gives better results in terms of the loss function for both Burger’s and Kuramoto-Sivashinsky equation compared to Gin et al.’s. They used two separate encoder blocks to solve two different systems. Despite using two separate encoder blocks, their results were not as par with the results, that we have obtained in this work. We are able to achieve better results even for three different types of systems using just one type of architecture.

In tableand, we have presented the values of total loss of TransRes Block, DenseRes Block and ConvRes Block, at 700epoch. We can clearly see from tablethat the value of training loss of our proposed network is relatively smaller than that of DenseRes Block. From table, we can see that both the training and validation losses of our network are significantly lower than that of ConvRes’s Block.

SECTION: Discussion & Conclusion
This paper introduces a deep learning architecture meticulously crafted to linearize Fisher’s reaction-diffusion equation. Using Koopman operator theory, we successfully found coordinate transformations for the PDE, resulting in a low-dimensional model. This architecture’s performance was then evaluated on additional PDEs, revealing its capacity to solve complex equations other than Fisher’s equation with remarkable success. Our approach’s consistent performance across different PDEs suggests that it could be used to solve a wide range of nonlinear problems. The architecture is scalable, and by increasing the number of heads or stacking more transformer layers, it can effectively linearize even highly nonlinear PDEs. It should be noted that our strategy is entirely data-driven, which eliminates the need for knowledge of the underlying equations. This makes it ideal for data where the governing equations are unknown.

In the future, further advancements can be made by optimizing these architectures. This could include not only changing the architecture and adjusting the loss functions, but also incorporating new advances in deep learning. Furthermore, continued research efforts might focus on improving the interpretability of the results produced by these frameworks. The application of these architectures extends beyond theoretical considerations to practical real-life problems. PDEs govern a variety of areas, including prey-predator interactions, disease modeling, climate dynamics, and many others, making these systems useful tools for understanding real-world occurrences.

SECTION: References