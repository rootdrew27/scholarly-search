SECTION: Personalized Speech Emotion Recognition in Human-Robot Interaction using Vision Transformers

Emotions are an essential element in human verbal communication, therefore it is important to understand individuals’ affect during human-robot interaction (HRI). This paper investigates the application of vision transformer models, namely ViT (Vision Transformers) and BEiT (Bidirectional Encoder Representations from Pre-Training of Image Transformers) pipelines for Speech Emotion Recognition (SER) in HRI. The focus is to generalize the SER models for individual speech characteristics by fine-tuning these models on benchmark datasets and exploiting ensemble methods. For this purpose, we collected audio data from several human subjects having pseudo-naturalistic conversations with the NAO social robot. We then fine-tuned our ViT and BEiT-based models and tested these models on unseen speech samples from the participants in order to dentify four primary emotions from speech: neutral, happy, sad, and angry. The results show that fine-tuning vision transformers on benchmark datasets and then using either these already fine-tuned models or ensembling ViT/BEiT models results in higher classification accuracies than fine-tuning vanilla-ViTs or BEiTs.

SECTION: IIntroduction

The increasing integration of social robots across various sectors, from healthcare to customer service, underscores their potential to revolutionize human-machine interaction[1,2,3,4]. A crucial factor in their application success is the ability to perceive and respond appropriately to human emotions, facilitating meaningful and engaging interactions[2,5,6,7]. In this context, Speech Emotion Recognition (SER) emerges as a critical field within human-computer interaction[8]. By enabling machines to understand and respond to the emotional nuances embedded in human speech (affective speech), SER can transform our interactions with technology, fostering more natural and empathetic communication[8]. When social robots can accurately interpret affective speech, they can adapt their behavior and responses, leading to more personalized and impactful human interactions[2]. This emotional connection ability holds tremendous potential for enhancing the effectiveness and acceptance of social robots in various real-world applications.The importance of affective speech in human-robot interaction (HRI) lies in its ability to enhance the robot’s social intelligence and facilitate natural communication[8,9]. Emotions play a fundamental role in human interactions. By understanding and responding to affective cues, robots can build trust, rapport, and cooperation with their human counterparts[10]. Affective speech recognition capability enables social robots to accurately perceive the emotional state of the user, allowing them to tailor their responses and provide appropriate support or feedback[11].The area of Speech Emotion Recognition (SER) has witnessed significant advancements over time, driven by the exploration of diverse feature extraction methods and suitable machine learning techniques. Early research focused on traditional approaches like Mel-frequency spectral coefficients (MFCCs) and prosodic features, laying the groundwork for subsequent extensions and improvements[12,13,14]. The emergence of deep learning further matured the area, with models like DNNs, RNNs, and CNNs demonstrating improved capabilities in capturing emotional nuances from speech[15,16,17].

Recent advancements in computer vision, particularly with the emergence of Vision Transformers (ViTs), have opened up new possibilities for leveraging visual data in SER[18]. ViTs, originally designed for image classification tasks, have demonstrated exceptional performance in capturing spatial dependencies and global contextual information[19,20]. Their ability to model long-range dependencies and extract high-level features makes them well-suited for analyzing visual representations of speech, such as spectrograms[21]. The application of ViTs in SER is still an emerging field, and several research directions warrant further exploration.

In this work, we evaluate vision transformer based models for speech emotion recognition. To the best of our knowledge, this work is one of the earliest in the literature to evaluate vision transformer based models for speech emotion recognition in pseudo-naturalistic verbal communications in HRI. This evaluation of ViT based models has been done for modeling the individual characteristics in SER. This means, given a set of audio clips from an individual with labelled emotions (here neutral, happy, sad, and angry), we can predict the speech emotion of that individual for a different set of sentences spoken during a one-to-one HRI. To support our claim, we collect data from human participants an engage them in a pseudo-naturalistic conversation with the robot (explained more in sectionIII-A).This paper makes the following contributions:

This work is among the first to investigate vision transformer-based models (both ViT and BEiT) for SER in the context of pseudo-naturalistic verbal HRI.

We show that personalization of SER models can be done by fine-tuning ViT and BEiT models on benchmark datasets and then further fine-tuning these on participant data and through ensembling the models.

Lastly, we also achieve state-of-the-art (SOTA) performance on the RAVDESS and TESS datasets by a full fine-tuning of the vision transformer-based models.

This paper has been arranged in the following manner: SectionIIoutlines the background literature supporting this work. SectionIIIdescibes the methodology, which includes the data acquisition (SectionIII-A), description about mel-spectrograms (SectionIII-B), datasets used (SectionIII-C), and problem formulation (SectionIII-D). This is followed by SectionIV, which discusses the results we obtained, and followed by the conclusion in SectionVI.

SECTION: IIRelated Works

The evolution of Speech Emotion Recognition (SER) has been marked by a continuous exploration of increasingly sophisticated techniques, each building upon the foundations laid by its predecessors. Early research in SER relied heavily on traditional approaches, such as Mel-frequency cepstral coefficients (MFCCs) and prosodic features[12,13,14]. MFCCs, derived from the human auditory system’s response to sound, capture spectral characteristics crucial for distinguishing various speech sounds, while prosodic features like pitch, intensity, and duration provide insights into the emotional tone of speech. These handcrafted features, though valuable, often struggled to capture the subtle and complex interplay of acoustic cues that contribute to emotional expression.

The advent of deep learning revolutionized the field of SER, offering a powerful framework for automatically learning intricate patterns and representations from raw speech data. Deep Neural Networks (DNNs), with their multiple layers of interconnected nodes, enabled the extraction of high-level features that better captured the subtle nuances of emotional speech[15]. Recurrent Neural Networks (RNNs), particularly Long Short-Term Memory (LSTM) networks, proved adept at modeling the temporal dynamics of speech, crucial for understanding the evolution of emotions over time[16]. Convolutional Neural Networks (CNNs), originally designed for image processing, demonstrated their effectiveness in capturing local patterns and spatial dependencies in spectrograms, further enhancing SER performance[17].

The authors in[22]proposed to use CNN and RNN pipelines along with data augmentation techniques to improve the robustness of these models. This robustness was crucial for a human-robot interaction scenario with robot’s ego noise. Further, the authors in[23]propose a machine learning pipeline for SER. Their approach involves using personalized and non-personalized features for SER. However, neither of these papers contributes to evaluating transformer-based architectures, which are currently SOTA in numerous fields of study[21].

A number of benchmark datasets have been developed for SER that capture speaker characteristics owing to the number of actors involved for generating the data. More information about these datasets have been discussed in SectionIII-C. Owing to this large number of datasets, numerous approaches have been proposed in the literature. Even with transformer-based architectures, limited work has been shown in the SER literature. The authors in[24]show the highest performance on the Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS)[25](described more in SectionIII-C), using a pre-trained xlsr-Wav2Vec2.0 transformer. A more recent transformer-based approach includes the work by the authors in[26]where they used a Whisper-based speech emotion recognition. Other attention mechanism-based approaches for the RAVDESS dataset include[27]. For the Toronto emotional speech set (TESS)[28], authors in[29]tested the accuracies for SER tasks using a vision-transformer-based architecture. These transformers-based approaches have also been evaluated on the Crowd Sourced Emotional Multimodal Actors Dataset (CREMA-D)[30,31]. The authors in[31]tested their approach called the improVed emotion-specific pre-trained encoder (Vesper) on benchmark datasets like Multimodal EmotionLines Dataset (MELD) and Interactive Emotional Dyadic Motion Capture (IEMOCAP) database in addition to the CREMA-D. Further, the authors in[32]approach to use Acoustic Word Embeddings (AWEs) to push the classification accuracies on the Emotional Speech Database (ESD) and IEMOCAP.

However, the literature on SER and the datasets available have not been extensively leveraged to model speaker characteristics in a one-to-one human-robot situation using these SOTA transformer architectures.

SECTION: IIIMethodology

SECTION: III-AData Acquisition

Six neurotypical adults were recruited to participate in a human-robot interaction study to classify their speech into four primary emotions. This one-to-one HRI involves two-way communication between the robot and the participants. Each participant asks pre-defined questions as shown in Figure1. These questions had been used for our previous studies during HRI[33]. The following are the questions we asked the participants to ask the robot:

Hi. What’s your name?

How are you doing?

Did you do anything fun yesterday?

What do you like doing?

Any plans for the weekend?

The robot responds with appropriate answers to those questions and asks those questions back to the participant. The participants’ replies are not pre-defined. They were asked to reply to the robot’s questions with short answers. For each of these question-and-answer pairs, each participant was asked to speak in an emotional tone depicting one of the four primary emotions, i.e., neutral, happy, sad, and angry. The voices of the participants were recorded during this pseudo-natural human-robot interaction where the questions that the participant asks were pre-defined but their answers weren’t.

SECTION: III-BMel Spectrogram

In this paper, since we are using vision based models, we convert the sound signals to 2D images. This is where we leverage the use of mel spectrograms. The mel spectrogram is used for better perception of sounds by humans. Consideringas the normal frequency, the frequency on the mel scale () will be given by[34,35,36]:

As can be seen form equation1, the mel scale is a logarithmic scale to convert the frequency of the sounds from Hz to mels. The audio signal first goes through a fast Fourier transform performed on overlapping signal segments. These frequencies are converted to the log scale and the amplitude is converted to decibels to make the color dimension as shown in Figure1.

SECTION: III-CDatasets

For fine-tuning our vision transformer-based models, we use four benchmark datasets from the literature.

RAVDESS[25]:This dataset has 1440 files containing data from 24 actors making sixty trials each. These actors cover seven emotions: calm, happy, sad, angry, fearful, surprise, and disgust. All of these emotions are deliberately displayed in the speech characteristics of each of the actors by speaking two sets of sentences, each with these seven emotional traits.

TESS[28]:TESS contains data from two actresses aged 26 and 64 years. Each of the actresses speak pre-defined sentences in different ways so as to create a total of 2800 stimuli. These cover seven emotions: happiness, sadness, fear, pleasant surprise, anger, disgust, and neutral.

CREMA-D[30]:This dataset captures six different emotions: happy, sad, neutral, anger, disgust, and fear. These stimuli were created by 91 actors generating a total of 7442 clips.

ESD[37]:This dataset captures the speakers’ emotions for five emotional classes: neutral, happiness, anger, sadness, and surprise. These emotional stimuli were recorded by 20 speakers, 10 of whom were native English speakers.

MELD[38]:It is a multiparty multimodal dataset that captures speakers’ emotions from the TV-series Friends. This dataset captures emotions in both continuous and discrete ways. Among the discrete emotions, it captures seven emotions: anger, disgust, sadness, joy, neutral, surprise, and fear.

For all of these datasets, we have used only four emotion classes that are common between these four datasets, i.e., neutral, happiness, sadness, and anger. In addition to it, we used only ten actors for the ESD dataset who were native English speakers.

SECTION: III-DProblem Formulation and Proposed Pipeline

For each of the datasets used, we generate mel-spectrograms of the speech data.
Given a set of mel-spectrograms extracted from the speech data, the task is to classify each spectrogram into one of four emotion categories: neutral, happy, sad, and angry. Each spectrogram,, where,, andis the index of the datapoint, is passed through two pipelines (see Figure2) to evaluate the performance of vision transformers for speech emotion recognition tasks. Here, represent the height, width, and the number of channels of the image respectively.

The formulation of both of these pipelines remains the same with the only difference of using a pre-trained base ViT encoder (vit-base-patch16-224) for the first pipeline (see Figure2a) whereas using a base BEiT encoder (microsoft/beit-base-patch16-224-pt22k-ft22k) for pipeline 2 (shown in Figure2b)[19,20]. Each imageis first divided into patches,, where P = 16 is the dimension of the image patch. So the output of the linear projection layer,, whereis the number of patches. The patch and position embedding is then done using:

where LN(.) is the layer normalization layer andpos_embedis the position embedding added to each vector at the end of the linear projection layer. Then the values in the sequence are weighted through learnable matrices: query (q), key (k), and value (v) to calculate self-attention given by the authors in[21,19]:

where,are learnable matrices. Then the self-attention is calculated as:

So, the multihead attention, which is the multiple self attention operations in parallel heads can be expressed as[21,19]:{dmath}MSA(z_norm) = [SA_1(z_norm);SA_2(z_norm);…;SA_k(z_norm)]U_msa

where,,is the dimension of each head,is the number of attention heads, andis the dimension of the transformer model. The output of the transformers encoder is given by:{dmath}^y = (MSA(z_norm) + z )+MLP(LN(MSA(z_norm) + z))

where MLP(.) is the multilayer perception.

SECTION: IVResults and Discussion

We evaluate both the ViT (Figure2a) and the BEiT (Figure2b) pipelines in two ways:

Approach 1:In this approach, we train the individual ViTdand BEiTdmodels, where. We split each of the datasets,intoand. Then we train separate ViTdand BEiTdmodels, individually for each of these datasets. Since we have a four class classification problem of classifying the mel spectrograms into four primary emotions, we use cross entropy loss.

Approach 2:In this we combine the datasets together:

and then fine-tune a ViTmixand a BEiTmixmodel on this mix training set.

We perform full fine-tuning of our models on two A5000 GPUs, using K-Fold-Cross validation (f-fold-cross-validation in our case) with a constant learning rate of. Further, we evaluate the performance of both pipelines for both Approach 1 and 2 using accuracy, precision, recall, and f-1 scores.

TableIIshows the performance of both Approach 1 and Approach 2 for both the ViT and BEiT models. Fine-tuning vanilla ViTs and vanilla BEiTs gives us some promising results when compared to the SOTA. For the RAVDESS dataset, we currently achieve SOTA using the vanilla-ViT model, with the highest performance of 97.49% accuracy as compared to the current SOTA, which has a classification accuracy of 86.70% using multimodal data[25]. For the TESS dataset, we again achieve SOTA using vanilla-ViTs and vanilla-BEiTs, which is very similar to the ones obtained by the authors in[29]. The classification accuracy for the CREMA-D dataset was the highest for the mixed dataset approach (Approach 2) with vanilla-ViTs, which is better than the performance of comparable transformer architectures presented by the authors in[39]and other non-transformer-based approaches[40,41]. For the ESD dataset, our peak classification accuracy (96.25%) was obtained by a vanilla-BEiT model fine-tuned only on, which is again comparable to the current SOTA (93.20%) as presented by the authors in[32]. Since MELD dataset has numerous speakers, it covers a wide-range of speaker characteristics (see Figure3). This can be see in the low classification accuracy of the MELD dataset from the TableII. We obtained peak accuracy when the BEiT model fine-tuned over. However, our results with the MELD come close to the classification accuracies presented by the authors in[26].

SECTION: IV-AHuman subjects’ study

We evaluated our speech emotion recognition in a pseudo-naturalistic human-robot interaction scenario using our fine-tuned ViTs and BEiTs. Since each participant asked five questions to the robot and responded to those five questions asked by the robot, we have 40 audio clips from each participant. We divided them into train and test datasets such that two sets of questions and answers each participant gave were separated for the test set. So, each participant had three questions and answer sets for train data. Each of those questions and answers was spoken in a way that depicts each of the four primary emotions of the individual. The split of the train and test data for each participant is shown in TableIII. Once the audio has been recorded from the participants, we convert the WAV files into spectrograms as shown in Figure1.

As described in SectionIII-A, each question-answer set was spoken in the four primary emotions. Hence, each participant had six audio clips for each emotion for the train set and four for the test set. Owing to the performance of Vision transformers-based approaches from TableII, we used similar approaches to evaluate the use of vision transformers for speech emotion recognition in pseudo-naturalistic human-robot interaction.

Model 1- Vanilla-ViT and BEiT:Each individual’s data is converted to mel-spectrograms, and then vanilla-ViT and BEiT models are fine-tuned.

Model 2- ViTmixand BEiTmix:The fine-tuned models from Approach 2 are fine-tuned on the participants’ mel-spectrograms.

Model 3- ViTensembleand BEiTensemble:We use five vanilla-ViTs and five vanilla-BEiTs and average the logits. If the output of each ViTi, whereand of each BEiTiareandrespectively, then the ensemble of models is:

Model 4- ViTensemble,dand BEiTensemble,d:In this approach, we use the ViTdand BEiTdmodels trained in Approach 1 on each of the benchmark datasets. So the ensemble works as follows:

TableIVshows the model performance of all the above-proposed models. It becomes evident that the best performance is obtained when we use two broad approaches. First, when we use ViTmixor BEiTmix. This means that fine-tuning a vision transformer-based model on already existing benchmark datasets like the RAVDESS, TESS, CREMA-D, ESD, and MELD helps in recognizing the individual speech emotion characteristics of an individual speaker. The reason can be seen in Figure3ashowing the t-sne plots of the vanilla-ViT embeddings. These t-sne plots show the ViT embeddings for each emotion, plotted for all the benchmark datasets and the participants’ data. The overlap between the speech characteristics of speakers from these benchmark datasets and the participants for our human-robot interaction study helped better classify speech emotion compared to vanilla ViTs or vanilla-BEiTs. For some participants, the ensemble models worked better since their speech characteristics didn’t exactly overlap with the benchmark datasets used in this paper. These outliers can be seen from the t-sne plots in Figure3. Fine-tuning Vanilla-ViTs or BEiTs on each participant data for recognizing an individual’s speech emotion had the worst classification accuracies as compared to its other counterparts shown in TableIII. The confusion matrix for each of the participants’ best performing models have been shown in Figure4.

SECTION: VEthics Statement

Since this paper includes a human subjects’ study, we took consent of all the participants on a consent form approved by the Institute Review Board (IRB Number: 18.0726). The participants had the opportunity to discontinue at any point of the study if they wanted to.

SECTION: VIConclusion

In this work, we address the gap in speech emotion recognition for pseudo-naturalistic and personalized verbal HRI. We evaluate the use of vision transformer based models for identifying four primary emotions: neutral, happy, sad, and angry from the speech characteristics of our participants’ data. We do this by first fine-tuning the vision transformer-based models on benchmark datasets. We then use these fine-tuned models to fine-tune them again on participants’ speech data and/or perform ensembling of these models. This helps us choose the best model for each participant, hence contributing towards understanding the emotional speech characteristics of each individual instead of proposing a group model. In addition to creating these personalized speech emotion recognition models, we also evaluate vanilla-ViT and vanilla-BEiTs on benchmark datasets like RAVDESS, TESS, CREMA-D, ESD, and MELD. We observed SOTA performances on some of these benchmark datasets.

In the future, we would like to recruit more human participants and collect data across different populations, including both neurotypical and neurodivergent populations. We would also like to examine multiple data modalities and examine how speech emotion correlates to modalities such as facial videos and physiological signals.

SECTION: References