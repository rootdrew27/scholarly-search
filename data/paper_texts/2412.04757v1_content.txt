SECTION: Ltri-LLM: Streaming Long Context Inference for LLMs with Training-Free Dynamic Triangular Attention Pattern

The quadratic computational complexity of the attention mechanism in current Large Language Models (LLMs) renders inference with long contexts prohibitively expensive.
To address this challenge, various approaches aim to retain critical portions of the context to optimally approximate Full Attention (FA) through Key-Value (KV) compression or Sparse Attention (SA), enabling the processing of virtually unlimited text lengths in a streaming manner. However, these methods struggle to achieve performance levels comparable to FA, particularly in retrieval tasks.
In this paper, our analysis of attention head patterns reveals that LLMs’ attention distributions show strong local correlations, naturally reflecting a chunking mechanism for input context. We propose Ltri-LLM framework, which divides KVs into spans, stores them in an offline index, and retrieves the relevant KVs into memory for various queries. Experimental results on popular long text benchmarks show that Ltri-LLM can achieve performance close to FA while maintaining efficient, streaming-based inference.

SECTION: 1Introduction

Recently, Large Language Models (LLMs) have made significant progress in extending context windows, as demonstrated by GPT-4(OpenAI et al.,2024)and LLaMA-3.1(Grattafiori et al.,2024)with a 128K window, GLM-4-9B(GLM et al.,2024)with a 1M window, and Gemini-1.5-pro(Google et al.,2024)with a 10M window.
Despite these advancements, many open-source LLMs that support extended context lengths still face substantial computational challenges during inference. These challenges stem primarily from the excessive Key-Value (KV) cache storage requirements and the quadratic computational complexity(Vaswani et al.,2017)of attention mechanisms.

KV cache compression has emerged as a promising approach to alleviate computational and memory bottlenecks during inference, with studies demonstrating its potential to improve efficiency(Zhang et al.,2023;Li et al.,2024;Lee et al.,2024;Liu et al.,2024a;Ge et al.,2024;Adnan et al.,2024;Cai. et al.,2024).
While these methods typically rely on heuristic rules to compress or discard less critical KV elements, their effectiveness is often limited by accuracy trade-offs and constrained applicability in specific contexts.

Recently, a novel approach called InfLLM has been proposed(Xiao et al.,2024), which introduces a block-based streaming mechanism, which can be likened to constructing a Full-Text Index while simultaneously performing Retrieval-Augmented Generation (RAG)(Gao et al.,2024).
Although this design aligns closely with human intuition for managing large volumes of information—storing older information and retrieving it as needed, its effectiveness is heavily influenced by the quality of text index construction and retrieval processes, underscoring the importance of robust indexing and retrieval mechanisms in achieving optimal results.

We evaluate InfLLM on the LLAMA3-8B-Instruct-262K111https://huggingface.co/gradientai/Llama-3-8B-Instruct-262kmodel on a set of Needle-In-A-Haystack(NIAH) tests consisting of eight questions. As depicted in Figure1, InfLLM struggles to provide correct answers.
Our analysis revealed that the majority of InfLLM’s failed cases originate from overlooking of the crucial tokens with low attention scores.

In this paper, we address this issue by preserving independent semantic spans to the greatest extent possible. Our findings reveal that the attention distributions of LLMs exhibit strong local correlations, manifesting as multiple triangular regions in the attention map. These triangular patterns naturally correspond to the model’s interpretation of semantic segmentation. Building on this observation, we employ the Non-Maximum Suppression (NMS) technique to develop a simple yet effective algorithm for identifying the boundaries of these spans. Subsequently, we dynamically generate index vectors for each span by leveraging a “voting” mechanism among its neighboring spans.

In summary, our contributions are as follows:

We investigate the reason why InfLLM struggles in NIAH test and identify that in the correct cases, the model’s recall is significantly higher than the wrong cases, and the recall varies considerably across different layers. Further, we attempt to analyze the effect of mandatory evidences injection, we found it can improve the accuracy, but it still largely depends on the model’s inherent capabilities.

Referring to the steps of InfLLM, we propose a novel methodLtri-LLM, which can efficiently and flexibly identifies semantic spans of the long context and store them in an offline cache, then retrieve them accurately when needed.

We employ LLAMA3-8B-Instruct-262K as the base model and conduct thorough evaluation on the long context benchmarks, including NIAH,-Bench(Zhang et al.,2024)and RULER(Hsieh et al.,2024). Experimental results show thatLtri-LLMcan achieve promising performance while maintaining equally computation cost as InfLLM.

SECTION: 2Preliminaries

In this section, we firstly introduce the overall framework of InfLLM, including the KV offloading and retrieval process. Then, through the analysis of recall and success rate on NIAH test, we prove that the bottleneck of InfLLM lies in the performance of retrieval.

Background.InfLLM splits the tokens into three parts in order, i.e., the Initial tokensof length, the Evicted tokensof lengthand the Local tokensof length.are responsible for modeling local dependencies and the(also known as Attention Sinks) maintain the numerical stability of the attention distribution. InfLLM processes the tokens in a streaming manner. At each step, it retrieves constant number of relevant blocks asfrom the context memory using the similarity score and apply attention to the concatenation of. When the length of the input tokens exceeds, it stores tokens from the beginning ofto context memory, picking tokens with high attention scores as the index vectors.

Response Quality Hinges on Recall.We construct a group of NIAH tests with eight bilingual questions. More details are shown in AppendixA. We carefully record whether the needle is incorporated in the retrieved blocks or not at each decoding step. The model’s responses are scored by GPT-4(OpenAI et al.,2024), only the responses with the highest score are considered as successful ones.
We plot the average recall across different decoder layers and decoding steps. Figure2demonstrates the needle recall for both successful and failed instances. Obviously, there is a discernible trend shows that the successful cases have a much higher recall comparing to the failed cases.

Mandatory Evidence Injection Transforms Unreasonable Responses.The observed relationship raises the question of whether increasing the recall could transform failed cases into successful ones. We validate this hypothesis by mandatorily injecting the blocks containing the needle intoon the failed cases. The results of conversion are shown in Table2. After the mandatory injection, nearly all English NIAH cases are successfully converted, whereas the lower conversion rate of Chinese NIAH cases is probably due to the tested LLAMA-3 model being predominantly trained on English corpus. This phenomenon confirms that the bottleneck of InfLLM lies in the retrieval recall.

cccccc\CodeBefore\BodyTask ID (Zh)#Failure#ConvertedTask ID (En)#Failure#Converted1  477  51  5  894  7732  682  638  6  775  7733  1021  2  7  212  2104  211  0  8  826  702

SECTION: 3Method

SECTION: 3.1Local Triangle-shaped Aggregated Attention Pattern

Figure3illustrates the attention map across different layers of the model. Tokens within the triangular regions exhibit higher mutual attention scores compared to those outside these regions. This phenomenon, commonly referred to as localized attention or block-sparse attention, has been highlighted in recent studies such as PyramidKV(Cai. et al.,2024)and MInference(Jiang et al.,2024). Leveraging this property, we construct indexes for distinct semantic spans, aiming to retain more semantic information while minimizing the associated memory footprint.

SECTION: 3.2Semantic Span Division and NMS Acceleration

We develop an algorithm that can fast determines the boundaries of these triangle regions. The formulation of the algorithm is given as follows. Given Queryand Key, whereis the number of heads,is the length of tokens, andis the dimension of each head, the attention map of Multi-Head Attention (MHA) of a single head,is defined as:

To prevent the earlier tokens from attending to the subsequent ones in auto-regressive LLMs, a lower triangular matrix mask is applied to the attention map. The mask matrixis defined as:

Then the final attention mapis:

Next, we sum the attention scores over heads, and define the Triangle Attention (TA) score of span betweenandas:

Since the upper diagonal elements ofare zero, we can apply an accumulation summation operator, such ascumsum, as shown in the second equation of Eq.4. Notably, as the elements of the attention map must be positive (as per Eq.4), the largest TA score is, located at the bottom-left corner of the attention map. The triangular regions stand out due to the significant score differences compared to their surrounding areas. Inspired by the concept of Baseline Reward in the REINFORCE algorithm(Williams,1992), we introduce a thresholdfor the attention map. By subtracting, scores below this threshold become negative, allowing us to effectively filter out unwanted TA scores and retain only the relevant ones:

From Eq.5, we can get the TA score for every span of text. Unfortunately, the spans with high TA scores are highly overlapped (e.g., a subset of a salient triangle region is also likely to be another salient triangle region).

To address this issue, the Non-Maximum Suppression (NMS) algorithm(Ren et al.,2017), originally designed to eliminate redundant boxes in object detection models, is employed to remove overlapping spans with low scores. As NMS is widely utilized in various efficient on-device models, we adopted the off-the-shelf implementation as part of our method. Further details of the algorithm are provided in AppendixE.

SECTION: 3.3Span Index Vector

In this section, we will discuss the static and dynamic methods of how to generate the index vectors of the evicted tokens. The static methods generate constant number of index vectors for each span whereas the dynamic methods generate a dynamic number of index vectors based on the confidence score of each span.

InfLLM adopts a static method which calculates the cumulative attention score for each token. Formally, lettingbe the accumulated attention score andbe the sliding window size, the accumulation score of tokenis,

InfLLM adopts the Keys of toptokens with the largestas the index vectors of a fixed-size block. We use this method to generateindex vectors for each span, whereis the number of spans for each block. We also tried to use the mean pooling of the Keys within each span as the index vector. Unfortunately, we found that the static methods performed poorly in the early experiments.

The static method treats the accumulated attention scores acquired by a token as a form of voting, suggesting that tokens with the highest votes can effectively represent a span.

However, in certain cases, the voters may lack sufficient knowledge about the candidates, making it difficult to identify truly outstanding ones. When voter confidence is low, retaining more index vectors for a span becomes crucial to ensure accurate retrieval. To address this, three metrics are introduced to evaluate voter confidence, enabling the allocation of fewer index vectors to spans with higher confidence and more to those with lower confidence.

The three metrics are based on the ratio of the TA score and its neighbors. As illustrated in Figure5, consider a block divided into four spans (Span1toSpan4), withSpan3as an example. The ratio of the area between the span and its neighbors is computed.
As shown in Figure5, assuming a block is divided into four spans (i.e.,Span1toSpan4) and takingSpan3as an example, we compute the ratio of the area between the span and its neighbors.
Three specific ratios are defined to capture the relationships ofSpan3with its row neighbors, column neighbors, and both.
A high ratio suggests that the neighbors have limited knowledge about this span, indicating insufficient voting confidence, necessitating more index vectors for this span. Conversely, a low ratio implies sufficient confidence, allowing us to represent the span with fewer index vectors.

Next, we develop a specific function to establish a correlation between the level of confidence and the number of index vectors to be retained. Specifically, we use a function where the input variable is the ratio of the span’s area to its neighbors’ area, denoted as, and the output variable is the ratio of index vectors to be retained, denoted as. Formally,is defined as follows:

whereis a hyper-parameter controlling the increasing speed of the function. As shown in Figure6, a largerimposes a stronger restriction on the number of index vectors. The number of the index vectors is calculated by the length of span times the, i.e.,. In practice, we often set a minimal and maximal number of index vector to restrict the number of index vectors stored in GPU Memory.

In LLMs, the distribution of area ratios across different layers can vary significantly, making it challenging to control the number of index vectors within a reasonable range using a fixed. To determine an appropriatefor each layer, we first collect the values ofunder different contexts and calculate the probability ofacross various ranges. Using these probabilities, we can calculate the expected number of index vectors for any giventhrough Eq.7. Consequently, for any specified compression ratio (i.e., the ratio between the number of tokens and the number of index vectors), we can find the smallestthat meets this compression ratio for each layer by consulting a lookup table. This approach retains more index vectors while ensuring the desired compression ratio, thereby improving recall.

SECTION: 3.4Overall Framework

In this section, we will introduce the Ltri-LLM framework step-by-step. Ltri-LLM mainly consists of three steps as shown in Figure7. In Ltri-LLM, we process the input context in a streaming manner with a fixed window size. The streaming-type fixed-size Local Attention Map is always stored in the GPU Memory as shown in the upper right side of Figure7. As the process goes on, we save the previous tokens to the Context Memory and then retrieve them as needed. Suppose we need to process a new block of context whose length is.

In the first step, we detach the left part of the attention map which is corresponding to the previoustokens in the GPU Memory. Next, we apply the method introduced in Section3.2to divide theintosemantic individual spans. Then, we use the approach from Section3.3to generate the index vectorsof these spans. We store the Keys of the previous tokensto the CPU Memory andto the GPU memory.

In the second step, we divide the tokens of the upcoming block using the same method as in the first step, generating in the index vectors for the Queries,. Then, we retrieve relevant tokens usingby calculating the similarity score with the index vectors stored in the Index Memory,. Specifically, assuming the-th span’sstored in the Index Memory consists ofindex vectors, and the-th span’sconsists ofindex vectors, the similarity score between the-thand the-th,, is computed as follows:

Next, we retrieve the relevant blocks based on the similarity scores. Specifically, considering that each block consists ofspans and there areblocks stored in the Index Memory, we sort the blocks by the maximum similarity of thespans. We then select the top K indices as the retrieval results, denoted as.

whereindicates the indices of spans corresponding to block.

In the third step, we perform attention betweenand the retrieved results. Specifically, we begin by concatenating the Keys of the initialtokens, the retrievedtokens, the nearesttokens, and the currenttokens. Then, we useto attend to these Keys. The outputs of this attention process are passed to the next layer for further computation, and the attention map ofis appended to the fixed-size local attention map stored in the GPU memory.

SECTION: 3.5Accurate and Persistent Collaborative Evidence Retrieval

In addition to the Triangle pattern, we also identified three components that can improve the retrieval accuracy, including Persistent Mechanism, Retrieval Heads, and Collaborative Voting.

Persistent Mechanism.As discussed in previous sections, the performance of Ltri-LLM heavily relies on the recall of the current block. However, during the decoding stage, the block becomes a single token, which can significantly amplify the variation in results. To mitigate this variation, we choose to retain the retrieved blocks from the last chunk of the prefilling stage throughout the decoding stage.

Retrieval Heads.Wu et al.(2024)discovered that only a small subset of attention heads significantly contributes to retrieval ability, and these are referred to as retrieval heads. To leverage these heads, we first identify them using the method outlined byWu et al.(2024), labeling attention heads with scores above 0.1 as retrieval heads. In our approach, when a layer contains multiple retrieval heads, we retain the one with the highest score. The selected retrieval heads and their corresponding scores are detailed in AppendixC.

Collaborative Voting.During the forward process, each layer has its own context manager and independently determines the location of evidence. When the-th layer needs to identify evidence, it can use the results from all preceding layers to make a comprehensive judgment. In practice, we retain the results from layers that have at least one retrieval head along with their corresponding retrieval head scores. For a thorough evidence location judgment, we suggest using a voting mechanism where the retrieval head scores act as voting weights.

SECTION: 4Experiments

Implementation Details.Ltri-LLM is consistent with InfLLM on the primary hyperparameters. Specifically, the number of initial tokens, local window size, memory unit size, GPU cache size, encoding chunk size, and score decay coefficient are set at 128, 4096, 128, 32, 512, and 0.1 respectively. Unless otherwise stated, in Section3.3.2, we employ the row ratio calculation method and designateas 3. Thevalue for the first four layers is assigned as 20. Furthermore, we ensure the last encoding chunk size of the prefilling stage can accomodate all query tokens without beging excessively large. While the attention map threshold, the Intersection over Union (IoU) thresholdfor the Non-Maximum Suppression (NMS) operator from torchvision and the maximum number of retained index vectors are detailed in AppendixB.

Dataset & Evaluation Metrics.We evaluate our method by three widely used benchmarks, i.e., Neelde-In-A-Haystack(Kamradt,2023),-Bench(Zhang et al.,2024)and RULER(Hsieh et al.,2024). Needle-In-A-Haystack is designed to test in-context retrieval ability of long context LLMs.-Bench comprises synthetic and realistic tasks spanning diverse domains to evaluate the comprehensive modeling capabilities for long context, presented in both English and Chinese. RULER expands upon the vanilla NIAH test to encompass variations with diverse types and quantities of needles. It also introduces new task categories including multi-hop tracing and aggregation to test behaviors beyond searching from context.

lccccccccccccc\CodeBefore\BodyMethods  Streaming  En.Sum  Zh.QA  En.QA  En.MC  En.Dia  Co.De  Ma.Fd  Re.PK  Re.Num  Re.KV  Avg. w/o KV  Avg.LLAMA3-8B-Instruct-262K†✗  20.2  12.9  12.4  67.3  6.0  22.1  26.6  100.0  100.0  14.4  40.8  38.2MInference  ✗20.810.612.565.97.522.332.3100.0100.012.841.338.5LM-Infinite  ✓  14.0  7.8  14.1  39.3  3.5  42.9  17.1  6.8  6.8  2.4  16.9  15.5StreamingLLM  ✓  14.9  8.0  14.1  38.9  4.0  42.6  16.7  6.8  6.8  2.4  17.0  15.5InfLLM  ✓16.38.5‡19.5  38.06.044.723.7100.0100.030.2  39.6  38.7Ltri-LLM✓  16.19.6‡21.244.56.041.626.3100.0100.064.040.642.9

Baselines.We compare our method with four training-free baselines, including three streaming methods: LM-Infinite(Han et al.,2023), StreamingLLM(Xiao et al.,2023)and InfLLM(Xiao et al.,2024)as well as a Non-streaming state-of-the-art long context accelerating method, MInference(Jiang et al.,2024), which retains the full attention between the last few tokens and the whole context. In LM-Infinite and StreamingLLM, we specify the the number of initial tokens and local tokens as 128 and 6144, respectively. For InfLLM, the number of initial tokens, local tokens, block size, the number of representative tokens and reserved blocks are set to 128, 4096, 128, 4, and 16, respectively. These configurations facilitate a fair comparison in terms of the number of tokens involved in the attention calculation. For MInference, we use the default configuration recipe for LLAMA3-8B-Instruct-262k.

Needle-In-A-Haystack.We set the test length of NIAH to span between 20K and 230K, with the needle’s position varying from 10% to 90%. Each NIAH experiment is conducted across 100 groups. In both InfLLM and Ltri-LLM, the needles might be split into multiple blocks, possibly impacting performance. To evaluate this effect, we impose a restriction that the needle should locate within one single block when creating the corpus, and compare the performance before and after applying this restriction. Figure8demonstrates that Ltri-LLM consistently outperforms InfLLM under both conditions and exhibits strong robustness even when the restriction is removed.

-Bench.Table4shows the results of-Bench. Ltri-LLM performs exceptionally well across retrieval tasks like Retr.PassKey, Retr.Number and Retr.KV. Ltri-LLM has outperformed all baseline models on the Retr.KV task, even significantly outshining the vanilla model of LLAMA3-8B-Instruct-262K. This suggests that Ltri-LLM has managed to retain crucial information while filtering out noise, thereby minimizing any distractions to the model. For other more realistic tasks such as En.QA and Zh.QA, Ltri-LLM achieves the best among all models and the best among streaming models, respectively. Overall, Ltri-LLM has outshone all other models by achieving the highest total average score in all 10 tasks, achieving the best among all models in 4 tasks, and the best among streaming models in 8 tasks. Even without the best-performing Retr.KV, Ltri-LLM can still achieve performance close to that of vanilla LLAMA3-8B-Instruct-262K and MInference. Given that MInference needs to retain the full attention of the last part of the input tokens and all previous input tokens, Ltri-LLM’s streaming manner offers greater potential for efficiency.

lcccccccc\CodeBefore\BodyMethods  Streaming  4K  8K  16K  32K  64K  128K  Avg.LLAMA3-8B-Instruct-262K†✗  97.2  91.8  87.3  80.8  77.4  72.2  84.4MInference  ✗92.887.287.382.382.877.184.9LM-Infinite  ✓  92.5  65.9  44.2  23.2  16.8  10.7  42.2StreamingLLM  ✓  92.5  65.6  44.0  22.9  16.8  11.5  42.2InfLLM  ✓  92.5  83.6  59.0  36.5  31.2  26.7  54.9Ltri-LLM✓92.883.776.672.367.666.776.6

RULER.Models are evaluated with context sizes ranging from 4K to 128K, and the average metric of all tasks is reported. Compared to other streaming baselines, Ltri-LLm has achieved a significant lead at ranging from 4K to 128K context lengths.
Another noteworthy phenomenon is that the performance of almost all methods will decrease as the evaluation length increases. As shown in the Figure9, we draw the delta value between adjacent lengths in RULER for different models. It’s particularly noticeable how the streaming baselines take a steeper dive, while the performance drop for Ltri-LLM and vanilla LLAMA-3-8B-Instruct-262K remains fairly consistent. This reflects that previous streaming baselines have difficulty in scaling up to longer texts, and Ltri-LLM has truly overcome this problem.
Although Ltri-LLM has a clear advantage over streaming baslines, there is still a gap with MInference. As shown in the Figure24, we compared the scores of Ltri-LLM and MInference on different tasks in RULER. The complete scores on every length can be viewed in the AppendixF. Ltri-LLM basically tied with MInference in the single NIAH test, but there was a noticeable gap in the more difficult multi-key NIAH test and variable tracking tasks.
We suspect that this shortcoming might be due to the streaming manner of the Ltri-LLM. In other words, Ltri-LLM is unaware of the final question during the phase of streaming context encoding. Although Ltri-LLM has made efforts to enhance its retrieval capabilities through TA, it still falls short when put under rigorous tests like the RULER, which demands high retrieval prowess.

SECTION: 5Ablation Studies & Analysis

Ablation Study.We conduct ablation studies on the NIAH test to evaluate the three components proposed in3.5. During these studies, the ratio mode, prefilling last chunk sizeandfor dynamic span index vectors are assigned to row, 32 and 3, respectively. Our main observations are: (1) The retrieval heads are crucial for performance. Disabling retrieval heads and using average results from all attention heads for retrieval hinders performance, suggesting that the other attention heads except retrieval heads, act as noise disturbances. (2) The persistent mechanism and the voting mechanism contribute positively. Notably, when performing ablation study, we enable the needle synthetic locationrestrictas mentioned in Figure8.

Hyperparameter Tuning.Initially, we should determine the attention map threshold, as described in Section3.2, along with the Intersection over Union (IoU) thresholdfor the Non-Maximum Suppression (NMS) operator from the torchvision222https://github.com/pytorch/visionlibrary. Specifically,andare jointly adjusted via random search for each individual layer to optimize the F1-score of semantic spans overlapping with the evidence.is searched within the interval (0.0001, 0.95) uniformly, whileis explored uniformly within the interval (0.01, 0.95). Detailed values ofandcan be found in AppendixD.
The typical reference values forandare 90thpercentile and 0.1, respectively.
For the prefilling last chunk size, we varyinand find the best performance is achieved whenas shown in Figure11.
Next, we fix the optimaland report the best performance for variousvalues in. Figure12suggests that Ltri-LLM keeps decent performance for smaller values of, while the performance deteriorates whenbecomes larger.
Finally, we fix the bestandvalues and explore different ratio modesin Figure13. The results indicate that all ratio calculation methods, i.e., row, col, and rowcol can yield ideal performance.

Compression Ratio.For each block of length, up tospan index vectors can be generated. Figure14shows the average number of span index vectors generated per block at all layers for variousvalues. We setuniformly for the first four layers. Generally, the average number of span index vectors per block increases with layer depth, reflecting a larger ratio value and aligning with the triangular attention pattern observed in Figure3. Additionally, asrises, the average number of span index vectors per block decreases, consistent with Equation7. For a sequence length, letbe the number of attention heads andthe number of applied retrieval heads; Ltri-LLM will approximately retainspan index vectors after prefilling. A relaxed lower bound for the compression ratiois.

lcccccccc\CodeBefore\BodyMethods  100K  200K  300K  400K  500K  600K  700K  800KLLAMA3-8B-Instruct-262K  23.344  47.781  72.188  96.594  121.000  145.438  169.844  194.250InfLLM-reprTopk12  2.241  4.587  6.930  9.273  11.616  13.962  16.305  18.648Ltri-LLMM=12,λ=30.027  0.057  0.087  0.117  0.148  0.178  0.209  0.240Ltri-LLMM=12,λ=40.025  0.052  0.081  0.110  0.139  0.168  0.198  0.227Ltri-LLMM=12,λ=50.022  0.046  0.072  0.098  0.125  0.152  0.180  0.207Ltri-LLMM=12,λ=60.019  0.040  0.063  0.086  0.111  0.134  0.159  0.184Ltri-LLMM=12,λ=70.017  0.035  0.055  0.077  0.098  0.119  0.141  0.163Ltri-LLMM=12,λ=80.015  0.031  0.048  0.067  0.086  0.105  0.125  0.144Ltri-LLMM=12,λ=90.013  0.027  0.043  0.060  0.077  0.094  0.111  0.129Ltri-LLMM=12,λ=100.012  0.024  0.039  0.054  0.069  0.084  0.100  0.116

SECTION: 6Related Work

Scaling Up Context Window of LLMs.The primary strategy for scaling up the context window of LLMs involves adjusting the parameters of position embeddings like Rotary Position Embedding (RoPE)(Su et al.,2021), including training-free methods(Chen et al.,2023;LocalLLaMA,2023;Peng et al.,2023;Liu et al.,2024b), and continual-training methods(Rozière et al.,2024;Fu et al.,2024;Bai et al.,2024). While these methods show promising results in long context tasks, the quadratic complexity of Full-Attention (FA) imposes heavy burdens on memory and inference time which greatly restricts the real-world application of long context LLMs.

Attention Pattern AnalysisStreaming-LLM(Xiao et al.,2023)and LM-Infinite(Han et al.,2023)identified the attention sink phenomenon, noting that preserving initial tokens aids in restoring the performance of sliding window attention.
FlexGen(Ge et al.,2024)discovered attention heads usually have different structures and propose four KV cache compression policies. MInference(Jiang et al.,2024)summarized three unique patterns in long context attention matrices and directly adopt sparse attention kernels in the pre-filling stage. PyramidKV(Cai. et al.,2024)discovered that shallow layers show more localized attention and deep layers show more massive activation and it assigns more KV cache to shallow layers and less to deep ones.

Long Context Inference AccelerationOne kind of the long context inference acceleration method is to reduce the memory space of KV while keep the computation complexity of attention unchanged, including reducing the hidden states space(DeepSeek-AI et al.,2024;Ainslie et al.,2023;Hooper et al.,2024)and share KV across layers(Sun et al.,2024;Brandon et al.,2024). Another kind is to reduce the space and computation simultaneously on the sequence dimension. Some of them involve additional training(Nawrot et al.,2024;Munkhdalai et al.,2024;Yang et al.,2024a)or another structure like State Space Models (SSMs)(Gu & Dao,2024;Lieber et al.,2024;Dao & Gu,2024). In this paper, we mainly focus on training-free acceleration methods, including dynamic sparse attention(Jiang et al.,2024;Tang et al.,2024;Ribar et al.,2024;Yang et al.,2024b)and KV compression methods(Zhang et al.,2023;Li et al.,2024;Lee et al.,2024;Liu et al.,2024a;Ge et al.,2024;Adnan et al.,2024;Cai. et al.,2024;Xiao et al.,2024).

SECTION: 7Conclusion

In this paper, we discover the inference obstacles of InfLLM by presenting the strong correlation between reasonable model responses and accurate evidence recall. Based on the observation of triangular LLM attention distribution, we propose Ltri-LLM, a novel approach that efficiently and flexibly identifies semantic spans within long contexts. Extensive experiments on NIAH,-Bench, and RULER verify the effectiveness of Ltri-LLM.

SECTION: References

SECTION: Appendix ADetails of Mandatory Evidence Injection

Besides the original NIAH test, we construct other seven additional bilingual NIAH tasks, as shown in Figure15. including factual and semi-open genres.
For each task, we evaluate LLM with 35 context lengths evenly distributed from 10K to 230K. The insertion position of needle is uniformly varied from 0% to 100% for each context length, resulting in a total combination of 1,225 tests.

Figure16and17illustrate the performance of LLAMA3-8B-Instruct-262K and InternLM-2-Chat-7B with InfLLM across all NIAH tasks as depicted in Figure15. Generally, InfLLM’s design allows the LLM direct evidence access during the decoding stage when the evidence is inserted at the very beginning or end of the prompt, leading to superior performance. Subsequently, we choose several unsuccessful combinations from each NIAH task, and activate the mandatory evidence injection mechanism during their second execution to observe whether the model’s response shifts from failure to success.

The experimental results depicted in Figure17aindicate that the approach of mandatory evidence injection intoeffectively convert most failure cases from LLAMA3-8B-Instruct-262K equipped with InfLLM into successful ones across all English NIAH benchmarks, while the similar results merely occur on the second Chinese NIAH test. The phenomenon is attributed to two primary factors: (1) The training corpus for LLAMA3-8B-Instruct-262K is predominantly English, contributing to exceptional performance on English benchmarks.
While InternLM2-Chat-7B(Cai et al.,2024), undergoing extensive pre-training on Chinese corpora, demonstrates observable conversion across both Chinese and English needle-in-a-haystack tests, as shown in Figure17b.
(2) The model response is deemed valid only if it achieves the highest score in the GPT-4 evaluation. Nevertheless, significant improvements in the model responses have been observed following the mandatory evidence injection sometimes, even if they have not met the success criterion yet.

To further validate the proposition in a more realistic scenario, we also examine the conclusion in a Chinese Question Answering dataset (Zh.QA) from InfiniteBench, which includes 175 examples with the average token length of more than 2,000 thousand. However, Zh.QA does not provide evidences corresponding to the answers. To locate the evidences, we conduct a two-round dialogue with GPT-4, as shown in Figure19. In the first round, we require GPT-4 to identify the exact snippet in the passage that corresponds to the reference answer for the given question. In the second round, we let GPT-4 determine whether the information in the snippet allows for an accurate deduction of the answer to the question. After these two rounds of dialogue, we end up with a reliable converted Zh.QA dataset with the NIAH format.

There are a total of 65 pieces of data with context length less than 1M tokens in the Zh.QA dataset. When using the LLAMA3-8B-Instruct-262K model with InfLLM framework for inference, there are 12 pieces of data whose model response get the highest score after GPT-4 evaluation. Through the mandatory evidence injection operation, the model response for 28 pieces of data are considered valid in the GPT-4 evaluation.

SECTION: Appendix BExperimental Details

TableBpresents the specific hyperparameter configurations for various tested benchmarks. As highlighted in Section5, the prefilling last chunk sizesignificantly impacts the performance, thus we calibrateto appropriate values for different benchmarks. The attention map thresholdand Intersection over Union (IoU) thresholdfor the Non-Maximum Suppression (NMS) algorithm strike a balance semantic span discovery and noise inclusion, as discussed in AppendixD. To establish a more universal set ofandvalues on a larger synthetic dataset is left for future work. It’s recommended to refer to Section5for the meanings and effects of notations,and.

lcccccccc\CodeBefore\BodyBenchmark  MaskSpanNIAHrow  3  4  12  random search  random search  32-Bench/En.Sumrow  3  4  12  random search  random search  32-Bench/Zh.QArow  3  4  12  random search  random search  64-Bench/En.QArow  3  4  12  random search  random search  48-Bench/En.MCrow  3  4  12  random search  random search  256-Bench/En.Diarow  3  4  12  random search  random search  48-Bench/Code.Debugrow  3  4  12  random search  random search  256-Bench/Math.Findrow  3  4  12  random search  random search  32-Bench/Retr.PassKeyrow  3  4  12  random search  random search  32-Bench/Retr.Numberrow  3  4  12  random search  random search  32-Bench/Retr.KVrow  3  4  12  random search  random search  48RULER/niah_single_1row  3  6  240.01  48RULER/niah_single_2row  3  6  240.01  48RULER/niah_single_3row  3  6  240.01  40RULER/niah_multikey_1row  3  6  240.01  40RULER/niah_multikey_2row  3  6  240.01  40RULER/niah_multikey_3row  3  6  240.01  96RULER/niah_multivaluerow  3  6  240.01  40RULER/niah_multiqueryrow  3  6  240.01  80RULER/vtrow  3  6  240.01  64RULER/cwerow  3  6  240.01  36RULER/fwerow  3  6  240.01  48RULER/qa_1row  3  6  240.01  24RULER/qa_2row  3  6  240.01  36

The involved random search results for attention map thresholdand Intersection over Union (IoU) thresholdfor the Non-Maximum Suppression (NMS) operator from the torchvision as follows:

cccc\CodeBefore\BodyLayer-0Layer-1Layer-2Layer-30.9311282274784112  0.9080013962575452  0.9261057430192952  0.9039131198271712Layer-4Layer-5Layer-6Layer-70.930722988460956  0.8813112880282284  0.89366157731785  0.9287055121351976Layer-8Layer-9Layer-10Layer-110.9063453252976758  0.9177581400806004  0.9306567072342092  0.9276351205222796Layer-12Layer-13Layer-14Layer-150.9242215766864216  0.9213086907256328  0.916481440773853  0.924288489083868Layer-16Layer-17Layer-18Layer-190.9254682715409672  0.927788224471432  0.9131444756355042  0.9187994563658362Layer-20Layer-21Layer-22Layer-230.9098651028870453  0.9193856460842624  0.9049153144323192  0.9362935214356204Layer-24Layer-25Layer-26Layer-270.916154415186644  0.9062840706971326  0.9090799581259756  0.8846966017709177Layer-28Layer-29Layer-30Layer-310.9101323777522524  0.924574279333147  0.9095718506155552  0.9473738506727576

cccc\CodeBefore\BodyLayer-0Layer-1Layer-2Layer-30.032575607787946555  0.11662581959521869  0.024889873774503527  0.03588605266631541Layer-4Layer-5Layer-6Layer-70.18327915839536324  0.10736651162446896  0.1325381654743932  0.1517180627068569Layer-8Layer-9Layer-10Layer-110.183028676828816  0.2298031295153093  0.04026268971247067  0.17620642501895548Layer-12Layer-13Layer-14Layer-150.06616139460524989  0.07382263433361028  0.05470754299052264  0.1456038445898742Layer-16Layer-17Layer-18Layer-190.015583363677548107  0.019572076662935905  0.08034528015969278  0.13836900028647664Layer-20Layer-21Layer-22Layer-230.04670461556957088  0.02119316782149739  0.036045965083449205  0.04315987329920304Layer-24Layer-25Layer-26Layer-270.05940025252392963  0.06281379336040903  0.07035992447924916  0.23457309192037937Layer-28Layer-29Layer-30Layer-310.1578743191757248  0.23826246953092736  0.07079766552692811  0.89644860502407

SECTION: Appendix CAdopted Retrieval Heads

The adopted retrieval heads and corresponding score are listed in triple (layer id, head id, score):

(5, 8, 0.21), (8, 1, 0.49), (10, 14, 0.45), (13, 6, 0.15), (14, 18, 0.15), (15, 30, 0.90), (16, 1, 0.50), (17, 29, 0.12), (19, 3, 0.30), (20, 14, 0.44), (22, 14, 0.33), (24, 27, 0.46), (26, 15, 0.14), (27, 7, 0.30)

SECTION: Appendix DSupplementary Contents on Semantic Span Division and NMS Acceleration

In contrary to the for-loop implementation for semantic span division, utilizing the NMS operator from the torchvision library facilitates faster and steady semantic span generation, as shown in Figure20.

To illustrate the influence of Intersection over Union Threshold, we additionally visualize the semantic span division results under differentin Figure21, with the attention map thresholdfixed at. It’s observed that lowervalues result in more dispersed semantic spans, as depicted in Figure20a. Asincreases, the divided semantic spans overlap in nearby regions in Figure20d. In summary, lowervalues encourage semantic span discovery, increasing the likelihood of evidence retention and the risk of noise inclusion. Conversely, highervalues can boost algorithm confidence in semantic span division results, but potentially losing evidences.

We visualize the semantic span division results for every individual attention layer from one block containing the needle in Figure22and23. In most cases, our proposed strategy can find at lease one span overlapping with the ground truth needle position. What’s more, there are situations where exist a semantic span that overlaps with the needle accurately.

SECTION: Appendix ENMS Input Filtration for Efficient Span Selection

After Eq.5, each pointin the attention map is associated with a token span intervaland triangular cumulative score. The selected points are used to construct the corresponding box for Non-Maximum Suppression (NMS) calculations. To decrease the computational complexity of the NMS process, we can optionally limit the number of points involved in the calculation. Here, we opt to select the highest scoring points on each anti-diagonal as our filtering strategy. The approach linearly correlates the complexity of the NMS process to the number of tokens in the attention map, and makes the remaining points more likely to correspond to the midpoint of the semantic spans.

Letbe the token interval covering the evidence. Ideally, the cumulative triangular score for,,,,should show a trend of first increasing and then decreasing. The segment fromtois located in the same semantic span, so the score gradually increases; the segment fromtocontains tokens outside of the semantic span, under the operation of minus attention map threshold, the score of segments gradually decreases.

SECTION: Appendix FDetailed Comparison with MInference on RULER Benchmark

@cccccccc@\CodeBefore\rowcolorlightgray!202\rowcolorlightgray!204\rowcolorlightgray!206\rowcolorlightgray!209\rowcolorlightgray!2011\rowcolorlightgray!2013\BodySEQ_LENGTH  niah_single_1  niah_single_2  niah_single_3  niah_multikey_1  niah_multikey_2  niah_multikey_3  niah_multivalue4096  100.0/100.0(+0.0)  100.0/100.0(+0.0)  100.0/100.0(+0.0)  100.0/100.0(+0.0)  100.0/100.0(+0.0)  100.0/100.0(+0.0)  92.0/92.0(+0.0)8192  100.0/100.0(+0.0)  100.0/100.0(+0.0)  100.0/100.0(+0.0)  100.0/100.0(+0.0)  100.0/100.0(+0.0)  100.0/100.0(+0.0)  87.0/79.0(-8.0)16384  100.0/100.0(+0.0)  100.0/100.0(+0.0)  100.0/100.0(+0.0)  96.0/95.7(-0.3)  100.0/85.7(-14.3)  100.0/81.2(-18.8)  98.0/78.0(-20.0)32768  100.0/100.0(+0.0)  100.0/100.0(+0.0)  100.0/100.0(+0.0)  100.0/95.8(-4.2)  96.0/84.0(-12.0)  96.0/63.2(-32.8)  87.0/78.0(-9.0)65536  100.0/100.0(+0.0)  100.0/100.0(+0.0)  100.0/100.0(+0.0)  100.0/72.7(-27.3)  100.0/85.0(-15.0)  92.0/50.0(-42.0)  98.0/86.0(-12.0)131072  100.0/100.0(+0.0)  100.0/94.1(-5.9)  100.0/100.0(+0.0)  100.0/82.6(-17.4)  100.0/77.3(-22.7)  52.0/85.7(+33.7)  95.0/88.0(-7.0)SEQ_LENGTH  niah_multiquery  vt  cwe  fwe  qa_1  qa_2  AVG4096  100.0/100.0(+0.0)  100.0/100.0(+0.0)  93.2/92.8(-0.4)  93.3/93.3(+0.0)  72.0/72.0(+0.0)  56.0/56.0(+0.0)  92.8/92.8(-0.0)8192  100.0/98.0(-2.0)  93.6/75.2(-18.4)  73.2/73.2(+0.0)  84.0/82.7(-1.3)  56.0/56.0(+0.0)  40.0/36.0(-4.0)  87.2/84.6(-2.6)16384  100.0/93.0(-7.0)  95.2/72.0(-23.2)  44.4/47.6(+3.2)  89.3/93.3(+4.0)  72.0/64.0(-8.0)  40.0/40.0(+0.0)  87.3/80.8(-6.5)32768  100.0/87.0(-13.0)  90.4/66.4(-24.0)  4.0/24.0(+20.0)  84.0/88.0(+4.0)  64.0/40.0(-24.0)  48.0/28.0(-20.0)  82.3/73.4(-8.8)65536  99.0/95.0(-4.0)  88.0/69.6(-18.4)  0.8/2.8(+2.0)  86.7/85.3(-1.3)  68.0/48.0(-20.0)  44.0/16.0(-28.0)  82.8/70.0(-12.8)131072  98.0/97.0(-1.0)  81.6/62.4(-19.2)  0.8/1.6(+0.8)  74.7/74.7(+0.0)  56.0/36.0(-20.0)  44.0/32.0(-12.0)  77.1/71.6(-5.4)