SECTION: The Clever Hans Effect in Unsupervised Learning

Unsupervised learning is a subfield of machine learning that has gained prominence in recent years[DBLP:conf/nips/BrownMRSKDNSSAA20,DBLP:journals/pieee/RuffKVMSKDM21,Krishnan2022]. It addresses fundamental limitations of supervised learning, such as the lack of labels in the data or the high cost of acquiring them. Unsupervised learning has achieved successes in modeling the unknown, such as uncovering new cancer subtypes[Li2009,Jiang2019]or extracting novel insights from large historical corpora[eberle-dh]. Furthermore, the fact that unsupervised learning does not rely on task-specific labels makes it a good candidate for core AI infrastructure: Unsupervised anomaly detection provides the basis for various quality or integrity checks on the input data[DBLP:books/sp/19/RettigKCP19,DBLP:books/sp/02/EskinAPPS02,DBLP:journals/ijcv/BergmannBFSS21,DBLP:journals/candie/ZipfelVFWKZ23]. Unsupervised learning is also a key technology behind ‘foundation models’[Bommasani2021,DBLP:conf/nips/BrownMRSKDNSSAA20,simclr-v2,DBLP:conf/icml/RadfordKHRGASAM21,Moor2023,dippel2024foundation]which extract representations upon which various downstream models (e.g. classification, regression, ‘generative AI’, etc.) can be built. The growing popularity of unsupervised learning models creates an urgent need to carefully examine how they arrive at their predictions. This is essential to ensure that potential flaws in the way these models process and represent the input data are not propagated to the many downstream supervised models that build upon them.

In this paper, we show for the first time that unsupervised learning models largely suffer from Clever Hans (CH) effects[Lapuschkin2019], also known as “right for the wrong reasons”. Specifically, we find that unsupervised learning models often produce representations from which instances can be correctly predicted to be e.g. similar or anomalous, although largely supported by data quality artifacts. The flawed prediction strategy is not detectable by common evaluation benchmarks such as cross-validation, but may manifest itself much later in ‘downstream’ applications in the form of unexpected errors, e.g. if subtle changes in the input data occur after deployment (cf. Fig.1). While CH effects have been studied quite extensively forsupervised learning[Lapuschkin2019,geirhos2020shortcut,DBLP:journals/inffus/AndersWNSML22,linhardt2023preemptively,Winkler2019], the lack of similar studies in the context ofunsupervised learning, together with the fact that unsupervised models supply many downstream applications, is a cause for concern.

For example, in industrial inspection, which often relies on unsupervised anomaly detection[DBLP:journals/ijcv/BergmannBFSS21,DBLP:journals/candie/ZipfelVFWKZ23], we find that a CH decision strategy can systematically miss a wide range of manufacturing defects, resulting in potentially significant costs. As another example, unsupervised foundation models, advocated in the medical domain to provide robust features for various specialized diagnostic tasks, can potentially introduce CH effects into many of these tasks, with the prominent risk of large-scale misdiagnosis. These scenarios (illustrated in Fig.1) highlight the practical implications of an unsupervised CH effect, which, unlike its supervised counterpart, may not be limited to malfunctioning in a single specific task, but potentially in all downstream tasks.

To uncover and understand unsupervised Clever Hans effects, we propose to use Explainable AI[montavon2018methods,gunning2019xai,arrieta2020xai,samek2021explaining,klauschen2023toward](here techniques that build on the LRP explanation framework[bach-plos15,DBLP:series/lncs/MontavonBLSM19,DBLP:journals/pr/KauffmannMM20]). Our proposed use of these techniques allows us to identify at scale which input features are used (or misused) by the unsupervised ML model, without having to formulate specific hypotheses or downstream tasks. Specifically, we use an extension of LRP called BiLRP[eberle-dh]to reveal input patterns that are jointly responsible for similarity in the representation space. We also combine LRP with ‘virtual layers’[vielhaben23,chormai24]to reveal pixel and frequency components that are jointly responsible for predicted anomalies.

Furthermore, our Explainable AI-based analysis allows us to pinpoint more formal causes for the emergence of unsupervised CH effects. In particular, they are due not so much to thedata, but to theunsupervised learning machine, which hinders the integration of the true task-supporting features into the model, even though vast amounts of data points are available. Our findings provide a novel direction for developing targeted strategies to mitigate CH effects and increase model robustness.

Overall, our work sheds light on the presence, prominence, and distinctiveness of CH effects in unsupervised learning, calling for increased scrutiny of this essential component of modern AI systems.

SECTION: 1Results

Using Explainable AI, we investigate the emergence of Clever Hans effects in a representative set of unsupervised learning tasks, including representation learning and anomaly detection.

SECTION: 1.1Clever Hans Effects in Representation Learning

We first consider an application of representation learning in the context of detectingCOVID-19cases from X-ray scans[Wang2020,DeGrave2021]. Simulating an early pandemic phase characterized by data scarcity, similar to[DeGrave2021], we use a dataset aggregation approach where a large, well-established non-COVID-19 dataset is merged with a more recent COVID-19 dataset aggregated from multiple sources. Specifically, we aggregate 2597 instances of the CXR8 dataset[Wang2017]from the National Institute of Health (NIH), collected between 1992 and 2015, with the 535 instances of the GitHub-hosted ‘COVID-19 image data collection’[cohen2020covidProspective]. We refer to these subsets as ‘NIH’ and ‘GitHub’, respectively.

Further motivated by the need to accommodate the critically small number of COVID-19 instances in the aggregated dataset and to avoid overfitting, we choose to rely on the representations provided by unsupervised foundation models[DBLP:conf/iccv/AziziMRBFDLKKCN21,DBLP:conf/eacl/EslamiMM23,Huang2023,dippel2024foundation]. Specifically, we feed our data into a pre-trained PubMedCLIP model[DBLP:conf/eacl/EslamiMM23], which has built its representation from a very large collection of X-ray scans in an unsupervised manner. Based on the PubMedCLIP representation, we train a downstream classifier that separates COVID-19 from non-COVID-19 instances with a class-balanced accuracy of 88.6% on the test set (cf. Table1). However, a closer look at the structure of this performance score reveals a strong disparity between the NIH and GitHub subgroups, with all NIH instances being correctly classified and the GitHub instances having a lower class-balanced accuracy of 75.2%, and, more strikingly, a false positive rate (FPR) of 40%, as shown in Table1. Considering that the higher heterogeneity of instances in the GitHub dataset is more characteristic of real-world conditions, such a high FPR prohibits any practical use of the model as a diagnostic tool. We emphasize that this deficiency could have been easily overlooked if one did not pay close attention to (or did not know) the data sources, and instead relied only on the overall accuracy score.

To proactively detect this heterogeneous, non-robust prediction behavior and to test whether it originates from the unsupervised PubMedCLIP component, we propose to use Explainable AI. Using the BiLRP[eberle-dh]technique, we investigate whether PubMedCLIP represents different instances as similar based on a correct strategy or a ‘Clever Hans’ strategy. The BiLRP method is illustrated in Fig.2and its mathematical formulation is given in the Methods section. The output of BiLRP for two exemplary pairs of COVID-positive instances is shown in Fig.3(left). The BiLRP explanation shows that the modeled similarity between pairs of COVID-positive instances comes from text-like annotations that appear in both images. This is a clear case of a CH effect, where the modeled similarity isright(instances share being COVID-positive), but for thewrong reasons(similarity is based on shared textual annotations). The unmasking of this CH effect by BiLRP warns of the risk that downstream models based on these similarities (and more generally on the flawed representation) may inherit the CH effect. This a posteriori explains the excessively high (40%) FPR of the downstream classifier on the Github subset, as the model’s reliance on text makes it difficult to separate Github’s COVID-positive from COVID-negative instances (cf. Supplementary Note A for further analysis). We note that, unlike the per-group accuracy analysis above, our BiLRP analysis did not require provenance metadata (GitHub or NIH), nor did it focus on a specific downstream task with its specific labels.

To test whether representation learning has a general tendency to evolve CH strategies beyond the above use case, we downloaded three generic foundation models, namely the original CLIP model[DBLP:conf/icml/RadfordKHRGASAM21], SimCLR[simclr-v1,simclr-v2]and BarlowTwins[DBLP:conf/icml/ZbontarJMLD21]. As a downstream task, we consider the classification, using linear-softmax classifiers, of the 8 classes from ImageNet[DBLP:conf/cvpr/DengDSLL009]that share the WordNet ID ‘truck’ and of the 16 ImageNet classes that share the WordNet ID ‘fish’ (see the Methods section for details). The test accuracy of each model on these two tasks is given in Table1(columns ‘original’). On the truck classification task, the CLIP model performs best, with an accuracy of 85.0%. On the fish classification task, the CLIP and supervised models perform best, with accuracies of 86.5% and 86.2%, respectively.

We use BiLRP to examine the representations of these unsupervised models. In Fig.3(center), we observe that CLIP-based similarities, as in PubMedCLIP, also rely on text. Here, a textual logo in the lower left corner of two garbage truck images is used to support similarity111The reliance on logos for the garbage truck class has been highlighted in previous work in the context of supervised learning[DBLP:journals/inffus/AndersWNSML22].. In contrast, SimCLR and BarlowTwins instead rely on the actual garbage truck, demonstrating that CH effects are specific to the unsupervised learning technique. In Fig.3(right), we observe thatallunsupervised models focus on humans and are unresponsive to fish features. CLIP focuses specifically on facial features. While the detection of human features may be useful for many downstream tasks, the suppression of fish features strongly exposes other downstream fish classification tasks to even mild spurious correlations, systematically leading to CH behavior.

The heterogeneity of strategies across models revealed by BiLRP suggests that, in addition to inhomogeneities in the unsupervised data, biases induced by the unsupervised pre-training algorithm222See also[chen-feature-sup,robinson-shortcuts,dippel2021towards,li2023addressing]for further analyses of these biases.play an important role in the formation of CH effects. The spurious amplification of humans in the center of the image observed for SimCLR and BarlowTwins can be attributed to the cropping mechanism these methods implement, where human features in the center are sufficient and robust to solve the SimCLR and BarlowTwins similarity tasks. In the case of CLIP (and PubMedCLIP), the systematic amplification of textual, facial, or other identifying features can be explained by the fact that these features often provide useful information for CLIP’s unsupervised image-text matching task.

The consequences of these Clever Hans effects are shownquantitativelyin Table1. We observe a systematic degradation of performance when moving from the original data to data subsets where the presence/absence of an artifact is no longer predictive of the class. In particular, for the truck data, we observe a sharp drop in the accuracy of the CLIP model from 85.0% on the original data to 80.5% on the same data with a logo inserted on each image (column ‘logo’ in Table1). For the fish data, a similar drop in accuracy is observed for SimCLR and BarlowTwins from 82.2% and 83.1% respectively on the original data to 78.6% and 75.6% respectively when only images containing humans are retained and class rebalancing is performed. This shows that the CH effects detected by our BiLRP analysis have concrete negative practical consequences in terms of the ability of unsupervised models and downstream classifiers to predict uniformly well across subgroups and to generalize. In comparison, the baseline supervised model generally shows more stable performance between the original data and the newly defined subsets. A detailed analysis of the structure of the prediction errors for each classification task, supported by confusion matrices, is given in Supplementary Note B.

SECTION: 1.2Clever Hans Effects in Anomaly Detection

Extending our investigation of the CH effect to another instance of unsupervised learning, namely anomaly detection, we consider an industrial inspection use case based on the popular MVTec-AD dataset[DBLP:journals/ijcv/BergmannBFSS21]. The dataset consists of 15 product categories, each consisting of a training set of images without manufacturing defects and a test set of images with and without defects. Since manufacturing defects are infrequent and heterogeneous in nature, the problem is typically approached using unsupervised anomaly detection[DBLP:journals/ijcv/BergmannBFSS21,DBLP:journals/pieee/RuffKVMSKDM21]. These models map each instance to an anomaly score, from which threshold-based downstream models can be built to classify between instances with and without manufacturing defects. Unsupervised anomaly detection has received considerable attention, with sophisticated approaches based on deep neural networks such as PatchCore[DBLP:conf/cvpr/RothPZSBG22]or EfficientAD[DBLP:conf/wacv/BatznerHK24], showing excellent performance in detecting a wide range of industrial defects.

Somewhat surprisingly, simpler approaches based on distances in pixel space show comparatively high performance for selected tasks[DBLP:journals/pieee/RuffKVMSKDM21]. We consider one such approach, which we call ‘D2Neighbors’, where anomalies are predicted according to the distance to neighbors in the training data. Specifically, the anomaly score of an instanceis computed aswhereis the set of available inlier instances (see the Methods section for details on the model and data preprocessing). This anomaly model belongs to the broader class of distance-based models[HARMELING20061608,DBLP:books/sp/Aggarwal2013,DBLP:journals/csur/ChandolaBK09], and connections can be made to kernel density estimation[Parzen1962,DBLP:journals/jmlr/KimS12]and one-class SVMs[DBLP:journals/neco/ScholkopfPSSW01]. Using D2Neighbors, we are able to build downstream models that classify industrial defects of the MVTec data with F1-scores above 0.9 for five categories (bottle, capsule, pill, toothbrush, and wood).

To shed light on the prediction strategy that leads to these unexpectedly high F1-scores, we investigate the underlying D2Neighbors anomaly model using Explainable AI. Specifically, we consider an extension of LRP for anomaly detection[DBLP:journals/pr/KauffmannMM20,Montavon2022]and further equip the explanation technique with ‘virtual layers’[vielhaben23,chormai24]. The technique of ‘virtual layers’ (cf. Fig.4) is to map the input to an abstract domain and back, leaving the prediction functionunchanged, but providing a new representation in terms of which the prediction can be explained. We construct such a layer by applying the discrete cosine transform (DCT)[10.3389/fnbot.2021.767299], shown in Fig.4(right), followed by its inverse. This allows us to explain the predictionsjointlyin terms of pixels and frequencies.

The result of our proposed analysis is shown in Fig.5for instances from two of the five retained MVTec categories (see Supplementary Note C for more instances). Explanations at thepixellevel show that D2Neighbors supports its anomaly predictions largely based on pixels containing the actual industrial defect. The squared difference in its distance function () encourages a sparse pixel-wise response of the model, efficiently discarding regions of the image where the new instance shows no difference from instances in the training data. However, we also see in the pixel-wise explanation that a non-negligible part of the anomaly prediction comes from irrelevant background pixels. Jointpixel-frequencyexplanations shed light on these unresolved contributions, showing that they arise mostly from the high-frequency part of the model’s decision strategy (Fig.5B and C).

The exposure of the model to these irrelevant high-frequency features, as detected by our LRP analysis, raises concerns about the robustness of the model under changing data conditions (e.g. after the model is deployed). We experiment with an innocuous perturbation of the data conditions, which consists of changing the image resizing algorithm from OpenCV’s nearest neighbor resizing to a more sophisticated resizing method that includes antialiasing. Resizing techniques have been shown in some cases to significantly affect image quality and ML model prediction[DBLP:conf/cvpr/Parmar0Z22], but their effect on ML models, especially unsupervised ones, has been little studied. The performance of the D2Neighbors model and other models before and after changing the resizing algorithm is shown in Table1(columns ‘original’ and ‘deployed’ respectively). The F1 score performance of D2Neighbors degrades significantly fromto. In particular, there is a large increase in the false negative rate (FNR) fromto(cf. Table1). In an industrial inspection setting, an increase in FNR has serious consequences, in particular, many defective instances are missed and propagated through the production chain, resulting in wasted resources in subsequent production stages and high recall costs. This performance degradation of D2Neighbors in post-deployment conditions is particularly surprising given that data quality has actuallyimproved. This is a direct consequence of the CH strategy we identified, namely D2Neighbors’ reliance on high frequencies. When antialiasing is introduced into the resizing procedure, the high frequencies that the D2Neighbors model uses to support its prediction disappear from the data, and this significantly reduces the anomaly scores of each instance, thereby increasing the FNR.

Interestingly, the performance degradation of D2Neighbors is even more severe when the-norm in the distance function is replaced by the-norm (cf. Table1). This can be explained by the fact that the-norm, unlike the-norm, does not implement pixel-wise sparsity and is therefore more exposed to irrelevant input regions. Conversely, the application of an-norm results in sensibly higher robustness than the original-norm. We note that even the more sophisticated PatchCore model[DBLP:conf/cvpr/RothPZSBG22](cf. Methods section), which integrates several robustness mechanisms including preprocessing layers and spatial max-pooling, is not fully immune to irrelevant high-frequency components and also shows a noticeable performance degradation in post-deployment conditions (cf. Table1).

Overall, our experiments show that performance on the available test data is an insufficient indicator of the quality of an anomaly detector. Its robustness to changes in the quality of the input data cannot be guaranteed. This can be diagnosed by Explainable AI and a careful subsequent inspection of the structure of the anomaly model.

SECTION: 1.3Alleviating CH in Unsupervised Learning

Leveraging the Explainable AI analysis above, we aim to build models that are more robust across different data subgroups and in post-deployment conditions. Unlike previously proposed CH removal techniques[DBLP:journals/inffus/AndersWNSML22,linhardt2023preemptively], we aim to operate on the unsupervised model rather than the downstream tasks. This potentially allows us to achieve broad robustness improvements while leaving the downstream learning machines (training supervised classifiers or adjusting detection thresholds) untouched.

We first test our CH mitigation approach on the CLIP model, which our Explainable AI analysis has shown to incorrectly rely on text. Specifically, we focus on an early layer of the CLIP model (encoder.relu3) and remove the feature maps that are most responsive to text. We then apply a similar CH mitigation approach to anomaly detection, which our Explainable AI analysis has shown to be overexposed to high frequencies. Here, we propose to prune the high frequencies by inserting a blur layer at the input of the model.

In both cases, the proposed CH mitigation technique provides strong benefits in terms of model robustness. As shown in Table1, rows ‘CH mitigation’, our robustified models substantially reverse the performance degradation observed in post-deployment conditions, reaching performance levels close to, and in some cases superior to, those measured on the original data.

SECTION: 2Discussion

Unsupervised learning is an essential category of ML that is increasingly used in core AI infrastructure to power a variety of downstream tasks, including generative AI. Much research to date has focused on improving the performance of unsupervised learning algorithms, for example, to maximize accuracy scores in downstream classification tasks. These evaluations often pay little attention to the exact strategy used by the unsupervised model to achieve the reported high performance, in particular whether these models rely on Clever Hans strategies.

Building on recent techniques from Explainable AI, we have shown for the first time that CH strategies are prevalent in several unsupervised learning paradigms. These strategies can take various forms, such as correctly predicting the similarity of two X-ray scans based on irrelevant shared annotations, or predicting that an image is anomalous based on small but widespread pixel-level artifacts. These flawed prediction strategies result in models that do not transfer well to changing conditions at test time. Most importantly, their lack of robustness infects many downstream models that rely on them. As we have shown in two use cases, this can lead to widespread misdiagnosis of patients or failure to detect manufacturing defects.

Therefore, addressing CH effects is a critical step towards reliable use of unsupervised learning methods. However, compared to a purely task-specific supervised approach, the addition of an unsupervised component, potentially serving multiple downstream tasks, adds another dimension of complexity to the modeling problem. In particular, one must decide whether to handle CH effects in the downstream classifier ordirectlyin the unsupervised model part. Approaches consisting of dynamically updating downstream models in response to changing conditions[sugiyama2007covariate,sugiyama2012machine,DBLP:conf/nips/IwasawaM21,Esposito2021], or revising their decision strategies with human feedback[DBLP:journals/inffus/AndersWNSML22,DBLP:conf/iclr/KirichenkoIW23,linhardt2023preemptively]are possible solutions to maintain high accuracy. However, these approaches may not be sustainable because CH mitigation must be performed repeatedly for each new downstream task. The problem may persist even after a flaw in the foundation model becomes known (e.g.[DBLP:conf/acl/NivenK19,Heinzerling2020NLPsCH]), due to release intervals and the high cost of retraining these models. Instead, our results argue for addressing CH effects directly when building the unsupervised model, with the goal of achieving persistent robustness that benefits existing and future downstream applications.

Using advanced Explainable AI techniques targeted at unsupervised learning, we have uncovered multiple and diverse CH effects, such as the reliance of CLIP (and its derivative PubMedCLIP) on textual annotations, or a systemic over-exposure of unsupervised anomaly models to noise. Interestingly, our analysis has revealed that these unsupervised CH effects differ from supervised ones in that they arise less from the data and more from inductive biases in the model and learning algorithm. These include spurious suppression/amplification effects caused by the representation learning’s training objective, or a failure of unsupervised anomaly detection architectures to replicate frequency filtering mechanisms found in supervised learning[DBLP:journals/jmlr/BraunBM08,DBLP:conf/icml/BasriGGJKK20,DBLP:conf/nips/Fridovich-KeilL22], leaving the learned models highly exposed to noise. Furthermore, our Explainable AI analysis not only provided new insights into the formal causes of unstable behavior in unsupervised learning. Our experiments also showed how pruning the high-frequency or spuriously amplified features revealed by our Explainable AI analysis leads to systematic performance improvements on difficult data subgroups or in post-deployment conditions. In doing so, we have demonstrated the actionability of our Explainable AI approach, showing that it can guide the process of identifying and subsequently correcting the faulty components of an unsupervised learning model.

While our investigation of unsupervised CH effects and their consequences has focused on image data, extension to other data modalities seems straightforward. Explainable AI techniques such as LRP, which are capable of accurate dataset-wide explanations, operate independently of the type of input data. They have recently been extended to recurrent neural networks[DBLP:series/lncs/ArrasAWMGMHS19], graph neural networks[schnake2022higher], transformers[DBLP:conf/icml/AliSEMMW22], and state space models[jafari2024mambalrp], which represent the state of the art for large language models and other models of structured data. Thus, our analysis could be extended to analyze other instances of unsupervised learning, such as anomaly detection in time series or the representations learned by large language models (e.g.[DBLP:journals/access/MunirSDA19,DBLP:conf/naacl/DevlinCLT19]).

Overall, we believe that the CH effect in unsupervised learning, and the uncontrolled risks associated with it, is a question of general importance, and that Explainable AI and its recent developments provide an effective way to tackle it.

SECTION: 3Methods

This section first introduces the unsupervised ML models studied in this work and the datasets on which they are applied. It then presents the layer-wise relevance propagation (LRP) method for explaining predictions, its BiLRP extension for explaining similarity, and the technique of ‘virtual layers’ for generating joint pixel-frequency explanations.

SECTION: 3.1ML Models and Data for Representation Learning

Representation learning experiments were performed on the SimCLR[simclr-v1,simclr-v2], CLIP[DBLP:conf/icml/RadfordKHRGASAM21], BarlowTwins[DBLP:conf/icml/ZbontarJMLD21]and PubMedCLIP[DBLP:conf/eacl/EslamiMM23]models. All include a version based on the ResNet50 architecture[he-resnet]and come with pre-trained weights. SimCLR augments the input images with resized crops, color jitter, and Gaussian blur to create two different views of the same image. These views are then used to create positive and negative pairs, where the positive pairs represent the same image from two different perspectives, and the negative pairs are created by pairing different images. The contrastive loss objective maximizes the similarity between the representations of the positive pairs while minimizing the similarity between the representations of the negative pairs. Barlow Twins is similar to SimCLR in that it also generates augmented views of the input image through random resized crops and color augmentation, and maximizes their cosine similarity in representation space. However, it differs from SimCLR in the exact mechanisms used to prevent representation collapse. In our experiments, we use the weights from the vissl333https://vissl.ai/library. CLIP (and its derivative PubMedCLIP) learns representations by using a large collection of image-text pairs from the Internet. Images are given to an image encoder, and the corresponding texts are given to a text encoder. The similarity of the two resulting embeddings is then maximized with a contrastive loss. In our experiments, we use the ResNet-50 weights from OpenAI444https://github.com/openai/CLIPfor CLIP. PubMedCLIP555https://github.com/sarahESL/PubMedCLIPis based on a pre-trained CLIP model fine-tuned on the ROCO dataset[roco-dataset], a collection of radiology and image caption pairs. For all representation learning experiments, thesupervised baselinesshare the same architecture as their unsupervised counterparts, but are trained in a purely supervised fashion using backpropagation.

The analysis and training of these models were performed on different datasets. The ImageNet experiments were performed on two ImageNet subsets. First, the ‘truck’ subset consisting of the 8 classes sharing the WordNetID ‘truck’ (minivan, moving van, police van, fire engine, garbage truck, pickup, tow truck and trailer truck). Then the ‘fish’ subset consisting of the 16 classes sharing the WordNetID ‘fish’ (tench, barracouta, coho, sturgeon, gar, stingray, great white shark, hammerhead, tiger shark, puffer, electric ray, goldfish, eel, anemone fish, rock beauty and lionfish). For the X-ray experiments, we combined the NIH ChestX-ray8 (CXR8) dataset666https://academictorrents.com/details/e615d3aebce373f1dc8bd9d11064da55bdadede0and the GitHub-hosted ‘COVID-19 image data collection’777https://github.com/ieee8023/covid-chestxray-dataset. The NIH dataset contributed 2597 negative images, while the GitHub dataset contained 342 COVID-positive and 193 negative images. All images were resized to 224x224 pixels and center-cropped. We split the patients 80:20 into training and test sets based on unique patient IDs. This resulted in 272 positive and 168 negative images in the training set from the GitHub dataset. To approximate an i.i.d. distribution in the training set, we added 2552 negative images from the NIH dataset, resulting in a total of 2992 training images (272 positive, 2720 negative). The test set consisted of 70 positive and 70 negative images, with 45 negative images from the NIH dataset and 25 from the GitHub dataset. This resulted in 2552 NIH images and 440 GitHub images in the training set, and 45 NIH images and 95 GitHub images in the test set.

SECTION: 3.2ML Models and Data for Anomaly Detection

TheD2Neighborsmodel used in our experiments is an instance of the family of distance-based anomaly detectors, which encompasses a variety of methods from the literature[HARMELING20061608,DBLP:books/sp/Aggarwal2013,DBLP:journals/csur/ChandolaBK09,DBLP:journals/pieee/RuffKVMSKDM21,DBLP:journals/csur/PangSCH21,DBLP:conf/icpr/RippelMM20]. The D2Neighbors model computes anomaly scores aswhereis the input,are the training data, andis a generalized-mean, with function. The function can be interpreted as a soft minimum over distances to data points, i.e. a distance to the nearest neighbors. In our experiments, the data received as input are RGB images of sizewith pixel values encoded betweenand, downsized from their original high resolution using OpenCV’s fast nearest neighbor interpolation. We setso that the average perplexity[10.1121/1.2016299]equals 25% of the training set size for each model.

We also consider the PatchCore[DBLP:conf/cvpr/RothPZSBG22]anomaly detection model, which uses mid-level patch features from a fixed pre-trained network. It constructs a memory bank of these features from nominal example images during training. Anomaly scores for test images are computed by finding the maximum distance between each test patch feature and its nearest neighbor in the memory bank. Distances are computed between patch featuresand a memory bank of location independent prototypes. The overall outlier scoring function of PatchCore can be written as. The functionis the feature representation aggregated from two consecutive layers at spatial patch location, extracted from a pre-trained WideResNet50. The features from consecutive layers are aggregated by rescaling and concatenating the feature maps. The difference between our reported F1 scores and those in[DBLP:conf/cvpr/RothPZSBG22]is mainly due to the method used to resize the images. We used the authors’ reference implementation888https://github.com/amazon-science/patchcore-inspectionas the basis for our experiments.

All of the above models were trained on the MVTec dataset. The MVTec dataset consists of 15 image categories (‘bottle’, ‘cable’, ‘capsule‘, ’carpet’, ‘grid’, ‘hazelnut’, ‘leather’, ‘metal nut’, ‘pill’, ‘screw’, ‘tile’, ‘toothbrush’, ‘transistor’, ‘wood’ and ‘zipper’) of industrial objects and textures, with good and defective instances for each category. For the experiments based on D2Neighbors, we simulated different data preprocessing conditions before and after deployment by changing the way images are resized from their original high resolution topixels. We first use a resizing algorithm found in OpenCV 4.9.0[opencv_library]based on nearest neighbor interpolation. We then simulate post-deployment conditions using an improved resizing method, specifically a bilinear interpolation implemented in Pillow 10.3.0 and used by default in torchvision 0.17.2[torchvision2016]. This improved resizing method includes antialiasing, which has the effect of smoothing the transitions between adjacent pixels of the resized image.

SECTION: 3.3Explanations for Representation Learning

Our experiments examined dot product similarities in representation space, i.e., wheredenotes the function that maps the input features to the representation, typically a deep neural network. To understand similarity scores in terms of input features, we used the BiLRP method[DBLP:journals/pami/EberleBKMVM22]which extends the LRP technique[bach-plos15,DBLP:series/lncs/MontavonBLSM19,samek2019explainable,samek2021explaining]for this specific purpose. The conceptual starting point of BiLRP is the observation that a dot product is a bilinear function of its input. BiLRP then proceeds by reverse propagating the terms of the bilinear function to pairs of activations from the layer below and iterating down to the input. Denotingthe contribution of neuronsandto the similarity score in some intermediate layer in the network, BiLRP extracts the contributions of pairs of neuronsandin the layer below via the propagation rule:

In practice, this reverse propagation procedure can be implemented equivalently, but more efficiently and easily, by computing a collection of standard LRP explanations (one for each neuron in the representation layer) and recombining them in a multiplicative manner:

Overall, assuming the input consists offeatures, BiLRP produces an explanation of sizewhich is typically represented as a weighted bipartite graph between the set of features of the two input images. Due to the large number of terms, pixel-to-pixel contributions are aggregated into patch-to-patch contributions, and elements of the BiLRP explanations that are close to zero are omitted in the final explanation rendering. In our experiments, we computed BiLRP explanations using the Zennit999https://github.com/chr5tphr/zennitimplementation of LRP which handles the ResNet50 architecture, and set Zennit’s LRP parameters to their default values.

SECTION: 3.4Explanations for the D2Neighbors Model

The D2Neighbors model we investigate for anomaly detection is a composition of a distance layer and a soft min-pooling layer. To handle these layers, we use the purposely designed LRP rules of[DBLP:journals/pr/KauffmannMM20,Montavon2022]. Propagation in the softmin layer () is given by the formula

a ‘min-take-most’ redistribution, whereis the same function as in. Each scorecan be interpreted as the contribution of the training pointto the anomaly of. To further propagate these scores into the pixel frequency domain, we adopt the framework of ‘virtual layers’[vielhaben23,chormai24]and adapt it to the D2Neighbors model. As a frequency basis, we use the discrete cosine transform (DCT)[10.3389/fnbot.2021.767299], shown in Fig.4(right), which we denote by its collection of basis elements. Since the DCT forms an orthogonal basis, we have the property, and multiplication by the identity matrix can be interpreted as a mapping to the frequencies and back. For the special case where, the distance terms in D2Neighbors reduce to the squared Euclidean norm. These terms can be developed to identify pixel-pixel-frequency interactions:. From there, one can construct an LRP rule that propagates the instance-wise relevanceto the pixel-frequency features:

where the variableis a small positive term that handles the case whereandoverlap. A reduction of this propagation rule can be obtained by marginalizing over interacting pixels (). Further reductions can be obtained by marginalizing over pixels () or frequencies (). These reductions are used to generate the heatmaps in Fig.5.

SECTION: 3.5Data Availability

All data used in this paper, in particular, NIH’s CXR8[Wang2017], GitHub-COVID[cohen2020covidProspective], ImageNet[DBLP:conf/cvpr/DengDSLL009], and MVTec-AD[DBLP:conf/cvpr/BergmannFSS19]are publicly available.

SECTION: 3.6Code Availability

Full code for reproduction of our results is available athttps://git.tu-berlin.de/jackmcrider/the-clever-hans-effect-in-unsupervised-learning.

SECTION: 3.7Acknowledgements

This work was partly funded by the German Ministry for Education and Research (under refs 01IS14013A-E, 01GQ1115, 01GQ0850, 01IS18056A, 01IS18025A and 01IS18037A), the German Research Foundation (DFG) as Math+: Berlin Mathematics Research Center (EXC 2046/1, project-ID: 390685689) and the DeSBi Research Unit (KI-FOR 5363, project ID: 459422098), and DFG KI-FOR 5363. Furthermore, KRM was partly supported by the Institute of Information & Communications Technology Planning & Evaluation (IITP) grants funded by the Korea government (MSIT) (No. 2019-0-00079, Artificial Intelligence Graduate School Program, Korea University and No. 2022-0-00984, Development of Artificial Intelligence Technology for Personalized Plug-and-Play Explanation and Verification of Explanation). We thank Stefan Ganscha for the valuable comments on the manuscript. Correspondence to KRM and GM.