SECTION: Customize Segment Anything Model for Multi-Modal Semantic Segmentationwith Mixture of LoRA Experts

The recent Segment Anything Model (SAM) represents a significant breakthrough in scaling segmentation models, delivering strong performance across various downstream applications in the RGB modality. However, directly applying SAM to emerging visual modalities, such as depth and event data results in suboptimal performance in multi-modal segmentation tasks. In this paper, we make the first attempt to adapt SAM for multi-modal semantic segmentation by proposing a Mixture of Low-Rank Adaptation Experts (MoE-LoRA) tailored for different input visual modalities. By training only the MoE-LoRA layers while keeping SAM’s weights frozen, SAM’s strong generalization and segmentation capabilities can be preserved for downstream tasks. Specifically, to address cross-modal inconsistencies, we propose a novel MoE routing strategy that adaptively generates weighted features across modalities, enhancing multi-modal feature integration. Additionally, we incorporate multi-scale feature extraction and fusion by adapting SAM’s segmentation head and introducing an auxiliary segmentation head to combine multi-scale features for improved segmentation performance effectively.
Extensive experiments were conducted on three multi-modal benchmarks: DELIVER, MUSES, and MCubeS. The results consistently demonstrate that the proposed method significantly outperforms state-of-the-art approaches across diverse scenarios. Notably, under the particularly challenging condition of missing modalities, our approach exhibits a substantial performance gain, achieving an improvement of 32.15% compared to existing methods.

SECTION: IIntroduction

Accurate segmentation of diverse objects is pivotal for various scene understanding applications, including robotic perception, autonomous driving, and AR/VR[1,2]. The Segment Anything Model (SAM)[3]represents a groundbreaking advancement in instance segmentation, particularly for RGB images. Trained on an extensive dataset of 11 million high-resolution images and over 1 billion annotated segmentation masks, SAM achieves exceptional zero-shot segmentation performance, enabling its application across diverse domains such as medical imaging, remote sensing, and more[4,5,6,7].

While SAM has revolutionized single-modality segmentation tasks, particularly for RGB images, its application to multi-modal segmentation presents unique challenges. Emerging domains often require integrating diverse modalities such as depth and event data, which capture complementary scene information but exhibit distinct characteristics from RGB data. Furthermore, the recently proposed SAM2 model[8]incorporates temporal dimensions for video segmentation, addressing additional complexities such as motion, deformation, occlusion, and lighting variations. These advancements extend SAM’s applicability to dynamic and multi-modal environments, but integrating cross-modal information while preserving SAM’s generalization capabilities remains under-explored.

Despite its success in single-modality segmentation, extending SAM to multi-modal semantic segmentation poses significant challenges. Each modality, such as LiDAR, radar, and event cameras, exhibits distinct spatial, temporal, and noise characteristics, complicating their seamless integration into SAM’s architecture[9]. SAM’s pre-trained features, optimized for RGB images, often result in suboptimal performance when directly applied to heterogeneous multi-modal data. Real-world scenarios further complicate this integration, as missing or unreliable modalities can degrade performance, and SAM lacks mechanisms to adaptively handle incomplete inputs[10,11,12]. Additionally, effective multi-modal fusion requires advanced techniques to align, weigh, and integrate inputs while preserving the complementary strengths of each modality. Achieving robust fusion requires addressing several challenges, including mitigating modality-specific noise, harmonizing discrepancies in spatial and temporal resolutions, and balancing the contributions of each input modality[13].

In this work, we present a novel framework that extends SAM2’s functionality to support multi-modal semantic segmentation. As shown in Figure1(a), our approach incorporates Low-Rank Adaptation (LoRA) modules designed for each modality, facilitating efficient modality-specific fine-tuning while preserving the generalization capabilities of SAM2’s pre-trained image encoder. To address the inherent challenges of multi-modal fusion, we develop a Mixture of LoRA Experts (MLE) routing mechanism that adaptively generates weighted feature representations, ensuring effective integration across modalities and mitigating inconsistencies caused by noise or missing inputs.
Meanwhile, we enhance the SAM2 segmentation pipeline by incorporating multi-scale feature extraction and fusion mechanisms. Specifically, we augment the original segmentation head with an auxiliary head designed to exploit complementary information across multiple scales, leading to improved segmentation accuracy.

Extensive experiments conducted on benchmark datasets, including DELIVER[13], MUSES[10], and MCubeS[14], demonstrate the superior performance of our framework in multi-modal semantic segmentation tasks. As illustrated in Figure1(b) and (c), our approach achieves a significant improvement of +4.9% on the DELIVER dataset with four modalities and +28.14% on the MUSES dataset with three modalities, compared to state-of-the-art methods.
Detailed ablation studies confirm the individual contributions of each module to the overall performance. Furthermore, additional experiments under challenging conditions, such as noisy or missing modalities, highlight the robustness and adaptability of the proposed model, emphasizing its practical utility in real-world scenarios. Notably, as shown in Figure1(d) and (e), our model achieves a performance gain of14.13%on the DELIVER dataset and32.15%on the MUSES dataset in these adverse settings, further establishing its efficacy and reliability.

Our contributions are outlined as follows:(I)We improve the SAM2 framework by integrating a MoE mechanism with LoRA modules for multi-modal semantic segmentation tasks. This design enables efficient modality-specific adaptation by training distinct LoRA modules for each modality and leveraging a dynamic routing mechanism to integrate features across modalities effectively.(II)We redesign the SAM2 segmentation pipeline by incorporating a modified segmentation head tailored for multi-modal input and introducing an auxiliary segmentation head. This configuration facilitates the effective fusion of multi-scale features, significantly improving segmentation accuracy.(III)Our method achieves state-of-the-art performance on three widely-used multi-modal benchmarks, ranging from synthetic to real-world scenarios, surpassing existing methods in terms of segmentation accuracy and generalization across diverse modalities.(IV)Extensive experimental evaluation demonstrates the robustness of the proposed framework under challenging conditions, including missing modalities and high levels of noise. The results highlight its adaptability and reliability for real-world applications.

SECTION: IIRelated Work

SECTION: II-AMulti-modal Semantic Segmentation

Multi-modal semantic segmentation seeks to leverage complementary information from multiple sensing modalities, such as RGB, depth, and thermal data, to assign semantic labels to each pixel, thereby improving the accuracy and robustness of scene understanding[15]. This task is predominantly addressed using encoder-decoder architectures, where the encoder extracts hierarchical features, and the decoder reconstructs pixel-level predictions[16,17,18].

The evolution of encoders has been significantly influenced by Fully Convolutional Networks (FCNs), which enable end-to-end learning for pixel-level predictions[19,20]. Notable advancements in FCNs include the introduction of dilated convolutions to expand the receptive field[21,22]and pyramid pooling modules to incorporate multi-scale contextual information[23]. DeepLab further refined these methods by combining atrous convolutions with fully connected conditional random fields to enhance segmentation boundaries and accuracy[24]. However, FCNs face challenges in capturing long-range dependencies, which are essential for understanding complex scenes. Transformer-based encoders address this limitation by employing self-attention mechanisms to model global context effectively[25,26,27,28,29,30,31]. Moreover, transformer-based decoders integrate robust multi-level context mining and process diverse multi-scale features extracted by the encoder, enabling precise and efficient segmentation, particularly in complex or high-resolution images[32,33,34,35].

Combining information from different modalities enhances scene understanding in multi-modal segmentation, especially in challenging environments where a single modality may be insufficient. Early fusion strategies combine data from all modalities at the input level, allowing the encoder to learn joint representations but risking redundancy or noise in the fused input[36,37,38]. In contrast, late fusion methods process each modality independently before combining features during decoding. This preserves modality-specific characteristics but may limit inter-modal interactions[39,40,41]. Adaptive fusion strategies, which dynamically integrate multi-modal data at various stages of the network, have emerged as a flexible solution. These approaches refine features across modalities at different abstraction levels, often incorporating cross-modal attention mechanisms or specialized modules to enhance feature interactions[42,43,44,45].

SECTION: II-BSAM for Semantic Segmentation

SAM[3]and DINO v2[46]are prominent foundation models for image segmentation, leveraging Vision Transformers as their backbone. SAM includes a mask decoder and a flexible prompt encoder that supports diverse inputs, such as points, bounding boxes, and text, enabling zero-shot instance segmentation. Despite its versatility, SAM faces challenges in semantic segmentation due to its training on large-scale datasets focused on object boundaries rather than semantic labels[47].
To adapt SAM for semantic segmentation, ClassWise-SAM-Adapter (CWSAM) introduces lightweight adapters, a classwise mask decoder, and efficient task-specific input preprocessing to assign semantic labels in challenging SAR imagery efficiently[48]. The SAM-to-CAM (S2C) framework refines Class Activation Maps (CAMs) using prototype-based contrastive learning and CAM-based prompting, improving class-specific segmentation masks[49]. Additionally, SAM’s current robustness across segmentation tasks diminishes when applied to non-RGB data such as depth or event-based data, highlighting the need for specialized adaptations[50].

SECTION: II-CParameter-Efficient Fine-Tuning with LoRA and MoE

Fine-tuning large pre-trained models like SAM for specific tasks often incurs high computational costs. Parameter-efficient fine-tuning (PEFT) techniques such as soft prompts, adapters, and LoRA provide efficient alternatives[51]. LoRA introduces low-rank matrices into pre-trained models, allowing efficient adaptation by fine-tuning a minimal number of additional parameters while keeping the majority of the model weights frozen[52]. Extensions like DyLoRA[53]and SoRA[54]dynamically adjust the rank during training, improving adaptability across diverse tasks.

LoRA’s modularity allows integration with MoE architectures, which dynamically activate specific LoRA modules based on task requirements. Routing mechanisms such as static top-k selection[55,56]or dynamic thresholding[57,58]enable efficient selection of LoRA modules. Structural integrations like LoRAMoE[59], which incorporates LoRA modules into feed-forward layers, and MoELoRA[60], which integrates LoRA modules into both self-attention and feed-forward layers, further enhance flexibility. MixLoRA[56]combines LoRA modules in self-attention layers and merges them with shared feed-forward layers to optimize computational efficiency and representation learning.

Although SAM demonstrates strong generalization capabilities, it faces limitations in adapting to semantic segmentation tasks involving non-RGB modalities. Our framework represents the first attempt to adapt SAM for multi-modal semantic segmentation by leveraging an MLE tailored to specific modalities, including depth, LiDAR, and event-camera data. We propose a novel routing strategy within the MoE framework to ensure adequate cross-modal consistency, addressing the challenges inherent in multi-modal integration.

SECTION: IIIMethodology

SECTION: III-APreliminary

Segment Anything Model.The SAM2 architecture is a transformer-based framework[61]developed for instance segmentation, integrating three key components: a hierarchical backbone, a Feature Pyramid Network (FPN)-based neck, and a mask decoder. The hierarchical backbone adopts the Hiera architecture[62]as a multi-scale feature extractor, embedding input images into high-dimensional feature spaces via a patch embedding mechanism. This backbone processes features hierarchically, doubling their dimensionality and reducing spatial resolution at each stage. These transformations leverage a combination of window-based multi-head self-attention and pooling operations, enabling the model to capture spatial and semantic relationships across varying scales. The FPN-based neck refines and consolidates these features by aligning feature dimensions from different stages, producing a unified multi-scale representation. Through its lateral connections and top-down pathways, the FPN merges fine-grained details from shallow layers with high-level semantic information from deeper layers. A sine-based positional encoding is incorporated to encode spatial relationships, enhancing the fused features for precise mask generation. The mask decoder employs transformer-based cross-attention with learnable mask tokens that iteratively interact with the fused features and positional encodings. These tokens are refined across multiple layers of cross-attention and feedforward operations. An upscaling module ensures that the final segmentation masks are high-quality and fine-grained. Moreover, the decoder’s ability to output multiple masks allows it to disambiguate overlapping regions and effectively handle complex scenes.

SECTION: III-BFramework Overview

Building on the SAM2 framework, we propose a customized SAM2 architecture, namelyMLE-SAMframework, designed explicitly for multi-modal semantic segmentation task, as illustrated in Figure2. This customization begins by freezing the pre-trained image encoder and fine-tuning it with LoRA layers, efficiently adapting the model to new visual modalities while preserving its intensive pre-trained knowledge. The image encoder processes input visual modalitiesto generate Semantic Feature Map (SFM), which are further transformed by the mask decoder’s convolutional module into two additional feature pyramids: a Fine-grained Feature Pyramid (FFP)and an Intermediate-resolution Feature Pyramid (IFP). These feature pyramids and the SFM enhance the model’s spatial and semantic representation capabilities.

To achieve an integrated feature representation, we propose a framework that combines the SFM, FFP, and IFP by averaging these representations across modalities to derive the integrated feature, where. To further refine this integration, a selective top-mechanism is employed, generating weighted feature mapsthat prioritize salient information for each index. These refined features,and, are subsequently fused into a unified feature representation, forming the input for downstream semantic segmentation.

The unified featureis processed using a dual-pathway mask prediction strategy to enhance segmentation accuracy. In the first pathway, the fused features are fed into the SAM2 mask decoder, which utilizes a frozen transformer block to extract mask tokens from the SFM. These tokens interact with the fine-grained and intermediate-resolution pyramids to construct a high-resolution feature representation. This representation is further refined by a hypernetwork to produce precise segmentation masks, denoted as.

In the second pathway, the fused features are processed by an auxiliary segmentation head comprising three Multi-Layer Perceptrons (MLPs) and a series of upscaling layers. The outputs of this pathway are concatenated, passed through dropout layers to prevent overfitting, and fused linearly to predict an alternative set of high-resolution masks,.
The final segmentation output is derived by combining the predictions from both pathways, leveraging their complementary strengths. This dual-pathway design effectively addresses the challenges posed by multi-modal data distributions and diverse feature scales, ensuring robust and accurate semantic segmentation across multiple modalities.

SECTION: III-CHierarchical Multi-Modal Feature Extraction with LoRA

Give the input set formodalities, where,, andrepresent the height, width, and number of channels of each modality, respectively. The indexdenotes a specific modality, such as RGB, depth, LiDAR, or event camera. Each modality is processed independently through the hierarchical backbone network of Hiera to extract multi-scale features.

Initially, a patch embedding operation transforms each inputinto an embedded feature mapas shown in Eq. (1), whereis a weight matrix,is a bias vector,is the dimensionality of the feature embedding, and,denote the down-sampled height and width after applying a down-sampling factor.

The backbone of SAM2 progressively reduces spatial resolution while increasing feature dimensionality overstages, producing multi-scale feature maps as defined in Eq. (2), where,, anddefines the down-sampling factor at stage. The number of channels at stageis denoted by.

Each stage employs window-based multi-head self-attention to extract features, as shown in Eq. (3), where,, andare the query, key, and value matrices,is the dimensionality of the key matrix, andsoftmaxapplies along the last dimension.

To enhance efficiency and modality-specific adaptation, we introduce a LoRA layer to update the query and value projections, as shown in Eq. (4), whereandare low-rank matrices withas the rank parameter. These updates yield augmented projections, as defined in Eq. (5). LoRA parameters are modality-specific and trained independently while freezing the backbone parameters, ensuring efficient cross-modal adaptation.

Hierarchical features are refined using an FPN, which integrates lateral and top-down pathways to enhance diverse multi-scale features. At each stage, the input feature mapundergoes a precise lateral convolution operation, yielding a refined modality-specific feature map. This operation reduces the channel dimensionality towhile preserving the essential spatial dimensionsand, ensuring robust consistency in spatial resolution and compatibility for subsequent fusion operations within the FPN.

Letdenote the set of layers where top-down fusion is applied. For each layer, top-down fusion combines feature representations from deeper layers with those at the current stage, producing the fused feature map. This fusion process is mathematically defined in Eq. (6).

Here,represents the fused feature map at stage, integrating modality-specific featureswith the upsampled features from the subsequent layer. TheUpsampleoperation adjusts the spatial resolution ofto match that of, ensuring accurate integration. The hierarchical refinement that underlies the multi-scale feature representation of the FPN is central to this fusion process.

SECTION: III-DDynamic Multi-Modal Feature Fusion with MoE and Routing Mechanisms

The FPN is employed to generate three distinct feature maps for each modality, designed to capture semantic and spatial information at multiple different resolutions: theSFM(), theFFP(), and theIFP(). To improve the overall representational capacity of the finer-resolution feature maps (and), 1x1 convolutional layers are applied to reduce their channel dimensions while preserving spatial resolution. Following these operations, the dimensions are transformed such thatand, ensuring a compact and efficient representation suitable for subsequent fusion and effective analysis.

To aggregate features across modalities, the integrated feature mapforis computed by averaging the features across all modalities, as shown in Eq. (7).

wheredenotes the feature map for modalityat pyramid level. This operation ensures uniform aggregation, capturing a holistic representation of multi-modal features. However, the equal-weight assumption inmay be suboptimal when certain modalities are more informative than others. To address this limitation, a MoE mechanism is introduced to assign dynamic weights to features based on their relevance, enabling the model to prioritize significant features while attenuating irrelevant information.

For the cross-modal routing procedure, spatially averaged embeddingsare computed for each modality and feature level as compact representations of spatial information. These embeddings, defined in Eq. (8), are derived by averaging spatial features over heightand width. Hererepresents the feature map of modalityat spatial locationfor level.

Routing weights, which quantify the importance of each modality for feature integration, are calculated using a linear transformation followed by an activation function, as described in Eq. (9), whereis the weight matrix,is the bias term, andrepresents a softmax function to ensure proper normalization of the routing weights.

The routing mechanism dynamically selects features from the most relevant modalities based on their routing weights. For each feature level, the top-modalities with the highest routing weightsare identified. This ensures that only the most significant modalities contribute to the final feature representation. The fused feature mapis then computed as Eq. (10), whereTop-kselects the weights corresponding to the top-modalities,denotes element-wise multiplication, andrepresents the feature map of modalityat level.

This fusion strategy enables the model to effectively adjust the contribution of each modality, integrating both global information and modality-specific nuances into a cohesive feature representation. By prioritizing the most relevant modalities for each feature level, the approach enhances the model’s capacity to handle multi-modal data and capture complementary information across modalities.

By combiningandto the unified feature map, the proposed framework effectively balances uniform aggregation for comprehensive feature representation and dynamic weighting for selective feature refinement, resulting in a robust multi-modal fusion strategy.

SECTION: III-EAdapted Mask Decoder with Auxiliary Segmentation Head

Next, we employ a dual-pathway mask prediction strategy on the unified feature mapto generate high-resolution segmentation masks.

In the first pathway shown in Figure3, we extend SAM2’s mask decoder to produce high-resolution multimasks. This involves generating high-resolution segmentation logits, denoted as, through a structured multi-scale fusion process. Here,represents the number of segmentation categories. The backbone features, which encapsulate global semantic context, are processed via a transformer-based decoder, producing low-resolution logits. These logits are iteratively refined by incorporating spatially detailed features from intermediate-resolution feature mapsand fine-grained feature maps. This hierarchical refinement process is mathematically described as Eq (11), wheredenotes the transformer-based decoding operation applied to,Upsampleperforms bilinear upsampling to match spatial resolutions, andConvis aconvolution for channel alignment.

As shown in Figure4, the second pathway utilizes a feature fusion mechanism to integrate multi-scale features into a unified high-resolution embedding. Specifically, backbone features,, andare first transformed via MLPs and upsampled to a common target resolutionusing bilinear interpolation. This results in upsampled feature maps,, and, respectively. These upsampled features are then concatenated along the channel dimension and passed through a linear fusion layer, followed by a prediction layer, to produce the high-resolution segmentation logitsas described in Eq. (12).effectively integrates features from multiple scales, whilegenerates the segmentation logits. This dual-pathway approach captures both global and local contextual information, thereby enhancing segmentation accuracy and robustness.

The training process minimizes a loss function that integrates the Online Hard Example Mining Cross-Entropy (OhemCrossEntropy) loss[63], which focuses on hard-to-predict pixels to improve model robustness and efficiency. The ground truth segmentation labelsare defined such that, where 255 indicates the ignore label. The OhemCrossEntropy loss for a single prediction mapis given by Eq. (13).

whereis the pixel-wise cross-entropy loss, andrepresents the set of hardest pixels, selected based on prediction difficulty. The normalization factorensures that a sufficient number of complex examples are included, where, andis the total number of valid pixels in the image.

The overall loss function incorporates the OhemCrossEntropy loss applied to bothand, as defined in Eq. (14).

whereare scalar weights that control the relative importance of each loss term.

SECTION: IVExperiments

SECTION: IV-AExperimental Setup

Datasets.To comprehensively evaluate the performance of the proposed MLE-SAM model in multi-modal semantic segmentation, three distinct datasets were selected, each targeting specific challenges in autonomous driving and material segmentation tasks. These datasets provide complementary benchmarks to address real-world complexities such as adverse weather conditions, sensor failures, and multi-modal fusion in diverse scenarios.

The DELIVER dataset[13]is a large-scale multi-modal benchmark designed explicitly for semantic segmentation in autonomous driving scenarios. Developed using the CARLA simulator, it incorporates data from four modalities:RGB (R), Depth (D), LiDAR (L) and Event (E), enabling advanced multi-modal fusion research. The dataset consists of 7,885 front-view images, each with a resolution of 1,042 by 1,042 pixels, partitioned into 3,983 images for training, 2,005 for validation, and 1,897 for testing. Semantic segmentation is supported across 25 distinct classes, with each data sample providing six panoramic views covering a field of view of. To emulate real-world challenges, DELIVER introduces four adverse weather conditions and five sensor failure cases, including motion blur, overexposure, and LiDAR jitter.

The MUSES dataset[10]is a multi-modal benchmark tailored for dense semantic perception in autonomous driving under challenging environmental conditions like rain, snow, fog, and nighttime. It provides 2,500 samples with high-quality 2D panoptic annotations spanning 19 semantic classes. The dataset is divided into 1,500 training samples, 250 validation samples, and 750 test samples, each captured at a resolution of 1,920 by 1,080 pixels. MUSES integrates synchronized data from three modalities: a frame camera (F), an event camera (E), and a LiDAR (L), offering diverse inputs for tasks including semantic segmentation, panoptic segmentation, and uncertainty-aware panoptic segmentation.

The MCubeS dataset[14]is a multi-modal benchmark designed for material semantic segmentation, focusing on dense per-pixel recognition of material categories in challenging outdoor scenes. It includes 500 annotated image sets capturing 42 scenes with four distinct imaging modalities: RGB, near-infrared (NIR), and polarization represented by the Angle of Linear Polarization (AoLP) and the Degree of Linear Polarization (DoLP). The dataset is divided into 302 images for training, 96 for validation, and 102 for testing, with each image at a resolution of high-quality 1920 by 1080 pixels. It annotates 20 material classes, including asphalt, concrete, metal, fabric, water, and grass types.

Multi-modal Segmentation Evaluation.We evaluated the proposed MLE-SAM method for multi-modal semantic segmentation against three state-of-the-art approaches, namely CMNeXt[13], CWSAM[48], and SAM-LoRA, across three benchmark datasets. For fairness comparison, the backbone architectures were standardized as follows: MiT-B0 was employed for CMNeXt, ViT-B served as the backbone for both CWSAM and SAM-LoRA, while MLE-SAM utilized Hiera-B+ as its backbone. Detailed implementation details is provided in AppendixReferences. The evaluation included various combinations of input modalities to assess each method’s ability to integrate and utilize multi-modal information. Additionally, quantitative analysis was conducted on the DELIVER dataset, comparing trainable parameters and performance under challenging environmental conditions such as cloudy, foggy, motion blur, overexposure, underexposure, LiDAR jitter, and event low resolution. This systematic assessment provides a comprehensive understanding of each method’s robustness and efficiency across diverse scenarios.

Segmentation Evaluation with Missing Modalities and Noise.We then evaluated the robustness of semantic segmentation models trained with all available modalities but tested under various combinations of individual and partial modalities on DELIVER and MUSES datasets. The robustness of MLE-SAM is analyzed under Gaussian and Random noise applied to different modalities, with mean Intersection over Union (mIoU) as the primary evaluation metric.
We implemented a noise augmentation module to simulate adverse conditions for injecting Gaussian or random noise into specified modalities. Gaussian noise was generated using a standard normal distribution scaled by 50.0, while random noise was uniformly sampled within the range [-100, 100]. The noise was directly added to the image data of the targeted modality, followed by clipping pixel values to the range [0, 255] to ensure validity and prevent overflow or underflow in pixel intensities.

SECTION: IV-BMulti-modal Segmentation Comparison

The performance comparison in TableIdemonstrates the efficacy of the proposed MLE-SAM model, a SAM-based approach, in semantic segmentation tasks on the DELIVER dataset. Across all tested modality combinations, MLE-SAM consistently achieves the highest mIoU scores, significantly surpassing the performance of competing methods. For the single-modality RGB configuration, MLE-SAM achieves an mIoU of 55.23%, outperforming CMNeXt and SAM-LoRA by margins of 3.94% and 3.39%, respectively. When utilizing RGB and Depth modalities, the mIoU increases to 63.57%, a gain of 3.96% over CMNeXt and 3.32% over SAM-LoRA. Incorporating Event data alongside RGB and Depth yields an mIoU of 62.69%, with improvements of 2.85% and 2.61% over CMNeXt and SAM-LoRA, respectively. The addition of all four modalities results in the best performance for MLE-SAM, achieving an mIoU of 64.08%, exceeding SAM-LoRA by 4.54% and CMNeXt by 4.90%. These results highlight the ability of MLE-SAM to effectively integrate multi-modal information, with performance gains becoming more pronounced as additional modalities are incorporated. Notably, the inclusion of all modalities leads to an mIoU improvement of 8.85% over the RGB-only configuration, underscoring the significant advantage of multi-modal fusion in semantic segmentation.

The results in TableIIfurther validate the superiority of MLE-SAM on the MUSES dataset. The model consistently achieves the highest mIoU scores across all modality combinations, significantly outperforming other methods. For single-modality Frame-camera inputs, MLE-SAM attains an mIoU of 73.95%, surpassing CMNeXt by 30.58% and SAM-LoRA by 8.04%. With the Frame-camera and Event modality combination, the mIoU improves to 74.73%, exceeding CMNeXt and SAM-LoRA by 31.34% and 6.77%, respectively. Adding LiDAR to Frame-camera further enhances the mIoU to 75.42%, representing a 28.39% improvement over CMNeXt and a 5.08% improvement over SAM-LoRA. The integration of Frame-camera, Event, and LiDAR modalities achieves an mIoU of 74.8%, maintaining MLE-SAM’s superior performance with gains of 28.14% and 4.72% over CMNeXt and SAM-LoRA, respectively. These findings highlight the robust capacity of MLE-SAM to leverage real-world multi-modal data effectively, enabling significant segmentation performance enhancements.

The results on both datasets reveal important insights into the relationship between dataset characteristics and model performance. While MLE-SAM demonstrates strong segmentation capabilities on both datasets, its higher performance on MUSES can be attributed to the alignment between the SAM pretraining corpus and the real-world nature of MUSES. As SAM-based models are pre-trained on diverse real-world images, they are inherently better suited to datasets like MUSES, which capture complex, realistic environmental conditions. Conversely, the simulated nature of the DELIVER dataset limits the full exploitation of SAM’s pre-trained knowledge.

TableIIIshowcases MLE-SAM’s performance on the MCubeS dataset, further affirming its capability for multi-modal semantic segmentation. With the RGB-AOLP modality combination, MLE-SAM achieves an mIoU of 50.61%, outperforming SAM-LoRA by 1.87%, CWSAM by 0.83%, and CMNeXt by a significant 13.40%. The inclusion of DoLP alongside RGB and AOLP raises the mIoU to 50.89%, surpassing SAM-LoRA by 1.54%, CWSAM by 2.62%, and CMNeXt by 12.17%. Adding NIR to the RGB-AOLP-DoLP configuration achieves the highest mIoU of 51.02%, with respective improvements of 1.56% over SAM-LoRA, 0.43% over CWSAM, and a remarkable 14.86% over CMNeXt. These results underscore MLE-SAM’s proficiency in integrating multi-modal information for dense per-pixel material segmentation, particularly in challenging outdoor scenes.

In summary, the experimental results across the DELIVER, MUSES, and MCubeS datasets consistently demonstrate the superior performance of MLE-SAM in leveraging multi-modal data for semantic segmentation. The model achieves substantial gains over state-of-the-art competitors by utilizing complementary information from multiple modalities. Moreover, the observed performance trends highlight the importance of dataset characteristics, with real-world datasets providing more opportunities for SAM-based models to exploit their pretraining strengths fully. The consistent improvements across diverse configurations underscore MLE-SAM’s robustness and scalability, establishing it as a robust framework for advancing multi-modal segmentation tasks.

SECTION: IV-CAblation Studies and Qualitative Analysis

The quantitative evaluation of modality combinations on the DELIVER reveals the relationship between trainable parameters and performance under various conditions. As shown in TableIV, under normal conditions (cloudy, foggy, and sunny), RGB-D performs best with mIoU values of 66.21%, 63.89%, and 65.58%, respectively. Combining RGB and Depth enhances feature richness and robustness. Under adverse conditions (night and rainy), RGB-D-E and RGB-D-E-L outperform, with mIoU values of 60.82% and 62.68% for night, and 62.01% and 62.71% for rainy conditions. Including sparse modalities like Event and LiDAR compensates for the limitations of dense sensors in low-light and high-reflection environments by capturing high-dynamic-range data.

RGB-D is most effective in handling motion blur in sensor failure scenarios, achieving an mIoU of 63.03% by leveraging complementary spatial and depth information. For more challenging conditions like overexposure, LiDAR jitter, and event low resolution, RGB-D-E-L offers the highest robustness, with mIoU values of 64.28%, 63.22%, and 64.15%, respectively. This improvement comes from combining dense modalities (RGB and Depth) with sparse modalities (Event and LiDAR), where sparse data enhances performance in conditions that limit dense sensors.

From a computational perspective, trainable parameters increase from 5.2 million for single modalities like RGB or Depth to 20.79 million for the RGB-D-E-L combination. Dense sensors excel in capturing detailed information but are sensitive to noise in extreme conditions. In contrast, sparse data from Event and LiDAR improves robustness by highlighting critical features in degraded scenarios. This analysis emphasizes the importance of multi-modal fusion in enhancing robustness and adaptability, balancing dense and sparse data to ensure consistent performance across diverse environments.

TableVevaluates the impact of integrated features, weighted features, and the auxiliary segmentation head on multi-modal semantic segmentation using the DELIVER with R-D-L-E modalities. The integration ofresults in a substantial improvement in segmentation performance, achieving an mIoU of 61.87% with 20.62 million parameters. Adding an auxiliary segmentation head with integrated features raises the mIoU to 62.03%, with a slight parameter increase (20.64 million). In contrast, the use of weighted featuresalone leads to inferior results, with mIoU scores of 58.35% and 57.99% when the auxiliary head is excluded and included, both requiring more parameters (20.77 and 20.79 million). The combination ofand, along with the auxiliary segmentation head, achieves the highest performance, with an mIoU of 64.08% and 20.79 million parameters. These results highlight the importance of combining both feature types, as their integration enhances feature representation and segmentation accuracy.

Figure5shows the extracted feature maps under adverse sensor conditions across various modalities. The performance of each modality is affected by its intrinsic characteristics, especially in challenging environments. For example, RGB features are sensitive to lighting changes, suffering significant degradation under overexposure or underexposure. Depth and LiDAR features are vulnerable to environmental disturbances like LiDAR jitter, which introduces noise in depth estimation and spatial measurements. In contrast, combining modalities enhances robustness by leveraging complementary strengths and mitigating the limitations of individual features.

For instance, in overexposure or underexposure conditions, depth features help capture detailed object information (e.g., trees and cars), compensating for RGB’s underperformance. Similarly, in the presence of LiDAR jitter, combining RGB and event features improves texture representation, preserving details like building structures. These results demonstrate the effectiveness of multi-modal fusion in creating more resilient feature representations under adverse conditions.

Figure6presents the t-SNE visualizations of pixel-level features from selected semantic classes under sensor failure scenarios, highlighting substantial variations in feature separability across modalities and failure conditions. Each point in the visualization corresponds to a pixel, color-coded by its semantic class, illustrating the underlying distribution of features in the high-dimensional space. In single-modality scenarios, sensor failures result in significant class overlap, reflecting a diminished discriminative capacity of the feature representations. Conversely, multi-modal training substantially improves feature separability, demonstrating the effectiveness of multi-modal fusion in constructing robust feature representations. Notably, dense modalities, such as RGB and depth, exhibit superior class separability compared to sparse modalities like event and LiDAR, underscoring the critical role of data density in preserving semantic integrity under adverse conditions. These results emphasize the potential of multi-modal approaches to enhance semantic segmentation performance, particularly in sensor-degraded environments.

MethodTrainingDELIVER datasetMeanRDELR-DR-ER-LD-ED-LE-LR-D-ER-D-LR-E-LD-E-LR-D-E-LCMNeXtR-D-E2.690.210.78-48.046.92-6.92--59.84----17.91-CWSAM12.335.428.16-27.2617.44-40.96--56.22----28.259.64SAM-LoRA18.3448.943.36-60.0818.34-48.94--60.08----36.8718.95MLE-SAM20.7748.594.68-62.8520.14-49.42--62.69----38.4520.54CWSAMD-E-L-37.568.136.5---37.4138.598.41---36.34-24.71-SAM-LoRA-49.523.814.53---51.0551.474.29---53.08-31.116.40MLE-SAM-56.024.072.13---56.4556.784.75---57.96-34.029.31CMNeXtR-D-E-L0.860.490.660.3747.069.9713.752.631.732.8559.0359.1814.7339.0759.1820.77-CWSAM12.335.428.166.223.5115.9115.5939.237.219.1128.728.8421.8444.1555.4325.444.67SAM-LoRA17.6248.582.923.1659.5417.6217.6248.5848.582.9259.5459.5417.6248.5859.5434.1313.36MLE-SAM15.850.280.742.0763.4715.5715.9150.4250.60.8663.1164.2615.6450.6864.0834.9014.13

Figure7presents the semantic segmentation results on the DELIVER dataset, illustrating the performance differences among various methods and modality combinations. The results indicate that integrating the R-D-E-L modality combination significantly improves segmentation accuracy and completeness compared to single-modal approaches. For example, MLE-SAM with only the RGB modality struggles to detect pedestrians under challenging conditions such as overexposure and LiDAR jitter. In contrast, the R-D-E-L combination accurately segments small objects like pedestrians. However, CWSAM and SAM-LoRA with the R-D-E-L combination exhibit suboptimal performance, particularly in segmenting buildings under overexposure, and all three methods encounter difficulties in identifying small objects during motion blur scenarios. Furthermore, CMNeXt with R-D-E-L fails to capture critical details, such as bus stations and lights, under LiDAR jitter conditions. These results underscore the robustness of MLE-SAM in leveraging comprehensive multi-modal data to achieve consistent and superior segmentation accuracy overall segmentation performance under sensor failure cases.

SECTION: IV-DGeneralization Evaluation with Partial Modality Testing

TableVIpresents a comprehensive evaluation of four semantic segmentation models—CMNeXt, CWSAM, SAM-LoRA, and MLE-SAM—trained on three modality combinations: R-D-E, D-E-L, and R-D-E-L. The models were tested using the DELIVER dataset under various modality scenarios. A key limitation of CMNeXt is its dependency on the RGB modality during training, restricting its flexibility compared to CWSAM, SAM-LoRA, and MLE-SAM, which support training without RGB. Among the evaluated models, MLE-SAM consistently achieves superior performance across all training configurations. Specifically, under the R-D-E training setup, MLE-SAM achieves a mean mIoU of 38.45%, outperforming SAM-LoRA and CWSAM by 1.58% and 10.2%, respectively. For the D-E-L configuration, MLE-SAM achieves 34.02%, surpassing SAM-LoRA by 2.91% and CWSAM by 9.31%. Similarly, under the R-D-E-L configuration, MLE-SAM achieves the highest mean mIoU of 34.90%, exceeding SAM-LoRA by 0.77% and CWSAM by 9.46%. These results highlight MLE-SAM’s effectiveness and adaptability across diverse training setups.

The impact of missing modalities during testing reveals critical insights into the interaction between dense and sparse modalities. When trained on R-D-E and tested on individual modalities, MLE-SAM demonstrates significant variability in performance, achieving 20.77% for RGB-only testing, 48.59% for Depth, and 4.68% for Event. This highlights the stabilizing role of dense data, such as RGB and Depth, compared to the sparse Event modality. A similar pattern emerges under the D-E-L training setup, where Depth testing yields 56.02%, substantially outperforming Event and LiDAR, which achieve 4.07% and 2.13%, respectively. For the R-D-E-L configuration, MLE-SAM demonstrates robust performance in dense testing scenarios, such as 50.28% for Depth and 63.47% for RGB-Depth. However, sparse-only cases, such as Event and LiDAR, result in significantly lower scores of 0.74% and 2.07%, respectively. These findings highlight the robustness of dense modalities in enhancing semantic segmentation performance. In contrast, while offering complementary information, sparse modalities exhibit limited effectiveness when utilized independently.

These performance patterns can be attributed to the intrinsic characteristics of dense and sparse modalities and their integration during training. Dense modalities like RGB and Depth offer rich spatial and structural information, enabling the model to learn stable and generalized features. In contrast, sparse modalities such as Event and LiDAR capture irregular and limited data, which, while applicable in specific contexts, are less reliable as standalone inputs. Training on R-D-E-L incorporates redundancy and the richness of dense data, leading to robust performance on dense subsets during testing. Conversely, reliance on sparse data during testing introduces noise, reducing predictive accuracy. Notably, excluding sparse modalities during training can mitigate these effects, as evidenced by the superior performance of RGB-Depth testing that achieves 63.47% under the R-D-E-L training setup. This suggests that while sparse modalities provide useful complementary features, overemphasis during training can hinder model generalization. MLE-SAM’s adaptive fusion mechanism effectively integrates dense and sparse modalities, ensuring superior performance across multi-modal setups.

MethodTrainingMUSES datasetMeanFLEFLFELEFLECMNeXtF-L3.342.48-47.03---17.62-CWSAM11.612.45-40.69---18.250.63SAM-LoRA53.6911.79-70.34---45.2727.66MLE-SAM70.912.96-75.42---53.0935.48CMNeXtF-E2.72-2.38-43.39--16.16-CWSAM25.14-1.85-41.77--22.926.76SAM-LoRA67.96--67.96--45.3129.14MLE-SAM74.62-1.34-74.73--50.2334.07CMNeXtF-L-E3.52.642.7710.286.633.1446.6610.80-CWSAM6.484.971.9813.9411.592.1549.9813.012.21SAM-LoRA48.5412.054.3770.0848.5412.0570.0837.9627.16MLE-SAM69.675.551.574.1169.55.5574.8042.9532.15

TableVIIcompares the performance of four models trained and tested on various modality combinations from the MUSES dataset. MLE-SAM consistently outperforms its counterparts, demonstrating robustness across modality combinations. For instance, when trained on Frame-camera and LiDAR, MLE-SAM achieves 53.09%, surpassing SAM-LoRA by 7.82%, CWSAM by 34.84%, and CMNeXt by 35.47%. This trend holds under the F-E and F-L-E scenarios, with improvements of 4.92% and 4.99% over SAM-LoRA, respectively.

However, missing modalities during testing significantly affect performance. For example, when trained on F-L-E but tested on sparse modalities like Event-camera or LiDAR, MLE-SAM’s scores drop to 1.5% and 5.55%, respectively. In contrast, when tested on dense Frame-camera data, MLE-SAM achieves 69.67%. These results highlight the critical role of dense data in maintaining segmentation quality, as dense modalities like Frame-camera provide essential spatial continuity and detail, while sparse modalities like Event-camera and LiDAR lack this richness.
These findings reinforce the advantages of MLE-SAM’s adaptive fusion mechanism. This mechanism effectively combines multi-modal inputs to mitigate the limitations of sparse data, making it particularly suited for real-world scenarios with intermittent modality availability.

SECTION: IV-ERobustness Evaluation Under Noisy Testing Conditions

TableVIIIevaluates the performance of three adapted SAM models, namely CWSAM, SAM-LoRA, and MLE-SAM, under Gaussian and random noise applied to four modalities. The results highlight key observations regarding the differential impact of noise on dense and sparse modalities, as well as the robustness of MLE-SAM compared to the other two models.

The analysis shows that Gaussian noise affects dense modalities (RGB, Depth) more than sparse ones (Event, LiDAR). For instance, CWSAM’s RGB mIoU dropped to 29.60% under Gaussian noise, while Depth achieved 53.87%. Sparse modalities were less affected, with Event and LiDAR maintaining mIoU values of 54.89% and 54.79%. Under random noise, RGB for CWSAM dropped further to 23.93%, and Depth to 53.18%, while Event and LiDAR remained robust, with mIoU values of 54.76% and 54.62%, respectively. This highlights the resilience of sparse modalities to pixel perturbations due to their localized data nature.

MLE-SAM showed superior robustness across all modalities, outperforming CWSAM and SAM-LoRA. Under Gaussian noise, MLE-SAM’s RGB mIoU was 57.00%, significantly higher than 29.60% for CWSAM and 53.83% for SAM-LoRA. Sparse modalities also benefited, with Event and LiDAR achieving 63.90% and 63.87%, reflecting improvements of 9.01% and 9.08% over CWSAM, and 4.35% and 4.33% over SAM-LoRA. Under random noise, MLE-SAM’s RGB mIoU declined slightly to 56.35%, still outperforming CWSAM and SAM-LoRA. Event and LiDAR maintained robust mIoU values of 63.89%, surpassing CWSAM by 9.13% and 9.27%, and SAM-LoRA by 4.34% across both noise types.
Comparing Gaussian and random noise, random noise introduced higher variability for dense modalities, reducing RGB mIoU in CWSAM from 29.60% to 23.93%. Sparse modalities were minimally affected, with stable mIoU values across models and noise types, underscoring their robustness to global perturbations.

Overall, these results emphasize the need for modality-specific strategies for noise resilience. Dense modalities require denoising techniques, while sparse ones are naturally robust. Among the models, MLE-SAM consistently outperforms CWSAM and SAM-LoRA, validating its effectiveness for multi-modal semantic segmentation in noisy environments.

SECTION: VConclusion and Future Work

This paper presented MLE-SAM, a novel adaptation of the SAM2 architecture tailored for multi-modal semantic segmentation. MLE-SAM incorporates LoRA-based adaptation, a selective feature weighting mechanism, and a dual-pathway mask prediction strategy. By effectively fusing dense and sparse modalities, MLE-SAM harnesses their complementary strengths to achieve precise segmentation while maintaining robustness across diverse conditions and datasets.

Extensive experiments demonstrate that MLE-SAM consistently outperforms state-of-the-art models in terms of mIoU across various datasets and modality combinations. Notably, the model exhibits resilience in challenging scenarios, including noisy inputs and missing modalities, underscoring the advantages of its multi-modal fusion approach. Dense modalities contribute detailed spatial information crucial for high-resolution segmentation, while sparse modalities enhance robustness in adverse or resource-constrained environments.

Future research can prioritize refining the multi-modal integration through advanced pretraining techniques, noise-tolerant module designs, and adaptive attention mechanisms for sparse feature enhancement. Developing dynamic fusion strategies to balance dense and sparse modalities seamlessly can improve MLE-SAM’s adaptability and effectiveness in real-world applications.

SECTION: References

[Implementation Details]
The input size for all images from the three datasets is standardized to 10241024 pixels. Image preprocessing includes data augmentation techniques such as random color jittering, horizontal flipping, Gaussian blurring, and random cropping to the target resolution of 10241024. Following these augmentations, the images are normalized using channel-wise mean and standard deviation values.

The source codes of CMNeXt[13]and CWSAM[48]were adapted for compatibility with the three datasets employed in this study. CMNeXt employs a self-query hub that dynamically selects informative features from auxiliary modalities, which are then fused with the RGB-based primary branch. Additionally, the parallel pooling mixer effectively extracts discriminative cross-modal cues. In this framework, CMNeXt relies on the RGB modality for multi-modal semantic segmentation. CWSAM introduces lightweight adapters within the SAM Vision Transformer image encoder and a novel class-wise mask decoder that generates multi-class, pixel-level predictions, tailored for semantic segmentation tasks. Furthermore, we developed SAM-LoRA, an extension of the SAM model incorporating distinct LoRA modules for each modality. Similar to MLE-SAM, we modify the SAM model by applying LoRA to the image encoder while freezing the remaining components of the SAM architecture. The LoRA adaptation is implemented by altering the query and value projections within the transformer’s attention mechanism. Specifically, the original qkv projection layer is replaced with a custom LoRA layer, which is individually trained for each modality. The masks generated by the modality-specific models are subsequently averaged to form a unified feature representation.

Model training was conducted on an NVIDIA A100 GPU with a batch size of 6. As shown in TableIX, the training process employed the AdamW optimizer[64], configured with an initial learning rate and a weight decay of 0.01, over 100 epochs. The Online Hard Example Mining Cross-Entropy loss function was used without class-specific weighting to handle imbalanced segmentation classes. To optimize learning, a warm-up polynomial learning rate scheduler was applied, with a power of 0.9. The learning rate was gradually increased during the first 10 epochs using a warm-up ratio of 0.1. The ranks of the LoRA modules were set to 32 to balance model capacity and computational efficiency.