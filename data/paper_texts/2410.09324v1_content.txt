SECTION: Token Pruning using a Lightweight Background Aware Vision Transformer
High runtime memory and high latency puts significant constraint on Vision Transformer training and inference, especially on edge devices. Token pruning reduces the number of input tokens to the ViT based on importance criteria of each token. We present a Background Aware Vision Transformer (BAViT) model, a pre-processing block to object detection models like DETR/YOLOS aimed to reduce runtime memory and increase throughput by using a novel approach to identify background tokens in the image. The background tokens can be pruned completely or partially before feeding to a ViT based object detector. We use the semantic information provided by segmentation map and/or bounding box annotation to train a few layers of ViT to classify tokens to either foreground or background. Using 2 layers and 10 layers of BAViT, background and foreground tokens can be separated with% and% accuracy on VOC dataset and% and% accuracy on COCO dataset respectively. We show a 2 layer BAViT-small model as pre-processor to YOLOS can increase the throughput by% -% with a mAP drop of% without any sparse fine-tuning and% with sparse fine-tuning. Our approach is specifically targeted for Edge AI use cases. Code and data are available at [Link].

SECTION: Introduction
Transformershave already demonstrated their ability to outperform traditional methods in Natural Language Processing (NLP) with models like BERTand RoBERTa. They are now commonly used in modern vision-related tasks such as classification, object detection, segmentation, and pose estimationas Vision Transformers(ViT).
Despite the advantages of ViTs over traditional CNN-based approaches, their high computational requirements pose significant challenge in deployment of these models on edge devices with limited memory and computational power. The ViT accepts small image patches (typicallysize) calledas input. As image resolution increases, more input tokens are generated, which increases the performance but reduces model throughput and latency.ViT is also used for object detection by models like DETRwhich uses learnable queries and encoder features to produce box predictions using decoder. Different variations of DETR-like models like,are proposed to create state of the art object detection models.

Zheng et.alshowed that the complexity of Deformable DETRiscompared to the decoder which suggests that focusing on efficiency of the encoder is very important. All the tokens do not have same importance and by reducing the number of tokens results in latency and throughput improvement. The technique to reduce the number of tokens by assessing the importance or relevance of each token is called. In this work, we aim to reduce the number of input tokens by introducing a novel token importance criteria for pruning with a minimal impact on performance. Our approach uses segmentation masks provided in the COCO (80 object categories)and PASCAL VOC (20 object categories)datasets to annotate each individual patch as foreground (FG) or background (BG). This annotation serves as a guide for ViT models in object detection tasks to determine the importance of each token. Sparse DETRand Focus DETRare two most impressive and state of the art techniques for token pruning. As show in Figure, sparse DETR uses the token importance score by computing cross-attention map in the decoder which reduces the number of tokens by 70%. Focus DETRon the other hand detects background tokens and prunes them to increase the throughput. We propose a method that uses background token detection similar to focus DETR but we our target usecase is Edge devices so we avoid using heavy CNN backbones, as proposed in Focus DETR, to detect background tokens which could be computationally very expensive.

We summarize our contributions as follows:

We introduce a novel Background Aware Vision Transformer () capable of separating FG and BG tokens

We introduce a modified Accumulative Cross Entropy Loss function for BG/FG classification.

We demonstrate that integration ofas pre-processing block of DETR/YOLOS like object detection model provides a good latency/accuracy trade-off and increased throughput of the model.

SECTION: Related Work
SECTION: Vision Transformers
Transformershave emerged as a dominant architecture in NLPas well as vision-related tasksand these modelshave achieved state-of-the-art performance for vision tasks including object detection such as DETR, RT-SETRand YOLOS. DETRemploys a combination of CNN-based backbones followed by transformers to address object detection tasks. Swin-transformersintroduced new ViTs that can serve as general-purpose backbones for computer vision tasks. WBDetrreplaced the CNN-based backbones in DETRwith a transformer-based backbone for object detection. Similarly, innovations continue to enhance ViT capabilities, such as, which introduces a K-dimensional score map to provide localized information about image patches. Recent work by Fang et al.proposes end to end object detection as sequence-to-sequence task. Our BAViT proposes an additional information about of these image tokens as BG and FG, which can be integrated as the pre-processing stage to filter out unnecessary patches.

SECTION: Runtime Memory Improvement
ViTsrequire substantial runtime memory, which limits their use on smaller devices. Many research efforts, including, propose methods to optimize the performance of vision transformers. Reformerintroduces architectural changes to the residual layers, replacing them with reversible residual layers to make the model more efficient. Sparse attentionproposes an alternative attention formulation through sparse factorization of the attention matrix, which is one of the most computationally expensive components in ViTs. Sparse Detrenhances the efficiency of DETR-like models by substituting dense attention with deformable attention. Other works, such as, replace standard dropout layers with structured dropout layers to improve the efficiency and robustness of transformers. Few methods focus on pruning heads by ranking them based on their estimated importance. Additionally, quantization approacheshave been explored to further improve the efficiency of ViTs.

SECTION: Token Pruning
The number of tokens contribute to quadratic complexity in ViTs during inference. However, all the tokens generated from the input image are not equally important; many primarily contain background information. Several research efforts, including, propose efficient approaches to remove unnecessary tokens, thereby improving the inference time.introduces a technique that efficiently scores the importance of tokens, discards background queries, and enhances the semantic interaction of fine-grained object queries based on these scores.proposes an adaptive method to hierarchically discard useless tokens and adjust computational costs for different input instances.suggests reusing pruned tokens at later stages of the model. Our work is very close to Focus DETRas both approaches focus on classifying tokens into FG and BG. However, Focus DETR uses a heavy backbone from DETR (like ResNet50, ResNet101) which is not suitable for edge devices. Also, Focus DETR proposes many modifications to the existing DETR model which requires model retraining or fine-tuning for a long time. Therefore, although the technique produces SOTA results, it is not feasible approach for edge devices. Our work proposes a simpler strategy for background token identification using a learnable small ViT model using 2 layers. Also, our approach produces foreground images which visibly looks very similar to Focus DETR produced foreground images but our approach uses a very small model, compared to Focus DETR, to achieve this. BAViT can be used as a separate module and integrated with other models at the pre-processing data stage, enabling faster performance and making the models suitable for smaller devices. Our target use case is small ViTs for edge devices, therefore it is difficult to compare our method with Focus DETR mAP/latency numbers which uses very large model and performs latency experiments on larger GPUs.

SECTION: Methodology
SECTION: Auxiliary Annotations
Transformers accept image patches (called tokens) of size (), created by dividing the input image into a sequence of square patches, as shown in Figure. ViTs use these patches to classify objects in the image through the attention mechanism. Popular datasets like Microsoft COCOand Pascal VOC, used for object detection and segmentation tasks, contain annotations such as bounding boxes and instance segmentation maps. We create a M-dimensional patch annotation vector for every input image, whererepresents the total number of tokens formed by dividing the input image intosmaller non-overlapping patches as shown in Figure. We compare the Jaccard similarity coefficientof each token with all the bounding boxes or segmentation map and it is labeled as one (Foreground - FG) if the overlap of a token with any of the bounding box is more than 0.5, otherwise it is labeled as zero (Background - BG) as shown in Equationand Equation. Figureshows a sample Pascal VOC image (left), bounding boxes (center), and image patches with BG patches in gray and FG patches in red color. When using segmentation maps to create the annotation vector, any image patch with more than 10% overlapping pixel with any class of segmentation map is considered foreground; otherwise, it is considered background. We trained BAViT model both using bounding box annotations and segmentation maps but most of the results presented in this paper are from annotated data using segmentation map.

whereis patch andis bounding box,is assigned label fortoken,is Jaccard coefficient,is threshold for selecting the token as foreground or background.

SECTION: BAViT Architecture
BAViT architecture is created by introducing few fundamental changes in the traditional ViT architecture as illustrated in Figure(left). We remove the CLS token and introduce a linear layer with two output classes for each token. Traditional ViT uses CLS token to encapsulate knowledge from all tokens and it provides the score for each class. On the contrary, BAViT calculates classification score for FG and BG classes for each token. Therefore, we do not need a CLS token. Accumulative Cross Entropy Loss () is calculated as defined in equation, and weights are updated via back propagation. This loss function can also be used with other loss functions targeting different vision tasks, such as object detection loss to help the model focus on important tokens. Since BAViT is supposed to be used as pre-processing step for token pruning, we decided to keep it light weight and used the model with only 2 layers (BAViT-small) to study the impact on YOLOS. However, we have provided BG/FG classification results with 10 layers as well (BAViT-large) as BAViT-small in result section to show the scalability and flexibility of this approach.

SECTION: Accumulative Cross Entropy Loss
In contrast to the traditional ViT classifier training, which involves introducing an additional classification token (CLS) and calculating loss only for that token, we propose a new loss function that calculates the Cross Entropy Lossfor each token individually and then aggregates these losses. This aggregated loss is termed as Accumulative Cross Entropy Loss (), as defined in.

whereis the the number of image samples,is the number of tokens per sample,is the number of classes (background and foreground).is the variable indicating whether the-th token in the-th sample belongs to class. It’s value is one if the token belongs to class, otherwise it is zero.is the predicted probability of the-th token in the-th sample being in class.

SECTION: Model Training
We use both Pascal VOCand COCO 2017to train BAViT and reported mAP (mean Average Precision) result on the validation dataset for both. Each training batch, denoted as (,,), consists oftokens of size, each with an embedding size of, and labels for each token indicating eitheror. We employed the Adamoptimizer with a step learning rate scheduler and trained the model for 100 epochs until convergence. The initial weights for the ViTmodel were loaded from ImageNet-1kdataset pre-trained model.

SECTION: BAViT Integration with ViT based Detection
The BAViT-small is added as a pre-processing block of the ViT based object detector as shown in Figure. We have used YOLOSas the object detection model , an architecture similar to DETRwith an exception that YOLOS provides an option to use the detector without a CNN backbone. Our method works directly on image tokens, so it cannot be applied to a CNN backbone based ViT object detectors. The BAViT model works oninput and YOLOS (tiny) expectsinputs to achieve the benchmark mAP. BAViT outputs the classification of each token as BG or FG with a total oftokens but the YOLOS model expectstokens so we upscale the tokens labels fromtokeeping the relative BG/FG patch position same. After the label scaling step, each oftoken is classified as BG or FG token. The YOLOS model only computes the FG tokens from first to the final layer. We also modify YOLOS model slightly so that it does not compute anything for the BG tokens and return zeros as the final output token for these tokens. All the FG tokens are processed in the usual manner. So, the modified BAViT + YOLOS-tiny model contains 14 layers, first 2 layers of BAViT and the 12 layers of YOLOS-tiny.

SECTION: Results
SECTION: BG/FG Classification Model
The BAViT model was trained with both 2 layers (BAViT-small) and 10 layers (BAViT-large) depth. Tabledisplays the token classification accuracy of these models on different datasets. BAViT-small is used for integration with object detection model (YOLOS) but we also trained the BAViT-large model to assess the impact on model accuracy. We found that BAViT-small achieved% accuracy, which was reasonable compared to BAViT-large’s% accuracy for the BG/FG classification task on VOC dataset given the difference in number of parameters for these two models. We also trained both models on COCO dataset as shown in Tableand used BAViT-small trained with COCO with mAP 70.88% as pre-processing block . YOLOS-tiny model has 6.5M parameters using 18.8 GFLOPS. Addition of BAViT-small over the native YOLOS-tiny marginally increases the total number of parameters (by 1.49M) and FLOP counts (+1.961 GFLOPS) but substantially reduced the amount of total number of tokens (25.63%) which has the quadratic impact over the computational complexity of the ViT models. On the other hand focus DETRmodels with ResNet50 backbone has 48M parameters using GFLOPS which is almost 8x times bigger and slower. Our results also suggest that it can be applied to different datasets with configurable number of layers based on latency and RAM constraints.

Figureshows the BAViT-large model output for COCO images where top image is the original image and bottom image is sparse image with all background patches shown in white color. There are few misclassified tokens where background is classified as foreground and vice versa. Foreground being classified as background is concerning so we added additional post processing block to improve the classification accuracy as explained in Appendix section. It is evident that our model is able to separate FG/BG patches effectively even with multiple objects from different classes. It is also clear from these images that sparsity varies based on the image and it can be even more than 70% in many images. COCO images have an average of% of background tokens which means that only% tokens are important.

SECTION: Token reduction using BAViT
As explained in section, we added BAViT-tiny to pre-process the image and classify each patch as FG or BG tokens before passing to YOLOS model. Using FG patches for all computation and ignoring all the BG patches, we can reduce the number of tokens in YOLOS-tiny model drastically. Equationshows the calculation used to calculate the average reduction in tokens for 5000 COCO validation images. Tableshows BAViT model used with different level of sparsity for token pruning and the impact on mAP due to the same. The sparsity is controlled by modifying the confidence threshold of background tokens. Our BAViT model adds extra complexity to the overall model but since this model has very low complexity and it works at much lower resolution , the overall number of token is less than the original model. for eg. the model with 34% sparsity reduces total tokens by 24% with an accuracy drop of 2.6% on COCO dataset. Please note that we are not demonstrating the results of fine-tuning for most of these models. However, we have finetuned one of the model with 35% sparsity and could improve the accuracy by 2 mAP points. It is important to note that we fine-tuned the model only for 30 epochs to improve the accuracy.

Although our method suffers a drop in mAP due to sparsification, it is still applicable to edge use cases whereas solution proposed in methods like Sparse DETRand Focus DETRcan’t be used. Focus DETR, being the SOTA in token pruning field, uses ResNet50 and ResNet101 backbones to detect background tokens, which makes it impractical for edge use cases with very limited memory and computational capabilities. Also, Focus DETR proposes significant changes in the model architecture which necessitates the model to be retrained which is very expensive. BAViT on the other hand does not need model retraining, whereas to compensate the drop in mAP due to sparsification, it can be fine-tuned for lesser number epochs (30 epochs is used for our experiments).

whereis total YOLOS tokens forimage,is total BAViT tokens forimage, s is sparsity percentage inimage and N is total number of images.

SECTION: Conclusion
In this work, we introduced a novel method for separating BG/FG patches in images by leveraging existing annotations from bounding boxes and segmentation maps to create localized annotations. We applied these annotations within a token classification training strategy, achieving an accuracy of up to 88.79% on the Pascal VOC dataset and 80.57% on the COCO dataset using a 10-layer transformer model. Notably, even with just 2 transformer layers, we were able to achieve over 75% accuracy on Pascal VOC and 70% on COCO dataset respectively. We also used BAViT-small model for pre-processing step to prune tokens of a YOLOS-tiny model. Our approach could reduce the number of tokens by 25% with a mAP drop of 3% on COCO dataset. This drop is shown to be recovered (less than 2% mAP drop) by sparse token finetuning by using just 30 epochs. BAViT approach is a low cost and low complexity alternative to SOTA methods like Focus DETRwhich works on large models not fitting on edge devices. Future work involves integrating our approach to YOLOS type of model to jointly train BG/FG classifier and object detector together to observe the accuracy-latency trade-off. Additionally, we also aim to achieve adaptive sparsity based on input image complexity, with a learnable threshold parameter similar to.

SECTION: References
SECTION: Appendix
In this appendix, we provide additional details about post-processing algorithm applied to improve BG/FG classification results. Figureshows the result of BAViT where orange patches are FG misclassified as BG and gray patches are correctly classified by the model. To minimize the error due to misclassified FG pixels, we use Connected Component Analysis (CCA), the traditional graph analysis algorithm to connect nodes with connected neighbors. In this case, each patch is considered as a node of the graph and CCA is performed by applying a convolutional kernel (shown in Figure) on the graph (FG=1, BG=0) and converting the graph node fromtofor all pixels with convolution output greater than. The CCA algorithm is applied for few steps to minimize the classification error. More steps reduces classification error but it increases number of FG patches which were BG in the ground truth image. We found 3 steps to be optimal based on different experiments, impact on accuracy and efficiency. Right image in Figureshows the result of our post processing convolution which brings the result very close to the ground truth. Please note that, we have not applied any post processing while reporting model’s accuracy infor fair evaluation. However,including the post processing convolution is expected to improve accuracy of the model significantly.