SECTION: 3D Unsupervised Learning by Distilling 2D Open-Vocabulary Segmentation Models for Autonomous Driving

Point cloud data labeling is considered a time-consuming and expensive task in autonomous driving, whereas unsupervised learning can avoid it by learning point cloud representations from unannotated data. In this paper, we propose UOV, a novel 3DUnsupervised framework assisted by 2DOpen-Vocabulary segmentation models. It consists of two stages: In the first stage, we innovatively integrate high-quality textual and image features of 2D open-vocabulary models and propose the Tri-Modal contrastive Pre-training (TMP). In the second stage, spatial mapping between point clouds and images is utilized to generate pseudo-labels, enabling cross-modal knowledge distillation. Besides, we introduce the Approximate Flat Interaction (AFI) to address the noise during alignment and label confusion. To validate the superiority of UOV, extensive experiments are conducted on multiple related datasets. We achieved a record-breaking 47.73% mIoU on the annotation-free 3D segmentation task in nuScenes, surpassing the previous best model by 3.13% mIoU. Meanwhile, the performance of fine-tuning with 1% data on nuScenes and SemanticKITTI reached a remarkable 51.75% mIoU and 48.14% mIoU, outperforming all previous pre-trained models.
Our code is available at: https://github.com/sbysbysbys/UOV

SECTION: Introduction

As neural-network-based 3D scene perception methods,e.g., object detection(Shi, Wang, and Li2019; Shi et al.2020; Zhou and Tuzel2018; Lang et al.2019), point cloud segmentation(Qi et al.2017a,b; Choy, Gwak, and Savarese2019),etc., become increasingly complex in their network architectures with a growing number of parameters, methods relying solely on enhancing model structures are reaching a point of saturation. Meanwhile, approaches to enhancing model performance through data-driven methods heavily rely on time-consuming and expensive manual annotations. Due to constraints such as insufficient class annotations, applying traditional point cloud perception methods to large-scale unlabeled data meets significant challenges.

Unsupervised learning is a powerful machine learning paradigm that enables representation learning from data without the need for manually annotated supervision signals. Multi-modal unsupervised algorithms for 3D scene perception tasks integrate the internal structure of point clouds with image or text knowledge to generate objectives. They bypass the costly manual annotations, bridging the gap between traditional 3D perceptual models and unannotated data.

Existing 3D unsupervised methods for annotation-free training(Chen et al.2023b; Zeng et al.2023; Chen et al.2023a; Peng et al.2023)aim to transfer knowledge from visual foundation models (VFMs),e.g., Contrastive Vision-Language Pre-training (CLIP)(Radford et al.2021)or Segment Anything (SAM)(Kirillov et al.2023), to point cloud representations. However, 3D unsupervised models based on CLIP(Radford et al.2021)suffer from intolerable noise, while SAM(Kirillov et al.2023)fails to correspond texts and images. Therefore, we seek high-quality image segmentation models with textual correspondences to serve as teacher models for 3D unsupervised learning. Recently, CLIP-based 2D open-vocabulary segmentation models(Zhou, Loy, and Dai2022; Yu et al.2023; Xu et al.2023; Cho et al.2023)have demonstrated excellent performances. They employ contrastive learning to extract textual and image features from a shared embedding space and are capable of segmenting and identifying objects from a set of open classes in various environments. These models provide us with image segmentation and labels corresponding to the segmented regions, as well as easy-to-extract text and image representations. Meanwhile, they significantly outperform other Visual Language Models (VLMs)(Radford et al.2021; Zou et al.2024,2023; Li et al.2023)in open-vocabulary segmentation tasks, as shown in Fig.5and Appendix D.

In this paper, we propose UOV, a 3DUnsupervised framework by distilling 2DOpen-Vocabulary segmentation models. It aims to address the difficulties of point cloud perception using unannotated data through 3D unsupervised learning.
UOV adopts a novel two-stage unsupervised strategy: The first stage uses Tri-Modal contrastive Pre-training (TMP) to warm up the network parameters, where we innovatively incorporate the textual information to enhance the semantic perception of point cloud representations; The second stage is pseudo-label guided annotation-free training, completing the knowledge distillation from 2D to 3D. Furthermore, to address the perceptual limitations of UOV, such as noise during alignment and label confusion, we introduce Approximate Flat Interaction (AFI). It provides a robust error correction mechanism for the above two stages through point cloud spatial interaction.

In the experiment, We selected four CLIP-based open-vocabulary segmentation models,ie., MaskCLIP(Zhou, Loy, and Dai2022), FC-CLIP(Yu et al.2023), SAN(Xu et al.2023), CAT-Seg(Cho et al.2023), as teacher models. To validate the performance of our method in unsupervised point cloud segmentation tasks, extensive experiments on multiple autonomous driving datasets were conducted. Firstly, UOV achieved a remarkable improvement of 3.13% mIoU, reaching a Top-1 accuracy of 47.74% mIoU in benchmark testing of annotation-free 3D segmentation on nuScenes(Caesar et al.2020). Furthermore, treating UOV as a pre-trained model, we conducted 1% data fine-tuning and 100% data linear-probing experiments on nuScenes, yielding mIoU scores of 51.75% and 56.35%. In comparison to the current best pretraining method, our approach achieved improvements of 4.16% and 4.81% mIoU, respectively. When fine-tuning with 1% data on SemanticKITTI (pre-training on nuScenes), UOV achieved a 48.14% mIoU. UOV demonstrated state-of-the-art performance across various experiments, validating its effectiveness.

Compared to scene understanding work based on point clouds (such as OpenScene), the introduction of text information allows UOV to generate pseudo labels for knowledge distillation. As a result, UOV directly predict outcomes, whereas scene understanding models match features between point clouds and text. As with fully supervised closed-set point cloud segmentation models, the directly predicted output is far superior to the output obtained through feature matching.

Unlike most previous unsupervised segmentation models, all the knowledge for training UOV comes from state-of-the-art 2D open-vocabulary segmentation models. These open-vocabulary segmentation models (such as FC-CLIP, CAT-Seg, and SAN) perform far better than other VLMs like maskCLIP, SAM, and SEEM. Additionally, they can simultaneously extract masks, labels, and features, avoiding the noise accumulation between different backbones. Therefore, UOV presents significant advantages compared to previous related works.

UOV employs a novel and efficient two-stage annotation-free training framework, which comprehensively utilizes state-of-the-art 2D open-vocabulary segmentation models for knowledge distilling. It innovatively introduces TMP and AFI, addressing the issues in previous works. Moreover, we introduce the superpixel-superpoint into annotation-free 3D segmentation for the first time. Experimentally, our approach not only breaks through in annotation-free semantic segmentation (Fig.1), but also notably outperforms prior state-of-the-art methods in other downstream tasks (Fig.2).

SECTION: Related Work

SECTION: CLIP-based 2D Open-Vocabulary Segmentation

2D open-vocabulary segmentation models aim to segment all categories in the real world. Traditional open-vocabulary image segmentation models(Zhao et al.2017; Xian et al.2019; Bucher et al.2019)attempt to learn image embeddings aligned with text embeddings. Inspired by Visual Language Models (VLMs),e.g., CLIP(Radford et al.2021)and ALIGN(Jia et al.2021), which have demonstrated remarkable performance in 2D tasks, recent studies have attempted to transfer CLIP’s outstanding zero-shot segmentation capability to open-vocabulary tasks(Xu et al.2023; Cho et al.2023; Zhou, Loy, and Dai2022; Yu et al.2023; Li et al.2022a; Ghiasi et al.2022; Ding et al.2022; Xu et al.2022; Liang et al.2023). In notable works, LSeg(Li et al.2022a)learns pixel-level visual embeddings from CLIP, marking the first exploration of CLIP’s role in language-driven segmentation tasks. More recently, MaskCLIP(Zhou, Loy, and Dai2022)obtains pixel-level embeddings by modifying the CLIP image encoder; SAN(Xu et al.2023)augments CLIP with lightweight side networks to predict mask proposals and categories; CAT-Seg(Cho et al.2023)proposes a cost-aggregation-based method to optimize the image-text similarity map. Additionally; FC-CLIP(Yu et al.2023)integrates all components into a single-stage framework using a shared frozen convolutional CLIP backbone. These works utilize CLIP as the central component of their network, granting them robust segmentation and recognition capabilities, as well as an image-text-aligned structure. Consequently, they can provide us with high-quality knowledge.

SECTION: 3D Unsupervised Learning in Large-scale Point Clouds

Unsupervised learning can be utilized for learning point cloud representations. Mainstream 3D unsupervised pre-training methods are mainly reconstruction-based(Boulch et al.2023; Lin and Wang2022; Hess et al.2023; Min et al.2023), or contrast-based(Li et al.2022b; Xie et al.2020; Zhang et al.2021; Liu et al.2021; Sautier et al.2022; Mahmoud et al.2023; Liu et al.2023). However, many of these methods are constrained by the quantity of point clouds, limiting their applicability to single-object or indoor scene learning. Prior attempts, such as PointContrast(Xie et al.2020), DepthContrast(Zhang et al.2021), SegContrast(Nunes et al.2022), and PPKT(Liu et al.2021), have built contrastive objectives on large-scale point clouds. Additionally, SLidR(Sautier et al.2022)adopts a novel approach by leveraging a superpixel-superpoint correspondence for 3D-to-2D spatial alignment, showing promising performance on autonomous driving datasets. Built upon SLidR, SEAL(Liu et al.2023)employs VLMs to aid in superpixel generation.

Recently, inspired by the achievements of CLIP(Radford et al.2021), numerous works have focused on reproducing the excellent performance demonstrated by CLIP in 3D, not limited to pre-training. In 3D scene understanding, CLIP2Scene(Chen et al.2023b), similar to OpenScene(Peng et al.2023)and OV3D(Jiang, Shi, and Schiele2024), embeds the knowledge of CLIP feature space into representations of 3D point cloud, enabling annotation-free point cloud segmentation; PLA(Ding et al.2023)and RegionPLC(Yang et al.2023)accomplish scene understanding through point-language alignment or contrastive learning framework; VLM2Scenes(Liao, Li, and Ye2024)exploits the potential of VLMs; and CLIP2(Zeng et al.2023)demonstrates perfect zero-shot instance segmentation performance through language-3D alignment at the semantic level and image-3D alignment at the instance level. Unlike the others, Chenet al.(Chen et al.2023a)utilizes CLIP to generate pseudo-labels and uses SAM(Kirillov et al.2023)to assist in denoising.

SECTION: Method

SECTION: Extracting Knowledge from CLIP-based 2D Open-Vocabulary Segmentation Models

In perceptual approaches to unknown classes in 2D, unlike zero-shot learning, open-vocabulary learning uses language data as supervision. In terms of network structure, MaskCLIP changes the image encoder of CLIP to propose pixel-level representations instead of image-level. SAN proposes a side adapter network attached to a frozen CLIP encoder; CAT-Seg employs a cost-aggregation-based method to improve CLIP; FC-CLIP adds a decoder, mask generator, in-vocabulary classifier, and out-vocabulary classifier after freezing the CLIP backbone. In most cases, the mask generator operates independently of the class generator. Pixel-level features generated by the modified CLIP-based network are max-pooled for each mask, and the objective loss is computed with the text features.

We can notice that, regardless of whether the CLIP backbone is frozen, whether the CLIP network’s architecture is modified, or whether additional network structures are appended to the side or rear of the CLIP network, the essence of these CLIP-based models lies in aligning image features with text features through contrastive learning.

The aforementioned methods hold a similar view with contrastive learning for point cloud pre-training. The difference is that the latter uses image-point cloud contrastive learning(Liu et al.2021; Sautier et al.2022)(some of the work uses data augmentation for single-modal contrastive learning). Most of them use SLIC(Achanta et al.2012), SAM(Kirillov et al.2023), and SEEM(Zou et al.2024)to guide mask segmentation and choose ResNet(He et al.2016)as the image encoder, which means that mask segmentation and mask feature generation are two completely independent modules. This not only increases the training time but also makes noise easily stack up across different models. At the same time, the lack of language guidance during segmentation will lead to a more random mask granularity. Fortunately, 2D open-vocabulary segmentation models perfectly address this issue, as we can not only extract labels and image embeddings from them but also obtain segmentation with appropriate granularity.

To summarize, we extract four interrelated, synchronously generated knowledge from CLIP-based open-vocabulary segmentation models: 1) images’ segmentations as masksfrom image set; 2) corresponding labelsfor; 3) image features corresponding to, denoted as; and 4) text features. Each of the above knowledge will play an important role in the following sections, as shown in Fig.3.

SECTION: Baseline of UOV

Given a point cloud, whererepresents the 3D coordinates of a point,denotes the point’s features.

are the labels ofandrepresents the images captured by a synchronized camera at the same moment. In contrast to supervised methods, our task does not utilize labelsduring training. We choose to employ a simple way of generating pseudo-labels for point clouds with the assistance of image segmentation: With masksobtained from image setas described in the above section, we use the labelscorresponding toas the pseudo-labelfor pixels in every mask. By leveraging known sensor calibration parameters, we establish a mappingto bridge the gap between domains of point clouds and images. Pseudo-labelsfor point cloudsare generated throughand mapping. For a 3D backbonewith the learnable parameter, we trainwith pseudo-labels. Given the sparsity of point clouds, it is obvious thatis not surjective. It is important to note thatis also not injective, as the projection area of LiDAR is not entirely covered by cameras, resulting in obvious pseudo-label-blank areas in point cloud. After knowledge distillation, these untrained regions exhibit label confusion, which will be discussed in the next section.

To align open vocabularies with the stuff-classes of autonomous driving datasets, we employ a class dictionary, whererepresents the number of stuff-classes. Texts belonging to the same classare uniformly mapped to the pseudo-label corresponding to, which implies that points corresponding toandare positive samples for each other.

SECTION: Tri-Modal Contrastive Pre-training (TMP)

In this section, we introduce Tri-Modal contrastive Pre-training (TMP). TMP innovatively integrates textual information into pre-training and removes the 2D backbone through pre-generation of the features, demonstrating excellent performance in both annotation-free training and fine-tuning. The illustration of TMP can refer to Fig.3.

To the best of our knowledge, most existing 3D pre-training methods(Liu et al.2021; Sautier et al.2022; Liu et al.2023)generate masks and mask features asynchronously for 2D-3D contrastive learning, which causes noise aggregation between different backbones. In TMP, we address this issue by synchronously generating masks and features before pre-training.

We aim to incorporate text-3D contrastive learning into pretraining. For different instances of the same category (e.g., Car A and Car B), they share the same textual features but have different image features. Theoretically, 2D-3D contrastive learning provides rich features from the instance level (needed for pretraining); text-3D contrastive learning, on the other hand, offers direct semantic embeddings for the 3D backbone from the semantic level (helpful for annotation-free segmentation). So the design intention of TMP is: Can integrating textual information into pre-training accomplish multiple tasks (pretraining and unannotated segmentation)? Experiment removing text-3D contrastive learning is conducted in Appendix B.

Regions with the same semantics may be divided into multiple parts (in 2D or 3D). During contrastive learning, different parts act as ”negative samples” to each other, causing features of parts with the same semantics to be pushed apart in the feature space. This phenomenon is called ”self-conflict”. The usage of superpixels and superpoints for segmentation in SLidR(Sautier et al.2022)is inspiring, which partly alleviates the issue of ”self-conflict” caused by point-level point set partitioning (e.g.-NN), such as ”negative sample 1” in Fig.4. In Appendix B, we conduct an ablation study on the use of superpixels and superpoints, and the results demonstrate their effectiveness.

However, the ”self-conflict” caused by the mask granularity randomness of some VLMs has not been properly addressed, such as ”negative sample 2” in Fig.4. For instance, SAM(Kirillov et al.2023)might separate the car-door and car-window because SAM is unaware of the appropriate segmentation granularity (both should belong to the same class: ”car”). This issue arises due to the lack of textual assistance. When guided by the text, a 2D open-vocabulary segmentation model would treat the entire car as a whole, thereby avoiding ”self-conflict”.

Given the point cloudand images, we have generated masksand useto map the labelsof masks to. We regard the set of pixels with corresponding points in the same maskas a superpixel, while the corresponding region of the point cloud as a superpoint,andestablish a bijection and ensuring. Assuming the point cloud backbonecomes with an output head, we replacewith a trainable projection head, projecting the point cloud featureofinto a-dimensional space such that, hereandrefer to mask features and text features provided by the 2D open-vocabulary semantic segmentation models. Firstly, we apply average pooling and normalization to each group of pixel featuresguided by superpoints to extract the superpoint embeddings. Then, we normalizeas the superpixel embeddingsand consider the masks-corresponding text features as. Finally, we employ a tri-modal contrastive loss to align,.

Superpixel-guided contrastive learning operates at the object level or semantic level, rather than at the pixel or scene level. The contrastive loss betweenandis formulated as:

whereis the feature ofsuperpixel andis the feature ofsuperpoint.denotes the cosine similarity anddenotes the temperature coefficient.is the mini-batch size.

Unlike the superpixel-superpoint contrastive loss, the text-superpoint contrastive loss does not exhibit ”self-conflict” on classes of a dataset. However, to ensure the uniformity of knowledge in TMP, we retained the class dictionaryas discussed inBaseline of UOV. In downstream tasks, texts of the same classshould be considered as positive samples for each other, so treating the point cloud regions corresponding toas ”negative samples” will inadvertently cause ”self-conflict”. Therefore, for text, we utilize the text feature’s cosine similarityweighted for other texts in the same classas ”semi-positive” samples to compute:

whereis the text feature corresponding to the mask.

Tri-modal contrastive loss is calculated as:

,are weights forand.

TMP has the following advantages compared to previous pre-training methods: 1) TMP eliminates the need for image encodings during pre-training. It does not require an image backbone, which reduces the training time. 2) Addition of textual modality. Text-superpoint contrastive learning achieves semantic-level alignment, directly endowing the point cloud backbone with semantic features. 3) Synchronous generation of superpixels and. This not only controls segmentation granularity but also prevents the aggregation of noise between different backbones.

SECTION: Approximate Flat Interaction (AFI)

Through the previous two sections of UOV, we noticed three technical difficulties for annotation-free semantic segmentation that need to be solved: 1) Unprojected point cloud regions caused by differing or occluded fields of view (FoV) among devices. This directly results in the region of the point cloud outside the image FoV remains untrained for long periods, thus the untrained area suffers from serious label confusion. 2) Label noise in 3D. This arises from matching errors between cameras and LiDAR, as well as noise inherent in 2D open-vocabulary segmentation models. Therefore, we need a robust error correction mechanism for UOV.

Inspired by Point-NN(Zhang et al.2023), we propose a non-parametric network for Approximate Flat Interaction (AFI). AFI essentially expects points to interact only among points lying on approximate planes, thereby preserving labels of relatively small objects,e.g., pedestrians, and vehicles, within a broad perceptual domain. The process of AFI is formulated as:

represents the predictions inBaseline of UOV, andindicates the minimum similarity between the directions when their directional features interact., on the other hand, denotes the point cloud labels predicted by the function. Meanwhile, we can choose to assist optimization through the pseudo-labelsgenerated by 2D open-vocabulary segmentation models. A more detailed description ofis stated in Appendix A.

During downsampling, AFI passes the directional features of the sampled center point through layer-wise interactions with neighboring points, and binds the correlation between two points based on 1) whether the two points are relevant in the same direction and 2) the tightness of the relevance between correlated directions. Through four rounds of downsampling, point-to-point interactions construct a network that, apart from points at the junctions, AFI ensures the surfaces formed by points on the same network approximate planes, thus tightly controlling interactions among points.

AFI is a robust error correction mechanism for UOV. The advantages of AFI are evident. 1) Wide-sensing domain: The perception domain for the point clouds with AFI is wide and possesses strong spatial perception capabilities. 2) Detachability: The entire AFI is detachable, and the auxiliary module for 2D images within the AFI is detachable. The effectiveness of AFI can be referenced in Appendix B: Ablation Study.

SECTION: Experiments

To validate the performance of our model, multiple experiments on two large-scale autonomous driving datasets, nuScenes(Caesar et al.2020)and SemanticKITTI(Behley et al.2019; Geiger, Lenz, and Urtasun2012)were conducted, as detailed inComparison ResultsandAblation Study. In nuScenes, there are 700 scenes for training, while the validation and test set each consist of 150 scenes, comprising a total of 16 semantic segmentation classes. During pre-training, only the train set was utilized, while we validated using specific scenes separated from the train set. SemanticKITTI has 19 classes, with its 22 sequences partitioned into specific train, validation, and test sets.

We followed the training paradigm of SLidR(Sautier et al.2022), employed MinkowskiNet18(Choy, Gwak, and Savarese2019)as the 3D backbone, and used a linear combination of the cross-entropy and the Lovász loss(Berman, Triki, and Blaschko2018)as training objective in annotation-free and downstream tasks. For 2D open-vocabulary segmentation models, we employed FC-CLIP(Yu et al.2023), SAN(Xu et al.2023), CAT-Seg(Cho et al.2023)for both TMP and annotation-free training, while using MaskCLIP(Zhou, Loy, and Dai2022)as a control group. The generation of mask features and text features were synchronized with the masks and mask labels. FC-CLIP(Yu et al.2023)employed panoptic segmentation, distinguishing different instances on thing-classes. MaskCLIP(Zhou, Loy, and Dai2022), SAN(Xu et al.2023), and CAT-Seg(Cho et al.2023)utilized semantic segmentation, not distinguishing instances with the same semantics in both TMP and annotation-free training. Their mask features are selected as the average pool of pixel features in semantically identical regions. In Tri-Modal contrastive Pre-training (TMP), our network was pre-trained for 40 epochs on 4 V100 GPUs with a batch size of 4, which takes about 80 hours.
For annotation-free training inBaseline of UOVand other downstream tasks, the network was trained for 5 epochs and 30 epochs on a single V100 GPU, each task taking approximately 3 hours and a batch size of 16. On a 4090 GPU, annotation-free training for 5 epochs only took one hour.
The temperature coefficientin Eq.1,2was set to 0.07, and the optimal results achieved for Eq.4when. In Eq.5, the minimum similaritybetween the directions when their directional features interact, was set to 0.995, implying that the maximum angular disparity of two interact point features is about 5.7°. In the network structure of AFI, downsampling was performed four times, with the downsampling rate beingfor the last three times. Additional details about AFI are provided in Appendix A.

SECTION: Comparison Results

In Tab.1, we compare UOV with the most closely related works on 3D semantic segmentation using the unannotated data of nuScenes: CLIP2Scene(Chen et al.2023b)designs a semantic-driven cross-modal contrastive learning framework; Chenet al.(Chen et al.2023a)utilizes CLIP and SAM for denoising; OpenScene(Peng et al.2023)extracts 3D dense features from an open-vocabulary embedding space using multi-view fusion and 3D convolution; OV3D(Jiang, Shi, and Schiele2024)seamlessly aligning 3D point features with entity text features. The optimal result of UOV’s single-modal annotation-free segmentation reaches 47.73% mIoU, surpassing the previous best method by 3.13% mIoU. Under image assistance, it achieves 47.89% mIoU. The gap between UOV and the fully supervised same backbone is only -26.93% mIoU.

Compared to the multi-task capability of VLMs, state-of-the-art 2D open-vocabulary segmentation models demonstrate greater capability in specialized domains, as shown in the Fig.5. Selecting professional teacher models enhances the performance of student models effectively.

We compared the performance of UOV-TMP (only employing TMP) and UOV (employing both steps) against other state-of-the-art methods on multiple downstream tasks in nuScenes and SemanticKITTI (all pre-trained on nuScenes), as shown in Tab.2. All methods utilized MinkowskiNet as the 3D backbone. Most of the compared state-of-the-art methods utilize point cloud-image contrastive learning. SLidR(Sautier et al.2022)and ST-SLidR(Mahmoud et al.2023)employ superpoint-superpixel correspondence granularity; ScaLR(Puy et al.2024)scales the 2D and 3D backbones and pretraining on diverse datasets; while SEAL(Liu et al.2023), similar to VLM2Scenes(Liao, Li, and Ye2024), employs VLMs in distilling. Our approach achieved optimal results with 1% data fine-tuning on nuScenes and SemanticKITTI, reaching 51.75% mIoU and 48.14% mIoU, respectively, demonstrating a respective improvement of +21.45% mIoU and +8.64% mIoU versus random initialization. Compared to the previously best results, UOV exhibited enhancements of +4.16% mIoU and +0.77% mIoU, respectively. Remarkably, the results of the fully supervised linear probing task on nuScenes reached 56.35% mIoU, displaying an improvement of +4.81% mIoU.

SECTION: Ablation Study

We conducted a series of ablation experiments on nuScenes. The ablation targets included different teacher models, TMP, AFI,etc. The results obtained validate the effectiveness of our designs, especially TMP and AFI. Please refer to Appendix B for details of the ablation study.

SECTION: Conclusion

We propose UOV, a versatile two-stage unsupervised framework that serves for both 3D pre-training and annotation-free semantic segmentation, achieving state-of-the-art performance across multiple experiments. The key to UOV is to leverage the high-quality knowledge of 2D open-vocabulary segmentation models. Moreover, We propose Tri-Modal contrastive Pre-training (TMP) and Approximate Flat Interaction (AFI) for the first time.

We hope that our work will contribute to more in-depth research on 2D-3D transfer learning. Additionally, to the best of our knowledge, there is currently a lack of work on annotation-free training in other 3D perception tasks such as object detection, trajectory tracking, occupancy grid prediction. We expect the emergence of other annotation-free 3D perception approaches.

SECTION: References

SECTION: Appendix AA: Details of AFI

Firstly, we remove the predictions in the label-confused area. Then, we utilize pseudo-labelsgenerated inBaseline of UOVto randomly cover model-predicted labels (note that the coverage step is optional). The probabilityfor random covering pointis defined as follows:

Here, variablerepresents the horizontal distance of point,donates the ratio of the occurrence probabilities of pseudo labels and predicts labels when, andrepresents the horizontal distance when.
We setandin Eq.6.

Secondly, we need to fill large unannotated regions and eliminate noise introduced by 2D open-vocabulary segmentation models:

For a set of regional point cloudswith corresponding features, assuming the center ofis, We try to perform directional clustering of relative coordinates for pointsexcluding. Considering the limited temporal effectiveness of conventional clustering methods, a novel way is explored:

We obtain the spherical Fibonacci latticeof the sphere by the following equation, where for, the normal vector for each point on the grid is denoted as:

whereis the golden ratio. Normal vectorsis shown in Fig.6(a).

After that, we perform clusteringby calculating the cosine similarity between the relative coordinates oftoand normal vectors.

After clustering, the mean direction vector, direction feature, and correlationare computed for each normal vector, where:

wheredenotes the average function, andrepresents the correlation between each pointand the central point. It is worth noting that during the first downsampling, the correlationis the proportion of points belonging to the same direction,ie.,. However, in subsequent downsampling steps, the correlationis obtained through the interaction among point cloudand other neighbor point clouds. This implies that we need to transmit the correlationsand the mean direction vectorsalong with the coordinates and features from the previous layer.

As shown in Fig.6(b), in downsamplings, assuming the mean direction vectors and correlations for two interacting point clouds,are (,) and (,), respectively. Assuming the direction from the centerof point cloudto the centerof point cloudis the positive direction, we calculate the cosine similaritybetween the vector connecting two central points and the mean direction vectors,:

Afterward we obtain the index sets of approximate-parallel vectors in positive and negative directions:

whererepresents the minimum arccosine value of two vectors recognized as approximately parallel, which is the same as defined in Eq.5.represents the distance of, and so forth.

Through the aforementioned sets of approximate-parallel vectors, we can obtain the correlationfrom point cloudto, which is computed by multiplying two components: the correlation along the approximately parallel direction, and the correlation based on the distance:

During the last three downsamplings, for point cloud, we obtain the correlationof every surrounding pointaround a central pointthrough Eq.10,11,12,13,14,15. Among them,andutilized in Eq.10,11correspond to the computation results of pointandin the previous layer. Afterwards,is converted to a probability distributionusing softmax:

Through Eq.10,11,12,13,14,15,16, we use the mean direction vectors, correlationsand featuresgenerated before each downsampling step to assist in generating the new correlationfor each pointaround. Then we use Eq.9to obtainfromandfrom. Afterwordsacts as weights for the directional featuresand contributes to the generation of central point features (Eq.17). Throughout the entire process, we bind the correlation between two points to 1) whether they are relevant in the same direction (corresponding toin Eq.13), and 2) how close the relevance is between the correlated directions (corresponding toin Eq.14). Through four rounds of downsampling, point-to-point interactions construct an approximate flat network, thus tightly controlling feature interactions among points (Fig.6(c)).

SECTION: Neural Network Architecture

Encoder:For point cloud, we use one-hot encoding of the predict labelsin Eq.5as pseudo-featuresfor point. Within each layer of the encoder, we first employ the farthest point sampling (FPS) followed by-nearest neighbors (-NN) for downsampling. It is necessary to transmit the mean direction vectorsand correlationsfrom the previous layer’s output during downsampling. Then, using Eq.7,8,9,10,11,12,13,14,15,16, we calculate the mean direction vectors, featuresand correlationsfor each point. Next, for each central point, we use the following formula to generate the new point feature:

whereis the original point feature, andrepresents original features of points in the neighborhood. The softmax operates on the features.

Decoder:Similar to the operations in the encoder, the upsampling layer of the decoder utilizes the directional correlationsgenerated at each layer of the encoder. After selectingnearest neighbors, the impact of distance on the upsampling weights is discarded:

whererepresents the original features of points in the neighborhood.denotes the correlation betweenand neighborin direction,represents the features of corresponding points in the encoder’s same layer (Eq.17). The definition ofis consistent with Eq.10,11.

Finally, we obtain the labelsat the final layer of the decoder.

SECTION: Appendix BB: Ablation Study

For the ablation study on different setups of UOV (Tab.3), we utilized MaskCLIP(Zhou, Loy, and Dai2022)as the control group, and FC-CLIP(Yu et al.2023), CAT-Seg(Cho et al.2023), and SAN(Xu et al.2023)as the experimental groups. Several sets of experiments were conducted while controlling for irrelevant variables. Meanwhile, for a deeper understanding of the impact of certain ablation targets, we conducted separate experiments employing CAT-Seg as the teacher model, as illustrated in the Tab.5and Tab.6.

As shown in Tab.3, when conducting ablation experiments with different experimental groups, SAN consistently outperforms FC-CLIP and CAT-Seg under any configuration, achieving a 44.16% mIoU in the UOV-baseline, which is 5.16% mIoU and 5.71% mIoU higher than the latter two. We record the accuracy (Acc) of different 2D open-vocabulary segmentation models in generating point cloud pseudo-labels in Tab.4, while also providing their performance on open-vocabulary segmentation in COCO(Lin et al.2014). We believe that the reasons different teacher models affect student models performance are 1) the ability to comprehend unknown classes, which can affect the accuracy of pseudo-labels, as the number of unknown classes in the class dictionaryis far greater than that of known classes; Meanwhile, 2) the degree of alignment between image and text features, with SAN exhibiting the largest disparity, making the fitting of the objective loss Eq.1,2more challenging during TMP. It is worth noting that models trained with FC-CLIP and CAT-Seg, even after TMP and AFI, do not outperform the baseline of SAN on annotation-free segmentation. This indicates thatA Great Teacher Model is All You Need.

The class dictionary serves to bridge the datasets’ stuff-classes with open vocabularies. Directly inputting semantic classes of the dataset as prompts into 2D open-vocabulary segmentation models is inefficient, as open-vocabulary models are better suited to understand prompts with smaller semantic granularity. As observed in Tab.5, not using a class dictionary resulted in a -6.14% mIoU decrease compared to using one.

In Tab.5, employing CAT-Seg(Cho et al.2023)as the teacher model, we investigate the efficacy of TMP through two ablation experiments: 1) replacing image features with the outputs of self-supervised ResNet50(He et al.2016)backbone using MoCov2(Chen et al.2020; He et al.2020), and 2) excluding text-point contrastive loss. Results from fine-tuning with 1% data and annotation-free point cloud segmentation experiments demonstrate the superiority of TMP over both 1) and 2). Moreover, we affirm the positive impact of TMP on the task of annotation-free point cloud segmentation. As we can see from Tab.3, incorporating TMP into baselines of FC-CLIP, CAT-Seg, and SAN improved their respective scores by 2.7% mIoU, 2.47% mIoU, and 3.26% mIoU, compared to pre-training with SLidR(Sautier et al.2022).

Superpoint and superpixel can avoid self-conflict among point groups (such as-NN groups) with the same semantics. In Tab.5,represents whether superpixel-superpoint is used; otherwise, we use-NN to divide up point cloud. The experimental results show that using superpixel-superpoint leads to performance improvements of +1.58% mIoU in the annotation-free segmentation and +4.74% mIoU in the 1% fine-tuning task.

In the experiment of adding a 2D backbone in Tab.6, similar to SLidR(Sautier et al.2022), we only activated the output layer of the added 2D backbone (with other layers frozen). The results show that TMP (w/o 2D backbone) has a 25% and 39% improvement in runtime over SLidR and TMP (w/ 2D backbone). Moreover, adding a 2D network does not provide performance advantages to TMP. At the same time, the storage burden of pre-generated features is negligible (¡7GB). After considering all factors, we retained the step of pre-generating 2D features.

AFI leverages 3D spatial information to enhance the segmentation of unannotated point clouds. Employing AFI on models of FC-CLIP, CAT-Seg, and SAN reached 41.89%, 41.79%, and 46.26% mIoU, a boost of 2.89%, 3.34%, and 2.1% mIoU over their baseline (Tab.3). It is noteworthy that the combined usage of TMP and AFI results in a greater improvement compared to using either one alone. However, this improvement is not linearly additive.

It is important to note that if we treat UOV as a pre-training method, it does not utilize the validation set in either stage. The two-stage UOV outperforms the one-stage UOV-TMP for downstream tasks on nuScenes, as shown in Tab.2. The superiority arises from the second-stage annotation-free training and the downstream tasks employing the same loss function. Essentially, this is over-fitting on the nuScenes data, which is particularly effective during linear probing. However, annotation-free training weakens the model’s generalization ability, resulting in inferior performance when transferred to other datasets,e.g., downstream tasks on SemanticKITTI (pre-trained on nuScenes) compared to using only TMP.

We present additional annotation-free segmentation results in Fig.7.

SECTION: Appendix CC: Liminations

Our framework can utilize image assistance during annotation-free segmentation, which is accomplished through post-processing with pseudo labels generated by 2D open-vocabulary models. However, our image assistance offers limited improvement (an average increase of 0.5% mIoU). Moreover, this work primarily addresses annotation-free learning tasks, wherein pseudo-labels are employed during training, resulting in a relatively weak understanding of point clouds. Besides, due to the specificity of the class dictionary and AFI, it may only be suitable for segmenting outdoor point clouds.

SECTION: Appendix DD: Attached Diagrams