SECTION: Skin Cancer Detection utilizing Deep Learning: Classification of Skin Lesion Images using a Vision Transformer
Skin cancer detection still represents a major challenge in healthcare. Common detection methods can be lengthy and require human assistance which falls short in many countries. Previous research demonstrates how convolutional neural networks (CNNs) can help effectively through both automation and an accuracy that is comparable to the human level. However, despite the progress in previous decades, the precision is still limited, leading to substantial misclassifications that have a serious impact on people’s health. Hence, we employ a Vision Transformer (ViT) that has been developed in recent years based on the idea of a self-attention mechanism, specifically two configurations of a pre-trained ViT. We generally find superior metrics for classifying skin lesions after comparing them to base models such as decision tree classifier and k-nearest neighbor (KNN) classifier, as well as to CNNs and less complex ViTs. In particular, we attach greater importance to the performance around melanoma, which is the most lethal type of skin cancer. The ViT_L32 model achieves an accuracy of 91.57% and a melanoma recall of 58.54%, while ViT_L16 achieves an accuracy of 92.79% and a melanoma recall of 56.10%. This offers a potential tool for faster and more accurate diagnoses and an overall improvement for the health care sector.

SECTION: Introduction
Skin cancer is the most common group of cancer diagnosed worldwide, with over 1.5 million new patients in 2020 alone (). Since the risks of the disease vary significantly by type, it is essential to identify the correct kind of skin cancer early. Once this information is available, proper treatment can be ensured. Generally, there are three broad types including basal cell carcinoma (BCC), squamous cell carcinoma (SCC) and melanoma. While most people die from the most common types BCC and SCC, melanoma poses a much greater threat with over 60,000 deaths out of 325,000 cases globally each year. In addition, the detection process is often lengthy as it requires an appointment and the time of a doctor. Automated diagnoses may speed up this process and address increasing shortages the healthcare system is regionally suffering from, thus potentially saving more lives ().

Accurately classifying various types of skin cancers, including melanoma, is a challenging task that requires nuanced differentiation from benign lesions. The variability in skin lesions, influenced by factors such as skin type, lesion location, and individual patient characteristics, adds to the complexity. Additionally, the sheer volume of cases globally creates a pressing need for scalable and efficient diagnostic methods. Previous studies mainly concentrates on convolutional neural networks (CNN). Although these networks significantly improve the accuracy of diagnosis compared to manual diagnosis, their limitations in terms of accuracy and specificity, particularly for melanoma, remain a concern as these metrics still hold potential for improvement.() and() demonstrate the potential of CNNs for improving diagnostic precision in their publications. However, more refined approaches are needed to achieve this goal.

Therefore, we join the mission of accurately detecting skin cancer and its subtypes by employing deep learning. The Skin Cancer MNIST: HAM10000 dataset is commonly used for the classification of skin cancer images, which we also utilize in our project (;). It contains approximately 10,000 images of 7 different skin cancer types. In our approach, we enrich the number of images with the help of data augmentation. While many projects rely on CNNs, we contribute to the existing literature by using an approach that has received less attention. Specifically, we build models based on two larger configurations of a pre-trained Vision Transformer (ViT), which are trained on several million images with tens of thousands of classes. This deep learning architecture for computer vision tasks features self-attention mechanisms that have gained popularity in recent years. ViT processes images as sequences of patches and handles them like tokens in natural language processing. This can lead to the superiority of these models in image classification compared to previous approaches (). For the smallest configurations of a pre-trained ViT, an application already exists, which we use together with a CNN-based model to evaluate our models.

The implications of the findings are significant. If validated in clinical settings, ViT models could offer a fast, accurate, and accessible tool for skin cancer screening, benefiting healthcare systems worldwide, especially in regions with limited access to dermatologists. This innovation holds promise not just for skin cancer but as a model for applying advanced machine learning techniques to other diagnostic challenges in medicine.

SECTION: Related Work
There are a variety of machine learning approaches in research that aim to classify images with diseases caused by skin lesions in general. Several publications use clinical images for a CNN approach (;). The successful performance of CNN-based models is also confirmed by their superior performance in skin cancer classification compared against 58 international dermatologists ().

For the classification of images from the Skin Cancer MNIST: HAM10000 dataset in particular, there are similarly neural network-driven approaches (;). As of December 2023, the Kaggle page of the dataset contains over 475 codes (). In research, this dataset is also used frequently in combination with a CNN approach to classify the skin cancer images (;;).() achieve a highly successful result with their CNN-based approach in combination with augmentation strategies to increase the number of images and also use transfer learning methods such as Residual Neural Network (ResNet).

In addition, the newer approach from 2020 for image classification called ViT is becoming increasingly relevant in the field. Empirical evidence shows that this approach leads to better results in classification tasks compared to common methods such as CNN ().() uses in his work the configurations ViT_B16 and ViT_B32 as well as an augmentation approach to assign the images from the Skin Cancer MNIST: HAM10000 dataset to one of the seven skin cancer types.

Overall, primarily existing research mainly uses the established CNN approach for classification or the smaller pre-trained ViT configurations. Furthermore, the existing work does not explicitly assess the performance of the respective model for the relatively most lethal skin cancer type of the dataset, melanoma.

SECTION: Methodology
Our approach, as can be seen in Figure, comprises different steps, which are presented in more detail in the following sections. We start with data preparation, which includes data cleansing. The dataset is then split into different subsets, with 80% used for the training set and 10% each for the validation and test set. The training dataset is then further processed by data augmentation. The next step is model generation, where we use pre-trained ViT models, in particular ViT_L16 and ViT_L32 architectures. In the training phase, the model parameters are optimized using the prepared training set, while the performance of the model is observed using the validation dataset. Various techniques are used in this phase to ensure efficient training. Finally, the model is evaluated on the test dataset. The following section provides further details on these steps.

This machine learning image classification project uses a ViT as its fundamental method. The general functionality of a ViT is presented for the first time in a paper by() and can be seen in Figure. At the beginning, an image is divided into smaller fixed-sized areas (patches). Each of the patches represents a region of the image. The pixel values within each patch are then reduced to a single vector. This allows the image patches to be treated as sequential data. A lower dimensional linear embedding is then produced from the flattened patches. This reduces the dimensionality of the data while retaining the important features. Position embeddings are subsequently added. This provides information about the spatial arrangement of the patches, which helps the model to understand the relative positions of the different patches in an image. The sequence of patch embeddings and positional embeddings is then entered into a standard transformer encoder as published by(). This encoder is composed of several layers, which contain two important components. Firstly, the multi-head self-attention mechanisms (MSPs). This is responsible for calculating attention weights so that input sequence elements are prioritized during prediction. And secondly, the multi-layer perceptron (MLP) blocks. To ensure stability and efficiency during training, a layer normalization (LN) is applied before each MLP block to scale and center the data appropriately. An optimizer is also used to adjust the hyperparameters of the model during training. The output from the Transformer encoder is not sent to a decoder, but to an MLP-head. This is added to the model sequence and serves as an additional adaptive classification token to work as a classifier.

The two models included in this project are created in the Python environment and use the configurations ViT_L16 and ViT_L32 of the pre-trained ViT models from the open source deep learning library Keras (Chollet et al., 2015). These differ from the ViT_B16 and ViT_B32 configurations in having 1024 hidden dimensions instead of 768, as well as 4096 instead of 3072 MLP dimensions. In addition, they have 16 attention heads, i.e. 4 more, and the encoder depth is twice as large at 24. So, the two configurations used in this project result in larger models. ViT_L16 and ViT_L32 differ in that the patch size is 16×16 for the former and 32×32 for the latter (). The additional structure of both variants is identical. At the start of the model creation, a distinction is made between the two configurations. As both pursue the same goal of assigning the images to one of the seven skin cancer types, the class’s argument is set to ’7’. The size of the images is 224×224, which means that patch size can be evenly divided by this. This is a recommended specification (). The activation function is set to softmax. The respective models are initialized with pre-trained weights, these are initially trained on the basis of, which has over 14 million images with 21,843 classes and is fine-tuned with the help of, which has 1 million images with 1,000 classes (). In addition, we specify in the function that the basic framework of the model, i.e. all layers except the last classification layer, is loaded with pre-trained weights and that the last classification layer is omitted. This is useful so that we can add our own user-defined classification layer. These two basic ViT models described serve as the first part of a sequential extension. The respective ViT model is followed by a flatten layer, which lowers the input from a 3D tensor to a 1D tensor. The flatten layer is followed by a batch normalization layer to normalize and stabilize the activations of the network. It is further extended by a dense layer (fully connected) with 28 neurons. The scientifically established non-linear rectified linear unit (ReLU) is applied as the activation function (). Its definition is as follows:

Again, a batch normalization layer is added, followed by a dropout layer and the final dense layer that has 7 (skin cancer types) neurons and the activation function softmax to facilitate the interpretation of the output values as class probabilities, as this is a crucial aspect in multi-class classification tasks (). The softmax function for this problem is designed according to the following equation:

When compiling the respective model, the optimizer is stochastic gradient descent (SGD). Since this is a multi-class classification problem, we use categorical cross entropy as the loss (). Three callbacks are integrated for the training of the respective neural network. The first is the early stopping, which stops the training if the validation loss during training does not improve for five consecutive epochs. This has the great advantage that it helps with overfitting, especially when using data augmentation (). Also included is an option to save the weights of the model whenever they contribute to an improvement in validation accuracy. The third callback modifies the learning rate if it is on a plateau during training and the validation loss does not improve by three consecutive epochs. Finally, the number of epochs for training is set to 20 and the number of batches to be processed in a training and validation epoch is 16 in both cases.

The performance evaluation focuses on the accuracy metric, measuring the proportion of correctly classified observations by the model. The accuracy for the respective trained model is calculated using the test data and is compared to a decision tree classifier (DTC) and a k-nearest-neighbor (KNN) classifier, which serve as base models. Due to the high number of CNN approaches for this problem, we compare our results only to the paper by(), which contains the highest performing approach among the considered works. In addition, the results of the ViT with the smaller configurations of() are also used for comparison. Here, however, it should be noted that we use different data for the test and validation set; this does not apply to the project by(). By doing so, we increase the reliability of performance across different samples (). Since our project, in contrast to the works we use for comparison, also focuses on how the best accuracy models perform for the relatively deadliest form of skin cancer, melanoma, we use in a second evaluation the metric recall (sensitivity) to assess this. We evaluate the recall of melanoma predictions by comparing the number of true positive cases to the total number of actual cases, which includes both true positive and false negative cases (another skin cancer type is predicted but it is in fact melanoma) (). We believe this is a crucial addition because a misdiagnosis that fails to identify melanoma when it is present carries more weight than a diagnosis that identifies melanoma when it is not present. This is because the first variant harbors a significantly higher mortality risk.

SECTION: Experiment
Before delving directly into the main analysis, we provide an exploratory data analysis (EDA) to get an understanding of the data used. The Skin Cancer MNIST: HAM10000 dataset (source) serves as the basis for our analysis and comprises a large (N=10,015) collection of dermatoscopic images as well as additional demographic features of patients (;). This dataset is accessed directly from Kaggle using an application programming interface ().
After removing missing values (e.g., unknown gender), we end up with 9,948 observations of which 54% are male (Appendix A: Figure). Figureillustrates the age distribution which looks approximately normal with the male group being around five years older on average and an overall mean of 52 which makes sense since people at that age are more likely to suffer from skin cancer. However, the most striking aspect in Figureis the wide age range emphasizing the importance for society as a whole. Not only adults and older people but also children and babies are affected, even though in fewer cases.

In total, we have seven labels for our response variable indicating the type of skin lesion. We clearly see that most instances are benign (Appendix A: Figure) while males and females appear to be equally affected by each type (Appendix A: Figure). In contrast to the population where BCC is the most prevalent type, it is underrepresented in the data whereas melanoma is far overrepresented. Hence, applying a model that has been trained on these data may lead to errors on new unseen data that is more similar to its population. On the other hand, the data confirms our expectations that the likelihood of malignant skin cancer increases with age (Appendix A: Figure). Lastly, the most common localization area is the back, especially for men (Appendix A: Figure; Appendix A: Figure) followed by lower extremity and trunk. These areas on the body occur quite consistently across gender and age groups (Appendix A:; Appendix A:).

Once the dataset has been cleansed and the EDA created, the unique images are filtered. This procedure prevents identical images from being in different sets. Afterwards, during the data split, 985 observations are assigned to the test set (10% of the total data), 987 are allocated to the validation set (10%) and 7976 to the training set (80%) prior to the application of data augmentation. This is in line with common practice (). We use data augmentation to address the uneven distribution of skin cancer types in our training dataset. We create additional images for the six lesser-represented classes, aside from melanocytic nevi. This strategy enriches the dataset and potentially minimizes overfitting risks with the original data. However, we acknowledge possible overfitting concerns for the augmented data (). The augmentation techniques that are used in this study include random rotations of up to 180 degrees, horizontal and vertical shifts, rotation and shearing transformations, random brightness adjustments, and zoom. Additionally, boundary outliers are filled using nearest points. These augmentation techniques are employed to ensure a more balanced class distribution, mitigating the dominance of melanocytic nevi.

The relevant evaluation metric for the models is accuracy, which are shown in Tablefor the respective models. The calculation of this metric involves looking at the number of correct predictions, i.e. true positives (TP) and true negatives (TN), in relation to the total number of predictions, i.e. true positives (TP), true negatives (TN), false positives (FP) and false negatives (FN) and is in mathematical terms defined as follows:

In addition, the recall for cancer class melanoma is also shown in Table. This is calculated according to the following formula:

Results from(). These do not determine the recall metric for melanoma. We compute it manually. Different data for test and validation sets.

Results from(). These do not determine the recall metric for melanoma. We compute it manually.

The results show that the base models, DTC and KNN classifier, cannot easily solve the underlying problem in an optimal way. The respective accuracy is 61.06% for the former and 65.45% for the latter base model. The two focused models in this report, which are based on a pre-trained ViT, perform significantly better. The ViT_L32 model achieves an accuracy of 91.57% and ViT_L16 an accuracy of 92.79%.

The confusion matrix for the better performing model, ViT_L16, can be seen in Figure. Considering the diagonal axis from top left to bottom right, it can be seen that most of the classifications are on this axis. This illustrates the good performance of the model, indicating that the vast majority of skin lesions are correctly classified. However, it can be noticed that a number of melanoma cases are incorrectly classified in the most represented type melanocytic nevi. For the ViT_L32 model, the corresponding matrix is found in Appendix B: Figure.

These results of the underlying models are also better compared to those of(), which have an accuracy of 74.73% (ViT_B32) and 81.88% (ViT_B16). Looking at the recall metrics for the skin cancer type melanoma of the respective models, it can be seen that our two key models deliver better performance. The recall is 42.68% and 212.53% higher, respectively, compared to the() models.With both models we also achieve a better accuracy than the best CNN model of(). However, only the ViT_L32 manages to outperform the recall of the CNN model. In addition, a comprehensive ablation study examining different customizations of the ViT_L16 model architecture and comparing their performance metrics on the basis of accuracy can be found in the Appendix C: Table.

A crucial component of the corresponding ViT models is, as already explained, an attention mechanism with 16 heads. The result of this is illustrated using an exemplary skin cancer image in Figure. On the left is the image of the skin lesion and on the right what it looks like after applying the attention mechanism. It can be seen that the mechanism recognizes the lesion and its contours very well. This component is elementary for the superior performance of the ViT models compared to the other models.

In addition, the results of the recall should be considered in a differentiated manner, as it would be possible to obtain a ’perfect’ recall for melanoma. This would require all observations to always be predicted for the melanoma class, as this would result in the number of false negatives being equal to 0. However, this would lead to a significant deterioration, since our main metric is accuracy and we only consider recall in second place, this procedure is not optimal.

SECTION: Conclusion
Our study shows that Vision Transformers (ViT) can be more effective than traditional methods like CNNs in classifying skin cancer. The main findings indicate that our ViT models, specifically ViT_L32 and ViT_L16, outperform baseline models (DTC and KNN) and previous studies that use smaller ViT configurations or CNNs. ViT_L16 achieves an accuracy of 92.79% and a melanoma recall of 56.10%, demonstrating its potential for accurate skin cancer detection. These findings highlight the effectiveness of ViT models in capturing intricate patterns in dermatoscopic images, owing to their advanced attention mechanisms.

Nevertheless, this study has limitations. The models are trained on a dataset with uneven representation of skin cancer types, which may have affected their performance on diverse real-world datasets. Furthermore, while the recall for the most lethal skin cancer type in our models is better than most reference models, the metric is slightly below 60% in both models, meaning that over 40% with melanoma are classified as something else. Additionally, ViTs, like many deep learning models, can be considered “black-boxes”. It is crucial to understand how these models make decisions, particularly in healthcare applications. The lack of interpretability could be a significant limitation in clinical settings where comprehending the reasoning behind a diagnosis is as important as the diagnosis itself. Moreover, ViT models, particularly larger configurations such as ViT_L16 and ViT_L32, necessitate significant computational resources for training and inference. The deployment of these technologies in resource-constrained environments, such as rural clinics or developing countries, may be limited. Lastly, the use of data augmentation techniques to address class imbalance may result in overfitting, particularly if the augmented data does not accurately represent real-world variations.

However, future research could benefit from exploring ways to optimize ViT models for more balanced datasets or using techniques to mitigate class imbalance. In addition, future research could broaden its scope to identify and classify other skin diseases, not just cancer, using ViT models or conduct a long-term study to evaluate the performance and reliability of ViT models over time, and their adaptability to evolving clinical guidelines and practices.
Investigating the integration of ViT models into clinical workflows could pave the way for their practical application in the early detection of skin cancer, potentially reducing the burden on healthcare systems and improving patient outcomes. This study presents the potential for advanced machine learning techniques to be used in medical imaging, which could revolutionize the approach and management of skin cancer diagnosis. The implementation of ViT models as part of clinical decision support systems in hospitals and clinics can assist dermatologists in making more informed and accurate diagnoses. In addition, a practical application of ViT models could be the development of user-friendly mobile applications for preliminary skin lesion analysis, which would provide an accessible tool for early detection.

SECTION: References
SECTION: Appendix
SECTION: Appendix A
SECTION: Appendix B
SECTION: Appendix C
*Part of the dense layer, which comes after the pre-trained ViT,

flatten and batch normalization layer.

**Part of the callbacks for training the network.