SECTION: MotionStone: Decoupled Motion Intensity Modulation with Diffusion Transformer for Image-to-Video Generation
The image-to-video (I2V) generation is conditioned on the static image, which has been enhanced recently by the motion intensity as an additional control signal. These motion-aware models are appealing to generate diverse motion patterns, yet there lacks a reliable motion estimator for training such models on large-scale video set in the wild. Traditional metrics,, SSIM or optical flow, are hard to generalize to arbitrary videos, while, it is very tough for human annotators to label the abstract motion intensity neither. Furthermore, the motion intensity shall reveal both local object motion and global camera movement, which has not been studied before.
This paper addresses the challenge with a new motion estimator, capable of measuring the decoupled motion intensities of objects and cameras in video.
We leverage the contrastive learning on randomly paired videos and distinguish the video with greater motion intensity.
Such a paradigm is friendly for annotation and easy to scale up to achieve stable performance on motion estimation.
We then present a new I2V model, named, developed with the decoupled motion estimator.
Experimental results demonstrate the stability of the proposed motion estimator and the state-of-the-art performance ofon I2V generation.
These advantages warrant the decoupled motion estimator to serve as a general plug-in enhancer for both data processing and video generation training.

SECTION: Introduction
Image-to-Video (I2V) generation animates static images into fun creative videos which has attracted broad interests in research and industry.
The key to achieving high-quality I2V results lies in synthesizing sufficient temporal dynamics, which requires effective frame-to-frame motion modeling. Some methodsintroduce additional conditions into diffusion models,optical flow, motion trajectories, or depth maps, to better capture motion dynamics.
However, these methods require complex and hard-to-obtain control conditions as inputs, and the training data must be meticulously preprocessed for model training, preventing them from reliably generalizing to videos in the wild.

Recently, several I2V worksexplore text-based motion control and introduce motion intensity as the essential control signal on motion patterns. For example, LivePhotoand Cinemoleverage text prompts to direct motion and integrate SSIMto modulate motion intensity.
Although these motion-aware models demonstrate improved controllability in motion and enhanced generation quality, the estimation of motion intensity remains inadequate due to the discrepancy between their motion modeling strategy and human motion perception. As a result, the diffusion model is unable to accurately capture the real motion intensity in a video clip during training, which negatively impacts the convergence process.

Furthermore, as shown in Fig., motion patterns in real-world videos could be very complicated including both object motion and camera movement.
Applying traditional motion extractors, which are not specifically designed for video motion modeling, to estimate motion across entire videos, is unattainable to distinguish between different types of motion, thereby limiting precise control over motion dynamics.
A straightforward way is to learn a motion estimator to predict human perception of object and camera motion intensity in videos.

In this paper, we introduce, a general I2V diffusion model to enable decoupled modeling and control of video motion.
The core ofis the independent motion estimator, comprising a motion modeling backbone and dual heads to disentangle object and camera motion.
Specifically, we first propose a video motion annotation method, which requires human annotators to distinguish the relative motion intensity of objects and cameras in randomly selected video pairs.
Then the proposed motion estimator is trained using a contrastive learning strategy with these relatively annotated video pairs.
For the structure of the motion estimator, we employ a learnable TAdaConvas the motion feature extractor, integrating the pairwise ranking loss and MLP-based motion heads to facilitate motion disentanglement.

During the training phase of the diffusion model, we freeze the pre-trained motion estimator and use its prediction result as an additional input for noise prediction at each step. In particular, we design a decoupled motion score injection method that allows the model to discern whether each motion intensity control signal originates from the camera or the object, thus achieving decoupled motion modeling in training.
Extensive quantitative and qualitative results demonstrate thatachieves state-of-the-art performance in text-guided motion control through its decoupled motion intensity guidance and conditional injection, as shown in Fig..animates diverse real-world images across various domains, skillfully decomposing motion into object and camera components.
With its decoupled motion guidance,allows users to customize motion intensity, enabling a wide range of motion effects.

SECTION: Related Work
Image animation aims to generate controllable videos using a static image as content conditioning. Early methodsfocus on modeling motion patterns for specific object types, limiting their ability to generalize motion control to other scenarios. To capture realistic motion from real videos, some methodsuse a set of videos with various motion patterns as references, transferring these motion patterns to images within the same category. Other approaches model motion for specific scenes, such as fluidsand human hair.
Additionally, some methodsconvert the human pose into additional conditions, such as depth maps or skeletal points, to guide video generation.
Although these methods achieve continuous motion control within a specific domain, their applicability remains limited due to the restricted training data and the frequent need for side control signals. Subsequently, some generalizable I2V modelstypically train on video data based on pre-trained I2V models. However, the generated videos often exhibit limited motion diversity due to structural limitations and conditions.

Recent methodsexplore using text as a condition to control motion in video. For instance, PIAattempts to animate specific domain images using text descriptions of motion. Other methodsfurther incorporate coarse-grained motion intensity estimates to generate videos with varying intensities or speeds. However, these approaches lack alignment with human perception, leading to suboptimal results in motion control. In this work, we propose a generalizable framework that uses flexible text as a guiding condition, enabling precise motion modeling in generated videos.

The text-to-video (T2V) models have made significant progress along the emerging diffusion models. Early T2V modelsharness the strong priors of existing text-to-image (T2I) models, adapting temporal modules trained on video data to enable video generation. For instance, Tune-A-Videofine-tunes a pretrained T2I diffusion model with a temporal attention mechanism in a one-shot manner. AnimateDiffintroduces a plug-and-play motion module that integrates seamlessly into existing personalized T2I diffusion models to animate images in a similar way. However, these models rely on U-Net-based denoising networks, which have limited their performance.

Recently, some workshave shifted the denoising network from U-Net to Diffusion Transformer, inspired by DiT. Video generation models with Transformers have strong spatiotemporal modeling abilities. Powered by large-scale training data, they can generate videos with rich content and motion. CogVideoXutilizes a 3D Variational Autoencoder and an expert Transformer with adaptive LayerNorm to produce coherent, extended-duration videos from text prompts. However, while these methods allow text to control content, they limit the fine-grained control over object and camera movement.

SECTION: Method
Our method is built on a pretrained video diffusion model, consisting primarily of a diffusion transformerand a 3D VAE. We first give a brief introduction to the process of the video diffusion model in Sec., followed by presenting the overall pipeline in Sec.. In Sec., we provide a detailed explanation of motion intensity estimation, and in Sec., we propose a new scheme for injecting motion intensity.

SECTION: Preliminaries
Diffusion Transformer demonstrates superior capabilities of spatiotemporal modeling in video generation compared to U-Net architecture. In this work, we select CogVideoXas the pre-trained model. Given a video, the 3D VAE encodercompresses video frames along the spatiotemporal dimensions to obtain a latent representation, where.
After that, the forward diffusion and reverse denoising processes are performed in the latent space. During the forward phase, noise is incrementally added to the latent vectorover a total ofsteps. At each time step, the diffusion process is defined as follows:

where, andis the cumulative products of noise coefficientat each time step. For the backward pass, a diffusion model performs iterative noise reduction, guided by the text promptand time step. The objective of this stage can be formulated as:

SECTION: Overall Pipeline
The framework of our model is shown in Figure. The model takes a reference image, a text prompt, and two disentangled motion intensities predicted by a motion intensity estimator as inputs. During training, we first extract the first frame from the input video to use as a conditioning reference for generation. The trained motion intensity estimator then predicts the camera and object motion intensities of the input video, providing two motion scores that guide the video generation process. During inference, users can specify the desired motion intensities for the object and camera, if available, to customize the generated video. The model takes a latentand concatenates the first frame latent of the video along the channel dimension to guide video generation. For frames beyond the first in the video sequence, zeros are padded in place. Subsequently, the model uses a text encoder to extract the features of the text prompt, which are then concatenated with the latent and fed into the diffusion transformer. Meanwhile, the two motion intensities predicted by the motion estimator are mapped to high-dimensional embeddings by MLP, then concatenated and added to the time step. This combined representation serves as a modulation condition for the vision and text features, enabling fine-grained control over the motion of video generation.

SECTION: Motion Intensity Estimation
To achieve precise control over motion intensity, we train an independent motion estimator to predict the intensity of object and camera motion within a video. We provide a detailed explanation covering three aspects: the construction of training data, the design of the motion estimator architecture, and the training configuration.

Training a motion estimator to accurately predict video motion typically requires labeling object and camera motion intensities for each video—a highly challenging task. Due to the complexity of video motion, assigning specific scores to object and camera movement is impractical, as people find it difficult to consistently rate motion intensities. To address this issue, we develop a simple and intuitive labeling approach. Rather than assigning precise scores, annotators compare video pairs, indicating which video exhibits stronger object or camera motion. This method largely streamlines the annotation process. We construct 5,000 video pairs, with annotators labeling the relative motion intensities for object and camera motion within each pair.

The motion estimator needs to simultaneously predict both object motion and camera motion for video. Therefore, when designing the structure of the motion estimator, the first requirement is a backbone that can effectively represent the motion in the video. Based on this motion representation, two heads (an object motion head and a camera motion head) are introduced to map the representation to two corresponding motion intensities. Given that our video generation network is quite large, it is crucial to limit the overall parameters and computational cost of the additional motion estimator. To achieve this goal, we use TAdaas the backbone for video motion representation. Given an input video, the motion representation of the video can be obtained through spatiotemporal modeling with TAdaConv. This process can be formulated by the following equation:

whererepresents the extracted motion representation features andrepresents the parameters of TAdaConv. Afterward, we apply global average pooling over the spatiotemporal dimensions on the extracted features, followed by two separate heads: one for object motion scoring and another for camera motion scoring. Both heads are composed of MLPs. Each head predicts the respective scores for object and camera motion in the video. This process can be formulated as follows:

whereandrepresent the object motion score and camera motion score, respectively,represents the parameters of the object or camera motion prediction head anddenotes global average pooling.

Since the training dataset only contains relative motion comparison labels in the video pair, we design a contrastive learning approach to train the motion intensity estimator. This method helps the motion intensity estimator predict the relative magnitude of object and camera motion in video pairs. We train the motion estimator using the ranking loss.

Specifically, given a videoas input to the motion estimator, we obtain object motion scoreand camera motion scorethrough Eq.and Eq.. Since our goal is to learn the relative rank in the video pair, we introduce the pairwise ranking loss to train the motion estimator:

here we assume that the object and camera motion ofis higher than.

However, training only with the ranking loss, the predicted scores from the motion estimator tend to cluster closely together. Such an estimator can distinguish relative motion between videos but is not practically usable as it lacks sufficient differentiation. Ideally, the predicted score should reflect clear distinctions (ranging from 1 to 10).

To make the motion estimator practically applicable, we randomly sample a subset of videos from our training dataset. Using the tracking methodcombined with object masks extracted by, we calculate tracking trajectories for the object and camera motion in each video. From these trajectories, we can approximate the average motion intensity of the objectand camera. We use them as pseudo-labels of video motion to conduct regression training for the motion estimator. The regression loss of the motion estimator training can be formulated as:

We then jointly train the estimator using both the ranking loss and regression loss with pseudo-labels derived from the tracking results. The overall training loss can be defined:

wheredenotes the balancing parameter.

SECTION: Motion Condition Injection Design
After training the motion estimator, it is crucial to inject the predicted motion intensity values into the backbone network as conditions.
Due to the distinct meanings, these two motion types can’t be directly compared and combined in the same way, so we propose a decoupled injection approach during the diffusion model training.

Specifically, we use two separate MLPs to learn high-dimensional mappings for the predicted object and camera motion vectors. These vectors are then concatenated and added to, allowing two conditions to remain disentangled, and preventing ambiguity in condition injection. As shown in Figure, the input motion intensities are first mapped to high-dimensional vectors, similar to, and then processed through two MLPs with the same channel dimensions, respectively. The outputs are concatenated and added to, collectively modulating the scaling coefficients.

SECTION: Experiments
SECTION: Implementation Details
We implementusing the CogVideoXframework. Our model training is conducted on 100,000 high-quality videos collected by ourselves, utilizing 8 A100 GPUs with batch size 16. The training is performed using Supervised Fine-Tuning (SFT). For each training video, we sample 49 frames and apply center cropping and resizing to standardize their resolution to. We condition the Image-to-Video model training on the first frame of each video alongside its associated text prompt.

We conduct user studies to compare our approach with previous methods. Please refer to the appendix for detailed results. For quantitative analysis, we use the WebVID validation set, where the first frame and corresponding prompt serve as conditions to generate videos. We employ specific metrics from VBenchto evaluate the generated videos, using Background Consistency to assess temporal quality, while aesthetics and imaging quality metrics are used to evaluate the visual quality of each frame.

SECTION: Comparisons with Existing Alternatives
We comparewith several recent Image-to-Video (I2V) methods. I2VGEN-XLand AnimateAnythingare classic I2V approaches that enable video generation conditioned on a given image and text. AnimateAnything also supports coarse control over motion intensity. SVDis a widely used I2V model that employs U-Net as its denoising network. Additionally, CogVideoXis an open-source video generation model based on the Diffusion Transformer.

We conduct quantitative experiments using WebVIDvalidation dataset. Specific metrics from VBenchare selected to evaluate the experimental results. Among these, the Background Consistency metric from the CLIP Scoreeffectively reflects the generative quality in the temporal dimension of video. Meanwhile, Aesthetic Quality and Imaging Quality respectively assess the aesthetic appeal and the quality of each individual frame in the generated video. It is important to note that we do not utilize the prompt suite from VBench; instead, we only employ their evaluation procedures and models.
As shown in Table, compared to U-Net-based generative models like SVD, I2VGEN-XL, and AnimateAnything, our approach demonstrates significant improvements in both temporal consistency and visual quality of the generated videos. Even relative to our baseline, CogVideoX, our method achieves noticeable enhancements. This is largely due to our model’s integration of motion intensity prediction and decoupled conditional injection, which effectively reduces the ambiguity between the motion described in text prompts and the actual motion intensity in the generated videos. This also demonstrates that precise motion intensity control signals can help video models converge more effectively.

In Figure, we select a set of representative samples to qualitatively comparewith I2VGEN-XL, SVD, AnimateAnything, and CogVideoX. We select cases involving people, animals, natural scenes, and fast-moving scenarios. As can be seen, the identity of the subject in the videos generated by I2VGEN-XL is not well-preserved, and there are occasional discrepancies between the motion and the text prompt. SVD also appears to face issues with preserving the identity of objects, and in fast-moving scenarios, the generated video exhibits limited motion intensity (e.g., the bicycle remains stationary). Although the video frames generated by AnimateAnything are well-aligned with the input image, in most scenes, the generated video is almost static, and there are occasional interruptions from other objects that interfere with the main subject. Compared to previous methods, CogVideoX shows some improvements in motion continuity. However, it occasionally fails to align with the content of the input image and exhibits limited motion. In Example 3, it generates content that does not adhere to the physical rules of the real world.

In contrast,is capable of generating videos that align well with both the input image and text. The generated videos exhibit substantial camera and object motion, producing visually appealing shots and motion that adhere to the laws of the physical world.

SECTION: Ablation Studies
In this section, we provide detailed analyses of proposed modules. We begin with quantitative experiments on all of the proposed modules, followed by a qualitative analysis of the controllability of object and camera motion intensities.
Finally, we evaluate the motion magnitude between video pairs in the validation set. Specifically, we compare the accuracy of our motion estimator against SSIM to verify its predictive effectiveness and its ability to decouple motion.

To validate the overall effectiveness of the proposed motion estimator, we compare the quantitative performance oftrained with fixed motion intensity (set to a default value of 5) versustrained with motion intensities estimated by the motion estimator. The experiments are conducted on the WebVID validation set. As shown in Table, the quality of videos generated bywithout the motion estimator (M) shows a noticeable decline. This is due to the variability in object and camera motion within the training data, using a fixed intensity value confuses the model’s understanding of video motion dynamics.

To quantitatively demonstrate the effectiveness of the proposed decoupled injection method, we compare the performance oftrained with decoupled versus non-decoupled motion intensity injection. In the non-decoupled injection approach, object and camera motion are not specifically separated along feature channels but are instead mixed and injected together.
As shown in Table, the performance ofwith non-decoupled injection (D) is inferior to that of the decoupled injection approach. This is primarily because object and camera motion occur in different spatial dimensions; mixing them together obscures their distinct contributions, making it challenging for the model to discern each type of motion, thus complicating the training process.

We further compare our method with previous methods for modeling motion intensity, which uses inter-frame SSIM(SSIM) or feature difference(S). Models are trained following these methods; however, since neither approach can decouple object and camera motion, we use a single motion intensity guidance to train the model. As shown in Table, both methods exhibit varying degrees of performance decline. This is because neither SSIM nor inter-frame feature difference aligns well with human perception of video motion intensity. Additionally, both methods fail to decouple complex video motion dynamics, instead modeling the motion intensity of the entire scene as a coarse, unified value, which leads to inaccurate estimations.

We further evaluate the trained motion estimator and the SSIM-based motion intensity estimation method on the validation set of the manually constructed video pair dataset introduced in Sec.. Since SSIM cannot decouple object and camera motion, we use its predicted overall motion intensity as a proxy for both object and camera motion intensities. We calculate the object and camera motion scores for both videos in a video pair and compare their relative magnitudes. The obtained comparison is then compared with the manually annotated ground truth (GT). If the predicted object or camera motion relationship is correct, it is scored as 1 point. The final motion intensity relationship prediction accuracy is calculated using this method, as shown in Table. Our motion estimator achieves excellent accuracy in predicting motion relationships, surpassing SSIM-based method by 28%. This demonstrates that the trained motion estimator effectively decouples object and camera motion in videos.

As demonstrated in Sec., we represent the intensity of object and camera motion in a video as a score, reflecting the magnitude and speed of both object and camera movements in the video. We perform ablation analysis of camera and object motion intensities in Figureand Figure, respectively.
In Figure, we control for camera motion and explore the impact of intensity control on two representative types of camera movements: zoom and pan. Since camera movement often influences object motion in scenes with moving subjects, we fix the object motion intensity at 5 to highlight the effect of controlling camera movement by varying its intensity level.
From the left set of images, we observe that when the camera motion intensity level is set to 2, the rightward pan of the camera is limited. As the motion intensity level increases, the amplitude of the rightward pan significantly grows, resulting in a more substantial shift in perspective.
For the images on the right, as the camera motion intensity level increases, the degree of camera zoom-in also increases. Figuredemonstrates thatwhile maintaining the intensity and speed of object motion, is capable of customizing the intensity and speed of camera motion.
In Figure, to highlight the control over object motion intensity and speed, we do not include camera motion prompts in the text and set the camera motion intensity level to the minimum value of 1, while varying the object motion intensity level. From the two sets of images, it is clear that as the given object motion intensity level increases, the speed and intensity of object motion in the generated video both become faster and larger.

SECTION: Conclusion
In this work, we propose, a general image-to-video (I2V) generation framework that enables decoupled modeling and control of video motion.
To achieve this, we train a dedicated motion estimator that directly predicts object and camera motion intensities in line with human perception. To address the challenge that human annotators cannot directly label absolute motion intensities, we develop a novel annotation method for video pairs specifically for training the motion estimator. We design a motion estimator with a backbone for video motion representation and disentangled heads to predict object and camera motion, trained using a contrastive learning strategy. Finally, we inject the predicted motion intensities into a diffusion model, thereby improving training convergence and user customization ability. This entire pipeline demonstrates impressive performance across diverse domains and task instructions.

SECTION: References
SECTION: Implementation Details
SECTION: Details on the Training Data for Motion Estimator
SECTION: User Study on Comparisons with Existing Alternatives
SECTION: Evaluation Metrics
SECTION: Limitation
SECTION: More Experiments
SECTION: More Ablations
SECTION: More Results