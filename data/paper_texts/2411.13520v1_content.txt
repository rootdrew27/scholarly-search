SECTION: Quantum Attention for Vision Transformers in High Energy Physics

We present a novel hybrid quantum-classical vision transformer architecture incorporating quantum orthogonal neural networks (QONNs) to enhance performance and computational efficiency in high-energy physics applications. Building on advancements in quantum vision transformers, our approach addresses limitations of prior models by leveraging the inherent advantages of QONNs, including stability and efficient parameterization in high-dimensional spaces. We evaluate the proposed architecture using multi-detector jet images from CMS Open Data, focusing on the task of distinguishing quark-initiated from gluon-initiated jets. The results indicate that embedding quantum orthogonal transformations within the attention mechanism can provide robust performance while offering promising scalability for machine learning challenges associated with the upcoming High Luminosity Large Hadron Collider. This work highlights the potential of quantum-enhanced models to address the computational demands of next-generation particle physics experiments.

SECTION: 1Introduction

The anticipated launch of the High Luminosity Large Hadron Collider (HL-LHC)[1]by CERN at the end of this decade is expected to generate an unprecedented volume of data, necessitating advanced computational frameworks and strategies to handle, process, and analyze this immense dataset efficiently. Classical computing resources, while effective, face significant limitations in scaling to the data and computational demands projected by such high-dimensional tasks. Addressing this challenge, quantum machine learning (QML)[2,3]has emerged as a promising solution.

Quantum vision transformers (QViTs)[4,5,6,7]have recently been proposed as hybrid architectures that integrate quantum circuits within classical vision transformer (ViT)[8]frameworks to reduce time complexity and improve performance in machine learning tasks involving high-dimensional data. Traditional ViTs employ self-attention mechanisms[9]and multi-layer perceptrons (MLPs)[10]to learn from image data, which has shown promising results in computer vision tasks across various domains. To advance these models further, researchers have explored replacing the classical linear projection layers in the self-attention mechanisms with ansatz quantum circuits (VQCs), a strategy designed to harness quantum computation for increased efficiency in parameter optimization and feature extraction.

Our work builds on this quantum-classical hybrid framework by utilizing quantum orthogonal neural networks (QONNs)[11,12]. This modification offers a fundamental advancement by enabling inherently orthogonal transformations that provide stability and improved gradient properties in the high-dimensional data spaces characteristic of high-energy physics. The orthogonality of QONNs allows for more efficient learning and enhances model robustness, especially beneficial in contexts where data complexity and noise pose significant challenges, such as jet classification in particle physics.

To demonstrate the efficacy of our QViT model based on QONNs, we apply it to the problem of distinguishing between quark-initiated and gluon-initiated jets using multi-detector jet images from the CMS Open Data Portal[13]. Jet classification is a well-studied problem in high-energy physics due to its implications for identifying fundamental particle interactions and informing experimental designs at particle accelerators[14].

This research represents a step forward in quantum-enhanced machine learning, particularly for tasks in high-energy physics. By leveraging QONNs within a transformer architecture, we aim to advance the capabilities of QML for processing high-dimensional datasets efficiently. Our model evaluation using CMS Open Data shows that QONNs offer both efficient computation and strong classification performance, highlighting their potential for practical use in physics and other fields.

SECTION: 2Architectures and Circuits

The architecture of our Quantum Vision Transformer (QViT) leverages a hybrid quantum-classical approach, extending the traditional vision transformer by embedding quantum orthogonal neural networks (QONNs) into its key components. Unlike classical transformers, which rely on entirely classical attention mechanisms and multi-layer perceptrons (MLPs), our model incorporates quantum orthogonal layers to enhance computational efficiency and the overall performance of attention-based operations on high-dimensional data. Inspired by recent advances in quantum transformers, where parametrized quantum circuits are utilized to encode data and perform orthogonal transformations, we adopt an architecture that replaces classical fully connected layers with quantum circuits, thereby allowing us to perform attention mechanisms within the quantum space. This quantum adaptation introduces orthogonal transformations through quantum-specific circuits, such as the pyramid circuits, which facilitate stable and efficient training by preserving gradient properties.

Our architectural approach begins with patch extraction and embedding, as in the classical vision transformer, but diverges in the attention mechanism and where QONNs play a central role. In the following subsections, we detail each stage of the architecture, covering patch extraction, self-attention with quantum orthogonal layer circuits, and MLPs.

SECTION: 2.1Vision Transformers

Vision Transformers (ViTs) have redefined image classification tasks by employing an attention-based mechanism that processes images as a sequence of smaller patches rather than relying on convolutional operations. By utilizing self-attention mechanisms, ViTs efficiently capture both local and global dependencies, making them highly effective for high-dimensional datasets. Our Quantum Vision Transformer (QViT) adapts the ViT architecture by embedding Quantum Orthogonal Neural Networks (QONNs) into its attention mechanism to enhance computational efficiency and performance, particularly for jet image classification.

The input image of size(height, width, channels) is divided into a grid ofnon-overlapping patches, each of size. Each patch is flattened into a 1D vector, where. These patch vectors are linearly projected into a fixed-dimensional embedding space of size:

whereis a learnable embedding matrix. A learnable class tokenis appended to the sequence of patch embeddings to enable classification. Additionally, positional encodingsare added to retain spatial information:

The sequence of embeddings, including the class token, is processed by a stack oftransformer encoder layers. Each layer consists of two sub-layers: Multi-Head Self-Attention (MHSA) and a feed-forward network (FFN). The self-attention mechanism computes attention scores to determine the importance of each patch in the context of others:

where,, andare the query, key, and value matrices derived from the input embeddings. Positional encodings ensure that spatial relationships are preserved during this computation.

The output of the transformer encoder is a refined set of patch embeddings and the updated class token. The class token, which aggregates global information from the input patches, is used for downstream classification.

For binary classification, the class tokenis concatenated with two auxiliary jet-level features, the effective massand transverse momentum:

This combined vector is passed through a Multi-Layer Perceptron (MLP) with a final sigmoid activation to predict the binary class label:

whereandare the weights and biases of the MLP, anddenotes the sigmoid function. This setup enables effective classification of quark- and gluon-initiated jets while leveraging both image-based and auxiliary features.

SECTION: 2.2Quantum Circuits

In our Quantum Vision Transformer (QViT) architecture, the quantum circuits are specifically designed to implement orthogonal transformations that are integral to the quantum orthogonal neural networks (QONNs) used in the model. These circuits utilize Reconfigurable Beam Splitter (RBS) gates for orthogonal transformations, data loading circuits to prepare quantum states, and quantum layers structured to compute attention coefficients. This section details the various components of the quantum circuits in our model, including the RBS gates, vector loading circuits, orthogonal layer circuits, and the attention coefficient circuit.

The RBS gate is a fundamental building block in our quantum circuit, acting as a two-qubit gate with a single tunable parameter,, which controls the rotation in the two-dimensional subspace defined by the basis. The RBS gate performs a rotation given by:

This gate swaps the statesandwith amplitudesand, while acting as the identity onand. In our circuit design, a network of RBS gates is employed to achieve an orthogonal transformation matrix, forming the basis of the quantum layer in our QViT. Each RBS gate in this network has an independent angle parameter, allowing for flexible control over the transformation. In our design, the RBS gate is implemented through a decomposition involving Hadamard () gates, controlled-Z () gates, and single-qubit rotations.

&\gateH\ctrl1\gateR_y(+θ/2)\ctrl1\gateH\qw\lstick\gateH\ctrl-1\gateR_y(-θ/2)\ctrl-1\gateH\qw

To prepare classical data for processing within quantum circuits, we implement vector loading circuits that efficiently map input vectors into quantum states. Specifically, we utilize unary amplitude encoding, where each feature of the input vectoris assigned to a corresponding qubit, ensuring the vector is normalized () for amplitude-based quantum encoding. The vectoris encoded into the following quantum superposition:

which corresponds to a unary representation where each amplitude is associated with a unique computational basis state.

To achieve this, the circuit begins in the all-zero state. The first unary stateis initialized by applying angate to the first qubit. A cascade ofReconfigurable Beam Splitter (RBS) gates is then applied to progressively entangle the qubits, encoding the amplitudes ofinto the quantum state. The rotation angles for the RBS gates,, are calculated recursively as follows:

where.

The resulting circuit efficiently prepares the desired quantum statewith onlyRBS gates. An example for an 4-dimensional input vector is illustrated in Figure3, where the unary states are sequentially loaded using RBS gates. This structure ensures compatibility with subsequent quantum orthogonal transformations, while maintaining a linear circuit depth.

&\gateX\gateRBS(α_0)\qw\qw\qw\lstick\qw\ctrl-1\gateRBS(α_1)\qw\qw\lstick\qw\qw\ctrl-1\gateRBS(α_2)\qw\lstick\qw\qw\qw\ctrl-1\qw

The normalization of the input vectorensures compatibility with quantum operations, as the amplitudes must satisfy. While this constraint may seem restrictive, it does not degrade the model’s performance. Orthogonal transformations, such as those implemented by quantum layers, are norm-preserving, ensuring that the learned representation is unaffected by the normalization. Additionally, normalizing all input vectors to the same norm is analogous to standard preprocessing practices in classical machine learning, where data is scaled to a uniform range.

The orthogonal layer circuit in our model is constructed as a pyramid-like structure of RBS gates, each with tunable parameters that independently control the transformation angles. This pyramidal structure mimics a fully connected neural network layer with orthogonal weights, enabling efficient implementation of orthogonal matrices in quantum space. Each row of RBS gates in the circuit contributes to a progressively complex entanglement pattern, allowing for high-dimensional transformations with fewer parameters than classical equivalents. This orthogonal layer serves as a replacement for the linear projection layers in the classical ViT, bringing quantum computational advantages in both efficiency and resource management.

&\gateRBS(θ_1)\qw\gateRBS(θ_3)\qw\gateRBS(θ_6)\qw\lstick\ctrl-1\gateRBS(θ_2)\ctrl-1\gateRBS(θ_5)\ctrl-1\qw\lstick\qw\ctrl-1\gateRBS(θ_4)\ctrl-1\qw\qw\lstick\qw\qw\ctrl-1\qw\qw\qw

The pyramid structure of the circuit ensures efficient implementation of orthogonal transformations. For anorthogonal matrix, the circuit usesRBS gates, matching the degrees of freedom of an orthogonal matrix. Each row of RBS gates adds complexity to the entanglement, enabling high-dimensional transformations with fewer parameters compared to classical implementations.

To compute attention coefficients in our QViT model, we employ a quantum circuit that calculates the dot product between the query and key vectors using quantum orthogonal layers. This circuit leverages data loaders for both the input vectors and a single attention coefficient. Each coefficient,, is calculated by first loadingandinto the circuit, followed by the orthogonal layer. The probability of measuring the output in the stateof the first qubit corresponds to the attention score.

This approach allows the attention mechanism to be implemented as a quantum operation, with non-linearity introduced through quantum measurement. The resulting coefficients are further processed to form the attention map, enabling the model to selectively focus on relevant features across patches in the input image.

[row sep=0.01cm]\qw&\gate[3]Load|x_j⟩\qw\gate[3]\pushW\push\qw\gate[3]Load⟨x_i|\meter\qw\qw\qw\qw\qw\qw\qw\qw

SECTION: 3Experiments

In this section, we describe the dataset used, the data preprocessing steps, and the hyper-parameters chosen for training our Quantum Vision Transformer (QViT) model. We evaluated our model on a binary classification task of distinguishing between quark-initiated and gluon-initiated jets using multi-detector jet images from the CMS Open Data.

SECTION: 3.1Data

We use the dataset available through the CMS Open Data Portal[13], which is commonly used in high-energy physics for quark-gluon discrimination tasks. The dataset consists of simulated jet images derived from quantum chromodynamics (QCD) processes, with jets generated and hadronized using the PYTHIA6 Monte Carlo event generator. The dataset includes 933,206 images, each of sizepixels, across three channels that represent different components of the CMS detector:

Tracks: Represents the reconstructed trajectories of charged particles passing through the inner tracking system of the CMS detector.

Electromagnetic Calorimeter (ECAL): Captures energy deposits from electromagnetic interactions, such as those caused by photons and electrons.

Hadronic Calorimeter (HCAL): Captures energy deposits from hadronic interactions, including those caused by jets of particles containing hadrons.

Each image captures the transverse energy depositions within the CMS detector, providing spatial and energy-based information essential for distinguishing between jets. These jets are classified as either quark-initiated or gluon-initiated, with balanced classes in the dataset.

In addition to the jet image data, two jet-level auxiliary features are included to enhance classification performance:

Transverse Momentum (): Represents the component of a particle’s momentum perpendicular to the beam axis. Theof a jet is a critical feature in particle physics, as it is conserved in high-energy collisions and provides a measure of the jet’s energy within the transverse plane.

Effective Mass (): The effective mass provides an indication of the energy distribution within the jet and its originating particles.

Bothandare essential features for quark-gluon discrimination, as they provide complementary information to the spatial and energy-based patterns within the jet images.

To prepare the dataset for input to our QViT model, we perform several preprocessing steps. Initially, we extract a random subset of 50,000 samples for efficient experimentation. The dataset is then split into training (70%), validation (15%), and test (15%) sets.

The pixel intensities of the jet images across the three channels (Tracks, ECAL, HCAL) were already distributed in a well-suited range, requiring no additional normalization or scaling. As such, the image data was used directly without applying min-max scaling to preserve the natural distribution of values.

However, the two auxiliary features—the effective mass () and transverse momentum () of each jet—were normalized using min-max scaling to a range of. This scaling was performed independently for the training set, and the same transformation parameters were applied to the validation and test sets to avoid data leakage.

SECTION: 3.2Hyper-parameters

The hyper-parameters for training the QViT model were carefully selected based on a combination of empirical experimentation and established best practices in deep learning for high-energy physics applications. These hyper-parameters were fine-tuned to optimize the model’s performance while maintaining computational efficiency. The key hyper-parameters are detailed below:

Projection Dimension: Each patch embedding has a dimension of 8, as larger circuit simulations would have significantly increased training time due to the exponential growth in computational cost.

Number of Patches: Eachimage is divided into non-overlapping patches of size, resulting in 25 patches per image. While this setup preserves spatial information, it also represents a considerable computational challenge, as the simulation requiresattention circuits per self-attention block, which already pushes the computational limits of current quantum simulators.

Transformer Encoder Blocks: A single transformer encoder block is used to perform feature extraction.

Attention Heads per Block: A single attention head per block is used, ensuring a simple yet effective self-attention mechanism capable of capturing dependencies across patches without overwhelming computational resources.

Dropout Rate: 0.5, applied to mitigate overfitting by randomly dropping units during training.

The model was trained for 15 epochs using the Adam optimizer with a learning rate ofand a batch size of 32, employing binary cross-entropy as the loss function due to the binary nature of the classification task. Training metrics included accuracy and the area under the receiver operating characteristic curve (ROC AUC). This setup allowed for a comprehensive evaluation of the QViT model’s capability to distinguish between quark-initiated and gluon-initiated jets.

SECTION: 4Results

This section compares the performance of the Quantum Vision Transformer (QViT) and the classical Vision Transformer (ViT) on quark-gluon jet classification. The primary goal is to evaluate the effectiveness of quantum attention mechanisms relative to classical ones by analyzing validation AUC, loss across epochs, and test set metrics.

SECTION: 4.1Validation Performance

The validation AUC and loss for the QViT and classical ViT models across 15 epochs are shown in Figure7. Both models achieve similar validation AUC, converging to approximately 0.675 by the final epoch. This indicates that the quantum attention mechanism in the QViT is as effective as the classical mechanism in distinguishing quark- and gluon-initiated jets.

SECTION: 4.2Test Performance

The final test set performance of the QViT and classical ViT models is summarized in Table1. Both models show comparable results, with QViT achieving slightly lower accuracy and AUC but demonstrating similar overall performance. These results highlight the potential of quantum attention mechanisms as a scalable alternative to classical approaches.

SECTION: 5Conclusions

In this work, we introduced a Quantum Vision Transformer (QViT) model that integrates quantum orthogonal neural networks (QONNs) into the attention mechanism for the challenging task of quark-gluon jet classification in high-energy physics. By embedding quantum circuits into the attention layers, the QViT efficiently processes high-dimensional data while maintaining comparable performance to classical Vision Transformers.

Our analysis demonstrates that quantum attention mechanisms are as effective as classical ones. The QViT achieves validation and test AUC values similar to those of the classical ViT, with minimal differences in accuracy and loss. These results underscore the potential of quantum models for applications in data-intensive fields like high-energy physics, where scalability and efficiency are critical.

Future work will explore enhanced quantum circuit designs and evaluate QViT on larger datasets with more complex tasks. As quantum hardware advances, QViTs could become a practical alternative for machine learning applications, leveraging the unique properties of quantum computing to offer competitive performance with reduced computational resources.

SECTION: 6Aknowledgments

We are thankful to Marçal Comajoan Cara, Cosmos Dong, Roy Forestano, Jogi Suda Neto and Eyup Unlu for useful discussions.
This research used resources of the National Energy Research Scientific Computing Center, a DOE Office of Science User Facility supported by the Office of Science of the U.S. Department of Energy under Contract No. DE-AC02-05CH11231 using NERSC award NERSC DDR-ERCAP0025759. SG is supported in part by the U.S. Department of Energy (DOE) under Award No. DE-SC0012447. KM is supported in part by the U.S. DOE award number DE-SC0022148. KK is supported in part by US DOE DE-SC0024407. Alessandro Tessi was a participant in the 2024 Google Summer of Code.

SECTION: References