SECTION: Enhancing Imbalance Learning: A Novel Slack-Factor Fuzzy SVM Approach
SECTION: 
Machine learning has revolutionized numerous fields by enabling the development of algorithms that can learn from data and make accurate predictions. Its applications span a wide range of domains, such as matrix games, signal propagation, analyzing criminal networks, diagnosis of Alzheimer’s diseaseand many others.

Support vector machines (SVMs)are one such machine learning algorithm that designed to find an optimal margin that separates data points into different classes. This is accomplished by solving a convex quadratic programming problem, which identifies two hyperplanes that maximize the margin between the classes.
SVMs employ the principle of structural risk minimization (SRM), which aims to prevent overfitting by minimizing the squared norm of the weight vector, effectively balancing the complexity of the model and its performance on training data. However, SVMs traditionally use the hinge loss function to measure classification errors. While hinge loss is effective in maximizing the margin, it is sensitive to outliers due to its unbounded nature.
Furthermore, SVMs exhibit suboptimal performance on imbalanced datasets, where one class significantly outnumbers the other, as they tend to favor the majority class, treating minority class instances as noise. This renders SVMs inefficient for handling class imbalance, a common occurrence in real-world applications such as breast cancer diagnosis, fraud detection, and so forth.

To address imbalanced datasets, two primary methodologies exist: data-level and algorithm-level approaches. Data-level methods involve preprocessing techniques to balance class sizes, such as oversampling, undersampling, and various resampling strategies. Oversampling introduces additional instances to the minority class, while undersampling reduces instances from the majority class. Algorithm-level methods, on the other hand, modify the training algorithm without altering the dataset. Techniques in this category include boundary-shifting methods, cost-sensitive learning, threshold adjustment strategies, and scaling kernel-based methods. Cost-sensitive learning, a powerful algorithm-level approach, assigns different weights to data samples based on their class. For instance,introduced the different error cost (DEC) model, where the imbalance ratio () determines the weight of minority class samples relative to majority class samples. While the DEC model improves over standard SVM, it remains sensitive to noise and outliers.

The fuzzy support vector machine (FSVM)enhances the standard SVM framework by incorporating a fuzzy membership (FM) function, which assigns lower weights to noisy and outlier samples. This adjustment aims to mitigate the impact of such anomalies on the overall classification performance. The FM function effectively reduces the influence of data points deemed less reliable, thereby refining the decision boundary established by the SVM. In the classical FSVM-CIL, three types of tactics are applied to describe FM functions, i.e., the separation between data points and the obtained decision boundary, the separation between data points and the pre-estimated decision boundary, and the separation between data points and their own class center. The FM functions based on these three strategies have an issue of misclassification by the approximated and computed decision hyperplane. For instance, in Fig., although pointsandare equidistant from the ideal hyperplane, pointis more critical for hyperplane construction than point. To avoid misclassifying, the hyperplane is pushed left, ensuring proper separation of the majority class, which makesmore important. Furthermore, the class imbalance is another factor for the incorrect display of the importance of the samples by FM values.

To overcome these limitations, slack-factor-based fuzzy support vector machine (SFFSVM)was developed. The SFFSVM model introduces the concept of a slack factor, which is used to define a FM function. The primary objective of incorporating slack factor values in the SFFSVM model is to move the decision hyperplane, initially derived from the DEC model, towards the optimal hyperplane(as depicted in Fig.).
Higher slack factor values indicate a higher probability that the sample is either an outlier or contains noise. Consequently, such samples are assigned lower FM values to reduce their impact on the decision boundary. This approach allows the model to maintain a more robust decision surface by diminishing the influence of less reliable data points. The SFFSVM model also has a drawback while defining FM values for the majority class samples. The FM function, defined for the majority class samples, assigns high FM values to samples misclassified by the DEC model hyperplane, provided their slack factor value is less than 2 (see Eq. ()). However,is not always the optimal choice, as shiftingto the right must be done carefully to prevent the misclassification of correctly classified minority class samples.

Taking motivation from the prior research, in this paper, we propose an improved slack-factor-based fuzzy support vector machine (ISFFSVM) model, which introduces a new parameter, named as the location parameter, denoted by. The advantage of introducing the location parameteris that the number of minority class samples that will be misclassified after shifting thehyperplane depends on the parameter. Consequently, the proposed ISFFSVM model is more efficient for class imbalance learning as it attempts to correctly classify a larger number of minority samples. Moreover, Fig.demonstrates that the decision boundary of the proposed ISFFSVM model is superior to the SFFSVM model in classifying minority class samples.The key highlights of the paper can be encapsulated as follows:

We introduce the ISFFSVM, which combines slack factor-based fuzzy membership with a novel location parameter, providing a robust solution for managing severe class disparity, reducing misclassification rates, and enhancing overall model performance.

The introduction of the location parameter provides a significant advancement by constraining the DEC hyperplane’s extension, mitigating the risk of misclassifying minority class samples, and optimizing the overall decision boundary adjustment process for better handling of class imbalances.

We demonstrate that the proposed ISFFSVM outperforms baseline classifiers on diverse real-world KEEL datasets, achieving higher F1-scores, Matthews correlation coefficients (MCC), and area under the precision-recall curve (AUC-PR), effectively addressing class imbalance challenges.

The remaining sections of the paper are organized as follows: The approaches for class imbalance learning available in the literature are discussed in Section 2. The proposed method is explained in Section 3. In Section 4, experimental comparisons are thoroughly presented and Section 5 includes conclusions with future work.

SECTION: 
This section briefly explains the related work on the imbalanced datasets. Over the past few decades, numerous algorithm-level techniques have been proposed. Some of them are given below:

SECTION: 
In DECmodel, a variant of SVM, an imbalance ratio () has been introduced to deal with class imbalance problems. The minority class samples’ misclassification cost isin the DEC. It is better than SVM on the imbalanced dataset due to the involvement of the. The optimization problem of DEC is as follows:

whereand.DEC model is increasing the membership of the minority class by multiplying the corresponding slack factors with. In this way, it assigns different FM values to the majority and minority class samples. However, like SVM, DEC model is sensitive to noise and outliers as it cannot distinguish them properly.

SECTION: 
We discuss the formulation of SFFSVMin this section.
SFFSVM defines slack-factor-based FM which helps to define different FM values for the misclassified and correctly classified samples based on their importance. In this way, it reduces the impact of imbalance and noise or outliers on the FM values.

The decision boundarycan be constructed by optimizing Eq. () on the dataset, whereandrepresent the best possible outcome. After obtainingon, we determine the slack factor value of a sampleusing the hinge loss function

A slack-factor-based FM is defined below based on the observation that the likelihood of misclassifying the related sample increases with the size of the slack factor values.

Here,

andis the smoothness parameter which determines how smoothly the DEC hyperplane moves.

SFFSVM defines a new FM function based on the above slack-factor-based FM function. It uses DEC to find the slack factor values and then define FM values for majority and minority data samples.
Conveniently, suppose thatanddenote the most ideal decision hyperplane onand a decision hyperplane derived by instructing DEC on, respectively. Depending on the values ofofcan divideintoand. Note that, we apply superscriptsandto represent the majority and minority class samples, respectively. For instance,denotes the collection of minority class samples correctly classified with a slack factor greater than or equal to zero and less than one.

The following FM values are set for the minority class samples:

Eq. () depicts that the FM value for correctly classified minority class samples decreases exponentially as the slack factor value increases. At the same time, FM value for wrongly classified minority class samples is zero.

The following FM values are set for the majority class samples:

Here, the FM value for majority class samples is set to one when their slack factor is less than, and decreases exponentially when the slack factor is greater than or equal to.

Implementation of the above memberships in the DEC model can be expressed as follows:

where

SFFSVM considers the concept of slack factor, which helps to resolve the issue of assigning equal membership to points,andas depicted in Fig.. Moreover, slack-factor-based FM functions are less affected by class imbalance problems since they employ DEC to find the FM values.

SECTION: 
FSVM-CILhas an issue of misclassification by the approximated and computed decision hyperplane. For instance, in Fig., pointsandare equally far from the ideal hyperplane, however,is more crucial in constructing the hyperplane than. Furthermore, the class imbalance is another factor for the incorrect display of the importance of the samples by FM values. To overcome this issue, slack-factor-based FM function has been proposed in the literature. In SFFSVM, all the majority class samples wrongly classified by the DEC hyperplane, i.e., they are on the right side of the DEC hyperplane and with slack factor value less than, are allocated one membership value. In other words, majority class samples between the decision boundary obtained by DEC and the supporting hyperplaneare given equal membership as the majority class samples which are correctly classified by the DEC hyperplane.

In the proposed ISFFSVM model, we introduce a new parameter, given in Eq. (), referred to as the location parameter. Unlike the SFFSVM model, we assign a membership value of one only to those majority class samples with slack factor values greater than zero and less than. For example, in Fig., consider points,, and, which have slack factor values less than(with). In contrast, pointsandhave slack factor values betweenand, inclusive. According to Eq. (), points,, andwill be assigned a membership value of one, whereas pointsandwill be assigned a membership value less than one. This approach ensures that correctly classified minority pointsandare less likely to be misclassified when shifting the DEC hyperplaneto the right, as high membership values are not assigned to samplesand. Thus, more minority samples will be correctly classified, which is our priority in class imbalance learning. However, as per the SFFSVM model,andwill also be assigned one membership value. Consequently, there is a very high chance that minority samplesandwill be misclassified which is contrary to our objective. Also, we can verify through Fig.that, in comparison to SFFSVM model, the proposed ISFFSVM model successfully classifies a larger number of minority samples.

The following DEC model is employed to obtain the FM values:

FM values set for the minority class samples are as follows:

FM values set for the majority class samples are as follows:

Implementation of the above membership function in the DEC model can be expressed as follows:

where

We are tuning the parameterin the range. It has been noted thatis not always the most suitable choice for the parameter; values other thanare appearing as the best value for. For instance, in Fig.(b), the decision boundary is drawn with. It can be seen that the decision boundary of the proposed ISFFSVM model is better as compared to SFFSVM model. The proposed ISFFSVM model misclassifies a lesser number of minority samples than SFFSVM model; consequently, it deals with class imbalance more competently than SFFSVM model.

Unlike DEC, the proposed ISFFSVM model assigns varying FM values to samples based on their significance. Additionally, unlike the SFFSVM model, it does not always use 2 as the threshold for assigning an FM value of one to majority class samples. With the introduction of the location parameter, the ISFFSVM model delivers more accurate FM values. For, the proposed ISFFSVM reduces to the baseline SFFSVM model, maintaining consistency while offering greater flexibility for improved performance whenis adjusted.

SECTION: 
This subsection delves into the mathematical foundations and theoretical advantages of the proposed ISFFSVM model, specifically focusing on the strategic incorporation of the slack factor and location parameter. The following are the key theoretical aspects that underpin the robustness and effectiveness of the ISFFSVM model:The proposed ISFFSVM model assigns a membership value of one only to majority class samples with slack factor values between zero and(i.e.,). For example, in Fig., points,, andwith slack factor values beloware assigned a membership value of one, while pointsand, with slack factor values betweenand 2 (i.e.,), receive a membership value less than one. This selective assignment allows for more precise handling of majority class samples.By ensuring that only majority class samples with slack factor values less thanreceive high membership values, the proposed ISFFSVM model better positions the DEC hyperplanewhen shifted to the right. This reduces the risk of misclassifying correctly classified minority class samples, such as pointsand(see Fig.), by not assigning high membership values to points with higher slack factor values likeand.The proposed ISFFSVM model prioritizes the correct classification of minority class samples by differentiating membership values based on the slack factor. Unlike the baseline SFFSVM model, where pointsandcould receive a membership value of one, potentially leading to misclassification of minority class samples likeand, the ISFFSVM model maintains a more accurate separation between majority and minority class samples.Fig.demonstrates the decision boundaries of both the baseline SFFSVM and the proposed ISFFSVM models. The proposed ISFFSVM model shows superior classification accuracy for minority class samples due to the nuanced assignment of membership values, which optimally positions the DEC hyperplane to favor the correct classification of minority samples.

SECTION: 
The proposed ISFFSVM introduces a novel approach to identifying and handling misclassifications, particularly false positives (FP) and false negatives (FN), which are critical in the context of imbalanced datasets. The model’s ability to differentiate between these types of errors is rooted in the innovative use of a slack-factor-based fuzzy membership function combined with a newly introduced location parameter,.

In the ISFFSVM framework, false positives occur when majority class samples are incorrectly classified as minority class samples. The model leverages the slack factor to compute fuzzy membership values, which determine the importance of each data point in influencing the decision boundary. For majority class samples, the ISFFSVM assigns fuzzy memberships based on their slack factor values and the location parameter. Majority class samples with slack factor values belowreceive a high fuzzy membership value (close to 1), while those with slack factor values greater than or equal toare assigned lower fuzzy membership values. This mechanism effectively reduces the weight of majority class samples that are more likely to be misclassified (i.e., close to the decision boundary), thereby lowering the FP rate by minimizing their influence on the decision boundary.

False negatives occur when minority class samples are incorrectly classified as majority class samples. The ISFFSVM model addresses this issue by adjusting the decision boundary to reduce the likelihood of minority class samples being misclassified. The location parameterplays a crucial role in this process. By shifting the decision hyperplane (M) based on the location parameter, the model ensures that correctly classified minority class samples are less likely to be misclassified as the decision boundary moves. Specifically, minority class samples with low slack factor values receive high fuzzy memberships, emphasizing their importance in defining the optimal decision boundary. This selective assignment of fuzzy memberships reduces the influence of majority class samples near the boundary and helps maintain a clearer separation, thereby decreasing the FN rate.

The introduction of the location parameterallows the ISFFSVM to finely tune the decision boundary’s position by adjusting the fuzzy membership values for majority class samples. Samples with slack factor values close toare carefully weighted to ensure that the decision boundary remains optimal for distinguishing between minority and majority classes. By doing so, the model reduces the chance of misclassifying critical minority class samples (reducing FN) and avoids undue influence from misclassified majority class samples (reducing FP).

Through this novel integration of the slack-factor-based fuzzy membership and the location parameter, the ISFFSVM effectively minimizes both FP and FN rates. The model dynamically adjusts the membership values to maintain a robust decision boundary that is less sensitive to noise and outliers, providing superior classification performance across various imbalanced datasets. The proposed method’s ability to differentiate and minimize both types of errors demonstrates its robustness and adaptability to real-world scenarios characterized by severe class imbalance.

This error analysis highlights how the proposed ISFFSVM effectively identifies and mitigates different types of misclassifications, thereby enhancing its overall classification performance and ensuring its suitability for imbalanced data applications.

SECTION: 
The computational complexity and scalability of the proposed ISFFSVM model is similar to that of the baseline SFFSVM model, with an additional time cost associated with tuning the location parameter. Letdenote the number of samples in the dataset andthe number of features. The computational complexity of training the ISFFSVM model, like the SFFSVM, primarily depends on the training of the SVM with the DEC formulation. Using the Bunch-Kaufman algorithm, the complexity of training DEC is bounded betweenand, whereis the number of support vectors, and typically. This gives an overall complexity of. The complexity of assigning fuzzy memberships to samples in ISFFSVM remains the same as in SFFSVM, which is. This arises from the need to evaluate each sample’s slack factor in order to compute its fuzzy membership value using the parameter. Therefore, the overall computational complexity of the proposed ISFFSVM model is, the same as the SFFSVM model. However, ISFFSVM requires an additional time cost for tuning the location parameter. This analysis highlights that the proposed ISFFSVM model maintains the same computational efficiency and scalability as the baseline SFFSVM model while offering improved performance by optimizing the decision boundary through the additional parameter.

SECTION: 
To evaluate the efficacy of the proposed ISFFSVM, we compared it with the following baseline models:Ensemble-based methods: Hashing-based Under-sampling Ensemble (HUE);Fuzzy-based methods: FSVM based on centered kernel alignment (CKA-FSVM)and Fuzzy SVM for class imbalance learning(FSVM-CIL: FSVM-CIL-exp and FSVM-CIL-lin);Other category: Different Error Cost (DEC), Complement Naive Bayes (CNB), and Synthetic Minority Oversampling Technique SVM (SMOTE-SVM);Oversampling-based category: Polynomial fit SMOTE (PF-SMOTE), SMOTE-Tomeklinksand, MWMOTE.
For the assessment of the models, we choose real-world datasets given by Knowledge Extraction Based on Evolutionary Learning (KEEL). All datasets are divided into two groups: one withand another with. The experiments are performed on a machine with Python 3.7
on the system with 2 Intel Xeon processors, 128 GB
of RAM, and 4 TB of secondary storage. The hyperparameter setting for all the models is taken the same as given in.
For the proposed ISFFSVM, the location parameteris chosen from the range.
Each dataset is divided into anratio for training and testing the models. We used the grid search method to tune
the hyperparameters via a five-fold cross-validation method. To reduce the variability of the results, we perform five-fold cross-validation ten times independently on each dataset.

SECTION: 
We select the area under the precision-recall curve (AUC-PR), the Matthews correlation coefficient (MCC), and the F1-score as the evaluation metrics, as accuracy is not suitable for assessing performance under class imbalance conditions. To define the F1-score and MCC, we first provide the formulas for precision and recall. These metrics are derived from the confusion matrix, which indicates the number of samples that are correctly or incorrectly classified by the model for each class.
The formulas for precision and recall are as follows:

where,, anddenote true positives, false positives, and false negatives, respectively.

Using these definitions, the F1-score and MCC can be computed as:

The AUC-PR measures the area under the curve formed by plotting precision against recall across different thresholds.

SECTION: 
In this subsection, we provide a detailed evaluation of the proposed ISFFSVM model against several baseline models using three metrics: F1-score, MCC, and AUC-PR. We analyze the results separately for lowand highdatasets.

The evaluation results on lowdatasets, as summarized in Tablesindicate that the proposed ISFFSVM model significantly outperforms the baseline models in terms of average performance across all three metrics. The detailed results for each of the lowdatasets, based on F1-score, MCC, and AUC-PR, are presented in Tables S.V, S.VI, and S.VII, respectively, in the supplementary file. The proposed ISFFSVM achieves an average F1-score of, surpassing the average F1-score of the two best-performing baseline models, including SFFSVM () and SMOTE-SVM () (see Table). This suggests that the ISFFSVM model maintains a better balance between precision and recall when handling moderate class imbalance. In terms of MCC, the proposed ISFFSVM achieves an average score of, surpassing the average MCC of the two best-performing baseline models, SMOTE-SVM (69.01%) and SFFSVM (68.52%) (see Table). This indicates that the proposed ISFFSVM is more effective at capturing the correlation between true and predicted labels, reducing both false positives and false negatives. Moreover, the proposed model attains the highest average AUC-PR score of, which is greater than that of baseline models like SFFSVM () and SMOTE-SVM () (see Table). These results highlight that the introduction of the location parameter and the slack-factor-based fuzzy memberships in the proposed ISFFSVM model leads to better discrimination capability and overall performance on lowdatasets.

The average performance evaluation on highdatasets, as presented in Table, further confirms the superiority of the proposed ISFFSVM model over the baseline models. The detailed results for each of the highdatasets, evaluated using F1-score, MCC, and AUC-PR, are provided in Tables S.VIII, S.IX, and S.X, respectively, in the supplementary file. The average F1-score of ISFFSVM is, which is higher than the average F1-scores of baseline models like SFFSVM () and SMOTE-SVM () (see Table). This demonstrates that ISFFSVM effectively handles severe class imbalance, maintaining better performance in comparison to baseline models. Similarly, the proposed model achieves the highest average MCC score of, which is greater than that of SFFSVM () and SMOTE-SVM () (see Table). The superior MCC score indicates that ISFFSVM establishes a robust decision boundary that reduces both false positives and false negatives. Additionally, the proposed ISFFSVM model achieves the highest average AUC-PR score of, outperforming the best two baseline models like SFFSVM () and SMOTE-SVM () (see Table). The high AUC-PR score highlights ISFFSVM’s strong capability to maintain a good balance between precision and recall, even in the presence of a highly skewed class distribution.

Overall, the proposed ISFFSVM model leverages its unique slack-factor-based fuzzy memberships and location parameter to achieve superior performance in handling class imbalance. It outperforms all the baseline models, including SFFSVM and SMOTE-SVM, in terms of all three metrics on both low and highdatasets, making it a robust and efficient solution for imbalanced classification problems.

To further validate the effectiveness of the proposed ISFFSVM model, we performed a comprehensive statistical analysis using ranking tests, the Friedman test, and the Nemenyi post hoc test. The results demonstrate that the proposed ISFFSVM model performs statistically better than the baseline models. A detailed discussion of the statistical analysis is provided in Section S.I.A of the supplementary file.

SECTION: 
To further demonstrate the effectiveness of the proposed ISFFSVM model, we conducted an evaluation using a dataset for diagnosing Schizophrenia patients. This dataset was sourced from the Center for Biomedical Research Excellence (COBRE) (available at). It consists of 72 subjects diagnosed with Schizophrenia, aged between 18 and 65 years (mean age of 38.1 ± 13.9 years), and 74 healthy control subjects, also aged 18 to 65 years (mean age of 35.8 ± 11.5 years). The features were extracted following the procedure described in. Tabledemonstrates the performance of the proposed ISFFSVM model against the baseline SFFSVM, focusing on three key metrics: F1-score, MCC, and AUC. The proposed ISFFSVM achieved an F1-score of, compared toobtained by the baseline SFFSVM. The improvement of approximately 0.94 in the F1-score demonstrates that ISFFSVM provides better precision and recall balance, particularly in managing class imbalance within the Schizophrenia dataset. Additionally, the proposed ISFFSVM model achieved an MCC of, compared tofor the SFFSVM. This increase ofsuggests that ISFFSVM delivers a more robust performance. Furthermore, the ISFFSVM model achieved an AUC of, slightly higher than the AUC ofobtained by the baseline SFFSVM. Overall, the proposed ISFFSVM model demonstrates superior performance compared to the baseline SFFSVM model across all three evaluation metrics: F1-score, MCC, and AUC. These results indicate that the proposed ISFFSVM model offers a more balanced and robust approach to handling the class imbalance in the Schizophrenia dataset, confirming its effectiveness in real-world medical diagnostic applications.

SECTION: 
To show the sensitivity of the location parameter, we plotted the F1-score corresponding to each possible value ofin the rangefor four datasets: Pima, Haberman, Yeast3, and Ecoli1 (see Fig.). The analysis reveals that the F1-score is highly dependent on the value of, with the best scores achieved at specific values for each dataset:atfor Pima,atfor Haberman,atfor Yeast3, andatfor Ecoli1. For the Pima dataset, the F1-score shows moderate fluctuations around the optimal value, indicating a moderate sensitivity to. The Haberman dataset demonstrates a steep decline in F1-score when deviating from, reflecting a high sensitivity to location parameter. In contrast, the Yeast3 dataset shows notable, though less pronounced, variability in F1-scores, highlighting the impact ofon performance. The Ecoli1 dataset displays relatively stable F1-scores with minimal variation, except for a few significant changes, suggesting lower sensitivity toin this context. Overall, this analysis underscores the critical role of carefully tuning the location parameterto maximize model performance across different datasets, as even small changes incan lead to substantial variations in F1-scores, particularly in datasets with greater class imbalance or variability.

SECTION: 
In this study, we have enhanced the existing slack-factor-based fuzzy support vector machine (SFFSVM) by introducing a novel location parameter, resulting in the improved slack-factor-based fuzzy support vector machine (ISFFSVM). In the SFFSVM model, misclassified majority class samples with slack factor values less thanare assigned a uniform membership value. However, the value ofis not always optimal, as it may lead to the misclassification of correctly classified minority samples when shifting the DEC hyperplane (). The incorporation of the location parameter addresses this issue by optimizing the decision boundary adjustment, preventing the DEC hyperplane from extending beyond a specific threshold and thereby mitigating the risk of misclassifying minority class samples. Extensive experimentation on diverse real-world KEEL datasets demonstrates that the proposed ISFFSVM consistently outperforms baseline models in terms of F1-score, MCC, and AUC-PR.

Despite these improvements, the proposed ISFFSVM model, being a variant of SVM, faces challenges in handling large-scale imbalanced datasets due to its inherent computational complexity. Future research can focus on integrating the ISFFSVM model with algorithms designed to efficiently manage large-scale data. Additionally, researchers could focus on developing adaptive methods to dynamically and efficiently adjust the location parameterduring the training process, thereby eliminating the need for manual tuning. Such methods could leverage meta-heuristic optimization algorithms or self-adaptive mechanisms to automate the adjustment process.

SECTION: Acknowledgment
This work is supported by the Science and Engineering Research Board (SERB), Government of India, through the Mathematical Research Impact-Centric Support (MATRICS) scheme under grant MTR/2021/000787. Mushir Akhtar acknowledges the Council of Scientific and Industrial Research (CSIR), New Delhi, for providing fellowship for his research under grant number 09/1022(13849)/2022-EMR-I.

SECTION: References