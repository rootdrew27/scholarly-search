SECTION: Enhanced anomaly detection in well log data through the application of ensemble GANs

Although generative adversarial networks (GANs) have shown significant success in modeling data distributions for image datasets, their application to structured or tabular data, such as well logs, remains relatively underexplored. This study extends the ensemble GANs (EGANs) framework to capture the distribution of well log data and detect anomalies that fall outside of these distributions. The proposed approach compares the performance of traditional methods, such as Gaussian mixture models (GMMs), with EGANs in detecting anomalies outside the expected data distributions. For the gamma ray (GR) dataset, EGANs achieved a precision of 0.62 and F1 score of 0.76, outperforming GMM’s precision of 0.38 and F1 score of 0.54. Similarly, for travel time (DT), EGANs achieved a precision of 0.70 and F1 score of 0.79, surpassing GMM’s 0.56 and 0.71. In the neutron porosity (NPHI) dataset, EGANs recorded a precision of 0.53 and F1 score of 0.68, outshining GMM’s 0.47 and 0.61. For the bulk density (RHOB) dataset, EGANs achieved a precision of 0.52 and an F1 score of 0.67, slightly outperforming GMM, which yielded a precision of 0.50 and an F1 score of 0.65. This work’s novelty lies in applying EGANs for well log data analysis, showcasing their ability to learn data patterns and identify anomalies that deviate from them. This approach offers more reliable anomaly detection compared to traditional methods like GMM. The findings highlight the potential of EGANs in enhancing anomaly detection for well log data, delivering significant implications for optimizing drilling strategies and reservoir management through more accurate, data-driven insights into subsurface characterization.

SECTION: Keywords

Well log data, Ensemble GANs, Anomaly detection, Gaussian mixture model, Reservoir management.

SECTION: 1Introduction

Well log data, such as gamma ray (GR), bulk density (RHOB), sonic travel time (DT), neutron porosity (NPHI), and deep resistivity (ILD), are fundamental to understanding subsurface geological formations[10,20,23,25,13,34]. Accurate interpretation of these datasets is vital for efficient reservoir management, directly influencing drilling and production decisions[27]. Proper modeling of geophysical data and the ability to detect anomalies are significant to optimizing reservoir management, with significant implications for both financial and operational outcomes. The accurate prediction of reservoir properties and anomaly detection in well log data can drive optimized drilling strategies and improve resource recovery[2,28,33]

Anomaly detection in well log data plays a key role in identifying deviations that might indicate potential issues or opportunities for elevated resource extraction. Traditional approaches, such as Gaussian mixture models (GMMs), have been extensively employed for anomaly detection tasks in various domains due to their ability to model complex data distributions[24]. GMMs are probabilistic models that assume data points are generated from a mixture of Gaussian distributions. Although GMMs are effective for simple datasets, they often face challenges in dealing with the complexity and high dimensionality typical of well log data[12]. The limitations of GMMs include an inability to capture intricate data structures and a tendency to oversimplify distributions, leading to less effective anomaly detection[37].

Generative adversarial networks (GANs) have shown promise in the generation of synthetic data, imputing missing values, and anomaly detection in structured/ tabular data like well logs[18,36,8,3,4,5]. Common challenges such as mode collapse and limited applicability to structured data have been reported[1,7]. To overcome these issues, ensemble generative adversarial networks (EGANs) was introduced, which aggregate multiple GANs to improve model robustness and performance[14]. EGANs are specifically designed to mitigate mode collapse, improve stability, and capture complex data distributions more effectively than single GAN models[22].

The motivation to compare EGAN with GMM arises from the need to assess advanced anomaly detection techniques relative to traditional methods for well log data. While GMMs are well-established, they often struggle to manage the high dimensionality and complexity inherent in well log datasets, resulting in suboptimal anomaly detection. On the other hand, EGANs can offer substantial improvements by leveraging the strengths of multiple GANs to better capture complex data distributions and improve anomaly detection accuracy[3]. This study benchmarks EGAN against GMM to quantify performance improvements in metrics such as precision, recall, and F1 score, offering a comprehensive evaluation of each model’s effectiveness[32,16,19].

A comparative summary of traditional methods, GMMs, and EGANs, including their strengths and limitations, is provided in Table1.

This study aims to thoroughly compare EGANs and GMMs, focusing on improvements in precision, recall, and F1 score. it also explores the potential of integrating EGANs into artificial intelligence (AI)-driven reservoir management systems for anomaly detection and more effective resource management.

The specific objectives of this research are to:

1. Assess EGAN’s performance in modeling data distributions and identifying anomalies that deviate from these distributions in well log data.

2. Benchmark EGAN’s performance against the GMM as a traditional method.

3. Quantify the performance gains achieved by EGANs.

SECTION: 2Methodology

SECTION: 2.1Data overview and exploratory data analysis

The datasets used in this study comprise comprehensive well log data from two wells in the North Sea Dutch region: F17-04 and F15-A-01. These wells provide detailed logging data for GR, DT, NPHI, and RHOB logs, totaling 6553 data points within a depth range of 1925-2605 meters. These wells were selected due to their comprehensive logging data, which is pivotal for accurate machine learning model (MLM) training and evaluation.

K-Means clustering was applied jointly across the dimensions of the dataset to leverage the multivariate relationships between features. Various cluster numbers (e.g., 2, 4, 6, 8, 10, and 12) were evaluated based on visual patterns and metric consistency. Among these, 10 clusters were selected as they provided the optimal balance between capturing inherent patterns and avoiding over-segmentation or excessive generalization, as confirmed by exploratory analysis and visual assessments. The data was then standardized using StandardScaler before clustering. This standardization performed a Z-score normalization, which transformed the data such that each feature had a mean of 0 and a standard deviation of 1. This transformation ensures that the features are on the same scale, which is crucial for clustering performance.

After applying KMeans, the dataset was filtered for clusters 0 and 1, which exhibited relevant characteristics for the anomaly detection task. Specifically, data from clusters 0 and 1 were selected, and data from other clusters were discarded. This ensures that only the data points belonging to these two clusters are retained for further analysis.

The isolation forest (IF) algorithm was employed after the clustering process to detect anomalies within the filtered data (from clusters 0 and 1). The IF algorithm flags potential anomalies by labeling data points as either -1 (anomalies) or 1 (normal). The model was trained on the training set and evaluated on the testing set, to identify outliers that can later be refined using advanced models like GMM and EGAN.

The data was split into training and testing sets using an 80-20 split, ensuring the model was evaluated on unseen data. The results highlight the areas where anomalies occur, offering a starting point for further refinement.

The clustering results for the GR, DT, NPHI, and RHOB logs are visualized in Figure 1, where blue dots represent training samples, green dots indicate normal testing samples and red stars highlight anomalies detected by the IF algorithm. These visualizations provide insight into the clustering process and the detection of potential anomalies, illustrating the data distribution, clustering effectiveness, and the initial anomaly detection results.

SECTION: 2.2Model configuration and hyperparameter settings

Each anomaly detection model GMM and EGAN was configured with tailored hyperparameters to optimize performance. The chosen hyperparameters are important as they directly impact the models’ accuracy, efficiency, and effectiveness in detecting anomalies. Table2 outlines the specific hyperparameter settings for each model, providing transparency and facilitating reproducibility of the study’s methodology.

SECTION: 2.3Proposed models’ workflow

The overall workflow for this study involved two key models: GMM and EGANs. Figure 2 illustrates the distinct workflows used for each model. Figure 2(a) represents the anomaly detection workflow for GMM, showcasing steps such as clustering, model training, evaluation, and result visualization. Figure 2(b) presents the workflow for EGANs, highlighting the processes of setting hyperparameters, defining network architectures, training the EGAN model, and evaluating its performance.

SECTION: 2.4Technical description for each model

The GMM is a probabilistic model that posits data points originate from a blend of multiple Gaussian distributions, each characterized by unknown parameters[9]. Given a d-dimensional vector x, the probability density function of the Gaussian mixture model can be formulated as in equation 1:

In this formulation,denotes the number of mixture components, and the mixture weightsadhere to the condition

Each component densityrepresents the probability density function of a Gaussian distribution characterized by amean vectorand acovariance matrixas shown in equation 2:

GMMs are capable of approximating any continuous probability density function with high precision, provided that an adequate number of Gaussian components are used, and their means, covariances, and weights are properly adjusted[31]. This makes GMMs suitable for modeling complex data distributions, especially in lower-dimensional settings. However, when applied to high-dimensional data or data with intricate structures, GMMs may struggle to capture the full complexity of the distribution, potentially oversimplifying it. In such cases, alternative methods may be required to better model the underlying data patterns[6].

Suspendisse vel felis. Ut lorem lorem, interdum eu, tincidunt sit amet,
laoreet vitae, arcu. Aenean faucibus pede eu ante. Praesent enim elit,
rutrum at, molestie non, nonummy vel, nisl. Ut lectus eros, malesuada
sit amet, fermentum eu, sodales cursus, magna. Donec eu purus. Quisque
vehicula, urna sed ultricies auctor, pede lorem egestas dui, et
convallis elit erat sed nulla. Donec luctus. Curabitur et nunc. Aliquam
dolor odio, commodo pretium, ultricies non, pharetra in, velit. Integer
arcu est, nonummy in, fermentum faucibus, egestas vel, odio.

GANs, first proposed by Goodfellow et al. (2014), consist of two neural networks in competition: the generator (G) and the discriminator (D). Through an adversarial training process, the generator aims to produce synthetic data that resembles real data, while the discriminator aims to differentiate between real and synthetic data. This adversarial training framework progressively upgrades the generator’s capacity to generate realistic data samples[14].

The GAN architecture involves two main components, the G and the D. Both are implemented as three-layer multi-layer perceptrons (MLPs), each with distinct roles and configurations. The G generates synthetic data samples, and the D evaluates the authenticity of these samples by distinguishing real data from the synthetic data generated by G.

The adversarial training process in GANs is formulated as a minimax game, where the generator and discriminator play against each other. The value functiongoverning this game is defined as follows in equation 3:

Whererepresents the discriminator’s estimation of the probability that the inputis real data,is the generator’s output given noise,is the data distribution, andis the noise distribution.

Here,strives to maximize the probability thatmisclassifies synthetic data as real, whileaims to minimize its classification error. This adversarial training continues iteratively, improving the generator’s ability to generate data that is indistinguishable from real data.

For anomaly detection, after the GAN is trained, it is the discriminator that is typically used to detect anomalies. In the context of anomaly detection, outliers or anomalies are considered as synthetic data thatclassifies as fake (or with a low probability of being real). Thus, the discriminator is used to evaluate whether new data points are outliers by assessing whether they fit the learned distribution of real data.

SECTION: 2.5Evaluation metrics for each model configuration

To comprehensively evaluate the effectiveness of the models employed in this study, several key metrics were employed. These metrics offer insights into various facets of the model’s predictive accuracy and reliability, particularly in the context of anomaly detection in well log data. The following metrics were used:

Precision (Prec): This metric measures the ratio of true positive predictions to the total predicted positives. It is key for understanding the proportion of relevant instances among the retrieved instances as shown in equation (4)[30][29].

Recall (Rec): Also known as sensitivity, this metric measures the ratio of true positive predictions to the total number of actual positives. It helps in understanding the proportion of actual positives that the model correctly identified, as shown in equation (5).

F1 Score (F1): The F1 Score, derived as the harmonic mean of precision and recall, serves as a unified measure that addresses both false positives and false negatives. Its utility is particularly evident in scenarios with imbalanced class distributions, as illustrated in equation (6).

Accuracy (Acc): Accuracy quantifies the proportion of correctly predicted instances (both true positives and true negatives) out of all instances. It provides an overall indication of the model’s correctness, as depicted in equation (7).

These evaluation metrics are essential for a comprehensive assessment of MLM, particularly in scenarios involving anomaly detection and classification. They help understand the models’ strengths and weaknesses, guiding improvements and optimizations in future work. Table 3 summarizes the detailed descriptions and references for these metrics.

SECTION: 3Results and discussion

The comparative study evaluated two advanced MLMs, EGANs, and GMM, using well log data from two North Sea Dutch wells, focusing on GR, DT, RHOB, and NPHI logs. The objective was to assess each model’s ability to classify and detect anomalies in these datasets.

SECTION: 3.1Gaussian mixture model analysis

The GMM analysis for the GR, DT, RHOB, and NPHI logs reveals distinct patterns and insights, as shown in Figure 3 with its four-column layout.

The scatter plots in the first column illustrate the anomaly detection results using the IF algorithm, as explained in the methodology section. The IF method effectively identifies anomalies in the GR, DT, RHOB, and NPHI datasets, marking outliers in red. The distribution of normal and anomalous points shows clear clustering, with most anomalies located at the edges of the data distribution. The data distributions identified in this stage will be used for the GMM in subsequent steps, where the model will focus on modeling the structure of the data distributions and detecting anomalies that deviate from these distributions.

The second column illustrates the training stage of the GMM, where the model fits Gaussian ellipses around the data clusters for each dataset (GR, DT, RHOB, NPHI). Since the dataset is unlabeled, the GMM performs unsupervised clustering, grouping the data points based on their inherent distributions. The model fits two Gaussian components to the data, as shown by the yellow and purple ellipses, which represent the high probability regions for each cluster. These ellipses capture the underlying structure of the data, effectively distinguishing between the normal data points in each dataset. The GMM’s training process enables it to model the data distribution, with the ellipses visualizing how the model clusters the data into two groups based on the learned parameters. The resulting model will later be used to identify anomalies by detecting points that deviate from these well-defined data distributions.

The third column presents the contour plots visualizing the probability density functions (PDFs) of the data distributions learned by the GMM. These plots show the regions of higher and lower probability, with contour levels indicating the model’s certainty in different areas. The test data points are overlaid on the contour plots to highlight their position relative to the defined probability threshold. Points in higher-density regions are considered normal, while those in lower-density areas are detected as anomalies. The contour plots reveal how well the GMM captures the data structure and how the model separates the clusters, demonstrating its ability to model data distributions effectively. These visualizations also provide insights into the areas where further refinement may be needed, particularly in distinguishing between closely related data points or detecting anomalies that fall outside the primary data distributions.

The fourth column presents the testing stage of the GMM, where the test data is evaluated and visualized alongside the model’s predictions. The contour plots show the GMM’s predicted probability density, with test data points overlaid. Points are color-coded based on whether they fall above or below a defined probability threshold, indicating their position in high-density (normal) or low-density (anomalous) regions. Points above the threshold are marked in blue (normal), and those below are marked in red (anomalies). These visualizations highlight how well the GMM separates the data into distinct density regions and show the model’s ability to distinguish between normal data and anomalies. Comparing results across datasets (GR, DT, RHOB, NPHI), the DT dataset exhibits a clear concentration of anomalies in low-density regions, aligning well with the GMM’s predictions. However, datasets like GR and NPHI exhibit some misclassifications of borderline points, suggesting that further threshold adjustments may be necessary. While the GMM effectively models data distributions, further refinement is required to improve anomaly detection, especially for overlapping data.

SECTION: 3.2Ensemble generative adversarial networks analysis

The EGAN analysis for the GR, RHOB, DT, and NPHI logs, as shown in Figure 4, demonstrates the model’s ability to effectively approximate the underlying data distributions and identify anomalies.

The first column provides scatter plots illustrating anomaly detection using the IF algorithm. In these plots, normal data points are marked in blue, and anomalies are marked in red. The clustering of points shows a clear distinction between normal and anomalous data, with anomalies generally located at the edges of the data distributions. This step establishes the foundation for later stages by identifying potential outliers, which are critical for the GAN’s learning process.

In the training stage (second column), the contour plots illustrate how the GAN learns the data distribution by modeling high and low-density regions. The GAN captures the underlying structure of the data through smooth gradients and clearly defined boundaries between normal and anomalous regions. These plots show how well the GAN approximates the real data distribution, with high-density areas (normal points) shaded in deeper colors and low-density areas (potential anomalies) shaded in lighter colors.

The data distribution (third column) further validates the model’s performance by visualizing the PDFs. The contours show the model’s confidence in different regions, with dense areas representing normal data and sparse areas representing potential anomalies. Test data points are overlaid on these contours, with points inside high-density regions classified as normal, while points outside these areas are considered anomalous. This visualization allows for a clear assessment of the model’s ability to separate normal and anomalous data.

In the testing stage (fourth column), the model’s performance is assessed by overlaying test data on the predicted contour plots. Test points are color-coded based on whether they fall above or below a defined probability threshold. Points within high-density regions are marked in blue, indicating normal data, while those outside these regions are marked in red, signifying anomalies. This color-coding highlights the GAN’s ability to differentiate between normal data and outliers, providing a clear visual understanding of its anomaly detection performance.

When comparing results across datasets (GR, DT, RHOB, NPHI), the DT dataset exhibits the most distinct separation between normal and anomalous points, with a significant concentration of points in the blue (normal) regions. The GR and NPHI datasets show more overlap between normal and anomalous points, suggesting that further tuning or adjustments to the threshold are needed to improve the GAN’s performance. These results indicate that the GAN is effective at modeling well-separated data distributions but may require additional refinement to handle datasets with overlapping or closely packed data points.

SECTION: 3.3Performance comparison of EGAN and GMM for anomaly classification using labeled data

The performance of EGANs and GMM was evaluated across several datasets using key metrics—precision, recall, and F1 score. Table 4 summarizes the results for both models, showing that EGANs generally outperformed GMM in precision and F1 score, demonstrating better ability to minimize false positives. For example, in the GR dataset, EGANs achieved a higher precision (0.62) and F1 score (0.76), while GMM had a lower precision (0.38) despite high recall (0.95). Similarly, EGANs consistently showed better precision and F1 scores in other datasets such as RHOB, DT, and NPHI, where GMM generally produced more false positives.

It is important to clarify that the classification performed in this study is univariate—each variable (GR, DT, RHOB, NPHI) is analyzed independently for anomaly detection. Although the bivariate distributions and contour plots presented in the figures are useful for visualizing the learned data distributions and model behavior, they do not influence the classification process. These visualizations are meant to provide insights into how the models separate the data in two-dimensional space but are not used in the actual anomaly classification, which is based on individual variables. Thus, the classification task remains univariate, with the bivariate plots serving only as a tool for model evaluation and understanding.

For a more detailed exploration of the code, datasets, and result visualizations used in this study, please visit the following GitHub repository:https://github.com/ARhaman/EGANs-vs.GMM. This repository contains Python scripts and Jupyter notebooks for model implementation and evaluation, well log data (GR, DT, RHOB, and NPHI) used in this study, and visualizations and results from the anomaly detection analysis.

SECTION: 4Future directions

This study highlights the effectiveness of EGANs in anomaly detection for well log data. However, there are several avenues for future work that could enhance and expand the findings:

Multivariate anomaly detection:While this study focused on univariate anomaly detection, future research should explore multivariate anomaly detection to leverage the relationships between different well log features such as GR, DT, RHOB, and NPHI. This would provide a more comprehensive approach to detecting complex anomalies that depend on the interactions between multiple variables.

Enhanced GAN architectures:To address the challenges of mode collapse and enhance the stability and performance of the models, future work could incorporate Wasserstein GANs (WGANs) or conditional GANs (cGANs). These architectures are well-suited for improving the learning capacity and data generation quality, which can be critical for high-dimensional well log data.

Real-time Anomaly Detection:In the context of reservoir management, there is a need for real-time anomaly detection. Future research should explore the adaptation of the current model to work in real-time systems, continuously updating anomaly detection as new well log data streams in.

Integration with Reservoir Simulation Models:To maximize the impact of anomaly detection, integrating EGANs into reservoir simulation models can offer a more robust solution. This integration would allow for more accurate predictions and early detection of issues in the subsurface, directly influencing drilling and resource extraction decisions.

Hyperparameter optimization:For EGANs using techniques like Bayesian optimization or grid search would further enhance the model’s ability to learn from well log data. Such optimizations could potentially improve both model performance and computational efficiency.

Model interpretability:It is crucial, especially in applied geoscience fields. Future efforts should focus on developing methods to explain the decisions made by GANs and EGANs using techniques like SHAP (SHapley Additive exPlanations) or LIME (Local Interpretable Model-agnostic Explanations). This will help practitioners better understand the model’s reasoning for anomaly classification.

Exploration of other anomaly detection techniques:In addition to EGANs and GMMs, it would be valuable to explore other advanced anomaly detection models such as Autoencoders and Isolation Forests. A comparative study with these techniques can provide insights into which methods are most effective for well log data analysis under different conditions.

By pursuing these future directions, researchers can enhance the robustness, accuracy, and applicability of anomaly detection models for well log data in reservoir management, ultimately leading to better decision-making and more efficient use of subsurface resources.

SECTION: 5Conclusion

This research demonstrates the robustness of EGANs in analyzing well log data, providing an effective solution for anomaly detection outside the data distribution and forecasting in geophysics. Extensive testing across various logs highlights the superior performance of EGANs, setting a new benchmark for anomaly detection in geosciences. While this study focuses on univariate anomaly detection, future research should extend to multivariate approaches, leveraging relationships between features like GR, DT, RHOB, and NPHI. EGANs consistently outperformed GMM in precision and F1 scores, reducing false positives while maintaining a strong recall balance. In contrast, GMM, although achieving high recall, often suffered from lower precision, leading to more false positives. These findings emphasize the importance of balancing precision and recall in selecting the appropriate model for anomaly detection tasks. A key advantage of EGANs is their adaptability, allowing for modification of confidence intervals to suit various domains. This flexibility enables users to tailor model sensitivity, making EGANs a robust tool for time series analysis in diverse industrial applications. Additionally, EGANs are well-positioned for real-time anomaly detection, a promising area for future exploration in reservoir management. This work’s novelty lies in comparing GMM’s predictive capabilities with EGANs’ advanced anomaly detection, setting a new standard for well log analysis in the oil and gas industry. Future research should explore other advanced anomaly detection techniques, such as autoencoders and isolation forests, to further enhance model robustness. Additionally, hyperparameter optimization using techniques like Bayesian optimization could improve both computational efficiency and anomaly detection performance.

SECTION: Author Contributions

Abdulrahman Al-Fakih:Formal analysis, Methodology, Software, Writing – original draft, Data preparation, Code creation.A. Koeshidayatullah:Resources, Supervision, Review & editing.Tapan Mukerji:Conceptualization, Review & editing, Scientific additions.SanLinn I. Kaka:Supervision, reviewed & edited.

SECTION: Declaration of Competing Interest and Use of Generative AI

The authors affirm that they have no known competing financial interests or personal relationships that may have influenced the work presented in this paper. During the preparation of this work, the author(s) used the ChatGPT language model from OpenAI for refining grammar and enhancing text coherence in this article. After using this tool, the author(s) reviewed and edited the content as needed and take(s) full responsibility for the content of the publication.

SECTION: Data Availability

The codes and datasets used in this study are available on GitHub athttps://github.com/ARhaman/EGANs-vs.GMM. The repository includes Python scripts, Jupyter notebooks for model implementation and evaluation, well log data (GR, DT, RHOB, and NPHI), and visualizations and results from the anomaly detection analysis.

SECTION: Acknowledgements

The authors would like to express their gratitude to the College of Petroleum Engineering at KFUPM for their invaluable support in presenting this work at international conferences. Special thanks are extended to the NLOG website and Utrecht University for providing the dataset. Additionally, the authors acknowledge EAGE for the opportunity to present this work at the European Conference on the Mathematics of Geological Reservoirs (ECMOR 24), held in Oslo, Norway.

SECTION: Abbreviations

AI = Artificial Intelligence

API = Application Programming Interface

cGANs = Conditional Generative Adversarial Networks

D = Discriminator

DT = Sonic Travel Time

EM = Expectation-Maximization (the algorithm used in GMM)

EGAN = Ensemble Generative Adversarial Networks

F1 = F1 Score

FP = False Positives

FN = False Negatives

G = Generator

GAN = Generative Adversarial Networks

GMMs = Gaussian Mixture Models

GR = Gamma Ray

ILD = Deep Resistivity

IF = Isolation Forest

K-Means = Number of Clusters

LIME = Local Interpretable Model-Agnostic Explanations

MLM = Machine Learning Models

MLP = Multi-layer Perceptrons

NPHI = Neutron Porosity

NixtlaClient = A client library provided by Nixtla for interacting with their API

Prec = Precision

PDFs = Probability Density Functions

Rec = Recall

RHOB = Bulk Density

SHAP = SHapley Additive Explanations

SVM = Support Vector Machine

TN = True Negatives

TP = True Positives

Tol = Tolerance for Convergence

URL = Uniform Resource Locator

WGANs = Wasserstein GANs

SECTION: References