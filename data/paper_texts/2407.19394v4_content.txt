SECTION: Depth-Wise Convolutions in Vision Transformers for Efficient Training on Small Datasets

The Vision Transformer (ViT) leverages the Transformer’s encoder to capture global information by dividing images into patches and achieves superior performance across various computer vision tasks. However, the self-attention mechanism of ViT captures the global context from the outset, overlooking the inherent relationships between neighboring pixels in images or videos. Transformers mainly focus on global information while ignoring the fine-grained local details. Consequently, ViT lacks inductive bias during image or video dataset training. In contrast, convolutional neural networks (CNNs), with their reliance on local filters, possess an inherent inductive bias, making them more efficient and quicker to converge than ViT with less data.
In this paper, we present a lightweight Depth-Wise Convolution module as a shortcut in ViT models, bypassing entire Transformer blocks to ensure the models capture both local and global information with minimal overhead. Additionally, we introduce two architecture variants, allowing the Depth-Wise Convolution modules to be applied to multiple Transformer blocks for parameter savings, and incorporating independent parallel Depth-Wise Convolution modules with different kernels to enhance the acquisition of local information. The proposed approach significantly boosts the performance of ViT models on image classification, object detection, and instance segmentation by a large margin, especially on small datasets, as evaluated on CIFAR-10, CIFAR-100, Tiny-ImageNet and ImageNet for image classification, and COCO for object detection and instance segmentation. The source code can be accessed athttps://github.com/ZTX-100/Efficient_ViT_with_DW.

[label1]organization=Department of Electrical Engineering and Computer Science, University of Kansas,city=Lawrence,
postcode=66045,
state=KS,
country=USA\affiliation[label2]organization=Amazon,city=Palo Alto,
postcode=94301,
state=CA,
country=USA\affiliation[label3]organization=Department of Computer Science, Toronto Metropolitan University,city=Toronto,
postcode=M5B 2K3,
state=ON,
country=Canada

SECTION: 1Introduction

Transformer models have demonstrated exceptional performance in Natural Language Processing (NLP) tasks by capturing long-range relationships through attention mechanisms[1]. However, the direct application of Transformer models to vision tasks is less intuitive, as images are inherently interconnected, and pixels exhibit close relationships. Vision Transformer (ViT)[2]addresses this challenge by dividing the image into fixed-size patches, linearly embedding each patch as a token. To capture 2D relationships among image tokens, positional embedding is introduced, compensating for the loss of 2D coordinate relationships in embedded image patches. ViT includes a learnable class token to interact with image patch tokens for image classification.

Despite its success, ViT often requires substantial data and longer training times due to the attention mechanism’s computational demands. The attention mechanism calculates the dot product of embeddings for each token pair, necessitating more time to learn the inductive bias that neighboring pixels share stronger relationships. Global attention in ViTs treats all tokens equally, neglecting the fact that neighboring image patches have higher relationships. In contrast, Convolutional Neural Networks (CNNs) naturally possess inductive bias due to local filters. However, CNNs may have a lower upper bound than ViTs because of their limited global view. In essence, ViTs outperform CNNs when datasets are large enough, and training times are sufficiently long, showcasing their superior performance under such conditions.

Image contents are inherently cohesive as a whole, and forcefully splitting them into patches can hinder the recognition process. Moreover, treating all patches equally in models like Vision Transformers (ViTs) sacrifices the inductive bias present in the images, requiring a more extensive training effort to converge. While some approaches involve overlapping patches, this introduces additional computational costs without fundamentally addressing the issue. In contrast, CNN models, by their nature, excel at filtering local pixels in a contiguous manner, which is crucial for image recognition, particularly when dealing with relatively small objects. However, the lack of global views may restrict the performance of convolutional models, especially in scenarios with abundant training data. The key question becomes: How can we efficiently integrate these two approaches, leveraging convolutions to support Transformer models, ensuring rapid convergence, and achieving superior performance?

In this paper, we introduce a straightforward yet effective method to seamlessly integrate convolutional and Transformer blocks, enabling the simultaneous learning of global and local information efficiently. Our approach leverages Depth-Wise Convolutions[3]to capture local information, while Transformer blocks are employed to capture global information. The Depth-Wise Convolutions serve as a shortcut, bypassing the entire Transformer block (attention+FFN). The final combination is achieved through summation, providing a unified representation of both Depth-Wise Convolutions and Transformer blocks. The Depth-Wise Convolutions are applied for each Transformer block, creating two paths after each block for the network to choose from. This design ensures a flexible and dynamic integration of the local and global features. Our method achieves a superior performance improvement with only a marginal increase in parameters and computations, particularly benefiting small datasets. Our approach enables small-size Transformer models to outperform some larger counterparts, showcasing their effectiveness and efficiency.

In summary, our contributions are outlined below.

We propose an efficient and effective approach to combining Depth-Wise Convolutions and Transformer blocks, allowing simultaneous capture of local and global information with minimal additional parameters and computational load. The proposed lightweight Depth-Wise module bypasses entire Transformer blocks to attain fine-grained details that might be missed otherwise. This module does not alter the internal structure of MHSA and FFN, making it a plug-and-play component that can be utilized by most Transformer models. Our approach demonstrates superior performance in image classification, object detection, and instance segmentation.

We developed two types of architectural variants. The first variant aims to reduce parameters and floating-point operations (FLOPs) by utilizing the Depth-Wise module to bypass multiple Transformer blocks. The second variant seeks to improve performance by incorporating multiple independent parallel Depth-Wise modules, each dedicated to enhancing local information.

We demonstrate that certain modules are dispensable when our approach is implemented in the training of Transformer models on small datasets. Furthermore, by applying our approach without these modules, we can reduce both parameters and FLOPs, while significantly enhancing the performance.

SECTION: 2Related Work

SECTION: 2.1Vision Transformers

ViT[2]introduces the Transformer models into vision recognition by splitting the images into fixed-size patches and then tokenizing each patch into the token so that the image patches can be utilized in the attention module of Transformer models. Many variations and improvements have been proposed[4][5][6]and applied to various vision tasks such as point cloud completion[7]and crowd counting[8]. DeiT[9]employs distillation tokens for attention learning from the teacher models to the student models. CaiT[10]introduces LayerScale to effectively train the ViT models with deeper layers so that the performance of deep ViT models could be further boosted. Hierarchical Vision Transformer architecture[11][12][13][14][15]are designed to better suit vision tasks by reducing the size of feature maps as the network progresses deeper, resembling the structure of CNN architectures.

To reduce the computational cost, some window-based Vision Transformer models have been proposed. Swin Transformer[16]restricts the self-attention of the tokens on small windows so that the inductive bias could be slightly introduced while significantly reducing the computational costs with the sacrifice of the global views. To mitigate the limitation of lacking global views, it also incorporates a shifted window mechanism, expanding the self-attention calculation to new shifted windows. Thus, the views of tokens are expanded. Other works, such as[17][18][19], attempt to increase the receptive fields with cross-window interactions so that the information between the windows could be exchanged and the tokens could exchange the information with other windows.

SECTION: 2.2Vision Transformers and Convolutions

CvT[20]designs a hierarchical Transformer architecture with a convolutional token embedding and a convolutional Transformer block utilizing a convolution projection to project the feature maps into query, key, and value. BoTNet[21]replaces the final three bottleneck blocks of the ResNet model with BoT blocks that contain MHSA layers so that the self-attention layer can aggregate the information attained by the convolutional layers. LocalViT[22]introduces the Depth-Wise Convolution into the Feed-Forward Networks in the Transformer block to add locality into the Transformer models. CMT[23]proposes a hybrid Transformer model to take advantage of Transformers and CNNs for global views and local features, respectively. MobileFormer[24]designs efficient networks to integrate MobileNet[3]and Transformer blocks with a two-way bridge in between so that both local features and global interactions can be effectively communicated and fused.

DHVT[25]integrates the convolutions into MLP and patch embeddings to introduce the inductive bias into the Transformer model, and introduces a dynamic feature aggregation module in MLP and a ”head token” in MHSA for diverse channel representation so that the gap between the Transformer models and CNN models could be eliminated. ViTAE[26]and its extension model ViTAEv2[27]utilize multiple dilated convolutions to downsample the feature maps and aid the MHSA module to attain the locality simultaneously. Mixformer[28]parallelizes window-based self-attention and Depth-Wise Convolution to extend the receptive fields and designs bi-directional interactions to exchange information of channel and spatial dimensions between them. DMFormer[29]proposes a Dynamic Multi-level Attention mechanism which is comprised of Depth-Wise Convolutions with multiple kernel sizes for various patterns and a gating mechanism for adaptability. ScopeViT[30]involves Depth-Wise Convolutions into Transformer architecture for scale-aware efficient training. DctViT[31]proposes a hybrid structure with convolutions and Transformers for higher accuracy on multiple vision tasks. The hybrid structures are also applied to wetland classification[32], salient object detection[33][34], referring image segmentation[35], etc.

The computational structures of some previous models focus on integrating convolutional networks into the Multi-Head Self-Attention (MHSA) or Feed-Forward Network (FFN), making convolutional networks essential components of Transformer architectures. Additionally, the convolutional components in some of these studies are not necessarily lightweight. In contrast, our approach aims to efficiently combine Transformer blocks with convolutions while minimizing computational overhead. It is designed as a versatile and straightforward module that can be easily integrated into various Vision Transformer models.

SECTION: 3Methodology

SECTION: 3.1Vision Transformers

ViT[2]introduces Transformers[1]into vision tasks by splitting the images into patches which are tokenized into tokens (). To preserve the positional relations of the image patches, learnable positional embeddings are added to each token to learn the 2D relations of the patches. The tokens and positional embeddings are illustrated in Eq. (1).demonstrates the class token andindicates the positional embeddings.

Eqs. (2) and (3) illustrate the Multi-Head Self-Attention (MHSA) layer and the feed-forward (FF) layer. The residual connection and pre-LayerNorm[36]are harnessed in both layers. The attention layer and feed-forward layer are formed as Transformer blocks and Transformer models are comprised of cascaded Transformer blocks. The class token is employed to classify the image and output the result.

ViT models leverage self-attention mechanisms to compute the similarity between each pair of patch tokens and then assign different weights to different tokens according to the similarities between the patch tokens. Nonetheless, ViT models often overlook the inductive bias inherent in images, where neighboring pixels or patches have more relations. This oversight can lead to slow convergence, requiring more training iterations to learn the inductive bias and demanding large datasets for optimal performance. In contrast, convolutions inherently possess an inductive bias due to local filters traversing the image, capturing local details. Recognizing the complementary nature of convolutions to Transformer models, particularly in scenarios with small datasets, we propose a lightweight approach using Depth-Wise Convolutions to enhance the convergence and performance of Vision Transformer models. This is particularly beneficial when training ViT models from scratch on limited datasets without additional assistance.

SECTION: 3.2Our Approach

Convolutional kernels excel at capturing fine details in images, a capability lacking in ViT models. The challenge lies in determining how and where to incorporate these kernels. To maintain a lightweight design without significantly increasing parameters and computational demands, we select Depth-Wise Convolutions to filter the local details. We utilize the Depth-Wise Convolution as the shortcut to bypass the entire Transformer block. Since the patch tokens are flattened to 1D, we have to reconstruct all patch tokens into 2D feature maps. The architecture of our proposed model is demonstrated in Fig.1. The DWConv module is harnessed in all Transformer blocks as complementary components.

from Eq. (2) is used to reshape the 1D tokens to 2D feature maps. The reshaped 2D feature maps are implemented GELU activation[37]and batch normalization[38]before being fed into the Depth-Wise Convolution (DWConv). The kernel we utilize for Depth-Wise Convolution is. The 2D feature maps are reshaped to 1D patch tokens and finally the reshaped 1D patch tokens () and the output of the Transformer block (Eq. (3)) are summed together. The summed result () is utilized as the input to the next block. This process is illustrated below.

The DWConv modules act as “supervisors” to supervise the Transformer blocks and they are complementary to each other. Each Transformer block is supervised by the DWConv modules to capture details that may be missed by the Transformer blocks. While the Transformer blocks play the main role in the architecture, the proposed lightweight DWConv modules are leveraged to retrieve local information, thereby enhancing the overall performance. Unlike some hybrid models that design complex hybrid architectures, our proposed approach demonstrates simplicity, effectiveness, and flexibility.

SECTION: 3.3Architecture Variants

In addition to the base architecture, we have designed several variants based on the core structure, as illustrated in Fig.2. In our base architecture, the Depth-Wise module bypasses each Transformer block. Additional variants are designed where the Depth-Wise module bypasses more Transformer blocks. These variants prove beneficial when working with Vision Transformers that have deeper layers, helping to reduce the number of parameters and computational costs.

Moreover, in Transformer architectures with multiple stages, the size of the feature maps is reduced and the dimension is increased in successive stages. To maintain the input and output sizes of the Depth-Wise module, we recommend limiting the bypass within each stage to prevent a Depth-Wise module from crossing stages when multiple Transformer blocks are bypassed. Alternatively, one Depth-Wise module can be used to bypass an entire stage, ensuring that each stage has only one corresponding Depth-Wise module for more efficient combinations.

Furthermore, the DWConv modules could operate in parallel with various kernel sizes to capture the local information independently, as illustrated in Fig.3. In the experiments, we leverage parallel DWConv modules with different kernel sizes to demonstrate the performance of the variants. To further reduce the number of parameters and computational cost, multiple independent parallel DWConv modules could be combined with the aforementioned variants so that multiple Transformer blocks are contained by the DWConv modules. In Fig.3,Transformer blocks are encompassed in the DWConv modules and.

Not modifying the structures of MHSA and FFN makes our approach more flexible for use with most Transformer models, rather than being designed for specific ones. The proposed architecture variants illustrate the flexibility of our methods compared to some existing hybrid architectures that combine convolutions and Transformers. The Transformers are greatly enhanced by the proposed DWConv module with minimal overhead. Additionally, the structure can be easily modified to further enhance the performance or save the parameters and computations with different variants.

SECTION: 3.4Complexity Analysis

Our proposed approach is a lightweight module that is employed with each Transformer block. Unlike some models inserting convolutional layers inside the Transformer blocks, the proposed module is separable from the Transformer block, making it a plug-and-play module applicable to most existing Vision Transformer models. The increased parameters are dependent on the depths and dimensions of the Transformer models. Since our module is independent for each Transformer block without sharing parameters, deeper Transformer models could have more parameters introduced. However, the increased parameters are negligible compared to the Transformer backbone.
For instance, the ViT-Tiny model used in our experiments has 12 blocks and a dimension of 192. With a depth-wise convolution ofkernel size, the increased parameters for the ViT-Tiny model are approximately(0.023M) which is negligible compared to the backbone with around 5.5 million parameters. Moreover, since the patch size is 16 and the images are resized to 224 and the size of the feature maps is, the increased calculations for ViT-Tiny model could be approximately calculated by(0.004G), which is trivial compared to the total 1.26G FLOPs. In the aforementioned calculation, the number of parameters and calculations of BatchNorm are ignored since they are insignificant to the model.

In the experiments, sometimes our methods could even reduce the number of parameters and FLOPs considering some modules and positional embeddings could be removed for training the small dataset when our approach is applied to the Vision Transformer models. The increased number of parameters and FLOPs that are trivial to the models are highly dependent on the number of layers and dimensions of the models. Additionally, they also depend on which architecture variants are employed for the Vision Transformer models.

Some hybrid architectures merge convolutional networks into the Transformer architecture inefficiently, introducing significant parameters and computations as convolutional networks become essential components of Transformer structures. Additionally, these methods are often designed for specific Transformer architectures, making them impractical for other Transformer models. In contrast, our approach is designed to be easily incorporated into various Vision Transformer models. Complexity analysis shows that our approach introduces negligible overhead, with the majority of parameters and computations still coming from Transformer structures. However, the performance improvements are significant, especially on small datasets.

SECTION: 4Experiments and Results

To verify the effectiveness and efficiency of our proposed approach, we select vanilla ViT[2], CaiT[10], and Swin Transformer[16]for the experiments on three small datasets: CIFAR10[39], CIFAR100[39], and Tiny-ImageNet[40]. We also evaluated the model on a relatively large dataset: ImageNet-1K[41]. Additionally, COCO[42]is utilized for the evaluation of object detection and instance segmentation.

SECTION: 4.1Classification Performance on Small Datasets

CIFAR10[39], CIFAR100[39], and Tiny-ImageNet[40]are exploited as small datasets for training and evaluating image classification tasks. The classification accuracy is defined as the ratio of correctly classified samples to the total number of samples. In our paper, we use Top-1 accuracy for classification.

ViT. We select ViT-Tiny and ViT-Small to conduct the experiments for all datasets. The parameter settings of ViT models are followed by[43]. The dimensions of ViT-Tiny and ViT-Small are 192 and 384, respectively. The MLP ratio is 4 for both models which indicates the MLP dimensions are 768 and 1536 for tiny and small models, respectively. The numbers of heads for tiny and small models in Multi-Head Self-Attention are 3 and 6 respectively so the dimension for each head is 64 for tiny and small models. The depths are 12 for ViT-Tiny and ViT-Small.

CaiT. We choose CaiT-xxs12 and CaiT-xxs24 as the base models for the experiments. The dimensions of CaiT-xxs are 192 and the number of heads is 4. The MLP dimensions are 768 and the class depths are 2. The main depths for CaiT-xxs12 and CaiT-xxs24 are 12 and 24, respectively.

Swin Transformer. Swin-Tiny is selected for the experiments. For Swin-Tiny model, the drop path rate is 0.2 and the window size is 7; The depths and numbers of heads are (2, 2, 6, 2) and (3, 6, 12, 24) for each stage, respectively.

Experimental Parameters. All experiments are conducted using AdamW[44]optimizer with 300 epochs and 20 epochs warmup. The weight decay is 0.05. The batch size for three small datasets is 128 with 4 NVIDIA P100 GPUs. The cosine decay learning rate scheduler is exploited. The base learning rate for Swin-Transformer on three small datasets is 2.5e-4, while the base learning rate for other experiments on small datasets is 5e-4. The images are resized to 224 and the patch size for both ViT and CaiT is 16.

Data Augmentation. Most regularization and augmentation settings follow[16], including color jitter, Auto-Augment[45], random erasing[46], MixUp[47], CutMix[48]. All experiments are trained from scratch on each dataset without the assistance of an extra dataset.

The vanilla ViT model splits the image into small patches which are embedded as tokens for Transformer blocks. Since the tokens are 1-dimensional without the 2-dimensional positional information, the vanilla ViT model utilizes a positional embedding which would be added to all tokens to learn the 2-dimensional positional relationship between tokens. Since convolutions with zero padding could encode the positional information[49], we also applied our approach to the ViT model without positional embeddings. The experimental results are illustrated in Table1.

From Table1we observe that removing the positional embeddings significantly deteriorates the performance of ViT-Tiny and ViT-Small, highlighting the importance of positional embeddings in Vision Transformers. When our method is applied to ViT models, there is a substantial improvement in performance, regardless of the presence of positional embeddings. For ViT-Tiny, the increased accuracy for CIFAR-10, CIFAR-100, and Tiny-ImageNet are around 2%, 4%, and 5%, respectively. For ViT-Small, the performance boost for CIFAR-10, CIFAR-100, and Tiny-ImageNet are nearly 2%, 6%, and 6%, respectively. Additionally, our method without positional embeddings even slightly reduces the number of parameters with much better accuracy. More importantly, the accuracy of ViT-Tiny with our proposed DWConv surpasses that of vanilla ViT-Small which has almost 4x the number of parameters and FLOPs by large margins, demonstrating the efficiency and effectiveness of our proposed method.

Moreover, extra experiments are implemented with different kernel sizes and parallel DWConv modules, as illustrated in Table2. Directly applying a shortcut connection with positional embedding without any modules to bypass the Transformer blocks significantly reduces the accuracy. The possible reason for that might be the low-level input embeddings for the Transformers, which is different from the high-level input features attained from CNNs[50].
Some large kernel sizes or parallel DWConv modules could boost the performance with a slightly higher number of parameters and FLOPs.

CaiT introduces LayerScale to improve the performance of deeper layer transformer models by multiplying a learnable diagonal matrix[10]by each residual block. The class attention is introduced before the final classifier to convert patch embeddings into the final class embeddings. Talking heads attention[51]is utilized in the model for further improvement of the performance. However, LayerScale[10]and talking heads attention[51]are not necessary when our proposed approach is applied to CaiT model on a small dataset. Moreover, talking heads attention is extremely time-consuming for small dataset training in our experiments. Thus LayerScale and talking heads attention are removed when our method is applied to CaiT-xxs12 and CaiT-xxs24, which is demonstrated in Table3, where “LS”, “TH” and “PE” illustrate LayerScale, talking heads attention and positional embeddings. Similar to the vanilla ViT model, the positional embeddings are not necessary when our method is introduced to the small dataset training.

It is evident from Table3that removing LayerScale, talking heads attention, and positional embeddings reduces the accuracy for small datasets except Tiny-ImageNet. When our DWConv modules are applied to CaiT models, the accuracy is significantly boosted for small datasets. For Tiny-ImageNet, the accuracy of CaiT-xxs12 with our proposed DWConv is tremendously improved by around 11% with less number of parameters and FLOPs since the aforementioned modules are eliminated when our approach is applied to CaiT models. Similar to ViT models, CaiT-xxs12 with our method has much higher accuracy than the original CaiT-xxs24 and almost half of the number of parameters and FLOPs compared to CaiT-xxs24 model. In addition, CaiT-xxs12 with our DWConv modules even has much better performance than the original Swin-Transformer (as shown in Table5) that has almost 4x the number of parameters and FLOPs than CaiT-xxs12.

To verify the architecture variant that multiple Transformer blocks are bypassed by the proposed DWConv modules, more experiments are conducted with CaiT, as demonstrated in Table4. The number of blocks indicates how many Transformer blocks are supervised by the DWConv modules in the architecture. The performance drops when more blocks are supervised by DWConv modules, but the accuracy is still much higher than the original models. The variant is appropriate when the layers are deeper to reduce the number of parameters and FLOPs while still maintaining relatively high accuracy.

The architecture of Swin Transformer consists of four stages with hierarchical feature maps. The size of feature maps is reduced by 2 on each side by merging adjacent image patches in the following successive stage. Shifted window-based self-attention[16]is proposed to extend the view of the tokens instead of limiting the view of the tokens in the windows they are assigned. In the experiments, we investigate the effectiveness of our method applied to Swin Transformer, which is demonstrated in Table5.

The shifted window approach does not have too much effect on the performance of small datasets. Swin Transformer model with our method has much better accuracy than the original model with negligible parameter overhead.
In addition, “kernel 3+5” means parallel DWConv modules have kernel size 3 and 5, respectively. Independent parallel DWConv modules with different kernels increase the accuracy in some cases, but the number of parameters and computations would be slightly increased.

We also utilize GRAD-CAM[52]to visualize the focus areas of the models, as depicted in Fig.4. The Transformer models exhibit global views of images, while Transformer models might overlook some objects due to a lack of local information, especially when the objects are relatively small. With our method, both global and local information could be captured and enhanced by each other, which could improve the performance of the models.

The convergence of our approach is significantly faster than the original models, which is demonstrated in Fig.5. The accuracy on val set is recorded for each epoch on Tiny-ImageNet for all models. Our method exhibits much higher performance and considerably faster convergence speed. Our approach could reach a similar accuracy at around or less than 100 epochs while the original models require 300 epochs to attain the same accuracy. Similar performance curves are observed for CIFAR-10 and CIFAR-100.

SECTION: 4.2Classification Performance on ImageNet-1K

In addition to the small datasets, we have also evaluated the models on a relatively large dataset, ImageNet-1K[41], to further verify the effectiveness of our approach. ImageNet-1K[41]contains nearly 1.3 million images for training and 50k images for validation. For ImageNet-1K[41], The batch size is 1024 with 8 NVIDIA V100 GPUs and the base learning rate is 1e-3.

We utilize CaiT[10]and Swin Transformer[16]to illustrate the performance of our approach on ImageNet. The kernel size for our method isand the DWConv module is applied to each Transformer block. When our approach is applied to the models, the positional embeddings and talking heads attention in CaiT are retained for better accuracy, while LayerScale is eliminated. For Swin Transformer, our approach is directly applied to the model without any other changes. The experimental results are demonstrated in Table6. We employ top-1 accuracy to measure the performance of the models and the results of ResNet models are extracted from[54]. The performance of CaiT and Swin-Transformer on ImageNet-1K is further boosted (up to 2%) by our method.

Moreover, in comparison to the convolutional counterparts like ResNet[53], our approach still has superior performance with insignificant parameters and FLOPs overhead. Especially when the layers of the Transformer models go deeper (e.g., CaiT-xxs24), the improvement is even higher on ImageNet.

We also visualize the convergence curve of CaiT-xxs24 on ImageNet. As illustrated in Fig.6, the convergence rate of our approach is much faster than the original model by a large margin when the epoch is less than 100. This experiment indicates that our proposed approach could achieve higher accuracy with significantly faster convergence speed on a relatively large dataset.

SECTION: 4.3Object Detection and Instance Segmentation

In addition to Image Classification, we apply the proposed approach to object detection and instance segmentation and conduct the experiments on COCO dataset[42]with Mask RCNN[58]and Cascade Mask R-CNN[59]. The backbone utilized in the experiments is Swin-Tiny[16]. The models are trained from scratch without pre-trained backbones.

For the experimental settings, we employ AdamW[44]as the optimizer and the warmup iterations are 500. We utilize four NVIDIA P100 GPUs to train the model with 2 samples for each GPU. The initial learning rate is set at 5e-5 and the learning rate is reduced by 10 at epochs 9 and 12, respectively. The image scale for the experiments is 1333800. The total epochs for the experiments are 12.

The results for the experiments of object detection and instance segmentation are illustrated in Table7, where “Cas Mask-RCNN” stands for “Cascade Mask-RCNN”. From the definition in COCO[42], “mAP” refers to the average precision results that are averaged over all classes. The average precision is calculated by averaging the results over IoU thresholds from 0.5 to 0.95 with a step of 0.05. Additionally, “” represents the average precision computed using only the IoU threshold of 0.5 and “” indicates the average precision computed using only the IoU threshold of 0.75. Additionally, the visualization of the original method and our proposed approach with Mask-RCNN[58]is illustrated in Fig.7. In Fig.7, the first row demonstrates the visualization results of the original model and the second row indicates the visualization results of our proposed model.

The experimental results clearly show that our approach improves the performance of backbone networks for object detection and instance segmentation, demonstrating the effectiveness of our proposed method across various vision tasks. The effectiveness likely stems from the fine-detailed information captured by the proposed DWConv module. Object detection and instance segmentation require detailed information for predicting object boundaries and pixel-level labels, respectively. Vision Transformers might lack the ability to capture extensive fine-detailed information, especially when used as the backbone. Our proposed DWConv module complements this limitation with minimal overhead.

SECTION: 4.4Analysis

The proposed DWConv module, which bypasses the entire Transformer block, demonstrates higher accuracy for image classification, object detection, and instance segmentation when the models are trained from scratch. Additionally, this module enables Transformer models to achieve a much faster convergence rate for image classification, especially on relatively small datasets. Furthermore, our approach can significantly enhance small-size Transformer models, even surpassing large-size original Transformer models with substantially more parameters and computations on small datasets for image classification. Our approach illustrates both the effectiveness and efficiency of Vision Transformer models.

Although our architecture performs well on relatively small datasets due to the inductive bias introduced by the DWConv module, the improvement might not be as pronounced when the dataset is relatively large. The abundant data can mitigate the drawbacks of Transformer models.
In addition, since the proposed DWConv module is lightweight and plug-and-play, it may not show significant improvement when models have a large number of parameters and computations. A large number of parameters and computations can increase the representation ability of Transformer models and potentially remedy the lack of inductive bias, albeit inefficiently. However, our proposed models enhance both the effectiveness and efficiency of Transformer models, achieving higher accuracy than some large models, despite having significantly fewer parameters and computations.

Moreover, our method may not show significant improvement for transfer learning. One strength of our proposed approach is that our light-weight module can be utilized in most Transformer models, potentially enhancing performance, particularly on small datasets, when training from scratch. The experiments in this paper are all conducted with training from scratch. The possible reason for the lack of significant improvement in transfer learning is that pre-trained models already possess substantial representation ability, reducing the necessity for the inductive bias introduced by our approach. Thus, our proposed module may not provide much additional benefit when pre-trained models are applied to other tasks or datasets for fine-tuning.

SECTION: 5Conclusion

In this paper, we have presented a straightforward yet impactful approach that utilizes Depth-Wise Convolution modules to bypass Transformer blocks, enabling Vision Transformer models to capture both global and local information with minimal overhead. Extensive experimental evaluations show that small Transformer models, when equipped with our method, outperform larger Transformer models with significantly more parameters and FLOPs on small datasets for image classification. Our approach also significantly improves performance on ImageNet-1K[41]for classification and COCO[42]for object detection and instance segmentation when trained from scratch. Additionally, we introduce several architecture variants tailored to different models and objectives. We anticipate that our method will inspire further research on Vision Transformers, particularly in the context of small datasets.

SECTION: Acknowledgements

The project is partly supported by the Natural Sciences and Engineering Research Council of Canada (NSERC) under grant numbers 1-51-48183 and 1-51-48933.

SECTION: References