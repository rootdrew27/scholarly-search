SECTION: Deep Learning based Computer-vision for Enhanced Beamforming

Meeting the high data rate demands of modern applications necessitates the utilization of high-frequency spectrum bands, including millimeter-wave and sub-terahertz bands. However, these frequencies require precise alignment of narrow communication beams between transmitters and receivers, typically resulting in significant beam training overhead. This paper introduces a novel end-to-end vision-aided beamforming framework that utilizes images to predict optimal beams while considering geometric adjustments to reduce overhead. Our model demonstrates robust adaptability to dynamic environments without relying on additional training data where the experimental results indicate a top-5 beam prediction accuracy of 98.96%, significantly surpassing current state-of-the-art solutions in vision-aided beamforming.

SECTION: IIntroduction

The rapid evolution of wireless communication systems
is driving the shift toward millimeter-wave (mmWave) and sub-terahertz (THz) spectrums to address the demand for high data throughput[1]. However, these high frequencies
present significant challenges, particularly due to the inherent propagation and penetration loses[2]. Therefore, obtaining higher beamforming gains by employing directional narrow beams generated by large antenna arrays is crucial. As numerous narrow beams are generated by the large antenna arrays, selecting the best beam pair for the transmitter (TX) and receiver (RX) introduces beam training overhead which is specially large in highly dynamic environments where the best beam pair needs to be updated quickly[3].

The evolution of beam selection techniques for mmWave and terahertz communication systems has progressed from Exhaustive Beam Sweeping (EBS)[2]methods to more sophisticated approaches[4,5]due to their substantial beam training overhead. EBS uses predetermined analog beam codebooks for beam sweeping at both the TX and RX ends, determining the optimal beam pair through a process that measures the received signal power.
Subsequent advancements sought to reduce this overhead through various strategies such as tree search algorithms[4]and multi-resolution codebook designs[5]which attempt to minimize the number of beams evaluated.
Recent innovations using wide beams on the initial level to narrow the search space and then applying high-resolution techniques show promise in reducing the overhead and improving the accuracy of beam predictions in dynamic scenarios[6].

In recent years, machine Learning (ML) solutions have been utilized to reduce the beam training overhead and improve accuracy of beam prediction both with[7,1,8,9,10]and without[11,12]the integration of additional sensor data such as GPS coordinates[10], LIDAR data[9], and colored (RGB) images[7,1,8].
Focusing on RGB images, approaches in[7,8]primarily leverage visual data without incorporating mmWave information for TX identification and beam prediction. This approach limits accuracy in scenarios where the transmitter varies, as relying solely on RGB data can lead to generalization issues. Similarly,[1]relies exclusively on RGB images and environmental semantics for beam prediction, neglecting the potential enhancements from using mmWave data.

In this paper we focus on RGB images and mmWave power profiles to unveil a robust end-to-end architecture that ranges from TX identification to precise beam prediction. Our approach features two innovative, Deep Learning (DL) inspired solutions. Different to the current state-of-the-art[13]that relies on two separate DL and ML models, our first solution employs a single DL model for TX identification and significantly reduces latency and computational costs. More importantly, our proposed solution for TX identification excels in generality, designed to detect the TX based solely on mmWave received signal powers without depending on color or shape.
The second DL-based solution introduced in our work focuses on beam prediction by accounting for image distortions, such as the vertical vanishing point caused by camera alignment, ensuring accurate beam directionality. By integrating these adjustments, our model achieves a high degree of generalization, making it suitable for widespread application across various operational contexts.
We employed the real-world multi-modal dataset, DeepSense 6G[14], for our experiments, training our models on Scenario 3 and testing them on Scenarios 3 and 4. These scenarios, set in nighttime conditions, pose additional challenges due to the noise associated with low-light environments, testing the robustness of our approach in adverse settings.
In summary, we designed an end-to-end framework from TX identification to beam prediction that offers the following contributions:

A TX identification system that detects the TX in the image based on mmWave power profiles. (SectionIII-A)

A TX tracking system that sustains tracking without requiring mmWave power profiles.
(SectionIII-B)

A geometriccally-aware beam prediction system that incorporates vertical vanishing points to improve the accuracy of predicting top-beams under varied imaging scenarios.
(SectionIII-C)

An enhanced beam prediction system with reduced beam training overhead that outperforms current state-of-the-art work[7,1]. (SectionIV)

Our dual-pronged strategy exemplifies a significant advancement in leveraging visual data for enhanced beam management in wireless networks by outperforming the current state-of-the-art methods,[7,1]by atleast 6% in each top-1,3,5 metrics for scenarios 3 and 4 of DeepSense 6G dataset.

SECTION: IISystem Model and Problem Formulation

SECTION: II-ASystem Model

This paper considers an Orthogonal Frequency Division Multiplexing (OFDM) mmWave system where the base station (BS) employs an RGB camera and an-element uniform linear array while the single mobile RX in consideration is equipped with an omnidirectional single-antenna. Let this OFDM-enabled system be equipped withsubcarriers and assume the TX, to utilize an oversampled pre-defined beamforming codebook,, whererepresents the total number of beamforming vectors and. Let the channel between the RX and the BS over the-th subcarrier be denoted by.
If the transmitted signal from the-th subcarrier at the BS is denoted as, the downlink received signal can be expressed as,

whereis the receiver complex additive white Gaussian noise at timewhich obeys a complex Gaussian distribution with mean zero and varianceandis the-th beamforming vector from the predefined codebookat time. The objective of beamforming would be to find the optimal beamforming vector at time, which we denote as, by maximizing the average received signal-to-noise-ratio (SNR) across all subcarriers,, for all.
Thencan be obtained as,

where.
In the following, we design a novel computer-vision-aided framework to assist in finding.

SECTION: II-BProblem Formulation

Theoretically, the optimal beam for a given time can be obtained by Eq. (2) using channel state information (CSI). However, with the increase in the number of antenna array elements, obtaining accurate CSI for all potential beams becomes impractical. As the number of antenna elements increases in modern sub-THz and mmWave communication systems, the resulting narrower beams complicate the task of identifying the optimal beam for transmission, thereby posing challenges in maintaining low latencies in dynamic environments. In such scenarios, one could perform an exhaustive search across all possible beams, computing the SNR for each to determine the optimal beam for communication at a given time. However, as the number of possible beams increases, which is the case in mmWave and THz communications, a large beam training overhead is required to determine the optimal beam. As a result, methods that help reduce the beam search space and assist in beam prediction is of great importance.
As a potential solution, this work focuses on utilizing RGB images, an out-of-band input source, to develop a novel ML framework that predicts a subset ofbeams from a codebook ofbeams. This approach results in reducing the beam search space by a factor of[9]. In the next section, we propose a novel ML framework that achieves this task by identifying and tracking the transmitter through space and time and predicting the top-beams, which are the N beams that yield the highest SNR for each instance.

SECTION: IIIProposed Solution: Vision-aided Beamforming

This section highlights our proposed data-driven end-to-end beam selection approach guided by RGB images and mmWave received signal power profiles. Our proposed approach comprises three main sections as illustrated in Fig.1.

Transmitter Identification: Accurately identifying the TX amongst other distractors present in the environment.

Transmitter Tracking: Tracking the detected TX in the subsequent frames until it leaves the cell coverage area of the BS.

Beam Prediction: Predicting the top-beams serving the tracked TX.

SECTION: III-ATransmitter Identification

To predict the optimal beam for the TX while it remains within the BS coverage area, the first critical step is accurately identifying the TX user among multiple potential distractors.
The images captured at the BS alone cannot determine which object is the TX.
To resolve this ambiguity, we introduce an additional mmWave power profile channel, which we append to the image input for the object detection model.
Since mmWave beams are highly directional, the mmWave power profile provides crucial information about beam directions, indicating where the received signal power is highest which can assist in more precisely narrowing down the transmitter locations.

Our proposed model for TX identification integrates the mmWave power profile directly into the object detection process, eliminating the need for a second ML model for TX identification as opposed to[13]that used a second ML model.
By incorporating the mmWave signal information into a single inference, our approach effectively reduces the latency and the number of parameters which leads to a streamline process with low computational complexity.
This is illustrated in Fig.2where the mmWave power information is added as a channel to the image, similar to how color images are composed of Red, Green and Blue (RGB) channels, where each channel represents different data dimensions in the image. We are thus building a modified 3-channel input image for the object detector.
The remaining two channels of the modified 3-channel image are configured such that

Second Channel - mmWave Power Profile: The second channel represents each beam’s received signal power profiles. The beams are aligned in a way that corresponds to the coverage area of the BS. Beams with higher received signal power indicate that the beam captures the TX object. In contrast, beams with lower signal values correspond to areas where objects, if present, are not transmitting signals to the BS.

Third Channel - Zero Padding: We add an all-zero third channel to maintain compatibility with the pre-trained weights of MS COCO[15], which are based on 3-channel RGB images.

To effectively use the mmWave power profile in conjunction with image data, we propose two methods to structure the first channel of the input image we use to train the YOLOv8 object detector[16]. In both these methodologies, we first pre-process the image by discarding the background information.

TX Identification Method 1: After pre-processing the image, we remove the colors of all objects, leaving only their shape. This ensures that the object detector cannot rely on color cues for TX identification.

TX Identification Method 2: In this approach, we remove the color and shape of all objects within the image. The resulting input contains no visual features, and the model is compelled to depend entirely on the mmWave channel data to detect and identify the TX object.

This 3-channel input is then fed into the YOLOv8 object detector.
The objective of the object detector is to optimize the mapping function,

whereandrepresent the image and mmWave power profile of each beam captured at timeandis the predicted bounding box (bbox) of the TX at time.contains, which are the horizontal and vertical coordinates of the center point of the bbox and the width and height of the bbox respectively.

To test our Tx identification process, we use Scenario 3 of the DeepSense 6G dataset[14]by splitting the data for each of the 64 beams in a 70:30 ratio for training and testing. The model is also tested on the entirety of Scenario 4 of the DeepSense 6G dataset. In summary, the model is trained on 1204 training samples randomly picked from each beam, and tested on 288 Scenario 3 and 275 Scenario 4 test samples.
The accuracy (Acc.) of this system is calculated as follows:

whereis the Indicator Function,is the number of samples in the test set and IoU is defined as the Intersection over Union of the two bboxes; the predicted TX,and the ground truth TX,for each time instanceandis the user-defined IoU threshold.

In Scenarios 3 and 4 of the DeepSense 6G dataset, the TX object across all video frames is consistently the same car. If we were to train an object detector without running the image through the pre-processing step where we remove either the color or both color and shape, the model would inadvertently learn visual characteristics such as the shape and color of the car to identify the TX, rather than relying on the mmWave channel input. This results in a model that bases its detection on visual cues, rather than the intended mmWave received signal power profiles, essentially training a model biased for a specific scenario. As such, the existing methods described in[7,8], do not offer a generalized solution. Our proposed methodology for TX identification addresses this by removing the visual cues and forcing the model to identify the TX based on the mmWave power profiles.

In TableI, we benchmark our results against the current state-of-the-art work[13], demonstrating the impact of removing visual cues and highlighting the superior performance of our object detector that uses the modified input image. We set the IoU thresholdfor our analysis.
In our proposed model for TX Identification, we utilizeconsecutive frames, as illustrated in Fig.1, to ensure accurate identification of the TX. This approach mitigates the risk of missed detection in early frames by allowing the model to capture the TX in subsequent frames.
The object classified as the TX most frequently across theseframes is then identified as the TX. We tested this approach with different values ofandto evaluate its effectiveness in improving detection accuracy as shown in TableI.
Furthermore, we compared the accuracy of our two proposed methods for TX Identification against a YOLOv8 model that uses the original RGB image with both color and shape information. Additionally, we tested a model where the second channel, representing the mmWave power profile, was replaced with an all-zero channel, effectively removing the received signal power profile. The results demonstrate that without the mmWave channel input, the model cannot accurately detect the TX, as expected. The slight increase in accuracy when using the original RGB image shows that shape and color do not play a significant role in TX detection through our proposed method, emphasizing that the mmWave power profile drives accurate identification in our approach. The comparison can be seen in TableI.

SECTION: III-BTransmitter Tracking

Following the precise detection of the TX by our TX identification methods, we deploy an object tracker DeepOCSort[17]to facilitate efficient tracking. This integration avoids the need for exhaustive beam searches in subsequent frames.
The tracking process is supported by a three-stage system as illustrated in Figure 1. Initially, the general YOLOv8[16]model detects objects in the environment.
The object tracker then updates with the new bounding box coordinates
and the labels of each object in subsequent frames, distinguishing between the TX and potential distractors.
Furthermore, once the TX is identified, we continue to track it without masking its color or shape. As the TX remains constant within the BS coverage area,
this approach ensures that the tracker can leverage visual cues to consistently recognize the TX’s identity, avoiding any potential errors in identification.

The tracker updates the labels for each object’s new bbox coordinates based on its history and visual data comparison between its previous frames. With the updated bbox coordinates of the TX, the final stage of our proposed approach uses this information to perform accurate beam prediction, ensuring efficient beam alignment.

SECTION: III-CBeam Prediction

In this subsection, we introduce a novel two-stage approach to predict the top-beams for the identified and tracked TX, discussed in sectionsIII-AandIII-B.
This method represents the culminating phase of our proposed solution for vision-aided beamforming.

Scenarios 3 and 4 of the DeepSense 6G dataset provide access to the codebook design used and, implicitly, the angular regions covered by each beam. This angular span of each beam is mapped onto corresponding portions of the image as illustrated in Fig.3(a). This mapping reveals that while there are overlaps between subsequent beams, each beam still covers a substantial and distinct portion of the image. However, the camera used to capture images in the dataset, is positioned above street level, where the TX object traverses.
By carefully considering the images in the dataset (such as the Original image shown in Fig.2), it can bee seen that the camera is slightly tilted downwards to capture a comprehensive view of the street.
This camera tilt introduces a distortion where vertical structures within the captured images do not appear vertical and we took this into consideration when designing our beam prediction solution.

To address this, we analyzed the images in the dataset to calculate the vertical vanishing point: The point at which the lines, which should appear vertical, converge due to the perspective introduced by the camera’s tilt. Building on the vanishing point analysis, we redesigned the beam mappings to better align with the visual data presented in the image.
The redesigned beam shapes, which account for this perspective skew, are illustrated in Fig.3(b).
Unlike the strict vertical alignment shown in Fig.3(a), the new beam directions stem from the calculated vanishing point.
This projecting of the beams reflects the perspective distortions present in the images,
thereby improving the reliability and accuracy of our beam prediction model.

Building on the vertical vanishing point and beam shape redesign, we isolate the TX within the image by removing all extraneous objects and background elements. This isolation is achieved using the bbox coordinates of the TX provided by the preceding tracking system, ensuring that only the pixels representing the TX remain in the image. Subsequently, we overlay each redesigned beam shape, adjusted for the effect of the vanishing point, onto this isolated TX image. We then identify a subset of beams from the original codebook that exhibit at least one overlap pixel with the isolated TX which is utilized in the subsequent stage of our approach.

We designed a custom neural network to predict the top-beam indices for each instance of the TX. The architecture of this neural network as illustrated in Fig.4comprises two distinct pathways:

Image Processing:This path processes the input image, an isolated TX image. It sequentially passes through three convolution blocks. Each block consists of a Convolutional layer (Conv), a Batch Normalization layer (BatchNorm), and a Rectified Linear Unit (ReLU) activation, followed by a Max Pooling layer (MaxPool). This sequence effectively captures and enhances the salient features of the image. Ultimately, the learned features are flattened. This flattening step converts the multidimensional feature maps into a one-dimensional feature vector, preparing it for concatenation with the second pathway.

Beam Search Space Vector Processing:This path processes the reduced beam search space, derived from the earlier step where each possible beam index is encoded as\say1 and all others as\say0. This encoding passes through two fully connected layers (FC). The features extracted from this pathway are concatenated with the flattened features from the first path.

After concatenating the feature vectors from both pathways, the combined feature vector undergoes further processing through two additional FC layers. Finally, it passes through a 64-unit classification layer, which selects the top-beam indices. Additionally, we incorporate a masking layer post-classification. This layer ensures that the output is constrained to one of the possible beams identified in the previous stage by setting the weights to zero for all other classes.

The results of our proposed beam prediction stage are presented in TableII. To underscore the significance of each component within our novel approach, we conducted three ablation studies:

First Ablation Study: This study evaluates the impact of image isolation by replacing the isolated image of the TX with the original image that includes other distractors and the background. This change tests the importance of focusing solely on the TX for accurate beam prediction.

Second Ablation Study: In this study, we remove the second pathway, the reduced beam search space input, and the associated masked output layer, which is also the input for the second pathway. This ablation assesses the effectiveness of reducing the beam search space and masking outputs for the Top-beam prediction.

Third Ablation Study: This study substitutes the redesigned beam shapes which account for the vertical vanishing point with the original beam shapes. This allows us to determine how critical the beam shape redesign is to the effectiveness of beam search space reduction.

The results of the ablation studies are presented in TableIIwhere we can observe that each detail within our method significantly contributes to achieving high accuracy. This validation proves the importance of our novel approach in optimizing beam prediction, highlighting the synergistic effect of combining image isolation, beam search space reduction, and strategic beam reshaping.
We trained our neural network using cross-entropy loss, the Adam optimizer with a learning rate of 0.001, over 30 epochs, and applied a learning rate decay of 0.0001 after 59% validation accuracy.

SECTION: IVNumerical Results

In preceding sectionsIII-A, andIII-C, we detailed the individual performances of TX Identification and Beam Prediction processors. This section synthesizes the collective performance of these components and benchmarks our overall approach against current state-of-the-art. For the numerical results in this section, we use the same parameters as described in SectionIII-A.
In Fig.5, we illustrate the overall performance of our proposed approach which exclusively utilizes vision data, in comparison to[1]and hybrid vision-position methods from[7]that incorporate additional sensing information like GPS data.
Fig.5(a)which is plotted for Scenario 3 shows that our proposed framework achieves 59.51%, 88.93% and 98.61% accuracies for Top-1, Top-3, and Top- 5 metrics, as defined in SectionII. Fig.5(b)which is plotted for Scenario 4 shows that our proposed framework achieves 56.36%, 82.27% and 93.57% accuracies for the same metrics.
Our integrated approach consistently surpasses existing methodologies by at least 6% across the Top-1, Top-3, and Top-5 beam prediction metrics.
Notably, our methodology achieves comparable accuracies to the combined approach in[7], emphasizing the effectiveness of our technique without data from sensors.

SECTION: VConclusion

This paper presents a comprehensive end-to-end solution for TX identification, TX tracking, and beam prediction that is adaptable to any dynamic communication scenario. Our unique method employs mmWave power profiles, integrated as a channel within a modified 3-channel input image, processed through a YOLOv8 object detector for TX detection. Subsequently, our system adeptly tracks the TX across subsequent frames and predicts the top-beams, finely adjusting for geographical nuances such as vertical vanishing points. Our approach significantly enhances accuracy—approaching near 100% in top-5 beam predictions while reducing training overhead by over 90%. The results affirm the usefulness of using RGB data as a sensory input, enabling near-perfect beam prediction across varied conditions with minimal overhead.

SECTION: References