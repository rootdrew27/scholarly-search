SECTION: Copula Approximate Bayesian ComputationUsing Distribution Random Forests

Ongoing modern computational advancements continue to make it easier to collect increasingly large and complex datasets, which can often only be realistically analyzed using models defined by intractable likelihood functions. This invited feature article introduces and provides an extensive simulation study of a new Approximate Bayesian Computation (ABC) framework for estimating the posterior distribution and the maximum likelihood estimate (MLE) of the parameters of models defined by intractable likelihoods, which unifies and extends previous ABC methods proposed separately. This framework,copulaABcdrf, aims to accurately estimate and describe the possibly skewed and high dimensional posterior distribution by a novel multivariate copula-based meta-tdistribution, based on univariate marginal posterior distributions which can be accurately estimated by Distribution Random Forests (drf), while performing automatic summary statistics (covariates) selection, and based on robustly-estimated copula dependence parameters. ThecopulaABcdrfframework also provides a novel multivariate mode estimator to perform MLE and posterior mode estimation, and an optional step to perform model selection from a given set of models using posterior probabilities estimated bydrf. The posterior distribution estimation accuracy of the ABC framework is illustrated and compared with previous standard ABC methods, through several simulation studies involving low- and high-dimensional models with computable posterior distributions, which are either unimodal, skewed, or multimodal; and exponential random graph and mechanistic network models, each defined by an intractable likelihood from which it is costly to simulate large network datasets. This paper also proposes and studies a new solution to the simulation cost problem in ABC, involving posterior estimation of parameters from datasets simulated from the given model that are smaller compared to the potentially large size of the dataset being analyzed. This proposal is motivated by the fact that for many models defined by intractable likelihoods, such as the network models when they are applied to analyze massive networks, the repeated simulation of large datasets (networks) for posterior-based parameter estimation can be too computationally costly, and vastly slow down or prohibit the use standardABCmethods. ThecopulaABcdrfframework and standardABCmethods are further illustrated through analyses of large real-life networks of sizes ranging between 28,000 to 65.6 million nodes (between 3 million to 1.8 billion edges), including a large multilayer network with weighted directed edges. The results of the simulation studies showed that, in settings where the true posterior distribution is not highly-multimodal,copulaABcdrfusually produced similar point estimates from the posterior distribution for low dimensional parametric models as previousABCmethods, but the copula-based method can produce more accurate estimates from the posterior distribution for high-dimensional models, and in both dimensionality cases, usually produced more accurate estimates of univariate marginal posterior distributions of parameters. Also, posterior estimation accuracy was usually improved when pre-selecting the important summary statistics usingdrf, compared toABCemploying no pre-selection of the subset of important summaries. For allABCmethods studied, accurate estimation of a highly-multimodal posterior distribution was challenging. In light of the results of all the simulation studies, the article concludes by discussing howcopulaABcdrfframework can be improved for future research.Keywords:Bayesian analysis, Maximum Likelihood, Intractable likelihood.

SECTION: 1Introduction

Statistical models defined by intractable likelihood functions are important for analyzing complex and large datasets from many scientific fields. The broad field of ABC provides alternative algorithms for estimating the approximate posterior distribution or MLE of the parameters of any model defined by a likelihood which may be intractable, either because it does not have an explicit form, the data set being analyzed is large, or because the model is high-dimensional (i.e., the model has many parameters).

For the model chosen to analyze a given observed dataset, represented by a set ofobserved data summary statistics(orobserved summaries) which are ideally sufficient (at least approximately), the original rejection (vanilla) ABC algorithm(Tavaréet al.,1997; Pritchardet al.,1999), here referred to asrejectionABC, obtains samples from the approximate posterior distribution of the model parameters from the subsample of many samples from the prior distribution, where for each prior sample, the summary statistics of a pseudo-dataset (orpseudo-data summaries) drawn from the model’s exact likelihood conditionally on the prior sample of the model parameters, is within a chosen smallin (e.g., Euclidean) distance to the observed data summary statistics. Then from this subsample, estimates of marginal posterior means, medians, density and marginal densities, and estimates of the density and mode and MLE of all the parameters can be obtained. A central object of ABC is theReference Table, consisting of many rows, where each table row contains a prior sample of the model parameters, and the corresponding summaries of a pseudo-dataset drawn from the model’s exact likelihood, conditional on this prior sample. The applicability ofrejectionABChinges on the ability to efficiently sample from the model’s likelihood. A main motivation forrejectionABCis that it produces exact samples from the posterior distribution, aswhen the summary statistics are sufficient(Marinet al.,2012). To explain, consider any parametric model with parameters(), a prior distributionover, and likelihoodfor any given datasetwith summary statistics. Further, ifis (minimally) sufficient for, then the posterior distributionsatisfiesfor all(Bernardo and Smith,1994, §5.1.4). Then for any random dataset, the posteriorconverges in distribution to the exact posterioras the distancebetweenandvanishes,, as thenbecomes the sufficient statisticsof. Therefore, whenis small and in more typical scenarios where sufficient statistics are unavailable, therejectionABCproduces samples from the approximate posterior distribution and the summary statistics are approximately sufficient forat best, for the given dataset and the model applied to analyze it. Often inrejectionABCpractice, the toleranceis chosen as a smallq% (e.g., 1%) quantile of the simulated distances, to define aq%-nearest-neighborrejectionABC(Biauet al.,2015). In ABC, asymptotically efficient estimation of posterior expectations relies on the number of summary statistics to equal to the number of parameters of the given model, where each parameter corresponds to a summary statistic being an approximate MLE of the model parameter(Li and Fearnhead,2018). Therefore, in scenarios where the number of summary statistics (summaries) exceed the number of parameters of the given model, semiautomaticrejectionABC(Fearnhead and Prangle,2012)is often applied, which pre-selects the summary statistics that are most important and equal in number to the number of parameters of the given model (here, referred to asrejectionABCselect), using any one of several available methods to select important summaries for ABC data analysis(e.g., Blumet al.,2013).

The originalrejectionABCmethod (and variants) quintessentially represents ABC, but for the purposes of obtaining reliably-accurate posterior inferences, it can be computationally inefficient or prohibitive to apply easily because it can require generating a huge number of prior samples, especially for models with more than a few parameters and/or from which it is computationally expensive to simulate pseudo-datasets from. Also, while the appropriate selection of relevant summary statistics becomes important in typical scenarios where they are not sufficient. Further, the choices of distance measure and smallare also important tuning parameters for the rejection (vanilla) ABC algorithm, which can impact the level of posterior inference accuracy. Therefore, more recent research has developed various Monte Carlo algorithms that can more efficiently sample the ABC posterior distribution, with possibly less dependence on the tolerance, distance measure, and summary statistics tuning parameters ofrejectionABC. Essentially all ABC methods can each be viewed as defining a specific approximate model likelihood providing a surrogate to the exact intractable likelihood of the given model, which measures the closeness of summary statistics of the observed dataset to the same summaries of a random dataset simulated from the model’s exact likelihood, conditionally on any proposed set of model parameters. The ABC field has had many reviews due to its wide theoretical scope and applicability(e.g., Sunnåkeret al.,2013; Karabatsos and Leisen,2018; Sissonet al.,2018; Grazian and Fan,2020; Cranmeret al.,2020; Craiu and Levi,2022; Karabatsos,2023; Pesonenet al.,2023; Martinet al.,2023).

The main research objective of this paper is to introduce and study a novel ABC framework,copulaABCdrf, for estimating the posterior distribution and the MLE of the parameters of models defined by intractable likelihood functions, from the given generated Reference Table. ThecopulaABCdrfframework unifies and extendscopulaABC(Liet al.,2017; Nottet al.,2018; Chen and Gutmann,2019; Kleinet al.,2024), ABC-Random Forests (abcrf)(Raynalet al.,2018), and ABC and other simulation-based (AMLE) methods for calculating the approximate MLE for intractable likelihood models(Rubio and Johansen,2013; Kajiharaet al.,2018; Picchini and Anderson,2017; Gutmann and Corander,2016; Yildirimet al.,2015; Deanet al.,2014; Gourierouxet al.,1993; McFadden,1989). Notably,copulaABCdrfprovides a single framework to perform all the tasks of inferences from a possibly high dimensional posterior distribution (including estimation of the posterior mean, median, mode, univariate marginal posteriors), MLE estimation, automatic summary statistics selection, and model selection. In contrast, nearly all the other proposed ABC methods, typicallyrejectionABCmethods, were proposed to perform only a small subset of these tasks in a non-unified manner. Therefore, a related objective of this paper is to compare the results ofcopulaABCdrfwith other existingrejectionABCmethods in terms of accuracy in estimating quantities from the posterior distribution. While somerejectionABCmethods were already mentioned, others are reviewed later in due course.

To elaborate,copulaABCdrfcombinescopulaABC’s ability to approximate high dimensional posterior distributions, based on estimation of one-dimensional marginal posterior distributions (unlikeabcrfandAMLEmethods), while separately estimating the dependence structure of the full multivariate posterior; with withabcrf’s ability to estimate these one-dimensional marginal distributions (of the dependent variables) using Random Forests (rf), conditionally on observed data summaries, while automatically selecting relevant summary statistics (covariates) for the given model and dataset under analysis (unlike nearly allcopulaABCandAMLEmethods); and the ability to calculate the approximate MLE (unlikeabcrfandcopula ABC). In particular,copulaABCdrfunlikeabcrfemploys a more modern, Distribution Random Forests (drf)(Ćevidet al.,2022, §4.1), which more directly targets the estimation of the one-dimensional marginal distributions of the model parameters from the Reference Table. A benefit of usingdrffor ABC is that it allows the data analyst to avoid the inefficient accept-reject procedure used in the originalrejectionABCalgorithm, while not requiring the analyst to pre-select summary statistics, and the tolerance and distance measure tuning parameters ofrejectionABC. BesidescopulaABCdrf, other methods in the literature use generative models to perform this multivariate density regression task of estimating the full joint posterior distribution, without using copulas. They include autoregressive flow models(Papamakarioset al.,2019)and conditional generative adversarial models(Wang and Rocková,2023), which have demonstrated to accurately estimate low-dimensional multimodal posterior distributions. Meanwhile,copulaABChas shown the ability to accurately estimate a skewed unimodal posterior distribution of a 250-parameter Gaussian model(Liet al.,2017; Nottet al.,2018).

ThecopulaABCdrfframework aims to estimate (at least approximately) the posterior distribution of model parameters of possibly-high dimensiondim, using a multivariate copula-based distribution estimated from a constructed Reference Table. It does so while accounting for any skewness in the posterior, and for the fact that the posterior converges to a multivariate Gaussian distribution as the data sample size increases, even for misspecified models, under regularity conditions(Kleijn and van der Vaart,2012). For the purposes of estimating the posterior mode and MLE in practical applications, such a posterior distribution estimate should be easy to compute its probability density function (pdf) and to sample from.

According to Sklar’s (1959) theorem, a continuous-variate random parameter vectorcan be described by a joint (posterior) cumulative distribution function (cdf), called a copula, which can be uniquely represented by its univariate marginal densities (distributions) and a copula density function describing its dependence structure. If the random vectorhas any discrete-valued parameters, then these discrete random variables can be continued using jittering methods(Denuit and Lambert,2005; Madsen and Fang,2011)which maintain a unique representation of the joint copula-based posterior cdf of, while ensuring that no information is lost and preserving the same dependence relationship among the model parameters.

Approximating a possibly skewed high-dimensional posterior distribution requires a suitable choice of multivariate copula-based distribution family. Choices are limited because it is difficult to construct high-dimensional copulas(Nelsen,2006, p.105). Meanwhile, skewness can be introduced into a multivariate copula-based distribution, either: (1) by using skewed univariate margins(e.g., Hutsonet al.,2015; Smith and Vahey,2016; Baillienet al.,2022); (2) by using a skewed copula density function to account for asymmetric dependencies in the multivariate distribution, such as a skewed, grouped Student-, or mixture ofGaussian orcopula density functions(e.g., Weiet al.,2016; Yoshiba,2018; Demarta and McNeil,2005; Daulet al.,2003; Kosmidis and Karlis,2016), or using a covariate-dependent copula density function(Smith and Klein,2021; Kleinet al.,2024; Acaret al.,2013); or (3) by combining both skewed marginals and skewed densities, but this can unnecessarily increase the complexity of the copula model(Baillienet al.,2022). In high dimensions, estimating the parameters of skewed copulas is challenging and cumbersome(Hintzet al.,2022b), and arguably the mixture copulas are over-parameterized, withcorrelation matrix parameters, while the number of correlation parameters grows quadratically with, and it is not easy to reliably estimatecorrelation matrices(Dellaportas and Tsionas,2019; Qu and Lu,2021). Besides, thecopulaABCdrfframework also aims to estimate a multivariate posterior mode, and the MLE as a mode of a posterior-to-prior pdf ratio, while the mode is robust to the shapes of the tails of the distribution.

Therefore, all things considered,copulaABCdrfemploys a novel multivariate meta-(Fanget al.,2002)copula-based distribution approximation of the posterior, which allows for multivariate skewness by using skewed marginals which are flexibly modeled and estimated bydrfs, respectively. This multivariate distribution is defined by univariate marginals, which are (covariate-dependent)drfregressions, along with a multivariateparametric copula that is independent of covariates, extending previous related copula models(Pittet al.,2006; Song,2000). The dependence structure of the meta-distribution is defined by the density of a multivariatedistribution with (posterior) degrees of freedom and scale matrix parameters, which accounts for symmetric tail dependence and inherits the robust properties of thedistribution, by inversely-weighting each observation by its Mahalanobis distance from the data distribution center(Langeet al.,1989).

Thedrftargets the whole distribution of the dependent variable more appropriately than the mean functions of the dependent variable achieved by the earlierrfandabcrfalgorithms. The capability ofdrfto automatically select summary statistics enables the new ABC framework to avoid the potential issues involved with pre-selecting summary statistics(Drovandiet al.,2023), and to avoid having to introduce an extra prior distribution to do this selection, as done in other ABC methods. The originalcopulaABCmethod(Liet al.,2017; Nottet al.,2018)was based on a Gaussian copula density, defined by tails that are thinner than those of the more-robustdistribution (but the correlation matrix parameters of the meta-Gaussian can be robustly estimated using robust correlations, as done in(Anet al.,2020)), and defined by a corresponding meta-Gaussian distribution with univariate densities estimated from the Reference Table by smooth kernel density estimation methods, conditionally on bandwidth parameters and summary statistics that need to be non-automatically preselected by the data-analyst.

Further, for scenarios where multiple models are considered to analyze the same dataset,copulaABCdrfemploys an optional step which can be used to perform model selection, based on posterior model probabilities that are estimated usingdrf(conditionally on the observed data summaries). This provides a simple minor extension of the previously-validatedrf-based approach to model selection in ABC(Pudloet al.,2016; Marinet al.,2018), by directly estimating theM-category multinomial distribution of the model index dependent variable, among theMmodels compared, instead of estimatingMseparate classificationrfs for binary (0 or 1 valued) indices of models.

Finally, thecopulaABCdrfframework calculates an estimate of the multivariate posterior mode and MLE of the parametersof the (selected) model. The MLE is the multivariate mode of the ratio of the meta-posterior pdf estimate to the prior pdf, or the posterior mode under a proper uniform prior supporting the MLE. The new multivariate mode estimator maximizes a meta-posterior density estimate (or finds the MLE by maximizing the ratio of the posterior density estimate to the prior density) over parameter samples of points from the Reference Table (or over an extra set of parameter samples of points drawn from the meta-posterior density estimate if necessary), which avoids a possibly costly grid search involved in multivariate mode estimation by global maximization. Similarly, other ”indirect” multivariate mode estimation methods(Devroye,1979; Abrahamet al.,2003,2004; Hsu and Wu,2013; Dasgupta and Kpotufe,2014)use the data sample points to estimate the mode from a nonparametric kernel,-nearest-neighbor, or parametric density estimate, while respectively, relying on choice of bandwidth, or, or on a successful normal transformation of the data. ThecopulaABCdrfframework employs a novel semiparametric approach to posterior mode and MLE estimation, because it is copula-based and thus applicable to high dimensions as shown by previouscopulaABCresearch(Nottet al.,2018)using Gaussian copulas and not usingrfor summary statistics selection algorithms. Other multivariate mode estimators seem only applicable to lower dimensions(see Chacón,2020, for a review).copulaABCdrfestimates the MLE (mode) while performing automatic summary statistics (variable) selection viadrfs for the meta-marginals.

Next, Section §2describes thecopulaABCdrfframework. Then §3investigates this framework through several simulation studies, which compare the results ofcopulaABCdrfwith the results from existingrejectionABCmethods. The first three simulation studies consider models with low-dimensional, high dimensional, and multimodal joint posterior distributions, being either analytically or simulation-based computable posterior distributions, in order to allow their direct comparisons with posterior distributions estimated under different ABC methods. The subsequent simulation studies and real data analyses in §3focus on models defined by intractable likelihoods. Most models defined by intractable likelihoods have the two properties that (1) it is possible to rapidly simulate datasets from the model’s likelihood, in which case ABC or synthetic likelihood methods(Picchiniet al.,2022; Picchini and Tamborrino,2024, and references therein)can be used to estimate the (approximate) posterior distribution and MLE of the model parameters; and/or (2) the MLE or another point estimator and its sampling distribution can be computed for the model, in which case the approximate posterior distribution of the model parameters can be computed using certain ABC or bootstrap methods(e.g., Karabatsos,2023; Newtonet al.,2021; Barrientos and Peña,2020; Lyddonet al.,2019; Zhuet al.,2016).

Therefore, the rest of §3focuses on models defined by intractable likelihoods which do not possess these two properties, at least in large data scenarios. They include Exponential Random Graph Models (ERGMs) and mechanistic models for network datasets, ubiquitous in scientific fields. For such a model, the likelihood is intractable for a large network dataset, and it is too costly or prohibitive to simulate large network datasets from the likelihood, and to compute the MLE or other point estimator of the model parameters from large networks. Along these lines, another contribution of this paper is a new solution to simulation cost problem in ABC, which involves constructing a Reference Table by simulating datasets from the exact model likelihood (given proposed model parameters, resp.) that are smaller compared to the potentially large size of the original dataset being analyzed by the model. This approach can be useful in settings where it is computationally costly to simulate data from the model, including the modeling of very large networks. Using these models, thecopulaABCdrfframework is illustrated through the analyses of three large real-life networks, of sizes ranging between twenty thousand to over 6 million nodes, including a large multilayer network with weighted directed edges. In these large data scenarios, usingcopulaABCdrfandrejectionABCmethods, posterior inferences for these models is achieved by employing summary statistics that are invariant to the size of the network. This allows the Reference Table to be constructed by simulating summary statistics based on networks of smaller tractable size compared to the size of the original observed large network dataset being analyzed, while summary statistics of the latter large network can be efficiently computed once using subsampling methods when necessary. As shown, thecopulaABCdrfframework provides practical methods for analyzing widely-available complex and large datasets. Finally, §4 ends with Conclusions with potential future research directions based on the results of this paper.

SECTION: 2Methods: ThecopulaABCdrfFramework

The proposedcopulaABCdrfframework is summarized in Algorithm 1, shown below. The algorithm can be applied to any set ofmodels. It outputs an estimate the ABC posterior distribution, a probability density function (pdf) based on corresponding cumulative distribution function (cdf), the selected best predicted model,amongmodels compared, and the model’s prior distribution (pdf), and its approximate likelihood of the generic form(Li and Fearnhead,2018)based on summary statistics vectorand some kernel density function.

Optional Step 2 of Algorithm 1, used when a set ofBayesian models is considered by the data analyst, performs model selection based ondrf-based(Ćevidet al.,2022)estimates of theposterior model probabilities, conditionally on observed data summaries. As mentioned in §1, Step 2 provides a simple minor extension to a previously-validated Random Forest approach to model selection in ABC(Pudloet al.,2016; Marinet al.,2018)by directly targeting the fullM-category multinomial distribution dependent variable, over theMmodels being compared in the given model selection problem; instead of runningMseparate classificationrfs for binary (0 or 1 valued) indices of models (resp.) as the previous Random Forest approach to model selection did. It has been discussed in the literature(e.g., Robertet al.,2011)that summary statistics can be insufficient for model comparison. Tree-based models can alleviate this issue when enough suitably-chosen summary statistics are used for model selection(Pudloet al.,2016).

Step 3 of Algorithm 1 aims to estimate the posterior distribution of possibly high-dimensional model parameters at least approximately-well, using a tractable multivariate copula-based distribution, based on Sklar’s (1959) theorem.

To explain Sklar’s theorem in terms of Algorithm 1, let

be the cdf of the approximate posterior distribution (approximate because the summariesare not necessarily sufficient), with corresponding univariate posterior marginal cdfsand pdfs.

Sklar’s (1959) theorem implies that there exists a copula(cdf) such that for all, the joint posterior cdf can be represented by

If the(cdfs) are continuous for all(with corresponding pdfs), thenis unique, with uniform distributedover the random(for) by the probability integral transform, and the copulais ‘margin-free’ in the sense that it is invariant under increasing transformations of the margins(Nelsen,2006, Theorem 2.4.3). Then for an arbitrary continuous multivariate distribution(e.g., Okhrinet al.,2017,  §13.2), its copula(cdf) can be determined from the transformation

with corresponding copula density (pdf)(for), where the(for) are inverse marginal distribution functions. Also, the
posterior distribution pdf ofis given by

As mentioned, the ABC framework (Algorithm 1) employs a novel multivariate meta-(Fanget al.,2002)copula-based distribution approximation of the posterior, which allows for multivariate skewness by using skewed marginals which are modeled flexibly and accurately estimated bydrfs, respectively. This multivariate distribution has marginals which are (covariate-dependent) regressions and a multivariateparametric copula that is independent of covariates, extending previous related copula models(Pittet al.,2006; Song,2000)through the use ofdrf. The meta-distribution is defined by acopula cdf(copula density pdf,) with degrees of freedomandscale matrix parameters, which is a correlation matrix if the density is that of a normal distribution.

As an aside, when the random vectorcontains any discrete variables, the joint cdfis not identifiable under Sklar’s theorem(e.g., Geenens,2020). Therefore, to maintain direct use of Sklar’s theorem using the-copula, Step 1(e) of Algorithm 1 can easily apply jittering(Denuit and Lambert,2005; Madsen and Fang,2011)to continue each (of any) discrete model parameter (integer-valued without loss of generality), while ensuring that no information is lost and preserving the same dependence relationship among all model parameters. Each (of any) discrete (integer) model parameterwith posterior cdfis continued (“jittered”) into a continuous parameterwith uniform random variable(withandfor) and posterior cdf

and pdf, wherefloor. The parameters ofandare exactly those of, andcan be recovered fromas.

Step 3 of Algorithm 1 performs two-stage semiparametric estimation(Genestet al.,1995)of the meta-posterior distribution parameters,. The first stage calculates nonparametric marginal posterior cdfs and pdfs,conditionally on, fromdrfs trained on columns of the reference table,for(resp.), while performing automatic summary statistics (covariate) selection. The second stage employs an Expectation-Maximization algorithm(Hintzet al.,2022a,b)to calculate the MLEof the copula density parameters of the meta-pdf based on these estimated cdfs and pdfs, for the subset of theNrows ofk= 1,…,dfor whichfor all, as this MLE is calculable only for this subset; which can be done using thenvmixR package(Hintzet al.,2022a).

Thedrfis trained (estimated) on the reference table(the training dataset), such that prior parameter samples(dependent variable) are regressed on the summary statistics covariate vectorswhile targeting the conditional distribution of the dependent variable, for each model parameter indexed by. Then based on each of thesetraineddrfs, the posterior distribution (cdfsand pdfs, for) can be accurately predicted (estimated)(Ćevidet al.,2022), conditionally on the summary statistics of the original observed dataset,. All this is done while performing automatic summary statistics (variable) selection from a potentially larger set of summary statistics, and accounting for the uncertainty in variable selection, without requiring the user to pre-select the subset of summary statistics relevant to the intractable-likelihood model under consideration.

Thedrfis a weighted nearest neighbor method(Lin and Jeon,2006)which performs locally-adaptive estimation of the conditional distribution through the aggregation of dependent variable predictions of an ensemble of randomized flexible Classification of Regression Trees (CARTs)(Breimanet al.,1984), respectively, where each CART is estimated from a random subsample of training dataset (drawn without replacement), and that each level of the CART’s binary tree is constructed by splitting the training data pointsbased on a covariateand its split pointboth chosen in such a way that the distribution of the dependent responsesfor whichdiffers the most compared to the distribution for which, according to the Maximal Mean Discrepancy (MMD)(Grettonet al.,2007). The MMD is a quickly-computable two-sample test statistic that can detect a wide variety of distributional changes. This way, the dependent variable distribution in each of the resulting leaf nodes is as homogeneous as

possible, to define neighborhoods of relevant training data points for every covariate value. Repeating this many times with randomization induces a weighting functionquantifying the relevance of each training covariate data point(for) for a given test point. Specifically, the weightis the proportion of times out ofCART subsampling randomizations thatends up in the same terminal leaf node as.

Thedrfestimates of the posterior conditional cdfs are given by:

where(with) are the order statistics of, these order statistics corresponding to weights. Alternatively, the marginal posterior density estimates, for, can, respectfully, be obtained by smoother, univariate local polynomial Gaussian-kernel density estimators (with spline interpolation to speed up computations) and automatic bandwidth selection(Sheather and Jones,1991), performed on the, using corresponding frequency weights, and with unbounded or bounded support(Geenens,2014; Geenens and Wang,2018), depending on the spaces (resp.) of the univariate parameters or the support of their priors, as appropriate, e.g., by using thekde1dR software package(Nagler and Vatter,2024)).

Extensive simulation studies(Ćevidet al.,2022, §4.1)showed thatdrfperforms well and outperforms other machine learning methods in terms of prediction accuracy, for a wide range of sample size and problem dimensionality, especially in problems where(number of covariates) is large and(dimensionality of the dependent variable) is small to moderately large, without the need for further tuning or involved numerical optimization. Thedrftraining and predictions can compute fast using thedrfR package(Michel and Ćevid,2021), with minimal tuning parameters. By default, adrfis estimated from the given training dataset using an ensemble ofCARTs, with every tree constructed (randomized) from a random subset being 50% of the size of the training set, and with a target of 5 for the minimum number of observations in each CART tree leaf(Ćevidet al.,2022, p.33). The estimated (induced) weighting function ofdrf, for each of the given model parameters(for), is used to estimate a parametric meta-multivariate copula-based distribution for these dependent variable observations, after having nonparametrically adjusted for the covariates(for)(in the spirit of Bickelet al.,1993). Alldrfcomputations mentioned can be undertaken using thedrfR package(Michel and Ćevid,2021).

Step 4 of Algorithm 1, with thedrfs estimated in Step 3, uses the weightsand marginal posterior cdf estimates (1) to calculate, for each model parameterfor, estimates of marginal posterior expectations, including estimates of the posterior mean (), variance (), and quantiles ():

Step 4 also calculates estimates of the multivariate posterior mode,and MLEof the parametersof the (selected) model. The MLE is the multivariate mode of the ratio of the meta-posterior pdf estimate to the prior pdf, or the posterior mode under a proper uniform prior supporting the MLE. For either the task of posterior mode or MLE estimation, a novel semiparametric mode estimator is being proposed here, which is applicable to high dimensions. Other multivariate mode estimators(see Chacón,2020, for a review)seem only applicable to lower dimensions. The new multivariate mode estimator maximizes a (posterior) density estimate, and finds the MLE by maximizing a ratio of the posterior density estimate to the prior density, over a sample of points. The sample of points is the set of parameter samplesfrom the Reference Table, or alternatively (if necessary), can be extra parameter samplesdrawn from the meta-t posterior density estimate. Such a sampling approach avoids a possibly costly grid search involved in multivariate mode estimation by global maximization. Similarly, other ‘indirect’ multivariate mode estimation methods(Devroye,1979; Abrahamet al.,2003,2004; Hsu and Wu,2013; Dasgupta and Kpotufe,2014)use the data sample points to estimate the mode from a nonparametric kernel,-nearest-neighbor, or parametric density estimate, while respectively, relying on choice of bandwidth, or, or on a successful normal transformation of the data. The newcopulaABCdrfframework (Algorithm 1) estimates the MLE (mode) while performing automatic summary statistics (variable) selection viadrfs for the meta-marginals. Therefore, this framework advances on previous simulation-based MLE estimation methods, which do not provide this automatic selection of summaries(Rubio and Johansen,2013; Kajiharaet al.,2018; Picchini and Anderson,2017; Gutmann and Corander,2016; Yildirimet al.,2015; Deanet al.,2014; Gourierouxet al.,1993; McFadden,1989).

In particular, the originalrejectionABCapproach to MLE and posterior mode estimation(Rubio and Johansen,2013)is based on a multivariate kernel density estimate of the posterior distribution estimated from the subset of prior parameter samples from the Reference table, corresponding to simulated summary statistics being a small distanceto the observed data summary statistics (and recall thatcan be chosen as a smallq% (e.g., 1%) quantile of the simulated distances). For example, for a 1- to 6-dimensional parameter, a multivariate kernel density estimate can be obtained using thekde()function of theksR package(Duong,2024)based on automatic selection of the bandwidth matrix. For a higher-dimensional parameter (of dimension), a multivariate kernel density estimate can be obtained by a product ofunivariate local polynomial Gaussian-kernel density estimators(Wasserman,2006, eq.6.47)based on automatic bandwidth selection(Sheather and Jones,1991)performed for each of theseddensities, while the accuracy of the estimator deteriorates quickly as dimensiondincreases(e.g., Wasserman,2006, p. 138). Then, given this posterior kernel density estimate, an estimate of the posterior mode is obtained by maximizing this density, over theq% subsamples of the Reference Table, gives an estimate of the posterior mode. Also, the MLE is obtained by maximizing the ratio of this posterior density to the prior density over these subsample, while of course, the MLE coincides with the posterior mode under a uniform prior distribution. Later, we sometimes refer to the rejection-based ABC algorithm for estimating the posterior mode or MLE, based on kernel density estimation, asrejectionABCkern,rejectionABCkern.select, orrejectionABCprodkern, depending on whether the rejection-based ABC algorithm used multivariate kernel density estimation (kern), perhaps using pre-selection of summary statistics (kern.select), or used multivariate product kernel density estimation (prodkern).

SECTION: 3Results of Numerical Examples

This section (§3) presents results of several simulation studies which evaluate and compare thecopulaABCdrf(Algorithm 1) and methods based on the standard (small)q% nearest-neighborrejectionABCalgorithm(Biauet al.,2015), in terms of estimation accuracy of various features of the posterior distribution, for three models formultivariate independently and identically distributed (i.i.d.) observations in §3.1, and for seven models for-node networks. Namely, three ERGMs in §3.3 and §3.7, and four mechanistic network models in §3.4-§3.5, after §3.2 provides a contextualizing overview of network science and modeling. To provide further illustrations of thecopulaABCdrfandrejectionABCmethods, these network models are applied to analyze real-life massive network datasets in §3.6-§3.8. Every application ofcopulaABCdrfandrejectionABCmethod is based on a Reference Table of sizesamples and the same given set of summary statistics, after considering that this size is an automatic default choice inABCapplications, and that some summary statistics used in this study are computationally costly especially for large datasets.

The simulation studies will in general summarize posterior estimation accuracy results of eachABCmethod by the average and standard deviation (error) of point estimates of parameters (marginal means and medians of the posterior distribution, and the posterior mode and MLE) and various fit statistics over 10 simulated data replicas from the given model generated from specified true data-generating model parameters, mentioned in the later subsections. These fit statistics, computed relative to these true data-generating model parameters, include the: Mean Absolute Error (MAE) and (the less outlier-robust) Mean Squared Error (MSE) of the above point estimates; coverage (indicator) of the marginal 95% and 50% posterior credible interval estimates of individual model parameters, to be compared to their nominal values; and Kolmogorov-Smirnov (KS) distance and test statistics of the null hypotheses that the estimated univariate marginal posterior distributions (respectively) match the corresponding true exact univariate marginal posterior distributions. Specifically, the weighted one-sample (two-tailed) KS distance and corresponding significance test statistic(Monahan,2011, p.358), based on the estimateddrfweights forcopulaABCdrf, or on equal sample weights (unweighted) forrejectionABC. The KS test statistic has a 95th percentile critical value of 1.358 and a 99th percentile critical value of 1.628, relative to the null hypothesis that the marginal posterior distribution of the given parameter ofcopulaABCdrf(orrejectionABC) equals the exact marginal posterior distribution. The KS statistics can only be computed for the three models for multivariate i.i.d. observations (in §3.1), since they are defined by tractable (Poisson and Gaussian) likelihoods, and thus allow for computations of the exact univariate marginal posteriors, either using direct numerical computation or by using MCMC or other suitable Monte Carlo methods. In contrast, for each network model considered in this paper, the likelihood function is intractable because it is either inexplicit or not computable when the network is of a sufficiently large size(e.g., with more than a handful of nodes,). This makes it impossible to compute the exact marginal posterior distributions, or at least computable in a reasonable time using Monte Carlo methods, and thus, impossible to compute the KS statistics for these models.

Specifically, §3.1 reports the results of simulation studies for three models formultivariate i.i.d. observations, namely: a joint Poisson and Gaussian mixture model defined by two parameters, having a unimodal joint posterior and exact univariate marginal posteriors, which can be directly calculated analytically; a bivariate Gaussian model, with five parameters (2 location and 3 covariance matrix parameters) having a multimodal joint posterior distribution; and a high-dimensional multivariate Gaussian model with 300 location parameters, with the first two parameters having a skewed posterior. Since each of the latter two Gaussian models is defined by a Gaussian likelihood pdf, the exact univariate marginal posterior distributions of the model parameters (respectively) can be calculated using standard MCMC methods. For each of these two models, this study estimates the exact univariate marginal posterior distributions using 10,000 samples generated from a componentwise MCMC Gibbs-and slice-sampling algorithm, which routinely displayed adequate mixing and convergence to the exact univariate marginal posterior distributions, according to univariate trace plots of the MCMC parameter samples. While these MCMC samples are correlated, more efficient estimation of posterior point estimates are obtained from all 10,000 stationary MCMC samples instead of from a thinned subsample(MacEachern and Berliner,1994).

More specifically, each sampling iteration of this MCMC algorithm: performed a Gibbs sampling update of the subset of mean location parameters by drawing a sample from its explicit multivariate Gaussian full conditional posterior distribution, given the data and the remaining model parameters which is easily derivable from the standard posterior calculus of Bayesian Gaussian models under conjugate or uniform priors(e.g., Bernardo and Smith,1994, Appendix A)(specifically, for the bivariate Gaussian model, the Gibbs sampling update involved simple rejection sampling from a uniform prior-truncated bivariate Gaussian for both mean parameters; and for the 300-variate Gaussian model, the sampling update remaining mean model parameters involved a direct draw from a 297-variate Gaussian with fixed diagonal covariance matrix); and for the remaining model parameters having no convenient known form for the full conditional posterior distribution(s), the MCMC algorithm employed a simple version of the slice sampler(Neal,2003)to perform a sampling update, by repeatedly sampling from a wide uniform distribution that surely supported the entire full conditional posterior density of the parameter, until a sample was obtained that yielded a full conditional posterior density value that exceeded the corresponding value of the slice variable updated in this MCMC iteration (for the bivariate Gaussian model, a trivariate slice sampler was used for all the covariance matrix parameters; and for the 300-variate Gaussian model, a univariate slice sampling update was performed for each of the first two location parameters already known to have a skewed joint posterior, while in each update, the shrinkage procedure(Neal,2003)was used to speed the search for the slice). For these two Gaussian models in particular, this componentwise MCMC Gibbs- and slice-sampling algorithm seems to provide the simplest posterior sampling algorithm, while making direct use of the Fundamental Theorem of Simulation(Robert and Casella,2004, §2.3.1)and using no tuning parameters. This is unlike alternative viable Monte Carlo methods, such as Metropolis-Hastings, Hamiltonian(Hoffman and Gelman,2014)and affine-invariant ensemble(Goodman and Weare,2010)sampling algorithms, which require the use of proposal covariance matrices, gradients, or other tuning parameters.

In general, for the simulation studies throughout §3, all the evaluations and comparisons ofcopulaABCdrfandrejectionABCwill be based on varying conditions of the number of data simulations from the likelihood,, relative to the total sample size, with, where for the network models,is the number of nodes of a network. When constructing a Reference Table for ABC, the strategy of simulating summary statistics based on simulating datasets of sizecan potentially be useful in situations where a large sizedataset is being analyzed and/or simulating from the given model is computationally costly. In such large () network scenarios, it is prohibitively costly and practically infeasible to simulate many networks of the same size (number of nodes) as the original network dataset over iterations in an ABC or synthetic likelihood algorithm, especially considering that already it can be prohibitively costly to simulate a single network from ERGM or mechanistic network models (e.g., ERGM given parameters simulates a network using MCMC), for a network of sufficiently large size (number of nodes), let alone to compute point-estimates (e.g., MLE) of the parameters of the given network model. These issues, in the context of Algorithm 1, are addressed by a strategy that simulates network datasets of a smaller size compared to the size of the network dataset under statistical analysis, while using network summary statistics (calculated on the observed dataset and each simulated dataset) that take on values that have the same meaning and are invariant to the size of the network dataset(s) being analyzed, including Maximum Pseudo Likelihood Estimates (MPLEs) of ERGM model parameters based on network size offsets(Krivitskyet al.,2011). These network size invariant summary statistics are reviewed in §3.2. The R packagesergm(Handcocket al.,2023),ergm.count(Krivitsky,2022), andigraph(Csárdiet al.,2024)were used to compute network summary statistics, MPLEs, and compute Monte Carlo MLEs (MCMLEs)(Snijders,2002)of ERGM models, with the MCMLE being a standard commonly-used MCMC approximation of the ERGM MLE. These MLEs will be compared with the MLEs ofcopulaABCdrfandrejectionABCby MAE and MSE. Note that some values of the ERGM parameters can concentrate probability mass on degenerate or near-degenerate networks which concentrate probability mass on a small subset of all possible network graphs with almost or exactly zero edges or almost or all possible edges among thenodes(Strauss,1986; Handcock,2003), which can lead to infinite values of MPLEs and MLEs for one or more ERGM parameters. Therefore, for the network modeling applications of §3.3-3.8 where MPLE summary statistics are used, the results of thecopulaABCdrfandrejectionABCmethods will only be based on the subset of the rows of the Reference Table corresponding to finite-valued MPLE summary statistics. This provided a natural way to address this degeneracy, at least for the models and summaries used in these sections.

For each of the multivariate i.i.d. models, ERGMs, and one of the mechanistic (DMC) network model investigated in the simulation studies of §3.1, §3.3, §3.5, and §3.7, the number of summary statistics (in) equals the number of model parameters (i.e., their dimensionality is equal, dim() = dim()). For each of the three other (Price, NLPA, and DMR) mechanistic network models investigated in the simulation studies of §3.4-§3.5, the initial number of summary statistics exceeds the number of model parameters (i.e., dim()dim()). The asymptotic efficiency of posterior expectations inABCrelies on dim() = dim()(Li and Fearnhead,2018), as mentioned in §1. Therefore, in the simulation scenarios of §3.4-§3.5, where the number of summary statistics exceeds the number of model parameters, we will also consider a semi-automatic approach torejectionABCbased on pre-selecting the dim() most important summary statistics (predictor variables), determined by training adrfregression of the prior samples ofon the corresponding samples of the summary statisticson the Reference Table, with the importance of each summary statistic (variable) efficiently calculated by a simple weighted sum of how many times each summary statistic was split on at each depth in the forest. This semiautomatic ABC method based on preselection of summaries usingdrf, calledrejectionABCselect, will be compared with the results obtained byrejectionABCusing all the available summary statistics (i.e., without any pre-selection), and the results obtained bycopulaABCdrf.

Preliminary results of simulation studies for theq% nearest-neighborrejectionABCalgorithm, for all models studied in §3.1, §3.3-3.5 and §3.7, showed that the 1% nearest-neighborrejectionABCalgorithm tended to perform best in terms of the MAE, MSE, and KS statistics, compared to 2% and 3% nearest-neighbors. Therefore, for space considerations, only the results for 1% nearest-neighborrejectionABCwill be shown throughout §3.

For thecopulaABCdrfmethod, it was found through preliminary simulation studies of all eight models considered in §3.1 and §3.3-3.5, that compared to the smooth univariate kernel density estimators (mentioned in §2), the empirical histogram density estimator (2) had a slightly higher tendency for producing superior posterior mode and MLE estimates based on the meta-posterior, for most models considered in the simulation study, according to MAE and MSE of each individual model parameter, when the dimension dimof the given model parameterswas less than 6. The empirical density estimator also has the advantage of not using a bandwidth parameter, while the choice of bandwidth is an important and non-trivial aspect for the accuracy of smooth kernel density estimation. However, it was found that when the given model parameterhas a sufficiently-high dimension, the product term(for) of the meta-density estimate (see Step 4 of Algorithm 1), based on the empirical density estimator, can become zero for a large majority of thesamples ofin the Reference Table, thereby requiring an extremely large sample(e.g., well-above, say, 10,000) to overcome this issue. Therefore, in such a high-dimensional parameter settings, the kernel density estimator has the advantage of reducing the frequencies of zero from the Reference Table, because it provides a smoother density estimator, and therefore does not require an extremely large sample for the Reference Table. Also, in a related issue, according to some preliminary simulation studies, done as a pre-cursor to the simulation studies reported later in §3.1 and §3.3-3.5, it turned out that the computation of the MLEof the-copula density parameters (in Step 4 of Algorithm 1) based on multiplying (re-scaling) theby, done to avoid evaluating the copula density at the edges of the unit hypercube (as advocated by(Genestet al.,1995; Genest and Nes̆lehová,2007)and others), was not obviously advantageous in terms of MAE and MSE accuracy of posterior mean, median, mode, and MLE estimation. Therefore, §3 throughout presents the results of the simulation studies based on Step 3 of thecopulaABCdrfAlgorithm 1, as described in §2.

Further, recall that both ofcopulaABCdrf’s alternative approaches to posterior mode and MLE estimation are presented in Step 4 of Algorithm 1. It was found through additional preliminary simulation studiescopulaABCdrfover all eight models considered in §3.1 and §3.3-3.5, that in terms of posterior mode and MLE estimation accuracy ofcopulaABCdrf, measured by MAE and MSE, the approach of drawing an extra set of parameter samples from the meta-posterior density estimate can be advantageous (compared to the first approach based on the original Reference Table), but only when the true posterior distribution is high-dimensional and symmetric or skewed (but not highly multimodal). This seems to be reflective of the geometry of the meta-density (distribution). Therefore, for space considerations, we primarily present the results of the posterior mode and MLE estimators ofcopulaABCdrfwhich is only based on the samples generated in the original Reference Table. We only focus on the extra-sample approach to mode and MLE estimation for a high-dimensional setting involving a skewed posterior distribution, and based on the smooth univariate kernel density estimators of the univariate posterior marginals.

SECTION: 3.1Simulation Study: Calculable Exact Posterior Distributions

Here, we first study the accuracy of Algorithm 1 to estimate the directly-calculable true posterior distribution, of a joint bivariate Poissonmodel and a two-component scale normal mixture model with common mean parameter, which has been studied in previous research on ABC methods. The Poissondistribution is defined by a tractable likelihood probability mass function (p.m.f.), assigned a gamma prior distribution(shapeand rate), and exact gamma posterior distribution, with the sample meana sufficient summary statistic for. The scale normal mixture model, the normal distributionwith latent covariateBernoulli, is assigned a uniform prior distribution, yielding the posterior distribution(effectively, with equality up to machine precision) with the summary statistic the sample mean. The Poisson and scale normal mixture distributions, together, define a bivariate distribution model, respectively, with parameters assigned the aforementioned independent prior distributions, and posterior distributions as marginal distributions of. This enables Algorithm 1 to estimate a bivariate posterior mode and MLE of.

Ten replications ofbivariate observationsfrom this bivariate model were simulated from the bivariate distribution model with true data generating parameters, for each of eight conditions defined by sizes of, andsimulations from the exact likelihood of the model.

Table 1 presents detailed representative results for someandwhile Tables 2 and 3 present the results for all the eight conditions of. Overall,copulaABCdrfoutperformedrejectionABCin terms of MAE, MAE, and KS statistics for each of the posterior distribution and mean, median, and mode estimation. TherejectionABCmethod outperformedcopulaABCdrfonly for posterior mean estimation of the mean location parameterof the normal mixture, especially for smallerand. Also,copulaABCdrfandrejectionABCperformed similarly and rather close to the nominal 95% posterior credible intervals, whilecopulaABCdrfperformed better thanrejectionABCin producingposterior credible intervals, while tending to produce interval estimates that were near the nominal values. BothcopulaABCdrfandrejectionABCalways produced significant KS statistics, i.e., estimates of the marginal posterior distributions of model parameters which significantly departed from the exact posterior distribution. Therefore, the univariatedrfregressions trained on the Reference Table of 10,000 samples were unable to very accurately estimate these marginal posterior distributions, but they estimated marginal posteriors that were more accurate than those obtained fromrejectionABC.

Finally, for both ABC methods, the accuracy of estimation from the posterior distribution tended to improve asis increased towards the full dataset sample size, more so for the rate parameter. This suggests that the approach of simulating datasets from the given model of sizesmaller (but not too small or zero), relative to the sizeof the observed dataset being analyzed, can be a reasonable approach for accurate estimation of the model’s posterior distribution, using either ABC method. Intuitively, the sufficient statisticfor the Poisson distribution parameteris sufficient for a dataset of size, and only approximately sufficient for smaller simulation sample sizes. Similarly, for the statisticsummarizing the parameterof the scale normal mixture model.

Next, for the simulation study, we consider a simple 5-dimensional Gaussian model forbivariate observations. While this model is simple, it nevertheless gives rise to a calculable multimodal posterior distribution which is challenging to estimate using traditional (e.g., rejection) ABC methods(Papamakarioset al.,2019; Wang and Rocková,2023). In particular, for the given set of 4 data observations, the model is defined by likelihood, with pdfdetbased on meansandcovariance matrix, and with mean parametersand covariance matrix parameters, with,, andtanh, assigned uniform prior distributions,,,, andas in(Wang and Rocková,2023). Approximating the posterior distribution of this model can be challenging since this posterior is complex and non-trivial, because of the signs of the parametersandare non-identifiable, and thus yield four symmetric modes (due to squaring), and because the uniform prior distributions induce cutoffs in this posterior distribution. For the simulation study for this Gaussian model, ten replicas ofbivariate observationswere simulated using true parametersas in(Wang and Rocková,2023). For analyzing data simulated from this Gaussian model usingcopulaABCdrfandrejectionABC, five summary statistics were used, being the MLEs of the mean and variance of the two variables, and of the covariance matrix parameter. The exact posterior distribution of this Gaussian model was estimated by 10,000 samples generated from the simple componentwise MCMC Gibbs- and slice- sampling algorithm mentioned earlier.

Table 4 presents the results of the simulation study for this Gaussian model with multimodal posterior distribution. Figure 1 shows trace plots of samples of the univariate marginals of 10,000 samples of the exact multimodal posterior distribution, generated by the MCMC Gibbs- and slice-sampler, conditionally on the first of the 10 data replications, for illustration. This trace plot shows the typical result of this MCMC sampling algorithm that was obtained for each Gaussian model studied in this subsection, and given each data replica, namely, that this MCMC sampler produced samples that quickly mixed and converged to the target posterior distribution. Table 4 shows that, on average, and according to MAE and MSE: the marginal posterior mean and median estimation of the true data-generating values of the parameters,, and,copulaABCdrfproduced marginal univariate means and medians that were on average near and closer to their respective (MCMC-estimated) exact values compared to 1% nearest-neighbor rejection ABC. However, for the estimation of the marginal posterior mean and median of the true data-generating values of the parameters,, the marginal posterior mean and median estimates of rejection ABC were closer than those ofcopulaABCdrf, while these estimates of both methods noticeably departed from the true data-generating values. Also, the posterior mode (and MLE) estimates tended to be more accurate on average for rejection ABC based on 5-dimensional kernel density estimation, compared tocopulaABCdrf. The marginal standard deviations tended to be closer to their (estimated) exact values forcopulaABCdrfcompared torejectionABC. Both ABC methods produced mean marginal 95% and 50% coverage of the true data-generating parameters that were similar to those of their respective (estimated) exact values. The marginal posterior distribution estimates ofcopulaABCdrfproduced smaller KS distances to the respective (estimated) exact marginal posterior distributions, compared to the distance of rejection ABC, but all values of the KS test statistics associated with these distances were significant, exceeding its 99% critical value 1.628 for the 2-tailed (one-sample) KS test. This indicated that each of the ABC methods again produced marginal posterior distributions of the model parameter that departed with their respective exact marginal posterior distributions. Overall,copulaABCdrfandrejectionABCperformed similarly with each other in terms of accuracy in estimation from the posterior distribution of this model, while their estimates were not very accurate. ForcopulaABCdrf, this is in retrospect not a very surprising result, because it is seemingly designed to handle skewed or symmetric posterior distributions but not highly multimodal posteriors.

Now we consider the twisted Gaussian model(Haarioet al.,1999), a high-dimensional parametric model defined by hundreds of mean location parameters, and having a calculable (and known) exact but skewed posterior distribution with a strong correlation between some of the parameters. This model has received much interest in the ABC literature since(Liet al.,2017; Nottet al.,2018). This is because it is challenging to estimate its posterior distribution, because the model has many (mean location) parameters, the model’s likelihood function only provides location information, and information about the dependence among these parameters mainly comes from the prior distribution. In particular, here we consider one 300-dimensional observationfrom the twisted Gaussian model defined by likelihoodwith 300 mean (location) parametersassigned the prior distribution (density function):

using, as in(Liet al.,2017; Nottet al.,2018)who considered a 250-parameter version of the model. The posterior correlation betweenandchanges direction depending on whether the likelihood locates the posterior in the left or right tail of the prior, according to earlier graphical illustrations of the prior(Nottet al.,2018).

For bothrejectionABCandcopulaABCdrf, the summary statistic is the single 300-dimensional observation, given by, with corresponding pseudo-data values(for) simulated from the model and used to construct the Reference Table for ABC, along with the prior samples of the mean locations parameters,. Preliminary runs ofcopulaABCdrfframework showed that it yielded better posterior mean estimates of model parameters when univariatedrfregression was performed on the Reference Table in a manner such that (for) each of theandwas regressed on, while thewas regressed on the corresponding, for each of; instead of regressing each individual model parameter on all 300 summary statistics. Therefore, the results of the former approach will only be reported. The exact posterior distribution was calculated using 10,000 samples from the MCMC Gibbs- and slice-sampling algorithm mentioned earlier.

Table 5 presents the results of the simulation study of the twisted Gaussian model, based on one simulated observation and true data-generating parameter,as in(Liet al.,2017; Nottet al.,2018). These results clearly show thatcopulaABCdrfoutperformed 1% nearest-neighbor rejection ABC in terms of estimation of the marginal posterior means median, mode, and the MLE, while producing marginal posterior distributions of the model parameters that tended to be smaller in KS distance compared to the corresponding (estimated) exact marginal posterior distributions (albeit, still with significant departures according to KS significance tests); while producing posterior means, medians, mode, and average marginal posterior interval coverages which were at least rather similar to those produced by the corresponding (estimated) exact values. In particular, for mode and MLE estimation, compared to the first mode and MLE estimation method (described earlier in Step 4 of Algorithm 1), the accuracy of the posterior mode and MLE method was improved by basing it on an extra 100,000 samples from the meta-posterior distribution, according to the second alternative method to posterior mode and MLE estimation, as with both mode and MLE methods (described earlier in Step 4 of Algorithm 1).

In summary, all the results of the simulation studies of this subsection together show thatcopulaABCdrf, as discussed in §1, is equipped to estimate more unimodal, skewed, and high dimensional posterior distributions (as in the Poisson-normal mixture model) or skewed (as in the 300-dimensional twisted Gaussian model), especially compared to the rejection ABC method. However, both ABC methods produced estimates of marginal posterior distributions of model parameters that departed from the exact marginal posteriors according to Kolmogorov-Smirnov distances and their corresponding significant values of their test statistics, and could not easily handle the estimation of multimodal posterior distributions (as in the 5-dimensional bivariate Gaussian model).

SECTION: 3.2An Overview of Network Data Modeling

Network science studies systems and phenomena governing networks, which represent relational data which shape the connections of the world and are ubiquitous and focal discussion points in everyday life and the sciences. A network consists of a set of nodes (i.e., vertices, actors, individuals, or sites, etc.) which can have connections (i.e., edges, ties, lines, dyadic relationships, links, or bonds, etc.) with other nodes in the network; e.g., a social network consists of friendship ties among persons (nodes). A directed network treats each pair of nodesandas distinct, whereas an undirected network does not. Each possible edge tie connecting any given node pair in a network may be (unweighted) binary-valued, or (weighted) count-, ordinal-, categorical-, text-, ranking-, continuous valued; and/or vector valued; possibly as part of a multi-layered (or multiplex, multi-item, or multi-relational) network representing multiple relationships among the same set of nodes. Network summary statistics include network size (number of nodes,), edge count and density (ratio of edge count to maximum edge count in network), node degree (a node’s outdegree is the number of edges starting from it, indegree is the number of edges going into it, while in an undirected network, the outdegree and indegree coincide and equal to degree), the degree distribution over nodes and its geometric weighted form(Snijderset al.,2006), degree assortativity(Newman,2003), counts of 2-stars and triangles, and global(Wasserman and Faust,1994)and average local(Watts and Strogatz,1998)clustering coefficients. The presence of a dyadic network tie can depend on the presence of other dyadic ties, nodal attributes, the network’s evolving dynamic structure and dyadic interactions over time, or on other factors. Also, the network’s structure itself may influence or predict individuals’ attributes and behaviors.

Network data are collected and analyzed in various scientific fields, including the social sciences (e.g., social or friendship networks, marriage, sexual partnerships); academic paper networks (collaboration or citation networks); neuroscience (human brain networks and interactions); political science (international relations, wars between countries, insurgencies, terrorist networks, strategic alliances and friendships); education (multilevel student relation data, item response data); economics (financial markets, economic trading or resource distribution networks); epidemiology (disease spread dynamics, HIV, COVID-19); physics (Ising models, finding
parsimonious mechanisms of network formation); biology and other natural sciences (protein-protein, molecular interaction, metabolic, cellular, genetic, ecological, and food web networks); artificial intelligence and machine learning (finding missing link in a business or a terrorist network, recommender systems, Netflix, neural networks); spatial statistics (discrete Markov random fields); traffic systems and transportation networks (roads, railways, airplanes); cooperation in an organization (advice giving in an office, identity disambiguation, business organization analysis); communication patterns and networks (detecting latent terrorist cells); telecommunications (mobile phone networks); and computer science and networks (e-mail, internet, blogs, web, Facebook, Twitter(X), LinkedIn, information spread, viral marketing, gossiping, dating networks, blockchain network, virus propagation). Network science has seen many reviews due to its longtime scientific and societal impacts(e.g., Wasserman and Faust,1994; Newman,2003; Goldenberget al.,2010; Snijders,2011; Pastor-Satorras and Vespignani,2004; Raval and Ray,2013; Newman,2018; Horvat and Zweig,2018; Bródka and Kazienko,2018; Ghafouri and Khasteh,2020; Loyal and Chen,2020; Hammoud and Kramer,2020; Kinsleyet al.,2020), while the high dimensionality of network data poses challenges to modern statistical computing methods.

Statistical and mechanistic network models are two prominent paradigms for analyzing network data. Astatistical network modelspecifies an explicit likelihood function for the given network dataset, available in closed-form up to a normalizing constant, making standard statistical inference tools generally available for these models, e.g., for parameter estimation and model selection. Statistical network models include the popular large family of ERGMs(Frank and Strauss,1986; Wasserman and Pattison,1996; Lusheret al.,2013; Harris,2013; Schweinbergeret al.,2020; Caimo and Gollini,2022)which uses observable network configurations (e.g.,-stars, triangles) as sufficient statistics. An ERGM is defined by a likelihood with normalizing constant, which is intractable for a network with more than a handful of nodes. The ERGM for a binary undirected or directed network is as follows:

with parameter vectorand its mapping to canonical parameters(with
(3) a linear ERGM if, and is otherwise a curved ERGM); sufficient statisticswhich are possibly dependent on any covariates; and for undirected binary networks, the spaceof allowable networks onnodes is, and for directed binary networks, the spaceis the same but without the restrictions. The ERGM likelihood (3) is
intractable for a binary network with more than a handful of nodes, as the normalizing constant of the model’s likelihood is a sum ofterms (orterms for a directed network) over the sample space of allowable networks. The basic ERGM form (3) allows it to be straightforwardly extended to a general ERGM representation for multi-layered
weighted (valued) networks shown later in §3.8, which combines certain ERGMs(Krivitsky,2012; Krivitskyet al.,2020; Caimo and Gollini,2022)and encompasses ERGMs: for binary networks; with size() offset adjusted (invariant) parameterizations(Krivitskyet al.,2011; Stewartet al.,2019); with nodal random effects(Thiemichenet al.,2016); and for valued (weighted)(Krivitsky,2012; Caimo and Gollini,2022), multilayered(Krivitskyet al.,2020), and dynamic networks(Hannekeet al.,2010; Krivitsky and Handcock,2014; Leeet al.,2020).

A mechanistic network model is able to incorporate domain scientific knowledge into generative mechanisms for network formation, enabling researchers to investigate complex systems using both simulation and analytical techniques. Amechanistic network modelis an algorithmic description of network formation defined by a few domain-specific stochastic microscopic mechanistic rules by which a network grows and evolves over time, which are hypothesized and informed by understanding and related critical elements of the given scientific problem. A typical mechanistic model generates a network by starting with a small seed network, and then grows the network one node at a time according to the model’s generative mechanism until some stopping condition is met, e.g., until a requisite number of nodes is reached, which can possibly number in the millions. There are easily hundreds of mechanistic network models, which originated in physics. For a long time, they were essentially the only type of network models formulated and studied using both mathematical and computer simulation methods. Mechanistic network models include the model ofPrice (1965,1976)used to study citation patterns of scientific papers, and theBarabasi and Albert (1999)model, which introduce preferential attachment rules for directed and undirected networks, respectively. Also, theWatts and Strogatz (1998)model, which produces random graphs with small-world properties, including short average path lengths and high clustering. Further, the duplication-divergence class of models, which describe the evolution of protein-protein interaction networks (where each node is a protein in an organism and two proteins are connected if they are involved in a chemical reaction together). This class includes the Duplication-Mutation-Complementation (DMC) model(Vazquezet al.,2003; Vazquez,2003)and the Duplication-Mutation-Random (DMR) model(Soléet al.,2002; Pastor-Satorraset al.,2003). Other mechanistic models include the KM model(Kretzschmar and Morris,1996; Morris and Kretzschmar,1997), used to study HIV epidemics(Morriset al.,2007; Palombiet al.,2012); along with other mechanistic models(Klemm and Eguiluz,2002; Kumpulaet al.,2007; Procopioet al.,2023). The Price and nonlinear preferential attachment models, and the DMC and DMR models, are further described later within the simulation studies reported in §3.4-§3.5.

We will apply Algorithm 1 to ERGM and mechanistic network models through simulation studies in §3.3-§3.5 and analyses and real-life large networks in §3.6-§3.8. Algorithm 1 can be used to analyze a network datasetusingERGM and/or mechanistic network models, based onscalar and/or vector network statistics,, for. As mentioned, for the analysis of a large network, it is computationally prohibitive to simulate one very large network from the ERGM or mechanistic network model. This issue can be addressed by a strategy which simulates network datasets of a smaller size compared to the size of the network dataset under statistical analysis, while using network summary statistics (calculated on the observed dataset and each simulated dataset) that take on values that are invariant to the size (number of nodes) of the network dataset being analyzed.

In terms of Algorithm 1, we can specify computationally efficient network summary statisticswhich can be used to directly compare between two networks that may have different node sets, size (number of nodes), and densities. In other words, summary statistics which allow for Unknown Node-Correspondence (UNC) network comparisons(Tantardiniet al.,2019, p.2), including MPLEs of size() offset adjusted (invariant) parameterized ERGMs(Krivitskyet al.,2011; Stewartet al.,2019), described below. Other computationally efficient UNC summary statistics include global and average local clustering coefficients (based on triangle counts)(Pržuljet al.,2004; Yaveroğluet al.,2014,2015; Faisalet al.,2017), degree distribution including its mean and variance or geometric form, degree assortativity (propensity for similar nodes to connect), and diameter (average of shortest path lengths over all pairs of nodes in a network)(Yaveroğluet al.,2014; Faisalet al.,2017). Generally speaking, specifying the summary statistics vector, by computationally efficient summary statistics which allow for UNC (size-invariant) comparisons of networks, enables Steps 1(c)-1(d) to simulate and compute these summaries of a network of a smaller size, and to directly compare these summaries with the corresponding summaries of a larger observed network dataset(of size) being analyzed, for each of thesampling iterations of Algorithm 1. This lowers computational cost, compared to computing and comparing such summary based on simulating networks of the same size () as the large observed network in each sampling iteration.

As mentioned, a novel contribution is that for either the ERGM or a mechanistic network model, the UNC network summary statistics vectormay also include MPLEs ofERGMs defined by sufficient statistics, respectively for, with these MPLEs adjusted by network size by specifying in each ERGM an offset term(edge count of network) with fixed coefficient(Krivitskyet al.,2011). The MPLE(e.g., Schmid and Hunter,2023)was introduced for lattice models(Besag,1974), and developed to estimate ERGM parameters(Strauss and Ikeda,1990; Frank and Strauss,1986)because the exact MLE of the ERGM is intractable for a binary network with more than a handful of nodes, as mentioned above. Specifically, for example, if the observed network datasetis an undirected binary network,onnodes, described by scalar or vector valued statisticsfor(where eachmay depend on covariates), then the summary statisticsof(or) can be specified as, whereis the MPLE of the ERGM (3) using sufficient statisticsand offset term the network edge countwith fixed coefficient. Each MPLE summary statistic(for) obtained by maximizing a logistic regression likelihood, as follows:

with, and network change statistics, whereandis networkwithandrespectively. Likewise, MPLEs can be obtained from a directed network, or from a weighted (undirected or directed) network based on a binary representation of a polytomous network(Caimo and Gollini,2022, §4.3). Such a logistic regression likelihood represents a special form of composite likelihood(Lindsay,1988; Varinet al.,2011)which assumes (dyadic) independence of theobservations,,
whereis the networkexcluding. Both the MPLE and MLE of the ERGM are consistent for a growing number of networks observed from the same set of fixed nodes(Arnold and Strauss,1991). The MPLE can be rapidly computed from large networks(Schmid and Desmarais,2017), using divide-and-conquer(Gaoet al.,2022; Rosenblatt and Nadler,2016; Minsker,2019)or streaming methods(Luo and Song,2019), if necessary.

SECTION: 3.3Simulation Study: Exponential Random Graph Models (ERGMs)

Now we consider a simulation study to evaluate and compare the ability ofcopulaABCdrf(Algorithm 1) andrejectionABCto estimate the posterior distribution of ERGM parameters, based on 10 replications ofnode undirected networks from the ERGM, and ofnode undirected networks simulated from this model, for each of five conditions of, being at least roughly,,,andofnetwork nodes. Specifically, the conditionsandfornodes, andandfornodes.

Under thesimulation conditions, each of the 10 network datasets were simulated from the ERGM (3) using true data-generating parametersand defined by network sufficient statistics being the number of 2-stars and triangles. For each simulated network datasetanalyzed by Algorithm 1 using the ERGM, this model was assigned apriorwithwhereis the Hessian matrix of the MPLE of, and the summary statistics were specified as, whereforare the MPLEs for these two network sufficient statistics respectively based on the edge countoffset with fixed coefficient. Likewise, forbased on the edge countoffset with fixed coefficient, and on the sizeof the network datasetsimulated in each iteration of Algorithm 1.

Under thesimulation conditions, each of 10 network datasets was simulated from the ERGM (3) using true data-generating parametersand defined by network sufficient statistics being the number of 2-stars (kstar(2)), the number of triangles (triangles), and the geometric weighted degree distribution (degree1.5) with decay parameter fixed at(Snijderset al.,2006, from equations 11-12, pp.112,126). For each simulated network datasetanalyzed by Algorithm 1 using the ERGM, this model was assigned aprior, given bywithand the summary statistics were specified as, whereforare the MPLEs for these three network sufficient statistics respectively based on the edge countoffset with fixed coefficient. Likewise, forbased on the edge countoffset with fixed coefficient, and on the sizeof the network datasetsimulated in each iteration of Algorithm 1.

Table 6 presents some detailed representative results forofnode networks, and forofnode networks. Table 7 presents the results for each of all the five conditions offornodes and fornodes. Fornodes, in terms of MAE and MSE,rejectionABCtended to outperformcopulaABCdrfin estimation accuracy of the posterior mean, median, and mode. Also,copulaABCdrfoutperformedrejectionABCin terms of MLE accuracy. Further,rejectionABCperformed similarly withcopulaABCdrfin terms of accuracy in estimation of 95% and 50% posterior credible intervals, while being typically near the 95% nominal values. In addition,rejectionABCwas slightly superior in estimation of the 50% posterior credible interval, but both methods often produced 50% interval estimates that were far from this nominal value. Also, whileis increased towards the full network dataset sample size ofnodes, the MAEs and MSEs tended to decrease for each of the point estimates (posterior mean, median, mode, and MLE) for both ABC methods. For,copulaABCdrfproduced MLEs that were competitive with MCMLEs.

Fornodes,rejectionABCtended to outperformcopulaABCdrfin terms of estimation accuracy of the posterior mean, median, and mode, especially for parametersand. Also,copulaABCdrfoutperformedrejectionABCin terms of MLE accuracy for all model parameters, and in terms of accuracy in estimation of 95% posterior credible intervals, while being typically near the nominal values. Further,rejectionABCtended to outperformcopulaABCdrfin terms of accuracy in estimation of 50% posterior credible intervals. Again, both ABC methods often produced 50% interval estimates far away from this nominal value. Finally, asis increased towards the full network dataset sample size ofnodes, for each of the two ABC methods, there was no clear pattern of the MAEs and MSEs decreasing for each of the point estimates, and for the posterior credible intervals to approach their nominal rates.

SECTION: 3.4Simulation Study: Preferential Attachment (PA) Models

Mechanistic network models defined by preferential attachment rules, including the Price model and the Nonlinear Preferential Attachment (NLPA) model, grows a directed or undirected network by introducing a new node at each stage and linking the new node to any given existing nodewith probabilityNew node attaches to old node), with some constant parameter, andis the degree of node(indegree for a directed network); and power parameter. In particular, the Price model grows a directed binary citation network one article (node) at a time (each edge being a paper citing another paper), withnew article cites existing article, withand constant parametercentered around 1, andis the in-degreeof existing article, such that each new article citesexisting articles on average, and the number of articles a new article cites follows a Binomialdistribution, with success probability parameter, and withset as the maximum outdegree of(Raynalet al.,2022). The unknown Price model parameters to be estimated from the observed directed network datasetare. For an undirected (binary) network, theBarabasi and Albert (1999)(BA) model has the same form, but without the assumptionsand, and based ondefined as the degree of existing node.

The NLPA model(Krapivskyet al.,2000)generalizes the BA model, usingnew node attaches to existing node, withthe degree of existing node. Since possibly, the number of nodes a new node attaches was assumed to follow a truncated Binomialdistribution which supports only positive counts. For the NLPA model, the unknown parameters to be estimated from the given undirected network dataset are. Thus from an observed network dataset, the unknown model parameters to be estimated are, andis chosen as the maximum degree of.

Now we consider simulation studies that evaluate and compare the ability ofcopulaABCdrf(Algorithm 1) andrejectionABCto estimate the posterior distribution of the parameters of the Price model, and the parameters of the posterior distribution of the NLPA model. The simulation studies are based on 10 replications ofnode directed networks from the Price model (undirected networks from the NLPA model, respectively); and ofnode undirected networks simulated from the Price model (undirected networks from the NLPA model, respectively). These simulations were done for each of five conditions of, being at least roughly,,,andofnetwork nodes. Specifically, the conditionsandfornodes, andandfornodes. The true data-generating parameters of the Price model were specified as, while the true data-generating parameters of the NLPA model were.

For each network datasetsimulated from the Price model and analyzed by Algorithm 1 using this model, this model was assigned a uniform prior(withunderandfor) and used vector of summariesspecified by network size invariant offset MPLEs of geometrically weighted degree, its decay estimate, and triangle count. For each network datasetsimulated from the NLPA model, and analyzed by Algorithm 1 using this model, the NLPA model was assigned a uniform priorand using a vector of summariesspecified by the network size invariant offset MPLEs of density, average clustering coefficient, and diameter. The MPLE summariesof each networkused offsets based onnodes (or). The MPLE summariesof each networkof smaller size(beingfor, andfor) were simulated in each iteration of the ABC algorithm used offsets based onnodes, and based on a Binomialdistribution (or the truncated binomial for NLPA).

Table 8 presents some representative detailed results of the simulation study ofcopulaABCdrfandrejectionABC, for the Price and NLPA models, for pairs of values ofandwith, namely,for, andfor. Tables 9 and 10 present the results of MAE, MSE, accuracy of 95% and 50% posterior interval credible interval estimates ofcopulaABCdrfandrejectionABCfor the Price and NLPA models, for each of the fiveconditions within each ofand.

In particular, since for each of the Price and NLPA models, there were 3 summary statistics relative to the two models parameters, we not only considered for each model implementations ofrejectionABCusing all 3 summary statistics. We also considered, for comparison purposes, each implementation ofrejectionABCbased on pre-selecting two of the most important of the 3 summary statistics, according to the results obtained from a variable importance analysis based on training adrfmultivariate regression on a Reference Table of simulated model parameters on all 3 summary statistics (covariate variables), as described earlier in §3. For the Price model,drfvariable importance analyses found that over all ten replicas of network datasets under theandcondition, that the network size invariant offset MPLEs of geometrically weighted degree, and its decay estimate, were always the most important summary statistics of the three total summaries. Also, for each of the all other simulation conditions ofand, thedrfvariable importance analyses found that over all ten replicas of network datasets, the most important summaries were always the network size invariant offset MPLEs of geometrically weighted degree and triangle counts. For the NLPA model, thedrfvariable importance analyses always found that network density and average clustering coefficient were the two most important network summaries of the three total summaries, compared to network diameter summary, for each of all the 10 network replicas, and within each of all conditionsandfornodes, and all conditions ofandfornodes.

From the results of Table 9, on the Price model, it can be concluded overall thatcopulaABCdrfandrejectionABCperformed similarly. Also,rejectionABCbased on usingdrfto preselect two of the three most important summaries, produced results that sometimes slightly improved but were usually similar to the results ofrejectionABCusing all three summaries. More specifically, for the parameterandnodes,copulaABCdrfslightly outperformedrejectionABCin MSE of posterior mode estimation and MLE, andrejectionABCoutperformedcopulaABCdrfin MAE of posterior mode and MLE. For the parameterandnodes,copulaABCdrfslightly outperformedrejectionABCin estimation accuracy of the 95% credible interval, while being close to the nominal level, whilerejectionABCslightly outperformedcopulaABCdrfin terms of MSE of estimation of the posterior mode and MLE. For the parameterandnodes,copulaABCdrfslightly outperformedrejectionABCin MSE of estimation of the posterior mode and MLE,rejectionABCoutperformedcopulaABCdrfin MAE of estimation of the posterior mode and MLE. For the parameterandnodes,copulaABCdrfoutperformedrejectionABCin accuracy in estimation of the 95% posterior credible interval, and slightly outperformedrejectionABCin terms of accuracy in estimation of the 50% posterior credible interval, while not being near the nominal rate a few times.

From the results of Table 10 on the NLPA model, it can be concluded that, overall,copulaABCdrfandrejectionABCperformed similarly, whilerejectionABCbased on usingdrfto preselect two of the three most important summaries producing results that often improved on results ofrejectionABCusing all three summaries. More specifically, for the parameterandnodes,copulaABCdrfslightly outperformedrejectionABCin MAE and MSE of posterior mean estimation, and MAE of posterior median estimation. Also,rejectionABCslightly outperformedcopulaABCdrfin MSE of posterior median estimation and in MAE and MSE in estimation of the posterior mode and MLE. For the parameterandnodes,copulaABCdrfslightly outperformedrejectionABCin accuracy of the 50% credible interval estimation, while not often being close to the nominal rate. In addition,rejectionABCoutperformedcopulaABCdrfin MAE of posterior mean and median estimation, and in MAE and MSE of estimation of the posterior mode and the MLE, and in estimation of the 95% posterior credible interval, while typically being close to the nominal rate; while for the parameterandnodes,copulaABCdrfslightly outperformedrejectionABCin accuracy of the 50% credible interval estimation, whilerejectionABCoutperformedcopulaABCdrfin terms of MAE and MSE in estimation of the posterior mead, median, mode, and the MLE, and in accuracy of estimation of the 50% posterior credible interval, while often being far from the nominal level. For the parameterandnodes,copulaABCdrfslightly outperformedrejectionABCin MAE in posterior mean estimation, MSE in estimation of the posterior median, the posterior mode and the MLE, and in accuracy in estimation of the 50% posterior credible interval, while not always close to the nominal level. Also,rejectionABCoutperformedcopulaABCdrfin MAE of estimation of the posterior mean, median, and 95% posterior credible interval coverage, while usually being close to the nominal level.

Tables 9 and 10 showed that, for each of thecopulaABCdrfandrejectionABCmethods, asgets smaller relative to, that roughly, and on the most part, MAE and MSE of estimates tend to get smaller, and the 95% and 50% posterior credible intervals more closely approach their respective nominal rates. Intuitively, the summary statisticsbecome less sufficient asgets smaller relative to.

SECTION: 3.5Simulation Study: Duplication-Divergence Models

The DMC model and the DMR model each grows an undirected network ofnodes by starting with a seed network ofnodes, and then repeats the following steps until requisite number of nodes () is reached: Add new node (), then add edge between the new node and each neighbor of a randomly chosen existing node,. Then, for the DMC model, the remaining steps are as follows: for each neighbor of the chosen node, randomly select either the edge between the chosen node and the neighbor, or the edge between the new node and the neighbor, and remove that edge with probability; and then add an edge between the chosen node and the new node with probability. Alternatively, for the DMR model, the remaining steps are as follows: each edge connected to the new node is removed independently with probability, and an edge between any existing node and the new node is added with probability.

Now we consider simulation studies which evaluate and compare the ability ofcopulaABCdrf(Algorithm 1) andrejectionABCto estimate the posterior distribution of the parameters of the DMC model, and the parameters of the posterior distribution of the DMR model. As before, the simulation studies are based on 10 replications ofnode undirected networks from the DMC model (undirected networks from the DMR model, respectively); and ofnode undirected networks simulated from the DMC model (undirected networks from the DMR model, respectively). These simulations are done for each of five conditions of, being at least roughly,,,andofnetwork nodes. Specifically, the conditionsandfornodes, andandfornodes. The true data-generating parameters for the DMC model were set as, while the true data-generating parameters of the DMR model were.

For each network datasetsimulated from the DMC model (DMR model, respectively) and analyzed bycopulaABCdrfandrejectionABCdrf, the DMC model (DMR model, resp.) was assigned a uniform prior(uniform prior, resp.) and using vector of summariesspecified by network size invariant offset MPLEs of undirected network summary statistics of mean degree and triangles (of local and global average clustering coefficients, and degree assortativity, resp.). The MPLE summariesof each networkused offsets based onnodes (or), while the MPLE summariesof each networkof smaller size(beingfor, andfor) were simulated in each iteration of the ABC algorithm using offsets based onnodes.

Table 11 presents some representative detailed results of the simulation study ofcopulaABCdrfandrejectionABC, for the DMC and DMR models, for pairs of values ofand. Specifically, for the DMC model,for, andfor; and for the DMR model,for, andfor. Tables 12 and 13 present the results of MAE, MSE, accuracy of 95% and 50% posterior interval credible interval estimates ofcopulaABCdrfandrejectionABCfor the DMC and DMR models, for each of the fiveconditions within each ofand.

In particular, since for the DMR models, there were 3 summary statistics relative to the two models parameters, we not only considered for each model implementations ofrejectionABCusing all 3 summary statistics. We also considered, for comparison purposes, each implementation ofrejectionABCbased on pre-selecting two of the most important of the 3 summary statistics, according to the results obtained from a variable importance analysis, based on training adrfmultivariate regression on a Reference Table of simulated model parameters on all 3 summary statistics (covariate variables). For this network model,drfvariable importance analyses found that over all ten replicas of network datasets under theandcondition, that the network size invariant offset MPLEs of global average clustering coefficients (importance measure of 1) and degree assortativity (importance measure of 0.60) were the two most important summary statistics. Meanwhile, for each of the all other simulation conditions ofand,drfvariable importance analyses found that over all ten replicas of network datasets, the network size invariant offset MPLEs of undirected network summary statistics, of local and global average clustering coefficients, were always the most important two summaries of the 3 total summary statistics.

From the results of Table 12, on the DMC model, it can be concluded that overallcopulaABCdrfandrejectionABCperformed fairly similarly. Specifically, for the parameterandnodes,copulaABCdrfslightly outperformedrejectionABCin MAE of posterior mean and median estimation, and in accuracy in estimation of the 50% credible interval, while usually being around the nominal rate. Also,rejectionABCslightly outperformedcopulaABCdrfin MSE of posterior mean estimation, and in MAE and MSE in the estimation of the posterior mode and MLE. For the parameterandnodes,copulaABCdrfoutperformedrejectionABCin MAE and MSE of estimation of the posterior mean, median, mode and MLE, as well as accuracy in estimation of the 95% credible interval and the 50% credible interval, while usually being around the nominal rate in each case, especially forand. For the parameterandnodes,copulaABCdrfslightly outperformedrejectionABCin accuracy in estimation of the 50% credible interval, while often not being around the nominal rate. Further,rejectionABCslightly outperformedcopulaABCdrfin MSE of the estimation of the posterior mode and MLE. For the parameterandnodes,copulaABCdrfoutperformedrejectionABCin MAE and MSE of the estimation of the posterior median, median, mode, and the MLE.

From the results of Table 13, on the DMR model, it can be concluded that, overall,copulaABCdrfandrejectionABCperformed rather similarly. Meanwhile,rejectionABCbased on usingdrfto preselect two of the three most important summaries, produced results that often noticeably improved on results ofrejectionABCusing all three summaries. More specifically, for the parameterandnodes,copulaABCdrfslightly outperformedrejectionABCin MSE of posterior mean estimation, MAE and MSE of posterior mode estimation, and in accuracy of the 50% credible interval estimation sometimes being close to the nominal rate. For the parameterandnodes,rejectionABCoutperformedcopulaABCdrfin MAE and MSE in the estimation of the posterior mean, median, and mode, MSE in posterior median estimation, and slightly outperformed in accuracy of the 95% credible interval estimation while usually being close to the nominal rate. Also,copulaABCdrfoutperformedrejectionABCin accuracy of the 50% credible interval estimation, while usually being not close to the nominal rate. For the parameterandnodes,copulaABCdrfslightly outperformedrejectionABCin MSE of posterior mean estimation, MAE in the estimation of the posterior median, mode, and the MLE, and in accuracy of the 50% credible interval estimation, while sometimes being rather near the nominal rate. Meanwhile,rejectionABCslightly outperformedcopulaABCdrfin terms of MSE in estimation of the posterior mode and MLE. Finally, for the parameterandnodes,rejectionABCslightly outperformedcopulaABCdrfin MAE and MSE in estimation of the posterior mode and the MLE, and in accuracy of 95% credible interval estimation, while usually being close to the nominal rate.

Tables 12 and 13 showed that, for each of thecopulaABCdrfandrejectionABCmethods, asgets smaller relative to, that at least roughly and on the most part, MAE and MSE of estimates tend to get smaller, and the 95% and 50% posterior credible intervals more closely approach their respective nominal rates. Intuitively, the summary statisticsbecome less sufficient asgets smaller relative to.

SECTION: 3.6Real Citation Network Analysis

With Algorithm 1 validated by simulation studies, we apply this algorithm to analyze real-life datasets. Specifically, we now analyze a large binary directed citationHepPhnetwork dataset, a arXiv High Energy Physics paper citation network ofpapers (nodes) andcitations (directed edges) among them(Rossi and Ahmed,2015). (This dataset was obtained from https://networkrepository.com/cit.php). This dataset is analyzed by the ERGM model and by the Price model, each model using a vector of summary statisticsbeing the network size invariant offset MPLEs (resp.) of geometrically weighted in-degree distribution (gwidegree), its decay (degree weighting) parameter estimate (gwidegree.decay), andtrianglecount. The ERGM, based on network sufficient statisticsgwidegree,gwidegree.decay, andtriangle, was assigned a trivariate normal prior. The Price model was assigned priorsand.

The MPLEs for the three network summaries (resp.) could not be computed directly on this large network; these computations were prohibitively long. Therefore, these 3 MPLEs were estimated by their geometric median over 100,000 subsamples of the 28,093 papers (nodes), each subsample inducing a subgraph of size, providing an outlier-robust divide-and-conquer (DAC) subsampling MPLE estimator(Minsker,2019), resulting in the following MPLEs (resp.): forgwidegree, -0.8418461; forgwidegree.decay, 0.2365466; fortriangle: 1.6904300. Recall from §3.2 that the MPLE is the MLE for logistic regression, and they are both consistent over a growing number of networks, observed from the same set of fixed nodes (while the MPLE assumes dyadic independence of the edge observations). These three DAC MPLEs then specified the vector of summary statisticsfor the real observed citationHepPhnetwork dataset, analyzed by each of the ERGM and Price model using Algorithm 1, with each model simulating a network of sizein each algorithm iteration.

Table 14 presents the posterior estimates and MLEs obtained fromcopulaABCdrf, this ABC algorithm. Running Step 2 of this algorithm, for model selection, output a posterior probability estimate offor the Price model, compared to the ERGM. This table also presents the results ofrejectionABC, for ERGM, and for the Price model based on all three network summary statistics. In addition, results are presented for the Price model after pre-selecting the most important summary statistics (variable) of the three total summaries, based on adrfregression fit to the Reference table, regressing the prior parameter samples on the samples of all three summary statistics. It was found that among all three network statistics, being the network size (n) invariant MPLEs ofgwidegree(importance measure of 0.14),gwidegree.decay(importance 0.07), andtrianglecount (importance 0.79), the first and third of these statistics were most important and thus pre-selected. Table 14 shows that for the Price model, the posterior-based parameter estimates were similar across allcopulaABCdrfandrejectionABCmethods, while for ERGM they were less similar.

SECTION: 3.7Real Multilayer Network Analysis

The multilayer, weighted, and directedBostonBomb2013Twitter network dataset consists ofpersons (nodes), andcount-weighted edges, acrosslayers: retweets, mentions and replies, occurring between dates 4-15-2013 and 4-22-2013 of the Boston Bombing Attacks of 2013(De Domenico and Altmann,2020). (The dataset was obtained fromhttps://manliodedomenico.com/data.php). Using Algorithm 1, theBostonBomb2013network dataset was analyzed by a three-Layer ERGM with Poisson reference measure,

which was assigned a multivariate normal priorfor 18 parametersof six network sufficient statistics (vector) specified for each of the 3 layers, namely: (1) Indicator of edge weight equal to 1 in the given layer (equalto(1)); (2) Indicator of edge weight greater than 1 in the given layer; (3) minimum mutuality (mutual.min) which is analogous to mutuality for binary networks; (4) triad closure represented by transitive weights with twopath = min, combine = sum, and affect = min(transitiveweights.min.sum.min), which is analogous to triangle count from a binary network; (5) triad closure represented by transitive weights with twopath = min, combine = max, and affect = min(transitiveweights.min.max.min), which is analogous to a count of transitive ties from a binary network; and finally (6), the termCMP, which specifies the reference measureby the Conway-Maxwell-Poisson (CMP) distribution for the count-weighted edges, with corresponding coefficient parameter, which controls the degree of dispersion relative to Poisson distribution. In particular,defines the Poisson distribution;defines a more underdispersed distribution; the valueleads to the Bernoulli distribution;defines a more overdispersed distribution; andcorresponds to the geometric distribution, being most overdispersed. More details about these network statistics of valued networks and the CMP distribution for ERGM are provided elsewhere(Krivitsky,2012).

These 18 ERGM sufficient statistics could not be easily computed on theBostonBomb2013network dataset, due to its massive size. Therefore, these sufficient statistics were approximated by computing 21 (mostly) UMP summary statistics, each of which were computable on the full network, for each of the 3 network layers. The values of the summariesof theBostonBomb2013network dataset are shown in Table 15.

ThecopulaABCdrf(Algorithm 1) was run on this model, based on these network data summaries, and based on summariesof each networkofnodes simulated from the ERGM, conditionally on given proposed model parameters, in each iteration of this algorithm. The posterior distribution and MLE estimates delivered by the algorithm are shown in Table 16. This table also shows the posterior parameter estimates ofrejectionABCusing the same summary statistics and. The posterior estimates noticeably differed between these two ABC methods.

To provide a further investigation, a simulation study ofcopulaABCdrfandrejectionABCwas conducted using the posterior mean estimates shown in Table 16 as the true data-generating parameters of the same multilayer ERGM, and based onnodes and10 and 20, while noting that even these analyses were very computationally expensive. Tables 17, 18, and 19 present the results of this simulation study. Table 17 shows the results show MSEs and MAEs for the posterior mean, median and mode estimates. Table 18 presents the MLE estimates compared to MCMLEs. Table 19 presents the mean 95% and 50% interval coverage. According to these three tables, both ABC methods were competitive, whilecopulaABCdrftended to produce superior results.

SECTION: 3.8Real Social Network Analysis

We now apply Algorithm 1 to analyze the massivefriendstersocial network ofpersons (nodes) and 1,806,067,135 undirected edges(Yang and Leskovec,2012)(This dataset was obtained from https://snap.stanford.edu/data/com-Friendster.html), using the NLPA model, assigned uniform priors, and using summary statistics(Average clustering coefficient, Diameter, Density). Also, these network summaries also computed from a network of sizesimulated from the model (given a set of proposed parameters) in each of theiterations of Algorithm 1. The algorithm routinely delivered posterior distribution and MLE estimates, shown in Table 20, despite the sheer size of the dataset. Also, this table reports the results ofrejectionABCusing all the three summary statistics, and the results ofrejectionABC. For therejectionABCanalysis, the first two summary statistics were pre-selected as the most important, among the three total summary statistics, namely, average clustering coefficient (with importance measure of 0.66), diameter (importance 0.22), and density (importance of 0.12). The importance of each summary was measured fromdrfregression performed on the Reference Table, as before, regressing the prior parameter samples on all three summaries. The preselection of summaries ledrejectionABCto produce posterior mean and median estimates that were similar to those ofcopulaABCdrf.

SECTION: 4Conclusions and Discussion

This article introducedcopulaABCdrfas a framework that unifies previous methods ofcopulaABCandabcrf, which were introduced separately in previous articles. This unified method aimed to provide, in a single unified framework, a wide range of inferences from the approximate posterior distribution (including posterior means, medians, mode, and the MLE) for models that may be defined by intractable likelihoods and many parameters, and model selection. All of this was done while being able to automatically select the subset of the most relevant summary statistics, from a potentially high number of summary statistics, without requiring the user to identify all the important summary statistics by hand before data analysis. Further,copulaABCdrfavoids the need to use tolerance and distance measure tuning parameters ofrejectionABCmethods.

This paper also proposed a new solution to the simulation problem in ABC, which is based on simulating datasets from the exact model likelihood of size smaller than the potentially-vary large size of the datasets being analyzed by the given model. This strategy could potentially be useful for models from which it is computationally-costly to simulate datasets from, including models for large networks such as ERGM and mechanistic network models, using UNC summary statistics calculated on simulated network datasets that were smaller than the potentially large network dataset being analyzed.

Based on the results of many simulation studies performed in this paper, evaluating and comparingcopulaABCdrf,rejectionABC, and semiautomatic ABCrejectionABCselectmethods based ondrf, the following general conclusions can be made:

For the low dimensional parametric models () considered in this paper,copulaABcdrfandrejectionABCmethods (includingrejectionABCselectwhich uses pre-selection of summary statistics usingdrf) were competitive in terms of MAE and MSE of estimation of marginal posterior mean and median, and of the posterior mode and MLE of the model parameters. Also,rejectionABCselectoften outperformedrejectionABCwithout pre-selection of the subset of summaries, and often produced posterior estimates that were similar than those ofcopulaABcdrf.

ThecopulaABcdrfframework tended to outperformrejectionABCmethods in the estimation accuracy of univariate marginal posterior distributions, according to KS distance to the exact univariate marginal posterior distributions. However, all KS tests of all ABC methods indicated statistically significant departures.

For high-dimensional parametric models () with posterior that was not highly multimodal,copulaABcdrffar outperformedrejectionABCmethods.

For all ABC methods, posterior estimation accuracy tended to be best when, and accuracy decreased asdecreased below 1, which is not surprising because then the summaries become less informative about the data.

For all ABC methods, estimation from the posterior distribution can be challenging when the true posterior is highly-multimodal.

Further, it was found in the simulations that the network size-offset MPLEs were useful, but too computationally costly, compared to the other UNC network summary statistics, which were far more rapidly computable because they did not require optimization algorithms. For future research, computational speed can be drastically improved by using parallel computing methods to compute each of the summary statistics while constructing the Reference Table.

The results suggest thatdrfis the main driving force ofcopulaABCdrf, as suggested, for example, by the fact thatcopulaABCdrfoften produced results similar to that ofrejectionABCselect. However, according to the KS tests,drfalways produced univariate marginal posterior estimates that significantly departed from the corresponding exact posterior densities. On a related note, after the initial writing of this manuscript in early January 2024 (see Acknowledgements),drfwas proposed as a tool for recursively estimating weights for sequential Monte Carlo ABC Algorithms(Dinhet al.,2024), for models defined by intractable likelihoods with less than five parameters. ThecopulaABCdrfframework, seemingly because it is based on the meta-posterior distribution approximation, was limited (like therejectionABCmethods) to the accurate estimation of more symmetric or skewed unimodal posterior distributions. The framework is partially motivated by the fact that the posterior giveni.i.d. observations converge to a multivariate normal distribution asymptotically asof i.i.d. observations, according to the Bernstein von Mises theorem, under mild regularity conditions(Kleijn and van der Vaart,2012).

In other words,copulaABCdrfbased on the meta-is limited in estimating highly-multimodal posteriors, at least according to one simulation study. For future research, this issue can potentially be addressed by using more flexible copula density functions which can handle nonlinear relationships among parameters, perhaps while taking advantage of recent developments in the continually-active field of copula-based multivariate density estimation. However, such more flexible nonlinear copulas can be very computationally costly to estimate, especially for high dimensional parameters. Still, finding a method to efficiently compute estimates of such a more flexible copula density in high dimensional settings seems worth pursuing in future research.

SECTION: Acknowledgments

The author thanks anonymous reviewers for providing helpful comments on a previous version of this manuscript, which helped improve its presentation. This manuscript was presented at a Biostatistics Seminar at the Medical College of Wisconsin during March 19, 2024, and at the conferences COMPSTAT 2024 (University of Giessen, 27-30 August 2024) and CFE-CMStatistics 2024 (King’s College London, 14-16 December 2024). The initial version of this manuscript, including the results of simulation studies and analyses of real networks, obtained using Algorithm 1 implementing Distribution Random Forests and copula modeling, was presented as a report in a grant proposal submitted to the U.S. National Science Foundation on February 14, 2024. The authors declare no conflicts of interest. The real datasets can be obtained through sources cited within the paper. The R software code files, used to simulate the data and to analyze the real and simulated datasets, can be obtained from the author viahttps://github.com/GeorgeKarabatsos/copulaABCdrf.

SECTION: References