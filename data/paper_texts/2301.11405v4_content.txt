SECTION: Discriminative Entropy Clusteringand its Relation to K-means and SVM
Maximization of mutual information between the model’s input and output is formally related to “decisiveness” and “fairness” of the softmax predictions, motivating these unsupervised entropy-based criteria for clustering. First, in the context of linear softmax models, we discuss some general properties of
entropy-based clustering. Disproving some earlier claims, we point out fundamental differences with K-means. On the other hand, we prove the margin maximizing property for decisiveness establishing a relation to SVM-based clustering. Second, we propose a new self-labeling formulation of entropy clustering for general softmax models. The pseudo-labels
are introduced as auxiliary variables “splitting” the fairness and decisiveness.
The derived self-labeling loss includes the reverse cross-entropy robust to pseudo-label errors and
allows an efficient EM solver for pseudo-labels.
Our algorithm improves the state of the art on several standard benchmarks for deep clustering.

SECTION: 
(MI) was proposed as a criterion for clustering by Bridle et al..
It is motivated as a general information-theoretic measure of the “correlation” between the dataand the class labels. Starting from MI definition as Kullback–Leibler (KL) divergence between the joint density and the product of the marginals forand,
Bridle et al.derive a simple clustering loss for softmax models.

The MI criterion also unifies some well-known generative
and discriminative approaches to clustering. In particular,
consider two equivalent entropy-based MI formulations

whereandare theand thefunctions over
distributions corresponding to the random variablesand.
Two terms in () can directly evaluate classespredicted by discriminative posterior models, e.g. softmax models. As detailed in Section,
these two terms represent standard clustering criteria,and,
used for “deep” clustering with neural networks.
On the other hand, the equivalent formulation of MI in () relates
to standard generative algorithms for clusteringand unsupervised or weakly-supervised image segmentation.
Such algorithms minimize the entropyof datain each cluster, where fitting density models helps to
estimate the entropy. Sectiondetails the relation of criterion
() to the most basic generative clustering algorithm, K-means.

Despite equivalence, criteria (,) can lead to clustering algorithms producing
different results depending on their specific generative or discriminative model choices, i.e..
For example, Figureshows optimal solutions for (a) K-means minimizing theof each cluster, i.e.
entropyassuming isotropic Gaussian density, and (b) the linear softmax model minimizing (). While both algorithms make linear decisions, K-means produces compact clusters due to its implicit bias to
isotropic Gaussian densities. In contrast, the linear softmax model finds balanced or “fair” clusters with “decisive” decision boundary corresponding to the, as we later prove in Theorem().
We will revisit Figureagain.

This paper’s focus is clustering based on softmax models and unsupervised entropy loss functions ()
inspired by. We refer to this general group of methods as.
The rest of the introduction provides the background and motivation for our work.
Sections-present the necessary terminology for the discriminative entropy clustering problem,
its regularization, and itsformulations. In particular,
Sectiondiscusses the significance of model complexity and data representation.
Finally, Sectionsummarizes our main contributions presented in the main parts of the paper.

SECTION: 
This Section introduces our terminology for entropy-based clustering
with softmax models. We consider discriminative classification models that could be linear (single layer) or
complex multi-layered, and assume probability-type output often interpreted as a
(pseudo-) posterior. Typically, such outputs are
produced by thefunctionmappingraw classifier outputs, e.g. real-valued “logits” or “scores”,
toclass probabilities

forming a categorical distributionrepresenting a point on the

For shortness, this paper uses the same symbol for functions and examples of their output, e.g. specific prediction values. In particular,may denote the prediction for any given input. Ifis an integer,denotes the prediction for
the specific examplein the training dataset.

The simplestsoftmax model can be defined as

where for any input vectorthe matrix of parametersproduces-logit vectormapped to a probability distribution by the softmax function.
For shortness, we use arepresentation for the linear classifier so thatstands for an affine transformation including the.

More complex non-linear network models compose a linear softmax model ()
with somelayers mapping inputto “deep” features

where the trainable parametersof theare distinguished from the linear classifier
parameters. The linear model ()
is a special case of () for. Typical “deep” representationsignificantly
changes the dimensions of the input. It is convenient to assume thatalways represents the dimensions of
the linear head’s input, i.e. the (homogeneous) matrixhas size.

Assuming softmax models as above, Bridle et al.derive the following clustering loss for data

based on the Shannon entropyfor categorical distributions.
The average entropy of the model output

representsin ().
The entropy of the average output

is the entropy of class predictions over the whole dataset corresponding toin ().

Loss () is minimized over model parametersandin () or
(), e.g. byor.
Larger entropyencourages “fair” predictions with a balanced support of all categories across
the whole dataset, while smallerencourages confident or “decisive” prediction
at each data point suggesting that decision boundaries are away from the training examples.

SECTION: 
Criterion () provides strong constraints for
well-regularized or simple parametric models. For example, in the case of linear softmax models (),
we prove the margin-maximizing property, see Figure(b), relating () to SVM clustering.

Clustering algorithms for softmax models can also be motivated by
the powerful representation of data behind the deep network models ().
Some criteria related to MI are also studied in the context of,,
but this is not our focus. We study (,) as a clustering criterion
where decisionsare optimized for fixed data. Nevertheless, this approach applies to
complex networks () where internal layers can be seen as
responsible for representation. Our experiments with networks do not evaluate the quality of
representation separately from clustering and view the internal layers mainly as an integral part of a complex model.
Instead, we are concerned with regularization of complex models in the context of clustering.

SECTION: 
Bridle & McKayargue that MI maximization may allow arbitrarily complex solutions for
under-regularized network models, as illustrated in Figure(a) for ().
They note that

””

For example, a Bayesian approach to network regularizationcombines training losses with the squarednorm of all network weights interpreted
as aor.
Followingand, regularized version of the entropy clustering loss () incorporates the norm of network parameters motivated as their isotropic Gaussian prior

whererepresents equality up to an additive constant andis a uniform distribution overclasses.
The equivalent loss formulation () uses KL divergence
motivated inby the possibility to generalize the fairness constraint to any target balancing
distribution different from the uniform.

Unsupervised representation learning techniquesare also relevant as mechanisms for constraining the network. In particular,techniques are widely used
for both clustering and representation learning.
For example, maximization of MI between predictions for inputand its augmentationcan improve representation.

For large network models employed in this work, we use only generic network
regularization techniques based on squarednorm of the network weights ()
and a standard self-augmentation lossdirectly enforcing consistent clusteringfor input pairs

whereis the set of all pairs of augmented examples. This loss implies that
similar inputs are mapped to deep features equidistant from the decision boundary.
We refer to this asbetween the space of inputsand
the embedding space of “deep” features, see Figure(b).

SECTION: 
Optimization of losses () or () during network training could be
done with standard gradient descent or backpropagation. However, the difference between the two entropy
terms implies non-convexity presenting challenges for the gradient descent.
This motivates alternative approaches to optimization. It is common to approximate () with someincorporating auxiliary or hidden variablesrepresentingfor unlabeled data points,
which are estimated jointly with optimization of the network parameters.
Typically, suchapproaches to entropy clustering iteratively
optimize the surrogate loss over pseudo-labels and network parameters, similarly to
Lloyd’s algorithm for-means or EM algorithm for Gaussian mixtures.
While the network parameters are still optimized via gradient descent,
the pseudo-labels can be optimized via more powerful algorithms.

For example,formulate self-labeling using the following constrained optimization problem with discrete pseudo-labelstied to predictions by

wherearedistributions, i.e. the vertices
of the probability simplex.
Besides proximity betweenand, the cross-entropy in ()
encourages the decisiveness, while the fairness is represented by the hard constraint. Assuming fixed pseudo-labels, the network training is done by minimizing
the standard cross entropy lossconvex w.r.t..
Then, model predictions are fixed and () is minimized over.
Note thatis linear with respect toand its minimum over
simplexis achieved by one-hot distributions corresponding toat each data point.
However, the “fairness” constraintconverts minimization of the cross-entropy loss over all pseudo-labelsinto a non-trivial integer programming problem that can be approximately solved via.

Self-labeling methods for entropy clustering can also use “soft”
pseudo-labelsas targets in. In general,
soft target distributionsare common
in the context of noisy labelsand network calibration. They can also improve generalization by reducing over-confidence.
In the context of entropy clustering, soft pseudo-labelscorrespond to
aof cross-entropy. Nevertheless, cross-entropy encourages decisive pseudo-labelssince.
The last term also implies proximityand, therefore,
the decisiveness of predictions.
Many soft self-labeling methodsrepresent the fairness constraint usingor, as in ().
In particular,formulates the following entropy-based soft self-labeling loss

representing the decisiveness and the fairness constraints.
Similarly to (), the network parameters in () are trained by the standard cross-entropy losswhenare fixed. Optimization over the relaxed pseudo-labelsis
relatively easy since negative entropy is convex and cross-entropy is linear w.r.t..
While there is no closed-form solution, the authors offer an efficient approximate solver.

SECTION: 
This paper provides new theories and algorithms for discriminative entropy clustering.
First, we examine its conceptual relations to K-means and SVM. We disprove the theories
inon the equivalence between the soft K-means and the linear case of entropy clustering
(,).
Figures,,,study a counterexample and Sectionpoints out specific technical errors in their proof.
Despite the equivalence ()(),
two discriminative and generative clustering algorithms can operate within different hypothesis spaces even
if both produce linear results. Further contradicting equivalence to K-means,
our theories in Sectionprove that linear entropy clustering (,)
has aproperty establishing a formal relation to SVM-based clustering.
In the general context of entropy clustering () with deep models (),
our results imply margin maximization in the, similar to kernel SVM.
Such non-linear methodologies, however, can not use arbitrary embeddings. Indeed, SVM should
restrict kernels (implicit embeddings) and networks should regularize (explicit) embedding functions.

Second, our Sectionproposes a new self-labeling loss and algorithm for
general entropy clustering (,).
We derive a surrogate for () with numerically important differences from (), ().
Assuming relaxed pseudo-labels, we observe that the standard formulation
of decisivenessis sensitive to uncertainty/errors natural for pseudo-labels.
We demonstrate that ouris significantly more
robust to the label noise. We also propose aform of KL-divergence as a stronger fairness term that
does not tolerate trivial clusters, unlike the standard fairness in ().
Our new self-labeling loss is convex w.r.tand allows
an efficient EM solver for pseudo-labels with closed-form E and M steps.
The new algorithm improves the state-of-the-art on many standard benchmarks
for deep clustering, as shown in Sectionempirically confirming our technical insights.

SECTION: 
This Section analyses the theoretical properties of entropy clustering loss ()
in the context of linear discriminative models (). Even such simple models
may require some regularization to avoid degenerate clusters. Sectionshows a form of regularization implying. But first, Sectionjuxtaposes (,)
and K-means as representativecases of discriminative and generative clustering
(,).

SECTION: 
There are many similarities between entropy clustering () with linear model
() and K-means. Both produce linear boundaries
and use (nearly) the same number of parameters,vs..
The former corresponds tolinear discriminants(w. bias) forming the columns of matrixin (), and the latter
corresponds tomeansrepresenting density models at each cluster.
Both approaches have good approximation algorithms for their non-convex () and NP-hardobjectives.
Two methods also generalize to non-linear clustering using more complex representations,
e.g. learnedor implicit (kernel K-means).

There is a limited understanding of the differences between linear entropy clustering and
K-means as most prior literature, including,
discusses () in the context of networks ().
One 2D linear example in(Fig.1) helps, but unlike our Figure,
they employ trivial compact clusters typical for the textbook’s illustrations of K-means. The two methods
are indistinguishable from the example in.
Moreover, there is a prior theoretical claimabout equivalence
between soft K-means and linear entropy clustering, assuming certain regularization.
We disprove this claim later in this Section.

Generative and discriminative formulations of MI (,) may also suggest the equivalence of linear entropy clustering and K-means. We already discussed how () relates to (), and now we show how K-means relates to ().
Indeed, the Lloyd’s algorithm for K-means minimizes the following self-labeling objective for hard pseudo-labelsand parameters

where each pointcontributes a squared distance to the
meanof the assigned cluster such that.
Isotropic Gaussian densitieswith covariancesallow an equivalent formulation on the second line
below ().
Its Monte Carlo approximation on the third line produces an expression with cross-entropy whererepresents the “true” density of data in clusterandis its cardinality.
The standard relationship between cross-entropy and entropy functions further implies an inequality

concerning the conditional entropy in (). The lastrelation
only ignores a constant factor, the whole dataset cardinality.
When recomputing cluster means, Lloyd’s algorithm minimizes the cross-entropy above. It
achieves its low bound, the entropy in the second expression, only when the clusters are isotropic Gaussian blobs, which
is an implicit assumption in K-means. This is when K-means works well and, as we just showed,
when it approximates generative MI clustering ().
Thus, K-means and linear entropy clustering are equivalent in the case of
isotropic Gaussian clusters. This equivalence is consistent with the toy example in.

The arguments above also suggest how the equivalence may fail for more complex clusters.
Indeed, our Figuredemonstrates no equivalence without isotropy. We can also refute
the general claim inabout the equivalence between the followingvariant of
K-means loss ()

and the linear case of their self-labeling entropy clustering formulation (,)
with linear classifier’s norm regularization

where theoperator averaging over data pointswas introduced in Section,
and therepresents averaging over bothandshortening the expression for
lossin (). The negative entropy in ()
encourages soft labeling, i.e.could be any categorical distribution.
This term is standard forformulations.
The last term in () is a constant needed in the equivalence claim.

The only difference between the entropy clustering losses () and () is the squared norm
of the linear classifier parameters, excluding the bias. This standard regularization encourages “softness”
of the linear classifier’s soft-max predictions, similar to the effect of the entropy term in soft K-means ().
However, unlike entropy clustering, soft K-means
fails on elongated clusters in Figuresandin the same way as
the basic K-means in Figure. Indeed, soft formulations of K-means are typically
motivated by a different problem - overlapping clusters. The proper generative mechanism to address
anisotropic clusters is to drop the constraint on the covariance matricesallowing a wider class of Gaussian density models, i.e. extending the.
For example, GMM can be viewed as an anisotropic extension of soft K-means, and it would work perfectly
in Figures,,,, similar to entropy clustering.

We also have a general argument for why linear entropy clustering is stronger than K-means.
As easy to check using Bayes formula, the posterior for isotropic Gaussian densities estimated by K-means
is consistent with model (). However, discriminative entropy clustering optimizes
() without any assumptions on densities, implying a larger hypothesis space.

Besides our counterexample in Figureand the general argument above, we found a critical error in. They ignore the normalization/denominator in the definition of softmax. Symbolhides it in their equation (5), which is treated as equality in the proof of Proposition 2.
Their proof does not work with the normalization, which is important for training softmax models.
The regularization termand constantplay the following algebraic role in their proof of
equivalence between () and (). Without softmax normalization,inside
cross-entropyin () turns into a linear term w.r.t. logitsand combining it
withandcreates a quadratic formresembling squared errors in K-means ().
In contrast, Sectionshows that regularizationin () is needed for
theproperty of entropy clustering illustrated in Figures(a) and(b). Our theories show that instead of K-means, in general,
linear entropy clustering is related to discriminative SVM clustering.

SECTION: 
This section establishes aproperty of the regularized decisiveness
for linear model ()

whereisnorm of the linear discriminant w/o the bias.
Without extra constraints, the decisiveness allows a trivial solution with a single cluster.
This can be avoided by focusing on a set ofsolutions, where
feasibility can represent balanced clusters or
consistency with partial data labels as in.

Our theory assumes an arbitrary set of feasible solutions but is also relevant for soft feasibility.
For example, the regularized entropy clustering loss for linear model ()

combines the margin-maximizing decisiveness with the fairness term
encouraging balanced clusters, see Fig.(a).
The direct relation to MIprovides an information-theoretic motivation for this loss,
as discussed in Section.
Our theories below extend the general conceptual understanding of the decisiveness,
entropy clustering, and establish their relation to unsupervised SVM methods.

As typical in SVM, our formal theories on the max-margin property of the regularized decisiveness ()
in Sections-are focused on the binary linear models, but some multi-class extensions are discussed in Sec..
Below we informally preview our max-margin property
claims about () providing some intuition that may be helpful due
to differences with standard margin maximization in SVM.

For example, basic SVM assumesfully-labeled data.extension allows a trade-off between margin maximization and linear-separation violations.
In clustering, however, any data can bein many different ways as there are no ground-truth labels to be violated. The “gap” can be measured for any pair of such clusters,
see Figs.(a,b).
Thus, the largest margin solution among all feasible (e.g. fair) linear clusterings
is well-defined.

SVM clusteringuses theinstead of decisiveness in () and
also assumes fairness, see Sec..
In both cases, however, linear classifier regularizationis important for margin maximization.
The regularized hinge loss outputs a max-margin solution for allbelow some threshold.
In contrast, our theories show that optimal solutions for the regularized decisiveness ()
converge to a max-margin clustering only as, see Figure,
though this subtle difference may be hard to discern in practice.

Our results are based on the max-margin theories for supervised losses in. We generalize them
to unsupervised clustering problems. Also, instead of (), we prove the max-margin property for
a larger class of regularized decisiveness measures usingentropyof any order(formally defined in the following Sections)

where Shannon’s entropy in () is a special case.
Focusing on binary clustering, Sectionestablishes our results for.
In this base case, Renyi entropycan be seen as a “self-labeled”allowing us to prove our result in Theoremby extending the standard max-margin property for logistic regression. Theoremin Sectionextends our results for () to all, including () as a special case. Then, Sectiondiscusses
the multi-class case. Finally, Sectionsummarizes
margin maximization by regularized entropy ()
and juxtaposes it with the regularized hinge loss in SVM clustering.

We extend the earlier notation, in part to suit the binary case.
For example, besidesoutput, we also use a scalarfunction. Below, we summarize our notation.
In particular, there is one significant modification specific to this theoretical Section.
Here we use only hard class indicators, thus it is convenient to have integer labelsinstead of categorical distributions, as in the rest of the paper.
This change should not cause ambiguity as labelsappear mainly
in the proofs and their type is clear from the context.

Our notation concerning labels, labelings, and linear classifiersis
mostly general.
Some parts specific toare designed to transition smoothly
to multi-class problems. Note that forlinear classifiers typically generatelogits, output a-categorical distribution using softmax, and use natural numbers as class labels.
In contrast, binary classifiers typically use a single logit,output, and class labelsor.
Our notation forrectifies the differences using natural class indicatorsand categorical distribution output.
The latter is justified by a simple property that a single-logit sigmoid classifier
is equivalent to a softmax for two logits
as only their difference matters.

- linear classifier is a vector for. It generates scalar raw output. Vectorincludes theas we assume
“homogeneous” data representation.

-norm that excludes the bias.

- soft-max forcan be represented using a scalarfunction

for.
Similarly to, we use symbolboth for specific values inand as a function name.

- binary class indicator/label

- hard class label produced by soft prediction

- class label at arbitrary data point

- class label at point

- labeling of the whole dataset

- hard class label produced by classifierfor an arbitrary data point.

- hard label produced byfor point.

- dataset labeling by classifier

- a set of all linear classifiers consistent with given labeling.
By default, this paper uses “weak consistency” allowing data points on the decision boundary, i.e. zero margins.
Setis non-empty only if labelingis (weakly) linearly separable.

- a set of feasible labelingsrepresenting allowed clusterings.
It could be arbitrary. One example is a set of all “fair” linearly separable clusterings.

- a set of all linear classifiersconsistent with allowed labelingsin.

- unit norm linear classifier such that

- a set of all unit-norm linear classifiers consistent with given labeling.

- a set of all unit-norm linear classifiers consistent with labelings in.

- maximum margin linear classifier of unit norm corresponding to given linearly separable labeling

Finally, we define the binary version of Renyi entropyof orderstudied by our max-margin clustering theories forin Sec.-.
One-to-one correspondence between distributionsand scalarsmakes it convenient to define fortwo equivalent functionsand

The expression in () is numerically ill-conditioned
when. In these three cases, Renyi entropy is better defined by the asymptotically consistent formulations

whereisindicator returningordepending
if the condition inside is true or not. Forone can use
the standard formula for the binary Shannon entropy

that has no numerical issues.
Figureillustrates binary Renyi entropy functionsfor different orders.

Our base case is Renyi entropyfor.
As evident from (), it resembles the standard(NLL) loss for supervised classification, a.k.a..
To emphasize this relation, we denote supervised NLL loss by.
Forit is

whereis a given binary label at a data point.
The obvious relation between the entropy () and NLL (),
see Fig.(a),

can be equivalently represented as “self-labeling” identity

whereis the “hard-max” label for prediction.

We exploit the “self-labeling” identity ()
to prove the max-margin clustering property for entropyby extending the known margin-maximizing property for logistic regressionreviewed below.

With some exceptions, margin maximization is
typically discussed in a fully supervised context assuming a given linearly separable labeling.
The max-margin property is easy to prove for theas
it vanishes above some threshold. Some monotone-decreasing losses,
e.g., also have max-margin property, but the proof requires careful analysis.
They prove a sufficient condition, “fast-enough” decay, for monotone non-increasing
classification losses that depend only on distances to classification boundary, a.k.a..

The standard logistic loss is defined w.r.t. raw classifier outputas

which is a composition of NLL () and sigmoid function.
Given true label, the corresponding supervision loss is

where

is a signed, i.e. distance from pointto the classification boundary.
The operatorreturnsif the argument is true andotherwise.
As easy to check, the sign of distancein () is positive for
correctly classified points, and it is negative otherwise,
see cyan color in Figure(a).

Logistic regression is known to satisfy conditions for
margin maximizing classification.

Logistic lossin () satisfies the following “fast-enough” decay
condition

which is a sufficient condition for margin-maximizing classification. This is case ""
in Theorem 2.1 from.

For completeness, we also state a special case of Theorem 2.1 infor logistic regression.
We use the following total loss formulation based on NLL ()

where the bar indicates averaging over all data points.

To establish the margin-maximizing property for clustering, we should drop full supervision where
true labelingis given. Instead, we formulate max-margin clustering for a given setof
allowed orlinearly separable binary labelingsfor dataset.
In this unsupervised context, labelingsrepresent possible data clusterings.
For example,could represent “fair” clusterings.
As a minimum, we normally assume that linearly separable labelings inexclude trivial solutions with empty clusters.

Naturally, labelingsdivide the data into clusters differently.
Thus, the corresponding inter-cluster gaps vary.

Assumeis a binary linearly separable labeling of
the dataset. Then, theorfor labeling (clustering)is defined as

whereis the max-margin unit-norm linear classifier for.

This definition associates the termwith labelings.
Alternatively,is often associated with classifiers.
Given a separable labeling, any consistent linear classifierhas
margin size.
Thefor labelingin Definitionis the
margin size of the optimal max-margin classifier.

Now we definein an unsupervised setting restricted to.
It extends the standard concept ofrestricted to one (true) labeling.
Our focus is on optimal labelings rather than classifiers.

Consider any setof allowed linearly separable binary labelingsfor dataset. Then,foris defined as

whereis the gap size for clustering, as in Definition.

The achieved maximum gap value definingis finite. Indeed,
gap sizes forare uniformly bounded by the diameter of the dataset

assuming thatexcludes trivial clusterings.

Our next theorem extends Theoremto clustering based on-decisiveness.
We define the total decisiveness as

where the bar indicates averaging over all data points.
The proof uses the equivalent expression on the right, which is a self-labeling version of NLL ()
based on () and ().

The proof requires analysis of the loss, but due to identity (),
it can follow the same steps as the analysis of the supervised loss () from
Theorem 2.1 in(case “a”). Indeed, they consider supervised classification with
a given true linearly separable labeling, but it appears in their proof only in the margin expressionsetting its right sign, see ().
Their analysis is restricted to classifiers consistent with, but we observe that
their supervised loss () has an equivalentformulation

using a simpler unsigned expression for the margins

as () is guaranteed to be non-negative for all consistent classifiers,
see Figure(a). Our formulation ofin ()
does not depend on labeling, as as long as.

In the context of clustering, i.e. unsupervised classification without given (true) labeling,
we show that entropy valuesrelate to
logistic loss and distances to the clustering boundary, a.k.a..
Indeed, as implied by self-labeling identity () and (), we get

for any linear classifier, which is always consistent with its “hard-max” labeling, by definition.
Our equation () uses the unsigned expression for margins (),
see Figure(b).

The formal max-margin analysis for classifiersinuses only logistic loss values,
which we expressed as (). The same analysis directly applies to the identical decisiveness values
() for classifiers,
even when such classifiers may have different labelings.
∎

Binary Renyi entropy, defined in (-)
for distribution,
is symmetricaroundfor any order,
see Figure.
We define a monotone non-increasing function, see Figure(b)

that can be used for training softmax classifierbased on the following supervision loss

Functionsatisfies the following identities, see Figure(b),

analogous to the relation () betweenand logistic regression,
see Figure(a). As in (),
the identities above imply the “self-labeling” identity

between decisivenessand the supervised loss ().

Similarly to Section, we will prove the margin maximization property for decisivenessusing the self-labeling relation () and the
standard theories for supervised losses.
Instead of logistic lossin (), we use

which is a monotone non-increasing supervision loss enabling an expression for() analogous to ()

whereare (signed) margins ().
Losssatisfies the “fast-enough decay” condition from Theorem 2.1 in.

For any positive value of parameter, loss functionin () satisfies

which is the sufficient condition for margin-maximizing classification according to Theorem 2.1 from.

The technical condition above is easy to check. We leave this as an exercise to the reader.
Note that two special casesrequire alternative (asymptotically consistent)
expressions for Renyi entropy, e.g.can use the formulation in ()
instead of (). Then, the expressions forandare different from (),
but they also have the exponential decay property, as easy to check.
∎

Propertyallows us to generalize our max-margin clustering Theoremto the total-decisiveness loss

wherereplaces "" in theloss ().

We follow the same arguments as in the proof of Theorem.
Equation () and self-labeling relation ()
imply identical expressions for-values
w.r.t. unsigned marginsin two cases:
(supervision) for classifiersconsistent with some given (true) labeling

and (decisiveness) for any classifier

which are analogous to logistic loss () and decisiveness ().
The only difference between these supervised and unsupervised cases is the set of allowed
classifiers. Since classification lossand decisivenesshave identical dependence on unsigned margins,
all arguments from Theorem 2.1proving
the margin maximizing property for the (exponential-decay) classification lossdirectly apply to the clustering loss.
∎

Note that there is no margin maximization forsincefor any finite data point.

We use the max-margin property for multi-label
classification in Theorem 4.1to prove the max-margin
property for clustering with regularized-decisiveness when.
We need to introduce some additional terminology.

First, the regularization for max-margin-clustering is based on the squared Frobenius
norm of classifier matrix

wherearenorms of linear discriminatorsfor each class, which are the columns of matrix.
As before, we exclude the bias parameters in the norms of.

We use the unit-norm multi-class classifiers consistent with any given labeling

as in the binary case in Section. The next definition of margin size for multi-class labelings
is consistent with.

Assumeis a linearly separable multi-class labeling forsuch that.
Then, theorfor labeling/clusteringis

whereis the max-margin unit-norm linear classifier for.

The multi-class margin in Definitionis determined by the two closest classes/clusters.
The gaps between the other pairs of clusters are ignored as long as they are larger.

Consider any setof allowed linearly separable multi-class labelingsfor datasetwhere. Then,foris

whereis the gap size for clustering, as in Definition.

Figure(b) is an example of multi-class clustering achieving the max-margin
(Def.) among fair solutions.
The tightest spot between the red and blue clusters determines.

We state max-margin clustering theorem for multi-class decisiveness of orderconsistent with the binary case ()

using the general definition of Renyi entropyfor

extending () to multi-class predictions.

Definitionabove is consistent with
multi-class-margin (10) in. Their multi-class max-margin
classification Theorem 4.1 extends to clustering with regularized
decisivenessdue to self-labeling relation (),
which also works forwith a properly constructed multi-class supervised loss. For example,
generalizing (,) to the case, one can use supervised loss

or its continuous variant, see Figure,

where the argument of entropyis distributionconsistent with labelthat is the closest to prediction.
It is necessary to check that, as a function of-dimensional,
supervised losssatisfies
the exponential decay condition (11) in Theorem 4.1.
The remaining proof of Theoremfollows the same arguments as in Theorem.

After establishing the max-margin property for regularized decisiveness ()
or (), it is interesting to review the implications for entropy clustering
and juxtapose it with SVM clustering. In particular, the formulation inis highly relevant.
They use the the regularized, as in, combined with a hardconstraint

where, unlike supervised SVM, binary targetsare optimization variables jointly estimated with the linear model parameters. Following our terminology from Section, objective () is a self-labeling loss with binary pseudo-labels.
It resembles () where hard constraints represent fairness and estimated pseudo-labels
are discrete/hard.
One can also relate the hinge loss in () to a truncated variant of
decisivenessencouraging linear discriminantsuch thatfor all data points.

However, one significant difference exists between () and (). Minimization of the normin supervised or unsupervised SVM, as in (),
is known to be central to the problem. It is well understood that it directly corresponds to margin maximization.
In contrast, entropy clustering methodstypically skipin their losses, e.g. (), though it may appear implicitly due to the omnipresence of the.
It is included in (), but for the wrong reason discussed
in Section.
It is also a part of (), but it is
motivated only by a generic model simplicity argument in the context of complex models. It is also argued inthat
decisivenessencourages
“decision boundary away from the data points”, which may informally suggest margin maximization,
but normis not present in their argument.
In fact, margin maximization is not guaranteed without norm regularization.

Similarly to SVM, our max-margin theories for regularized decisiveness are focused on a linear binary case
where guarantees are the strongest, but the multi-label case is also discussed.
Non-linear extensions are possible due to high-dimensional embeddingsin (),
but the quality guarantees are weak, as in kernel SVM. One notable difference is that non-linear SVMs
typically usedata kernels, i.e. implicit high-dimensional embeddings.
In contrast, discriminative entropy clustering can learn.

Also, note that our max-margin theories apply to entropy clustering losses combining regularized decisiveness with
fairness, e.g. (), only if fairness is not compromised in the solution.
If decisiveness and fairness require some trade-off, the max-margin guarantee becomes less clear.
This is similar to soft SVM when the margin size is traded off with margin violations.
Empirically, on balanced toy examples, the max-margin property is evident for () and
its self-labeling surrogates, see Sec.and Sec..
We conjecture that margin maximization could be formally extended to
many self-labeling formulations of entropy clustering loss including the norm,
but leave the proof to the reader.

SECTION: 
The theories above improve the general understanding of discriminative entropy clustering and show certain conditions when
the linear classifier norm regularization leads to margin maximization. Note that the genericimplicitly represents the norm regularization, but it may not satisfy these conditions. For example, it often includes the bias parameter, and its strengthmay not be set correctly.

This section addresses other limitations of prior entropy clustering algorithms in the context of Shannon’s entropyas in ().
We focus on self-labeling (Sec.) and show that the standard cross-entropy formulation of decisiveness is sensitive to pseudo-label errors. Sectionargues that theis more robust to label noise.
We also propose. Sectionderives an efficient EM algorithm for minimizing our loss w.r.t. pseudo-labels, which is a critical step of our self-labeling algorithm.

SECTION: 
We derive our self-labeling surrogate bythe regularized entropy clustering loss () into two groups: regularized decisiveness and fairness. The latter, expressed by KL-divergence with distributionas in (), uses auxiliary splitting variables. Such soft pseudo-labelsare tied to predictionsby an extra divergence term
(the last one)

Combining the entropy with the last KL term gives

which has one notable difference from
the standard self-labeling surrogate losses () and () from:
the cross-entropy term reverses the order of its argumentsand.
The standard order is motivated in the supervised
settings whereis a true one-hot target andis the estimated distribution. However, this motivation is questionable when bothandaresoft distributions. Moreover, Figure(b) argues that the reverse cross-entropy can improve the robustness of network predictionsto errors in estimated pseudo-labels, as confirmed by our training experiments in Figure.
The reversal also works for estimating soft pseudo-labelsas
the second argument in the cross-entropy function is standard for an “estimated” distribution.

We also observe that the standard fairness term in (,,) is theKL divergence w.r.t. cluster volumes, i.e. the average predictionsor pseudo-labels.
It can tolerate highly unbalanced solutions wherefor some cluster,
see the dashed curves in Figure(a).
We propose the, a.k.a., KL divergence, see the solid curves Figure(a),
which assigns infinite penalties to highly unbalanced trivial clusterings.
We refer to this as.
Thus, our final self-labeling surrogate loss is

Due to the argument reversal in the cross-entropy and KL terms, our
loss () represents stronger constraints forandwhen compared to (). However, besides well-motivated numerical properties, it also matters that () has an efficient solver for pseudo-labels discussed in the next Section.

SECTION: 
Efficient minimization of a self-supervised loss w.r.t pseudo-labelsfor given predictionsis a critical component of all iterative self-labeling techniques, see Sec..
While our loss () is convex w.r.t., optimization is done
over the probability simplex and a good practical solver is not a given. Note thatworks as afor the constraint. This could be problematic for the first-order methods, but a basic Newton’s method is a good match, e.g.. The overall convergence rate of such second-order methods is fast, but computing the Hessian’s inverse is costly. Below we derive an(EM) algorithm that is more efficient, see Table.

Assume that model parameters and predictions in ()
are fixed,and.
Following, we introduceauxiliary latent variables,
distributionsrepresenting normalized support of each clusteroverdata points.
In contrast,distributionsshow support for each class at every point.
We refer to each vectoras a.
Note that here we focus on individual data points and explicitly index them by.
Thus, we useand. Individual components of distributioncorresponding to data pointis denoted by scalar.

First, we expand our loss () using our new latent variables

Due to the convexity of negative, we apply Jensen’s inequality to derive an upper bound, i.e. (), to. Such a bound becomes tight when:

Then, we fixas () and solve the Lagrangian of () with simplex constraint to updateas:

We run these two steps until convergence with respect to some predefined tolerance. Note that the minimumis guaranteed to be globally optimal since () is convex w.r.t..
The empirical convergence rate is within 15 steps on MNIST. The comparison of computation speed on synthetic data is shown in Table. While the number of iterations to convergence is roughly the same as Newton’s methods, our EM algorithm is much faster in terms of running time and is extremely easy to implement using the highly optimized built-in functions from the standard PyTorch library that supports GPU.

Inspired by, we also adapted our EM algorithm to allow for updatingwithin each batch. In fact, the mini-batch approximation of () is an upper bound. Considering the first two terms of (), we can use Jensen’s inequality

whereis the batch randomly sampled from the whole dataset and the bar operator is the average over data points in the batch.
Now, we can apply our EM algorithm to updatein each batch, which is even more efficient. Compared to other methodsusing auxiliary variables, we can efficiently updateon the fly while they only update once or just a few times per epoch due to the cost of computingfor the whole dataset. Interestingly, we found that it is important to updateon the fly, which makes convergence faster and improves the performance significantly, as shown in Figure. We use this “batch version” EM throughout all the experiments. Our full algorithm for the loss () is summarized as follows.

SECTION: 
Here we briefly compare our optimization method for () and the algorithm for
() in. Their loss is convex with respect to.
In contrast, the reverse cross-entropy in our loss () is non-convex as a natural consequence of
our focus on robustness to pseudo-label errors. However, both losses are non-convex w.r.t. parametersof the interior network layers responsible for representationin ().
As for the optimization w.r.t. the pseudo-labels, both losses are convex. Our EM algorithm
converges to the global optima for, while they use an approximate closed-form solution for, but it does not have
optimality guarantees. Both self-labeling algorithms iteratively optimize the corresponding surrogate losses
over pseudo-labelsand model parameters,.

SECTION: 
We test our approach to clustering on several standard datasets using different network architectures. We also evaluate different losses in semi-supervised settings. All the results for prior work are reproduced using either the corresponding public code or our implementation using the same experimental settings.

For the clustering problem, we use four standard benchmark datasets: MNIST, CIFAR10/100and STL10. We followto use the whole dataset for training and testing unless otherwise specified.
As for the semi-supervised setting, we conduct experiments on CIFAR10 and STL10. We split the data into training and test sets as suggested by the official instructions of the datasets.

To evaluate the quality of clustering, we set the number of clusters to the number of ground-truth category labels. To calculate the accuracy, we use the
standard Hungarian algorithmto find the best one-to-one mapping between predicted clusters and ground-truth labels. We don’t need this matching step if we use other metrics, i.e. NMI, and ARI. For the evaluation of semi-supervised classification, we directly calculate the accuracy of the test set without the mapping step.

For clustering with fixed features, we setin our loss to 100. The learning rate of the stochastic gradient descent is set to 0.1 and training takes 10 epochs. The batch size was set to 250. The coefficients for thenorm of the weights are set to 0.001, 0.02, 0.009, and 0.02 for MNIST, CIFAR10, CIFAR100, and STL10 respectively. We also tuned the hyperparameters for all other methods.

As for the training of VGG4 in Section, we use Adamwith learning ratefor optimizing the network parameters. We set the batch size to 250 for CIFAR10, CIFAR100, and MNIST, and we used 160 for STL10.
For each image, we generate two augmentations sampled from “horizontal flip", “rotation" and “color distortion".
We set theto 100 in our loss.
The weight decay coefficient is set to 0.01.
We report the mean accuracy and Std from 6 runs with different initializations while we use the same initialization for all methods in each run. We use 50 epochs for each run and all methods with tuned hyperparameters reach convergence within 50 epochs.
As for the training of ResNet-18, we still use the Adam optimizer, and the learning rate is set tofor the linear classifier andfor the backbone. The weight decay coefficient is set to. The batch size is 200 and the number of total epochs is 50. Theis still set to 100. We only use one augmentation per image, and the coefficient for the augmentation term is set to 0.5, 0.2 and 0.4 respectively for STL10, CIFAR10, and CIFAR100. Such coefficient is tuned separately for IMSAT and MIADM.

SECTION: 
Our results in Sectionstate that the regularized decisiveness () using Renyi entropy of any orderleads to margin maximization under certain conditions, e.g. linear model and convergence of the regularization constantto zero. However, such conditions may not be satisfied in practice. For example, only fixed values ofcan be evaluated. Also, unsurprisingly, better results are typically obtained by non-linear network models where a linear classifier head is trained jointly with fine-tuning the representation layers. The corresponding non-convex losses may also be harder or easier to optimize depending on.

It follows that different values ofmay affect the practical results. However, our self-labeling algorithm in Sectionassumesand uses the closed-form EM steps applicable only to the Shannon’s decisiveness. To evaluate Renyi decisiveness for various, we apply the standard stochastic gradient descent to a combination of () with Shannon’s fairness

We use the standard STL10 dataset following the experimental setting for clustering from.
All hyperparameters are separately tuned in each column.

Tablesuggests that larger values ofmay lead to better empirical results. Intuitively, this could be explained as follows.
Smallerinduces larger gradients for the Renyi decisiveness, see Figure, as the network prediction becomes more confident during training.
This creates strong local minima for the network parameters that are hard to escape.

The results in Tableencourage the development of an efficient pseudo-labeling algorithm for,
which is left for future work.
The experiments in the following Sections use our efficient EM-based algorithm from Sectionassuming Shannon’s entropy, i.e..
Note that our self-labeling optimization algorithm significantly outperforms
the standard stochastic gradient descent for, see "our" vs IMSAT in Table.

SECTION: 
In this section, we test our loss as a proper clustering
loss and compare it to the widely used Kmeans (generative) and other closely related losses (entropy-based and discriminative). We use the pre-trained
(ImageNet) Resnet-50to extract the features.
For Kmeans, the model is parameterized by K cluster centers. Comparably, we use a one-layer linear classifier followed by softmax for all other losses including ours. Kmeans results were obtained using the scikit-learn package in Python. To optimize the model parameters for other losses, we use stochastic gradient descent. Here we report the average accuracy and standard deviation over 6 randomly initialized trials in Table.

SECTION: 
In this section, we train a deep network to jointly learn the features and cluster the data. We test our method on both a small architecture (VGG4) and a large one (ResNet-18). The only extra standard technique we add here is self-augmentation as discussed in Section.

To train the VGG4, we use random initialization for network parameters.
From Table, it can be seen that our approach consistently achieves the most competitive results in terms of accuracy (ACC). Most of the methods we compared in our work (including our method) are general concepts applicable to single-stage end-to-end training. To be fair, we tested all of them on the same simple architecture. But, these general methods can be easily integrated into other more complex systems with larger architecture such as ResNet-18.

As for the training of ResNet-18, we found that random initialization does not work well. Following, we use the pre-trained weight from the self-supervised learning. For a fair comparison, we followed their experimental settings and compared ours to their second-stage results. Note that they split the data into training and testing. We also report two additional evaluation metrics, i.e. NMI and ARI.

In Table, we show the results using their pretext-trained network as initialization for our entropy clustering. We use only our clustering loss together with the self-augmentation (one augmentation per image this time) to reach higher numbers than SCAN, as shown in the table below. More importantly, we consistently improve over the most related method, MIADM, by a large margin, which clearly demonstrates the effectiveness of our proposed loss together with the optimization algorithm.

SECTION: 
Although our paper is focused on self-labeled classification, we find it also interesting and natural to test our loss under a semi-supervised setting where partial data is provided with ground-truth labels. We use the standard cross-entropy loss for labeled data and directly add it to the self-labeled loss to train the network initialized by the pretext-trained network following. Note that the pseudo-labels for the labeled data are constrained to be the ground-truth labels.

SECTION: 
Our paper clarified several important conceptual properties of the general discriminative entropy clustering methodology. We formally proved that linear entropy clustering has a maximum margin property, establishing a conceptual relation to
SVM clustering.
Unlike prior work on discriminative entropy clustering, we show that classifier norm regularization is important
for margin maximization.
We also provided counterexamples disproving a recent theoretical claim of the equivalence between(soft K-means) and(linear softmax classifier optimizing decisiveness and fairness). We juxtapose the two methods as generative and discriminative approaches to MI-based clustering.

We also discussed several limitations of the existing self-labeling formulations of discriminative entropy clustering
and proposed a new loss addressing such limitations. In particular, we replace the standard (forward)
cross-entropy by thethat we show is significantly more robust to errors in estimated soft pseudo-labels. Our loss also uses a strong formulation of the fairness constraint motivated by aversion of KL divergence. We designed an efficient EM algorithm for minimizing our loss w.r.t. pseudo-labels; it is significantly faster than standard alternatives, e.g. Newton’s method.
Our empirical results improved the state-of-the-art on many standard benchmarks for end-to-end deep clustering methods.

SECTION: References
SECTION: