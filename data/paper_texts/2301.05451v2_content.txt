SECTION: TeD-Q: a tensor network enhanced distributed hybrid quantum machine learning framework
SECTION: Introduction
Quantum computing is one of the most promising directions that may save us from Moore’s law. Recent results show that quantum computing is indeed advantageous over classical computing in certain problems. With the progress in quantum hardware, quantum software and research in algorithms are also catching up. Quantum software plays a crucial role in bridging quantum hardware and real-world applications. There has been a decent amount of work in quantum software platforms in recent years. Some of these already allow users to access real quantum hardware, while others focus more on building a complete software environment for better quantum algorithm development.

TeD-Q is a Python-based quantum programming framework that has differentiable functionality. It is optimized particularly for quantum machine learning problemsand variational quantum algorithms, which constitute the majority of contemporary Noisy Intermediate-Scale Quantum (NISQ) algorithms. TeD-Q provides a universal framework for programming on different backends, including quantum hardware, quantum simulator, and distributed GPU accelerators. In terms of top-level applications, users could enjoy the convenience of using TeD-Q without worrying about how to deal with different backends. One could simply treat a quantum neural network (QNN) training as in classical machine learning.

With TeD-Q, the quantum circuit is treated as a Python function. It is free to choose either PyTorch or JAX as the interface with the execution backends. Due to this systematic integration, one could also leverage the rich features provided by the well-developed AI-oriented librariesto help the implementation of quantum algorithms better. Specific benefits include batch execution, Just In Time compilation (JIT), distributed parallel GPU optimization, and auto-differentiation for backpropagation.

TeD-Q has both full amplitude quantum simulation and a tensor network enhanced multi-amplitude simulation. TeD-Q will estimate the complexity of both modes, and users could choose on their own which mode to execute the code.

TeD-Q is also equipped with a built-in tensor network module and contraction path optimizer JDtensorPath, which could provide additional improvements on the performance in the tensor network simulation mode. This is achieved by optimizing the tensor network contraction order.

Besides, TeD-Q provides a graphical circuit composer that makes it user-friendly to beginners with intuitive visualization.

Tableshows the comparison among TeD-Q and three major general-purpose quantum machine learning platforms – PennyLane, Qiskitand Paddle Quantum.

PennyLane is a cross-platform Python package for programming quantum and hybrid quantum-classical algorithms. It builds upon the idea of auto differentiable programming and integrates mainstream classical machine learning libraries with quantum algorithms to provide an easy-to-use interface. Qiskit is an open source quantum software development framework of IBM. It integrates PyTorch and can be executed with IBMQ on real quantum hardware. Paddle Quantum is developed by Baidu, it is based on the PaddlePaddle deep learning platform and can access real quantum computers Quantum Leaf.

The main distinct features of TeD-Q are hypergraph-based tensor network contraction (TNC) and paralleling via index slicing. Simulation of quantum circuits normally can be carried out by matrix multiplication and tensor network contraction. Current quantum software platforms generally support only matrix multiplication mode, which is enough while the number of qubits is below roughly 38. However, when the number of qubits exceeds 38, the time cost and storage cost surge exponentially. The TNC mode of TeD-Q could easily handle quantum circuit simulation with qubit number larger than 38. Hypergraph-based TNC method can provide several orders of magnitude improvement in both computation and memory complexity while paralleling via index slicing delivers advantages in GPU device communication. Since the order of tensor network contraction will affect the time cost a lot, TeD-Q also provides a dedicated tensor network contraction optimizer, called JDTensorPath. These new features are designed to improve the efficiency of large-scale quantum circuit simulation.

Compared to Qiskit, Pennylane and Paddle Quantum, TeD-Q also featured a reusable mechanism. In most of the quantum algorithms including NISQ algorithms, the parameterized quantum circuit will be evaluated multiple times without changing the circuit structure. In TeD-Q, the quantum state and quantum gate objects inside a quantum circuit will be reused in every evaluation. This feature can save a considerable amount of time for class instantiation in small quantum circuits. This advantage could be easily seen in the performance comparison of multiple qubits rotation example in Section.

SECTION: Architecture of TeD-Q
TeD-Q is designed for fast and efficient implementation of quantum algorithms. For this purpose, the components of application design, computation backends, and the interface in between are packed into relatively independent modules. TeD-Q is composed of four main modules, QInterpreter, Backends, Interface, and Optimizer, as shown in Fig..

Any quantum algorithm will eventually be passed through and processed as classical codes. This is where the QInterpreter plays its role, transforming quantum algorithms into executable codes. Quantum state and quantum gate classes are the basic building blocks of QInterpreter. A series of quantum gates applied to the quantum state will make up a quantum circuit.

The quantum circuit object in TeD-Q is agnostic with respect to simulation and hardware backends. It is an abstraction of user-defined quantum function, that bridges the high-level quantum algorithms and low-level backend-specific instructions. QInterpreter of TeD-Q provides tools for obtaining and manipulating representations of quantum operators and measurements. These tools and representations are useful for generating a quantum circuit. The user needs to provide a quantum function, the number of qubits, and the parameters or the shape of the parameters (in this case, random parameters will be generated and used) to construct a quantum circuit.

A quantum function is defined as a normal Python function. It accepts classical input parameters and uses their elements for quantum gate parameters. The user needs to put quantum operations inside the quantum function in the order that matches how they are applied. A keyword argument "qubit", which is a list of integers, need to be specified to denote which qubit each quantum operation applies on. After all the quantum operations, a single or a list of measurements must be placed as part of the return statement. Code Listingshows a simple example of defining a quantum function and constructing a quantum circuit from it.

The quantum circuit is not yet executable since the computation mode and backend have not been chosen. After specifying these, it can be compiled into an executable compiled circuit. The compiled circuit can be used like a standard Python function to compute the result. The order of input parameters must be the same as the order of the corresponding quantum gates. A quantum circuit can be run on different backends by manually compiling it with different specifications, as shown in Code Listing. This is why it is called a reusable quantum circuit.

SECTION: Simulation modes
SECTION: State vector propagation mode
In quantum theory, the wave function is used as a mathematical description of the quantum state of a system. A state vector is denoted by a complex ket vector. In this mode, quantum gates act on the state vector as a sequence of matrix operators, so that the resulting state after each operation is:

whereis the matrix operator. The state vector propagation mode is the default simulation backend of TeD-Q. A user-defined initial quantum state can be used by putting tedq.InitStateVector() as the first line of the quantum circuit definition function as shown in Code Listing. Notice that the length of the initial state vector must match, whereis the number of qubits. If no initial state is specified, TeD-Q will set it to the ground stateby default. With TeD-Q, this mode allows a normal laptop to deal with a non-trival quantum circuit of up toqubits.

SECTION: Tensor network contraction mode
In the state vector propagation mode, the number of amplitudes grows exponentially with the number of qubits,complex numbers are needed to describe the quantum state for an n-qubit system. Therefor, it is extremely difficult for classical state vector-based simulator to handle NISQ devices with more than 50 qubits. TeD-Q integrates the tensor network contraction method and mainstream deep learning frameworks – PyTorch and JAX, providing an efficient built-in tensor network-based quantum simulator. This mode can easily simulate certain quantum circuits ofqubits on a single CPU or GPU.

The workflow of the tensor contraction mode in TeD-Q consists of the following steps: (a) manipulating the quantum circuit according to its output type and converting it into a tensor network by the built-in TensorNetwork module; (b) applying structural simplification on the tensor network; (c) searching for best contraction sequences for the simplified tensor network; (d) slicing the contractions down to memory limit; (e) carrying out the actual contraction with the backend library according to sliced contraction sequences. The computational complexity of steps (a) and (b) are very low, which can be done by a single CPU thread. Step (b) is optional, however, it will generally reduce the size of the tensor network significantly. Except for the default trivial pathfinder, two cutting-edge hyper-graph partition-based packages – CoTenGra& JDtensorPath can also be chosen for step (c). JDtensorPath was originally a built-in module in TeD-Q and was separated into an independent software for better code structure management. It perfectly matches TeD-Q’s design paradigm and will be introduced in the Section. Step (d) can also be carried out with CoTenGra or JDtensorPath libraries. This is also an optional step and will cost some extra overhead of the total contraction floating point operations (FLOPs). In JDtensorPath, step (c) and (d) are done parallelly in CPUs. The path found in step (c) or (d) can be re-used in step (e) to obtain the result of the same quantum circuit but with different input parameters. Step (e) is the most time-consuming part, which will be executed repeatedly in the machine learning training loop. Step (e) can be run on a single CPU, single GPU, or a cluster of GPUs.

A quantum circuit can be simulated in various ways, of which tensor network method is increasingly popular recently years. TeD-Q can represent a quantum circuit with the corresponding graphical tensor network. Both the quantum state and any quantum gate can be represented as tensors while the tensor network preserves the topological structure of them. The resulting graphthus contains vertexes associated with the tensors, while the edges are labeled by their indices. The rank of a tensor is given by the number of edges connecting to it. Since a single qubit lives in a 2D Hilbert space, meaning each index takes value from, a rank-k tensor will requirestorage space. To calculate the probability of finding certain final statefrom the output of the quantum circuit, i.e., the tensor network needs to be contracted, meaning shared indices between vertices will be summed up; while open edges remains. Fig.gives an example of a trivial two qubits quantum circuit and its graphical tensor network representation in TeD-Q. Each vertex in the tensor network is associated with a tensor representing either the quantum state or one quantum gate.andare tensors corresponding to states of the two qubits whileandare representing single qubit Pauliand rotationalquantum gates respectively; the two qubits CNOT gate corresponds toand the measurement in thedirection is presented by. Since the expectation value is derived by, to calculate it with tensor network, we must append the tensor network corresponding to the complex conjugate of the original quantum circuit, which is. Thegate could then be represented bywhich in tensor form will be:

while the tensor form of the CNOT gate is:

Thus, the expectation value can be calculated by the summation:

which can be verified with the traditional vector mode calculation. It is shown that the actual order of carrying these summation (for instance, whether to sumorfirst) does matter a lot and finding a good contraction order composes most part of the tensor network algorithm.

TeD-Q supports an efficient local processing of the tensor network prior to the searching of the contraction order by a set of simplifications based on its structure and sparsity of the tensors. The simplifications include tensor shape squeezing, diagonal and anti-diagonal reduction, and rank simplification, which are designed to decrease the complexity and the rank of the tensor. After the local pre-procession, the tensor network will be transferred from a line graph into a hyper-graph (a generalization of the graph that allows an edge connecting any number of vertices).

An illustration of the tensor network and its hypergraph representation is shown in Fig.(a). Notice that the red hyper-edge (corresponding to index) is connected with three vertices (tensors). The original tensor network is defined as:

where the upper-case and lower-case letters denote tensors and its indices respectively.

The time and memory complexity of tensor network contraction depend on its tensor ranks and contraction order. Despite finding optimal contraction sequences is theoretically a non-deterministic polynomial-time (NP)-hard task for general cases, efficient heuristic algorithmsexist in practice for obtaining ’good enough’ sequences. Several established algorithms are supported by TeD-Q from third party libraries, including brute-force search, branching strategy, greedy and random-greedy path, dynamic programming approach, line-graph tree decomposition via QuickBB or FlowCutter, community detection, Boltzmann-greedy agglomerative contraction trees, and hypergraph partitioning method. The default basic pathfinder of TeD-Q supports the first four methods. For better performance, we adopt the hyper-graph partitioning method which is original proposed in CoTenGra and develop our own hypergraph based pathfinder library – JDtensorPath.

The key idea of this algorithm is to recursively divide the hypergraph of a given tensor network, and explicitly build an incomplete binary contraction tree in a ’top-down’ fashion as shown in Fig.b. The root node of the tree corresponds to the initial whole graph of the tensor network and the effective tensor of the final result. Bi-partitioning the hypergraph of the parent node will create two child nodes that represent the effective intermediate tensors. The contraction tree can be generated in this recursively divisive manner. The contraction sequences are obtained by ascending the tree from the bottom up, and combining two child nodes to a parent node corresponds to a pair contraction of two intermediate tensors.

The FLOPs for an individual contraction (node bi-partitioning) is given by:

whereis the product of the dimensions of the outer indices (indices that are not contracted) andis that of the contracted indices (which equals toif no index is contracted). For each node bi-partitioning,is fixed; therefore, the cost of the corresponding pairwise contraction can be minimized by minimizing the sum of the weighted edges (indices) cut by the partition. This method is like a greedy approach from top to down since it only considers the cost of every single contraction. Furthermore, JDtensorPath not only requires partitioning of the current node to be optimal, but also takes the partitioning of its child nodes into consideration, so that the overall tree size and total contraction FLOPs could be minimized. Specifically, it is required that the outer indices of the parent node must be distributed as evenly as possible on two child nodes. This requirement can reducein the partitioning of the child nodes and thus affects the FLOPs of their contractions. The memory usage of a pairwise contraction is given by the sum of the sizes of all the related tensors. Since the parent node tensor of each contraction is fixed, evenly distributing its indices on children node tensors can also reduce the memory usage. This is due to the fact that, supposingis fixed and defining,to be the product of the dimensions of remaining indices of children node tensors respectively during contraction, the memory cost of the contractionwill be minimized whengiven.

Advanced multilevel hypergraph partitioning framework KaHyParis employed to construct the contraction tree, modern parallel and distributed Python package–Ray is used to generating multiple trials in parallel. JDtensorPath can provide high-quality contraction sequences to TeD-Q and enables tensor network contraction mode to achieve orders of magnitude speedup for various quantum circuits simulation.

Slicing is an optional step. It is designed to solve the memory limit problem and provides an efficient way for parallel computing. Even with an excellent contraction path, contraction of a large tensor network is still limited by its intrinsic memory complexity. In addition, the elaborated data dependency between the tensors becomes a bottleneck for contraction with multiple computational units since it needs a lot of inter-processor communication. Slicing the indices can split the original tensor network into multiple tensor networks with identical structures (same hypergraph), each with a memory cost small enough to fit into a single processing unit. The simulator can execute these sub-tasks in parallel without dependencies or sequentially depending on the computational resources. The value of the original tensor network is given by summing up the partial results obtained from the contractions of each sub-tasks, in which the assignment of the selected sliced indices is fixed. Take Fig.as an example, after slicing, the original tensor network can be further represented as:

Finding the optimal subset of indices to slice is also an NP-hard task with complexity, whereandare the number of all and sliced indices, respectively. The strategy here is to use a heuristically greedy algorithm described as the following:(a) find out all intermediate tensors that are larger than the target size;(b) pick up one index shared by the largest tensors or appears most frequently among step (a) tensors random;(c) repeat the above steps until the contraction width is low enough.

Finding slicing indices is strongly entangled with searching for the best contraction path. Slicing is based on an existing contraction path, but the optimal path for the original tensor network is not guaranteed to be suitable for slicing. Therefore, JDtensorPath proposes a two-phase indices-slicing-incorporated contraction path finding heuristic. The first phase generates contraction tree trials followed by 128 pseudo-slicing trials, which only calculate the FLOPs without actually slicing indices on contraction trees. In the second phase, apply “real" slicing trials (default is 1024 times) on the contraction tree with the lowest FLOPs after pseudo-slicing. Then, the simulator can obtain a “good enough" slicing indices and contraction path for sub-tasks in a reasonable searching time. The implementation of the slicing heuristic is based on set operations, and the simulator will store only the best-sliced contraction tree. By doing so, it becomes a very efficient and memory-saving algorithm. Besides, this slicing subroutine can also be carried out in parallel in the CPU computation using the Ray framework.

TeD-Q enables users to customize their control of the tensor network contraction process. Code Listingshows an example of compiling a quantum circuit with tensor network contraction mode using JDtensorPath.

To enable the tensor contraction mode, user needs to specify the options for slicing and hypergraph-based optimizer, shown as “" and “", and pass these options and a contraction path optimizer, shown as “", into the circuit compiler. The compiled circuit would be constructed in tensor contraction mode. The detailed description of each option and argument can be found in the github repository and the documentation of TeD-Q.

SECTION: Variational quantum circuit
Optimization is an important common factor of most NISQ algorithms, including quantum machine learning and quantum chemistry simulation. It relies on a hybrid quantum-classical optimization process on the parameterized quantum circuit, or the so-called variational quantum circuit (VQC). For instance, quantum neural network (QNN) can be regarded as a VQC. In quantum machine learning tasks, a VQC could work as a component together with some pre- and post-processing like feature extraction and prediction. Typically, the cost function of a task can be taken as the expectation value of a VQC:

The optimization process relies on the gradient of the circuit output with respect to the parameters of the quantum gates. The parameter shift and finite differential methods are the common methods to obtain gradients of a quantum circuit for both quantum hardware and simulation, which are the built-in methods of TeD-Q. However, the simulation with such methods usually takes a very long time due to the computational complexity. Meanwhile, the demand for fast simulation of circuits with large number of qubits has become urgent due to recent development of large-scale quantum devices. To this end, TeD-Q provides an efficient method to compute the gradient of a circuit in the simulation via the conventional backpropagation technique and the newly-developed tensor-network engine of TeD-Q.

SECTION: Backpropagation method
The backpropagation method in TeD-Q is based on the well-developed automatic differentiation libraries, such as PyTorchand JAX, which have been widely used in classical machine learning. These libraries are integrated into TeD-Q as simulation backends. When the backend is specified, TeD-Q will convert the matrix data of the quantum circuit to the corresponding format. While evaluating the quantum circuit, known as the "forward" computation, the backend will store all of the intermediate results. The backend will use the chain rule to obtain the gradient of the gate parameters from the circuit output, known as backpropagation. This function will store the gradient of each intermediate result so that the gradient of all of the parameters can be evaluated at one time. Although the backpropagation method cannot be applied to real quantum devices, this is the fastest method to obtain the gradient in quantum simulations. Thus, this is the default method for the simulation backends.

SECTION: Parameter shift and finite differential method
In real quantum devices, the gradients of the variables can only be obtained by the parameter shift method or finite differential method. The parameter shift method can provide the analytical derivatives, while the finite-differential method provides only numerical derivatives.

In TeD-Q, we implement the parameter shift method based on the theoretical paperand recent developments by. This method can be applied to quantum gates with the form, whereis a Hermitian operator with two or four eigenvalues, andis the parameter of the gate. For a single gate parameter, the analytical derivative can be calculated by evaluating the expectation value of the circuit twice with the parameter value beingand, as shown in Equation, whereis the shift andis the shift constant. TeD-Q’s backend will shift the parameters automatically with proper spacing for each gate. For the simulation backend, like PyTorch or JAX, the user can use the parameter shift method to obtain the gradient by specifying it in the differential option while compiling the circuit, as shown in Code Listing. For the hardware backend, the parameter shift is the default and only method to obtain the gradient.

SECTION: Quantum to classical computing interface
The design paradigm of TeD-Q provides seamless integration of quantum and classical processing. In this hybrid algorithm, a quantum circuit that is parameterized by tunable variables is seen as a normal Python function with a specific interface. It can be inserted into a classical optimization function or wrapped in a layer of a machine learning framework module.

TeD-Q supports two interfaces – JAX array and PyTorch tensor. For the JAX simulation backend, the JAX array is the default one, but the PyTorch tensor interface can also be used via the interface keyword argument (interface="pytorch") while compiling the quantum circuit. As for the PyTorch simulation backend and hardware backend, TeD-Q only supports the PyTorch backend at the time of writing. The interfaces provide a seamless connection to the classical part in "forward" calculation and in "backward" gradient extraction. For simulation backends with their own interface and default backpropagation gradient method, the integration is straightforward, relying on built-in data structure and automatic differentiation algorithm of JAX or PyTorch. For other cases (JAX backend with PyTorch tensor interface, hardware backend, parameter shift method), the classical backpropagation algorithm cannot process the quantum information inside the quantum circuit. For this situation, the quantum circuit is treated as a black box. TeD-Q implements custom autograd functions and will compute and provide the gradient (one output) or Jacobian (multiple inputs and multiple outputs) of the quantum circuit with respect to its classical inputs and variables. Code Listingshows an example of connecting the JAX backend quantum circuit to the classical calculation by the PyTorch tensor interface. More detailed examples can refer to Section. The integration is imperceptible to the user, via JAX or PyTorch, with the flexibility to run the hybrid quantum-classical algorithms code on CPUs, GPUs, and TPUs.

SECTION: Use case
The detailed introduction and manual of the user interface can be found in the online documentation. It includes step-by-step examples demonstrating the feature of Ted-Q. In the following section, We briefly go through the main features and its use cases.

SECTION: Basic quantum circuits
Quantum circuit evaluation is the basic function of TeD-Q. It provides a universal set of quantum gates so all kinds of quantum circuits can be constructed and simulated. With the tensor network engine described in Section, TeD-Q can process the simulation faster than the conventional quantum simulators. Besides, TeD-Q is equipped with an easy-to-use graphical circuit composer and circuit drawer. These tools are well integrated with TeD-Q’s simulator. Users can use it for quick prototyping of the quantum circuit. Moreover, the hardware backend in Ted-Q provides an interface to the real quantum devices like Qiskit. Users Can switch the simulation to a real experiment with a simple command. The following sections demonstrate these features via TeD-Q API.

Consider a simple two-qubit quantum circuit with agate on qubit, agate on qubitsand, and a rotation gateon qubitapply to the initial zero state. And we want to obtain its quantum state with some input rotation angles. TeD-Q can implement the simulation as code block. Qubits are counted from 0 and multiqubit registers labeled with the first (zeroth) qubit on the left. Measurement of a quantum state is only available by the simulation backend, while both simulation and hardware backend enables expectation values and probability measurements. Keyword argument “requires_grad" can disable the gradient feature and improve the simulation speed since the backend doesn’t need to store the intermediate state for backpropagation.

Circuit composer in TeD-Q is a WYSIWYG (what you see is what you get) editor for a quantum circuit. The graphical composer can be initialized with a single line command, as shown in Code Listing. It consists of a panel for the template quantum gates and the several qubits indicated by lines, as shown in Figure. Users can construct the circuit by dragging the icon of the quantum gate to a specific qubit line or remove one quantum gate by dragging it away from the qubit line. Users can also adjust the sequence of the operations by pulling the icons to the desired location.

The composing result can be converted and fed into TeD-Q’s tensor network engine. The user can obtain the simulation result using the same method mentioned in the previous section, as shown in the Code Listing. Users can also get the circuit definition from the composing result for further development with TeD-Q API.

Besides the circuit composer, the circuit initialized by the script can also be visualized with the circuit drawer, as shown in the Code Listing. It follows the same drawing style as the circuit composer. Moreover, the function will automatically fit the circuit to the correct size of the plot.

Aside from simulation, the circuit can also be adopted to real quantum devices with the hardware backend. At the time of writing, TeD-Q only supports IBM-Q quantum devices, and more quantum hardware devices will be available soon. The built-in parameter shift method is responsible for obtaining gradients of the quantum circuit, and the PyTorch tensor interface is provided for communication with other functions. Code Listingshows an example of compiling a quantum circuit into two executable quantum objects with both simulation and hardware backends and tries to minimize their difference by tuning input parameters with a built-in gradient-descent optimizer.

SECTION: Variational quantum algorithm
A variation quantum algorithm consists of a variational quantum circuit and a classical optimizer. The method to obtain the gradient of a gate parameter in a quantum circuit is described in Section. Recursively updating the gate parameters allows the circuit to be optimized according to a certain cost function. It’s widely used in current NISQ devices and quantum simulations. With TeD-Q’s tensor engine, simulation can be processed at a very high speed compared to other simulation packages. Besides, with the hardware backend, the variational algorithm can also be adapted to a real quantum device. The following example shows a simple circuit based on the variational quantum algorithm to demonstrate how to construct a VQA circuit by TeD-Q API.

SECTION: Quantum machine learning
Machine learning deals with the problem of generalization from finite data in high-dimensional Hilbert spaces. It is a very appealing application for quantum computation since both of them involve statistics theories at a fundamental level. Users can transform classical feature information into the quantum state through encoding schemes (basis encoding, amplitude encoding, angle encoding, etc.):

VQA can be designed as a linear model to classify data explicitly in Hilbert space. With a nice performance of TeD-Q’s tensor engine, a more complex VQA circuit can be constructed and simulated, which allows testing of a large-scale quantum and hybrid quantum-classical machine learning algorithm with the simulator.

TeD-Q equips several pre-defined quantum circuit templates, or ansatz, commonly used in the quantum machine learning field to enable the fast development of the circuit for quantum machine learning. The following briefly introduces three templates in TeD-Q – Random-generated layer, Fully-connected layer, and Hardware-efficient layer.

The randomly-generated layer can provide a circuit with a single- and two-qubits gate while their locations and parameters are random (Fig.). It’s suitable for simulating a non-linear transformation, as shown in TeD-Q’s Quanvolution Neural Network example.

The fully-connected layer is a parameterized circuit to provide maximum entanglement among each qubit. The number of layers is called depth, which the user can customize. Each qubit is connected to the other qubits in each layer by a CNOT gate followed by a two-parameterized rotation gate (Fig.). This template is usually used to simulate a classical fully-connected layer in quantum machine learning.

The hardware-efficient layer is a practical implementation of a fully-connected circuit. In the real quantum device, a two-qubit operation is only efficient between the qubit nearby the qubit. Therefore, the CNOT gates in this template are only placed between the qubits nearby a selected qubit (Fig.).

Users can easily exploit quantum machine learning by replacing some layers of a well-developed classical machine learning model with a wrapped quantum layer. Code Listingillustrates how to build the pipeline in TeD-Q.

The above example shows a way of constructing a hybrid quantum-classical machine learning model. TeD-Q can straightforwardly integrate quantum algorithms with various well-established machine learning practices. The quantum circuit model behaves as if it was a pure classical one. All the following processes, like cost function or optimization, can be seamlessly adopted. In the next , a pure quantum machine learning example is introduced and used as a benchmark to compare the performance of TeD-Q and other software.

SECTION: Performance
We benchmark TeD-Q’s performance with other quantum software through two examples – multiple qubits rotation (MQR) and many body localization (MBL). MQR is a very simple algorithm which uses hardware-efficient network to flip the quantum state from all zeros stateto all ones state. It is suitable for evaluating TeD-Q’s performance on small and simple circuit which can be run on personal computer.
MBL is a relatively complex problem of great significance both theoretically and practically. It is used to demonstrate the great advantage of TeD-Q tensor network contraction mode. The source code of the comparison can be found on TeD-Q’s Github.

SECTION: Multiple qubits rotation
In this simulation, the initial quantum state is set to be all zeros statefollowed by a variational hardware-efficient network and expectation value measurement on Pauli-Z basis for each qubit. The learning is done by minimizing sum of the measurements through tuning parameters via gradient descent method. And the performance of platforms are evaluated by the time cost for finishing 500 iterations of training. Since there is a great demand for user to develop and learn quantum algorithm through simple circuit on their personal computer, the evaluation is done with single AMD Ryzen5 3500U mobile processor CPU. Fig.shows the advantage of TeD-Q in this application.

SECTION: VQE of Hydrogen Molecule
Variational quantum eigensolver (VQE) method is one of the popular applications of quantum computing in the NISQ era. It is a concrete example of variational quantum algorithm (VQA) used for solving quantum chemistry problems, specifically, the eigenstate problem. In a VQE algorithm, a parameterized quantum circuit would prepare a quantum state, dubbed as the ansatz state, which is supposed to be the ground state of the Hamiltonian of the molecule. By repeatedly measuring the ansatz state with different measurement settings, the expectation value of the Hamiltonian could be evaluated. An optimization process could be applied to the parameters of the quantum circuit so that the expectation value could be minimized.

Here we use the VQE of Hydrogen molecule to illustrate the performance of TeD-Q versus other platforms. In this experiment, there are totally four qubits and the quantum circuit contains 12 freely changing parameters. (The problem can in principle be simplified to use just one qubit, but we did not do this since we only want to compare the speed on the same ground.)

We run the experiment on a Macbook Air with Intel Corei5-1030NG7 CPU. As we could see from Fig., TeD-Q converges to an error ofHartree about 3 times faster than Pennylane and around 15 times faster than Qiskit. With a limited time of execution, for instance, within 5 seconds as shown in Fig., TeD-Q can correctly plot the energy curve of the ground state with different H-O distances, while Pennylane can almost do it and Qiskit being far away from that. When the time limit is set to 30 seconds Pennylane can converge to the correct energy curve but Qiskit still cannot, as shown in Fig.. From this comparison, we can see TeD-Q indeed has advantages in simulating traditional VQE algorithm over the compared platforms.

SECTION: Quantum GAN
Classical generative adversarial network (GAN) has found its position in artificial intelligence via rapid growing applications recent years. The idea is to utilize a generative network to create new data that resembles the original data, while a discriminative network tries to evaluate the distance. The two networks contest with each other to play a zero-sum game, which leads to improved quality of the newly generated data.

Quantum GAN borrowed this idea and wishes to boost the performance by quantum computing. While it is possible to use QNN in both the generative and discriminative networks, here, we adopt both the patch and non-patch quantum GAN methods where only the generative part is made quantum.

In this example, the generator tries to mimic the handwritten zeros from a small set of grayscale samples images with size. Please seefor details of the QGAN example. Similar to the multiple qubits rotation example, we run the experiment with an AMD Ryzen5 3500U mobile processor CPU. As we can see from the result Fig., in either patch or non-patch scenario, TeD-Q outperforms Pennylane in the training of the quantum GAN model.

SECTION: Quantum Architecture Search
Quantum architecture search (QAS) was recently proposed to generate better variational ansatz circuits for given VQA tasks. By constructing an ansatz pool, QAS iteratively samples a new ansatz at every step of the optimization in the VQA. It can be shown that by doing this, the resulting ansatz improves the performance of the VQA in terms of both circuit output and learnability in learning tasks.

In the training of the QAS task, the ansatz circuit is replaced by new sample from the pool at every step of the optimization, which requires the backend to compile the new ansatz circuit at every step. Thus, the difference of the performance mainly depends on the speed of implementing each circuit. We run the experiment on a Macbook Air with Intel Corei5-1030NG7 CPU. The result in Fig.shows the time the training takes with increasing iteration numbers. TeD-Q is more than 2 time faster than Pennylane when performing QAS tasks.

SECTION: Many body localization
Many body localization (MBL) is a phase of an isolated many-body quantum system that prevents it from thermalization; that is, a system in this phase can preserve the initial information as time evolves. It provides a possibility to hold the quantum effect for a long time. The experimental and theoretical research in this field has been dramatically progressing in recent years due to the rapid development of quantum technology. In this section, we demonstrate that TeD-Q can process a simulation of a 1-D MBL along with a quantum neural network with more than 50 qubits at a reasonable speed, which is not achievable by conventional quantum simulators.

For an isolated many-body quantum system, it evolves under its intrinsic Hamiltonian. The interaction between the particles exchanges information and energy so that the close system reaches thermal equilibrium as the whole system is the thermal bath of its subsystem. The information of its initial state is eventually washed out. However, in the presence of sufficient disorder potential, the system can reach the many-body localization phase and prevent it from falling into thermal states even though the particles in the system still interact.

The system would fall into thermalization with a small disorder potential, known as the ergodic state. On the other hand, with sufficient disorder power, the system would be in the MBL phase and stay in its initial state, known as the localized state. However, multiple measurements are required to distinguish a localized state from an ergodic state for a many-body system. Following the circuit architecture shown in, we develop and train a quantum neural network for classifying the states of a 1-D many-body system. With this QNN circuit, we can identify whether the circuit is either an ergodic state or a localized state with a single measurement on the first qubit.

In our simulation, the qubits are placed in a chain and prepared in the Neel state. It is followed by a simulated Hamiltonianincludes the interactions between the nearest qubits and a controllable random disorderfor each qubit, which is

and the system is evolving under this Hamiltonian for a period of time. For the state classification, the circuit is then connected to a classifier built in a digital-analog quantum neural network, proposed by. It is consist ofsingle-qubit operation, which is the digital part, and an unitary operation, which is the analog part, on each qubit.

Finally, the only measurement was placed on one of the qubits. The parameters of QNN would be trained so that the measurement of it can classify the state of the system. The circuit diagram is shown in Figure.

The simulation parameters, likeand, follow the setting used in, which are the properties of a 64-qubit quantum computer. The parameters in the classifier are then trained with the binary cross entropy loss so that the expected value of the the selected qubit can be 1 when the system is in an ergodic state and 0 when the system is in a localized state.

The performance of TeD-Q is evaluated by the time cost for finishing 50 epochs of 1D-MBL training. The circuit depth is about 89, and each epoch consists of 40 training iterations of the circuit. For the comparison, we also set up the same simulation using Qiskit and Pennylane. CPU simulations use Intel(R) Xeon(R) Processor E5-2680 v4 @ 2.4GHz CPU while GPU simulations use Nvidia Tesla P40 24 GB GPU.

Figurecompares these three quantum simulation methods on CPU and GPU. TeD-Q can provide similar behavior as PennyLane while using state vector propagation mode. This method becomes computationally infeasible while the simulation involves more than 20 qubits. However, TeD-Q, using tensor network contraction mode, can process the same simulation with up to 50 qubits within a comparable time.

SECTION: Discussion
Generally, the state vector propagation mode of TeD-Q and PennyLane have similar performance both in CPU and GPU. This is with-in our expectation since both of them calculate the full amplitude and use backpropagation method for obtaining gradient. TeD-Q has a slight advantage () over Pennylane on CPU for qubits less than 11, this can be explained by the reusable mechanism that saves the time for state and gate class instantiation.

Tensor network contraction mode shows its power on large qubit systems. It can reduce the computation and memory complexity substantially. But currently its performance is constrained by inefficient use of GPU. Most pair contractions in the contraction path involve small tensors, they contribute very small portions of the overall computation and memory complexity. But the transmission of these small tensors to GPU will cost sizable overhead. This overhead problem can be improved by pre-computing contractions with small sizes on CPU and only performing the large ones on GPU, which will be included in the future release of TeD-Q.

SECTION: summary
In this work, we introduced TeD-Q, an open-source software framework for quantum machine learning, variational quantum algorithm, and quantum computing simulation. With tensor contraction, simulation of quantum circuit with large number of qubits is possible. TeD-Q seamlessly integrates classical machine learning libraries with quantum simulators, allowing users to leverage the power of classical machine learning while training quantum machine learning models. TeD-Q is also empowered with the capability of training quantum machine learning models in a distributed manner. This is an important feature considering that most NISQ quantum algorithms require results from multiple instances of the same quantum circuit. In TeD-Q, the quantum circuit is seen as a Python function, which makes it easy to use. TeD-Q is device independent so that users can run the circuits on different backends, including software simulators (JAX and PyTorch) and quantum hardwares.

To better facilitate the implementation of quantum machine learning, TeD-Q supports automatic differentiation, for instance, backpropagation, parameters shift, and finite difference methods, to obtain gradient. It also has a flexible interface that bridges quantum circuits to powerful machine learning libraries (like PyTorch). In addition, TeD-Q supports visualization functionality, where users can visualize quantum circuits and training progress in real-time.

There are still challenges in future works of TeD-Q. One crucial aspect is how to leverage distributed computation’s ability to solve real quantum machine learning problems. Even though TeD-Q supports distributed computation, algorithms, and applications that make the most use of it are still lacking. Another aspect that TeD-Q can improve is to work completely compatible with different kinds of quantum hardwares on the cloud. We have demonstrated how TeD-Q can access the hardware resources on the IBM cloud in Sectionand in later releases support for other platforms, e.g. Pennylane and Amazon, will be introduced. In principle, TeD-Q is ready to connect with any device; however, due to the lack of quantum hardware resources, this has not been tested. It would be exciting to see TeD-Q working with different hardware backends and implementing real distributed quantum algorithms, especially QML algorithms, in the future.

X.-Y. W., Y.-X. D., and D.-C. T. conceived the project. Y.-C. C. and X.-Y. W. designed the architecture of the software. Y.-C. C., X.-Y. W., and C.-Y. K. did the coding and carried out the numerical simulation. X.-Y. W., Y.-X. D., Y.-C. C., C.-Y. K. and D.-C. T. analyzed the results. All authors contributed to discussions of the results and the development of the manuscript. Y.-C. C., C.-Y. K., and X.-Y. W. wrote the manuscript with input from all co-authors. X.-Y. W., Y.-X. D., and D.-C. T. supervised the whole project.

SECTION: References