SECTION: ProtGO: A Transformer based fusion model for accurately predicting Gene Ontology (GO) terms from full scale Protein Sequences
Recent developments in next-generation sequencing technology have led to the creation of extensive, open-source protein databases consisting of hundreds of millions of sequences. To render these sequences applicable in biomedical applications, they must be meticulously annotated by wet lab testing or extracting them from existing literature. Over the last few years, researchers have developed numerous automatic annotation systems, particularly deep learning models based on machine learning and artificial intelligence, to address this issue. In this work, we propose a transformer-based fusion model capable of predicting Gene Ontology (GO) terms from full-scale protein sequences, achieving state-of-the-art accuracy compared to other contemporary machine learning annotation systems. The approach performs particularly well on clustered split datasets, which comprise training and testing samples originating from distinct distributions that are structurally diverse. This demonstrates that the model is able to understand both short and long term dependencies within the enzyme’s structure and can precisely identify the motifs associated with the various GO terms. Furthermore, the technique is lightweight and less computationally expensive compared to the benchmark methods, while at the same time not unaffected by sequence length, rendering it appropriate for diverse applications with varying sequence lengths.

KArtificial IntelligenceAttentionBERTDeep Learningfine-tuningFusion modelsGene Ontology (GO) termsMachine LearningProtBertPretrainingProtein annotationProtein sequencesSelective Transfer LearningTransformers

SECTION: Introduction
The swift advancement of sequencing technologies has accelerated the growth of extensive protein sequence databases and made it both economically viable and practical to build large protein repositories both in the private domain and under open-source licenses. The substantial augmentation in the accessibility of protein sequences led to the establishment of protein databases such as Uniprot, with hundreds of millions of data points. Each day, about one hundred thousand proteins are added to the ever-expanding global public protein databases. To maximize the utility of these protein databases, they must be accurately annotated by the scientific community. Domain experts and curators exert considerable effort to manually annotate these protein sequences to address this issue; nonetheless, this procedure is exceedingly slow and resource intensive resulting in successful annotation of only approximately 0.03% of proteins available in public databases.

The demand for expedited annotation of extensive protein datasets has led to the emergence of many automated and semi-automatic approaches. Various annotation systems have been developed over the years, employing distinct approaches to label proteins. One approach involved comparing the sequence of the target protein to other previously identified proteins to identify commonalities. The target received identical annotation to the known protein if substantial sequence segments between them exhibited alignment. BLASTwas among the most effective tools employing this methodology for annotating protein sequences.

A subsequent prevalent approach for protein annotation employed functional features to characterize target proteins. A protein is ascribed a certain function if it possesses the pattern linked to that function. Subsequent iterations of these models began employing profile Hidden Markov models (HMMs), enhancing their robustness and capacity to identify soft matches. These models calculated the likelihood of a new sequence possessing a specific function if the likelihood exceeded a set threshold. They are rapid and effective, resulting in numerous notable protein databases, such as Pfam and InterPro, utilizing them to annotate their extensive protein collections. However, these models, while rapid and precise, possess certain limits.

Conventionally, they calculated the likelihood of each functional motif separately, resulting in reduced efficiency and accuracy when different functional groups shared a common characteristic. Furthermore, the signatures employed to identify a certain function in a protein sequence frequently necessitate human curator involvement and are not entirely automated, rendering the entire process quite tedious and laborious. These constraints the need for additional research in the area to develop more efficient annotation systems to manage the increasing volume of proteins that are continuously added to databases everyday.

Machine learning algorithms, particularly a variant known as deep learning characterized by models with multiple hidden layers, have garnered significant interest recently due to their success across various applications. Initial deep learning models focused on computer vision and machine translation applications, but they rapidly diversified into additional domains such as medical imaging and bioinformatics.

The functioning of a deep learning model typically comprises of two phases. The initial phase involves training, during which labeled and semi-labeled data is used to ascertain the model’s weights, succeeded by the inference phase, where the acquired weights are employed to classify an unknown sample. The later layers of models, which engage in more generalized functions, may be utilized across analogous tasks. This approach is referred to as transfer learning, wherein, rather than training a model from scratch for a new task, the model’s weights can be initialized from a comparable pretrained application. Subsequent to the transferring process, the model weights undergo training on the fresh data in a process referred to as fine-tuning. This method yields improved accuracy with reduced training data, requiring less training time and lower computational resources.

In light of the success of deep learning models across diverse domains and applications, numerous studies have been conducted to implement them in protein functional prediction and classification tasks. Additionally, deep learning models have been applied to protein structure predictionand protein design.

A significant advancement in protein annotation commenced when researchers began inputting substantial volumes of unstructured raw data into extensive deep-learning models. This exempted the models from human biases, facilitated the identification of previously unrecognized patterns, and mitigated bottlenecks resulting from the time consuming curation process. Recent advancements in this domain encompass DeepEC, an ensemble Convolutional Neural Network (CNN) model that annotates protein sequences with Enzyme Commision (EC) numbers.

However, it can only annotate sequences shorter than one thousand units and necessitates a minimum of ten data points from a single class for optimal functionality. Other notable advancements in the area include CNN-based ensemble models for annotating unaligned protein domain sequences, the application of a transformer-based model to enhance both accuracy and computational efficiency, the training of various models to comprehend protein sequence structures, and the development of a transformer-based model for predicting protein functionality. But these studies do not address whole protein sequences, which are frequently significant for bioinformatics applications. This issue has been addressed in Proteinfer, which use both a singular and an ensemble CNN architecture to annotate whole protein sequences with GO terms and Enzyme Commission (EC) numbers.

We have previously developed a ProtBertbased model named ProtECto accurately annotate EC numbers with high accuracy. This work further builds upon that to propose a classification based fusion model to predict Gene Ontology (GO) terms from full scale protein sequences that achieves state-of-the-art performance while at the same time addressing some of the major shortcomings of the previous models.

The primary novelties of our study are outlined below:

Achieving state-of-the-art accuracy on GO term annotation of full-scale protein sequences.

Employing a singular model rather than an ensemble of models to enhance system efficiency and usability, while at the same time making it more light weight.

The training time, GPU memory, and other computational resources needed to utilize the model are considerably lower compared to other similar methods.

The proposed model features fully automated hyperparameter optimization, facilitating training without the need for a development or validation dataset.

A superior comprehension of the protein sequence structure, which is demonstrated by the minimal accuracy disparity between the random and clustered dataset splits.

Negligible dependency on the input sequence length, exhibiting robustness and the ability to be deployed in applications with significant input sequence length variability.

The paper is structured into four sections. The first section presents an overview of the study accompanied by appropriate literature reviews. The second section delineates the model architecture comprehensively and explains the training and evaluation techniques. The third section presents the experimental results of the proposed model and the performance comparison with other benchmark methods. Finally, the last section finishes the work and offers additional discussions on potential future research directions.

SECTION: Model
This section first describes the input and output of the model, the dataset being used in this study, and the steps taken to preprocess the dataset. Next, the architecture of the proposed model is described in detail along with the training and evaluations steps and the experimental setup.

SECTION: GO Terms
GO terms are keywords used to annotate protein sequences which contains various information like the gene that was used to transcribe the protein molecule, the type of organism which primarily uses the protein, the functionality of the protein, the biological processes and pathways connected to it, etc. The annotation of the protein sequence with GO terms is of utmost importance to the biological community as it helps identify the characteristics and properties of the protein along with its usage and functions within an organism. The GO terms could be divided into three categories based on the type of information they contain:

Activities at the molecular level executed by gene products. Molecular function delineate processes that transpire at the molecular level, such as "catalysis" or "transportation." It denote activities rather than the entities (molecules or complexes) executing the actions, and do not delineate the location, timing, or context of the action. Molecular functions typically relate to activities executed by individual gene products (such as proteins or RNA), while certain activities are carried out by molecular complexes consisting of numerous gene products. Broad functional terms include catalytic activity and transporter activity, whereas narrower functional words encompass adenylate cyclase activity and Toll-like receptor binding.

This type of GO term describes the location occupied by a macromolecular apparatus relative to cellular compartments and structures. The gene ontology delineates the locations of gene products in two manners: the first method is by cellular anatomical entities, where a gene product performs a molecular function. Cellular anatomical entities encompass cellular components like the plasma membrane and cytoskeleton, along with membrane-bound compartments such as the mitochondrion. The other method is with the help of stable macromolecular complexes to which they belong, for instance, the clathrin complex.

The extensive procedures, or ’biological programming,’ executed through various molecular activities. Instances of general biological process terminology include DNA repair and signal transduction. Examples of narrower terminology include pyrimidine nucleobase biosynthesis process and glucose transmembrane transport.

SECTION: Dataset
The protein sequences and their corresponding GO terms used in this study has been extracted from UniProt, which is a large public repository of protein sequences. The UniProt archive consists of two different databases; the first is called Swiss-Prot, which consists of about half a million protein sequences annotated manually by domain experts. The other database is known as TrEMBL and consists of around 250 million unreviewed sequences. The dataset used in this study has been prepared from the sequences taken from the SwissProt database as it contains reliable labels and could be used to train and test our model.

Two different dataset splits are used to train and evaluate our model. The first is the random split, where the training, development and testing sets are created by randomly splitting the whole dataset in a 8:1:1 ratio. The second dataset split is called the clustered split, where a tool called Unirefis used to divide the dataset into sections where the sequences in each splits are different in structure and has the least possible common segments between them. This makes the clustered split dataset much more challenging compared to the former. The number of datapoints in the random and clustered split datasets are given in Tableand Table, respectively.

Next, during the preprocessing steps, the sequences in the dataset that does not have any GO term annotations associated with them are filtered out. After that, the list of GO terms present in the dataset are collected and divided based on their aspects, which are biological processes, cellular components and molecular functions. Each dataset is next split into three parts where each contains only GO terms confined to one aspect. Only the top 100 most populous GO terms from each aspect are used for training and evaluation as many of them only appear in the dataset very rarely. The number of protein sequences in each aspect for the training, development and testing sets are also given in Tableand Tablefor the random and clustered datasets respectively.

After separating out the datasets based on the three aspects, the GO terms are one-hot encoded where, a value of 1 is assigned to the terms that are present for a particular protein sequence and 0 if they are absent. Next, the protein sequences are tokenized and inputted into the algorithm while the one-hot encoded GO terms serves as the truth labels or targets.

SECTION: Model Architecture
A block diagram illustrating the detailed architecture of the proposed model is given in Fig.. Three transformer based modules are used in the model, where each specializes on one aspect of the GO terms. The protein sequence is first tokenized, where each amino acid in the sequence is represented by an unique alphabet. The tokenizer takes the input sequence and converts it into a string of vectors which could be now inserted into the transformer architecture.

The protein sequence is copied and parallelly inserted into all three of the transformer blocks; Prot_CC, Prot_MF, and Prot_BP. Each of these modules predict the GO terms related to one of the aspects. Next, the predictions from all three blocks are combined together to create the total output of the model.

The internal architecture of each of the prot_X block is also given in Fig.. After the input protein sequences are tokenized, they are fed into three encoders namely, the positional encoder, the token encoder and the segment encoder. Each of the encoder is responsible for generating one aspect of the input tokens. The token encoder and the segment encoder are together referred to as non-positional encoders.

Internally, each of the encoders contain learnable weights which could be factored with the inputs to generate the output embeddings. The positional enbeddings contains information about the position of each token in the sequence, while the token embeddings describes the type of token in each position. Lastly, the segment embeddings mostly remains dormant in this case as the protein sequences are not divided into multiple segments. After all the three types of embeddings are generated, they are summed together and fed into the transformer encoder.

The encoder module of the transformer is made up of multiple encoder layers stacked on top of each other. The proposed model consists of 30 encoder layers, where each consists of a multihead attention module followed by layer normalization and mean pooling. Each encoder layer also consists of a skip connection or residual network that facilitates the propagation of early layer information to the later layers of the model. A model dimensionality of 1024 is used along with 12 heads for the attention layers. After the transformer encoder extracts the embeddings from the input sequence, they are fed into a classification layer which predicts the GO terms that the input sequence are associated with. Lastly, the labels from all three submodules are collected and pooled together to form the final model output.

SECTION: Training and Evaluation
The first step in the training process involves pretraining the Prot_X blocks on the BFD100 dataset which consists of 2,122 million protein sequences of variable lengths. The pretrained variant of ProtBert was obtained from the HuggingFace website. During pretraining, segments of the sequence, determined by the masking probability, were concealed, and the model was instructed to predict the masked tokens. This enables the model to comprehend and assimilate intricate patterns within the data, as well as the interrelations among various amino acids in the protein sequence. The ProtBert module comprised of 30 hidden layers, each containing 1024 nodes, with an intermediate hidden layer size of 4096. The model includes 16 attention heads and contained a total of 420 million parameters. It was pretrained inutilizing 128 nodes and 1024 TPUs for 800,000 steps on sequences limited to a maximum length of 512, followed by an additional 200,000 steps on sequences with a maximum length of 2,000. A masking probability of 15% was implemented in conjunction with a Lamb optimizer. A learning rate of 0.002 and a weight decay rate of 0.01 were chosen.

To adapt the ProtBert module for our purposes, we acquired the pretrained weights and fine-tuned them on enzyme sequences. Nonetheless, fine-tuning substantial transformer models such as ProtBert demands significant data and computational resources. To address this, we have executed selective fine-tuning, wherein certain layers of the model are adjusted while others retain their pretrained values. This strategy enhances accuracy in large models with limited data availability. The fundamental architecture of the ProtBert model, along with the selectively fine-tuned layers, is illustrated in Fig.. An Negative Log Likelihood (NLL) loss function is used to pretrain the modules in an unsupervised manner. The layers which were frozen to get the most efficiency for the ProtBert modules were referenced from.

Next, the pretrained Prot_Bert modules are selectively finetuned on the processed random and clustered datasets. The training set from each type of split is used to train the model, while the testing set has been reserved for the evaluation and comparison with benchmarks methods. The cross entropy loss function has been used during the finetuning process and is given in equation 1. Here, N is the total number of samples in the dataset while M is the total number of classes, y represents the true label andstands for the predicted labels. An Adam optimizer has been used to train the model on a total of 10 epochs with an initial learning rate of 5e-4. The HuggingFacetrainer has been used to train the model which automatically adjusts and optimizes most of the hyperparameters along with a gradient accumulation step of 32. The model was trained on a single Nvidia RTX2080Ti GPU which required around 3.5GB of memory and took approximately 80 hours to be completely trained.

After the training step, the model was evaluated on the held out testing set for both the random and clustered split datasets. The results obtained in the experiment was compared with the benchmark models and are presented in detail in the next section.

SECTION: Results
This section presents the evaluation statistics and performance comparison of the proposed ProtGO model with previous state-of-the-art benchmark models. Other analysis results including the Receiver Operating Characteristic (ROC) curve and sequence length analysis of the proposed model have also been outlined and discussed in this section.

Several evaluation metrics have been used to comprehensively compare the results of the models on both the random and clustered dataset splits. Tableshows the performance of the proposed model on all three GO aspects, namely, Biological Processes, Molecular Function, and Cellular Component on the random split dataset using the overall Accuracy, F1 score, Precision, and Recall evaluation metrics. The results show that the proposed model, ProtGO has been able to perform significantly better compared to the main benchmark methods Proteinfer and ProteinferEN. Here, ProteinferEN refers to the ensembled version of the Proteinfer model, where several base models have been trained and later put together to generate the final output.

ProtGO outperformed ProteinferEN by around 3% in terms of Accuracy in all three GO terms aspects while showing better F1 scores as well. The Precision scores of ProtGO are also significantly better than the other methods. However, in terms of the Recall scores, ProteinferEN shows better results out of the three compared models. This delineates that the ProteinferEN model is better at avoiding false negative samples compared to ProtGO and vice versa. But the trade off between the false positive and false negative samples could be controlled using a minimum decision threshold hyperparameter, which has been set to the default value for all the algorithms. Hence, a better indicator of performance is the F1 score, which combines the values of Precision and Recall to produce a single metric. In terms of the F1 score, the ProtGO module has been able to consistently outperformed the benchmark methods in both the datasets.

Tableshows the performance comparison of the proposed model along with the benchmark methods on the clustered split dataset. The results are similar to the random split, with ProtGO performing better in terms of Accuracy and F1 score in all three GO aspects. However, the gap between the performance between ProtGO and the benchmark methods is more in case of the clustered split compared to the random split dataset. This shows that ProtGO is a more robust model and have a strong understanding of the structure and deep motif patterns of the protein structure. Hence, it performs similarly for the random and clustered test dataset while the benchmark models show significant difference in Accuracy for the two datasets. There is a gap of around 7% in Accuracy between the proposed model and the benchmark methods for the clustered split dataset which is significantly greater than that seen for the random dataset split. A similar trend could also be seen in the F1 score, Precision and Recall scores for the two splits.

SECTION: ROC Curves
Next, the Receiver Operating Characteristic (ROC) curve is shown in Fig.for the ProtGO model on the two datasets. This plot shows the ability of a binary classifier model to differentiate between the positive and negative samples. In order to transform the ROC curve for multi-class classifications as in this study, the class wise ROC curves have been generated by considering all other classes as negative samples, followed by micro averaging all the classes to get the overall ROC curve. A straight line along the origin with a gradient of 1 represents a random classification and is shown by a black line in the figure which could to be used as a reference. while, an inverted L shaped curve touching the origin and the top two corners would represent a perfect separation between the positive and negative samples.

The ROC plots for the ProtGO model on both the random and clustered dataset splits show near perfect separation of the negative and positive samples with a large area under the curves. The model for the Molecular Function (MF) GO aspect performs the best in both the datasets with the other two closely following it. The AUC values which stands for "Area Under the Curve" is also above 99% for the random split dataset while that for the clustered split is above 98%. These results indicate that the proposed model is excellent at differentiating between the various classes with great ease.

SECTION: Sequence Length Analysis
In the Sequence Length Analysis (SLA) experiment, the variability of the test Accuracy on the length of the input sequence is studied along with the distribution of input sequences in the dataset. Fig.shows a histogram to represent the abundance of input sequences of different lengths in the dataset along with the test Accuracy of the ProtGO model for the various input sequence lengths in the random split dataset for all three GO aspects. The frequency distribution plot peaks at around 750 tokens while most of the sequences are confined between a range of 300 to 1200 amino acids. It could be seen from the second plot in Fig.that the Accuracy of the proposed model holds approximately steady till 1000 tokens while experiencing a gradual downward shift after that. This happens as the input tokens are truncated at 1000 tokens in this study for the ease of computation whereas the input sequence length could be easily increased without having to make any algorithmic changes to increase Accuracy for longer sequences. The Accuracy of the Molecular Function GO aspect does not dip even for very long sequences which shows that these GO terms are relatively easier to predict compared to the others. Whereas, the Accuracy of the other two GO aspects fall to around 70% at very long sequence lengths.

SECTION: Conclusion
In this study, a lightweight, easy to use, transformer-based fusion machine learning model have been proposed which could predict GO terms with high Accuracy from full scale protein sequences. The performance of the model have been tested on two datasets named Random and Clustered split for all three GO aspects. The results show state-of-the-art performance on several different evaluation metrics compared to other recent related methods. The ProtGO model have been able to outperform the benchmark methods used in this study in terms of Accuracy and F1 scores by significant margins in both the random and clustered split datasets. Moreover, the results show that the margin of improvement for the ProtGO model from the benchmark methods in terms of both Accuracy and F1 scores have been significantly greater for the more challenging clustered dataset compared to the random split dataset. This indicates that the proposed architecture is better at deciphering and interpreting deep patterns and structures within the protein sequences which leads to the performance gap being greater for the more challenging dataset.

The ROC curve for the ProtGO model have also been generated for both the datasets and it shows the models ability to successfully differentiate between the positive and negative samples very effectively. This is also demonstrated by the AUC score which is above 99% and 98% for the random and clustered split datasets respectively. Lastly, the sequence length analysis study for the ProtGO model is also conducted where, the model demonstrating limited variability of test Accuracy as the sequence length of the input data is varied. The Accuracy is more stable upto a sequence length of a 1000 amino acids as the input sequences are truncated at that value. This indicates that the Accuracy of the model could be further increased at longer sequence lengths by raising the input sequence truncation threshold hyperparameter.

Despite several scientific studies completed recently on protein annotation utilizing deep learning models, there is a deficiency of research focused on models specifically built for GO term annotation, particularly in scenarios characterized by limited data availability. Additional efforts are necessary to gather supplementary datasets pertaining to GO term annotations to enhance the efficacy of deep learning models and improve the overall Precision of GO term predictions. Furthermore, several protein annotation models that predict distinct functionalities could be integrated into a unified framework, thereby optimizing shared computations in the analysis of protein structure. These are reserved for future endeavors.

SECTION: References