SECTION: Continual Adaptation of Vision Transformersfor Federated Learning
In this paper, we focus on the important yet understudied problem of Continual Federated Learning (CFL), where a server communicates with a set of clients to incrementally learn new concepts over time without sharing or storing any data. The complexity of this problem is compounded by challenges from both the Continual and Federated Learning perspectives. Specifically, models trained in a CFL setup suffer from catastrophic forgetting which is exacerbated by data heterogeneity across clients.
Existing attempts at this problem tend to impose large overheads on clients and communication channels or require access to stored data which renders them unsuitable for real-world use due to privacy.
In this paper, we attempt to tackle forgetting and heterogeneity while minimizing overhead costs and without requiring access to any stored data. We study this problem in the context of Vision Transformers and explore parameter-efficient approaches to adapt to dynamic distributions while minimizing forgetting. We achieve this by leveraging a prompting based approach (such that only prompts and classifier heads have to be communicated) and proposing a novel and lightweight generation and distillation scheme to consolidate client models at the server.
We formulate this problem for image classification and establish strong baselines for comparison, conduct experiments on CIFAR-100 as well as challenging, large-scale datasets like ImageNet-R and DomainNet.
Our approach outperforms both existing methods and our own baselines by as much as 7% while significantly reducing communication and client-level computation costs. Code available at.

SECTION: Introduction
Federated Learning (FL) is a privacy-preserving learning paradigm that enables learning a global model through communication with a distributed set of clients. These clients have exclusive access to private data, and collaborate with a central server to learn a shared task by communicating parameters such as model weights, gradients, or learning statistics. For example, the popular FedAvgmethod works by iteratively aggregating client models by averaging their model weights. Classical FL methods such as FedAvg have garnered significant attention due to the increasing demand for user privacy and the growth of edge computing.

However, currently most federated learning methods focus on learning statically, that is across a fixed set of categories determined. In non-federated works, on the other hand, there has been a great deal of progress on learning an increasing number of categories incrementally, referred to aslearning (and more specifically). In addition to the problem of catastrophic forgetting, incremental learning introduces challenges to typical federated learning (FL) scenarios by inherently involving non-Independent and Identically Distributed (non-IID) data, which
has been shown to cause issues of model divergence. Whileapproaches have been developed, they do not support thedynamic data distributionsthat occur in continual learning and the real-world. For example, such a setting has immense practical impact to applications such as healthcare, autonomous vehicles, and chat-bots.

Therefore, in this paper we look at the understudied problem of. While a few CFL methods exist, they address the issue of forgetting using approaches such as: reducing inter-client interference, knowledge distillation, and image-level generative replay.
Specifically, they often communicate full model weights, real/synthesized image-level data, or gradients. Additionally, some methods store old data in memory buffers or train a generative model to mimic local data; at the very least, all methods share complete models parameters with the server which can lead to privacy leaks with advancements in model inversion and other extraction techniques..

To mitigate forgetting while adhering to the core principles of CFL, we propose HePCo:eterogeneousromptnsolidation (Fig.). Our method is driven by the goals of (i) minimizing communication costs, (ii) improving client privacy, and (iii) client-level computation efficiency. We first propose to leverageprompting-based methods, which have shown successful results in the rehearsal-free continual learning setting. This also has the benefit of utilizing frozen Vision Transformer backbones, meaning that only prompts and classifiers have to be transmitted, reducing communication. The key contribution of our approach is then to answer the question of how to merge prompts from different clients in a scalable manner. Towards this end, we propose a lightweight method for generating pseudo-dataand distilling client model information. Importantly, we distill data from both the past task label distribution as well as the current task label distribution, preventing both catastrophic forgetting and performance degradation due to client heterogeneity.

We modify and extend popular continual prompting based approaches to the setting of Continual Federated Learning (CFL), implementing a range of methods that we will open-source.

We introduce a light-weight pseudo-latent knowledge distillation mechanism to aggregate client models trained on non-IID data partitions. Importantly, our approach incurs no additional client-side computation and does not store or generate training data.

We outperform both existing methods and contributed baselines by as much as 7% while drastically reducing communication costs by only sharing a small set of model parameters, which constitutes only ~9.5% of the total parameters.

SECTION: Related Work
Continual learning algorithms fall into several primary groups. Some methods involve expanding the model’s architecture as new tasks arise, while others regularize the model with prior task knowledge either in the weight space or the prediction space. Additionally, rehearsal-based methods leverage stored data or samples from a generative model. Despite their efficacy, these methods might compromise data privacy and impose substantial memory costs, which justifies the need forrehearsal-free strategies. In the realm of rehearsal-free continual learning, some studies focus on an online learning perspective utilizing a pre-trained model, or explore prototype-based techniques to avert catastrophic forgetting. Other work proposes deep-model inversion to produce images for rehearsal, yet the computational expense and data-privacy issues render this approach challenging. NCDwFperforms classifier inversion and pseudo-replay in the latent-space to reduce forgetting in a novel class-discovery setting. Recent works have demonstrated thatwithin a static, pre-trained transformer model for continual learning achieves state of the art performance without any rehearsal data. Our work builds on the foundations of these prompting methods, but in a federated setting.

Federated learning involves a number of clients learning over local data, to be aggregated by a global server. One of the earliest methods for this is FedAvg, which simply averages the parameters of the clients weighted by the relative amount of data they were trained on.
The most investigated challenge in federated learning (FL) is client heterogeneity, which can cause local models to diverge and the aggregated global model to perform sub-optimally. In this work, we discuss FL works tackling this issue in two categories: parameter-averaging methods and knowledge distillation methods. To combat high heterogeneity in local data, methods like FedProx, FedPD, FedDyn, and SCAFFOLDare used, out of which FedProx is aalgorithm useful in settings where the number of clients is very large which prevents the server from keeping a track of all participating clients. The rest arealgorithms that maintain client states. Knowledge-distillation-based methodsusually use additional data to perform distillation on the server or client side to robustly aggregate local models. FedFTGoperates in a data-free setting by generating pseudo-data through inversion in the image space. In contrast, we achieve knowledge distillation using pseudo-data generated in latent space to 1) reduce computation overhead stemming from inversion in a higher-dimensional space and 2) fine-tune both the classifier and the prompt components simultaneously which is essential to mitigate forgetting.

Most of the current CFL methods suffer from various limitations in terms of performance, efficiency and privacy. FedWeiTaims to learn better client models by leveraging indirect experience of other client models. The objective introduced in this work minimizes interference across client weights. A knowledge base of previous task parameters for all seen clients is maintained at both the server and client side and is updated during each round through client-server communication. Each client selectively utilizes the parameters in this knowledge base through attention masks to gain indirect experience. FedWeiT incurs considerable overheads in terms of communication, computation and storage stemming from maintaining and updating this knowledge base. GLFCuses a prototype based approach with a memory buffer to store old data. This poses a threat to privacy of client data. CFedproposes a distillation based approach that makes use of an unlabelled dataset to aggregate client models as well as to rehearse old tasks. CFeD performs distillation at both client and server side. However the requirement for a curated dataset can severely impact real-world applicability. TARGETattempts to combat forgetting through replay of old tasks driven by generated images. FedCILleverages an Auxiliary Classifier GAN (ACGAN) to alleviate forgetting by synthesizing old images for replay. However, generating images to mimic local datasets can be viewed as a privacy risk especially in light of recent research on model inversion attacks. In contrast, our approach prioritizes client privacy by generating in the latent space, thereby eliminating the need to generate in the image space. Further, this benefits communication and compute efficiency. GALintroduces the task of Federated Continual Novel Class Learning, where FL methods are expected to discover and learn unlabelled novel class data. In contrast, our work explores a supervised continual learning setup where new labeled instances are incrementally introduced. The recent work Fed-CPrompt, investigates the use of prompting for federated class-incremental learning. To combat client divergence, their approach introduces a contrastive loss at the client side. Client aggregation is performed at the server using the standard FedAvg algorithm. Our work differs in that we do not introduce additional objectives at the client side. Instead, we propose a novel method for combining client models at the server to mitigate forgetting.

SECTION: Problem Formulation
In this section, we describe the formulation by introducing the class-incremental learning and heterogeneous federated learning aspects in our CFL setting.

We focus on thelearning scenario, where a model is tasked with learning new classes over time. Under this setting, a global model is learned through a sequence oftasks. Following the standard assumption in continual learning, the sets of categoriesseen across distincttasks are mutually exclusive. As this is done in a federated setup, each task is learned throughindependent rounds by randomly sampling a set ofclientsC= {,,, …,} in each round. In asetting, the total number of clients is kept very large to simulate real-world FL applications (like mobile devices). The server does not keep track of clients as new clients are visited in each round. Further, previously seen data cannot be accessed at any time during the training.

To simulate a real-world heterogeneous federated learning scenario, we use three configuration parameters to control the level of heterogeneity with increasing granularity:,, and. At the top level, the most common heterogeneity is varying local dataset sizes across the clients. A specific clientcan be exposed to a subset of the current task datasetas their local dataset, and the sizevaries from each other. We denote this as the. At a lower level,
the local datasetconsists of a subset of the categories from those in the current global task. Specifically, a global taskconsists of categories, wheredenotes the number of unique categories. In a given round, each clientsees data containingcategories. We denoteas thewhich is a value between 0 and 1. At the lowest level, each category can have a different amount of data. We followto also create a long-tail distribution for local data which is different for each client. This distribution is governed by an. If, each clientis allocated samples uniformly fromcategories. In summary, a smaller split ratio, a smaller category ratio, or a smaller imbalance ratioincreases heterogeneity thereby increasing the complexity of the task. Formalizing the setting in this manner enables us to methodically vary the parameters, thereby simulating a heterogeneous setting. This formalization unifies the setups previously employed separately across federated and continual learning landscapescombining their distinct characteristics.

As discussed before, combining client models in a heterogeneous setting causes the obtained model to forget global knowledge from the previous rounds as shown by. Also, training clients on locally-available datasets induces forgetting of global knowledge outside the local distributions.measures such performance drops induced by the data heterogeneity (non-IIDness) across clients.measures the drop in performance on old tasksafter learning a new task.

SECTION: Background: L2P
L2P (Learning to Prompt)is a continual learning method that learns a set of model embeddings (prompts) that can be dynamically inserted into a pretrained vision transformer. (ViT). Prompts hold task-specific information that instructs the model to solve the corresponding tasks. L2P maintains a prompt poolof size M, whereare prompt parameters withas the prompt length (chosen as a hyperparameter) andthe embedding dimension. Each prompthas an associated key. An input imageis converted into a visual queryby passing it through the frozen vision transformer encoder. Prompts are selected from the pool by measuring the cosine similarity between associated keys and the visual query. Topprompts that have the highest key-query similarity are chosen to be inserted into the transformer. Here, the ViT encoder is frozen and the prompts and keys are learned using two separate optimization losses.

SECTION: Method
In this section, we describe our novel approach called HePCo (eterogenousromptnsolidation) which tackles forgetting and heterogeneity using a data-free distillation strategy applied in the model’s latent space. Knowing the limitations of FedAvg under non-IID data partitioning, we propose to fine-tune the global model at server side by distilling knowledge from client models. However, unlike prior CFL works, we first propose to leverage the current state of artin continual learning. Such methods optimize learnable parameters that augment the input to a transformer model (prompt tuning) or its underlying attention mechanism (prefix tuning). These methods have been shown to obtain strong performance in traditional rehearsal-free continual learning settings. These prompting-based approachescan be thought of implicit mechanisms to isolate model parameters across tasks. Implementing a prompting scheme at the client level can thus prevent forgetting of past tasks while efficiently adapting to new tasks.

Despite these advantages, there is a key challenge in applying prompting to a federated setting: It is not obvious how the server shouldprompts learned by the individual clients on heterogeneous sources of data. Naively averaging the prompt weights is suboptimal (as we show in Sec.) and simply maintaining a growing prompt pool scales poorly with the number of clients. Our key novelty is therefore to propose a lightweight, applied to the latent-space of the model, which greatly mitigates intra-task and inter-task forgetting. Crucially, rather than averaging different client prompts, we perform distillation of these prompts in a data-free manner by. Importantly, the generation and distillation operations are computationally cheap as they are carried out in the latent space of the model. This design prioritizes privacy and efficiency, which are crucial for federated learning. In summary, with our method, communication costs between clients and the server are low, heterogeneous information is effectively aggregated, and the method achieves state-of-art performance. Below we detail our method and depict it in Fig..

SECTION: Client Side: Decomposed Prompting
While L2P is quite successful in protecting against forgetting, its performance is limited by certain design choices, as noted by. We therefore adapt L2P to side-step these issues, including for our baselines, resulting in better accuracy across the board. Specifically, rather than using discrete prompts obtained via a hard maximum function which restricts capacity and introduces an additional hyperparameter (, corresponding to top-prompts), we form our final promptpby taking sum of theweighted by their cosine scores. Such a prompt is inserted into a subset of the self attention layers of the ViT encoder. This subset of layers is determined by hand as in.

In the federated learning setting, each client hosts the aforementioned prompting scheme and learns the key and prompt matrices along with the classifier in an end-to-end fashion while keeping the ViT backbone frozen. After learning the local task, in our method, the clients transfer the key, prompt and classifier weights to the server. By sending only a limited number of parameters to the server, the overall communication load is significantly reduced compared to sharing the entire model. Our approach requires clients to share only the prompt and classifier weights. The server lacks knowledge of the exact locations for inserting the prompts, as this information is known only to the respective clients. To reconstruct the exact client model, the server would need to conduct an exhaustive search over the number of layers in the transformer and try various combinations to determine the precise insertion positions. This method represents a positive step towards enhancing client privacy compared to approaches that share complete model weights.

SECTION: Server Side: Latent Generation
At the end of each round, the server receivesprompt and classifier weights collected from the active clients.
Letindicate a weight parameter that encompasses key, prompt and classifier weights. In the first stage, we obtain the server model by averaging client weights as:. We call these as the.

Due to data heterogeneity, the local model weights diverge which degrades the performance of this aggregated model. We therefore propose to fine-tune the server model using data-free distillation in the latent space to prevent this degradation. We generate pseudo data in the latent space of thewhich is essentially the output space of the vision encoder.
The advantage of generating in this space is that it allows us to fine-tunethe classifier and the key-prompt weightswithout needing a forward-pass through the encoder. We use a lightweight feedforward neural network as our conditional generator with adimensional output. This generator takes as input a class label (from categories in current task) and a noise vector of dimensionsampled from the standard normal distribution. We encode the class label using an embedding layer and concatenate the obtained class embedding with the noise vector to form the input of the generator. From the generator, we obtain a pseudo latent of dimensionconditioned on the class label as follows:

whereis the generated pseudo latent andis the noise vector. For effective knowledge distillation, pseudo data should conform to the latent space of the client models. We optimize for a classification loss which is a weighted sum of classification losses for each individual client, similar to. The total classification loss can be given as:

whereis the cross-entropy loss between the prediction of local modelcgiven latentand sampled class label.
Here,denotes the classifer (last layer).
However, optimizing for just the classification loss encourages the generator to produce pseudo latents which are easy to be classified and hence less effective for distillation. Our goal is to generate latents that create a discrepancy between the provisional server model and clients, thereby providing a learning opportunity for the server.
To promote the generation of such,
we maximize two disagreement losses (one for prompts and one for classifier)
between server and client models. As stated earlier, latents in this space can be forwarded through both prompting and classifier mechanisms. Hence, to measure the disagreement with respect to the classifier, we compute the Kullback-Leibler (KL) divergence between the predictions of the classifier corresponding to the provisional server model and each individual client. Next, to measure the disagreement with respect to the prompting module, we introduce a Mean-Squared Error (MSE) loss between the final prompts generated by the server and all clients. By training the generator to maximize these losses, we increase the potency of pseudo-latents for distillation in both parts of the network.

wheredenotes the softmax function anddenotes the prompting mechanism described in. We train the generator by optimizing for these losses jointly as:

The trained generator is then used to perform data-free knowledge distillation which helps combat intra-task forgetting and allows the server model to achieve a high accuracy on the global data distribution of the current task. However, as the generator is trained to generate pseudo-data corresponding to the current task distribution only, a model fine-tuned with this pseudo-data suffers from inter-task forgetting as shown in our ablation experiments in Section. To prevent this, we train a separate copy of the generator () to generate latents corresponding to the previously seen tasks. At the server side, we assume access to the key, prompt and classifier weights corresponding to the previous task’s global model which is the fine-tuned global model after theround of the task. Similar to above, we optimize for the classification and disagreement losses jointly. Here the classification lossis computed for the previous task server model andandare computed between this and the provisional server model.
We empirically show that using pseudo-data corresponding to past tasks for knowledge distillation helps mitigate inter-task forgetting to a great extent.

SECTION: Server Side: Latent Space Knowledge Distillation
Once the generator is trained to generate pseudo-latents corresponding to the current and previous tasks, we use it to efficiently fine-tune the provisional server model.
We use the pseudo-latents obtained from the generator to fine-tune both the classifier head and key-prompt weights (KandP) without requiring a forward pass through the full model.
We use the key, prompt and classifier weights corresponding to the current round client models and the last-task server model to fine-tune the server model. As it operates in a low dimensional latent space and updates a small subset of parameters, this distillation process is much cheaper in terms of computation compared to training the entire model. Also, this design does not require clients to share entire models with the server which reduces the client-server communication costs to a great extent and improves privacy of the client model. While we introduce additional server overhead, it is important to note that, in the context of CFL, clients are typically edge devices with limited computing power, while servers have ample computational resources. We prioritize client-level efficiency while making efficient use of the server’s resources. In Section, we compute this
overhead and show that it is competitive to that incurred by existing state-of-the-art (SOTA) methods.

To perform knowledge distillation, we first generate a batch of pseudo-data from the generators corresponding to the current round and previous task. We mix the current and previous task batches to form a single composite batch according to a hyperparameter namedwhich determines the size of the previous task batch relative to the current round batch. We use this composite batch of pseudo-latents to fine-tune the key-prompt weights and the classifier weights separately.

To fine-tune the key-prompt weights, we first obtain distillation targets in the form of final promptspby passing the pseudo latents through the prompting mechanism of the teacher models (clients and previous-task server model). Notably, we do not require full models to generate these targets, as having only the key-prompt and classifier weights is sufficient. Now, to fine-tune the key-prompt weights of server model, we optimize for the Mean Squared Error (MSE) loss between the final prompts predicted by the provisional server model and each individual teacher model (clients and previous-task server).

wheredenotes the MSE loss between clientand the provisional server model anddenotes the MSE loss between the provisional model and the previous task server model. Further,is an indicator variable which is set to 1 ifwas seen in previous tasks and 0 if present in current task.Next, we fine-tune the classifier layer of the provisional server model. As discussed in Section, pseudo-latents were obtained from the generator by conditioning on randomly sampled class labels. The composite batch of pseudo-latents used in the previous step is employed here again as input to the classifier, and the cross-entropy loss is computed between the predictions of the provisional server and the class labels upon which the pseudo-latents were conditioned. This approach allows us to fine-tune the classifier weights using the same batch of pseudo-latents used to fine-tune the key-prompt weights. Operating in the latent space allows us to efficiently fine-tune the key-prompt and classifier modules without requiring forward passes through the entire model. Our ablation studies in Sectionhighlight the efficacy of this approach.

SECTION: Experiments
We use the ViT-B/16 backbonepretrained on Imagenet-1Kas the encoder for our method and all baselines. We use a prompt pool size () of 100 and a prompt length () of 20 with dimension () being 768 and insert prompts into 1-5 Multi-head Self Attention (MSA) layers of the ViT encoder following the standard practiceand perform prefix-tuning as done in, by prepending prompts to the keys and values of the MSA layers. The classifier () is a fully-connected layer with input dimensionand output dimension equal to the number of classes. We implement the generatorusing a three layer fully-connected network and train it for 100 epochs. We encode the class label using an embedding matrix and concatenate the class embedding to the sampled noise vector before feeding into the generator.

We conduct our experiments on three image classification datasets. First, we adapt CIFAR-100to our formulation as it is a commonly used benchmark in CFL. Additionally, we evaluate our methods on the larger-scale ImageNet-Rand DomainNetwhich have been used in recent continual learning worksbut haven’t been explored in a continual federated learning setting. These datasets capture real-world distribution shifts that can be challenging for models pre-trained on ImageNet to generalize to. The total number of classes for CIFAR-100, ImageNet-R and DomainNet are 100, 200 and 345 respectively. We divide these datasets into 10-task (CIFAR-100, ImageNet-R) and 5-task (DomainNet) benchmarks. The 10-task setups contain a longer task sequence with small number of classes per task whereas the 5-task setup has a shorter task sequence with more classes per task. CIFAR and ImageNet have 10 and 20 classes, while DomainNet has 69 classes per task. Following, we use 20% of the training set as our validation dataset to determine hyperparameters for our approach and all competing baselines.

We learn each task throughcommunication rounds by selectingstateless clients per round. Thus, we have 100 total rounds for a 10-task setup and 50 for a 5-task setup. For all experiments reported in Tables-, we use a category ratiowhich means that if a task contains 10 categories, each active client is randomly assigned 6 of these categories. We analyze the affect of different category ratios in Sec.. Overall, HePCo outperforms existing methods across all category ratios, particularly excelling in scenarios with smaller category ratios, which signify higher heterogeneity and, consequently, a higher level of task complexity. Further, we use a split ratiowhich allows a client to be assigned 10% of the images corresponding to the subset of categories. We train local models for 10 epochs per round.

.

We evaluate all methods using the standard continual learning metrics of (1) final average accuracywhich is the accuracy averaged over alltasks after learning thetask and (2) average forgettingwhich measures the drop in performance on previous tasks after learning a new task averaged over alltasks. As noted by,is the more informative metric as it encompasses both forgetting and plasticity (new task performance). Our approach provides parameter efficiency which is reflected by reduced communication and local computation costs; albeit at the expense of a small overhead at the server. We quantify this communication efficiency by specifying the number of parameters shared by our approach relative to the original model size in.

We compare our method against existing state-of-the-art approaches: TARGET, CFedand Fed-CPrompt as well as two strong baselines that we introduce. TARGET and CFeD mitigate forgetting by replaying old task data through generative methods or by assuming access to surrogate datasets. Fed-CPrompt uses a prompting-based approach similar to ours but introduces a contrastive loss at the client side. This additional loss aims to mitigate forgetting by alleviating heterogeneity across clients and tasks. At the server side, Fed-CPrompt aggregates clients using the standard FedAvg algorithm. In contrast, our approach does not introduce any additional computation at the client’s end. Instead, we focus on altering the aggregation procedure at the server to combat forgetting. For fair comparison, we adapt Fed-CPrompt to our experimental setup and use the same Vision Transformer (ViT) architecture as in our method. Similarly for all other methods, we adapt their implementations to use the same ViT backbone with proper modifications. We tune hyperparameters for our proposed method and all compared baselines. We observe that the FedLwF methodused in prior CFL work
performs poorly owing to the complexity of our setting. As the heterogeneity across clients increases, coupled with an increase in the length of the global task sequence, the performance of FedLwF deteriorates catastrophically. We instead adapt LwF.MC with sigmoid binary cross-entropy loss as described inwhich is a strong continual learning baseline to our setting. We call this method FedLwF.MC which achieves a much better performance than its vanilla counterpart. Additionally, we introduce a simple yet strong prompting-based methods which we call FedAvg-Prompt. FedAvg-Prompt differs from our method in the model aggregation part at the server side where the clients are simply averaged to obtain the server model. Additionally for completeness, we report the performance of FedAvg-FT where the entire client models are sequentially finetuned on new task data (as opposed to learning only prompts and classifier)
and aggregated using FedAvg. Finally, we report the performance of ourscheme in a centralized, traditional continual learning setting. This can be thought of as an upper bound performance for all prompt-based methods included here.

SECTION: Main Results
The results presented in Tablesanddemonstrate the dominant performance of our method in terms of average accuracy and forgetting across across all datasets and setups. The gains achieved by our method are more pronounced in the ImageNet-R setup which has longer task sequences and offer a significant shift from the pretrained distribution. Our approach achieves absolute improvements of up to 7% in average accuracy compared to the best baseline. All baselines that fine-tune the entire model are seen to struggle with longer sequences (CIFAR, Imagenet-R), showing significant forgetting. Under the class-balanced setting of Table, our approach achieves absolute improvements of more thanon ImageNet-R in average accuracy compared to TARGET, which is the current SOTA. For the class-imbalanced settings in Table, our approach outperforms the competition bymargins. The notable performance drops observed across all methods highlight the complexity of this setting. Most importantly, HePCo achieves these solid results while enjoying low communication costs and without introducing any additional costs at the client-side. Furthermore, our approach faithfully aligns with the principles of Federated Learning (FL) by not assuming access to any storage, be it surrogate datasets or generated images, in contrast to methods like CFed, TARGETand GLFC.

SECTION: Additional Analysis
We perform ablations experiments on CIFAR-100 in thesetting from Table.

By removing the previous task server model from the distillation and generation steps, we highlight its importance in alleviating forgetting. By ablating this component, we observe a significant drop in performance indicated by a rise in forgetting () and a drop in average accuracy (). The underlying intuition is that without the replay of past task data, the method strongly prioritizes learning of the current task leading to a loss of knowledge from previously seen tasks. In other words, using past task latents for replay mitigates inter-task forgetting.

To demonstrate the effectiveness of disagreement losses in generation, we set the lambda coefficientsandto zero and observe adrop in accuracy.
As discussed before, the intuition here is that in absence of the disagreement losses, the generator is prone to generate easily discriminable examples that lead to low classification loss but are less effective in distillation. To further highlight the importance of the individual losses, i.eand, we individually ablate them and observe performance drops.

Our approach uses pseudo-latents to fine-tune the key-prompt and classifier weights of the server model. In this experiment, we ablate the decision of fine-tuning the prompt components and the classifier separately and observe a decline in accuracy in both cases. The drop in performance is more pronounced when we do not perform distillation for the classifier. This experiment highlights our decision to fine-tune both prompt components and classifiers by operating in the latent space.

Figureshows the performance of all methods for different values of. We observe that HePCo consistently outperforms competing methods without requiring any hyperparameter or design changes. The performance gap between HePCo and the competing methods widens with decreasing category ratio, indicating its effectiveness in high heterogeneity settings.

Our method introduces additional parameters forming the prompting mechanism. The additional parameters amount to ~9.4% of the original size of the ViT encoder. Our method only needs to communicate the learnable parameters in the model which are the classifier and key-prompt components amounting to ~9.5% of the original model size. Methods that finetune the entire model need to learn and communicate all parameters in the encoder and classifier. Hence, our approach requires only 9.5% of the communication costs compared to all other methods that share complete models. Furthermore, the current state-of-the-art methods like CFed and TARGET require communicating a dataset of images (obtained from the surrogate dataset or a generative mechanism) after every round or task which significantly increases the communication overhead in addition to sharing complete models! For a ViT-B/16 architecture, sharing a complete model amounts to330.3of information. In addition to complete model weights, TARGET sends a buffer of 8k synthesized image-level data from server to clients to perform distillation leading to a communication volume of714.7. In contrast, our approach, only sends31.37of data in each client-server exchange. We report this information shared (in) during each client-server communication in Table.

Our method does not require any extra computation at the client side but introduces an overhead at the server side. This overhead includes the time required to train the generators and perform knowledge distillation. To quantify this overhead, we conducted benchmarking using 2 NVIDIA TITAN RTX GPUs in a 5 client setup, as described in the experiments section. We report results in Tablein terms of overhead time in seconds (s) per round. Our method adds an extra 220 seconds of computational time at the server side per round, in contrast to the 98 seconds introduced by CFed and the 128 seconds incurred by TARGET. It is crucial to emphasize that our method does not impose any additional overhead on the client side, unlike CFed and TARGET which incur 68 seconds and 62 seconds per client respectively. In those methods, the client is responsible for learning the current task as well as distilling knowledge from past tasks. By transferring the computation load from the client to the server, we prioritize client-level efficiency. In most practical federated learning scenarios, edge devices have limited computational capacity compared to the server. Our approach prioritizes client-level efficiency, even if it entails a slight trade-off in server-level efficiency.

As our method operates in a stateless FL setup, we do not require clients to maintain any state information or additional storage. Our approach requires the server model to store the classifier and prompt components corresponding to the last task model which is used in distillation resulting into a storage cost equal to ~9.5% of the base encoder model size. Other baselinesincur extra storage costs at the client side equal to the size of entire encoder and classifier i.e ~86M parameters. Additionally, CFed and TARGET incur costs equivalent to storing an entire image dataset at both server and individual client levels. We report the storage costs incurred by these methods (in) at both server and client side in Table. The storage cost does not include the space needed to store the current round model itself. For CFed, we use the Caltech-256as the surrogate dataset as prescribed in. The storage overhead in Tableaccounts for storing this dataset at both client and server sides.

SECTION: Conclusion
In conclusion, we propose HePCo (Heterogeneous Prompt Consolidation) for continual federated learning. We formalize the setting and provide a methodical approach to simulate real-world conditions combining perspectives from continual and federated landscapes. Our method harnesses the prompt learning capabilities of foundation models to facilitate an efficient distillation framework for consolidating heterogeneous clients. By generating pseudo-data in a low-dimensional latent space, our approach enables parameter-efficient and data-free distillation of information from clients to server. We demonstrate the superior performance of our method compared to existing state-of-the-art methods through a series of experiments that emulate challenging real-world scenarios. By requiring clients to share parts of their models, we significantly reduce communication costs and enhance privacy. Importantly, our approach does not impose any additional overheads on the client side, making it highly valuable for real-world deployment.

SECTION: Discussion
It is worth noting that prompting-based methods are still relatively new and not extensively studied, making the explainability of these prompts challenging. Therefore, future work should focus on testing the robustness of these methods in diverse setups to ensure their effectiveness in different scenarios.
Considering the typical asymmetry in the availability of computational resources across servers and clients, our approach prioritizes client-level efficiency. Yet, the computation overhead introduced at the server, may be an issue for some use-cases. Although the generation and distillation procedures are relatively lightweight, they still rely on availability of server-side compute resources, which may not be universally accessible in all scenarios. Additionally, our approach necessitates clients to use pretrained vision transformers, leaving open the question of how this framework can be extended to accommodate other architectures. These are interesting avenues for future research.

The machine learning community is increasingly leaning towards the adoption of large-scale models for various applications. However, updating these models with new data poses a significant challenge. Retraining models from scratch each time new data arrives is computationally expensive and can have substantial financialand environmentalimplications. Our approach offers a solution by enabling incremental learning on new data without the need for complete model retraining. Additionally, our use of prompting techniques allows for significant reductions in communication and local computation costs while enhancing privacy, which is especially critical for on-device edge computing applications.

SECTION: Acknowledgements
This material is based upon work supported by the National Science Foundation under Grant No. 2239292

SECTION: References
SECTION: Appendix
SECTION: Algorithm
To better illustrate our proposed method, we present a whole picture of the method in Algorithm. The algorithm describes our complete procedure for a global task, where.

SECTION: Experimental Details
For fair comparison, we use the ViT-B/16 backbone pretrained on Imagenet-1K as the encoder for all methods. We implement our methods in PyTorch and use the PyTorch Image Models libraryto obtain pretrained checkpoints. We use 2 NVIDIA A40 GPUs for all experiments. For each result reported in this paper, we calculate the mean and standard deviation over separate runs.

For all methods, we use the Adamoptimizer with0.9 and0.999. We resize images toand normalize to [0,1].

Following DualPrompt, we use 20% of the training dataset as our validation data and conduct a hyperparameter search. We tune hyperparameters for both our approach and all competing baselines.
We use a batch size of 64 for both local and server-side training, determined after searching over values of 16, 32, 64, 128. For our method and the prompting-based baselines, we use a learning rate of 1e-3, while for baselines that tune the entire model (FedAvg, FedLwF.MC), we use 5e-5. We search for learning rates among {}. Our method employs a three-layer fully-connected network as the generator. We encode class labels using an embedding matrix of length 64 and concatenate it with a 64-dimensional noise vector. This dimension was chosen after searching over 32, 64, 128, 256. While our approach is robust to various values, 64 performs best. The generator architecture has input sizes of [128, 256, 1024] per layer, with an output size of 768 which is the dimension of the visual query. We opt for a three-layer architecture to maintain lightweight generation and training. We train the generator for 100 epochs using a batch size of 64 and a learning rate ofusing the Adam optimizer. After a logarithmic scale search, we find a learning rate in the range [5e-5, 1e-4] to provide optimal results.

We fine-tune the server model using a learning rate offor 200 epochs. Through grid search, we find that the model is robust to various learning rate and epoch combinations, with these values providing the best average accuracy on the validation set. We use aof 0.5 for our method, which means we mix 50 pseudo-latents corresponding to previous tasks for every 100 pseudo-latents corresponding to the current task. We conduct a search over values like [0, 0.125, 0.25, 0.375, 0.5, 0.625, 0.75, 0.875, 1] and find 0.5 to result into the best average accuracy. We observe atrade-off controlled by this hyperparameter with larger values leading to lower forgetting () but lower current task accuracies () and smaller values yielding the opposite effect. Through a hyperparameter search within [0, 5] at 0.1 increments, we chooseandvalues to be 1 and 0.1 respectively. We find these values to work best across all datasets reported in the paper. Overall,values between [0.6, 3] yield similarly good results, with too low (close to 0) and too high (close to 5) values leading to poor performance.