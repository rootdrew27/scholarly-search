SECTION: Enhancing Mathematical Reasoning in LLMs with Background Operators

We propose utilizing background operators for mathematical reasoning in large language models (LLMs). To achieve this, we define a set of fundamental mathematical predicates as the basic building blocks. For each mathematical problem, we develop a Prolog solution that includes problem-specific predicates and intermediate predicates derived from these background operators, ensuring that each solution adheres to the defined operator set. We introduce the MATH-Prolog corpus, which is derived from the counting and probability categories of the MATH corpus. For efficient data augmentation, we apply K-fold cross-validated self-training. This method incrementally generates new Prolog solutions for each fold, incorporating those verified as correct into the training set throughout the model training process. Our experimental results demonstrate that 5-fold cross-validated self-training effectively identifies new, accurate Prolog solutions, achieving an accuracy of 84.6% on the cross-validated set, and 84.8% on the test set during fine-tuning the Meta-Llama-3.1-8B-Instruct model. This approach successfully uncovers new solutions with fully computable inference steps for previously unseen problems. Additionally, incorporating the background mathematical predicates into the prompt enhances solution coverage.

Enhancing Mathematical Reasoning in LLMs with Background Operators

Jiajun Chen∗and Yik-Cheung Tam∗Shanghai Frontiers Science Center of Artificial Intelligence and Deep LearningNew York University Shanghai{jc11815,yt2267}@nyu.edu

SECTION: 1Introduction

Recently, large language models have shown success in tackling mathematical problems such as chain of thoughtWei et al. (2022); Zhihong Shao (2024), tree of thoughtYao et al. (2023), program of thoughtChen et al. (2023), and graph of thoughtBesta et al. (2023). For example, chain of thought has an advantage that each reasoning step is described verbally which is good for human understanding. However, verbal reasoning steps are not computable and therefore hard for machines to verify for flaws. Generating computer programs with Python codes such as Sympy for mathematical reasoning is promisingGou et al. (2023). However, this approach has limitation for logical reasoning since the programming language is procedural in nature. Using Prolog language to describe a mathematical problem would be more powerful. Solving a mathematical problem is converted into searching for a final answer such that problem-specific constraints are satisfied.

In this paper, we cast solving mathematical reasoning problems via Prolog which is realized as a set of predicates. A search process facilitated by an external interpreter, e.g. Prolog, is performed to find the feasible values of the predicates serving as constraints. Each constraint in Prolog must be satisfied to arrive at a final answer. Many questions in the MATH corpus can be casted as search problems satisfying a set of constraints. For example, for a problem “If two numbers are randomly chosen without replacement from, what is the probability their sum is greater than their product?”, the constraints are: 1) The two numbersandbelongs to the set; 2); 3). We want a language model to write Prolog code that declares the constraints and search for a set of feasible solutionsto compute the final probability.

To enable a large language model to generate Prolog code, our proposal is to curate MATH-Prolog, a corpus for solving mathematical reasoning problems based on Prolog. Then, we fine-tune a large language model on the MATH-Prolog to learn how to generate Prolog codes. To enforce standardized solutions, we curate a set of pre-defined background mathematical predicates, e.g.andas standard predicates so that all Prolog solutions must be constructed based on these background operators.
As another viewpoint, two predicatestakes two input symbolsandand outputswhich is an input to predicateresulting in an output. Therefore, a Prolog solution is an instance of a directed computation graph as illustrated in Figure1, where the input nodes define the problem-specific predicates inferred from the problem statement, and the intermediate nodes are the background operators which are connected with each other so that the final resultis arrived. A pretrained large language model is finetuned to generate a computation graph to solve a mathematical reasoning problem.

Our research has the following contributions: 1) We curate and open-source the MATH-Prolog dataset focusing on counting and probability to enable large language models to generate declarative Prolog codes to solve competition-level mathematical reasoning problems. 2) We devise a cross-validated self-training algorithm that generates and explores Prolog code solutions during model finetuning on unseen problems. 3) Experimental results show that cross-validated self-training algorithm generates codes that lead to 84.8% accuracy on the test set.

SECTION: 2Corpus Design

We design the MATH-Prolog based on the MATH corpus, a benchmark for competition-level mathematical reasoning problems. We choose counting and probability in MATH for corpus curation consisting of 771 problems on the training set. For each problem in the training set, we curate a Prolog solution which consists of two parts: 1) Problem-specific predicates which are directly inferred from the problem statement; 2) The Solve predicate composing the intermediate predicates as constraints required to solve the given problem.
In total, we have 54 background operators manually crafted from high-school textbooks, and all Prolog solutions are standardized to this operator set. Some background operators such asare included since some training questions require the palindrome constraint.
With the help of GPT-4 and few-shot prompting technique, we handcraft Prolog solutions of a subset of the problems in the training set, and use GPT-4 to generate Prolog solutions on the rest of training problems followingYang et al. (2024). Since the MATH corpus is competition level, many of the GPT-4 generated solutions actually do not evaluate to correct final answers. Therefore, we manually check the generated solutions and make code adjustments so that the evaluated Prolog codes result in correct reference answers. This costs one manual labor for about one month to curate the high-quality and 100% accurate MATH-Prolog training corpus for research. Due to limited manual resources, we did not manually curate the 474 problems on the test set. Inspired byWang et al. (2024), we employ a self-training algorithm to automatically generate Prolog solutions for the test set.

SECTION: 2.1Findall Predicate

Given that we have a finite set of problem-specific predicates and a finite set of predicates, the correct solution can be derived systematically by assembling the variables and predicates. From a graphical perspective, these variables and predicates can be viewed as nodes, with directed edges representing the flow of inputs and outputs within the solution. However, exhaustively traversing all possible combinations of variables and predicates is computationally expensive. To address this challenge, we employ thepredicate to represent the constraints derived from the problem statements, effectively narrowing down the search space.is a standard predicate defined in SWI-Prolog.
By leveraging thepredicate, we can express the relevant constraints in a compact manner.
Figure2gives an example of the usage ofpredicate that returns a list of valid pairssatisfying the constraints required by the problem.
Overall, about 26% of the Prolog solutions in the training set involves thepredicate. Usingdeparts from the chain of thought solution which is mainly verbal natural language reasoning without invoking constraint search.

SECTION: 2.2Solution Diversity

For mathematical reasoning problem, there may exists multiple solutions with variations. For example, a problem “How many ways are there to put 5 balls in 2 boxes if the balls are not distinguishable but the boxes are?”, one Prolog solution employsand another solution uses.
Our preliminary experiment shows that even we use a finetuned large language model to generate solutions on training questions, the generated solutions can be different from the original training references.
We believe that improving solution diversity will help improve the performance of a large language model especially for data augmentation.
Therefore, we devise a cross-validated self-training procedure to sample multiple different solutions during model finetuning. Algorithm1shows the self-training procedure that requires a training and a generation set without overlap. For K-fold training, we set the k-th fold as a generation set and the rest as a training set.

SECTION: 3Experiment

SECTION: 3.1Setup

We used the MATH corpusHendrycks et al. (2021)and chose the counting and probability domain described in Section2to construct the MATH-Prolog corpus.
Initially, we employed GPT-4 to automatically generate Prolog codes with background operators for domain-specific questions, followed by manual correction to ensure accuracy. Prior to implementing the self-training procedure, we applied a filtering process to exclude training instances that could not be resolved using the provided numerical data or domain-specific commonsense knowledge. This resulted in a final set of 625 well-constructed Prolog samples, which we denote as. To implement the self-training procedure described in2.2, we partitioned the dataset into five folds, denoted. The input prompting format follows the instruction prompts used in Stanford AlpacaTaori et al. (2023). We removed samples exceeding 2700 tokens to ensure that a training run can be fit into a 4090 GPU. All background operators were included in an input prompt to measure its effect. (see AppendixA.3for details).

We conducted all experiments using the LLaMA-3.1 8B modelet. al. (2024). To reduce memory consumption while preserving performance, we employed 8-bit quantization and LoRAHu et al. (2022)for efficient model training.
We divided the training set into five folds and applied self-training to sample two correct samples per question on the heldout fold.
We experimented self-training using the test set as a generation set to discover Prolog solutions. See AppendixA.6for further training details.

We report accuracy (correct solution coverage) as a metric:

whererefers to the Swi-Prolog interpreter
, andis set to 5 folds. The metric measures the ability of a large language model to discover Prolog solutions in the unseen dataset.

SECTION: 3.2Results

As shown in Figure3, the coverage of correct generation achievesfor the 5-fold cross-validation experiment. When we used the test set in self-training, test set accuracy was 75.7% with background operators listed in the input context compared to 71.1% without background operators after running for 3900 self-training steps. This showed that the introduction of background operators in the input context helped, and agreed with what were observed in the cross-validated self-training. When we used the derived test solutions from the first-round self-training and augmented them into the training set and rerun self-training again, the test set accuracy was increased to 84.8%. We observed that some generated solutions exploits the compositional and decompositional nature of predicates (SeeA.1for examples). Their equivalence
represent different problem-solving strategies giving diverse and high-quality solutions.

As shown in Figure3, using background operators in an input prompt exhibits a more efficient learning trajectory. By inspection, the model first tackled easy questions that involves one operator in early training steps, while complicated questions were tackled that involves multiple predicates at later training steps. For solution discovery sake, more training steps were preferable.

SECTION: 4Related Work

The Chain-of-Thought (COT) prompting approachWei et al. (2022)was among the first attempts toward generating logical, verbal step-by-step reasoning. Several advanced techniques have been developed to enhance the reasoning capabilitiesZhou et al. (2023); Zhu et al. (2023); Huang et al. (2022); Liang et al. (2023). However, most of these approaches rely on natural language reasoning, which faces limitations in identifying flaws in verbal reasonings. To overcome this bottleneck, a notable attempt involves utilizing trees and graphs for reasoning, as graph-based structuresZhang et al. (2020)have demonstrated potential in solving mathematical word problemsKipf and Welling (2016).Yu et al. (2023); Jiang et al. (2024)employed forward and backward data augmentation via variable and final answer masking on COT solutions for data augmentation.
Our work extends the concept of graphical reasoning using predefined predicates to generate diverse and computable solutions.

External tools can be integrated into large language models to effectively enhance both the reasoning capabilities and interpretability of LLMsCobbe et al. (2021); Mishra et al. (2023); Gou et al. (2023); Gao et al. (2023); Shao et al. (2023); Chen et al. (2023); Trinh et al. (2024). Prolog, as a symbolic declarative language, maximizes the advantages for symbolic reasoning. It not only improves the logical coherence of natural language generationVakharia et al. (2024)but also strengthens the model’s ability in arithmetic reasoningYang et al. (2024); Tan et al. (2024); Borazjanizadeh and
Piantadosi (2024). Our work employs standardized background operators for solving competition-level mathematical problems.

SECTION: 5Conclusion

Our work introduces MATH-Prolog, a dataset specifically designed for tackling competition-level mathematical problems using declarative logic programming. With predefined predicates, we create a structured and interpretable framework for solving mathematical problems. Our cross-validated self-training algorithm effectively generates full solutions with computable intermediate steps on unseen questions. Our experiments demonstrates its efficiency and accuracy, showcasing its potential for broad application in systematic mathematical problem-solving.

SECTION: 6Limitation

While our approach of incorporating predefined background operators provides a productive and interpretable method to define standardized solutions, its applicability has not yet been tested in other domains. Currently, we have only developed Prolog codes for the counting and probability domain, which may limit the scope of our experiments. However, we believe that the method can be generalized to other domains within the MATH corpus. Additionally, we have observed that large language models occasionally generate Prolog code with syntax errors, which reduces the accuracy of code generation. Future work would involve expanding this approach to other domains of the MATH corpus. Moreover, constrained decodingLu et al. (2022); Geng et al. (2023)to eliminate syntax errors in generated code deserves further exploration. Lastly, our experiments have been limited to LLaMA-3.1 8B model. Future research would explore the effectiveness of our method to other models.

SECTION: References

SECTION: Appendix AAppendix

SECTION: A.1Diversity Examples

In Figure4, consider the question: "Ben rolls four fair 20-sided dice, each with faces numbered from 1 to 20. What is the probability that exactly two of the dice show an even number?" Initially, we define the predicateto represent the likelihood of a specific outcome for a die. However, after applying the cross-validated self-training procedure, the model identifies an alternative interpretation of the problem, where the probability of observing either an even or odd number is both. This result highlights the model’s ability to extract implicit information from the problem statement, which can deviate from our original understanding.

When generating a solution for a question such as "Ryan has 3 red lava lamps and 3 blue lava lamps. He arranges them randomly in a row on a shelf, and then randomly turns 3 of them on. What is the probability that the leftmost lamp is blue and off, and the rightmost lamp is red and on?", the model discovers an alternative solution strategy. Graphically, the new solution involves a decomposition of the combination operator. If we denote the combination operator as, the permutation operator as, the factorial operator as, and the division operator as, then. This type of decomposition offers a structured and robust approach for augmenting new diverse samples.

SECTION: A.2background operators

Below, we provide a detailed overview of the predicates used in the counting and probability domain of the MATH-Prolog corpus. We describe the input-output behavior of each operator, as well as its potential for decomposition, which is leveraged in the data augmentation process.

SECTION: A.3Instruction Prompt

Below are the instruction prompts we use for different settings:

SECTION: A.4Training Samples

SECTION: A.5Error Analysis

Below, we present some common errors generated by the LLaMA-3.1 8B model during the self-training procedure, to better understand the limitations and challenges in generating Prolog programs. These examples were selected from experiments that included predicates in the input prompts. The majority of problematic solutions exhibit syntax errors, while others fail to correctly identify and apply the constraints specified in the problem statements. A few typical syntax errors are listed below, with the erroneous lines highlighted in bold. Detailed explanations of each error are provided in the accompanying comments.

SECTION: A.6Training Details and Computational Budget

We applied LoRA to fine-tune the query and value weight matrices within the transformer blocks. After exploring various hyperparameter configurations, we selected a LoRA rank and scaling factor ofto achieve an optimal trade-off between performance and computational efficiency. To collect diverse Prolog solutions, we ran the self-training experiments for 100 epochs with batch size of 16, and learning rate of. For a self-training run, we used 1 NVIDIA RTX 4090 GPU to finetune Meta-Llama-3.1-8B-Instruct model on each fold, taking around 2 days to finish. The sampling hyperparameters include temperature scaling of, top-k sampling of, and nucleus sampling of.