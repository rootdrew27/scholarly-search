SECTION: PGN: The RNN’s New Successor is Effectivefor Long-Range Time Series Forecasting

Due to the recurrent structure of RNN, the long information propagation path poses limitations in capturing long-term dependencies, gradient explosion/vanishing issues, and inefficient sequential execution. Based on this, we propose a novel paradigm called Parallel Gated Network (PGN) as the new successor to RNN. PGN directly captures information from previous time steps through the designed Historical Information Extraction (HIE) layer and leverages gated mechanisms to select and fuse it with the current time step information. This reduces the information propagation path to, effectively addressing the limitations of RNN. To enhance PGN’s performance in long-range time series forecasting tasks, we propose a novel temporal modeling framework called Temporal PGN (TPGN). TPGN incorporates two branches to comprehensively capture the semantic information of time series. One branch utilizes PGN to capture long-term periodic patterns while preserving their local characteristics. The other branch employs patches to capture short-term information and aggregate the global representation of the series. TPGN achieves a theoretical complexity of, ensuring efficiency in its operations. Experimental results on five benchmark datasets demonstrate the state-of-the-art (SOTA) performance and high efficiency of TPGN, further confirming the effectiveness of PGN as the new successor to RNN in long-range time series forecasting. The code is available in this repository:https://github.com/Water2sea/TPGN.

SECTION: 1Introduction

Under the premise of accurate time series forecasting, long-range forecasting tasks offer an advantage over short-range forecasting tasks as they provide more comprehensive information for individuals and organizations to thoroughly assess future changes and make well-informed decisions.
Due to its practical applicability across various fields (i.e., energy(Zhou et al.,2021), climate(Angryk et al.,2020), traffic(Yin and Shang,2016), etc), long-range forecasting has attracted significant attention from researchers in recent years.

Long-range time series forecasting tasks can be broadly classified into two categories.
One task is to utilize abundant inputs to forecast future outputs(Liu et al.,2022a;Jia et al.,2023), while another task is to predict longer-range futures with fewer historical inputs(Zhou et al.,2021,2022a;Wang et al.,2023).
Although existing studies have shown that ample historical inputs can introduce more information to improve prediction performance(Jia et al.,2023;Liu et al.,2024), considering factors such as the load capacity of training devices and data collection, the utilization of limited historical inputs to predict longer-range futures remains an important research topic.
Therefore, this paper sets the task goal as predicting longer outputs with fewer inputs.

In recent years, deep-learning-based methods have achieved remarkable success in time series forecasting (for further discussions, please refer to Section2and AppendixB).
These methods can be roughly categorized into four based paradigms:Transformers(Zhou et al.,2021;Wu et al.,2021;Liu et al.,2022a;Zhou et al.,2022a;Nie et al.,2023;Ni et al.,2023;Liu et al.,2024;Dai et al.,2024),CNNs(Wu et al.,2023;Wang et al.,2023;Luo and Wang,2024),MLPs(Zeng et al.,2023;Xu et al.,2024;Wang et al.,2024), andRNNs(Jia et al.,2023).
It is worth noting that RNNs have received relatively less attention over an extended period of time.This discrepancy is primarily attributed to the limitation of RNNs’ recurrent structure, which leads to the persistence of excessive long pathways for information propagation.

In fact, shorter information propagation paths lead to less information loss(Tishby and Zaslavsky,2015), better captured dependencies(Liu et al.,2022a), and lower training difficulty(Wang et al.,2023).
However, RNNs heavily rely on a sequential recurrent structure to transmit information, making it challenging for them to capture long-term dependencies and suffer from the issue of gradient vanishing/exploding(Pascanu et al.,2013).
Meanwhile, due to its sequential computation, even though RNNs have a theoretical complexity that is linear with respect to sequence length, their actual running speed can be even slower than thecomplexity of the Vanilla-Transformer(Vaswani et al.,2017).
Some RNN-based models(Hochreiter and Schmidhuber,1997;Chung et al.,2014)have tried to enhance performance by incorporating specialized gated mechanisms.
However, compared to the inherent limitations of the RNN structure, these improvements in information selection and fusion are merely a drop in the bucket.

Based on this motivation, this paper proposes a novel general paradigm calledParallelGatedNetwork (PGN)as the new successor to RNN.
PGN introduces a Historical Information Extraction (HIE) layer to replace the recurrent structure of RNN, and then further selects and fuses information through gated mechanisms, effectively reducing the information propagation paths to, as shown in Figure1(l).
This enables PGN to better capture long-term dependencies in input signals.
Additionally, since computations for each time step can be parallelized, PGN achieves significantly faster execution speed while maintaining the same theoretical complexity ofas RNN.

Despite the advantages of PGN in terms of efficiency and capturing long-term information, it cannot be directly applied to time series forecasting tasks for optimal performance.
This is because, based on 1D modeling, PGN struggles to capture periodic semantic information(Jia et al.,2023)effectively.
Fortunately, the idea of transforming data from 1D to 2D and modeling it(Wu et al.,2023;Jia et al.,2023;Dai et al.,2024), proves effective in addressing above limitation.
When employing 2D modeling for time series, information captured along rows reflects short-term changes, while information along columns represents long-term periodic patterns.
Due to the distinct characteristics of these two types of information, it is reasonable to model them separately.
Furthermore, considering that periodicity is present throughout the entire time series, both in the past and in the future, it is important to prioritize this consideration when modeling.

Based on these motivations, we propose a novel PGN-based temporal modeling framework calledTemporalParallelGatedNetwork (TPGN).
TPGN establishes two distinct branches to capture long-term and short-term information in the 2D input series.
To focus on modeling long-term information, we utilize PGN to model each column of the 2D inputs, preserving their respective local periodic characteristics.
Simultaneously, leveraging the advantages of patch(Nie et al.,2023)in capturing short-term changes, TPGN initially aggregates the short-term information into patches and subsequently merges them to obtain global information.

By integrating the information from both branches, TPGN achieves comprehensive semantic information capture for accurate predictions.
It also should be noted that other methods can substitute PGN and be used in the long-term information extraction branch of TPGN, undoubtedly enabling TPGN to be a general framework to model temporal dependencies.
Furthermore, TPGN maintains a efficient computational complexity of.
To better illustrate the advantages of TPGN, inspired by(Jia et al.,2023), we have provided a information propagation comparative diagram in Figure1and an analysis table in Table3.

The main contributions of this paper can be summarized as follows:

We propose a novel general paradigm called PGN as the new successor to RNN, as shown in Figure1(l). It reduces the information propagation path to, enabling better capture of long-term dependencies in input signals and addressing the limitations of RNNs.

We propose TPGN, a novel temporal modeling framework based on PGN, which comprehensively captures semantic information through two branches, as shown in Figure1(m). One branch utilizes PGN to capture long-term periodic patterns and preserve their local characteristics, while the other branch employs patches to capture short-term information and aggregates them to obtain a global representation of the series. Notably, TPGN can also accommodate other models, making it be a general temporal modeling framework.

In terms of efficiency, PGN maintains the same complexity ofas RNN. However, due to its parallelizable calculations, PGN achieves higher actual efficiency. On the other hand, TPGN, serving as a general temporal modeling framework, exhibits a favorable complexity of. For a more detailed comparison of complexities, please refer to Table3.

We conducted experiments on five benchmark datasets, and the results indicated that TPGN achieved an average MSE improvement of 12.35in various long-range time series forecasting tasks compared to the previous best-performing models. Furthermore, in comparison to the average performance of specific models across all tasks, TPGN achieved an average MSE improvement ranging from 14.08to 37.44. Additionally, experimental evaluations on computational complexity confirmed the efficiency of TPGN.

SECTION: 2Related Works

SECTION: 2.1Modeling Interaction Cross Temporal Dimension

The methods that focus on temporal modeling can be broadly categorized into four paradigms: RNN-, CNN-, MLP-, and Transformer-based.
The limitations of RNNs(Hochreiter and Schmidhuber,1997;Chung et al.,2014;Salinas et al.,2020)have been discussed in Section1. Despite some methods(Chang et al.,2017;Yu and Liu,2018;Jia et al.,2023)trying to alleviate these limitations, the recurrent structure still hinders their further development.
CNNs(Franceschi et al.,2019;Sen et al.,2019)offer advantages in efficiency and shorter information propagation paths, but primarily constrained by limited receptive fields(Wu et al.,2023), resulting in an increase in the information propagation path as the length of the processed signal increases.
Although some methods(Wang et al.,2023;Luo and Wang,2024)have increased the receptive field to address these issues, the 1D modeling approach makes it challenging for them to directly capture periodicity.
The advantages of MLP(Zeng et al.,2023)lie in its simple structure and high operational efficiency.
Some advanced models have further enhanced the performance of MLPs by applying them in the frequency domain(Xu et al.,2024)or introducing multi scales(Wang et al.,2024), which could lead to higher execution overhead.
Classic Transformer-based methods either struggle to capture semantic information(Wu et al.,2023;Nie et al.,2023)due to point-wise attention mechanisms(Vaswani et al.,2017;Zhou et al.,2021,2022a)or have high complexity(Wu et al.,2021;Liu et al.,2022a), limiting their ability.
Subsequently, this problem was effectively addressed by utilizing patches(Nie et al.,2023).
However, they still suffer from the 1D modeling issue mentioned earlier or the problem of limited receptive fields.
More detailed discussion and analysis can be found in AppendixB.

SECTION: 2.2Modeling Interaction Cross Variable Dimension

For handling variable dimensions, there are generally four categories: variable fusion processing, variable independent processing, modeling based on Transformers, and modeling based on Graph Neural Networks (GNNs).
Traditional fusion processing methods, due to the heterogeneity of multiple variables(Zhou et al.,2021), introduce excessive noise, resulting in worse performance compared to independent processing of variables(Nie et al.,2023).
However, by applying attention mechanisms and Graph Neural Networks (GNN) on the variable dimension to replace independent modeling of variables, it is possible to successfully capture the correlations and differences between variables, thereby significantly improving the performance of multivariate modeling.
Representative methods for modeling variable relationships based on Transformers include Crossformer(Zhang and Yan,2022)and iTransformer(Liu et al.,2024), while GNN-based representative methods include CrossGNN(Huang et al.,2023)and FourierGNN(Yi et al.,2023).
They provide excellent inspiration for analyzing and modeling multivariate time series.

SECTION: 3Methodology

In this section, we first introduce our proposed novel paradigm calledParallelGatedNetwork (PGN), and explain how it reduces the information propagation paths, overcomes the limitations of RNNs, and emerges as the new successor to RNNs.
Next, we present our newly designed temporal modeling framework calledTemporalPGN(TPGN), which incorporates two separate branches to comprehensively capture semantic information.
Finally, we provide a comprehensive complexity analysis to evaluate the computational efficiency of our methods.

SECTION: 3.1Parallel Gated Network (PGN)

Building upon the previous analysis, the limitation of RNNs lies in the excessively long information propagation paths of its recurrent structure, which directly leads to a series of issues, such as difficulty in capturing long-term dependencies (performance), low efficiency in sequential computations (efficiency), and gradient exploding/vanishing (training difficulty).
Indeed, some RNNs leverage specialized gated mechanisms, such as LSTM(Hochreiter and Schmidhuber,1997)and GRU(Chung et al.,2014), which do have advantages in information selection and fusion.
However, when faced with the disastrous limitation of RNNs, their advantages become insignificant.

Based on this, we propose a novel general paradigm called PGN as the new successor to RNNs.
PGN draws the advantages of RNNs while reducing information propagation paths to, thereby addressing the limitation of RNNs.
The information propagation illustration and structure of PGN are shown in Figure1(l) and Figure2(a), respectively.
On one hand, to enable PGN to capture information from all preceding time steps within short information propagation paths, we introduce a linearHistoricalInformationExtraction (HIE) layer to aggregate information from the entire history at each time step.
Importantly, at this stage, the computation of each time step of the signal is independent of others, allowing for effective parallel processing.
On the other hand, PGN leverages gated mechanisms to inherit the advantages of information selection and fusion.
It is important to emphasize that in PGN, we utilize only a single gate to simultaneously control the information selection and fusion in a parallel manner across all time steps of the sequence, resulting in reduced computational overhead.
When given an input signalof length, the computation process of PGN can be formalized as follows:

whererepresents the operation of filling the front of the processed signal along the length dimension with a zero-filled vector of size.is a linear layer with weight matricesand bias vectors. It aggregates all relevant historical information for each time step in parallel by sliding along the sequence length dimension, andrepresents the output of this operation.
The weight matricesand bias vectorsare utilized in the computations.andare the intermediate variables involved in the gated mechanism.
The symbolrepresents the element-wise product, while() andtanh() denote the sigmoid and tanh activation functions.represents the output of PGN.

SECTION: 3.2Temporal Parallel Gated Network (TPGN)

The specific objective of time series forecasting task is to predict the future series of lengthgiven a historical sequence of length.
As stated in Section1, PGN may not be effective in directly extracting periodic semantic information, which limits its application in time series forecasting tasks.
Inspired by(Wu et al.,2023;Jia et al.,2023;Dai et al.,2024), we transform the input series from 1D to 2D for modeling. To fully capture the short-term changes and long-term periodic patterns with different characteristics in the rows and columns of the 2D input, we introduce two branches to model them separately.
The information propagation diagram and overall structure of TPGN are shown in Figure1(m) and Figure2(b).

To enable TPGN to directly capture periodic semantic information, inspired by previous works(Wu et al.,2023;Jia et al.,2023;Dai et al.,2024), we reshape the series from 1D to 2D.
Notably, we do not need to introduce multiple scales of periods like in TimesNet(Wu et al.,2023)and PDF(Dai et al.,2024), as it would result in increased computational overhead.
Instead, we draw inspiration from WITRAN(Jia et al.,2023)and solely reset the sequence based on the natural scale of the time series.
In addition, to minimize the negative impact of data fluctuations on model training, inspired by(Liu et al.,2022b,2024), we have introduced a normalization layer along the temporal dimension.
When given an input sequenceand temporal external feature(andrepresent the number of variables and temporal external features), this module can be mathematically expressed as:

Here,represent the normalized series,represents the concat operation, and the hyperparametersshould be determined based on the characteristics of different datasets.
To combine each variable of the input series with the temporal feature, we need to expandby adding an extra dimension to match the shape of.
Additionally,needs to be expanded by adding a dimension and repeatedtimes to match the shape of.
Afterwards, we concatenate the results and reshape them according to the natural periodof series through, andrepresents the output of this module.andrepresent the number of rows and columns in the 2D input, respectively.

As TPGN focuses on modeling the temporal dimension, which is crucial for any variable in the time series. In the following discussions, we will focus on an example variableto provide a detailed explanation, whererepresents the input.
To better capture long- and short- term information while preserving their respective characteristics, we have designed two branches as illustrated in Figure1(m) and Figure2(b).

In the long-term information extraction branch, we directly capture the information using PGN.
On one hand, it effectively captures the long-term repetitive historical information for each time step.
On the other hand, through the gated mechanism, it selects and fuses the current and historical information at each time step, thereby preserving the long-term periodic characteristics to the maximum extent. Specifically, this branch can be formulated as follows:

whererepresents the input being passed through the PGN paradigm.
It is important to note that PGN operates along thedimension.
The advantage of this approach is it preserves the individual characteristics of each column, better serving forecasting.
The output is denoted as.
To facilitate the utilization of long-term information for prediction purposes, we aggregate the information from all rows in each column using a linear layer.
The output of this branch is denoted as.

In the short-term information extraction branch, considering the advantage of patch in aggregation short-term information, we first utilize a linear layer to aggregate the short-term information into patches. Then, another linear layer is used to further fuse the patches into the global information of the series. The specific process can be formulated as follows:

whereoperates along thedimension, andis its output.
Subsequently,further aggregates the patchesand obtains the global representationof the sequence.
Finally, to facilitate subsequent predictions, we repeatalong the first dimensiontimes to obtain a new representation with the same dimensionas the output of the long-term information extraction branch.

In this module, begin by concatenating the information representations derived from the two branches of TPGN.
The concatenated information encompasses the local long-term periodic characteristics observed across various columns in the 2D input series, along with the globally aggregated short-term information.
Subsequently, we take the previously aggregated representation with comprehensive semantic information and pass it through a linear layer to predict future values at different positions within the period.
The formulation of this module is as follows:

whererepresents the concat operation.
The output dimension afteris, wheremultiplied byequal forecasting series length.
Finally, the above output will be permuted and reshaped to 1D dimension byoperation, the result.

SECTION: 3.3Complexity Analysis

While PGN processes signals in the time dimension in parallel, each step still involves processing all its historical information.
Therefore, the theoretical complexity of this part is still linear with respect to the lengthof the signal being processed.
The complexity of the gated mechanism is independent of the signal length, so the complexity of PGN can be expressed as.
Noted that PGN has the same theoretical complexity as RNN, but due to the parallelized ability of PGN computations, it has much higher efficiency in practice compared to RNN.

Since TPGN has two separate branches, it is necessary to analyze them separately.

For the long-term information extraction branch, TPGN applies the PGN paradigm along the number of, the complexity of this step is indeed linear with respect to, denoted as.
The aggregation of all rows of information is accomplished through a linear layer, which still has a complexity proportional to. Therefore, the complexity of the long-term information extraction branch can be expressed as.

For the short-term information extraction branch, TPGN applies two linear layers.
The first linear layer compresses the time dimension fromto, while the second linear layer compresses the other time dimensionto, therefore, their complexities are respectivelyand.

Sincemultiplied byequals the input sequence length(), the complexitiesandare both equal to.
For the two branches of TPGN, the complexities are both.
Therefore, the complexity of TPGN is also.

SECTION: 4Experiments

To validate the performance of TPGN, we followed WITRAN(Jia et al.,2023)and conducted experiments on five real-world benchmark datasets that span across energy, traffic, and weather domains. More details about the datasets can be found in AppendixC.

We conducted a comprehensive comparison of eleven methods in our study.
These methods include one RNN-based method: WITRAN(Jia et al.,2023), three CNN-based methods: ModernTCN(Luo and Wang,2024), TimesNet(Wu et al.,2023), MICN(Wang et al.,2023), three MLP-based methods: FITS(Xu et al.,2024), TimeMixer(Wang et al.,2024), DLinear(Zeng et al.,2023), four Transformer-based methods: iTransformer(Liu et al.,2024), PDF(Dai et al.,2024), Basisformer(Ni et al.,2023), PatchTST(Nie et al.,2023), and FiLM(Zhou et al.,2022b).
It should be noted that certain earlier methods such as Vanilla-Transformer(Vaswani et al.,2017), Informer(Zhou et al.,2021), Autoformer(Wu et al.,2021), Pyraformer(Liu et al.,2022a), and FEDformer(Zhou et al.,2022a)have been extensively surpassed by the methods we selected.
Hence, we did not include these earlier methods as baselines in our comparison.
For further discussion on these methods and details of the experimental setup, please refer to AppendixBand AppendixD.

SECTION: 4.1Experimental Results

It is important to emphasize that while there have been numerous works focusing on modeling the relationships among multiple variables in time series, they still need to effectively capture information along the temporal dimension to better accommodate multivariate time series.
In contrast, our method primarily concentrates on modeling the temporal dimension.
To mitigate any potential negative impact caused by the heterogeneity of multivariate data, we followed the experimental setup of WITRAN(Jia et al.,2023), conducted experiments using a single variable.
To ensure fairness, we conducted parameter search for each baseline model, enabling them to achieve their respective optimal performance across different tasks.
For further experiment details, please refer to AppendixD.

We conducted four tasks on each dataset for long-range forecasting, and the results are shown in Table1.
For instance, considering the task setting 168-1440 on the left side of Table1, it signifies that the input length is 168, and the forecasting length is 1440.
It is worth noting that our proposed TPGN achieves state-of-the-art (SOTA) performance across all tasks, with an average improvement of MSE by 12.35and MAE by 7.25compared to the previous best methods.
In particular, TPGN demonstrates an average reduction in MSE of 17.31for the ECL dataset, 9.38for the Traffic dataset, 3.79for the ETTh1dataset, 12.26for the ETTh2dataset, and 19.09for the Weather dataset.
Furthermore, we calculated the average improvement values of TPGN compared to each method across all tasks and displayed them in the last row of Table1.
Based on the aforementioned results, it can be concluded that TPGN is capable of effectively handling long-range forecasting tasks in various domains.
For further experimental results and showcases, please refer to AppendixEand AppendixI.

It is important to emphasize that through Table1, we observed that as the forecasting task length increased, all models generally experienced varying degrees of performance decline.
However, TPGN appeared to exhibit slower decline trend.
To further validate the performance of TPGN, we expanded our experimental settings by selecting representative methods from different paradigms in Table1: WITRAN (RNN-Based), TimesNet (CNN-Based), TimeMixer (MLP-Based), and iTransformer (Transformer-Based), and compared them with TPGN.
The experimental results on the ECL dataset are depicted in Figure3, and more experimental results can be found in AppendixF.
It can be observed that as the forecasting task length gradually increases, TPGN exhibits a stable decline in performance and consistently outperforms the other comparative methods.
This strongly indicates that TPGN effectively captures the comprehensive information contained in fewer inputs and applies it well to the forecasting tasks.

For other aspects experiments and analysis of our methods can be found in AppendixG.

SECTION: 4.2Ablation Study

To validate the roles of the two information extraction branches in TPGN, we conducted tests on the performance of the model when using only one branch.
Additionally, to verify the effectiveness of PGN, we performed ablation experiments by replacing PGN with GRU and MLP, respectively.
"TPGN-long" represents only using the long-term information extraction branch, while "TPGN-short" represents only using the short-term.
"TPGN-GRU/-LSTM/-MLP" respectively represent replacing PGN in the long-term information extraction branch with GRU, LSTM and MLP.
The results of these experiments are presented in Table2.

Through the ablation experiments, we can draw the following conclusions:
(1) The two branches designed in TPGN are reasonable as they capture long-term and short-term information respectively, while preserving their respective characteristics.
In most cases, using only one branch leads to subpar results due to incomplete capture of essential features.
(2) The branch capturing long-term information in TPGN is more significant.
This can be observed by comparing the performance degradation when using only one branch versus using both branches together.
Especially for strongly periodic data like traffic, in some tasks, using only the long-term information capture branch can achieve good results.
This also aligns with our earlier mention in Section1about the significance of prioritizing the modeling of periodicity.
(3) Compared to GRU and LSTM, which have more gates, PGN introduces only one gate but still achieves better performance.
This strongly demonstrates the ability of PGN to serve as the new successor to RNN.
(4) The comparison between "TPGN-GRU/-LSTM/-MLP" and the baseline results demonstrates the strong generality and performance of the TPGN framework.
Despite their inferior performance compared to TPGN, in some tasks, they even surpass the previous SOTA time series forecasting methods.

SECTION: 4.3Efficiency of Execution

Although this paper primarily focuses on predicting longer-range future outputs using short-range historical inputs, we conducted two sets of comparative experiments to comprehensively evaluate the efficiency of our proposed method. In the first set of experiments, we kept the input length fixed at 168 and varied the output length to 168/336/720/1440 to study the impact of forecasting length on the actual runtime efficiency of the model. In the second set of experiments, we fixed the output length at 1440 while varying the input length to 168/336/720/1440 to investigate the influence of historical input series length on the actual runtime of the model. The efficiency analysis considered both time and memory aspects. We selected representative methods from each paradigm based on the experimental results in Table1as the comparative methods. We fixed the batch size at 32, the model dimension size at 128, and conducted the tests using a single-layer model. The results are shown in Figure4. Due to the much higher time and memory overhead of TimesNet compared to the other comparative models, we have omitted it from Figure4to provide a clearer illustration of the overhead details of the other models. Similarly, FiLM is not included in the time comparison chart.

From Figure4, it can be observed that although TPGN does not have the lowest time and memory overhead, it achieves a decent level of efficiency in both time and space aspects.
It is important to note that TPGN is a model with only one layer, while most of other models require the introduction of deeper layers, which inevitably leads to higher overhead.
This undoubtedly further demonstrates that our method not only achieves SOTA performance but also delivers satisfactory efficiency.

SECTION: 5Conclusions

In this paper, we propose a novel general paradigm called Parallel Gated Network (PGN). With itsinformation propagation paths and parallel computing capability, PGN achieves faster runtime speed while maintaining the same theoretical complexity as RNN ().
To enhance the application of PGN in long-range time series forecasting tasks, we introduce a novel temporal modeling framework called Temporal PGN (TPGN) with an excellent complexity of.
By employing two branches to separate the modeling of long-term and short-term information, TPGN effectively capture periodicity and local-global semantic information while preserving their respective characteristics.
The experimental results on five benchmark datasets demonstrate that our PGN-based framework, TPGN, achieves SOTA performance and high efficiency. These findings further confirm the effectiveness of PGN as the new successor to RNN in long-range time series forecasting tasks.

SECTION: References

SECTION: Appendix ALimitation and Future Outlook

It is important to acknowledge that our focus in this work was primarily on temporal modeling, without specifically addressing the modeling of relationships between variables.
Nevertheless, we can draw inspiration from other methods specialized in variable modeling.
Incorporating an additional component to model variable relationships and integrating it into the TPGN framework is a promising direction for better adaptation to multivariate prediction tasks, which we plan to explore in future research.
Additionally, we will continue to investigate the broader application of the PGN paradigm as a replacement for RNN in various time series analysis tasks and other research areas.

SECTION: Appendix BMore Detailed Discussions of Related Works

Traditional methodssuch as ARIMA(Box and Jenkins,1968), Prophet(Taylor and Letham,2018), and Holt-Winters(Athanasopoulos and Hyndman,2020)are often constrained by theoretical assumptions, which limits their applicability in time series forecasting tasks with dynamic data changes. In recent years,deep neural networks (DNNs)have made significant advancements in the field of time series analysis.
DNNs can be categorized into four main paradigms:RNN-based,CNN-based,MLP-based, andTransformer-basedmethods.

RNN-basedmethods(Hochreiter and Schmidhuber,1997;Chung et al.,2014;Rangapuram et al.,2018;Salinas et al.,2020)rely on recurrent structures to capture sequential information, which leads to long information propagation paths and brings about various limitations, as discussed in Section1.
In terms of performance, RNN-based methods struggle to capture long-term dependencies effectively. Moreover, their theoretical complexity scales linearly with the sequence length, but their practical efficiency is often low due to sequential computation. Additionally, during training, RNNs are prone to the issues of gradient exploding/vanishing(Pascanu et al.,2013).

To alleviate these issues, some methods have modified the conventional information propagation approach.
DilatedRNN(Chang et al.,2017)introduces a multi-scale dilated mechanism, which aggregates information at each time step. Although it can shorten the information propagation path by selecting the branch with the maximum skipping step, the path remains linearly related to the sequence length, which is still relatively long.
SlicedRNN(Yu and Liu,2018)addresses the efficiency problem of RNNs by dividing the sequence into multiple slices for parallel computation. However, the length of the information propagation path remains the same as the traditional RNNs.
WITRAN(Jia et al.,2023), as an emerging time series forecasting method, reshapes the sequence into a 2D dimension and performs simultaneous information propagation in both directions. This approach improves computational efficiency and reduces the information propagation path to.
However, it is still relatively long for effective information extraction.

Overall, the limitations imposed by the recurrent structures of RNNs have hindered their further development.

CNN-basedmethods(Bai et al.,2018;Franceschi et al.,2019;Sen et al.,2019)have a theoretical complexity of, due to their parallel ability, they often exhibit higher practical efficiency compared to RNNs.
However, CNNs are typically constrained by limited receptive fields, requiring the stacking of multiple module layers to capture global information from inputs.
The number of modular layers grows superlinearly with the sequence length, leading to an information propagation path ofin CNN-based methods.
Here,is superlinearly related to the sequence length.
In the case of the 2D modeling method TimesNet(Wu et al.,2023), where the input lengths in both directions are, the information propagation path would be.
MICN(Wang et al.,2023)and ModernTCN(Luo and Wang,2024)effectively reduce the information propagation path by enlarging the receptive field of the convolutional kernel. However, due to their 1D modeling approach, they may not perform as well as TimesNet(Wu et al.,2023)in capturing periodic characteristics.

MLP-basedmethods are highly favored due to their simple structure, resulting in lower computational complexity and information propagation path.
This simplicity makes MLP models easy to implement and train, contributing to their popularity.
Dlinear and NLinear(Zeng et al.,2023)optimize the original Linear model through the methods of sequence decomposition and re-normalization methods, enabling direct future prediction based on historical inputs.
However, due to their limited ability to extract deep semantic information, they may not achieve excellent forecasting performance.
TimeMixer(Wang et al.,2024)employs two specialized modules to analyze and predict time series data from multiple scales. While this approach can effectively capture periodicity, incorporating multiple scales in computations inevitably leads to increased computational costs and training difficulties.
FITS(Xu et al.,2024)treats time series prediction as interpolation and transforms the time series into the frequency domain. It operates on the frequency domain using a specially designed block LPF (Low-Pass Filter) and a complex-valued linear layer for final forecasting. However, FITS may still overlook explicit local variations present in the sequence.

Transformer-basedmethods still dominate the majority of the field.
The advantage of methods based on point-wise attention mechanism, such as Vanilla-Transformer(Vaswani et al.,2017), Informer(Zhou et al.,2021), and FEDformer(Zhou et al.,2022a), lies in theirinformation propagation path. However, previous studies(Wu et al.,2023;Jia et al.,2023)have clearly pointed out their limitation in capturing semantic information from time steps.
On the other hand, other methods that utilize non-point-wise attention mechanisms still have other limitations.
Although Autoformer(Wu et al.,2021)can capture the periodicity of time series to some extent through sequence decomposition, it is far less direct compared to methods like TimesNet(Wu et al.,2023). Additionally, its complexity remains high at.
Pyraformer(Liu et al.,2022a), through the special design of its pyramidal structure, is also able to effectively capture the periodic characteristics of sequences. However, it is still constrained by the limitations of the convolution kernel when initializing the node of pyramidal structure. Additionally, Pyraformer still maintains a high complexity of.
PatchTST(Nie et al.,2023)captures local semantic information through patches, whererepresents the stride length, and further reduces the complexity to. However, it still cannot directly capture the periodic characteristics of series.
iTransformer(Liu et al.,2024)primarily focuses on modeling the relationships between variables, including the relationships between time series variables and external time features. For the temporal dimension, iTransformer adopts a direct patch-based approach, which makes it challenging to effectively extract periodic patterns and other local characteristics.
PDF(Dai et al.,2024)also follows the approach of transposing the original 1D sequence into a 2D representation for modeling. Specifically, it utilizes CNNs to process short-term information, which is undoubtedly constrained by the limitations of convolution itself. When it comes to handling long-term periodic information, PDF also adopts a patch-based approach, which may not fully capture all the periodic characteristics present in the sequence.

To highlight the advantages of our proposed PGN paradigm and TPGN framework compared to previous methods, we have organized an information propagation diagram as shown in Figure1. Based on the above analysis, we further compiled the various strengths, information propagation paths, and theoretical complexities of different models, which are presented in Table3.

SECTION: Appendix CMore Detailed Description of the Datasets

In this section, we will provide a comprehensive overview of the datasets utilized in this paper.
(1)Electricity111https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014(ECL) contains the hourly electricity consumption of 321 customers from 2012 to 2014.
(2)Traffic222http://pems.dot.ca.govcontains the hourly data the road occupancy rates measured by different sensors on San Francisco Bay area freeways, collected from California Department of Transportation.
(3)ETT333https://github.com/zhouhaoyi/ETDatasetcontains the load and oil temperature data recorded every 15 minutes from electricity transformers in two different areas, spans from July 2016 to July 2018.
(4)Weather444https://www.bgc-jena.mpg.de/wetter/contains 21 meteorological indicators (such as air temperature, humidity, etc.) and was recorded every 10 minutes for 2020 whole year.

Due to the varying granularity of data acquisition for each dataset, in order to ensure that they contain the same semantic information for the same task, we followed(Jia et al.,2023)to aggregate them at an hourly level for experimentation. The target value for ECL is ’MT_320’, for Traffic is ’Node_862’, for ETT is ’oil temperature (OT)’, and for Weather is ’wet_bulb’. They are all split into the training set, validation set and test set by the ratio of 6:2:2 during modeling.

SECTION: Appendix DExperimental Setup Details

To ensure a fair comparison of the performance of each model, we set the same search space for the common parameters included in each model. Specifically, (1) The model dimension sizeis set to: 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024. (2) The number of layers for the model’s encoder and decoder is set to: 1, 2, 3. (3) The number of heads for the attention mechanism is set to: 1, 2, 4, 8.
In addition, for the individual hyperparameters specific to each model, we also referred to their respective original papers and conducted parameter search accordingly. This ensured that we took into account the optimal configurations for each model in our experiments.
For models that have variants, such as MICN-regre and MICN-mean, we treat the variants as separate hyperparameters and include them in the search process. The aforementioned procedures ensure that the reported results represent the optimal performance of each model under the same comparison conditions.

TPGN and all baselines were trained using L2 loss and the ADAM optimizer(Kingma and Ba,2014)with an initial learning rate of.
All of them are implemented using PyTorch(Paszke et al.,2019)and conducted on NVIDIA RTX A4000 16GB GPUs.
The batch size was set to 32 and the maximum training epochs was set to 25.
If there was no degradation in loss on the validation set after 5 epochs, the training process would be early stopped.
We saved the model with the lowest loss on the validation set for final testing.
The mean square error (MSE) and mean absolute error (MAE) are used as metrics.
We followed(Jia et al.,2023)and set the seed to 2023 to ensure the reproducibility of the results.
All experiments are repeated 5 times and we set the mean of the metrics as the final results, as shown in Table1.

SECTION: Appendix EError Bars

During the model training process, we conducted tests using the parameters that achieved the lowest loss on the validation set. This process was repeated five times, and the error bars were calculated and presented in Table4. The results in Table4clearly demonstrate the stability of our proposed method, TPGN, further confirming its superior overall performance compared to other baseline models, being SOTA approach.

SECTION: Appendix FComprehensive Experiment for Performance Variations with Different Forecasting Lengths

We conducted a comprehensive experiment to analyze the performance variation of the models across different forecasting lengths, as mentioned in Subsection4.1. According to the experimental results in Table1, we selected representative methods from different paradigms: WITRAN (RNN-Based), TimesNet (CNN-Based), TimeMixer (MLP-Based), and iTransformer (Transformer-Based), and compared them with TPGN. The results of dataset ECL have already been presented in Figure3. Therefore, in this section, we present the other dataset results, as shown in Figure5-Figure8, and further analysis them.

Through the results in Figure3, Figure5, Figure6, Figure7, and Figure8, we can clearly observe that as the sequence length increases, all models show varying degrees of performance degradation across different datasets. However, TPGN not only consistently maintains SOTA performance across all tasks but also exhibits slower performance decay with increasing forecasting lengths in most cases. This demonstrates the significant advantage of TPGN in predicting longer-range tasks, further confirming its ability to effectively extract information from limited inputs and apply them well to prediction tasks.

SECTION: Appendix GRobustness Analysis

To assess the robustness of TPGN, we conducted experiments following the settings of MICN(Wang et al.,2023)and WITRAN(Jia et al.,2023), and introduced a simple white noise injection.
Specifically, a random proportionof data was selected from the original input sequence, and random perturbations within the rangewere applied to the selected data, whererepresents the original data.
Subsequently, the injected noisy data was used for training, and we recorded the MSE and MAE metrics in Table5.

It can be observed that as the perturbation ratio increases, there is a slight increase in the MSE and MAE metrics in terms of forecasting. This indicates that TPGN exhibits good robustness in handling data with low noise levels (up to 10) and has a significant advantage in effectively handling various abnormal data fluctuations.

SECTION: Appendix HParameter Sensitivity

In our model, we have only two hyperparameters. One is the number of hidden units, denoted as, which is a common hyperparameter in many models. Its value is determined through parameter search and further validation on the validation set. During our parameter search process, we have observed that different models exhibit significant variations in the optimalvalues for the same task. Similarly, even for the same model, this variability persists when facing different tasks. Therefore, in this paper, we focus our analysis solely on the parameter, as it showcases the notable differences and its impact on the model’s performance.

In the Subsection3.2, we mentioned that the parametershould be determined based on the dataset, which has been previously acknowledged in related works(Jia et al.,2023). However, to validate the appropriateness of ourselection in the preparatory work, we conducted a statistical analysis on the variations across different task datasets, as shown in Table6.

Through statistical analysis of the dataset, we have found that there are some differences between the training set and the validation set. These differences are due to the inherent characteristics of time series data and are considered normal fluctuations. If the variances of the training set and validation set are roughly in the same range, it can be assumed that their fluctuations are roughly consistent. In this case, there are no significant differences in data distribution between the training set and validation set, so there is no need for normalization. The hyperparametershould be set to 0.

However, when there is a relatively large difference in variance between the training set and the validation set (approximately twice or half), it indicates a significant difference in data distribution between the two sets. In such cases, the hyperparametershould be set to 1 to facilitate better training of the model.

Additionally, weather datasets are a special case because they contain negative values, which leads to their mean being close to zero and a significant difference between the variance and mean. This is reasonable for weather data. On the other hand, traffic datasets do not have negative values. Therefore, even if the variances of the training set and validation set are similar, their differences can still be observed when analyzed in conjunction with the mean.

SECTION: Appendix ICase Study

To highlight the advantages of TPGN in long-range time series forecasting tasks, we compared TPGN’s showcase with the showcases of the second-best and third-best models, selected based on MSE as the evaluation metric, for each dataset. The forecasting results showcases are shown in Figure9to Figure23.

The comparison of the aforementioned cases clearly demonstrates the superiority of our method in terms of forecasting results. This unequivocally reaffirms the SOTA performance of TPGN across various domains. Moreover, it provides further evidence that PGN, as the novel successor to RNNs, can be effectively applied to long-range time series forecasting tasks.