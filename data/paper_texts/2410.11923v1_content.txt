SECTION: Spatial-Temporal Bearing Fault Detection Using Graph Attention Networks and LSTM

Purpose:This paper aims to enhance bearing fault diagnosis in industrial machinery by introducing a novel method that combines Graph Attention Network (GAT) and Long Short-Term Memory (LSTM) networks. This approach captures both spatial and temporal dependencies within sensor data, improving the accuracy of bearing fault detection under various conditions.

Methodology:The proposed method converts time series sensor data into graph representations. GAT captures spatial relationships between components, while LSTM models temporal patterns. The model is validated using the Case Western Reserve University (CWRU) Bearing Dataset, which includes data under different horsepower levels and both normal and faulty conditions. Its performance is compared with methods such as K-Nearest Neighbors (KNN), Local Outlier Factor (LOF), Isolation Forest (IForest) and GNN-based method for bearing fault detection (GNNBFD).

Findings:The model achieved outstanding results, with precision, recall, and F1-scores reaching 100% across various testing conditions. It not only identifies faults accurately but also generalizes effectively across different operational scenarios, outperforming traditional methods.

Originality:This research presents a unique combination of GAT and LSTM for fault detection, overcoming the limitations of traditional time series methods by capturing complex spatial-temporal dependencies. Its superior performance demonstrates significant potential for predictive maintenance in industrial applications.

Keywords:Bearing Fault Diagnosis, Graph Attention Network (GAT), Spatial-Temporal Modeling, Machinery sensor data, CWRU Bearing Dataset, Time series data

SECTION: 1Introduction

Mechanical equipment is an essential part of modern industries because of advancements in science and technology, making health monitoring technologies critical for ensuring system integrity. The reliability and safety of industrial machinery depend heavily on rotating components, making condition monitoring and fault diagnosis necessary for maintaining operational efficiency and preventing failures[1]. Rolling bearings are critical components in rotating machinery and are prone to failure because of high-speed motion, heavy loads, and exposure to high temperatures. Such failures can directly affect machine performance, leading to safety risks and costly maintenance. Industry data shows that defects in rolling bearings account for 30-40% of failures in rotating machinery[2].

Bearing fault diagnosis has progressed through three stages: manual experience-based approaches, signal processing-based methods, and AI-driven techniques. Traditional methods work well for diagnosing simple systems with singular faults but struggle with complex systems involving multiple faults or unpredictable failures[3]. As machinery grows more complex, traditional approaches are becoming inadequate[4]. The availability of large amounts of operational data from sensors has paved the way for AI techniques like deep learning, which have shown promise in extracting fault patterns, improving diagnostic accuracy, and reducing costs[5]. However, deep learning models often assume data independence, limiting their ability to capture interdependencies crucial for early fault detection[6].

Traditional time series analysis methods cannot capture both local and global dependencies, particularly in non-linear, irregular, and multi-scale temporal data. They also overlook the spatial structure present in different segments. This paper introduces a cutting-edge fault diagnosis approach combining Graph Attention Networks (GAT)[7]and Long Short-Term Memory (LSTM) networks[8]to address these challenges.

The approach enhances fault detection by capturing both spatial relationships between components and temporal dynamics in sensor data. A graph-based representation of time series data is constructed using entropy-based segmentation and Dynamic Time Warping (DTW)[9]to compute segment similarities. The GAT captures spatial dependencies, while LSTM models temporal patterns. The attention-based neural network model improves the accuracy, reliability, and interpretability of fault detection, focusing on detecting complex systems like bearing faults.

The objectives are:

Develop a graph-based representation of time series data by segmenting with entropy and computing segment similarities using DTW.

Design an attention-based neural network model using GAT for spatial dependencies and LSTM for temporal dynamics.

Optimize segmentation and graph construction by determining the ideal window size for entropy-based segmentation to minimize noise and computational complexity.

SECTION: 2Literature Review

Extensive research has been conducted on intelligent bearing fault diagnosis, a key aspect of predictive maintenance and condition monitoring. Early machine learning methods, such as Principal Component Analysis (PCA)[10], Support Vector Machines (SVM)[11], and K-Nearest Neighbor (KNN)[12], achieved notable success in fault classification. These models significantly improved classification accuracy over traditional signal processing techniques by utilizing statistical and geometric approaches[6]. However, classical algorithms struggle with high-dimensional data and often fail to capture the underlying nonlinearities in noisy, complex environments, limiting their effectiveness in varying operational conditions[13].

A significant limitation of traditional models lies in their difficulty in modeling intricate nonlinear interactions between input features, such as time-domain and frequency-domain signals, and output labels (fault categories)[14]. Despite their initial success, shallow learning models frequently underperform in environments with fluctuating operating settings or significant noise[15].

Deep learning, on the other hand, offers a breakthrough solution. As an advanced subset of machine learning, deep learning can model complex nonlinearities through multiple layers of abstraction, enabling it to handle high-dimensional data and uncover subtle patterns that traditional models often overlook[16]. Techniques such as Convolutional Neural Networks (CNNs)[17]and Recurrent Neural Networks (RNNs)[18]have demonstrated great efficacy in extracting features from raw vibration data and capturing temporal dependencies, making them particularly well-suited for fault diagnosis[19]. However, CNNs often require large labeled datasets for training, which can be a significant limitation when data is scarce or expensive to acquire. Janssens et al.[20]were pioneers in applying CNNs to bearing fault diagnosis by leveraging the spatial structure of data to capture covariance in frequency decomposition from accelerometer signals. While this approach was innovative, it is computationally expensive and prone to overfitting with small datasets. Guo et al.[21]refined this by incorporating adaptive learning rates and momentum components to balance training speed and accuracy, though even this enhancement struggles with noisy and imbalanced datasets, affecting robustness. Xia et al.[22]further improved CNN training by integrating temporal and spatial information from multi-sensor data, but their method still requires extensive pre-processing and domain expertise to achieve high accuracy.

Zhang et al.[23]developed a method to process vibration signals of varying sequence lengths using residual learning and 1D convolutional layers for precise feature extraction. While this approach improved accuracy, the model’s complexity increased training time and computational costs, limiting its practicality for real-time applications. Meng et al.[24]advanced this by combining deep convolutional networks with residual learning, which enhanced diagnostic accuracy even with limited training data, though the model remains vulnerable to overfitting in imbalanced datasets. Zhang et al.[25]employed a deep fully convolutional neural network (DFCNN) to transform vibration signals into images for improved input handling. However, converting time-series data into images can result in information loss and increased model complexity. Choudhary et al.[26]explored thermal imaging alongside CNNs for diagnosing fault conditions in rotating machinery, but incorporating thermal data adds complexity, and accurate thermal imaging is expensive and sensitive to environmental conditions. Xu et al.[27]proposed the Online Transfer Convolutional Neural Network (OTCNN), leveraging pre-trained CNNs and source domain features to adapt to real-time data, enhancing the accuracy of rolling bearing fault diagnoses. Although effective, OTCNN is susceptible to domain shift when the source and target domains differ significantly.

Shao et al.[28]employed maximum correlation entropy as a loss function in a deep autoencoder (DAE), optimizing key parameters using the artificial fish swarm algorithm to better match signal characteristics. Despite these improvements, this model remains sensitive to hyperparameter selection and may not generalize well across different fault types. Mao et al.[29]introduced a novel loss function incorporating a discriminant regularizer and symmetric relation matrix to capture structural discriminant information for fault types. However, the increased complexity of the model lengthens training time, and tuning the regularizer remains challenging. Hao et al.[30]employed an ensemble deep autoencoder (EDAE) with Fourier transforms as input for fault diagnosis. While Fourier transforms help isolate frequency-domain features, they may overlook time-domain information, leading to incomplete representations of fault characteristics. Similarly, Liu et al.[31]transformed sensor data into time-frequency representations (TFRs) using continuous wavelet transforms and fed them into an autoencoder enhanced by a Wasserstein generative adversarial network. This approach effectively handles nonstationary signals but is computationally expensive and sensitive to the choice of wavelet functions. Li et al.[32]utilized a deep convolutional autoencoder (DAE) with a modified loss function to analyze wavelet transmissibility data, but wavelet-based methods are computationally intensive and may struggle to capture complex temporal dependencies. Qu et al.[33]proposed a semi-supervised learning method based on a variational autoencoder (VAE), which improved classification performance with limited labeled data by leveraging the VAE’s generative capabilities. However, VAEs can experience mode collapse, where the model fails to generate diverse outputs, limiting generalization. Finally, Chang et al.[34]optimized the integration of variational mode decomposition (VMD) and a stacked sparse autoencoder (SSAE) using the Dung Beetle Optimization (DBO) algorithm. While this approach shows promise, the DBO algorithm is computationally expensive and may converge slowly in high-dimensional spaces.

Chen and Li[35]were pioneers in utilizing deep belief networks (DBNs) for bearing fault diagnosis by combining them with stacked autoencoders (AE) through a multi-sensor feature fusion approach. Although their method improved fault detection accuracy, DBNs generally suffer from slow training times and sensitivity to the initialization of network weights. Jin et al.[36]proposed a strategy that integrates VMD, feature extraction (FE), and an enhanced DBN. Their approach processes noisy vibration signals with VMD, extracts key features, and employs an improved butterfly optimization algorithm to fine-tune DBN hyperparameters for precise fault diagnosis. However, while the butterfly optimization algorithm improves accuracy, it remains computationally expensive and may not scale well with large datasets. Pan et al.[37]enhanced DBN performance using a free energy sampling method in persistent contrastive divergence (FEPCD), focusing on multi-domain feature extraction to identify fault characteristics. Despite its efficacy, this method increases computational overhead and requires significant domain expertise for feature engineering. Elsamanty et al.[38]applied PCA to reduce dimensionality and generate uncorrelated principal components (PCs), preserving most of the data’s variability. These PCs were then used as inputs to a backpropagation neural network (BPNN) for robust fault diagnosis. While PCA helps reduce data complexity, it may discard critical information, potentially limiting diagnostic accuracy in complex cases.

In addition to CNNs, AEs, and DBNs, several other deep learning methods have been applied to bearing fault diagnosis. Generative adversarial networks (GANs) and their variants have shown promise in tackling bearing fault detection challenges[39,40]. Moreover, incorporating Long Short-Term Memory (LSTM) networks has improved RNN performance, yielding successful outcomes in bearing fault diagnosis[41,42]. Reinforcement learning has also been applied to boost diagnostic accuracy[43,44].

Graph Neural Networks (GNNs) have notably advanced the processing of graph-structured data by integrating convolutional networks, recurrent networks, and deep autoencoders. Xiao et al.[45]introduced a GNN-based method for bearing fault detection (GNNBFD), constructing a graph based on sample similarity. This graph is processed by a GNN, which performs feature mapping, allowing each sample to incorporate information from neighboring nodes, thus enriching its representation. Likewise, the Granger Causality Test-based Graph Neural Network (GCT-GNN)[46]enhances fault detection by organizing time-domain and frequency-domain features into a feature matrix for causal analysis using GNNs.

In contrast to these methods, the proposed model combines a Graph Transformer Network (GTN) with a Long Short-Term Memory (LSTM) network to capture both temporal and spatial relationships in bearing sensor data. The Graph Transformer efficiently handles spatial dependencies between sensor nodes, while the LSTM captures temporal dynamics, offering a more comprehensive understanding of the data that surpasses existing GNN-based approaches. This combination enables superior fault detection by considering both spatial correlations and temporal patterns in the sensor readings. In the next section, the proposed methodology is illustrated.

SECTION: 3Methodology

SECTION: 3.1Data Processing

This section presents a method that integrates concepts from information theory, time series analysis, and graph theory to construct a graphfrom time series data. The process involves several key steps: calculating entropy, segmenting the time series, determining the optimal window size, computing Dynamic Time Warping (DTW) distances[9], and constructing a graph based on segment similarities. The entire workflow for generating the graph from time series data is illustrated in Figure1.

To effectively segment the time series data for graph construction, the method uses entropy as a guiding metric. The principle behind this approach is that regions with high entropy naturally serve as boundaries where the time series should be divided into segments. Entropy is computed using Shannon entropy[47], defined as follows:

For a discrete random variablewith a probability distribution, the Shannon entropyis given by:

whererepresents the probability of the-th outcome.

A crucial aspect of the segmentation process is determining the segment length, commonly known as the window size. The chosen window size should capture significant patterns while minimizing the influence of noise. To achieve this, the method aims to find an optimal window size based on entropy information. The entropyof a given time series segmentensures that the segment is informative and accurately reflects the underlying dynamics of the time series.

To segment the time series, it is divided into windows of sizewith a step size. The entropy for each segment is then calculated, and the average entropyfor each window size is computed as:

whereis the number of segments for window size, andis the-th segment.

Although dividinginto segments of sizeis essential, the challenge lies in determining the optimal value of. To address this, a notable relationship between entropy and window size has been observed. As Pincus[48]points out, asincreases, the number of possible configurations within that window also increases. This leads to a rise in entropy, which may reflect the increased window size rather than any genuine change in the underlying structure of the time series. To make entropy values comparable across different window sizes, the average entropy is normalized by the logarithm of the window size, as defined below:

This normalization corrects the artificial inflation of entropy associated with larger windows, allowing for a more accurate and fair comparison of the entropy values. This approach better captures the true complexity and dynamics of the time series across various window sizes.

Using the normalized entropy for each segment and every window size, our objective is to determine the optimal window sizethat maximizes the normalized entropy:

After determining the optimal window size, the time series is divided into overlapping windows of this size, called segments, as shown in Figure2. This allows for localized analysis of the time series, capturing variations and patterns that might not be apparent in the entire series.

For a time seriesof length, with a window sizeand step size, the segmentsare defined as:

for.

The similarity between segments is then measured using Dynamic Time Warping (DTW), which calculates the minimal distance by warping the time axis to optimally align the series.

Given two time seriesand, the DTW distanceis defined as:

whereis the Euclidean distance between the pointsand, andrepresents a warping path that aligns the sequences.

The similarity between two segmentsandis then defined as the inverse of the DTW distance:

The time series segments are represented as nodes in a graph, with edges indicating the similarity between these segments, as measured by DTW. Specifically, a graphis constructed, where each nodecorresponds to a segment. An edgeis added between the nodesandif the similarity between the segmentsandexceeds a threshold:

The weight of each edge represents the degree of similarity between the connected segments.

SECTION: 3.2Architecture

The proposed method, Attention-Based Time Series Analysis Graph Model (ATBTSGM), is illustrated in Figure3. It involves feeding a graphinto a neural network architecture that leverages an attention mechanism to capture the importance of each node in relation to its neighbors. Each nodeis associated with a feature vector, representing the node’s relevant attributes. The model employs the attention mechanism from[49]to compute attention scores between pairs of nodesandwithin a neighborhood.

The attention mechanism, as illustrated in Figure4, begins by calculating an attention scorebetween two neighboring nodes based on their feature vectors and learned parameters. The attention score is computed as:

whereis a learnable weight vector,is a weight matrix applied to the feature vectors,denotes the concatenation operation, and LeakyReLU introduces sparsity in the attention scores.

These raw attention scoresare normalized across all neighbors of a node using the softmax function, yielding the attention coefficients:

Here,represents the set of neighbors of node, and the softmax normalization ensures that the attention scores sum to one, making them interpretable as probabilities of importance.

The final representation of each node is obtained by aggregating the feature vectors of its neighbors, weighted by their corresponding attention coefficients:

whereis a non-linear activation function, such as Exponential Linear Unit (ELU), that introduces non-linearity into the model’s output.

To improve the model’s robustness, a multi-head attention mechanism is incorporated, where multiple attention heads independently calculate attention coefficients. The results from these heads are concatenated or averaged to form the final output:

This multi-head attention mechanism enables the model to capture diverse aspects of the graph structure, increasing its effectiveness and robustness.

Here,represents the number of attention heads, and this multi-head approach enhances the model’s ability to capture diverse patterns by learning from multiple perspectives simultaneously.

Following the attention layers, the node-level features are aggregated across the entire graph using a global mean pooling operation. This operation produces a single feature vector representing the entire graph, which is then reshaped to serve as input to a Long Short-Term Memory (LSTM) network. The LSTM captures temporal dependencies in the sequence of graph features by processing the input through its gates: the forget gate, input gate, cell state update, and output gate.

The hidden state from the final time step of the LSTM encapsulates the learned temporal information and is passed through a fully connected (feedforward) layer for classification. The output of this layer is processed into log-probabilities for each class, ensuring that the model is well-suited for classification tasks by combining the spatial structure of the graph with the temporal dynamics of the data.

SECTION: 3.3Algorithms

Algorithms1and2are responsible for encapsulating the entire process described in Section3.2. The main focus of Algorithm1is on the data processing aspect, whereas Algorithm2specifically deals with the classification of sensor data.

Algorithm1constructs the graphfrom time seriesusing entropy and DTW. It initially partitions the time series into windows of varying sizes, and calculates entropy for each segment to determine the optimal window size. It then re-segments the time series using. Subsequently, it obtains similarity scores by calculating DTW distances between all pairs of segments. It represents segments in the graph as nodes and creates edges between nodes that have a similarity score above a threshold. The overall algorithm complexity is, factoring in the length of the time series, number of segments, and segment length.

Algorithm2employs an attention mechanism and an LSTM network to process a graphwith node features, and generates log-probabilities for classification. It computes attention by calculating scoresfor nodes and their neighbors, normalizes them to obtain coefficients, and uses these coefficients to combine the features of nearby nodes. To ensure robustness, it utilizes multihead attention, pools the resultant features, and inputs them into an LSTM to capture temporal relationships. It performs classification by utilizing the LSTM output.

The attention calculations play a significant role in determining the complexity, resulting in an overall time complexity of, whererepresents the number of nodes,represents the average number of neighbors, andrepresents the feature dimension.

SECTION: 4Experimental Results

SECTION: 4.1Data Description and Analysis

Table1summarizes the Case Western Reserve University (CWRU)[50]bearing fault dataset used in this study. This dataset captures various fault conditions at three key locations—Ball, Inner Race, and Outer Race—along with data for a healthy bearing condition (“None”). For each fault location, three fault diameters (0.007, 0.014, and 0.021 inches) were recorded at a sampling frequency of 12 kHz. The datasets, labeled as A, B, and C, correspond to motor loads of 1 hp, 2 hp, and 3 hp, respectively, as detailed in Table1.

To facilitate effective model training, the continuous data stream is segmented into uniform segments based on a predefined sampling rate. This segmentation ensures that all data samples have consistent length, which is essential for training robust models. The stride-based approach creates overlapping segments, enhancing the diversity and quantity of training data without requiring additional raw data. Each segmented sample is assigned a label corresponding to its category or class, a critical step for supervised learning tasks. This segmentation and labeling process improves the model’s ability to learn and generalize.

Each of the three datasets (A, B, and C) contains multidimensional data across 10 distinct classes. To compare these datasets, we conducted statistical tests, including the T-Test and the Kolmogorov-Smirnov (KS) Test, as shown in Figure5. The T-Test results indicate that there is no statistically significant difference in the means of the datasets. Specifically, the p-values for the comparisons between Dataset A vs B, Dataset A vs C, and Dataset B vs C are 0.5380, 0.1486, and 0.4077, respectively. In contrast, the KS Test reveals significant differences in the distributions of the datasets. For example, the p-value for Dataset A vs B is, while the p-values for Dataset A vs C and Dataset B vs C are both 0.0, indicating substantial divergence between all datasets.

In this study, an entropy-based method is employed to determine the optimal window size for segmenting the dataset, which is crucial for node creation in graph-based models. Figure6shows the normalized entropy versus window size for datasets A, B, and C. As the window size increases, entropy declines, signifying reduced randomness or variability in the data, leading to smoother time series representations. By examining these entropy values, an optimal window size range (30 to 40 samples) is identified, balancing noise reduction and data smoothing. This approach ensures the model captures the most relevant features for analysis, such as identifying bearing faults. Thus, entropy serves as a reliable metric for time series segmentation, allowing the model to focus on meaningful patterns in the data while minimizing the effects of noise.

SECTION: 4.2Model Evaluation Metrics

The effectiveness of the fault prediction model in maintaining and diagnosing mechanical systems is evaluated using the following metrics:

Precision[51]measures the proportion of correct positive predictions made by the model, defined as:

wheredenotes the number of true positives (correctly predicted faults) andrepresents false positives (incorrect fault predictions). A high precision score indicates that when the model predicts a fault, it is likely to be accurate, minimizing unnecessary maintenance actions and reducing system downtime and costs.

Recall[51]evaluates the model’s ability to correctly identify actual faults, expressed as:

wheredenotes false negatives (missed faults). High recall ensures potential issues are detected early, preventing critical system failures and enhancing operational safety.

TheF1 Score[51]combines Precision and Recall, providing a balanced performance metric, calculated as:

This metric is useful when balancing fault detection accuracy and minimizing missed faults is essential.

Accuracy (ACC)[51]is the ratio of correctly classified instances to the total number of instances, given by:

whererepresents true negatives (correctly predicted non-faults).

False Alarm Rate (FAR)[51]measures the proportion of false positives out of all negative instances, calculated as:

Area Under the Curve (AUC)[51]is derived from the Receiver Operating Characteristic (ROC) curve and represents the model’s ability to distinguish between fault and non-fault cases. It is computed as the area under the ROC curve, which plots the trade-off between Recall and FAR.

These metrics comprehensively assess the model’s performance, including accuracy, fault detection sensitivity, and the ability to minimize false alarms, making them crucial for evaluating the effectiveness of the fault prediction model in mechanical systems.

SECTION: 4.3Result Analysis

For a comprehensive evaluation of the proposed model’s performance, K-Fold cross-validation is employed, a robust technique that ensures generalization and prevents overfitting. The process of K-Fold training involves dividing the dataset intosubsets or “folds.” The model is trained onfolds in iterations, with one fold used for validation each time. This process repeatstimes, with each fold serving as the validation set once. By averaging the final performance across alliterations, a more accurate and resilient estimate of the model’s performance is obtained. To accelerate processing speed, the experiments are conducted on a powerful computing system that includes an Intel Xeon processor with 52 cores running at 3.5 GHz, 64 GB of RAM, and a 32 GB graphics card. The model is trained separately on Datasets A, B, and C to examine its generalization ability across different data sources.

After training, performance is assessed using three key metrics: Precision, Recall, and F1 Score, as summarized in Table2. The model consistently demonstrates strong performance across all datasets, achieving near-perfect results. Specifically, it reaches 99% Precision, Recall, and F1 Score on both Datasets A and C, showcasing reliable fault detection capabilities. On Dataset B, the model achieves a flawless 100% across all metrics, indicating robustness and effectiveness in detecting faults under different conditions.

The consistently high scores across all datasets underline the model’s reliability and robustness in real-world predictive maintenance tasks, where both precision and recall are crucial for efficiently detecting bearing system faults. Figure7presents heatmaps of the classification reports for each dataset, illustrating the Precision, Recall, and F1-Score per class. On Dataset A, the model performs exceptionally well, with minor reductions in Recall for class 3 (0.92) and in F1-Score for class 7 (0.91), while still maintaining an overall accuracy of 99%. Dataset B achieves perfect classification across all metrics and classes. For Dataset C, the model again performs strongly, with slight decreases in Recall for class 1 (0.89) and class 3 (0.92), maintaining an overall accuracy of 99%.

The proposed ATBTSGM model is compared against several baseline models, including: (1) GNNBFD[45], (2) Neural Network-based SO-GAAL (Single Objective Generative Adversarial Active Learning), (3) CutPC (a graph-based clustering method using noise cutting), (4) LOF (Local Outlier Factor), (5) distance-based methods including KNN, and (6) IForest, as mentioned in[45]. These comparisons are conducted across three datasets (A, B, and C). Key evaluation metrics such as Accuracy (ACC), Detection Rate (DR), False Alarm Rate (FAR), and Area Under the Curve (AUC) are used to compare performance, as summarized in Tables3and4.

The ATBTSGM model demonstrated exceptional performance across all datasets, achieving the highest ACC (99.40% for Datasets A and C, and 100% for Dataset B). Its detection rates (DR) were equally impressive, ranging from 96.3% to 98%. Furthermore, the model maintained a very low False Alarm Rate (FAR), with values as low as 0.1 for Datasets A and B, and 0.2 for Dataset C. ATBTSGM also attained near-perfect AUC values, reinforcing its reliability and superior performance when compared to other baseline models.

In contrast, GNNBFD, though performing well, showed slightly lower ACC and DR, and a higher FAR (up to 0.750 for Dataset B). Other models, such as SO-GAAL and CutPC, performed moderately, while LOF and KNN struggled, particularly on Dataset B, with low DR and high FAR.

Overall, the ATBTSGM model consistently outperformed its competitors across all metrics, demonstrating its robustness, low false alarm rates, and high fault detection capabilities, making it an optimal choice for predictive maintenance applications.

The generalization ability of the proposed ATBTSGM model was evaluated by training and testing it on different combinations of Datasets A, B, and C, using Precision, Recall, and F1-Score as key performance metrics. Figure8presents the model’s performance, demonstrating its robustness across different dataset configurations. For instance, training on Dataset A and testing on Dataset B yields near-perfect results, with 99% achieved across all three metrics. However, when tested on Dataset C, a slight performance decrease is observed, with Precision at 95%, Recall at 96%, and F1-Score at 96%.

Similarly, when trained on Dataset B, the model maintains strong performance when tested on Dataset A, achieving 98% Precision, 97% Recall, and 97% F1-Score. Testing on Dataset C results in a minor decline, with all metrics around 95%. Training the model on Dataset C delivers consistent results when tested on both Datasets A and B, with metrics consistently near 95%. This cross-dataset evaluation highlights the model’s ability to generalize across varying data sources, a critical feature for real-world applications, where data often vary in structure and noise levels.

Cross-dataset testing underscores the model’s strong generalization abilities, particularly when trained on Datasets A and B. The performance remains close to perfect on similar datasets but drops slightly when tested on Dataset C, indicating that Dataset C may contain greater variability or complexity. This suggests potential areas for model enhancement, particularly in handling more diverse datasets.

Figure9compares the ATBTSGM model’s accuracy in various dataset transfer scenarios against baseline models. The FFT-SVM model[52], for example, achieves an average accuracy of 66.6%, while FFT-MLP[53]and FFT-DNN[20]models show improved accuracy at 80.4% and 78.1%, respectively. The WDCNN[54]model performs well, with an average accuracy of 90.0%, which is further improved to 95.9% with the WDCNN (AdaBN) model[54]. The RNN-WDCNN[55]model boosts performance further, reaching 96.2% on average. Finally, the ATBTSGM model surpasses all other models, achieving an average accuracy of 96.5%, with individual results ranging from 95.0% to 99.0% across dataset transfers.

SECTION: 5Discussion

The superior performance of ATBTSGM can be attributed to two key factors: its graph-based data representation that incorporates entropy and its capability to capture temporal dependencies within time series data.

ATBTSGM employs a graph-based representation of the data, leveraging entropy to evaluate uncertainty and complexity in the dataset. Entropy acts as a measure of the informational content within the data, enabling the model to focus on the most relevant and meaningful features. This approach allows the model to uncover subtle and intricate patterns, particularly effective for fault detection tasks where hidden relationships between features are critical. By capturing these relationships, the model can perform a more sophisticated analysis of the data than traditional machine learning methods that treat data as independent points.

In addition, the model excels at capturing temporal dependencies within time series data. Time series data often involve sequential correlations, where each data point is influenced by previous values. By leveraging the graph structure, ATBTSGM effectively incorporates this temporal context, crucial for understanding the dynamic behavior in systems like fault detection. This ensures that the model retains essential contextual information, leading to more accurate predictions.

These two factors work together to significantly enhance ATBTSGM’s performance. Its ability to capture both temporal relationships and graph-based structural patterns results in high accuracy, improved detection rates, and a reduced false alarm rate across all datasets. This combination makes ATBTSGM a robust and reliable solution for real-world predictive maintenance and fault detection applications.

SECTION: 6Conclusion

Across multiple datasets, the ATBTSGM model consistently demonstrates exceptional performance in fault detection, with high Precision, Recall, and F1-Score metrics. With exceptional accuracy on Datasets A and C (99%) and impeccable performance on Dataset B (100%), the model has shown its reliability and resilience in identifying faults in diverse scenarios. The heatmaps further validate the model’s strong classification ability in 10 different classes, with only slight performance decreases in certain cases. Additionally, the model’s superior performance in real-world predictive maintenance tasks is highlighted by its low False Alarm Rate (FAR) and high Detection Rate (DR) across all datasets. In addition to surpassing baseline models in accuracy and generalization, the ATBTSGM model also maintains strong fault detection capabilities, low error rates, and high resilience to varying data characteristics. By offering both high accuracy and reliable predictive maintenance, the ATBTSGM model is established as the optimal choice for fault detection in complex mechanical systems.

Despite the impressive results of the ATBTSGM model on diverse datasets, there is potential for future research to improve its scalability for larger and more intricate datasets, considering the substantial amount of data generated by real-world systems. Moreover, the inclusion of domain adaptation and transfer learning techniques enables the model to adjust to diverse operating conditions without extensive retraining. Implementing Transformer-based architectures could further amplify the model’s capability to capture long-term dependencies in its temporal representation. A potential area for future research is the implementation of real-time fault detection with reduced computational latency. Furthermore, it is important to note that the issue of robustness to noisy or incomplete data, commonly faced in industrial applications, requires further consideration. Enhancing the model’s explainability and interpretability is vital for establishing trust in high-stakes predictive maintenance scenarios, while mitigating computational complexity and generalizing to unseen fault types are necessary for practical deployment.

SECTION: Statements and Declarations

SECTION: Competing Interests

The authors declare that there are no competing interests associated with this research work.

SECTION: Funding

This research did not receive any specific grant from funding agencies in the public, commercial, or not-for-profit sectors.

SECTION: Informed Consent

Informed consent was obtained from all individual participants included in the study.

SECTION: Data Availability

The datasets generated and/or analyzed during the current study are available in upon reasonable request from the corresponding author.

SECTION: References