SECTION: Improving Vision Transformers by Overlapping Heads in Multi-Head Self-Attention
Vision Transformers have made remarkable progress in recent years, achieving state-of-the-art performance in most vision tasks. A key component of this success is due to the introduction of the Multi-Head Self-Attention (MHSA) module, which enables each head to learn different representations by applying the attention mechanism independently. In this paper, we empirically demonstrate that Vision Transformers can be further enhanced by overlapping the heads in MHSA. We introduce Multi-Overlapped-Head Self-Attention (MOHSA), where heads are overlapped with their two adjacent heads for queries, keys, and values, while zero-padding is employed for the first and last heads, which have only one neighboring head. Various paradigms for overlapping ratios are proposed to fully investigate the optimal performance of our approach. The proposed approach is evaluated using five Transformer models on four benchmark datasets and yields a significant performance boost. The source code will be made publicly available upon publication.

SECTION: 
Since the introduction of Transformerfrom language to vision, Vision Transformers have gradually dominated many vision tasks such as image recognitionand object detection. Due to the global attention mechanism in Transformers, Vision Transformers could yield better performance in vision tasks when the training data is abundant and the training epochs are enough. Although the convergence rate of Vision Transformers is frequently slower than their Convolutional counterparts, global information communication is one of the key elements for the success of Transformer models.

Multi-Head Self-Attention (MHSA)is the core module for Transformer models so that the self-attention mechanism can be implemented in multiple heads to learn different representations. The self-attention mechanism is originally designed for processing language data to capture the long-range relationship between words in language. When self-attention is applied to vision tasks for image classification and recognition, each image patch token could interact with all other patch tokens for the global views of the image, which is significant for the excellent performance of Vision Transformers. MHSA represents that the queries, keys and values are split into different heads and the self-attention is computed in each head independently. Transformer models with Multi-Head Attention frequently yield better performance than those with Single-Head Attention. Thus most Transformer models utilize Self-Attention with multiple heads for better representation learning and performance. Even though MHSA boosts the performance of the Transformer models, the relationships and interactions between different heads are rarely investigated.

The success of Transformer models mainly lies on the effective information exchange between different tokens so that each token can have global views of the context information. Due to the superior performance of MHSA, most Transformer models utilize MHSA by default. However, the queries, keys and values are divided for each head without overlapping and there is no information exchange when the attention is computed in each head. In other words, when calculating the attention in the current head, it does not have the information in other heads. Although the tokens will be processed by linear projections after the attention, the information exchange is only limited to each token.

In this paper, we claim that information exchange during attention calculation in each head can improve the performance of Vision Transformers. This can be realized by overlapping queries, keys and values in each head with queries, keys and values of neighboring heads. For this purpose,
we propose Multi-Overlapped-Head Self-Attention (MOHSA) to improve the Multi-Head Self-Attention mechanism by overlapping the heads so that,, andin each head are overlapped by,, andof their neighboring heads when the attention is calculated, as illustrated in Fig.. By overlapping the heads, the information in other heads could also be involved in the calculation of the attention in the current head. Since the overlapping would slightly increase the dimension of the tokens after concatenating tokens from different heads, the linear projections would decrease the dimension of the tokens to the original size. The information communication between the heads could yield better performance for Vision Transformers. Moreover, we design various paradigms for overlapping ratios to investigate the best performance of our proposed approach. We explore several Vision Transformer models on CIFAR-10, CIFAR-100, Tiny-ImageNetand ImageNet-1kto verify the effectiveness of our proposed MOHSA.

Our major contributions are summarized below.

We propose Multi-Overlapped-Head Self-Attention (MOHSA) and demonstrate that Vision Transformer models could be improved by overlapping the queries, keys and values of the current head with the queries, keys and values of adjacent heads when the attention is computed. Several Vision Transformer models are exploited on various datasets to demonstrate the effectiveness of our proposed method.

Various variants based on the overlap dimensions are proposed to thoroughly investigate the optimal performance of MOHSA. The proposed MOHSA could be integrated into most Vision Transformer models to enhance their performance with negligible overhead.

To the best of our knowledge, our work is the first to investigate the overlapping approach between different heads for MHSA when the attention is calculated.

SECTION: 
SECTION: 
Vision Transformerutilizes the image patches which are embedded as tokens and a class token as the token inputs for the Transformer encoder to recognize the images. Swin-Transformerreduces the computational cost by limiting the implementation of attention calculation in each window and expands the view of the tokens by shifting the windows. Shuffle Transformerproposes spatial shuffle to exchange information across windows. MSG-Transformerharnesses MSG tokens to represent local windows and the information communication between windows is implemented with MSG tokens. PVTdesigns Vision Transformer with a hierarchical structure without convolutions by shrinking the feature maps gradually. CaiTand DeepViTinvestigate Vision Transformers with deeper layers. MobileViT seriesand EdgeViTsexplore Vision Transformers on mobile-level applications. Some worksintroduce convolutions into the Vision Transformers to take advantage of both convolutions and Transformers. PiTexplores the possibility of using individual pixels instead of patches as the tokens for Vision Transformers.

SECTION: 
DeepViTexplores a deeper layer Vision Transformer by proposing re-attention that mixes the attention maps among all heads with a learnable matrix before multiplying with the values. The re-attention is similar to talking-heads attentionwhich is originally employed for language tasks and also utilized by CaiT. Talking-heads attentionapplies learnable linear projections to the heads in Multi-Head Attention before and after the softmax function to exchange the information for the attention maps between heads in the attention module.

The aforementioned methods mix attention maps using linear projections, allowing information exchange between heads only after the attention maps are calculated. However, the mixed attention maps are then applied to values that do not have information exchange between heads. In contrast, we propose a method that enables communication between heads during the attention computation by overlapping the heads with,, and. Although this approach introduces a slight increase in computation and parameters, it significantly enhances the performance of Vision Transformer models across various datasets.

SECTION: 
SECTION: 
Transformerhas an attention mechanism to compute the long-range relationship between tokens. The attention calculation includes queries, keys, values and the dimension of queries and keys. The queries and keys are implemented dot product to calculate the weights which are utilized to the values to compute the final results. The matrix format of attention calculationis illustrated in Eq. ().is employed to prevent the large value input for the softmax function after computing the dot product.

Multi-Head Self-Attentionis utilized in the Transformer model for better performance so that different heads can learn different representations, which is better than one-head attention. The attention in each head is illustrated in Eq. ().,, andare divided as,, andin each head respectively and the attention calculation is implemented in each head independently for different aspects of learning.

Finally, MHSA could be represented as Eq. (). The results of all heads are concatenated andis the projection matrix.

Vision Transformeris the Transformer model applied to vision tasks, mainly image classification and recognition. For Vision Transformer, only the encoder is utilized for feature extraction. The images are divided as same-size patches which are embedded as tokens for the Vision Transformer. Layer Normalizationis often employed as the normalization before MHSA and FFN. Vision Transformercould be applied to vision tasks and achieve significant performance on vision tasks due to the effective information exchange for attention mechanism. the self-attention mechanism calculates the dot product of each token with all other tokens effectively exchanging the information between them. In vision tasks like image classification and recognition, each patch token has a global view of the image, which is important for each patch to attain the context information.

Nonetheless, the attention is implemented independently in each head and one head does not have the attention information of other heads when the attention is computed. Although the projection matrix is implemented after all the heads are concatenated, only the information in each token is exchanged. There is no information exchange when computing the attention in each head. Thus we propose to overlap the information of neighboring heads to enhance the information exchange when the attention is computed in each head.

SECTION: 
Based on the aforementioned analysis, we have proposed a simple yet effective approach that improves Multi-Head Self-Attention to enhance the performance of Vision Transformer. To make information exchange between heads, we exploit soft division instead of hard division when,, andare divided into different heads. The process could be illustrated in Eq. () for,, andrespectively. We utilize “part” to illustrate partial overlapping with adjacent heads in Eq. ().

In Eq. (),,andare the original hard division results for, which is shown in Eq. (). For soft division,,andalso include partial information in their neighboring heads so that,,are overlapped with two adjacent heads, which is demonstrated in Fig.. The left figure indicates the original implementation of the hard division of,,to different heads and the right figure represents our proposed implementation of the soft division of,,to different heads. The,, andwould overlap with two adjacent heads to construct,, andfor calculating the attention. For the first head and the last head which only have one adjacent head, zero padding is utilized to construct two neighboring heads for the first and the last head.

After the attention for the overlapped,, andis calculated for each head, the results are concatenated together, as illustrated in Eq.-. The overlapped heads would slightly increase the dimension of the tokens after concatenation. Thus the projection matrixwould project the concatenated dimension () to the original token dimension () so that the token could be fed into the next layer with the same dimension. The projection matrixin Eq. () projects the concatenated non-overlapped heads () to the original token dimension (). Since the dimension of overlapped headsis slightly larger than the dimension of non-overlapped headsand the number of headsis unchanged, the projection matrixin our proposed MOHSA would have slightly more parameters than the projection matrixin the original MHSA.

SECTION: 
The overlapping ratios for the overlapped heads are crucial to the effectiveness of the proposed approach. In the experiments, we design several paradigms for the overlapping ratios. In this work, we utilize overlap dimensions to demonstrate the overlapping ratios, as illustrated in Fig.. From Fig., the number of overlap dimensions for each head is the overlap dimension of its one-side adjacent head. In Fig., the blue sections are the original parts of the heads and the red sections are overlapped parts with two adjacent heads. For the first and the last head which have only one side adjacent head, zero padding is exploited for the missing neighboring heads.

In addition to the fixed overlap dimensions for all layers, we also implement some variants of the overlapping ratios by changing the overlap dimensions according to the depths of the layers. In this paper, one layer includes the attention module and the FFN module. The variants for the overlapping ratios are demonstrated in Table. In Table, “inc (x layers)” represents the overlap dimension is increased by 1 every x layers, and “dec (x layers)” indicates the overlap dimension is decreased by 1 every x layers, which is a reverse process of ”inc (x layers)” based on the depths of the layers. In addition, “0-indexed” illustrates the overlap dimension starts from 0 for “inc” and ends to 0 for “dec”, and “1-indexed” demonstrates the overlap dimension starts from 1 for “inc” and ends to 1 for “dec”. For instance, inc (2 layers) with “0-indexed” (represented by “inc-0 (2)” in the experiments) for a total of 12 layers has overlap dimensions (0, 0, 1, 1, 2, 2, 3, 3, 4, 4, 5, 5) for layer from 1 to 12, and dec (1 layer) with “1-indexed” (represented by “dec-1 (1)” in the experiments) for total 12 layers has overlap dimensions (12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1) for layer from 1 to 12.

SECTION: 
In the experiments, we select three representative models to investigate the proposed approach: vanilla ViT, deeper layer Transformer CaiT, and window-based hierarchical Transformer Swin-Transformer. More specifically, ViT-Tiny, ViT-Small, CaiT-xxs12, CaiT-xxs24and Swin-Tinyare selected to explore the effectiveness of the proposed approach. The parameter settings for the models are shown in Table. For Swin-Tiny, the number of heads and depths are varied based on the stages. CaiT-xxs12 and CaiT-xxs24 have two extra class layers. The models will be trained and evaluated on CIFAR-10, CIFAR-100, Tiny-ImageNetand ImageNet-1k.

The experiments are implemented by 100 epochs with 20 epochs warmup. The optimizer is AdamW. The experiments for CIFAR-10, CIFAR-100, Tiny-ImageNetare run on 4 NVIDIA P100 GPUs with total batch size 128. The experiments for ImageNet-1kare conducted on 4 NVIDIA V100 GPUs with a total batch size of 512. The initial learning rate for ViT and CaiT is 0.0005. For Swin-Tiny, the initial learning rate for Tiny-ImageNet is 0.00025, and for ImageNet is 0.0005. The images are resized to 224 for the experiments. Some other settings follow the settings in Swin-Transformer.

SECTION: 
The ablation study for ViT-Tiny and ViT-Small on CIFAR-10 is demonstrated in Table. In Table, “fixed 1” represents the overlap dimension is 1 for all layers, and “fixed half” indicates the overlap dimension is half of the head dimension for all layers. “inc” indicates the overlap dimension is increased with the depths of the layers increasing and “dec” represents the overlap dimension is decreased with the layers going deeper. The two numbers following “inc” or “dec”, for instance, “inc-0 (3)” illustrates the overlap dimension is increased by 1 every 3 layers and it is “0-indexed”. More details about the variants of overlapping ratios can be found in Sec..

The best accuracy for ViT-Tiny is 85.65 for variant “inc-0 (6)” and the best accuracy for ViT-Small is 87.30 for variant “inc-0 (1)”. For ViT-Tiny, “fixed half” demonstrates higher accuracy than “fixed 1”. For ViT-Small, “fixed half” has lower accuracy than “fixed 1”. For a fixed paradigm of overlap dimension, more ratios of overlapping cannot guarantee higher accuracy.

Although the overlapped heads would slightly increase the computations and parameters, from the experimental results we can see that the increased numbers of parameters and FLOPs are negligible. Tableillustrates the experimental results of CaiT-xxs12 on CIFAR-10. The best result 80.04 significantly boosts the performance of CaiT-xxs12 by 3% with negligible overhead.

SECTION: 
Tabledemonstrates the experiments of ViT-Tiny and ViT-Small on CIFAR-100. The best result for ViT-Tiny is 63.01 with “inc-0 (2)” and for ViT-small is 64.97 with “dec-1 (3)”. Both results improve the models by more than 1% than the original models. Similar to the results on CIFAR-10, “fixed half” has higher accuracy for ViT-Tiny, and “fixed 1” illustrates better performance for ViT-Small. In addition, “fixed half” does not demonstrate effectiveness for ViT-Small.

Tableillustrates the performance of CaiT on CIFAR-100. For CaiT-xxs12, using 1 overlap dimension for all layers could boost the accuracy by nearly 2%. For CaiT-xxs24, the accuracy is significantly enhanced by 5% with “inc-1 (3)”. Additionally, “inc-1 (3)” is much better than using half dimension of the head as the overlap dimension which utilizes more parameters and FLOPs.

SECTION: 
The experimental results on Tiny-ImageNet are demonstrated in Table. The accuracy of ViT-Tiny could be enhanced by 1.61% utilizing “inc-1 (2)”. ViT-Small is boosted by 1.27% with “inc-1 (1)”. Moreover, using half of the head dimension as the overlap dimension is better than using 1 as the overlap dimension for all layers for ViT-Tiny, while it is not the case for ViT-Small.

For CaiT-xxs12 and CaiT-xxs24, “fixed 1” that utilizes fixed overlap dimension 1 for all layers could significantly improve the accuracy by 2.39% and 3.58%, respectively. Furthermore, the accuracy is enormously enhanced by 7.41% for CaiT-xxs24 with “inc-1 (1)”. The effectiveness of our method is significant on CaiT models and we can ignore the slightly increased parameters and computations.

For Swin-Tiny, “fixed 1” paradigm could greatly enhance the accuracy by 1.23%, and the performance could be further boosted by a large margin with “dec-1 (1)”. The effectiveness of overlapped heads is also demonstrated on the Transformer model with window-based hierarchical architecture.

SECTION: 
By training and testing on ImageNet, we could investigate the effectiveness of our proposed approach on large datasets. The experimental results on ImageNet are demonstrated in Table. Almost all models equipped with our proposed MOHSA have significant improvements on ImageNet.

ViT-Tiny could be boosted by more than 1% with “fixed half” that uses half of the head dimension as the overlap dimension for all layers. ViT-Small is enhanced by 0.79% with “dec-0 (1)” with negligible overhead. Additionally, utilizing 1 as the overlap dimension for all layers is not effective for ViT-Tiny on ImageNet. But using 1 as the overlap dimension is better than using half head dimension as the overlap dimension for all layers for ViT-Small.

Our approach has even more performance enhancement on CaiT models. CaiT-xxs12 is greatly improved by 1.17% with “inc-1 (1)” and the accuracy of CaiT-xxs24 is significantly increased by 3.70% with “inc-0 (1)”. For CaiT-xxs24, the accuracy is boosted by more than 3% by using only 1 as the overlap dimension for all layers, which illustrates the effectiveness and efficiency of our proposed method by remarkably boosting the performance of the models with minimum overhead.

For Swin-Tiny, simply using 1 as the overlap dimension is not effective on ImageNet. Even though increasing the overlap dimension to half of the head dimension improves the accuracy of Swin-Tiny, it achieves almost the same result as the paradigms of varying the overlap dimension by the depths of the layers which have fewer parameters and computations.

Tableillustrates the ablation study on applying overlap paradigm “inc-0 (1)” to,, orby using ViT-Tiny and CaiT-xxs12 on ImageNet. From the experimental results shown in Table, applying our approach tohas more effect on boosting the performance, and applying the overlapping approach to,, andcould yield the best performance.

SECTION: 
In the experiments, we utilize different variants of our proposed MOHSA on various Vision Transformer models and datasets to illustrate the effectiveness of MOHSA. Overall, the enhancement of the models on various datasets is remarkable with such insignificant overhead.
For different variants of our proposed method, the results manifest some differences. For a fixed paradigm that the overlap dimension is the same for all layers, the performance could be enhanced by only 1 overlap dimension in most cases and increasing the overlap dimension for fixed mode cannot guarantee better results. Additionally, the variants of varying overlap dimensions based on the depths of the layers demonstrate superior performance than the fixed paradigm on various models and datasets in most cases. Compared to the fixed paradigm with a high overlap dimension, varying overlap dimensions with the depths of the layers could save the number of parameters and computational costs.

Fig.manifests the accuracy of CaiT-xxs24 on the val or test set with the training proceeding between the original models and our proposed models. “inc-1 (3)”, “inc-1 (1)” and “inc-0 (1)” are selected as ours for comparison for CIFAR-100, Tiny-ImageNet, and ImageNet, respectively. The accuracy between the original models and our models demonstrates almost no difference at the early stage of training, while diverges significantly with the training proceeding.

Moreover, the best improvements for various datasets are mostly CaiT models. The possible reason might be that CaiTutilizes talking-heads attentionbefore and after the softmax function. Equipped with our MOHSA, CaiT might have more effective information exchange between different heads and illustrate superior performance.

SECTION: 
In this paper, we have proposed a simple yet effective module MOHSA, to improve the original MHSA in Vision Transformers by overlapping the heads, allowing for information exchange during attention calculation in each head. To the best of our knowledge, this is the first work to propose overlapping heads and achieve significant enhancement across various datasets for different Vision Transformer models. We hope our work will inspire the community to further explore the structure of Vision Transformers.

SECTION: References