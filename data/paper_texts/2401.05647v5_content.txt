SECTION: Quantum kernel machine learning with continuous variables

The popular qubit framework has dominated recent work on quantum kernel machine learning, with results characterising expressivity, learnability and generalisation. As yet, there is no comparative framework to understand these concepts for continuous variable (CV) quantum computing platforms. In this paper we represent CV quantum kernels as closed form functions and use this representation to provide several important theoretical insights. We derive a general closed form solution for all CV quantum kernels and show every such kernel can be expressed as the product of a Gaussian and an algebraic function of the parameters of the feature map. Furthermore, in the multi-mode case, we present quantification of a quantum-classical separation for all quantum kernels via a hierarchical notion of the “stellar rank” of the quantum kernel feature map.
We then prove kernels defined by feature maps of infinite stellar rank, such as GKP-state encodings, can be approximated arbitrarily well by kernels defined by feature maps of finite stellar rank. Finally, we simulate learning with a single-mode displaced Fock state encoding and show that (i) accuracy on our specific task (an annular data set) increases with stellar rank, (ii) for underfit models, accuracy can be improved by increasing a bandwidth hyperparameter, and (iii) for noisy data that is overfit, decreasing the bandwidth will improve generalisation but does so at the cost of effective stellar rank.

SECTION: 1Introduction

The quantum machine learning (QML) community has recently begun to explore whether quantum resources may be useful for kernel machine learning[1,2,3]. While research has typically focused on improving traditional classical kernel methods, such as support vector machines, classical kernelisation is in fact far more ubiquitous. Kernels, which essentially provide a similarity metric between data points, appear as filters in convolutional neural networks[4], can represent attention matrices in transformer networks[5], are used as training signals for generative networks[6], and can provide a key mechanism for causal discovery[7]. Clearly, there is much to be gained by understanding whether quantum kernels can provide an advantage over their classical counterparts[8,9,10,11,12].

This recent exploration has led to the development of quantum kernel selection tools[13], generalisation bounds[14], optimal solution guarantees[1,15], and has resulted in several physical implementations[16,17,18,19]. The community has learned that although entangled quantum kernels—including those generated by deep parametrised quantum neural networks (PQNN)—are highly expressive, such expressivity typically comes at a cost. This is the so-called “exponential concentration” problem analogous to the barren plateau problem in quantum neural networks[20]—as quantum kernels become more expressive, they typically also become exponentially harder to learn and less likely to generalise[21,22,23]. Essentially, the value of the kernel between different datapoints decreases as a function of the size of the problem—for discrete variable quantum kernels, a highly expressive kernel yields exponential concentration. Recent numerical work suggests it may nonetheless be possible to overcome these learning difficulties by manipulating a bandwidth hyper-parameter to tune the expressivity of the quantum kernel[24,25], a technique inspired by bandwidth tuning of classical Gaussian kernels[26]. Finding the sweet spot where the quantum kernel is both learnable and generalises well, but is nonetheless still classically hard to simulate is, however, an open challenge[27]. Robustness to noise also presents a further unexplored challenge to such kernel tuning techniques.

As a consequence, the QML community has to some extent converged on a new quest. Rather than seeking a quantum advantage for kernel machine learningper se,physicists are now searching for inductive biases that specific quantum kernels, or families of quantum kernels, may bring to particular ML tasks[22]. The thinking is inspired by the tremendous advantage convolutional neural networks have provided imaging tasks due to their translational invariance[4]. To this end, the group theoretic structure of some specific quantum kernels has been used to exploit structure in certain classical learning problems to prove quantum advantage[17,16,28]. As such, there is strong motivation to identify and understand new classes of quantum kernels.

An outstanding key challenge to substantial progress is the theoretically opaque nature of quantum kernels. Classical kernels employ the “kernel trick”— one avoids explicitly evaluating the kernel in feature space by instead using an analytic representation acting in the original data space (e.g. the Gaussian function, or other RBF kernel). This provides direct access to the kernel and permits theoretical analysis. In contrast, quantum kernel values are accessed via estimating inner products through quantum measurement. Thus, quantum measurement expectation values approximate each kernel matrix entry up to some additive error. Functional forms of the kernel are rarely available and theoretical understanding of quantum kernels is somewhat limited.

Interestingly, almost all the work on quantum kernels to date has focused on discrete, finite dimensional quantum systems, such as those generated by parameterised quantum circuits. While a few works have evaluated specific, continuous variable, infinite dimensional quantum encodings[29,2,30,31,32], there is as yet no unifying approach to CV quantum kernels.

In this paper, we use the holomorphic representation of continuous variable quantum states[33]to find a closed form expression for an arbitrary CV multi-mode kernel,
which allows for insight into the general theoretical structure of CV quantum kernels—every kernel can be expressed as the product of a Gaussian and an algebraic function of the four key parameters of the feature map.
This structure permits some preliminary intuition into possible trade-offs between bandwidth hyperparameter tuning (to improve generalisation and learnability) and consequent loss of quantum advantage[27]as kernel values will be close to zero beyond a certain distance.
Furthermore, the use of holomorphic functions allows us to present a framework with a very natural taxonomy of kernel “quantumness”, achieved via the notion of the stellar rank of the quantum kernel feature map. Stellar rank ultimately provides useful guarantees as to the hardness of classical simulation[33,34,35]of such kernels.

The paper is organised as follows: in section2we introduce relevant mathematical notation and background. In section3.1we review holomorphic representations of quantum CV systems (based on[33]). In section3.2we formalise how one may define a CV quantum kernel using the tools of holomorphic functions, show it satisfies necessary properties and comment on the dependence of classical simulability on stellar rank. In section4, we present the multi-mode general CV encoding and show that all finite rank kernels can be expressed as the product of a Gaussian and algebraic function term. We also prove that quantum kernels defined by feature maps of infinite stellar rank, such as GKP and cat-state encodings, can be approximated to arbitrary precision with kernels defined by feature maps of finite stellar rank. Section5presents an example of a single mode displaced Fock state encoding. We use this example kernel to simulate five learning experiments to verify the expected general behaviour, using a uniform data set and a data set with annular structure. General qudit kernels are presented in section6, where we show that these kernels are a subset of the general multi-mode case.
While for particular quantum kernels there are undoubtedly simpler analytic forms, our goal here is to highlight the general form, characterise the stellar rank, and provide insight into some general features that are likely to be applicable to all CV quantum kernels. To this end, we explore the notion of bandwidth tuning and how it affects our example kernels.
We conclude with section7, where we discuss findings and make suggestions for future work.

SECTION: 2Preliminaries

SECTION: 2.1Notation

Here we fix notation and formalise the necessary mathematics.

Vectors are denoted in bold unless otherwise specified (e.g.) as are matrices, the latter with capital letters only (e.g.). Sets and vector spaces are written in mathematical calligraphic font (e.g.). Complex numbers will be stated explicitly or as 2-dimensional real vectors, most commonly we use. Conjugates of complex vectors or matrices are written with superscript asterisk (e.g.). Overlines instead represent completion of sets, e.g..

will always be a pure quantum state. We reservefor the Fock state number of such a quantum state. We write all kernels using, whether they are quantum or classical will be made explicit within the text. We defineas the set of natural numbers including.

Hypergeometric functions are written as, withrepresenting the specific form. Polynomials ofare written asand Gaussians as. As usual,is the gamma function:

for.

The inner product of a specific Hilbert space,, is written as. Unless otherwise specified, the normis given bywhereis the space in whichhas a well defined inner product.

We define holomorphic functions, denoted by, as complex functions which are complex differentiable in a neighbourhood about every point.is a holomorphic function dependent on some classical data. Stellar functions are a subset of holomorphic functions with finite roots and written as the product of a polynomial and Gaussian term. Asis used as our Fock state number, our stellar rank (the number of complex roots of) is also.

SECTION: 2.2Introduction to classical kernel machine learning

The core tenet of kernel machine learning (ML) is the application of linear statistical methods to complex, non-linear data. The data—while not separable in the original data space—can be linearly separated after transformation into a higher dimensional space. The key advantage from kernel methods is the use of the ‘kernel trick’, where one does not need to explicitly compute the data embedding. This trick has found its way into many applications such as ML classification, regression, and clustering[26].

Given some data from the input space,, akernelis a function,(whereis either the real,, or complex numbers,), such that for all,

whereis the feature map, which encodes the data into a Hilbert space,.
We usually take the feature map such that the data is not linearly separable inbut is in—commonly achieved by takingto be a higher dimension than. The kernel,uses the inner product in our feature space to form a measure of similarity between data.

By Mercer’s theorem, any function that is (conjugate) symmetric and
positive semi-definite111More precisely:for all., i.e.,and any

is a kernel, and there exits a mappingsuch that Eq. (2) is holds. This is the crux of the kernel trick: the function,, can be computed directly without ever needing to computeor even know what itis.

For simplicity, we will describe the supervised learning case. We are given some labelled data set,and aim to find a mappingfor new unlabeled data points, whereis determined by some structure, pattern or probability distribution within the data. The solution to this learning problem is given by,

where we defineas the loss function characterising the performance of the learned function andis the Hilbert space of learning functions we are considering[36].is a monotonically increasing regularisation function to reduce overfitting, favouring a smooth function with better generalisation.

Associated to every such feature space is a unique Reproducing Kernel Hilbert Space (RKHS). This is a space of functions that can be constructed as the completion of the span of kernels,

wherehas the reproducing property:

The construction of the RKHS permits a solution - by the representer theorem - to equation4given by,

for some[37].

Formal analysis of classical kernel functions allows one to characterise three key quantities: learnability, expressivity and generalisation. Learnability describes how well the optimal kernel as defined in equation7can be found as a function of the size of the problem. Expressivity is used to measure the complexity of problems kernels can linearly classify. If a kernel is universal (i.e. perfectly expressive), it can precisely separate any two given sets from a compact metric space of finite training data[36]. All universal kernels are also characteristic, and can thus be utilised in probabilistic ML applications[38]. In section5we provide an example of a characteristic CV quantum kernel.

Generalisation theory aims to assess the quality of the learning scheme. Generalisation bounds provide a measure of how well the kernel - given some finite data set from a distribution- can be applied to randomly sampled data fromthat was not in the initial data set. The boundedness of evaluational functions in any RKHS,yields generalisation bounds given by

is the size of the labelled data set andis a function of the specific kernel and the loss function from equation4[39]. Such approaches are often defined in terms of VC dimension or fat-shattering dimension[40], however, these bounds all represent worst case scenarios and have limited practical relevance. In practice, bandwidth hyperperameter tuning, which essentially changes the length scale of the kernel, is most often used to improve generalisation. If bandwidth is too small, the kernel will treat most new data points as very far from any training observation, while a bandwidth that is too large creates a kernel that will treat each new data point as nearly equidistant to all training observations. Neither will result in good generalization and clearly bandwidth tuning can have a profound impact on learnability and generalisation. Such tuning is similarly computationally very expensive, although recent techniques utilising Jacobian control have shown some improvements[41].

SECTION: 2.3Background on quantum kernel machine learning

Recently, formal similarities between kernel methods and quantum machine learning (QML) methods have become well established[1]. Essentially, QML methods encode data non-linearly into a higher dimensional Hilbert space in which quantum measurement defines a linear decision boundary. For example, in supervised machine learning we can encode our data in the Hilbert space of the quantum system asand then learn the measurement that optimally separates the data. Typically, this state is prepared with some unitary gate operatorthat acts on the vacuum statesuch that. The kernel function is then defined using the Hilbert-Schmidt inner product as,

An important subtlety is that in quantum kernels, we no longer use the same “kernel trick” as in the classical case; kernel entries are not evaluated using a closed form function,, on the original data. They are instead, approximated via the quantum measurement of. This also means that the RKHS of quantum kernels is rarely characterised, though the existence of the quantum feature map directly implies the existence of a RKHS.

While the majority of quantum kernels are characterised using the qubit circuit formalism, several specific examples of CV quantum kernels exist. Schuld’s excellent summary paper includes a description of a coherent state kernel encoding[2]and Tiwari et. al. construct a mathematical representation of coherent quantum kernels using generalised hypergeometric functions[30]. Ghobadi presents a single-mode squeezed and a single photon (Fock) state quantum kernel and derives a non-classicality witness - a necessary but insufficient condition for quantum advantage - for each[29], and Bowie et. al. describe an experimental platform which exploits Hong–Ou–Mandel interference to evaluate a kernel based on a temporal encoding[32]. There is to date, however, no unifying framework from which to understand these individual results. In the following section we introduce the relevant background on holomorphic representations of CV quantum states to understand our approach.

SECTION: 3CV quantum kernels

SECTION: 3.1Representing CV quantum states as holomorphic functions

Quantum information processing (QIP) is often separated into two paradigms: discrete variable QIP and continuous variable QIP. The former utilises finite dimensional Hilbert spaces and qubits or qudits, whereas the latter utilises infinite dimensional Hilbert spaces and qumodes. In the discrete case, non-Clifford operations or magic states are identified as necessary for bounded error quantum polynomial (BQP) complete, non-classically simulable, computation[42]. Analogously, non-Gaussian operations or non-Gaussian states are identified as necessary for BQP-complete computation in the CV setting[43]. In recent work, Chabaud et. al. present a measure of non-Gaussianity which permits a more nuanced quantification of the computational power of CV quantum computing platforms[33]. In this approach, CV states are fully characterised by holomorphic functions. Such functions can be thought of as quasi-probability distributions, similar to Wigner functions or Husimi functions. In the CV case, we can decompose a finite rank holomorphic function as a stellar function - a product of a Gaussian and polynomial in. The polynomial is characterised by its roots and accounts for the non-Gaussianity of the quantum system[33].

For single bosonic modes, with orthonormal basis, we can encode our state using the canonical coherent states as,

One can treat these as phase-space wave functions of a corresponding quantum state. Instead of representing quantum states as infinite countable vectors as seen in the Fock state description, they can be characterized as holomorphic functions through the transformation,

for all. Hence a particular quantum state, decomposed into its Fock basis ascan be transformed as,

which is called the stellar function of the state. This stellar function corresponds to an expansion as a sum in the overcomplete basis of Glauber canonical coherent states[33]. Using the Hadamard-Weirstrass factorisation theorem, we can rewrite these stellar functions as

where the constants,are each dependent on. Here,is given as the so-calledstellar rankof the function, which provides a notion of quantumness as Gaussian states have a stellar rank of zero. For stellar functions of finite rank (i.e. finite roots of the polynomial,), we can write our function as separable in Gaussian and polynomial functions as,

This decomposition can be written as,

for, which is the form that we will use in the remainder of the paper.

Stellar functions live in the Segal-Bargmann space, the separable infinite-dimensional Hilbert
space of holomorphic functionsover, satisfying the normalization condition,

which constrainsin Eq. (15). The SB space has the inner product,

In the SB space, our operators are functions of the creation and annihilation operators acting on the Hilbert space of our quantum states, and are mapped to differential operators in the SB space by,

whereacts on a holomorphic function by multiplying it byandtakes the partial derivative of it with respect to. It follows that any unitary evolution acting on an element of the SB space remains within the space.

Common examples of zero stellar rank functions are vacuum states, coherent states, squeezed states and two-mode squeezed states. Fock states ofparticle number have stellar rank. Important properties of the stellar rank as a measure of non-Gaussianity include the fact that it is conserved under Gaussian operations, that the states of finite stellar rank form a dense subset of the SB space, and that operationally one can climb the hierarchy by acting on a given state with a creation operator. States with infinite rank, such as GKP[44]or cat states, are outside the stellar hierarchy and do not have an obvious measure of quantumness. However, they can be approximated with arbitrary precision using finite-rank states[45].

We will next use these representations of CV quantum states to develop analytic representations of CV quantum kernels.

SECTION: 3.2CV quantum feature maps

Given some metric space of our datawe can define our CV holomorphic kernel as follows. Firstly, let us encode our datato a pure quantum state which we then decompose into its Fock basis,

Using the transformation from equation11we yield,

This allows our data to be encoded into some continuous variable state via holomorphic function,
which forms our data encoding,

From this, we provide a natural definition of the CV quantum kernel as,

We note that this function is positive semi-definite (appendixA) and symmetric[2]and thus a valid kernel.

The evaluation of such a kernel can be shown to be equivalent to a CV sampling computation[34]. Such computations can be exactly simulated intime provided there are two or more modes[34,35]. Using this notion of computational hardness we extend the concept of stellar rank from quantum states to the kernel itself.

Finally, we note that one can define the RKHS as the completion of the span of the kernel function for some data set. We see that the Segal-Bargmann space can be understood as the RKHS of the Gaussian kernel which itself is universal (appendixB).

SECTION: 4General CV kernels

We begin our analysis of CV quantum kernels by considering the general multi-mode case, as any quantum advantage will require.

A general-mode state of total stellar finite rankcan be represented by the holomorphic function

where,is a Gaussian andis a polynomial[33].

In general,

wherewith componentsand,with components,andwhich are labeled by the vector222In the case ofand, the polynomial will be.. The actual values of these components will depend on the particular choice of encoding, and will therefore be functions of the data.

Explicitly, for some, the feature map is, for,

and the quantum kernel is

Calculating this inner product (see appendixC.1) requires the evaluation ofintegrals, each of the form

where

are constants, which depend on the integer values ofand.

This can be done algorithmically, (see appendixD), and we are able to obtain a closed form expression for the general-mode kernel

where

and,,, andare defined recursively as

which depend on the initial encoding ofandand

The initial values of these parameters are defined in appendixD(Eqs. (83), (84), and (86)).

While the detail of this kernel function is opaque, we note that any CV encoding of a finite stellar rank will always have a kernel of the form of Eq. (29), and can be expressed as a product of a Gaussian and an algebraic function of the parameters of the feature map,,andgiven in Eq. (25). Specifically, the algebraic function is a solution to the polynomial equation of the form. Furthermore, as the modulus squared of the inner product between two physically encoded CV quantum states is always bounded between 0 and 1, this means the kernel is always finite. Mathematically, we can also see this by noting that Eqs. (23) and (24) are defined on the SB space, the inner product is bounded.

Given this structure, we can infer some general properties of the CV kernel. The Gaussian term will cause exponential suppression of kernel values beyond a certain threshold, and while this threshold can be manipulated by bandwidth tuning, this will most likely be at the cost of effective stellar rank. In section5, we will show that this is indeed the case for the specific case of the single mode displaced Fock state kernel; however we stress the details will differ for each specific encoding.

In general, when, the strong classical simulability of the general multi-mode kernel will scale exponentially on the order of[34,35]. We also find that in the case of, calculating Eq.  (29) also scales exponentially in, the mode of the encoding, given that. The scaling withcomes from the general form within the product of sums,

representing a deeply nested sum of depth. Due to the recursion relation of the’s, (Eq. (32)), each of the sums within this product are dependent on the index of prior sums and the total length of each sum also increases as a function of. It can be easily seen that the computational complexity ofnested sums each of lengthscales as. As such, we see that our general kernel’s classical simulability scales as at leastfor some.

This is a useful property as in practice; an easy way of increasing the stellar rank of a CV quantum feature map is to increase the number of modes rather than directly increasing the stellar rank of a single mode.

Finally, we consider the case of infinite stellar rank. CV states of infinite stellar rank can be approximated arbitrarily well in trace distance[33]:

where we useto denote a state of infinite stellar rank andto denote a state of finite stellar rank. Since we only consider pure states, the trace distance can be easily expressed in terms of the inner product:

From this fact, it can be shown (details in appendixE) that given two states of infinite stellar rankand, and two states of finite stellar rankandsuch that

for some small, then

In other words, kernels defined by CV feature maps of infinite stellar rank can be approximated arbitrarily well by kernels of the form of Eq. (29), which is defined by feature maps of finite stellar rank.

SECTION: 5Displaced Fock state kernel

In this section, we proved a simple example of a CV quantum kernel.
We construct an analytic expression for a kernel generated from a displaced, single-mode
Fock state encoding which we show this is rotationally and translationally invariant, radial and characteristic.
While for a single mode encoding, we do not have exponential growth in simulablity with stellar rank, we will use stellar rank to generate intuition for the multi-mode encoding.

SECTION: 5.1Closed form & analytic properties

We first consider a simple single-mode bosonic kernel which encodes data through a general Fock statewith an applied displacement unitary. To encode 2 pieces of information, we parameterise the displacement operator by the complex number. This operator acts on any holomorphic function as

Since displacement is a Gaussian operation, it will not change the stellar rank of; therefore the encoded statewill have a stellar rank of. Explicitly, the encoded state is

and with this encoding, the quantum kernel is

where

After the integration and some algebra (see appendixFfor details), we can write down the displacement kernel in closed form as:

where the’s are defined in Eq. (28).
In appendixF.2, we list the explicit form of this kernel for the first 9 Fock states, which can be used to directly calculate the kernel entries via the kernel trick. However beyond, and for more complex kernels, classical simulation quickly becomes intractable.

From Eq. (42), it is clear that the displacement kernel is the product of a Gaussian and a polynomial of degreein bothand, because

Using this closed form expression we are also able to show that the kernel is translation (shift) and rotation invariant (see appendicesF.3andF.4). From these properties, we find that (see appendixF.5)

and so it is a radial kernel. This combined with the fact that the Fourier transform of this kernel is also the product of a Gaussian and a polynomial of degree, which has support over the entire Fourier domain, except at a finite number of points (see appendixF.6), means that the displaced Fock state kernel is a characteristic kernel[46].

In order to generate some intuition as to how such a kernel will behave, in figure1we plot the displaced Fock state kernel function,, for training data chosen from a uniform distribution offrom 0 to 8, for various values of the stellar rank,. We see in the figure that as the value ofincreases, so does the number of zeros in the kernel function, as expected. Additionally, as the stellar rank increases, the kernel’s ability to distinguish between large distances in the original data space,, improves. For example, thekernel function will evaluate as zero for any distancegreater than 4, whereas thekernel will have non-zero values up to a distance of 6.

We can also see that for kernels of finite stellar rank, due to multiplication by the Gaussian factor, there will be a threshold value beyond which all kernel evaluations will be exponentially close to zero. Additionally, it appears that as stellar rank increases, the amplitudes of the maxima diminish. This would suggest that as the stellar rank increases, kernel values outside the central peak will become increasingly suppressed.
This is further supported by the fact that the displaced Fock state kernels integrate to a constant value,(appendixF.7), which limits how large each maximum can be.
Given these two results, we would expect, therefore, that displaced Fock state CV kernels of high stellar rank will generate kernel values that are increasingly concentrated at low values, a feature that we observe in the learning experiments conducted below. Given kernel values are always statistically approximated by repeated measurement, as kernel values become smaller they require more measurements to remain distinguishable. Consequently, we expect high stellar rank models with low kernel variance will also become increasingly vulnerable to shot noise.

SECTION: 5.2Bandwidth tuning

This observation has some of the flavour of exponential concentration and recent work has proposed bandwidth tuning as a possible mitigation technique[24]. Let us therefore consider the consequences of implementing a bandwidthon the encoded data by taking

whereis a real hyperparameter. Physically, this corresponds to a reduction in the value of the displacement of the Fock state,, which results in a non-linear transformation of the kernel.

In figure2, we explore the effect of the bandwidthon the value of the displaced Fock state kernel function. We take some data, which we choose to be a uniform distribution offrom 0 to 6, and apply a bandwidth so that

We find that for this particular choice of distribution, a bandwidth ofmoves all of the data away from the tail of the function (where values are exponentially close to zero). For example, all data separated in the original space by valuesbuthave kernel values of near zero for(left figure) but for a bandwidth of, we can see these data points can now be discriminated. Unfortunately, it is also the case that data points that were easily distinguishable for, such as the two points closest to the y axis, are less distinguishable following bandwidth tuning to. Furthermore, we also see that this bandwidth reduces theeffectivestellar rank of the kernel function, since there are now onlymaxima, rather thanover the range of the distribution (although we note that the actual stellar rank of the kernel function does not change, since it is still defined for all). On the other hand, a bandwidth is, moves more of the data into the tail of the function as compared with no bandwidth, exponentially suppressing data with values larger than. In the case of a very small bandwidth,, the kernel function is effectively a Gaussian. In conclusion, it is reasonable to expect that the hyperparameter,will need to be carefully chosen for each problem.

SECTION: 5.3Learning experiments

In this section we present some performance metrics and decision boundaries for implementations of a range of displaced Fock state kernels. Inspired by previous work showing the advantage of matching the structure of kernel with that of the data[17,16,28], we create a task to exploit the underlying structure (radial symmetry) of the displaced Fock state kernel333Our source code can be found athttps://github.com/rishigoel2003/Quantum-Kernel-Machine-Learning-with-Continuous-Variables. The task is a supervised learning classification task using an annular data set constructed by combining multiple instances of the Scikit-learn data set method,make circles. We combine 3 instances of the method, each with different parameters to yield 3 concentric circles of data with binary labels. We define 500 data points in the data set for each set of circles, and an equal number for each label. We create three data sets: one with circles that are close together and a small amount of noise (0.05), one which modifies the first by flipping the labels above theaxis, and one which modifies the first by increasing the separation between the circles and adding more noise (0.3). The three data sets are illustrated in figure3. When we increase the separation between the circles in the third data set, we maintain the ratios of radii of the blue and red circles as 0.3 for the inner circles, 0.8 for the middle circles and 0.9 for the outer circles. In all three learning simulations, the train-test split is the default 75%, 25% split.

First, we test on data set 1.
In figure4we plot the test data and decision boundaries and accuracy for five kernels, three displaced Fock state kernels with, the Scikit-learn Gaussian kernel without hyperparmeter tuning and the same Gaussian kernel which has been tuned using Bayesian optimization. We do not perform any tuning on the displaced Fock state kernels. We see that the accuracy of the displaced Fock state kernels improves with increasing stellar rank (first three panels) and theandkernels significantly outperform the default classical Gaussian kernel (fourth panel). However, when the Gaussian kernel has been tuned, it is also able to classify the data to a high degree of accuracy (fifth panel). Tuning the Gaussian kernel via Bayesian optimisation to achieve this accuracy takes a significant amount of computational time, while the displaced Fock state kernels require no hyperparameter tuning, suggesting that these quantum kernels are better suited for this task.

In data set 2, we add an additional complexity by flipping the labels in data set 1 above theaxis, and again test on five kernels: the displaced Fock state kernel with stellar rankwith no hyperparameter tuning, and the Gaussian kernel without and with hyperparameter tuning. The decision boundaries, test data and accuracy of these kernels is plotted in figure5. Similar to the tests on data set 1, we find that the accuracy of the displaced Fock state kernels improves with increasing stellar rank (first three panels), since higher stellar rank kernels can identify finer structure. In particular, theandkernels were well suited for this learning task. Thekernel had very poor accuracy, so it was omitted from this figure. We also found that the Gaussian kernel also performed well, but again only after tuning via computationally expensive Bayesian optimisation (fourth and fifth panel), suggesting it is less suited for such a classification task.

Next, we consider the effect of the bandwidth hyperparmeter on the displaced Fock state kernels. In figure6, we use data set 1 to determine the effect of the bandwidth on the accuracy of thedisplaced Fock state kernel, which under-fits the data.
Increasing the bandwidth from 1 to 1.5 results in an improvement in accuracy from 62% to 97%. However, increasing bandwidth to 15 results in over-fitting and test accuracy starts to decline again.

In figure7we use data set 3 to determine how the bandwidth affects generalisation for noisy data. We see that for thewhile bandwidth tuning can improve the performance from 60% to 63%, the decision boundary becomes closer to that of a lower rank kernel (last panel). Continued bandwidth tuning eventually results in a kernel that approximates an un-tuned Gaussian kernel. This corroborates the theoretical behaviour we illustrated in figure2: decreasing the bandwidth decreases the effective stellar rank of the kernel.

Overall, these learning simulations demonstrate the displaced Fock state CV kernel is well suited to learn on data sets whose structure matches the symmetry of kernel function (e.g. annular data). Additionally, tuning the bandwidth hyperparemeter can improve accuracy if the model is under-fit, or the data is noisy.

SECTION: 6Qudit kernels

Thus far we have only examined the CV case and it is interesting to ask if any of our results are applicable in the more familiar discrete qubit/qudit case. In fact, qudits of dimensioncan also be represented as complex polynomials in the SB space as[33]

whereis a normalization factor, which only depends onandand ensures that

In appendixG.1, we show Eq. (47) requites that

up to a global phase.

We note that a tensor product ofqudits, each of dimension, can always be written as a single qudit of dimension, so we will only consider the single-mode case.

As with the CV kernels, we consider the qudit kernel to be

where the dataare encoded into the the statesandrespectively. In the case of mixed states, where the data is encoded into a density matrix, we will consider the vectorisation, which stacks the columns of the matrix to form as single vector[1]. The vectorised state can then be represented as Eq. (46).

With the encoded qudits represented as polynomials in the SB space, we can calculate the inner product as (see appendixG):

as one would expect.

In appendixG.2, we show this same expression can also be calculated from the general multi-mode kernel (Eq. (29), by setting,and, showing thatallquantum kernels that can be written as Eq. (9) can be written as either an algebraic function, a Gaussian, or the product of a Gaussian and an algebraic function.

SECTION: 7Conclusions & future work

In this paper we use the holomorphic representation of continuous variable quantum states to present a closed form kernel which describes how one might encode data for CV quantum kernel machine learning. In doing so, we are able to identify that all quantum kernels of finite stellar rank can be expressed analytically as products of Gaussian and algebraic function of the parameters of the feature map,,,and. In the case of two or more modes,
the measure of stellar rank, a quantity that is easy to characterise for practical bosonic implementations, neatly captures the classical hardness of simulating these kernels, which scales as. We then prove kernels defined by feature maps of infinite stellar rank, such as GKP-state encodings, can be approximated arbitrarily well by kernels defined by feature maps of finite stellar rank. Furthermore, by analysing a simple single mode CV kernel which are expressed as the product of a Gaussian and a polynomial, we are able to develop intuition for how multi-mode kernels will behave as we increase their “quantumness” as measured by stellar rank. We see that it is likely that one will encounter problems analogous to exponential concentration, and while bandwidth tuning may mitigate this to some extent, this will trade-off with maintaining effective stellar rank and robustness to shot noise. We have also shown that it is possible to construct characteristic CV quantum kernels that embody both translational and rotational invariance, with an explicit example given as a kernel constructed by a displaced Fock state. We leverage this structure, by creating a supervised learning classification task on annular data. We find that displaced Fock state kernel is well suited for this task, and the classification accuracy increases with increasing stellar rank.
While we have not simulated any multi-mode kernels here, we would expect similar behaviour in terms of kernel value suppression beyond a threshold value due to the Gaussian term, and also increasing stellar rank.

It will be important in future work to quantitatively characterise the bandwidth generated trade-offs for particular target problems and specific physical implementations. We also leave for future work an analysis to consider the effects of various appropriate noise models. Of relevance is recent work employing Kerr kernels derived from encoding data into the phase and amplitude of Kerr coherent states to construct Kerr quantum kernels. Empirical analysis demonstrates the robustness of Kerr coherent states to various noise models and the authors attribute this to their flexibility in accommodating different hyperparameters[47]. Finally, we note a concurrent work has recently appeared that uses phase space negativity to estimate the efficiency of the classical simulation of quantum kernel functions for bosonic systems[48]. It will be interesting in future to understand the connection between this work and the results presented here.

SECTION: Acknowledgements

This work has been supported by the Australian Research Council (ARC) by Centre of Excellence for Engineered Quantum Systems (EQUS, CE170100009). We wish to thank Aleesha Isaacs, Carolyn Wood, Riddhi Gupta, Gerard Milburn, Andrew White and Nicolas Menicucci for useful discussions.

SECTION: References

Supplementary Material

SECTION: Appendix AProof that inner products are positive semi-definite

Inner Products of the formare positive semi-definite.

A matrixis positive semi-definite if and only if

We can construct our Gram Matrix,, from our defined inner product by

for.

Hence to show the inner product is positive semi-definite, it is sufficient to show the gram matrix is positive semi-definite. We can see for any,

Note that by definition, inner products are also conjugate symmetric. That is,

SECTION: Appendix BSegal-Bargmann space is a RKHS of the Gaussian kernel

Our formulation of CV quantum kernels heavily utilises the Segal-Bargmann space. Knowing properties about this space can allow us to further develop our kernels. Here we explicitly prove that this space contains the Gaussian kernel which is well known to be universal. We can also show that this kernel is universal. Universality of kernels is a measure of their expressibility, characterising how well the kernel can classify the data from a metric space. A universal kernel is one which can learn any function for empirical loss minimisation from equation4. To prove such a property we can show that the RKHS is dense inside the space of continuous functions () of our data. This is equivalent to showing that the kernel, decomposed into a power series of inner products, has only positive coefficients[49].

The following proof is adapted from[50]. Let us denote our reproducing kernel asfor some fixed. Hence by the reproducing property (Eq.(6)) we have,

By the definition of the inner product in the SB space we can rewrite this as,

Here we can see that the reproducing kernelis the kernel function for the SB space. By definition (Eq. (59)) we have,

and so we yield,

In terms of any complete orthonormal set,we can write the evaluational functional as,

We know that strong convergence (a proven property of the SB space) implies pointwise converge which yields,

regardless of which orthonormal basiswe choose. If we specify a specific basis, namely,

whereis a monotone increasing sequence of integers, we find,

as the Gaussian reproducing kernel. It is well known that the Gaussian kernel, expanded into a power series in the inner product of our data space, has positive coefficients. As such, the SB space is an RKHS with a universal kernel. This is useful as Gaussian kernels are the foundation of classical machine learning and through the SB space, our quantum kernel has direct access to this kernel[50,51].

SECTION: Appendix CAn integration required to calculate the closed form of CV kernels

In this section we will calculate the integral

forwithand.
Since CV quantum states, when represented as holomorphic functions, are always a product of a Gaussian and a polynomial, the evaluation of any CV quantum kernel will require the evaluation of integrals of the form of Eq. (67).

SECTION: C.1Explicit calculation

The explicit evaluation of Eq. (67) is

whereis the confluent hypergeometric function, which have the property that for:

whereis the Pochhammer symbol

We explicitly prove Eq. (69) in sectionC.2.

By considering the cases of evenand oddseparately, Eq. (69) can be used to simply Eq. (68) and write it as a product of a Gaussian and a polynomial.

This will use of the fact that for

Whenis even:

where

and whenis odd:

where

Therefore, combining Eqns. (71) and (73) gives the general result:

where

SECTION: C.2Proof the confluent hypergeometric functionis a product of an exponential and a polynomial

In this section, we explicitly prove Eq. (69) and show a confluent hypergeometric function of the formcan be written as a product of an exponential and a polynomial of degree.

This proof requires two properties of confluent hypergeometric functions[52,(13.6.1),(13.3.4)]

Proposition:
For, the confluent hypergeometric function

whereis the Pochhammer symbol.

Proof(by induction):

The base case foris given in Eq. (77a).

Assume that Eq. (78) holds forand consider thecase

Therefore if Eq. (78) holds forthen it holds for.

By the induction rule, Eq. (78) holds for all.

SECTION: Appendix DExplicit calculation of the general-mode CV kernel

In this section, we provide the details of the calculation of the general-mode kernel (Eq. (29)).

The first step in calculating the-mode kernel is to rewrite Eq. (26) to become a series of nested integrals each of the form of Eq. (75).

Starting from the general-mode inner product,

Unsurprisingly the integrand is also of the form

i.e. the product of a Gaussian and a polynomial.

We will setfor each. After some algebra, which we detail below, this substitution will allow for Eq. (80) to be written as a nested set of integrals, each of the form of Eq. (75).

First simplify the Gaussian part of the integrand:

where
for odd

and for even

Next, simplify the polynomial part of the integrand:

where

Therefore, the-mode inner product can be written as:

which is in the required form.

Now we can carry out the integrations starting with theintegral, which is of the form Eq. (75):

Next, the goal is to simplify the expression so that theintegral is of the form of Eq. (75).

Start by simplifying the exponential part:

where

Then use the multinomial expansion,

to simplify the polynomial part:

where for

Therefore the-mode inner product becomes

where the last equality results from following a similar simplification as was done after the integration with respect toand we define:

The nextintegrals can be evaluated iteratively following the same steps as above which results in the following closed form of the-mode inner product:

where

are defined recursively and the’s are defined in Eq. (28). We note that this expression the product of Gaussian and an algebraic function in the parameters of the feature map:,,,

SECTION: Appendix EApproximating CV kernels of infinite stellar rank

In this section we will show that kernels formed from pure states of infinite stellar rank can be approximated arbitrarily well by kernels of finite stellar rank.

CV states of infinite stellar rank can be approximated arbitrarily well in trace distance by states of finite stellar rank[33]. That is

where we useto denote a state of infinite stellar rank andto denote a state of finite stellar rank. Since we are considering pure states, the trace distance can be easily expressed in terms of the inner product

Now since

whereis the phase, we can define a new state with the same stellar rank asas

so that

Also note that for any state,

and

Next stating from

we will bound the inner product between the state of infinite stellar rank and another state of finite stellar rank.

Therefore

Similarly, we can show

Therefore

and combining Eqs. (107) and (109) gives

Similarly, for the state of infinite finite stellar rankthat is close in trace distance to the state of finite stellar rank:

Eqs. (110) and (111) can be combined to bound on the inner product
of two states of infinite stellar rank,

Therefore

where

And

Therefore

and combining Eqs. (113) and (116) gives

Finally, using the fact that

it can be shown that

and

so

In other words the kernel defined by CV states of infinite stellar rank can be approximated arbitraily well by a CV kernel of finite stellar rank

SECTION: Appendix FProperties of the displaced Fock state kernel

In this section we show the details of the derivation and properties of the displaced Fock state kernel (Eq. (42)).

SECTION: F.1Derivation of Eq. (42)

The displaced Fock state inner product is calculated from Eq. (41) by first settingand applying the trinomial expansion so that theandintegrals can be written in the form of Eq. (75).

Now using Eq. (75), the displacement inner product can be directly written as:

which is the product of a Gaussian and a polynomial of degreeinand.

SECTION: F.2Explicit examples of the displaced Fock state kernel

In this section we write down the explicit form of the first nine displacement kernels.

SECTION: F.3Showing the displaced Fock state kernel is translation invariant

A translation invariant kernel has the property that

For the case of the displaced Fock state kernel, the vector, and we define. Now the shifted kernel is

where

Change variables, so the inner product is now

Therefore:

and the displacement kernel is translation invariant.

SECTION: F.4Showing the displaced Fock state kernel is rotation invariant

A rotation invariant kernel has the property that

For the case of the displaced Fock state kernel, the rotation matrix is

Under this transformation, the complex number, so the rotated kernel is

where

Change variables, so the inner product is now

and
the displaced Fock state kernel is rotational invariant.

SECTION: F.5Showing the displaced Fock state kernel is a radial kernel

From these translation and rotation invariance of the displaced Fock state kernel, can also show that

and therefore

it is a radial kernel.

Furthermore, the polynomialis a polynomial of,,and, but, there is no way of constructing

out of such a polynomial. However there is a way of constructing

and therefore

SECTION: F.6The Fourier transform of the displaced Fock state kernel

Now define, and use the facts that the displaced Fock state kernel is

and is the product of a Gaussian and a polynomial of degreein, to write is as

for an appropriate choice of.

The two-dimensional Fourier transform can be easily calculated as

which can be simplified further using the hypergeometric identity in appendixC.2

which is also the product of a Gaussian and a polynomial of degreein.

SECTION: F.7Showing the displaced Fock state kernel integrates to

In this section, we calculate the displaced Fock state kernel from the operator definition:

Starting from

we calculate the inner product as

and the displaced Fock state kernel

Note that this is a much simpler form than that which we found Eq. (42), indicating that there may be a way to significantly simplify the general multi-mode kernel Eq. (29). We leave this exploration for future work.

Now defineand integrate the kernel over all

The binomial coefficients can be further simplified by using the following properties for,, and[53]:

where

is the falling factorial.

With these properties, the integral becomes:

Since, the falling factorial becomes

which is zero unless, so

for all.

SECTION: Appendix GCalculation of the qudit kernel

In this section we show the details of the calculation of the qudit kernel (Eq.(50)),

In the function representation, the inner product of two qudits is

SECTION: G.1Calculating the value of

Recall, that in the case when,,so

where the second equality results from the definition of the qudit stellar function (Eq. (47)).
Therefore,

so we choose

SECTION: G.2Calculation of the qudit kernel from the general multi-mode kernel

In this section, we calculate the qudit kernel from the general multi-mode kernel (Eq. (29)).

First we note that when

Since a qudit can always be represented as a function of a single complex variable, we set. Additionally from Eq. (46) we find that

and by the recursion relations (Eq. (31))

Now the inner product becomes

where in the second line, we substitute in the recursion relation for(Eq. (32)) and in the fourth line we substitute in the values ofand(Eq. (86)).

From Eq. (28),

and so

where in the second line, we use the fact that for,

Clearly, this matches Eq. (154) forand.