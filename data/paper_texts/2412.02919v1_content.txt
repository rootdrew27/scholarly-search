SECTION: Higher Order Transformers: Efficient Attention Mechanism for Tensor Structured Data
Transformers are now ubiquitous for sequence modeling tasks, but their extension to multi-dimensional data remains a challenge due the to quadratic cost of the attention mechanism. In this paper, we propose Higher-Order Transformers (HOT), a novel architecture designed to efficiently process data with more than two axes, i.e. higher order tensors.
To address the computational challenges associated with high-order tensor attention, we introduce a novel Kronecker factorized attention mechanism that reduces the attention cost to quadratic in each axis’ dimension, rather than quadratic in the total size of the input tensor. To further enhance efficiency, HOT leverages kernelized attention, reducing the complexity to linear. This strategy maintains the model’s expressiveness while enabling scalable attention computation.
We validate the effectiveness of HOT on two high-dimensional tasks, including long-term time series forecasting, and 3D medical image classification. Experimental results demonstrate that HOT achieves competitive performance while significantly improving computational efficiency, showcasing its potential for tackling a wide range of complex, multi-dimensional data.

SECTION: Introduction
The Transformer architecturehas revolutionized sequence modeling across various domains, including computer vision, speech recognition, and reinforcement learning, due to its self-attention mechanism, which effectively captures long-range dependencies and complex patterns in sequential data. However, extending Transformers to handle higher-order data—such as multidimensional arrays or tensors—poses significant challenges due to the quadratic computational and memory costs of the attention mechanism, limiting their application in tasks involving high-dimensional inputs, such as video processing, multidimensional time series forecasting, and 3D medical imaging. High-order data are prevalent in many real-world applications, including climate modeling, which relies on multidimensional time series data capturing temporal and spatial variations; 3D medical imaging, which adds depth to traditional 2D images; and recommendation systems, where user-item interactions over time and context are modeled as multidimensional tensors. Efficiently processing such data requires models capable of capturing intricate dependencies across multiple dimensions while avoiding prohibitive computational costs.

Several efforts have been made to adapt Transformers for multidimensional data. A common approach is to reshape or flatten the multidimensional input into a sequence, effectively reducing the problem to a one-dimensional case. While this method allows the use of standard Transformers, it disregards the inherent structural information and local dependencies present in the data, as the positional encoding may also fail to communicate this information. Consequently, models may fail to capture essential patterns and exhibit suboptimal performance. Another line of research focuses on applying attention mechanisms along each dimension independently or in a sequential manner. For example, axial attentionprocesses data along one axis at a time, reducing computational complexity. As another example,applies spatial and temporal attention sequentialy. However, this approaches may not fully capture interactions between different dimensions simultaneously, potentially overlooking important cross-dimensional dependencies.

In this paper, we introduce, a novel architecture designed to efficiently process high-dimensional data represented as tensors. An overall view of our architecture is presented in Figure. Our key contributions are as follows:

We propose a Kronecker decomposition of the high-order attention matrix, significantly reducing the computational complexity.

To push the boundaries of efficiency, we integrate kernelized attention mechanism into our model reducing the complexity from quadratic to linear with respect to the input size.

We validate the effectiveness of HOT on two challenging high-dimensional tasks: long-term time series forecasting and 3D medical image classification. In addition, we provide comprehensive ablation study on various aspects of HOT.

The code of our method will be made publicly available.

SECTION: Related Work
In recent years, various strategies have been developed to make Transformers more efficient for high-dimensional data. One common approach is to flatten the input tensor into a sequence, as in the Vision Transformer (ViT), which treats image patches as tokens. However, this approach disregards the structural dependencies within the data. To better handle multidimensional structures, axial attention mechanisms like the Axial Transformerapply self-attention along each axis sequentially, reducing complexity but often missing cross-dimensional dependencies crucial for tasks like 3D medical imagingand climate modeling. Similarly, the Sparse Transformerreduces computation by attending to subsets of the input but struggles with global interactions.Tensorized Transformersutilize tensor decompositions to reduce memory usage but focus primarily on compression rather than improving cross-dimensional attention. Linear Transformersand Performerbypass the quadratic softmax bottleneck with linearized attention, making them scalable for long sequences but limited in capturing complex multidimensional relationships. Sparse methods like Longformerand Reformeralso reduce complexity by restricting attention to local neighborhoods, but they fail to handle global dependencies in higher-dimensional contexts. Recent works have further improved efficiency and cross-dimensional attention. iTransformeroptimizes for multivariate time series forecasting by reversing attention across variables, while Crossformeruses cross-dimensional attention to capture dependencies between spatial and temporal dimensions, specifically in time series tasks. CdTransformertackles the challenge of cross-dimensional correlations in 3D medical images with novel attention modules, reducing computational costs and capturing 3D-specific dependencies.

SECTION: Preliminaries: Tensor Operations
In this section, we introduce key tensor operations that are fundamental to the high-order attention mechanism. Vectors are denoted by lowercase letters (e.g.,), matrices by uppercase letters (e.g.,), and tensors by calligraphic letters (e.g.,). We useto represent the Kronecker product andto denote the tensor product along mode. The notationrefers to the setfor any integer.

Slices and fibers extend the familiar concept of matrix rows and columns to higher-dimensional tensors, providing powerful ways to analyze and manipulate multi-way data.

We conclude this section by stating a useful identity relating matricization, modeproduct and the Kronecker product.

These definitions establish the foundational operations on tensors, which we will build upon to develop the high-order attention mechanism in the next section.

SECTION: High Order Transformer
SECTION: High Order Attention
In this section, we first review the self-attention mechanism in Transformer layers, which we extend to higher orders by tensorizing queries, keys, and values, thereby formulating higher order transformer (HOT) layers.

Given an input matrixas an array of-dimensional embedding vectors, we form the query, key, and value matrices for each attention headas:

with weight matricesand output matrix, whereis the heads’ hidden dimension. The standard scaled dot-product attentionis defined by:

is theattention matrix and the Softmax function is applied row-wise.

Although scaled dot-product attention is widely used and has shown great promise across various domains, it comes with limitations that highly impact its scalability. In addition to its quadratic computational complexity, it is originally designed for 1D sequences and can not directly handle higher-order data (e.g., images, videos, etc.) without modification (such as flattening all the dimensions into one). These limitations motivate the development of high-order attention mechanisms that can efficiently handle tensor structured data with multipledimensions.

We now show how the full attention mechanism can be applied to higher-order inputs. Given an input tensor, whereare the sizes of the positional modes andis the hidden dimension, we start by generalizing the attention mechanism to operate over all positional modes collectively. We first compute the query (), key (), and value () tensors for each headby linear projections along the hidden dimension:

wheredenotes multiplication along the-th mode (the hidden dimension).

The scaled dot-product attention scoresare then given by

whereandare the materializations of the query and key tensors, and the Softmax function is again applied row-wise. Each positional index is considered as a single entity in the attention calculation. The output of the high-order attention functionis computed by applying the attention weights to the value tensor:

Lastly, the output is reshaped back to the original tensor shape.

While the high-order attention mechanism enables models to capture complex dependencies across multiple dimensions simultaneously, it suffers from significant computational and memory challenges. Specifically, the attention weight tensor scales quadratically with the number of positions, leading to the computational complexity of, which is impractical for large tensors. To address this issue, we propose a low-rank approximation of the high-order attention matrix using a Kronecker product decomposition. This approach dramatically reduces computational complexity while retaining the expressive power of the attention mechanism.

SECTION: Low-rank Approximation Via Kronecker Decomposition
We parameterize the potentially large high-order attention matrixusing a first-order Kronecker decomposition (Figure) of the form:

where eachis a factor matrix corresponding to the attention weights over the-th mode for head. Note that having a first-order kronecker decomposition does not mean thatis of rank one, as. Substitutingwith its approximation in Eq. (), we obtain

where each head independently considers one of the modalities.

While not all matrices can be factored into a single Kronecker product as in Eq. (), we show in Theorembelow that any attention matrix can be decomposed as a sum of such Kronecker products.
The summation across all heads appearing in Eq. () functions analogously to a rankKronecker decomposition, where the Kronecker rankcorrespond to the number of heads. The following theorem shows that a rankKronecker decomposition is capable of approximating any high-order attention matrix arbitrarily well, asincreases; ensuring that no significant interactions are missed. This theoretical aspect is crucial for ensuring that the attention mechanism can potentially adapt to any dataset or task requirements.

Proof is presented in the Appendix. 
∎

Now we delve into the computation of the factor matrices. As mentioned before, each matrixrepresents first-order attention weights over the mode. Thus, they can be computed independently using the standard scaled dot-product attention mechanism. Since the input to the attention module is a high-order tensor, computing first-order attention matrices require reshaping of the input query, key, and value tensors. We propose to use a permutation-invariant pooling functionsthat takes a high-order tensor as input and only preserves the-th mode and the hidden dimension. In this work, we consider summation over all modes except the-th and last one as the pooling function. We then compute the-th mode attention matrix

with pooled matricesandat a computational cost of.

Explicitly constructing the full attention matrixin Eq. () from the factor matriceswould negate the computational savings of the Kronecker decomposition. Instead, we exploit properties of the Kronecker product and associative law of matrix and tensor multiplication to apply the attention without forming. Formally, it is easy to check that

We can thus multiply each of the attention matrices one by one with the value tensor. The operation on each modeyields a computational complexity of, resulting in an overall complexity of. Thus, for an HOT layer withheads of width, the total complexity is.

While using factorized attention dramatically reduces the computational cost compared to naive high-order attention, the quadratic terms appearing in the final complexity reflect the inherent computational demand of the scaled dot-product attention mechanism, which can itself become substantial for large tensors. To mitigate this final challenge, we integrate kernelized linear attention into the proposed high-order attention mechanism.

SECTION: Linear Attention With Kernel Trick
Following the work by, we approximate the Softmax function in Eq. () using a kernel feature map:

whereis the diagonal normalization matrix serving as a normalizing factor. Substituting the Softmax function with Eq. () instead of the Softmax in the multiplication between the value tensorand factor matrixon moderesults in:

The choice of kernel functionis flexible, and we utilize the same kernel function as in, which has been validated both theoretically and empirically. In Eq. (), we simply used the associative law of matrix multiplication again to reduce the computational complexity of applying a first-order attention matrix on one mode fromtogiving us a final complexity of the proposed multi-head factorized high-order attention of. We include the pseudo code
for the whole HOT method in Algorithm.

SECTION: Experiments
We thoroughly evaluate HOT on two high order data tasks, validating the generality of the proposed framework. At each subsection, we introduce the task, benchmark datasets, and baselines used, and discuss the performance results. Implementation details are presented in the appendix. We close the section by reviewing ablation studies that further confirm our theory and design choices.

SECTION: Long-range Time-series Forecasting
Given historical observationswithtime steps andvariates, we predict the futuretime steps.

We extensively include 5 real-world datasets in our experiments, including ECL, Exchange, Traffic, Weather used by Autoformer, and Solar-Energy proposed in LSTNet. Further dataset details are in the Appendix.

We choose 11 well-acknowledged forecasting models as our benchmark, including (1) Transformer-based methods: ITransformer, Crossformer, Autoformer, FEDformer, Stationary, PatchTST; (2) Linear-based methods: DLinear, TiDE, RLinear; and (3) TCN-based methods: SCINet, TimesNet.

Comprehensive forecasting results are provided in Table, with the best results highlighted in bold and the second-best underlined. Lower MSE/MAE values reflect more accurate predictions. As seen in the table, our proposed method, HOT, outperforms all baseline models across all datasets, achieving the best MSE and MAE scores in every case. Specifically, HOT provides significant improvements on larger, more complex datasets such as ECL and Traffic, where capturing multivariate dependencies is critical. For smaller datasets like Exchange and Weather, HOT also outperforms baselines, but the gap between HOT and other models, like iTransformer, is narrower, which could be due to the smaller number of variates in these datasets, reducing the advantage of higher-order attention. Moreover, while other high-performing models like iTransformer and PatchTST deliver competitive results, they come with higher computational complexities. For example, iTransformer, with a complexity of, scales poorly with the number of time steps and variates, making it less efficient for large datasets. In contrast, HOT maintains a lower complexity of, which scales better with both dimensions and time, especially for higher-dimensional data. This efficiency is particularly important for large datasets such as ECL and Traffic, where HOT balances performance and computational cost, outperforming even models like FedFormer and Crossformer, which have similar or higher complexity. Overall, HOT not only achieves superior accuracy but also offers improved scalability and efficiency for multivariate time series forecasting tasks with much fewer parameters.

SECTION: 3D Medical Image Classification
Given a 3D imagewith width, height, and depth, we predict the image class probabilityover a set ofclasses.

MedMNIST v2is a large-scale benchmark for medical image classification on standardized MNIST-like 2D and 3D images with diverse modalities, dataset scales, and tasks. We primarily experiment on the 3D portion of MedMNIST v2, namely the Organ, Nodule, Fracture, Adrenal, Vessel, and Synapse datasets. The size of each image is(3D).

We choose 11 medical image classifier models including ResNet-18/ResNet-50with 2.5D / 3D / ACSconvolutions, DWT-CV, Auto-Keras, and Auto-sklearn, MDANet, and CdTransformer.

The results presented in Tabledemonstrate the superior performance of our Higher Order Transformer (HOT) across multiple medical imaging datasets. HOT achieves the highest accuracy and AUC on Organ, Fracture, Adrenal, and Vessel datasets, and the second-best performance in Synapse and Nodule showcasing its robust classification capabilities. While models like CdTransformer achieve better performance on Nodule and Synapse, they do so with a significantly increased computational complexity ofcompared to HOT’s. Additionally, HOT consistently outperforms other state-of-the-art methods such as DWT-CV and MDANet over both metrics, balancing high performance with lower computational demands and much fewer parameters.

SECTION: Ablation Study
To verify the rational business of the proposed HOT, we provide detailed
ablations covering analyses on rank of the attention factorization, attention order, attention type, and lastly, individual model components namely high-order attention module and the feed-forward module.

In this section, we evaluate the impact of the number of attention heads on the performance of HOT across both medical imaging and time series datasets. The attention rank, governed by the number of heads, plays a critical role in the model’s ability to capture diverse patterns across multiple dimensions by approximating the original high-order attention. We conduct ablation experiments by varying the number of heads to observe how it affects model accuracy and error rates. For 3D medical image datasets (FigureLeft), increasing the number of attention heads initially improves accuracy, however, after a certain threshold, performance declines. This drop is due to the fixed hidden dimension, which causes the dimension of each head to decrease as the number of heads increases, reducing each head’s ability to capture rich features and leading to less expressive attention mechanisms. For the time series datasets (FigureRight), the use of more heads improves performance, with the MSE consistently decreasing as the number of heads increases.

We conducted an ablation study to explore the effects of increasing the attention order on the performance of our proposed HOT. The attention order refers to the number of dimensions over which attention is applied, extending beyond traditional sequence-based attention to handle high-dimensional data effectively. We evaluate performance on both time series forecasting and 3D medical image classification tasks under different attention configurations. As shown in Tablesand, applying higher-order attention consistently improves the performance across all datasets, outperforming configurations with lower-order attention.Importantly, all models maintain the same number of parameters and the same computational and memory complexity for each dataset, highlighting that the performance gains are attributable to the increased attention order without adding. Details of memory consumption and training time are presented in the appendix.

SECTION: Conclusion
In this paper, we addressed the challenge of extending Transformers to high-dimensional data, often limited by the quadratic cost of attention mechanisms. While methods like flattening inputs or sparse attention reduce computational overhead, they miss essential cross-dimensional dependencies and structural information. We introduced Higher-Order Transformers (HOT) with Kronecker factorized attention to lower complexity while preserving expressiveness. This approach processes high-dimensional data efficiently, with complexity scaling quadratically per dimension. We further integrated kernelized attention for additional scalability and a complexity scaling linearly per dimension. HOT demonstrated strong performance in tasks such as time series forecasting and 3D medical image classification, proving both its effectiveness and efficiency. Future work could enhance HOT’s interpretability by analyzing attention maps or exploring alternative pooling methods for better information aggregation. Additionally, adapting HOT as an autoregressive model could enhance spatial and temporal coherency for generative tasks like video synthesis and climate forecasting.

SECTION: References
SECTION: HOT Algorithm
SECTION: Universality of the Kronecker Decomposition
Letbe the tensor obtained by reshaping the attention matrixinto a tensor of sizeand letbe the tensor obtained by merging each pair of modes corresponding to one modality. Letbe the CP rank ofand letbe a CP decomposition, wheredenotes the outer product and eachfor(see, e.g.,for an introduction to the CP decomposition). By reshaping eachinto a matrix, one can check that

from which it follows that, as desired.

The second part of the theorem comes from the fact thatis a well known upper bound on the CP rank of a tensor of shape(see again).
∎

SECTION: Datasets Details
SECTION: Long-range Time-series Forecasting
We evaluate the performance of the proposed HOT model on seven real-world datasets: (1) Exchange, which contains daily exchange rates for eight countries from 1990 to 2016, (2) Weather, consisting of 21 meteorological variables recorded every 10 minutes in 2020 at the Max Planck Biogeochemistry Institute, (3) ECL, which tracks hourly electricity consumption for 321 clients, (4) Traffic, collecting hourly road occupancy data from 862 sensors on San Francisco Bay area freeways between January 2015 and December 2016, and (5) Solar-Energy, recording solar power production from 137 photovoltaic (PV) plants, sampled every 10 minutes in 2006.

We follow the data processing and train-validation-test split protocol used in TimesNet, ensuring datasets are chronologically split to prevent any data leakage. For forecasting tasks, we use a fixed lookback window of 96 time steps for the Weather, ECL, Solar-Energy, and Traffic datasets, with prediction lengths of 96, 192, 336, 720. Further dataset details are presented in Table.

SECTION: 3D Medical Image Classification
We conduct experiments on the 3D subset of the Medical MNIST dataset. All datasets have an image size ofvoxels, allowing for consistent 3D image classification across different medical domains. The images come from various sources, ranging from human CT scans to animal microscopy, and have been adapted to create challenging classification tasks. Details are presented in Table.

OrganMNIST3D is based on the same CT scan data used for the Organ{A,C,S}MNIST datasets, but instead of 2D projections, it directly uses the 3D bounding boxes of 11 different body organs. The dataset is adapted for a multiclass classification on organ identification from volumetric medical data.

NoduleMNIST3D originates from the LIDC-IDRI dataset, a public repository of thoracic CT scans designed for lung nodule segmentation and malignancy classification. For this study, the dataset has been adapted for binary classification of lung nodules based on malignancy levels, excluding cases with indeterminate malignancy. The images are center-cropped and spatially normalized to retain a consistent voxel spacing.

AdrenalMNIST3D features 3D shape masks of adrenal glands collected from patients at Zhongshan Hospital, Fudan University. Each shape is manually annotated by an expert endocrinologist using CT scans, though the original scans are not included in the dataset to protect patient privacy. Instead, the dataset focuses on binary classification of normal versus abnormal adrenal glands based on the processed 3D shapes derived from the scans.

FractureMNIST3D is derived from the RibFrac dataset, which contains CT scans of rib fractures. The dataset classifies rib fractures into three categories (buckle, nondisplaced, and displaced), omitting segmental fractures due to the resolution of the images.

VesselMNIST3D uses data from the IntrA dataset, which includes 3D models of intracranial aneurysms and healthy brain vessels reconstructed from magnetic resonance angiography (MRA) images. The dataset focuses on classifying healthy vessel segments versus aneurysms, with the models voxelized into 3D volumes.

SynapseMNIST3D is based on high-resolution 3D electron microscopy images of a rat’s brain, with the dataset focusing on classifying synapses as either excitatory or inhibitory. The data were annotated by neuroscience experts, and each synapse is cropped from the original large-scale volume and resized.

SECTION: Implementation Details
The convolution encoder is a single 1D convolution layer with kernel size and stride both set to 4 applied on the temporal axis. This is equal to dividing the input timeseries into patches of size 4 and applying a linear projection to the hidden space of the model. Rotary positional encodingis used only for the time axis. The output of the transformer is pooled before being fed to the final MLP layer by either taking the average or flattening. We conduct forecasting experiments by training models on each dataset. Following the same split of training/validation/test sets as in, the model weights from the epoch with the lowest MAE on the validation set are selected for comparison on the test set.

The convolution encoder is implemented as a multilayer 3D convolution with a total downsampling by a factor of 4, while Rotary positional encoding is used for all three spatial dimensions. The output of the transformer is pooled before being fed to the final MLP classifier by either taking the average or flattening. We conduct classification experiments by training models on each dataset. Following the official split of training/validation/test sets, we train all models on the training sets for 100 epochs. The model weights from the epoch with the highest AUC score on the validation set are selected for comparison on the test set.

All the experiments are implemented in PyTorch and conducted on a single NVIDIA A100 GPU. We utilize ADAMwith an initial learning rate ofand L2 loss for the timeseries forecasting task and cross-entropy loss for the medical image classification task. The batch size is uniformly set to 32 and the number of training epochs is fixed to 100. We conduct hyperparameter tuning based on the search space shown in Table.

SECTION: Additional Results