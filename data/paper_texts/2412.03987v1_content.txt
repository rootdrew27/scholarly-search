SECTION: MTMT: Consolidating Multiple Thinking Modes to Form a Thought Tree for Strengthening LLM

Large language models (LLMs) have shown limitations in tasks requiring complex logical reasoning and multi-step problem-solving. To address these challenges, researchers have employed carefully designed prompts and flowcharts, simulating human cognitive processes to enhance LLM performance, such as the Chain of Thought approach. In this paper, we introduceMTMT(Multi-thinking Modes Tree), a novel method that interacts with LLMs to construct a thought tree, simulating various advanced cognitive processes, including but not limited to association, counterfactual thinking, task decomposition, and comparison. By breaking down the original complex task into simpler sub-questions, MTMT facilitates easier problem-solving for LLMs, enabling more effective utilization of the latent knowledge within LLMs. We evaluate the performance of MTMT under different parameter configurations, usingGPT-4o minias the base model. Our results demonstrate that integrating multiple modes of thinking significantly enhances the ability of LLMs to handle complex tasks.

MTMT: Consolidating Multiple Thinking Modes to Form a Thought Tree for Strengthening LLM

Changcheng LiXiangyu WangQiuju ChenUniversity of Science and Technology of ChinaXiren ZhouHuanhuan Chen

SECTION: 1Introduction

With the development of natural language processing, large language models (LLMs) have come to play a pivotal role in the field. There is hope that these models can solve most natural language problems and even achieve Artificial General Intelligence (AGI). However, despite increasing the amount of data and model parameters, the natural language capabilities of large models have not achieved the previously astonishing breakthroughs. Since the release of GPT-4(OpenAI et al.,2023), other large language models may have achieved better results in some areas compared to GPT-4, but there has been no significant change(Anthropic,2024; Reid et al.,2024). Additionally, issues such as model hallucinations, unfaithful explanations, lack of memory, and inadequate logical reasoning continue to challenge researchers.

To address these issues, many contextual learning schemes have been proposed to improve the performance of large models in various domains. For instance, algorithms like Chain of Thought(Wei et al.,2022)and many other algorithms, similar to Chain of Thought methods, enhance model output accuracy by decomposing tasks and employing a series of evaluation and retrospection techniques. Sys2 attention(Weston and Sukhbaatar,2023)improves outcomes by asking about key segments in the prompts given to the LLM, thereby strengthening focus on important content. Additionally, methods such as the work ofMa et al. (2023)use counterfactuals to enhance the LLM’s ethical level.

All the aforementioned methods are very similar to the thinking patterns people use when solving complex problems. Psychology suggests that humans have two modes of thinking: System 1 and System 2(Kahneman,2011). Inspired by neuroscience and psychology(Daw et al.,2005; Kahneman,2011), we believe that the characteristics exhibited by today’s LLMs are very similar to System 1, which is highly intuitive and relies on probabilities. As defined in the work ofYu et al. (2024), we define System 1 reasoning as the immediate responses generated by the LLM for the given problems. System 2 reasoning refers to any method that involves producing intermediate tokens, such as approaches that perform searches or use multiple prompts before delivering a final answer.

Building on this foundation, we designed an algorithm called MTMT (Multi-thinking Modes Tree), a graph that interacts repeatedly with the model to achieve the effects of System 2. As shown in Figure1, the MTMT algorithm integrates various thinking modes to enhance the logical reasoning capabilities and accuracy of answers provided by LLMs. By thinking around the original complex task, we derive many simpler sub-questions. Through repeated interactions with the LLM, we aim to extract all the potential knowledge and solution strategies stored within it regarding this issue, thereby achieving the best performance of the LLM in a zero-shot setting.

The MTMT employs a state machine-like approach, based on the current state, we select a thinking mode (e.g., decomposition, comparison, analogy, reasoning and so on) and generate the corresponding prompt. The results from each thinking mode are stored as nodes in a graph. Next, we process the existing information and finally determines the next strategy to use and whether to continue or stop. The structural diagram of MTMT and the node flowchart are shown in Figures1and2.

We will demonstrate the algorithm’s effectiveness on the GPQA, TruthfulQA, and GSM8K datasets. On the GPQA dataset, we achieved an accuracy of 44.0% without using external knowledge. Specific results can be found in Table1. In fact, as a general strategy, our approach can achieve good results across various datasets without the need to adjust prompts for different datasets.

The main contributions of this paper are as follows:

A System 2 reasoning method integrating multiple thinking modes is proposed. These diverse thinking modes enable large models to think more comprehensively and deeply, improving accuracy.

Through the thinking graph, we can capture the entire reasoning process of the LLM, making it easier to trace the source of errors, which enhances the interpretability of the answers.

By interacting with the large model in various ways, we can uncover as much hidden potential knowledge about the problem within the model, thereby enhancing its performance.

As a generalizable thinking method, it can be widely applied to other types of problems without modification.

SECTION: 2Related Work

SECTION: 2.1Dual Systems in Humans

System 1 and System 2, or fast and slow thinking(Kahneman,2011; Frankish,2010; Evans,2008), are common terms used in psychology. In the bookThinking, Fast and Slow, System 1 refers to the brain’s automatic and fast processes that operate with little effort and without a sense of conscious control. In contrast, System 2 focuses attention on mental activities that require effort, such as complex calculations. The operations of System 2 are often associated with the subjective experience of agency, choice, and concentration. Dual system theory is widely applied in psychological explanations of various behavioral phenomena in economic, social, and animal-conditioning contextsKahneman et al. (2002); Loewenstein and O’Donoghue (2004); Killcross and Blundell (2002); Dickinson and Balleine (2002).

Meanwhile, a broad range of neural and behavioral data suggests(Daw et al.,2005)that the brain contains multiple systems for behavioral choice, including one associated with the prefrontal cortex and another with the dorsolateral striatum. This serves as one of the pieces of evidence that support the existence of dual systems as a real structure within the brain.

SECTION: 2.2Dual Systems Models

Currently, LLMs also share many similarities with humans’ System 1. As probabilistic models, they tend to output the most “relevant” answers to a given question and perform well when answering simple and familiar questions(OpenAI et al.,2023). However, when the questions become more complex, the logical reasoning abilities of large models are challenged. In fact,Chiang et al. (2023)demonstrated that there are inherent limits to the problem-solving capabilities of large language models with a transformer architecture.

Surprisingly, simply prompting the model with “let’s think step by step” enables it to break down complex problems into smaller steps, leading to a significant increase in accuracy. This type of approach, known as the Chain of Thought(Wei et al.,2022), has become one of the most popular methods to improve the performance of LLMs. The effectiveness of Chain of Thought has been demonstrated byPrystawski et al. (2023); Feng et al. (2023). Next, we will categorize and introduce several System 2 models.

Many improved versions have been developed based on the Chain of Thought approach. Tree of thoughts (ToT)(Yao et al.,2023a)integrates the model’s abilities to produce and evaluate thoughts with search algorithms like breadth-first or depth-first search. Graph-of-thought(Yao et al.,2024), a graph-based framework advancing traditional sequential methods to better align with the non-linear characteristics of human thinking. For complex reasoning tasks with multiple valid paths, Self-Consistency Chain of Thought(Wang et al.,2023)generates diverse reasoning chains by sampling from the language model’s decoder. It then identifies the most consistent final answer by marginalizing these sampled chains. Tree-of-mixed-thought(Hu et al.,2023)integrates one-stop reasoning (fast) with Tree-of-Thought (slow) and applies this combined approach to multi-hop visual reasoning tasks.

In addition, many other prompt-based approaches have been proposed. For example, similar to Chain of Thought, the Divide-and-Conquer Program divides the entire task into smaller sub-tasks that can be processed in parallel(Zhang et al.,2024); prompts inspired by Aristotle’s method of teaching students(Chang,2023); TextGrad(Yuksekgonul et al.,2024), which uses gradient descent on large model texts through prompts; Sys2 Attention(Weston and Sukhbaatar,2023), which directs LLMs to focus on important parts of the text through prompts; Take a Step Back(Zheng et al.,2024), which extracts high-level abstract concepts; and the work ofMa et al. (2023)using counterfactuals to improve the moral reasoning of models .

Most of the methods mentioned above focus on a specific thinking mode that people use to solve problems, and some are tailored to specific types of problems or datasets, which limits their generalization. We aim to integrate multiple thinking modes and delegate the sub-tasks that arise during the thinking process to LLMs, thereby achieving better overall performance.In addition, we hope MTMT can also be applied in other fields in the future, such as RAG (Retrieval-Augmented Generation)Shinn et al. (2023); Yao et al. (2023b); Asai et al. (2023).

SECTION: 3Methodology

SECTION: 3.1Problem Definition

Using the symbolto define the large language model (LLM) andas the question being asked, the MTMT is represented as, with the initialization as follows:

whererepresents the first sub-question derived fromregarding, andis the answer provided by the LLM to the first sub-question.

For the subsequent-th step, the iteration proceeds as follows:

This represents the repeated “communication” between System 1 and System 2.
Based on the answers from the previoussteps and the question, we derive theand interact with the large model to obtain the answer(note that theincludes the information needed for the LLM to answer it).

Each time we complete a sub-question directly connected to, we obtain an answer based onand:

whererepresents the answer generated forduring the-th iteration.
Once certain criteria are met (see Section3.3),will be considered as the final answer.
During the execution of this algorithm, we ensure that the sub-questionis intuitive, simple, and suitable for System 1 (LLM) to answer. We then perform information extraction, evaluation, modification, and filtering on the obtained answersbefore applying them to the original question.

The overall process for Equations (3) and (4) is as follows (also shown in Figures2) :

Based on different thinking mode sub-questions, and considering the known pairs, generate the appropriate prompt and obtain the model’s response.

We will re-engage the model to perform information extraction in a dictionary format (see AppendixBfor more details). This process generates nodes that store the extracted information (for logical processing) and the original response (for further model generation), which are then added to the graph.

Select the next thinking mode type to be used and transition to the designated node.

Notably, we do not require the LLM to output results in a specific format on the first response. Instead, we ask it to extract the relevant information after its initial answer based on the content provided. This is because discouraging prompts, such as “just tell me the result without any explanation,” can negatively affect the LLM’s reasoning abilityZhao et al. (2024); Zhou et al. (2023).

SECTION: 3.2Thinking Mode

Specifically, we categorize thinking modes into the following major types:decompose,association,compare,importance,inferenceand others. Each major type contains several different prompts. See AppendixAfor specific details about each prompt. The use of different thinking modes serves two main purposes: generating thinking nodes and performing operations on different nodes.

By generating prompts based on the thinking modes, we can obtain a wealth of useful information.
For example, consider the following question:

Question:There are only three people on the playground Xiao Ming, Xiao Hong, and Xiao Li. Xiao Ming is running, Xiao Hong is playing tennis with someone, so what is Xiao Li doing?

We can inquire about the question type:

What is the task mentioned above?specify the type of task (e.g., mathematics, biology, general knowledge and so on).

And obtain the model’s response:

The task mentioned above is a logic puzzle or a riddle.

This information can help LLM addressand. Additionally, the importance thinking mode can assist in focusing on the critical information within. The decomposition thinking mode breaks down the target problem into several steps. The association thinking mode helps us find a series of related pieces of information based on the given data.

Thinking modes not only generate a wealth of information for creating thinking nodes but also influence the entire graph with various prompts. For example, in the importance category,unimportant_pointhelps us select usefuldata. In the decompose category,decompose_taskdetermines whether to proceed with task decomposition and how many steps to break it down into.

SECTION: 3.3Thinking Node

The questionserves as the root node. For generating subsequent nodes, the perplexity is used to measure whether the model is “confident” or “confused” about the question. In LLMs, for a given response, whererepresents the-th token andrepresents the total number of tokens, the perplexity is calculated as:

The generation, regeneration and deactivation of other nodes follow these guidelines:

Each node has a perplexity threshold, which is calculated as follows:

whereis the perplexity threshold of the i-th node,is the initial perplexity threshold,is the proportionality coefficient, andrepresents the number of nodes in the shortest path from nodeto the root node (excluding the root node).

By calculating perplexity, if the perplexity of a given response exceeds a certain threshold, we will continue generating sub-nodes using other types of strategies for that node’s question to obtain more information and methods until the perplexity requirement is met. If a node and its parent node both exhibit “confusion”, a breadth-first search (BFS) approach is adopted, prioritizing further exploration of the parent node using different strategies to extract more information.

For selecting a thinking mode, if a specific mode has already been assigned by a previous thinking mode, we follow the assigned strategy. Otherwise, a thinking mode will be randomly chosen from all available modes to generate the next node. Initially, we always let the MTMT go throughtask_recognitionanddecompose_task.

After generating information for other sub-nodes, we regenerate the node. This process is repeated until the perplexity requirement is satisfied. Once the perplexity condition is met, we use thedifference_answerin compare category to compare the quality of the answers generated in both instances and ultimately select the most suitable response.

Not all information stored in each node is always useful; irrelevant or incorrect information can even reduce the model’s accuracy. For nodes that have met the perplexity requirement, we use theunimportant_pointin importance category to assess whether the information from the sub-nodes contributes to resolving the parent node’s question. If it does, we incorporate this information into the prompt used for regenerating the parent node.

SECTION: 4Experiments

SECTION: 4.1Base Model

We will use OpenAI’s latest large language model, GPT-4o mini(OpenAI,2024), as the System 1. GPT-4o mini enables a broad range of tasks with its low cost and latency, and it performs well in extracting structured data, making it an ideal base model for our experiments.

SECTION: 4.2Datasets

We will test the model’s performance on the following three datasets.

GSM8K(Patel et al.,2021):The dataset contains math word problems geared toward an average middle-school curriculum, which is also repeatedly adopted by prior work as a key benchmark for arithmetic reasoning. We use a total of 1,319 data points from its test set.

GPQA(Rein et al.,2023):Google-proof Question Answering (GPQA) is a recent benchmark where challenging multiplechoice questions in physics, biology, and chemistry are created and labeled by domain experts who have or are pursuing PhD degrees. In this benchmark, experts and skilled non-experts are reported to achieve 81and 22accuracy respectively, demonstrating the difficulty of the questions. We use a total of 448 data points from its dataset.

TruthfulQA(Lin et al.,2021):TruthfulQA is a classification (multi-choice) task designed to test LLMs’ propensity to dispense harmful information. The dataset contains 654 test instances. We use a total of 817 multiple-choice questions from the dataset.

SECTION: 4.3Baseline

Here are the baselines for comparison. These baselines are widely used to evaluate the accuracy of LLMs. See AppendixCfor specific prompts.

GPT-4o mini:Using the original model directly on the dataset.

GPT-4o mini + CoT:GPT-4o mini model is queried with zero-shot CoT promptingKojima et al. (2022): “Let’s think step by step” is appended to the question.

GPT-4o mini + 3-shot:Three examples with corresponding answers are added to the prompt input of the GPT-4o mini model.

GPT-4o mini + CoT 1-shot:For 1-shot, One demonstration example of a question and answer pair is provided in the prompt, where the answer is in the style of CoTWei et al. (2022).

SECTION: 4.4Result and Compare

The results of different methods on various datasets are shown in Table1.

From the table, we can see that our method shows a 5.2% and 3.1% improvement on the TruthfulQA and GPQA datasets, respectively, compared to direct LLM responses. However, the difference on the GSM8K dataset is negligible, likely because GSM8K mainly involves middle school-level math problems with lower complexity. The base model has already achieved good results on GSM8K and is confident in its responses, as shown in Figure4, where almost no additional thinking nodes were generated.

We then tested the impact of different parameters on MTMT. Considering the API computational limitations, we set the maximum number of generated thinking nodes to 30.The results
are shown in Figure3, Figure4and Figure5.

Here, we denoteas the average number of nodes andas the depth of the graph, with the following calculation formula:

whererepresents the-th data point in the dataset, andrepresents the-th thinking node generated for the-th data point. The line graph shows that, in terms of accuracy, the accuracy of GPQA declines asincreases, while TruthfulQA shows an upward trend. This aligns with the intuitive difficulty of the two datasets—GPQA presents more challenging tasks, requiring the generation of more thinking nodes. Additionally, the choice ofdoes not significantly impact accuracy, likely due to our maximum limit of 30 thinking nodes and the breadth-first strategy, preventing MTMT from generating overly deep thoughts.

For bothand, asincreases, the values continuously decrease, indicating a reduction in the number of generated thinking nodes and the depth of the graph. Additionally, the value ofalso affects the graph’s depth; a smallerresults in a deeper graph.

At the same time, we also explored the impact of the temperature parameter in LLMs on the accuracy of the MTMT model (with the perplexity threshold set at 1.25). The experimental results are shown in Figure6.

From the figure, we can observe that as the temperature of the LLM increases, its “creativity” also rises, but the accuracy significantly decreases, even yielding worse results compared to the model’s direct output. Moreover, further increasing the temperature leads to the model’s inability to extract relevant information in the specified format. Therefore, in other experiments, we consistently set the default temperature to 0.

SECTION: 4.5Ablation Study

To evaluate the effectiveness of different types of thinking modes, we conducted ablation experiments on the GPQA and TruthfulQA datasets. In these experiments, we remove one specific thinking mode and test its impact on accuracy. We conducted ablation experiments on the GPQA dataset under the conditions ofand, and on the TruthfulQA dataset under the conditions ofand. The results are shown in Table2and Table3.

From the table, we can observe that removing different thinking modes has varying impacts on the results. The removal of the decompose mode has the most significant effect, leading to a 4.7% and 2.1% decrease in accuracy on the GPQA and TruthfulQA datasets, respectively. On the GPQA dataset, the removal of the association mode also resulted in a 4.5% decrease in accuracy.Other thinking modes show similar effects, causing accuracy drops ranging from 2.7% to 0.6%

SECTION: 4.6Analysis

MTMT has enhanced the LLM’s logical reasoning and explainability in various instances. For example, in response to the question,“What are the richest countries in South America by GDP per capita?”, MTMT prompted the model to identify the type of question and the methods required to solve it (such as understanding GDP per capita and identifying the countries). This interaction enabled the model to conclude,“It’s important to note that while Venezuela has historically had a high GDP per capita due to its oil wealth, its current economic situation has significantly affected its GDP per capita ranking.”This corrected an error found in the baseline output.
Such instances are not isolated within this approach, indicating that we can achieve further advancements in the LLM’s controllable output, logical reasoning, and explainability.

The errors in MTMT can be categorized into three main types:overly difficult tasks,information accumulation, andbase model issues. For instance, in the GPQA dataset, the prevalence of advanced mathematics increases computational demands. Given our computational limits, we restrict the generation of nodes to a maximum of 30 for each question, which may not be sufficient for the number of nodes needed to solve such complex problems.

Information accumulation is another challenge. As different nodes are generated, the amount of information sent to the base model increases, leading to confusion about which points to focus on. Summarizing and refining information may help address this issue.

Additionally, many errors stem from the underlying model. For example, in certain tasks, the model fails to produce output in the required format, rendering it ineffective. In simpler questions, such as comparing 3.8 and 3.11, the model can also provide incorrect answers. Such errors are likely to accumulate during the node generation process in MTMT. However, we believe that as LLMs continue to develop, these types of issues will diminish.

SECTION: 5Conclusion

Recently, many studies have focused on designing prompts based on specific thought processes and repeatedly engaging large language models (LLMs), achieving remarkable results. This paper introduces a method called MTMT, which aggregates various human problem-solving approaches to enable LLM to provide more accurate answers. This multi-faceted thinking approach offers the model additional background knowledge and guidance, breaks down complex tasks into sub-questions that are easier to solve accurately. Across three different datasets, this method outperformed models using Chain of Thought (CoT). Moreover, we further investigated the impact of different parameters on MTMT’s accuracy and conducted ablation experiments on various thinking modes. Overall, MTMT can enhance the LLM’s ability to solve complex problems.

SECTION: 6Limitations

Much relevant information may affect the model’s performance. Overly long prompts might prevent the base model from focusing on the question to be answered. In future work, incorporating some concise methods in the thinking mode could help address this issue.
Also, search methods like MTMT require more resources (e.g., GPT-4o mini API costs) than few-shot learning to improve task performance. However, adjusting the perplexity threshold allows users to customize the trade-off between performance and cost. As more efficient smaller models are introduced in the future, these costs are expected to decrease.
Finally, future work could consider integrating a memory module into the network, enabling it to utilize relevant temporary memories to answer related questions, thereby enhancing the model’s safety and long-term memory capabilities.

SECTION: References

SECTION: Appendix AThinking Mode Prompt Template

The use of different thinking modes serves two main purposes: generating thinking nodes and performing operations on different nodes. Table4shows the prompts of different thinking modes.

SECTION: Appendix BInformation Extraction Prompt Template

We also need to extract relevant information from the LLM’s responses to perform additional logical operation and the final answer (e.g., multiple-choice options like A, B, C, D or numerical answers) also needs to be extracted using this template (see Table5).

The format_instructions are generated from the Langchain library (version: 0.2.3), an open-source software library. For a set of attributes to be extracted, given their descriptions, the generated format instructions are shown in Table6.

Table8represents various objects and their corresponding descriptions.

SECTION: Appendix CBaseline Prompt Template

And here are the prompt templates of baseline (see Table7).