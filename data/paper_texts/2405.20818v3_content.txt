SECTION: Abstract

The iterated learning model is an agent model which simulates the transmission of of language from generation to generation. It is used to study how the language adapts to pressures imposed by transmission. In each iteration, a language tutor exposes a naïve pupil to a limited training set of utterances, each pairing a random meaning with the signal that conveys it. Then the pupil becomes a tutor for a new naïve pupil in the next iteration. The transmission bottleneck ensures that tutors must generalize beyond the training set that they experienced. Repeated cycles of learning and generalization can result in a language that is expressive, compositional and stable. Previously, the agents in the iterated learning model mapped signals to meanings using an artificial neural network but relied on an unrealistic and computationally expensive process of obversion to map meanings to signals. Here, both maps are neural networks, trained separately through supervised learning and together through unsupervised learning in the form of an autoencoder. This avoids the computational burden entailed in obversion and introduces a mixture of supervised and unsupervised learning as observed during language learning in children. The new model demonstrates a linear relationship between the dimensionality of meaning-signal space and effective bottleneck size and suggests that internal reflection on potential utterances is important in language learning and evolution.

SECTION: Author summary

Languages tend to be expressive: distinct sentences communicate distinct meanings, and compositional: sentence parts consistently correspond to meaning parts. In the iterated learning model a tutor teaches a pupil their language, in this case a mapping from binary vector ‘meanings’ to ‘signals’, by exposing them to a set of meaning-signal pairs. Subsequently, the pupil becomes the tutor for a new naïve pupil. Successive cycles of learning and generalization lead to an expressive, compositional and stable language. However, while an agent’s map from signals to meanings is implemented by its neural network, the complementary map from meanings to signals is calculated through ‘obversion’ which becomes impractical for larger languages and is unrealistic, requiring consideration of all possible meaning-signal pairs. Here, we propose a more computationally tractable and more realistic model in which both the encoder and decoder are neural networks, concatenated together as an autoencoder, and pupils are required to learn from a mix of unsupervised and supervised examples, as children do. The model demonstrates the emergence of languages that are expressive, compositional and stable, and reveals a linear relationship between the optimal size of the transmission bottleneck and the dimensionality of the language’s meaning-signal space.

SECTION: Introduction

In this paper we introduce a new iterated learning model (ILM) for language learning; compared to the existing model, this new model is less computationally demanding and is closer to human language learning.

One of the most important questions in linguistics asks to what degree our linguistic ability prepares us for the specific forms that human languages take[1,2,3]. It may, however, be possible to sidestep this question by asking two perhaps more pertinent questions: (1) what specific properties do human languages have that make it possible for us to to learn and interpret them and to use them to communicate about the diverse and often unforeseen topics we need to discuss and (2) how do languages come to have these properties? Languages are made easier to learn and easier to interpret by their expressivity, compositionality and stability; that is, by the tendency for a language to be able to express many different meanings using different sentences, to compose meaningful sentences from meaningful sub-parts and to change slowly over generations. There are different words for left and right, for example, but the word ‘left’ in ‘you write with your left hand’ means something very similar to the word ‘left’ in ‘turn left at the junction’. Moreover, the difference in meaning between the expressions ‘turn left, then right’ and ‘turn right, then left’ is represented by the different structural organization of their component words. Of course, languages are neither completely expressive nor completely compositional, ‘they are on the left’ typically refers to position but, in some contexts, could refer to politics, and a sailor in some cases would use the word ‘port’ when left might be expected. Nonetheless, expressivity and compositionality are, invariably, important features of languages and might be considered to mark the boundary between a simple system of signs and utterances and a true language[4,5,6]. The third property, stability, is also important. Since parents need to be able to understand what their children say to them, at least some of the time, it is important that language does not change drastically from generation to generation.

Expressivity, compositionality and stability are only three of a long list of properties a ‘good’ language needs in order that it be easy to learn and effective for communication. However, these three are among the most basic, indeed compositionality is a version of Frege’s principle of compositionality[7,4]which is central to his very influential theory of language.
Here, we focus on expressivity, compositionality and stability because of their importance and because they can be measured in the sort of simple language used in our simulations. In ILM-based investigations of how language can acquire important basic properties through iterated learning, these three are intended to stand in for the multiple properties needed for a language to be useful and learnable.

We look at the ILM,[8,9,10,11], a model of learning in which a language evolves across generations; this model provides a putative description of how a language might become expressive, compositional and stable. However, as we describe below, the model, requires an ‘obversion’ step which makes it computationally expensive and unrealistic as a simulation of human language learning. Here we propose a solution to this and argue that our new model suggests the importance to language learning and to language evolution of an internal process within the language learner, translating from some meaningful state or idea to an utterance that could be used to express it, and back to how that the meaning of that putative utterance might be interpreted.

SECTION: The iterated learning model

The ILM models how a language evolves as a tutor teaches their language to a pupil and the pupil, in turn, becomes a tutor and teaches their language to a new pupil. The model demonstrates the gradual emergence of a language that is expressive, compositional and stable, despite the fact that at every iteration the pupil begins in an identical state of ignorance. Here a language is defined as a map from a set of ‘meanings’ to a set of ‘signals’ and, broadly, the goal for the iterated learning model is to shed light on the conditions under which languages that are expressive, compositional and stable arise. We will define measures,,andfor expressivity, compositionality and stability; these measures all take values inand we define an XCS language as one where all three exceed. In our limited context this will serve as a definition of a ‘good’ language; their precise definitions are giving inS1 Appendix..

The ILM was introduced in[8,9,10]. In its original form, it demonstrated that the emergence of language compositionality can be attributed to the influence of a transmission bottleneck on language acquisition taking place over a number of successive generations[9,12]. This original version of the model was used to explain the mixture of irregularity and regularity found in languages[9]. However, this ILM used a language generated by a simple formal grammar and relied on a set of deduction rules that the pupil used to deduce the formal grammar from exemplars. This made it difficult to disentangle the role of the deduction rules from the iterated dynamics of language learning. This was addressed in[11]where a new version of the ILM is presented which did not require these deduction rules.

This ILM has a decoder based on a neural network and incorporated obversion, an algorithm to derive an encoding map from the decoding map, which had been previously defined in a more general context in[13]. This obversion step is problematic in two ways: it is computationally expensive, limiting the ILM to small example languages and it is unrealistic, it makes the transition from pupil to tutor a discrete step during which the pupil must calculate the probability associated with each signal as an encoding of every possible meaning, something very different to what happens in human language learning. Our purpose here is to propose an ILM which does not require obversion.

This is not to suggest that the Obverter ILM has not been useful. For example, since it was introduced an obverter-based learning models has been applied to pictures[14]. This is exciting since it simulates the communication aspect of language use and links this class of language-learning model to classic Lewis signalling games[15]. It is hoped that an obversion-free version based on our proposal here would make it computationally possible to investigate this further. Population-based versions of the ILM have been considered[16,17,18], this is important since language evolution involves space and even geography as well as time, but the Obverter ILM has the disadvantage that an agent cannot learn and teach at the same time, something the new model introduced here will address.

The ILM has also been discussed in a broader Bayesian framework,[19,20]and this has been applied to populations[21]and to cultural evolution[22,23]. Similar models of language development include the talking heads experiment,[24], the seeded ILM[25], the neural iterated learning model,[26], and an extension of the restaurant process,[27], to language[28]. These modeling-based approaches also have a counterpart in participant studies using artificial mini-languages, where similar issues of expressivity, compositionality and stability are also considered, see[29,30,31]for reviews; this approach has been successful in revealing human constraints on language evolution[32,33].

In the ILM, agents candecodesignals, that is, map them to meanings, and canencodemeanings, that is map them to signals. Atutoragent with its own private encoder and decoder mappings is charged with teaching its language to an initially naïvepupilagent, that is, an agent with initial mappings that are empty, random or arbitrary. To do this, the tutor presents a series of meaning-signal pairs to the pupil, where each signal is created by encoding a target meaning using the tutor’s encoder. The presentation of each example is an instance of supervised learning, like an adult pointing to a boy stealing biscuits from a cookie jar and saying ‘Look! The boy is stealing biscuits from the cookie jar’. The pupil uses these examples to train its own decoder, learning a mapping between signals and meanings. After this training, the pupil ‘grows up’ and becomes a tutor itself. To act as a tutor it also requires its own encoder, which is derived from its trained decoder by a process of obversion. The set of examples presented to the pupil is only a subset of the full set of possible meaning-signal pairs; this is the transmission bottleneck. The new tutor next trains a new naïve pupil who again initially starts with language mappings that are empty, random or arbitrary, and in this way the process iterates. This cycle of iteration requires the tutor to generalize, in particular, because the transmission bottleneck means that it will, in general, present examples it did not itself learn when it was a pupil, or, because it has not fully learned the exemplars it has been presented by its own tutor.

Despite each pupil starting its training in the same naïve state with no knowledge of the language that the tutor has developed, the ILM demonstrates a tendency for the language that is being learned to change systematically over successive agent ‘lifetimes’. Under certain conditions the language stabilizes so that successive agents learn the same language and this language exhibits high levels of expressivity and compositionality.

Whether or not the ILM achieves this kind of outcome depends on the size of its transmission bottleneck. Since this is a subset of the full set of meaning-signal pairs, when the pupil becomes a tutor they must generalize from what they have learned in order to be able to present their pupil with a new set of meaning-signal pairs comprising random meanings, each paired with the signal that encodes it in the tutor’s language. When the bottleneck is too small, the language does not stabilize. Where the bottleneck is too large, the result may be a language which is non-expressive and non-compositional, that is, one in which the relationship between each meaning and its associated signal is arbitrary and unstructured.

SECTION: The neural-network based ILM

Here, the neural-network based ILM presented in[11]will be referred to as theObverter ILMto distinguish it from the Semi-Supervised ILM presented in this paper. In the Obverter ILM, and indeed in all the ILMs presented here, the meanings and signals are both binary-vectors so that. An individual bit,, in a meaning vectorwill be called a ‘fact’. A bit,, in a signal vectorwill be called a ‘word’, in fact, ‘morpheme’ would be more accurate, since the signal vector is intended to model any aspect of how meaning is encoded, but, for convenience, ‘word’ will be used. An agent has two maps,
an encoding map from meaning to signal:

and a decoding map from signal to meaning:

A glossary is provided in Table1to summarize the notation.

The decoder,, is a neural network with input, hidden and output layers each of size. It is useful to be careful at this point with the notation; the neural network itself maps a meaning,, to a vectorwhereis the probability that theth word in the corresponding signal is one. This map is called:

whereis the space of-vectors whose entries are all probabilities. To add further to the notation, a decision mapis defined, this maps a vector of probabilities to a binary vector by mapping a probabilityto one andto zero. In this way, or for those who prefer more formal notation:. As a final note,also defines a map from a meaning and signal pair to a probability:

whereandifis one andifis zero. There is a guide to the notation in Fig.1.

The encoder,, is more complicated; it takes the form of a list tabulating the preferred signal corresponding to each meaning in. This list is calculated by obversion, an inversion-like calculation performed on the decoder map when an agent transitions from pupil to tutor. Obversion is described in more detail inS2 Appendix., but the key point is that it requires the calculation of theentries in the table of probabilities given byfor allpairs.

In the Obverter ILM the pupil starts as a naïve agent; it is presented with examples of signal-meaning pairs by the tutor agent and uses these to train its decoder map,, using stochastic gradient descent. The set of example meaning-signal pairs,, is produced by randomly selecting a subset of meanings and mapping these to signals using, the tutor’s encoder. The setis smaller than the set of meanings and so it constitutes a bottleneck in language transmission: in particular the set of examples a tutor presents will generally be different from the examples they learned from and so the tutor is forced to generalize beyond its training corpus. This is an important element of this ILM and the setis referred to as thebottleneck set. After training, the pupil obverts its decoder map,, to construct its encoder map,, and becomes the tutor to a new naïve pupil. This cycle is repeated many times in order to explore how language structure changes over multiple generations. Fig.2gives a diagram of the generational structure; pseudocode is provided inS3 Appendix..

This model succeeds in simulating the evolution of an XCS language. However, obversion is costly. Already forthe table of probabilities is cumbersome, having 65,536 entries. Asgrows it rapidly becomes too large to calculate easily. It is also unrealistic. One of the signature properties of language is its open-ended power to encode novel meanings. It is certainly hard to imagine that when a child is learning language they conjure up all possible subjects of future discussion and estimate the probability that each would be expressed by every possible sentence. The main aim of this paper is to introduce a new ILM which does not require obversion and to confirm that expressivity, compositionality and stability still arise under conditions analogous to those required for the original Obverter ILM.

SECTION: A new ILM that does not need obversion

In this paper we propose a Semi-Supervised ILM which does not require obversion. Dispensing with the ILM’s obverter improves its ecological validity, that is, it moves the model closer to the situation it is intended to shed light on. The particular semi-supervised approach employed here seems especially apt for modeling language learning since human language learning does include a mix of explicit instruction and implicit exposure. Furthermore, as a result of avoiding the computational costs of obversion, this version of the ILM supports larger languages. This should prove useful in future work when using the ILM to study language contact or language change in structured communities[16,17,18]and should also make it easier to incorporate and study other aspects of language structure and development.

Our ILM, the Semi-Supervised ILM, employs a novel agent architecture; in addition to using a neural network for the decoding map, mapping signals to meanings, it also uses a separate neural network for the encoding map, mapping meanings to signals, and, crucially, an autoencoder, mapping meanings through signals and back to meanings. This is described in detail below, but, roughly speaking, the autoencoder is formed by concatenating the decoder and encoder networks. During training, the pupil is sometimes given a meaning-signal pair as part of a supervised learning trial for either its encoder or decoder, but more often it is given a meaning in an unsupervised learning trial on the autoencoder network as a whole. This unsupervised learning might be considered akin to a child observing what is happening around them and imagining how it might be described and then checking this description back against their observation. This additional element of the training appears crucial to the development of an expressive, compositional and stable language. Fig.3gives a diagram of this generational structure.

SECTION: Results

In the Semi-Supervised ILM both the pupil’s encoder and decoder need to be trained so example meaning-signal pairs fromare used as meaning-signal pairs to train the encoder and as signal-meaning pairs to train the decoder. This simple approach will not work on its own. Since the decoder does not contribute to the encoding of meanings as signals when constructingit has no role in the evolution of the language and is, as it were, ‘just along for the ride’. Consequently, while the addition of the decoder network may make the agent more realistic, from a language learning point-of-view it serves no function and, as far as the language is concerned, this ILM is no different from one with only an encoder.
An ILM with only an encoder never becomes expressive; experiments show that the neural network learns to map all meanings to one of the signals represented in the bottleneck set and so the language soon evolves towards one where all meanings are mapped to a single signal. The ILM with only an encoder is discussed further inS4 Appendix..

To resolve this it is useful to consider the situation with human language. During human language learning, only a portion of a child’s language exposure is ‘supervised’ in the sense that an adult purposefully and explicitly presents a meaning-signal pair. It is clear, on reflection, that the presentation of learning examples to a child is complex, with some ‘supervised’ child-directed speech including clear indications of meaning, some wholly ‘unsupervised’ learning based on overheard speech or based on cogitation during observation, and a range of intermediate examples where a child hears utterances related to some activity or situation that they are engaged in, or where some indication of meaning is available or can be inferred, but has not been made explicit or precise.

This is an important and well-studied topic, see[34,35,36,37,38,39]for example. It is also a complex topic, with child-adult interactions changing as the child becomes older and without any culturally-preserved norms around the structure of a child’s language exposure. Crucially, a classic study,[40]replicated in[41,42], demonstrates that during early language acquisition explicit approval or disapproval by a parent of children’s utterances depends on whether the utterance is socially appropriate not on whether it is grammatically well formed. Furthermore, in one fascinating quantitative study it is demonstrated that the proportion of speech that is child-directed varies hugely between children in the United States and children in Yucatec Mayan villages[43]: despite the difference, children in both environments learn to speak.

Furthermore, the discussion of which exemplars are presented to children during learning typically focuses on the relationship between the child’s language exposure and the trajectory of the child’s language acquisition, with the suggestion that this is best studied as an example of semi-supervised learning[44,45]. Here, however, the hope is to study how languages develop rather than how children learn, and the mixture of supervised and unsupervised exemplars in learning suggests a solution to the problem of expressivity collapse.

In the Semi-Supervised ILM, an agent is considered to have three neural networks: a decoder,, an encoder,, and an autoencoder,, composed ofand:

or, whereis defined as the neural network mapping a meaning input to a vector of word probabilities without the final step mapping the probabilities to zeros and ones. This is illustrated in Fig.4. This means that the two neural networks,and, are coupled because they are used together to form the autoencoder.

During training,is used in three ways: whileis a set of meaning-signal pairs, it is obviously possible to change the order to get signal-meaning pairs, or to ignore the signal to get a meaning. During each round of training a randomly chosen meaning-signal example fromis present to, a randomly chosen signal-meaning is presented toandmeanings are presented to. This final training step trains the autoencoder to map meanings to signals and back to meanings by measuring how close the input and output are to each other. This is intended to mimic a child observing the world and tryingto internally match its observations to self-generated utterances and map these utterances back to what it is observing.

Fig.5demonstrates the Semi-Supervised ILM’s ability to produce an XCS language.
The performance of the Obverter ILM is reprised in Fig.5(A-C). As in[11]it is demonstrated that an XCS language quickly arises within the ILM: Fig.5A-Cshow that after 40 generations all three properties are close to their maximum. Fig.5(D-F) and, again, in Fig.5(G-I) show that the Semi-Supervised ILM also succeeds in evolving an XCS language.
In Fig.5(D-F) the meanings used to train the autoencoder are all meanings from. This may seem unrealistic: a child’s observations are not restricted to the states of the world that it has heard described. In the case ofthis does not appear to have a crucial affect on language evolution: using an independently randomly selected and larger set of meanings for autoencoder training produces similar results Fig.5(G-I).
However, this no longer holds for larger languages. Using a larger set,, of meanings selected at random and independently ofis required for annXCS language to evolve; this is demonstrated in Fig.6.
In Fig.6(A-D) the ILM is evolved withandand does not reach an XCS language, in Fig.6(E-G), whenandandare selected independently, it does.
The value 480 is picked for illustrative purposes, in fact, lower values work nearly as well; a plot showing how performance changes as the size ofchanges is give inS5 Figure.

In Fig.7a still larger language is consider:where the set of meanings and of signals each have over a million elements. This means that the set of expressive and compositional encoders is tiny compared to the vast number of possible maps. Since obversion would require calculating a table withentries this is too large to easily test the Obverter ILM and so it is unknown whether the Obverter ILM produces an XCS language in this case. As seen in Fig.7(A-C), for the Semi-Supervised ILM the languages become expressive and compositional but the mean stability is just a bit above. This might be considered realistic since stability is a harsh measure for compositional languages, swapping just two words gives a stability of. However, if this instability is a problem, it is one that can be solved, for example by increasing the number of nodes in the hidden layer of the decoder and encoder networks: Fig.7(D-F) shows the behaviour when the hidden layer has 30 nodes while the signal and meanings still have length 20. With this larger hidden layer the language rapidly evolves into an XCS language.

Figure8is perhaps surprising. This set of simulations examines the size of the bottleneck as, the bit-length of the meaning and signal spaces, is changed. For this a precise, though of course slightly arbitrary, definition of ideal size is used: the model is run until,andall exceed athreshold, indicating that an XCS language has developed. The number of generations is noted, this is repeated 25 times and the mean calculated. The size of the bottleneck that enables the threshold to be reached most quickly, on average, is taken as the best bottleneck size. It turns out that this value increases linearly with. This is despite the size of the meaning and signal sets increasing exponentially with; forthe optimal bottleneck is close to a quarter of the number of meanings, while forit is only about a 40th. In fact, this is not unique to the Semi-Supervised ILM: although it does not seem to have been previously noted, the Obverter ILM demonstrates the same linear dependence on bottleneck size that was observed for the Semi-Supervised ILM, seeS6 Figure.

In both cases the measure used to quantify this effect is somewhat arbitrary and noticeably noisy. Furthermore, as will be discussed in much more detail below, the performance of the Semi-Supervised ILM does not fall that much asincreases, it produces an XCS language even when, that is when it is not actually a bottleneck. However, the conclusion is clear, the number of exemplars needed for the ILM to work increases with, not: evolving an XCS language relies on matching facts to words, not meanings to signals. In this way, language in the ILM evolves towards one that can be learned for a smaller example set than would be required for non-compositional language, giving a simple example of how the ‘poverty of stimulus’ identified in human language learning,[46,47], could be suggested to impose a structure on language, rather than require a particular instinctive awareness in the human brain.

An expressive language supports a diversity of signals and meanings and as a consequence may be challenging to learn under conditions of limited time and cognitive effort. A compositional language, however, is learnable, even if it is expressive, because a pupil need only learn the rules of composition, rather than a much larger mapping from states of the world to arbitrary, unstructured phrases.

As a simple demonstration of this, we can consider how well the neural networks inside the Semi-Supervised ILM’s agents learn as the initially arbitrary language matures into an XCS language. Of course, the learning implemented here, using stochastic gradient descent, is very different from anything that plausibly happens in the brain, so it would be wrong to suggest that the Semi-Supervised ILM’s learning dynamics align with the learning dynamics of a human language learner. However, the ease with which the neural networks reduce their loss function is likely related in some general way to how easy the language is to learn. In Fig.9the loss achieved by each of the pupil’s neural networks is plotted against training epoch for each generation. Since each pupil always starts in a naïve state, the initial loss is always the same at epoch one, but for higher generations, and hence more mature languages, the loss falls more quickly.

More broadly though, the issue of how well a language supports communication goes well beyond our approach here. If languages are subject to selective pressures to minimize ambiguity or reduce cognitive load on language users, these pressures might be seen to shape language change within an iterated learning model. Given its lower computational demands, the Semi-Supervised ILM could be extended to incorporate such additional pressures on language, for example related to the cognitive load experienced by a speaker while forming sentences or the ease with which these sentences can be processed or unambiguously understood by a listener.

Provided the size of the bottleneck is large enough, the performance of the Semi-Supervised ILM does not appear to depend in a sensitive way on eitheror. This can be seen in Fig.10which plots the values of,andfor both five and 20 generations whileandare varied. It is striking that increasing the size of the bottleneck setdoes not substantially reduce performance. This is not true of the Obverter ILM, Fig.11shows that performance is reduced if the bottleneck is too large.

Since the bottleneck is considered essential for the ILM to evolve an XCS language, it is surprising that the performance of the Semi-Supervised ILM does not appear to fall substantially as the size of the bottleneck increase. To examine this the performance of the ILM with, that is, with no bottleneck in the transmission from tutor to pupil, is examined in Fig.12. In Fig.12A-Cwiththe Semi-Supervised ILM develops an XCS language even when the bottleneck size matches the number of meanings and signals and, although the Obverter ILM does not reliably evolve an XCS language, it performs better than might have been anticipated, Fig12D-F.

This means that the cycle of generalization from a bottleneck set is not the only aspect of the ILM dynamics that leads to an XCS language. It is notable that for the Semi-Supervised ILM the stability is not near one for the first few generations, indicating that the pupil has not fully learned the language for the tutor, even though it has been presented a complete set of examples. This suggests that it is difficult to train the encoder and decoder while also training the autoencoder for languages with low values ofand. This is not surprising, if the language has poor expressivity it will not be able to map all meanings back to themselves and so while the language remains unexpressive the goal of training the autoencoder is in opposition to the goal of training the decoder and encoder. It appears both that learning in the autoencoder forces a sort of generalization towards a more compositional language and that because the pupil, in early generations, has not fully learned the language of the tutor it is forced to generalize even when there is no bottleneck.

SECTION: Discussion

One draw back of the ILM is that it involves a large number of meta-parameters and we have only explored a subset of these in detail. We have not presented any description of how performance depends on parameters like learning rate, the number of epochs, the number of presentations to the autoencoder in each epoch, or on any broad consideration of how details of the neural network architecture or training algorithm affect the results. While we have done some preparatory exploration to find values that work, it would be useful to examine these details more formally and more rigorously in future.

The novel Semi-Supervised ILM introduced here demonstrates that XCS languages, languages that are expressive, compositional, and stable, arise without the use of an obverter. While natural languages exhibit these properties to a substantial degree, they also represent a trade-off between precision, learnability and ease of production[48,49]. That is, expressivity, compositionality and stability are not the only universal properties that are central to natural language. Fundamentally, however, in addition to being able to support communication, all languages must share one crucial property: they must be learnable. As described above, XCS languages are more easily learned and the ILM demonstrates, in a simple context, how languages might spontaneously evolve to have formal properties that make them easier to learn from a limited corpus of exemplars.

The original Obverter ILM and the Semi-Supervised ILM introduced here have a very simple model of a language, consisting entirely of single bit ‘words’ and ‘facts’ and with signal and meaning spaces of matching sizes. This is sufficient to explore concepts like expressivity, compositionality and stability but further work should include more of the complexity and structure or language and of the meanings that language is used to communicate. It is hoped that the lower computational complexity of the Semi-Supervised ILM will make it possible to being to address this shortcoming in future.

A similar hope is that further elaboration of the model can incorporate more of thestructurepresent in language or in putative ‘spaces of states of the world’; this includes the complex hierarchical interplay of different types of facts, be they objects, both real and abstract, actions and modifiers along with a parallel set of factors related to biases, ecological salience, novelty and familiarity. In both the Obverter ILM and Semi-Supervised ILM simple rules defining ‘bottom up’ ingredients in language evolution, such as learning and a generational structure lead to languages that incorporate what at first seems like formal ‘top down’ requirements like ‘a word codes for the same fact, irrespective of context’. As such, it is exciting to ask if applying the ILM to the more complex examples that the Semi-Supervised ILM will make computationally feasible will help us to understand how additional complex and ‘formal’ rules can arise.

The ILM has a very simple tutor-pupil structure; in reality, humans do not learn from a single tutor. In some societies, such as those described in[43]or in other earlier, albeit contested, anthropological accounts such as[50], children at the age language learning occurs are embedded in a rich social environment, but are not in any obvious way ‘tutored’. Similarly the wealth of data provided by the difference in experience between children who spend time at a nursery or crèche and those who are cared for more exclusively in the family home, does not seem to produce any clear difference in the speed of language acquisition.

Furthermore, language acquisition and language change continue past early childhood and, indeed, the language of teenagers is notoriously mutable. Interestingly, there is even evidence from human experiments with an artificial language that compositionality can emerge without inter-generational transmission[51]. All this indicates that a model of language development should not be restricted to isolated tutor-pupil interactions. In this, the Semi-Supervised ILM presents an advantage. In the Obverter ILM obversion takes place at a discrete transition from pupil to tutor while for the Obverter ILM there is no such transition: an agent can learn and teach at the same time. Consequently, the Semi-Supervised ILM should lend itself to models where agents learn from each other as a community.

There are two obvious ways to add an autoencoder phase to learning in the ILM, the approach used here with the autoencoder mapping meaning to meaning and the ‘inside-out’ version of this withmapping signal to signal. The first represents the child describing observations to themselves, while the second represents the child overhearing speech and trying to guess its meaning. It is clear that both types of autoencoder could be justified as representing an aspect of the real-world situation and one approach would be to use both. For simplicity, we have not reported that here, but in simulations using both there appears to be even faster evolution of an XCS language. However, using only the signal-to-signal autoencoder does not work: it appears that one of the barriers to language evolution, at the initial stage, is that all meanings map to a small subset of the possible signals and so the set of signals generated by the tutor encoder will include only a few signals and training the autoencoder on this small set reduces the expressivity of the language still further.

Currently the pupil learns in part through supervised learning. The tutor supplies input-output pairs and these are used in a classic neural network learning paradigm. However, as discussed above, directly supervised learning, as in ‘here is a cat’ / ‘what is that animal?’, is only a small part of a child’s learning experience. Indeed, if supervised learning was the key to child learning then language supervision would most likely be highly optimized and hence highly preserved across cultures. In fact, this is far from the case, for example, in[52]language learning in three different cultures is compared and, for example, whereas parents in white middle-class Anglo-American families look directly at a child learner and talk to it in the modified speech register often called parentese, in Kaluli families mothers carry the child in such a way that it is facing in the same direction as she is while answering on the child’s behalf in tripartate interactions using an unmodified version of Kaluli. In the Semi-Supervised ILM, the autoencoder, which is the analogue of reflection using internalized language, is an important part of language learning; it appears to be the key to a language developing with the key properties of expressivity and compositionality, while supervised language on serves to ‘nudge’ that internal dynamics towards a language that matches the tutor’s.

SECTION: Conclusion

Simple models like the ILM dispense with the semiotics and logic of language; they do not allow for the possibility that an understanding of meaning and of logic might be required to account for the structure of language. The agents are also unreflective, in the sense that they do not attempt to identify deficiencies in their language with a view to improving it. This does not make them uninteresting; it is useful to note how even the simplest of models, such as the ILM, can demonstrate possible mechanisms for the presence of features in language and of the emergence of properties that might otherwise have been attributed to more complex design.

This study introduces a novel ILM that integrates both supervised and unsupervised learning to simulate the evolution of expressive, compositional, and stable languages without relying on obversion. By employing neural networks for both encoding and decoding processes, and, crucially, combining them in the form of an autoencoder during unsupervised learning, the model can claim greater ecological validity, mirroring in this respect the complex dynamics of human language acquisition. This underscores the importance of semi-supervised learning in the development of language structures, offering new insights into the mechanisms underlying language evolution and suggesting that languages can evolve properties that make them easier to learn.

SECTION: Materials and methods

SECTION: The Obverter ILM

The mapis an all-to-all feedforward neural network with one hidden layer. All layers have sizeand are initialized using the defaultflux.jlXavier initialization. Both the hidden layer and the output layer have sigmoid non-linearities. The network is trained using stochastic gradient descent with learning ratefor twenty epochs. In each epoch every pair inis presented once; the order of presentation is random for each epoch. Since it has no tutor, the initial tutor calculates its encoder though obverting based on a decoder which is an untrained neural network.

SECTION: The Semi-Supervised ILM

The mapsandare all-to-all feedforward neural networks with one hidden layer. Except in Fig.7(D-F) all layers have size. All layers are initialized using the defaultflux.jlXavier initialization. Both the hidden layer and the output layer have sigmoid non-linearities. The mapisand so it is an all-to-all feedforward neural network with three hidden layers. The three networks are trained at the same time using stochastic gradient descent with learning rate. The number of epochs is again. At the start of each epoch two copies ofare made, each in a different random order; at each iteration an item from the first copy is presented toand an item from the other to;randomly chosen signals fromare then presented to. Having no tutor itself, the first tutor uses an untrained encoder to supervise its pupil.

SECTION: Quantifying the ILM properties

Precise definitions of,andare given inS1 Appendix.. These three quantities all have values between zero and one and each has a non-zero bias which depends on. To make comparison acrosseasier, in all the graphs presented here the bias has been removed so the quantity plotted iswhere in each caseis a suitable estimate of the background value. Forandthis is the value calculated for a naïve agent, averaged over 40 examples, forthis is the value calculated between 20 pairs of naïve agents.

SECTION: Software and libraries

All code was written inJuliaand run onJulia 1.9.3, neural networks were trained usingFlux v0.14.6and plotting usedGadfly v1.3.4andColors v0.12.10. Data was handled usingDataFrames v1.5.0andCSV v0.10.11. The code is available atgithub.com/IteratedLM/2023_12_ailm.

SECTION: Supporting information

Quantifying language properties.

Stability measures the degree of language agreement between two language users, that is, the probability that when one agent uses their language to express a meaningusing their preferred signal, the other agent will be able to use their language to recover the same meaningfrom signal.

whereis one agent’s decoder andis the other agent’s encoder.

Expressivity measures the degree to which a language uses different signals to express each of the possible different meanings; roughly speaking it measures the lack of ambiguity in the encoder map. This is straightforward to quantify as the size of the range ofas a fraction of the size of the signal set,:

Compositionality quantifies the degree to which a language consistently encodes a single fact using a single word, irrespective of context. The basic idea is to quantify the extent to which this is true for each fact in turn and then average these fact-wise measures to arrive at a compositionality measure for the whole language. Recall that a fact is one component in a meaning:in. For any given,will always be either zero or one, and, asranges over all its possible values,will be zero for half the time and one for the other. Now consider, one of the words inwhere. Thisis also either zero or one and, indeed, consideringfor all possibleit is possible to calculate. Let

be the entropy for this probability distribution; a value ofindicates that the fact ‘1’ at meaning locationmaps onto the words ‘0’ and ‘1’ at signal locationwith equal probability, whereas an entropy valueindicates that the fact ‘1’ at meaning locationmaps exclusively to the word ‘1’ or exclusively to the word ‘0’ at signal locationindicating compositionality. Ifthen the basic idea is that

However, it turns out that finding a good definition of compositionality is trickier than might be expected. While the formula forabove does a good job for expressive languages, it does a poor job for less expressive languages. A more complicated definition is needed, though the core idea is captured by. This more complicated definition avoids the same word being used to convey more than one fact. First, a set of optimalis calculated, one for each fact:

then, for each word, the lowest value inis chosen, or, if there is no corresponding value in, the word is assigned the value one:

Finally,

Obversion

Obversion, in this context, is introduced in[13]to describe an inferential step in communication akin to inversion of a signal-meaning mapping and is used in the ILM to derive an encoder mapfrom a decoder map. It is useful to introduce a mild abuse of notation. A meaningis a binary vector of length. This is easily mapped to an integer between one and, sois used here both to refer to a meaning vector and as an index for the elements of; the same also holds for,mutatis mutandis. Now, to define obversion, a probability table is calculated:

This is the probability that the signalcorresponds to the meaningaccording to the decoderand is calculated as described in Eq.4. Now we define the encoder,, that obvertsas:

Thus, the preferred signalcorresponding to meaningis the one with the highest probability. It should be noted thatis not a probability distribution over values of. While, the same is not true for.

Pseudocode

Here the value autoN=, bottleN=, numEpochs is the number of epochs, typically 20, numAuto is the number of training steps for the autoencoder in each epoch, this is typically 20 and is referred to asin the text. The Boolean setAEqualSetB is true ifand false otherwise.

The one-way ILM

Perhaps the most obvious approach to avoiding obversion is to model only one half of the language learning task and use agents with only an encoder. In this approach the tutor is given a set of meanings to present to the pupil and uses its encoder to pair them with associated signals. The pupil uses this set of meaning-signal pairs to train its own encoder, and then becomes a tutor for a new naïve pupil. This approach is obviously less satisfactory than the full ILM because it does not consider the ability of agents to decode signals into meanings. In fact, our first approach to removing the obverted used thisone-wayILM approach, but, at least in our various attempt, this approach did not work; thie one-way ILM always failed to evolve an expressive language. On the contrary, a stable language in which most if not all meanings are mapped to the same signal or a small set of similar signals arises as this type of mapping is readily learnable from a small set of training examples and is stable in the absence of any pressure to avoid ambiguity. This result also points to an effect of the obversion process beyond acting as a convenient approach to constructingfrom: by picking a ‘winner’ among the signals proposed to correspond to a given meaning, the obverter encourages expressivity, though it does not in any formal sense, guarantee that the encoder is expressive.

It is possible that the collapse of expressivity exhibited by the one-way ILM could be prevented by adding a contrastive term to the model’s objective function. In this approach the neural network encoder is rewarded, as usual, for mapping a signal to the same meaning as the tutor supplied. However, in addition, the network would be punished if it allowed two recently witnessed signals to be mapped to the same or similar meanings. Our own experiments with this approach have not been successful and our belief is that this is because the contrastive term only deals with the meaning-signal pairs in the bottleneck set: the collapse in expressivity seems to be result of generalisation tending to associate unseen meanings with signals that were experienced during learning. In the remainder of this paper a different approach is taken.

The Semi-Supervised ILM performs better for larger.A-Cplot mean values for,andas a function of the size of, withfixed at 160, after 25 generations (solid line) and five generations (dashed line) for 25 independent replicates per data point. The ribbons depict standard deviations around the 25-generation mean.

For the Semi-Supervised ILM the ideal bottleneck size is smaller than you might expect.For each bottleneck size considered, 25 replicates of the model are run until values of an XCS language develops. The best value of the bottleneck was estimated and this is plotted inA; the line shows the standard best linear fit, with slope 8.4 and intercept -11.5. The mean number of generations taken to reach thethreshold at this bottleneck size is plotted inBin red, in pink is the mean number of generations when a bottleneck is used which is one away from the optimal value.

SECTION: Acknowledgments

We are grateful to Marcos Oliveira who suggested that we investigate further the surprisingly strong performance of the Semi-Supervised ILM when there is no bottleneck. SB was supported by UKRI grant EP/Y028392/1AI for Collective
Intelligence (AI4CI), and was carried out using the computational facilities of the Advanced Computing Research Centre at the University of Bristol,www.bristol.ac.uk/acrc/. We are grateful to Dr Stewart whose philanthropy provided some of the compute resource used in this project.

SECTION: References