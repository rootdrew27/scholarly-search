SECTION: Transformers Can Navigate Mazes With Multi-Step Prediction

Despite their remarkable success in language modeling, transformers trained to predict the next token in a sequence struggle with long-term planning.
This limitation is particularly evident in tasks requiring foresight to plan multiple steps ahead such as maze navigation.
The standard nextsingletoken prediction objective, however,
offers no explicit mechanism to predict multiple steps ahead—or revisit the path taken so far.
Consequently,
in this work we study
whether
explicitly predicting multiple steps ahead (and backwards) can improve transformers’ maze navigation.
We train parameter-matched transformers from scratch, under identical settings, to navigate mazes of varying types and sizes with standard next token prediction andMLM-, an objective explicitly predicting multiple steps ahead and backwards.
We find thatMLM-considerably improves transformers’ ability to navigate mazes compared to standard next token prediction across maze types and complexities.
We also findMLM-training is 4more sample efficient and converges 2faster in terms of GPU training hours relative to next token training.
Finally, for more complex mazes we findMLM-benefits from scaling to larger transformers.
Remarkably, we find transformers trained withMLM-outperform larger transformers trained with next token prediction using additional supervision from A* search traces.
We hope these findings underscore the promise of learning objectives to advance transformers’ capacity for long-term planning.

Despite their remarkable success in language modeling, transformers trained to predict the next token in a sequence struggle with long-term planning.
This limitation is particularly evident in tasks requiring foresight to plan multiple steps ahead such as maze navigation.
The standard nextsingletoken prediction objective, however,
offers no explicit mechanism to predict multiple steps ahead—or revisit the path taken so far.
Consequently,
in this work we study
whether
explicitly predicting multiple steps ahead (and backwards) can improve transformers’ maze navigation.
We train parameter-matched transformers from scratch, under identical settings, to navigate mazes of varying types and sizes with standard next token prediction andMLM-, an objective explicitly predicting multiple steps ahead and backwards.
We find thatMLM-considerably improves transformers’ ability to navigate mazes compared to standard next token prediction across maze types and complexities.
We also findMLM-training is 4more sample efficient and converges 2faster in terms of GPU training hours relative to next token training.
Finally, for more complex mazes we findMLM-benefits from scaling to larger transformers.
Remarkably, we find transformers trained withMLM-outperform larger transformers trained with next token prediction using additional supervision from A* search traces.
We hope these findings underscore the promise of learning objectives to advance transformers’ capacity for long-term planning.

SECTION: 1Introduction

Transformers trained to predict the next token in a sequence have become the de facto approach in today’s best language models(Dubey et al.,2024; Gemma,2024). Despite their remarkable success, such transformers encounter challenges when tasked with planning and decision-making over extended horizons. This limitation becomes particularly evident in tasks requiring foresight such as maze navigation.

To effectively navigate a maze, a model must have the foresight to plan ahead multiple steps.
The de facto next token prediction training approach, however, offers no explicit mechanism to predict multiple steps ahead or revisit the path taken so far.
The model is trained to only predict the next step in the input sequence given the previous steps. Prior work has shown next token prediction can fall prey to shortcuts in navigation tasks, particularly as path complexity increases(Bachmann & Nagarajan,2024).
Consequently, we ask:Can explicitly learning to predict multiple steps ahead (and backwards) improve transformers’ ability to navigate mazes?

To answer this question,
we isolate the effect of learning objectives by training transformers from scratch to navigate mazes.
Inspired by prior work to remedy shortcomings of next token prediction(Bachmann & Nagarajan,2024; Gloeckle et al.,2024), we explore the
theMLM-objective fromKitouni et al. (2024a)as an alternative to next token prediction.MLM-proposes
masking arbitrary subsets of the input sequence to explicitly predict a variable number of steps ahead and backward as shown inFigure1.
We then assess whetherMLM-by explicitly predicting multiple-steps during training can improve transformers’ performance on maze navigation.

We operate with a collection of mazes with varying levels of grid-size complexities.
Two common types of mazes generation approaches are studied that differ in shortest path solution lengths as well as maze text representations.
For one setting, we train transformer models for both objectives, standard next token prediction andMLM-. In the other setting, we compareMLM-against published results on next token training fromLehnert et al. (2024).
Finally, we compare learning objectives across several transformer model sizes by measuring maze navigation, data sample efficiency, as well as training efficiency in terms of GPU hours to convergence.

Our results indicateMLM-can improve maze navigation accuracy and training efficiency compared to standard next token prediction.
Remarkably, we find a transformer trained withMLM-outperforms larger transformers trained with next token prediction using additional supervision from A* search traces(Lehnert et al.,2024). Specifically, relative to standard next token prediction training, we find that:

MLM-considerably improves transformers’ ability to navigate mazes.

MLM-outperforms comparable next token transformer models across every maze type and grid size complexity tested. For example, an 8M parameter transformer trained withMLM-can perfectly solve all mazes of grid sizes up to 20x20, whereas next token training peaks at 20.6% navigation accuracy on held-out 20x20 test mazes (shown inFigure1).

MLM-outperforms next token transformers trained with additional A* search trace supervision on complex mazes. For example, on 30x30 mazes an 8M parameter transformer reaches 85.5% navigation accuracy withMLM-, improving on the 70.2% navigation accuracy of a 175M parameter transformer trained with next token prediction and additional A* search trace supervision.

MLM-training is 4x more data-efficient in terms of training samples. For simpler mazes (5x5) solved by bothMLM-and next token prediction,MLM-is 2x more efficient in GPU hours needed for convergence.

MLM-benefits from scaling to larger transformers for more complex mazes. For example scalingMLM-from a 3M to an 8M parameter transformer boosts performance from 85% to perfect navigation on 20x20 mazes.

These findings suggest that the learning objective is critical to transformer’s maze navigation abilities, offering a promising direction for future research in long-horizon planning tasks.

SECTION: 2Related Work

Ivanitskiy et al. (2023b)show transformers trained on maze navigation tasks learn internal states that allow a decoding of the entire maze. Despite this emergent state however,Bachmann & Nagarajan (2024)shows the limits of next token prediction objectives for basic graph navigation tasks. In particular, the work identifies a Clever-Hans cheat based on shortcuts in teacher forced training similar to theoretical shortcomings identified inWang et al. (2024b). This demonstrates that while transformers can represent world states for mazes, they may struggle in planning that requires significant foresight. A remedy found byBachmann & Nagarajaninvolves removing the teacher forced supervision. Their view inspired us to look further into the training objective to encourage more explicit planning.

Many deep learning approaches for maze navigation use reinforcement-learning objectives(Akmandor et al.,2022; Wang et al.,2024a; Tamar et al.,2016; Wang et al.,2024c; Kong et al.,2024).Liu & Borisyuk (2023)compares the navigation strategies learned by reinforcement learning to those observed in animals suggesting some similarities in learning dynamics.Janner et al. (2022)study reinforcement learning reward modeling with a diffusion objective with applications to planning tasks including maze navigation. While reinforcement learning approaches excel at tasks involving interaction and games, reinforcement learning has played a relatively minor role in foundation model pretraining.Outside of reinforcement learning approaches,Lehnert et al. (2024)successfully train transformers with the next token objective to perform maze navigation. Crucially, they can vastly improve performance via additional supervision. By exposing the model to a trace of an A* algorithm solving the maze, they gain significant performance and data efficiency. Interestingly, just like inBachmann & Nagarajan (2024), the remedy to failure on a navigation task seems to involve changing the supervision structure. We directly compare this approach with theMLM-objective trained without any supervision from A* search traces.

Kitouni et al. (2024a)usedMLM-, which can be seen as a diffusion objective(Austin et al.,2021; Kitouni et al.,2024b), to mitigate the reversal curse in language modelling(Berglund et al.,2024), where models trained to answer questions in one way can not generalize to an inverse, semantically equivalent formulation. They also show thatMLM-performs well in the graph navigation task fromBachmann & Nagarajan (2024).Sahoo et al. (2024); Austin et al. (2021); Li et al. (2022)incorporate diffusion objectives in masked language modeling for general purpose language models.He et al. (2022)adds a diffusion objective to further train a pretrained BERT model showing improvements over standard BERT training in terms of perplexity and BLEU score on language tasks.

SECTION: 3The role of learning objectives in maze navigation

We examine how the standard next token learning objective manifests itself in maze navigation, a task requiring planning multiple steps head. We contrast next token prediction withMLM-, a training objective explicitly encouraging predicting multiple steps ahead and backward.

SECTION: 3.1Predicting the next step with standard training

The de facto learning objective used to train language models is next token prediction. This objective, which is also referred to as an autoregressive (AR) or causal-masked prediction objective, when paired with the transformer architecture has shown great success in language tasks at scale.
Specifically, given a sequence of inputs, the next token learning objective minimizes

whereindicates the index of the input sequence. This simple objective maximizing the probability of the next token given the previous tokens in the sequence has led to remarkable fluency in language tasks(Dubey et al.,2024; Gemma,2024). However,
transformers trained with next token prediction exhibit limits in terms of planning.

In maze navigation, as shown inFigure1, next token prediction amounts to predictingonly the next stepgiven the path so far. The learning objective inEquation1does not explicitly encourage predicting multiple steps ahead.Bachmann & Nagarajan (2024)suggests the lack of multi-step prediction in standard next token training limits transformers’ ability to navigate even simple graphs. One pitfall highlighted byBachmann & Nagarajan (2024)is that models fall prey to short-sighted shortcuts such as the Clever-Hans cheat, show because the model does not plan far enough ahead.Dziri et al. (2024)show similar limits for other multi-step problems, especially as problem complexity increases.

SECTION: 3.2Predicting multiple steps ahead and back withMLM-

One remedy discovered byBachmann & Nagarajan (2024)avoids supervision through teacher-forcing by allowing the model to predict the entire path before applying a gradient. However, this approach is slow to train, since it requires the sequential generation steps.

Gloeckle et al. (2024)provide an elegant way to reason multiple tokens into the future by having multiple prediction heads. They found this method to have beneficial effects on decoder models of size 13B and above when employing up to 8 prediction heads for the 8 next tokens.
Motivated byGloeckle et al. (2024)we consider an explicit objective predicting multiple tokens both ahead and backwards with a variable, rather than fixed context size. Specifically, we study theMLM-objective fromKitouni et al. (2024a)which predicts any subset of tokens given any others as context, hoping to capture long-term context dependence and explicit multi-step prediction.

MLM-proposes
masking arbitrary subsets of the input sequence to explicitly encourage the model to predict multiple steps ahead and backwards. The masking ratio, which determines the portion of the input that is masked, is drawn uniformly from [0, 1] thereby encouraging a variable prediction window. Specifically, for a uniformly sampled maskwith masking rateover the input sequence, theMLM-learning objective minimizes

whereis the context used for prediction, equivalent to the complement of the masked target elements. Incidentally, this method is reminiscent of BERT(Devlin et al.,2019), but with a uniform masking rate and without token substitution.(Kitouni et al.,2024a, see their Figure 2)argue that since the uniform masking rate exposes the model to different length sequences to be completed and to draw information from, there is no distributional shift in a generative inference step.

For maze navigation, as shown inFigure1, theMLM-objective inEquation2amounts to predicting multiple steps at various points in the navigation path thereby explicitly planning ahead and back multiple steps.

We study the role of the learning objective in maze navigation by comparing standard next token prediction toMLM-. We ask:can modifying only the learning objective to predict multiple steps ahead and back enable transformers to navigate complex mazes?

SECTION: 4Methods

To study the role of learning objectives for maze navigation, we train transformer models from scratch to generate the shortest navigation path for mazes of increasing complexity.
We design our experiments such that transformer models are parameter-matched and trained under identical regimes to isolate the effect of next token versusMLM-learning objectives.
We assess models’ ability to accurately navigate previously unseen mazes as well as their efficiency in terms of training samples and GPU training hours.

SECTION: 4.1Mazes and Their Representations

We consider two maze generation approaches across several levels of grid-size complexities
to ensure our findings are not specific to a single type of maze or representation, but hold more generally.

First, we utilize the maze generation method fromIvanitskiy et al. (2023a)to generate 2 dimensional mazes via the randomized Depth First Search (DFS) method. This method works by constructing a path from a uniformly random start node in a depth-first manner.
This generation approach yields long paths (relative to A* mazes described below), but does not allow ambiguity: the shortest path is also the only path that does not backtrack and thus overlap with itself. An example 10x10 DFS maze in show on the right panel ofFigure2.
The mazes are serialized into strings that enumerate the edges of the maze connection graph as a set of tuples. The start node, goal node and solution path are appended to form the full text that the model trains with. We generate 500k mazes across five levels of complexity as measured by the grid size of the maze spanning 5x5, 10x10, 20x20, and 30x30.

Second, we use the deterministic A* maze dataset fromLehnert et al. (2024). Start and goal cell were uniformly sampled in a 2-dimensional grid with walls randomly placed in 30–50% of cells (see middle panel ofFigure2). The shortest paths are discovered via the A* algorithm and added to the dataset if the shortest path is at least of length, whereindicates the maze grid size (for anxmaze).
In A* mazes, grid cells are tokenized with individual tokens for x and y coordinate, which increases the input sequence length relative to the graph tuple encoding used for DFS.
In both datasets, the solution path is the last part of the string.
In contrast to the DFS mazes, however, A* mazes have many possible solutions, out of more than one are possibly the shortest ones.Lehnert et al. (2024)experiment with both randomly and deterministically (heuristically) choosing the shortest path that the model sees as ground truth. We choose 10x10, 20x20 and 30x30 mazes from the deterministic setting, seeSectionD.2for additional details.

Together these maze generation approaches allow us to study mazes of varying complexities (in terms of grid size), differing distributions of shortest path lengths, as well as different maze text encoding approaches. InFigure2we show the distribution differences between solution path lengths for DFS versus A* mazes across three levels of grid-size complexities. Additionally in the middle and right panels, we show sample generations for DFS and A* mazes.

SECTION: 4.2Standard Next Token Prediction and A* Search Dynamic Supervision

We evaluate the standard next token prediction learning objective for maze navigation. To do so, we train transformers from scratch on text representations of maze solutions similar toIvanitskiy et al. (2023b). Mirroring the objective of modern language models the transformer predicts the next token based on the previous tokens in the maze solution path (seeEquation1). We investigate various transformer model sizes to understand the effect of model scale. We also evaluate the standard decoder-only transformer architecture as well as the encoder-decoder architecture fromLehnert et al. (2024). Finally, to better contextualize our findings we also report the next token model fromLehnert et al. (2024)trained with additional A* search trace supervision for the A* maze setting.

SECTION: 4.3MLM-

We contrast next token prediction with theMLM-objective, explicitly predicting multiple steps both ahead and backward.
We closely follow the training setup inKitouni et al. (2024a), including the encoder-decoder transformer architecture with RoPE positional embeddings (seeSectionsD.1andD.3).
Identical to the next token baselines, theMLM-objective is trained on text representations of the maze solutions.
Generation during inference is done in the same way as for the standard next token baselines, generating one token at a time from left to right, with temperature 0 (argmax).
Since the uniform masking rate inMLM-(seeEquation2) exposes the model to different sequence prediction and context lengths, there is no distributional shift in a generative inference step as shown in Figure 2 ofKitouni et al. (2024a).
ForMLM-, we also train transformers of varying model scales ranging from 3M to 25M parameters to study the effect of model scale on maze navigation.

SECTION: 4.4Experimental setup

To isolate the effect of training objectives,MLM-versus next token prediction, we train all models from scratch using an identical setup.

We train transformers for up to 3000 epochs on 100,000 mazes for each setup. The performance of each model is evaluated on a held-out test set of 2000 mazes with the same configuration as the training set.
To ensure the baseline comparisons for next token prediction are competitive,
we conduct a sweep over learning rate choices and weight decay values (shown inAppendixB). We select the best choice of hyperparameters based on held-out shortest path accuracy for 10x10 DFS mazes. The architecture used to trainMLM-is an encoder-decoder (as inKitouni et al. (2024a), detailed inSectionD.3), but for next token training in DFS mazes we found a decoder-only architecture to be superior to theMLM-encoder-decoder, seeSectionA.2.
For A* mazes, we report the best available numbers fromLehnert et al. (2024)for next token prediction.

We evaluate models in terms of maze navigation accuracy, data efficiency as measured by the number of training mazes, and training efficiency in terms of GPU training hours needed for convergence.
To assess the correctness of a generated path similar toLehnert et al. (2024)we compare whether the full path matches the shortest path. We additionally compare the token-wise accuracy inSectionA.1to assess navigation paths that only slightly deviate from the shortest path. Finally, to complement the overall maze navigation accuracy, we assess training dynamics by comparing convergence curves on training and held-out tests mazes.

SECTION: 5Results: Learning to Navigate Mazes withMLM-Training

We compare the next token andMLM-objectives via maze navigation accuracy across three dimensions: maze complexity, training data efficiency and computational efficiency. We also investigate scaling laws as well as analyze the training dynamics ofMLM-.

SECTION: 5.1MLM-and standard next token training in DFS mazes

First, we compare the objectives in the setting with DFS generated mazes described in the first part ofSection4.1.
We train 8M parameter transformer models across mazes with grid sizes ranging from 5x5 to 30x30. We findMLM-is able to perfectly navigate mazes of up to a grid size of 20x20 and achieve nearly 3x the performance of next token training on more complex 30x30 mazes as shown inTable1. For example, even on comparatively small mazes of size 10x10 we find next token performance saturates below 50% accuracy. In contrast, a model of the same size can navigate 30x30 mazes with over 90% accuracy when trained withMLM-.

To evaluate the data efficiency ofMLM-relative to that of next token, we train 8M parameter transformer models while varying the number of mazes seen during training. We operate on maze sizes of 5x5 and 10x10 and train both models for 2000 epochs.
As shown inFigure3, we findMLM-is able to navigate both 5x5 and 10x10 mazes with only 25k training samples, while next token requires all 100k mazes to reach full accuracy in 5x5 and reaches a peak performance of less than 50% with 75k training samples, suggestingMLM-is 4more data efficient.

We compare the convergence rates both on training and held-out 5x5 mazes forMLM-and next token prediction. We choose this small setting because this is solvable by both objectives.
We find as shown inFigure4

MLM-converges 2.17x faster in terms of the number of training epochs.
We additionally control for computational overhead in terms of GPU training hours, we find training on the same data for 2k epochs using 8M parameter transformers on 8 Tesla V100 32GB GPUs takes 13.7 hours for next token versus 17.7 hours forMLM-. Accounting for this additional 7% overhead, we find as shown inFigure4MLM-ismore efficient than a comparable next token model on small DFS mazes. As a caveat, we note that on 10x10 mazes, next token training crosses the 40% performance threshold faster thanMLM-, indicating faster initial learning before saturating at peak of 46% accuracy on held-out test mazes.

SECTION: 5.2MLM-and next token training with A* Mazes

In this section, we train models withMLM-on the deterministic A* maze dataset fromLehnert et al. (2024)as described in the second part ofSection4.1.
We compare those models to the ones trained inLehnert et al.with and without additional supervision from A* search traces.
For example, a nearly 2x larger 15M parameter transformer trained with next token prediction achieves 13.3% navigation accuracy on 30x30 mazes whereasMLM-reaches 85.5% navigation accuracy.
The results can be found inTable2. The 8M parameterMLM-trained transformer compares favorably with all models fromLehnert et al.trained on 100k mazes. This holds true even when aiding the training with additional supervision provided by the A* search trace, which boosts next token training by a significant margin.

SECTION: 5.3Understanding the training dynamics of MLM-U Compared to next token

We compare the convergence rates both on training and held-out 10x10 DFS mazes for MLM-U compared to next token parameter-matched 8M parameter models inFigure5. Although we observe faster training convergence for next token models as shown on the left, we see the next token model is not able to generalize from the training data, with performance saturating at around 50%, while MLM-U is able to perfectly solve 10x10 mazes.
This suggests while next token training is susceptible to overfitting, whereMLM-exhibits good generalization without overfitting. We attribute this to the increased difficulty of the objective.MLM-is tasked to predict any subset of path tokens from any other, while next token training only ever sees the same sequence of conditionals for each maze.

Here, we investigate the effect of scaling transformer model size for 20x20 DFS mazes, one the more challenging settings where next token training yields 22% accuracy. As shown inFigure6MLM-training improves navigation accuracy from 85% to perfect navigation accuracy when transformer model size is scaled from 3M to 8M parameters.
For next token prediction, we also observe improvements with transformer model scale, but at a relatively slower rate.
A more than 8x increase in model size, from 3M to 25M, for a model trained with the next token objective yields a 43% relative performance improvement.

SECTION: 5.4Positional encodings need more floating point precision

As we scaledMLM-training to more complex mazes, we found the precision of the positional encodings to be particularly important for good maze navigation performance.
Unlike the learnable ((Radford et al.,2019)) and sinusoidal encodings in the
original transformer paperVaswani et al. (2023)which are added to the input,MLM-uses Rotational Positional Encodings (RoPE,(Su et al.,2023)), which
bias the query and key vectors in the attention mechanism as a function of their relative positions.
To better understand the role of these positional embedding precision we train an 8M parameter transformerMLM-on a small set of 100 DFS mazes with increasing grid size complexities.
We found with 16-bit precision positional encodings (float 16 via the automatic mixed precision, AMP, package in PyTorch)
as shown inFigure7(right),MLM-generally predicted the correct paths, but failed get the exact positions right, skipping some and duplicating others, resulting in low navigation accuracy on more complex (25x25 and larger) training mazes.

With full 32-bit precision positional encodings however, we foundMLM-was able to reach perfect navigation accuracy even on these more complex mazes. For example, as shown inFigure7on 30x30 mazesMLM-only reached 50% navigation accuracy with 16-bit positional encoding precision whereas with 32-bit positional encodingsMLM-solved 30x30 mazes perfectly. This suggests for larger grid sizes, higher precision in the positional encoding allowed the model to properly map the learned paths to their proper positions on the maze. We observed a similar improvement in performance with larger training data (100k samples) on 30x30 DFS mazes. In particular, by increasing the precision from 16 to 32-bits for positional encodings,MLM-performance on 30x30 DFS mazes improved from 40% to 93.8% highlighting the importance of higher positional encoding precision.

While positional encodings have been tailored to next token prediction objectives, less emphasis has been placed on the best positional encoding strategies for masking objectives such asMLM-.
Consequently, the above observations lead us to question whether current approaches are optimal for objectives such asMLM-.
A promising path for training on more complex mazes with larger grid sizes could stem from a better understanding of how best to encode positions for longer-term planning objectives.
Therefore, we consider the detailed study of positional bias in masking objectives likeMLM-crucial for future work.

SECTION: 6Discussion

By adjusting the learning objective from next token prediction to one that explicitly predicts multiple steps ahead and back (MLM-), we show transformers can learn to effectively navigate mazes.
Fortunately, training with an explicit multi-step objective is also more efficient both in terms of training samples as well as GPU training hours and offers nice model scaling benefits with maze complexity.

We hope these findings spur the research community to explore learning objectives as a lever to address one of the main limitations of today’s best transformer models: multi-step planning. In future work we hope to explore the role of learning objectives in a broader range of multi-step planning tasks.

Of course, such an approach also comes with the typical limitations of transformers, including a fixed context length, which can limit or degrade the training speed of transformers as maze size grows. We observed the importance of positional encodings inMLM-training, particularly for more complex mazes. We suggest that there is more understand about the role of positional encodings for planning and identify this as important future work.
Furthermore, we acknowledge the increased hardness of theMLM-objective. Instead of predicting the same token always with the same context, the context is randomly sampled every time the same training data is observed. For a sufficiently long sequence, the model will never see the same problem twice due to the exponentially increasing number of possible contexts. We cannot say how this impacts generalization speed in general, although we saw some favorable evidence in this work.
In an effort to keep the comparison as straight forward as possible, we usedMLM-exactly as described inKitouni et al. (2024a). However, multiple improvements are possible. At inference time, it might be beneficial to generate tokens according to some heuristic about model certainty as opposed to left-to-right. Additionally, the uniform masking rate applied the same way to each token is certainly the simplest, but unlikely the optimal method. A semantic heuristic could favorably impact performance. A possible intuition here is that for many mask realizations, the problem is too easy or too difficult for the model, and it wastes time in those batches. Instead, over-sampling masks that make the problem hard but solvable might yield vastly increased convergence speeds.

In all, these findings shine light on a promising path forward for research to improve long-horizon planning in transformers, with lots of potential for future work.

SECTION: References

SECTION: Appendix AAdditional Results

SECTION: A.1Per Token Results

To evaluate the possibility of the generated paths deviating only slightly from the shortest paths, we also compute the token-wise accuracy of the generated paths compared to the shortest path.
InTable3andTable4we present per-token accuracies for the experiments fromTable1andTable2.

SECTION: A.2Comparing transformer models for Next Token training

We compare two choices of architecture for autoregressive training with transformers: 1) the standard decoder architecture commonly used in modern language models, 2) the encoder-decoder architecture used for MLM-U. We train two 8M parameter transformer models with each of these architectures on 100k DFS 10x10 mazes and evaluate performance on held-out mazes. As shown inFigure8, we find the common decoder-only architecture converges more quickly and generalizes better than the comparable encoder-decoder architecture. We use the stronger decoder-only baseline for our experiments.

SECTION: Appendix BAblations for Hyperparameters

We conduct hyperparameter ablations for learning ratesFigure9and weight decay inTable5.
We train the next token model with 8M parameters for 500 epochs on 100k 10x10 training mazes and evaluate per-token held-out accuracy to select the best learning rate. Based on this sweep we select 0.001 as the learning rate we use for all our experiments. ForMLM-we found learning rates to have negligible effect beyond an upper bound to ensure training stability. We select 0.001 as well. We found large weight decay values to be detrimental for next token training, seeTable5. InMLM-, we generally don’t see overfitting and therefore also don’t need any weight decay. We choosefor next token and no weight decay forMLM-. We found training to be most stable with the AdamW optimizer with beta valuesandand batch sizes of 128 and above.

We evaluate models of two different sizes: 8M parameter models with a width of 128, a depth of 40 and 4 heads per attention layer. For 25M parameter models, the width is 256 with a depth of 32 and also 4 heads per attention layer. In the case of an encoder-decoder, both encoder and decoder have depth/2 layers.
During development of the experiments, we found that deeper models generally do slightly better in the 8M parameter setting, both innext token training and inMLM-.

SECTION: Appendix CMLM-and Next token Failure Modes

InFigure10we give some visual examples ofMLM-failure modes on 30x30 DFS mazes using the 8M model fromSection5.1. Often, the general path taken is mostly correct, but it takes a wrong turn or two and then backtracks to follow the right track, possibly ending up only a few steps short of the goal node.Figure11shows example failure cases of the next token model. Often, there is a general tendency towards the right path, but we find frequent backtracks, traversals through walls and often completely wrong end points.

Figure12shows failures for the 8M model trained on the A* mazes, fromSection5.2. Note that in two of those failure cases (bottom left and right), the paths predicted are equivalent shortest paths. However, since we are checking for exact match in the deterministic A* setting fromLehnert et al. (2024), those count as faulty. In those instances, the model does not seem to have picked up the way in which symmetry between shortest paths is broken in the deterministic dataset.
Note that there also exist other failures that cause parsing errors and can therefore not be depicted. Those make up about half of all failure cases in the validation dataset for this 8MMLM-model.
The failure cases inFigure13for the 30x30 A* maze case are conceptually similar. However, the model fails in some additional ways. For instance, it sometimes misses –or malforms– a step, which ends up being displayed as a diagonal move (left top and bottom). Or it predicts traversal through a wall (top right).
The bottom right path is a proper shortest path, but the model does not predict the last move correctly.

SECTION: Appendix DMore details on the Experimental Setup

SECTION: D.1MLM-training

TheMLM-models are exposed to the same maze representation, start and end cells and subsequent solution path.
Unlike the next token baselines the loss is not a next token prediction loss, but a masking loss reminiscent of the BERT training objective. Tokens are masked with a specific probability and the objective judges the model predictions on the masked tokens via the cross-entropy. In BERT, the masking rate is fixed, butMLM-draws masking rates uniformly for each batch.Kitouni et al. (2024a)give an intuition for why uniform masking rates are advantageous. Since the uniform masking rate exposes the model to different length sequences to be completed and to draw information from, there is no distributional shift in a generative inference step, see Figure 2 inKitouni et al. (2024a).

For this specific case of maze navigation, the only tokens that can be masked are part of the solution path. The model is never tasked to predict the maze representation or start or goal cells.Kitouni et al. (2024a)report that theMLM-objective is best trained with a specific encoder-decoder architecture. The encoder has blocks in the layout of GPT-2 with a RoPE positional bias. The decoder input is a sequence of multiple copies of the same learnable token such that the decoder only has information about the positional bias via RoPE. See implementation details inSectionD.3.

SECTION: D.2Maze Generation Details

We study two different kinds of mazes in this work. They have different properties and are represented in different formats. With that, we aim to demonstrate that our findings are not specific to a single type of maze or representation, but hold more generally.

First, we utilize the maze generation method fromIvanitskiy et al. (2023a)to generate 2 dimensional mazes via the randomized Depth First Search (DFS) method. This method works by visiting all grid cells in a depth-first manner. From a uniformly random start node, it uniformly picks a neighbor cell and removes walls between both cells whenever the target cell was not previously visited. If a cell does not have unvisited neighbors, it is declared a dead end and the algorithm backtracks until a cell with unvisited neighbors is found, starting a new "descent", like in standard depth first tree search. A goal cell is uniformly sampled. This generation algorithm makes for long paths, but does not allow ambiguity. The shortest path is also the only path that does not backtrack from dead ends.
The mazes are serialized into strings that enumerate the edges of the maze connection graph as a set of tuples. The start node, goal node and solution path are appended to form the full text that the model trains with. We generate 100’000 mazes for each maze dimension, spanning 5x5 to 30x30.

Second, we use the deterministic A* maze dataset fromLehnert et al. (2024). Start and goal cell were uniformly sampled in a 2 dimensional grid. Mazes were generated by randomly selecting 30-50of the cells to be walls and A* was used to solve those mazes. For anxmaze, the sampled problem is added to the dataset if the solution path is at least of length. In contrast to the DFS mazes, these mazes have many possible solutions, out of more than one are possibly the shortest ones.Lehnert et al. (2024)experiment with both randomly and deterministically (heuristically) choosing the shortest path that the model sees as ground truth. Also unlike the DFS mazes, the text representation describes the set of walls rather than connections and puts the goal and final cell before everything else. In both datasets, the solution path is the last part of the string. Following, the setup inLehnert et al. (2024)we train on mazes of varying complexities with grid sizes
10x10, 20x20 and 30x30. We train only 100k mazes and reserve 2k mazes each for validation.

For a direct comparison of the maze setups, refer toFigures15and15. They depict how the prompt and response are made from maze instantiations of the A* and DFS type.

Notably, the tokenizers for A* and DFS mazes treat cell representations differently. In DFS mazes each grid cell is one distinct token. This is done to avoid making the sequences too long. In A* mazes, grid cells are tokenized with individual tokens for x and y coordinate. We believe this presents a better inductive bias than individual tokens for each grid cell, but also increases the sequence length significantly. Since the solution paths are generally much shorter in these mazes, the extra sequence length is affordable. SeeFigure2for a comparison of path lengths between A* and DFS mazes.

SECTION: D.3Implementation of Encoder-Decoder

Here we show the exact encoder-decoder algorithm used forMLM-training on mazes, as it differs slightly from traditional models. Specifically, the difference lies in the fact that the decoder only sees a sequence of equal embeddings and only gathers information about the mazes from the cross attention with the encoder. Positional information is brought in via RoPE on queries and keys.

SECTION: Appendix EMiscellaneous experiments

SECTION: E.1Ordered masks

One of our motivations for utilizing a training scheme likeMLM-is that such a scheme enables more explicit reasoning over tokens that are further in the future than the immediate next token, hopefully aiding longer-horizon planning.
In light of this view we evaluate the following ablation: InMLM-each token in the solution path is masked with some (uniformly drawn) probability, independently of other tokens. Instead, we uniformly pick a position in the solution path and mask all tokens to the right of this position. Then we predict all of those tokens as a function of the context to the left of the chosen position. This method relates closer to the method used to solve the Star-Graph problem inBachmann & Nagarajan (2024). However, we find that this method is far inferior toMLM-in the 10x10 A* maze setting tested. The maximum per-token accuracy observed is 73%, with less than 4% full path accuracy.

SECTION: E.2Generalization to smaller mazes

To see whether and how MLM-U and next token trained models perform out of their immediate training distribution, we evaluate models trained on 20x20 DFS mazes on smaller (10x10) mazes. Limitations in length generalization prohibit non-zero accuracies on larger mazes, but experiments on smaller mazes yield interesting results, seeTable6. In all experiments, we tokenize the 10x10 mazes via the 20x20 tokenizer. This is important because the 10x10 and 20x20 tokenizers in our training methods assign different tokens to the grid cells. While next token trained decoders can achieve non-trivial accuracy on smaller mazes out of the box, changing only the tokenizer,MLM-can not.In order to recover good performance inMLM-, we embed the 10x10 maze into the upper left corner of a random 20x20 maze in an effort to bring the smaller maze closer to the training distribution.