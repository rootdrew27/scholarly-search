SECTION: Deep Fourier-embedded Network for Bi-modal Salient Object Detection

The rapid development of deep learning provides a significant improvement of salient object detection combining both RGB and thermal images. However, existing deep learning-based models suffer from two major shortcomings. First, the computation and memory demands of Transformer-based models with quadratic complexity are unbearable, especially in handling high-resolution bi-modal feature fusion. Second, even if learning converges to an ideal solution, there remains a frequency gap between the prediction and ground truth. Therefore, we propose a purely fast Fourier transform-based model, namely deep Fourier-embedded network (DFENet), for learning bi-modal information of RGB and thermal images. On one hand, fast Fourier transform efficiently fetches global dependencies with low complexity. Inspired by this, we design modal-coordinated perception attention to fuse
RGB and thermal modalities with multi-dimensional representation enhancement. To obtain reliable detailed information during decoding, we design the frequency-decomposed edge-aware module (FEM) to clarify object edges
by deeply decomposing low-level features. Moreover, we equip proposed Fourier residual channel attention block in each decoder layer to prioritize high-frequency information while aligning channel global relationships. On the other hand, we propose co-focus frequency loss (CFL) to steer FEM towards minimizing the frequency gap. CFL dynamically weights hard frequencies during edge frequency reconstruction by cross-referencing the bi-modal edge information in the Fourier domain. This frequency-level refinement of edge features further contributes to the quality of the final pixel-level prediction. Extensive experiments on four bi-modal salient object detection benchmark datasets demonstrate our proposed DFENet outperforms twelve existing state-of-the-art models. The code of DFENet is available athttps://github.com/JoshuaLPF/DFENet.

SECTION: IIntroduction

Bi-modal salient object detection (BSOD) deals with two modalities. In this paper, we deal with two modalities: RGB and thermal images.

Although RGB-based salient object detection has made considerable strides in recent years[1,2,3], it is difficult to separate salient objects from their surroundings in challenging scenes like complicated backgrounds or low contrast. To address this challenge, depth information was introduced to complement RGB features, enhancing the understanding of scenes[4,5,6,7]. However, the accuracy and stability of depth information are easily affected by light intensity, occlusions, and other factors, limiting its application range. While thermal infrared information offers a natural advantage in environmental adaptability. Even in adverse weather or under-light environments, thermal images can still adequately characterize objects by capturing their infrared radiations. Consequently, RGB and thermal (RGB-T) BSOD is gaining traction[8,9,10,11].

Transformer[12]profits from its excellent ability to acquire global dependencies and outperforms convolutional neural networks (CNNs) in various computer vision tasks[13,14]. Transformer-based models are also continuously emerging in RGB-T BSOD, showcasing impressive performance[15,16,17,18,19,20]. However, there is a nontrivial problem that the memory consumption and complexity of Transformers are overwhelming, particularly in handling the fusion of high-resolution bi-modal features and high-resolution inputs[20]. To mitigate computational expenses, existing Transformer-based models typically employ several strategies: constructing CNN-based fusion modules[15,16,18,17], utilizing CNN-based backbones[18,19], restricting multi-head self-attention (MHSA)-based modules to low-resolution feature fusion[4,21], and additionally developing lightweight student models[22]. The compromise strategies, where only certain components can explore relationships from a global perspective, have lead to less effective results.
This prompts us to question whether there exists an operation that can encapsulate global properties while remaining lightweight. Recently, benefiting from its powerful global representation, fast Fourier transform (FFT)[23]delivers remarkable performance in image enhancement[24,25,26,27]. We notice that FFT is globally based, with each element after the transform representing all previous elements, and boasts memory-friendly consumption and low complexity. Motivated by this observation, we propose the purely FFT-based DFENet that aims at aligning global relationships in every stage for realizing RGB-T BSOD.

Owing to differences in imaging techniques, there exist vast disparities in the information distribution between RGB and thermal modalities.
The two modalities uniformly depict common salient objects with differing degrees of saliency in the spatial dimension and depict distinct salient regions in the channel dimension. To solve this problem, we propose a Transformer-like module, modal-coordinated perception attention (MPA), to model the complementarity of two modalities in both spatial and channel dimensions. In MPA, We adopt a re-embedding strategy to unfold the channel-dimensional FFT in Fourier spatial components of bi-modal, aiming to achieve multi-enhancements of a single representation.

Given that BSOD is a pixel-level dense prediction task, the decoder needs to integrate low-resolution encoder features from different levels to predict the high-resolution saliency map with details. Consequently, high-frequency information should be devoted more attention in the decoding stage. However, different levels of features exhibit distinct inductive biases. Low-level features possess rich edge details alongside messy noise, while high-level features favor semantic information. To tackle these challenges, we design an edge-guided frequency pyramid decoder. Specifically, we first design the frequency-decomposed edge-aware module (FEM) to clarify edge features amidst cluttered textures, thereby guiding the integration of multi-level features. By deeply decomposing features, FEM can extract the edge frequencies efficiently in a multi-step manner. Second, we build a pyramid structure where features are concatenated top-down to each shallower level to alleviate inter-level bias. To tackle the validity and alignment of the features, we apply proposed Fourier residual channel attention block (FRCAB) after each concatenation. Inspired by RCAB[28], in FRCAB, we embed Fourier channel attention (FCA) into residual-in-residual (RIR) structure. RIR enables the model to sidestep abundant low-frequency information and focus on learning high-frequency information. FCA enhances model discrimination learning by considering global dependencies between channels in the frequency domain to adaptively align channel features.

Besides, due to the inherent biases of neural networks, vital visual details tend to be lost as the model deepens. Meanwhile, in the process of reconstructing high-frequency edge information, the phenomenon of spectral bias[29]makes the model tend to skip hard frequencies and learn frequencies with higher priority. However, the spectrum generated by existing edge modules based on spatial domain learning is sparse[16,30,22]. This means that every frequency is treated equally, making it difficult to characterize edges with high accuracy. To solve these problems, inspired by[31,32,33], we propose co-focus frequency loss (CFL) to guide FEM in the frequency domain while optimizing the whole model. In CFL, we measure the prediction and ground truth (GT) through the Fourier frequency distance and introduce original edge frequency information of RGB and thermal modalities as a cross-reference to dynamically weight hard frequencies. Frequency-level refined edge features further improves the quality of the salience map in spatial domain learning.

In summary, the main contributions of this paper are as follows:

We rethink the acquisition of long-range dependencies in BSOD. Inspired by the efficient global representation of FFT, we propose the deep Fourier-embedded network (DFENet), a purely FFT-based model aimed at achieving global relationship alignment in each stage while minimizing memory consumption and complexity, outperforming twelve existing state-of-the-art (SOTA) models on four RGB-T BSOD benchmark datasets. To the best of our knowledge, this is the first Fourier-based supervised model in a series of salient object detection tasks.

We propose modal-coordinated perception attention (MPA) to obtain faithful complementary information of two modalities, which employs a re-embedding strategy to filter the global frequency information in both spatial and channel dimensions.

In the decoding stage, we propose frequency-decomposed edge-aware module (FEM) with feature deeply decomposed to clarify object edges from the chaotic background. We propose Fourier residual channel attention block (FRCAB) for each decoder layer to emphasize the high-frequency information and global dependencies in the channel dimension.

We propose co-focus frequency loss (CFL) by cross-referencing bi-modal to solve hard frequencies during edge feature reconstruction, assisting salience map learning in the spatial domain towards prosperity.

SECTION: IIRelated Work

SECTION: II-ARGB and depth bi-modal salient object detection (RGB-D BSOD)

To address the limitations of RGB-based salient object detection, depth information is introduced to better perceive the shape and spatial relationships of objects. Existing RGB-D BSOD models can be roughly divided into CNN based and transformer based.

CNN-based RGB-D BSOD models typically leverage foundational attention mechanisms, such as convolutional block attention module[34]in[35,36,37,38,39], dilated convolution[40]in[41,42], atrous spatial pyramid pooling[43]in[44,45,46], etc. In addition, Ji et al.[6]proposed an unsupervised network that automatically generates pseudo-labels through iterative follow-up refinement.
In[5], a depth calibration and fusion framework was proposed to overcome the prevalence of noise and ambiguity in depth images. Yao et al.[47]augmented semantic representation by integrating depth maps into the single-stream encoder.

To deal with the limitations imposed by the local connectivity of CNNs, several transformers[12,14]were introduced. Liu et al.[48]introduced three ViTs[13]with shared weights to refine three high-level fused features. A pure transformer network was proposed in[4]for the simultaneous detection of salient objects and edges. Sun et al.[21]introduced swin transformer[14]as a dual-stream encoder and handles high-level features with MHSA. Wu et al.[7]proposed a cross-modality interaction parallel transformer to fuse RGB and depth features, along with a contrast learning strategy. Unlike the above MHSA-based spatial domain models, we propose the FFT-based DFENet for solving BSOD from the frequency perspective. Moreover, DFENet boasts friendly memory requirements and its input resolution can be easily upgraded toon a single GPU (please see Sec.IV-C).

SECTION: II-BRGB and thermal bi-modal salient object detection (RGB-T BSOD)

In complex environments with changing light or occlusion, depth information becomes unreliable. Thermal information depicts the infrared radiation emitted by objects and is more environmentally adaptable. Hence, researchers[8,9,10]introduced thermal modality into salient object detection. Following RGB-D BSOD, recent efforts in RGB-T BSOD primarily focus on inter-modal fusion. In[49,50,51,52,53,11,54], CNN-based fusion strategies were designed to fuse multi-modal, multi-scale, and multi-granular features. Specifically, Song et al.[11]utilized graph structures to reduce the impact of negative information. Inspired by style transfer, Chen et al.[54]devised a modal transfer mechanism to bridge complementary features. In addition, Tu et al.[55]proposed a weak alignment model for RGB and thermal modalities. Cong et al.[56]rethought the role of thermal modality and introduced it according to light variations.

To obtain faithful long-range relationships, researchers introduced transformers for various purposes, such as backbone[15,16,17], post-processing of fused features[18,17], or feature fusion[19]. In[19], Pang et al. proposed a cross-modal and cross-scale transformer and designed a token re-embedding strategy to alleviate complexity. However, due to the substantial memory consumption of MHSA, it is challenging to include transformers in each stage of the model. For example, in[20], a pure transformer model was proposed with swin transformer[14]in encoder and decoder, along with a cross-modal fusion transformer to fuse bi-modal features. Undoubtedly, the computational demands were substantial, requiring four RTX3090 GPUs for training. Several studies[57,58,30]also pursued lightweight modeling, yet their performance remains below expectations. Zhou et al.[22]embraced knowledge distillation, designing a wavelet transform (WT)-based WaveNet aimed at acquiring semantic and geometric knowledge from a complex teacher model[16]. But WT is locally based, fails to capture long-range dependency, and is prone to biases from its teacher model. Considering the above factors, we design a purely FFT-based DFENet. Where FFT is a globally based transform, which has an advantage over MHSA and WT in obtaining global relationships. This property allows each component of DFENet to filter and optimize global information.

SECTION: II-CFast Fourier transform (FFT)-based Networks

Recently, although MHSA-based transformers[13,14]have achieved undisputed excellence, the contradiction between the perceived field size and the computational cost could not be gracefully resolved. In 2021, Rao et al.[59]proposed a simple FFT-based global filter to learn long-distance dependencies with log-linear complexity as an alternative to MHSA. Following this, Guibas et al.[60]constructed the token mixer as a globally continuous global convolution. Further, Tatsunami et al.[61]inserted the concept of data correlation into the global filter to dynamically extract image features. Despite these advancements, prior works have predominantly focused on the spatial dimension, overlooking the exploration of intrinsic discriminative features in the channel dimension. Zhou et al.[62]considered this problem and designed two branches to perform FFT in the spatial and channel dimensions for image restoration.

Meanwhile, researchers also delve into FFT-based supervised models for various downstream tasks, including image enhancement[24,25,26,27], image inpainting[63], image compression[33], and image reconstruction[64,65]. However, there remains a notable scarcity of research on FFT-based supervised models, particularly in the realm of image segmentation[66,67]. Many of these studies merely scratch the surface by focusing superficially on high-frequency and low-frequency components of Fourier signal, without delving into further decomposition and analysis. In this paper, we propose a simple yet practical FFT-based BSOD model that provides insights into the above problems. To the best of our knowledge, this is the first FFT-based supervised model in salient object detection tasks.

SECTION: IIIDeep Fourier-embedded Network (DFENet)

In this section, we introduce our DFENet, showcasing its overall framework in Fig.1. Initially, we employ the FFT-based CDFFormer[61]as a dual-stream encoder to extract features from RGB and thermal modalities, indicated asand, respectively. These features are then processed by modal-coordinated perception attention (MPA), aiming to explore cross-modal fusion features. Subsequently,are input into frequency-decomposed edge-aware Module (FEM) to obtain detailed edge features. In the decoding stage, we design edge-guided frequency pyramid decoder (EFPD) that integrates features in a top-down cascade guided by edge features from FEM. In EFPD, we adopt a pyramid design to cope with object scale variations, where each layer concatenates the corresponding fusion featureas well as features of all previous layers, and undergoes a Fourier residual channel attention block (FRCAB) at the end to generate refined features. Finally, we perform bi-modal learning with proposed co-focus frequency loss (CFL), the binary cross-entropy loss (BCE)[68], and the intersection-over union loss (IoU)[69]to optimize DFENet comprehensively.in the frequency domain facilitates global optimization, prioritizing the resolution of hard frequencies during the edge-frequency reconstruction process. Meanwhile,andin the spatial domain are committed to achieving substantial pixel-wise saliency prediction.

SECTION: III-AModal-coordinated Perception Attention (MPA)

The differences in feature distribution between RGB and thermal modalities, arising from their distinct physical meanings, also harbor a wealth of complementary information. First, RGB modality conveys richer appearance and texture details, whereas thermal modality emphasizes overall shape.
Therefore, the importance of feature channels varies across different modalities. Second, salient objects demonstrate varying degrees of saliency in different modalities, leading to differences in spatial representation. To tackle these challenges, we introduce the FFT-based MPA as a proposed solution. MPA initially obtains spatial Fourier components of two modalities, based on which channel Fourier components are augmented separately, and obtains the frequency representation of enhanced salient objects in the spatial dimension. The spatial-based Fourier component serves as an efficient representation of global information, delving into its channel dimensional representation will further enhance its representation.

The diagram of MPA is shown in Fig.2, which follows the structure of transformer and mainly consists of a modal-coordinated perception filter (MPF), CBS operation, and a DWConv-based FFN. This process can be expressed as:

whereandare the output of MPF and MPA.comprises aconvolution layer, batch normalization (BN), and StarReLU[70].is element-wise addition.is adepth-wise convolution layer.

In MPF, we initially conduct FFT on two-stream features separately in the spatial dimension:

Then, we extract the amplitudeofandoffor the secondary transform, applying FFT in the channel dimension (CFFT):

Next, to augment channel Fourier components, we takeconsisting of twoconvolution layers and a LeakyReLU to enhance the amplitude component, the phase componentofand,of, respectively. After that, channel-enhanced components are composed into complexes and undergo the inverse Fourier transformation in the channel dimension (ICFFT). The process can be denoted as:

To effectively combine channel-enhanced components with original amplitude components, we perform a shortcut connection in each branch. After binding the amplitude and phase in spatial Fourier components of two branches as complexes, we employ element-wise addition to facilitate complementarity of two modalities:

Finally, we perform the spatial Fourier component enhancement. To obtain a reliable spatial representation, we introduce dynamic filter[61], which mainly consist of-dimensional learnable global filtersand a multi-layer perception (MLP)for assigning weights. Recent studies[71,72]have demonstrated that improved data dependency leads to enhanced generalization. Therefore, we introduceandinto dynamic filter to provide initial weights for global filters. The final process can be given as:

and,

where

whereis element-wise multiplication. In this paper,is set to 8 in all experiments.

SECTION: III-BFrequency-decomposed Edge-aware Module (FEM)

The precise identification of object details is crucial for improving detection performance. However, it is well known that low-level features in the first two layers contain not only boundary details but are also cluttered background interference. This situation poses a great challenge for the decoder to make accurate predictions. To clarify boundary details and eliminate noises, we propose FEM, specifically designed to supply the decoder with reliable edge features.

In Fig.3, we provide a depth decomposition of RGB and thermal image pairs in the frequency domain. It can be observed that the amplitude and low-frequency components predominantly encompass style and content information, whereas the phase and high-frequency components encapsulate clear edge details. Leveraging this insight, we devise the pivotal component of FEM, edge frequency extraction block (EFEB), as shown in Fig.4. Initially,undergo processing through the phase enhancement process (PEP). Following integration, the high-frequency component is extracted by an adaptive high-pass filter, and ultimately, the output is refined by a FRCAB (Please see Sec.III-Cfor details).

In PEP, a given inputis performed FFT in the spatial dimension initially, followed by the enhancement of its phase component:

Then, EFEB can be expressed as:

Finally, the outputof FEM in Fig.1can be calculated as:

whereis up-sample, andis concatenation.stands for a series of operations, BN, ReLU and up-sample.

SECTION: III-CEdge-guided Frequency Pyramid Decoder (EFPD)

In neural networks, low-level features hold fine-grained information, while high-level features contain more semantic information. To elegantly balance inductive biases of features at different levels and produce a high-resolution saliency map, we design EFPD, which is guided by edge features from FEM, and the features from MPA are progressively concatenated from higher to lower levels and gradually up-sampled. Following each concatenation, we equip a Fourier residual channel attention block (FRCAB) to deal with frequency feature consistency and reliability issues.

We observe that low-resolution features hold rich low-frequency information and are treated equally in channels, thus hindering the generation of the high-resolution saliency map. To address these issues, we propose FRCAB, which integrates Fourier channel attention (FCA) and the residual-in-residual (RIR) structure, as shown in Fig.5. RIR enables the decoder to concentrate more on high-frequency information. FCA extracts inter-channel statistics to further enhance the discrimination of global information by the model. Given an input, after processing by, its global vector is first extracted using global average pooling (GAP). After that, CFFT is performed andoperations are used to enhance the amplitudeand phasecomponents. Finally, ICFFT is performed and RIR is constructed. FRCAB can be described as:

After obtaining FRCAB, EFPD can be expressed as:

wherestands for the final saliency map.

SECTION: III-DBi-domain Learning

We train our DFENet with various learning strategies in the spatial and frequency domains to generate an accurate saliency map and reliable edge features, respectively.

1) Frequency Domain Learning:Important visual details are lost as the model deepens, as well as the spectral bias phenomenon, can make it difficult for the model to learn high-accuracy edge features. To solve these problems, we propose co-focus frequency loss (CFL) to provide effective guidance for FEM. CFL is designed based on FFT, enabling it to optimize global information while assisting FEM in overcoming hard frequencies, cross-referencing details in original feature spaces, and generating reliable edge features. For every pixelin the frequency domain, CFL can be defined as:

where

denotes the edge of objects in ground truth, obtained by the Canny operator[73].

To concentrate learning on hard frequencies, we present the co-focus frequency matrix (CFM)to diminish the weights associated with simpler frequencies. We utilizeandas co-guidance and compress their channelto 1. As discussed in Sec.III-B, the phase component contains plentiful edge frequencies. Thus, we set amplitude components of,to mean to emphasize their phase components. CFM can be written as:

whereis a scaling factor that is set to 1 in all experiments. The introduction ofandnot only supplies raw frequency information but also enhances the representation of edge information in both modality feature spaces.

2) Spatial Domain Learning:To ensure that decoder features are correct during the streaming process, we construct a multi-level loss withand:

Similarly, the saliency loss is defined as:

In summary, the total lossfor training is denoted as:

wheredenote weight parameters corresponding to different losses. In this paper, we uniformly fix them to 1 without further fine-tuning.

SECTION: IVExperiments

SECTION: IV-AExperimental Setup

1) Datasets:To demonstrate the effectiveness of our proposed model, we conduct a series of experiments on four publicly available RGB-T BSOD benchmark datasets: the VT821[8], VT1000[9], VT5000[10], and VI-RGBT1500[11]datasets. The VT821 dataset contains 821 pairs of RGB-T images from different environmental conditions, and its missing regions in the thermal images pose a significant challenge to the task due to manual alignment. The VT1000 dataset comprises 1000 aligned RGB-T images with detailed annotations tailored for evaluation, providing a reliable benchmark for performance comparison. The VT5000 dataset offers 5000 pairs of high-resolution, low-bias RGB-T images that cover a broad range of challenges and scenes. The VI-RGBT1500 dataset consists of 1500 RGB-T image pairs captured under different illuminations, to validate the benefits brought by the thermal modality. In this paper, as in previous works[19][56], we take the 2500 RGB-T image pairs from the VT5000 dataset as the training set, and the remaining along with the other three datasets as the test set.

2) Evaluation Metrics:To completely evaluate the results of each experiment, we introduce six widely used evaluation metrics: E-measure[74]considers the image level statistics and local pixels difference between the prediction and the ground-truth. S-measure[75]evaluates the structural similarity between the prediction and the ground-truth. F-measure[76]is a comprehensive metric, calculated by weighted harmonic averaging on precision and recall. Weighted F-measure[77]is considered here as a complement to F-measure
to overcome problems such as interpolation defects, dependency defects, etc. Mean absolute error[78]denotes the average discrepancy between each pixel in the prediction and the ground-truth. Precision-recall (PR) curve[19]is drawn based on precision-recall pairs, with the curve closer toindicating better model performance.

3) Implementation Details:We provide two versions of DFENet, medium (-m) and big (-b). Their backbone networks are initialized with the training parameters of CDFFormer-m and CDFFormer-b[61], respectively. The remaining parameters in models are initialized according to default settings in PyTorch. Specifically, our models are trained on an NVIDIA RTX3090 GPU following Adam optimization[79]with a batch size of 5 for 160 epochs. The initial learning rate of the training is 3e-5 and decays to one-tenth for every 80 epochs. Besides, we set the input size toand perform a series of data augmentation operations like random cropping, horizontal flipping, and multi-angle rotation to improve the model generalization capability.

SECTION: IV-BModel Comparison

To verify the effectiveness of proposed DFENet, we conduct a comparison with twelve SOTA RGB-T BSOD models, including CSRNet[57], MIDD[53], OSRNet[58], SwinNet[16], ADF[10], TNet[56], MGAI[11], LSNet[30], IFFNet[15], HRTransNet[17], CAVER[19], and WaveNet[22]. For a fair comparison, all saliency maps used for the model comparison are acquired from the corresponding homepages or by running open-source code.

1) Quantitative Evaluation:TableIand Fig.6present a comparison of six evaluation metrics between our DFENet and twelve SOTA models. In summary, DFENet outperforms existing SOTA models across all six metrics on four benchmark datasets. This superiority can be attributed to the exceptional information discrimination and global comparison capabilities of our DFENet, along with the proficiency of CFL in optimizing global frequencies and resolving hard frequency issues.

Specifically, our DFENet-b surpasses the previously best-performing WaveNet across all four datasets, especially in,, andwith an average lead of 0.58, 0.71, and 1.14. Furthermore, on the VT821 dataset, our DFENet-m achieves comparable performance to WaveNet, boasting a slight lead of 0.54and 0.93inand, respectively, while experiencing a minor lag of 0.21, 0.11, and 0.20in,, and. Similarly, on the VT1000 dataset, our DFENet-m holds a lead of 0.42inbut shows a slight lag of 1.4in. However, on the more challenging VT5000 and VI-RGBT1500 datasets, our DFENet-m has a substantial lead compared to WaveNet, with,,,, andleading by an average of 0.64, 0.39, 0.85, 1.5, and 0.10, respectively.

In addition, the comparison of F-measure and PR curves in Fig.6further validates the leading performance of our DFENets. In therow of Fig.6, F-measure curves of DFENets demonstrate the highestacross various thresholds. In therow of Fig.6, PR curves of DFENets are closer to the point with coordinates (1,1) compared to those of other SOTA models. This finding indicates DFENets’ superior balance between precision and recall, further demonstrating their outstanding performance.

2) Qualitative Evaluation:Fig.7illustrates saliency maps of our DFENets and twelve existing SOTA models in various complex scenes. Compared to other SOTA models, our DFENets can more accurately detect salient objects in complex scenes. Even in cases where one modality fails locally (-,,, androws) or both modalities provide poor-quality information (and-rows), DFENets can mitigate noise and extraneous contextual cues, enabling precise delineation of salient objects from the background. This is because the FFT-based MFA reliably captures long-range dependencies and effectively bridges bi-modal complementary information. Moreover, in fine-grained object detection, existing models, including WaveNet, incur losses in object depiction. In contrast, our DFENets excel in capturing intricate object details, such as the legs of a dog (row), a person on a motorcycle (row), and the frame of a chair (row). This superiority can be attributed to FEM, FRCAB, and CFL. FEM provides reliable edge details through the depth decomposition of features. FRCAB extensively embedded within the model enhances its robustness, further improving its performance in various scenes. More critically, CFL effectively addresses the problem of lost visual signals and hard frequencies unattended in the frequency domain.

SECTION: IV-CAblation Study

To comprehensively assess the effectiveness of each component, based on our DFENet-m, we conduct ablation experiments on the VT821, VT1000, and VT5000 datasets by comparing models with (w/) and without (w/o) them, and replacing them with classical methods that operate similarly. We present quantitative and qualitative evaluations of ablation experiments in TableII, Fig.8and Fig.9, respectively.

1) Effectiveness of backbone network:In DFENet-m, we utilize CDFFormer-m[61]as the dual-stream backbone, which has a complexity comparable to ResNet-50[80]but boasts a more powerful feature representation capability. To validate its effectiveness, we select three other FFT-based backbones for comparison, including GFNet[59], AFNO[60], and DFFormer-m[61]. They share the same dimension and network depth as CDFFormer-m. In the-rows of TableII, the model equipped GFNet and AFNO yields unsatisfactory results. Specifically, the evaluation metrics across three datasets indicate inferior performance compared to the model with CDFFormer-m. This trend is further highlighted in Fig.8, where they struggle to accurately detect salient objects in complex scenes. Meanwhile, the model with DFFormer-m demonstrates uneven performance on three datasets. In therow of TableII, while it exhibits a slight advantage in the VT821 dataset, its performance lags behind that of the model with CDFFormer-m on the remaining two datasets.

In addition, we attempt to apply spatial domain-based backbones such as VGG[81], ResNet[80], and swin transformer[14]to DFENet. However, instability (loss reported as “Nan”) occurs during training, which may originate from the unstable gradient caused by incompatibility between spatial domain operations and FFT-based operations.

2) Effectiveness of MPA:First, we remove MPA and replace it with element-wise addition, labeled ”w/o MPA” in TableIIand Fig.8. We observe that the model lacking the crucial component MPA experiences a obvious performance decline on three datasets. Not only do all evaluation metrics deteriorate, but the model also forfeits its capability to extract complementary information from RGB and thermal modalities. In Fig.8, when a part of salient objects in one or both modalities shows a different appearance, the model without MPA encounters considerable interference, impeding the accurate detection of complete salient objects.

Second, to further validate the effectiveness of MPF, the core of MPA, we replace MPF with the widely recognized multi-head cross-attention (MHCA)[4,19]. Given the considerable complexity and memory demands of MHCA, we apply it to the last three low-resolution layers, while the largest resolution layer is still equipped with element-wise addition. As can be seen in therow of TableII, the model with MHCA does not perform as well and even has less gain than the model with element-wise addition. Additionally, as illustrated in Fig.8, it struggles to identify salient objects in scenes with multiple objects. The above findings reaffirm the limitations inherent to spatial domain operations in FFT-based models.

Finally, we visualize in Fig.9the feature alterations after processing by MPA. Even in instances of localized failure within the RGB (case 1) or thermal modalities (cases 2 and 3), MPA demonstrates remarkable resilience, successfully fitting both modalities without succumbing to noise interference. The outcomes of the above experiments collectively reinforce the high efficacy of MPA.

3) Effectiveness of FRCAB:To verify the effectiveness of proposed FRCAB, first we remove it and label “w/o FRCAB” in TableIIand Fig.8. We note a significant decrease in metrics,,,, andon three datasets, with an average reduction of 1.12, 0.87, 1.69, 2.16, and 0.12, respectively. The visualization results give the same conclusion, when removing the FRCAB, the model would be susceptible to cluttered backgrounds, lighting variations, and thermal imbalances.

Then, we replace proposed FRCAB with RCAB[28], labeled ”w/ RCAB” in TableIIand Fig.8. Comparing theand last rows of TableIIreveals that while the RCAB-equipped model performs admirably, it consistently falls short of the FRCAB-equipped model across all metrics on three benchmark datasets. Clearly, FRCAB exhibits superior channel alignment capabilities compared to RCAB, which can stably improve the model’s robustness.

In Fig.9, the features integrated by the pyramid strategy are further refined by FRCAB. Alongside showcasing the effectiveness of EFPD, it demonstrates the formidable performance of FRACB. Based on the acquisition of channel dependencies, FRCAB empowers the model to prioritize high-frequency information.

4) Effectiveness of FEM:To validate the gains made by proposed FEM, we remove it and present results in therow of TableII. The comparison reveals that without FEM, all metrics of the model exhibit a decrease to varying degrees. Furthermore, the visualization results depicted in Fig.8fail to portray complete object boundaries. In contrast, as illustrated in Fig.9(a), FEM can consistently provide clear boundary details, regardless of the complexity of the scene, which can offer a powerful boost to the model.

5) Effectiveness of CFL:To observe CFL’s impact, we replace it using BCE loss, denoted as ”w/ BCE” in TableIIand Fig.8. The performance of the BCE-equipped model is weaker than the CFL-equipped model on three datasets. Moreover, to visually highlight the significance of CFL, we display the 3D frequency spectrum of results obtained by both the model with CE and with CFL in Fig.10. It becomes evident that 3D spectrograms constructed with CE supervision produce oscillations and frequency distributions that deviate further from GT. Conversely, the presence of CFL optimizes the loss of frequency reconstruction and brings the frequency statistics closer to GT. Both of these findings can support the importance and effectiveness of CFL within DFENet.

6) Increase input resolution:The MHSA/MHCA-based operations are unsuitable for high-resolution bi-modal fusion due to their excessive memory consumption and complexity. Conversely, FFT-based operations possess inherent advantages in this context. To illustrate this, we escalate the input resolution to. Note that all existing RGB-T BSOD models have the input resolution ranging fromto, and this is the first high-resolution RGB-T BSOD model. Initially, the comparison of the last two rows of TableIIindicates that it exhibits very similar performance to the DFENet-m withinput size on the VT821 and VT1000 datasets. However, its performance on the VT5000 dataset notably lags behind that of the original DFENet-m. Subsequently, when juxtaposed with WaveNet in TableI, they demonstrate comparable performance on three datasets. Overall, the above analysis highlights that while there is a slight degradation in model performance upon increasing the input resolution to, the model remains highly competitive compared to existing SOTA models.

7) Complexity analysis:We also present the computational complexity of SOTA models and each component of DFENet in TableIand TableII. Overall, the proposed DFENet is a heavyweight model. Compared to knowledge distillation-based WaveNet, although DFENet-m has better performance, it has 68.5M more parameters (Params) and 75.7G more floating point operations (FLOPs), only slightly smaller than the WaveNet’s teacher network, SwinNet. Specifically, computing consumption is mainly concentrated in the backbone network, CDFFormer-m. Our designed MPA and FEM have tiny computational consumption. Compared to element-wise addition, MPA is only 4.9M and 1.6G more in Params and FLOPs. The FEM is with 0.6M Params and 4.0G FLOPs. Additionally, our FRCAB has quite similar complexity to RCAB, only 1.0M more in Params.

SECTION: IV-DFailure Cases

Despite the outstanding performance of our DFENet, it still struggles when facing some complex situations. In certain instances, depicted in Fig.11, our DFENet erroneously segments the background region when it exhibits more discriminative features than foreground objects, such as higher regional contrast, greater temperature change, and faster gradient change. Instead, it is easier for the human attention mechanism to focus on central objects or living organisms within a scene. This problem exemplifies the fact that DFENet cannot analyze the scene exhaustively. To solve this problem, future enhancements may be to add prior information such as the central prior, or leverage more powerful feature encoders.

SECTION: IV-EFuture Work

Although the proposed DFENet achieves leading performance, its complexity is still somewhat heavy. Balancing lower complexity with better performance remains a primary goal for future endeavors. In addition, FFT-based operations exhibit astonishing performance. However, current research on FFT-based supervised models are scarce, and they demonstrate poor compatibility with spatial domain operations (please see Sec.IV-C). Hence, the FFT- or frequency domain-based model holds promising research prospects and merits further attention.

SECTION: VConclusion

In this paper, we propose DFENet with bi-modal learning to explore the potential of FFT in solving RGB-T BSOD. Specifically, to fully bridge the complementary information between two modalities, we introduce MPA, which embeds channel FFT into spatial Fourier components, achieving deep-level and multi-dimensional representation enhancement. Compared to MHSA-based operations, MPA can be easily applied to high-resolution bi-modal feature fusion. To balance inductive biases of multi-level features, facilitating the saliency prediction, we design EFPD with the pyramid structure. In EFPD, based on the analysis of bi-modal frequency decomposition, we first design FEM to reconstruct accurate object edge (high-frequency) features to guide the integration of multi-level features. Second, to focus on high-frequency information, we deploy FRCAB in each decoder layer. FRCAB obtains global dependencies in the channel dimension of the frequency domain, effectively improving the model’s robustness. In the training stage, to supervise FEM in generating high-fidelity edge features, we propose CFL to solve hard frequencies in the frequency domain and enhance edge representations in two modalities’ feature spaces. The refinement of frequency-level edge features facilitates the learning of the salient map to a higher quality in the spatial domain. We demonstrated with extensive experiments that DFENet surpasses twelve existing SOTA models on four RGB-T BSOD datasets and exhaustively demonstrate the representation capabilities of FFT-based operations, providing a new paradigm for a series of salient object detection tasks.

SECTION: References