SECTION: Solving Monge problem byHilbert space embeddings of probability measures

We propose deep learning methods for classical Monge’s optimal mass transportation problems, where where the distribution constraint is treated as penalty terms defined by the maximum mean discrepancy in the theory of Hilbert space embeddings of probability measures. We prove that the transport maps given by the proposed methods converge to optimal transport maps in the problem withcost. Several numerical experiments validate our methods. In particular, we show that our methods are applicable to large-scale Monge problems.

SECTION: 1INTRODUCTION

Our aim in this paper is to propose numerical methods for Monge’s mass transportation problem, described as follows: given two Borel
probability measures,onand a cost function, to minimize
Our problem is to minimize

over all Borel measurable mappingsuch that. Here,denotes the pushforward ofwith, i.e.,for any Borel set.

We shall briefly describe the background of the Monge problem. Monge problem was proposed by Gaspagnol Monge in 1781[Monge, 1781].
In the 20th century, this problem was expanded by Kantorovich to make it mathematically easier to handle, and is now called the Monge-Kantrovich problem[Kantrovich, 1942],[Kantrovich, 1948].

The Monge problem, as extended by Kantrovich, is called the Monge-Kantrovich problem.
The first existence and uniqueness result was established by Brein,[Breiner, 1987],[Breiner, 1991].
Gangbo and McCann further developed the generalized problem[Gangbo and McCann, 1996].
Mikami also provided a probabilistic proof of Breiner’s result[Mikami, 2004].

Algorithms for solving the Monge-Kantrovich problem can be traced back nearly 100 years[Tolstoi, 1930].
Since the advent of mathematical programming, these algorithms have been a field of significant interest[Dantzig, 1951].
This is largely because Dantzig’s initial motivation for developing mathematical programming was related to solving transportation problems[Dantzig, 1949],
and it was later discovered that optimal transportation problems and minimum cost flow problems are equivalent[Korte and Vygen, 2012].
Research has advanced in the development of solvers that apply mathematical programming to machine learning[Bauschke and Combettes, 2011],
as well as in solvers used in dynamic optimal transport (OT)[Papadakis et al., 2014].
Today, it remains one of the most actively researched fields.

We describe recent solvers developed to address the Monge-Kantrovich problem. In recent years, the Monge-Kantrovich problem has been actively applied in machine learning. One prominent approach involves solving the entropy optimization problem by introducing an entropy regularization term, using the Sinkhorn algorithm[Peyré and Cuturi, 2019]. The Python libraryPOT[Flamary et al., 2021]provides a solver for various optimal transport problems, including entropy-regularized optimal transport.

When solving the optimal transport problem, computations can scale with the cube of the input data size. Therefore, it is crucial to support large-scale computations and to leverage GPUs for numerical calculations. Optimal transport solvers designed for large-scale computations include the Python libraryGeomloss[Feydy et al., 2019]. Another tool, the Python libraryKeOps[Charlier et al., 2021], optimizes the reduction of large arrays using neural networks and kernel formulas.

Additionally, theOTTlibrary[Cuturi et al., 2022], based on the high-performance JAX library for numerical calculations, offers implementations that can solve problems such as the Monge-Kantrovich problem.

Our aim is to derive a numerical solution for the basic mathematical analysis of the original Monge problem, rather than the Monge-Kantrovich problem. In particular, the goal is to develop algorithns capable of performing GPU-based numerical computations and handling large-scale calculations. The method for deriving the numerical solution primarily utilizes the embedding theory of probability measures, which was previously applied by Nakano[Nakano, 2024]to obtain a numerical solution for the Schrödinger bridge problem. In this study, we also apply this theory to derive a numerical solution for the Monge problem. The penalty method is employed to find the optimal solution, with the use of Maximum Mean Discrepancy (MMD) as the penalty function being a novel approach. Unlike existing methods, this method is independent of input data size.
We also confirm this through numerical experiments.

This paper is structured as follows. In the next section, we review some basic results on the theory of Hilbert space embedding of probability measures and describe a numerical method that describes a theoretical approximation solution to the Monge problem withcost.
Section3gives numerical experiments.

SECTION: 2PENALTY METHOD

SECTION: 2.1Hilbert space embeddings of probability measures

We shall give a quick review of theory of Hilbert space embeddings of probability measures, as developed in Sriperumbudur et al.[Sriperumbudur et al., 2010].
Denote bythe set of all Borel probability measures on.
Letbe a symmetric and
strictly positive definite kernel on, i.e.,forand
for any positive distinctand,

Assume further thatis bounded and continuous on.
Then, there exists a unique Hilbert spacesuch thatis a reproducing kernel onwith norm[Wendland, 2010].
Then consider themaximum mean discrepancy (MMD)defined by,
for,

It is known that ifis an integrally strictly positive definite thendefines a metric on.
Examples of integrally strictly positive definite kernels include the Gaussian kernel,, whereis a constant, and
the Matérn kernel,, whereis the modified Bessel function of
order. It is also known that Gaussian kernel as well as Matérn kernel metrize the weak topology on[Sriperumbudur et al., 2010],[Nakano, 2024].

Define

Then,

SECTION: 2.2Proposed methods

Letbe as in Section2.1.
We proposed a penalty method for Monge problem by Hilbert space embeddings of probability
measures, described as follows:

Note that the second term penalizes the distance between the laws ofand.
Moreover,
the second term in the above formula can be expressed discretely as follows: given IID samplesand, an unbiased estimator ofis given by

[Gretton et al., 2006].
Then, we approxiateby a classof deep nenual networks.
Eachcan be given by a multilayer perception with input layer,hidden layer, and output layer, whereand for,,,
for some matricesand vectors.
Heredenotes the number of units in the layer, andis
an activation function. Then the parameteris described byand.
For, the integral term in (1) is replaced by (2) andas follows: by Subsetcion 3.2. in[Nakano, 2024],

The algorithm is described below and we test our one algorithm thorough a numerical experiment.

SECTION: 2.3Theoretical result

For givenconsider the minimization problem

over all Borel measurable mappings.
Take arbitrary positive sequencesandsuch that

Then takesuch that

Then we have the following:

Let. Suppose thatis absolutely continuous with respect to the Lebesgue measure and that

Suppose moreover thatmetrizes the weak topology on. Then,

whereis the unique optimal transport map.
In particular,converges toin law under.

First, note that under our assumption, an optimal transport map does exist uniquely (see, e.g., Theorem 2.12 in[Villani, 2021]and Theorem 1.25 in[Santambrogio, 2015]).

Step (i). We will show (4).
This claim can be proved by almost the same argument as that given in the proof of Theorem 3.1 in[Nakano, 2024], but we shall give a proof for reader’s convenience.
Assume contrary that

for some. Then there exists a subsequencesuch that

Sinceis a metric, we have,
whencefor any. This means

Thus, the sequenceis bounded, and so
there exists a further subsequencesuch that

whereand.
Now chooseandsuch that

where.
With these choices it follows that

which is impossible.

Step (ii). Next we will show (5). Let.
Forwe have

for some constant. Thus we can take a sufficiently largesuch that.
This means thatis tight, whence
there exists a subsequencesuch thatconverges weakly to some. Since we have assumed thatmetrizes the weak topology, we get

This together with the step (i) yields

whence. Hence we have shown that each subsequencecontain a further subsequencethat converges weakly to.
Then, by Theorem 2.6 in Billingsley[Billingsley, 2013],
we deduce thatconverges weakly to.
Denote bythe 2-Wasserstein distance betweenand. Then we haveand the duality formula

where the supremum is taken over all bounded continuous functionsandsuch that,.
See, e.g., Proposition 5.3 in Carmona and Delarue[Carmona and Delarue, 2018].
Letbe arbitrary. Then there existandsuch that

Further, sinceis bounded and continuous,
there existssuch thatand

for. With these choices it follows that for,

Then lettingwe get

Sinceis arbitrary, we deduce.
On the other hand, (4) immediately leads to.
Therefore,, as wanted. ∎

SECTION: 3NUMERICAL EXPERIMENTS

Here we test our two algorithms through several numerical experiments.

SECTION: 3.1Interpolation of synthetic datasets

All of numerical examples below are implemented in PyTorch on a Core(TM) i7-13700H with 32GB memory in this subsection.
In these experiments, we describe three experiments on synthetic datasets.
Date size is set to befor each experiment.
The Gaussian kernel, cost functionand the Adam optimizer with learning lateis used.
Here, the functionis described by a multi-layer perception withhidden layer.
These result that obtained after aboutepochs.
Penalty parameterdefined by.

In this experiment, the initial distribution is
the well-known synthetic dataset generated by two “moons” (Figure1),
and the target distribution is the one generated by two “circles” (Figure2).

We can see from Figure2that the proposed method faithfully generates the target distribution.
Figure3shows the change in loss, and the loss converges in the first 500 epochs.

In this experiment, the initial distribution is a two-dimensional uncorrelated normal distribution with mean 0 and variance 1 (Figure4),
and the final distribution is the synthetic dataset generated by the two moons as in Section3.1.1(Figure5).

We can see from Figure5that the proposed method again generates the target distribution correctly with a small variance.
Figure6shows the change in loss. We can see that the loss converges in the first 500 epochs.

In this experiment, the initial distribution is a two-dimensional uncorrelated normal distribution with meanand variance,
and the target distribution is a two-dimensional uncorrelated normal distribution with meanand variance.

For normal distributions, we see from Figures7-9that stable generation is achieved as in Sections3.1.1and3.1.2.

SECTION: 3.2Comparison with POT

We compared the performance with POT, the existing Python library as mentioned in Section1.
Here, the initial distribution is a two-dimensional uncorrelated normal distribution with meanand variance,
and the target distribution is a two-dimensional uncorrelated normal distribution with meanand variance.
The Gaussian kernel, cost functionand the Adam optimizer with learning late 0.0001 is used.
Here, the functionis described by a multi-layer perception with 2 hidden layer.
Penalty parameterdefined by.
In this experiment, we compare the number of points that can be calculated, the number of times the calculation can be run, and the accuracy achieved when using an AMD EPYC 9654 with 768GB of memory.
The experimental result is described in Table 1, where SD stands for standard deviation.

Here, we set the number of batch size to be,
the top row of the table shows the number of training data to be,
the number of test data to beand so the number of iterations is,
the bottom row of the table shows the number of training data to be,
the number of test data to beand so the number of iterations is.
In the upper row, the size of the test data was maintained,
when the data size was set to be(in other words, when the training data was increased), it was not possible to calculate due to CPU usage time.
In the lower row, the size of the training data was maintained, when the size of the test data was set to be, the calculation became impossible due to the size of the CPU memory.

Next, we perform a similar experiment on NVIDIA H100.
The experimental result is described in Table 2, where SD again stands for standard deviation.

Here, we set the number of training data to be,
the number of test data to beand so the number of iterations is.
The above results show that when the test size was set to beand
the size of the training data was maintained, the calculation became impossible due to the size of the GPU memory.
In addition, when the size of the test data was maintained
and the number of epochs was set to be, it was not possible to calculate due to GPU usage time.

Next, we compared the calculation speed of the CPU and GPU.
On both GPU and CPU, we set the number of training data to be, the number of test data to beand so the number of iterations is.
The epoch number is 1.

The CPU calculation speed took 20 times longer
than the GPU calculation speed.

Then, we use the solverot.sinkhorn()in POT to compare the performance of POT with that of our algorithm on an AMD EPYC 9654 with 768GB of memory.
The computational complexity of this solver is known to be, whereis the input data size.

In table 3, calculations were repeated about 10 times for data sizes of,and, and the average values were calculated.

Numerical experiments in this subsection show that our proposed method is a promising option for solving large-scale Monge problems.

SECTION: REFERENCES