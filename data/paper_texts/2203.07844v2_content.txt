SECTION: 1Introduction

Many real-world prediction problems involve a temporal dimension and typically require the estimation of numerical sequential data referred to as time series forecasting. Time series forecasting is one of the major stones in data science playing a pivotal role in almost all domains, including meteorology(Murat et al.2018), natural disasters control(Erdelj et al.2017), energy(Bourdeau et al.2019), manufacturing(Wang & Chen2018), finance(Liu2019), econometrics(Siami-Namini & Namin2018), telecommunication(Maeng et al.2020), healthcare(Khaldi et al.2019b)to name a few.
Accurate time series forecasting requires robust forecasting models.

Currently, Recurrent Neural Network (RNN) models are one of the most popular machine learning models in sequential data modeling, including natural language, image/video captioning, and forecasting(Sutskever et al.2014, Vinyals et al.2015, Chimmula & Zhang2020).
Such RNN models are built as a sequence of the same cell structure, for example, ELMAN cell, Long-Short Term Memory (LSTM) cell or Gated Recurrent Unit (GRU) cell. The simplest RNN cell is ELMAN, it includes one layer of hidden neurons. While, LSTM and GRU cells incorporate a gating mechanism, three gates in LSTM and two gates in GRU, where each gate is a layer of hidden neurons.
Many other cell structures have been introduced in the literature(Zhou et al.2016, Lu & Salem2017, Mikolov et al.2014, Pulver & Lyu2017). However, to solve time series forecasting tasks, the building of RNN models is typically limited to the three aforementioned cell structures(Sezer et al.2020, Runge & Zmeureanu2021, Liu et al.2021, Rajagukguk et al.2020, Alkhayat & Mehmood2021), as they provide very good accuracy(Runge & Zmeureanu2021, Sezer et al.2020).

Nevertheless, building robust RNN models for time series forecasting is still a challenging task as there does not exist yet a clear understanding of times series data itself and hence there exist very little knowledge about what cell structure is the most appropriate for each data type.
In general, when facing a new problem, practitioners select one of the most popular cells, usually LSTM, and use it as a building block for the RNN model without any guarantee on the appropriateness of this cell to the current data.
The objective of this work is two-fold. It presents a comprehensive characterization of time series behaviors and provides guidelines on the best RNN cell structure for each behavior. As far as we know, this is the first work providing such insights.
The main contributions of this study can be summarized as follows:

Provide a better understanding of times series data by presenting a comprehensive characterization of their behaviors.

Determine the most appropriate cell structure for each time series behavior (i.e., whether a specific cell structure should be avoided for certain behaviors).

Identify differences in predictability between behaviors (i.e., whether certain behaviors are easier or harder to predict across all cell models).

Provide useful guidelines that can assist decision-makers and scholars in the process of selecting the most suitable RNN-cell structure from both, a computational and performance point of view.

The remainder of this study is organized as follows: Section 2 states the related works. Section 3 presents a taxonomy of time series behaviors. Section 4 presents a taxonomy of RNN cells. Section 5 describes the experiment. Section 6 exhibits and discusses the obtained results. Finally, the last section concludes the findings and spots light on future research directions.

SECTION: 2Related works

The last decades have known an explosion of time series data acquired by automated data collection devices such as monitors, IoT devices, and sensors(Murat et al.2018, Erdelj et al.2017, Bourdeau et al.2019).
The collected time series describes different quantitative values: stock price, amount of sales, electricity load demand, weather temperature, etc.
In parallel, a large number of comparative studies have been carried out in the forecasting area(Parmezan et al.2019, Godahewa et al.2021, Athiyarath et al.2020, Divina et al.2019, Choubin et al.2018, Sagheer & Kotb2019, Bianchi et al.2017). These studies can be divided into two categories: (1) the first category tries to find a universal forecasting model for any field(Parmezan et al.2019, Godahewa et al.2021, Athiyarath et al.2020). They compare multiple models on a set of datasets from different fields to conclude which model is the universal predictor. The selection of the used datasets is not based on any clear criteria. Whereas, (2) the second category focuses on selecting the most performing forecasting model for a specific field(Divina et al.2019, Choubin et al.2018, Sagheer & Kotb2019, Bianchi et al.2017).
They compare a set of models on one or multiple datasets coming from the same field. Nevertheless, the best predictor does not ensure stable performance over different datasets even if they come from the same field. Actually, after an extensive analysis of highly diverse datasets,(Keogh & Kasetty2003)demonstrated that there is a need for more comprehensive time series benchmarks and more careful evaluations in the data mining community. In addition, datasets should have a large size to train and test the models and incorporate specific behaviors that challenge their modeling. Such knowledge of the dataset properties is required to facilitate a better interpretation of the modeling results.

The natural approach to create such benchmarking time series datasets is to collect data from real applications. For instance, the NN5 dataset(Crone2008), the CIF 2016 dataset(Godahewa et al.2020), the M4 dataset(Makridakis et al.2018), and the Monash archive that gather 20 publicly available time series datasets(Godahewa et al.2021). Although, real time series are always business-oriented which make them either proprietorial or expensive to obtain(Dau et al.2019), they can take decades to become mature and ready to be used for machine learning purposes, their diversity testing is tedious(Dau et al.2019, Spiliotis et al.2020), and most importantly, their Data Generation Processes (DGPs) are unknown which make the interpretation of the models and the explanation of their decisions challenging.

An alternative solution is to generate synthetic time series datasets with known embedded patterns(Olson et al.2017). For instance,(Zhang & Qi2005)investigated the issue of how to effectively model artificial time series with deterministic behavior due to the existence of trend and seasonality using Artificial Neural Networks (ANNs).(López-Caraballo et al.2016)examined ANNs on time series with noiseless and noisy chaotic behavior generated by Mackey-Glass series.(Li & Lin2016)applied the Self-Constructing Fuzzy Neural Network (SCFNN) on chaotic time series including Logistic and Henon data.(Yeo2017)evaluated the performance of LSTM on three different time series with chaotic behavior (delay-time chaotic dynamical systems, Mackey-Glass and Ikeda equations).(Fischer et al.2018)presented an experimental evaluation of seven machine learning models applied on a set of eight DGPs reflecting linear and nonlinear behaviors.(Parmezan et al.2019)used 40 synthetic datasets of deterministic, stochastic, and chaotic time series to compare eleven parametric and non-parametric models.(Kang et al.2020)used the Mixture Auto-Regressive (MAR) models to create the GRATIS dataset based on which they compared different statistical models.
However, all the aforementioned studies remain non-comprehensive of the main time series behaviors that can be faced in real datasets.

Another categorization of these comparative studies can be made based on the types of the evaluated models. Here, three categories can be set: (1) Studies based on parametric models where scientists try to evaluate the forecasting performance of different statistical models(Godahewa et al.2021, Yu et al.2020, Kim & Jung2018). (2) Studies based on non-parametric models where they assess the performance of machine learning models(Granata2019, Dudek2016, Sagheer & Kotb2019). (3) Studies based on parametric and non-parametric models where both types of models are compared(Parmezan et al.2019, Khaldi et al.2019a, Yamak et al.2019, Siami-Namini & Namin2018). RNNs are one of the most used machine learning models in time series forecasting(Sezer et al.2020). Nevertheless, their usage is limited to three RNN variants (ELMAN, LSTM, and GRU). In the financial field,(Sezer et al.2020)reported that from 2005 to 2019, 52.5% of publications used RNN models to perform time series forecasting, where the LSTM model represents 60.4%, ELMAN (vanilla RNN) represents 29.7%, and GRU represents 9.89%. In the energy field,(Runge & Zmeureanu2021)stated in their review that ELMAN, GRU, and LSTM are the main applied deep learning models to building energy forecasting. In the environmental field,(Liu et al.2021)asserted in their review, from 2015 to 2020, that LSTM and GRU are the most practical RNN models in air quality forecasting. In the renewable energy field,(Rajagukguk et al.2020)reviewed, from 2005 to 2020, that in photovoltaic power forecasting, 60% of publications used LSTM, 20% used ELMAN, and 13% used GRU. While, in the solar irradiance forecasting, they reported that LSTM represents 44% of the used deep learning models, ELMAN represents 25% and GRU 19%. Similarly,(Alkhayat & Mehmood2021)reported in their review, from 2016 to 2020, that ELMAN, LSTM, and GRU are the only RNN models used in wind and solar energy forecasting. They outlined that the usage of RNNs within this field increased from 2% in 2016 to 25% in 2020.

Therefore, there is a strong need for a comprehensive analysis of different types of RNN models, including the above three variants, in forecasting the main time series behaviors, and a strong need for an RNN-based model guide to assist practitioners in their process of selection and structure of the best RNN cell for each time series behavior.

SECTION: 3Taxonomy of times series behaviors

As far as we know, this is the first work introducing a complete formal characterization of real-world time series. Time series emerging from real-world applications can either follow a stochastic mechanism or a chaotic mechanism and are usually contaminated by white noise(Wales1991, Cencini et al.2000, Zunino et al.2012, Box et al.2015, Boaretto et al.2021).

SECTION: 3.1Stochastic behavior

In stochastic behavior, real times series are generated by a random stable system, and can exhibit the following behaviors:

Deterministic behavior. These time series are characterized by the existence of deterministic patterns. They usually incorporate at least one of the following patterns: increasing or decreasing deterministic trend, simple deterministic seasonality, and complex deterministic seasonality (Figure1). The trend pattern is a long-term evolution in the data, it can be increasing or decreasing, and it can have different forms (linear, exponential, and damped)(Montgomery et al.2015). An increasing trend can appear in the demand for technologies in the social fields, while a decreasing trend is related to epidemics, mortality rates, and unemployment(Parmezan et al.2019). The seasonality pattern can be described as the occurrence of short-term regular variation that repeats at known and relatively constant time intervals. This type of pattern can occur in different types of data including time series of sales, e.g., the increase in sales of warm clothing in winter and air conditioners in summer(Parmezan et al.2019).

Random-Walk behavior. The time series of this behavior are characterized by the existence of unit-root patterns. This behavior appears when the time series have a stochastic increasing or decreasing trend and/or stochastic seasonality (Figure2). Here the current observation is equal to the previous observation plus a random step. The lag between these two observations is equal to one in the case of trend and equal to the seasonality period in the case of seasonality.

The presence of deterministic or random-walk behavior induces the non-stationarity in time series. The stationarity is a relevant feature in time series which basically implies that the mean, the variance, and the covariance do not depend on time(Montgomery et al.2015).
These two types of behaviors are the most important features in time series, and usually characterize business and macro-economic data(Salles et al.2019, Liu et al.2019).

Nonlinear behavior. Time series observations can often exhibit correlations with different degrees of non-linearity (Figure3). This type of behavior is present in almost all real-world time series data, such as in stream-flow forecasting(Wang et al.2020), and in financial markets forecasting(Bukhari et al.2020).

Long-memory behavior. Some time series may present properties of long-range dependencies in time which implies a strong coupling effect between the values of observations at different time steps, i.e. the correlations between observations has a slower exponential decay compared to short-range dependencies (Figure4). This type of behavior can occur in hydrology forecasting(Papacharalampous & Tyralis2020), network traffic forecasting(Ramakrishnan & Soni2018), financial market forecasting(Bukhari et al.2020), etc.

SECTION: 3.2Chaotic behavior

The chaotic mechanism can be expressed as a nonlinear deterministic dynamical system that is often unknown or incompletely understood(Li & Lin2016). Real-time series can exhibit a noisy chaotic behavior (Figure5), these time series are sensitive to initial conditions (butterfly effect) where small smooth perturbations in the system or measurement errors generate an abrupt change in the behavior of the time series (bifurcation). This type of behavior is unstable since it tends to be deterministic in short term but random in long term. Such kind of time series are usually present in many sciences and engineering fields such as weather forecasting(Tian2019), financial markets forecasting(Bukhari et al.2020), energy forecasting(Bourdeau et al.2019), intelligent transport and trajectory forecasting(Giuliari et al.2021), etc.

Based on the literature(Chandra & Zhang2012, Montgomery et al.2015, Liu et al.2017, Fischer et al.2018), the five aforementioned behaviors (deterministic, random-walk, nonlinear, long-memory, and chaotic) are the main behaviors encountered in real applications.
Real-world time series can either express an individual behavior or an aggregation of more than one behavior. To identify which type of behavior a real-world time series can include, different statistical preprocessing tools and tests can be applied(Grau-Carles2005, Inglada-Perez2020).
In Table1, we present a set of tools that can be used to identify the existence of the five aforementioned behaviors in time series data.

SECTION: 4Taxonomy of RNN cells

Humans do not start their thinking from zero every second, our thoughts have persistence in the memory of our brains. For example, as the reader reads this paper, he/she understands each word based on his/her understanding of the words before.
The absence of memory is the major shortcoming in traditional machine learning models, particularly in feed-forward neural networks (FNNs). To overcome this limitation, RNNs integrate the concept of feedback connections in their structure (Figure6, whereandare the input state and the hidden state at time step, respectively). This mechanism enables RNNs to have a certain memory capable of capturing the dynamics in sequential data by conveying information through time.

RNN models are built based on one specific cell structure which is the core of all computations that occur in the network. Multiple cell structures have been created since 1989 (Table2).
The early cell structure is named JORDAN(Jordan1989), where at each time step the previous output state is fed into the cell (Figure7(a)). Later, the ELMAN cell was proposed by(Elman1990). Unlike the JORDAN cell, each time step in the ELMAN cell calls the previous hidden state (Figure7(b)). In 2016, a combination of both JORDAN and ELMAN cells in one cell named multi-recurrent neural network (MRNN) was evaluated by(Abdulkarim2016). In this cell structure, at each time step, both previous output and hidden states are presented to the cell (Figure7(c)).

It was proved that the ELMAN cell suffers from the vanishing and exploding gradient problems that impede the capturing of long-term dependencies(Pascanu et al.2013). To overcome the memory limitation of this cell, novel cell structures have been proposed. In 2014, the Structurally Constrained Recurrent Network (SCRN) was proposed by(Mikolov et al.2014). They integrated a slight structural modification in the ELMAN cell that consists in adding a new slowly changing state at each time step called context state(Figure8(a)). In 2015,(Le et al.2015)created a new cell called Identity Recurrent Neural Network (IRNN) as a modification of ELMAN by setting the ReLu as the activation function, the identity matrix as an initialization of the hidden states weight matrix, and zero as an initialization of the bias (Figure8(b)).

A different way to handle the vanishing and exploding gradient problems resulted in creating different cell structures characterized by the gating mechanism that regulates the flowing of the information flux. The gates can be seen as filters that only hold useful information and selectively remove any irrelevant information from passing through. To perform this control of information (i.e., which information to pass and which information to discard), the gates are equipped with parameters that need to be trained through the model learning process using the back-propagation through time algorithm(Werbos1990). Thus, this mechanism provides the RNN cell with an internal permanent memory able to store information for long time periods(Weston et al.2014, Graves et al.2014).

In 1997, the first version of this type of cell named Long-Short Term Memory with No Forget Gate (LSTM-NFG) was created by(Hochreiter & Schmidhuber1997). This cell contains two gates: the input gateand the output gate. Later in 2000, the concept of the forget gatewas introduced by(Gers et al.2000)creating LSTM-Vanilla that has been widely used in most applications (Figure9(a)). In the same year, the LSTM cell with peephole connections (LSTM-PC) was proposed by(Gers & Schmidhuber2000). The peephole connections connect the previous cell statewith the input, forget, and output gates (Figure9(b)). These connections enable the LSTM cell to inspect its current cell states(Gers & Schmidhuber2001), and to learn precise and stable timing without teacher forcing(Gers et al.2002).

In 2014, the Gated Recurrent Unit (GRU) cell was proposed by(Cho et al.2014)as a simpler variant of LSTM that shares many of the same properties. The idea behind GRU cell was to reduce the gating mechanism of LSTM cell from three gates to two gates (relevance gateand update gate) in order to decrease the number of parameters and to improve the learning velocity (Figure10(a)). In 2015, ten thousand RNN cell structures were evaluated by(Jozefowicz et al.2015)using a mutation-based search process. They identified a new cell architecture that outperforms both LSTM and GRU on some tasks. This cell consists in adding a bias of 1 to LSTM forget gate creating the LSTM-FB1 cell. Further, they discovered three optimal cell architectures named MUT1, MUT2, and MUT3 that are similar to GRU but have some modifications in their gating mechanism and in their candidate hidden state(Figure10(b)and10(c)).
During that year, coupling both the input and the forget gates into one gate was proposed by(Nina & Rodriguez2015)creating the LSTM-CIFG cell (Figure11(a)). Further, the differential LSTM cell was proposed by(Veeriah et al.2015)to solve the impact of spatial-temporal dynamics by introducing the differential gating scheme in LSTM cell.

In 2016, the Minimal Gate Unit (MGU) cell was created by(Zhou et al.2016)to further reduce the number of parameters by decreasing the gating mechanism to one forget gate. This variant has a simpler structure and fewer parameters compared to LSTM and GRU cells (Figure11(b)).
In the same year, eight variants of LSTM-PC cell (based on modifying, adding, or removing one cell component at each time) were evaluated by(Greff et al.2016)on three different types of tasks: speech recognition, polyphonic music modeling, and handwritten recognition. They demonstrated that the forget and the output gates are the most critical components in LSTM cell. In addition, their results show that none of the evaluated variants can overcome the LSTM-PC cell.
During that year, the phased LSTM cell was introduced by(Neil et al.2016), where they added a time gate that updates the cell sparsely, and makes it converge faster than the basic LSTM.
Further, highway connections were added to GRU and LSTM cells by(Irie et al.2016).
In 2017, LSTM with working memory was created by(Pulver & Lyu2017), where they substituted the forget gate with a functional layer whose input depends on the previous cell state.
In 2019, a Gated Orthogonal Recurrent Unit (GORO) was introduced by(Jing et al.2019), where they added to the GRU cell an orthogonal matrix that replaced the hidden state loop matrix.

While very powerful in long-term dependencies, the basic cells (LSTM, GRU, and MGU) have a complex structure with a relatively large number of parameters. In 2017, the concept of parameter reduction was differently tackled through the creation of new cells called SLIM(Lu & Salem2017, Dey & Salem2017, Heck & Salem2017)while maintaining the same number of gates in the basic cells. These variants aim to reduce aggressively the parameters in order to achieve memory and time savings while necessarily retaining a performance comparable to the basic cells.
The new parameter-reduced variants of the basic cells eliminate the combinations of the input state, the hidden state, and the bias from the individual gating signals, creating SLIM1, SLIM2, and SLIM3, respectively. The SLIM1 cell consists in removing from the mechanism of all the gates the input state and its associated parameter matrix (Figure12(a)). The SLIM2 cell consists in maintaining only the hidden state and its associated parameter matrix (Figure12(b)). Whereas, the SLIM3 cell consists in removing the input state, the hidden state, and their associated parameters matrices (Figure12(c)). The cellular calculations within the displayed RNN cells along with the evaluated ones are provided in Appendix.

SECTION: 5Experimental structure

Two experiments have been carried out in this study. The first experiment analyzes the utility of each LSTM-Vanilla cell component in forecasting time series behaviors. The second experiment evaluates different variants of RNN cell structures in forecasting time series behaviors. In this section, we first describe the process we followed to generate the dataset for each time series behavior (Section5.1). Then, we present the selected models for the first and the second experiment (Section5.2). Finally, we provide the setup of the used models (Sections5.3,5.4,5.5, and5.6).

SECTION: 5.1Synthetic data generation

To simulate the five aforementioned time series behaviors, we used 21 different DGPs with white Gaussian noise. From each DGP we created time series of length 3000 observations replicated 30 times through a Monte Carlo simulation experiment using different initial random seeds for the white noise term(Table3).

To generate time series with deterministic behavior, 5 DGPs were used (Table4): Trend process (T), Simple Seasonality process (SS), Complex Seasonality process (CS), Trend and Simple Seasonality process (TSS), and Trend and Complex Seasonality process (TCS).
To simulate the random-walk behavior, 3 DGPs were used (Table6): Trend Random-Walk process (TRW), Seasonal Random-Walk process (SRW), and Trend and Seasonal Random-Walk process (TSRW).
To simulate time series with nonlinear behavior, we used 6 most popular nonlinear models commonly used in the forecasting literature having an increasing level of non-linearity(Zhang et al.2001)(Table5): Sign Auto-Regressive process (SAR), Nonlinear Moving Average process (NMA), Nonlinear Auto-Regressive process (NAR), Bilinear process (BL), Smooth Transition Auto-Regressive process (STAR), and Threshold Auto-Regressive process (TAR). These models are motivated by many nonlinear characteristics commonly observed in practice.
To artificially generate time series with long memory behavior, we used the Auto-Regressive Fractionally Integrated Moving Average process ARFIMA(p,d,q) since it is one of the best-known long memory processes(Liu et al.2017). In order to evaluate the performance of RNN cell structures with respect to DGPs with an increasing memory structure, 2 DGPs were created based on the variation of the fractional order of ARFIMA process. A higher fractional orderimplies longer dependency structure (Table7). To ensure the stationarity of the generated time series, we set the values of the fractional order strictly less than 0.5.
Finally, to simulate the noisy chaotic behavior, the 4 most known chaotic DGPs were used (Table8): Mackey-Glass process, Lorenz process, Rössler process, and Hénon-Map process. Then, we added the white Gaussian noiseto the deterministic signals to create noisy chaotic time series(Sangiorgio et al.2021).

SECTION: 5.2RNN-cells used for experiments 1 and 2

To evaluate each cell structure with respect to each times series behavior, we conducted two experiments as summarized in Table9. The first experiment evaluates LSTM-Vanilla and 11 of its variants created based on one alteration in the basic Vanilla architecture that consists of (1) removing, (2) adding, or (3) substituting one cell component (TableLABEL:tab1):
(1) The first three variants NIG (No Input Gate), NFG (No Forget Gate), and NOG (No Output Gate) were created through the deactivation of the input gate, the forget gate, and the output gate, respectively. The four subsequent variants NIAF (No Input Activation Function), NFAF (No Forget Activation Function), NOAF (No Output Activation Function), and NCAF (No Candidate Activation Function) were constructed through the elimination of the input, forget, output, and candidate activation function, respectively.
(2) The two subsequent variants PC (Peephole Connections), and FGR (Full Gate Recurrence) were designed through the creation of new connections between the cell state and the gates, and between the current states and the previous states of the gates, respectively.
(3) Eventually, FB1 (Forget Gate Bias 1) and CIFG (Coupled Input Forget Gate) was conceived by setting the forget gate bias to one, and by coupling the input and the forget gate into one gate, respectively.

The second experiment evaluates and analyzes the performance of 20 possible RNN-cell structures:JORDAN, ELMAN, MRNN, SCRN, IRNN, LSTM-Vanilla, GRU, MGU, MUT1, MUT2, MUT3, and 9 SLIM variants mapping LSTM, GRU, and MGU. A summary of the evaluated cells related to each experiment is presented in Table9, and the cellular calculations inside each cell are presented in TableLABEL:tab2.

The studied RNN cells have different degrees of complexity, which is referred to as theoretic complexity (Table9). This complexity is defined by the number of parameters inside each cell which depends on the number of inputs, the number of hidden nodes, the number of context nodes (in the case of the SCRN model), and the number of outputs. During the hyperparameter tuning the complexity of the cell may increase or decrease depending on the optimal number of hidden nodes found. Thus, the complexity of the cell defined after the hyperparameter tuning is called empirical complexity.

SECTION: 5.3Data preparation

Before starting the modeling process, each time series data was partitioned into three subsets: the first 2000 observations were used to train the models in order to find the best parameters, the next 500 observations were used to select the best configuration of hyperparameters of each model, and the last 500 observations were used to test the out-of-sample performance of these models (Figure13).
Each of these partitions was normalized, then converted from unstructured into structured data (Figure14) by reshaping it based on the number of time steps(estimation window size) and the number of horizons(forecasting window size). Thus, we converted the raw time series datainto a structured data.
Whereandbecause we are handling one-time-step-ahead forecasting.

SECTION: 5.4Parameter optimization

To find the best set of parameters (weightand bias), first, the model parameters were initialized by Kaiming method(He et al.2015), then optimized using a stochastic gradient descent-based algorithm named Adam(Kingma & Ba2014)(Algorithm1) by minimizing the cost functionthat describes a Mean Square Error (MSE) using a mini-batch size of 100 (Table10).

Whereis the predicted value of instancecomputed using the identity output activation function,is the weight matrix between the hidden and the output layer, andis the output layer bias.

SECTION: 5.5Hyperparameter tuning

Within this section, we present the approach used to select the best architecture for each model. This architecture is defined by two hyperparameters: 1) the number of time steps, and 2) The number of hidden neurons. To find the best combinationwe used a Grid Search algorithm (Algorithm2) that loops over each possible combination in a grid created in two-dimensional space defined byand. The other hyperparameters such as the initial learning rate and the minibatch size were set based on the literature(Parmezan et al.2019)(Table10).

Using each combination, the model was trained ten times for a maximum number of 500 epochs in which we save the best set of parameters that minimizes the average MSE over the ten runs in the validation set (Algorithm2). The objective behind this is to avoid the bias generated by the parameter initialization method and also to select the most stable model’s architecture.

The maximum numberof evaluated hidden neurons is 10 because we are using univariate time series (Table10), while the maximum numberof evaluated time steps is described in Table11for each behavior.
To defineused in the chaotic time series, we used the values recommended by(Parmezan et al.2019)since we used their chaotic datasets.
In the long-memory behavior,was selected based on the last relevant peak visualized in the ACF (Auto Correlation Function) graph and the PACF (Partial Auto Correlation Function) graph.
In the Nonlinear behavior, we already know that the time series were generated with an order equal to 2, but in order to find the best number of time steps, we setfor two reasons: 1) we need to simulate the fact that the model is not aware of the DGP, which is a mandatory condition to use machine learning models. 2) we cannot use the ACF and PACF tests in this case because they can only capture the linear correlations in the data, however, we are using nonlinear data which necessarily contains nonlinear correlations.
In the deterministic behavior, we usedfor the T DGP, andfor the remaining DGPs using the seasonality period.
In the random-walk behavior, we usedfor the TSRW DGP,for the SRW DGP, andfor the TRW DGP.

The hyperparameter tuning was performed using the first replicate for each DGP because changing the white noise generator to create these 30 replicates does not change the linear/nonlinear correlations in the time series, it only impacts the randomness of the data. Therefore, the combinationremains appropriate for all the 30s replicates.
In total, we performed 200 experiments for each model (Table3) which sums up to 6200 experiments in the whole study.
Once the best architecture of each model was found, the training and the validation sets were merged to form a new training set on which the best model’s configuration is retrained for 500 epochs, then evaluated on the test set.

SECTION: 5.6Performance evaluation

To evaluate the forecasting performance of the models, a set of statistical metrics were used. These statistical metrics together provide assistance to compare the models and select the best one. The used metrics are classified into four subsets:

Error-based metrics: these metrics only measure the number of errors made by a predictive model by evaluating the goodness-of-fit of the model. Two metrics were used: the Mean Absolute Error (MAE) and the Root Mean Square Error (RMSE). The RMSE measures the standard deviation of residuals, it penalizes large errors. While the MAE measures the average magnitude of the residuals, it is less prone to outliers.

Information criterion-based metrics: these metrics deal with the trade-off between the goodness-of-fit of the model and the simplicity of the model. They penalize complex models by combining the number of errors committed by the model, the number of parameters (i.e., weights and biases) employed by the model to generate the output, and the sample size. They measure the amount of information lost by a model. A low value of these metrics implies less information loss, therefore high model quality. A set of five main statistical criteria were employed: Akaike Information Criterion (AIC)(Akaike1969), Bayesian Information Criterion (BIC)(Findley1991), Amemiya Prediction Criterion (APC)(Amemiya1980), Hocking’s Sp (HSP)(Hocking1976), and Sawa’s Bayesian Information Criterion (SBIC)(Sawa1978).

Whereis the sum square error,is the number of parameters used by the model (empirical complexity), andis the standard deviation of the prediction errors.

Naïve-based metric: this metric compares the performance of the predictive model with the naïve model. It is called Theil’s U (TU) coefficient(Parmezan et al.2019). The naïve model assumes that the best value at timeis the value obtained at time. The values of this metric can be interpreted based on different ranges as follows: if, the model’s performance is lower than the naïve model. If, the model’s performance is the same as the naïve model. If, the model’s performance is higher than the naïve model. If, the model is trusted to carry out future predictions.

Direction change-based metric: this metric is called Prediction Of Change In Direction (POCID)(Parmezan et al.2019). It measures the accuracy in the direction changes. It accumulates, over time, the differences in the direction change between the predicted and the observed values. It penalizes the model when its direction change in the predicted values between two consecutive time steps (and) is different than those of the observed values. This metric is complementary to analyzing the prediction errors. A high value of this metric implies a high similarity degree in the direction changes between the predicted values and the observed values.

All the aforementioned metrics were computed by averaging their values over the 30 replicates of the test set of all the DGPs of one specific behavior.

Choosing the best model can be very challenging when different performance metrics are available. To facilitate this task, a new metric was created using the combination of all the aforementioned metrics. We call this metric Multi-Criteria Index Measure (MCIM).
Unlike the error measures, which generate values that need to be minimized, the POCID index must be maximized. Therefore, a complement of the POCID was created called Error Rate (ER):

Afterward, the MCIM metric was defined by computing the average over all the normalized values of the used metrics. The normalization was used because the performance metrics have different ranges of values. The normalization was performed using the Min-Max method.

Whereis the set of values of the Performance Indices of model,is the performance index typeof model, andis its normalized value.

is the set of evaluated models,is the minimum value of the performance index typeover the set of evaluated models, andis the maximum value of the performance index typeover the set of evaluated models.

The created MCIM metric was employed to rank all the evaluated RNN architectures and to select the best RNN structure for each specific time series behavior.
However, relying solely on the MCIM metric may not be statistically reliable to select the best model. To quantify the likelihood of a model being the most performing, to improve our confidence in the models’ interpretation and selection, to compare the performance of the RNN models, and to further examine whether any observed difference is statistically significant or it is only due to noise or chance, statistical significance tests should be used.

The choice of these tests depends on (1) the prediction task (classification or regression), (2) the data distribution, and (3) whether the models are compared on the same data or not.
In our case, (1) we are solving forecasting tasks which is a type of dynamical regression, (2) we assume that the distribution of our data is unknown, and (3) all the models are tested on the same data. Therefore, a regression non-parametric paired statistical test referred to as Friedman Wilcoxon-Holm signed-rank test111This statistical test was performed using the frameworkhttps://github.com/hfawaz/cd-diagramre-adapted to test forecasting modelswas used to detect pairwise significance.

First, the Friedman test(Friedman1940)between each pair of models is performed with a significance level of 5% to reject the null hypothesis (p-value < 0.05). The null hypothesis of this test states that the pair of models are similar.
Then, a pairwise post-hoc analysis based on the Wilcoxon-Holm method (with Holm’s alpha 5% correction) was used to compare the results(Benavoli et al.2016, Wilcoxon1992, Holm1979, Garcia & Herrera2008, Abdulkarim2016).
To visualize this pairwise comparison and to highlight the difference significance, a critical difference diagram proposed by(Demšar2006)was generated where a thick horizontal line groups a set of RNN models that are not significantly different.

SECTION: 6Results and discussion

In this section, we present the results of the two conducted experiments: (1) The first experiment consists of evaluating and analyzing the role of each component in the LSTM-Vanilla cell with respect to the five time series behaviors. The evaluated architectures were generated by removing (NIG, NFG, NOG, NIAF, NFAF, NOAF, and NCAF), adding (PC and FGR), or substituting (FB1 and CIFG) one cell component. (2) The second experiment aims at evaluating and analyzing the performance of a multitude of RNN cell structures available in the literature (JORDAN, ELMAN, MRNN, SCRN, IRNN, LSTM-Vanilla, GRU, MGU, MUT1, MUT2, MUT3, and 9 SLIM variants) in forecasting the five behaviors.

SECTION: 6.1Experiment 1: Utility analysis of LSTM cell components in forecasting time series behaviors.

The impact of each component (i.e., input gate, forget gate, output gate, coupled input-forget gate, input activation function, forget activation function, output activation function, candidate activation function, fixing the forget bias to 1, peephole connections, and full gate recurrence) on the performance of LSTM-Vanilla model for predicting the deterministic, random-walk, nonlinear, long-memory, and chaotic behaviors are presented in this section.

Tables12to16present the average results of the 10 used statistical metrics for each variant of the LSTM-Vanilla model run on the test set of the five types of time series behaviors. It can be noticed that the error-based metrics select models with low residual values, however, the information criterion-based metrics tend to select less complex models with low forecasting errors. Based on the new proposed MCIM metric, the structure CIFG is the most adapted to forecast time series data with deterministic (Table12) and random-walk (Table13) behaviors. The NOG variant outperforms the other models in forecasting data with nonlinear behavior (Table14). The NIG design is more adapted for data with long-memory behavior (Table15). The Vanilla structure reveals the higher ability to forecast time series data with chaotic behavior (Table16).

Figures15to19display the distribution of the TU and the POCID metrics over all the 12 LSTM variants for the five time series behaviors.
In the deterministic behavior, all the models perform better than the naïve model for more than half of the used data (Figure15(a)). The POCID levels of NIAF, NCAF, and FB1 are lower than the remaining models (Figure15(b)).
In the random-walk behavior, all the models perform less than the naïve model for more than half of the used time series data (Figure16(a)). They have approximately the same POCID amounts (Figure16(b)).
Within the nonlinear and the long-memory behaviors, the models performed less than the naïve model only for a small number of time series data (Figures17(a)and18(a)). The POCID levels of all the models are almost similar (Figures17(b)and18(b)).
With respect to the chaotic behavior, for more than half of the data, the models proved that they can be trusted to forecast such type of behavior (Figure19(a)). Similarly, the POCID values of all the used models are almost equal (Figure19(b)).

The direction changes of the predicted values are more similar to the ones of the real values (almost oscillating around 60%) with the deterministic, random-walk, nonlinear, and chaotic behaviors (Figures15(b),16(b),17(b), and19(b)) than with the long-memory behavior where the values are only hovering around 40% (Figure18(b)).

To strengthen our interpretation of the models’ performances with respect to the five time series behaviors, Figure20outlines the results of the statistical significance test between the used models for each data behavior. For the deterministic behavior, the CIFG structure is statistically different than all the other models (Figure20(a)). With the nonlinear behavior, the NOG variant is statistically better than the remaining models (Figure20(c)). With respect to the random-walk behavior, the CIFG and the NOAF have almost the same rank (Figure20(b)). The NIG followed by the CIFG achieved the best results with the long-memory behavior (Figure20(d)). With the chaotic behavior, the NOG followed by the NIG are better ranked than the Vanilla model recommended by the MCIM value (Table16), however, there is no statistical difference between them (Figure20(e)). For this type of behavior, the NOG variants are the most recommended by the used statistical test.

SECTION: 6.2Experiment 2: Performance analysis of different RNN cell structures in forecasting time series behaviors.

The performance of each RNN cell structure (JORDAN, ELMAN, MRNN, SCRN, IRNN, LSTM-Vanilla, GRU, MGU, MUT1, MUT2, MUT3, and 9 SLIM variants) for predicting the deterministic behavior, random-walk behavior, nonlinear behavior, long-memory behavior, and chaotic behavior is provided in this section.

Tables17to21highlight the outcomes of the statistical metrics for each RNN model run on the five types of time series behaviors. Based on our MCIM metric, the model MGU-SLIM3 outperforms the other models with respect to the deterministic (Table17) and the nonlinear (Table19) behaviors. MGU-SLIM2 achieves better results than the remaining models when applied to data with random-walk (Table18) and long-memory (Table20) behaviors. Eventually, LSTM-SLIM3 exhibits higher abilities to forecast time series data with chaotic behavior (Table21).

Figures21to25show the distribution of the TU and the POCID metrics over all the 20 RNN structures for the five time series behaviors.
In the deterministic behavior, all the models perform better than the naïve model for more than half of the used data except the JORDAN model (Figure21(a)). The models display different POCID levels but generally oscillate around 60% (Figure21(b)).
In the random-walk behavior, all the models perform less than the naïve model for more than half of the used time series data (Figure22(a)). The basic RNN variants (ELMAN, JORDAN, MRNN, and IRNN) have smaller POCID values (less than 60%) compared to the other models (Figure22(b)).
With regard to the nonlinear and the long-memory behaviors, the models are performing widely better than the naïve model (Figures23(a)and24(a)). These models proved that they can be trusted to forecast nonlinear data for more than half of the time series. There are small variations in the POCID levels of these RNN models (Figures23(b)and24(b)).
For the chaotic behavior, the RNN models are better than the naïve model for more than half of the time series data (Figure25(a)). Similarly, there are small variations in the POCID levels of the RNN models (Figure25(b)).

The direction changes of the predicted values are less similar to the ones of the real values for the long-memory behavior (oscillating around 40%) (Figure24(b)), however, they are more similar to the remaining behaviors with a percentage more than 50% (Figures21(b),22(b),23(b), and25(b)).

Figure26displays the statistical significance test results for top the 10 ranked models with respect to the five behaviors. With the deterministic behavior, the MUT2 followed by MGU, MGU-SLIM3, and GRU-SLIM3 are statistically similar such that the MUT2 variant is better ranked than the other models (Figure26(a)).
With the random-walk behavior, all the 10 models are statistically similar such that SCRN followed by MGU-SLIM2 have the higher ranks (Figure26(b)).
For the nonlinear behavior, ELMAN is the top one ranked model that is significantly different than the remaining models (Figure26(c). This outcome appears to be different than the one presented in Table19where this model has the rank of five.
For the long-memory behavior, MGU-SLIM2 and GRU-SLIM2 have similar ranks (Figure26(d), while LSTM-SLIM3 is the best-ranked model for the chaotic behavior (Figure26(e).

The results of the two experiments are summarized in Table22. The MUT2, the SCRN, and the ELMAN models are the most recommended RNNs to forecast time series data with deterministic, random-walk, and nonlinear behaviors, respectively. Whereas, the MGU-SLIM2 and the LSTM-SLIM3 are the most recommended for the long-memory and chaotic behaviors, respectively.

SECTION: 7Conclusions

In this paper, we proposed a comprehensive taxonomy of all possible time series behaviors, which are: deterministic, random-walk, nonlinear, long-memory, and chaotic. Then, we conducted two experiments to show the best RNN cell structure for each behavior. In the first experiment, we evaluated the LSTM-Vanilla model and 11 of its variants created based on one alteration in its basic architecture that consists in (1) removing (NIG, NFG, NOG, NIAF, NFAF, NOAF, and NCAF), (2) adding (PC and FGR), or (3) substituting (FB1 and CIFG) one cell component. While, in the second experiment, we evaluated LSTM-Vanilla along with a set of 19 RNN models based on other recurrent cell structures (JORDAN, ELMAN, MRNN, SCRN, IRNN, GRU, MGU, MUT1, MUT2, MUT3, and 9 SLIM variants).

To evaluate, compare, and select the best model, different statistical metrics were used: error-based metrics (MAE and RMSE), information criterion-based metrics (AIC, BIC, APC, HSP, and SBIC), naïve-based metric (TU), and direction change-based metric (POCID). To facilitate the task of the best model selection, a new statistical metric was proposed (MCIM). Further to improve our confidence in the models’ interpretation and selection, Friedman Wilcoxon-Holm signed-rank test was used.

In the first experiment, We showed that the CIFG model is the most suitable for non-stationary time series due to the existence of deterministic behavior or random-walk behavior. We also, experimentally, proved that the NOG model is the most performing for the nonlinear and chaotic behaviors, while the NIG model outperformed the other models with respect to long-memory behavior.
In the second experiment, over the 20 evaluated RNN models, the best forecasting results were achieved by the new parameter-reduced variants of MGU (MGU-SLIM2) and LSTM (LSTM-SLIM3) for the long-memory and chaotic behaviors, respectively. For the deterministic behavior, the best significant model is MUT2. In the case of the random-walk behavior, the most significantly better model is SCRN. Finally, for the nonlinear behavior, the ELMAN model is the best significant model.

Based on the outcomes of both experiments, we arrived to demonstrate that the SLIM3 version of the LSTM cell has the highest ability to increase the performance of the RNN model in forecasting chaotic behavior. While the SLIM2 version of the MGU model is recommended in the case of time series with long-memory behavior. Finally, the MUT2, SCRN, and ELMAN variants are the strongly advocated models to forecast time series data with deterministic, random-walk, and nonlinear behaviors, respectively.
The outcomes of our study are limited to time series with a single behavior. However, in real-world problems, combined behaviors (i.e., more than one behavior in the same time series) can also occur. In future work, evaluating the best RNN cell with respect to such types of time series can complement the guidelines provided by this study.

SECTION: Acknowledgements

This work was partially supported by DETECTOR (A-RNM-256-UGR18 Universidad de Granada/FEDER), LifeWatch SmartEcomountains (LifeWatch-2019-10-UGR-01 Ministerio de Ciencia e Innovación/Universidad de Granada/FEDER), DeepL-ISCO (A-TIC-458-UGR18 Ministerio de Ciencia e Innovación/FEDER), BigDDL-CET (P18-FR-4961 Ministerio de Ciencia e Innovación/Universidad de Granada/FEDER).

SECTION: References

SECTION: Appendix AArchitectures of the studied RNN cells

In this section, we provide the cell structures of the different RNN models along with their cellular calculations. To understand the calculations, TableLABEL:Nompresents the deception of the mathematical notations.
To better understand the components inside the LSTM-Vanilla cell, we present below the role of the main elements:

Input state: it contains the data features at time step.

Output state: it contains the output of the model at time step.

Hidden state: it represents the short-term memory of the cell at time step.

Cell state: it represents the long-term memory of the cell at time step.

Candidate cell state: it contains the new information we can use to update the cell state at time step.

Input gate: it filters from the current candidate cell state the information that should be used to update the current cell state.

Forget gate: it filters from the previous cell state the information that should be used to update the current cell state.

Output gate: it filters from the current cell state the information that should be exposed to the external network (the next time step and the next hidden and/or output layer).