SECTION: Toward Understanding BERT-Like Pre-Training for DNA Foundation Models

With the success of large-scale pre-training in language tasks, there is an increasing trend of applying it to the domain of life sciences. In particular, pre-training methods based on DNA sequences have received increasing attention because of their potential to capture general information about genes. However, existing pre-training methods for DNA sequences largely rely on direct adoptions of BERT pre-training from NLP, lacking a comprehensive understanding and a specifically tailored approach. To address this research gap, we provide the first empirical study with three insightful observations. Based on the empirical study, we notice that overlapping tokenizer can benefit the fine-tuning of downstream tasks but leads to inadequate pre-training with fast convergence.
To unleash the pre-training potential, we introduce a novel approach called RandomMask, which gradually increases the task difficulty of BERT-like pre-training by continuously expanding its mask boundary, forcing the model to learn more knowledge. RandomMask is simple but effective, achieving state-of-the-art performance across 6 downstream tasks. RandomMask achieves a staggering 68.16% in Matthew’s correlation coefficient for Epigenetic Mark Prediction, a groundbreaking increase of 19.85% over the baseline and a remarkable 3.69% improvement over the previous state-of-the-art result.

SECTION: IIntroduction

In recent years, the integration of Transformer architectures, extensive datasets, and self-supervised pre-training techniques has significantly advanced the field of natural language processing (NLP)[1,2,3,4,5]. Similarly, these advances find an echo in the study of DNA sequences, where complex interactions among elements such as promoters, enhancers, and transcription factor binding sites mirror the intricate semantic relationships in language[6,7,8,9]. The power of pre-trained language models in distinguishing these subtle and interconnected patterns springs from pre-training on extensive, unlabeled data. Fortunately, projects like the Human Genome Project have provided a wealth of DNA sequence data[10], setting the stage for developing genomic pre-training models.

The prospect of utilizing pre-trained language models to uncover the hidden knowledge from vast DNA sequences is highly promising. Pioneering models like DNABERT[11], LOGO[12], and the Nucleotide Transformer[13]have demonstrated significant progress in the analysis of DNA sequences by BERT-like pre-training model. Considering that current DNA modeling primarily focuses on understanding existing sequences rather than generating new ones, BERT-like models’ bidirectional context understanding capability is typically more crucial than the unidirectional generative capability of GPT-like models.

Significant advancements have been made in DNA foundation models recently, influenced by the success of BERT. DNABERT, introduced by[11], applies BERT-like architectures to learn representations of DNA sequences. By leveraging Transformers’ bidirectional nature, DNABERT captures dependencies and relationships between nucleotides, enabling a deeper understanding of genetic information[14]. It has demonstrated enhanced performance on tasks like DNA sequence classification, variant calling, and gene expression prediction. Another notable advancement is the Nucleotide Transformer (NT) proposed by[13]. NT utilizes a significantly larger number of parameters compared to DNABERT, leading to notable performance enhancements. As the field continues to evolve, further refinements and novel approaches are expected, leading to more advanced analysis and interpretation of genetic information[15,16].

However, pre-trained models for DNA sequences often directly leverage NLP methods such as BERT[1], neglecting the unique characteristics of DNA sequences. Figure1illustrates both overlapping and non-overlapping tokenizer strategies employed in DNA analysis, such as DNABERT and Nucleotide Transformer (NT)[13]. Despite the sophisticated tokenizer strategies, these models usually fail to capture the characteristics of DNA sequences, as shown in Figure2. First, genomes contain functional elements with specific long sequence patterns ranging from tens to hundreds of long nucleotides, such as promoters ([17,18]),on building upregion-levelgenomic information. Furthermore, as exemplified by the simple genetic substitution (e.g.GAA to GTA) that leads to sickle cell anemia[19], even a single nucleotide change in the genome can deeply affect gene function, making capture of thenucleotide-levelinformation crucial as well. This complexity underscores the necessity for models tailored to DNA sequences’ region-level and nucleotide-level information.

A deeper understanding of BERT-like models for DNA is needed to develop pre-training methods suitable to the DNA characteristics. Specifically, our observations reveal several crucial phenomena: 1) Regardless of the source of pre-trained weights—whether from models using overlapping or non-overlapping tokenizer, using overlapping tokenizer consistently improves performance in downstream tasks. This improvement is likely due to its sensitivity to single nucleotide changes. 2) During pre-training, overlapping tokenizer rapidly produces distinct K-mer embeddings and achieves exceptionally low losses, whereas non-overlapping tokenizer tends to produce more ambiguous embeddings and continuous loss reduction. 3) Models pre-trained with overlapping tokenizer tend to show a pattern in their intermediate layers, concentrating self-attention narrowly on specific tokens. It may suggest an issue of under-training in these layers, and the model’s ability to model regional-level information is insufficient[20]. In summary, while the overlapping tokenizer method improves fine-tuning performance, it also faces challenges during pre-training, including rapid convergence and potential under-training risk.

Building upon these insights, we believe that modeling DNA sequences should consider single nucleotide features and region-level information. We propose RandomMask, a technique that increases the complexity of pre-training tasks for models using overlapping tokenizer. The overlapping tokenizer helps the model capture DNA single nucleotide features, and RandomMask lets the model learn DNA region-level information by reconstructing DNA sequences of different lengths. RandomMask dynamically expands masking boundaries during BERT-like pre-training, introducing evolving challenges. Observing the mechanism of attention in the middle layer effectively addresses the issue of rapid convergence observed in these models, which can otherwise lead to a superficial understanding of complex DNA patterns.

Empirically, RandomMask has set new benchmarks, achieving state-of-the-art
(SOTA) performance on 6 downstream tasks[16,21]. In the task of epigenetic mark prediction, RandomMask achieved a mean Matthew’s correlation coefficient of 68.16%, improving the baseline by 19.85% and exceeding the previous SOTA by 3.69%.

The contributions of this paper are summarized as follows:

We conducted a thorough analysis of BERT-like pre-training for DNA sequences. Our findings reveal that the K-mer overlapping tokenizer enhances performance during the fine-tuning phase, regardless of whether models are pre-trained with overlapping or non-overlapping weights. However, the common overlapping tokenizer method leads to rapid convergence and under-training during the pre-training phase.

To address these issues and unleash the potential of pre-training, we introduced RandomMask. This novel method dynamically expands the masking boundaries, increasing the complexity of the pre-training task and encouraging the model to learn richer and more robust knowledge of DNA sequences.

We evaluated RandomMask on 6 downstream tasks, where it consistently achieved superior performance. Notably, in the epigenetic mark prediction task, RandomMask reached a mean Matthew’s correlation coefficient of 68.16%, surpassing the baseline by 19.85% and exceeding the current SOTA by 3.69%.

SECTION: IIPreliminaries

SECTION: II-AK-mer tokenizer

K-mer tokenizer involves dividing DNA sequences into subsequences of length K using a sliding window mechanism. Here, “K” represents the window size and determines the length of each subsequence. This framework has two commonly used strategies:OverlappingandNon-overlappingtokenizer. Overlapping tokenizer, used by DNABERT, involves a window size ofand a stride of 1. This approach would tokenize the DNA sequence “ATGACG” into subsequences ATG, TGA, GAC, and ACG using a 3-mer window. In contrast, non-overlapping tokenizer, employed by the Nucleotide Transformer, uses both a window size and stride of. This results in subsequences like ATG and ACG for the same sequence using a 3-mer window.

SECTION: II-BSignificance of Single Nucleotide Resolution

Single nucleotide resolution is crucial for a wide range of DNA-related tasks. Recognizing its significance, Nguyen et al. emphasized this aspect in their study HyenaDNA[15]. They argued that a stride of 1 is essential for models to identify and extract detailed information about individual nucleotides accurately. From this perspective, they advocated for a single nucleotide tokenizer strategy that employs a stride of 1 to achieve enhanced resolution at the single nucleotide level.

SECTION: IIIObservations

To examine the effect of different tokenizer methods, we performed two exploratory experiments and gained three insightful observations.

It is common practice to adopt consistent tokenizer methods for pre-training and fine-tuning. Contrary to this conventional wisdom, which posits that tokenizer inconsistencies may impair the model’s ability to apply learned knowledge effectively, our results suggest otherwise. Overlapping tokenizer consistently outperforms non-overlapping tokenizer in DNA downstream tasks, regardless of the tokenizer method pre-training employed. This finding indicates that overlapping tokenizer is particularly advantageous for DNA sequence analysis by nature.

In order to delve deeper into the underlying differences between overlapping and non-overlapping tokenizer, we conducted an extensive analysis of the pre-training process. This analysis allowed us to gain two more insightful observations: (1) Overlapping tokenizer leads to a more organized embedding space with exceptionally reduced loss, while non-overlapping tokenizer results in a less structured embedding space with a gradual, continuous decrease in loss. (2) The standard MLM task appears insufficiently challenging for models using overlapping tokenizer, thus hindering the sufficient training of attention mechanisms.

SECTION: III-AFine-tuning Stage

We performed a series of comparative experiments on diverse downstream benchmark tasks. Two pre-trained models were employed, namely “DNABERT” and “Nucleotide Transformer”, both pre-trained on the whole human genome. “DNABERT” was pre-trained using overlapping tokenizer, whereas “Nucleotide Transformer” was pre-trained using non-overlapping tokenizer. Then we fine-tuned these two models on the benchmark consisting of 6 downstream tasks111More details are summarized in TableIIin SubsectionV-C.. The results are shown in TableI.

In TableI, we observe that regardless of the pre-training method employed, models fine-tuned with overlapping tokenizer consistently outperform non-overlapping tokenizer. Specifically, DNABERT demonstrates improvements in all 6 tasks, with an average increase of 9.17% in MCC. Similarly, the Nucleotide Transformer also improves in all 6 tasks, with an average increase of 7.39%.

We claim that the performance gap between overlapping and non-overlapping tokenizer stems from the intrinsic superiority of overlapping tokenizer for DNA downstream tasks. Additionally, contrary to conventional belief, which suggests that inconsistency between pre-training and fine-tuning may hinder performance, our finding reveals that directly using overlapping tokenizer leads to a significant improvement in the performance of DNA downstream tasks, regardless of the chosen pre-training method.

SECTION: III-BPre-training Stage

To gain a deeper understanding, we thoroughly analyze the pre-training process. This involves pre-training two models, namely “DNABERT” with overlapping tokenizer and “DNABERT” non-overlapping tokenizer, on the entire Human Genome[10].

We compare the progression of embedding space and loss values between the two models. We use the t-SNE algorithm[22]to visualize the embedding space and present the results in Figure4. Comparing the two embedding spaces, we notice a notable distinction between the outcomes achieved by DNABERT when using overlapping and non-overlapping tokenizer. For overlapping tokenizer, as the loss decreases quickly, the embedding space becomes increasingly organized, resulting in a clear clustering of tokens when the loss reaches a low level. On the other hand, for non-overlapping tokenizer, the loss continuously decreases but remains relatively high, with limited organization in the embedding space.

Upon closer examination of Figure3, we observe that each major cluster corresponds to the clustering of the central two nucleotides of each token, and the marginal nucleotides determine the distribution of tokens within the cluster. We refer to these two central nucleotides in each token as the “representative elements” of the token. These representative elements establish the crucial one-to-one correspondence between tokens and nucleotides, which is the key factor contributing to the superior performance of overlapping tokenizer.

We now give an intuitive analysis of the convergence of the two models. The rapid convergence and exceptionally low loss value of DNABERT with overlapping tokenizer demonstrate the model’s proficiency in solving the MLM task. However, it also implies that the pre-training task leads to early overfitting. Nevertheless, The model’s ability to recognize representative elements and utilize the highly organized embedding space allows it to efficiently narrow down the search scope and accurately identify masked tokens. Consequently, the model effortlessly accomplishes the original MLM task, as masking six tokens is essentially equivalent to masking a single nucleotide, which is a relatively simple task.

As previously discussed, the rapid convergence and exceptionally low loss value of DNABERT with overlapping tokenizer imply that the original MLM task is too simple for the model. This raises the possibility that the model has not been extensively trained, potentially limiting its ability to reach its full potential. In this section, we delve deeper into the analysis of the behavior of both models to validate the proposal and gain further insights.

We visualize their attention mechanism. The results are shown in Figure5(a) and (b). We observe that the intermediate attention mechanisms of DNABERT with overlapping tokenizer are overly concentrated on the first token, the [CLS] token, with only the final layer focusing on a few nearby tokens. On the other hand, the attention mechanism of DNABERT with non-overlapping tokenizer is more evenly and diversely distributed across the sequence.

This phenomenon suggests that the model with overlapping tokenizer effectively learns a shortcut, whereby it only relies on the final layer to memorize a limited set of mappings from nearby tokens to the output predictions. Therefore, the intermediate layers remain mostly untrained. For a model with non-overlapping tokenizer, since the nearby tokens have no explicit information about the masked token, this shortcut is not available.

Previous work[20,23,24]on analyzing BERT-like architectures has shown that the diversity of attentional patterns in the middle layer of BERT is key to the model’s ability to model region-level information. Thus, the under-trained middle layer of overlapping models implies a lack of ability to model region-level information.

SECTION: III-CSummary

Since then, the previous analysis can be summarized as follows: DNA modeling needs to consider the accurate modeling of single nucleotides and the information of the whole region. Although the poor performance of the current BERT-based overlapping DNA pre-training model[11]has led subsequent studies such as NT[13]and DNABERT-2[16]to abandon this tokenizer approach, our analysis suggests that the overlapping tokenizer actually contributes to the modeling of single nucleotides. The underlying reason for the poor performance of the BERT-based overlapping DNA pre-training model is that the MLM pre-training approach in traditional NLP fails to adequately train the intermediate layers of the model, thus weakening its ability to model regional information.

SECTION: IVMethod

Since our method randomly expands the masking boundaries during the MLM pre-training stage, we call it RandomMask.

Tokenizer: We employ 6-mer overlapping tokenizer for both pre-training and fine-tuning, as previously outlined, due to its effectiveness in capturing a comprehensive array of DNA sequence features. However, the rapid convergence characteristic of 6-mer overlapping tokenizer during the pre-training phase may lead to lack training. This, in turn, can significantly limit the model’s performance potential. To address this issue, we introduce a novel pre-training strategy.

Pre-training Strategy: To mitigate the drawbacks of overlapping tokenizer during the pre-training phase, we propose an approach that progressively expands the masking boundary centered on the masked nucleotide. This pushes the model to learn continuously. Inspired by the curriculum learning strategy in[25], we divided the 500k pre-training steps of DNABERT with 6-mer overlapping tokenizer into five distinct phases. The length of consecutive mask tokens is randomly chosen between the minimum and maximum values. Enhance the ability of the model to capture region-level information by allowing the model to reconstruct DNA sequences of different lengths. The minimum length of consecutive masks is set to 6, and the maximum length increases by increments of 2 at each stage. Specifically, in the training step, theof a DNA tokens sequenceare obtained through Algorithm1, whereis a pre-defined probability value, e.g.,. Then, we can get mask tokensfor MLM pre-training.

SECTION: VExperiments

We train two BERT-like DNA pre-trained models, with incorporating the RandomMask (denoted as “+ RM”) technique. DNABERT + RM is trained on the human genome[10]. DNABERT2 (6mer) + RM is trained on multi-species genome, following the DNABERT2 pre-training datasets[16]. We evaluate the models across 6 downstream tasks. All experiments follow identical settings following DNABERT[11]and DNABERT2[16]to ensure a fair comparison.

SECTION: V-AExperimental Setup

Architecture:The backbone networks of DNABERT + RM and DNABERT2 (6mer) + RM are chosen according to the configurations used in DNABERT[11]and DNABERT2[16]. Each of them consists of 12 Transformer Encoder layers with 768 hidden units and 12 self-attention heads. We adopt the overlapping 6-mer tokenizer method for our models. The vocabulary size is 4,101, with 4,096 tokens representing the combinations of the four nucleotides in 6-mer arrangements, and the remaining 5 tokens are reserved for special purposes.

Baseline:For a comprehensive comparison, we select the following methods as baselines. DNABERT[11]is an early pre-training model for DNA sequences. DNABERT is pre-trained on the human genome using an overlapping 6mer tokenizer. DNABERT2[16]is the latest improved version of DNABERT, which uses genes from several species as pre-training data. DNABERT2 also introduces Byte Pair Encoding (BPE) tokenizer for the first time in DNA sequence pre-training. All these methods greatly improve the performance of the model. Also, they provide DNABERT2 (6mer) using overlapping 6mer tokenizer. The Nucleotide Transformer (NT)[13]is a large language model of DNA sequences from Instadeep and Nvidia. NT uses a non-overlapping 6mr tokenizer. NT-500M-human indicates pre-training on the human genome using a model with a parameter count of 500 million. NT-2500M-multi indicates pre-training on the genomes of multiple species using a model with a parameter count of 2500 million. These models are open-source, and all fine-tuning hyperparameters are detailed in Appendix C.

Pre-training:DNABERT + RM is pre-trained on the human genome[10]for 480k steps with a batch size of 512, typically requiring around 2 days using 8 NVIDIA Tesla A100 GPUs. DNABERT2 (6mer) and DNABERT2 (6mer) + RM are trained on the multi-species dataset[16]for 500k steps with a batch size of 4096, generally taking about 7 days using 8 NVIDIA Tesla A100 GPU.

Fine-tuning:The models are evaluated on 6 downstream tasks, including Epigenetic Marks Prediction (EMP)[26,27], Transcription Factor Prediction on human and mouse genomes (TF-H and TF-M), Promoter Detection (PD)[18], Core Promoter Detection (CPD), and Splice Site Prediction (SSP)[28]. These datasets are from the Genome Understanding Evaluatio (GUE) proposed by DNABERT2[16]. Hyperparameters for fine-tuning are adapted from DNABERT2[16], The Nucleotide Transformer[13]and HyenaDNA[15].These tasks (EMP, TF-M, TF-H, PD, CPD, and SSP) utilize Matthew’s correlation coefficient (MCC) as the evaluation metric.

SECTION: V-BMetric

The Matthews Correlation Coefficient (MCC) is a metric that is widely used in classification problems to evaluate the performance of models. It is defined as:

where:

TP = Number of True Positives

TN = Number of True Negatives

FP = Number of False Positives

FN = Number of False Negatives

True Positives and True Negatives represent accurate predictions of the model, while False Positives and False Negatives denote incorrect predictions.

SECTION: V-CList of DNA Downstream Tasks

TableIIhighlights the importance of nucleotide and region-level information modeling in DNA downstream tasks. Below is additional information on these tasks.

Epigenetic Mark Prediction (EMP): This task aims to determine whether the input sequence is an epigenetic mark in the yeast genome, particularly focusing on the occupancy of acetylated and methylated nucleosomes. The dataset includes various histone modifications such as H3, H4, H3K9ac, H3K14ac, H4ac, H3K4me1, H3K4me2, H3K4me3, H3K36me3, and H3K79me3. Recognizing these epigenetic marks is crucial for understanding gene expression regulation, chromatin structure, and their impact on gene function.

Transcription Factor Binding Site Prediction (TF-M and TF-H): This task is focused on identifying whether the input sequence is a transcription factor (TF) binding site in the mouse (TF-M) or human (TF-H) genome. Accurately identifying these binding sites is essential for revealing gene regulatory networks, understanding gene expression patterns, and exploring the molecular mechanisms of diseases.

Promoter Detection (PD): This task aims to determine whether the input sequence is a proximal promoter region in the human genome. Proximal promoters play a critical role in initiating transcription, making their recognition important for understanding gene regulation, identifying disease-associated genetic factors, and developing gene therapy strategies.

Core Promoter Prediction (CPD): Similar to proximal promoter detection, this task aims to determine whether the input sequence is a core promoter region. The core promoter is located near the transcription start site (TSS) and the start codon and is essential for transcription initiation. Recognizing core promoters is important for understanding the mechanisms of gene expression initiation and its regulation across different cell types and conditions.

Splice Site Prediction (SSP): This task determines whether the input sequence is a splice donor or acceptor site in the human genome. Splice sites are crucial for alternative splicing, which contributes to protein diversity and plays a significant role in understanding the impact of aberrant splicing in genetic disorders. Accurate recognition of splice sites is vital for exploring gene expression diversity, understanding disease mechanisms, and developing gene editing therapies.

SECTION: V-DResults

The main results are presented in TableIII. Our method, RandomMask, consistently outperforms the other methods, achieving state-of-the-art performance on 6 DNA downstream tasks. The additional performance on every dataset is detailed in Appendix A.

For instance, in the Epigenetic Marks Prediction (EMP) task, our method DNABERT2 + RM achieved an average Matthews Correlation Coefficient (MCC) of 68.16%, surpassing the previous best SOTA by 3.69%. In Transcription Factor Prediction (Mouse) (TF-M), our method achieved a MCC of 76.28%, respectively, outperforming the baseline values of 66.37% and 63.67%. Our approach outperformed other methods for promoter detection, and core promoter detection achieved competitive performance.

In conclusion, applying the RandomMask strategy with overlapping 6mer tokenizer significantly enhances the performance across 6 DNA downstream tasks.

SECTION: V-E6-mer vs BPE

In Figure6, we conduct comprehensive experiments to compare our RandomMask (RM) method with DNABERT2 (BPE) and DNABERT2 (6mer)[16]. Here, DNABERT and DNABERT2 (6mer) are open-source models that use overlapping 6mer tokenizer. DNABERT2 (BPE) is an open-source model that uses BPE tokenizer.

Compare DNABERT + RM and DNABERT2. From Figure6, the performance of our pre-trained DNABERT + RM is slightly better than DNABERT2 (BPE).

The results in the DNABERT2 (BPE) and DNABERT2 (6mer) show that if we just replace the BPE tokenizer with the 6mer tokenizer, the model’s performance will decrease.

DNABERT2 (6mer) + RM is the model performance after using RandomMask. It can be seen that the performance of the overlapping 6mer tokenizer model has been greatly improved after using RandomMask, far exceeding DNABERT2 (BPE) and DNABERT2 (6mer).

SECTION: V-FModel Representation Analysis

Firstly, RandomMask obtains a clear embedding. Comparing the t-SNE plots of III, VI, and IX in Figure4, the model trained with RandomMask (IX) obtains the clearest embedding space. As stated in our analysis section, the clearer the embedding space, the more it helps improve the model’s ability to model single nucleotides of DNA sequences.

Secondly, RandomMask can greatly enhance the attentional diversity of the DNABERT intermediate layer. As mentioned in our analysis section, the more diverse the model’s intermediate layer attention mechanisms are represented, the better the model is at modeling regional information. By comparing Figure5(b), (c), and (d) of the visualization of the attentional mechanism, the model pre-trained with RandomMask (d) obtained the most diverse intermediate layer attention mechanisms. It shows that RandomMask makes the model better at modeling regional information by allowing the model to reconstruct DNA sequences of different lengths.

Thirdly, RandomMask alleviates the problem of overlapping 6mer tokenizer pre-training loss converging too fast. In Figure4, we can see that DNABERT’s loss (Figure4(a)) will quickly decrease to an extremely low value. If RandomMask (Figure4(c)) is used, the loss will increase at the start of each stage, giving it enough space to decrease. We can see a decrease at each stage in the loss curve with RandomMask. RandomMask enhances the generalization of the model by increasing the difficulty of the pre-training task.

SECTION: VIConclusion

While overlapping 6-mer tokenizer offers distinct advantages in fine-tuning downstream tasks, their propensity for fast convergence can hinder comprehensive pre-training. RandomMask emerges as a potent solution, leveraging adaptive masking to push models to learn more effectively and deeply. RandomMask ensures that models can handle DNA sequences’ nuances and broad patterns (nucleotide and region-level information) by continuously increasing task difficulty and expanding mask boundaries. Using RandomMask during BERT-like DNA pre-training improves the performance of the model. In particular, the performance improvement of RandomMask is more obvious when overlapping 6-mer tokenizer is used.

SECTION: References

SECTION: Appendix

SECTION: VI-AAdditional Results

TableIVshows the results for each dataset on the 7 downstream tasks.

SECTION: VI-BSensitivity Analysis on Sequence Length

In TableV, we investigate the effect of sequence length. The row labeled “Same-length” shows the effect of creating a sequence with the same length as the “Overlapping” sequence by repeating the tokens from the ”Non-overlapping” sequence K-1 times.

Examples shown in Figure7, if the “Non-overlapping” sequence is token by 3-mer “,” the “Same-length” sequence would be “.” In other words, the ”Non-overlapping” sequence “” is repeated 2 times to match the length of the “Overlapping” sequence.

This method allows us to compare the performance of non-overlapping and overlapping sequences of the same length. The judgments of the comparison are displayed as follows:

An interesting phenomenon. In NT that uses non-overlapping 6-mer for pre-training, stretching the sequence length will indeed produce obvious gains in TF-M, TF-H, and CPD. Combined with Table 1 in the paper, the common feature of these three tasks is that the DNA sequence length is short. The DNA sequence lengths of EMP, PD, and SSP are 500, 300, 400, and 250 nucleotides, respectively. However, the DNA sequence lengths of TF-M, TF-H, and CPD are 100, 100, and 70 nucleotides, respectively, and these are shorter than others.

But in general, using the overlapping tokenizer to obtain more diverse tokens achieves better performance than simply lengthening the sequence length (Same-length) of both the overlapping pre-training (DNABERT) model and the non-overlapping pre-training model (NT).

SECTION: VI-CHyperparameters

TableVIsummarizes the default hyperparameter settings for various configurations of the DNABERT models.

TableVIIpresents the default hyperparameter settings for the Nucleotide Transformer model across various downstream tasks.

Lastly, TableVIIIdetails the default hyperparameter settings for the HyenaDNA model.