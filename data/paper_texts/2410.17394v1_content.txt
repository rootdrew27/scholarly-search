SECTION: packetLSTM: Dynamic LSTM Framework for Streaming Data with Varying Feature Space

We study the online learning problem characterized by the varying input feature space of streaming data. Although LSTMs have been employed to effectively capture the temporal nature of streaming data, they cannot handle the dimension-varying streams in an online learning setting. Therefore, we propose a dynamic LSTM-based novel method, called packetLSTM, to model the dimension-varying streams. The packetLSTM’s dynamic framework consists of an evolving packet of LSTMs, each dedicated to processing one input feature. Each LSTM retains the local information of its corresponding feature, while a shared common memory consolidates global information. This configuration facilitates continuous learning and mitigates the issue of forgetting, even when certain features are absent for extended time periods. The idea of utilizing one LSTM per feature coupled with a dimension-invariant operator for information aggregation enhances the dynamic nature of packetLSTM. This dynamic nature is evidenced by the model’s ability to activate, deactivate, and add new LSTMs as required, thus seamlessly accommodating varying input dimensions. The packetLSTM achieves state-of-the-art results on five datasets, and its underlying principle is extended to other RNN types, like GRU and vanilla RNN.

SECTION: 1Introduction

Online learning, characterized by streaming data, where data instances arrive one by one, has been studied extensively(Gama,2012; Neu & Olkhovskaya,2021; Agarwal et al.,2008). Recently, there has been a growing focus on online learning in environments with varying input feature spaces. Examples include movie sentiment classification and crowdedness severity prediction(He et al.,2023; Agarwal et al.,2024). These varying input features, termed haphazard inputs(Agarwal et al.,2023), are denoted as, whereindicates the dimensionality of input, varying over time. The field of haphazard inputs is expanding, prompting the introduction of new methods, applications, and appropriate datasets as elaborated in sectionAof the Appendix.

The current landscape is focused on developing new methods. Predominantly, haphazard inputs are modeled using classical approaches like naive Bayes(Katakis et al.,2005), decision stumps(Schreckenberger et al.,2022;2023), and linear classifiers(Beyazit et al.,2019), favored for their dynamic architectures. However, there is a push towards developing dynamic deep learning solutions(Agarwal et al.,2022;2023), motivated by the capabilities of neural networks. Nevertheless, current methodologies have not adequately leveraged the streaming nature of data. To bridge this gap, we advocate using Recurrent Neural Networks (RNNs)(Hochreiter & Schmidhuber,1997; Zhang et al.,2021a), which can effectively exploit the temporal dynamics of data.

We introduce a novel architecture, termed packetLSTM, designed to dynamically adapt to varying input feature space. This framework employs a unique ensemble of Long Short-Term Memory (LSTM) units, each dedicated to a specific input feature. The packetLSTM allows for robust interaction among its LSTMs, fostering the integration of global information while preserving feature-specific knowledge within each unit’s short-term memory. The packetLSTM facilitates continuous learning without the risk of catastrophic forgetting in online learning environments(Hoi et al.,2021). The feature-based learning, coupled with a dimension-invariant aggregation operator, allows packetLSTM to dynamically activate, deactivate, and add new LSTMs as needed, leading to adeptly managing haphazard inputs. The main contributions of our work are as follows. (1) We introduce the first RNN-based framework, packetLSTM, to effectively handle haphazard inputs in online learning. (2) The packetLSTM exhibits learning without forgetting capabilities in an online learning setting. We demonstrate this capability in a challenging scenario where certain features are absent for extended time periods. (3) The principles underlying the packetLSTM framework are adaptable and can be extended to other types of RNNs, like Gated Recurrent Units (GRUs) and vanilla RNNs. We substantiate this adaptability by developing packetGRU and packetRNN models. (4) We achieve state-of-the-art results across five datasets, as shown in Figure1. (5) We introduce a strong baseline based on the Transformer called HapTransformer and demonstrate that HapTransformer outperforms other baselines; however, it is still inferior to packetLSTM.

SECTION: 2Related Works

The initial approach to address haphazard inputs utilized naive Bayes andstatistics to dynamically incorporate features, update existing feature statistics, and select a feature subset(Katakis et al.,2005). This method was further expanded by employing an ensemble of naive Bayes classifiers for predictive analysis(Wenerstrom & Giraud-Carrier,2006). Subsequent research has focused on inferring unobserved features from observed ones using various techniques, including graph methods(He et al.,2019; Sajedi & Razzazi,2024), and Gaussian copula(He et al.,2021; Zhuo et al.,2024), followed by the application of classifiers across the complete feature space. Concurrently, another line of research projects data into a shared subspace to learn a linear classifier using empirical risk minimization(Beyazit et al.,2019)or online gradient descent(Zhou & Matsushima,2023). Distinctly,You et al. (2024)maintains an informativeness matrix of each feature to update a linear classifier. Another research direction explores the use of decision trees to handle haphazard inputs. Specifically,Schreckenberger et al. (2022)proposed Dynamic Forest, which uses an ensemble of decision stumps, each based on a feature from a selected subset of all seen features. However, this approach can result in numerous decision stumps, promptingSchreckenberger et al. (2023)to introduce a refined approach that constructs only one decision stump for each feature.Lee et al. (2023a)proposed to utilize adaptive random forest on a fixed set of features, created through imputation assuming large buffer storage.

Despite the dynamic nature of the above classical methods in modifying their architectures, the era of big data necessitates adopting deep learning approaches to effectively model haphazard inputs. To date, seminal contributions in this domain include works byAgarwal et al. (2022;2023), which are based on neural networks. The Auxiliary Network(Agarwal et al.,2022)incorporates parallel hidden layers to process each input feature. Conversely, Aux-Drop(Agarwal et al.,2023)implements selective and random dropouts within its layers to handle haphazard inputs. Although these models operate under specific assumptions, recent advancements by the same authors(Agarwal et al.,2024)provide simple solutions to mitigate these assumptions in haphazard inputs. While all the methods discussed above handle haphazard inputs, none effectively exploits the temporal dynamics of streaming data, which we achieve using RNNs, specifically LSTMs.

RNNs are among the most popular models in sequential data analysis(Salehinejad et al.,2017; Allen-Zhu & Li,2019). Various techniques are developed to capture the temporal dynamics of data, including stacking LSTMs to establish a hierarchical framework(Hermans & Schrauwen,2013; Wang et al.,2017), as well as designing specialized time-modeling LSTM units(Che et al.,2018; Kazemi et al.,2019). In this article, we utilize Time-LSTM(Zhu et al.,2017)as our LSTM units because of its demonstrated capability in modeling both short-term and long-term interest. LSTMs have also been extensively used in the field of multi-modality(Xie & Wen,2019; Liu et al.,2018; Xu et al.,2020; Lee et al.,2023b). However, to the best of our knowledge, the RNN-based models proposed in the multi-modal domain or any other domain are not capable of modeling data with varying input dimensions in an online learning setting as discussed in sectionBof the Appendix. Therefore, we propose a new dynamic RNN framework in this article, filling a significant gap in the research landscape of RNNs and haphazard inputs.

SECTION: 3Preliminaries

In this article, we represent time in superscript and feature id in subscript. For example,represents the value of feature 2 () at time. For ease of readability, we slightly adjust the notation of time here. Instead of denoting by, we use. When time is referenced individually, it is denoted as. Moreover,indicates a random time, anddenotes the time preceding. A complete list of notations is provided in sectionCof the Appendix.

Haphazard inputs exhibit six characteristics which are illustrated in Figure2(b). These characteristics are: (1)Streaming data, which are received sequentially and processed without storage. (2)Missing data, which are features present in some instances but may be absent in subsequent ones likeat time. (3)Missing features, that are not received from the onset; however, their availability is known like. (4)Sudden features, that arrives unexpectedly without prior indication of their existence likeat time. (5)Obsolete features, which can cease to exist after any instance, such as. (6)Unknown number of total features, results from the combined effect of missing data, missing features, sudden features, and obsolete features.

The characteristics of haphazard inputs result in a varying feature space. We define a universal feature space () as the set of features encountered till time. The specific set of features present at timeis represented byand is termed current feature space as shown in Figure2(b). The universal feature space will grow with time due to the emergence of sudden features and missing features. For example,at time, andat timein Figure2(b). In an ideal condition,can contract with the removal of obsolete features; however, since the cessation of obsolete features is unknown,may not decrease in practice.

The haphazard input received at timecan be represented by, where. Here,represents the cardinality of a set. The corresponding ground truth is denoted by, whereandis the number of classes. This paper deals with the binary classification problem but can be easily extended to multi-class scenarios. The model, denoted by, operates in an online learning setting with, whererepresents the initialized state of the model. After processing, the model’s state is represented by. At each time, the model receives, andprocessesto yield a prediction. Upon the revelation of, the lossis computed, whereis a loss function. The model then updates fromtofor the subsequent instances based on. This iterative process continues for each instance.

SECTION: 4Method

The packetLSTM consists of a pack of LSTMs, each dedicated to a distinct feature, as illustrated in the gray box in Figure2(a). We utilize LSTMs due to their proven effectiveness in capturing temporal dynamics of data(Kazemi et al.,2019; Wang et al.,2017; Zhang et al.,2021a).

Due to the varying dimensions of input feature space, a single LSTM cannot process all features, necessitating one LSTM per feature. Each LSTM () receives inputs comprising the feature value (), time delay (), its previous short-term memory (), and common long-term memory (), as labeledInputin Figure2(a). Themeasures the time difference between the current timeand the last observed timefor feature. This temporal information is critical to the model because feature availability varies(Zhu et al.,2017; Che et al.,2018).

At each instance, only LSTMs corresponding to available features are activated. For example, at time, the absence of featureresults in the deactivation of LSTM, depicted by a sketched gray box in Figure2(a). Thus, packetLSTM dynamically handles all the characteristics of the haphazard inputs by activating, deactivating, or adding new LSTMs as needed.

The common long-term memory (), which aggregates information from all long-term memories of active LSTMs, facilitates the interaction among features, crucial for enriched knowledge acquisition(Zhang et al.,2021b). Thiscontains global information, while the short-term memory of each LSTM retains the local information about individual features, alleviating the issue of catastrophic forgetting in an online learning setting. The aggregation operator, being dimension-invariant, accommodates the variable number of features. All active short-term memories are aggregated to generate a predictive short-term memory (). These memories are subsequently concatenated and processed through a fully connected classifier to produce the final output(see Figure2(a)).

The model parameters are updated based on the lossin an online learning setting. New LSTMs are introduced with initialized memories (,) and a time delay () set to zero for sudden (e.g.,at) and missing features (e.g.,at), as shown in Figure2.

Based on the current feature spaceat time, the output of each LSTM is given by. Numerous LSTM variants exist in the literature that model time delay (), such as decay in GRU-D(Che et al.,2018), time gate in Phased LSTM(Neil et al.,2016), and Time2Vec(Kazemi et al.,2019). In this article, we employ Time-LSTM(Zhu et al.,2017)because of its highly demonstrated capability in modeling time information. Time-LSTM utilizes time delay to capture the short-term interest for current instances and preserves these delays to address long-term interest for future instances. We specifically utilize Time-LSTM 3 (T-3) among its three versions – Time-LSTM 1, Time-LSTM 2, and T-3 – because it integrates the input and forget gates(Greff et al.,2016), offering a model that is both less computationally intensive and concise. The formulation of T-3 (and other time modeling variants) for featureat time, within the packetLSTM framework, represented by, is discussed in the sectionDof the Appendix.

Our packetLSTM framework differs from T-3 in its approach to handling input feature spaces. Unlike T-3, which utilizes a single LSTM unit for a fixed feature space, our approach employs a distinct LSTM unit for each feature to manage a varying feature space. This necessitates a different input configuration for each LSTM compared to the T-3. Specifically, rather than using the previous long-term memory of each LSTM as an input, the packetLSTM framework utilizes a common long-term memory, which facilitates interaction among features. Moreover, while T-3 accepts the short-term memory from timeas input at time, the packetLSTM inputs the short-term memory from timeat time, whereis not necessarily equal toin haphazard inputs.

Next, a dimension-invariant aggregation operator () determines the common long-term memory and the predictive short-term memory as

Theoperator, defined as, accepts a variable number of inputs, specifically,inputs. Given thatandare vectors, theoperator performs aggregation element-wise. Common examples of dimension-invariant aggregation operators include mean, maximum, minimum, and summation. Finally, the predictionis generated by a fully connected neural network (FCN), applied on the concatenation ofandas.

SECTION: 5Experiments

We consider 5 datasets – magic04(Bock et al.,2004), imdb(Maas et al.,2011), a8a(Kohavi et al.,1996), SUSY(Baldi et al.,2014), and HIGGS(Baldi et al.,2014)– with details provided in sectionEof the Appendix. The motivation to choose these datasets is three-fold. First, they include both real (imdb) and synthetic datasets. Second, the number of instances varies from 19020 in magic04 to 1M in HIGGS. Third, the number of features ranges from 8 in SUSY to 7500 in imdb. Therefore, the diversity in the number of features and instances allows us to determine the efficacy of packetLSTM effectively. The imdb dataset is haphazard in nature. However, the synthetic dataset needs to be transformed into haphazard inputs. Following the baseline papers(Beyazit et al.,2019; Agarwal et al.,2023), we transform synthetic datasets based on the probability values, where0.25 means only 25% of features are available at each time instance. We consider0.25, 0.5, and 0.75. The synthetic dataset preparation is further discussed in sectionEof the Appendix.

We compare all models using five metrics: number of errors, accuracy, Area Under the Receiver Operating Characteristic curve (AUROC), Area Under the Precision-Recall Curve (AUPRC), and balanced accuracy. Each metric is discussed in sectionFof the Appendix. The balanced accuracy is the primary comparison metric in the main manuscript, while detailed comparisons using the other metrics are presented in sectionKof the Appendix. We adhere to the standard evaluation protocol for haphazard inputs, which is detailed in sectionGof the Appendix.

We consider 10 baseline models, including NB3(Katakis et al.,2005), FAE(Wenerstrom & Giraud-Carrier,2006), DynFo(Schreckenberger et al.,2022), ORF3V(Schreckenberger et al.,2023), OLVF(Beyazit et al.,2019), OCDS(He et al.,2019), OVFM(He et al.,2021), Aux-Net(Agarwal et al.,2022), Aux-Drop(Agarwal et al.,2023), and OLIFL(You et al.,2024). Additionally, there are a few other models – see sectionHof the Appendix – applicable to the field of haphazard inputs. However, we could not include these models because of the lack of open-source code and the challenges associated with implementing them.

We ran all the models five times, except for the deterministic models – NB3, FAE, OLVF, and OLIFL – which consistently produce identical outcomes across runs. The results are reported as meanstandard deviation (std). For packetLSTM, max and mean aggregation operators were used in synthetic and real datasets, respectively. Additionally, Z-score normalization was implemented in an online manner, as discussed in section6. The implementation details and hyperparameter search are discussed in the sectionIandJof the Appendix, respectively.

The balanced accuracy () of all the models is presented in Table1. The packetLSTM outperforms all the models across each dataset by a significant margin except in a8a with0.25. To quantify the performance enhancement of packetLSTM over the previously best-performing baseline (), we calculate the % improvement as, and these results are shown in the last column of Table1. Among synthetic datasets, packetLSTM surpasses theby at least 9.44% and 2.05% in magic04 and HIGGS, respectively. Moreover, packetLSTM consistently exhibits superior performance across allwithin the SUSY dataset. In a8a, packetLSTM achieves the best result in 2 out of 3 cases, except at0.25, where its performance is comparable. In the real dataset (imdb), packetLSTM outperforms theby a margin of 3.5%. Thus, packetLSTM demonstrates superior efficacy in both synthetic and real datasets.

SECTION: 6Ablation Studies

We conduct ablation studies within packetLSTM to assess the impact of each component and identify optimal variants, adhering to the hyperparameters described inImplementation Detailsof section5, unless specified otherwise. We report the mean of the balanced accuracy in the main manuscript (Table2), with std detailed in sectionLof the Appendix. We calculate the number of wins for each component across all dataset combinations, defining a win as achieving the highest balanced accuracy or being within a 0.05 margin of the top value, accounting for variability indicated by the std. For example, in the magic04 dataset at, the packetLSTM with the Min aggregator reports a slightly higher balanced accuracy (61.36) than the Max (61.33) but exhibits greater std (0.14 vs. 0.07). This variability suggests that some runs using the Min aggregator achieved lower balanced accuracy than the Max. Therefore, a 0.05 margin mitigates such discrepancies.

The Max operator, in general, performed best with 7 wins, as shown in theAGGcomponent of Table2. However, to determine the best operators, we compare them one by one (seein Table2). (1) The performance difference between Min and Max is negligible, indicating both are suitable choices. (2) Between Sum and Mean, there is no evident superiority. However, the Sum is sensitive to the number of features, while the Mean remains relatively stable unless the feature distribution changes substantially. We tested this by alternating(from 0.25 to 0.75) every 100 instances in the a8a dataset, which has the most features among the synthetic datasets. The Mean operator (balanced accuracy = 62.77) substantially outperformed the Sum operator (54.16), establishing the Mean as the superior operator. (3) Table2indicates that the efficacy of the Mean increases as data availability decreases from= 0.75 to 0.25. Further experiments at= 0.05 and 0.95, shown in sectionMof the Appendix, validate this trend. Consequently, Max is recommended for scenarios with high data availability, while the Mean is preferable otherwise. This is further supported by the imdb dataset (= 0.0165), where the Mean (85.06) significantly surpassed the Max (77.90). Consequently, we utilized the Max aggregator for all the synthetic datasets and the Mean operator for the real dataset.

We investigated 4 variants of time modeling: T-3, Time-LSTM 2, Time-LSTM 1, and Decay(Che et al.,2018). As observed in theTime Modelcomponent, T-3 performed the best. Moreover, LSTM, without any time modeling (None), performed the worst, affirming the beneficial impact of time modeling for haphazard inputs. Moreover, all the 4 variants outperform thein 11 out of 13 scenarios except in a8a (0.25) and SUSY (0.75).

The packetLSTM utilizes current feature space for the predictive short-term memory. However, it is also possible to consider the universal feature space by aggregating the short-term memories from all LSTMs linked to the universal features for the predictive short-term memory. The current feature space has 13 wins compared to 11 wins of universal feature space (see theFeatcomponent). Therefore, the current feature space is sufficient for enhancing the predictive capability of packetLSTM.

The final prediction in packetLSTM concatenates common long-term and predictive short-term memory. We assessed the importance of each memory type by comparing the original packetLSTM, termedBoth, with its variants that either exclude predictive short-term memory (Only LTM) or common long-term memory (Only STM), as shown in theConcatcomponent.Bothsignificantly outperformed others, securing 12 wins, confirming that both memory types are crucial. While common long-term memory holds global information, predictive short-term memory provides local information, and individually, they surpassin most datasets except in a8a and HIGSS (0.5).

We observed significant variation in the range of feature values across datasets. For example, in magic04, the first feature ranges from 4.28 to 334.18. Therefore, we performed streaming normalization by using five methods: Min-Max, Decimal Scaling, Mean-Norm, Unit Vector, and Z-score, as detailed in sectionNof the Appendix. Table2shows that Z-score performed the best, underscoring the need for streaming normalization in handling haphazard inputs. Remarkably, the packetLSTM, even without any normalization (labeled asNone), surpassedin all datasets except for a8a (0.25), showcasing the superiority of packetLSTM.

The packetLSTM framework’s principles are adaptable, leading us to develop packetRNN and packetGRU architectures. We outline these architectures’ formulations and performance in sectionOof the Appendix and theRNNcomponent of Table2, respectively. Both packetRNN and packetGRU deliver competitive performances compared to. Specifically, packetRNN outperformsin the magic04, imdb, SUSY, and HIGGS datasets, while packetGRU excels in magic04, a8a (0.75), SUSY, and HIGGS. These results confirm the effectiveness of the packet architecture. We evaluate packetLSTM without any time modeling component for a fair comparison among packet architectures. The packetRNN emerges as the best performer, although it significantly underperforms in the a8a dataset. Consequently, we use LSTM for the packet architecture. Furthermore, LSTM’s ability to incorporate long-term memory allows it to encode global information, a characteristic absent in both GRU and vanilla RNN.

The time complexity of packetLSTM is, whereis the number of instances,denotes the complexity for processing all activated LSTMs at time, andencompasses constant-time operations such as aggregations and final predictions. Here,is the hidden size of the LSTM, and the functionvaries between 1 and, often close to 1. Thus, the packetLSTM scales well in terms of time complexity with feature size. The space complexity at timeis, whereis the space complexity of one LSTM andaccounts for the final prediction network and aggregated memories. The introduction of sudden and missing features increases the space complexity, showing potential scalability limits. However, packetLSTM easily handles the 7500 features in the imdb dataset, affirming its capability to handle high-dimensional data.
Detailed calculations of time and space complexities and a strategy to mitigate space complexity are provided in sectionPandQof the Appendix.

The effectiveness of packetLSTM is further evidenced through its comparison with a single LSTM model, which necessitates the availability of all features. This requirement can be met through imputation, although it requires contradicting the sixth characteristic of haphazard inputs where the total number of features is unknown. Nonetheless, for the purpose of evaluating packetLSTM, we compare it against a single LSTM, employing three imputation techniques: forward fill, mean of the last five observed values of a feature, and Gaussian copula(Zhao et al.,2022). Table3demonstrates that packetLSTM significantly outperforms single LSTM with all imputation-based techniques across all datasets, except in a8a at= 0.75. Further details on the single LSTM model, hyperparameter search, the Gaussian copula model, and its specific performance issues with the a8a and imdb datasets are discussed in sectionRof the Appendix.

SECTION: 7Challenging Scenarios

We design three challenging experiments to explicitly elucidate the efficacy of packetLSTM to (1) handle sudden features, (2) handle obsolete features, and (3) demonstrate the learning without forgetting capability. We considered the HIGGS and SUSY datasets for these experiments due to their substantial number of instances. In the main manuscript, we discuss the results corresponding to HIGGS, while similar conclusions for SUSY are discussed in sectionS.2of the Appendix. For comparative analysis, we opted for OLVF, OLIFL, OVFM, and Aux-Drop baselines based on their superior performance in the HIGGS dataset, as indicated in Table1. We divided the dataset into 5 intervals, each consisting of 20% of the total instances, with successive intervals containing the next 20% of instances. For the exact values of Figure3and4, refer to sectionS.1of the Appendix.

Here, each subsequent interval includes an additional 20% of features, starting with 20% in the first interval and increasing to 100% by the fifth interval. This progression, termed the trapezoidal data stream(Zhang et al.,2016; Liu et al.,2022), is illustrated in Figure3by a pink-shaded region. Model performance is expected to enhance with increased data volume. All models, except OLVF, exhibit this increasing performance trend. Notably, packetLSTM outperforms other models in each interval, demonstrating its superior capability in handling sudden features.

Here, all features are initially present in the first interval, then only the first 80% remain in the second interval, decreasing sequentially as shown in Figure3. Intuitively, the model’s performance should deteriorate as data volume decreases, a pattern observed in all models except Aux-Drop. The Aux-Drop’s performance drops significantly in the second interval, suggesting a misleading impression of improvement in the third. The packetLSTM outperforms all other methods in each interval, demonstrating its superior ability to handle obsolete features.

We assess the model’s performance in a scenario where feature sets are disjoint across consecutive intervals and reappear after a gap, as shown in Figure4. The first 50% of features are present in the first, third, and fifth intervals, while the remaining 50% appear in the second and fourth intervals. Ideally, effective learning without forgetting would result in improved performance upon the reappearance of the same features. However, apart from packetLSTM and OLIFL, all models show declining performance as they progress from the first to the third and fifth intervals and from the second to the fourth interval. Even OLIFL fails in the SUSY dataset as its performance declines from the second to the fourth interval (see sectionS.2of the Appendix). The packetLSTM’s ability to retain knowledge is attributed to each feature’s local information stored in its corresponding LSTM’s short-term memory. To quantify knowledge retention, we compare packetLSTM with a version retrained (packetLSTM-retraining) at each interval using the available features (see the rightmost graph of Figure4). The packetLSTM’s balanced accuracy improves by 1.61 and 1.89 at the third and fifth intervals, respectively, and by 0.93 at the fourth interval, demonstrating effective learning without forgetting. Notably, there is also a performance increase in the second interval despite those features not being previously observed. This increase is due to the global information learned during the first interval, aiding subsequent predictions. Overall, packetLSTM outperforms all other baselines in each interval. It is important to distinguish that catastrophic forgetting within an online learning setting differs from online continual learning (see sectionTof the Appendix).

SECTION: 8Transformer on Haphazard Inputs

Despite the lack of application of the Transformer(Vaswani et al.,2017)in the field of haphazard inputs, its inherent ability to manage variable-size inputs makes it a natural choice. Therefore, we also investigate Transformer-based methodologies for modeling haphazard inputs.

We consider padding inputs with zeros or truncating them, which necessitates specifying a fixed input length (). If the number of features in an instance () is less than, the Transformer pads the input with zeros, and ifexceeds, it truncates the excess features, potentially leading to information loss. A potential, albeit inefficient, solution is to set an excessively high. Nonetheless, to assess how packetLSTM compares to Transformer, we conducted two experiments:Only Values, padding available feature values, andPairs, pairing each feature value with its feature ID and padding the sequence. The packetLSTM outperforms Transformer with padding across all dataset scenarios (see Table3). Further details can be found in sectionU.1of the Appendix.

Given the variable size of inputs, natural language can be seen as an application where features arrive one by one, and most are missing. We compared packetLSTM with DistilBERT(Sanh et al.,2019)and BERT(Devlin et al.,2019), detailed in sectionU.2of the Appendix. We observe that both DistilBERT and BERT are unable to perform classification on haphazard inputs with a balanced accuracy of around 50 in all cases.

The Set Transformer(Lee et al.,2019), designed for variable-length inputs, has been previously used in offline learning. We employ it for online learning to handle haphazard inputs. Results in Table3indicate that packetLSTM significantly outperforms Set Transformer across all datasets, possibly due to Set Transformer’s assumption of permutation invariance, which does not hold for haphazard inputs. More details are provided in sectionU.3of the Appendix.

To tackle permutation invariance, we introduce HapTransformer, which transforms each feature into a distinct learnable embedding, a technique similarly utilized in prior research(Huang et al.,2020; Gorishniy et al.,2021; Somepalli et al.,2021). However, these models do not accommodate variable-sized inputs. The learnable embeddings are subsequently processed by the Set Transformer’s decoder, allowing implicit communication of feature identities. Details on the architecture and hyperparameters are provided in sectionU.4of the Appendix. Despite its strengths, packetLSTM surpasses HapTransformer in all dataset scenarios (see Table3). HapTransformer struggles particularly with datasets having high feature counts like imdb and a8a, and requires significant computational time. However, it remains a strong baseline, outperforming other baseline models in the SUSY and HIGGS datasets, as detailed in Tables1and3.

SECTION: 9Conclusion

In conclusion, our work introduces packetLSTM, a dynamic framework for handling streaming data with varying feature dimensions in real-time learning environments. Significantly, the underlying principles of packetLSTM are extendable to other architectures, such as vanilla RNN and GRUs, demonstrating the model’s adaptability to different neural architectures. The packetLSTM not only outperforms existing methods across various datasets but also showcases flexibility in adapting to changing data conditions without forgetting previously learned information. Our research opens new avenues for further exploration of dynamic neural architectures and sets a new benchmark for online learning with haphazard inputs.

SECTION: 10Ethics Statement

The packetLSTM framework introduces a novel method for handling streaming data with variable features, potentially impacting numerous fields such as healthcare, finance, autonomous systems, environmental monitoring, and personalized recommendation systems. In healthcare, it can improve real-time patient monitoring by adapting to new or absent data types, enhancing patient care. Financial sectors can benefit from more stable predictive models for trading and risk management due to their ability to process erratic market data. Autonomous systems can gain reliability by effectively managing inconsistencies in sensory data, while environmental monitoring may achieve greater accuracy in tracking ecological changes, aiding policy decisions. In digital platforms, it may refine personalized recommendations by adjusting to user behavior changes, and in educational technology, it may personalize content to student needs, improving engagement and learning outcomes.

Due to the potential application of packetLSTM in the above-discussed crucial and sensitive fields, it is necessary to consider ethical concerns. The packetLSTM framework must navigate several ethical issues to align with the European Union’s Artificial Intelligence Act (EU AI Act). Deploying packetLSTM technology requires careful consideration, including data privacy, transparency in decision-making, and addressing data biases to prevent discrimination and ensure fairness across diverse user groups. By addressing these challenges, packetLSTM can significantly enhance efficiency and personalization across multiple domains while upholding high ethical standards.

SECTION: 11Reproducibility

All the information to reproduce the result is available in sections4,5,6,7, and8of the main manuscript. Additional information is also provided in sectionsI,J,R, andUof the Appendix. The code can be found athttps://github.com/Rohit102497/packetLSTM_HaphazardInputswith sufficient instructions to faithfully reproduce all the experimental results. The link to the datasets is provided in sectionEof the Appendix. We report the resources used to conduct the experiments in sectionIof the Appendix. Moreover, for the benchmark results, we also report the time taken by each individual model in sectionKof the Appendix.

SECTION: Acknowledgements

We acknowledge the various funding that supported this project: Researcher Project
for Scientific Renewal grant no. 325741, UiT’s thematic funding project VirtualStain
with Cristin Project ID 2061348, and the H2020 project for OrganVision with Project ID 101123485.

SECTION: References

SECTION: Appendix APractical Applications

With the progress of the field of haphazard inputs, more datasets are expected to become available. For instance,Schreckenberger et al. (2023)introduced the crowdsense dataset in 2023, collected from environmental sensors (measuring sound pressure, eCO2 level, and eTVOC level) across Spain’s 56 largest cities over 790 days from January 1st (2020) to February 28th (2022). The feature space varies as the new sensing data continuously emerges while many old sensors stop to provide data. The aim is to predict government restriction severity based on sensed regional crowdedness. We could not include this dataset in our study because it has only 790 instances, which is very small for a deep-learning model. Nevertheless, we anticipate the emergence of larger datasets in this domain. Note that we demonstrate the superior performance of packetLSTM compared to article(Schreckenberger et al.,2023)in our study in Table1in the main manuscript.

Another potential application is in the study of sub-cellular organisms, as discussed in(Agarwal et al.,2024). The mitochondria undergo morphological changes during the drug discovery process, leading to fusion, fission, and kiss-and-run events. Quantifying these events is crucial for biologists, yet direct observation is impractical due to hundreds of mitochondria within a single cell. Fusion and fission introduce obsolete and sudden features, while kiss-and-run events can lead to sudden, obsolete, and missing features. The combination of haphazard input and segmentation models may effectively model the mitochondrial dynamics.

SECTION: Appendix BMulti-Modal Learning vs packetLSTM

The multi-modality requires handling data from different data domains, like images, audio, etc. Some studies in the multi-model domain utilize LSTMs (more generally RNNs), as discussed below.

LSTM-MA(Xie & Wen,2019)converts the multi-modal image slices to feature sequences using pixel and superpixel constraints. These are then fed to the LSTM, followed by a fully connected neural network to predict the final class label. Finally, the segmentation image is obtained by combining all the classified nodes of a slice.

Liu et al. (2018)proposed an RNN framework for online action detection and forecasting on the fly from the untrimmed stream data. The data have two modalities, i.e., RGB video and skeleton sequence data. The paper uses deep ConvNet and motion networks to extract embedding from RGB and skeleton data, respectively, and the embeddings are processed individually by stacked LSTM.

AE-MSR(Xu et al.,2020)works on two modalities, namely, audio and video, to perform speech recognition. Here, both audio and video data are passed to an AE sub-network to generate visual features and enhanced audio magnitude. The generated features are further passed to different element-wise attention-gated recurrent units (EleAtt-GRU) encoders. This generates visual and audio context, respectively. Both contexts are further passed to a decoder consisting of EleAtt-GRU to generate outputs.

Within the multi-modal domain, to the best of our knowledge, the total number of features/modalities always remains the same and is known at the
outset (= 0). Some recent work also considers missing modality in the multi-modal domain(Lee et al.,2023b), but still, the total number of modalities is known at the outset (= 0). Therefore, the above method cannot accommodate haphazard inputs. Whereas the packetLSTM can dynamically change its architecture to adapt to varying feature spaces in real-time environments.

SECTION: Appendix CNotations

All the notations used in this article are provided in Table4with their corresponding meanings.

SECTION: Appendix DDifferent Time-Modeling Variants Within the Context of packetLSTM

In this article, we present the packetLSTM framework with four different time modeling variants. Primarily, we employ Time-LSTM 3(Zhu et al.,2017)within the packetLSTM framework. Additionally, we explore the use of Time-LSTM 1, Time-LSTM 2, and decay(Che et al.,2018)into the packetLSTM architecture. The following paragraphs discuss all these time-modeling variants within the context of packetLSTM.

The formulation of Time-LSTM 3 (and other time modeling variants) for featureat time, within the packetLSTM framework, represented by, is given by. Mathematically, the computation ofis expressed as

whereandrepresents the input gate, time gate 1, time gate 2, and output gate of the LSTMat time, respectively. The cell stateinfluences the current prediction through the output gate and the short-term memory. The functionsandare sigmoid functions, whileandare tanh functions. Thedenotes the weight associated with the short-term memory within the input gate of. This definition extends analogously to allparameters. The symbolindicates the Hadamard product, and all theparameters are peephole connection weights(Gers & Schmidhuber,2000).

Similar to Time-LSTM 3, Time-LSTM 2 has two gates. Therefore, equations 2, 3, 4, 7, and 8 hold true for Time-LSTM 2, along with an addition of forget gate (), an update of the equation of cell state (), and long-term memory ().

Time-LSTM 1 only contains a single time gate. Therefore, the formulation of Time-LSTM 1 within the packetLSTM framework is given by equations 2, 9, 3, 10, 7, and 8.

The decay mechanism introduced byChe et al. (2018)attenuates the short-term memory by a learnable factor. Subsequently, the process adheres to the conventional operations of a vanilla LSTM, which is given by

SECTION: Appendix EDatasets

The detailed descriptions of each dataset are provided in Table5and further elaborated below:

magic04(Bock et al.,2004): It is a Monte Carlo simulated dataset for the registration of high-energy particles in an atmospheric Cherenkov telescope. The binary classification task associated with magic04 is to distinguish between a shower image caused by primary gammas (1) and cosmic rays in the upper atmosphere (0). The magic04 dataset can be accessed via this link111https://archive.ics.uci.edu/dataset/159/magic+gamma+telescope.

imdb(Maas et al.,2011): It is a movie sentiment classification dataset with labels as positive (1) or negative (0). Following the previous literature in the field of haphazard inputs(Beyazit et al.,2019; Agarwal et al.,2024), we consider the training subset of the data provided in this link222https://ai.stanford.edu/~amaas/data/sentiment/.

a8a(Kohavi et al.,1996): It is the income data from a census conducted in 1994. The task is to classify where the income exceeds $50k per year. The dataset is pre-processed, resulting in 123 features. The a8a dataset can be accessed via this link333https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html.

SUSY(Baldi et al.,2014): It is a Monte Carlo simulated data of kinematic properties of particles with a binary classification task of predicting between a signal process (1) where SUSY particles are produced and a background process (0) with the
same detectable particles. Following article(Agarwal et al.,2023), the first 8 features are considered. The SUSY data can be accessed via this link444https://archive.ics.uci.edu/dataset/279/susy.

HIGGS(Baldi et al.,2014): Similar to SUSY, it is a Monte Carlo simulation data associated with a binary classification task to differentiate between a signal process (1) where new theoretical Higgs bosons are produced and a background process (0) with identical decay products but distinct kinematic features. We consider the first 21 features of the HIGGS dataset(Agarwal et al.,2023). The HIGGS data can be accessed via this link555https://archive.ics.uci.edu/dataset/280/higgs.

We created haphazard input datasets from synthetic datasets based on probability values, as defined in the baseline papers(Beyazit et al.,2019; Agarwal et al.,2023). Specifically, 100% of features at each time instance is simulated as available independently of each other following a uniform distribution. For example, if0.25, 25% of features at each time instance is only available. From each synthetic dataset, three subsets are generated corresponding tovalues of 0.25, 0.5, and 0.75. This approach creates a spectrum of datasets, from highly unavailable data to those with extensive data availability, thereby facilitating the testing of models under varying conditions of data accessibility. Notably, 98.35% of data values are unavailable in the imdb dataset (see Table5).

SECTION: Appendix FMetrics

This is defined as the number of instances incorrectly classified by the model. A model is deemed more effective when it exhibits a lower number of errors. However, note that the number of errors may not be a suitable metric for imbalanced data.

The Model’s accuracy can be determined by calculating the total number of correct predictions divided by the total number of instances. Similar to the number of errors, accuracy may not serve as a reliable metric in the context of imbalanced datasets, as it can be misleadingly high when the majority class is predicted correctly while neglecting the minority class.

The Area Under the Receiver Operating Characteristic Curve (AUROC) measures a model’s capability to differentiate between positive and negative classes. The ROC curve plots the true positive rates against the false positive rates. The area of this curve, the AUROC, is a value for which higher numbers indicate superior model performance.

AUPRC stands for Area Under the Precision-Recall Curve. It is similar to AUROC; however, it utilizes precision and recall rather than true and false positive rates. AUPRC is particularly valuable for assessing the performance of models on imbalanced datasets, providing insight into the model’s ability to identify positive instances.

This is calculated as the average of specificity and sensitivity. This metric offers a more nuanced assessment of model performance across imbalanced datasets compared to traditional accuracy, as it equally weighs the correct identification rates of both positive and negative classes.

SECTION: Appendix GEvaluation Protocol

We adhere to the standard evaluation protocol for haphazard inputs and, more generally, online learning. As described in theMathematical Formulationparagraph of Section3in the main manuscript, the model is evaluated and then trained iteratively. There is no separate evaluation set. The model first receives input features at time, makes a prediction, and then the ground truth is revealed. The model calculates the cross-entropy loss using the prediction and ground truth. Each instance is processed once and not revisited, reflecting the online learning characteristics of batch size 1 and epoch 1. The prediction logits and the labels for each
instance are stored, and upon processing all the inputs, the balanced accuracy and other metrics are determined based on these data. This evaluation method is consistently applied across all models in this study.

SECTION: Appendix HBaselines Not Included For Comparison

The following models apply to the field of haphazard inputs: OLCF(Zhou & Matsushima,2023), OIL(Lee et al.,2023a), DCDF2M(Sajedi & Razzazi,2024), OFSVF(Zhuo et al.,2024), DFLS(Li & Gu,2023), RSOL(Chen et al.,2024), OVFIV(Qin & Song,2024), RAIL(Kim et al.,2024), OLBD(Yan et al.,2024b), OLCDS(Zhou et al.,2024), and OLIDSPLM(Yan et al.,2024a). However, the inclusion of these additional models in our article was not feasible due to the lack of open-source code and the challenges associated with implementing them, which stemmed from either insufficient detail or the complexity of the models.

SECTION: Appendix IImplementation Details

All the models were implemented using the PyTorch framework, and the experiments were conducted on an NVIDIA DGX A100 machine. Since the model operates in an online learning setting, CPUs were exclusively used to sequentially process the data.

The size for both the long-term and short-term memory components was set at 64. The final fully connected network is a two-layer neural network with a ReLU non-linear activation function. Stochastic gradient descent was employed for back-propagation. Cross-entropy loss and AdamW optimizer were used. The learning rate for magic04, imdb, a8a, SUSY, and HIGGS is set as 0.0006, 0.0008, 0.0009, 0.0008, and 0.0002, respectively.

In the case of Aux-Drop baseline(Agarwal et al.,2023), the online deep learning framework was adopted as the foundational model. The implementation strategies for the majority of baseline models were in accordance with the guidelines provided byAgarwal et al. (2024), except for the OLIFL model(You et al.,2024), for which the implementation from the OLIFL article was utilized. The OVFM model(He et al.,2021), which typically requires buffer storage to store the inputs and thus contradicts the principles of online learning, was adapted by limiting the buffer storage to two instances.

SECTION: Appendix JHyperparmeter Searching

We followed the strategy employed in article(Agarwal et al.,2024), where the best hyperparameters for the synthetic dataset are found at0.5 and subsequently used for0.25 and 0.75. The best hyperparameters for all baseline models, except OLIFL, are derived from article(Agarwal et al.,2024), and the same hyperparameter search protocol is applied to both OLIFL and packetLSTM. Details of the hyperparameter search values and the selected best values are presented in Table6and Table7. The hyperparameter search process for packetLSTM and OLIFL is discussed next.

We determine the best values of hyperparameters sequentially. First, we fixed the learning rate and aggregation operator to 0.0001 and mean, respectively, and varied the hidden sizes of the short-term and long-term memory, being set at 32 and 64. We found the best hidden size to be 64. Next, we experimented with four aggregation operators, namely, mean, max, min, and sum, maintaining a fixed learning rate of 0.0001 and a hidden size of 64. We found max and mean as the best operators for synthetic and real datasets, respectively. Finally, with the best hidden size and aggregation operator established, we tested a range of learning rates (0.05, 0.01, 0.005, 0.001, 0.0005, 0.0001). We searched in the vicinity of the optimal learning rate found in the previous step. For example, 0.0005 was found to yield good results on the magic04 dataset, prompting further exploration of nearby values – 0.0002, 0.0003, 0.0004, 0.0006, 0.007, 0.0008, and 0.0009. Among these, 0.0006 emerged as the most effective learning rate. The same process was followed for all the datasets.

We fixed the option second (min(C, 2loss/inner_product(X))) to determineand tested a range ofvalues (10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001, 0.000001). Similar to packetLSTM, we searched in the vicinity of the optimalvalue found in the previous step. In addition to this, we also experimented with the first option (loss/inner_product(X)) to determine.

SECTION: Appendix KBenchmarking Results on Other Metrics

In the main manuscript, we compared the models in terms of their balanced accuracy. We also provide the comparison of models on other metrics and time in Table8.

SECTION: Appendix LAblations Studies Results with Standard Deviation

The ablation study conducted in section6of the main manuscript reports only the mean balanced accuracy across five runs, as shown in Table2. The comprehensive results, including both the mean and the standard deviation, are presented in Table9.

SECTION: Appendix MMean vs Max Aggregation Operator

Table10presents the comparison between the Mean and Max aggregation operators in the packetLSTM framework. It is evident from Table10that the performance of the Mean operator relative to the Max increases as data availability decreases from= 0.95 to 0.05. The best hyperparameters found at= 0.5 is used for= 0.05 and 0.95.

SECTION: Appendix NStreaming Normalization

The details of each streaming normalization technique are discussed below.

Min-Max Normalization: Here, we utilize two placeholders, namely,and, which represents the maximum and minimum value of featuretill time. The min-max normalization is then defined as.

Decimal Scaling: Here, all the values are scaled down by a predefined threshold as. For all the datasets, we set3.

Z-score Normalization: Here, the values of each feature are normalized based on their running means () and standard deviation () as, whereand. The notationdenotes the count of the featuretill timeand. We utilize the above way of computing running mean and variance because of its superior numerical stability(Knuth,1981).

Mean Normalization: Here, the values of each feature are subtracted by their running means as.

Unit Vector Normalization: Here, we consider the whole input feature as a vector and normalize the vector as, whererepresents thenorm.

SECTION: Appendix OpacketRNN and packetGRU

The packetRNN framework is illustrated in Figure5(a). Unlike the LSTM, which incorporates both short-term and long-term memory, the RNN possesses only a single memory element, known as the hidden state (). Consequently, the packetRNN lacks a mechanism for integrating global information and instead maintains local information within its hidden state. These hidden states are combined using a dimension-invariant aggregation operator to generate a common hidden state for final predictions. The mathematical working of a Vanilla RNN unit within the packetRNN framework is given by

and the final output is given by

The vanilla RNN in Figure5(a) can be substituted with GRU to establish the packetGRU framework. Similar to the vanilla RNN, the GRU contains only one memory component but is enhanced with two gates: the update gate () and the reset gate (). The operations of GRU within the packetGRU framework are defined by the following equations:

and the final output of packetGRU is given by the equation14.

SECTION: Appendix PComplexity Analysis

The time complexity of packetLSTM is, whereis the number of instances,is the time complexity to process all the activated LSTMs at timecorresponding tofeatures, andbroadly denotes the constant time required to perform other fixed operations like aggregations and final prediction. We utilize the torch.matmul() function for matrix multiplication of LSTMs. The time required for each LSTM is dependent on its hidden size () and performs 3 matrix multiplication of sizeandrequiring a time complexity of. However, we vectorize the operation ofLSTMs by single matrix multiplication ofand. Note that the time required by torch.matmul() does not scale linearly with; rather, it just takes a small overhead depending on the type of hardware and other dependencies. Here, we denote this overhead as a function ofaswhere. This is further corroborated by the time required by packetLSTMs on each synthetic dataset with differentvalues (see Table8). For example, The time required to process the whole HIGGS dataset takes 4396.83 and 4500.17 seconds for0.25 and 0.5, respectively. Even though the value ofdoubles from0.25 to 0.5, the time required doesn’t increase by the same factor. Therefore, the time required by theLSTMs at timewould be. Finally, since it is an online learning task and each instance is processed sequentially, the total time complexity ofinstances would be. It is difficult to find the exact form of the function. However, based on the time required by packetLSTM on each dataset with increasing, it can be safely assumed that the value ofis closer to 1 than. Therefore, the packetLSTM model demonstrates scalability in terms of time complexity corresponding to the number of features.

The space complexity of packetLSTM is directly dependent on the space complexity of an LSTM. Let us denote the space complexity of an LSTM by. At each time, we have a universal feature space ofcardinality, therefore, correspondingLSTMs are present in the packetLSTMs. The space complexity of the final prediction network and aggregation function are fixed and denoted by. Therefore, the total space complexity of packetLSTM at timecan be given by. Note that the space complexity of packetLSTM increases with the arrival of sudden and missing features. Therefore, the limitation of packetLSTM is that it is not scalable in terms of space complexity corresponding to the feature size. However, it is noteworthy that packetLSTM effectively manages up to 7500 features, as demonstrated with the imdb dataset. Additionally, we present a strategy to further mitigate space complexity limitation in sectionQof the Appendix.

Here, we provide the number of learnable parameters in the packetLSTM framework with Time-LSTM 3 as the time modeling unit. The mathematical formulation is given by equations 2-8. The input gate (), time gate 1 (), time gate 2 (), cell state (), long-term memory (), and output gate () requires,,,,, andlearnable parameters, respectively. Therefore, the total number of learnable parameters in an LSTM unit is. The fully connected layer accounts forparameters. Therefore, the total number of learnable parameters at each instance is. For all the experiments, the value ofis 64. In this article, the maximum number of parameters for each dataset would correspond to the worst-case scenario where all the features are present, resulting in a maximum of183K,131M,2M,148K, and375K learnable parameters for magic04, imdb, a8a, SUSY, and HIGGS, respectively.

SECTION: Appendix QDropping Features to Resolve Space Complexity

The space complexity increases with the number
of features, as discussed above. However, packetLSTM easily handles even the 7500 features in the imdb dataset. The total number of learnable parameters of packetLSTM for the imdb dataset is131M. Therefore, packetLSTM with 1B parameters and a hidden size of 64 can handle around57K features. Hence, we argue that packetLSTM can deal with high-dimensional data. However, we also propose a solution to curb the space complexity by defining a maximum limit (say) on the number of LSTMs. When the number of features in the universal feature space, we dropfeatures. Here, dropping the feature means that the corresponding LSTM is reinitialized and assigned to some new features that arrived at time. The dropped feature can come in future instances (). However, this feature will then be considered as a sudden feature. We employ KL-Divergence to determine the dropped features. After processing
instance, packetLSTM has the short-term memory of each LSTM and the common long-term memory. The KL divergence between each short-term
and common long-term memory is determined. The feature corresponding to the short-term memory, which has the lowest KL-Divergence, is dropped.
This is because the common long-term memory already holds the dropped feature’s short-term memory information and is used for final prediction. We
also put a limit on the number of times a feature is seen () by packetLSTM before it can be considered for dropping. We experimented with the imdb dataset since it has the highest number of features (7500). The best hyperparameters found for packetLSTM on the imdb dataset in Table7are used here. Theandare set to 100 and achieved a balanced accuracy of 78.92, which still performs better than 7 out of 10 baseline models (see Table1in main manuscript). The 78.92 balanced accuracy is lower than the packetLSTM without any limits (85.06). So, there is a tradeoff between performance and the space complexity.

SECTION: Appendix RSingle LSTM

The three employed imputation techniques are:

Forward fill: The missing value of a feature is imputed with its last observed value.

Mean: The missing value of a feature is imputed with its forward mean determined in a rolling manner. That is, the lastobserved values of a feature are considered to calculate its mean. Here.

Gaussian Copula:(Zhao et al.,2022)proposed using Gaussian copula for online streaming data with a known, whereis the total number of features. This method argues that data points are generated from a latent Gaussian vector, which is then transformed to match the marginal distributions of observed features. However, Gaussian Copula requires storing a matrix ofinstances to perform imputation. The method fails if the matrix’s determinant is 0 or if a feature’s values within the matrix are identical. The magic04 and SUSY require= 5, and HIGGS needs= 30. The method does not work for a8a and imdb for even= 300, excluding its application to these datasets.

The hyperparameter search of a single LSTM is performed similarly to packetLSTM. The hyperparameters of the single LSTM model are hidden size, number of layers, and learning rate. We searched for the best hidden size among 32, 64, 128, and 256. We determined the optimal number of layers between 1, 2, 3, and 4. Similar to packetLSTM, we tested a range of learning rates (0.001, 0.0005, 0.0001, 0.00005). We further searched in the vicinity of the optimal learning rate found in the previous step. The optimal hidden size and number of layers for each dataset are found to be 32 and 1, respectively. The best learning rate is 0.001, 0.0006, 0.0001, 0.0002, and 0.0008 for magic04, a8a, SUSY, HIGGS, and imdb, respectively. Similar to packetLSTM, we employed Z-score streaming normalization.

SECTION: Appendix SChallenging Scenarios on HIGGS and SUSY

SECTION: S.1HIGGS

Here, we provide the exact value of the balanced accuracy used in Figure3and4from section7of the main manuscript. Table11provides the results associated with the experiments on sudden, obsolete, and reappearing features on the HIGGS dataset. Additionally, Table12details the comparative results of packetLSTM versus packetLSTM retrained across five data intervals.

SECTION: S.2SUSY

We perform three challenging experiments – sudden features, obsolete features, and reappearing features – on the SUSY dataset, replicating the experiments conducted on the HIGGS dataset. The data creation settings for the SUSY dataset are identical to those employed for the HIGGS dataset. We considered the OLVF, OLIFL, OVFM, and Aux-Drop baselines for comparative analysis. The performance of each method is illustrated in Figures6and7, with precise values detailed in Tables13and14.

Similar to the approach used for the HIGGS dataset, 20% of the features are sequentially added at each interval in the SUSY dataset, with values rounded to the nearest integer. Given that SUSY comprises 8 features, the initial data interval includes features 1 and 2, the subsequent interval contains features 1 through 3, and this incremental addition continues as depicted in the leftmost graph of Figure6. All models exhibit a pattern of performance improvement as sudden features are introduced at each interval, as evident in Figure6. Notably, the packetLSTM consistently outperforms all baselines in each data interval, followed by Aux-Drop, as detailed in Table13. This underscores the efficacy of packetLSTM in handling sudden features.

The data arrival pattern in this experiment is the inverse of that observed in the sudden feature experiment, as illustrated in the middle graph of Figure6. Generally, the performance of all models declines as features become obsolete in each subsequent interval. Interestingly, an increase in performance from the fourth to the fifth interval is observed in models such as OVFM, Aux-Drop, and packetLSTM. A plausible explanation for this phenomenon may relate to the nature of the SUSY data, where the exclusion of feature 6 notably impacts performance more severely in the fourth interval. The packetLSTM consistently outperforms other models throughout each data interval, followed by Aux-Drop, demonstrating its robust capability in handling obsolete features effectively.

The performance of packetLSTM, Aux-Drop, and OLVF improves when previously encountered sets of features reappear, as depicted in Figure7. Furthermore, packetLSTM exhibits performance gains of 0.09, 0.05, and 0.08 in the third, fourth, and fifth intervals, respectively, compared to its retrained counterpart, as shown in the rightmost graph of Figure7and Table14. This enhancement reaffirms the ‘learning without forgetting’ capability of packetLSTM. Notably, packetLSTM is the sole method that consistently demonstrates the ’learning without forgetting’ capability across both the HIGGS and SUSY datasets. Moreover, packetLSTM consistently gives the best result in four out of five data intervals, followed by Aux-Drop, as detailed in Table13, underscoring its superior performance.

SECTION: Appendix TCatastrophic Forgetting in Online Learning versus Online Continual Learning

In this article, we present the ‘learning without forgetting’ capability of packetLSTM. The ‘learning without forgetting’ capability is also referred to as mitigating catastrophic forgetting in the existing literature(Hoi et al.,2021). We explore this capability in an online learning setting, where the model receives and processes instances sequentially without any buffer storage.

The problem of catastrophic forgetting is extensively studied in a parallel and widely recognized field of online continual learning(Li & Hoiem,2017; Wang et al.,2024). Online continual learning is the field of machine learning where a model continually learns new tasks. In this scenario, tasks are delivered sequentially, yet all the instances for each task are presented at once. Consequently, the learning for each task is conducted in an offline setting, utilizing all data related to a task before transitioning to the subsequent one. For example, a model may initially learn from data related to task 1 in an offline batch mode. Subsequently, it proceeds to learn from data pertaining to task 2 in a similar batch mode, and this pattern continues. The introduction of new tasks may result in the addition of new classes, a shift in data distribution, or the arrival of an entirely new task domain. In the context of online continual learning, catastrophic forgetting refers to the challenge a model faces in retaining its ability to perform previously learned tasks.

Although online continual learning shares similarities with online learning regarding sequential task learning, it differs significantly in its execution within individual tasks. In online continual learning, the model adheres to a batch training paradigm, which diverges from the principles of online learning algorithms. Consequently, the phenomenon of catastrophic forgetting, as observed in online learning, distinctly differs from that in online continual learning. It is worth noting that the packetLSTM framework, with certain modifications, may potentially be adapted for use in online continual learning to address catastrophic forgetting. However, this application extends beyond the scope of our article.

SECTION: Appendix UTransformer

SECTION: U.1Padding

The two input padding methods are:

Only Values: For each dataset, zeros are added to the sequence following the available feature values until, where, whereis the total number of features. Note that this contradicts the sixth characteristic of haphazard inputs. However, to compare packetLSTM with Transformer, we assume thatis known. The specificfor each dataset is defined in the column labeled ‘#Features’ in Table5of the Appendix. An example of an input instance at timewhere feature 1 andis available is [,, 0, …, 0].

Pairs: Each available feature value is paired with its corresponding feature ID, and the sequence of these pairs is padded with zeros till. The feature ID ranges from 1 to, and. An Example is [, 1,,, 0, …, 0]

Padded inputs are initially processed through a linear embedding layer, which outputs embedding of dimension. The input dimension for this layer isandforOnly ValuesandPairs, respectively. The embedding is then passed to an encoder. The encoder consists ofencoder layers. Each encoder layer comprisesheads and maintains dimensions of sizefor both input and output. The encoder’s output feeds into the same fully connected neural network utilized in packetLSTM, consisting of a linear layer of dimensions (,), a ReLU activation, and another linear layer leading to the output classes, culminating in a softmax layer for predictions.

The hyperparameters search is performed sequentially, as in the packetLSTM. We searched the optimal value ofamong 32, 64, 128, 256, and 512,among 1, 2, 4, 8, and 16,among 1, 2, 3, and 4. Similar to packetLSTM, we tested a range of learning rates (0.001, 0.0005, 0.0001, 0.00005). We further searched in the vicinity of the optimal learning rate found in the previous step. Optimal hyperparameters include aof 128 for magic04 and 32 for other datasets,of 4 for SUSY and imdb, 8 for magic04, and 16 for a8a and HIGGS. Theis 1 for all the datasets, with the learning rate of 0.0002 for magic04 and a8a and 0.0001 for SUSY, HIGGS, and imdb. The Z-score is employed for streaming normalization.

SECTION: U.2Natural Language

We experimented with two models, DistilBERT(Sanh et al.,2019)and BERT(Devlin et al.,2019), to process haphazard inputs. The inputs to the model are created in two ways:

Values: Sequences of available feature values, formatted as a string (example “”).

Input Pairs: Sequences where each feature value is paired with its feature ID (example “[] []”).

We utilize the default hidden size of both DistilBERT and BERT models, and learning rates are determined by hyperparameter search among 0.001, 0.0005, 0.0001, and 0.00005. The balanced accuracy on the imdb and synthetic datasets (with= 0.5) is around 50 for both models (see Table15), with the highest being 50.89 for DistilBERT usingValueson the magic04 dataset. Given that the models did not learn to classify haphazard inputs and considering the extensive computation time required for the BERT models, we did not conduct further experiments.

SECTION: U.3Set Transformer

We utilize the encoder and decoder provided in the Set Transformer article(Lee et al.,2019), which is permutation invariant and handles variable length inputs. The hyperparameters are hidden size, number of heads, number of induction (inducing points), and learning rate. Please refer to Set Transformer(Lee et al.,2019)for more details about the architecture.

We conduct a hyperparameter search similar to packetLSTM. We searched the optimal value of hidden size and the number of induction points among 32, 64, 128, 256, and 512, and the number of heads among 1, 2, 4, and 8. Similar to packetLSTM, we tested a range of learning rates (0.001, 0.0005, 0.0001, 0.00005). We further searched in the vicinity of the optimal learning rate found in the previous step. The optimal hidden sizes are 64 for magic04 and 32 for the rest of the dataset. The best value of the number of induction points is 64 for magic04 and HIGGS, 32 for imdb and a8a, and 256 for SUSY. The optimal number of layers is 1 for magic04 and HIGGS, 2 for imdb and a8a, and 8 for SUSY. The best learning rate is 0.0001, 0.00007, 0.0002, 0.00008, and 0.00006 for magic04, imdb, a8a, SUSY, and HIGGS, respectively.

SECTION: U.4HapTransformer

We create embeddings of dimension () for each feature, initialized with He initialization(He et al.,2015), and designed to be learnable to capture
feature representations effectively. At each time instance, only embeddings for available features are utilized and are collectively denoted as. Given that the shape ofwould be (1,,), it varies in length due to the changing number of features at time(). Therefore, the decoder of the Set Transformer(Lee et al.,2019), which handles variable length inputs, is utilized to process. The decoder accepts an input of sizefor each feature and includesnumber of heads. The output from the decoder is then passed through a softmax layer to generate predictions.

We conduct a hyperparameter search for,, and learning rate following the methodology used for packetLSTM. We determined the optimalamong 32, 64, 128, 256, and 512, andamong 1, 2, 4, and 8. The best learning rate was determined among 0.001, 0.0005, 0.0001, and 0.00005. We further searched in the vicinity of the optimal learning rate found in the previous step. Optimal values identified include aof 64, 512, 256, 32, and 64,of 2, 1, 2, 2, and 4 for magic04, imdb, a8a, SUSY, and HIGGS, respectively. The learning rate is found to be 0.0005, 0.00007, 0.00009, 0.00009, and 0.00008 for magic04, imdb, a8a, SUSY, and HIGGS, respectively.