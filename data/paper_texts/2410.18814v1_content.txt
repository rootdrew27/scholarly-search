SECTION: Hybrid Quantum-Classical Feature Extraction approach for Image Classification using Autoencoders and Quantum SVMs
In order to leverage quantum computers for machine learning tasks such as image classification, careful consideration is required: NISQ-era quantum computers have limitations, which include noise, scalability, read-in and read-out times, and gate operation times. Therefore, strategies should be devised to mitigate the impact that complex datasets, whether large-scale images or high-dimensional data, can have on the overall efficiency of a quantum machine learning pipeline. This may otherwise lead to excessive resource demands or increased noise. We apply a classical feature extraction method using a ResNet10-inspired convolutional autoencoder to both reduce the dimensionality of the dataset and extract abstract and meaningful features before feeding them into a quantum machine learning block. The quantum block of choice is a quantum-enhanced support vector machine (QSVM), as support vector machines typically do not require large sample sizes to identify patterns in data and have short-depth quantum circuits, which limits the impact of noise. The autoencoder is trained to extract meaningful features through image reconstruction, aiming to minimize the mean squared error across a training set of images. Three image datasets are used to illustrate the pipeline: HTRU-1, MNIST, and CIFAR-10. We also include a quantum-enhanced one-class support vector machine (QOCSVM) for the highly unbalanced HTRU-1 set, as well as classical machine learning results to serve as a benchmark. Finally, the HTRU-2 dataset is also included to serve as a benchmark for a dataset with well-correlated features. The autoencoder achieved near-perfect reconstruction and high classification accuracy for MNIST, while CIFAR-10 showed poorer performance due to image complexity, and HTRU-1 struggled because of dataset imbalance. This highlights the need for a balance between dimensionality reduction through classical feature extraction and prediction performance using quantum methods.

SECTION: Introduction
Quantum computing has led to the emergence of the field of quantum machine learning, which blends together the concepts of classical machine learning and the principles of quantum mechanics. Quantum computing, and by extension quantum machine learning (QML), has challenges to overcome before its promised advantages or utility can be leveraged. Two of the most critical challenges facing quantum computing in the current NISQ-era are noiseand runtime. Quantum systems are susceptible to noise, which is accentuated at higher dimensions. Higher-dimensional circuits with many gates also tend to take much longer to run as compared to their classical counterparts. The need for scalable fault-tolerant quantum computersand quantum error correctionis only going to become more pressing, but in the meantime ways of reducing the dimensionof large datasets are crucial if they are to be encodedonto a quantum computer for machine learning purposes. This is especially true for image processing tasks, as encoding the entire image pixel-wise onto a quantum computer is not practical for large images.

A field where this is pertinent is astronomy. Observational datasets can be large and many of these datasets contain images. Astronomers already use machine learning for data preprocessing, analysis, predictions, and more which means there will always be a need for more accurate and faster techniques or approaches. Quantum computing still has far to go before it can be a rival, let alone be a replacement, for classical machine learning. Despite this fact, QML can be applied to astrophysical and astronomical data on real devices or simulated to gain an indication of how well they can be expected to perform if the hardware were to become practical and fault tolerant in the future.

The chosen use case, similar to our earlier comparative study, is that of pulsar data. A pulsars is a specific type of stellar remnant that forms after the death of a star at a sufficient solar mass, but not enough mass to become a black hole. Pulsars are characterized as highly dense objects where the total collapse into a black hole is avoided only because the object’s self-gravity cannot overcome neutron degeneracy pressure. Identifying pulsars is important and forms part of astronomical surveys, as well as investigations with respect to gravitational wave detection and pulsar astronomy.

This paper focuses on image classification of the three-channel HTRU-1image dataset through a quatum-enhanced support vector machine (QSVM). We include classical machine learning results and a quantum-enhanced one-class support vector machine QOCSVM for anomaly detection. This was done since the HTRU-1 set is highly imbalanced. The minority class (pulsars) only make upof the dataset. We also include this for the MNISTand CIFAR-10datasets and apply the chosen methods to the HTRU-2 datasetas well.

In order to encode images, specifically three-channel images from HTRU-1 and CIFAR-10, onto a quantum device, amplitude encodingis jointly implemented with a classical feature extraction and dimensionality reduction pipeline. Feature extraction methods include edge detection, histogram of oriented gradients (HOG), principal component analysis (PCA), convolutional neural networks (CNNs)etc. The chosen feature extraction scheme is a classical ResNet10-inspired convolutional autoencoder (CAE), trained for image reconstruction. The latent space is isolated and flattened after training for use in our machine learning tasks. QSVMs and QOCSVMs typically require only a few samples to learn the pattern in the dataset, however, the trade-off is quadratic scaling. In order to fit a SVM to a dataset, each sample in the training set has to be compared to every other sample. This is relevant both during training and testing.

The rest of the paper is structured as follows: Section 2 will briefly explain anomaly detection and the chosen QML methods. It will also explain the classical feature extraction pipeline. Section 3 will discuss the methods in more detail. Data preprocessing steps, choice of hyperparameters etc. can be found here. Section 4 is the results section and includes both the anomaly detection and supervised classification results. Section 5 is the conclusion. Two appendices are included: Appendix A gives more information on the metrics used for performance evaluation and Appendix B outlines the model architectures.

SECTION: Theory
SECTION: Classification and anomaly detection
Classification is a machine learning task where the goal is to accurately predict the class labels of two or more classes in a dataset. Anomaly detectioncomes in different forms, such as time-series anomaly detection, outlier detection, and minority sample prediction. Some approaches to anomaly detection include clustering-based methods, density estimation methods, statistical methods, isolation forestsetc. In our case, supervised anomaly detection, it is similar to classification in the sense that the goal is to accurately predict or flag a sample in a test set as an anomaly (belonging to the minority class). There are key differences between standard classification and anomaly detection, including the imbalance in the dataset and how the model is trained. The class imbalance is heavily skewed towards the samples that are considered normal. Training the model involves only using normal data. Anomalies are only introduced during testing. The model then identifies anomalous samples based on deviations beyond a certain threshold from normal behavior. Since we have ground truth in the form of labeled data, the anomaly detection we implement can be classified as supervised anomaly detection. From now on we may call samples belonging to the minority class anomalies.

Consider the following equation describing supervised classification and anomaly detection:

Here,andrepresent the input and output domains, respectively.denotes an-dimensional feature vector encompassingfeatures. The labelbelongs to the set, wheredesignates normal samples anddenotes anomalies. The functionsignifies the optimized or trained model, withrepresenting any parameters that need to be optimized during training.

To evaluate the performance of a supervised anomaly detection or classification approach, we use a test set, which must include both normal and anomalous samples. The effectiveness of the anomaly detection model can be assessed using many metrics, where we choose to use the following ten metrics: accuracy, proportion of postive predictions, precision, recall, negative prediction value, specificty, false positive rate, false negative rate, F1 score and Matthew’s Correlation Coefficient. We include all ten for a holistic comparison, but the most important metrics focus on the positive class, which in this use case are the anomalies (pulsars). Precision and recall do just this: precision measures the proportion of correctly predicted anomalies from all samples predicted as anomalies and recall measures the proportion of correctly predicted anomalies from all anomalous samples in the test set. Proportion of positive predictions (PPP) is just the amount of anomalies predicted, correct or otherwise. This value has to be aroundfor the HTRU-1 set, but will change depending on the dataset. The other metrics are discussed in Appendix.

SECTION: Support Vector Machines
Standard support vector machines (SVMs)are best described as maximal margin classifiers that work by mapping input features to a higher-dimensional feature space through a feature map. In this space, a separating hyperplane is determined using important samples, known as support vectors, which help define the orientation and location of the hyperplane.

The decision boundary in the feature space can be expressed as:

whereis the weight vector,is the feature map, andis the offset (bias). The hyperplane is defined by the set of pointswhere this equation holds.

To compute the similarity between data points without explicitly mapping them to the higher-dimensional space, SVMs utilize kernel functions. This method, known as the kernel trick, allows the SVM to efficiently operate in the higher-dimensional space. The choice of kernel influences the position and orientation of the hyperplane, enabling the SVM to separate data even when the relationship between features and class labels is complex and non-linear.

One-class support vector machines (OCSVMs)extend the principles of standard SVMs by training exclusively on normal samples. During testing, anomalous samples (pulsars) are introduced, and anomalies are flagged based on their position in feature space relative to a separating boundary.

In feature space, all normal data typically fall within this boundary. Samples outside of it are considered anomalous. The OCSVM utilizes a decision function to assess the distance from each point to the separating hyperplane, with any value greater thanindicating a normal sample and any value less thanindicating an anomaly. This distance is called the anomaly score.

The OCSVM is trained solely on normal samples, represented by the training set, to learn the boundary that separates normal behavior from anomalies. The decision functionmaps the input spaceto a signed distance in, defining a hyperplane in feature space. During testing, this function is given by

with a sampleclassified as normal ifand as an anomaly if.

In contrast, standard SVMs are trained on labeled data from two classes, aiming to find the optimal hyperplane that separates these classes with maximum margin. The decision function in standard SVMs closely resembles that of OCSVMs, focusing on which side of the hyperplane a sample falls on, rather than whether it lies inside or outside of it. This decision function is referred to as the decision score. Unlike OCSVM, standard SVMs utilize both positive and negative samples during training.

Quantum kernels refer to kernel functions estimated or determined on a quantum computerwhere the feature space corresponds to the quantum mechanical Hilbert space after feature encoding. To implement a quantum-enhanced SVM and OCSVM, we utilize a quantum kernel. We try to keep the quantum circuit as simple as possible, to limit depth-related noise. This is achieved by minimizing gate-based noise that would be introduced if the circuit were run on real devices, thereby also limiting computational costs. We extend a known methodby amplitude encoding one feature vector and then applying a second encoding block, which is simply the complex conjugate transpose of the first encoding block applied to a second feature vector. We express amplitude encoding as:

where the features from the feature vectors are embedded into the amplitudes of adimensional quantum state. Amplitude encoding is only possible if

which implies that all feature vectors should be normalized before amplitude encoding. This ensures that the quantum state represents a valid physical state according to the requirements of quantum mechanics.

Next, we measure with respect to a projector matrix. The projector matrix approach allows us to compute the quantum kernel, or the inner product between quantum states, directly. This eliminates the need for a SWAP test, which is a common but resource-intensive method for estimating state overlap that utilizes ancilla qubits. By measuring with respect to the projector matrix, we simplify the computation and efficiently evaluate kernels or similarities between feature vectors encoded in the quantum states.

The kernel function can be expressed as:

where both encoding operators have been applied and the measurement is performed with respect to the projector matrix

which is also the zero state density matrix.

SECTION: Classical Autoencoder
Classical autoencoders (CAEs)are similar to standard neural networks or convolutional neural networks (CNNs). In these networks, nodes are interconnected via weights, which determine the strength of the connections between neurons. These weights are learned through an iterative optimization procedure, typically using a gradient descent algorithm via forward and back propagation. The learning process aims to minimize a loss function that measures the difference between the input and the reconstructed output, ensuring that the network accurately captures the essential features of the input data.

The primary goal of autoencoders is to abstract the input data into a set of feature maps: compact representations of the input that capture its most relevant characteristics and spatial information. This abstraction process occurs during the encoder phase of the autoencoder, where the input data is progressively transformed into a lower-dimensional representation known as the latent space. The latent space serves as a compressed version of the input data. It contains the most critical information needed to reconstruct the original input.

Once the data is encoded into the latent space, the abstract feature maps are passed through the decoder, the remaining component of the autoencoder network. The decoder’s goal is to take the abstract representations in the latent space and decode them back into the original input format. The effectiveness of an autoencoder is measured by how well the decoded output matches the initial input data, with the best autoencoders producing outputs nearly indistinguishable from the original inputs. The idea can be visualized as seen in Figure.

CAEs operate through convolution operations, which are the core building blocks of convolutional neural networks. These operations involve matrix multiplication, where a learnable filter or kernel scans over the input image. The filter is applied to small sections of the image at a time, capturing local patterns such as edges, textures, and other features. The result of these operations is a set of feature maps or matrices that represent the presence of specific features in different regions of the image. These feature maps are then passed through successive layers, leading to increasingly abstract and complex representations of the original image features.

Beyond image reconstruction, autoencoders are also valuable for feature extraction, where the encoder is used to learn compact representations of input data. These representations can be used as input features for other machine learning models, enabling more efficient and accurate data analysis. This supports why we decided to utilize autoencoders for feature extraction. Convolutional Autoencoders (CAEs) are typically designed with convolutional layers, which capture spatial hierarchies in the data, and pooling layers, which reduce the dimensionality of the feature maps.

SECTION: Methodology
SECTION: Datasets and feature extraction
As mentioned in the introduction, we will be considering three datasets as part of the feature extraction and anomaly detection pipeline. These are the HTRU-1 pulsar, MNIST, and CIFAR-10 image datasets. Additionally, the tabular HTRU-2 dataset is included for benchmarking. All datasets are available in the data availability statement.

The HTRU-1 image dataset contains 60,000 samples, each of which is a three-channel image of 32x32 pixels. The three channels are similar to RGB values, but instead represent other quantities for each sample: Channel 1: Period Correction - Dispersion Measure Surface; Channel 2: Phase - Sub-band Surface; Channel 3: Phase - Sub-integration Surface.

The dataset is divided into a training set of 40,000 samples and test and validation sets of 10,000 samples each, from which subsets will be sampled for training and testing purposes in the quantum blocks. Onlyof this dataset represents anomalies (pulsars). The CAE for image reconstruction is trained using the entire training set. Data preprocessing includes min-max-scaling and data augmentation. The data augmentation implemented are random horizontal and vertical flips and random rotations up to a maximum of 15 degrees.

The HTRU-2 dataset consists of 17,898 samples of eight features each. Approximatelyof the dataset consists of anomalies (pulsars). The results from this dataset will serve as a benchmark to compare the results on the image dataset above, since it is assumed that the features from HTRU-2 are all meaningful. We do not have to apply the feature extraction scheme here, as the samples are not images.

MNIST is a gray scale image dataset of 28x28 images of the numerical digits 0-9. We isolate the first two digits, 0 and 1, creating a separate dataset for binary classification to serve as a benchmark for both the image reconstruction scheme and comparison with quantum methods.

CIFAR-10 is similar to HTRU-1, being three-channel 32x32 pixel images. There are 50,000 samples in the training set and 10,000 in the test set, which contains 10 classes of everyday objects. Similar to MNIST, we isolate the first two classes (airplane and automobile) for binary classification, serving as a second benchmark for image reconstruction.

The chosen CAE architecture, designed for feature extraction and trained via image reconstruction, should balance model complexity and generalization. More complex and deeper models are often capable of reconstructing images with higher fidelity, but they could also be more prone to overfitting. This is when the model learns noise or irrelevant details and essentially memorises the training set. Vanishing gradients, where the learning process stagnates and fails to make meaningful progress, is also a consideration. It is important to note that sometimes the model has to be adapted depending on the input that it will receive.

Included in these considerations are the points that were mentioned previously: we will be isolating the latent space for input into a quantum block. Since quantum computing has slow data read-in times and SVMs in general scale quadratically, it is imperative to keep the latent space and total sample amount at a sufficiently low, yet effective number to complete the task. In Figurethe idea of isolating the latent space and replacing the decoder with the quantum block is illustrated. This approach is similar to using a hybrid convolutional neural network and variational quantum circuit in conjunction for similar purposes.

All autoencoders were designed in such a way that the latent space, when flattened, contained precisely 64 abstract features for efficient encoding using amplitude embedding. 64 features require a 6-qubit system in order for amplitude embedding to accommodate all features. More features in the latent space would imply both an improved image reconstruction and feature extraction performance, which would ultimately lead to an improved performance in the quantum block results. We chose 64 features, as stated before, specifically to find a balance between computational complexity and performance. This is due to the fact that quantum machine learning circuits, especially for quadratically scaling circuits for QSVMs (and QOCSVMs), can have large runtimes. Our goal was to assess whether a low-dimensional latent space with a small number of features would suffice for a robust model.

SECTION: Specifics
We initially started with a really simple CAE architecture of three convolutional layers in the encoder and decoder each. In the encoder, the images would progressively be scaled down in dimensionality to a latent space of precisely 64 features. The decoder would then be the inverse of the encoder, trained with its own set of parameters to decode the latent space back into the initial image. This architecture was too simple for the HTRU-1 dataset, which led to the exploration of pre-trained ResNet18 modelsfor the encoder part and custom decoders designed to fit into the latent space of ResNet. ResNet18, although extensive, was overly complex. We settled for the middle ground of a ResNet10-inspired architecture, which in essence means that the architecture of ResNet10 was taken as the base skeleton architecture for the encoder, but changes were made to it depending on the need. Changes would include kernel sizes, stride, number of input/output channels per layer, among other changes.

The final architecture layer layout chosen is illustrated in Tablein Appendix. The loss function chosen was mean squared error loss. We considered using the structural similarity index measure (SSIM), but the HTRU-1 images are too abstract for a more visual measure. The optimizer for forward and back propagation during the image reconstruction training was set to the Adam optimizer at a learning rate of. We also implemented built in L2 regularization using a weight decay of. The training set was split further into a ratio of 80:20 for a new training and validation set, for validation purposes. The convergence of the validation loss would serve as an indication of overfitting. Finally, training was done in batches of 256 samples per epoch and we set the number of epochs at, as this was the point of loss convergence for HTRU-1. After the image reconstruction models were trained, we extracted features from the entire dataset by sending it through the encoder. At this point sub-sampling was introduced for a smaller, but representative set through stratification to the original label distribution, for feeding into the quantum block. There was separation made for anomaly detection and classification training sets here, since anomaly detection may only be trained on normal samples.

We used scikit-learn as the Python library for fitting the quantum kernel SVMs to the training data before applying it to a test set. In all SVM runs, this includes both classical and quantum results. We introduced class weight parameters offor HTRU-1 runs, to weigh the anomalous samples more during training. Recall that a label of 1 represents normal samples and -1 anomalous samples. The class weights are set specifically to reflect the 2:98 ratio of anomalous samples to normal samples in the HTRU-1 set and are calculated by the inverse frequency of class appearances. This was changed tofor HTRU-2 runs and was removed entirely for MNIST and CIFAR-10 runs due to their balanced class distributions. The class ratios are 9:91 for HTRU-2 and practically 50:50 for MNIST and CIFAR-10. We did similarly for the OCSVM runs by introducing a ’nu’ parameter of 0.2 for HTRU-1 and 0.9 for HTRU-2. We did not apply OCSVMs to MNIST or CIFAR-10, since these sets are essentially balanced. The ’nu’ parameter controls the proportion of data considered outliers and the minimum fraction of support vectors, balancing the model’s strictness in detecting anomalies. The choice of kernel for the classical runs was the standard radial basis function.

Finally we chose a sub-sample of 500 training and testing samples for training and testing the quantum blocks. This is different from the training and test sets during the image reconstruction. The shape of the input into the SVMs is therefore (500 samples, 64 features) which lead to a 6-qubit quantum state when amplitude encoded for quantum runs. We chose 500, since it seemed to be the trade-off point between computational demand and number of samples for pattern learning.

A natural question arises when comparing the performance of the same model architecture applied to different image datasets. MNIST consists of grayscalepixel images, whereas HTRU-1 and CIFAR-10 consist of three-channelpixel images. This leads to the concern that a similar model architecture, when applied to more complex images with additional channels, is expected to perform worse. A natural way to compensate for this would be to complicate the model architecture by changing the layers or increasing the size of the latent space. However, as previously mentioned, overly complex models, such as ResNet18, did not work well for HTRU-1. This raises the question: what about increasing the features in the latent space?

To investigate this, we utilized a simplified model architecture and applied it separately to the three grayscale channels corresponding to the three channels of the original HTRU-1 images. Thus, we have 64 features in the latent space per channel. If the results are similar, this indicates that, regardless of where the features are extracted from, the dataset does not lend itself to effective reconstruction and eventual separation. Conversely, if performance improves, it suggests that a larger latent space would be beneficial, however, it would also increase the qubit requirements in the quantum block.

In essence, we have three image reconstruction models, each yielding a separate set of 64 features for each channel. These features were used in their own respective sets of SVMs and OCSVMs (both classical and quantum). Overall, this approach involves three image reconstruction models and three pairs of SVMs for classification and anomaly detection. Additionally, we considered introducing weighted voting in an approach that resembles ensemble learning. All hyperparameters were kept identical to those in the previous section.

SECTION: Results
SECTION: Main Results
Using the autoencoder to encode the images into a reduced latent space of 64 features per sample, which is then amplitude encoded into 6 qubits for use in a QSVM and QOCSVM, along with classical results, leads to the results as shown in Figures,and.

Before we get to the quantum machine learning results, we can look at the classical image reconstruction part. In Figurewe have plots that show the loss convergence of both the training and validation loss for the HTRU-1, MNIST, and CIFAR-10 datasets. In all cases the loss converges, with a convergent validation loss. This indicates limited overfitting in the image reconstruction process.

The images included in this figure are two examples of an initial image and its reconstructed image during testing, from each of the two classes in the datasets considered. Here it should visually be clear that the reconstruction for MNIST is nearly perfect, apart from slightly worse resolution. The HTRU-1 and CIFAR-10 images are reconstructed markedly worse. The final loss values, after convergence, are considerably low for both datasets, which indicates that either the model architecture is not complex enough for these datasets or that the datasets do not lend themselves well to being reproduced. From our testing with different architectures, it is clear that it is difficult to reproduce images as abstract as HTRU-1 images without a considerably deep autoencoder or a much larger dataset with more samples. Overall the model performs better for CIFAR-10 reconstruction and there is an indication of further improvement if we allowed for more epochs.

Figureshows plots for the anomaly score (or decision score for the MNIST and CIFAR-10 cases) for some of the methods performed. The green horizontal line indicates the decision function at zero, which is a representation of the hyperplane in feature space. Any sample above the line, or inside the separating boundary, is considered normal and any sample below the line, or outisde the boundary, is considered anomalous. The-axis is simply the sample index in the test set.

It is visually clear from the HTRU-2 plots that both the QSVM and QOCSVM are able to separately the data. There are some false predictions, but overall it seems to capture samples that are considered more anomalous than others. The HTRU-1 plot shown is an example of only the QSVM, which does not perform as well as for the HTRU-2 runs. Most anomalies are still flagged, but the proportion of mistakes is visually more pronounced. In the MNIST plot it is clear that the separation is really obvious, which connects well with the performance during reconstruction. CIFAR-10 performs worse, with a lot of sample overlap, but there is still a visual distinction. In this figure only the QSVM is given for MNIST and CIFAR-10, since it does not make sense to use a OCSVM for balanced datasets.

We can calculate the performance metrics of all the runs performed by taking the true positives, true negatives etc. from the confusion matrices or the anomaly score plots. These values are illustrated as a collection of histograms for comparison in Figure.

The MNIST case clearly shows that the hybrid model of classically trained encoder and quantum block delivers essentially perfect results. This indicates that the features extracted by the encoder part, trained through image reconstruction, captures the necessary information for the machine learning task of classification through a SVM. CIFAR-10, just as the image reconstruction performed worse, the final classification performs worse as well. Here the expectation is precision and recall around.

Applying the same for the HTRU-1 set is not as successful. Here the accuracy, NPV and specificity values are inflated, because of the imbalanced nature of the dataset. It is easier to predict negative samples correctly, even when randomly guessing. The two metrics to look at more carefully, as stated before, are the precision and recall metrics. The precision bars show that both the quantum and classical OCSVMs perform much worse as compared to the standard SVMs. Interestingly the quantum runs (QOCSVM) seems to perform slightly better than the classsical radial basis function SVMs. When we switch to a standard SVM, the classical SVM clearly performs better, with some variability in the metric indicated by error bars.

The recall bars indicate a more even performance between the four methods. The QOCSVM seems to perform the best out of the three methods, which is promising for its application as a quantum method for anomaly detection of imbalanced image datasets sent through encoders for feature extraction, but only if the image reconstruction is successful. Despite the improved performance over the other methods, the metric is still less than 0.6, which does not bode well for the pipeline as useful when using recall as a metric, specifically in this case for the HTRU-1 dataset.

The graph for HTRU-2, which serves as a benchmark of a simple dataset with only eight correlated features, shows what can typically be expected when applying only the quantum block. This is because the dataset in this case does not consist of images, implying that only the quantum block has to be applied here. The same pattern is observed in the precision, the main difference is that the metric values are simply higher. A promising observation is the recall performance here, which are all close to or above 0.8.

Another observation is that the false negative rate (FNR) for the HTRU-1 dataset is significantly higher compared to the other datasets. This aligns with the model’s performance in predicting positive cases, particularly when evaluating precision and recall. Additionally, the false positive rate (FPR) is considerably lower, suggesting that the abundance of normal samples facilitates the model’s ability to accurately identify these instances. This finding is further supported by the negative predictive value (NPV) and specificity metrics presented in all three graphs. The F1 score and Matthews correlation coefficient (MCC), which aim to balance these case-specific metrics, are also included in the analysis.

We include PPP only to guage the weighting applied during the SVMs and the ’nu’ parameter during the OCSVMs. We told the models to only output anomalies at a rate corresponding to the original distribution so we would expect a value of aroundfor MNIST and CIFAR-10,for HTRU-2 andfor HTRU-1.

SECTION: Channel-wise approach
This section presents the results from treating each of the three channels in the HTRU-1 dataset as separate grayscale images, employing three identical models in a channel-wise approach. Based off the normalized inverse losses in Figurethe image reconstruction quality is best for channel 1. We also see that the simplified model architecture takes a lot more epochs to converge than our ResNet10 architecture. It also includes some examples of images that were reconstructed by the three separate models. These examples are visually consistent with the final normalized inverse losses.

Since channels 2 and 3 performed worse, the expected consequence would be an improved classification and anomaly detection performance for the best performing image reconstruction model, but instead the better performance was observed for channels 2 and 3. An explanation for this could be the fact that channel 1 images typically contain a concentration of intensity that is easier to reconstruct than the occasional intense vertical lines in channels 2 and 3 for the anomalous samples. It might be easier for the SVMs to distinguish between samples that have no vertical lines and ones that have traces of vertical lines in sub-optimally reconstructed images, than in images that are reconstructed at a better quality, but with no obvious distinguishing features. The suggestion would be to implement a weighted voting scheme in an ensemble learning machine learning approach if the decision is to treat images as three separate gray scale images. How the weighting of this ensemble approach would be chosen is unclear. Here we considered using the inverse losses, however, as stated earlier, this would not make much sense, since the best performing reconstruction model did not yield the best performing anomaly detection model.

Comparing the channel-wise approach histograms in Figureto the main results histograms from before, the observations are more complicated. Here the recall and precision values will obviously depend on which channel is being used in the pipeline. Some channels will contain more information that is useful for the SVMs to leverage for classification. Here the indication is that precision in the QOCSVM is generally lower for channel 1 as compared to channel 2 and 3, whereas the QSVM performs better for channel 1. Channels 2 and 3 are seemingly better for anomaly detection and channel 1 is better for classification. In terms of recall, a different observation is made. The results are still channel dependent, but now the QSVM outperforms all other methods, including the CSVM for channel 1. The classical methods outperform the quantum methods for channel 2 and 3, however the QOCSVM wins against the COCSVM, while the CSVM wins out against the QSVM. In this case you would prefer to do anomaly detection with a quantum model and classification with a classical model. The other metrics tell a similar story as was observed in the main results section.

SECTION: Conclusion
We used a classical ResNet10-inspired convolutional autoencoder (CAE) trained for image reconstruction to extract 64 abstract features in the latent space of the encoder for input into simulations of a quantum machine learning block made up of either a QSVM or QOCSVM. We applied this to MNIST, CIFAR-10 and the HTRU-1 image datasets. We also applied the quantum blocks to the HTRU-2 dataset, which contains correlated features to serve as benchmark. This was all compared against classical counterparts. Also included is a channel-wise approach, while using a simpler architecture that outputs 64 features in the latent space per channel.

The feature extractor trained for image reconstruction is able to extract meaningful features for classification through our classical and quantum SVMs for MNIST. The performance of the hybrid pipeline does not perform optimally for the HTRU-1 image set, which can be attributed to a few reasons, namely the abstractness of the images, the information contained within the image itself, the lack sufficient model complexity for the HTRU-1 images etc. This was addressed in part by introducing a channel-wise approach, which essentially tripled the amount of features that were extracted from the input images. Despite the increase in the number of features extracted, which essentially meant an increase in qubit number, it did not dramatically improve the model. We observed channel-dependent results, where channels 2 and 3 tend to be more important for anomaly detection. The explanation for this was that channels 2 and 3 contained vertical lines that are easier to distinguish for a OCSVM, despite channel 1’s better image reconstruction quality. Overall the HTRU-1 set proved to be too abstract for an image reconstruction task and ultimately leads to less than optimal results during separation. The CIFAR-10 results fall directly in the middle between MNIST and HTRU-1. It peforms at an expected recall and precision of around 0.75, which indicates separation, despite visually poor image reconstruction. The image reconstruction could, however, be improved if more epochs were allowed. Finally, the HTRU-2 benchmark shows that anomaly detection is possible for a dataset consisting of well-correlated features and is viewed as another middle point between the perfect MNIST results and what the HTRU-1 images.

We believe that the classical image reconstruction pipeline, coupled with a quantum block can be implemented for image classification and anomaly detection, dependent on the image set in question. This is obvious from the holistically different results for MNIST, CIFAR-10, and HTRU-1.

This work can be extended by performing a broad architecture search to find the optimal autoencoder for image reconstruction and to try out other quantum blocks, such as variational circuits. Other feature extraction methods can also be looked at, where examples of some ideas we considered included FFT, edge detection, and noise removal. Running the pipeline through real hardware would be the main missing link, however, the expected results from this study would be far worse if they were to be run on real quantum devices. Quantum machine learning in general is severely limited by the current NISQ-era hardware, not only because of noise, but also because of runtime requirements.

SECTION: Funding
This work was funded by the South African Quantum Technology Initiative (SA QuTI) through the Department of Science and Innovation of South Africa.

SECTION: Data Availability Statement
The,,, andare available for download. MNIST and CIFAR-10 can also be imported from machine learning libraries such as TensorFlow and PyTorch.

SECTION: Performance Metrics
Accuracy measures the proportion of correctly classified samples (both normal and anomalous) out of the total number of samples, giving an overall indication of the model’s performance. Negative Prediction Value (NPV) assesses the proportion of correctly predicted normal cases from all samples predicted as normal, reflecting how well the model identifies normal cases. Specificity quantifies the proportion of correctly predicted normal cases out of all normal cases, indicating the model’s ability to correctly identify non-anomalies. The False Positive Rate (FPR) represents the proportion of normal cases incorrectly predicted as anomalies, highlighting the rate of false alarms. The False Negative Rate (FNR) shows the proportion of anomalies incorrectly predicted as normal cases, pointing out how often anomalies are missed. The F1 Score combines precision and recall into a single metric, providing a balanced measure of the model’s performance on positive cases. Matthew’s Correlation Coefficient (MCC) offers a comprehensive measure of binary classification quality by considering all four confusion matrix categories, particularly useful for imbalanced datasets.

In imbalanced datasets, accuracy, Negative Prediction Value (NPV), and specificity can be misleading. Accuracy may be high if the model predominantly predicts the majority class, failing to capture the minority class accurately. NPV can be inflated if the dataset has many more normal cases, making it seem like the model is performing well in predicting normal cases, even if it misses anomalies. Specificity might also appear high if the majority class is dominant, giving a false impression of model performance on the minority class. Metrics focusing on the minority class, like precision and recall, often provide a clearer picture of model effectiveness in such cases, which is why particular emphasis is placed on these metrics.

SECTION: Autoencoder Architecture
There are three convolutional autoencoder architectures to take note of, all of which result in 64 features in the latent space. The main architecture for MNIST, which takes as input gray scale 28x28 pixel images, the main architecture for HTRU-1 and CIFAR-10, which take as input three-channel 32x32 pixel images, and the three separate channel-wise models which take as input single-channel 32x32 pixel images as input. The two main architectures are given in Table, where it is important to note that the final output layer is cropped to either 28x28 or 32x32 depending on the dataset before matching in the mean squared error loss calculation.

The channel-wise approach autoencoder architecture is designed to compress and reconstruct grayscale images using a simplified architecture. The encoder consists of three convolutional layers, each with a kernel size of 3x3, strides of 2, and padding of 1, reducing the spatial dimensions and channel size from 16 to 8 to 4. ReLU activations were utilized. The decoder employs three transposed convolutional layers with the same kernel size, strides, and padding. An output padding of 1 to reverse the spatial reduction was used. The decoder works by progressively increasing the channel size from 4 to 8 to 16. The final layer restores the original input channel count, aiming to accurately reconstruct the input images.

SECTION: References