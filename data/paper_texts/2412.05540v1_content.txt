SECTION: Towards 3D Acceleration for low-power Mixture-of-Experts and Multi-Head Attention Spiking Transformers
Spiking Neural Networks(SNNs) provide a brain-inspired and event-driven mechanism that is believed to be critical to unlock energy-efficient deep learning.
The mixture-of-experts approach mirrors the parallel distributed processing of nervous systems, introducing conditional computation policies and expanding model capacity without scaling up the number of computational operation.
Additionally, spiking mixture-of-experts self-attention mechanisms enhance representation capacity, effectively capturing diverse patterns of entities and dependencies between visual or linguistic tokens.
However, there is currently a lack of hardware support for highly parallel distributed processing needed by spiking transformers, which embody a brain-inspired computation. This paper introduces the first 3D hardware architecture and design methodology for Mixture-of-Experts and Multi-Head Attention spiking transformers. By leveraging 3D integration with memory-on-logic and logic-on-logic stacking, we explore such brain-inspired accelerators with spatially stackable circuitry, demonstrating significant optimization of energy efficiency and latency compared to conventional 2D CMOS integration.

SECTION: 
Transformer models have significantly enhanced capabilities in language and vision tasks, gaining widespread adoption across diverse application domains.
Compared to Convolutional Neural Networks (CNNs), the self-attention mechanism in transformers captures contextual relationships globally among all tokens in a long sequence, unifying global and local sequence details into a unified representation.
Within transformer architectures, Mixture-of-Experts (MoE), inspired by conditional computing mechanisms in neuroscience, has been widely adopted. MoE enables individual experts to specialize in learning specific features or storing task-specific knowledge, achieving significant performance improvements. By leveraging a learnable gating function to efficiently route input tokens to appropriate experts, MoE decouples computational cost from model parameter size, achieving high scalability.

As the third generation of neural networks, spiking neural networks (SNNs) exhibit a closer resemblance to biological neurons than conventional non-spiking artificial neural network (ANN) counterparts. SNNs leverage powerful temporal coding, enable spatiotemporal computation through binary activations, and achieve ultra-low energy consumption on dedicated neuromorphic hardware platforms.
Recent advancements in spiking neural network-based transformer models have successfully integrated self-attention mechanisms from traditional transformers into spiking neuron architectures. These models demonstrate superior performance over conventional network architectures, mirroring the trend observed in ANNs where vision transformers outperform ResNets.

Currently, there is a lack of dedicated hardware architectures designed for spiking transformers, particularly for spiking MoE transformers, presenting unique design challenges.
First, existing neuromorphic accelerators typically provide limited parallelism, constrained either to temporal dimensionsor spatial dimensions. Moreover, these designs are predominantly tailored for accelerating spiking CNNs, making them ill-suited to address the computational demands and unique characteristics of large-scale spiking MoE transformer models.
Second, implementing brain-inspired algorithms on 2D silicon faces significant hardware overhead, including high memory access costs, complex routing requirements, and low computational density. These limitations prevent 2D designs from achieving the high computational efficiency of three-dimensional brain-like computing architectures because they defy the need for low power consumption in SNN.
Third, a naive 3D expert-by-expert implementation doesn’t fully exploit the parallelism in spiking MoE transformers, leading to repeated weight loading and ignoring the distributed parallelism of spiking experts.

In this work, we leverage face-to-face (F2F)-bonded 3D integration technology to design dedicated spiking Mixture-of-Experts (MoE) transformer accelerators incorporating spiking Multi-Head Attention (MHA) mechanisms. The proposed architecture employs both memory-on-logic and logic-on-logic configurations.

• We present the first dedicated 3D accelerator architecture for spiking MoE transformers, efficiently exploring spatial and temporal parallelism weight reuse within modularized spiking experts and exploiting spiking expert parallelism for both MoE and MHA, supporting a scalable and efficient spike-based computation in MoE transformers.

• We explore the first 3D memory-on-logic and logic-on-logic interconnection schemes for the parallel distributed spiking MoE transformers to significantly reduce energy consumption and latency, thereby delivering highly efficient spiking neural computing systems with minimal area overhead.

Compared to the 2D CMOS integration, the 3D accelerators dedicated for spiking MoE transformers offer substantial improvements. For the spiking MoE and MHA workloads, it provides a 3%-5.1% increase in effective frequency, 39%-41% area reduction, 26.9%-29% memory access latency reduction and up to 14.4% power reduction.

SECTION: 
SECTION: 
The spiking neuron models, as temporal activation functions, are widely used in SNNs. Leaky Integrate-and-Fire (LIF) models and Integrate-and-Fire (IF) models are commonly adopted. The LIF model simulates and mimics a neuron’s response, exhibiting the following temporally discrete behaviors over multiple timesteps:

The Spiking Multi-Head Attention Mechanism splits thefeatures of spikingintoslices, denoted as the number of attention heads, as, where. Here,denotes the number of timesteps, anddenotes the number of tokens. Eachandformulatesspiking attention maps across. Each attention map is applied to the spiking value to generate the output as.

SECTION: 
Mixture-of-Experts (MoE) is a machine learning architecture that has gained traction for its high scalability. MoE models, leveraging a learnable routing networkto compute gating scores forexperts, intelligently route input tokens to one or more of the most appropriate experts. These models are typically built on top of transformer-based models, where the traditional feed-forward network in each transformer layer is replaced with a combination of a gating network and multiple experts. To scale efficiently, MoE typically distributes experts across multiple GPUs, assigning one or more experts to each GPU while replicating non-expert parameters across all GPUs. However, the high energy consumption of GPUs conflicts with the low-power requirements of spiking neural networks (SNNs). To address this challenge, we propose a compact distributed MoE accelerator based on 3D integrated circuits, bridging the gap between scalability and energy efficiency.

SECTION: 
Various neuromorphic accelerators have been developed to support SNN inference at multiple levels, including devices, circuitsand architectures.
While these accelerators contribute to the advancement of generic SNNs, they lack optimization for the scalable spiking MoE transformers.
The exploration of 3D integrated circuits (3D ICs) for SNN hardware primarily focuses on monolithic 3D (M3D)and face-to-face (F2F) bonding techniques, which are often based on traditional liquid state machine (LSM) architectures.
Although these methodologies improve power-performance-area (PPA) metrics, they are limited by constraints in neuron count, which hinder effective dataflow optimization.

SECTION: 
SECTION: 
Spiking Mixture-of-Expert(MoE) layers in spiking transformers encompass five core processing steps:
➀ Spiking (tokens) Conditional Routing(SCR),
➁ Spiking Synaptic Integration(SSI),
➂ Spiking Membrane potential Accumulation(SMA),
➃ Spiking Conditional Generation(SCG), and
➄ Spiking Aligned Merging(SAM).
Among these steps, ➀ and ➄ are inter-expert operations, which handle communication for routing and merging between different spiking expert pathways, while ➁➂➃ are intra-expert computations, responsible for the computationally intensive processing leveraging each individual spiking expert’s knowledge.

Step ➀ processes the pre-synaptic activationto compute spiking expert scores, which quantify the importance of each spiking expert for a given spiking token. The top-K spiking experts are selected and assigned to the corresponding spiking tokens by routing.
In step ➁, each spiking expert processes the routed pre-synaptic spikes tokens for expert, denoted as. Each expert adapts pre-trained expert-specific weight, to compute the (post-)synaptic integration.
The step➂ sequentially accumulates the synaptic integration of each neuronfor each tokenat timestep, denoted byonto the membrane potential at timestep, to update membrane potential.
Following this, each spiking expert, in step ➃, performs conditional spike generation at each timestep as outlined in Equ., generating.
Finally, as a merging function, ➄ aggregates the outputs from all expertsto generate the alignedas outputs.

SECTION: 
In our proposed 3D integrated two-tier Spiking MoE accelerator design, as shown in Fig.(a), we assemble multiple modularized Spiking Expert (SE) cores on a single chip and place two sharable expert-weight Global Buffers (GLBs) between the distributed SEs on the top tier. A centralized spiking activation GLB is also placed on the top tier, managing the spiking workload. Additionally, a two-tier spiking token router is placed between the four SE cores to enable the spiking expert parallelism as shown in Fig.(c) by handling ➀ and ➄.
Each SE core is responsible for performing computations corresponding to ➁+➂+➃ in parallel.

As illustrated in Alg., the router adaptsto compute expert scores and based on the scores to route spiking tokens to distributed spiking expert cores. Each spiking expertreceives routed workloadand applies expert-specific weightlocally to generatein parallel, which is then merged and written back to the Activation GLB.
Correspondingly, within the spiking token router, an expert routing score array is placed at bottom tier which computes the routing score for tiled tokens and the scores can be vertically extracted to the top tier. Then, the router selects the top-K spiking experts for each token, and routes the packed spiking tokens to multiple spiking experts.
Within each SE core, the systolic PE array core placed at the bottom tier and a dedicated spiking generator core is placed at the top, executing kernel-fused operations of ➁+➂+➃.

As illustrated in Fig.(a), the optimized dataflow proceeds as follows:
Pre-synaptic activation tiles () and expert routing weights () are first vertically loaded from the Spiking Act GLB and Weight GLB at the top tier to the bottom tier (Fig.❶).
Next, the expert routing array computes scores ofexperts fortokens in parallel, with the routing scores extracted to the top-tier router (❷ and ❸).
The router then assigns spiking tokens to expert cores based on top-K selections, while the corresponding expert weightsare preloaded into SE cores from the Weight GLBs (❹ and ❺).
Subsequently, within each SE core, spiking tokens and weights are vertically loaded from local buffers to the bottom tier (❻), and synaptic integration forneurons overtokens andtimesteps is performed within the dense spatiotemporal systolic array (❼).
The integrated results are then extracted into spiking generators which compute membrane potentials and conditionally generate output spikes(❽), and the router write the aligned results back to the Act GLB (❾).

When processing assigned workload for an expert, the dense array within each modularized spiking expert core unrolls the workload with extreme fine spatiotemporal granularity. It maps 1-bit spiking activities to different columns, while multi-bit weights of output features are mapped to different rows.
Multi-bit weights are propagated horizontally, being reused across tokens and timesteps; 1-bit spiking activitiespropagate vertically and are reused across output neurons.
As shown in Fig.(c), each PE is designed with a synaptic integration-stationary approach. Synaptic integrations stored within PEs are 3D-extractable via dedicated readout ports, enabling spiking generators in the top tier to access data efficiently, thereby improving computation density.

SECTION: 
SECTION: 
The computation of spiking MHA layers is another bottleneck and has several key operations: ➀ the computation of spiking attention maps (), ➁ attention-weighted synaptic integration (), which provides inputs to a set of LIF neurons for generating the final binary spike-based attention output, ➂ membrane potential accumulation of these LIF neurons, and ➃ conditional generation of the LIF neuron output spikes as the final attention output.
In operation ➀, the spiking queryand spiking key, initially shaped as, are divided into. Here,represents the number of timesteps; N denotes the number of tokens;andindicate the number of self-attention heads and the number of features per head, respectively. A spiking attention mapis computed for each head at each timestep. For instance, the spiking attention map at-th timestep for-th self-attention head results from the binary matrix multiplication of the spiking query and key at the specific head and timestep.
In ➁, the attention-weighted synaptic integration is executed for each head at each timestep. The spiking attention map, serving as the attention weights, is combined with the spiking value, shaped into compute attention-weighted synaptic integration, denoted byshaped as.

SECTION: 
In the MHA mechanism described above, the computations of different heads can be neatly separated and processed by different spiking attention expert cores. The 3D MHA accelerators partition along with features and dispatch the partitioned features into different spiking attention experts to compute spiking outputs in parallel, and then neatly concat them and write through to the spiking Act GLB, as illustrated in Alg.. As illustrated in Fig.(a), ❶ loads spiking Q/K/V tokens to a spiking attention dispatcher on the top tier. The spiking dispatcher neatly partitions spiking features, required for different attention heads, and routes the spiking features to distributed spiking attention expert local buffers(LBs) in ❷ and ❸.

Each modularized spiking attention expert acceleration core performs ➀-➃ mentioned in Sec.. This acceleration performs kernel-fused spiking attention operations as mentioned in Alg.by adapting reconfigurable PEs in Fig.(b).
In step ❹, spiking,andare vertically loaded into buffers at the bottom tier. Then,andare streamed into the array horizontally and vertically(❺), to compute spiking attention and store the attention matrixin internal registers within the reconfigurable array, which avoids a data movement of multi-bit attention. In ❻,is streaming from top to down again to accumulate the attention-weighted synaptic integration that streams out at the right side of the PE array. Subsequently, in step ❼, the synaptic integration at the timestep is accumulated onto the membrane potential of the previous timestep stored on the top tier to generate conditional spikes in parallel spiking generators. Finally, the dispatcher writes through the generated spike activities back to spiking Act GLB in ❽.

SECTION: 
SECTION: 
We evaluated the spiking MoE transformer models on CIFAR10 and CIFAR100. The models utilize 8-bit quantized weights and 16-bit quantized synaptic integration. Each expert is allocated the same amount of weight parameters and trained for 100 epochs. We set feature size, patch size, head size and batch size to 128, 44, 16 and 512, respectively. We set top-1 routing to keep an approximate number of operations with increasing the number of experts. We demonstrate that as the number of spiking experts scale up, the performance is improved significantly in Tab..

In this work, we use the commercial 28nm PDK to implement both 2D and 3D F2F designs. The 2D design consists of 6 metal layers, while the 3D design features a double metal stack of the 2D design, with the F2F bond pitch varying from 0.5to 1. We use Synopsys Design Compiler to synthesize the RTL to a gate-level netlist and Cadence Innovus to perform physical synthesis. We utilize the pin-3Dflow, where the top and bottom dies are iteratively optimized across placement, CTS, routing, and sign-off stages.
For memory, we utilize SRAM modules generated by a commercial memory compiler for various global buffers, local buffers, and other storage functions within our system architecture. On the top tier, for MoE accelerators, we adapt two 8K128b SRAM units employed for the Weight GLB A and Weight GLB B, which store shareable expert weights. For both MoE and MHA accelerators, an additional 8K128b SRAM unit is adapted to manage and store spiking activations. Within each modularized expert core, 3K128b SRAM units are employed for Activation, Weight, and Synaptic Integration Local Buffers. Additionally, smaller 96128b SRAM macros are allocated for the Query (Q) buffer, Key/Value (K/V) buffer, and Spiking (S) buffer on the bottom tier. Two 96256b SRAM macros are configured to serve as extended X buffers.
The memory macro placement is determined based on the architecture information in Fig.(for the MHA accelerator) and Fig.(for the MoE accelerator).
In MoE accelerators, we use aPE array with arouting score computing array; For the modularized spiking attention expert core, the size of reconfigurable attention array is.

SECTION: 
In Fig.and Fig., the placement and layout differences between the 2D and 3D designs of spiking MLP accelerators and spiking self-attention accelerators are presented.
In Fig.(a)(b), the 2D design occupies, while the stacked 3D spiking design occupies. On the top tier, the W GLBs and Act GLB are placed between expert cores, as shown in Fig.(c)(d), while modularized Expert LBs and spiking generators are placed at the edges. Within each expert module, the weight and spike buffers are placed on the edge, and the spiking spatiotemporal array is positioned below the spiking generators.
In Fig., the 2D design of the spiking attention accelerator occupies, while the stacked 3D spiking design occupies. In the 3D MHA design, the Act GLB is stored on the top tier, while other memories are stacked at the edges.

The performance comparison between 2D and 3D implementations of modularized spiking expert MHA and MoE accelerators, as shown in Tab., demonstrates improvements across multiple metrics through 3D integration. In terms of performance, the 3D structure operates at higher effective frequencies of 2.24 GHz and 1.74 GHz for MHA and MoE, respectively, compared to their 2D counterparts at 2.13 GHz and 1.69 GHz, while achieving a reduced area footprint.

The area results of the 3D implementation show 39% and 41% reductions in MHA and MoE designs, respectively. Specifically, the MHA design area decreases from 5.53to 3.36, while the MoE design reduces from 2.97to 1.75. This area reduction is achieved while maintaining similar cell counts, indicating effective vertical integration without functionality loss. Power consumption results also show improvements in the 3D structure. In the MHA implementation, total power consumption decreases from 912 mW to 896 mW; the MoE design shows a larger improvement, with total power consumption reducing from 6989 mW to 5983 mW, representing a 14.4% reduction. These improvements are observed across internal, leakage, and memory access power components.

SECTION: 
Memory access efficiency is a critical factor in determining the overall performance of neural network accelerators. Our analysis focuses on the memory access characteristics of both 2D and 3D implementations of MHA and MoE designs, as shown in Tab..
The 3D structure demonstrates improved memory access performance compared to its 2D counterpart. In the MHA design, memory access latency decreases from 160 ps to 112 ps, representing a 30% reduction. Similarly, the MoE implementation shows a 15% reduction in memory access latency, from 202 ps to 172 ps. These reductions in latency can be attributed to the shortened interconnect distances achieved through vertical integration. The improved memory access efficiency is also reflected in power consumption. The MHA design shows a 29% reduction in memory access power, decreasing from 6.23 mW to 4.41 mW. Similarly, the MoE design achieves a 26.9% reduction, from 7.11 mW to 5.2 mW. These reductions in both latency and power consumption demonstrate how 3D integration can optimize memory access patterns in neural network accelerators.

SECTION: 
The wirelength comparison between 2D and 3D implementations, as shown in Tab., demonstrates notable reductions through 3D integration. For the single expert implementation, the MHA design shows a slight decrease from 0.621 m to 0.616 m, while the MoE design achieves a more significant reduction from 2.178 m to 1.959 m. The improvement becomes more pronounced in the 4-modularized expert systems, where the MHA design’s wirelength reduces from 3.654 m to 3.290 m, and the MoE design shows a substantial decrease from 11.352 m to 9.816 m.
The reduction in wirelength directly contributes to the improved performance metrics observed in both designs. This is particularly evident in the hierarchical memory access characteristics detailed in Tab..

At the Global Buffer (GLB) level, the MHA design shows reduced activation latency from 220 ps to 209 ps, with power consumption decreasing from 10.9 mW to 7.56 mW. The MoE design demonstrates even more substantial improvements, with activation GLB latency reducing from 148 ps to 117 ps and notable reductions in weight GLB latencies from 241/147 ps to 94/71 ps. The Local Buffer (LB) and buffer-level metrics show similar improvements, with particularly significant reductions in the activation and weight buffer latencies and power consumption. These improvements can be attributed to the optimized wirelength and more efficient signal routing achieved through 3D integration.
These results demonstrate how the reduced wirelength in 3D designs contributes to enhanced system performance through improved signal propagation and reduced power consumption in memory access operations.

SECTION: 
We present the first dedicated 3D acceleration for MoE and MHA spiking transformers, leveraging spatial and temporal parallelism, modularized 3D spiking expert acclerators, and efficient interconnections. Our 3D acceleration achieves significant improvements over 2D CMOS integration, delivering scalable and energy-efficient spiking neural computation with minimal hardware overhead. This work enables a practical deployment of large-scale spiking MoE transformers.

SECTION: References