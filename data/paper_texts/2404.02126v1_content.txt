SECTION: Rematch: Robust and Efficient Matching of Local Knowledge Graphsto Improve Structural and Semantic Similarity

Knowledge graphs play a pivotal role in various applications, such as question-answering and fact-checking. Abstract Meaning Representation (AMR) represents text as knowledge graphs. Evaluating the quality of these graphs involves matching them structurally to each other and semantically to the source text. Existing AMR metrics are inefficient and struggle to capture semantic similarity. We also lack a systematic evaluation benchmark for assessing structural similarity between AMR graphs. To overcome these limitations, we introduce a novel AMR similarity metric,rematch, alongside a new evaluation for structural similarity called RARE. Among state-of-the-art metrics,rematchranks second in structural similarity; and first in semantic similarity by 1–5 percentage points on the STS-B and SICK-R benchmarks.Rematchis also five times faster than the next most efficient metric.111Our code forrematchand RARE is publicly available at:https://github.com/osome-iu/Rematch-RARE

Rematch: Robust and Efficient Matching of Local Knowledge Graphsto Improve Structural and Semantic Similarity

Zoher Kachwala,   Jisun An,   Haewoon Kwak,   Filippo MenczerObservatory on Social MediaIndiana Universityzkachwal@iu.edu, {jisun.an, haewoon}@acm.org

SECTION: 1Introduction

Knowledge graphs provide a powerful framework for multi-hop reasoning tasks, such as question answering and fact-checkingYasunaga et al. (2021); Vedula and Parthasarathy (2021). Even for closed-domain tasks like long-form question answering and multi-document summarization, knowledge graphs derived from individual documents — referred to aslocal knowledge graphs— exhibit superior performance compared to plain textFan et al. (2019). This highlights the significance of automatically parsed knowledge graphs in both large-scale and fine-grained structured reasoning applications.

The Abstract Meaning Representation (AMR) framework leverages acyclic, directed, labeled graphs to represent semantic meaning (knowledge) extracted from textBanarescu et al. (2013). As illustrated in the example of Fig.1, AMRs capture the relationships between concepts and their roles in a sentence. They have been applied to a variety of natural language processing tasks, including summarization and question answeringLiu et al. (2015); Hardy and Vlachos (2018); Bonial et al. (2020); Mitra and Baral (2016). Recent work has also shown that AMRs can reduce hallucinations and improve performance in factual summarization and text classification tasksRibeiro et al. (2022); Shou et al. (2022).

However, evaluating the quality of knowledge graphs like AMRs hinges critically on the ability to accurately measure similarity. This assessment must consider a dual perspective. Firstly, the similarity between two AMRs should reflect structural consistency, guaranteeing that the similarity between two AMRs aligns with the similarity of their structural connections. Secondly, AMRs should exhibit semantic consistency, ensuring that the similarity between two AMRs aligns with the similarity of the texts from which they are derived. Therefore, an effective AMR similarity metric must successfully account for both structural and semantic similarity, all while overcoming the resource-intensive nature of matching labeled graphs.

Current AMR similarity metrics fall short in several key areas. Firstly, their computational efficiency hinders the comparison of large AMRs extracted from documentsNaseem et al. (2022). Secondly, these metrics struggle to accurately capture the semantic similarity of the underlying text from which AMRs are derivedLeung et al. (2022a). Additionally, while recent efforts like BAMBOOOpitz et al. (2021)have evaluated metrics on AMR transformations, we still lack a large-scale benchmark to systematically evaluate the ability of AMR metrics to capture structural similarity.

Our work introduces a structural AMR benchmark calledRandomized AMRs with Rewired Edges(RARE) and proposesrematch, a novel and efficient AMR similarity metric that captures both structural and semantic similarity. Compared to the state of the art,rematchtrails the best similarity metric on RARE by 1 percentage point and ranks first on the STS-BAgirre et al. (2016)and SICK-RMarelli et al. (2014)benchmarks by 1–5 percentage points. Additionally,rematchis five times faster than the next most efficient metric.

SECTION: 2Background

SECTION: 2.1Abstract Meaning Representations

Abstract Meaning Representation (AMR) is a structural, explicit language model that utilizes directed, labeled graphs to capture the semantics of textBanarescu et al. (2013). AMR is designed to be independent of surface syntax, ensuring that sentences with equivalent meanings are represented by the same graph.
An AMR comprises three fundamental components: instances, attributes, and relations.

Instancesare the core semantic concepts. Structurally, they are represented by nodes in the graph. AMRs have two types of instances. One utilizesPropBankPalmer et al. (2005), a dictionary of frames that map verbs and adjectives. The other comprises entities. Considering the sentence in Fig.1,“He did not cut the apple with a knife,”the AMR contains aPropBankinstancecut-01and three entity instances:he,appleandknife.

Attributescapture details about instances, such as names, numbers, and dates. These values are represented as constant nodes. Structurally, an attribute is identified in the graph as the edge from an instance node to a constant node. For example, in Fig.1, the attributepolarityis specified for the instancecut-01, where-is the constant that represents the negation of the verb.

Relationsrepresent the connections between instances. In Fig.1, the instancecut-01has three outgoing relations:ARG0,ARG1, andinst. These come fromPropBank’scut-01frame and link to the agent (he), the patient (apple), and the instrument (knife), respectively.

SECTION: 2.2AMR Similarity

Graph isomorphism is a test to determine whether two graphs are structurally equivalent.
The class-wise isomorphism testing with limited backtracking (CISC) algorithm efficiently identifies isomorphic relationships in labeled graphsHsieh et al. (2006), such as AMRs. But a pair of AMRs may not have the same number of nodes, which violates a key assumption of graph isomorphism. A more appropriate approach is subgraph isomorphism, which determines whether a smaller graph is isomorphic to a subgraph of a larger graph. Subgraphs of directed acyclic graphs, like AMRs, can be enumerated in polynomial timePeng et al. (2018), enabling efficient application of the CISC test to each pair of smaller AMR and larger AMR subgraphs.
However, even if two AMRs are not subgraph-isomorphic, they may still exhibit similarities in meaning and structure.
Next, we describe various existing approaches to measure the similarity between AMR graphs.

SECTION: 2.3AMR Similarity Metrics

Smatchis a prominent tool for evaluating AMR parsersCai and Knight (2013). It establishes AMR alignment by generating a one-to-one node mapping, considering node and edge labels. To efficiently explore this vast mapping space,smatchemploys a hill-climbing heuristic.

Similar tosmatch,s2matchOpitz et al. (2020)also establishes a node alignment between two AMRs. However, instead of relying on AMR labels,s2matchutilizes GloVe word embeddingsPennington et al. (2014). To address the extensive search space, it uses the same hill-climbing heuristic adopted bysmatch.

Sembleugenerates path-based n-grams from AMRs by leveraging node and edge labelsSong and Gildea (2019). The final similarity score for an AMR pair is determined by calculating the BLEU scorePapineni et al. (2002)between their n-grams. By avoiding a one-to-one node alignment,Sembleuefficiently bypasses the issue of exploring a large search space.

The Weisfeiler-Leman Kernel (WLK) and Wasserstein Weisfeiler-Leman Kernel (WWLK) for AMRs also utilize graph features for computing similarityOpitz et al. (2021).WLKfirst constructs node features by recursively aggregating AMR node and edge labels. Then it generates a frequency-based feature vector for each AMR and calculates a similarity score using their inner product.WWLKextendsWLKwith features based on aggregated node embeddings (GloVE) instead of node labels. Since WWLK is a supervised metric, we do not consider it in our evaluation.

SECTION: 3Methods

In this work, we proposerematch, an AMR similarity metric that aims to capture both the structural and semantic overlap between two AMRs.

A straightforward approach to match two labeled graphs involves identifying the alignment between node labels. However, labeled graphs often contain duplicate labels, necessitating an exhaustive exploration of all one-to-one combinations among nodes within the same label group to determine the optimal match. The resulting matching complexity hinges on the size of node groups with shared labels.
This is why algorithms likesmatchands2matchdo not scale well to large AMRs, where these node groups can be large.

Graph features constructed using an ordered concatenation of edge-node bi-grams are utilized in both isomorphism tests like the CISC and kernels like Weisfeiler-LemanShervashidze et al. (2011). This approach is effective: it consistently produces smaller node groups compared to those based solely on node labels. Matching between two graphs is significantly accelerated as a result.

Inspired by this idea of exploiting graph features for efficiency,rematchcomputes the similarity between two AMRs by analyzing the overlap of semantically rich features, which we callmotifs.
Unlike the ordered graph partitions used by CISC and Weisfeiler-Leman Kernel, which rely on node and edge labels, AMRmotifsare unordered graph partitions that leverage AMR instances, attributes, and relations.
This approach allowsrematchto capture meaning across three semantic levels: specific facts (attributes), main concepts (instances), and the relationships among concepts (relations).
Fig.2illustratesrematchthrough an example.
Next, we delve into the three orders of semantic motifs that we use forrematch. We extract these motifs using the Python package PenmanGoodman (2020).

Attribute motifsare pairs of attributes and constants associated with AMR instance nodes. For the bottom AMR in Fig.2,talk-01has attribute motif (polarity-), indicating a negation. The firstnamehas the attribute motif (op1"Helen") and the secondnamehas (op1"Maya"), identifying the name values. The remaining instances do not have any attributes.

Instance motifsleverageVerbatlas, a resource that mapsPropBankframes to more generalized framesDi Fabio et al. (2019). If an instance in the AMR corresponds to a Verbatlas frame, the latter is used instead. Otherwise, the originalPropBankinstance is retained. For example, in Fig.2,talk-01is replaced by the more generalized Verbatlas framespeak.
The generation of instance motifs follows two approaches. If an instance lacks associated attributes, the instance itself serves as its motif. However, if attributes are present, instance motifs are constructed by combining the instance with each of its attribute motifs. For the bottom AMR in Fig.2, the instance motif fortalk-01is (speak(polarity-)), indicating a negation of the verb. For the twopersoninstances and thepoliticsinstance, the instances themselves become their motifs, namely (person) and (politics). Finally, the instance motifs for the twonameinstances are (name(op1"Helen")) and (name(op1"Maya")) respectively, identifying the names in the conversation.

Relation motifsare constructed for relation edges in an AMR graph. Each relation motif comprises three elements: an instance motif of the source instance, the relation label, and an instance motif of the target instance. A relation can have multiple relation motifs, one for each unique combination of source and target instance motifs.
For the bottom AMR in Fig.2, the relation motifs forARG0,ARG1andARG2are:
((speak(polarity-))ARG0person), indicating a person is the speaker of the conversation; ((speak(polarity-))ARG1politics), indicating that the topic of conversation is politics; and ((speak(polarity-))ARG2person), indicating that a person is the recipient of the conversation.
For the twonamerelations, the motifs are: ((person)name(op1"Helen"))), identifying "Helen" as the name of one person; and ((person)name(op1"Maya")), identifying "Maya" as the name of the other person.

Each AMR is represented by the union of its instance, relation, and attribute motifs. Therematchscore between two AMRs is determined by calculating the Jaccard similarity between their respective motif sets, as illustrated in Fig.2.

SECTION: 4Evaluation

We evaluate the effectiveness ofrematchon three types of similarity: structural similarity, semantic similarity, and BAMBOOOpitz et al. (2021), a hybrid benchmark that modifies AMR semantics through structural transformations. Additionally, we assess the efficiency ofrematch.

SECTION: 4.1Structural Similarity (RARE)

Given that AMRs are graphical representations of text, an AMR similarity metric should be sensitive to structural variations between AMRs, even if its labels remain unchanged.

Since there is no established evaluation of AMR metrics on structural similarity, we have developed a new benchmark dataset calledRandomized AMRs with Rewired Edges(RARE). RARE consists of English AMR pairs with similarity scores that reflect the structural differences between them.

In the construction of RARE, we adopt an iterative randomization technique commonly used for graph rewiring. This involves repeatedly selecting a random pair of directed edges and swapping either their source or target nodes to establish new connections. This way each node’s in-degree and out-degree are preserved. In applying this approach to AMRs, we swap a random pair of edges between either attributes or relations.
This allows us to quantify the structural changes made to the AMR through the number of swapped edges.

RARE does not add or remove edges as these modifications would amount to adding or removing information.
Systematic edge insertion or deletion would also introduce additional complications, such as having to decide the set of edges that could be added or removed while keeping the network connected.
By swapping edges alone, we guarantee that the AMRs being compared have the same information in terms of size, density, and connectivity.

We generate a spectrum of modified graphs from an original AMR, ranging from the unchanged graph to one where all edges are rewired, subject to some constraints that preserve the integrity of AMRs:

Structural Constraints.AMRs are acyclic, connected graphs that allow no multiedges (more than one edge between the same pair of nodes). To preserve these properties during the rewiring process, pairs of swapped edges must maintain these constraints in the modified AMR.

Semantic Constraints.These constraints relate to swapping attributes and relations:

Attributes have an inherent connection with constants in AMRs. Hence, while rewiring a pair of attribute edges, only the source instance node should be swapped. This restriction ensures that the association between the attribute and its corresponding constant remains intact. For example, the constant node-should remain associated solely with the attribute edgepolarity.

Relations in AMRs connect two instances. When rewiring a pair of relation edges, only the target instance node should be swapped. This restriction maintains the association between the relation’s source instance and the relation itself. For example,PropBankinstances have a predefined set of relations with which they can be associated. The instance nodetalk-01can only be associated with edgesARG0,ARG1, andARG2.

Each pair of AMRs, consisting of an original AMRwithedges and its corresponding rewired AMRwithswapped edges, is annotated with the following similarity score:

To generate the RARE benchmark, we licensed the English AMR Annotation 3.0Knight, Kevin et al. (2020)containing 59,255 human-created AMRs. Using the process described above, we get 563,143 rewired AMR pairs annotated with similarity scores per Eq.1. Since the original AMR Annotation 3.0 corpus has an unusual training-development-testing split, we merge, shuffle, and re-split AMR 3.0 into training (47,404), development (5,925), and test (5,926) sets to get an 80-10-10 split ratio that is more consistent with standard benchmarks. The resulting RARE training-development-test sizes are 450,067, 56,358, and 56,718, respectively. The creation of training and development splits could facilitate the future development of supervised AMR metrics. For the current evaluation, AMR structural similarity metrics are evaluated on the RARE test split.

We evaluate a similarity metric by computing the Spearman correlation between its scores and the ground truth values from Eq.1, across a set of pairs of original and modified AMRs. We refer to this as thestructural consistencyof the metric.

SECTION: 4.2Semantic Similarity

A fundamental tenet of AMRs is that if two pieces of text are semantically related, their corresponding AMRs should exhibit a degree of similarity. But a metric could deem two AMRs similar even when their textual sources have very different meanings. As an example, for two completely unrelated sentences “Spanish bulls gore seven to death” and “Obama queries Turnbull over China port deal,”smatchassigns a non-zero score due to the similarity in their argument structureLeung et al. (2022b). To tease out such shortcomings, we evaluate each AMR similarity metric by considering many pairs of sentences. For each pair, we compare the similarity generated by the metric for the corresponding AMRs to a ground-truth similarity score between the sentences generated by human annotations.

We utilize two standard sentence similarity benchmarks for English: STS-BAgirre et al. (2016)and SICK-RMarelli et al. (2014). To account for variations in AMR parsing accuracy, we employ four different AMR parsers:springBevilacqua et al. (2021),amrbartBai et al. (2022),structbartDrozdov et al. (2022), and themaximum Bayes smatch ensembleLee et al. (2022).

Given a set of sentence pairs and corresponding AMR pairs, we evaluate a similarity metric by computing the Spearman correlation between its scores for the AMR pairs and the human-annotated similarity values for the sentence pairs. We refer to this as thesemantic consistencyof the metric.
Note that semantic consistency can be used to evaluate any similarity method for sentences, not only AMR-based ones.
For both structural and semantic consistency, we use Spearman rather than Pearson correlation because we do not assume that the similarity values are normally distributed.

SECTION: 4.3Hybrid Similarity (BAMBOO)

In addition to the structural and semantic consistency discussed earlier, we evaluate the robustness of AMR metrics using theBenchmark for AMR Metrics Based on Overt Objectives, or BAMBOOOpitz et al. (2021). BAMBOO assesses the ability of AMR similarity metrics to capture semantic similarity between English sentences while modifying the structure of the corresponding AMRs.

BAMBOO incorporates three types of graph modifications: synonym replacement, reification, and role confusion.
Consider the example sentence “He lives in the attic,” represented by an AMR where the nodelive-01connects to nodesheandatticvia the edgesARG0andlocation, respectively.
Synonym replacement swapsPropBankinstances with equivalent terms. In the example,live-01might be replaced byreside-01.
Reification transforms a relation into a new instance. In the example, thelocationedge might be replaced by a new nodebe-located-at-91connected tolive-01andatticvia newARG1andARG2edges, respectively.
Finally, role confusion swaps relation roles. In the example, the relationslocationandARG0might be swapped such that the modified AMR would represent the sentence “The attic lives in him.”
BAMBOO applies these modifications to the original train, test and dev splits of the STS-B, SICK-R, and PARADolan and Brockett (2005)datasets.

Given a set of modified AMR pairs, BAMBOO evaluates an AMR metric by the Spearman correlation222The original formulation of BAMBOOOpitz et al. (2021)used Pearson correlation. Here we use Spearman because, as for structural and semantic consistency, we do not assume that the similarity values are normally distributed.between its scores and the similarity between the corresponding sentence pairs. We call thishybrid consistencyof the metric.

SECTION: 4.4Efficiency

As discussed earlier, the computational complexity associated with node alignment is a crucial challenge for comparing AMRs. To address this issue, we evaluate the search spaces explored by various metrics and the required runtime.

We establish a realistic test bed using the AMR Annotation 3.0 once again. For this evaluation, we randomly sampled 500,000 pairs from thepossible AMR combinations.
For each pair of AMRs, the search spaces for node alignment algorithms likesmatchands2matchis

wheredenotes the set of matching candidates infor node.
For feature-based algorithms, likesembleu,WLK, andrematch, we record the search space using

wheredenotes the feature set for graph.
For each pair of AMRs, we also record the runtime.

SECTION: 5Results

SECTION: 5.1Structural Consistency

Table1reports on the structural consistency of the AMR similarity metrics on the RARE test split. We can see thatsmatchperforms the best, followed closely byrematch,sembleuands2match. The subpar performance ofWLKcan be attributed to their reliance on features using all of a node’s neighbors. This approach results in changes to node features regardless of the number of modified neighbors, failing to capture the nuances of neighborhood changes.

SECTION: 5.2Semantic Consistency

Table2reports on the semantic consistency of the similarity metrics for different AMR parsers.Rematchoutperforms all other metrics by 1–5 percentage points, across all parsers and benchmarks.
Thembseandamrbartparsers perform best for the STS-B and SICK-R datasets, respectively.

So far we have focused on methods that use AMRs to calculate the semantic similarity between sentences.
Table3reports on the evaluation of alternative similarity methods on the same benchmarks.
Like AMR-based methods, these are also unsupervised (not trained specifically) for textual semantic similarity.
AMR outperforms some representations like GloVe and RoBERTa but lags behind the state-of-the-art method SimCSEGao et al. (2022).

SECTION: 5.3Hybrid Consistency

Table4reports on the hybrid consistency of AMR similarity metrics on the four different tests of BAMBOO, across three different datasets. The results vary considerably across graph modifications and datasets; none of the methods is a clear winner.Rematchachieves best results in three out of twelve tests and lags slightly behinds2matchon average.

SECTION: 5.4Efficiency

Fig.3shows the search spaces explored by AMR metrics for increasing values of, the average size of each pair of AMRs. The size of each AMR is determined by the sum of the number of instances, attributes, and relations. Approaches that find node alignment between AMRs, likesmatchands2match, explore search spaces that grow exponentially with. Feature-based methods, likesembleu,WLK, andrematch, in contrast, explore significantly smaller spaces.

Fig.3also shows the runtimes for increasing. By using a hill-climbing heuristic, node-alignment metrics effectively overcome the exponentially growing search spaces. However, they are significantly less efficient compared to feature-based metrics. For large values of,smatchands2matchdisplay an approximately quadratic time complexity.Sembleu,WLK, andrematch, on the other hand, demonstrate a linear complexity.

In terms of absolute runtime on the test bed,rematchis the fastest metric, with a runtime of 51 seconds. This is five times faster thansembleu, which took 275 seconds.Smatch,s2match, andWLKtrailed further behind, requiring 927, 7718, and 315 seconds. All metrics executed the test bed on a single 2.25 GHz core.Rematch,sembleu, andsmatchneeded 0.2 GB of RAM, whereass2matchandWLKrequired 2 GB and 30 GB, respectively. We leave the efficiency comparison against non-AMR similarity methods like GloVe, RoBERTa and SimCSE as future work.

SECTION: 5.5Ablation Study

To assess the impact of the three types ofrematchmotifs — attribute, instance, and relation — on structural and semantic similarity, let us conduct an ablation study, in which we remove one or more types of motifs at a time. The results are presented in Table5. Instance motifs have the most significant influence on semantic similarity, particularly when combined with relation motifs. Conversely, relation motifs exert the strongest influence on structural similarity, especially when complemented by instance motifs.

To evaluate the overall effectiveness of motifs, we also assess the performance ofrematchthrough the use of AMR labels alone. For the bottom AMR in Fig.2, the label set is {talk-01,person,politics,name,ARG0,ARG1,ARG2,name,-,"Helen","Maya",polarity,op1}. Note thatperson,name, andop1appear only once in the set. Similar torematchmotifs, we calculate the Jaccard similarity between two AMR label sets.
As shown in Table5, the decline in structural consistency when using AMR labels is substantial, given the absence of structural information in the label sets.
In contrast, the decline in semantic consistency is relatively modest, indicating that AMR labels play a significant role in capturing semantics.

SECTION: 5.6Error Analysis

On structural consistency, we find thatrematchunderperforms when RARE swaps attribute edges connected to instance nodes with many relations. While the change might seem minor (a single swapped edge), the nested motif structure ofrematchamplifies the difference: mismatches in attribute motifs extend to instance motifs and all connected relation motifs, leading to a significant discrepancy in the overall similarity score.

The nested nature ofrematchcan also sometimes underestimate semantic similarity. For instance, consider the sentences “Work into it slowly” and “You work on it slowly.” The first sentence’s AMR associates animperativeattribute with the verbwork-01. This feature is missing in the second sentence. Consequently,rematchgenerates different instance and relation motifs, resulting in a lower similarity score compared to the ground-truth similarity.

More often, the nested motif generation grantsrematchan advantage in semantic consistency tasks: it allowsrematchto handle negation more effectively compared to other metrics. For example, the sentences “You should do it” and “You should never do it” have a lower similarity score inrematchdue to the presence of the negative (-)polarityattribute.

SECTION: 6Conclusion

This paper introducesrematch, a novel and efficient metric for AMR similarity.Rematchleverages semantic AMR motifs to outperform existing metrics in both semantic consistency and computational efficiency. Additionally, we present RARE, a new benchmark designed to evaluate the structural consistency of AMR metrics. Using RARE, we demonstrate the strong sensitivity ofrematchto structural changes in AMRs.

AMR matching was originally introduced to evaluate and enhance AMR parsers.
Through improved matching, metrics likerematchimprove parsing, whichindirectlybenefits downstream uses of AMRs.
Butrematchshows that AMRs encode richer semantics than previously assumed. Thus, improved AMR matching alsodirectlybenefits downstream applications, like semantic textual similarity.

Future research should explore the full potential of AMRs for natural language understanding. Natural Language Inference (NLI) is a prime example, where AMR-based systems have already shown promiseOpitz et al. (2023). An even more intriguing direction would be to develop methods that perform NLI solely through AMR matching, capitalizing on the rich structure and semantics encoded within AMRs.

SECTION: 7Limitations

Current AMR metrics, includingrematch, have limitations for downstream tasks like semantic textual similarity. One key issue is their inability to capture similarity between words. This can lead metrics likerematchto misclassify two sentences with different wordings but equivalent meaning as dissimilar.S2matchattempts to address this limitation by using word embeddings for node alignment, but our analysis shows that this approach offers minimal improvement in semantic consistency at a high computational cost.
Recently, this limitation was addressed by a novel self-supervised metric called AMRSimShou and Lin (2023). It trains Siamese BERT models on flattened silver AMR pairs generated from one million sentences sampled from Wikipedia.

Another limitation ofrematchis that it uses motifs associated with single edges (paths of length one). While this approach works well for short-text semantic similarity, it might not capture the more complex semantics present in AMRs derived from longer documents. In other words,rematchmight struggle to compare the meaning of longer texts.

We thank Ramón Fernandez Astudillo for helpful discussions.
This work was supported in part by the NSF through an NRT Fellowship (grant 1735095), the
Knight Foundation, and Craig Newmark Philanthropies. We also acknowledge the Lilly Endowment for computational support through the Indiana University Pervasive Technology Institute.

SECTION: References