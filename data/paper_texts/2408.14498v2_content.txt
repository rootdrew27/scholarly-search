SECTION: Multi-Normal Prototypes Learning for Weakly Supervised Anomaly Detection

Anomaly detection is a crucial task in various domains. Most of the existing methods assume the normal sample data clusters around a single central prototype while the real data may consist of multiple categories or subgroups. In addition, existing methods always assume all unlabeled samples are normal while some of them are inevitably being anomalies. To address these issues, we propose a novel anomaly detection framework that can efficiently work with limited labeled anomalies. Specifically, we assume the normal sample data may consist of multiple subgroups, and propose to learn multi-normal prototypes to represent them with deep embedding clustering and contrastive learning. Additionally, we propose a method to estimate the likelihood of each unlabeled sample being normal during model training, which can help to learn more efficient data encoder and normal prototypes for anomaly detection. Extensive experiments on various datasets demonstrate the superior performance of our method compared to state-of-the-art methods. Our codes are available at:https://github.com/Dongzhijin/MNPWAD

SECTION: IIntroduction

Anomaly detection aims to identify samples that deviate significantly from the general data distribution, known as anomalies or outliers[1]. These anomalies often indicate potential problems or noteworthy events that require attention[2]. The significance of anomaly detection spans a variety of critical domains, such as network security[3], Internet of Thing (IoT) security[4], and financial fraud detection[5]. In each of these areas, effective anomaly detection methods are essential due to the substantial impact of anomalies, which can lead to severe consequences such as security breaches[6], financial losses[7], compromised privacy[8], and even risks to human health and safety[9,10].

In real-world scenarios, obtaining labeled data for anomaly detection is often challenging and costly[1,11,12]. Although unsupervised methods[13,14]can bypass the need for extensive labeling, they struggle to achieve optimal performance without knowing what true anomalies look like. This limitation frequently results in the misidentification of many normal data points as anomalies[15]. To address these challenges,weakly supervised anomaly detectionmethods have emerged[16,17]. These methods leverage a small portion of labeled anomalous data in conjunction with a large amount of unlabeled data, primarily composed of normal instances. This kind of approach is particularly significant in practical applications where we can only label a small fraction of anomalies.

However, existing anomaly detection methods still face several significant challenges. First, most of the existing anomaly detection methods[18,19,16]assume that normal data clusters around a single central prototype. This assumption oversimplifies real-world scenarios where normal data often comprises multiple categories or subgroups, as illustrated in Fig.1. Such methods struggle to capture the complexity and diversity of normal data, leading to potential misclassification of normal samples as anomalies[20].
Second, existing methods always assume all unlabeled data are normal while they inevitably contain some anomalous samples. Due to the assumption, many studies[21,20,16]directly employ large amounts of unlabeled data as inliers for model training to identify the distribution of normal data patterns. These methods are vulnerable to the presence of occasional anomalies (i.e., anomaly contamination)[22]within the unlabeled data, and the detection accuracy rapidly declines as the proportion of mixed-in anomalies increases[15,17].

To address the problems stated above, we propose a reconstruction-based multi-normal prototype learning framework for weakly supervised anomaly detection. Different from existing reconstruction-based anomaly detection methods[21], we treat the normal samples and anomalous samples differently during the latent representation learning with consideration of the likelihood of each unlabeled sample being normal. To better estimate the likelihood of normal and detect anomalies, we propose to learn multiple normal prototypes in the latent space with deep embedding clustering and contrastive learning. Finally, we compute a comprehensive anomaly score with consideration of both the information of sample reconstruction and multiple normal prototypes.

The main contributions of this paper can be summarized as follows:

We propose a novel anomaly detection framework that combines reconstruction learning with multi-normal prototype learning. Extensive experiments across various datasets demonstrate that our method significantly outperforms state-of-the-art methods.

We propose to build multiple normal prototypes with deep embedding clustering and contrastive learning to better model the distribution of normal data for anomaly detection.

We propose to estimate and take into consideration the likelihood of each unlabeled sample being normal during model training, which can enhance resistance to anomaly contamination and more effectively detect unseen anomaly classes.

SECTION: IIRelated Work

SECTION: II-AAnomaly Detection Based on Prototype Learning

One-class classification methods[23]are a fundamental category of traditional anomaly detection techniques. These methods are trained solely on normal data and identify anomalies by detecting deviations from a central prototype. Examples include One-Class SVM (OC-SVM)[24], Support Vector Data Description (SVDD)[25], Deep SVDD[18], and DeepSAD[19], all of which assume that normal samples cluster around a single prototype in feature space, with anomalies located at the periphery. However, some studies have shown that normal data frequently consists of multiple categories or subgroups[20], rendering a single prototype insufficient[26]. To address these limitations, multi-prototype learning approaches have been developed to better represent the diversity within normal data, thereby enhancing anomaly detection capabilities. This concept has been applied across various domains, such as video anomaly detection[27], image anomaly detection[28], and time series analysis[29,30,31]. These advancements highlight the potential of multi-prototype learning, particularly in the context of tabular data for weakly supervised anomaly detection.
By capturing the diversity inherent in normal data through multiple prototypes, our proposed method aims to more effectively detect anomalies that would otherwise be missed by single-prototype models.

SECTION: II-BWeakly Supervised Anomaly Detection

Weakly supervised anomaly detection has emerged as a solution to the limitations of purely unsupervised methods, which struggle to identify what anomalies look like due to the lack of labeled examples[15]. In weakly supervised settings, a small number of labeled anomalies are combined with a large amount of unlabeled data to enhance detection performance. Existing weakly supervised methods face notable challenges. Many rely heavily on large amounts of unlabeled data to identify normal data patterns, making them susceptible to contamination by occasional anomalies within the unlabeled dataset[21,19]. This lack of effective mechanisms to mitigate the impact of these anomalies can significantly degrade detection performance, with accuracy declining rapidly as the proportion of mixed-in anomalies increases[21,20]. Moreover, a common issue across these methods[15,17,19,16]is overfitting to the limited labeled anomalies. This over-fitting weakens the generalization capability of the models, especially when the number of known anomalies is small or when the models are confronted with previously unseen anomaly types during the testing phase. As a result, detection performance suffers, highlighting the need for methods that can better leverage the small amount of labeled data while effectively utilizing the distribution patterns within a large pool of unlabeled data.

SECTION: IIIMethodology

SECTION: III-ATask Definition

Letdenote the sample set, which contains two disjoint subsets,and. Here,represents a large set of unlabeled samples, and(where) denotes a small set of labeled anomaly samples. Our objective is to learn a scoring functionthat assigns anomaly scores to each data instance, with the larger value ofindicating a higher likelihood ofbeing an anomaly.

SECTION: III-BOverview of Our Method

To detect anomalies in a weakly supervised setting, we propose a novel approach combining reconstruction learning and multi-normal prototype learning, as illustrated in Fig.2. It consists of three main components: 1)Reconstruction Learning Moduleaims to guide the latent representation learning with differentiated treatment to reconstruction errors of normal and anomalous samples; 2)Multi-Normal Prototypes Learning Moduleaims to model the distribution of normal data better and utilizes deep embedding clustering and contrastive learning to build multiple normal prototypes for anomaly detection; 3)Unified Anomaly Scoring Moduleaims to compute a comprehensive anomaly score with consideration of both the information of sample reconstruction and multiple normal prototypes. The detailed design of each component is described below.

SECTION: III-CReconstruction Learning Module

The primary purpose of this module is to transform the data into a latent space that captures the essential features of each sample, enabling effective differentiation between normal and anomalous samples. These latent representations are subsequently utilized in the Multi-Normal Prototypes Learning Module and serve as one of the inputs to the Unified Anomaly Scoring Module. Unlike traditional reconstruction-based methods[32,33], which aim to minimize the reconstruction error for all samples without considering their labels, our approach incorporates label information and the likelihood of unlabeled samples being normal.

First, we employ an encoder-decoder structure for reconstruction learning. The encoder functionmaps an inputto a latent representation:

The decoder functionmaps the latent representationback to a reconstructionof the original input:

whereandare the parameters of the encoder and decoder neural networks, respectively.

Second, we specifically design a reconstruction loss for weakly supervised anomaly detection. On the one hand, we minimize the reconstruction error to ensure that the latent representations capture the most relevant characteristics of the normal data. For unlabeled samples, the reconstruction loss is defined as:

whereis a regression loss function like Mean Squared Error (MSE), andindicates the probability of an unlabeled sample being normal, reducing the impact of anomaly contamination in the training process by assigning them lower weights. The next module will detail howis derived.

On the other hand, to distinguish normal samples from anomalies, we apply a hinge loss[34]to ensure that normal samples have a lower reconstruction loss compared to anomalies, maintaining a sufficient margin. Therefore, the reconstruction loss for labeled anomalies is defined as:

The overall reconstruction loss, combining the losses for both unlabeled data and labeled anomalies, is given by:

SECTION: III-DMulti-Normal Prototypes Learning Module

We assume the normal sample data may consist of multiple categories or subgroups and satisfy multi-modal distribution. Therefore, we propose to learn multiple prototypes to represent the normal data. Specifically, we utilize deep embedding clustering and contrastive learning to learn multiple normal prototypes.

At the beginning of training, we initializenormal prototypes by clustering normal data’s latent representations from the pre-train Encoder, denoted as. During the model training and inference phases, we need to calculate the similarity between a given sampleand each prototypeto determine which category or subgroup the sample belongs to. Specifically, we assume the samples of each category or subgroup satisfy the Student’s-distribution, and calculate the similarity between sampleand prototypeas follows:

wheredenotes the latent representation of sample,is the degree of freedom of the Student’s t-distribution,denotes the latent representation of the-th normal prototype. To ensure consistency and stability during training, both the normal prototypeand the latent representationare normalized before calculating the similarity.

In our anomaly detection framework, the weights of unlabeled samples are crucial for adjusting the influence of each sample during the training process.
The weights, which represent the probability of unlabeled samples being normal, help to mitigate the contamination effect of anomalies within unlabeled data.
We define the weightfor an unlabeled samplebased on its maximum similarity to any normal prototype:

whereis the sigmoid activation function, andis a scaling parameter that adjusts the sensitivity of the similarity scores.

Unlabeled samples with higher similarity to normal prototypes are assigned greater weights, increasing their influence in the loss function associated with unlabeled data and focusing the model on preserving the characteristics of normal samples. Conversely, unlabeled samples with lower similarity scores, likely to be anomalies, receive smaller weights, reducing their impact on the model’s learning process. This strategy effectively minimizes the adverse effects of anomaly contamination, ensuring the model remains sensitive to the features of normal behavior while suppressing the noise introduced by unlabeled anomalies.

To enhance the accuracy of anomaly detection, we propose to construct multiple normal prototypes to capture the diversity within normal data. However, initial prototypes may not fully represent the underlying structure of the normal data. Therefore, we refine these prototypes through a two-step process.

First, recognizing that the relationship between normal samples and multiple normal prototypes aligns with an unsupervised clustering problem,
we employ Deep Embedding Clustering[35]to simultaneously learn feature representations and cluster assignments. This process involves optimizing a KL divergence loss for unlabeled samples:

where

This loss function encourages the prototypes to align closely with the central tendencies of the clusters formed by the normal data, ensuring that the learned prototypes accurately reflect the true structure of the normal data distribution.

Second, to solidify the distinction between normal data and anomalous data, we introduce a contrastive learning approach. Our goal is to ensure that normal samples exhibit high similarity to at least one of the refined normal prototypes while maintaining low similarity for anomalous samples across all prototypes. This separation is enforced through a contrastive loss function:

whereensures that unlabeled anomalies do not achieve high similarity to any normal prototypes.
Through this contrastive learning process, normal samples are encouraged to cluster around one of the central prototypes in the latent space, while anomalous samples are pushed to the periphery, far from any normal prototype. This method ensures a clear and robust separation between normal and anomalous data, significantly enhancing the effectiveness of the anomaly detection framework.

Finally, we combine the Clustering Loss (8) with Contrastive Loss (9) to get the overall loss for this module:

SECTION: III-EUnified Anomaly Scoring Module

To detect anomalies with consideration of both the reconstruction errorand the multi-normal prototype information, we design a unified anomaly scoring module. Inspired by the residual network[36], we also take the latent representationof the given sampleas the input. In summary, the concatenated vector forms the input to the unified anomaly score evaluatorto get an anomaly score:

whereis the maximum similarity to any normal prototype,denotes the trainable parameters of the unified anomaly score evaluator, which is composed of a multi-layer perceptron with a sigmoid activation function at the final layer.

We aim for the anomaly scores to be close to 1 for anomalous samples and close to 0 for normal samples. Thus, we design the following loss function:

where the weightsare applied to mitigate the influence of anomalies in the unlabeled data, focusing the model’s learning on the more reliable, normal-like samples.

SECTION: III-FTraining and Inference

Following[37], we pre-train an AutoEncoder on unlabeled data, initially using denoising autoencoders for layer-wise training. After this, we fine-tune the entire AutoEncoder by minimizing reconstruction loss to refine the latent representations. Once fine-tuned, the encoder is used to extract latent representations of the unlabeled data. These representations are then clustered using k-means to determinecluster centers, which are used to initialize the normal prototypes.

Our model is trained in an end-to-end manner using mini-batches consisting ofunlabeled samples andlabeled anomalies. For each mini-batch, we compute three losses:,, and, as defined by Equation (3), (10), and (12), respectively. To effectively balance these three losses dynamically, we utilize a technique known as Dynamic Averaging, inspired by[38]. The total loss is calculated as follows:

where,, andare updated per epoch based on the rate of descent of the corresponding losses.

All parametersare jointly optimized with respect to the total loss ensuring a comprehensive and integrated training process for the model.

During the inference phase, each data instanceis passed through the Reconstruction Learning Module to obtain its latent representationand reconstruction error. The Multi-Normal Prototypes Learning Module then calculates the similarity between the latent representation and the normal prototypes, selecting the maximum similarity score. The three features are fed into the anomaly score evaluatorto compute an anomaly score for each instance.

SECTION: IVExperiments

SECTION: IV-AExperimental Setup

This section outlines the datasets used in our experiments, covering a broad range of scenarios across domains like network security, Internet of Thing (IoT) security, financial fraud detection, and others. All 15 datasets are summarized in TableI, providing robust benchmarks for evaluating anomaly detection methods with their diverse scenarios and varying anomaly ratios.
The datasets include:

UNSW-NB15: Network traffic records with various attack types such as DoS, Reconnaissance, Backdoor, and Analysis[39].

Aposemat IoT-23: Network traffic data from IoT devices, capturing both Command and Control (C&C) activities, direct attacks, DDoS, and Okiru[40].

Credit Card Fraud Detection: Identifies fraudulent transactions[15].

Vehicle Claims and Vehicle Insurance: Detects anomalies in vehicle-related insurance claims[41].

MAGIC Gamma Telescope: Classifies high-energy gamma particles in physics and chemistry[42].

Census Income: Predicts high-income individuals[43].

Pendigits: A pen-based handwritten digit recognition dataset[44].

Bank Marketing: Targets customer subscription predictions[45].

To construct a comprehensive comparison, we adopt several different types of baseline methods. These algorithms are grouped into two categories based on their supervision level: 1)Weakly supervised algorithms, such as RoSAS[17], PReNet[15], DevNet[16], DeepSAD[19], and FeaWAD[21], leverage limited labeled data alongside a larger set of unlabeled data, offering a direct comparison to our approach; 2)Unsupervised algorithmslike DeepIForest[46], DeepSVDD[18], and iForest[47], operate without any labeled data, offering a baseline to assess the benefits of incorporating even minimal labeled data in anomaly detection.
By comparing our method against these robust and diverse algorithms, we aim to demonstrate its effectiveness and flexibility in detecting anomalies with minimal labeled data.

The same as previous studies[17,15,16], we primarily use the Area Under the Precision-Recall Curve (AUC-PR) as our main evaluation metric, supplemented by the Area Under the Receiver Operating Characteristic Curve (AUC-ROC). The AUC-PR is particularly effective for imbalanced datasets, emphasizing the model’s ability to correctly identify anomalies (true positives) while minimizing false positives and false negatives, making it ideal for anomaly detection where anomalies are rare. The AUC-ROC provides an aggregate measure of the model’s performance across all classification thresholds, illustrating its ability to distinguish between positive and negative classes. These two metrics both range from 0 to 1 and higher values indicate better performance. To ensure robust and reliable results, we report the average performance over 5 independent runs and employ the paired Wilcoxon signed-rank test[48]to statistically validate the significance of our findings.

We use the Adam optimizer with a learning rate ofand weight decay of. The latent representation dimensionis set to 8, and the hyperparameters are configured as follows: batch sizes,; margin; Student’s t-distribution degree of freedom; and similarity scaling parameter. The number of normal prototypes k is determined via clustering analysis on the unlabeled samples. The implementation of the compared methods is primarily based on the DeepOD framework[46]andscikit-learn(sklearn).

SECTION: IV-BPerformance Comparison

The same as in[1], we evaluate the performance of our proposed anomaly detection method across multiple datasets with a fixed labeled anomaly ratio of 1.0% for each dataset. The labeled anomaly ratio is defined as the proportion of anomalies in the training dataset that are labeled.
This means that in practical anomaly detection scenarios, we only need to label anomalies included in 1.0% of the training data, ensuring that the weakly supervised anomaly detection setup does not affect the proportion of anomalies in the training set regardless of the inherent anomaly ratio of the dataset.

TableIIshows the performance of different methods on the fifteen benchmark datasets. Our method consistently outperformed other approaches, achieving the highest average scores in both AUC-PR and AUC-ROC metrics, with statistically significant improvements with over 99.9% confidence. Specifically, in terms of AUC-PR, our method achieved the best results on 13 of the 15 datasets, and was the second-best on the remaining two, closely approaching the top results. The average AUC-PR of our method was 0.642, which is significantly higher than other methods. Similarly, for the AUC-ROC metric, our method achieved an average of 0.886, surpassing all other methods. The statistical validation further confirmed that the improvements brought by our method are significant, providing robust and reliable anomaly detection performance in weakly supervised settings.

Besides, the weakly supervised algorithms, such as our method, RoSAS, and DevNet, perform significantly better than all unsupervised algorithms on almost all datasets. This confirms that with the appropriate use of a few labeled anomaly samples, we can get better anomaly detection performance.

SECTION: IV-CEffects of More or Less Labeled Anomalies

This experiment aims to examine the effects of Labeled Anomaly Ratios on the model’s effectiveness in detecting anomalies, helping us understand the trade-off between supervision level and detection performance.
The Labeled Anomaly Ratios refer to the proportion of labeled anomaly samples within the dataset.
A higher ratio implies more known information about anomalies, bringing the learning process closer to a supervised setting and potentially improving the model’s anomaly detection capabilities, albeit at a higher labeling cost. Conversely, a lower ratio indicates less labeled information and lower labeling costs but challenges the model’s detection abilities due to reduced guidance. Because AUC-ROC and AUC-PR show almost consistent results, we only present AUC-PR results here. Additionally, due to space limitations, we only showcase a subset of datasets. In Fig.3, theaxis represents Labeled Anomaly Ratios ranging from 0.5% to 20%, and theaxis represents the AUC-PR values for each dataset across all methods (except for unsupervised methods, as they are not affected by the labeled anomaly ratios).

From Fig.3, it is evident that our method consistently achieves the best performance across nearly all Labeled Anomaly Ratios. Specifically, as the Labeled Anomaly Ratios decrease, our method’s performance degrades more slowly and maintains relatively high detection performance, while other methods exhibit a significant decline in most datasets. This indicates that our method does not overly rely on the small fraction of labeled anomalies and effectively utilizes the modeling of multiple normal prototypes in a large amount of unlabeled data.

On the other hand, from the perspective of anomaly contamination, as the proportion of labeled anomalies decreases, the proportion of unlabeled anomalies contaminating the unlabeled data increases. Fig.3shows that, unlike other methods, our method’s detection performance declines more slowly as the anomaly contamination ratio increases, maintaining a relatively good detection performance. This demonstrates the robustness of our model against anomaly contamination. This robustness is attributed to the model’s design, which estimates the likelihood of each unlabeled sample being normal and uses this estimation as a weight to reduce the impact of contaminating anomalies.

SECTION: IV-DDetection Performance on Unseen Anomalies

A crucial aspect of an effective anomaly detection system is its ability to generalize from known anomalies to detect previously unseen ones. This capability is essential for maintaining robustness in dynamic environments where new types of anomalies may emerge. To evaluate our method’s performance on unseen anomalies, we constructed several experimental datasets using the UNSW-NB15 dataset, which contains four types of anomalies: DoS, Rec, Ana, and Bac. In each experiment, two types of anomalies were selected: one used exclusively in the test set (unseen) and the other included in the training and validation sets (seen).
Consistent with our weakly supervised anomaly detection settings, the labeled anomaly ratio in the training set was maintained at 1.0%. This setup resulted in 12 different experimental datasets.

The results are presented in TableIII, showing the AUC-PR scores for different methods across various combinations of seen and unseen anomaly types.
Our method achieved the highest average AUC-PR of 0.765, significantly outperforming other methods by more than 20%. The statistical confidence level for this improvement exceeds 99.0%.
This experiment demonstrates that our method can effectively generalize from known anomalies to detect previously unseen ones, maintaining strong detection performance and robustness in dynamic environments.
The success of our approach may be attributed to its ability to maintain multiple normal prototypes, which better represent the diverse nature of normal behavior. This focus on distinguishing data points based on their contrast with the prototypes, rather than relying exclusively on previously limited labeled anomalies, likely contributes to the observed improvements.

SECTION: IV-EAblation Study

We conducted an ablation study to evaluate the contribution of each component in our model. TableIVsummarizes the results across 15 datasets. The full model achieves the highest average AUC-PR of, demonstrating the effectiveness of the complete framework. Below, we analyze the impact of removing each component in order.

w/o Pretraining:Removing the pretraining step generally decreases performance, as it negatively impacts the model initialization. Overall, the effect of pretraining is more pronounced in smaller datasets, while its impact diminishes as dataset size increases. For instance, in the relatively small Aposemat IoT-23 datasets such as DDoS () and Okiru (), pretraining significantly boosts performance. In contrast, larger datasets such as UNSW-NB15 DoS () show a relatively smaller degradation when pretraining is removed. These results indicate that pretraining provides essential initialization benefits, particularly for datasets with limited size. Although the degradation is moderate, pretraining proves beneficial across diverse scenarios, enhancing the robustness of the model.

w/o:The reconstruction lossis crucial for learning representations of normal data. Removing this component results in significant performance degradation in datasets like Ana () and C&C (). These results demonstrate that reconstruction loss helps model normal patterns and separate them from anomalies effectively. However, its absence has a smaller effect on datasets like Rec, where the AUC-PR remains stable ().

w/o Decoder:Removing the decoder (and thus the reconstruction error) shows varied effects. In the Okiru dataset, the AUC-PR remains unchanged (), suggesting that the decoder plays a less significant role for simpler data structures. However, on the Fraud dataset, the AUC-PR drops substantially (), highlighting its importance for datasets with highly imbalanced distributions.

w/o:The sample weighting mechanismis designed to dynamically adjust the contributions of samples, particularly to address noise, imbalance, and ambiguity in the training process. Its removal results in varying levels of degradation depending on the dataset characteristics. For example, in the Okiru dataset (), the absence ofleads to a substantial decline, highlighting its role in filtering noisy or ambiguous samples. Even in datasets with moderate imbalance, such as Rec (), removingreduces performance, demonstrating its utility in enhancing the model’s robustness.

w/o MNP:The exclusion of multi-normal prototypes (MNP) leads to the most significant performance degradation across almost all datasets, confirming its critical role in the framework. Notably, the Attack () and DDoS () datasets exhibit the largest drops, further demonstrating that single-prototype representations fail to capture the multi-modal nature of normal data.

SECTION: IV-FImpact of Normal Prototypes: Empirical and Synthetic Analysis

In this section, we examine the effect of varying the number of normal prototypes through both empirical studies on real-world datasets and controlled experiments on synthetic data.

As illustrated in Fig.4, which presents results on four representative real-world datasets, the number of prototypes significantly influences the anomaly detection performance. For most datasets, using multiple prototypes yields better results, supporting our hypothesis that normal data often exhibits multi-modal characteristics. In the Rec dataset, the prototype count has minimal effect, suggesting lower sensitivity to this parameter. However, in the C&C and Census datasets, the number of prototypes plays a critical role—both too many and too few prototypes lead to substantial performance degradation. In the Bank dataset, despite fluctuations in performance with varying prototypes, the results consistently surpass those achieved with a single prototype.

To further validate the multi-modal nature of normal data and the effectiveness of multiple prototypes, we conducted experiments on a synthetic dataset designed with explicit multi-modal characteristics. The synthetic dataset consists of three Gaussian clusters representing normal samples and a small number of uniformly distributed anomalies, as shown in Fig.5(a). We compare the anomaly detection results using multi-normal prototypes and a single normal prototype. Fig.5(b) demonstrates the effectiveness of multi-normal prototypes in accurately capturing the three distinct modes of the normal data distribution. The model assigns low anomaly scores to normal samples within their respective clusters while effectively identifying anomalies scattered across the feature space. In contrast, Fig.5(c) shows the results with a single normal prototype. Here, the model fails to adapt to the multi-modal distribution of the normal data, resulting in higher anomaly scores even for normal samples located farther from the single prototype. Additionally, anomalies positioned in the central region, equidistant from the three normal modes, are often missed due to their proximity to the single prototype. This failure illustrates that a single prototype cannot adequately represent diverse normal patterns, leading to poorer anomaly detection performance.

SECTION: IV-GSensitivity Analysis

We assess the model’s sensitivity to three key hyperparameters: the latent representation dimension, the margin, and the scaling parameter. As shown in Fig.6, we analyze the trends by selecting four representative datasets (Rec, C&C, Census, and Bank).

For, the model shows relative stability across different values, withproviding a good trade-off between performance and computational complexity. Increasingbeyond 8 does not result in significant performance gains and may introduce unnecessary computational overhead. The consistent performance acrossvalues highlights the model’s ability to effectively capture the underlying data structure even with relatively small latent dimensions.

For, a margin value of approximatelyachieves the best performance across all datasets. Smaller margins, such as, fail to adequately separate the reconstruction losses of normal and anomalous samples, leading to reduced detection accuracy. On the other hand, larger margins, particularlyand, hinder the model’s capacity to learn anomaly-specific latent representations, resulting in substantial performance degradation.

For, which controls the scaling of dynamic weights, the optimal value is around. Smaller values (e.g.,) reduce the effect of dynamic weighting, limiting the model’s ability to suppress noisy samples, while larger values (e.g.,) overemphasize specific samples, potentially causing overfitting or instability. The improvements observed arounddemonstrate its critical role in balancing sample contributions effectively during training.

SECTION: VConclusions

In conclusion, our proposed anomaly detection method demonstrates significant improvements over existing techniques by effectively leveraging a small amount of labeled anomaly data alongside a large amount of unlabeled data. Our method’s robustness to anomaly contamination and its ability to generalize to unseen anomalies make it highly suitable for real-world applications across various domains, including network security[6], financial fraud detection[7], and medical diagnostics[9]. Future work could explore extending our method to other types of data, such as images, to further validate its versatility and effectiveness. For the reconstruction learning component, we could also consider using more advanced variants like Variational AutoEncoders (VAEs)[49]to enhance feature extraction capabilities.

SECTION: References