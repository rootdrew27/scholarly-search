SECTION: On the impossibility of discoveringa formula for primes using AI

The present work explores the theoretical limits of Machine Learning (ML) within the framework of Kolmogorov’s theory of Algorithmic Probability, which clarifies the notion of entropy as Expected Kolmogorov Complexity and formalizes other fundamental concepts such as Occam’s razor via Levin’s Universal Distribution. As a fundamental application, we develop Maximum Entropy methods that allow us to derive the Erdős–Kac Law and Hardy–Ramanujan theorem in Probabilistic Number Theory, and establish the impossibility of discovering a formula for primes using Machine Learning via the Prime Coding Theorem.

God made the integers; all else is the work of man.

Leopold Kronecker

SECTION: 1Compressing human concepts

You meet a French mathematician at the Reykjavik airport with a million things on his mind but
at any moment he is only thinking of one particular topic. Assuming that this list of concepts is known a priori, what is the minimum number of binary questions, asked in sequential order, that you would need to determine what he is thinking about? In the worst case,

So we might as well play a game ofquestions. Moreover, the popularity of this game suggests that any human concept may be described using at mostbits of information. If we may solve this particular inductive problem, might it be possible to solve the general problem of scientific induction?

SECTION: 2Kolmogorov’s theory of Algorithmic Probability

Using Kolmogorov’s theory of Algorithmic Probability, we may apply Occam’s razor to any problem of scientific induction including the
sequential game ofquestions. However, it is easy to forget that this requires overcoming a seemingly insurmountable scientific obstacle which dates back to von Neumann:

“My greatest concern was what to call it. I thought of calling it ’information,’ but the word was overly used, so I decided to call it ’uncertainty.’ When I discussed it with John von Neumann, he had a better idea. Von Neumann told me, ’You should call it entropy, for two reasons. In the first place your uncertainty function has been used in statistical mechanics under that name, so it already has a name. In the second place, and more important, no one really knows what entropy really is, so in a debate you will always have the advantage.” – Claude Shannon

This was accomplished through an ingenious combination of Shannon’s Theory of Communication with Alan Turing’s Theory of Computation. What emerged is the most powerful generalisation of Shannon’s theory for algorithmics and coding theory, so Kolmogorov Complexity and Shannon Entropy share the same units, and Kolmogorov Complexity elucidates the Shannon Entropy of a random variable as its Expected Description Length. Furthermore, assuming that the Physical Church-Turing thesis is true, Kolmogorov’s theory of Algorithmic Probability formalizes Occam’s razor as it is applied in the natural sciences.

In the case of our game, we may formulate Occam’s razor using the four fundamental theorems of Algorithmic Probability before approximating Kolmogorov Complexity, which is limit–computable,
using Huffman Coding in order to solve the game ofquestions.

The implicit assumption here is that the second player is able to encode the knowledge of the first player because both players share a similar education and culture.

SECTION: 3Fundamental Theorems of Algorithmic Probability

These are known results[5], however we provide all proofs for completeness in the Appendix.

SECTION: 3.1Kolmogorov’s Invariance Theorem

Letbe a Turing–complete language that is used to simulate a universal Turing machine. Letbe an input ofthat produces the binary string. Then theKolmogorov Complexity(orMinimal Description Length) ofis defined as

wheredenotes the output ofon input.

Kolmogorov’s Invariance Theorem states that the above definition is asymptotically invariant to the choice of. Namely, any other Turing–complete language (or, equivalently, another universal Turing machine)satisfies

That is,

for some positive constantthat depends only onand.

Interpretation:

The minimal descriptionsuch thatserves as a natural representation of the stringrelative to the Turing–complete language. While it may be demonstrated thatand therefore, is not computable, the fourth fundamental theorem asserts that the Expected Kolmogorov Complexity is asymptotically equal to the Shannon entropy of a random variable. Hence, Kolmogorov Complexity is computableon average.

SECTION: 3.2Levin’s Universal Distribution

TheAlgorithmic Probabilityof a binary stringmay be defined as the probability ofbeing generated byon random input, whereis a binary string generated by fair coin flips:

However, this measure is not guaranteed to converge: we can choose one such inputand use it as a prefix for somethat is aboutbits longer thanand such thatproduces the same binary string:. Then we find that:

forfrom any subset of integers. Thus, we can’t guarantee thatconverges.

Levin’s idea effectively formalizes Occam’s razor: we need to consider prefix–free Turing–complete languages only. Such languages are easy to imagine: if we agree that all documents end with an instruction that cannot appear anywhere else, then we have a prefix–free language.

Given that any prefix–free code is uniquely-decodable, it satisfies the Kraft-McMillan inequality. Thus, we obtain Levin’s Universal Distribution:

where from hereon we considerto be prefix–free, andnow corresponds toprefix–free Kolmogorov Complexity.

SECTION: 3.3Levin’s Coding Theorem

In the setting of prefix–free Kolmogorov complexity, Levin’s Coding theorem states that

Hence,

Interpretation:

Relative to a prefix–free Turing–complete language(or, equivalently, a universal prefix–free Turing machine), the number of fair coin flips required to generate the shortest program that outputsis on the order of. Thus, from a frequentist perspective, the entropy of the Universal ’a priori’ Probability that we observe the eventhas anormal orderof:

Though we can’t estimate Kolmogorov Complexity, we may use Lemma 7.2 to approximate the normal order of Kolmogorov Complexity. As an immediate consequence, we may estimate the normal order of Algorithmic Probability.
Hence, we may reliably evaluate the Expected Kolmogorov Complexity of a random variable although Kolmogorov Complexity is not computable.

It follows that Levin’s Coding theorem allows us to formalize the notion ofentropy of an event.

SECTION: 3.4Maximum Entropy via Occam’s razor

Given a discrete random variablewith computable probability distribution, it holds that

whereis the Shannon Entropy ofin base.

Interpretation:

The Shannon Entropy of a random variable in baseis asymptotically equal to Expected Kolmogorov Complexity, which provides us with a precise answer to Von Neumann’s original question. This theorem also allows us to assert that Kolmogorov Complexity is measurable on average where the expectation is calculated relative to Levin’s Universal Distribution.

Moreover, as an immediate consequence, we may deduce the Principle of Maximum Entropy. This follows first from the equivalence of Shannon’s Source Coding theorem with the Asymptotic Equipartition Theorem, which we use to prove this fundamental theorem. Second, the Principle of Maximum Entropy that refers to constrained optimisation methods for estimating the source distribution from data is a specific application of Shannon’s Source Coding theorem. Thus, machine learning systems that minimise the–Divergence are implicitly applying Occam’s razor.

But, what exactly do we mean by random variable? In a computable Universe the sample space of a random variablerepresents the state–space of a Turing Machine with unknown dynamics whose output sequence is computable. As the generated sequence is computable, it is finite–state incompressible in the worst-case i.e. a normal number. Hence, a random variable corresponds to a stochastic source that is finite–state random.

This definition comes from a well–known correspondence between finite–state machines and normal numbers that establishes that a sequence is normal if and only if there is no finite–state machine that accepts it.

SECTION: 4Preliminaries: the Game of 20 Questions

1.The Game ofQuestions is played between Alice and Bob who are both assumed to be trustworthy and rational. Thus, Alice and Bob both perform sampling and inference using Levin’s Universal Distributionover a shared alphabet.

2.For the sake of convenience, we shall assume thatrepresents entries in the
Britannica Encyclopedia and.

3.Bob selects an objectand Alice determines the object by asking binary “yes / no”
questions in a sequential manner, encoded using a prefix–free code, sampled from.

4.Alice’s goal is to minimize the expected number of questions which is equivalent to determine ansuch that

5.In this setting, the Shannon Entropy may be understood as a measure of hidden information and we shall show that (*) has a solution using Huffman Coding.

SECTION: 4.1An approximation to the Universal Distribution

Input:

An alphabetof symbols and a discrete probability distributionwithequal to the frequency of.

Output:

A prefix–free codewhereis the codeword for.

Goal:

Let the loss function be, that is the weighted length of code. We want to solve the minimization problem:

If the Turing–complete languageis prefix–free, we obtain:

where.

We may obtain an approximation to the Expected Kolmogorov Complexity, the lower-bound, via an entropy coding method such as Huffman Coding. This yields the desired prefix–free code.

SECTION: 4.2Huffman Coding

1.The technique works by creating a binary tree of nodes where leaf nodes represent the actual bytes in the input data.

2.A node may be either a leaf node or an internal node.

3.Initially, all nodes are leaf nodes each of which represents a symbol and its frequency.

4.Internal nodes represent links to two child nodes and the sum of their frequencies.

5.As a convention, bit ’0’ represents following the left child and bit ’1’ represents following the right child.

The simplest coding algorithm uses a priority queue where the node with the lowest probability is given the highest priority:

Create a leaf node for each symbol and add it to the priority queue.

While there is more than one node in the queue:

Remove the two nodes of highest priority (i.e. lowest probability) from the queue.

Create a new internal node with these two nodes as children and with probability equal to the sum of the two nodes’ probabilities.

Add the new node to the queue.

The remaining node is the root node and the tree is complete.

Assuming that the nodes are already sorted, the time complexity of this algorithm is.

SECTION: 4.3Discussion

Does the solution found via Huffman Coding agree with our intuitions?

Assuming that internal nodes are given labelswhile leaf nodes are given labelsthe information gained from any sequence of questionsmay be determined from the entropy formula

where the order of the internal nodes may be determined by sorting the verticeswith respect to their–probabilities. In principle, children of a parent node represent refinements of a particular concept, so the tree depth represents our depth of understanding. This degree of understanding may be measured in terms of the entropy. Hence, we have a satisfactory solution to the Game ofquestions.

Zooming out, we may consider the ultimate impact of Kolmogorov’s formalisation of scientific induction which Kolmogorov foretold[3]:

Using his brain, as given by the Lord, a mathematician may not be interested in the combinatorial basis of his work. But the artificial intellect of machines must be created by man, and man has to plunge into the indispensable combinatorial mathematics. – Kolmogorov (1983)

In fact, Kolmogorov’s theory of Algorithmic Probability may be viewed as a theory of machine epistemology. As for what may potentially limit the scope of machine epistemology relative to human epistemology, thebig questionssection of[2]may shed some light.

SECTION: 5Acknowledgements

We would like to thank Anders Södergren, Ioannis Kontoyiannis, Hector Zenil, Steve Brunton, Marcus Hutter, Cristian Calude, and Igor Rivin for constructive feedback in the preparation of this manuscript.

SECTION: 6Maximum Entropy methods for Probabilistic Number Theory

SECTION: 6.1The Erdős-Euclid theorem

In essence, this proof demonstrates that the information content of finitely many primes is insufficient to generate all the integers. Originally due to Kontoyiannis[4].

Letbe the number of primes that are less or equal to a given natural number. Let us suppose that the set of primesis finite so we havewhereis constant forbig enough. Then we can define a uniform integer–valued random variable, such that

for some integer–valued random variablesand, such thatis square–free. In particular, as we know that, the upper bound for Shannon’s Entropy from Jensen’s inequality implies:

Also, sinceis a binary variable, we have.

Using Kolmogorov’s definition of Entropy, we obtain the asymptotic relation for the typical code length:

and we may deduce the following inequality:

which implies:

This clearly contradicts the assumption thatis a constant for any natural, and provides us with an effective lower bound on the prime counting function.

SECTION: 6.2Cheybshev’s theorem via Algorithmic Probability

An information-theoretic derivation of Chebyshev’s theorem (1852), an important precursor of the Prime Number Theorem, from the Maximum Entropy Principle. Another proof was given by Ioannis Kontoyiannis in[4].

Chebyshev’s Theorem:

We rediscover Chebyshev’s theorem:

which tells us that the expected information gained from observing a prime number in the intervalis on the order of.

Proof:

For an integer sampled uniformly from the intervalwe may define its random prime factorization in terms of the random variables:

As we have no prior information about, it has the maximum entropy distribution among all possible distributions on.

While the Kolmogorov Complexity ofis not computable, we may calculate its Expected Kolmogorov Complexity using Corollary7.3which tells us that almost all integers are incompressible:

On the other hand,

By combining (3) and (4), we find:

We also know that the geometric distribution maximizes the entropy ofunder the condition thatis fixed[1, Theorem 12.1.1]. Hence,

Thus, we rediscover Chebyshev’s theorem:

where this entropy formula is invariant to the choice of base of the logarithm.

As an important corollary, we may deduce that the base-e entropy of a typical prime number in the intervalis on the order of. We may begin by noting that the event thatis prime is given by the superposition of the eventof which there aredistinct possibilities and the null eventwhich is unique. Hence, due to asymptotic independence we have:

where

and

Thereforewhich tells us that a typical prime number in the intervalbehaves as if any location in the intervalis a priori equiprobable.

SECTION: 6.3The Prime Coding Theorem

In the following analysis, we consider an information-theoretic dual to the Prime Number Theorem via Chebyshev’s theorem.

We define theprime encodingwhereifandotherwise, as being the empirical realisation of a sequence of independent binary random variables. From an information-theoretic perspective, Chebyshev’s theorem states that the average code length of a prime number in the intervalis given by:

While Chebyshev’s theorem is invariant to the choice of base of the logarithm, it tells us that the prime numbers are arranged uniformly inso from eachwe may expect one bit of information:

Furthermore, as each prime number contributesbits of information on average, in the limit of lossless compression Shannon’s Source Coding theorem tells us that the expected code length foris given by:

Finally, using the Prime Number Theoremwe may deduce the Prime Coding theorem:

so the natural base is optimal for prime counting and we may conclude that the locations of all primes inare statistically independent of each other.

It follows that a machine learning model may not be reliably used to predict the location of the–th prime number given prior knowledge of the location of theprevious primes.

In consequence, no prime formula may be approximated using Machine Learning. In particular, Riemann’s Explicit formula for prime counting:

is not learnable. Hereis the Möbius function,is the logarithmic integral function, andindexes every zero of the Riemann zeta function.

SECTION: 6.4The empirical density of the primes and their source distribution

The Shannon Source Coding theorem informs us that the uniform source distribution is the correct generative model for all prime numbers.

To clarify the Prime Coding Theorem in terms of random variables:

we must clarify the relation between the empirical density of primes and their source distribution of which they are a unique realisation.

To be precise, we may model the prime encodingas the realisation of a non-stationary sequence of uniformly distributed random variables. Thus, if the eventsare arranged uniformly in the discrete intervalthen the expected information gained from observing all events inis given by:

Furthermore, if we consider that each event is unique sois a unit fraction then we ought to model the sum of entropies:

and to further our analysis we may define the Lagrangian function:

Thus, we find that:

and in order to satisfy the asymptotic formula (1), we find that:

Therefore, the empirical density of primes which is described by the Prime Number Theorem:

is the natural consequence of a uniform source distribution. Moreover, as this is the simplest correct model for generating the prime numbers, Occam’s razor also informs us that it is the correct generative model for all prime numbers.

SECTION: 6.5Information-theoretic derivation of the Prime Number Theorem

An information-theoretic derivation of the Prime Number Theorem via Occam’s Razor.

If we know nothing about the distribution of primes, in the worst case we may
assume that each prime less than or equal tois drawn uniformly from. So our source of primes is:

whereis the Shannon entropy of the uniform distribution.

Now, we may define the prime encoding ofas the binary sequencewhereifis prime andotherwise. With no prior knowledge, given that each integer is either prime or not prime, we havepossible prime encodings in. As almost all binary strings of lengthare incompressible, the normal order ofmust satisfy:

Moreover, if there areprimes less than or equal tothen the average number of bits per arrangement gives us the average amount of information gained from correctly identifying each prime inas:

as there aredistinct ways to sample uniformly fromand a frequency ofassociated with the event that.

In light of the last three arguments and Shannon’s Noiseless Coding Theorem (a.k.a. Shannon’s Source Coding Theorem), we may deduce thatmust also satisfy the asymptotic relation:

as the locations of the prime numbers are necessary and sufficient to encode.

Rearranging the last asymptotic relation, we rediscover the Prime Number Theorem:

Furthermore, this derivation of the empirical density which we may observe (5) via the source distribution which is not directly observable (1) indicates that the prime numbers are empirically distributed as if they were arranged uniformly.

This generative model implies that independently of the amount of training data and computational resources at their disposal, if the best machine learning model predicts the nextprimes to be atthen for largethis model’s statistical performance will converge to a true positive rate that is no better than:

Hence, the true positive rate for any machine learning model converges to zero.

SECTION: 6.6An information-theoretic derivation of the Erdős–Kac theorem

The Algorithmic Probability of a Prime Factor

Given the integerwith random prime factorisation:

we may define the Algorithmic Probability of the eventusing the Prime Coding Theorem:

From (2) and (3), we derive the Algorithmic Probability of the eventfor large primes:

Likewise, the event that any two distinct primesare simultaneously observed occurs with Algorithmic Probability:

as formulas (2) and (3) tell us that any two prime numbersare statistically independent.

The Expected number of Unique Prime Divisors

For any integer, we may define its number of Unique Prime Divisorswhereifandotherwise. Thus, we may calculate the Expectation:

where we used Mertens’ Second theorem.

The Standard Deviation of

As the random variablesare independent, the variance ofis linear in:

since.

The Erdős–Kac theorem

In order to prove the Erdős–Kac theorem, it remains to show thatsatisfies the Lindeberg condition for the Central Limit Theorem:

wheredenotes the expectation value ofrestricted to outcomes.

Proof:

Given our analysis of the Algorithmic Probability of a prime factor:

where without normalising we may observe that:

as we are a priori guaranteedbits of information from determining the distinct prime factors of. Normalising, so the frequency distribution is dimensionless:

Given this normalised distribution, we may evaluate the expectation:

which is greater than or equal in value tofor anysince. It follows that for largethis expression simplifies to:

and therefore the Lindeberg criterion is satisfied for any:

Thus, we may conclude that:

converges to the standard normal distributionas.

Discussion:

This theorem is of great interest to the broader mathematical community as it
is impossible to guess from empirical observations. In fact, it is far from certain that Erdős and Kac would have proved the Erdős–Kac theorem if its precursor, the Hardy–Ramanujan theorem, was not first discovered.

More generally, at a time of Big Data and the imminent supremacy of AI, this
theorem forces the issue of determining how some scientists were able to formulate correct theories based on zero empirical evidence. While the
Erdős-Kac theorem has the form of a statistical observation, the normal order
ofonly begins to emerge forwhere.

Within the current scientific paradigm, non-trivial scientific discoveries of this kind that are provably beyond the scope of scientific induction(and hence machine learning) do not yet have an adequate explanation.

SECTION: 6.7The Hardy–Ramanujan theorem

The Hardy–Ramanujan theorem states that, given any, almost all integers satisfy:

It follows that, which measures the number of distinct prime factors of, has normal order.

Proof:

Given the random variablewith particular realisation, we may encode the distinct prime factors ofusing the prime encoding:

On the other hand,measures the information gained from observing the distinct prime factors ofsince:

This means that exactlybinary questions asked in sequential order are necessary and sufficient to identify. Hence, using Kolmogorov’s Invariance theorem:

As each prime factor ofcontributes exactly one bit of information, the Expected Kolmogorov Complexity ofis asymptotically:

Moreover, we may note that ifdenotes the Algorithmic Probability of the eventwhere,

where (5) follows from Levin’s Coding theorem:

Furthermore, given the asymptotic relation:

we may apply the Asymptotic Equipartition Theorem.

In particular, we may observe that for largeeachis sampled i.i.d. from the finite set of possible prime encodingsand the typical set satisfies:

As an immediate consequence, we may apply the Asymptotic Equipartition Property(AEP) to a typical prime encodingwhich yields:

whereasdue to the Law of Large Numbers.

Finally, the AEP informs us that elements of the typical sethave the same asymptotic probability. Hence, the asymptotic relation:

holds almost surely and we may conclude that as:

SECTION: 7Proofs of Fundamental Lemmas for Kolmogorov Complexity

Although we don’t prove these results for prefix-free languages, by
Kolmogorov’s Invariance theorem they readily generalise to prefix-free Universal
Turing Machines.

There exist algorithmically random strings.

Let us suppose that for all. Then for allthere exists somesuch thatand. Clearly, ifthen.

Observe that there areprograms of length less than, while the number of lengthstrings is. By the pigeonhole principle, if all strings of lengthhave a programsuch thatthen there must be a program that produces two different strings. Thus, we have a contradiction.
∎

Almost all finite strings are incompressible.

Letbe some non–negative constant. The number of programs of length less thanis,
which leaves us withprograms of length greater than or equal to.
∎

Almost all integers are algorithmically random.

The set of finite strings is countable so there is a bijective map fromtoand we may define the Kolmogorov Complexity as a map from integers to integers,. As almost all finite strings are incompressible, it follows that almost all integers are algorithmically random: for almost all integers.
∎

Kolmogorov Complexity is not computable.

Let us suppose there exists a programof lengthsuch that for any binary stringwe havefor some Turing–complete language. Observe that there exist algorithmically random stringsof length, and that these strings can be arranged in the lexicographic order. Let this set be called. This implies that we are able to useto determine whetheris the first string of lengththat satisfies

for any given. As the setenumerable due to the computability ofand the lexicographic order of inputs to, the–th stringthat satisfies (*) requiresbits of information so. Hence, for sufficiently largewe have a contradiction.
∎

SECTION: 8Proofs of Fundamental Theorems for Algorithmic Probability

SECTION: 8.1Proof of Kolmogorov’s Invariance theorem:

The following is taken from [5].

From the theory of compilers, it is known that for any two Turing-Complete languagesand, there exists a compilerexpressed inthat translates programs expressed ininto functionally-equivalent programs expressed in.

It follows that if we letbe the shortest program that prints a given stringthen:

where, and by symmetry we obtain the opposite inequality.

SECTION: 8.2Proof of Levin’s Universal Distribution:

This is an immediate consequence of the Kraft-McMillan inequality.

Kraft’s inequality states that given a sequence of stringsthere exists a prefix code with codewordswhereif and only if:

whereis the size of the alphabet.

Without loss of generality, let’s suppose we may order thesuch that:

Now, there exists a prefix code if and only if at each stepthere is at least one codeword to choose that does not contain any of the previouscodewords as a prefix. Due to the existence of a codeword at a previous stepcodewords are forbidden as they containas a prefix. It follows that in general a prefix code exists if and only if:

Dividing both sides by, we find:

SECTION: 8.3Expected Kolmogorov Complexity equals Shannon Entropy

Let’s suppose that i.i.d. dataare generated by sampling
from the distributionon the finite set. Then it may be demonstrated
that Expected Kolmogorov Complexity equals Shannon Entropy up to an additive constant:

Now, by definition:

whereis a Universal Distribution that holds for our particular Observable Universe so that:

and Unitarity is guaranteed through an Oracle that identifies it with the Universal Wave Function, asis a computer that simulates the Observable Universe.

Proof:

If we carefully consider the Asymptotic Equipartition Theorem,

we may define a natural measure on the atypical set:

Thus, we may observe thatso there must exist a positive exponentsuch that as:

Moreover, given thatfor:

Hence, we may deduce:

Finally, due to the Asymptotic Equipartition Theorem we may determine that:

SECTION: 8.4Corollary: Levin’s Coding theorem

If we carefully consider propositions (8) and (9), we may deduce:

Thus, we may derive Levin’s Coding theorem for i.i.d. time-series data:

which may be readily generalised to non-stationary data via the Asymptotic Equipartition Theorem.

SECTION: 8.5Gödel’s incompleteness theorem

We reproduce the proof of Gödel’s first incompleteness theorem in [5].

Definition of consistency and soundness:

We say that a formal system(definitions, axioms, rules of inference) isconsistentif no statement which can be expressed in the system can be proved to be both true and false in the system. A formal system issoundif only true statements can be proved to be true in the system. Hence, a sound formal system is consistent.

Proof:

Letbe a finite binary string of length. We say thatis c-random iffor some. We recall from Lemma 2 that the fraction of sequences that may be compressed by more thanbits is bounded by.

Now, let’s consider a sound formal systemthat is powerful enough to express the statementis c-random. Let’s supposemay be described inbits. By this we mean that there is a fixed-size program of lengthsuch that, when input the number, outputs a list of all valid proofs inof length. We claim that, for all but finitely many random stringsand, the sentence ’is c-random’ is not provable in.

Let’s suppose the contrary. Given, we may exhaustively search for a proof that a string of lengthis random, and print it when we find such a string. This procedure, to printof lengthuses onlybits of data which is much less than. However,is random due to the proof and the fact thatis sound. Hence,is not consistent, which is a contradiction.

SECTION: References