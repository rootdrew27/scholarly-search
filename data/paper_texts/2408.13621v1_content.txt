SECTION: Preliminary Investigations of a Multi-Faceted Robust and Synergistic Approach in Semiconductor Electron Micrograph Analysis: Integrating Vision Transformers with Large Language and Multimodal Models
Characterizing materials using electron micrographs is crucial in areas such as semiconductors and quantum materials. Traditional classification methods falter due to the intricate structures of these micrographs. This study introduces an innovative architecture that leverages the generative capabilities of zero-shot prompting in Large Language Models (LLMs) such as GPT-4(language only), the predictive ability of few-shot (in-context) learning in Large Multimodal Models (LMMs) such as GPT-4(V)ision, and fuses knowledge across image-based and linguistic insights for accurate nanomaterial category prediction. This comprehensive approach aims to provide a robust solution for the automated nanomaterial identification task in semiconductor manufacturing, blending performance, efficiency, and interpretability. Our method surpasses conventional approaches, offering precise nanomaterial identification and facilitating high-throughput screening.

SECTION: Introduction
Semiconductors have been the backbone of technological advancements in modern electronics, driving growth and innovation in computing and communication systems, among others. The semiconductor process comprises three main stages: (a) design and development, during which fabless firms create chip blueprints, specifying the architecture, functions, and specifications of the miniaturized chips; (b) fabrication, where specialized foundries manufacture chips by etching integrated circuits onto silicon wafers using intricate technologies; and (c) testing and assembly, during which chips undergo rigorous testing and are subsequently assembled into protective packages for integration into electronic devices. This collective effort results in the production of high-quality semiconductor components suitable for a wide range of applications. The state-of-the-art imaging and analysis methodsare indispensable in semiconductor manufacturing for the development of next-generation miniaturized chips, especially those sized at 7 nm or smaller. The pursuit of miniaturized chips below 7 nm technologies introduces a level of complexity and precision that significantly increases the risk of errors in the manufacturing process. These errors can compromise the consistency of high-quality chips and amplify the variability in chip performance, posing a substantial challenge for manufacturers aiming to produce reliable and advanced chips at this scale. The semiconductor industry utilizes various advanced electron beam tools, including scanning and transmission electron microscopy, to create images or micrographs of semiconductor materials, structures, and devices at the micro and nanoscale with high resolution and detail. These tools contribute to quality control, process monitoring, failure analysis, and materials characterization in the semiconductor industry. Automated labeling of electron micrographs, though advantageous, poses a considerable challenge due to the level of detail, complexity of patterns, and information density involved. These challenges arise primarily from the high inter-category similarity (similar-looking or indistinguishable) between different nanomaterials, high intra-category dissimilarity within nanomaterials (distinct or differing appearances), and the presence of intricate visual patterns in nanomaterials across various scales (spatial heterogeneity). The complexities of automated nanomaterial identification tasks are illustrated in Figure.

Recently, unimodal Large Language Models (LLMs) such as GPT-4 (language-only)which are pre-trained autoregressive large-scale models on extensive, diverse text corpora in unsupervised learning settings following a fundamental paradigm “prompt and predict” approach, have significantly transformed natural language processing(NLP), achieving improved performance across a wide range of NLP tasks, demonstrating better logical reasoning abilities, and generating human-like text. Zero-shot Chain of Thought(Zero-Shot CoT)and Few-Shot (In-Context) learning(Few-Shot ICL)are prompt engineering strategies for designing and crafting tailored prompts for utilizing general-purpose LLMs in specialized language-based tasks or associated new, unseen problem-solving scenarios, thereby eliminating the need for traditional task-specific fine-tuning. Zero-Shot CoT relies on customized instructions without requiring explicit task-specific demonstrations(input-output pairs), requiring the language model to generalize from the implicit knowledge acquired during training to generate the output for the downstream task. Conversely, Few-shot ICL incorporates a few guiding demonstrations to learn from analogy along with the task-centric instructions to guide LLMs to generate the output simply by conditioning on the prompts. In recent times, OpenAI’s GPT-4 with Vision (GPT-4V), which possesses the ability to process and understand images, represents a significant advancement in the domain of large multimodal models (LMMs). It is more versatile than GPT-4, as it has broken the text-only barrier of previous language models, introducing visual understanding and analysis as a new dimension. GPT-4V is designed to accept multiple modalities, including both images and text as input, and generate text outputs. GPT-4V incorporates visual processing capabilities, enabling it to analyze image inputs provided by the user in conjunction with text, thereby facilitating visual question answering. Despite its advanced capabilities, when tested on SEM imagesfor nanomaterial categorization, GPT-4V incorrectly classified them, highlighting the limitations of LMMs. Figureillustrates these limitations of GPT-4V.

Despite advances in the use of language-only LLMs such as GPT-4, LMMs like GPT-4V and other behemoths across various scientific disciplines, the synergistic integration of foundational LLMs and LMMs with computer vision algorithms in semiconductor research, particularly for the automated electron micrograph identification task, remains an underexplored approach in the evolution of intelligent semiconductor manufacturing processes. In this study, we introduce an automated nanomaterial identification framework, which is built upon cross-modal electron micrograph representation learning, referred to asfor brevity. Figureillustrates the framework. The objective is to utilize the complementary strengths of LMMs, LLMs, and small-scale language models(LMs) to establish a more robust and accurate predictive framework. Closed-source LLMs like GPT-4, while proficient in language understanding, have a black-box nature, and lack interpretability for downstream applications since they typically do not provide direct access to logits or token embeddings. In addition, their jack-of-all-trades approach makes them incredibly resource-intensive for repurposing and often poorly aligned with specialized tasks. On the other hand, open-source smaller LMs like BERTfollowing “pretraining and fine-tuning” approach, while cost-effective for repurposing through fine-tuning to align with specialized tasks and be interpretable, may fall short in terms of reasoning and generalization, often yielding less coherent and contextually relevant responses compared to LLMs. LMMs such as GPT-4Vare more potent and versatile than language-only LLMs, as they incorporate multi-sensory capabilities that combine visual and language understanding. This enables users to instruct the multimodal model to analyze image inputs alongside textual information. Consequently, it offers the ability to perform complex tasks that require an understanding of both text and visual inputs, producing output that is contextually relevant to the combined data. LMMs excel in multimodal processing, with their remarkable capabilities to analyze images, identify objects, transcribe text, and decipher data, but they grapple with challenges related to interpretability, bias, unpredictability, and resource consumption. Navigating these challenges among LLMs, LMMs, and small-scale LMs demands a fine balance between performance, efficiency, and interpretability. Our study introduces a novel approach to the automatic nanomaterial identification task, combining the strengths of LMMs, LLMs, and small-scale LMs. The main contributions of this work can be summarized as follows:

An input image is divided into patches treated as tokens, converted into 1D vectors, and enhanced with positional embeddings for location context. A classification token is added to achieve a global image representation. This token sequence is processed by a transformer architecture, specifically ViT, to identify relationships between different image regions. The output corresponding to the classification token provides a comprehensive image representation. This appraoch employs the transformers to encapsulate the entire image visual context by treating the classification token’s latent representation as an image-level embedding.

Our study leverages powerful LLMs through Language Model as a Service (LMaaS), using their text-based inference APIs without accessing the parameters and gradients of the LLMs. Compared to fine-tuning task-specific LLMs, LMaaS efficiently deploys a single versatile LLM for various tasks using task-specific prompts. It optimizes tasks without backpropagation, ensuring low costs irrespective of the model size. The open-ended natural language CoT prompts guide LLMs to generate comprehensive textual descriptions of nanomaterials, covering their structure, synthesis methods, properties, and applications. After obtaining these technical descriptions, a smaller pretrained language model (LM) is used to summarize the LLM-generated content and compute high-level text embeddings by aligning them through supervised fine-tuning for the downstream nanomaterial identification task. Furthermore, in a cross-modal alignment scenario, a scaled dot-product attention mechanism matches image embeddings with their corresponding text-level embeddings. This mechanism calculates similarity scores to determine the best text match for a given image, ensuring robust alignment between different modalities. In brief, we utilize CoT LLM prompting to extract domain-specific knowledge and obtain image-aligned (nanomaterial-specific) text-level embeddings.

Utilizing few-shot prompting can quickly adapt LMMs to perform new tasks, like nanomaterial identification, without extensive fine-tuning. By providing LMMs with a limited set of image-label pairs, these models can predict the category of unfamiliar or unseen nanomaterial images. Two strategies, random and similarity-driven sampling, influence the selection of these pairs. Instead of updating model parameters through supervised learning, this approach leverages the multimodal model’s inherent knowledge, generating prediction embeddings by solely conditioned on the prompt.

We utilize a hierarchical multi-head attention mechanism to compute a cross-modal embedding from image-level, text-level, and prediction embeddings. This robust framework effectively integrates diverse information across these modalities, producing a holistic representation that can improve nanomaterial identification tasks.

SECTION: Problem Statment
Our study focuses on the classification of electron micrographs, an inductive learning challenge where the goal is to categorize previously unobserved micrographs by leveraging a labeled dataset, represented as. We train a multi-modal encoder, defined by the non-linear transformationon the labeled dataset in the context of supervised machine learning to predict the labels () for unlabeled micrographs (). Within this context,denotes the trainable parameters, with the primary aim being to reduce the loss, which is framed as:

whererepresents the predictions from the multi-modal encoder,signifies the cross-entropy loss.

SECTION: Proposed Method
Let’s consider an input image, which is represented as a 3D tensor with dimensions, whererepresents the image’s height in pixels,represents its width in pixels, andrepresents the number of channels of each pixel within the image. We divide the image into smaller, non-overlapping regions or patches to obtain a sequence of tokens. Each patch is treated as a token and has a fixed size with spatial dimensions of, wheredenotes the patch size. The total number of patches is given by. We then linearly encode each patch, each of which has an overall size of, to flatten it into a 1D vector represented as, whereis the patch embedding dimension. To provide the model with spatial information, we add positional embeddings to each patch. These positional embeddings are learnable vectors representing the position of each patch within the image grid. They help us understand the relative positions of different patches, and we add the position embeddings element-wise to the patch embeddings. In addition, we append a classification tokento the patch sequence. This token aggregates information from all patches and provides an embedding of the entire patch sequence, creating a global representation of the entire input image. We input this augmented token sequence into ViT, which consists of multiple stacked transformer encoder layers. Each encoder layer processes the patch embeddings hierarchically, refining representations at different abstraction levels. After passing through the transformer layers, we consider only the output embeddingcorresponding to thetoken as the representation of the entire image. This embedding aggregates information from all the patches and summarizes it. In summary: (a) We split the input image into patches, flatten them into 1D vectors, augment them with positional embeddings to provide spatial information, and include a classification token. (b) We process the resulting sequence of patch embeddings using a transformer-based architecture to capture long-range dependencies and relationships between different regions of the electron micrograph.

We access LLMs through the LMaaShosted by the cloud-based service provider, which provides a text-based black-box API interaction to send text inputs and receive generated text without access to the underlying model parameters or gradients or the model’s internal mechanisms. We utilize open-ended natural language prompts, designed to be flexible and non-restrictive, to instruct LLMs to generate the detailed technical descriptions related to various aspects of nanomaterials, including their structure, properties, and applications. We query LMMs to generate detailed descriptions of nanomaterials by employing a customized prompt template for zero-shot generation tasks, steering them through CoT prompts in a zero-shot context. This process involves extracting pre-existing domain-specific knowledge embedded within the language model parameters acquired during training to generate in-depth, technical descriptions of nanomaterials that are both detailed and comprehensive. The customized CoT prompt format is as follows:

The structured prompts are designed to facilitate a comprehensive, in-depth exploration of various facets, ranging from fundamental properties to practical applications and potential risks associated with these nanomaterials. Prompting LLMs generates text that responds to and elaborates on the specific aspects mentioned in each prompt.

Tableshows the sample responses generated by GPT-4 to the natural language prompts on MEMS devices. In the subsequent section, we will describe our approach to integrate technical descriptions into a small-scale LM for fine-tuning on the supervised nanomaterial identification task.

Our approach employs a smaller pretrained language model (LM) to summarize the technical descriptions generated by a large language model (LLM) on nanomaterials. In our study, we utilize a pre-trained small-scale LM, DeBERTa, an improved version of the BERT architecture. We fine-tune this small-scale LM on the generated technical descriptions for domain-specific customization on the downstream task. This helps the language model learn the statistical relationships between words and phrases in the large corpus of LLM textual outputs, thereby facilitating the generation of context-aware token embeddings. We input the text sequences generated by LLMs (denoted as) into themodel, which then generates expressive token embeddings as follows:

where the deep contextualized embeddings are denoted as, whererepresents the number of tokens inandis token embedding dimension. We perform weighted average of all token embeddings for a comprehensive representation of the entire text. We use a softmax attention mechanism to compute interpretable attention weights, denoted as, and subsequently use these weights to sum-pool the token embeddings, encoding the textual descriptions into a fixed-size, high-level text representation as follows:

whereis a learnable vector and the subscriptrefers to token. The text-level embeddingencapsulates the relevant and concise information at the core of the domain knowledge in technical descriptions, which is extracted from the general-purpose LLMs for each nanomaterial.

We employ the multi-head self-attention mechanism to align image embeddings with their corresponding text embeddings for image-text matching purposes. This approach emphasizes specific aspects or features of the image that is semantically relevant to the textual descriptions, taking into consideration the different facets of the relationship of the cross-domain modalities (both text-level and image embeddings). The mechanism calculates similarity scores between the provided image embedding and all text embeddings. The text embedding with the highest similarity score is considered the best match for the given image embedding. We initially concatenate the text-level embeddings for the different nanomaterial categories to obtain a unified text-level embedding, where c refers to the total number of nanomaterials. We compute the value and key projections for the unified text-level embedding, which represents the combined semantic information of all nanomaterials, for each headas follows:

Similarly, the query projection for image embeddingfor each headis as follows:

where,, andare trainable weight matrices. We now utilize the scaled-dot product attention mechanismto compute the normalized attention score, which measures the semantic similarity between the image embedding and each text embedding for a specific attention headas follows:

wheredenotes the dimensionality of the keys. The attention weights align complementary information from the cross-domain embeddings, focusing on relevant information for cross-modal alignment and capturing richer semantics by allowing the framework to dynamically weigh different parts of the input based on their relevance to the context. We then compute the weighted sum of the value projection as follows:

We concatenate the outputs across different heads because it encapsulates perspectives from multiple heads which focus on different aspects or features, thus producing a more comprehensive and robust alignment between the two modalities. We project the outputs to obtain the final representation as follows:

whererepresents the total number of heads anddeontes the trainable weight matrix. We now compute the cosine similarity between the two cross-domain embeddings as follows:

where, and we select the best match based on the highest similarity value. The index is determined for the text-level embedding with the highest similarity score as follows:

Here,is the index of the best-matching text-level embedding. So, the best-matching text-level embedding would be:

This is essentially a matching mechanism that seeks to find the best pairwise alignment among the various text-level embeddings and the image embedding. We utilize backpropagation error in the downstream supervised multi-classification task to fine-tune the ViT and smaller LMs to maximize the pairwise alignment between the complementary image embedding () and its corresponding text-level embedding (). To summarize,incorporates the prior knowledge obtained from LLMs for the appropriate nanomaterial underlying the elecron micrographs as auxiliary information to support multi-modal learning.

Few-shot prompting enables LMMs such as GPT-4V to adapt to new tasks without the need for explicit, gradient-based fine-tuningusing the labeled data. This approach allows LMMs to learn by analogy, utilizing only a few input-output pairs specific to the downstream task. Few-shot prompting leverages the implicit knowledge embedded in pretrained LMM parameters to adapt to new tasks through task-specific demonstrations, thereby avoiding the need to repurpose LMMs with parameter updates. The context-augmented prompt provides task-specific instructions and demonstrations(input-output pairs), enabling LMMs to generate outputs conditioned on the prompt for improved generalization performance on the new, unfamiliar tasks. In the case of nanomaterial identification tasks, few-shot prompting involves creating a context-augmented prompt using a few input-output mappings, which are a small number of image-label pairs as demonstrations sampled from the training data relevant to the query(new/unseen) image. These mappings provide relevant context to aid in understanding and classifying unseen images. The task-specific instruction is related to the query image, instructing the multimodal model to predict its associated label. At inference time, given test images, few-shot prompting predicts an output label based on the conditional probability distribution,. To explore how the quality and quantity of few-shot demonstrations affect the performance in nanomaterial identification tasks, we consider two distinct sampling strategies: “Random” and “Similarity-driven Sampling”. The random approach involves selecting demonstrations (image-label pairs) arbitrarily from the training data without any specific criteria, serving as a baseline for evaluation. In contrast, the similarity-driven sampling strategy employs cosine similarity to identify the top-images that most closely resemble a given query image within the training data. This strategy operates under the hypothesis that demonstrations which are more representative of the query image’s data distribution can potentially enhance model adaptability and accuracy. By utilizing both the diverse strategies to construct augmented prompts, we aim to provide a comprehensive analysis of how different demonstration sampling methods impact the effectiveness of few-shot learning in nanomaterial identification tasks. Furthermore, the efficacy of the demonstrations is inherently related to the sampling methods used to retrieve the top-images that align most closely with the query image. To further explore the impact of the volume of demonstrations on performance, we adjust the number of demonstrationsfor each query image. In summary, our objective is to provide LMMs with a context-augmented prompt, comprising image-label pairs selected from the training data, along with task-specific instructions that guide the LMMs in predicting the nanomaterial category of the query image. This task evaluates the LMMs’ ability to predict nanomaterial categories based on the prompt without any parameter updates, distinguishing it from traditional supervised learning, where models are fine-tuned on labeled data. For each query image, the LMMs generate a-dimensional one-hot vector, wheredenotes the predefined number of nanomaterial categories. This vector is linearly encoded into a high-dimensional space to produce a prediction embedding, which encapsulates the LMMs predictions. Here,represents the embedding dimension and. An example of an ICL prompt is as follows,

The prediction embedding likely contains valuable information about potential outcomes, allowing the framework to refine its cross-modal representation for better alignment with desired results. Given the complexity of nanomaterials structures and properties, this prediction embedding has the potential to capture some of that complexity, guiding the framework toward correct identification through the integration of prior knowledge and auxiliary information. A general purpose GPT-4V is accessible to ChatGPT Plus subscribers with a usage cap at chat.openai.com. However, it’s not currently available for public use through Multimodal Modeling as a Service (MMaaS) — a cloud-based service that accepts both image and textual inputs as prompts. By utilizing black-box GPT-4V through MMaaS as an on-demand service, typically hosted on cloud servers and accessed via an API, users can design task-specific prompts to query pre-trained LMMs for solving multimodal tasks of interest. This is analogous to how users might access LLMs via Language Modeling as a Service (LMaaS) to tackle language-specific tasks. GPT-4V is anticipated to become publicly accessible by mid-November 2023. APIs are designed for large-scale and concurrent requests and are ideal for integration into automated systems. Conversely, websites might not efficiently handle numerous interactions in rapid succession, and automating tasks on them could be prohibited. Manually sending inputs for GPT-4V for the entire training dataset would be a daunting and tedious task. Instead, we select a subset of images from the whole training dataset, termed ‘Confounding or Ambiguous Micrographs’, for few-shot prompting of GPT-4V. The selection process for these images is as follows: The SEM electron micrographs, initially sized atpixels, were downscaled topixels. They were then normalized using z-score normalization and flattened into one-dimensional vectors. Their dimensionality was further reduced using Principal Component Analysis (PCA) before employing the K-Means clustering algorithm. We choseclusters based on a predefined number of nanomaterial categories based on benchmark dataset. This method identifies images that are challenging to classify by measuring distances from centroids, assessing cluster variance, and calculating the Silhouette Score. Ground truth labels aid in the evaluation of the clustering’s effectiveness. We sampled a fixedof the ambiguous images from the entire dataset. For these images, we apply few-shot prompting of GPT-4V to predict labels. The goal is to learn the optimal projection layer weight matrices, which transform the GPT-4V predictions (one-hot vectors) into a high-dimensional space, producing a prediction embeddingthat encapsulates the GPT-4V predictions. The projection layer is subsequently trained through the supervised learning task. This training aims to minimize the cross-entropy loss and obtain optimal weights.

We compute the cross-modal embedding, denoted as, using a hierarchical multi-head attention mechanism that integrates the original image embedding, the text-level embedding, and the prediction embedding. This mechanism offers a robust framework for integrating diverse information from different modalities. As a result, it can produce a more holistic representation that encompasses a wide range of information, potentially improving the performance of nanomaterial identification tasks. In general, the multi-head attention mechanism uses multiple heads to capture different attention patterns, allowing the model to recognize a variety of relationships in the data from multiple subspace representations. Given queries Q, keys K, and values V, the scaled dot-product attention is defined as follows:

whereis the dimensionality of the keys. Theattention mechanism employs multiple heads to integrate various attention patterns into a unified representation. Each of these heads utilizes the scaled dot-product attention on distinct linear transformations of the input queries, keys, and values. These transformations are parameterized by matrices,, and. The final output is derived by concatenating the results from these heads and subjecting it to a subsequent matrix transformation.

Given the context described earlier, the unified attention layer employs the multi-head attention mechanism in a hierarchical fashion to derive the cross-modal embedding. The procedure consists of two main stages: (1)Here, the unified attention layer focuses on the image embeddingin relation to the text-level embedding. The result is an intermediate embedding,, which amalgamates details from both image and text modalities through the multi-head attention mechanism. The primary intent of this step is to incorporate relevant textual information guided by the image’s context. This can be mathematically described as follows:

(2)During this stage, the previously derived intermediate embeddingundergoes further refinement. The unified attention layer aligns this embedding with the prediction embedding, computing the final cross-modal representation. This stage aims to combine insights from the intermediate representation with the prediction embedding, creating a comprehensive representation that seamlessly integrates various modalities. This can be represented mathematically as:

In summary, the unified attention layer uses multi-head attention mechanisms to capture and integrate information from multiple different modalities (image, text, prediction) in a hierarchical manner, resulting in the comprehensive cross-modal embedding that can be used for nanomaterial identification tasks. This mechanism employs multiple sets of learned weight matrices to emphasize various aspects or relationships within the data. Consequently, this approach has the potential to foster robust and enriched embeddings capable of capturing complex patterns. Additionally, it aids in focusing on contextually relevant information and in achieving semantic alignment across different embeddings, thereby enhancing the capacity to identify and utilize important features in the input data. Finally, we linearly transform the final unified cross-modal embedding to obtain a probability distributionover the possible outcomes, as follows:

represents the probability distribution across nanomaterial categories. We apply the argmax operation toto determine the most likely nanomaterial category predicted by the framework. In our approach, we concurrently carry out three tasks: (i) we compute image embeddings using an image encoder, (ii) we utilize zero-shot CoT prompting with LLMs to generate technical descriptions of nanomaterials and fine-tune smaller pre-trained LMs on the generated descriptions—subsequently, a softmax attention pooling mechanism is employed to produce text-level embeddings, and (iii) we employ few-shot prompting of LMMs to derive prediction embeddings. Following this, we jointly optimize these different embeddings using a hierarchical multi-head attention mechanism for supervised learning tasks. The overarching goal is to reduce the cross-entropy loss and improve multi-class classification accuracy. Furthermore, the MHA adeptly captures and aligns diverse data sources, making it indispensable for multi-modal integration and analysis. This capability is especially crucial in fields like nanomaterial analysis, where multiple modalities offer complementary insights.

SECTION: Experiments And Results
Our study utilized the SEM datasetto automate nanomaterial identification. The expert-annotated dataset spans 10 distinct categories, representing a range of nanomaterials, such as particles, nanowires, and patterned surfaces, among others. It contains approximately 21,283 electron micrographs. Figureprovides a representation of the different nanomaterial categories in the SEM dataset. Despite initial findingson a subset, our research was based on the complete dataset. The original dataset curators, did not provide predefined splits for training, validation, and testing, so we employed the k-fold cross-validation method. This strategy facilitated a fair and rigorous comparison with popular baseline models. To further validate our proposed framework, we evaluated it on several open-source material benchmark datasets relevant to our study, encompassing diverse applications. This allowed us to demonstrate the efficacy of our framework and its applicability to a broader range than just the SEM dataset.

To measure the effectiveness of our proposed framework, we conducted an in-depth analysis contrasting it with popular computer vision baseline models. We compared our framework to supervised learning models, notably Convolutional Neural Networks (ConvNets) and Vision Transformers, and self-supervised approaches like Vision Contrastive Learning. The results of this analysis are shown in Table. To ensure an fair and rigorous comparison, all tests were conducted under uniform settings across different algorithms. We assessed performance using the Top-accuracy metric, specifically forvalues of 1, 2, 3, and 5. Notably, our framework outperformed the best-performing baseline model, T2TViT (), demonstrating a significantimprovement in Top-1 accuracy and a modestgain in Top-5 accuracy. Tablepresents a comparison between our framework and a selection of supervised learning-based baseline models. This includes several GNN architecturesas well as Graph Contrastive Learning (GCL) algorithms. Impressively, our framework establishes a new state-of-the-art benchmark, outperforming all other baselines on the benchmark dataset. Figuresandshows the radar charts corresponding to the results shown in Tablesand. The underlying hypothesis of our framework is that ViTs can be employed for initial explorations and the generation of baseline results in this context. Zero-shot CoT prompting of LLMs can be leveraged to enhance the initial outcomes of ViTs by utilizing the implicit domain-specific knowledge embedded within the language model’s trainable parameters to obtain expressive cross-modal embeddings. On the other hand, few-shot (in-context) learning of LMMs can be utilized to further refine the framework’s predictions by providing demonstrations from the training data, potentially leading to a more robust and accurate predictive framework for nanomaterial category prediction. The experimental findings validate this hypothesis and further advancements in the semiconductor industry—a domain where traditional deep learning techniques often underperform due to their lack of a holistic and nuanced approach. Such shortcomings could hinder breakthroughs in the semiconductor industry.

The landscape of computer vision has been profoundly influenced by convolutional networks (ConvNets or CNNs). The pioneering LeNetset the stage for ConvNets, which were subsequently employed in a variety of vision tasks ranging from image classificationto semantic segmentation. Over recent years, groundbreaking architectures like ResNet, MobileNet, and NAShave further refined the capabilities of CNNs. However, the introduction of vision transformers (ViTs)marked a paradigm shift, leading to the development of numerous enhanced ViT variants. These advances encompass pyramid architectures, local attention mechanisms, and innovative position encoding methods. Drawing inspiration from ViTs, the computer vision community has also delved into the potential of Multilayer Perceptrons (MLP) for vision tasks. Current vision-based frameworks in the semiconductor manufacturing sector fall short in various aspects, especially when compared to the recently proposed advancements in generative deep learning and multimodal learning. Many existing solutions fail to capitalize on the detailed analysis achievable through the synergy of LLMs, LMMs, and small-scale LMs with electron micrographs. Moreover, the existing frameworks typically analyze electron micrographs (nano images) at a singular modality, through the use of architectures such as ConvNets, ViTs, or MLP-Mixer, missing the opportunities that a multi-modality fusion approach could offer in enhancing classification accuracy. Furthermore, the industry has not fully embraced the utilization of zero-shot CoT LLMs prompting for generating technical descriptions of nanomaterials, which can significantly enhance domain-specific insights essential for nanomaterial identification tasks. Furthermore, the semiconductor manufacturing sector has not fully tapped into the emerging in-context learning capabilities of LMMs with few-shot prompting for predictive nanomaterial analysis, even though these capabilities could significantly enhance the accuracy of nanomaterial predictions. This glaring gap in the integration of image-based and linguistic insights renders current architectures less comprehensive and nuanced, potentially impeding breakthroughs in the semiconductor industry. Instead of relying solely on conventional classification methods, the new framework incorporates both image-based and linguistic insights by leveraging the capabilities of ViTs and LLMs, respectively, as well as the predictive abilities of LMMs. This framework aims to facilitate a more comprehensive and nuanced analysis of electron micrographs, holding significant promise for advancements in the semiconductor industry through automated nanomaterial identification. These advancements highlight the ongoing push for innovation in semiconductor manufacturing, driven by the escalating demand for more powerful and efficient electronic devices.

SECTION: Conclusion
To conclude, we conducted the first in-depth study aimed at achieving state-of-the-art performance in nanomaterial characterization. We have introduced an innovative framework that employs ViTs as the foundational layer, further enriched through the multi-modal fusion approach of zero-shot CoT prompting of LLMs and refined with few-shot (in-context) learning of LMMs. Our experiments confirm the superiority of this framework, indicating its transformative potential for semiconductor manufacturing in the age of advanced electronic devices.

SECTION: References
SECTION: Technical Appendix
SECTION: Ablation Study
Figureillustrates an overview of the framework. The proposed framework involves four components: (a) The first component, the electron micrograph encoder, takes an input image and divides it into smaller patches. These patches are transformed into tokens, enriched with positional embeddings for spatial information, and a classification token is added as a separate token to represent the overall image content. The resulting augmented token sequence is fed into a ViT model to generate an embedding that represents the entire image. (b) Next, the zero-shot CoT LLMs prompting technique uses cloud services to access LLMs for generating detailed descriptions about various aspects of nanomaterials. Structured prompts guide the LLMs in generating in-depth descriptions on topics ranging from the fundamental properties of nanomaterials to their practical applications. Subsequently, we fine-tune smaller LMs on the descriptions generated by the LLMs to obtain context-aware token embeddings. We then perform sum-pooling attention mechanism to obtain contextualized text-level embeddings, which capture the core knowledge in the generated texts. (c) Finally, the cross-modal alignment employs the scaled dot-product attention mechanism to match the image embeddings with the corresponding nanomaterial-specific text-level embeddings. This process highlights image features relevant to the textual descriptions, aiding in the identification of text-level embeddings that correspond to the image. (c) Few-shot prompting enables LMMs to quickly adapt to new tasks without traditional fine-tuning on labeled data. Using a small set of input-output pairs, these models learn tasks by drawing from their vast pre-existing knowledge acquired during training on vast, diverse text corpora. In the context of nanomaterial identification, LMMs utilize a handful of image-label pairs from the training data to classify new, unseen images and obtain prediction embeddings. Demonstrations can be selected either randomly or based on their similarity to the (c) Few-shot prompting enables LMMs to quickly adapt to new tasks without traditional fine-tuning on labeled data. Using a small set of input-output pairs, these models learn tasks by drawing from their vast pre-existing knowledge acquired during training on vast, diverse text corpora. In the context of nanomaterial identification, LMMs utilize a handful of image-label pairs from the training data to classify unseen query images and obtain prediction embeddings. Demonstrations can be selected either randomly or based on their similarity to the query image. (d) Finally, the unified attention layer, through the hierarchical multi-head attention mechanism, combines information from the original image embedding, a text-level embedding, and the prediction embedding, optimizing for accuracy in nanomaterial categorization. To evaluate the efficacy of the individual components and validate the design choices for their inclusion in the framework, we conducted an ablation study. In this study, we selectively disabled specific components to create various ablated variants, which were then evaluated using the SEM dataseton nanomaterial identification. Compared to our proposed original framework, which serves as the baseline, the ablated variants exhibited a notable decline in performance, underscoring the importance of the components that were disabled. The ablation study results support the hypothesis that each component is crucial for the framework’s peak performance in nanomaterial identification. Ablated variants excluding the zero-shot CoT LLMs prompting with cross-modal alignment, few-shot prompting with LMMs, and unified attention methods are labeled as proposed framework “w/o LLMs”, “w/o LMMs”, and “w/o MHA”; “w/o” is shorthand for “without”. In the case of “w/o MHA”, we concatenate the cross-domain embeddings and then transform them through a linear layer to predict the label. The findings from the ablation study are presented in Table. The ‘w/o LLMs’ variant exhibits a significant decrease in performance compared to the baseline, with a noteworthy drop ofin, underscoring the crucial role of zero-shot CoT LLMs in prompting. This technique extracts detailed nanomaterial technical descriptions from closed-source large language models (LLMs), then fine-tunes smaller language models on these descriptions to produce context-aware token embeddings. It subsequently employs weighted sum-pooling mechanism to derive text-level embeddings.

Cross-modal alignment correlates these text embeddings with image embeddings via scaled dot-product attention, thereby synchronizing and matching information across different types of data modalities in a unified representation space. Similarly, the ‘w/o LMMs’ variant performs notably worse than the baseline, exhibiting a drop ofin. This underscores the significance of few-shot prompting to predict the label of query image, which leverages LMMs to rapidly adapt to new tasks with limited demonstrations (image-label pairs) and generate prediction embeddings. In addition, the ‘w/o MHA’ variant showed a significant performance deterioration compared to the baseline, marked by adrop in. This deterioration can largely be attributed to the overly simplified linear operator in the output layer. This further emphasizes the importance of the unified attention layer, which amalgamates insights from image, text, and prediction embeddings using a hierarchical multi-head attention mechanism to optimize nanomaterial categorization accuracy. Similarly, in comparison to the baseline, the ‘w/o LLMs’ variant exhibits a 13.03decrease in, the ‘w/o LMMs’ variant shows a 14.48reduction, and the ‘w/o MHA’ variant experiences a significant drop of 21.31in the same metric. Our ablation study highlighted the significant contributions of each component within our framework. When individual components were omitted, we observed a consistent drop in performance. These findings validate the integral role that each component plays in achieving optimal performance of the holistic framework. In summary, each component serves a specific purpose, and their integration ensures a comprehensive approach to nanomaterial categorization. Their inclusion is justified by the need for detailed analysis, adaptability, and holistic consideration of both visual and textual data in categorizing nanomaterials.

SECTION: Empirical Insights into Nanomaterial Classification
We conducted comprehensive experiments to assess our proposed framework’s effectiveness in classifying electron micrographs of various nanomaterials, from simple to intricate patterns. Our primary goal is to highlight the framework’s classification capabilities. Nanomaterials exhibit a diverse range of patterns due to differences in composition, morphology, surface properties, crystallinity, and synthesis methods. These patterns, as captured in electron micrographs, reflect the materials’ unique properties, structures, and potential applications. They can depict anything from isolated nanoparticles to complex aggregations, and from crystalline to amorphous structures. Accurate interpretation of these patterns is vital for understanding and leveraging each nanomaterial category’s distinctive qualities. Electron micrographs provide detailed nanoscale insights, revealing structural and morphological features essential for various applications in materials science and related fields. As demonstrated by the experimental results displayed in Figure, our framework can effectively generalize across diverse nanomaterials, even those with complex patterns. The figure presents bar plots of the framework’s performance on multiple metrics, color-coded by category. We evaluated its performance on the SEM datasetusing standard metrics such as precision (P in), recall (R in), and F1-score (F1 in). These findings underscore the framework’s effectiveness and emphasize its relevance in materials science and nanotechnology. We employ a comprehensive multi-metric evaluation for robust comparison with baseline models, anchored by a confusion matrix that details our framework’s performance in classifying electron micrographs across nanomaterial categories. The matrix includes key metrics for multi-class classification: True Positives (TP), which are micrographs correctly identified as belonging to a category; False Negatives (FN), which are micrographs that belong to a category but are incorrectly classified as not belonging; True Negatives (TN), which are micrographs correctly identified as not belonging to a category; and False Positives (FP), which are micrographs incorrectly identified as belonging to a category. From these metrics, we calculate precision (the ratio of correctly classified micrographs to the total classified as belonging to a category, TP / (TP + FP)), recall (the proportion of actual micrographs from a category that were correctly classified, TP / (TP + FN)), and the F1-score, which is the harmonic mean of precision and recall, providing an overall measure of effectiveness in micrograph categorization. Thus, our multi-metric approach, anchored by the confusion matrix and its derived metrics, ensures a rigorous evaluation of our framework. This approach facilitates a detailed understanding of our model’s effectiveness in categorizing electron micrographs across various nanomaterial categories. It is crucial to highlight that the SEM dataset exhibits significant class imbalance. Notably, our framework achieves higher classification scores for nanomaterial categories with a substantial number of labeled instances, outperforming its performance on categories with fewer instances. The notable success with fewer labeled instances can be attributed to our proposed framework’s reduced reliance on nanomaterial-specific relational inductive biases, which distinguishes it from conventional methods. In conclusion, our extended experiments have bolstered our confidence in the framework’s ability to generalize and accurately categorize various nanomaterials using electron micrographs. We believe that these advancements will greatly benefit the wider community by accelerating materials characterization and related research.

SECTION: Experimental Setup
The SEM datasetcomprises electron micrographs with a resolution ofpixels. For our analysis, we resized these topixels. During preprocessing, we normalized the images, adjusting their mean and covariance to a consistent value of 0.5 across all channels, ensuring values between [-1, 1]. These downscaled and normalized images were segmented into distinct, non-overlapping patches, treated as a sequence of tokens with a-pixel resolution. Both the patch dimension () and the position embedding dimension () were set to. We employed a 10-fold cross-validation for evaluation, training forepochs with an initial learning rate ofand a batch size of. The unifying (cross-modal) attention layer had the number of attention heads (H) set toand the key/query/value dimensionality () to. To optimize theframework’s performance, we used early stopping on the validation set and a learning rate scheduler, decreasing the rate by 50if the validation loss did not decrease for five epochs. The Adam optimization algorithmwas used to fine-tune the framework’s parameters. Our proposed framework seeks to enhance multi-class classification precision by combining the strengths of large multimodal models (LMMs) such as GPT-4V, large language models (LLMs) like GPT-4, and smaller language models (LMs). The methodology utilizes LLMs to generate detailed technical descriptions for nanomaterials, capturing crucial linguistic insights critical for nanomaterial identification. We access LLMs, such as GPT-4, through a Language Model as a Service (LaMaaS) platform using text-based APIs, with GPT-4’s maximum token sequence output being 4096. GPT-4 with Vision (GPT-4V) extends GPT-4 by adding visual processing capabilities, allowing it to analyze image inputs alongside text. While GPT-4 is limited to text processing, GPT-4V can handle both text and image inputs, enabling applications like visual question answering. We employ few-shot prompting with GPT-4V, using a small number of demonstrations (image-label pairs included in the prompts) to guide its understanding and to predict the label of a query image. To optimize resource utilization, we trained our deep learning models, which are built upon the PyTorch framework, on two V100 GPUs, each with 8 GB of GPU memory. Given the significant computational demands of prompting Large Language Models (LLMs) and Large Multimodal Models (LMMs), we repeated each experiment twice and reported the average results.

Our baseline methods are organized into four main categories. First, we employ Graph Neural Networks (GNNs) for the supervised multi-class classification of vision graphs. Second, we utilize Graph Contrastive Learning (GCL) methods, which involve creating multiple correlated graph views through stochastic augmentations of the input graph data to maximize mutual information between these views. GCL methods aim to enhance the similarities between positive graph views and reduce dissimilarities between negative graph views sampled from different images. Typically, these methods use the Graph Attention Network (GAT)to learn node-level embeddings, with graph-level embeddings obtained via sum-pooling of node embeddings. For classification, we apply supervised learning using the Random Forest (RF) algorithm, which utilizes these embeddings to predict nanomaterial categories. We then evaluate the efficiency of these unsupervised embeddings by assessing the RF model’s accuracy with holdout data. Third, we employ supervised learning with Convolutional Neural Networks (ConvNets) for the classification of electron micrographs. Lastly, Vision Transformers (ViTs) are used for supervised classification, utilizing sequences of image patches from electron micrographs as input, while Vision Contrastive Learning (VCL) techniquesare self-supervised algorithms designed for contrastive learning in computer vision, utilizing the ResNet architecture for feature extraction. As for our data representation, we construct vision graphs from electron micrographs using the Top-K nearest neighbor search, where patches are treated as nodes, and edges represent pairwise associations between semantically similar neighboring nodes. We employ a 32-pixel patch size and choose K=5 for simplicity to avoid multi-scale vision graphs with varying patch resolutions.

We carried out an extensive hyperparameter tuning for our framework, specifically examining the embedding dimension () and batch size (). We evaluatedvalues from the setandvalues from the set. Using the random-search approach, we measured performance based on Top-1 classification accuracy on the validation set. This thorough analysis determined the optimal settings for our framework to beand, with a corresponding Top-1 accuracy of 0.9161.

SECTION: Benchmarking with open-source material datasets
The NEU-SDD dataset() comprises 1,800 grayscale images captured through electron microscopy, showcasing various surface imperfections in hot-rolled steel strips. This comprehensive dataset is categorized into six types of surface defects—, with each category represented by 300 images standardized to a resolution of 200200 pixels. Illustrative examples from these defect categories are presented in Figure. Furthermore, we conducted a detailed comparative study using a range of established algorithms to evaluate the proposed method’s performance, with a particular focus on multi-class classification tasks for identifying these surface defects.

The CMI datasetencompasses 600 high-definition electron microscope images that display varying stages of metal panel deterioration. Corrosion experts have meticulously labeled these images following the ASTM-D1654 standard criteria, which employ a scale ranging from 5 to 9 to denote the severity or extent of corrosion. The dataset provides 120 unique images for each level of the corrosion grade defined by this scale, with each image offering a detailed resolution of 512×512 pixels. Figureshowcases a set of example images for each corrosion category. We evaluate the effectiveness of our proposed multiclass classification method by benchmarking its ability to accurately assign corrosion grades against a range of established algorithms.

The KTH-TIPS datasetis a detailed collection of 810 electron micrograph images, each measuring 200200 pixels, and featuring ten different types of materials. The dataset is notably diverse, including textures such as, captured under various conditions of lighting, orientation, and scale. Figuredisplays a representative set of these images, providing a visual overview of the dataset’s range. To assess the effectiveness of our proposed technique, we conducted a thorough comparative study, benchmarking it against established algorithms for classifying the various textures or materials represented in the KTH-TIPS dataset.

Tabledisplays CoT prompts that focus on various types of surface defects. Each prompt is tailored to initiate a comprehensive exploration of the defects characteristics, causes, and impacts. The structured prompts ensure a detailed and systematic exploration, providing clarity and comprehensive coverage. Consequently, this approach yields precise information and a nuanced understanding of the defects implications. Similarly, Tablepresents a series of CoT prompts designed for an in-depth examination of metal panel corrosion. These prompts guide the research towards a nuanced exploration of different corrosion grades and their effects on metal panels. This structured approach facilitates the study, analysis, and practical application of findings, such as the automatic rating of corrosion in metal panels.

The notion that a single, universal CoT prompt can address all tasks with GPT-4 is inaccurate when it comes to generating technical descriptions of nanomaterials as metadata. Specialized prompts tailored to specific needs are crucial for effectively utilizing AI in the analysis of nanomaterials across different diverse datasets. Customized prompting is essential in AI, as demonstrated by the use of GPT-4 to generate prompts(questions) specific to nanomaterial categories in various datasets, as opposed to relying on a single universal CoT prompt for all across various datasets.

Tablepresents prompts designed to thoroughly cover a wide range of information exploring the characteristics and applications of various materials and providing detailed technical descriptions of these materials. Tableoffers a detailed juxtaposition of the efficacy of our method against several standard approaches, with evaluations conducted on multiple datasets. The outcomes of these experiments reveal that our approach consistently attains unparalleled results, highlighting its effectiveness and reliability.