SECTION: BN-AuthProf: Benchmarking Machine Learning for Bangla Author Profiling on Social Media Texts
Author profiling, the analysis of texts to uncover attributes such as gender and age of the author, has become essential with the widespread use of social media platforms. This paper focuses on author profiling in the Bangla language, aiming to extract valuable insights about anonymous authors based on their writing style on social media. The primary objective is to introduce and benchmark the performance of machine learning approaches on a newly created Bangla Author Profiling dataset, BN-AuthProf.
The dataset comprises 30,131 social media posts from 300 authors, labeled by their age and gender. Authors’ identities and sensitive information were anonymized to ensure privacy. Various classical machine learning and deep learning techniques were employed to evaluate the dataset. For gender classification, the best accuracy achieved was 80% using Support Vector Machine (SVM), while a Multinomial Naive Bayes (MNB) classifier achieved the best F1 score of 0.756. For age classification, MNB attained a maximum accuracy score of 91% with an F1 score of 0.905. This research highlights the effectiveness of machine learning in gender and age classification for Bangla author profiling, with practical implications spanning marketing, security, forensic linguistics, education, and criminal investigations, considering privacy and biases.

SECTION: 
Author profiling has become indispensable with the growing usage of social media, where anyone can write anything. It is the study of specific texts to discover unique characteristics of an author, such as gender, age group, region, and language, based on their writing style and content. As social media platforms expand, the need to unravel the characteristics of authors becomes increasingly apparent in order to ensure the safety of online activities and communication. Author profiling has significant implications for security, forensic linguistics, education, research, and marketing. For example, social media ads are now mostly curated based on author characteristics, tailoring content to different demographics. Companies are leveraging authors’ profiles to understand reviews and better grasp their target audience. Forensic linguists utilize author profiling to determine the linguistic profile of suspects, aiding investigations. Moreover, it could be useful in detecting fake profiles and news. Nevertheless, discerning the identities behind texts is a complex and challenging task that needs further attention, especially for a low-resourced language as Bangla.

While author profiling has been extensively explored in languages like Dutch, English, Greek, and Spanish, it remains largely unexplored in Bangla due to the unavailability of a benchmarked dataset. This paper aims to fill this gap by benchmarking classical machine learning (ML) and deep learning (DL) approaches for predicting authors’ gender and age based on Bangla textual content from social media platforms. For this purpose, we extensively compiled theby collecting author IDs with substantial Bangla posts with their consent. The dataset consists of 300 anonymized authors and 30,131 manually curated Facebook posts in Bangla, tagged with age and gender information.

Theof this paper are summarized as follows:

We studied author profiling in the Bangla language, aiming to predict demographic factors such as age group and gender based on textual content. To the best of our knowledge, this is the first work of its kind in Bangla.

We introduced the BN-AuthProf Dataset that includes 30,131 Facebook status from 300 Bengali authors, each labeled with age and gender. We also compared nine popular ML/DL algorithms for benchmarking.

We have drawn several conclusions and recommendations for the research community to consider. Additionally, our results could serve as a strong baseline for future machine learning experiments in this domain.

SECTION: 
In the recent past, automatic profiling of authorship has seen a multitude of literary developments. Here we categorically present these key findings.

Though researchers have extensively explored approaches across various languages, they are still in their infancy when it comes to low-resource languages like Bangla. A lexical model was used in an early attempt to predict age and gender in English on social networks. This system used regression and classification models and yielded results (91.9% accuracy) in line with state-of-the-art age and gender prediction. Basile et al.employed a gender prediction model across English, Spanish, Arabic, and Portuguese on Twitter data which performed between 0.68 and 0.98, with an average accuracy of 0.86 on the testset. For identifying Indianic native languages, TF-IDF and n-gram features showed promising results. Alsmear et al.studied gender identification in Arabic texts, employing various classification techniques.

Peersman et al.investigated age and gender prediction on social networks, emphasizing the challenge of short text lengths (average of 12 words). They utilized emoticons, image sequences, and character/word n-grams as features, achieving optimal results with a balanced training dataset. Parres et al.introduced diverse features, including structural metrics, part-of-speech analysis, emoticons, and themes derived from Latent Semantic Analysis (LSA). A Random Forest classifier with 311 features for age and 476 features for gender prediction yielded the highest accuracy. However, this approach was relatively slow.

Techniques primarily utilize supervised learning approaches. Hamda et al.utilized Support Vector Machine (SVM) classifier for age, gender, and language variety identification in Arabic, achieving accuracy up to 95.97% for age and 81.53% for gender. In contrast, Schaetti et al.combined TF/IDF with Convolutional Neural Networks (CNNs) to predict language variety and gender, showing higher accuracy rates of 98% on Portuguese. Our model did not perform as well as theirs, even if it worked great for CNN and other neural networks.

Hsieh et al.investigated author profiling on, achieving high accuracy in tasks such as age and gender prediction. Guimaraes et al.explored age categorization oncorpus, achieving notable results using CNN. Zhao et al.investigated age and gender identification inand achieved accuracies of 72.10% for gender and 81.15% for age classification. Nguyen et al.conducted a comparative analysis, highlighting limitations in current computational approaches. The study reveals that over 10% ofusers do not conform to their biological sex, and older users are often mistaken for younger ones.

Datasets like PAN have been instrumental in hosting various author profiling tasks, exploring traits such as gender, age, native language, and personality. While SVM remains a popular choice among participants, techniques like random forest and logistic regression have also been employed. Character-level CNNs and RNNs also demonstrated success, offering alternatives to word-based approaches. In the 2016 PAN task, Vollenbroek et al.achieved the highest average accuracy of 52.58% using a linear SVM with n-grams and part-of-speech feature. However, Modaresi et al. found that part-of-speech tagging was not sufficient for domain-independent profiling. CLEF-PAN 2018 challengeincluded gender prediction from texts and photos, introducing new subtasks and language options.

The availability of a corpus is a need for developing any NLP technique in any language. The lack of an author dataset is the biggest challenge to authorship classification in the Bengali language. We created this dataset (BN-AuthProf) in order to address this problem. The dataset includes 30,131 manually collected Bangla Facebook posts with age and gender tags, along with 300 anonymized authors. Various classical machine learning and deep learning techniques were employed to evaluate the dataset by investigating optimized hyperparameters (such as n-gram, vectorizer, loss, activation, kernel size, batch size, and epoch).

SECTION: 
In this section, we comprehensively explore the BN-AuthProf dataset, its creation, labeling process, and features.

SECTION: 
Social media platforms like Facebook, Twitter, and Instagram are widely utilized in both Bangladesh and India. Facebook, in particular, holds substantial prominence, with approximately 90.46% usage in Bangladesh, and India leading globally in terms of Facebook users. Consequently, Facebook was chosen as the primary platform for data collection. Our author selection includes Facebook users from Bangladesh and West Bengal (an Indian province), spanning various professions. We manually collected textual data from 300 authors, each contributing numerous Bengali posts. We contacted each author before collecting their data and only chose those who gave consent, following ethical data collection practices. Personal information, such as user IDs, was systematically removed, and numerical IDs from 1 to 300 were assigned for data accumulation and labeling. We followed several guidelines during data collection:

We only selected authors having a substantial amount of posts ranging from 80 to 100 status.

Status containing more than 10 words were included to ensure data quality in terms of semantics.

Statuses with mixed languages were excluded to maintain focus on Bengali.

Statuses containing personal information or sensitive data were avoided to maintain the privacy of authors.

Statuses containing URLs or external links were excluded to maintain data integrity.

Duplicated statuses from the same author were removed to ensure dataset uniqueness.

Only original posts from chosen authors were included, excluding shared content.

SECTION: 
We have classified authors based on two criteria: age and gender. Our dataset comprises 300 text documents (one per author), each named ‘#User_Id.txt’, and a separate single file titled ‘#Truth.txt’. For example ‘1.txt’ file contains the first authors’ posts, with each line representing an individual post. Simultaneously, the corresponding labels (i.e., age, gender) are placed within the ‘#Truth.txt’ file, employing a specific format: ‘’.

In the context of the BN-AuthProf dataset, gender classification employs ‘’ to signifyand ‘’ for. In terms of age classification, labels span four ranges: ‘’, ‘’, ‘’, and ‘’. Fig.provides a visual representation of our file structure.

SECTION: 
Tabledelineates the quantitative statistics of our dataset, where we observed a notable imbalance in gender and age groups, reflecting broader societal trends in Bangladesh.

SECTION: 
The pie charts in Fig.illustrate the distribution of gender and age labels in our original dataset. There is a notable inconsistency in the distribution of these labels, which poses a challenge during benchmarking with machine learning algorithms by introducing bias toward the ‘M’ gender label and the ‘25-34’ age group. To address this issue, we have employed a solution that involves randomly generating additional data from the original dataset to produce a balanced training set.

Initially, our dataset included data from 227 male and 73 female authors. To rectify this gender imbalance, we created 154 additional data entries for female authors. This augmentation was achieved by randomly selecting status updates from the existing female data and mixing them to create new, synthetic examples. Regarding age, we had 43 instances for the ‘18-24’ group, 127 for the ‘25-34’ group, 82 for the ‘35-49’ group, and 48 for the ‘50-xx’ group. To align these groups more closely with the dominant ‘25-34’ group, we generated an additional 84 instances for the ‘18-24’ group, 45 instances for the ‘35-49’ group, and 79 instances for the ‘50-xx’ group. The same random oversampling technique for generating gender-specific data was applied to balance the age groups. Tableprovides insights into the adjustments made to address the initial imbalance in gender and age labels.

SECTION: 
For benchmarking, we created the experimental dataset that combines original data with augmented data (see Table). This experimental dataset is divided into two distinct categories: Gender and Age. We further split each category into training, development, and testing sets. We allocated approximately 80% of each author’s posts to training, 10% to validation, and 10% to the testing portion. Tableand Tablepresent detailed statistics of the data. It is important to note that we only used the augmented data in the training set, while the validation and test sets consisted solely of original data. This decision ensures that the validation and test sets are not repeated during our evaluations.

SECTION: 
For benchmarking, we used five classical ML models: Support Vector Machine, Naïve Bayes, Decision Tree, K-nearest Neighbor, Logistic Regression, and four deep learning models: LSTM, BiLSTM, CNN, BiLSTM + CNN. Fig.outlines a structured approach for assessing the performance of the aforementioned models on the BN-AuthProf dataset.

SECTION: 
As we required only Bangla status updates, we had to clean irrelevant information from the original dataset. During the data preprocessing, we cleaned the data, tokenized it, and transformed the text into sequences. Fig.shows a general overview of the data preprocessing pipeline with a flowchart.

Briefly, we start by Removing URLs’ from the data. Next, any ‘Line Feed’ or ‘Newline Character’ (i.e.,,) is removed. After that, we remove any types of Latin Characters’ that might constitute the English language. We also remove ‘Whitespaces’, ‘Unwanted Characters’, and the ‘Hashtag Symbol’ since these elements do not contribute to the overall semantics of the data. However, we did not remove emojis, as they tend to polarize the status for certain groups, such as younger generations.

Since each status or post consists of multiple sentences, we need to chunk them into smaller units. This process is known as tokenization. Thus, in the next step, we determine whether we are applying classical machine learning approaches or deep learning techniques. For the deep learning models, we utilize ‘Sentence Tokenization’, whereas for classical ones, we tokenize each input into words. For the deep learning approaches, we convert each sentence into a fixed-sized integer sequence (i.e., vector), where each element of the sequence represents either a word or padding. In contrast, for classical ML, the tokenized words are transformed into numerical representations using frequency-based techniques such as counting or TF-IDF (Term Frequency - Inverse Document Frequency).

SECTION: 
The classical ML models are mostly statistical in nature and straightforward to utilize. However, they require handcrafted feature engineering, which offers greater insights into the models’ behavior. Below, we provide details about the settings we used during our experiments with these classical models:

Support Vector Machine (SVM) is a supervised learning algorithm primarily used for classification tasks. It constructs an optimal decision boundary, or hyperplane, between data points of different classes. SVMs have been shown to excel in author profiling in the PAN shared task. For gender classification, SVM employs binary classification using various kernel functions such as sigmoid, poly, and linear. Age classification utilizes multiclass techniques including One-vs-One (OvO) and One-vs-Rest (OvR), along with kernels like RBF and polynomials. We utilized both tf-idf and count vectorizers with character n-gram (range 1-8) and word n-gram (range 1-7) features.

Naïve Bayes (NB) classifiers leverage Bayes’ theorem to predict class labels based on feature independence assumptions. They are computationally efficient, making them ideal for rapid model development and prediction. Multinomial, Bernoulli, and Complement NB models are employed during the benchmarking with similar settings for n-gram features as SVM.

Logistic Regression (LR) is a statistical method used to predict binary outcomes by modeling the relationship between a dependent variable and one or more independent variables. The tunable parameters in this model include different solvers: saga, liblinear, newton-cg, and lbfgs.

K-Nearest Neighbor (KNN) is a nonparametric, supervised learning algorithm used for both classification and regression tasks. It classifies data points based on the majority vote of their nearest neighbors, with proximity serving as the determining factor. To optimize the performance of KNN, various distance metrics such as Minkowski, Euclidean, and Manhattan, as well as neighbor values ranging from 1 to 5, are explored.

Decision Trees (DT) are tree-structured classifiers that recursively split data based on feature attributes. They offer a simple yet powerful method for classification and regression tasks. Criteria functions like Gini and entropy, along with splitting strategies like best and random, are employed to construct and prune decision trees.

SECTION: 
Deep learning is a subset of machine learning that employs multi-layered neural networks for prediction tasks. Instead of relying on manual feature engineering, various hyperparameters play a pivotal role in performance tuning. Here, we will provide details about the hyperparameter settings we utilized during our experiments:

Long Short-Term Memory (LSTM), a type of recurrent neural network (RNN), excels at handling sequential data by capturing long-term dependencies. In contrast, Bidirectional LSTM (BiLSTM) processes input in both directions, leveraging two layers of 128 LSTM cells each. We set varying activations, batch sizes, and epochs to optimize the performance of these models.

Convolutional Neural Networks (CNN) process sequential data using embedding layers, with the model architecture featuring convolutional layers with max-pooling and sigmoid activation. Trained with varying batch sizes and epochs, CNN is effective in feature extraction.

The combined model integrates BiLSTM and CNN, leveraging their complementary strengths. Employing the ’adam’ optimizer and different loss functions, this model is trained with varying batch sizes and epochs. All deep learning models shared the same hyperparameters including maximum length, vocabulary size, and embedding dimensions, trained with adam optimizer,a learning rate of 0.001, and 16 samples per batch for 3, 5, 10, and 15 epochs.

SECTION: 
We evaluate the performance of different models on the experimental datasets (see Tablesand). For task-specific supervision, we use the training data, while the validation set plays a pivotal role in tuning hyperparameters and optimizing the model. We perform evaluations on our test set using the best settings obtained during validation. We use,,, andas evaluation measures.

We also employed K-fold cross-validation to account for the model’s generalizability. We used, dividing the whole dataset into 10 distinct folds for iterative processing to obtain a meanscore.

SECTION: 
In our study, we conducted an extensive evaluation of numerous combinations of base classifiers for both gender and age prediction tasks. Among the classical models for, the SVM model achieved the highest accuracy ofand F1-score of. This result was achieved using an SVM with a polynomial kernel, word n-gram of (2, 3), and Tf-Idf feature vectorization. The Multinomial Naïve Bayes (MNB) proved to be a robust choice for gender prediction, delivering an accuracy ofand f1-score of. MNB was particularly effective when accompanied by character n-gram of (2, 4) and Count feature vectorization.

Regarding, the Multinomial NB model outperformed others with an impressive accuracy ofand F1-score of. This remarkable accuracy was achieved using word n-gram of (1, 8) and Count feature vectorization. Additionally, the SVM model, equipped with a polynomial kernel plus word n-gram of (2, 5), and Tf-Idf Vectorizer, achieved a competitive accuracy ofand f1-score of. The results are summarized in Table, showcasing the highest accuracy and f1-score attained by the two best models for gender and age classification.

SECTION: 
In this section, we present a thorough assessment of our gender classification experiments. We evaluated the performance of both classical machine learning and deep learning models, employing various feature extraction techniques, kernel functions, and vectorization methods. To commence, the bar chart of Fig.showcases the distribution of accuracy of the benchmarked models. Remarkably, the Support Vector Machine (SVM) employing a word analyzer with a polynomial kernel, (2, 3) n-gram, and Tf-Idf Vectorizer achieved the highest accuracy of 80.6%. The best combinations of hyperparameters achieved 75.67% accuracy for deep learning models in gender prediction (see Table).

Tableprovides a comprehensive overview of the gender classification results, based on precision, recall, and F1-score scores. Notably, the SVM algorithm exhibited the highest precision score of 0.898, whereas the NB performed well in recall, achieving a score of 0.796.

We employed 10-fold cross-validation to assess the robustness of our system. Tablepresents the mean accuracy scores for the classical models. Support Vector Machine (SVM) emerged as the most accurate model, with an accuracy of 75.6%, while Decision Tree (DT) exhibited the lowest accuracy at 74%. Interestingly, NB consistently performed well, whereas K-Nearest Neighbor (KNN) and Logistic Regression (LR) showed the same accuracy.

To investigate further the performance of SVM and NB, we present the confusion matrices in Fig.. It is evident that NB excelled in classifying females, accurately identifying 58 instances. However, SVM displayed a notable bias towards classifying instances as males, whereas NB demonstrates a balanced performance in both gender categories. In order to examine this phenomenon, we present the precision versus recall curve on varying classification margins (C values) of SVM in Fig.. Examination of this curve reveals a gradual decline in precision once recall exceeds approximately 0.2. Accordingly, we established a threshold value of 0.67 for C, where precision is notably high while recall remains comparatively lower for the SVM classifier.

Turning to deep learning-based methods, Tablesummarizes the results using the ‘adam’ optimizer in all cases. BiLSTM and CNN achieved the highest accuracy of 75.67% when using the ‘binary crossentropy’ loss and ‘mean absolute percentage error’ loss functions. Both LSTM and CNN exhibited the lowest results with ‘mean absolute error loss’.

Age classification is a multi-class classification task, and we compared the models based on their overall average prediction scores among the classes. Fig.provides a histogram of accuracy for our machine learning and deep learning models on age classification. Here, among all the models, Multinomial Naïve Bayes (MNB) achieved an impressive accuracy score exceeding 91%. However, the overall average accuracy of the remaining models spans from as low as 31% to as high as 71%.

Tablelists the results of age classification for the classical models. We observe that the NB algorithm outperformed all other models with the highest accuracy score of 0.91% with similar precision and recall scores. SVM also demonstrated decent performance, with a precision of 0.88 and an F1 of 0.58. Tableprovides 10-fold cross-validation scores for various classical models. SVM in particular had the highest score of 42.3%, whereas KNN and DT had the lowest accuracy.

We examined the confusion matrices for SVM and NB classifiers in Fig.. The SVM classifier demonstrates a distinct reliance on the 25–34 age class, showcasing notable proficiency in accurately categorizing instances within this particular age range. However, it is noteworthy that, within the 18-24 age class, only 15 instances were appropriately categorized, while the remaining 28 instances were misclassified and attributed to the 25-34 age class. Furthermore, SVM’s performance displayed inconsistencies in accurately classifying instances within the 35-44 and 50-xx age classes, resulting in misclassifications. In contrast, NB exhibited a commendable level of accuracy across all age categories.

Moving to deep learning models, exhibiting lower performance in general for age prediction. CNN stands out with the highest accuracy of 71%. Meanwhile, LSTM and BiLSTM achieved accuracy scores ranging from 14% to 42% respectively. Tableprovides an overview of deep learning-based age prediction using the ‘adam’ optimizer in all cases. Interestingly, changing the activation function or optimizer did not significantly impact the scores.

Our evaluation indicates that classical machine learning methods generally outperform deep learning in gender and age classification tasks. Naïve Bayes consistently emerged as a top performer. Additionally, SVM demonstrates strong performance. Specific model configurations and optimizers played a crucial role in DL models. These findings provide valuable insights into the application of machine learning algorithms for author profiling.

SECTION: 
We have examined the field of author profiling for Bangla, extracting information about authors based on their writing style. This domain has been largely unexplored due to the unavailability of a dataset, despite having 300 million Bengali speakers worldwide. This paper presents thedataset encompassing 300 Bengali authors and 30,131 labeled Facebook status. Our investigation primarily revolved around crafting and evaluating machine learning and deep learning models for gender and age classification. Through rigorous experimentation, we attained promising benchmarks, particularly with Support Vector Machine and Naive Bayes algorithms. For our future work, we intend on expanding categories beyond age and gender and transitioning towards multi-label author profiling through transformer-based language models.

SECTION: References