SECTION: Multiple-Exit Tuning: Towards Inference-Efficient Adaptation for Vision Transformer
Parameter-efficient transfer learning (PETL) has shown great potential in adapting a vision transformer (ViT) pre-trained on large-scale datasets to various downstream tasks. Existing studies primarily focus on minimizing the number of learnable parameters. Although these methods are storage-efficient, they allocate excessive computational resources to easy samples, leading to inefficient inference. To address this issue, we introduce an inference-efficient tuning method termed multiple-exit tuning (MET). MET integrates multiple exits into the pre-trained ViT backbone. Since the predictions in ViT are made by a linear classifier, each exit is equipped with a linear prediction head. In inference stage, easy samples will exit at early exits and only hard enough samples will flow to the last exit, thus saving the computational cost for easy samples. MET consists of exit-specific adapters (E-adapters) and graph regularization. E-adapters are designed to extract suitable representations for different exits. To ensure parameter efficiency, all E-adapters share the same down-projection and up-projection matrices. As the performances of linear classifiers are influenced by the relationship among samples, we employ graph regularization to improve the representations fed into the classifiers at early exits. Finally, we conduct extensive experiments to verify the performance of MET. Experimental results show that MET has an obvious advantage over the state-of-the-art methods in terms of both accuracy and inference efficiency.

SECTION: 
Vision transformer (ViT)has achieved remarkable success in various recognition tasksdue to its excellent modelling abilities. Unfortunately, it is hard for us to train a well-performing ViT from scratch on the datasets with limited samples. To solve this dilemma, a common practice is to pre-train a ViT on large-scale datasets like ImageNetand then fully fine-tune the pre-trained model for downstream domains. However, this practice is storage-inefficient because we need to store the whole tuned model for each downstream task.

Parameter-efficient transfer learning (PETL)aims to adapt large pre-trained models to downstream tasks using only a fraction of trainable parameters. It has been the predominant method for transferring the knowledge contained in pre-trained ViTs. Compared to full fine-tuning, PETL is much more storage-friendly since we just need to store the learnable parameters. Most existing PETL methods focus on reducing the number of trainable parameters as much as possible. They either insert extra learnable blocksor tokensinto a frozen ViT backbone or selectively tune some modules of a pre-trained ViT while freezing the others. At test stage, these methods will allocate the same computational resources to all inputs. A pre-trained ViT can be viewed as a feature extractor and our goal is to extract suitable representations from it. The representation ability of a ViT is supported by its encoder layers. This begs the question: is it necessary to represent all inputs with all encoder layers? In real-world applications, a large number of samples are easy to recognize, which means that their representations can be easily learned. Ideally, if all inputs are represented with suitable encoder layers, we will save lots of computational resources on easy samples and thus boost the inference efficiency of the fine-tuned model.

Motivated by early-exiting dynamic neural networks, we propose to allocate suitable encoder layers to inputs by incorporating multiple exits into a pre-trained ViT backbone. Easy samples will exit at early exits when they are represented properly while only sufficiently challenging samples will pass through all encoder layers. In this way, we can reduce the computational resources for easy samples and speed up the whole inference process. In ViT, the representations of inputs are extracted by class tokens and there is only one class token for each sample. For a neural network with multiple exits, it is optimized by minimizing the cross-entropy losses of all exits. Nevertheless, the representation-learning processes of a sample at different exits may negatively influence each other due to the gradient conflicts caused by different losses, which will be discussed in Section IV.D.

In this paper, to address the aforementioned problem, we propose a simple yet effective method named multiple-exit tuning (MET), to transfer the representative knowledge contained in a pre-trained ViT to downstream tasks. Our proposed method is totally different from the existing ones. Fig.shows the inference difference between MET and conventional PETL methods. In MET, multiple exits are inserted into the pre-trained backbone and samples flow to different exits depending on whether they are easy to recognize or not. Easy samples will flow to early exits and only hard enough samples will exit at the last exit. By contrast, all samples in conventional PETL methods will be recognized by the classifier after the last encoder layer.

MET includes exit-specific adapters (E-adapters) and graph regularization. All E-adapters share the same down-projection and up-projection matrices for the purpose of efficient storage. Graph regularization is applied to enhance the intra-class compactness and inter-class separability of data points at early exits. To sum up, the main contributions are as follows.

We introduce parameter-efficient E-adapters to disentangle the transformations of class token, facilitating the extraction of representations for each exit.

To improve the learning abilities of early classifiers, two self-supervised graphs are constructed to regularize the exit-specific representations in training process.

Extensive experiments conducted on 28 downstream tasks demonstrate that MET is more competitive than the state-of-the-art methods.

The rest of this paper is organized as follows. Related work and preliminaries are reviewed in Section II and Section III. The proposed method is introduced in Section IV. Experiments are conducted in Section V. Conclusion and future work are given in Section VI.

SECTION: 
SECTION: 
Over the past several years, parameter-efficient transfer learning (PETL) has mainly focused on adapting the transformers pre-trained on large-scale datasets to various downstream domains with a few learnable parameters. The existing studies can be roughly categorized into four branches: partial tuning, prompt tuning, extra-module tuning, and mixed tuning methods.

is a simple type of PETL method which directly tunes a small subset of the parameters existing in backbones while freezing the others. The methods belonging to this category include LN-tune, BitFit, DiffFit, linear probe, etc.introduces a set of trainable tokens to the inputs of transformer modules. For example, Jia et al.tune a pre-trained backbone by prepending learnable tokens to the inputs of encoder layers; Han et al.add key-value prompts and visual prompts to multi-head attention (MHA) modules and encoder layers, respectively; Tu et al.prepend extra query tokens for each MHA operator to aggregate the intermediate representations of encoder layers; Li et al.directly prepend continuous task-specific vectors to the input of the entire pre-trained backbone and only optimize these added vectors when tuning on the datasets of downstream tasks. By inserting extra blocks into a pre-trained backbone,has attracted increasing attention from researchers. Adapter, side-tuningand reparameterizationare popular research topics in extra-module tuning. The input of adapter is first mapped into a low-dimensional space through a down-projection matrix followed by a nonlinear function, and then remapped into the original space by an up-projection matrix. Each transformer layer consists of an MHA block, a feed-forward network (FFN) module and two layer normalization operators. Adapter often adjusts the representations of samples from downstream tasks by tuning the inputs of encoder layersor the outputs of MHA and FFN, or by learning the complementary information of FFNor transformer layers. In side-tuning, side networks, such as E3VA, DTL, SANand LSA, are designed to extract the useful information existing in pre-trained backbones. Reparameterization introduces learnable blocks during training phase, but these modules are perfectly incorporated into the whole pre-trained model at inference stage, avoiding extra computational cost. In the related work about reparameterization, such as Hydra, FacT, RLRR, LoRA, the inserted blocks are proposed with multiple linear transformations.refers to the methods which are the combination of different PETL methods. For instance, UniPELTincorporates LoRA, prefix tokens and adapter; NOAHadapts neural architecture search techniques to find the optimal combination of LoRA, adapter, and VPT.

Most existing studies on PETL are proposed to solve the storage-expensive problem. In these studies, all inputs share the same computational resources in inference stage. In fact, easy samples can be correctly classified with less computational cost than hard samples, and thus these methods are inference-inefficient for easy samples. Dynamic tuning (DyT)is recently introduced to reduce the redundant computation in inference process by activating or deactivating tokens dynamically. Our method (MET) is designed to assign suitable encoder layers for different samples. DyT and MET are fundamentally different.

SECTION: 
Dynamic neural networks can achieve a good trade-off between inference efficiency and learning performance by adapting their structures or parameters for different inputs during inference stage. As a typical dynamic neural network, early-exiting dynamic neural network (EDNN)inserts multiple exits along the depth direction. In training phase, all losses of exits are jointly optimized. During inference, if a sample’s prediction confidence obtained by the classifier assigned at a certain exit exceeds a threshold, this sample will exit at this exit; otherwise, it will flow to the next exit. For more details, please refer to.

Based on the above schemes, easy samples will exit at early exits and only sufficiently difficult samples will flow to the last exit, thereby saving the computational cost for easy samples. In this paper, by incorporating the learning schemes of EDNN into transfer learning, we introduce a storage-friendly and inference-efficient tuning method.

SECTION: 
SECTION: 
A vanilla vision transformer (ViT)consists of a patch embedding (PE) layer, encoder layers and a prediction head. As the basic layer in ViT, PE layer transforms an input imageinto a sequence of-dimensional tokens, whereis the resolution of,is the-th flattened patch,is the resolution of each image patch,is the projecting matrix and. The mathematical expression for this layer is as follows.

whereis the prepended learnable class token;represents the position embeddings;andare the transformed class token and features in PE layer.

Layer norm (LN), MHA and FFN are three basic components for each encoder layer. In the-th encoder layer, we have

whereandare the transformations about class token and features in MHA block;andare the corresponding representations obtained by this layer;,is the total number of encoder layers;and.

We haveafterencoder layers. In ViT,is taken as the final representation to make predictions. The predicting resultcan be obtained by

whereis a linear classifier.

SECTION: 
Adapter is widely used to tune the intermediate representations contained in pre-trained backbones by two learnable projection matrices and one nonlinear activation function. Letanddenote the input and output of an adapter, we have

whereis a nonlinear activation function,andare down-projection and up-projection matrices. Inserting multiple adapters into a pre-trained backbone at different locations has become a popular strategy in PETL, where all projection matrices are different and trainable. The relationship between every two adapters is depicted in Fig..

SECTION: 
Our proposed method is shown in Fig.. Multiple-exit tuning (MET) consists of two modules: exit-specific adapters (E-adapters) and graph regularization. The inserted E-adapters are used to extract discriminative representations for the classifiers at different exits. Since different exits require different representations about class token, the number of disentangled representations is equal to that of exits. Suppose there areexits in a pre-trained ViT backbone. All representations about class token and feature tokens pass through the same encoder layers together before reaching an exit. When arriving at an exit, the specific representation of class token is adopted to make predictions, while the others continue the forward propagation process. The graph regularization is introduced to boost the learning abilities of early classifiers.

SECTION: 
Following the existing work on early-exiting dynamic neural networks, when fine-tuning a pre-trained ViT withexits, we can define its objective function as

whereis the-th prediction head;represents the cross-entropy loss of;is the encoding function for the-th exit with parameter;is the number of training samples;represents the-th training sample andis the ground-truth label of.

According to the learning scheme (see equation (4)) of ViT,should be the representation of’s class token for the-th classifier. However, there is only one class token in ViT. If all classifiers share the representations of class token, their performances may negatively interact with each other, which will be discussed in Section IV.D.

To avoid the optimization conflicts caused by the shared representations, we introduce E-adapters to disentangle the transformations of this token, based on the work on adapter. There are two E-adapters inserted before MHA and MLP modules in each encoder layer. In each of these E-adapters, we expect to obtain exit-specific representations by different projection matrices. However, if we follow the relationship demonstrated in Fig., it will introduce a large quantity of trainable parameters, sinceE-adapters are inserted into a pre-trained ViT backbone.

To reduce the number of learnable parameters, we design a partial-sharing projection scheme. As illustrated in Fig., all E-adapters share the same down-projection and up-projection matrices. Low-dimensional transformation matrices are applied to capture the differences among E-adapters. Diagonal matrices are used to adjust the suitable embeddings for different exits. Each E-adapter can be viewed as the combination of multiple adapters with shared projection matrices. As shown in Fig., the learning scheme of E-adapter is influenced by two factors: location and the number of exit-specific representations in input. If the input to the-th E-adapter contains() exit-specific representations (see Fig.(a)), according to the property of adapter, we have

where;andare the shared down-projection and up-projection matrices;is the output of the ()-th adapter and;andare low-dimensional transformation matrices for this E-adapter;are exit-specific diagonal matrices;denotes the representation for the-th exit andis the corresponding transformation of;represents the input features;is the transformed features of the ()-th adapter;andare the final outputs. The first E-adapter at the first encoder layer is applied to obtain specific representations for exits (see Fig.(b)). This E-adapter is formulated as follows

whereand. As for the second E-adapter at the last encoder layer (see Fig.(c)), it is equivalent to one adapter, which is defined as

where.

Compared to the vanilla adapters, our proposed partial-sharing projection scheme is much more parameter-efficient. Suppose three exits are inserted into the last three encoder layers of a pre-trained ViT backbone. If we directly follow the relationship shown in Fig., we will haveextra parameters. There arelearnable parameters in our designed scheme. Since, we have. For example, when fine-tuning a pre-trained ViT-B/16 backbone, the number of trainable parameters is reduced by approximately 98.48%, demonstrating the powerful parameter-compression capability of the designed scheme.

For simplicity, we denote the mathematical formulation of the-th E-adapter (containingexit-specific representations) aswhereand. In the-th encoder layer of our proposed method, equations (2) and (3) become

where();();is the set of exit-specific representations in this encoder layer;is the set of intermediate representations of;is the representation of the-th exit;is equal tofor adapter;() is the set of exit-specific representations obtained by the ()-th encoder layer. When a specific representation reaches its corresponding exit, it will end the forward propagation process. Therefore, if an exit is assigned before the-th encoder layer, we have, otherwise,. For sample, we assume that the-th exit is inserted after the-th encoder layer. Accordingly, equation (6) is reformulated as

where the representations for all exits are normalized by the same LN module which has been specifically trained for the prediction head in a pre-trained ViT.

SECTION: 
In multi-exit neural networks, the consumed computational resources have a decisive influence on the performances of classifiers. The classifier at the deepest layer usually has the best performance because of the sufficiently computational resources. Existing studies demonstrate that improving the learning abilities of classifiers at early exits is necessary to boost the inference efficiency of an early-exiting dynamic neural network. In MET, the exit-specific representations are fed into corresponding linear classifiers to make predictions. As for a linear classifier, its performance is normally affected by intra-class compactness and inter-class separability. When the inputs exhibit poor intra-class compactness and small inter-class separability, it is hard to obtain a good linear classifier.

In this section, we boost the learning abilities of the classifiers with limited computational resources by improving the intra-class compactness and inter-class separability of their inputs. Graph has been proven to have the ability to effectively describe the relationship among samples. It includes vertices and edges, where vertices represent samples and edges are the corresponding similarities. Similar to, Euclidean distance is adopted to measure the difference between two data points. Accordingly, for the-th classifier, the intra-class compactness of its inputs can be calculated by

whereisnorm,is the intra-calss similarity between samplesand. Similarly, the inter-class separability is formulated as

whereis the inter-class similarity of the two samples.

Compared to the other classifiers, the classifier at the-th exit has more advantages to capture the relationship among data points. As the performance of the-th classifier is directly determined by its output logits, we construct the similaritiesandas follows.

whereis the output logit of the-th classifier for;stands for the similarity of two vectors;means that samplesandare from the same class whileindicates that the two samples belong to different classes. For simplicity, cosine function is adopted to measure the similarity between samples. Finally, the input representations for the-th classifier can be enhanced by

whereandare the numbers of nonzero elements inand, respectively.

SECTION: 
As discussed above, cross-entropy losses are introduced to fine-tune a pre-trained ViT with multiple exits and graph regularized losses are designed to boost the performances of the classifiers at shallow layers. The total objective function of MET is formulated as

whereis a trade-off parameter. Note thatandare constructed based on the output logits of the last classifier. As the logits should be optimized by the corresponding cross-entropy loss, we detach the gradients of the logits when constructing similarity matrices to avoid the negative influence brought by early exits.

SECTION: 
As indicated by equation (4), the representation of the class token in the-th encoder layer is employed to make predictions in ViT. When ViT is assigned with multiple exits, we need to extract representations for the prediction heads at different exits. A simple approach is to allow these prediction heads to access the representations of the class token at corresponding depths. However, it is difficult for these classifiers to perform well because they may have conflicts in optimizing the shared representations of class token. The details are as follows.

Suppose the-th and-th exits are inserted after the-th and-th encoder layers. The cross-entropy loss of the-th classifier is. In the ()-th encoder layer, the class token’s representationis obtained by MHA and MLP blocks. In MHA, we have the following operations.

whererepresents the softmax function;;;andare the normalizedand;are applied to obtain query, key and value matrices;andis the number of attention heads;is the output of the-th attention head;is a mapping matrix. In MLP, we have

whererepresents the GELU function;andare up-projection and down-rpojection matrices;is the normalized. At the-th exit, the cross-entropy loss can be formulated as.

For an early-exiting dynamic neural network, it is optimized by jointly minimizing the losses of all exits. When directly attaching classifiers along the network’s depth, classifiers at early exits exert a detrimental effect on the subsequent representation-learning processes. According to equations (23)-(25),heavily depends on the class token’s representation obtained from the-th encoder layer. During the joint optimization process, the loss at the-th exit probably hurts the quality of, negatively affecting the performance of the-th classifier.

SECTION: 
In this section, extensive experiments are conducted to evaluate the performance of MET. Specifically, we first verify the adaptation ability of MET on VTAB-1K benchmark, then test its generalization ability on few-shot learning and domain generalization benchmarks, and finally conduct ablation studies for further analysis of the proposed method. In this paper, we select the ViT-B/16pre-trained on supervised ImageNet-21Kas the backbone for all compared methods and MET. In MET, we assign 7 prediction heads to the ViT-B/16 backbone. To be specific, for each of the last 7 encoder layers, it is followed by a prediction head. In other words, the-th prediction head is inserted after the ()-th encoder layer. For compared methods, we directly run the released code or use the reported results. Following other dynamic neural networks, the computational budget of each method is measured in GFLOPs.

SECTION: 
The comparison results on VTAB-1K benchmark are reported in TABLE. VTAB-1K is a widely used transfer learning benchmark. It contains 19 visual classification datasets which can be divided into three categories:,, and. Each dataset only has 1000 training data points. The details are given in TABLE, where ’#Class’ represents the number of classes, ’#Train’, ’#Val’ and ’#Test’ denote the number of samples in training, validation and test sets, respectively. Similar to VPT, we apply standard data augmentations, including image normalization and resizing each input into aimage, for these samples.

To evaluate the performance of MET, we compare it with two traditional baselines and several competitive PETL methods. The two traditional baselines are full fine-tuning (denoted as Full) which updates the whole pre-trained backbone, and linear probing (denoted as Linear) which only updates the prediction head. PETL methods include Adapter, LoRA, NOAH, VPT, FacT-TT, ARC, Res-Tuning, Hydra, RLRR, SCT, and DyT. The parameter settings of MET are presented in TABLE. Following, we also use the validation set split from the training set to determine the hyper-parameters in TABLEand then directly report the top-1 accuracy on the test set for each dataset. As a dynamic neural network, MET has two prediction modes: anytime classification (static inference) and budgeted batch classification (dynamic inference). In static inference mode, we only need to reserve a suitable prediction head and store the extra parameters inserted into the backbone. In the other inference mode, to ensure a fair comparison with DyT, we let MET have a comparable parameter scale. To achieve this goal, we only reserve three prediction heads. In this case, MET is run with prediction heads {1,4,7} or {5,6,7}. For a better comparison, we denote the dynamic MET with smaller/larger GFLOPs as MET(small)/MET(large).

Note that in static inference mode, MET only has one prediction head, and thus ’Exit’ in TABLErefers to the corresponding exit. From TABLE, we have the following observations. 1) Although PETL methods with static inference obviously perform better than linear probing and full fine-tuning, most of them have the same computational cost as the two traditional tuning methods, and even some of them, such as Adapter, VPT, Res-Tuning, and SCT, introduce extra GFLOPs. 2) In static inference mode, MET outperforms all compared methods in terms of classification accuracy and computational cost. Specifically, it exceeds the method with the second-best performance (RLRR) by 0.5% and simultaneously reduces GFLOPs by 11.2%. 3) In dynamic inference mode, compared to DyT, MET can achieve more competitive classification performance with significantly fewer GFLOPs, while maintaining a comparable parameter scale. These observations demonstrate that our proposed method is both storage-friendly and inference-efficient.

SECTION: 
Similar to, we select five fine-grained visual recognition datasets in few-shot settings to test the learning ability of our proposed method under low-data regime. The details of these datasets are described in TABLE. We evaluate MET under {2,4,8,16}-shot settings. TABLEshows the corresponding parameter-settings for MET. We choose Adapter, LoRA, NOAH, VPT, Res-Tuning, and SCTas comparison methods, as their advantages in few-shot learning have been verified by extensive experiments.

Figs.-show experimental results on fine-grained datasets. On the datasets of Stanford Cars, Food101, and Oxford-Pets, dynamic MET evidently outperforms the compared methods in terms of GFLOPs and accuracy. On the Oxford-Flowers102 dataset with {2,4,8,16}-shot settings, dynamic MET achieves the similar accuracies of the second-best method while reducing GFLOPs by 20.7%, 22.8%, 28.8% and 40.2%, respectively. On FGVC-Aircraft dataset, both static MET and dynamic MET underperform SCT in 2-shot and 4-shot settings but they are more competitive than SCT in the other settings. From these figures, it is easy to find that static MET with the 6-th exit performs better than many of these compared methods in most cases while consuming fewer GFLOPs. Here, we show the average GFLOPs and the overall performance in Figs.and. According to the results in Fig., we can observe that MET has an obvious advantage over Adapter, LoRA, NOAH, VPT, Res-Tuning and SCT in two inference modes. Fig.displays that SCT and Res-Tuning perform better than the other compared methods and there is little difference in learning performance between the two methods. Taking SCT as an example, we record the GFLOPs required for dynamic MET to achieve the classification performance similar to that of SCT. Note that we record the GFLOPs of the best performance if dynamic MET underperforms SCT. The results presented in Fig.clearly illustrate that dynamic MET requires the fewest GFLOPs. Compared to SCT, dynamic MET can reduce GFLOPs by 16.5%, 15.8%, 20.5%, and 26.5% in {2,4,8,16}-shot settings. These phenomena demonstrate the efficiency and effectiveness of MET under low-data regime.

SECTION: 
In some real-world applications, the distribution of test data is different from that of training data. The trained models usually suffer from performance drops in this circumstance. To evaluate the generalization ability of MET to domain shift, following, we train our method on ImageNet dataset and directly test it on four variants of this dataset. To be specific, the training set sampled from ImageNet-1K contains 16 images per category and test sets include ImageNet-A, ImageNet-R, ImageNet-Sketch, and ImageNetV2. ImageNet-A and ImageNet-R contain adversarially-filtered examples and artistic renditions of ImageNet-1K. ImageNet-Sketch consists of the sketch images of the same categories in ImageNet. Compared to ImageNet, ImageNetV2 is collected from different sources. For the purpose of comparison, we also report the experimental results of Adapter, LoRA, NOAH, VPT, Res-Tuning and SCT on test sets. For MET, we follow the parameter settings described in TABLE.

Experimental results are reported in Fig.. The static MET at the 6-th exit requires less computational cost than Adapter, LoRA, NOAH, VPT, Res-Tuning, and SCT. At this exit, we find that: 1) static MET has a clear advantage over VPT, LoRA, and Adapter, and performs better than all compared methods on ImageNet-Sketch dataset; 2) the performance of static MET is similar to that of NOAH. In dynamic inference mode, MET significantly outperforms all compared methods on these test sets. When achieving the performance similar to that of the second-best compared method on ImageNet-A, ImageNet-R, ImageNet-Sketch, and ImageNetV2 datasets, dynamic MET can reduce GFLOPs by 31.5%, 34.8%, 31.5%, and 40.8%. The above comparisons illustrate MET’s superiority in handling the domain shift problem, thereby demonstrating its robust generalization ability.

SECTION: 
MET consists of E-adapters and graph regularization. As demonstrated in Section IV.A, E-adapters are inserted before the MHA and FFN modules in each encoder layer. Here, we investigate the influence of different components on the performance of MET. Without loss of generality, we conduct experiments on VTAB-1K benchmark.

To illustrate the impact on different exits, we report the group-wise average accuracies of all exits in TABLE, where ’#GR’ represents graph regularization and the best results are highlighted in bold. For convenience, we give the notations in the first column. For example, in METMG, E-adapters are inserted before MHA, and early exits are supervised by graph regularization. The performance of METMG is similar to that of METFG, indicating that the two locations are equally important. In comparison with METMF, MET shows a clear advantage across all exits and can obtain a 1.12% gain in terms of average accuracy. This observation demonstrates that graph regularization has the ability to enhance the performances of early classifiers. Comparing the experimental results of METMG, METFG and METMF, we find that the three factors contribute similarly to the overall performance. All in all, the E-adapters from the two locations and graph regularization are essential for MET.

In the proposed method, E-adapters are introduced to extract exit-specific representations. As analyzed in section IV.D, if multiple exits share the representations of class token, some optimization conflicts may negatively affect the final performance. Here, we only keep one diagonal
matrix in MET to demonstrate the influence. As displayed in Fig., MET with shared representations performs much worse than MET. Since different exits have different requirements for the intermediate representations of class token in encoder layers, these requirements probably conflict with each other. The exit at deeper depth usually has a better performance because of more computational resources. However, the second exit underperforms the first one in MET with shared representations, illustrating that the loss at the second exit is not well-optimized. According to these results, our proposed method can effectively solve the optimization conflicts.

SECTION: 
In this paper, a novel inference-efficient transfer learning method, named multiple-exit tuning (MET), is introduced to adapt a ViT pre-trained on large-scale datasets to various downstream domains. Unlike most existing PETL methods, MET focuses on assigning suitable computational resources for data points during inference stage. To achieve this goal, multiple exits are inserted into a pre-trained ViT backbone in MET, which allows samples to flow to suitable exits, thereby reducing the computational cost of processing easy samples. MET is composed of E-adapters and graph regularization. E-adapters are employed to extract appropriate representations for different exits. To reduce the number of trainable parameters, E-adapters are designed with a partial-sharing projection scheme. The classifiers at early exits usually underperform the one at the deepest exit due to limited computational resources. Two supervised graphs, which characterize the intra-class and inter-class relationship among samples, are adopted to regularize the representation-learning processes of early exits. We evaluate the performance of MET on 28 downstream tasks. As demonstrated by experimental results, MET achieves the state-of-the-art performance in terms of both classification accuracy and inference efficiency.

Early-exiting dynamic neural networks usually have difficulties in determining whether an input is easy or not at the image level for some tasks such as object detection and semantic segmentation. This inherent characteristic implies that our proposed method is mainly proposed for image classification tasks. As for future work, we plan to explore effective strategies to expand the application scope of MET.

SECTION: References