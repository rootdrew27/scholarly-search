SECTION: State-Space Modeling of Shape-constrained Functional Time Series111December 1, 2024

Daichi Hiraki1, Yasuyuki Hamura2, Kaoru Irie3and Shonosuke Sugasawa4

1Graduate School of Economics, The University of Tokyo,

2Graduate School of Economics, Kyoto University

3Faculty of Economics, The University of Tokyo

4Faculty of Economics, Keio University

Abstract

Functional time series data frequently appears in econometric analyses, where the functions of interest are subject to some shape constraints, including monotonicity and convexity, as typical of the estimation of the Lorenz curve. This paper proposes a state-space model for time-varying functions to extract trends and serial dependence from functional time series while imposing the shape constraints on the estimated functions. The function of interest is modeled by a convex combination of selected basis functions to satisfy the shape constraints, where the time-varying convex weights on simplex follow the dynamic multi-logit models. To enable posterior computation by an efficient Markov chain Monte Carlo method, a novel data augmentation technique is devised for the complicated likelihood of this model. The proposed method is applied to the estimation of time-varying Lorenz curves, and its utility is illustrated through numerical experiments and analysis of panel data of household incomes in Japan.

Key words: Filtering; Lorenz curve; Markov Chain Monte Carlo; Monotone function

SECTION: Introduction

Functional data has been seen in many scientific fields, including economics and social sciences(e.g. Horváth and Kokoszka,2012; Kokoszka and Reimherr,2017), where the object to be analyzed is expressed in the form of functions. Such a function is often limited to some subclass of functions, or its functional shape is constrained by monotonicity, convexity and other functional properties. Naturally, the estimation of functions under multiple constraints have been of research interest in statistics and econometrics.

Typically, such shape constraints can be addressed in terms of the differences of functional values.
Modeling the differences of the functional values directly leads to the realization of shape-constrained functions.
For example, a univariate function that varies gradually is modeled by specifying the prior distribution of the first-order difference between two consecutive functional values (e.g., seeFaulkner and Minin2018and references therein).
The use of a truncated prior can easily realize the monotonically increasing/decreasing functions on discrete support(Okano et al.,2023).
This approach can also define stochastic processes on functional spaces, including the square of Gaussian processes for derivatives of functions(Wang and Berger,2016; Lenk and Choi,2017; Kobayashi et al.,2021), the Gaussian process truncated on constrained functional space(Ray et al.,2020)and spline regression models with constrained coefficients(Shively et al.,2009,2011).
However, the posterior inference of the aforementioned approaches requires complicated Markov chain Monte Carlo (MCMC) methods, which hinders the extension to the hierarchical and dynamic models for multiple functions.

From the viewpoint of functional time series, several models for time-varying functions have been studied. Examples include autoregressive processes(King et al.,2019)and shrinkage processes for trend estimation(Kowal et al.,2017; Wakayama and Sugasawa,2024), but they do not address shape constraints.
An exception is the Bayesian non-parametric approach to monotonic functions inCanale and Ruggiero (2016), which combines MCMC and approximation Bayesian computation for posterior inference.

In this research, we consider a flexible modeling of time-varying monotone and/or convex functions.
Our model is based on convex combinations of basis functions that satisfy monotonicity and/or convexity with time-varying weights constrained on the simplex.
In modeling the dynamic convex weights, we simply take the inverse softmax transformation of the weights to define the real-valued state vector, then model its dynamics by the standard, Gaussian autoregressive process. This approach results in the state space models with the non-linear observational equation, for which the posterior sampling algorithm is not trivial. For posterior computation, we propose a novel data augmentation approach and prove that the model of interest is conditionally a dynamic linear model, enabling fast sampling of state variables by filtering and smoothing. The key idea of our augmentation is to compute the exponentiated quadratic term of the weight vector and write it as the binomial likelihood, to which the well-known Pólya-gamma augmentation can be applied(Polson et al.,2013; Glynn et al.,2019).

As an important application of the shape-constrained functional inference, we focus on the problem of estimating the Lorenz curves based on the aggregated income data. The Lorenz curves, together with the Gini coefficients computed from the curves, have long been utilized in economics as the measures of economic inequalities. By definition, the Lorenz curve must be monotonically non-decreasing and convex, hence the inferential problem of the Lorenz curve falls well within the scope of our study on the inference for the shape-constraint functions.
The standard approach to inference on the Lorentz curve is to fit a hypothetical income distribution and derive the Lorenz curves from the estimated income distribution.
The models and methods of parameter estimation related to this approach include generalized methods of moments(Hajargasht et al.,2012), mimimum distance estimation(Hajargasht and Griffiths,2020), Dirichlet likelihood(Chotikapanich and Griffiths,2002)and approximate Bayesian computation(Kobayashi and Kakamu,2019).
Among them, the (generalized) Dirichlet model has been extended to the state state models for the time-varying Lorentz curve(Kobayashi et al.,2022).
However, the use of Dirichlet likelihood could lead to severe misspecification of the observation model, in addition to the need to specify a particular parametric class of income distributions.
Our approach requires no distributional assumption on income distributions, hence can define a flexible, nonparametric model for the Lorenz curve.

The rest of the paper is organized as follows.
In Section2, we introduce our proposed state-space model and its application to the time-varying Lorenz curve.
The detailed posterior computation algorithm is discussed in Section3.
We demonstrate the numerical performance of the proposed method through simulation studies in Section4, and application to Japanese income data in Section5.
Concluding remarks are given in Section6.
Additional computational details and numerical results are provided in the Supplementary Material.

Notation.For-dimensional vector,is the-dimensional sub-vector obtained by deleting-th entry offor. Formatrix,, and, we denote theentry ofby, and the-th row vector ofwith-th column entry being deleted by.is defined similarly.is thesubmatrix ofwith the-th row and-th column being deleted.is the identity matrix of size.is the-dimensional vector of ones, whilemeans the indicator function.is the-dimensional multivariate normal distribution with meanand variance, and we write. We also write its density function evaluated atby.is the truncated normal distribution supported on interval.is the inverse-gamma distribution with shapeand rate.is the uniform distribution over.

SECTION: State-Space Models

SECTION: State-space models using basis functions

Suppose that we are interested in time-varying functiononfor.
We do not observedirectly, butfor some pre-specified augment values of, which are contaminated by additive noises as

where the distribution of noiseis defined later in (2).
In what follows, we assume that the number of observed points, as well as the points themselves, does not change over time, but our method can be easily generalized to the situation whereandcan vary across time points.

Given the shape constraints of interest (e.g., monotonicity and/or convexity), denote the subclass of functions under the constraints by. We further assume thatis closed under convex combination; this condition is satisfied ifis the set of monotone and/or convex functions.
In our application,is the set of all possible Lorenz curves, or increasing and convex functions on.
We model the unknown function as a convex combination ofbasis functions in:

wherefor,is a pair of fixed parameters that define the basis function, andis on the-dimensional simplex for allin the sense that

Sinceandis closed under convex combination, we have, or the shape constraints are always imposed on.
The choice of the number of basis functions, as well as the basis functions, is essential in flexibly modeling of the target function. Examples of the basis functions are given in Section2.3and the effect of the choice of the basis functions on inference is investigated in details in Sections4and5by numerical studies.
Using a sufficiently largeand appropriate basis functions, most of the functions inare expected to be approximated accurately by the convex combination above. Theoretical studies that support this approach includeMallick and Gelfand (1994), where the beta cumulative distribution functions are shown to be dense in the class of continuous distribution functions on.

It is also noteworthy that, in our model, the basis functiondoes not change over time, but the convex weight vectordoes. Hence, the model forcharacterizes the dynamics of function.
To introduce a time-series structure in the convex weights, we first define real-valued state variableby mapping weight vectorby the inverse softmax function below:

Then, for-dimensional vector, we introduce a vector autoregressive model as the state equation, namely,

whereis the mean vector, andandare the unknown coefficient and covariance matrix of size, respectively. In our application, we assume thatand, whereandfor, hence eachfollows a univariate AR(1) process independently. Priors forand initial valueare set so that the AR(1) processes become stationary, as explained later in Section3.4.

Letand.
The observational equation foris set as

The state equation (1) and observational equation (2) define the state space model we propose.
The choice of models for observational varianceis open to the end users. In our study, we setfor allandand define a prior distribution for; the observational noises are independent and identically distributed across timeand argument. The prior foris given in Section3.4.

Since we observe the functional value with the Gaussian additive noise, the mean ofunder the model (2) is. In other words, we assume that the mean function is constrained in. To be precise, for augment, we have

The variance ofequals the error variance.
This shows that the proposed state space model is indeed the time series extension of the additive noise model, where the error variance does not contain the information about the mean function.

SECTION: Difference from mixture approach

Another standard approach to the constrained mean function is to use a mixture of normals, where the means of the mixture components are the basis functions.
We here address why such standard approach is not preferable in the context of our research, despite its computational simplicity.
If one models responsenot by the convex combination of the basis functions, but by the mixture of them, the model is expressed as

Both the mixture model and ours have the same mean in (3), thus we can achieve the shape constraints on the mean function by either model. However, our model in (2) has the variance, while the variance of the mixture model is complicated as

Clearly, our model separates the observational variance from the mean structure, while the mixture model has the variance to depend on the mean. This separation in the usage of parameters helps the interpretation and prior elicitation of those parameters.

Note also that, in the mixture model, using the same variance parameter ofacross the mixture components is restrictive in the sense that all the components have the same variance. Thus, in using the mixture model, we should use a variance parameter customized for each mixture component, or, instead of. In the numerical examples, where we setin our model, it is fair to use the following mixture model:

where we use independent inverse gamma prior for. The posterior inference for this mixture model is straightforward, for we can use the Pólya-gamma augmentation directly. For details on the prior settings and computation, see the Supplementary Materials.

SECTION: Example: Estimation of time-varying Lorenz curve

As an example of application of the proposed model, we extensively study the estimation of the time-varying Lorenz curve. Here, we do not specify the income distribution explicitly, but work directly on the Lorenz curve under the required shape constraints. We denote the Lorenz curve at timeby. The Lorenz curve maps income level,, to the share of the total income,. Lorenz curvemust be monotonically non-decreasing, and satisfy the boundary conditions,and(e.g.,Rasche et al.1980).
In addition, if the income distribution is continuous, then the Lorenz curve must be convex.
Since we model the Lorenz curve as the convex combination of the basis functions, we must impose the shape constraints and boundary conditions listed above onto the basis functions.

We consider two classes of the basis functions. One is the cumulative distribution functions of the beta distribution(Gelfand and Ghosh,1998);

whereand. We call this choice the beta basis function. Since this basis function is the cumulative distribution function of the continuous distribution supported on, the required shape constraints and boundary conditions are satisfied.
The value of the basis function, or the incomplete beta function, can be numerically evaluated.

Another class of basis functions we consider is given by, where(e.g. Rasche et al.,1980).
For convenience, we call this form the Pareto basis function, for this class includes the Lorenz curve of the Pareto distribution of income whenand.
It is easy to verify that this basis function also satisfies the shape constraints and boundary conditions. The Gini coefficient of this basis function, as explained below, is obtained in a simple form.

The Gini coefficient (or Gini index) summarizes the Lorent curve into a single numerical value as the measure of economic inequality. Given Lorenz curve, the Gini coefficient is defined by

or the twice of the area between the Lorenz curve and 45-degree line. For most income distributions and Lorenz curves, this integral is intractable and requires intensive computational efforts for posterior inference, such as sequential Monte Carlo methods(Kobayashi and Kakamu,2019).
However, since we have expressed the Lorenz curve as the convex combination of the basis functions, we can simplify the expression of the Gini coefficient above as

whereis the Gini coefficient of the-th basis function given by

Thus, the computation of the Gini coefficient reduces to that of, which can be evaluated easily for both the beta and Pareto basis functions. In particular, the Gini coefficient of the Pareto basis function is available in the closed form, or(Rasche et al.,1980).
Given the values of’s, the Gini coefficient is the function of convex weights.
Once the posterior samples ofare obtained, then we can construct the samples offor posterior inference. This is convenient especially in computing the posterior quantiles to assess the uncertainty about, as well as the posterior mean/median as point estimates.

SECTION: Posterior computation

SECTION: Overview

In this section, we derive an augmented model for posterior inference under the proposed model and provide the Gibbs sampler algorithm. Given the observed data, the posterior distribution ofandis given by

In implementing the Gibbs sampler, one must sample from the conditional posteriors ofand. For parameters, the conjugate priors are available and utilized for posterior sampling. For, the conditional posterior distribution does not have a familiar form sinceis a nonlinear function of.

To obtain an efficient sampling scheme for, we propose a novel data augmentation technique using Poisson and Pólya-gamma random variables.
Sinceis the softmax function of, one might expect that the Pólya-gamma augmentation would become applicable as practiced in the Bayeian analysis of the multinomial regression models(Glynn et al.,2019).
Unfortunately, this is not the case in our model, because the likelihood ofis theexponentiatedbinomial likelihood, whereappears in the augment of the exponential function. To this likelihood, the Pólya-gamma augmentation cannot be directly applied.
Below, we show that additional Poisson-distributed latent variables make the likelihood ofbe conditionally binomial, which enables the Pólya-gamma augmentation.

SECTION: Augmentation by latent variables

For notational simplicity, we writeand.
First, noting that, we have

where.
Next, we transform the latent-dimensional vectorin the exponential scale by settingforand define-dimensional vector. Then, it holds thatand.

We propose the following augmentation for each.
Letbe an-dimensional vector obtained by deletingin. Define, which does not contain.
Then, we have

whereis a symmetric-matrix with entries,andand defined as

Computing this quadratic form further, we obtain

The final formula above is the product of two exponential functions and has the following series expression:

which is viewed as the mixture using Poisson-distributed latent variablesand.
Noting that, the binomial likelihood ofis seen in the expression above, to which the Pólya-gamma augmentation is applied as

whereis the density function of, or the Pólya-gamma distribution with parametersand.
In this expression, we can read off the linear and Gaussian likelihood of; we will discuss how this expression is utilized in computing the full conditional ofin the next subsection.

The full conditional distributions of the newly-introduced latent variables can be read-off easily.
The full conditionals ofandare mutually independent and

whereis the Poisson distribution with mean.
Furthermore, the full conditional ofare mutually independent and the conditional distribution ofis.

SECTION: Sampling of latent

For each, conditional on,,and, the likelihood ofbecomes

being proportional towith fictitious data. This conditional likelihood is Gaussian and linear inand convenient for posterior computation together with the Gaussian prior defined in (1).
However, note that we have conditionedto obtain this expression, hence the full conditional we are working on here is not the joint distribution of, but the conditional distribution offor fixed. Accordingly, the conditional prior must be computed.

For someand, consider the full conditional posterior of. The prior used in this computation consists ofand, both of which are defined from the expression in (1). The former is further computed as the function ofand proportional to the conditional distribution of, and the latter is also obtained similarly.
While these are the univariate normal distributions, their conditional means and variances involve the manipulation of matrices of size. In general, computing the conditional means and variances at every MCMC iteration could be cumbersome and computationally costly.

In our application, we setandas often assumed in econometric analysis, so that the prior in (1) becomes the independent, univariate AR processes with stationary mean. This simplifies the expression ofin terms ofas. Furthermore, since the prior process ofis independent of the othersfor, the full conditional posterior ofbecomes a multivariate normal distribution and can be obtained as the joint posterior of the pseudo model:

This is a dynamic linear model, for which an efficient algorithm of simulation from the conditional posterior is known as the simulation smoother(De Jong,1991; De Jong and Shephard,1995). The simulation of state variables under this pseudo DLM is integrated into posterior computation as one step of the Gibbs sampler algorithm(Carter and Kohn,1994; Frühwirth-Schnatter,1994).

SECTION: Summary of Gibbs sampler

To complete the Gibbs sampler algorithm, we detail the sampling step forand. In general, under the conjugate priors for, the full conditionals become normal, truncated (univariate) normal and inverse-Wishart distributions, respectively, from which it is easy to simulate. In our application, where we assumeand, we use independent conjugate priors,,,forand.
We also assume the stationarity ofand employ the prior.

The sampling steps for’s, the unknown parameters and the latent variables are summarized as follows:

Sampling of parameters; Conditional on, and with the other latent variables being marginalized out, generate the samples of parameters as follows.

(Sampling of)For, denote the sample obtained at the previous iteration by. Then, we generate a candidate, where

and accept it with probability

(Sampling of)For, the full conditional distribution ofis, where

(Sampling of)For, the full conditional distribution ofis, where

(Sampling of)The full conditional distribution ofis, where

Sampling of state variables; For each, generateand, as follows.

(Sampling of latentand)Generateandfrom (5).

(Sampling of latent)Generatefrom.

(Sampling of)Generateby using the simulation smoother to the pseudo dynamic linear models in (6).

Owing to the novel data augmentation, all the sampling steps are simply the direct simulation from familiar distributions.
Although the proposed algorithm introduces multiple latent variables and needs iterative updates of each element of, the mixing of the Markov chains is quite reasonable in our numerical examples, as we confirm in Section4.

SECTION: Simulation Studies

In this section, together with the real data analysis of Section5, we investigate the performance of the proposed model and computational method numerically. The goal of our numerical study is threefold: the illustration of the proposed method, the sensitivity analysis of the choice of basis functions, and the comparison with other possible approaches.

SECTION: Simulation data

Throughout this section, we use the dataset simulated from the process described in this subsection. We setand consider two choices of the true number of basis functions:. For, the augments are set as() or(). The basis functions used to generate the synthetic data are theBeta basis functions with parameters,, and. Then, we generate the synthetic data,for each, from the additive noise model below:

where we set,and. For coefficient, we setand consider three cases:.

SECTION: Settings and illustration of the proposed models

For the simulated dataset, we apply the proposed state space models, defined by (1) and (2), with diagonaland.
For the unknown parameters, we employ prior distributions,,,, andfor.

We first confirm that the posterior inference under the proposed model is successful when using the same basis functions as those in the data generating process.
In this experiment, we generated 30,000 posterior samples after discarding initial 10,000 MCMC samples as burn-in periods. The raw computational time to generate those samples in the case ofwas 240 minutes when the proposed Gibbs sampler was executed inRon our laptop computer with 1.6GHz Dual-Core Intel Core i5 processor equipped with 8 GB of RAM.
The posterior means are close to the true values, and the 95% credible intervals cover most of the true values.
The effective sample sizes (ESS) are sufficiently large in all scenarios, showing the efficiency of the proposed Gibbs sampler. For more details, see the Supplementary Materials.

SECTION: Comparison with mixture approach

We next consider the mixture model— an easier approach to the shape constraint on mean functions— and observe its data fit in the additive noise situation. The mixture model we consider has the observational equation in (4), while the same state equation (1) is used for the convex weights. The prior for component variance is set asfor allindependently.
The basis functions, or the component means of the mixture, are those used in the data generating process. The prior for the other parameters, as well as the number of the MCMC iterations and burn-in period, are the same.

The comparison with the proposed method is made via the posterior distribution of convex weights, Gini coefficient, andfor each timeandcomputed by the MCMC samples. Note that the Gini coefficient under the mixture model (4) is the weighted average of the Gini coefficients of the basis functions and easily evaluated.
Table1presents the root mean squared errors (RMSE) of posterior means, empirical coverage probabilities (CP) and average lengths (AL) of 95% credible intervals forandaveraged overunder the proposed functional state space models (FSSM) and mixture models.
The CPs of the FSSM are around the nominal level for all the parameters, showing reasonable posterior uncertainty quantification. The RMSE and AL values of the FSSM underare smaller than those undersince more data are observed when.
By contrast, the CPs of the mixture approach are all significantly lower than the nominal level. This undercoverage, together with the higher RMSEs, indicates a misfit of the mixture model in the additive noise situation.
A possible reason for this is that, as pointed out in Section2.2, parameteris used in modeling not only the mean function but also the observational variance, hence the inference on the convex weights is strongly affected by the observational noises.

SECTION: Other models and basis functions

We next investigate the estimation performance of the proposed and other methods, using the synthetic datasets of.

The proposed model (FSSM): So far, the basis functions of the data generating process have been used in estimation as well (referred to as “oracle”).
Here, we also implement a set of seven Pareto basis functions (referred to as “misspecified”) with parameters=,,,,,, and. The same prior and MCMC settings are used for this model.

Autoregressive Gaussian process model (ARGP): Gaussian processes are the standard models for functional data. Here, we use a Gaussian process to model the evolution of the time-varying function. The model has the state space representation as:

where, andare unknown parameters, andis acovariance matrix whose-element is, or the exponential correlation function.
Note that the shape constraints are not imposed on the estimated functions. While using this “vanilla” version of ARGP, we also consider the projection method(Lin and Dunson,2014)as the “projected” ARGP— a post-processing approach that imposes the monotonicity on the sampled functional values under the ARGP model. In implementing this model, we set, or the uniform distribution on interval,,and.
For details on the computational method, see Chapter 11,Banerjee et al. (2014).

Multivariate dynamic linear model (DLM): Viewing the observed functional values as the multivariate time series data, we apply a dynamic linear model. Specifically, a local level model with the random walk state evaluation is considered:

In specifying observational and state variances, we consider two models. One is the constant variances,andwith inverse-Wishart (IW) priors:and. The other is a conjugate stochastic volatility (SV) model; we set, specifyby discounting with a discount factor of, and apply the matrix-beta inverse-Wishart processes forwith discount factor. For details on the model and computational method, see Chapter 10,Prado et al. (2021).

The ARGPs and DLMs considered here are easily estimated by the standard Gibbs sampler. However, one needs more efforts for the computation of the Gini coefficients under those models, which is detailed in the Supplementary Materials.

We first computed the posterior means of the time-varying Gini coefficients under the listed models, which are summarized in Figure1.
It is noteworthy that the FSSM with the misspecified Pareto basis functions can provide accurate point estimates, implying the robustness of the proposed methods to the choice of basis functions. The ARGP and DLM significantly underestimate the true Gini coefficients. The estimation accuracy improves in all models as more data becomes available (), but the estimates closest to the true values are still provided by the FFSMs.

In addition, we conduct the posterior predictive analysis ofvia the posterior predictive loss(Gelfand and Ghosh,1998)to compare the data fit of the candidate models. Specifically, the posterior predictive distributions are summarized into two measures: the sum of posterior predictive variances (PPV) and the total posterior predictive squared errors (PPSE, or the sum of variance and squares of bias).
The results are presented in Table2.
The PPVs and PPSEs of the misspecified FSSM are only slightly larger than those of the oracle model in all scenarios, by which we confirm the successful posterior analysis even with the misspecified basis functions. The ARGP models are outperformed by the proposed methods in both measures, and the effect of post-projection is negligible in this example. The DLM with the constant variances (indicated by IW) has the smaller PPV than the FSSMs when much data are available (), but its PPSE is larger than those of the FSSMs due to the bias caused by the limited flexibility of this DLM.

In the Supplementary Materials, we present the RMSEs, CPs and ALs of functional valuesfor several augments, showing the superiority of the proposed models.

SECTION: Application to Japanese Income Survey Data

In this section, we estimate the dynamic Lorenz curves and Gini coefficients using the monthly income share data in Japan.
The data is retrieved from the Family Income and Expenditure Survey prepared by the Ministry of Internal Affairs and Communications of Japan (available athttps://www.e-stat.go.jp/en).
Our dataset contains the income shares of theincome classes of the 10,000 working households surveyed between January 2000 and September 2018 (), being adjusted to the population size.
These classes are equally sized; each class covers 20% of the households, hence,,and.
A similar dataset is analyzed inKobayashi et al. (2021)by using the parametric Dirichlet likelihood to model time-varying Lorenz curve.

Here, we apply the proposed methods with beta basis functions and three sets of hyperparameters:

Note that Basis Set 2 is nested in Basis Set 3.
We plot the basis functions used in Basis Sets 1 and 3 with the observed functional values in Figure2. This figure confirms that these choices are not overly misspecified; the convex combination of these basis functions is expected to explain the observed functional values.
In implementing the proposed models, we assigned the same prior distributions as those in Section4.
Furthermore, we also applied the comparative methods used in Section4.
In all cases, 80,000 posterior samples are generated after discarding the initial 20,000 samples as burn-in period.

First, we compute the log PPV and PPSE of each model (see Section4for details). The results are provided in Table3.
The three proposed FSSMs have smaller PPVs and PPSEs than the ARGPs and DLMs and show their better fit to the dataset used in this example.
Among the three basis function choices, the model with Basis Set 1 best fits the data in both measures.
It is worth noticing that, in Figure2, the basis functions used in Basis Set 1 are not necessarily close to the observed functional values.
This observation indicates that the variations in basis functions could contribute to the overall fitting.
In the Supplementary Material, we provide the summary of posterior inference on the model parameters and time series plots of the posterior means of convex weights.

Figure3presents posterior predictive means of the income shares for the bottom 20, 40, 60, and 80% (), obtained by the FSSM (Basis Set 1), ARGP and DLM.
The FSSM can successfully estimate smoothed time trends. By contrast, the DLM provides overly smoothed time trends.
The ARGP model tends to overfit the data and fails to extract meaningful time trends, possibly due to the Gaussian process without shape restrictions being overly flexible.

In Figure4, we present the time series of the estimated Gini coefficients. In the same figure, we also show the non-parametric upper and lower bounds of Gini coefficients computed by using observed points(Mehran,1975).
The estimated Gini coefficient of the three FFSMs are almost identical and included in the intervals of the theoretical lower and upper bounds at all time points.
By contrast, the estimates of ARGP are almost identical to the theoretical lower bound since it tends to overfit the observed data as confirmed in Figure3.
The DLM also underestimates the Gini coefficient; its estimates are even smaller than the theoretical lower bound.

SECTION: Concluding Remarks

In this studies, we proposed a state-space modeling for time-varying functions with shape restrictions (in particular, monotone and convex functions with boundary conditions) and developed an efficient posterior computation algorithm using a novel data augmentation strategy combined with the filtering/smoothing algorithm.
We also considered other possible approaches, including the mixture model, Gaussian processes and time series models, but they were not as competitive as the proposed model in our simulation and real data applications.

One potential limitation of the proposed method is that it may not be able to capture abrupt structural changes in time-varying functions due to the Gaussian autoregressive models for dynamic convex weights.
This issue could be solved by revisiting the modeling of state variables.
From the viewpoint of computational statistics, it is worth emphasizing that the data augmentation strategy used in this paper can also be applied to other models involving convex weights in the location parameter. Finally, although we focused on the times series analysis in this study, the proposed state-space model could be extended to handle spatial or spatio-temporal data, which will be left to a future study.

SECTION: Acknowledgement

We thank Kazuhiko Kakamu for his helpful comments on the early version of this paper.
This work is partially supported by Japan Society for Promotion of Science (KAKENHI) grant numbers 20H00080, 21H00699, 22K13374, 22K20132, 20J10427 and 19K11852.

SECTION: References

Supplementary Material for “State-Space Modeling of Shape-constrained Functional Time Series”

This Supplementary Material provides the details of posterior computation algorithms of comparative methods and additional numerical results.

SECTION: MCMC algorithm of mixture approach

The MCMC method for the mixture model can be derived easily. With latent variablebeing introduced in the model, we have

The likelihood ofis the softmax function:

wherefollows the dynamic model (1) and.
The full conditional distributions are described as follows:

(Sampling of)The full conditional probability beingis

(Sampling of)Givenfor, the likelihood ofcan be augmented as

where,andis the density of.
Note that the above augmentation can be done separately for.
The full conditional distribution ofis.
Under the augmentation, the full conditional posterior ofis equivalent to the posterior of the pseudo model:

where

Then, we can implement the algorithm of filtering and smoothing to generatefor.

(Sampling of)By assuming the prior, the full conditional distribution is, where

SECTION: The Gini coefficients under ARGPs and DLMs

SECTION: ARGPs

The Gini coefficient ofis given by

Denote theequally-spaced grid points onby, i.e.,,, andfor all. Then we can approximate the integral above by the Riemann sum as

Hence, the posterior samples ofcan be constructed from those of, which can be generated easily when using the ARGPs, as explained below.

We first note that thearguments of the observed functional values, or, are not necessarily identical to thegrid points introduced above. Here, for simplicity, we assume thatandare included in the grid points. Then, letbe the vector, whereandby the boundary conditions. Then, we can relate this vector to the mean function in the main text,; there exists thematrixsuch that

Similarly, we re-define the observation as the-dimensional vector,

With these variables, the target model has the state-space representation as

whereis acovariance matrix whose-element isforand the others are 0, and

In Sections4and5, we set, that is,for. Note that these grids include. The priors forare specified in the main text.

SECTION: DLMs

Unlike the ARGPs, we cannot implement the model-based interpolation of the functional values under the DLMs. That is,is not available (or does not exist in the definition of the model) for. Hence, the approximation of the Gini coefficients must rely only on the functional values on the observed points, or.
We calculate the approximate Gini coefficients by using the posterior samples ofby

where,,, and.
The summation in the expression above is (the twice of) the area of the polygon obtained by connecting each point of. Withbeing replaced with observed functional values, the above expression gives the non-parameteric lower bounds used in the main text.

SECTION: Additional Simulation Results

SECTION: Point and interval estimates of the proposed method

TableS1summarizes the posterior analysis under the proposed FSSM with the basis functions being correctly specified.
The effective sample size (ESS) is confirmed to be sufficiently large in both cases of. The ESS increases indue to the increase of the information at each time.
Regarding the estimation performance, the posterior means are reasonably close to the true values, and the coverage of the true values by thecredible intervals is successful.
It is also reasonable that the posterior means undertend to be closer to the true values with shorter credible intervals than those undersince we have more data and information with larger.

SECTION: The estimation of the Lorenz curves under various models

An in-depth comparison is made via the estimation of functional valuefor.
TableS2presents the RMSE of posterior means, AL and CP ofcredible intervals of, averaged over.
The FSSM with the oracle basis functions attain the smallest RMSEs, as expected, since it
uses the same basis functions as in the data generating process.
With the misspecified Pareto basis functions, the RMSEs are almost as small as those of the oracle model. This result implies the robustness and flexibility of the FSSMs, although the choice of sufficiently largeand appropriate basis functions are still important. The RMSEs of the ARGPs and DLMs are much higher than those of the FSSMs, showing the limitation of their model flexibility in functional estimation under the shape constraints. Note that the difference between the vanilla and projected ARGPs is almost negligible. In fact, most of the functional values sampled from the vanilla ARGP satisfied the shape constraints.

FiguresS1andS2are the time series plots of the posterior means ofunder the FSSMs, ARGPs and DLMs.
It is observed that in both cases (and), the proposed FSSM precisely estimates the true values of, and the difference between “oracle” and “misspecified” basis functions is limited.
On the other hand, the estimates of ARGPs are highly variable than the true values. The DLMs provide over-smoothed estimates; this explains the large bias in posterior predictive analysis discussed in the main text. The increase of available information improves the model fit. For example, it is visually clear that the estimates ofunder the APRG are less volatile when.

SECTION: Detailed MCMC results in Japan Income Survey Data

We provide posterior summary (posterior means andcredible intervals) of the unknown model parameters in FSSM (with Basis Set 1) in TableS3.
It is confirmed that ESS are sufficiently large for all the parameters.
The estimates ofare close to unity, especially for, implying high autocorrelations of dynamic convex weights.

FigureS3is the time series plots of the posterior means of.
The dynamics of the estimated weights might look subtle visually, but surely contribute to the volatile behaviors of the Lorenz curves and Gini coefficients in Figures3and4.
The largest weight is placed on the basis function of Beta(1.5, 1.0), which is the closest one to the observed functional values on.