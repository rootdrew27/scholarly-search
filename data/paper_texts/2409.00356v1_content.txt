SECTION: Contrastive Augmentation: An Unsupervised Learning Approach for Keyword Spotting in Speech Technology

This paper addresses the persistent challenge in Keyword Spotting (KWS), a fundamental component in speech technology, regarding the acquisition of substantial labeled data for training. Given the difficulty in obtaining large quantities of positive samples and the laborious process of collecting new target samples when the keyword changes, we introduce a novel approach combining unsupervised contrastive learning and a unique augmentation-based technique. Our method allows the neural network to train on unlabeled data sets, potentially improving performance in downstream tasks with limited labeled data sets. We also propose that similar high-level feature representations should be employed for speech utterances with the same keyword despite variations in speed or volume. To achieve this, we present a speech augmentation-based unsupervised learning method that utilizes the similarity between the bottleneck layer feature and the audio reconstructing information for auxiliary training. Furthermore, we propose a compressed convolutional architecture to address potential redundancy and non-informative information in KWS tasks, enabling the model to simultaneously learn local features and focus on long-term information. This method achieves strong performance on the Google Speech Commands V2 Dataset. Inspired by recent advancements in sign spotting and spoken term detection, our method underlines the potential of our contrastive learning approach in KWS and the advantages of Query-by-Example Spoken Term Detection strategies. The presented CAB-KWS provide new perspectives in the field of KWS, demonstrating effective ways to reduce data collection efforts and increase the system’s robustness.

SECTION: 1INTRODUCTION

Keyword Spotting (KWS) is a fundamental application in the field of speech technology, playing a pivotal role in real-world scenarios, particularly in the context of interactive agents such as virtual assistants and voice-controlled devices. KWS is designed to detect a small set of pre-defined keywords within an audio stream. This capability is crucial for two primary reasons. First, it enables the initiation of interactions through specific commands like "hey Siri" or "OK, Google," effectively serving as an explicit cue for the system to start processing subsequent speech. Second, KWS can identify sensitive words within a conversation, thereby playing a vital role in protecting the privacy of the speaker. Given these applications, it is crucial to develop accurate and reliable KWS systems for effective real-world speech processing[9,11,18].

Despite the considerable advancements in KWS, a significant challenge that persists is the acquisition of sufficient labeled data for training. This is especially true for positive samples, which are often harder to obtain in large quantities. This issue is further exacerbated when the keyword changes, as it necessitates the collection of new target samples, a process that can be both time-consuming and resource-intensive. To address these challenges, we propose a novel approach that leverages the power of unsupervised contrastive learning and a unique augmentation-based method. Additionally, another potential problem is redundant information, speeches are noisy and complex, where only some key phrases are highly related to the keywords. However, convolutional methods treat all the word windows equally, ignoring that different words have different importance and should be weighted differently within word windows. Besides, the sliding windows used in the convolutional methods produce a lot of redundant information. Thus, it is important to reduce the non-informative and redundant information and distinguish the contributions of different convolutional features.

Our method enables the neural network to be trained on unlabeled datasets, reducing the reliance on extensive labeled data. This technique can greatly enhance the performance of downstream tasks, even in scenarios where labeled datasets are scarce. Additionally, we propose that speech utterances containing the same keyword, regardless of variations in speed or volume, should exhibit similar high-level feature representations in KWS tasks. To achieve this, we present a speech augmentation-based unsupervised learning approach. This method leverages the similarity of bottleneck layer features, along with audio reconstruction information, for auxiliary training to improve system robustness.

In addition to these innovations, we propose a compressed convolutional architecture for the KWS task. This architecture, designed to tackle the issue of redundant information, has demonstrated strong performance on the Google Speech Commands V2 Dataset. By doing so, it enables the model to learn local features and focus on long-term information simultaneously, thereby enhancing its performance on the KWS task.

Our approach is inspired by recent advancements in the field of sign spotting and spoken term detection. For instance, Varol et al.[21]demonstrated the effectiveness of Noise Contrastive Estimation and Multiple Instance Learning in sign spotting, which could provide insights into the use of contrastive learning in KWS. Similarly, the works of Tejedor et al.[19,20]on Query-by-Example Spoken Term Detection (QbE STD) highlight the potential of QbE STD strategies in outperforming text-based STD in unseen data domains, reinforcing the potential advantages of our proposed method.

Our major contributions in this work are as follows:

We introduce a compact convolutional architecture for the KWS task that achieves strong results on the Google Speech Commands V2 Dataset.

We develop an unsupervised loss and a contrastive loss to evaluate the similarity between original and augmented speech, as well as the proximity within each minibatch.

We introduce a speech augmentation-based unsupervised learning approach, utilizing the similarity between the bottleneck layer feature, as well as the audio reconstructing information for auxiliary training.

Theremainder of this paper is structured as follows. Section2provides an overview of related work in the areas of data augmentation, unsupervised learning, and other methodologies of KWS tasks. Section3offers a background on contrastive learning. Section4details the proposed model architecture and our augmentation-based unsupervised contrastive learning loss. Section5discusses the configuration, research questions, and experimental setups. Section6presents the experimental results and compares them with other pre-training methods. We also discuss the relationship between pre-training steps and the performance of downstream KWS tasks. Finally, Section7concludes the paper with a summary of our findings and potential avenues for future work.

SECTION: 2RELATED WORK

Data augmentation is widely acknowledged as an effective technique for enriching the training datasets in speech applications, such as Automatic Speech Recognition (ASR) and Keyword Spotting (KWS). Various methods have been explored, such as vocal tract length perturbation[5], speed-perturbation[8], and the introduction of noisy audio signals[4]. More recently, spectral-domain augmentation techniques, such as SpecAugment[15]and WavAugment[7], have been developed to further improve the robustness of speech recognition systems. In this work, we extend these efforts by applying speed and volume perturbation in our speech augmentation method.

While supervised learning has been the primary approach in the KWS area, it often requires large amounts of labeled data, which can be challenging to obtain, especially for less frequently used languages. This has sparked growing interest in weakly supervised and unsupervised approaches. For example, Noisy Student Training, a semi-supervised learning technique, has been employed in ASR[16]and subsequently adapted for robust keyword spotting[17]. Additionally, unsupervised methods for KWS have been investigated[3,10,25], yielding promising outcomes. Building on these efforts, we propose an unsupervised learning framework for the keyword spotting task in this paper.

The Google Speech Commands V2 Dataset is a widely used benchmark for novel ideas in KWS. Numerous works have performed experiments on this dataset, introducing various architectures and methods. For instance, a convolutional recurrent network with attention was introduced by[2], and a deep residual network, MatchboxNet, was proposed by[12]. More recently, an edge computing-focused model called EdgeCRNN[24]was introduced, along with a method that integrates triplet loss-based embeddings with a modified K-Nearest Neighbor (KNN) for classification[22]. In this work, we also evaluate our speech augmentation-based unsupervised learning method on this dataset and compare it with other unsupervised approaches, including CPC[13], APC[1], and MPC[6].

SECTION: 3PRELIMINARY STUDY OF CONTRASTIVE LEARNING

In the context of a classification task involvingclasses, we consider a datasetwithtraining samples. Eachrepresents an input sentence ofwords, and eachis the corresponding label. We denote the set of training sample indexes byand the set of label indexes by.

We explore the realm of self-supervised contrastive learning, a technique that has demonstrated its effectiveness in numerous studies. Giventraining sampleswith a number of augmented samples, the standard contrastive loss is defined as follows:

Here,is the normalized representation ofis the set of indexes of the contrastive samples, thesymbol denotes the dot product, andis the temperature factor.

However, self-supervised contrastive learning does not utilize supervised signals. A previous study [Khosla et al., 2020] incorporated supervision into contrastive learning in a straightforward manner. It simply treated samples from the same class as positive samples and samples from different classes as negative samples. The following contrastive loss is defined for supervised tasks:

Despite its effectiveness, this approach still requires learning a linear classifier using the cross-entropy loss apart from the contrastive term. This is because the contrastive loss can only learn generic representations for the input examples. Thus, we argue that the supervised contrastive learning developed so far appears to be a naive adaptation of unsupervised contrastive learning to the classification

SECTION: 4Proposed Method

The keyword spotting task can be framed as a sequence classification problem, where the keyword spotting network maps an input audio sequenceto a set of keyword classes. Here,represents the number of frames, anddenotes the number of classes. Our proposed keyword spotting model, depicted in FigLABEL:fig(A), consists of five key components: (1) Compressed Convolutional Layer, (2) Transformer Block, (3) Feature Selection Layer, (4) Bottleneck Layer, and (5) Projection Layer.

SECTION: 4.1Compressed Convolutional Layer

The Compressed Convolutional Layer replaces the CNN block in the original design. This layer learns dense and informative frame representations from the input sequence. Specifically, it utilizes convolutional neural networks (CNNs), an attention-based soft-pooling approach, and residual convolution blocks for feature extraction and compression.

Just as in the original CNN block, the convolution operation is applied to each frame. Given the input sequenceand the-th filter, the convolution for the-th frame is expressed as

whereis the learned parameter of the-th filter.

To eliminate redundant information in the speech dataset, we propose an attention-based soft-pooling operation on the frame representations learned by the previous equation. Specifically, given a frame, its neighboring frames, and the corresponding filter, we first learn the local-based attention scoreswith softmax function, and then conduct the soft-pooling operation to obtain the compressed representation as in the following equation:

We now have a denoised matrixthat represents the input sequence. To avoid vanishing gradients and facilitate model training, we introduce residual blocks on top of the compressed features. In particular, we replace the batch norm layer with the group norm layer. Letdenotes the number of residual blocks, we have

whereis the operation of the residual convolution block.

SECTION: 4.2ResLayer Block

The output from the Compressed Convolutional Layer,, is then fed into the Transformer Block. This block captures long-term dependencies in the sequence via the self-attention mechanism:whereis the number of self-attention layers.

Following the Transformer Block, the Feature Selecting Layer is implemented to extract keyword information from the sequence.

Here, the lastframes ofare gathered, and all the collected frames are concatenated together into one feature vector.

After the Feature Selecting Layer, a Bottleneck Layer and a Projection Layer are added. These layers map the hidden states to the predicted classification classes.

Finally, the cross-entropy (CE) loss for supervised learning and model fine-tuning is computed based on the predicted classesand ground truth classes..

SECTION: 4.3Augmentation Method

Data augmentation is a widely utilized technique to enhance model performance and robustness, particularly in speech-related tasks. In this study, we delve into speed and volume-based augmentation in the context of unsupervised learning for keyword detection. A specific audio sequence, represented as, is defined by its amplitudeand time index.

Regarding speed augmentation, a speed ratio symbolized byis established to modify the speed of. The following formula describes this process:. For volume augmentation, similarly, we set an intensity ratio,, to alter the volume of, as presented in the following equation:. By using various ratiosand, we can generate multiple pairs of speech sequences,, to facilitate the training of the audio representation network via unsupervised learning. The fundamental assumption is that speech utterances, regardless of speed or volume variations, should exhibit similar high-level feature representations for keyword-spotting tasks.

SECTION: 4.4Contrastive Learning Loss

We aim to align the softmax transform of the dot product between the feature representationand the classifierof the input examplewith its corresponding label. Letdenote the column ofthat corresponds to the ground-truth label of. We aim to maximize the dot product. To achieve this, we learn a better representation ofandusing supervised signals.

The Dual Contrastive Loss exploits the relation between different training samples to maximizeifhas the same label as, while minimizingifcarries a different label from.

To define the contrastive loss, given an anchororiginating from the input example, we takeas positive samples andas negative samples. The contrastive loss is defined as follows:

Here,is the temperature factor,is the set of indexes of the contrastive samples,is the set of indexes of positive samples, andis the cardinality of. Similarly, given an anchor, we takeas positive samples andas negative samples. The contrastive loss is defined as follows:

Finally, Dual Contrastive Loss is the combination of the above two contrastive loss terms:

As illustrated in Fig.1(B), the structure of the proposed unsupervised learning method rooted in augmentation, involves two primary steps akin to other unsupervised strategies: (1) unsupervised data undergoes initial pre-training and (2) supervised KWS data is then fine-tuned. The pre-training phase sees the extraction of a bottleneck feature by training the unlabelled speech, which is subsequently used for KWS prediction in the fine-tuning stage.

In pre-training, the paired speech datais fed into CNN-Attention models with identical parameters. Sinceis derived from, the unsupervised method we’ve developed assumes that bothandwill yield analogous high-level bottleneck features. This implies the speech content remains identical regardless of the speaker’s speed or volume. The network’s optimization, therefore, must highlight the similarity betweenand. The Mean Square Error (MSE)is utilized to determine the distance betweenand’s output.

In this context,represents the dimensions of the bottleneck feature vector, whileandcorrespond to the bottleneck layer outputs for the original speechand the augmented speech, respectively.

The network also includes an auxiliary training branch designed to predict the average feature of the speech segment input, helping the network learn the intrinsic characteristics of speech utterances. To achieve this, the average vector of the input Fbank vectoris first calculated along the time axis. A reconstruction layer connected to the bottleneck layer is then used to reconstruct this average Fbank vector. The MSE lossis applied to measure the similarity between the original and reconstructed audio vectors along the feature dimension.

In this context,denotes the dimension of the Fbank feature vector, andrepresents the mean vector of. The lossbetween the augmented average audioand the reconstructed featurecan be similarly defined as:

Hence, the final unsupervised learning (UL) loss functioncomprises of the three aforementioned losses, and

Whereare the factor ratios of each loss component.

In the fine-tuning stage, the average feature prediction branch is discarded, and a projection layer, followed by a softmax layer, is added after the bottleneck layer for KWS prediction. The original network’s parameters can either be kept fixed or adjusted during fine-tuning. Our experiments indicate that adjusting all parameters enhances performance, so we choose to update all parameters during this phase.

SECTION: 5EXPERIMENT SETUP

In this section, we evaluated the proposed method on keyword spotting tasks by implementing our CNN-Attention model with supervised training and comparing it to Google’s model. An ablation study was conducted to examine the impact of speed and volume augmentation on unsupervised learning. Additionally, we compared our approach with other unsupervised learning methods, including CPC, APC, and MPC, using their published networks and hyperparameters without applying any additional experimental tricks [23]-[25]. We also analyzed how varying pre-training steps influence the performance and convergence of the downstream KWS task.

SECTION: 5.1Datasets

We used Google’s Speech Commands V2 Dataset[23]for evaluating the proposed models. The dataset contains more than 100k utterances. Total 30 short words were recorded by thousands of different people, as well as background noise such as pink noise, white noise, and human-made sounds. The KWS task is to discriminate among 12 classes: "yes", "no”, "up”, "down", "left", "right”, "on", "off", "stop", "go", unknown, or silence. The dataset was split into training, validation, and test sets, withtraining,validation, andtest. This results in about 37000 samples for training, and 4600 each for validation and testing. We applied the HuNonspeech1real noisy data to degrade the original speech. In our experiments, this strategy was executed using the Aurora4 tools. Each utterance was randomly corrupted by one of 100 different types of noise from the HuNonspeech dataset. The Signal Noise Ratio (SNR) for each utterance ranged from 0 to 20 dB, with an average SNR ofacross all datasets.

“vo-pre." means volume pre-training; “sp-pre." is speed pre-training; “vo-sp-pre." indicates volume & speed pre-training; “contras." is contrastive learning.

As with other unsupervised approaches, a large unlabeled corpus, consisting of 100 hours of clean Librispeech[14]audio, was used for network pre-training through unsupervised learning. Initially, the long utterances were divided into 1-second segments to align with the Speech Commands dataset. Following this, the clean segments were mixed with noisy HuNonspeech data using Aurora 4 tools, employing the same corruption mechanism as the Speech Commands.

SECTION: 5.2Model Setup

The model architecture consists of:

CNN blocks with 2 layers, a 3x3 kernel size, 2x2 stride, and 32 channels.

Transformer layer with 2 layers, a 320-dimensional embedding space, and 4 attention heads.

Feature Selecting Layer retains the last 2 frames with a 2x320 dimension.

Bottleneck Layer with a single fully connected (FC) layer of 800 dimensions.

Project Layer with one FC layer outputting a 12-dimensional softmax.

Reconstruct Layer with one FC layer outputting a 40-dimensional softmax.

The factor ratio is set to,,, and.

To demonstrate the effectiveness, we compared with other approaches:

Supervised Learning: Used Google’s Sainath and Parada’s model as baseline.

Unsupervised Learning:

Contrastive Predictive Coding (CPC): Learns representations via next step prediction.

Autoregressive Predictive Coding (APC): Optimizes L1 loss between input and output sequences.

Masked Predictive Coding (MPC): Utilizes Transformer with Masked Language Model (MLM) structure for predictive coding, incorporating dynamic masking.

SECTION: 6EXPERIMENTAL RESULTS

SECTION: 6.1Comparision of KWS Model (RQ1)

The table compares the classification accuracy of three different KWS models: (1) the model by Sainath and Parada (Google), (2) the CAB-KWS model without volume augmentation, and (3) the CAB-KWS model with speed augment. It can be observed that the CAB-KWS model with speed augment achieved the highest classification accuracy on both the development (Dev) and evaluation (Eval) datasets. This research question aims to investigate how the inclusion of data augmentation techniques, specifically speed augment in this case, improves the performance of KWS models compared to models without these techniques. The results could be used to guide future development of KWS models and to optimize their performance for various applications.

SECTION: 6.2Ablation Study (RQ2)

The CAB-KWS keyword spotting model is an advanced solution designed to improve the classification accuracy of speech recognition tasks. The ablation study presented in the table focuses on evaluating the impact of different pre-training techniques, such as volume pre-training, speed pre-training, combined volume and speed pre-training, and combined volume, speed, and contrastive learning pre-training, on the model’s performance. By comparing the classification accuracy of CAB-KWS when fine-tuned on two datasets, Speech Commands and Librispeech-100, we can better understand the effectiveness of these pre-training techniques and their combinations.

Firstly, Tab.2shows that the CAB-KWS model with speed pre-training (sp-pre.) outperforms the model with volume pre-training (vo-pre.) in both datasets. This result indicates that speed pre-training is more effective in enhancing the model’s classification accuracy than volume pre-training. However, the combination of volume and speed pre-training (vo-sp-pre.) further improves the model’s performance, demonstrating that utilizing both techniques can lead to better keyword spotting results.

Moreover, the inclusion of contrastive learning (contras.) in the pre-training process yields the highest classification accuracy in both Speech Commands and Librispeech-100 datasets. The CAB-KWS model with combined volume, speed, and contrastive learning pre-training (vo-sp-pre-contras.) outperforms all other models, highlighting the benefits of incorporating multiple pre-training methods. This result emphasizes the goodness of the CAB-KWS model, as it demonstrates its adaptability and capability to leverage various pre-training techniques to enhance its performance.

The CAB-KWS model’s strength lies in its ability to capitalize on different pre-training methods, which can be tailored to suit specific datasets and tasks. By combining these techniques, the model can learn more robust and diverse representations of the data, leading to improved classification accuracy. This adaptability makes the CAB-KWS model particularly suitable for a wide range of applications in keyword spotting and speech recognition tasks, where performance and generalizability are of utmost importance.

In conclusion, the goodness of the CAB-KWS keyword spotting model is showcased through its ability to integrate various pre-training techniques, such as volume pre-training, speed pre-training, and contrastive learning, to improve classification accuracy. The ablation study demonstrates that the combination of these methods leads to the highest performance across different datasets, highlighting the model’s adaptability and effectiveness in handling diverse keyword spotting tasks. This advanced model, with its robust pre-training methods and fine-tuning capabilities, offers a promising solution for speech recognition applications and can contribute significantly to advancements in the field.

SECTION: 6.3Comparison with Unsupervised Models (RQ3)

The CAB-KWS model is a sophisticated keyword spotting solution that integrates multiple pre-training techniques to improve classification accuracy in speech recognition tasks. The Tab.3provided presents a comparison of the CAB-KWS model with three other models that employ individual pre-training methods, namely Contrastive Predictive Coding (CPC), Autoregressive Predictive Coding (APC), and Masked Predictive Coding (MPC). By comparing the performance of these models, we can gain insights into the effectiveness of the CAB-KWS model and highlight its advantages over models based on single pre-training techniques.

The comparison in the table reveals that the CAB-KWS model consistently achieves the highest classification accuracy on both the development (Dev) and evaluation (Eval) datasets when fine-tuned on Speech Commands, regardless of the pre-training data source (Speech Commands or Librispeech-100). This result underlines the goodness of the CAB-KWS model as it demonstrates its ability to effectively utilize multiple pre-training techniques to outperform models that rely on individual pre-training methods.

The CAB-KWS model’s superior performance can be attributed to its ability to integrate and capitalize on the strengths of various pre-training techniques. By combining different methods, the model can learn more diverse and robust representations of the data, which in turn leads to improved classification accuracy. This adaptability makes the CAB-KWS model particularly suitable for a wide range of applications in keyword spotting and speech recognition tasks, where performance and generalizability are crucial.

Furthermore, the CAB-KWS model’s consistent performance across different pre-training data sources indicates its flexibility and robustness. It is not limited by the choice of pre-training dataset, which is an essential aspect of its goodness. This characteristic allows the model to be adaptable and versatile, enabling its use in various speech recognition applications with different data sources.

In summary, the CAB-KWS keyword spotting model showcases its goodness by effectively combining multiple pre-training techniques to achieve superior classification accuracy compared to models based on individual pre-training methods. Its consistent performance across different pre-training data sources highlights its adaptability, making it a promising solution for diverse speech recognition tasks. The CAB-KWS model’s ability to harness the strengths of various pre-training techniques and deliver enhanced performance demonstrates its potential to contribute significantly to advancements in the field of speech recognition.

SECTION: 7CONCLUSION

This paper presents a robust approach for the Keyword Spotting (KWS) task. Our CNN-Attention architecture, in combination with our unsupervised contrastive learning method, CABKS, utilizes unlabeled data efficiently. This circumvents the challenge of acquiring ample labeled training data, particularly beneficial when target keywords change or when positive samples are scarce. Furthermore, our speech augmentation strategy enhances the model’s robustness, adapting to variations in keyword utterances. By using contrastive loss within mini-batches, we’ve improved training efficiency and overall performance. Our method outperformed others such as CPC, APC, and MPC in experiments. Future work could explore this approach’s application to other speech tasks and investigate other augmentations or architectures to enhance performance. This work marks a significant step towards more reliable voice-controlled systems and interactive agents.

SECTION: References