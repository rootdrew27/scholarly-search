SECTION: Exploring Driving Behavior for Autonomous Vehicles Based on Gramian Angular Field Vision Transformer
Effective classification of autonomous vehicle (AV) driving behavior emerges as a critical area for diagnosing AV operation faults, enhancing autonomous driving algorithms, and reducing accident rates. This paper presents the Gramian Angular Field Vision Transformer (GAF-ViT) model, specifically designed for analyzing AV driving behavior. The GAF-ViT model is developed upon a novel integration of three key components: GAF Transformation Module, which transforms multivariate driving behavior representative sequences into multi-channel images; Channel Attention Module, which prioritizes relevant behavioral features to enhance classification effectiveness; and Multi-Channel ViT Module, which employs advanced image recognition techniques to accurately classify the resulting multi-channel driving behavior images. This framework not only facilitates detailed analysis of complex multivariate driving behavioral data but also leverages the capabilities of vision-based pattern recognition methods to uncover subtle driving behavior nuances. Experimental evaluation on the Waymo Open Dataset of trajectories demonstrates that the proposed model outperforms baseline models, achieving state-of-the-art performance. Furthermore, an ablation study effectively validates the efficacy of individual modules within the model.

SECTION: 
Over the last decade, the automotive industry has increasingly focused on advancing autonomous vehicle (AV) technology to enhance road safety and substantially mitigate accidents, as distractions and errors from drivers are attributed to an estimated 94% of all incidents. Concurrently, research suggests that the behavior of AVs should mimic that of human drivers to ensure coherent understanding by drivers of other vehicles and adherence to human cognitive patterns. In light of this, numerous scholars have devoted themselves to developing models that minimize the behavioral gap between AVs and human drivers. Nonetheless, a study conducted by the Insurance Institute for Highway Safety (IIHS), which analyzed over 5,000 accidents reported by law enforcement through a nationwide survey, reveals that even with a human-like driving approach and sensors offering a 360-degree view of the surroundings, AVs may only circumvent one-third of accidents. This includes those caused by detection failures and incapacitation, while the majority – encompassing those arising from speeding, intervention from other drivers, etc.,- remain inevitable. The study suggests that if AVs exhibit the same level of aggressiveness as human drivers on the road, crashes will continue to occur. Alternatively, setting autonomous driving systems to adopt a conservative approach in mixed traffic flow may reduce the likelihood of fatal crashes but has the potential to generate bottlenecks. Such a prudent approach could frustrate or irritate other drivers and significantly elevate the probability of rear-end collisions, especially in complex decision-making scenarios like intersections and four-way stops.

Analogous to traditional vehicles, where effectively identifying human drivers’ behavior serves as an additional informational reference for surrounding vehicles – enabling proactive decision-making and reducing the crash probabilities –identifying and classifying behavior for AVs is crucial. This not only has the potential to guide the evaluation of the stochasticity and stability of autonomous driving algorithms but also fosters the improvement of functionality.

However, to the best of the authors’ knowledge, the majority of research to date has principally focused on the classification of traditional drivers’ behavior, often neglecting the varied behaviors exhibited by AVs. Another segment of the research places a greater emphasis on the motion of AVs in a specific spatiotemporal context - either predicting the vehicle’s state in the ensuring planning horizon based on historical data or making optimal decisions in response to environmental changes. In light of these insights, this paper primarily focuses on the relatively stable and comprehensive behavioral characteristics displayed by autonomous vehicles in mixed traffic flow.

Specifically, a Gramian Angular Field Vision Transformer (GAF-ViT) model is proposed for analyzing AVs’ driving behavior. The Gramian Angular Field (GAF) is a mathematical representation of time series data, capturing the pairwise angular relationships between data points within a time series. This approach decodes time series data into images, thereby facilitating the application of computer vision methods for time series classification. GAF encompasses two types: Gramian Angular Summation Field (GASF) and Gramian Angular Difference Field (GADF). In this study, the driving behavior of AVs is represented by a multivariate feature matrix, which is constructed from different feature sequences. Each feature sequence is transformed into two images in terms of GASf and GADF. The final imagery, representative of the behavioral multivariate matrix, includes the concatenation of two images for each feature, followed by the concatenation of all image sets corresponding to all features. The constructed multi-channel images are then classified through the Vision Transformer (ViT), a powerful deep learning architecture that extends the Transformer model from natural language processing (NLP) to computer vision tasks. In addition, a channel attention structure is incorporated and applied to the generated multi-channel images to differentiate the importance of distinct feature images. The Channel attention mechanism was originally developed to enhance the importance of certain channels or feature maps while suppressing others within a neural network layer. Through the effective classification of AV driving behaviors, the model assists developers in recognizing various AV driving behaviors and identifying the hazardous ones of certain vehicles, thereby contributing to the timely adjustments or updates of autonomous driving algorithms or control of AVs to prevent accidents. The proposed GAF-ViT model is evaluated on the processed Waymo Open Dataset of trajectories and achieves the best performance among benchmark models. The major contributions of this study include:

1) An approach based on GAF is proposed to visualize driving behavior features for AVs, effectively converting intricate driving behavior data into a visually interpretable format.

2) An innovative GAF-ViT model is introduced, capable of transforming multivariate feature sequences representing behavior into multi-channel images. This enables efficient classification of driving behavior for AVs through effective recognition of these images.

3) Domain knowledge is seamlessly integrated into the model through a channel attention mechanism, which highlights the most relevant driving behavior features. This integration significantly augments the model’s performance.

The remainder of this paper is organized as follows. Section II reviews existing methods for classifying traditional human drivers’ behavior. Section III presents the overall structure as well as each specific module of the developed model in detail. Numerical experiments and results are discussed in Section IV. Section V concludes the paper and addresses possible future works.

SECTION: 
One major approach to classifying and analyzing drivers’ behavior is based on the modality of data, which underscores the variety of data sources pertinent to studying driving behavior. The following section provides a breakdown of methodologies tailored to each type of data.

SECTION: 
These methods primarily use visual data from cameras mounted inside or outside the vehicle. In-vehicle cameras primarily capture driver-related information, including facial expressions, gaze, and head posture, thereby becoming crucial for monitoring driver behavior, detecting distractions, and identifying driver fatigue. Conversely, the integration of exterior cameras, which concentrate on the vehicle’s surroundings to detect lane departures and monitor the traffic environment, contributes to classifying driving behaviors. These behaviors relate to lane-keeping, adherence to traffic rules, collision avoidance, and interactions with the surrounding environment, providing a more holistic view of driver actions and prevailing road conditions. However, it is noteworthy that, although camera-based methods exhibit exemplary performance, especially in intricate traffic scenarios, they may be cost-prohibitive and computationally demanding due to the financial implications of hardware installation and maintenance, as well as the computational requirements for processing visual camera data.

SECTION: 
General trajectory methods for driving behavior analysis leverage data collected from various inertial or positioning sensors to construct vehicle trajectories, including speed, acceleration, gyroscopic and accelerometric measurements, GPS, etc. This data type is important in classifying driving behaviors such as abrupt maneuvers, aggressive driving, and adherence to speed limits. Models typically constructed to utilize this data for classifying drivers’ behaviors encompass Kalman filter-based classifiers, machine learning-based neural networks such as Support Vector Machine (SVM) and Random Forest (RF), Long-Short Term Memory (LSTM)-based deep learning architectures, and Transformer-based models. Typically, trajectory data is structured as a multivariate time series, incorporating various driving behavior features for optimal utility. They are generally highly accurate, real-time, and weather-resistant, affording a comprehensive understanding of driving behaviors. In this study, trajectory data is exclusively used for model training and evaluation.

Car-following behavior is a critical aspect of trajectory-based methods for driving behavior analysis, focusing on the dynamics between a leading vehicle and a following vehicle. Recent advancements have utilized machine learning and deep learning techniques to enhance the accuracy and robustness of car-following behavior predictions. For instance, Qin et al. developed a CNN-LSTM model that combines convolutional neural networks (CNNs) with LSTM networks to analyze and predict car-following behavior using trajectory data, demonstrating superior accuracy and generalization ability compared to classical models. Another study by Fan et al. employed an LSTM to investigate the impact of driving memory on car-following behavior, highlighting the significance of historical driving data in predicting future behaviors. Additionally, Qin et al. proposed a car-following model based on a combination of LSTM and Transformer networks, with the focus on reconstructing input features from trajectory data to improve model performance under data loss scenarios. Furthermore, Li et al. explored the identification of automated vehicles using car-following trajectory data and developed learning-based models that significantly enhance the accuracy of vehicle classification in mixed traffic environments. These advanced models capture the temporal dependencies and complex interactions inherent in car-following scenarios from trajectory data, contributing to a more effective and adaptive traffic management system. By incorporating car-following behavior when studying driving behavior, researchers can better address the nuances of driving behavior, ultimately enhancing the accuracy of behavior classification and prediction in automated driving systems.

SECTION: 
Smartphone-based methods for classifying driving behaviors utilize sensors such as accelerometers and GPS equipped in smartphones to capture pertinent data. These methods offer notable advantages, including cost-effectiveness, portability, and real-time monitoring, attributable to the widespread use of smartphones. Nonetheless, they may encounter challenges related to data accuracy, sensor variability across different smartphone models, privacy concerns, limitations due to available sensor types, and considerations related to battery life. Overall, while smartphone-based approaches prove to be well-suited for large-scale monitoring and individual driver supervision, they may necessitate additional considerations for tasks that require high precision and specialized analysis.

SECTION: 
SECTION: 
The proposed GAF-ViT model can process any generic multivariate sequential data indicative of specific autonomous driving behavior and generate the corresponding class label. The input multivariate data, denoted as, can be defined as follows:

whererepresents a series of driving behavior features such as speed, acceleration, headway, etc., to effectively identify the specific behavior that all features, collectively referred to as, the GAF-ViT model incorporates three essential modules: GAF Transformation Module, Channel Attention Module, and Multi-Channel ViT Module, as illustrated in Figure 1.

Specifically, the GAF Transformation Module converts the inputinto a multi-channel image. The Channel Attention Module is employed to enhance or attenuate the significance of specific feature channels and is connected with the Multi-Channel ViT Module, which ultimately classifies the image into a specific driving behavior. Subsequent sections will provide detailed illustrations of each module.

SECTION: 
The Gramian Angular Field (GAF) technique is predominately utilized in time series analysis and signal processing. It articulates time series data in a way that captures the intrinsic temporal relationships and patterns. Previous studies indicate that GAF is particularly adept at analyzing and visualizing the cyclic or periodic behavior inherent in time series data. Moreover, the GAF transformation uniquely facilitates the application of general vision-based pattern recognition models to time series analysis. This leverages their spatial pattern detection capabilities to uncover intricate temporal relationships and features within the data. Thus, it offers a novel approach that amalgamates time-domain analysis with visual pattern recognition techniques. As noted in the previous section, there are two types of GAF: GASF and GADF. Generally, given a univariate time series data denoted as vector, the normalized vectorofcan be determined as follows:

where the elementinis within. The scaled vectorcan then be expressed in polar coordinates, transitioning from Cartesian coordinates, as follows:

whererepresents the angular value anddenotes the radius in the polar coordinate. Finally, two types of GAF can be calculated according to the following equations:

Hence, for each feature vector of, two images are obtained in terms of GASF and GADF. These two images are then stacked together. The necessity of stacking GASF and GADF together, rather than using them separately, arises from the fact that the transformation of either GASF or GADF is not injective. This means that different time series can produce identical GASF or GADF images due to the symmetry of cosine and sine functions. For instance, reversing the sign of every point in a time series results in a new time series with each value negated. However, this reversal does not alter the resulting GASF image, as the cosine of an angle plus its supplementary angle yields the same value. Hence, employing both sine and cosine functions to generate two types of GAF images and then combining them ensures that the angular values are uniquely determined, thereby addressing the non-injectivity issue. Ultimately, all the stacked images for all feature vectors are concatenated, forming the multi-channel image. The whole process of the GAF Transformation Module is illustrated in Figure 2.

SECTION: 
Before training a classification model to understand driving behavior from generated multi-channel images, the channel Attention Module (CAM) is employed to facilitate the learning of distinct weights for different channels along the channel dimension while maintaining uniform weights across spatial dimensions. Intuitively, various behavior features uniquely contribute to defining vehicle driving behavior. For instance, speed may be a more direct indication of a car’s threatening state compared to acceleration. As images transformed from feature sequences are allocated to their respective channels in the multi-channel image, the application of varying learnable weights to distinct channels quantifies this contribution, thereby improving classification accuracy. Figure 3 illustrates the structure of the Channel Attention Module.

Specifically, the approach encompasses three primary steps. Let the constructed multi-channel image be denoted as tensor, characterized by width, height, and depth. Firstly, global average pooling is applied across both width and height dimensions to compress, thereby encoding the comprehensive spatial feature of each channel into a singular feature. This operation results in an intermediate output tensorof dimensions. The squeezing process is mathematically formulated as follows:

Sequentially, two fully connected (FC) layers are used to discern the dependencies across channels, with weights given by the FC layers being normalized via a Sigmoid function, denoted as. The function confines the weights within therange while ensuring their sum equals 1. The process to generate the final weightsfor the channels can be formulated as follows:

whereandrepresent weights generated within the FC layers anddenotes the ReLU activation function.

Finally, by multiplying the final weights by their corresponding channels, the weights are effectively integrated into the input multi-channel image, leading to a newly scaled multi-channel image that will serve as the input of the classification model.

SECTION: 
In recent years, Transformer models, particularly those based on self-attention mechanisms, have emerged as the foremost choice for Natural Language Processing (NLP) tasksdue to their exceptional performance. The Vision Transformer (ViT) applies Transformer architecture to image processing, involving the segmentation of an image into numerous patches and subsequently processing these linearly arranged patch sequences as input to the Transformer model. In this context, the concept of image patches is similar to the concept of tokens in NLP tasks. This study illustrates the structure of the adapted Multi-Channel ViT Module, utilized for classifying multi-channel images representative of driving behavior and scaled as delineated in the previous section, in Figure 4.

Given an input image, denoted as, it is reshaped into a sequence of flattened patches, expressed as, where,andrepresent the height, width and number of channels of the scaled multi-channel image, respectively,refers to the patch size, whileindicates the number of patches. Subsequently, each patch is linearly embedded into a lower-dimensional feature space, with an extra class embedding and position embedding appended to the patch embedding. The embedding processes can be expressed as follows:

wheredenotes the embedded class label,represents the-th patch,defines the linear patch embedding function, withrepresenting the position embedding function, andbeing the output of the embedding operations. The Transformer encoder comprises a sequential array of Transformer blocks, and the total number of Transformer blocks is. Each Transformer block encompasses a multi-head self-attention (MSA) block, followed by a multi-layer perceptron (MLP) block, as shown in the following equations:

wheredenotes the layer normalization function,andcorrespond to the outputs from the MLP and MSA blocks, respectively. Residual connections are also incorporated subsequent to each MSA and MLP block. The MLP block comprises two fully connected layers, utilizing a GeLU activation function. Ultimately, the predicted score for each driving behavior class is generated through a feed-forward neural network connected to the Transformer encoder, as expressed in the following equation:

SECTION: 
Based on the elaboration of each module presented in previous sections, this section demonstrates a general implementation of the proposed GAF-ViT model. It accepts the multivariate driving behavior feature matrixas input and yields the predicted behavior class labelas output.

Multivariate driving behavior feature matrix

Predicted driving behavior class label

SECTION: 
SECTION: 
The proposed GAf-ViT model is trained and evaluated on a trajectory dataset derived from the Waymo Open Dataset. This dataset has undergone preprocessing steps, including outlier removal and denoising. The refined dataset contains three key features of AV driving behaviors: speed, acceleration, and jerk. All three features are used in this study for both model training and testing purposes. The dataset includes 2,704 trips, with most trips lasting approximately 20 seconds in duration and recorded at a time interval of 0.1 seconds. By filtering out trips with a consistent speed of 0 or less, 2,695 meaningful trajectories are retained for analysis. Figure 5 visualizes position (), speed (), acceleration (), and jerk () variations over time () within a trajectory from the processed dataset.

In addition, it’s important to note that the lengths of trajectories in the dataset vary. The most prevalent trip lengths constitute approximately 86.76% of the 2,695 trajectories and are 198 or 199 data points long. Consequently, for the experimental dataset, trajectories with a length of 198 or 199 were selected. These sorted trajectories were then split into two trips, and the final data point of those with a length of 199 was removed from each, resulting in each trip having a length of 99. This process yielded 4,674 trajectories as the final dataset prepared for input into the developed model in this study.

SECTION: 
The proposed GAF-ViT model is developed using PyTorch. Some implementation details are described as follows:

1)Each trajectory concatenates three corresponding feature sequences: speed, acceleration, and jerk, into a 3-dimensional matrix, and there are 4,674 matrices in total. To discern the behavior classes represented by the constructed feature matrices, the QuickBundles (QB) algorithm is applied to cluster these multivariate matrices, while the elbow method aids in determining the optimal number of clusters. The QB algorithm is particularly well-suited for multivariate streamline clustering due to its efficiency and adaptability to high-dimensional data. Unlike many traditional clustering methods that may struggle with the complexity and computational demands of processing multivariate time series data like trajectories, QB efficiently handles large datasets by grouping streamlines based on a fast approximation of their similarity. Thus, the hierarchical nature of QB allows for a scalable approach to clustering, enabling the handling of diverse driving behaviors and patterns with varying levels of detail. The detailed implementation of QB-based multivariate driving behavior clustering is outlined in Algorithm 2. Specifically, in this method, the distance between the last row vectors of two matrices, known as endpoint features, is computed for simplicity. This simplification allows for a faster computation of similarity or dissimilarity between streamlines, focusing on their overall orientation rather than their detailed shape or path. The cosine distance computation then evaluates how parallel or divergent these vectors are, serving as a proxy for the similarity between the matrices’ overall directions. The cosine distance for two given endpoint feature vectors can be calculated following the equations below:

whereandare the endpoint feature vectors of two matrices.

Set of multivariate matrices, optimal threshold

Clusters

QB clustering effectively reveals four types of driving behaviors across all trajectory samples. Table I presents the number of samples and basic statistics of speed mean (), acceleration standard deviation (), and jerk standard deviation () for each class. It is notable that the statistical values in Table I provide a class indication; actual speed, acceleration, and jerk values within a trajectory can fluctuate significantly, both within and across classes. Therefore, based on each class’s indicators, interpretations are also provided in Table I. With the identified driving behavior class labels, training of the proposed GAF-ViT model can enable efficient recognition of specific driving behavior classes through the multivariate input feature matrix.

2)Feeding the 3D multivariate driving behavior feature matrices into the GAF Transformation Module, corresponding multi-channel images are generated. For visualization purposes, Figure 6 shows two examples of transformed images, utilizing both GASF and GADF methods, for each feature of an input matrix across each class. It can be seen that even among different samples from the same class, the imaging of identical features, such as speed, demonstrates a consistent pattern.

3)In this study, it is anticipated that the scaled multi-channel images generated from the Channel Attention Module maintain the same size as the input multi-channel images. Consequently, the reduction ratio is set to 1 to preserve the consistent dimensionality of the channel-wise information from input to output.

4)As addressed above, each trajectory used in this study has a length of 99, and three features are considered. Thus, each input matrix measures, yielding a transformed multi-channel image size of. Accordingly, the selected hyperparameters for the Multi-Channel ViT Module are shown in Table II.

5)In this study, all trajectories are randomly split into 80% for training, 10% for validation, and 10% for testing. A batch size of 8 and an initial learning rate 1e-5 are employed. The model is trained for 50 epochs utilizing the Cross-Entropy loss function and the AdamW optimizer, with weight decay regularization applied to prevent over-fitting.

6)In effectively assess the model performance, the metrics of Accuracy, Precision, Recall, and the F1 score are used, each of which can be calculated using the following equations:

where TP, TN, FP and FN refer to the number of true positive, true negative, false positive, and false negative cases, respectively. Figure 7 illustrates the curves representing variations in loss and accuracy over epochs for both training and validation sets. Notably, the performance begins to converge after approximately 50 epochs on the training set, while on the validation set, it converges after roughly 15 epochs.

Figure 8 displays the confusion matrix based on the testing set.

SECTION: 
The developed GAF-ViT model is compared with the following benchmark models, widely adopted for multivariate time series classification:

LSTM: Long Short-Term Memoryis a type of recurrent neural network (RNN) designed to capture long-range dependencies within sequential data. Employing a memory cell alongside gates to regulate information flow, LSTMs prove efficacious for tasks involving sequential data, such as time series classification and prediction.

MLP: Multilayer Perceptron, a feed-forward neural network characterized by multiple layers of interconnected neurons, stands out as a versatile model. It demonstrates capability across various tasks, including classification, regression, and function approximation.

FCN: Fully Convolutional Networkis primarily designed for image segmentation and related tasks. It replaces fully connected layers with convolutional layers, enabling it to accommodate input data of varied sizes.

LSTM-FCN: LSTM-FCN Hybrid modelcombines the strengths of both LSTM and FCN architectures employing LSTM layers to discern temporal dependencies and utilizing FCN layers for adept feature extraction.

GRU-FCN: Similar to LSTM-FCN, GRU-FCN Hybrid modelcombines the GRU (Gated Recurrent Unit) with FCN layers for time series data classification.

mWDN: Multiscale Weighted Dense Networkincorporates multiscale dilated convolutional layers and weighted dense connections to capture both local and global features in time series data for effective classification.

MLSTM-FCN: Multiscale LSTM-FCN Hybrid modelcombines LSTM and FCN layers with a multi-scale approach. It uses LSTM layers to capture temporal dependencies at different scales and FCN layers for feature extraction.

TST: Time Series Transformeris based on the Transformer architecture and is designed specifically for time series data. It has demonstrated robust performance across various time series classification tasks by employing self-attention mechanisms to discern temporal dependencies.

gMLP: Gated Multilayer Perceptronrepresents a variation of the conventional MLP architecture, incorporating gated activation functions. This model introduces gating mechanisms into the MLP layers to enhance sequential data modeling.

The performance of the GAF-ViT model compared with the benchmark models in terms of Accuracy, Precision, Recall, and the F1 Score is shown in III.

Table III presents experimental results, revealing that the proposed GAF-ViT model outperforms other baseline models across all evaluated metrics. Both wMDN and gMLP deliver relatively promising results through specially designed architectures to capture additional information from the time series.

SECTION: 
An ablation study was conducted to further explore the impact on the performance associated with the removal of critical modules from the GAF-ViT model. Specifically, two ablated models were constructed to assess the indispensability of the Channel Attention Module and the GAF Transformation Module, respectively. Without the Channel Attention Module, multi-channel images resulting from the GAF Transformation Module are directly used for classification, implying that all driving behavior features are considered equal in defining a specific driving behavior type and classifying the resulting behavior. Conversely, without the GAF Transformation Module, the input 3D driving behavior matrix is reshaped to a size identical to the original multi-channel image through a linear layer for consistency before being fed through subsequent modules. The results from these two models are depicted in Figure 9.

The results show that the performance of the original GAF-ViT model is better than both ablated models in terms of Accuracy, Precision, Recall, and the F1 Score, which confirm the necessity of both the Channel Attention Module and the GAF Transformation Module.

SECTION: 
This study develops a Gramian Angular Field Vision Transformer (GAF-ViT) model for AV driving behavior imaging and classification. The model comprises three modules: GAF Transformation Module, Channel Attention Module, and Multi-Channel ViT Module. GAF Transformation Module converts multivariate driving behavior feature sequences into corresponding multi-channel images. Subsequently. Channel Attention Module is designed to assign variant weights to different feature images across diverse channels, correlating to their importance. Ultimately, Multi-Channel ViT Module classifies AV driving behaviors by interpreting the weighted multi-channel images. Experimental outcomes indicate that the GAF-ViT Model outperforms benchmark models, widely used for classifying human driving behaviors. It demonstrates the immense potential for real-world deployment. This model stands to benefit research and validation teams by facilitating the detection of anomalies in AV operations and refining control algorithms. This ensures the adaptive behavior of AVs in varied traffic scenarios, ultimately ensuring the safety of vehicles, passengers, and other participants while minimizing accident occurrences.

Acknowledging the computational intensity of the GAF-ViT model due to the inherent demands of Vision Transformers, a practical deployment strategy involves leveraging edge cloud or roadside unit (RSU) infrastructure. These systems possess greater computational capabilities than the vehicle’s onboard systems, enabling more flexible computational environments. By offloading heavy data processing and inference tasks from the vehicle to these external units, the model can benefit from higher computational power and storage capacities, processing and analyzing driving behaviors more efficiently and with lower latency. This setup not only addresses computational constraints but also enhances the scalability of deploying advanced deep learning models like GAF-ViT in real-world autonomous driving applications. Such an arrangement facilitates continuous model improvement and updating without directly impacting the vehicle’s operational efficiency, making it a viable solution for integrating sophisticated AI-driven behavior analysis into the autonomous driving ecosystem. Future research directions include: 1) Training and evaluating the model on more comprehensive datasets spanning a wider range of behavior features to enhance model generality and robustness, 2) Developing new modules that assimilate various data modalities, including those from cameras, LiDAR, or Radar-based visual and audio information, to further enhance the model’s performance and efficacy.

SECTION: References