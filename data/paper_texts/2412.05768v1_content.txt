SECTION: Uncovering Uncertainty in Transformer Inference
We explore the Iterative Inference Hypothesis (IIH) within the context of transformer-based language models, aiming to understand how a model’s latent representations are progressively refined and whether observable differences are present between correct and incorrect generations. Our findings provide empirical support for the IIH, showing that thetoken embedding in the residual stream follows a trajectory of decreasing loss. Additionally, we observe that the rate at which residual embeddings converge to a stable output representation reflects uncertainty in the token generation process. Finally, we introduce a method utilizing cross-entropy to detect this uncertainty and demonstrate its potential to distinguish between correct and incorrect token generations on a dataset of idioms.

SECTION: Introduction and Related Work
Transformer-based architecturescurrently dominate artificial intelligence applications and serve as the underlying architecture for most Large Language Models (LLMs). While LLMs show impressive emergent abilities, these models exhibit limitations such as hallucinations and biased outputs which pose significant societal challenges. Inaccurate outputs can mislead users, while malicious actors can exploit AI models to create deceptive images, videos, and text to represent fictional occurrences as truth. Mitigating harms caused by model misuse, biased outputs, or misalignment with human values is a primary motivation behind research and policy decisions related to AI interpretability.

In this work we investigate a novel method for detecting uncertainty during the token generation process of transformer-based language models. One framework that has emerged for understanding the feed-forward behavior of residual models, such as the transformer, is the Iterative Inference Hypothesis (IIH). This hypothesis posits that predictions are formed in the residual stream, and that each block in a residual architecture incrementally updates these predictions in a direction of decreasing loss. A related line of research on in-context learning has suggested that transformers trained on autoregressive tasks are closely related to formulations of iterative optimization algorithms, namely gradient descent.

We combine these two threads of research, framing transformer inference as an optimization process that iteratively updates theinput embedding (i.e. the last word in the input sequence) to converge toward the most likely next-token embedding given the context and model weights. We propose methods for evaluating how input embeddings evolve towards output embeddings and find preliminary evidence of observable differences between correct and incorrect outputs, suggesting that these metrics could serve as useful indicators of a model’s output certainty. Our contributions are as follows:

We find preliminary evidence that thetoken embedding in the residual stream follows a path of decreasing loss in token embedding space, supporting the IIH.

We propose a novel method for detection of uncertainty during the token generation process and find preliminary evidence that this metric reflects observable differences in correct and incorrect generations.

SECTION: Methods
Here we define methods that offer insight into how representations in the residual stream evolve during inference. The transformer architecture, excluding token embedding and unembedding, can be succinctly represented by the following recurrence relation:whererepresents the set of token embeddings in the residual stream after an update from layer. Each layer contains attention and feed-forward sublayers. Withas the set of input token embeddings plus positional encodings, the residual stream is the sequencefor a model withlayers. In this work we are particularly interested in tracking the evolution of the embedding of theinput token,, as shown in red in Figure. Its residual representation after the final layer update,, is used to predict the next token in an autoregressive framework.

In the convolutional architecture of the original residual network (ResNet), the residual stream contained shortcut connections that transformed the basis of latent space in order to reduce its dimensionality at regular intervals. However, in the transformer, the residual stream has an entirely linear structure that preserves the basis of the embedding space. Each layer adds an output to each residual embedding, corresponding to translations in token embedding space. Thus the residual stream can be viewed as a path through token embedding space, and each point along the path can be mapped back to a distribution over tokens via methods like logit lens. To get the intermediate distribution predicted by the residual stream after layer, we passthrough the output layer norm and then to the model’s output head to obtain logits over the vocabulary. We refer to these logits as residual predictions. This framework forms the basis of our methods and analysis.

SECTION: Residual Cross-Entropy
We present a method by which cross-entropy can be used to measure the evolution of residual predictions during token generation. The IIH posits that the output of each layer updates the residual prediction in a direction of decreasing loss. We thus examine per-layer changes in cross-entropy, the loss function used to train transformers on autoregressive language tasks, including the specific model we examine here (GPT-2 XL).

Cross-entropy is a measure of dissimilarity, requiring a candidate and a target distribution. We use the residual predictions described above as the candidate, and for the target distribution we examine two choices: (1) the one-hot distribution of the token sampled deterministically by taking the argmax the model’s predicted probabilities, denoted, and (2) the one-hot distribution of the ground truth next token given by the dataset, denoted. These measures are equivalent to the negative log likelihood of the sampled and target tokens respectively, as elaborated in. When the sampled output is correct, or, these two measures are the same. Examining how layer updates affect cross-entropy with respect to the training objectiveis necessary for our evaluation of the IIH, but the ground truth next token may be unavailable or ambiguous at inference time, makingmore practical for analyzing the residual stream in an online setting. Some existing works have examined Kullback-Leibler divergence between residual predictions and the final logits output by the model, rather thanas we do here. Our approach implicitly penalizes generations that have high output entropy, which is advantageous for our study of uncertainty during generation. We provide a comparison between the two in.

SECTION: Model
To investigate how representations evolve in the residual stream we examine GPT-2 XL, which has 48 layers and 1.5 billion parameters. We chose this model as it has low compute requirements while still being representative of frontier model architectures. In addition, it has widely used open source implementations and official weights made publicly available by OpenAI on HuggingFace.

SECTION: Inference Data
For our preliminary study of the IIH and model uncertainty, we chose a simple dataset with a wide variety of generation difficulty: English idiom completion. An idiom completion task provides a relatively clear-cut, single token "correct" and "incorrect" answer, making it straightforward to evaluate. This dataset is intentionally challenging, where some answers would be hard or nearly impossible to guess given the input context or brevity of the idiom.

The idiom dataset consists of 330 static idioms taken from the EPIE Dataset. To build our dataset, we split each static idiom so that the final word serves as the "correct" output for the model. To guide the model in completing the idiom, we added instructions to the start of the idiom phrase. The instructions read: "The following prompt is the beginning of a popular English idiom, please respond with a single word to complete the phrase." Thus, each prompt in this dataset consists of the instructions + the first words of an idiom. We excluded 29 idioms from the EPIE dataset because the target outputs were represented by more than one token in the vocabulary, such as ["help", "ful"].

SECTION: Results
In this section, we examine the evolution of residual representations in GPT-2 XL on the idiom inference dataset by analyzing the per-layer change in cross-entropy. Our main objectives are, first, to evaluate whether there is evidence supporting the IIH, and second, to determine if these metrics reveal a noticeable difference between correct and incorrect output distributions that could aid in developing a measure of uncertainty for a model’s predictions.

The cross-entropy per model layer for the idiom dataset is presented in Figure. To generate these results, we first feed a prompt into the model, recording theresidual embedding before and after the update from each layer. We then calculate the cross-entropy between these residual predictions and a target. For Figurea we take the target to be the token predicted by the model,, as described in Section. For Figureb, we let the target be the ground-truth next token,, from the idiom dataset. Distributions of correct generations () for each figure are plotted in blue and incorrect generations are plotted in red. This approach allows us to clearly observe the evolution of representations that ultimately lead to correct predictions during the inference process.

In other words, Figurea displays how the embeddings in the residual stream evolve towards an arbitrary output representation, while Figureb shows how it evolves towards the most likely next token according to the dataset. A distinct separation is observable between the correct and incorrect distributions for both cross-entropy plots, suggesting these measures may be useful for understanding the certainty of a model’s output as it is being generated. The separation is more pronounced when the target is the ground truth, as a result of the incorrect generations failing to converge to the correct representation in embedding space. Figureb demonstrates clear evidence for the IIH, with the median layer update decreasing loss with respect to the ground truth nearly monotonically throughout the model for both correct and incorrect generations. We provide a table with the average decrease in cross-entropy per layer for (b) in.

In Figurea we show the cross-entropy distributions of the model’s output logits with respect to, corresponding to the vertical slice over the last layer in Figurea. We observe that the distribution of correct generations is exponential with a mean of, indicating that correct generations tend to converge more closely to a one-hot distribution and thus implying their output distributions have lower entropy. The incorrect predictions are normally distributed with a mean ofand a standard deviation of, indicating that final predictions tend to be further in distribution fromand thus have higher entropy. By visual inspection, any generations resulting in a output cross-entropy greater thanare very likely to be incorrect for this dataset. In Figureb we plot a receiver operating characteristic (ROC) curve for the output cross-entropy and observe an area under the curve (AUC) of, indicating that output cross-entropy is a strong predictor of correct vs incorrect generation on the idiom dataset. This was corroborated with a Mann-Whitneytest yielding astatistic with the same value.

A potential application of this metric is visualized in Figure, measuring the output cross-entropy per token on an open ended generation task. Again, we measure the cross-entropy distributions of the model’s output logits with respect tosince the model has access to this information at inference time. This generation was produced using a seed of 42, a temperature of 0.8, and the prompt "Alan Turing".

In response, GPT-2 XL generates a number of erroneous historical facts, each of which is observed to have a spike in cross-entropy relative to the rest of the sequence. For reference, Alan Turing was born in 1912 in London, England, attended King’s College in Cambridge, and died on June 7, 1954. The cross-entropy values are lowest when the language heavily implies the next token, such as with generated tokens 9, 10, 16, 24, 37, and 44. Here the space of possible next tokens is heavily constrained, in contrast to open-ended portions of the sequence encountered when generating tokens 8, 22, 33, 40, and 41. Large cross-entropy values in this sequence are also observed on for tokens representing dates, locations, and other facts - see tokens 1, 22, 26, 35, and 42.

This example concisely illustrates trends observed over numerous generation studies, but it alone is not sufficient for general claims. Anecdotally, the cross-entropy measure appears to capture both uncertainty inherent in the prompt and the uncertainty of the model. It appears to be insufficient to disambiguate between these two sources of uncertainty. Additionally, if a model is confidently incorrect, this metric would not capture any uncertainty to indicate a potential mistakea limitation humans face as well.

Output cross-entropy can be computed cheaply at inference time and these results indicate that it can potentially be used by a naïve classifier to indicate the likelihood of an incorrect generation. Future work will examine if this result holds across models and datasets. In the appendix, we present a table showing the intermediate predictions for the highest and lowest cross-entropy results in Figurea. These samples further show that output cross-entropy appears to correspond with prompt open-endedness and the model’s uncertainty given the prompt. These combined results indicate that the correctness and perhaps amount of certainty for the next generated token can be measured by the rate and degree to which embeddings converge to stable output representations in the residual stream of transformers.

SECTION: Conclusion
In this work we investigate the Iterative Inference Hypothesis as applied to the transformer architecture on autoregressive language modeling problems. We provide a mechanism for investigating the evolution of predictions in the residual stream and find empirical evidence to support the hypothesis. In addition, we propose a novel method for observing uncertainty during the token generation process by measuring the cross-entropy between the model output logits and a one-hot distribution representing the deterministically sampled token. Using this metric we find distinct differences between distributions of correct and incorrect generations on an idiom dataset, and we observe that this output cross-entropy appears to correspond with model uncertainty given a prompt.

Future work will aim to address limitations of this preliminary study by expanding our analysis to a broad range of datasets and language models of varying sizes. We will additionally extend our study to multi-token generations and explore the use of output cross-entropy as an uncertainty measure and potential flag for hallucinations. Finally, we intend to explore additional convergence metrics that may better predict correct versus incorrect generations, then examine how broadly applicable they are across models and datasets. The ultimate goal of this research is to develop methods for assuring the quality of language model output with minimal computational cost.

SECTION: Acknowledgments and Disclosure of Funding
We thank Cash Costello and Patrick Emmanuel for providing helpful feedback. Funding was provided via an internal research grant from Johns Hopkins University Applied Physics Lab. Compute hours were provided on Oak Ridge National Laboratory’s Summit supercomputer as a part of an award for this project from the National Science Foundation’s National Artificial Intelligence Research Resources (NAIRR) Pilot Program.

SECTION: References
SECTION: Appendix / supplemental material
SECTION: Cross-Entropy Measurement
Given two discrete probability distributionsandover support, the cross-entropyofrelative tois defined as

In our case,represents the vocabulary of tokens,represents the probabilities predicted by the model over, andrepresents some target distribution over. For our methods, we taketo be a one-hot encoding representing either the sampled tokenor the ground truth token. With a one-hot encoding for, the above summation yields only one non-zero term

whereordepending on our choice of target. This is also the negative log likelihood of tokenas predicted by the model. Our cross-entropy experiments can be equivalently framed as measuring how the negative log likelihood of a target token changes throughout the inference process.

SECTION: Best and Worst Case Sample Generations
We present the residual predictions for the generations with the highest and lowest output cross-entropy scores vs model predictionson the idiom dataset in Figure. These correspond to left- and right-most samples on the cross-entropy axis in Figurea. We use the logit lens technique to recover token predictions from the residual stream after each layer update as described in.

Many of the idiom prompts are one word and are extremely difficult to complete, even for humans. This open-endedness lends itself to high uncertainty, which is captured by the cross-entropy as shown here. The samples with the lowest cross-entropy have prompts with multiple words that heavily imply a specific next-token, severely constraining the set of possible next-tokens. In contrast, samples with the highest cross-entropy have short prompts with very common words that are could have many valid next tokens, resulting in a very open-ended generation task. We observe output cross-entropy reflecting the open-endedness of the prompt via the corresponding uncertainty from the model.

SECTION: Choice of Divergence Target
Here we compare KL divergence between residual predictions and model output logits versus residual predictions and a one-hot encoding of the top predicted logit. Note that KL divergence and cross-entropy between two distributions differs only by a constant. By definition, the final residual prediction is equivalent to the model output logits, thus the KL divergence approaches zero for all generations, as observed in Figurea below. If the output logits of the model exhibit high entropy, then they will have a higher divergence when measured against to a one-hot representation of the top predicted logit. This can be observed in Figureb. We find this bias useful for distinguishing correct and incorrect generations.

SECTION: Idiom Dataset Loss Table
Figurebelow shows average change in loss after each layer update to predictions in the residual stream for the idiom dataset. White cells denote no change, blue cells denote a decrease, and red cells denote an increase. Nearly all layer updates move the residual prediction in a direction of decreasing loss, supporting the IIH. Layers 20-38 appear to contribute the most to reducing residual prediction loss for correct generations.

SECTION: Idiom Dataset Cosine Similarity
We also measure how representations change directly in token embedding space by measuring the cosine similarity between each intermediate embeddingand embeddings of the two choices of targets described in.