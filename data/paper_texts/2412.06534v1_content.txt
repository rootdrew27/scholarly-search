SECTION: Inverting Visual Representations with Detection Transformers
Understanding the mechanisms underlying deep neural networks in computer vision remains a fundamental challenge. While many prior approaches have focused on visualizing intermediate representations within deep neural networks, particularly convolutional neural networks, these techniques have yet to be thoroughly explored in transformer-based vision models. In this study, we apply the approach of training inverse models to reconstruct input images from intermediate layers within a Detection Transformer, showing that this approach is efficient and feasible for transformer-based vision models. Through qualitative and quantitative evaluations of reconstructed images across model stages, we demonstrate critical properties of Detection Transformers, including contextual shape preservation, inter-layer correlation, and robustness to color perturbations, illustrating how these characteristics emerge within the model’s architecture. Our findings contribute to a deeper understanding of transformer-based vision models. The code for reproducing our experiments will be made available at.

SECTION: Introduction
Recent advancements in deep neural networks (DNNs), particularly transformer-based architectures, have achieved remarkable results on vision tasks such as object detection, semantic segmentation, and image classification. Despite their compelling performance, the internal mechanisms of these networks remain largely opaque, hindering a clear understanding of how predictions are made. Enhancing network interpretability, i.e., understanding the mechanisms underlying the network’s functionality, is crucial for ensuring safety, optimizing performance, and identifying potential weaknesses.

Feature inversion, introduced by Dosovitskiy and Brox, is one of the first techniques developed to interpret the processing capabilities of DNNs for vision. Building on a substantial body of work on generating images from intermediate representations, their method involved training an inverse network for each layer of a convolutional neural network (CNN) to reconstruct input images from intermediate representations. By analyzing these reconstructed images and their distinct characteristics from various layers, they uncovered important insights into the underlying mechanisms of CNNs.

While feature inversion provided valuable understanding of the CNN explored in their study, the approach has not seen widespread application to more advanced iterations of CNNs or transformer-based vision models. This is mainly due to the computational challenges inherent to feature inversion, as training a separate inverse model for each intermediate layer of interest becomes impractical for the large and complex DNNs of today.

In this work, we revisit feature inversion and extend it to a modern transformer-based vision DNN: the Detection Transformer (DETR). To address the significant computational challenges, we propose a modular approach to feature inversion. We invert distinct components or modules of DETR independently. This modular strategy significantly reduces the size of inverse networks required while maintaining the interpretability of reconstructed images. Notably, we observe that these inverse modules can be effectively applied across all intermediate layers, including those for which they were not explicitly optimized.

Leveraging these inverse modules, we qualitatively analyze reconstructed images from different stages of DETR. Hypotheses drawn from this qualitative evaluation are further validated through quantitative analyses, demonstrating key properties of DETR, such as contextual shape preservation, inter-layer correlation, and robustness to color perturbations.

SECTION: Related work
In the context of DNNs for computer vision, synthesizing images from intermediate representations is a valuable technique for network interpretability. A central branch within synthesizing images for network interpretability is activation maximization. Activation maximization generates images that activate specific network components, such as neurons, channels, or entire layers. While approaches vary in utilized techniques, the general goal is to synthesize images highlighting what specific parts of the network are responsive to. For example, synthesized images usually show simple features, such as edges, in lower layers, and more complex patterns in higher layers.

A related approach of synthesizing images concerns the feature inversion of intermediate representations within DNNs. Unlike activation maximization, which targets individual network components, feature inversion reconstructs input images from intermediate representations, enabling researchers to assess the information retained at each layer. This approach aids in interpreting network behavior by examining specific features in these reconstructions. For example, Dosovitskiy and Broxargued that color information remains accessible in the top layer of AlexNet.

These techniques for image synthesis have primarily been applied to convolutional neural networks (CNNs). In recent years, transformer-based vision models have been introduced. The analysis of vision models based on transformer networkshas so far focused on various other techniques such as analyzing the robustness against various image perturbations (e.g., occlusions or natural adversarial examples) representational similarity, and loss landscape analysis. Only recently has activation maximization been applied to vision transformers. To the best of our knowledge, our work is the first to synthesize images from intermediate representations using feature inversion within a transformer-based vision model.

SECTION: Methods
To efficiently apply feature inversion to DETR, we trained separate inverse models for distinct modules of the architecture. Specifically, for each of DETR’s modules (backbone, encoder, decoder, and prediction head), we trained corresponding inverse models (,,,, and) to reverse the path of information flow, as illustrated in.

For a technical description of our method, we first outline the default forward path of information flow in the DETR architecture. In the forward path, an input imageis processed through the ResNet-50backbone, extracting feature maps and reducing the spatial dimensions by a factor of 32. Assuming, the backbone feature map is then flattened and linearly projected to produce the backbone forward embedding. Next, the DETR encoder processesusing six transformer blocks, generating contextualized encoder forward embeddings. The DETR decoder module takesalong with 100 learnable object queries, and through a series of self-attention and cross-attention layers, refines these object queries to produce decoder forward embeddings. Finally, the prediction head utilizesto generate object classification scores, whereis number of object classes, and bounding box coordinates.

For the inverse path, all inverse modules were trained using the MSE loss between their outputs and the corresponding forward path embeddings independent of each other. Target forward path embeddings were generated for the COCO 2017 dataset. The pre-trained forward weights of DETR were kept fixed during the training process, and the inverse modules were trained in isolation.

Thetakes the embeddingand uses a 6-layer CNN to reconstruct the original input image as. Thetakes, and using the same architecture as the DETR encoder reconstructs the backbone inverse embedding. Thereceives zero-initialized image queriesalong with, and refines these image queries gradually by employing a structure similar to the forward decoder but operating on image queries instead of object queries. It outputs the encoder inverse embeddings. Finally, thetakes, and reconstructs the decoder inverse embeddingsusing a multi-layer perceptron network.

With this final inverse module, we took an additional step by applying one-hot encoding suppression on the class scores, preserving only the predicted classes with the highest probabilities in, while retaining their bounding boxes. These were fed to the final, which reconstructs. DETR can use a ”no object” class to handle images where the number of objects is fewer than 100 object queries. During training, if a query does not match any ground-truth object, it is classified as ”no object” (i.e., background). Since not all possible object categories are included in COCO’s ground-truth annotations, this background class may also encompass objects that are unknown to DETR. Thewas designed to analyze the model’s behavior when only the highest-confidence classes, including ”no object,” are provided for inversion. In this way, reconstructions can partially preserve the scene’s complexity by assuming that something should occupy the corresponding predicted locations.

SECTION: Results
SECTION: Reconstructing modules’ embeddings
After training a set of inverse modules, we conducted a qualitative evaluation of the reconstructed images from various stages of DETR, as shown in. We observed that structural and spatial information are generally well preserved in the reconstructions from backbone embeddings. Moreover, color shifts emerged as early as the encoder stage, which becomes more pronounced in the decoder. These color shifts resulted in reconstructed objects appearing closer to prototypical colors associated with their classes. For example, red was often assigned to buses, stop signs, and apples, green to potted plants, and orange to oranges. Beyond just color, the decoder stage also exhibited contextual shifts, where objects were reconstructed with prototypical semantic features even if they were not present in the original image. For instance, the presence of a tie often resulted in the model reconstructing the person wearing a suit, despite the original image not containing that. This indicates that DETR’s deeper layers incorporate semantic features and contextual cues that align with learned object associations.
Our analysis shows that most irrelevant information for object detection from the background scene is effectively removed after the decoder stage, allowing DETR to focus on features crucial for detection while discarding less useful details.

This selective filtering can also explain some detection errors. By comparing reconstructions before and after the decoder stage, we observed that false negatives can occur when certain objects, deemed irrelevant by the model, are entirely removed, leaving no trace in the reconstructions. Conversely, for false positive detections, the reconstructions showed cases where irrelevant features associated with an incorrect class are amplified, leading to misclassifications. For additional reconstruction results on such examples, please refer toin the supplementary material.

When analyzing reconstructions from grayscale images, we observed that removing color information did not significantly impair DETR’s ability to detect objects. The reconstructions still displayed colors typical of the detected classes, emerging after the encoder.

Further analysis of reconstructions from decoder and prediction head stages showed that along this forward and inversion path it has been learned to encode not only classes but also prototypical shapes and spatial relationships. For example, human body parts and their spatial relations were contextually well reconstructed. Even if the figures are not anatomically perfect, the reconstructions captured recognizable human forms and varied poses differing from the original image. Similarly, objects like buses were reconstructed with distinctive features, sometimes from angles that differ from the input, demonstrating the model’s ability to leverage class and bounding box information to generate plausible representations.

In the last experiment of this section, we applied one-hot encoding suppression on the class scores by preserving only the class with the highest confidence for each object query while retaining all bounding box predictions. This allowed us to investigate how the removal of lower-confidence class information affects the reconstruction. By limiting the class information to only the most confident predictions, we could observe the impact of class uncertainty on the reconstructed output and the resulting information loss. In some cases, this led to hallucinations of semantically relevant objects, such as generating additional chairs around a potted plant that were not present in the original image.

The diagram presented inshows the MSE loss of reconstructions across different stages of the DETR model, suggesting that the most significant information loss occurs within the decoder. This aligns with the model’s architecture, as the decoder has the role of transforming object queries into high-level abstract representations tailored for object classification and localization.

SECTION: Contextualizing reconstructions
Similar to Dosovitskiy and Brox, we contextualized our method for network interpretability by comparing reconstructions obtained from our default model with reconstructions obtained from a fine-tuned DETR fine-tuned for reconstruction. For this purpose, we started with a pre-trained DETR, an inverse backbone, an inverse encoder, and an inverse decoder and retrained all the parameters of both DETR and the inverse modules end-to-end. Specifically, we used a weighted combination of the detection performance losswith default hyperparameters as defined by Carion et al., and a reconstruction lossdefined as the MSE between an input image and its reconstruction from the decoder embedding, comparable to the method proposed by Rathjens and Wiskott. The total loss is expressed as:

The hyperparameterallows for a trade-off between the two losses. For example, when, the model is optimized solely for detection performance, while settingoptimizes the model exclusively for reconstruction performance.

Notably, this procedure differed from our default training approach in two key ways. First, it involves updating DETR’s parameters alongside the inverse modules. Second, images are reconstructed directly from the decoder embedding, rather than optimizing the inverse modules independently as described in.

illustrates reconstructions for various images with fine-tuned models as well as our default model. The left column displays the original images, while the subsequent columns show reconstructions generated with progressively smallervalues. Across all examples, a consistent pattern emerges: for highvalues, the reconstructions exhibit high quality, faithfully capturing details and colors. Asdecreases, reconstruction quality deteriorates, increasing blurriness, loss of detail, and noticeable color shifts. This degradation is particularly pronounced in the last two columns. Interestingly, there are apparent differences in the last two columns, as thecolumn introduces a gray tone to reconstructions. While both DETRs corresponding to these columns were optimized purely for object detection, the inverse models were optimized differently. One represents our default approach of training inverse modules independently, whereas the other was trained using the method described in. These differences highlight the significant impact of the two optimization techniques.

We quantitatively analyzed this pattern in, which shows the MSE alongside the average precision (AP) for our default and fine-tuned models. The results align with the qualitative assessment of the reconstructed images: asdecreases, the reconstruction error increases. Notably, asdecreases, object detection performance improves, highlighting a trade-off between reconstruction quality and object detection performance in DETR.

From the contextualization of our approach, we draw the following conclusions. Firstly, feature inversion is a viable method for interpreting DETR, as reconstruction performance is inherently linked to object detection performance, meaning that the properties of reconstructed images reflect the information processing within DETR. Secondly, training inverse modules independently offers distinct advantages over optimizing the inverse model to reconstruct images from the decoder embedding directly. While the latter approach achieves better overall reconstruction performance, it diminishes the contextual characteristics of the reconstructions. Visual inspection of these images, as shown in, along with the strong reconstruction performance of an average-guessing baseline (see), suggests that this optimization tends to push reconstructions toward a grayish average image of the dataset, thereby reducing interpretability.

SECTION: Case study: coloring objects
Having contextualized our method, we investigated the role of color information in DETR in greater detail. To this end, we recolored specific objects in input images. We evaluated the influence of these modifications on reconstructions from intermediate embeddings at different stages within the network and the influence on object detection performance. For recoloring, we used the segmentation annotations from the COCO dataset to apply five different color filters in the HSV color space to various object categories. For each filter, we adjusted the hue of specified objects to red, green, or blue or shifted the hue values by 120 or 240 degrees. We then evaluated the reconstructions and the object detection performance with AP. Since segmentation annotations in COCO are most accurate for large objects, all evaluations were conducted on objects with segmentation masks of at leastpixels.

illustrates recolored images and the influence on DETR with several examples (additional examples are provided inof the supplementary material). Each row presents an example from a different object category (from top to bottom, stop sign, bear, apple, and bus) with a different color filter. For each example, images are shown at different processing stages of DETR, displaying reconstructions from different embeddings. We observed that color changes were preserved in the backbone embedding for all objects and filters but faded or disappeared almost entirely in the encoder embedding. There was almost no color information from the input in the decoder embedding. Instead, colors shifted towards prototypical representations (e.g., red for the stop sign and bus, brown for the bear, or red and yellow for the apples). This finding contrasts sharply with previous results of inversion of feature analysis in CNNs, where color information was largely preserved throughout all layers.

While it is unclear whether DETR adjusts or removes colors entirely, resulting in prototypical fills, these transformations suggest that the model exhibits robustness to color changes. This robustness is further supported quantitatively in the last column, with the exception of the apple category, where we show AP performance for unchanged objects compared to all color filters across rows for the respective objects. The graphs confirm this observation, as AP values remain relatively consistent regardless of color changes or object category.

Building on our observations of the color robustness of DETR, we analyzed the individual influence of different color perturbations. To do this, we applied the previously described color perturbations to images and reconstructed them from various stages of DETR.provides an example of reconstructions for one image under different color perturbations. Consistent with earlier observations, color perturbations are noticeable in reconstructions from the backbone embedding but gradually diminish in reconstructions from later stages of DETR. Interestingly, reconstructions from the encoder and decoder embeddings converge to the same color and not to individual prototypical colors, becoming increasingly similar. Additionally, while reconstructions from the backbone and encoder embeddings are broadly similar in shape, objects in the decoder reconstructions show shape distortions that vary depending on the applied filter.

We quantified this effect by calculating the average pairwise MSE between reconstructions for perturbed and unperturbed images at each embedding stage.presents these results. We observed that the average pairwise MSE decreases progressively from the input to reconstructions derived from the backbone and encoder embeddings, reflecting increasing similarity. However, the MSE returns to input levels in reconstructions from decoder embeddings. This observation aligns with our qualitative analysis, confirming that reconstructions converge to the same or similar colors the farther they progress through the DETR architecture. The increase in average pairwise MSE at the decoder stage is likely not due to color divergence but caused by distortions in object shapes.

SECTION: Reconstructing intermediate representations
In contrast to the design philosophy of CNNs – where intermediate layers often vary in dimensionality (with exceptions, e.g., intermediate layers within ResNet building blocks) – transformer-based DNNs use a consistent dimensionality across intermediate layers within their encoders and decoders. This consistency enables using all intermediate representations as inputs to inverse modules, even if the inverse module is optimized for a different representation. For instance, all intermediate encoder representations can be fed intoeven thoughis optimized for the encoder embedding.

Using our inverse modules, we leveraged this feature to evaluate DETR’s intermediate encoder and decoder representations.shows an illustrative example (additional examples are provided inof the supplementary material). Specifically, the first column depicts reconstructions obtained by feeding the encoder’s intermediate representations into. In the second column, the same intermediate representations were passed throughand then through. The last column displays results for intermediate decoder representations passed through, followed byand.

As expected, we obtained the best reconstruction performance for the layer that each inverse module was trained on: encoder input (backbone embedding) for, encoder layer six (encoder embedding) for, and decoder layer six (decoder embedding) for. The quality of reconstructions gradually decreased as we moved farther away from the layers the inverse modules were optimized for, a pattern particularly evident with the input tosince object queries initially hold values independent of the input image.

Despite this degradation in quality, we generally observed strong shape preservation between intermediate layers, particularly when feeding intermediate encoder representations to. Most variations inandmanifest as color shifts, while reconstructions fromshow greater stability in color than shape. The overall stability in reconstructions across layers is noteworthy, as inverse modules might be expected to produce only noisy outputs when applied to intermediate embeddings.

From these observations, we draw two conclusions. Firstly, intermediate embeddings in transformer-based DNNs change gradually across layers, as suggested by Raghu et al. for ViTsor by Lui et al. for LLMs. Secondly, inverse modules are practical tools for interpreting transformer-based DNNs, as a single inverse module can be applied across multiple layers, eliminating the need to train separate inverse modules for each layer.

SECTION: Discussion
In this work, we took a closer look at DETR by inverting the three main modules of the network architecture – its backbone, encoder, and decoder. We used these inverse modules to study DETR’s, by reconstructing input images from different stages. Following the premise that reconstruction quality reveals the information present in each layer, we evaluated the reconstructed images for distinctive features. We inferred that DETR is preserving object representations within its encoder and decoder, is robust to color changes, and that features evolve gradually across the architecture.

Our findings align with previous analyses of vision transformers (ViT), which have been studied in the literature using different tools. For instance, the gradual change of representations was reported for ViT by Raghu et al.. Additionally, the robustness of ViT to various image perturbationsis comparable to our findings of DETR’s robustness to color perturbations. This suggests that the properties we observed for DETR and those reported for ViT are not specific to these architectures but may reflect general patterns in transformer-based DNNs for vision.

Our approach draws inspiration from Dosovitskiy and Brox’s work, where they inverted intermediate representations in AlexNet. Compared to their study, we find that DETR is more shape- but less color-preserving than AlexNet. These differences align with recognized distinctions between CNNs and transformer-based vision models in the literature.
An important difference between our work and Dosovitskiy and Brox’s is that we did not train separate inverse models for each intermediate layer. Instead, we inverted DETR’s main components, allowing us to perform similar analyses with fewer inverse models, thanks to the consistent dimensionality across transformer layers and the gradual evolution of features within the architecture. This efficiency suggests that our method is well-suited for studying transformer-based DNNs in vision tasks and opens pathways for future interpretability studies, such as exploring newer versions of DETR or ViT.

While our approach is efficient, a primary limitation of feature inversion, in general, is that properties observed across reconstructed images must be carefully interpreted. This caution arises from the uncertainty of whether a specific property originates from a transformer module itself or from its inverse counterpart. For instance, whether the prototypical colors observed in reconstructions are introduced during the forward pass through the transformer model or added by the inverse module to minimize the reconstruction error is unclear.

The success of our modular inversion method in DETR could have implications beyond computer vision. For instance, in computational neuroscience, generative models of episodic memory require the integration of both discriminative and generative processes. Future iterations of such models might unify a transformer-based DNN and its inverse within a single architecture. Similarly, transformer-based DNNs could be promising candidates for biologically plausible learning architectures, as they can leverage local reconstruction losses.

SECTION: Acknowledgments
This work was supported by a grant from the German Research Foundation (DFG),
”Constructing scenarios of the past: A new framework in episodic memory”, FOR 2812, project number 419039588, P5 (L.W.).

SECTION: References
SECTION: Extended reconstructions from DETR stages
SECTION: Extended reconstructions for color perturbations
SECTION: Extended reconstructions from intermediate layers