SECTION: Transformers Meet Relational Databases

Transformer models have continuously expanded into all machine learning domains convertible to the underlying sequence-to-sequence representation, including tabular data. However, while ubiquitous, this representation restricts their extension to the more general case ofrelational databases. In this paper, we introduce a modular neural message-passing scheme that closely adheres to the formal relational model, enablingdirectend-to-end learning of tabular Transformers from database storage systems. We address the challenges of appropriate learning data representation and loading, which are critical in the database setting, and compare our approach against a number of representative models from various related fields across a significantly wide range of datasets. Our results demonstrate a superior performance of this newly proposed class of neural architectures.

SECTION: IIntroduction

While the approaches to mathematical modeling of complex systems, ranging from control theory to machine learning (ML), evolved in various independent ways, one aspect remained almost universal — thedata representation. Irrespective of the used models, from decision trees to neural networks, virtually all ML libraries expect input samples in the form of fixed-size numeric tensors, most often just (feature) vectors. Assuming the data samples as independent points in-dimensional spaces is extremely convenient and allows for building directly upon the elegant foundations of linear algebra and multivariate statistics[1]. However, actual real-world data is not stored in numeric vectors or tensors but mostly in the interlinked structures of internet pages, knowledge graphs, and, particularly,relational databases. Indeed, while there are numerous data storage formats, the traditional relational database management systems (RDBMS) arguably dominate the industry, from medicine and engineering to enterprise application domains[2].

In recent years, we have witnesseddeep learningto quickly dominate all perceptual domains, from vision and speech to language. Nevertheless, it remains very rare to encounter neural models on the classictabular datawith heterogeneous features, where standard statistical models, mainly various decision tree ensembles[3], still appear to lead the benchmarks[4]. Improving the performance of the neural models, primarily the omnipresentTransformerarchitecture[5], on tabular datasets gains increasing amounts of attention, sometimes quoted as the “last unconquered castle” for deep learning[6]. Nevertheless, generalizing Transformers from the tabular to the fullrelationaldata model posits arguably an even bigger challenge.

In this paper, we introduce a new class of such deep learning architectures aimed directly at relational database representation while utilizing insights from the established field ofrelational learning[7], which is concerned with such generalizations of statistical models.

The core contribution of our work, put into context of related work in Sec.II-F, is the design of a new neural message-passing scheme following the formal relational model while deeply integrating the existing (tabular) Transformer architectures. The implementation of the proposed framework is readily available at Github.111https://github.com/jakubpeleska/deep-db-learning

SECTION: IIRelated Work

While the body of work on using deep learning with relational databases themselves is extremely scarce, there are established machine learning areas that either use neural models on simpler data structures or address relational structures with other (non-neural) models. In this section, we first briefly review these fields, often overlooked in deep learning, to properly position the contribution of our work (Sec.II-F).

SECTION: II-ATabular models

Tabular neural models[8]are concerned with transferring deep learning strategies into the (classic) tabular data setting, currently still largely dominated by standard statistical models, such as gradient-boosted trees[3]. These commonly aim to amend the Transformer architecture[5]to better fit the complex, often heterogeneous and discrete, attribute structure of the tabular data.
Some notable models in this category include the TabNet[9], which uses a custom-modified transformer-based architecture; TabTransfomer[10], which focuses on categorical values while utilizing the original Transformer Encoder structure; SAINT[11], which introduced the concept of inter-sample attention; and Trompt[12], which takes inspiration from prompt learning of language models.
We note that these tabular Transformers are sometimes (confusingly) referred to as “relational.” However, they do not follow the actual relational (database) model and cannot be (directly) used as such.

SECTION: II-BStatistical relational learning

For decades[13], proper learning with actual relational representations has been the concern of the little-known field of Relational machine learning[14]. It builds heavily on the formalism of first-order logic (FOL)[15], in which the tabular representation and the corresponding models are effectively viewed aspropositional, while the database representation, corresponding formally to a subset of FOL, requiresrelationalgeneralization(s) of such models.
Many such FOL-based methods have been proposed, mostly following the paradigm of Inductive Logic Programming (ILP)[16], later extended with probabilistic methods under the umbrella of Statistical Relational Learning (SRL)[7]. The most appropriate SRL works capable of learning from database representations then follow the paradigm of “lifting,”[17]referring to the generalization of classic statistical models into the relational setting.
However, building on the FOL foundations, the SRL models typically do not scale well and, importantly, do not offer the latent representation learning capabilities of neural networks.

SECTION: II-CPropositionalization

From the SRL view, the Tabular Transformers address the exact same representation expressiveness as their classic tree-based counterparts they aim to surpass. The tabular, also known as “attribute-value,” data format
is an established ML representation perpetuating the whole field.
While much of the real-world data structures, such as relational databases, donotfit into this representation, a natural urge arises to transform such structures into the expected format and proceed with the standard models. This practice, generally referred to aspropositionalization[18], is the traditional method of choice that has dominated the industry[19,20]. Propositionalization is essentially a data preprocessing routine where relational substructures get extracted and aggregated into standard statistical (tabular) attributes corresponding to various select-join-aggregate (SQL) routines in the database setting.
Building on decades of practice, the resulting (statistical) models using the resulting attribute vectors typically perform very well. However, their representation learning capabilities are principally limited, as the preprocessing (denormalization) stepnecessarilyintroduces an information loss.

SECTION: II-DNeuro-symbolic models

An interesting area on the intersection of proper relational (logical) representations and deep learning is known as Neural-Symbolic Integration[21]. There is a (small) number of neuro-symbolic frameworks that operate with some (subset of) FOL representation, effectively covering the relational databases while marrying the principles of neural networks through deep integration, such as Neural Theorem Provers[22], Logic Tensor Networks[23], or Lifted Relational Neural Networks[24]. These methods are, in theory, capable of actual deep learning from relational databases. However, to the best of our knowledge, none of these methods scales to real-world database sizes due to the complexity associated with their FOL-based foundations, except for those that follow some form of the propositionalization scheme under the hood, such as[25].

SECTION: II-EDeep relational models

The closest related work consists of extending standard neural models towards relational representations.
The most prominent models in this category are Graph Neural Networks (GNNs)[26]designed for end-to-end learning with graph-structured data. There are currently hundreds of the original GNN model[27]variants, some of which are close in spirit to our proposal, particularly some of the hyper-graph[28]and multi-relational[29]extensions towards knowledge-graph applications[30]. Nevertheless, the graph-based view adopted within this stream of research is generally not concerned with the salient features specific to relationaldatabases, particularly with the rich inner structure of the individual records.

There have been only very few works that address (some of) the database-specific aspects. Particularly, the original work of[31]followed by an (unsuccessful) pre-training procedure in[32], and the work of[33], which further incorporated feature engineering and random architecture search to improve its performance.
A different line of work has been to utilize techniques from pre-training (large) language models while treating related database tuples as sentences, similarly to the tabular models[34], such as in[35].

In a similar spirit, the authors of[36]presented a (draft) vision for foundational database models, later shifting focus to scaling up GNNs for the task in[37]by leveraging symmetries. Likewise, a recent position paper of[38]aimed to establish “relational deep learning” as a new machine learning subfield while introducing a framework for benchmarking the GNN models,222focusing heavily on the temporal dimension of database records, which we explore experimentally in App.Csuch as[39],[40], and[41].

SECTION: II-FOur Contributions

Our work can be seen as a continuation of these deep relational learning efforts, most notably the work of[41]that this paper directly expands. Particularly, we extend the existing GNN paradigm by tightly integrating the Transformer architecture into the relational message-passing scheme.
Thus, apart from proper treatment of the inter-relational structure, we also incorporate, in the spirit of the tabular Transformers, theintra-relationalstructure of the attributes, embedded end-to-end within the same learning scheme. Covering the GNN efforts as a special case, we introduce the most complete framework for deep learning withactualrelational (SQL) databases, demonstrating superior results over the widest range of available benchmark datasets reported thus far.

SECTION: IIIBackground

SECTION: III-ARelational Databases

The principles of relational databases are formally based on therelational model[42], rooted in FOL[15], providing a unified declarative specification for managing structured data, irrespective of the particular software implementation. This abstraction allows the definition of any database as a collection of-ary relations defined over the domains of their respective attributes, managed by the RDBM system to ensure consistency of the data with the integrity constraints of the logical database schema.
The key concepts to be used in this paper are as follows.

Formally, an-ary relationis a subset of the Cartesian product defined over the domainsof itsattributesas, where. Each relationconsists of a heading (signature), formed by the set of its attributes, and a body, formed by the particular attribute values, which is commonly viewed as atableof the relation.

Attributesdefine the terms of a relation, corresponding to thecolumnsof the respective table. Each attribute is a pair of the attribute’s name and atype, constraining the domain of each attribute as. An attributevalueis then a specific valid value from the respective domain of the attribute.

Antuplein a relationis a tuple333the ordering is instantiated through the naming of the attributesof attribute values, whererepresents the value of the attributein. The relation can thus be defined extensionally by theunorderedset of its tuples:, corresponding to therowsof the table.

Besides the domain constraints, the most important integrity constraints are the primary and foreignkeys. Aprimarykeyof a relationis a minimal subset of its attributesthat uniquely identifies each tuple:

Aforeignkeyin relationthen refers to the primary keyof another relationas

This constitutes the inter-relations in the database, with the RDBMs managing thereferential integrityof.

SECTION: III-BDeep Learning

Deep learning[43]is a paradigm characterized by the use ofgradient descentto optimize parameters of nested functions, commonly viewed through their computation graphs, referred to asneural networks. The main conceptual idea lies in learninglatent representationsof the data corresponding to the inner layers of the networks, generally constrained to the form of fixed-size numeric tensors, which restricts directly applying deep learning to relational databases. While passing beyond that limitation, we will generalize upon concepts known from two neural architectures that address two forms or related (simpler) structured representations ofsequencesandgraphs.

The Transformer[5]is a popularsequence-to-sequencemodel, relying primarily on the “attention” mechanism for inter-relating the given sequence tokens. Each input tokenhere isembeddedinto a continuous vector representation:, and combined with a “positional encoding” capturing its positional role:.
Theself-attentionmechanism then inter-relates all pairs of the input tokens to update their values as

where,, andare the so-called “query”, “key”, and “value” matrix projections (“roles”) of the input embeddings. This efficient matrix computation can (optionally) be further repeated in parallel with separateprojection matrices (multi-head attention).

In addition to the self-attention, Transformers employcross-attentionfor tasks involving two distinct streams of sequences. In cross-attention, the query matrixis derived from the target () sequence decoder’s input, while the keyand valuematrices are derived from the source () sequence encoder’s output as.
In either case, the updated valuesthen position-wise pass through two standard feed-forward network (FNN) layers:, followed by layer normalization to reduce internal covariate shift, and residual connections for improved gradient propagation.

GNNs are a general class of neural models aimed atgraph-structureddata using the concept of (differentiable)message-passing[26]. Given an input graph, with a set of nodesand edges, letbe the vector representation (embedding) of nodeat layer.
The general concept of GNNs can then be defined through the following sequence of functions:

Messagefunctioncomputes messages for each edge:

Aggregationfunctionaggregates the messages for each:

Updatefunctionupdates representation of each:

The particular choice of the message, aggregation, and update functions then varies across specific GNN models, which are commonly composed of a predefined numberof such layers, enabling the message-passing to propagate information across-neighborhoods within the graph(s). Note that the attention module of the Transformer follows the same schema while assuming a fully connected graph.

SECTION: IVProposed Architecture

In this section, we describe the proposed learning representation and the relational message-passing architecture designed for end-to-end deep learning of Transformers from databases.

SECTION: IV-AData and Learning Representations

In order to directly follow the inductive bias of the relational database model (Sec.III-A), we consider the learning representation of a database as atwo-level multi-relational hypergraph, where (i) each relationforms-ary hyperedges corresponding to the-tuplesintra-relatingits attributes, and (ii) each pairof such relationsinter-relatedthrough the foreign key constraintsforms another set of hyperedges from the respective tuple pairs. Note we consider all the tuple attributesto form the link, and not just their keys, as these may also be composite, possibly spanning the whole tuple as a corner case of, hence the forming ofhyperedgesinstead of just edges here.

Additionally, for each such foreign-key tuple pair, we also consider the “reverse” hyperedgeto be able to fully propagate learning representations throughout the database, irrespective of the (ad-hoc) ordering choices of the database designer.

We then use the tuple pairs ofandto build a bi-directional bi-partite hypergraph, connecting the tuples of the individual relations, for each foreign key constraint in the database schema.

We aim at direct deep learning from raw database storage systems with as little preprocessing as possible while retaining the proper relational model semantics[42], for which we consider the relations’ attribute valuesas the minimal processing unit, building on the formal assumption ofatomicity[42].
However, the current RDBMSs do not preserve the respective attribute type semantics required for deep learning. For instance, for integer-type (“int”) columns, the information on whether the data contained are of nominal, ordinal, or cyclic nature is missing. Similarly, string-type (“varchar”) columns may either contain actual text or encode discrete categories. However, such information is crucial to properly process the data with the neural models.

A distinction must also be made about attributes that form the key constraints as to whether they convey actual information or serve merely the referential purpose. To resolve such issues while avoiding manual data preprocessing, we have built an automated procedure that attempts to determine all such information from the database schema based on a combination of simple heuristics and selected data statistics.
Once the schema (Sec.III-A) is detected with all the attributetypesdetermined, we first proceed with theirencodingto numerical values. Notably, we (optionally) transform the textual types with a pre-trained language model, particularly Sentence-BERT[44](App.C).

We then continue withembeddingof the attributes in an appropriate fashion. Particularly, following methods from the tabular Transformers (Sec.II), we use a simple lookup table that stores embeddings of the detected categorical types, and “stack” or “linear” embedding of the numeric types (see App.C-Afor details). Additionally, we (optionally) include the cyclic (“date/time”) types with a special embedding respecting the periodic structure of the timestamp[45]. Importantly, each attribute has its own embedding function to allow for separate latent spaces.

For machine learning, we need to establish what constitutes the learning samplesin the given relational setting. In this paper, we consider the standard (self-)supervised scenario where a single attributeof a single target relationforms the output labels. Nevertheless, in contrast to the (classic) tabular setting, the input examplescan no longer be considered as i.i.d. tuples.

There are generally two cases: either (a) the database contains separate relational samples where each rowof the target tablebelongs to a single learning instance, or (b) the database cannot be split into such separate components, withpossibly spanning the whole hypergraph structure. To extract batches of the learning samples, irrespective of the structure, we follow a simple breadth-first-search (BFS) procedure, starting from each rowof the target tableand expanding over all the tables related through the foreign key constraints, in both the referenced and the referencing directions, while checking for loops.444Due to the possible interdependence between the samples, care must be taken to prevent information leakage about the labels, for which we mask out all target labels from the target columnofwhen processing the samples. Overlooking this precaution led to some inappropriate accuracy reports in some of the related works.

A salient feature of relational databases is that they can be very large, for which we optionally allow to run the loading nativelyin-databasethrough recursive SQL (self-)joins
with which minibatches of the hypergraph samplesmay be fetched into memory in a lazy fashion (with caching) from the, possibly remote, RDBMS. To make sure that the resulting hypergraph samples fit into memory, particularly in the (b) case, we (optionally) bound the BFS with a depth limit.555In inductive learning settings, this limit can be set to correspond to the perimeter of the relational receptive field of the subsequent neural message-passing, corresponding e.g. to the number of layers in GNN models (Sec.III-B), without loss of information.

Nevertheless, in the (most) cases where the whole database simply fits into memory, the whole hypergraph structure can be conveniently loaded and accessed with the more flexible neighborhoodsamplingtechniques[46]. Particularly, we utilize the heterogeneous graph sampling routine introduced in[47], which proved most suitable for our relational setting.

SECTION: IV-BNeural Architecture Space

To natively facilitate deep learning on the two-level hypergraph structure of the relational model (Sec.IV-A), we introduce a general two-level neural message-passing scheme composed of modular differentiable parameterized operations defined on the levels of (i) individualattributes(ii) and (sets of) relatedtuples. We further divide these operations w.r.t. their input-output characteristics into three categories:

standardTransformations

-aryCombinations

permutation-invariantAggregations

where,andmay refer to either the attributesor the tuples.
Note that this can be seen as an extension of the “message-aggregate-update” paradigm of the GNNs (Sec.III). An instance of the proposed scheme is outlined in Fig.1.

Every instantiation of the scheme starts with the embedding (Sec.IV-A) transformation (“Embedder”) of the individual relationattribute values, resulting into an-tuple of vectorsper each original tuplefrom.666The attribute embedding dimensionswithin and across the relations may generally differ, so as to accommodate the possibly varying information loads in the tables, but in this paper we set them to be the same for simplicity.Each such tuplethen undergoeseither(i) an attributecombination

that merges the attribute embeddings into a joint tuple embeddingor (ii) a tupletransformation

that keeps the attribute embeddings separate as.
In either case, the resulting tuple representationsubsequently enters the second level of neural computation where it gets combined with all the tuples related through the second type of hyperedges (Sec.IV-A). Particularly, eachundergoes a tuplecombination

with each, where, resulting into a set ofrepresentations for each such pair ofand the related.
Each such set of the combined representations then undergoes a tupleaggregation

where, to obtain onerepresentation.
Finally, weaggregateall such tuple representations

from all thelinked relations back into a single final tuple representationfor each.
Importantly, the same computation is performed simultaneously foreachrelationin the database, and the resulting representations may be used again as input into subsequent layers of the same computation scheme in the classic spirit of deep learning.

Additionally, the scheme allows for optional intermediate blocks (dashed borders in Fig.1).

First and foremost, this includes a “post-embedding” block that addresses the outlined division into the two options of (i) attribute combinationand (ii) transformationin the first step of the scheme. Notably, combining the attributes in the (i) case
disposes of the original column structure of, reducing the data dimensionality fromto, and turning the remainder of the scheme into a largely standard single-level heterogeneous GNN computation[48], as explored in some of the related works (Sec.II). Such operation can range from a simple concatenation toTabular Transformersthat themselves combine columns into a single row embedding, such as Trompt[12]or TabNet[9]. Opting for the (ii) transformation then retains the original tabular structure throughout the scheme, for which we utilize operations ranging from simple positional encoding to tabular Transformer blocks retaining the columns, such as the SAINT[11]and TabTransformer[49].

The subsequent (optional) tupletransformationthen follows the same logic while beingrepeatedlyapplied at the beginning of each layer of the scheme, for which the chosen model has to comply with the respective interface.
Finally, the scheme allows for a closing (optional) tuple combination, facilitating a residual connection stream in the overarching relational part.

SECTION: IV-CTheDBFormer

Technically, any differentiable parameterized operations that satisfy the corresponding input-output interface of the transformation, combination, and aggregation operators can be used in their respective places within the scheme, some of which are presented in our experiments (Sec.V).
Nevertheless, we highlight one particular instantiation that we deem to most closely integrate the essence of the original Transformer architecture[5]with the relational database model (Sec.III), which we further refer to as theDBFormer, depicted in Fig.1.

Firstly, the model instantiates a Transformer Encoder in place of the tupletransformation, facilitatingself-attentionover the relations’ attributes in the standard spirit of the tabular Transformers[8], but repeated across the database and over the layers, as part of the relational scheme. Secondly, the model also usescross-attentionin place of the tuplecombinationas

essentially forming a Transformer Decoder from the remaining part of the scheme per each pair of interrelated relations.

We hypothesize that the cross-attention module used in this place might be able to extract the necessarylatentrelational features, as exploited with the successful propositionalization methods (Sec.II), but in a fully end-to-end fashion through gradient descent. Based on the notable expressiveness of Transformers[50], the select-join-aggregate operations normally used to construct such relational features should be well within the hypothesis space of the resulting architecture, in which we assume the query, key, and value roles of the input tokens to correspond to the foreign-key, primary-key, and column-value roles of the individual attributes, respectively.
The idea is that the self-attention firstly transforms the tuple attributes w.r.t. each other within the tables, the cross-attention then learns their contextual interactions with attributes from the referenced tuples, and the attention-sum finally weights all their importance w.r.t. the referencing tuples.

SECTION: VExperiments

We test777The source code for the experiments can be found athttps://github.com/jakubpeleska/deep-db-learningand the web server serving the database datasets is made publicly available athttps://relational.fel.cvut.cz/a number of instantiations of the proposed scheme against representative models from the distinct related work categories (Sec.II) through standard supervised classification and regression tasks across a wide range of diverse relational database datasets.

SECTION: V-ADatasets

While RDBMs are some of the most widespread data storages, publicly available relational database benchmarks are considerably scarce. There are numerous collections of classic
tabular[52]and structured datasets[53,54], including graphs[55,56], some of which are conceptually close to the database setting[57,58]. Nevertheless, none of these provideactualrelational database representations.888Instead, they present simplified CSV, JSON, or XML files that do not fully represent the RDBM setting.Thus, as part of this work, we have re-established the most complete resource collection in this area, originally created by[51], where we currently maintain overof actual (SQL) database datasets from various domains, together with historical
scoreboards and additional statistics.999This collection also covers most of the previous benchmarks from the domain of relational learning (Sec.II).In this paper, we narrow these down to 19 classification (Tab.III) and 16 regression (Tab.IV) datasets, filtering out (uninteresting) databases that are either too small or too trivial to fit.
The remaining datasets are of highly diverse characteristics w.r.t. their sizes, schemas, structures, and application domains, as further detailed in App.B.

SECTION: V-BRelated work models

As a baseline instance of the scheme, we consider a simpletabularFNN model[6]operating solely on the target table, i.e., ignoring all the inter-relations. This naive strategy is useful in revealing whether the given dataset task is indeed relational in nature or not.
From the statisticalrelational learning(Sec.II), we choose the state-of-the-art RDN-boost[59], which, following the lifting strategy, can (very roughly) be seen as a relational generalization of the popular gradient-boosted trees[3].
As thepropositionalizationrepresentative, we select the FastProp algorithm followed by XGBoost[60]– a battle-proof combination as promoted in[20], which leads a number of the relational dataset scoreboards[51].
To cover theneuro-symbolicarea, we further emulate the popular CILP++ method[25]by connecting propositionalization with a FNN model in a similar fashion.
We were unable to put any of the few recent deep relational learning proposals (Sec.III) into operation, but some of the closest GNN-based works can be viewed as conceptually close to the reduced (attribute combination) variants of the scheme (Sec.IV-B).

SECTION: V-CScheme instantiations

As the space of all the possible neural models within the proposed scheme is very large, we tested only a few selected instantiations. This means selecting some particular parameterized differentiable operations in place of the initial Embedder module, and the attributeand tupletransformations, combinations, and aggregations(Sec.IV-B).

This model, already detailed in Sec.IV-Cand Fig.1, consists oflayers where each can be defined as

With this instantiation, we further tested extending the initial baseline Embedder (Sec.IV-A), transforming merely thecategoricalandnumericalvalues to embedding vectors with the use of lookup tables and linear transformations respectively, with a number of ablations described in detail in App.C.

This model can be seen as a “reduced” version of the proposed scheme for its use of the attribute-combination function that flattens the columns’ dimension as, whereand. The reduced dimensionality then allows for the use of standard graph convolution modules. Particularly, we employed the SAGE[61]convolution, with which therepeating layers can be described as

The model uses the baseline Embedder, and the residual combination module is skipped.

This instance is designed to closely follow the tabular architecture of Trompt, as introduced in[12]. The Trompt Encoder is used once at the beginning as the “post-embedding” (Sec.IV-B) module to transform the data. Therepeating layers then have a simple definition of

where the tuple transformation and closing combination modules are skipped, and

Notably, the model utilizes the Trompt Decoder as a prediction head and has a custom Embedder that extends the baseline by following the categorical embeddings with Layer Normalization[62]. It also uses linear transformation of numerical values followed by a ReLU activation and Layer Normalization.

Another tested instance based on a tabular Transformer is the DB extension of TabNet[9]. The TabNet encoder is formed by a series of repeated Feature Transformers, each followed by the Attention Transformer.101010For further description of the Feature and Attention Transformers, we refer to the original article[9].

Similarly to the DB GNN, TabNet belongs to the “reduced” category. Its Embedder processes only thecategoricalvariables through the embeddings lookup table, and thenumericalvariables are duplicated to the target dimension by the Stack Embedder (App.C). Itsrepeated layers can be defined as

whereis defined in Equation1.

The SAINT instance refers to the tabular model introduced in[11]. The model takes a Transformer Encoder layer and extends it by a second block that uses “Intersample Attention,” the details of which can be found in the article[11].

The scheme’s instance utilizes the “SAINT Encoder” layer as the tuple transformation operation in a mixture with thecross-attentionfor the tuple combination. The model also uses the baseline Embedder with an extension that a ReLU activation function follows the linear transformation. Therepeated layers can be defined as

The last experimental instance is based on the TabTransformer[10]model. The TabTransformer architecture preprocesses only thecategoricalattributes, whilenumericalattributes are simply passed through Layer Normalization. Thecategoricalcolumns are then passed through a Transformer Encoder block.

Similarly to the TabNet instance, the Embedder uses lookup table embeddings forcategoricalattributes and a Stack Embedder fornumericalattributes to avoid transformations of the values. The rest of therepeating layers are defined as follows

withdefined in Equation1.

SECTION: V-DParameterization

We follow a largely standard parameterization routine across all the methods. For the propositionalization-based related work, the number of relational features ranges around, depending on the depth of a custom BFS procedure that we implemented to improve their default performance, and the boosting works with the optimized default ofandbase estimators. For the neural methods, including the baseline tabular FNN, we follow a standard deep learning setup of tuning the embedding dimensions, learning rate, and batch size, detailed further in App.A-B.

SECTION: V-EResults

Our classification and regression results with the models (Sec.V-B,V-C) are summarized in TableIand TableII, respectively. Firstly, we see that many of the datasets are simply not accessible (N/A) to the tabular models (Tabular), in cases where the target table does not contain any informative attributes. Nevertheless, in the few cases where it does, even simple tabular models (FNN) perform very well, in accordance with[6]

The RDN-boost is a sophisticated SRL (Relational) method that does capture the relational inter-dependencies for which it, however, needs to set up “modes,”[59]which we implemented in a rather straightforward fashion, possibly explaining its generally weaker performance. We note that we were unable to put the method into operation in the regression setting; hence, it is missing from the respective table.
More importantly, the method does not scale well to larger datasets, reported (also) with the missing values. This issue was partially shared with the other relational methods, too.
The getML (Fastprop+XGBoost) system[20], on the other hand, performed very well out-of-box, validating the strength of the propositionalization (Propos.) practice[63]. Similarly, the propositionalization-based neuro-symbolic (Ne-Sy) approach of CILP++[25]performed very strongly, too.

Finally, instantiations of the proposed scheme generally displayed superior performances, with a small number of exceptions where the propositionalization shone. The overall best results were displayed by the proposedDBFormermodel (Sec.IV-C), demonstrating the strength of the close integration between the original Transformer architecture and the relational model. Nevertheless, the GNN instantiations, as well as the Tabular Transformer integrations with Trompt[11]and TabNet[9], exhibited strong performances, too.

SECTION: VIConclusions

We introduced a general scheme that extends Transformers for deep learning from relational databases, utilizing a custom message-passing mechanism that adheres to the relational model of the common RDBMS. Our experiments with various instantiations of the scheme demonstrate its viability and superior performance as compared to commonly used methods from the associated fields of relational learning.

To improve the performance even further, incorporating self-supervised pre-training, in the spirit of the tabular models (Sec.II), for domain transfer across differentdatabasesseems like a promising avenue for future work.

SECTION: Appendix AExperimental Setup

The output of the last layer, produced for the target table, is flattened if necessary and processed by a FNN prediction head withlayers, with each of the hidden layers followed by ReLU activation and, optionally, Batch Normalization[64]. For the standard gradient descent training in the classification tasks, the FNN output feeds into cross-entropy loss and MSE loss for the regression tasks, respectively.

For the metrics used in the results reporting, we simply leverage accuracy for the classification tasks and, to provide a somewhat comparable metric, a “Normalized Root Mean Squared Error” (NRMSE) is used across the regression tasks. Thefunction is defined as

whereis the mean of all the training target values, andfunction is defined as

SECTION: A-AEnvironment

All the executed experiments discussed in SectionVused a simple hyperparameter optimization pipeline. The pipeline consisted of Ray[65], used for the distribution of resources and model training management; Optuna[66], used for searching over the hyperparameter space; and MLFlow[67], used for aggregating the parameters and metrics.

As for hardware, the training runs were split into two categories based on the dataset size, more precisely based on the number of rows in the target table (App.B). The runs on the datasets with less than or equal to 10,000 rows were trained on a single core of theAMD EPYC 7742 64-CoreProcessor and runs on larger datasets were executed onNVIDIA A100-SXM4 40GBGPU with a maximum of 4 runs sharing a single GPU.

SECTION: A-BHyperparameters

There were 16 runs per model and dataset executed as part of the hyperparameter search, each running for 4000+ training steps111111With an exception of models that reached a hard training limit of 2 hours, however, this limit was surpassed on only the most extensive datasets such as “tpcd” (App.B) with large models. Nevertheless, extending this limit possibly allows for future improvements.on a standard 70:30 training-validation split. All the neural models used vanilla Adam[68]optimizer with a learning rate set as a hyperparameter on a logarithmic space within. The heterogeneous graph sampling routine (HGSampling), as described in SectionIV-B), facilitated the data sampling where the batch size was parametrized by the dataset size, with a hyperparameter scale factor from an exponential space in the interval, and limited to a value of, whereand; hence the batch size always remained in the interval of. The embedding dimensionwas also a hyperparameter in the search space, defined as a choice from the set of. The number of layersinside the scheme’s instances was set as a random integer from.
The decision-making decoder FNN head was parametrized by the number of linear layersthat was 1, 2, or 3, where each hidden layer hadchannels and a flag whether to use the “Batch Normalization.”

SECTION: Appendix BDatasets

The database datasets[51]used for the classification and regression tasks can be viewed in TablesIIIandIV, respectively. The tables contain statistics about the relational databases that they represent: ‘Num. Rels.’ - number of relations inside the database, ‘Num. Edge. Types’ - number of primary, foreign key pairs, ‘Num. Targ. Cols.’ - number of non-key columns in the target table, ‘Avg. Targ. Edges’ - the average number of references from a single target table row to other tables, ‘Total Num. Rows’ - the overall number of rows in all tables of the database, such as ‘Total Num. Edges’ - the overall number of primary, foreign key pairs between all tables of the database, ‘Text Col.’ - whether the database contains non-key text attribute, and ‘Time Col.’ - whether the database containsdatetimeattribute.

SECTION: Appendix CDBFormerablation studies

In this appendix section, we report the ablations performed with the mainDBFormermodel. The ablations are aimed to assess the sensitivity of the results w.r.t. (i) the selection of the initial embedding and (ii) the selection of the hyperparameters.

SECTION: C-AEmbedders

The initial processing of data can often significantly influence the effectiveness of a model. Building on the work done in the field of tabular models (Sec.II), there is a variety of possible approaches. The categorical variables are almost always encoded with a simple embedding lookup table, with the exception of the models that do not use categorical variables at all, e.g., Excelformer[69]. Nevertheless, for the other variable types, several options may be considered.

Stack Embedder: the simplest option to increase the dimensionality of thenumericattributes is to copy the valuetypes in the embedding vector, whereis the target dimension of the embeddings.

Linear Embedder: a linear layer withno activationfunction, one input channel, andoutput channels is another common way to create the embedding vectors out ofnumericvariables.

Text Embeddings Transcoder: as discussed in SectionIV-B, plain text data from the database can be processed by a pre-trained language model. While it is unlikely that the language model embedding dimension will match the set-out dimension, a linear layer with no activation can again be leveraged to address the dimensionality difference.

Timestamp Embedder: the most sophisticated embedding we considered is to account for the possible periodical information that might be encapsulated by the year, month, day, etc., of the timestamp attributes, for which the embedder first uses cyclic encoding with a combination of positional encoding to dimension, where, and only then puts the output through the linear layer to get embeddings of dimension.

The classic tabular Transformer models usually only take the opportunity to combine simple embedding for thecategoricalvariables with either the Stack or Linear Embedder for thenumericalvariables. However, usage of the text and timestamp attributes can potentially lead to performance gains.
TheDBFormer, representing the leading model of this paper, was thus further tested with an additional list of such embedding options as follows:

Baseline (base): the embedder uses onlycategoricalandnumericalvariables with a simple embeddings lookup table and a Linear Embedder.

With Text (text): extends the baseline embedder withtext embeddingstransformed by the Text Embeddings Transcoder.

With Time (time): extends the baseline embedder with datetime attributes transformed by the Timestamp Embedder.

TableVcompares the performance of the baselineDBFormersetting to the one leveraging the textual embeddings. As can be seen, the textual embeddings significantly improve the model performance, confirming the usefulness of the information present in the often overlooked textual attributes.

The recently proposed work of[57]heavily emphasized the time dimension in the
relational database setting. To experimentally evaluate its importance, TableVIshows the comparison of theDBFormermodel utilizing the time attributes with the Timestamp Embedder to its baseline version. As can be seen, the Timestamp Embedder strongly improves the performance on almost all relevant datasets, again validating the importance of the information present in the time attributes. Employing both text and time attributes thus showed significant improvements in performance.

SECTION: C-BHyperparameter sensitivity

All the previous experiments were carried out with the utilization of the reported hyperparameter optimization (App.A-B). To test the robustness of the mainDBFormerarchitecture, we also present results without the hyperparameter tuning over three versions of the model listed below.

Large: embedding dimension = 64, schemelayers = 4, attention heads = 4, decoder hidden layers = 2, decoder hidden channels = 64

Medium: embedding dimension = 32, schemelayers = 3, attention heads = 4, decoder hidden layers = 2, decoder hidden channels = 64

Small: embedding dimension = 16, schemelayers = 2, attention heads = 2, decoder hidden layers = 2, decoder hidden channels = 32

All three models were trained with a learning rate of 0.0001 using the vanilla Adam optimizer. The dropout rate inside the attention modules was set to 0.1, and all the decoder heads utilized the Batch Normalization. The initial Embedder module did extend the baseline with both the Text Embeddings Transcoder and the Timestamp Embedder in all cases, with all the remaining settings (App.A) being fixed.

The results in TableVIIshow that theDBFormermodel keeps displaying superior results, even without the hyperparameter tuning, and demonstrates the robustness of the architecture. Notably, theLargemodel is within 3% of the accuracy of the optimizedDBFormer*model (highlighted in bold) on the majority of the classification datasets. TheMediumandSmallmodels then performed adequately well, even outperforming theDBFormer*in a few cases where the hyperparameter optimization apparently did not find the best settings (highlighted by underlining).

SECTION: References