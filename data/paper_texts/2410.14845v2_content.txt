SECTION: Classification of Wolf Rayet stars using Ensemble-based Machine Learning algorithms

We develop a robust Machine Learning classifier model utilizing the eXtreme-Gradient Boosting (XGB) algorithm for improved classification of Galactic Wolf-Rayet (WR) stars based on Infrared (IR) colors and positional attributes. For our study, we choose an extensive dataset of 6555 stellar objects (from 2MASS and AllWISE data releases) lying in the Milky Way (MW) with available photometric magnitudes of different types including WR stars. Our XGB classifier model can accurately (with an 86% detection rate) identify a sufficient number of WR stars against a large sample of non-WR sources. The XGB model outperforms other ensemble classifier models such as the Random Forest. Also, using the XGB algorithm, we develop a WR sub-type classifier model that can differentiate the WR subtypes from the non-WR sources with a high model accuracy (). Further, we apply both XGB-based models to a selection of 6457 stellar objects with unknown object types, detecting 58 new WR star candidates and predicting sub-types for 10 of them. The identified WR sources are mainly located in the Local spiral arm of the MW and mostly lie in the solar neighborhood.

SECTION: 1Introduction

Wolf-Rayet (WR) stars belong to the evolved class of Population I stellar objects, mainly arising from the evolutionary trajectory of massive O-type Main Sequence (MS) stars. In a binary system, MS O-type objects () undergo mass transfer to produce WR stars, either via Roche Lobe overflow or through the formation of a common envelope; while single WR star () formation is governed by the interplay of mass-loss mechanisms and rotational mixing processes. Spectroscopically, based on elemental abundances WR stars can be classified as: WN (He and N), WC (He and C), and WO (C and O) types. Based on the relative line strengths of the diagnostic emission lines(van der Hucht,2001), these stars are further classified: WR-Late (WRL; with lower-ionization lines due to low temperature) and WR-Early (WRE; hotter subtypes emitting lines from higher ionization states). Some WNL-type stars show weak emission lines of hydrogen present in their outer envelopes. Such objects are called non-classical WR stars and are suffixed with ’h’. With surface temperature ranging fromK and luminosity spanning from, WR stars are known for broad emission lines propelled by supersonic stellar winds (=10003000) which likely undergo Type Ib/Ic supernovae explosion, thereby contributing significantly towards both mechanical and chemical enrichment of the surrounding interstellar medium and providing the necessary habitat for young star formation. WR stars often occur in clusters, making them suitable tracers of massive star formation regions in the MW(Davies et al.,2012)and in Starburst Galaxies(Schaerer & Vacca,1998). WR stars exhibit strong free-free emission leading to excess flux in the near-IR (NIR) bands making them distinguishable from other stellar objects.

From IR(Mauerhan et al.,2009; Shara et al.,2012; Rosslowe & Crowther,2015)and optical surveys(Rate & Crowther,2020), a significant population (670,Rosslowe & Crowther (2015)) of WR stars in the MW has been detected, while simple population models(Rosslowe & Crowther,2015)predict that around 2000 WR stars may be present. However, distinguishing WR stars from other stellar objects solely based on optical colors and magnitudes has been challenging because of strong interstellar dust extinction. Meanwhile, the IR bands suffer from much less obscuration than the optical bands, thus benefiting the detection of WR stellar candidates. With the advent of modern instruments such as JWST, Roman telescope, etc., we are bound to discover several thousands of such evolved stellar objects lying in the Local Group of galaxies.

Selection criteria based on broad-band IR (2MASS and GLIMPSE) colors and magnitudes byHadfield et al. (2007); Mauerhan et al. (2011)have been successful in filtering non-WR from WR candidates. Free-free emission and/or emission from circumstellar dust leads to color-excess in the IR which benefits WR candidate identification based on the color-color diagrams. With the increase in IR survey data over the last few decades (2MASS, IRTF, WISE, Herschel, etc.) it becomes cumbersome to manually perform this task. Machine learning (ML) techniques are well suited for handling large datasets and offer a powerful and flexible approach to stellar classification. Previously, distance-based algorithms such as K-Nearest Neighbor (KNN) and Support Vector Machine (SVM) have been used for classification of WR stars but on a very small dataset of objects(Morello et al.,2018; Dorn-Wallenstein et al.,2021). A comparative study byYoshino et al. (2023)showed that ensemble-based algorithms such as XGBoost (XGB) and Random Forest (RF) perform consistently better in stellar classification than a simple Decision tree, KNN, or SVM models.

In our current study, we use the XGB methods to build the most efficient stellar classifier customized to proficiently identify WR stars based on the IR colors and positional coordinates of the objects. The method is designed to identify WR stars in large datasets comprised of various types of IR-bright objects. We describe the methodology used to develop the classifier
models in Sec.3. Our results are given in Sec.4including a comparative evaluation with other ensemble methods such as Random Forest. Also, we conduct novel research on the application of ML models to distinguish the WR subtypes (WC and WN) from the non-WR objects (in Sec.4.2). A list of new WR stars (and their chemical subtypes) predicted by both models on an unlabelled and unseen Galactic stellar dataset is presented in Sec.4.3. Further, we discuss the performances of both the models against color-selection methods in Sec.5. Concluding remarks are presented in Sec.6.

SECTION: 2Data Sources

For this study, we utilize the apparent photometric magnitudes of stellar objects in the IR bands. The NIR and mid-IR (MIR) data used in this study were observed using the 2-Micron All Sky Survey (2MASS) and the Wide-field Infrared Survey Explorer (WISE). The 2MASS photometric data were observed across 3 broad-bands: J (1.25), H (1.65) and(2.16) bands while the WISE data were observed in 4 different broad-bands: W1 (3.4), W2 (4.5), W3 (12) and W4 (22). We selected all the sources with an available object-type flag from the SIMBAD database and then cross-matched them (within a radius of 5”) with the Vizier catalogs of 2MASS and WISE surveys: II/246/out(Cutri et al.,2003)and II/328/allwise(Cutri et al.,2021)respectively. To address the dense source distribution near the Galactic Center, we eliminated redundant sources by further cross-matching such sources with a 1” radius. In the dataset, other than WR stars, we consider various types of stellar objects such as MS, Asymptotic Giant Branch (AGB; including the subtypes Mira and OH/IR), Be, Red Super Giant (RSG), Long Period Variable (LPV), Young Stellar Objects (YSO), C, S, High Mass X-ray Binary (HMXB), Emission line (Emline), Yellow Super Giant (YSG), RRLyrae, Ae, Red Giant Branch (RGB), High Proper Motion Star (High PM), RCrB, beta Cepheid Variable (bCepV), Eclipsing Binary (EB), RV Tauri Variable (RVTauV), Orion Variable (OrionV), Classical Cephieds (Ce), Hot Sub-dwarf, Horizontal Branch (HB), Variable, Symbiotic, Pulsar, Blue Super Giant (BSG), post-AGB (pA) and Planetary Nebulae (PNe).

SECTION: 3Methods

In this study, we follow the procedure as described in the following subsections (Sec.3.1-3.5). Before defining the ML model, it is crucial to pre-process the dataset which includes sorting the data and selecting the model features.

SECTION: 3.1Sorting Data and Features

We chose stellar sources with Dec-63.5∘in order to exclude objects present in the (Large and Small) Magellanic clouds. Thereafter, we cleaned the dataset by choosing 10242 Galactic stellar objects with available apparent magnitudes in both 2MASS (,,) and WISE (,,,) bands. Further, we selected objects based on the quality (flagged as A, B, or C ; where flags are defined from A-D, with A as best data with high Signal-to-Noise (S/N) while D as poor without any S/N information) of their 2MASS photometric data (seeSkrutskie et al. (2006)for S/N thresholds of the quality flags).

FollowingMauerhan et al. (2011), by visual inspection we identify the color space based on the concentration of WR stars in our dataset. Accordingly, we employ the following selection criteria: J, H,, W1, W2, W3, and W4. The constrained color space leads to better sampling and increases the chances of WR identification(Mauerhan et al.,2011). The color-color plots of the stellar objects in our final dataset are shown in Fig.1. Fig.2shows the population distribution for different types of objects present in the final dataset (hereafter, Dataset-1 comprising 6555 objects). The majority of the objects in our dataset are AGBs and Be-type stars with WR stars being about 7% of the total stellar population. Objects with a population lower than 40 are categorized as ”Others” (see Table1). It must be noted that there is a lack of PNe, pAs, and YSOs in our dataset which is mainly due to the color-selection criteria adopted in this study. However, adding these sources would not impact our model, as long as the samples on which we run the ML models on have the same color cuts applied as the training sets. Thereafter, we declare a label column that contains discrete values, i.e. either 1 (for WR) or 0 (for non-WR) as reported in SIMBAD. In our study, the objects are not refined into temperature classes which enables us to introduce a high imbalance in our dataset by treating all non-WR sources (i.e. 6122 objects) as one large (majority) class while the WR stars (i.e. 433 objects) are labeled as the other (minority) class. This makes our (binary classifier) models much easier to implement than earlier (multi-class classification) studies(Morello et al.,2018). To treat the highly imbalanced dataset where WR stars belong to the minority class, we useRandomOversampler111https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.RandomOverSampler.htmlfromimbalanced-learn222https://imbalanced-learn.org/stable/index.html. Random oversampling is a technique used in ML to address class imbalance in datasets. Class imbalance occurs when one class (the minority class) is significantly underrepresented compared to another class (the majority class). This can lead to biased models that favor the majority class (in this case non-WR sources) and perform poorly on the minority class (WR sources). In random oversampling, examples from the minority class are randomly selected and duplicated until the class distribution is more balanced. By increasing the number of instances in the minority class, the classifier can learn from more representative data and make better predictions for that class. Alternatively, we could have under-sampled the majority class randomly many times, but chose to oversample instead because under-sampling might eventually lead to underfitting in the model.

For the second part of our study, we utilize a different dataset (Dataset-2) with a separate column in which we further designate the WR subtypes (WN and WC) as two separate entities: non-WR (as 0), WC (as 1) and WN (as 2). As the smaller number of WO-type sources creates a large imbalance that affects the accuracy, we exclude them from the dataset. We further cross-match with SIMBAD to choose WR candidates with known sub-types lying in the same IR color space as the objects in Dataset-1. The population density of different spectral types present in Dataset-2 (with total candidates of 6510) is shown in Fig.3. In this case, as well, we treat the minority classes (WC and WN) using the random-oversampling method as mentioned earlier.

We randomly divide both datasets (1 and 2) into two parts: Training datasets (Trd-1 and Trd-2) containing 80% of their respective samples and testing data sets (Tsd-1 and Tsd-2) containing 20%.

As for the model features, we choose a list of features that consists of positional attributes (Right Ascension and Declination) as well as the IR colors (,,,,and). For this work, we do not include the magnitudes, as neither distance estimates nor extinction values are known for the whole sample. Although IR colors can be affected by distance-dependent extinction, this effect is much smaller than the effect on magnitudes or even optical colors.Mistry et al. (2022)used the positional coordinates (RA and DEC) along with other observational attributes as model features for the identification of CVs present either in MW or other nearby galaxies. Stars of any particular class are found mostly at certain regions of any galaxy (including the MW) which helps in their identification regardless of the galaxy in which they are found. Therefore, including the positional attributes as model features will surely facilitate the detection of WR stars, especially within the MW. Also, we have a large number of sources spread across the observable disc of the MW without any significant bias in the Galactic plane (see Fig.4). Therefore, our model is well-suited to WR identification across the MW, but we do not recommend its use outside of the MW.

SECTION: 3.2Machine Learning models

We deploy two supervised ML model algorithms:Gradient Boosting(GB)(Friedman,2001)andRandom Forest(RF)333https://scikit-learn.org/stable/modules/ensemble.htmlalgorithms provided byScikit-learn(Pedregosa et al.,2011)respectively. Supervised ML models utilize labeled datasets to train algorithms for predictions or decisions for classification problems over very large datasets. By providing the algorithm with labeled examples, supervised learning enables the model to generalize patterns and make accurate predictions on unseen or blind data. Any ensemble classifier is commonly defined using a set of parameters that can comprise of:n_estimator(number of base learners),max_features(maximum number of features for a learner),max_depth(maximum number of nodes in a learner), andsubsample(random sampling ratio of data to avoid overfitting).

The XGB444https://github.com/dmlc/xgboost(Chen & Guestrin,2016)library is an advanced implementation of GB classification methods. This boosting technique combines multiple weak learners (decision trees) to generate a stronger model. In brief, adecision treebreaks down the feature space into smaller sections through repeated splitting based on feature values, creating a tree-shaped structure where eachinternal noderepresents a decision point, eachbranchsignifies the result of that decision, and eachleaf noderepresents the final predictions of the model. The predictions from the ensemble of such weak learners are used to estimate thetraining lossfunction that is used as an input for the next learner. Each learner is constructed sequentially, aiming to minimize the overall loss of the ensemble while incorporatingregularizationto prevent over-fitting. The XGB is faster and more efficient than the standard GB as it can build base learners in parallel for a large dataset. The XGB-tree classifier model not only depends on the common parameters (see Sec.3.2) but also on thelearning_rate(step size of learner that modifies the feature weights for the next learner),min_child_weight(minimum weight for the sum of the features in the learner branch), andmin_split_loss(minimum loss reduction for splitting a leaf node). Earlier studies for example,Gomes et al. (2023)andChao et al. (2019)showed the efficient capability of XGB algorithms as applied to a regression model to predict rotation periods, and a classifier model to distinguish between stars and galaxies, respectively.

For comparison, we also develop an RF classifier model(Breiman,2001), which trains the model with several random subsets of the features using an ensemble ofDecision Treemodels. Each tree in the forest is trained on a random subset of the data and a random subset of features, promoting diversity among the trees. The feature selection by the model is done based on the choice of theentropyfunction. The bootstrap aggregation used for estimating the final model prediction (from several trees) reduces over-fitting. RF models work better than a singledecision treemodel because of this bootstrapping. Like any other ensemble model, an RF classifier is mostly based on the choice of the general parameters (as mentioned in Sec.3.2). It is considered that RF classifiers with built-in parallelization are less prone to over-fitting than a sequential GB classifier. Therefore, using an RF model as a benchmark is an effective way to evaluate the performance of the XGB model.

SECTION: 3.3Model Performance

The models are evaluated from the confusion matrices that represent the classification of objects present in a test dataset. The model performance is further judged based on the the four important metrics: Accuracy(A), Precision (P), Recall (R), and f1-score (f1):

where TP, FP, and FN denote True Positive, False Positive, and False Negative respectively. The model’s Accuracy (Eqn.1) indicates the total number of correctly classified instances (both positive and negative classes) from the total number of instances on which the model was applied. The harmonic mean of the precision and recall is denoted by the f1-score which indicates the overall efficiency of the model in predicting only the positive class. A high value of precision means the model predicts fewer FPs while recall indicates the rate at which the model predicts the positive class of objects. In this work, WR sources belong to the positive class whereas the negative class represents the non-WR type objects.

SECTION: 3.4Feature significance

The important features used in our work are decided from the values of the individual performance metrics (from eqn.1-4). Firstly, we develop classifier models for each of the features and identify which feature achieves the highest f1-score (eqn.4). To find the optimal set of features that are significant for the f1-score of the classifier, we use theRecursive Feature Elimination with Cross-Validation(RFECV555https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html) method. To prevent data leakage, we define a pipeline that fits thisRFEfunction on the oversampled 5-randomized subsets (using theStratifiedKfold666https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.htmltechnique) of the training dataset and calculate the average f1-score. In brief,StratifiedKfoldmodule (fromscikit-learn) randomly splits the dataset such that the ratio between positive and negative classes of objects remains the same in each of the subsets of the dataset. The classifier model predicts the best set of features from the cross-validation scores generated across all the k-fold subsets. In brief, cross-validation repeatedly splits the dataset into a specified number of subsets (in this case k-fold), trains the model on any combination of k-1 subsets, and tests the model on anysubset. This approach helps reduce overfitting by confirming that the model performs consistently well across different data segments, rather than relying on a single train-test split.

SECTION: 3.5Hyper-parameter tuning

Our classifier (based on the significant features) is trained and cross-validated on randomly oversampled subsets (using theStratifiedKfoldapproach) of the TrD. Usingoptuna(Akiba et al.,2019)we perform a Bayesian-optimization and optimize the f1-score. The best set of hyper-parameters are derived from a grid of model parameters that achieve the highest f1-score.
Finally, another classifier model is defined using the determined set of model hyper-parameters and based on the significant features. This final classifier is fitted on the complete oversampled TrD. Thereafter, this model is applied to the TsD, and the labels are predicted.

SECTION: 4Results

SECTION: 4.1WR versus non-WR classification

Using the RFECV method (as discussed in Sec.3.2.1) the optimal number of features of the classifier model is determined. In Fig.5, we show the RFECV curves with our set of features.

It is seen that with the sequential addition of more features, the mean of the f1-score (across the Stratified 5-fold randomized subsets of Dataset-1) increases. The mean f1-score saturates beyond W3-W4, which means the model reaches its maximum predictive ability using only these 8 features. Therefore, models based on these features and arranged in the same order (as shown in Fig.5) achieve the highest f1-scores. Hence, we use these features to define another XGB and RF classifier which are trained over 5-randomly oversampled subsets of TrD-1 to find the best set of hyper-parameters (see Table2) using Bayesian Optimization technique. Figure6illustrates the first decision tree with model features and classifying conditions of the final XGB model. During the training process, the decision tree automatically chooses and updates the order of the features based on their significance in the classification of objects. Also, the model works on the principle that if a feature value of an object satisfies the selection criteria (’yes’), it is passed on to the next internal node while if it does not (’no’, in our case), then it is passed on to some other relevant internal node.

We derive the Precision-Recall (PR) curve (see Fig.7) to compare the performance of the classifiers. The high precision of the RF classifier shows that it detects fewer FPs than the XGB classifier. The Area Under the Curve (AUC) in the case of the XGB classifier is found to be the same as that of the RF classifier. The maximum f1-scores (marked in the plot) achieved by the models indicate that the XGB performs better than the RF model. This is also seen from Table3.
We also show the performance of the classifiers using these confusion matrices (see Fig.8); which provide a detailed breakdown of the model’s predictions by comparing the actual and predicted classes. The diagonal elements represent the true predicted instances and the remaining elements indicate the wrong classifications. We note that the XGB classifier (Fig.8(b)) outperforms the RF-classifier (Fig.8(a)) in the accurate predictions of the minority class. We further compare their performances in Sec.5.

In Figures9and10, we show the color-color predictions of TPs by the final classifiers when applied to the test dataset (TsD-1). In our datasets, the AGB and Be-type stars are the dominant classes which are known for IR colors similar to that of WR stars. The NIR color space of classical Be-type stars (with free-free emission from the ionized circumstellar disks) overlaps with that of the WR stars and is detected by the models (see Fig.9(a) and10(a)) as FPs that mostly lie in the clustered region of the color spaces(Lee et al.,2011). The Spectral Energy Distribution (SED) of WR stars with color-excess due to strong free-free emission and/or circumstellar dust in general peaks around the W2-band(Lau et al.,2023), coinciding with those of AGB stars with dust emission(Suh,2020). This overlap results in the poor detection statistics of WR stars in the W2-W3 color space (see Fig.9(b) and10(b)). However, at longer MIR bands, the dust emission from the AGBs is significantly less pronounced than that from WR stars, making WR stars more easily identifiable in the W3-W4 color space (see Fig.9(d) and10(d)). This phenomenon can be understood from the context of the circumstellar envelopes for WR stars and those of AGB. WR stars experience higher mass loss rates and have larger radii than AGB stars, leading to more extended circumstellar envelopes. These envelopes emit strongly, causing noticeable excesses in various color bands. AGB stars also show similar excesses due to their dusty circumstellar envelopes. However, this effect is absent in the W3-W4 band because of their less extended outer shells.

SECTION: 4.2WR sub-type classification

From the previous section, we can conclude that the XGB model is a highly efficient classifier model. Therefore, we develop a multi-class XGB classifier model to identify the WR sub-types (WC and WN-types) from the non-WR objects. We follow the same feature selection approach using the RFCEV method (as discussed in3.4) to find the important features required by the model to achieve the best f1-score. As this has more than two classes, we use the macro f1-score which is the average of f1-scores for different classes. Thereafter, we determine the optimal parameters (using the Bayesian optimization technique) upon training and cross-validating the model on 5 randomized subsets of Dataset-2 (with known spectral types) and optimize the macro f1-score. The final classifier (Table4) is fitted on the oversampled training dataset (Trd-2) and tested on Tsd-2 to determine the model predictions.

The prediction of the model on the TsD-2 is shown in the form of a confusion matrix (see Fig.11). From Table5, we find that the WC and WN subtypes are detected with a recall of 71% and 60% respectively. A lower number of misclassifications between WC and WN with an overall high detection accuracy () is a strong indicator of the model’s ability to accurately distinguish between WR sub-types. We find more misclassification of non-WR sources (i.e. FPs) as WN-type stars compared to WC-type stars. Such non-WR sources are mostly the classical Be-type sources(Lee et al.,2011)which contaminate the color space of WN-type stars. The WC-type stars show color excess due to strong carbon emission lines and sometimes due to circumstellar dust in the-band(Mauerhan et al.,2011)making them distinguishable from the dominant classes (i.e., Be and AGB) of non-WR sources.

Note. — Complete catalogue of predicted WR stars is available in a machine readable format. Here we have shown only those WR candidates that have been identified by both the object and subtype models.

SECTION: 4.3New WR star identification

Finally, we apply our models on a large and different dataset from SIMBAD comprising stars with unknown object types. For this, we retrieved 18750 stellar objects (restricted to Galactic sources using the same positional criteria as mentioned in Sec.3.1) from SIMBAD with no information on their object types. The data is sorted in the same manner (i.e. objects with available photometric magnitudes and present in the same color space) as mentioned in Sec.3.1and apply the XGB object type classifier model on the reduced dataset (comprising 6457 objects). The model classifies 58 objects as WR stars (see Table6). Also, we apply the subtype classifier to the same dataset to identify the subtypes. Among the predicted list of objects, the model identifies the subtypes of 10 of the 58 WR star candidates detected by the object classifier (see Table6).
We also obtain the parallaxes for the WR star candidates by cross-matching them with Gaia DR3(Gaia Collaboration et al.,2016,2023)and find that except two (with missing parallax information), all of them are located within 2 kpc from the sun. This can be seen in Fig.12, where we overplot the WR candidates on a 2D X-Y plot fromReid et al. (2019)showing all the Galactic arms. We find that most of our detected sources lie on the Local arm with a few located on the Perseus arm. This is expected as the Local arm is known to be a high-mass star-forming region and WR stars are very likely to be found in these regions(Rosslowe & Crowther,2015). Given the spatial distributions of our WR candidates, we expect a majority of them to be actually WR stars. However, there is a chance of FPs in the detected list of WR candidates as the dataset may contain objects which differ from the training data (i.e. weaker sources or weak/high extinction). Therefore, a spectroscopic follow-up of these objects is suggested to confirm our results. The above dataset may also contain various other types of bright astronomical objects that may not have been considered (such as OB-type stars, Compact Hiiregions, etc.) during our model development which may affect the detection of WR stars. We plan to include them in future work.

SECTION: 5Discussion

We compare our classification results with those ofMauerhan et al. (2011). From Fig.9and Fig.10, we note that our models can identify WR stars across the broad IR color space with enhanced accuracy than what was achieved from selecting WR stars from so-called sweet-spot in the color-color diagrams. Further, our models can detect WR stars with much greater accuracy (see Table3) than detected (50%) byMauerhan et al. (2011)from the color-based manual identification criteria. Additionally, both of our models (XGB and RF) perform much better than the earlier KNN(Morello et al.,2018)and SVM(Dorn-Wallenstein et al.,2021)models in accurate label predictions of WR stars. This shows that the ensemble-based algorithms are superior to the instance-based algorithms in classifying stellar candidates from a dataset consisting of large, imbalanced, and heterogeneous objects.

We claim that both of our object classifier models (see Sec.4.1) can identify WR stars present across different metallicity regions(Rosslowe & Crowther,2015)of the MW. Overall, based on the f1-score (see Table3), we note that the XGB does a slightly better WR-identification task than the RF model. The high recall of the XGB classifier occurs due to the smaller number of FNs detected compared to the RF classifier model.
However, we find a better f1-score for XGB compared to RF. The XGB-classifier can identify the WR sources lying in the clustered regions of the IR color spaces (see Fig.9) better than the RF model (see Fig.10).

Both the classifiers predict that (from Table7) the major contaminants for WR sources in the NIR color space are the Be-type objects which was also reported inFaherty et al. (2014). While those in the mid-IR are the O-rich AGBs. In the list of FPs, one of the objects is an HMXB system of WR stars. Therefore, it can be rather considered to be a TP.

In Sec.4.2, we find that the subtype XGB-classifier model classifies both the WC and WN type objects from the non-WR sources with good accuracy rates. This means that the model can distinguish between the WR subtypes without the need for corresponding narrow-band IR colors or spectroscopic follow-ups. It can be seen (from Table8) that among the WC-type stars, most of the TPs detected by the classifier are of WCL-type. This shows that the model can identify the characteristic color-space of these objects that are mainly found in metal-rich galaxies(Crowther,2007)such as the MW. The contaminants for WC-class are mostly the WN stars(Faherty et al.,2014)that are mostly early-type while the reverse is not observed (see Table9in AppendixA). Further, it is noted (from Table8) that the TPs corresponding to the WN-type stars span across WNE to WNL-type. However, mostly the WNL-type objects dominate the list as they exhibit color-excess in the W3-W4 space due to free-free emission. The model can also detect the WNh-type objects that are classified as non-classical WR stars. However, we note that the WNL and WCL show similar color patterns leading to the observed misclassifications by the model (see Fig.11). This is further confirmed by the detected FPs corresponding to the WN-type objects (see Table9in AppendixA). This happens because of the excess emission due to free-free emission and sometimes due to circumstellar dust formed in earlier stages of evolution(Crowther,2003).

SECTION: 6Conclusion

In this study, we performed ML classification of WR stars from a large dataset (6555) of stellar objects of types ranging from MS to AGBs. For this, we developed a highly efficient and robust XGB classifier based on the IR colors and positional coordinates. The models achieve their highest classification efficiency using only 8 features: RA, Dec, J-H, H-,-W1, W1-W2, W2-W3, and W3-W4. These features are found to be the most significant in the identification of our objects. The models also perform significantly better than the basic color-color approach of detecting the WR stars. A similar comparison of the XGB classifier with an RF model shows that the former classifies the WR stars with better accuracy than the latter. The models predict that the major contaminants for WR stars are the hot Be-type stars followed by the O-rich AGB sources.

We also developed a novel WR-subtype classifier using the XGB algorithm capable of distinguishing between WN and WC subtypes with exceptional accuracy. The model performs a much better task in the classification of WC-type stars than the WN class as the color characteristics of the latter overlap with those of the Be-type stars that act as the dominant class of contaminants. The ML model can easily identify the WR-Late type stars, making it more suitable for detecting WR stars in the MW. The model predicts that WNE-type objects are significant contaminants of the WC-class, while WCL-type objects affect the WN-class. Also, the model is capable of identifying the non-classical WNh-type stars. We also applied the object classifier model on an unlabelled dataset (comprised of 6457 stellar sources) and detected 58 new WR candidates. Also, using the WR subtype classifier, we identified the chemical sub-types of 10 of the detected sources. The identified WR candidates are mainly located in the solar neighborhood and within the MW’s Local spiral arm that hosts massive star formation regions. In future work, we plan to do a spectroscopic follow-up for our WR candidates. Also, we plan to develop ML classifiers using the same algorithms to identify WR stars across metal-rich galaxies in the local group.

SECTION: 7Acknowledgements

The authors thank the reviewer for providing constructive comments and suggestions that significantly improved the work. The authors also thank Dr. Loránt O. Sjouwerman for his valuable suggestions and discussions that greatly enhanced the work. This research has made use of the CDS SIMBAD database, VizieR catalogue access tool and cross-match service, Strasbourg Astronomical Observatory, France. This publication makes use of data products from the Two Micron All Sky Survey and Wide-field Infrared Survey Explorer which is a joint project of the University of Massachusetts and the Infrared Processing and Analysis Center/California Institute of Technology, funded by the National Aeronautics and Space Administration and the National Science Foundation. S. Kar and R. Das thank the S. N. Bose National Centre for Basic Sciences under the Department of Science and Technology (DST), Govt. of India, for providing the necessary support to conduct research work. Y.P. and R.B. acknowledge support from the National Aeronautics and Space Administration (NASA) under grant number 80NSSC22K0482 issued through the NNH21ZDA001N Astrophysics Data Analysis Program (ADAP).

SECTION: Appendix AWR-subtype model misclassification

We present the classification shortcomings of the WR sub-type classifier model based on the XGB algorithm (see Sec.4.2) in Table9. Further, the obtained results are discussed in Sec.5.

SECTION: References