SECTION: Unsupervised Learning of Multi-modal Affine Registration for PET/CT

Affine registration plays a crucial role in PET/CT imaging, where aligning PET with CT images is challenging due to their respective functional and anatomical representations. Despite the significant promise shown by recent deep learning (DL)-based methods in various medical imaging applications, their application to multi-modal PET/CT affine registration remains relatively unexplored. This study investigates a DL-based approach for PET/CT affine registration. We introduce a novel method using Parzen windowing to approximate the correlation ratio, which acts as the image similarity measure for training DNNs in multi-modal registration. Additionally, we propose a multi-scale, instance-specific optimization scheme that iteratively refines the DNN-generated affine parameters across multiple image resolutions. Our method was evaluated against the widely used mutual information metric and a popular optimization-based technique from the ANTs package, using a large public FDG-PET/CT dataset with synthetic affine transformations. Our approach achieved a mean Dice Similarity Coefficient (DSC) of 0.870, outperforming the compared methods and demonstrating its effectiveness in multi-modal PET/CT image registration. The source code of this work is available athttps://bit.ly/3XTJrJh.

SECTION: IIntroduction

Affine registration plays a crucial role in multi-modal imaging systems, including positron emission tomography (PET) and computed tomography (CT). The precise alignment of functional and anatomical images is essential for both image reconstruction in PET and subsequent theranostic procedures. Although deep learning (DL)-based registration methods have shown notable success in various medical imaging modalities and applications[1], their adoption in PET/CT has been limited, primarily relying on traditional pair-wise optimization-based methods. In this study, we make two key contributions: first, we explore the effectiveness of DL-based approaches for PET/CT image registration; second, we investigate an alternative to the commonly used mutual information (MI)—the correlation ratio (CR)[2], which, while historically significant in multi-modal image registration, has been overlooked in recent times.

SECTION: IIMethods

Letandrepresent the moving and fixed images, respectively. We deploy a deep neural network (DNN) to take in this image pair and predict a set of affine parameters. Our focus is on the three-dimensional (3D) scenario, which involves 12 total affine parameters: three each for translation, rotation, scaling, and shearing. These parameters are then applied in a warping function to align the moving imagewith the fixed image. We adopt our previously developed network,TransMorph(TM)[3], which employs a Swin Transformer followed by multi-layer perceptrons to predict these parameters. The DNN is trained with the negative CR as the loss function (i.e.,), detailed in sectionII-B.

SECTION: II-AMulti-scale Instance-specific Optimization

After the DNN predicts the affine parameters, these can serve as an initialization for the optimal transformation parameters. We then employ instance-specific optimization (ISO), iteratively refining these parameters for the corresponding image pair. This refinement is achieved by treating the affine parameters as model parameters and updating them through gradient descent. Although the DNN is trained on a dataset of images, it might not fully capture the unique characteristics of individual cases—a phenomenon known as the amortization gap[4]. Therefore, ISO fine-tunes the affine parameters to the specific instance, enhancing the alignment accuracy. Given that there are only 12 parameters to adjust, this refinement process is exceptionally swift, especially on GPUs.

Here, we introduce a multi-scale ISO strategy that optimizes parameters across multiple image resolutions. The pseudocode for this process is detailed in Algorithm1. This approach involves five image scales. Throughout these scales, the affine parameters are progressively refined based on the image similarity measure (or the loss function) between the transformed moving image and the fixed image, leading to the optimized final transformation parameters.

SECTION: II-BDifferentiable Correlation Ratio via Parzen Windowing

The CR for image registration was initially introduced in[2]. It operates on the principle that the intensity values incan be predicted using values from image, whereandmay belong to the same or different modalities. The CR is calculated as the ratio of the variance of the conditional expectation ofgivento the variance of, and is mathematically expressed as.
In the original method,is implemented using a discrete method that involves iterating through discrete image intensity values, as detailed in[2]. Yet, such an implementation cannot be used for DNN training due to its non-differentiable nature. Instead, we employ a Parzen windowing method with Gaussian kernels to approximate the conditional expectation, similar to the approach used for MI in[5]. The conditional expectation given a particular intensity bin is approximated as:

wheredenotes the-th voxel location andrepresents the-th intensity bin. Here,is a Gaussian kernel, given by:

whereis the center of the-th intensity bin, andis the bandwidth of the Gaussian kernel. The variance of the conditional expectation is calculated as:

whereand, withrepresenting the total number of voxels. The differentiable CR is then defined as, whereis simply the variance of image. Given that the CR is naturally asymmetric, where, we address this by using a symmetric loss functionas the final form for our loss function. This loss is further extended to a local patch-based version, as similarly done in[5]. This modified loss,, is computed by averaging the negative CR across all local patches.

SECTION: II-CDataset and Implementation Details

Our evaluation of the proposed method was conducted using the AutoPET dataset[6], which comprises 896 FDG-PET/CT scans. We adopted a dataset division ratio of 7:1:2, resulting in 628, 90, and 178 images for training, validation, and testing, respectively. All images were resampled to the same voxel dimension of. The CT images were processed through an automated segmentation algorithm, TotalSegmentator, which provided 40 anatomical label maps for method evaluation using Dice similarity coefficient (DSC). For the registration task, random affine transformations were applied separately to the paired PET and CT images of the same subject, with the objective to align the PET image with the corresponding CT image. We compared our CR-based loss against the widely used MI, using the same DNN. The DNN was first trained using global versions of the loss functions, followed by local patch-based versions for ISO. As a baseline, a traditional optimization-based method employing MI from the ANTs package111https://github.com/ANTsX/ANTswas compared.

SECTION: IIIResults and Conclusion

Figure1presents both qualitative and quantitative comparisons among various methods. Results indicate that the conventional optimization-based approach using MI performs well, even surpassing DL methods that do not incorporate ISO. However, the introduction of multi-scale ISO significantly enhances alignment accuracy. As depicted in the box plot, the DNN trained using CR alongside ISO excels over all other methods in terms of performance across different organs, a finding corroborated by TableI.

In this study, we introduced an unsupervised DL-based affine registration technique for PET-CT imaging. This involves a novel implementation of differentiable CR combined with a multi-scale ISO framework. The proposed method has been validated on a public dataset and demonstrates its efficacy in multi-modal affine registration.

SECTION: References