SECTION: LuxEmbedder: A Cross-Lingual Approach to Enhanced Luxembourgish Sentence Embeddings

Sentence embedding models play a key role in various Natural Language Processing tasks, such as in Topic Modeling, Document Clustering and Recommendation Systems. However, these models rely heavily on parallel data, which can be scarce for many low-resource languages, including Luxembourgish. This scarcity results in suboptimal performance of monolingual and cross-lingual sentence embedding models for these languages. To address this issue, we compile a relatively small but high-quality human-generated cross-lingual parallel dataset to trainLuxEmbedder, an enhanced sentence embedding model for Luxembourgish with strong cross-lingual capabilities. Additionally, we present evidence suggesting that including low-resource languages in parallel training datasets can be more advantageous for other low-resource languages than relying solely on high-resource language pairs. Furthermore, recognizing the lack of sentence embedding benchmarks for low-resource languages, we create a paraphrase detection benchmark specifically for Luxembourgish, aiming to partially fill this gap and promote further research.111https://github.com/fredxlpy/LuxEmbedder

LuxEmbedder: A Cross-Lingual Approach to Enhanced Luxembourgish Sentence Embeddings

Fred Philippy1,2,
Siwen Guo1,
Jacques Klein2,
Tegawendé F. Bissyandé21Zortify S.A., Luxembourg2University of Luxembourg, Luxembourg{fred, siwen}@zortify.com{jacques.klein, tegawende.bissyande}@uni.lu

SECTION: 1Introduction

The development of sentence embedding models has been instrumental in applications such as Bitext Mining(Artetxe and Schwenk,2019), Information Retrieval(Thakur et al.,2021), and most recently Retrieval Augmented Generation(Lewis et al.,2020). Generative Large Language Models are not capable of handling these tasks as effectively, making sentence embedding models crucial in these areas. However, these models depend on large-scale parallel data to function effectively, a resource readily available for high-resource languages but sorely lacking for low-resource languages(Zhou et al.,2018).

One way to address this issue is to apply cross-lingual sentence embedding models(Chidambaram et al.,2019; Artetxe and Schwenk,2019; Reimers and Gurevych,2020; Yang et al.,2020; Feng et al.,2022; Wang et al.,2022), which aim to embed various languages into a common shared representation space. This approach is intended to boost the performance of low-resource languages by leveraging cross-lingual transfer, where knowledge gained from high-resource languages contributes to the understanding and processing of low-resource languages. However, due to the significant differences in data availability, these models still exhibit a large performance gap between high-resource and low-resource languages.

Luxembourgish, a West-Germanic language spoken by about 400 000 people, is one of the many languages that face this challenge. While translation models for Luxembourgish exist(NLLB Team et al.,2022; Song et al.,2023), their performance remains significantly inferior to that of high-resource languages, hindering the creation of parallel data using methods like back-translation. This limitation also applies to general-purpose generative LLMs, making the direct creation of synthetic parallel data impractical as well. Our research aims to address this issue by collecting a comprehensive set of high-quality human-generated cross-lingual parallel data specifically for Luxembourgish. With this data, we train a sentence embedding model,LuxEmbedder, tailored specifically for Luxembourgish by leveraging cross-lingual transfer.

Although cross-lingual sentence embedding models harness the strength of cross-lingual transfer to improve low-resource language performance, we argue that this does not eliminate the necessity for parallel data in these languages. Our findings demonstrate that incorporating these languages in parallel training datasets is essential, as it significantly improves alignment within cross-lingual models, particularly among other low-resource languages, in contrast to relying solely on high-resource language parallel data.

Another major challenge is the evaluation of sentence embedding models in low-resource languages, given that the primary benchmarks, such as MTEB(Muennighoff et al.,2023)and BEIR(Thakur et al.,2021), predominantly support English and a few other high-resource languages.
To address this, we establish a new paraphrase detection benchmark for Luxembourgish, facilitating future research and improving the language’s representation in NLP. To thoroughly evaluate our enhanced model,LuxEmbedder, we use our own benchmark along with three additional evaluation tasks. The results indicate thatLuxEmbedderoutperforms not only other open-source models but also proprietary models in the majority of cases.

SECTION: 2Dataset & Benchmark Construction

We create cross-lingual parallel data and a Luxembourgish paraphrase detection benchmark. See AppendixAfor details and Figure1for an overview.

SECTION: 2.1Cross-Lingual Parallel Data (LuxAlign)

We collect news articles from RTL.lu, a Luxembourgish news platform that publishes in Luxembourgish (LB), English (EN), and French (FR). Due to the lack of explicit mapping between language versions, we use the OpenAI text embedding modeltext-embedding-3-small222https://platform.openai.com/docs/guides/embeddings/embedding-modelsto align articles across language pairs. LaBSE(Feng et al.,2022)is then employed to extract parallel sentences from these aligned pairs for LB-FR and LB-EN.

SECTION: 2.2Luxembourgish Paraphrase Detection (ParaLux) Benchmark

Then, we repeat the same process but focusing exclusively on Luxembourgish articles. Within each article, using the same setup, we extract parallel sentences, which can be considered near-paraphrases, from which we hand-pick high-quality samples for our benchmark. From these paraphrased pairs, we promptGPT-4o333https://openai.com/index/hello-gpt-4o/to generate adversarial negative samples for each pair. Given its limited language capabilities in Luxembourgish, the generated adversarial negative samples are then checked and, if needed, corrected by a human annotator to ensure high quality and accuracy.

Through this methodology, we gather 25 996 LB-EN, 86 293 LB-FR samples forLuxAlign, and 312 samples forParaLux.

SECTION: 3LuxEmbedder

SECTION: 3.1Training

Given its cross-lingual capabilities and its already existing support of Luxembourgish, we use LaBSE(Feng et al.,2022)as our base model, which we further train on both LB-EN & LB-FR parallel subsets fromLuxAlign.

We train the model using a batch size of 16 for 3 epochs with a constant learning rate ofusing a contrastive loss function. We reserve 1% of the data for evaluation, on which we evaluated every 500 steps, and retained the model with the best loss on the development set. The negative pairs for the loss function are created by randomly pairing each Luxembourgish sentence with the translation of another sentence from the dataset.

SECTION: 3.2Evaluation

We comprehensively compareLuxEmbedder’s performance across multiple tasks against a variety of open-source and proprietary baseline models.

SECTION: 3.3Baselines

We provide more details on the used models in AppendixB.2.1.

Developed by Cohere,embed-multilingual-light-v3.0andembed-multilingual-v3.0are multilingual embedding models, designed to handle over 100 languages, including Luxembourgish, producing embeddings of size 384 and 1 024, respectively.

OpenAI’stext-embedding-3-smallandtext-embedding-3-largemodels generate embeddings with dimensions of 1 536 and 3 072, respectively. Despite the native API feature for embedding shortening, we use the full dimensions in our experiments. While these models have been assessed on the multilingual MIRACL benchmark(Zhang et al.,2023), there is no official information on the number of supported languages.

We also compareLuxEmbedderagainst two open-source multilingual sentence embedding models that support Luxembourgish. These models are LaBSE(Feng et al.,2022), which generates cross-lingual sentence embeddings for 109 languages, and LASER(Artetxe and Schwenk,2019; Heffernan et al.,2022), which incorporates a multilingual teacher sentence embedding model and language-specific student models for 200 languages.

We further extend our evaluation to include mBERT, a multilingual BERT(Devlin et al.,2019), variant pre-trained on 104 languages, and LuxemBERT(Lothritz et al.,2022), a monolingual Luxembourgish BERT model. In our experiments, we leverage both CLS embeddings and MEAN-pooled embeddings from these models.

SECTION: 3.4Evaluation Tasks

Additional details on the specific evaluation setup can be found in AppendixB.2.2.

Using SIB-200(Adelani et al.,2024), a 7-class classification dataset, we perform similarity-based zero-shot classification. First, we fill each label into a pre-defined template sentence, and separately encode both the input document and all potential template-embedded labels. Then, the class with the most similar embedding to the input document is chosen, assessing the model’s ability to generalize to new, unseen tasks without any task-specific training. To account for variability, we repeat this process for 5 different label templates and report the average performance.

For cross-lingual transfer performance, we use the embeddings generated by the respective model to fine-tune a classifier on the SIB-200 dataset in six different high-resource source languages and evaluate directly on the Luxembourgish test set.

We evaluate the model’s proficiency in accurately retrieving or matching parallel sentence pairs from a bilingual corpus using the Tatoeba dataset. Since the original Tatoeba test set(Artetxe and Schwenk,2019)does not include Luxembourgish, we use the LB-EN, LB-NL, and LB-DE test sets developed by theTatoeba Translation Challenge(Tiedemann,2020).

Lastly, we evaluate the model on our newly created benchmark for paraphrase detection. This task involves determining which of two sentences is a paraphrase of a given anchor sentence. It tests the model’s ability to discern nuanced semantic equivalence, which is critical for applications like plagiarism detection, question answering, and information retrieval.

SECTION: 3.5Results

LuxEmbedderdemonstrates superior performance among open-source models in all four tasks and even outperforms all tested proprietary models in 3 out of 4 tasks (Table1). Onlytext-embedding-3-largemodel shows superior cross-lingual transfer performance.

In particular, we observe considerable improvements inLuxEmbedder’s performance on both monolingual tasks, Zero-Shot Classification and Paraphrase Detection, relative to its base model, LaBSE. This confirms the efficacy of our cross-lingual approach for Luxembourgish.

SECTION: 4Cross-Lingual Alignment

In this section, we investigate the impact of fine-tuning models on parallel data for cross-lingual alignment between and within high-resource (HR) and low-resource (LR) languages.

To measure the cross-lingual alignment, we use Flores-200(NLLB Team et al.,2022), which includes parallel sentences across 200 languages, making it an ideal resource for assessing cross-lingual alignment. We use the Centered Kernel Alignment (CKA) method(Kornblith et al.,2019)to calculate the level of alignment by comparing the embeddings of parallel sentences from different languages.

We fine-tune LaBSE on three different language pairs: LB-EN, LB-FR, and EN-FR444Created using the same process as described in §2.1., each time using 20 000 parallel sentences from our newly compiled datasets. After fine-tuning, we assess cross-lingual alignment by comparing alignmentwithinHR languages and LR languages, as well asbetweenLR and HR languages555As HR and LR languages we select the 10 languages with the most and least training data in LaBSE which are also covered by Flores-200..

Our observations (Figure2) reveal that when fine-tuning on parallel data, the alignment within the model generally increases. HR languages benefit equally from fine-tuning on any of the three language pairs. However, we observe that the alignment of LR languages benefits more when Luxembourgish is part of the training data compared to fine-tuning on HR language pairs alone.

These results indicate the critical importance of including LR languages, such as Luxembourgish, when collecting parallel data. Incorporating LR in the training process enhances cross-lingual alignment, not only for the respective language pair but also for other LR languages, more effectively than focusing solely on HR languages.

SECTION: 5Conclusion

Sentence embedding models struggle with low-resource languages due to a shortage of parallel data. To address this problem, we collected high-quality, human-generated cross-lingual parallel data for Luxembourgish and developed an enhanced version of a cross-lingual sentence embedding model specifically adapted to Luxembourgish. This model outperforms open-source as well as proprietary models in almost all evaluations conducted in our study. Our findings also stress the importance of incorporating low-resource languages in parallel data collection, as evidence suggests that this enhances embedding alignment for both the target language and other low-resource languages within the same model more effectively than using high-resource language pairs alone. Therefore, we believe this research encourages further creation of parallel corpora for low-resource languages.

SECTION: Limitations

It is important to note that we do not compare our embedding model against general-purpose generative LLMs. We acknowledge that some of these models, which are significantly larger in terms of parameter count, may outperformLuxEmbedderin certain tasks. Nonetheless, the primary objective of our paper is not to compete with generative models . Instead, our focus is on providing a robust sentence embedding model capable of solving specific tasks such as information retrieval, document clustering, and similar applications where generative language models may not be as effective.

Additionally, we acknowledge that our data is limited to the news domain, due to its availability. However, our goal is to use this data to boost the model’s retrieval performance, facilitating future expansion into various other domains by mining a more diverse range of parallel data.

SECTION: Ethical Statement

In the newly createdParaLuxbenchmark, the adversarial counterparts of the paraphrases have been edited in a way that some of the edited sentences may contain non-factual information. Therefore, we strongly recommend using this data solely, as designed, for evaluation purposes and not for training, to ensure the integrity of model development.

Furthermore, our datasets, based on news articles, naturally include the names of individuals. As the text is publicly available and anonymization would greatly diminish data quality, we chose not to anonymize it. We believe that preserving the original context of publicly accessible information is essential for maintaining data integrity and the effectiveness of our research.

SECTION: Acknowledgments

We are grateful to RTL Luxembourg for providing the raw data necessary for our research. Their support significantly facilitated our efforts.

SECTION: References

SECTION: Appendix AData Collection & Processing

Here, we outline the method used to create cross-lingual training data and the paraphrase detection benchmark, providing examples in Tables2and3.

We gather news articles from the Luxembourgish news platform RTL666https://www.rtl.luwritten in Luxembourgish, French, and English, covering different time periods: from January 1, 1999 for Luxembourgish, from September 1, 2011 for French, and from January 1, 2018 for English, up until May 10, 2024. We first remove all URL tags and extraneous metadata, and filter out articles with fewer than 100 characters, as these are often just traffic or sports updates, which were not relevant for our study. To ensure linguistic accuracy, we use the OpenLID(Burchell et al.,2023)to identify and exclude articles that are not in the intended language.

Subsequently, we embed each article using the OpenAItext-embedding-3-smallmodel to facilitate cross-language article matching. To identify potential parallel articles in different languages, we first narrow down the candidates by considering only those articles published within a one-day window of the target article. Among these candidates, we select the one with the highest cosine similarity to the target article’s embedding, provided the similarity score exceeds 0.65.

In parallel, we extract sentences from each article using the NLTK777https://www.nltk.orglibrary. For Luxembourgish, in the absence of a dedicated sentence tokenizer, we use the German tokenizer. After splitting the articles into sentences, we employ OpenLID once again to remove any sentences identified as being in the wrong language. Additionally, we filter out sentences with fewer than 10 characters or fewer than three words.

Next, we embed each sentence using LaBSE, focusing on sentences from articles already matched with articles in another language. For each sentence, we restrict the candidates to sentences from the corresponding matched article, minimizing the risk of false positives. We then select the candidate sentence with the highest cosine similarity, provided it exceeds a similarity threshold of 0.7. After identifying all sentence pairs, we filter out pairs where the length difference is greater than 50%. To create a seed dataset forParaLux, we replicate this process within Luxembourgish articles alone.

SECTION: Appendix BTraining and Evaluation Details forLuxEmbedder

All our training processes and experiments were run on 4 A100 GPUs within a few hours.

SECTION: B.1Training

Given a sentence embedding modelwith parameters, for a sentence pairand its label(1 if positive pair, 0 if negative pair), the contrastive loss function is defined as:

where

is the margin value, defining the minimum distance that samples withing a negative pair should have

withandbeing the cosine distance in our experiments.

SECTION: B.2Evaluation

Due to the proprietary nature of Cohere’s models,embed-multilingual-light-v3.0andembed-multilingual-v3.0, as well as OpenAI’stext-embedding-3-smallandtext-embedding-3-large, detailed information about their training data and model architecture is not publicly available. We refer readers to their online documentation888https://cohere.com/blog/introducing-embed-v3999https://openai.com/index/new-embedding-models-and-api-updates/for any details.

Our experiments with open-source models involve base multilingual BERT (cased)(Devlin et al.,2019)and LuxemBERT(Lothritz et al.,2022). These models feature identical architectures, including 12 attention heads and 12 transformer blocks, each with a hidden size of 768. mBERT’s vocabulary size is 30 000, whereas LuxemBERT’s is 119 547. Both models have about 110 million parameters.

Additionally, we incorporate LaBSE(Feng et al.,2022), which also serves as the foundational model forLuxEmbedder. LaBSE is derived from the base multilingual BERT (cased) but features an expanded vocabulary of 501 153 tokens. It has been trained using a combination of monolingual data and bilingual translation pairs.

To assess cross-lingual transfer performance, we use embeddings from the respective model to fine-tune a classifier on the SIB-200(Adelani et al.,2024)dataset in several high-resource source languages, then evaluate it directly on the Luxembourgish test set.

The SIB-200 dataset includes over 200 languages, with 701 training, 99 development and 204 test samples per language.

In our experiments, however, we only train separately on French, English, German, Japanese, Chinese, and Russian. Additionally, we fine-tune on Luxembourgish, but this is not included in the average performance reported in Table1. The classifier is a simple linear layer with 7 output nodes, trained with the Adam optimizer and the cross-entropy loss function. Training is performed for 500 epochs with a constant learning rate of. We evaluate the classifier once per epoch and select the model with the best development loss. Each training process is repeated 4 times using different seeds to ensure robustness, and we report the average performance per source language in Table4.

To assess the zero-shot classification capabilities of different model, we again use the SIB-200 dataset(Adelani et al.,2024). We independently encode the input and all potential labels, integrating each label within a prompt template. The class whose embedding has the highest cosine similarity to the input document is selected.

We use five different prompt templates to evaluate the classification performance and report the average performance per template in Table5. These templates are:

[LABEL]

An dësem Beispill geet et em [LABEL].This example is about [LABEL].

D’Thema vun dësem Text ass [LABEL].The topic of this text is [LABEL].

Hei gëtt iwwer [LABEL] geschwat.Here we are talking about [LABEL].

Dëst Dokument beschäftegt sech mat [LABEL].This document deals with [LABEL].

The labels in Luxembourgish we use in this classification task areTechnologie(technology),Reesen(travel),Politik(politics),Gesondheet(health),Ennerhalung(entertainment),Geographie(geography) andSport(sports).

We initially considered the Tatoeba dataset, but it lacks Luxembourgish in the original set. Instead, we used Luxembourgish-English, Luxembourgish-Dutch, and Luxembourgish-German test sets from theTatoeba Translation Challenge(Tiedemann,2020), which include 346 LB-EN, 291 LB-EN, and 292 LB-DE sample pairs.101010https://huggingface.co/datasets/Helsinki-NLP/tatoeba_mtWe conducted experiments in both retrieval directions and reported the full results in Table6.

To assess performance onParaLux, the model encoded the anchor sentence and both paraphrase candidates. The candidate with the greatest cosine similarity to the anchor was chosen as the predicted paraphrase.

SECTION: Appendix CFull Results

Here, we report the full experimental results from the evaluations on Cross-Lingual Transfer (Table4), Zero-Shot Classification (Table5) and Bitext Mining (Table6) conducted in Section3.5.

SECTION: Appendix DDetails on the Cross-Lingual Alignment Experiments

In Section4, we measure the alignment of language-specific subspaces using the Centered Kernel Alignment (CKA) methodKornblith et al. (2019). The CKA score of two representation matricesand, whereis the number of samples andis the embedding dimension of the model, when using a linear kernel, is given by

whereis the Frobenius norm.

Since parallel cross-lingual data is essential for computing the CKA across various languages, we use the Flores-200 dataset(NLLB Team et al.,2022), which includes human-curated translations between English and 204 other languages. Specifically, we use the devtest split, containing 1 012 aligned sentences per language.

We choose the 10 languages with the highest and lowest amounts of training data in LaBSE, which are also included in Flores-200, to represent the HR and LR languages. As LR languages, we usebod,snd,tuk,ydd,wol,asm,smo,xho,nya, andsot. As HR languages, we useeng,rus,jpn,zho,fra,deu,por,nld,spa, andpol.

The exact CKA values across all language pairs are provided in Figure3.