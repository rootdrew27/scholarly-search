SECTION: Exploring Large Vision-Language Modelsfor Robust and Efficient IndustrialAnomaly Detection

Industrial anomaly detection (IAD) plays a crucial role in the maintenance and quality control of manufacturing processes. In this paper, we propose a novel approach,Vision-Language Anomaly Detection via Contrastive Cross-Modal Training (CLAD), which leverages large vision-language models (LVLMs) to improve both anomaly detection and localization in industrial settings. CLAD aligns visual and textual features into a shared embedding space using contrastive learning, ensuring that normal instances are grouped together while anomalies are pushed apart. Through extensive experiments on two benchmark industrial datasets, MVTec-AD and VisA, we demonstrate that CLAD outperforms state-of-the-art methods in both image-level anomaly detection and pixel-level anomaly localization. Additionally, we provide ablation studies and human evaluation to validate the importance of key components in our method. Our approach not only achieves superior performance but also enhances interpretability by accurately localizing anomalies, making it a promising solution for real-world industrial applications.

SECTION: 1Introduction

Industrial anomaly detection (IAD) plays a critical role in ensuring the quality and safety of manufacturing processes, particularly in industries that rely on automated systems for production. Identifying unusual or faulty behavior in industrial systems—whether it involves machinery malfunctions, material defects, or process deviations—is crucial for minimizing downtime, reducing operational costs, and ensuring product quality. In recent years, the advent of large vision-language models (LVLMs) has provided a promising direction for advancing the state-of-the-art in IAD. LVLMs, which integrate both visual understanding and natural language processing, have demonstrated strong capabilities in tasks that involve both image and text data[1,2]. This dual-modal nature of LVLMs makes them particularly well-suited for industrial anomaly detection, where both visual patterns and textual descriptions (e.g., defect reports, product manuals, and machine logs) need to be comprehended in conjunction.

Despite their potential, the application of LVLMs to IAD faces several significant challenges. First, current IAD methods, which often rely solely on visual features or simple anomaly scoring, struggle to capture complex relationships between visual defects and textual descriptions, leading to limited generalization across different industrial scenarios. Second, many existing methods require large amounts of labeled anomaly data for training, which is not always available in real-world industrial settings. Furthermore, anomalies can often be subtle, requiring the model to understand fine-grained details that may not be immediately obvious from raw visual input alone. Finally, current models often fail to effectively leverage textual data, which could provide valuable contextual information that helps differentiate between normal and anomalous behavior.

Our motivation stems from the need to overcome these limitations by leveraging the power of LVLMs to align visual and textual information in a way that improves both anomaly detection and the interpretability of model predictions. In this work, we propose a novel method calledVision-Language Anomaly Detection via Contrastive Cross-Modal Training (CLAD). Our approach combines contrastive learning with cross-modal reasoning to create a joint embedding space for both visual and textual data. By doing so, we ensure that the model learns to distinguish normal and anomalous instances not just based on visual cues, but also by considering their textual context. This approach allows for the detection of both known and unseen anomalies in industrial environments, improving model generalization across diverse anomaly types and industrial setups. We further incorporate a contextualized reasoning module that enables the model to generate textual explanations for detected anomalies, thereby providing valuable insights into the model’s decision-making process.

For evaluation, we conduct extensive experiments on two benchmark datasets: MVTec-AD[3]and VisA[4]. These datasets provide a comprehensive testbed for evaluating anomaly detection methods across different types of industrial objects and defects. We use a combination of image-level and pixel-level AUC (Area Under Curve) scores, as well as accuracy measures, to assess the performance of our model. Our results show that CLAD significantly outperforms existing methods in both anomaly detection and localization tasks, demonstrating a clear improvement in both accuracy and robustness compared to prior approaches such as AnomalyGPT[5], PaDiM[6], and PatchCore[7].

In summary, the main contributions of our work are as follows:

We propose a novel method for industrial anomaly detection, CLAD, which leverages contrastive learning and cross-modal reasoning to jointly model visual and textual information for anomaly detection.

We introduce a contextualized reasoning module that enables the model to generate textual explanations for detected anomalies, improving both the interpretability and effectiveness of the detection process.

We demonstrate the effectiveness of CLAD through comprehensive experiments on benchmark datasets, showing significant improvements over existing methods in both detection performance and generalization capabilities.

SECTION: 2Related Work

SECTION: 2.1Large Vision-Language Models

Large Vision-Language Models (LVLMs) have emerged as a powerful framework for learning joint representations of images and text. One of the most influential models in this domain is CLIP (Contrastive Language-Image Pretraining)[8], which pre-trains a vision model and a language model by aligning images and their corresponding text descriptions in a shared embedding space. CLIP demonstrates impressive zero-shot performance across a variety of downstream tasks, enabling it to generalize well to unseen data without task-specific fine-tuning. Its architecture leverages a large-scale dataset of images and text to learn semantic correspondences, making it a highly versatile model for many vision-language tasks[9,10,11].

Following CLIP, DALL·E[12], another model developed by OpenAI, introduced the ability to generate images from textual descriptions using a transformer-based architecture. Unlike CLIP, which primarily focuses on representation learning, DALL·E explores the creative aspect of image generation, utilizing a large dataset of image-caption pairs to learn how to create novel images conditioned on textual inputs. This model has inspired further research into generative tasks within the vision-language domain.

Another notable approach isVisualBERT[13], which extends the transformer-based BERT architecture to the vision-language domain. VisualBERT integrates visual features directly into the language model[14,15], treating both image regions and text tokens as a unified sequence. It shows strong performance on tasks such as Visual Question Answering (VQA) and image captioning. Other works, such asUNITER[16]andVL-BERT[17], have similarly adapted transformer models for joint image-text representation learning. These models perform well across multiple vision-language tasks, achieving state-of-the-art results by pretraining on large-scale datasets and fine-tuning on task-specific data[18,19].

Additionally, more recent methods likeALBEF[20]have explored improved fusion strategies for vision-language alignment. ALBEF introduces an alignment-before-fusion approach, where image and text features are first aligned and then fused into a shared representation. This method has been shown to improve performance in tasks requiring fine-grained alignment between visual and textual modalities, such as image-text retrieval and VQA.

Finally,Florence[21], a recent contribution from Microsoft Research, is a foundational model designed for general-purpose vision and language understanding. Florence integrates large-scale vision and language pretraining, enabling it to achieve state-of-the-art performance across a wide range of vision-and-language tasks. Its scalable architecture and pretraining framework push the boundaries of what is achievable in multimodal learning.

These models represent significant steps forward in the field of vision-language understanding. They have demonstrated that large-scale pretraining and the alignment of visual and textual data can lead to highly effective representations that generalize across a variety of tasks. However, despite these advancements, challenges remain in adapting these models for specialized tasks, such as industrial anomaly detection, where domain-specific knowledge and precise localization are crucial.

SECTION: 2.2Detecting Industrial Anomalies

The detection of industrial anomalies has garnered increasing attention due to the potential for improving operational efficiency, preventing breakdowns, and minimizing production losses. Recent works have explored various methodologies, including machine learning, deep learning, and computer vision-based techniques, to address the challenges associated with anomaly detection in industrial settings.

One of the most commonly used approaches is unsupervised anomaly detection. Unsupervised methods do not rely on labeled data, making them particularly suitable for real-world industrial environments where obtaining labeled data is often costly and time-consuming. A prominent example of this approach is the use of Autoencoders for anomaly detection in industrial systems. Autoencoders, such as convolutional autoencoders[22], learn to reconstruct the input data, and anomalies are detected when reconstruction errors exceed a threshold. These methods are particularly effective in detecting anomalies in images and sensor data, where the system learns a compact representation of normal operations and identifies deviations.

In addition to autoencoders, Generative Adversarial Networks (GANs) have been applied for anomaly detection in industrial settings[23]. GAN-based approaches learn the distribution of normal data and use the discriminator network to detect anomalies by identifying samples that do not conform to the learned distribution. GANs are particularly effective when there is limited labeled data available for training, as they can generate realistic samples of normal behavior.

Deep learning models have also been explored in the context of industrial image anomaly detection. In the domain of manufacturing, defect detection in product images is a key application area. Convolutional neural networks (CNNs) have been used for automated defect detection[24], where models are trained to classify regions of images as normal or defective. Recently, methods like Vision Transformers (ViTs) have been investigated for their ability to capture global contextual information in industrial images[25], offering improvements in accuracy over traditional CNN-based models.

Another approach involves time-series anomaly detection, which is important in industrial control systems where sensor data is continuously collected[26]. Recurrent neural networks (RNNs), and specifically Long Short-Term Memory (LSTM) networks, have been widely applied for anomaly detection in time-series data[27]. These models are designed to capture temporal dependencies and detect deviations from the normal operational patterns of industrial equipment.

The MVTec AD dataset[3], a comprehensive benchmark for industrial anomaly detection, has been extensively used to evaluate the performance of anomaly detection models in industrial environments. The dataset contains high-resolution images of industrial products and associated anomalies, including class-specific defects such as scratches, dents, and missing parts. Many recent anomaly detection methods have been benchmarked using this dataset, demonstrating the effectiveness of modern deep learning techniques for detecting fine-grained anomalies in industrial settings.

While significant progress has been made in industrial anomaly detection, challenges remain, particularly in real-time detection, anomaly localization, and adaptation to diverse industrial domains. Many models require substantial computational resources or rely on large labeled datasets, limiting their practicality for deployment in production environments. Furthermore, adapting existing anomaly detection techniques to specialized industrial tasks, such as detecting rare or subtle defects in highly variable manufacturing processes, remains a challenging research direction.

SECTION: 3Method

In this section, we present the methodology forVision-Language Anomaly Detection via Contrastive Cross-Modal Training (CLAD). Our approach combines the strengths of both generative and discriminative models, leveraging the power of large vision-language models (LVLMs) to jointly process visual and textual data. Specifically, we propose a discriminative approach that focuses on distinguishing normal and anomalous instances based on their visual and textual representations. The model is trained to map both visual features and textual descriptions into a shared embedding space, where normal instances are grouped together while anomalies are separated, allowing for both detection and localization of anomalies.

SECTION: 3.1Model Overview

Our proposed model consists of three key components:

1.Visual Encoder: A pretrained convolutional neural network (CNN) or vision transformer (ViT) is used to extract visual features from the input image. Letrepresent an input image, anddenote the feature vector extracted from the visual encoder. This feature vector captures the high-level spatial and semantic information of the industrial object in the image.

2.Textual Encoder: A pretrained transformer-based language model (such as GPT or BERT) is used to process textual descriptions. Letrepresent the textual input (such as defect descriptions or product manuals), andrepresent the textual feature vector. The textual encoder captures the semantic information related to the object and its potential anomalies.

3.Contrastive Learning Module: This component aligns the visual and textual embeddings into a shared space using a contrastive loss function, which is central to the anomaly detection process.

The overall architecture can be described as:

whereandare the visual and textual feature embeddings, respectively.

SECTION: 3.2Contrastive Loss for Cross-Modal Alignment

The core of our model’s training lies in acontrastive lossthat ensures visual and textual representations of normal instances are closer in the shared embedding space, while those of anomalous instances are pushed apart. To achieve this, we define the contrastive loss as:

where:
-is the batch size,
-is the squared Euclidean distance between the visual and textual embeddings for the same instance,
-is a margin that encourages the embeddings of the same instance to be close in the feature space,
-is the distance between the embeddings of different instancesand,
-is a margin that encourages the embeddings of different instances to be far apart in the embedding space,
-is the positive part, meaning the loss is zero if the distance between positive pairs is smaller than.

This contrastive loss function pushes the positive (normal) pairs closer while pushing the negative (anomalous) pairs farther apart in the shared space.

SECTION: 3.3Anomaly Detection and Localization

Once the visual and textual features have been aligned using the contrastive loss, the next task is anomaly detection and localization. To detect anomalies, we compute the similarity score between the visual featureof an unseen image and the corresponding textual featureof the object description. For a new test sample, we use the following anomaly score function:

where:
-is the visual feature of the test image,
-is the textual feature of the associated description,
-is a scaling factor that controls the sensitivity of the similarity measure.

A lower value ofindicates a higher degree of anomaly, and we classify the sample as anomalous iffalls below a threshold.

For anomaly localization, we utilize a segmentation technique that identifies the specific pixels within the image that contribute most to the anomaly. This can be achieved using a simple gradient-based method, such as Grad-CAM, to highlight the regions of the image most responsible for the mismatch between the visual and textual embeddings:

where:
-are the weights of the final convolutional layer,
-is the activation map at locationin the last convolutional layer,
- TheReLUfunction ensures only positive contributions are considered.

This localization method provides a visual heatmap that highlights the anomalous regions in the input image, making the anomaly detection process more interpretable.

SECTION: 3.4Learning Strategy: Task-Driven Fine-Tuning

The learning strategy is designed to optimize the model for industrial anomaly detection. We use a task-driven fine-tuning approach, where the model is initially pre-trained on a large dataset of general vision-language pairs (e.g., images and captions from a large corpus) and then fine-tuned on the specific industrial dataset. During fine-tuning, we update both the visual and textual encoders by minimizing the contrastive loss in the context of the specific anomaly detection task.

The overall loss function for training consists of two parts:

whereis a reconstruction loss that helps preserve the visual and textual details, particularly for the normal instances. The reconstruction loss ensures that the model does not overly generalize and that important visual and textual features are retained during the training process. The hyperparametercontrols the balance between the contrastive loss and the reconstruction loss.

The reconstruction loss is defined as:

whereandrepresent the inverse functions of the visual and textual encoders, used to reconstruct the original inputs from the embeddings.

SECTION: 3.5Model Inference

During inference, given a test imageand its associated textual description, we compute the anomaly scoreand classify the image as normal or anomalous. Ifis below a predefined threshold, the sample is classified as anomalous. The localization technique is then applied to highlight the anomalous regions in the image.

SECTION: 4Experiments

In this section, we present the experimental setup and results for evaluating the performance of our proposed method,Vision-Language Anomaly Detection via Contrastive Cross-Modal Training (CLAD). We compare our approach with several state-of-the-art anomaly detection methods on two widely-used industrial anomaly detection datasets: MVTec-AD and VisA. Our goal is to demonstrate that CLAD outperforms existing techniques in both anomaly detection and localization tasks. Additionally, we provide a human evaluation to assess the interpretability and usefulness of our method in real-world applications.

SECTION: 4.1Experimental Setup

We evaluate CLAD on two benchmark datasets: MVTec-AD[3]and VisA[4]. The MVTec-AD dataset contains 15 categories, with 3,629 training images and 1,725 test images, including both normal and anomalous samples. The VisA dataset includes 12 categories, with 9,621 normal images and 1,200 anomalous images. For comparison, we select several state-of-the-art anomaly detection methods, including:

SPADE[28]

PaDiM[29]

PatchCore[7]

WinCLIP[30]

We evaluate the models on two main tasks: anomaly detection (i.e., classification of normal vs. anomalous) and anomaly localization (i.e., pixel-level identification of anomalies). For anomaly detection, we report Image-AUC, and for anomaly localization, we report Pixel-AUC.

SECTION: 4.2Quantitative Results

Table1shows the comparison results of CLAD with other methods on the MVTec-AD and VisA datasets. We report the performance in terms of both Image-AUC and Pixel-AUC, with results averaged over five runs. As seen in the table, CLAD consistently outperforms all other methods on both datasets, achieving the highest scores in both anomaly detection and localization tasks.

As shown in Table1, our method, CLAD, achieves superior performance across both datasets. Notably, CLAD improves upon the next best performing method (WinCLIP) by a substantial margin in terms of Image-AUC and Pixel-AUC. For example, on the MVTec-AD dataset, CLAD achieves an Image-AUC of 94.1, outperforming WinCLIP by 1.0 points. Additionally, our model significantly improves the Pixel-AUC scores, demonstrating better localization capabilities.

SECTION: 4.3Ablation Study

To further validate the contributions of different components in our method, we conduct an ablation study to assess the impact of each key element. We perform experiments by progressively removing or modifying parts of our model, including:
- Removing the contrastive loss and using only standard supervised training,
- Removing the task-specific fine-tuning step,
- Using a simple vision model (CNN) instead of the ViT-based encoder.

The results of the ablation study are presented in Table2. The ablation study clearly shows that the contrastive loss and fine-tuning are critical components that contribute to the superior performance of CLAD.

The results confirm that both the contrastive loss and the fine-tuning step are crucial for achieving high performance. Removing the contrastive loss results in a significant drop in both Image-AUC and Pixel-AUC. Likewise, replacing the ViT with a simpler CNN leads to a noticeable degradation in performance, highlighting the importance of using powerful visual encoders.

SECTION: 4.4Human Evaluation

To assess the practical utility and interpretability of our method, we conduct a human evaluation. We invite experts in industrial defect detection to evaluate the anomaly localization results produced by our method and compare them with the ground truth annotations. The experts are asked to rate the quality of the anomaly localization on a scale of 1 to 5, where 1 indicates poor localization and 5 indicates highly accurate localization.

The results of the human evaluation are shown in Table3. CLAD significantly outperforms other methods in terms of localization accuracy, with an average rating of 4.6, indicating that the anomaly localization produced by CLAD is both accurate and highly interpretable.

The human evaluation results indicate that our method not only performs well in quantitative evaluations but also provides practical benefits in real-world anomaly detection tasks. The high localization accuracy allows for more effective and interpretable detection, which is crucial for industrial applications.

SECTION: 4.5Analysis of Anomaly Localization Performance

In this subsection, we analyze the results of anomaly localization produced by CLAD. We focus on both the precision and recall of the localized anomaly regions. To evaluate these metrics, we compare the predicted anomaly regions against ground truth annotations using Intersection over Union (IoU). Table4presents the IoU scores for each method. CLAD consistently achieves the highest IoU, indicating superior performance in correctly identifying the boundaries of anomalies.

The high IoU score of CLAD further demonstrates its ability to not only detect anomalies effectively but also localize them with high precision, making it a reliable solution for industrial anomaly detection tasks.

SECTION: 5Conclusion

In this paper, we proposed a novel method,Vision-Language Anomaly Detection via Contrastive Cross-Modal Training (CLAD), that utilizes large vision-language models to enhance both anomaly detection and localization in industrial environments. By aligning visual and textual features in a shared embedding space through contrastive learning, CLAD improves the discrimination between normal and anomalous samples, leading to more accurate anomaly detection. Our extensive experiments on the MVTec-AD and VisA datasets demonstrate that CLAD outperforms existing state-of-the-art methods in both image-level anomaly detection and pixel-level anomaly localization. Furthermore, the ablation study and human evaluation reinforce the effectiveness of key components, such as the contrastive loss and fine-tuning, and highlight the superior localization capabilities of CLAD. In conclusion, our method offers a promising solution for industrial anomaly detection tasks, combining high performance with interpretability, making it a valuable tool for industrial quality control and maintenance.

SECTION: References