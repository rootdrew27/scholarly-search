SECTION: Tackling the Abstraction and Reasoning Corpus with Vision Transformers: the Importance of 2D Representation, Positions, and Objects
The Abstraction and Reasoning Corpus (ARC) is a popular benchmark focused onin the evaluation of Artificial Intelligence systems. In its original framing, an ARC task requires solving a program synthesis problem over small 2D images using a few input-output training pairs. In this work, we adopt the recently popularapproach to the ARC and ask whether a Vision Transformer (ViT) can learn the implicit mapping, from input image to output image, that underlies the task. We show that a ViT—otherwise a state-of-the-art model for images—fails dramatically on most ARC tasks even when trained on one million examples per task. This points to an inherent representational deficiency of the ViT architecture that makes it incapable of uncovering the simple structured mappings underlying the ARC tasks. Building on these insights, we propose, a ViT-style architecture that unlocks some of the visual reasoning capabilities required by the ARC. Specifically, we use a pixel-level input representation, design a spatially-aware tokenization scheme, and introduce a novel object-based positional encoding that leverages automatic segmentation, among other enhancements.
Our task-specificmodels achieve a test solve rate close to 100% on more than half of thepublic ARC tasks strictly through supervised learning from input-output grids.
This calls attention to the importance of imbuing the powerful (Vision) Transformer with the correct inductive biases for abstract visual reasoning that are critical even when the training data is plentiful and the mapping is noise-free. Hence,provides a strong foundation for future research in visual reasoning using transformer-based architectures.

SECTION: Introduction
Developing systems that are capable of performing abstract reasoning has been a long-standing challenge in Artificial Intelligence (AI). Abstract Visual Reasoning (AVR) tasks require AI models to discern patterns and underlying rules within visual content, offering a rigorous test for evaluating AI systems. Unlike other visual reasoning benchmarks such as Visual Question Answering (VQA)and Visual Commonsense Reasoning (VCR)that rely on natural language inputs or knowledge of real-world physical properties, AVR tasks do not include any text or background knowledge. Instead, they focus purely on visual abstraction and pattern recognition.
One prominent example of AVR is the Abstraction and Reasoning Corpus (ARC), which is designed to evaluate an AI’s capacity for generalization in abstract reasoning. Each ARC task involves transforming input grids into output grids by identifying a hidden
mapping often requiring significant reasoning beyond mere pattern matching (cf.). While the ARC’s original setting is one of few-shot learning, there has been recent interest in studying the ARC in a data-rich setting where task-specific input-output samples can be generated, allowing for the evaluation of deep learning-based solutions.

In this paper, we explore the potential of vision transformers to solve ARC tasks using supervised learning.
We assess how well transformers can learn complex mappings for a single task when provided with sufficient training data. Our exploration highlights fundamental representational limitations of vision transformers on the ARC, leading to three high-level findings that we believe provide a strong foundation for future research in visual reasoning using transformer-based architectures:

Despite the ARC grids’ relatively simple structure compared to the much larger, noisier natural images they are typically evaluated on, a vanilla ViT performs extremely poorly onof the tasks with an overall test accuracy of(cf.,). This is despite using a training set of one million examples per task. Following a failure analysis, we hypothesize that the vanilla ViT fails because it cannot accurately model spatial relationships between the objects in an ARC grid and the grid boundaries.

Using a 2D representation strategy based onto represent the ARC input-output pairs,solvesof all test instances – a marked improvement (cf. Section). About 10% of the tasks remain poorly solved. After further failure analysis on these tasks, we discover that certain complex visual structures are difficult for. We hypothesize this is due to limitations of the transformer architecture itself in that it is designed to prioritize token embeddings over positional encodings that can make it challenging to capture intricate spatial relationships.

We improved’s spatial awareness by learning to combine absolute, relative, andpositional information (cf. Section), resulting in substantial performance boosts, with some ARC tasks progressing from unsolved to fully solved (). The final test accuracy is, with more than half of the tasks being solved to an accuracy ofor more.

SECTION: Related Work
​​​​​ is an emerging field that seeks to measure machine “intelligence”. Unlike many popular studies that focus on visual reasoning with multi-modal input, AVR focuses on reasoning tasks where the inputs are strictly images. The goal of AVR tasks is to discover abstract visual concepts and apply them to new settings. While the ARC is a generation task using abstract rules, other AVR tasks include classification tasks with explicit rules, such as the Raven’s Progressive Matricesand Odd-One-Out. We refer the readers tofor a more detailed introduction to AVR.

A Transformer architecture is based on the
attention mechanism. Following successes in natural language processing, recent studies have extended the Transformer to the vision domain. State-of-the-art approaches involve dividing the image into rectangular “patches”, where various techniques such as dynamic patch sizes allow for more effective capture of local information. Vision Transformers have been successfully used to perform various image-to-image generation tasks such as inpainting, image restoration, colorization, and denoising.

Due to the set-based (permutation-invariant) nature of attention, Positional Encodings are used to inject positional information in a Transformer. State-of-the-art Positional Encodings include Absolute Positional Encodings (APEs) where unique encodings are added to the inputs directly, Additive Relative Positional Encodings (RPEs)that measure the relative positions between tokens by modifying the attention logits, and various hybrid methods. Vision Transformer research has adapted these concepts, implementing both APEsand RPEsto incorporate positional information about the image patches.

Since the introduction of the ARC, the development of solvers has been an active research area. The earliest successful approaches consisted of an expressive Domain Specific Language (DSL) and a program synthesis algorithm that searched for a valid solution program expressed in the DSL. These include DAG-based search, graph-based constraint-guided search, grammatical evolution, library learning, compositional imagination, inductive logic programming, decision transformers, generalized planning, reinforcement learning, and several others. These models achieved up to 30% on the private ARC test set.

Recently, Transformer-based Large Language Models (LLMs) were shown to exhibit an apparent ability to perform “reasoning”spurring interest in using LLMs as part of an ARC solver. Such methods were prompted to perform program synthesis on a DSLas well as general-purpose languages such as Python, with the best-performing model achieving 42% on the public ARC evaluation set. LLMs were also explored as standalone solvers, where they were asked to produce the output grids directly instead of outputting a program. Although pre-trained LLMs proved ineffective when generating the output grid pixels directly, its performance was shown to be improved by object representation. The vision variant of a state-of-the-art LLM, GPT-4V was shown to be ineffective.

The current state-of-the-art solver has achieved 46% on the private test set at the time of writingbut is not publicly available or described in detail. We do know that it is a pre-trained LLM that is fine-tuned on millions of synthetic ARC tasks generated using the RE-ARC generatorand combined with test-time fine-tuning. Despite the visual nature of ARC tasks, Transformer-based LLM approaches convert the images into strings, which does not fully capture all relevant structural information.

SECTION: Vanilla Vision Transformer for the ARC: An Initial Approach
We first implement a vanilla Vision Transformer architecture as detailed inandas a solver for the ARC.
Consider an input imagedivided intonon-overlapping patches. Each patchis flattened in raster order and indexed bybefore being projected into a-dimensional embedding space. Letdenote the initial input to the Transformer for patch. For the-th Transformer layer,, and for a single attention head, the following operations are performed:

Here,is the embedding of patchandis the positional encoding. Following the standard ViT implementation of, the Absolute Positional Encoding (APE) is calculated as a learnable 1D encoding:

whereis a learned matrix assigning a-dimensional vector to each of the possiblepositions;is the maximum input length.

As seen in, ARC tasks areand require mapping an input image to an output image. Because image dimensions may vary across instances of the same task and even between the input and output grids of the same instance, any model that generates candidate solutions to an ARC input must be able to “reason” at the pixel level. We adapt the ViT architecture to this setting by making the following key modifications:

We introduce a decoder with cross-attention using the same positional encoding and attention mechanisms of the encoder. After the final decoder layer, the output embeddingof patchis projected linearly and a softmax function is applied to predict pixel-wise valuesas.
The cross-entropy loss is computed as the sum over pixels,.

To achieve the required pixel-level precision for the ARC task, we employ a patch size of, effectively treating each pixel as an independent input token.

To handle variable-sized grids, the flattened list of tokens is padded to a fixed maximum length. This configuration enables the model to process and generate ARC task outputs pixel-by-pixel.

SECTION: Experiments
To evaluate ViT’s reasoning capabilities comprehensively, we treat each of the 400 public training ARC tasks as an individual AVR problem. We generate a dataset of 1 million input-output pairs per task using the RE-ARC generatorand train all of our models (the vanilla ViT andmodels) in a supervised manner from scratch.

The ViT baseline consists of three layers with eight attention heads and a hidden dimension of 128. We trained the model on various single-core GPU nodes, including P100, V100, and T4, using a batch size of 8 for one epoch. We chose to train for one epoch because most models showed signs of convergence within the epoch. Due to computational resource limitations, we evaluated our major milestone models on the full set of 400 tasks. However, for the ablation studies hereafter, we used a randomly sampled subset of 100 tasks. For more details on the training process, please refer to.

We evaluate the model primarily on the percentage of solved instances, using a strict criterion: an instance is considered solved only if all generated pixels, including padding and border tokens, exactly match the ground truth. This approach is stricter than the original ARC metric which permits up to three candidate solutions.

shows that the vanilla ViT performs poorly: a significant percentage of tasks have a near 0% solve rate despite the million training examples per task. This points to fundamental limitations of the ViT architecture that inhibit abstract visual reasoning. In the following sections, we analyze failure cases and investigate methods for enhancing the visual reasoning ability of ViT.

SECTION: Visual Tokens: a Better Representation for ViT
The basic version of ourframework builds on the vanilla ViT but includes three simple yet highly effective changes to the representation of the ARC grids. We refer to these changes asto emphasize a departure from the language-based tokenization perspective in the particular setting of the ARC.

We observed that a large portion of the incorrect outputs from the vanilla ViT had incorrect grid sizes, a flagrant failure mode; An example is visualized in(ViT-Vanilla). We hypothesize that this is due to the vanilla ViT implementing padding in a “1D” manner, wheretokens are applied to the sequence after flattening, thus losing the two-dimensional context. To address this issue, we implemented 2D padding, wheretokens are applied to the imagebefore being flattened in raster order into a sequence for transformer processing (see).

However, this design introduces a new drawback: the model must now predicttokens as part of the output grid. In initial experiments, we observed that the model tends to ignore thesetokens (that do not receive attention), erroneously predicting over the entiregrid rather than focusing on the valid input region. An example of this issue is shown inof. To address this, we definetokens and enable attention to these tokens, allowing the model to properly account for the padded regions as well as the valid output region.

The implementation of 2D padding did not completely alleviate the previously observed failure cases. We further observed that for some tasks, when the output is cropped to the true grid dimensions, the predictions within the valid region are correct, underscoring the importance of proper boundary handling. We show an example inof. Inspired by the use of end-of-sequence (EOS) tokens likein Natural Language Processing (NLP), we introduceto explicitly define the grid boundaries (cf.):

() mark row transitions in thegrid.

(,, and) delineate the truegrid boundaries.

The introduction of border tokens enables the model to more effectively distinguish the task grid from the padding. Furthermore, as we see in ViT-Vanilla failure cases (), it is ambiguous to recover the 2D positions from a 1D sequence of predicted tokens alone. Border tokens also provide a fixed 2D template to fill in, which implicitly helps reconstruct the correct 2D positions and makes it easier to debug the related grid logic.

With the introduction of 2D padding and border tokens, our setup now operates on fixed-size, two-dimensional input-output pairs that are aligned with a universalcoordinate system. This allows us to adopt existing positional encoding (PE) strategies from the literature (see Section). After empirical analysis, we implement a (non-learned) 2D sinusoidal APE for, which is defined as follows:

whererepresents either theorcoordinate,is the index of the positional encoding dimension, andis the total embedding dimension.

SECTION: Results
shows substantial improvements in test accuracy due to the 2D visual tokens just described.(a) illustrates the improvement in the percentage of solved instances for each task. We observe an average performance boost of 48.34% compared to the baseline ViT across the 400 tasks. This model, referred to as ViTARC-VT, demonstrates that the new representation with 2D visual tokens significantly enhances the model’s ability to handle AVR tasks.

To quantify the contribution of border tokens, we performed an ablation study. As seen in, the absence of border tokens leads to a 4.59% decrease in accuracy, emphasizing their importance in helping the model delineate task grid boundaries and maintain spatial consistency in the input representation. For more detailed numerical results, refer toin.

SECTION: Analysis
While ViTARC-VT delivers strong results—approximately 40% of ARC tasks achieved over 90% solved test instances—there remain certain tasks where the model struggles. Specifically, around 10% of ARC tasks have less than 5% of test instances solved, even after training on a large dataset containing one million examples per task. Closer examination reveals that tasks involving complex visual structures, such as concave shapes, holes, or subgrids, are consistently problematic. These challenges highlight certain architectural limitations, particularly the model’s difficulty in segmenting multi-colored objects, where positional information should ideally play a more dominant role.

To better understand this behavior, we refer back to:.
In this setup, the absolute positional encoding,, is directly added to the input embedding,, so that it adjusts the token’s representation without overwhelming its semantic content. This works effectively in NLP tasks, where the semantic meaning of tokens generally takes precedence over their position. However, in vision tasks, especially those requiring detailed visual reasoning, spatial relationships often carry as much importance as, if not more than, the content of the tokens. For tasks in the ARC that involve complex multi-colored objects, such as subgrids, accurately encoding positional information becomes crucial.illustrates a specific case where the model fails to group pixels within a multi-colored subgrid correctly. The cross-attention map reveals that the model overly relies on color similarity, resulting in confusion between similarly colored pixels in different positions. This indicates a lack of sufficient attention to spatial relationships, which is essential for such tasks and guides us to develop further enhancements in the next section.

SECTION: Recentering Positions & Objects for Spatial Reasoning in ViT
Our observations on the failure cases of ViTARC-VT lead us to implement further enhancements to tackle tasks with complex visual structures by better encapsulating the positional information of pixels and objects.

To better balance the importance of positional information and tokens, we modifyby learning weight vectors for the encodings, i.e.,

whereandare learnable vectors of the same dimension as the encoding vectors, anddenotes element-wise multiplication. This effectively allows the model to learn the optimal balance between input tokens and positional encoding.

Furthermore, our implementation of 2D APE as described in, whereis the concatenation ofand, allows the vector-based mixing coefficients to focus on specific coordinates, which further improves the model’s reasoning capability over specific pixels.

Motivated by the example in, we aim to enable the model to distinguish between pixels in different spatial regions, such as the color-3 (green) pixel in the cyan box versus the one in the yellow box. In this example, the positional difference between the two pixels is just 1 along the-coordinate. APE encodes this difference as a small shift; while the transformer is theoretically capable of capturing these spatial relationships, in practice often requires many training epochs.

To better account for spatial relationships in two-dimensional grids, we adapt the Relative Positional Encoding (RPE) approach from ALiBiand extend it to 2D. ALiBi introduces additive positional biases to the attention scores based on the relative positions of tokens. In its original 1D form, ALiBi defines the positional bias as the following:

whererepresents the relative positional offset between tokensand, andis a predefined slope that penalizes tokens based on their distance.

Extending to 2D, we introduce distinct slopes for the “left” and “right’ directions, applied based on the 1D raster order of pixels. This approach accounts for both the inherent 2D structure of the data and the sequential nature of the generation process. Specifically:

Pixels located above or to the left of the current pixel in 2D space are assigned a bias.

Pixels located below or to the right are assigned a bias.

Hence, the 2D-RPE bias is computed as:

whererepresents the 2D Manhattan distance between coordinatesand. The slope valuesandare applied based on the relative positions of the pixels.

In this work, we leverage both 2D-RPE and 2D sinusoidal APE within our model. In contrast to observations made in Swin, where a degradation in performance was noted when combining RPE with APE, our results demonstrate a marked improvement. The inclusion of 2D-RPE allows for more precise modeling of relative spatial relationships, complementing the global positional information provided by APE. This synergy proves particularly effective for tasks demanding fine-grained spatial reasoning.

For tasks involving multi-colored objects, or more generally, tasks that require objectness priors, external sources of knowledge about object abstractions can be integrated into the model. We inject this information through a novelobject-based positional encoding.
We extend the 2D sinusoidal APE defined inby introducing the object indexas an additional component to the pixel coordinates. This results in a modified positional encoding:

In object detection models, two primary segmentation methods are bounding box segmentation and instance segmentation, the latter of which captures precise object boundaries. For simplicity, we adopt bounding box segmentation to derive the object index, as fine-grained distinctions at the instance level can already be addressed by the model’s attention mechanism, as illustrated in.demonstrates how bounding box information is obtained and incorporated into the positional encoding.

This design integrates seamlessly with the PEmixer introduced earlier, as it enables the model to dynamically adjust its reliance on the object indexbased on the task’s needs. In scenarios where the object index provides valuable abstraction, the model can prioritize it, while in cases where the object-based method is less effective, the model can fall back on thepositional information.

By leveraging object-based encoding, we offload the task of object abstraction to external methods,
For our experiments, OpenCV’s contour detectionproved sufficient for generating object indices in the ARC tasks, demonstrating the practical effectiveness of this approach in supporting the model’s reasoning capabilities.

SECTION: Results
We arrive at our final model, ViTARC, which contains all the improvements mentioned inand. As shown in, the model is a significant improvement over both the baseline ViT-Vanilla and ViTARC-VT due to the proposed positional enhancements.

Furthermore,(b) highlights the generalization of these improvements across tasks, with an additional 9.02% increase in solved instances compared to ViTARC-VT. ViTARC-VT itself already achieved a significant boost over ViT-Vanilla, culminating in a total improvement of 57.36% over the baseline ViT-Vanilla.

further illustrates the impact of each enhancement on task performance. All three contribute to the overall improvement, with 2D-RPE providing the largest gain, followed by PEmixer and OPE. Notably, without 2D-RPE, the model’s performance drops below that of ViTARC-VT. This occurs because OPE, while effective in specific tasks, is not consistently reliable. In these cases, ViTARC must fall back on theembeddings from 2D-APE, which are less expressive due to their lower dimensionality compared to ViTARC-VT. The inclusion of 2D-RPE recovers these positional signals at the attention level, ensuring robust performance even when object-based cues are insufficient.

For a comprehensive breakdown of the task-level performance and the numerical details of these ablations, please refer to.

SECTION: Conclusion
This paper introduced, a Vision Transformer architecture designed to address the unique challenges posed by the Abstraction and Reasoning Corpus. A key finding of our work is that positional information plays a critical role in visual reasoning tasks. While often overlooked when adapting transformers from NLP to vision, our results demonstrate that even simple enhancements to positional encoding can significantly improve performance on ARC tasks. Furthermore, we show that incorporating object indices as additional positional information via OPEs provides a meaningful improvement in handling complex spatial relationships in ARC tasks.

In pixel-perfect generative tasks like ARC, where resizing or patching methods are not applicable, we demonstrate that padding can be effectively managed through the introduction of additional border tokens. This innovation allows the model to maintain spatial consistency and correctly reason about grid boundaries, leading to more accurate pixel-level transformations.

Our results also show thatcan learn complex transformation rules at the pixel level, providing a promising direction for handling the abstract reasoning required in ARC. However, it is important to note thatsolves task-specific instances of ARC in a data-driven approach, treating each ARC task independently. This method does not fully solve ARC, which requires the ability to generalize across different tasks—a challenge that remains open for future research.

In summary, this work highlights the importance of positional information and object-based encodings in abstract visual reasoning that leads to our novel contribution of thearchitecture.advances the application of Vision Transformers for pixel-level reasoning and suggests further avenues for improving generalization capabilities in models tackling visual reasoning tasks.

SECTION: References
SECTION: Vanilla ViT Failure Analysis
SECTION: Training Details
This section provides a comprehensive overview of the training setup, including hyperparameters, hardware specifications, and other relevant details regarding the training process.

Our model consists of 3 layers with 8 attention heads and a hidden dimension of 128. The model was trained on various single-core GPU nodes, including P100, V100, and T4, with a batch size of 8 for 1 epoch. The typical training time per task ranges from 6 to 10 hours (wall clock).

The dataset was generated using Hodel’s generators, producing 1 million samples, which were then split into training, validation, and test sets with 998,000, 1,000, and 1,000 instances, respectively. The generation time varies between 3 and 12 hours, depending on the task. A fixed random seed (1230) was used for both dataset generation and model training to ensure reproducibility.

Due to computational resource constraints, the ablation study was performed on a randomly sampled subset of 100 tasks from the total 400, also selected using seed 1230.

SECTION: Full Results for Task-specific Accuracies
SECTION: Main models on full 400 tasks
SECTION: Ablation models on sampled 100 tasks