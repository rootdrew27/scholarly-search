SECTION: FlexDiT: Dynamic Token Density Control for Diffusion Transformer
Diffusion Transformers (DiT) deliver impressive generative performance but face prohibitive computational demands due to both the quadratic complexity of token-based self-attention and the need for extensive sampling steps. While recent research has focused on accelerating sampling, the structural inefficiencies of DiT remain underexplored. We propose, a framework that dynamically adapts token density across both spatial and temporal dimensions to achieve computational efficiency without compromising generation quality. Spatially, FlexDiT employs a three-segment architecture that allocates token density based on feature requirements at each layer: Poolingformer in the bottom layers for efficient global feature extraction, Sparse-Dense Token Modules (SDTM) in the middle layers to balance global context with local detail, and dense tokens in the top layers to refine high-frequency details. Temporally, FlexDiT dynamically modulates token density across denoising stages, progressively increasing token count as finer details emerge in later timesteps. This synergy between FlexDiT’s spatially adaptive architecture and its temporal pruning strategy enables a unified framework that balances efficiency and fidelity throughout the generation process. Our experiments demonstrate FlexDiT’s effectiveness, achieving a 55% reduction in FLOPs and a 175% improvement in inference speed on DiT-XL with only a 0.09 increase in FID score on 512512 ImageNet images, a 56% reduction in FLOPs across video generation datasets including FaceForensics, SkyTimelapse, UCF101, and Taichi-HD, and a 69% improvement in inference speed on PixArt-on text-to-image generation task with a 0.24 FID score decrease. FlexDiT provides a scalable solution for high-quality diffusion-based generation compatible with further sampling optimization techniques. Code will be available at.

SECTION: Introduction
Diffusion modelshave emerged as powerful tools in visual generation, producing photorealistic images and videos by gradually refining structured content from noise. Leveraging the scalability of Transformers, Diffusion Transformer (DiT) modelsextend these capabilities to more complex and detailed tasksand form the backbone of advanced generation frameworks such as Sora. Yet, despite their impressive generative capabilities, DiT models face significant computational limitations that restrict their broader applicability, especially in scenarios where efficiency is paramount.

The computational burden of DiT models arises mainly from the need for numerous sampling steps in the denoising process and the quadratic complexity of Transformer structures with respect to the number of tokens. Although recent research has focused on reducing this burden by accelerating the sampling process through techniques such as ODE solvers, flow-based methods, and knowledge distillation, these approaches often overlook the core architectural inefficiencies of DiT itself. Unlike U-Net-based architectures, which mitigate complexity through a contracting-expansive structure, DiT’s reliance on token-level self-attention incurs high costs that scale with model size and token count. This bottleneck highlights a critical need for DiT-specific innovations that manage token density intelligently, balancing efficiency and quality without sacrificing generation fidelity.

To address this challenge, we examine the distribution of attention across DiT’s layers and sampling steps, seeking to uncover structural patterns that could guide more efficient token management. As illustrated in Figure, attention maps reveal that tokens capture different levels of granularity across layers: in the bottom layers, such as layer 0, attention maps appear uniformly distributed, indicating a focus on broad, global features akin to global pooling. Meanwhile, middle layers alternate in their attention, with certain layers (e.g., layer 6 and layer 26) capturing local details, while others (e.g., layer 22) emphasize global structure. This observation is quantified in Figure, where the variance in attention scores highlights DiT’s consistent pattern of alternation between global and local feature extraction across layers.
In addition to spatial analysis, Figurealso reveals insights across sampling steps. We observe that, while the pattern of alternation between global and local focus remains stable, attention variance increases as the denoising process advances, indicating a growing emphasis on local information at lower-noise stages. This analysis yields three core insights: (1) the initial self-attention layers exhibit minimal variance, showing low discrepancy in feature extraction across tokens; (2) DiT’s architecture inherently alternates between global and local feature focus across layers, a pattern consistent across all sampling steps; and (3) as denoising progresses, the model increasingly prioritizes local details, dynamically adapting to heightened demands for detail at later stages.

Building on these insights, we propose FlexDiT, which reimagines token density management as a dynamic process adapting across both spatial layers and temporal stages. Spatially, shallow layers capture smooth, global features, making complex self-attention less efficient. Subsequent layers alternate between high-frequency local details and low-frequency global information, where sparse tokens efficiently capture global features, and dense tokens refine local details. Temporally, as denoising advances, the need for local detail increases, motivating a dynamic token adjustment strategy.
This dual-layered adaptation integrates layer-wise token modulation—balancing efficiency and detail within each sampling step—and a timestep-wise pruning strategy that adjusts token density dynamically. Together, these spatial and temporal adaptations allow FlexDiT to achieve computational efficiency without sacrificing high-fidelity detail, as reflected in FlexDiT’s architectural design and pruning strategy, tailored to meet the unique spatial and temporal demands of the generation process.

: FlexDiT’s architecture employs a three-segment design that aligns token density with the features each layer captures. In the bottom layers, we replace self-attention with a poolingformer structure to capture broad global features through average pooling, reducing computation while preserving essential structure. In the middle layers, FlexDiT introduces Sparse-Dense Token Modules (SDTM), blending sparse tokens for global structure with dense tokens to refine local details. This design enables FlexDiT to retain efficiency while maintaining stability and detail. In the top layers, the model processes all tokens densely, focusing on high-frequency details for refined output quality.

: Complementing spatial adjustments, FlexDiT’s timestep-wise pruning rate strategy adapts token density across denoising stages. In early stages, where broad, low-frequency structures dominate, FlexDiT applies a high pruning rate, conserving resources. As the process progresses and intricate details emerge, the pruning rate decreases, allowing for a gradual increase in token density. This adjustment aligns computational effort with each stage’s complexity, dynamically balancing efficiency and detail to optimize resources for high-fidelity output.

Our empirical results substantiate the effectiveness of FlexDiT’s integrated approach. FlexDiT reduces the FLOPs of DiT-XL by 55% and improves inference speed by 175% on 512512 ImageNetimages, with only a increase of 0.09 in FID score. On the Latte-XL dataset, FlexDiT achieves a 56% reduction in FLOPs across standard video generation datasets, including FaceForensics, SkyTimelapse, UCF101, and Taichi-HD. Additionally, on the more challenging text-to-image generation task, we achieve a 69% improvement in inference speed on PixArt-with a 0.24 FID score reduction. These results demonstrate FlexDiT’s capability to provide an efficient, high-quality architecture for diffusion-based generation, compatible with further sampling optimization techniques for enhanced efficiency.

SECTION: Related works
Although diffusion modelshave achieved impressive performance in generation tasks, their generation speed remains a significant bottleneck for broader applications. This limitation arises primarily from the step-wise reverse diffusion process and the high computational cost associated with single model evaluations.

This direction focuses on optimizing sampling steps in diffusion models. Some approachesdesign new solvers to achieve faster sampling with fewer steps. Consistency models (CMs)are closely related to diffusion models, achieving by distilling pre-trained diffusion models or training from scratch. These models learn a one-step mapping between noise and data, and all the points of the sampling trajectory map to the same initial data point. Flow-basedapproaches straighten the transport trajectories among different distributions and thereby realize efficient inference. Knowledge distillation are applied by some methodsto reduce the sampling steps and match the output of the combined conditional and unconditional models. Parallel sampling techniquesemploy Picard-Lindelöf iteration or Fourier neural operators for solving SDE/ODE.

Another avenue focuses on improving inference time within a single model evaluation. Some methods attempt to compress the size of diffusion models via pruningor quantization. Spectral Diffusionboosts structure design by incorporating frequency dynamics and priors. OMS-DPMproposes a model schedule that selects small models or large models at specific sampling steps to balance generation quality and inference speed. Deepcachereuse or share similar features across sampling steps. The early stopping mechanism in diffusion is explored in.

Most of the above methods are designed for general diffusion models. Recently, the Diffusion Transformer (DiT) has emerged as a more potential backbone, surpassing the previously dominant U-Net architectures. Unlike U-Net-based models, DiT’s inefficiency stems largely from the self-attention mechanism. Limited research has specifically addressed the efficiency of DiT-based models.identifies the redundancies in the self-attention and adopts timestep dynamic mediator tokens to compress attention complexity, achieving some performances but limited FLOPs reduction, as this approach primarily reduces the number of keys and values rather than the overall tokens count. In Vision transformers for classification task, numerous methodssuccessfully removed redundant tokens to improve performance-efficiency trade-offs. ToMeSDis the first to explore token reduction in diffusion models. However, its performance on DiT was suboptimal, as shown in. U-Net can be regarded as a sparse network due to its contracting and expansive structure. EDTapplies a novel architecture similar to U-Net, while they still have 1.4 FID score gap compared to its baseline model MDTv2via 2,000K iterations. These findings indicate that directly transferring token reduction techniques from classification tasks or reusing U-Net structures may be insufficient for DiT. Instead, a specialized token reduction strategy is needed.

We analyze the structure of DiT and observe that local and global features are captured alternately across layers. Inspired by this insight, FlexDiT introduces a novel token reduction strategy that decouples the generation process into separate integration of global and local information, leveraging sparse tokens for global feature extraction and dense tokens for local detail refinement. By alternating between sparse and dense tokens, FlexDiT ensures high efficiency and performance.

SECTION: Method
We first provide an overview of diffusion models and DiT in Section. FlexDiT architecture is then introduced in Section, followed by our timestep-wise pruning rate strategy in Section. Finally, Sectiondetails the initialization and fine-tuning of our network.

SECTION: Preliminary
generate visual contents (such as images and vidoes) from random noise through a series of denoising steps. These models typically consist of a forward diffusion process and a reverse denoising process. In the forward process, given an visual inputsampled from the data distribution, Gaussian noiseis progressively added oversteps. This process is defined as, whereanddenote the sampling step and noise schedule, separately. In the reverse process, the model removes the noise and reconstructsfromusing, whereandrepresent the mean and variance of the Gaussian distribution.

exhibits the scalability and promising performance of transformers. Similar to ViT, DiT consists of layers composed of a multi-head self-attention (MHSA) layer and a multi-layer perceptron (MLP) layer, described as

wheredenotes input tokens,is the number of tokens, andis the channel dimension. The parametersare produced by an adaptive layer norm (AdaLN) block, which takes the condition embedding and sampling step embedding as inputs.

SECTION: FlexDiT
The FlexDiT architecture is illustrated in Figure, where the transformer layers of DiT based model are divided into three groups: bottom, middle, and top. The bottom layers leverage poolingformers to efficiently capture global features. The middle layers contain multiple sparse-dense token modules (SDTM), which decouple the representation process into global structure capture and local details enhancement using sparse and dense tokens, respectively. The top layers retain the standard transformers, processing dense tokens to generate the final predictions at each sampling step. The primary computational savings are achieved through sparse tokens, hence the majority of transformer layers are located in the middle section to maximize efficiency.

As shown in the first subfigure in Figure, attention scores in the bottom layers are uniformly distributed, with each token evenly extracting global features, akin to a global average pooling operator. To investigate further, we conducted a toy experiment where the attention maps in the first two transformer layers were replaced with matrices filled with, without any fine-tuning. Figureshows that, using the same initial noise and random seed, the images generated by both the original DiT-XL and the modified DiT-XL are nearly identical, suggesting that complex self-attention calculations offer limited additional information. Given that self-attention in the bottom layers captures global features, we question whether sparse tokens can be used in these layers. Extensive experiments, however, demonstrated that applying sparse tokens in the lower layers leads to unstable training; maintaining a full set of tokens in these layers is essential.

Based on above analysis, we adopt poolingformers to replace the original transformers. In poolingformer, we remove the queries and keys as attention maps are not computed. Instead, we perform global pooling on the valueand integrate it into input tokens, which can be represented as

whereis the mean along the dimension of,i.e.,, and then repeatingtimes to shape. The parameters from adaLN block are omitted in all the equations for brevity. Our poolingformer can be viewed as a special case of the model in, behaving identically when the average pooling kernel size equals the input size.

We present the sparse-dense token module (SDTM) to generate sparse and dense tokens processed by corresponding transformer layers. The high-level idea is to decouple global structure extraction and local detail extraction. Sparse tokens capture global structure information and reduce computational cost, while dense tokens enhance local detail and stabilize training. Sparse and dense tokens are converted each other within SDTM.

Initially, SDTM introduces a set of sparse tokens, where M is the number of sparse tokens, typically. We define pruning rateto represent the sparsity degree. Sparse token interact with full-size dense tokens via an attention layer to integrate the global information:

where the triplet input ofare queries, keys, and values in turn. Sparse tokens can be initialized by adaptively pooling the dense tokens. To store the structure information, we first reshape the dense tokensinto the latent shape. For instance, if the input is an image, we reshape it into shape, then pool it across the spatial dimensions to shape, whereand. The intentions of spatial pooling initialization are two folds. First, the initial sparse tokens can distribute uniformly in space and the representation of each sparse tokens is associated with a specific spatial location, beneficial for downstream tasks like image editing. Second, it can prevent the semantic tokens from collapsing to one point in the following layers.

The generated sparse tokens are fed into subsequent transformer layers, referred to as sparse transformers. Since, our SDTM can substantially reduce the computational cost.

At the rear of SDTM, we restore dense tokens from sparse tokens. First, we reshape the sparse tokensinto structure shapeand upsample it to the same shape as input dense tokens. We introduce two linear layers to combine upsampling sparse tokens with dense tokens, which is represented as:

where,are the weights of two linear layers. Then, to further incorporate sparse tokens, we utilize an attention layer to perform a reverse operation of the generation of sparse token to produce the restored dense tokens, which is written as:

At the end of SDTM, several transformer layers, referred as to dense transformers process full-size dense tokens to enhance local details.

We cascade multiple SDTMs in our network. By repeating sparse and dense tokens, the network effectively preserves both structural and detailed information, achieving substantial reductions in computational cost while maintaining high-quality generation outputs.

SECTION: Timestep-wise pruning rate.
We have observed that the tokens display varying denoising behavior over sampling steps. They generate the low-frequency global structure information in the early denoising stage and the high frequency details in the late denoising stage. The token count requirement is progressively increase along sampling steps. We exploit this by dynamically adjusting the pruning rateacross sampling steps, increasing tokens as sampling progresses.

Givensampling steps, we propose a sample-specific approach to dynamically adjust the pruning rate, thereby controlling the number of sparse tokens across sampling steps. We define a range for, such that. Observing that generation quality is highly relative with the later denoising stages (in Section), we holdconstant atfor the firstsampling steps. Subsequently, we adjust the pruning ratelinearly based on the current sampling step. The specific formula foris provided as follows:

During training, we sample stepand compute the correspondingaccording to Eq.. However, the tokens of input should be same to train the model in batch, and the randomness of sampling fromalso need to be maintained. To solve this contradiction, we modify the linear function of Eq.whento piecewise function. Normally, the model is trained by multiple GPUs. We request thesampled in a specific piece in each GPU. Therefore, in each iteration, it can achieve both uniformly random sampling and training in batch.

SECTION: Initialization and fine-tuning.
Our method fine-tunes pre-trained DiT models to improve efficiency. The processes of generating sparse tokens and restoring dense tokens utilize off-the-shelf transformers, avoiding the need for additional networks. During fine-tuning, the parameters of transformers in DiT-based model are loaded into the corresponding transformers in our FlexDiT, with two exceptions: (1) queries and keys are absent in Poolingformers, hence related parameters in pre-trained model are not required; (2) the weightsandin Eq.are initialized by full zeros and identity matrix, receptively.

SECTION: Experiments
FlexDiT is applied in three representative DiT-based models, DiT, Latte, and PixArt-for class-conditional image generation and video generation, and text-to-image generation, respectively.

SECTION: Class-conditional image generation
We conduct our experiments on ImageNet-1kat resolutions ofand, following the protocol established in DiT. For DiT-XL, the model consists of 2, 24, and 2 transformer layers in the bottom, middle, and top segments, respectively. The middle segment includes 4 sparse-dense token modules, which comprise 1 sparse token generation transformer, 3 sparse transformers, 1 dense token recovery transformer, and 1 dense transformer. For DiT-B, the bottom, middle, and top segments contain 1, 10, and 1 transformer layers, respectively. The middle segment consists of 2 sparse-dense token modules, including 1 sparse token generation transformer, 2 sparse transformers, 1 dense token recovery transformer, and 1 dense transformer.

As poolingformers and sparse transformers can interfere with the function of position embedding, we reintroduce sine-cosine position embedding at each stage of dense token generation. For DiT-XL, we use the checkpoint from the official DiT repository, while for DiT-B, we utilize the checkpoint provided by. All training settings and hyperparameters follow those in the original DiT paper. The strength of classifier-free guidanceis set to 1.5 for evaluation and 4.0 for visualization.

Following prior works, we sample 50,000 images to compute the Fréchet Inception Distance (FID)using the ADM TensorFlow evaluation suite, along with the Inception Score (IS), sFID, and Precision-Recall metrics. Throughput is evaluated with a batch size of 128 on an Nvidia A100 GPU.

The results of our approach on the class-conditional image generation task using ImageNet are presented in Table. We evaluate our method on two model sizes, DiT-B and DiT-XL, and at two resolutions,and. At pruning rates, FlexDiT-XL achieves a 43% reduction in FLOPs and an 87% improvement in inference speed, with only a 0.11 increase in FID score. This indicates significant redundancy within DiT, as using only about 25% tokens in specific layers maintains similar performance levels. By increasing the token count, we observe slight improvements over baseline performance with 145% improvement in throughput. The impact of our approach is even more pronounced at a resolution of. With a higher pruning rate than theresolution, our method yields a superior performance-efficiency trade-off. By pruning over 90% of the tokens, we achieve a 55% reduction in FLOPs and a 175% increase in throughput, with only a 0.09 increase in FID score. When reducing the pruning rate further, our results slightly surpass the baseline, delivering a 145% throughput improvement.

Fromto, the FLOPs increase by only 4.4 times, whereas the speed declines by a factor of 6.3, demonstrating that the primary speed bottleneck in DiT structure is the number of tokens. Our method effectively reduces token count, resulting in significant gains, particularly for high-resolution content generation. For instance, we achieve a 55% reduction in FLOPs atwhile enhancing speed by 175%. Compared to other methods, our real-world speed improvements far exceed those achieved with similar reductions in FLOPs.

To further validate the effectiveness of our method, we visualize some samples generated from FlexDiT-XL at aresolution with a pruning rate(-43% FLOPs) and compare them with DiT-XL in Figure. Each cell in the same position corresponds to images generated from the same random seed and class. Since our model is fine-tuned from the pre-trained DiT, the overall styles and structures of the two subfigures are similar. The images generated by FlexDiT still retain rich high-frequency details. Furthermore, FlexDiT images demonstrate more precise structure, as evidenced by the accurate depiction of the “golden retriever’s” nose and the “macaw’s” eyes, which appear misplacement or missing in the images generated by DiT-XL. We provide samples at a resolution ofin Supplementary..

SECTION: Class-conditional video generation
We conduct experiments at a resolution ofon four public datasets: FaceForensics, SkyTimelapse, UCF101, and Taichi-HD. Latte comprises two distinct types of transformer blocks: spatial transformers, which focus on capturing spatial information, and temporal transformers, which capture temporal information. To accommodate this separation of spatial and temporal feature extraction, we adjust our model’s schedule within the sparse-dense token module. Specifically, we employ two transformers to prune spatial tokens and temporal tokens separately. The sparse tokens are then processed by two sparse transformers, and finally, two transformers are used to recover the temporal and spatial tokens. Dense transformers are discarded in this setup. All other model configurations are the same as in DiT, and we follow the original training settings and hyperparameters used for Latte.

To evaluate performance, we sample 2,048 videos, each consisting of 16 frames, and measure the Fréchet Video Distance (FVD). Throughput is measured using a batch size of 2 clips on an Nvidia A100 GPU.

Tablepresents the main results of our approach. Leveraging the additional temporal dimension in video data allows for a higher pruning rate. Our method achieves a 56% reduction in FLOPs while maintaining a competitive FVD score compared to the baseline, demonstrating its effectiveness in video generation tasks.

SECTION: Text-to-image generation
We further assess the effectiveness of our method on text-to-image generation, which presents a greater challenge compared to class-conditional image generation. We adopt PixArt-, a text-to-image generation model built based on DiT as the base model. Our model is initialized using the official PixArt-checkpoint pre-trained on SAM datasetcontaining 10M images. We further fine-tune it with our method on a subset of SAM dataset including 1M images. In PixArt-, the transformer architecture includes two types of attention layers: a self-attention layer and a cross-attention layer, which integrates textual information. We apply our method to the self-attention layers to either reduce or recover tokens.

For evaluation, we randomly select text prompts from SAM dataset and adopt 100-step IDDPM solverto sample 30,000 images, with the FID score serving as the evaluation metric. Throughput is evaluated with a batch size of 128 on an Nvidia A100 GPU.

As shown in Table, our FlexDiT achieves an FID score comparable to the original PixArT-with significantly accelerating the generation, showing that our method is effective for text-to-image generation task and shows a strong general-purpose ability in various generation fields.

We visualize some samples of our method and compare them with original PixArt-in Figure. The classifier-free guidance is set to. The results demonstrate that our method effectively maintains both image quality and semantic fidelity.

SECTION: Ablation study
All the following ablation experiments of FlexDiT-XL are conducted on ImageNet at aresolution.

The alternation between sparse and dense tokens is a key factor contributing to the success of our method. Sparse tokens efficiently capture global structures, while dense tokens capture detailed information. In FlexDiT, we adopt four SDTMs by default.
Tableshows the effect of using different numbers of SDTMs. To isolate the impact of SDTM count, we maintain the same FLOPs across models by adjusting the number of sparse and dense transformers while varying only the number of SDTMs. In configurations with only one SDTM, the model is modified into a U-shaped architecture, allocating 6 transformers to the bottom segment, 16 to a single SDTM, and 6 to the top segment. Note that in this configuration, standard transformers are used in the bottom layers instead of poolingformers prevent any performance degradation that arises from the use of poolingformers. The results indicate that fewer SDTMs reduce interactions between global and local information, resulting in weaker performance. Furthermore, when FlexDiT is reduced to a U-Net structure, training stability is compromised, resulting in training collapse. We do not experiment with more than four SDTMs, as increasing SDTM count beyond four offers diminishing returns in FLOP reduction. Using four SDTMs achieves an optimal balance between performance and efficiency.

In Table, we examine the impact of varying the number of poolingformers in the model. We keep the total number of transformers in the bottom and top segments constant across configurations. When using three poolingformers, performance declines substantially due to the global average pooling operator’s limited ability to adaptively capture information. For configurations with one or two poolingformers, the models achieve same results. The poolingformer is more efficient and consume fewer parameters than the standard transformer; therefore, we default to using 2 poolingformers. Reducing the poolingformer count to zero results in training instability, suggesting that maintaining full-size tokens in the initial layers is crucial. This finding aligns with the observations in Figure: in DiT, the first two attention maps exhibit a uniform distribution, allowing us to replace them with poolingformers, but further replacement disrupts information flow and reduces performance.

Our FlexDiT is a model compression method that can be seamlessly integrated with efficient samplers, such as DDIMand Rectified Flow (RFlow). Note that the RFlow variant of DiT was trained by us via 2,000K iterations. As shown in Table, when our method is combined with the 25-step DDIM and 5-step Rectified Flow samplers, it achieves approximatelyandimprovements in inference speed, respectively, compared to the standard 250-step DDPM. Notably, our method does not introduce a significant performance gap relative to the baseline, demonstrating that it can be effectively combined with efficient samplers.

To evaluate the effectiveness of the timestep-wise pruning rate strategy, we conduct ablation experiments, as shown in Table. In these experiments, we maintain the same number of FLOPs while varying the number of tokens along timesteps, either using a constant token count or dynamically adjusting the token count. The results demonstrate that dynamically tuning the pruning rate significantly improves performance. Additionally, Tablehighlights an observation: the performance of the dynamic pruning strategy is particularly relative with the token count of the later stages of denoising. For instance, the configuration with token counts ranging fromtoonly shows a 0.02 FID score increase compared to the configuration with a constanttoken count.

SECTION: Limitations
The primary limitation of our method arises from the manually pre-defined its structure, containing the number of layers in each module and the number of sparse tokens.

SECTION: Conclusion
In this paper, we introduced FlexDiT, a novel method for enhancing the efficiency of DiT by leveraging token sparsity. Through a strategically layered approach that incorporates SDTM and timestep-wise pruning rate, FlexDiT significantly reduces computational complexity while maintaining high generation quality. By decoupling global and local feature extraction with sparse and dense tokens, FlexDiT achieves efficient utilization of resources in both image and video generation tasks. Our experimental results demonstrate substantial reductions in FLOPs with minimal performance degradation, validating FlexDiT’s effectiveness across multiple datasets. Moreover, FlexDiT complements existing methods aimed at optimizing sampling steps, enabling further improvements when used in tandem with enhanced ODE solvers. This work presents a step forward in developing scalable, efficient diffusion models suitable for high-resolution content generation, opening up new possibilities for practical applications of DiT-based architectures.

SECTION: References
SECTION: Captions in Figure
SECTION: Additional visualization