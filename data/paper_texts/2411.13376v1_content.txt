SECTION: ODTE - An ensemble of multi-class SVM-based oblique decision trees
We propose ODTE, a new ensemble that uses oblique decision trees as base classifiers. Additionally, we introduce STree, the base algorithm for growing oblique decision trees, which leverages support vector machines to define hyperplanes within the decision nodes. We embed a multiclass strategy (one-vs-one or one-vs-rest) at the decision nodes, allowing the model to directly handle non-binary classification tasks without the need to cluster instances into two groups, as is common in other approaches from the literature. In each decision node, only the best-performing model (SVM)—the one that minimizes an impurity measure for the n-ary classification—is retained, even if the learned SVM addresses a binary classification subtask. An extensive experimental study involving 49 datasets and various state-of-the-art algorithms for oblique decision tree ensembles has been conducted. Our results show that ODTE ranks consistently above its competitors, achieving significant performance gains when hyperparameters are carefully tuned. Moreover, the oblique decision trees learned through STree are more compact than those produced by other algorithms evaluated in our experiments.

[inst1]organization=Departamento de Sistemas Informaticos.,addressline=Universidad de Castilla-La Mancha,
city=Albacete,
postcode=02071,
country=Spain

SECTION: Introduction
is a problem which basically consists into assigning a label to a given instance or example from a predefined and finite set of labels. Formally, objects or instances are defined over a set of features or attributes, each one taking values in a predefined domainwhich can be of numerical or discrete nature. On the other hand, the target orvariableis of discrete nature and take values in a finite and mutually exclusive set of labels. Then, acan be viewed as a function. In machine learning (ML) or data mining (DM)the goal is to learn this function or classifier from data.

is a main task in ML whose objective is learning a classifier from previously labelled data, i.e. objects or instances for which the correct class or label is known. This kind of data,, usually comes from historical cases which have been previously solved, and the availability of thelabelfor each-dimensional objecthelps the learning algorithm tothe learning process, trying to minimise the error made on the given instances, whose correct label is known.

A plethora of different paradigms to cope with the supervise classification problem is available in the literature: instance-based methods, logistic regression, Bayesian network classifiers, decision trees, support vector machines, neural networks, ensembles, etc. Currently, there is no doubt that from the point of view of accuracy, methods based on neural networks, especially deep learning approaches, and ensembles (multi-classifiers), are the most competitive ones. Nonetheless, these two outstanding approaches, while being not mutually exclusive, have their own niche of application. Thus, deep learning-based methods excel in the processing of unstructured data (image, video, etc.) and require large amounts of data to work properly. On the other hand, ensembles tend to be used more with structured (tabular) data and, in general, can work with moderate or even small amounts of data. In this paper we consider the case of structured data, so we focus on the use of ensembles as ML paradigm.

In essence, an ensemble is a multiclassifier, that is, a set of models is learned from different samples of the data, and new cases are classified by all of them and their results aggregated to provide a final output. The use of different strategies when learning the models and aggregating their results, give rise to different ensemble methods: bagging, boosting, stacking, etc. These strategies are usually generic, i.e. they can be instantiated with any base classifier, although there are also ensemble models based on a specific type of classifier, such as Random Forestor Gradient Boosting, which use a classification or regression trees as a base model. Even in the case of generic classifiers, decision trees are also often the most widely used choice as a base classifier, due to the fact that they are models with low bias and high variance, which is exactly what is usually reduced by means of the ensemble-based aggregation, thus resulting in a more robust and stable classifier. In this study we focus on the use of decision trees as the base models for the ensemble.

(DT)are model-based classifiers whose inner structure is a tree. Basically, inner (or decision) nodes represent questions posed over the predictive attributes or features, while leaf nodes contain the class label to be returned as output. When a new instancemust be classified, the tree is traversed from the root to a leaf by choosing for each decision node the appropriate branch according to node question and the instance values for predictive attributes. Once a leaf node is reached, its associated class label is returned as outcome. The structural simplicity of DTs is one of its main advantages, as each path from the root to a leaf can be considered as a human-readable rule, which makes a DT an interpretable model able to justify its decisions. Furthermore, despite the fact of its simplicity, DTs performance is competitive with other state of the art classifiers,
having been successfully and recently applied e.g. to real-time estimation of human pose by Microsoft Kinect.

In literature we can find different families of decision trees, which mainly differ in the type of question/test included in the decision nodes, which in turn translate into different geometrical partitions of the representation (input) space. The most standard DT models are the so-called orthogonal ones, which consist of placing decisions of the type,?, whereis an attribute or feature andbeing a threshold. Well known algorithms used to induce this type of DTs are C4.5and CART, that use information criteria likeandrespectively to select the best splitting test () at each decision node. However, although being successful ML algorithms, this type of DTs has the limitation of dealing only with decision boundaries that are parallel to the axes (features), and when this is not the case their size grow because several consecutive tests are included in order to approximate non axis-parallel boundaries (see Figure).

decision trees (ODT)were introduced to capture decision boundaries not parallel to the axes. In this way it is possible to obtain more compact and generally more accurate trees. To represent these type of boundaries, the tests associated with the decision nodes are more complex, involving a functiondefined over a subset of the attributes, which on the other hand makes the resulting models lose part of their interpretability (see right part of Figure). Despite this, there are a few efforts in the literature that address this issue, and model-agnostic can also be used.
In the original proposals of ODTs the functionwas a linear combination of features, but later more complex ML models like support vector machines (SVM) or neural networks (NN) have also been used.

Nowadays, ensemble algorithms constitute state of the art ML algorithms, usually being the first choice for practitioners to approach classification problems, especially in the case of structured (tabular) data. Because of the success of ensemble techniques, research on ODTs and their use as base models in ensemble classifiers, has gained attention in recent years. In this article we focus on the use SVM-based ODTs as base classifiers in ensemble algorithms. In order to do so, it is necessary to deal with three different problems or choices; (1) how to deal with domains where the class variable has more than two labels?; (2) what kind of SVM algorithm to use?; and (3) what ensemble technique(s) to use for classifier design?. On the contrary to recent proposals in this research line (see Section 2) that require internal transformations to actually work with binary class problems in the decision nodes, or the use of complex and sophisticated SVM models, our proposal focuses on their ease of use by practitioners, avoiding the internal transformation of the class variable and using standard SVM algorithms that can be found in commonly used ML libraries. In fact our proposed algorithm is available as an integrated classifier in SckitLearn. The extensive experimental analysis carried out shows that the proposed ensemble, ODTE, outperforms alternative algorithms on average, standing out for its ease of use and its integration in standard software, which allows its parameters to be easily adjusted to adapt to different domains (datasets).

In summary, our main contributions in this study are:

We introduce ODTE, a new ensemble algorithm based on Oblique Decision Trees.

We present STree, a novel algorithm for learning oblique decision trees that serve as the base classifiers in ODTE. This method effectively handles a multi-class target variable by producing a single model. The core idea behind the method is to embed a multiclass strategy (OvR or OvO) within each internal node, selecting only one of the learned SVMs to guide the splitting process, specifically, the one that minimizes impurity for the n-ary classification task.

We conduct an extensive experimental evaluation on a benchmark comprising 49 datasets, comparing our methods against state-of-the-art competing algorithms. The results highlight the superior performance of ODTE and STree, especially when hyperparameters are finely tuned.

The source code for both ODTE and STree, along with all datasets used in our study, is published on GitHub, Pypi and Zenodo to facilitte reproduciblity in future research.

In the next sections we review recent research in the design of ODT-based ensembles (Section) and detail the main components of our proposal (Section). Then, in Sectionwe describe the experiments carried out to evaluate the proposed algorithm, which involve a significant benchmark of datasets and competing algorithms. Finally, our conclusions are presented in Section.

SECTION: Related work
SECTION: Oblique Decision Trees
Constructing a Decision Tree (DT) from data typically involves a recursive partitioning process that divides the data into multiple subsets based on a selected test () at each internal node. This recursive partitioning stops when the data arriving at a node predominantly belongs to a single category, at which point the node becomes a leaf. Thus, the critical aspect of the DT learning process lies in determining the appropriate test or division for an internal node. In the case of axis-parallel DTs, information theory-based or statistical metrics are employed to evaluate which test most significantly diminishes the uncertainty of the class variable. Commonly utilized measures include Shannon entropy (as seen in C4.5) and the Gini index (as used in CART).

In oblique Decision Trees (DTs), more sophisticated multivariate tests are utilized, resulting in models that are generally more compact and accurate. However, selecting the appropriate test for a given internal node is more computationally demanding. In the majority of Oblique DT (ODT) algorithms, the test comprises a linear combination of the input attributes, specifically,. The objective is to identify theparameters that define the hyperplane, which, in turn, produces the binary partition that most effectively reduces the uncertainty of the class variable. In the CART-LC algorithm, a coordinate descent method is employed for parameter optimization, whereas OC1enhances this approach by incorporating multiple restarts and random perturbations to avoid local optima. Both CART-LC and OC1 start the optimization with the optimal axis-parallel partition. WODTtransforms the optimization challenge by adopting a continuous and differentiable weighted information entropy function as the objective, allowing for the use of gradient descent for optimization. Furthermore, WODT also distinguishes itself by initiating with a random hyperplane. Additionally, metaheuristic algorithms have been applied to overcome local optima in this context.

Recent advancements in ODTs have focused on enhancing optimality and scalability, as discussed in. Efforts have been made to develop sparse oblique trees that achieve low generalization error and offer rapid inference, as highlighted in. Additionally, tackling imbalanced data has become a priority, with optimization strategies now being directed towards non-linear metrics to address this issue.

SECTION: SVM-based Oblique Decision Trees
Beyond linear functions, advanced machine learning models such as neural networks or support vector machines (SVMs) have been explored as criteria for splits, enabling the application of both multivariate linear and nonlinear tests. This paper concentrates on employing SVMsto develop the tests for internal nodes. The conventional SVM algorithm aims to find the optimal separating hyperplane that maximizes the margin between the training data for a binary class variable. To address non-linearly separable challenges, the input vectors might be transformed into a high-dimensional feature space, thereby converting the problem into one of linear classification. This transformation and the solution process can be efficiently combined through the use of the kernel trick, which allows for direct computation with various types of kernels (e.g., linear, polynomial).

The literature showcases a variety of Oblique Decision Tree (ODT) methodologies that incorporate Support Vector Machines (SVMs) for determining the hyperplane at each internal node. Thus, traditional SVMs equipped with linear, radial-basis function, and polynomial kernelshave been employed for this purpose. More sophisticated SVM algorithms, such as the multisurface proximal SVM (MPSVM) and the twin bounded SVM (TBSVM), are highlighted inandrespectively. Both MPSVM and TBSVM are designed to learn two hyperplanes, each one optimized to be closer to the data samples of one class and farther from those of the opposing class; classification of instances is then achieved by evaluating the distance of instances to both hyperplanes.

The algorithms indemonstrate capability in addressing multi-class issues. In the approach discussed by in, a unique one-vs-rest binary problem is created for each class, leading to the development ofSVM models. This method involves constructing a vector of lengthfor every instance, where each dimension corresponds to the distance from the respective hyperplane. Subsequently, instances are categorized intoclusters via the X-means algorithm, with the cluster countdenoting the number of branches emanating from an internal node. In the methods proposed by Ganaie et al.and Zhang et al., a method is used at internal nodes to split class labels into two groups based on Bhattacharyya distance, which then informs the binary classification tackled by MPSVM and TBSVM, respectively.presents a method where the multi-class issue is simplified to a binary challenge at each internal node by opposing the class with the largest number of instances against the aggregate of other classes. Inthe authors work on multiclass categorization by introducing a mathematical programming model designed to optimize the internal structure of binary oblique decision trees. This model innovatively determines fictitious classes at the internal nodes, derived from the misclassification errors observed at the leaf nodes, thus ensuring the tree maintains a binary structure. Ultimately,introduced a classification method utilizing SVM-trees combined with K-means to address the issue of imbalanced fault classification in industrial process applications, whileproposed a method to induce oblique label ranking trees to deal with the non-standard classification problem of label ranking.

SECTION: Ensembles of oblique decision trees
Due to the rise of ensemble classifiers, oblique decision trees have naturally become prominent as base classifiers in these models. For instance,introduces a new decision forest learning scheme where the base learners are Minimum Message Length (MML) oblique decision trees.study the consistency of combining oblique decision trees with boosting and random forests, andapplies evolutionary algorithms to construct ensembles of oblique decision trees.

Due to the affinity with our work, we highlight ensembles of oblique decision trees based on the use of SVMs. Notable examples include the models proposed in, which are based on twin bounded SVMs, the models proposed in, which incorporate heuristic techniques for enhancing the oblique decision tree construction process, and the models proposed in, which use multi-surface proximal SVMs.

Finally, the importance and rise of ensemble models based on the use of oblique decision trees is demonstrated by their penetration in various application domains. Thus, ensembles of oblique decision trees have been considered for classifying gene expression data, imbalanced fault classification in industrial processes, hyperspectral data classification, image classification, generating explanations for neural network classifications, and developing resource-efficient models for neural signal classification, among others.

SECTION: Proposed method: ODTE
We introduce the Oblique Decision Tree Ensemble (ODTE), a bagging-inspired ensemble with oblique decision trees as base classifiers. These trees utilize an SVM to compute hyperplanes at each inner node. In general, a binary classification problem is addressed, requiring the learned hyperplane to separate the two classes. In our description, we will treat the learned classifier as a function rather than focusing on the hyperplane inherent to the classifier (SVM). While SVM primarily deals with binary classification, a key objective of our approach is to effectively handle multi-class target variables. Unlike traditional methods that implement one-vs-rest (OvR) or one-vs-one (OvO) strategies as a separate external process, our method integrates these strategies directly at each internal node of the tree. In our proposal, we select the classifier (hyperplane) that yields the best binary partition to be included as the conditional test in the node.

Through the use of the OvR or OvO strategies at each decision node, we eliminate the need for clustering the set of instances into two groups, as previously required in other studies. Intuitively, grouping labels into a binaryclass should result in more compact and balanced trees, however, our experiments show that segregating one label from the others as early as possible tends to produce more compact trees. Another critical aspect is our approach to select the hyperplane after facing one class label against the rest or one class label against other in a pairwise fashion. Unlike methods that typically favor the majority class label, we consider the distribution of the remaining labels across the two subsets of the partition. We then select the classifier (SVM) that results in the least impure partition, based on the distribution of the class labels in the obtained partition.

From a technical perspective, our ensemble algorithm (ODTE) (Algorithm), utilizes sampling with replacement to obtain the bootstrap datasets. Each (sampled) dataset is then used to train an SVM-based oblique decision tree by using our proposed STree algorithm (Algorithm). For inference, each ODT in the ensemble processes the given instance, and majority voting is employed to determine the final outcome (label).

In the rest of this section, we provide a detailed description of our proposed algorithm, STree, designed to learn an SVM-based ODT from data. As usual, STree is a recursive algorithm that begins by receiving a training dataset. Then, the method proceeds as follows:

If the stopping condition is met, e.g., max depth has been reached or there is almost only one class label in the sample, a leaf node is created having asthe more frequent class label, i.e. the mode, in the dataset.

Otherwise, a binary split is created using a classifier obtained by an SVM learning algorithm. Therefore, the inner classification problem(s) must be reformulated as a binary one. Basically, if the number of labels () in the sample is greater than 2 -notice that as the tree grows in depth, not all the labels will be present- in the received set, then a multiclass classification strategy is employed. The classifier corresponding to the SVM whose application produces the partition with the higher information gain with respect to the class variable, is stored as the splitting test in the current node. The detailed steps of this process are outlined in algorithm, however, let us to highlight the main stages here:

If, we are dealing with aproblem. The SVM algorithm is applied to learn the classifier (hyperplane), denoted as. This classifier is then used to partition the instances ininto two subsets:and, based on whether the output ofassigns the labelorto the instance. Specifically, the instances above the corresponding hyperplane are assigned the label, while those below the hyperplane are assigned the label. The classifier () is stored in the node to serve as the decision rule during inference (line 16, Algorithm).

Ifwe are in asetting. In this scenario, we resort to techniques that transform a multiclass classification problem into several binary classification problems, although in the end, we will select only one of these problems (models) to be included in our tree (line 18, Algorithm).

. Depending on the multiclass strategy used we have:

(OvO) strategydefinesbinary classification problems on all possible pairs of classes:(label 1 vs label 2),(label 1 vs label 3),,(label 1 vs label),,(label 2 vs label 3),,(labelvs label).
Let,, be the classifier learned by the SVM algorithm for the-th binary classification problem (labelvs label;), by using only the instances inlabeled withor(line 20, Alg.).

(OvR) strategy, definesbinary classification problems by considering, respectively, each label as the positive class and the union of the remaining labels as the negative class. Let,, be the classifier learned by the SVM algorithm for the-th binary classification problem (labelvs all labels minus) (line 22, Alg.).

. Letanddenote the partitions generated by one of the learned models,, from the SVM algorithm, when applied to all instances in. In the case of the OvO approach,andcorrespond to the instances with predicted labelsor(represented asor) for the learned model. In the case of the OvR approach,andcorrespond to the instances with predicted labelsor(represented asor) for the learned model(line 25, Alg.).

Letbe a measure which evaluates the impurity of the class variablein. Then, we select the classifiersuch that

withordepending on the multiclass strategy, OvR or OvO, used in the learning stage.is then stored in the current node for future use in splitting and inference (line 25, Alg.).

Notice that in our pseudocode we actually maximize the gain with respect to the initial partition, which is equivalent to minimize the impurity measure.

Once the classifier () and the corresponding partition () have been selected, two branches are created for this node: positive, for those instances being classified asbyand, negative, for those instances being classified asby. Two recursive calls are then launched withandas the dataset of input instances respectively (lines 29-30, Alg.).

In our implementation we made the following design decisions:

As there is no agreement about which kernel is better or worse for a specific domain, we allow the use of different kernels (linear, polynomial and Gaussian radial basis function).

Shannon entropy is used as the impurity measure, and Information Gain is used to select the(SVM) that yields the purest partition.

Although a pruning stage has not yet been designed for STree, a pre-pruning strategy can be implemented by setting a maximum depth threshold for the tree.

For inference, given an instance to be classified, the tree must be traversed from the root to a leaf node, whose associated label is returned as the outcome. At each inner/decision node, the stored classifieris used, and depending on the obtained label,or, the instance follows theorbranch.

SECTION: Experimental evaluation
In this Section we describe the comprehensive experiments carried out to evaluate our proposal.

SECTION: Benchmark
For the evaluation of our algorithm in comparison with state-of-the-art methods, we use a benchmark of 49 datasets, previously used into test their ensembles of oblique decision trees. Specifically, 45 of the selected datasets come from the UCImachine learning repository, while the other 4 (Id 27 to 30 in Table) are related to a fisheries fecundity problem (seefor the details).

The Tableshows an identifier (Id) for each dataset, its name, the number of instances (), features () and class labels ().

SECTION: Algorithms
The following algorithms have been included in the comparison:

. This is our proposed algorithm with default hyperparameters: 100 trees, STree as base estimator, don’t limit number of features and don’t limit the size of the bootstrap. Also default hyperparameters are used for STree.

. This is the tuned version of our proposed algorithm. In this case the hyperparameters for the base estimators were optimized for each dataset using a gridsearch. The obtained values for each hyperparameter and dataset are available at.

. Twin Bounded Random Forest uses the fundamentals of Random Forests such as Bagging and random subspaces. Each tree is built with a bootstrap of the training data with a random subspace of features, and uses Twin Bonded Support Vector Machineto generate the hyperplane needed, in this case 2 hyperplanes are generated, for splitting at each non-leaf node. The algorithm is run with a default parameter settingand.

. This is the tuned version of the previous model. The hyperparameters selected by the authors in the provided code are selected for each dataset (see).

. Twin Bounded Rotated Forest uses Principal Component Analysis (PCA) to rotate the datasetand different rotation matrices are used to build each tree. The whole set of features are used to compute the hyperplanes in each node. The algorithm is run with a default parameter settingand.

. This is the tuned version of the previous model. The hyperparameters selected by the authors in the provided code are selected for each dataset (see).

. Twin Bounded subrotation Forest Algorithm mixes PCA rotation and random subspaces, this way the rotation is applied only to the subset of features that is assigned for each tree. The algorithm is run with a default parameter settingand.

. This is the tuned version of the previous model. The hyperparameters selected by the authors in the provided code are selected for each dataset (see).

. We have used WekaBagging Classifier to wrap J48-SVMODTas base classifier. Thealgorithm proposed inthat also uses J48-SVMODT as base classifier, has not been included in the experiments since it is clearly surpassed by BaggingJ48-SVMODT.

. We have used bagging classifier from scikit-learnto wrap WODTas base classifier.

and. State-of-the-art ensemble algorithms taken from scikit-learn.

As a common parameter, 100 trees are learned as base classifiers in all the ensemble algorithms.

SECTION: Reproducibility
ODTE and STree have been implemented inasclassifiers. For the sake of reproducibility and boosting future comparisons, we provide the code of both classifiers in:and. Furthermore, to make easier reproducing our experiments, the datasets have been uploaded to.

For the rest of algorithms we have used publicly available versions of RandomForest () and XGBoost pre-built binary wheels for Python from PyPI (Python Package Index) have been used. The code for WODT (), J48SVM-ODT () and TBSVM-ODT () have been provided to us by their authors. To create the ensembles for the base algorithms WODT and J48SVM-ODT we have wrappered them by using the public implementation ofavailable inandrespectively.

All the experiments have been run in an Intel(R) Xeon(R) CPU E5645 at 2.40GHz running Linux operating system.

SECTION: Results and analysis
To evaluate the performance of each pair (algorithm, dataset), we conducted a ten-times five-fold cross validation (105cv). The same 10 seeds were used across all pairs to randomize the data prior to cross-validation. Since no severe imbalance is present in any dataset (see), accuracy was used to compare the performance of the tested algorithms.

We distinguish two scenarios: all the algorithms are run by using a default setting for the hyperparameters (see Table); and some of the algorithms are run by using hyperparameter tuning In this second case only our proposed algorithm () and the algorithms for which the hyperparameter tuning is available (,and) are considered. The mean and standard deviation over the 50 runs of the 105cv are reported, respectively, in Tables&.

As a first impression, looking at the tables we can observe thatandare the winner algorithms both in number of wins and average accuracy. The difference is impressive in the scenario involving hyperparameter tuning, although all the algorithms are clearly benefited from the tuning process.
To properly analyze the results, we conducted a standard machine learning statistical analysis procedureusing thetool. First, a Friedman test () is performed to decide whether all algorithms perform equivalently. If this hypothesis is rejected, a post hoc test is performed using Holm’s procedure () and taking as control the highest-ranked algorithm from the Friedman test. Specifically, we have conducted two statistical analyses, one for each scenario (Tablesand).

From the analyses we can draw the following conclusions:

In the default setting, Friedman test reports a p-value of 1,3061930e-11, thus rejecting the null hypothesis that all the algorithms are equivalent. The results of the post hoc Holm’s tests are shown in Tableby using ODTE as control, as it is the top ranked algorithms. The columnrepresents the ranking of the algorithm obtained by the Friedman test and the-value represents the adjusted p-value obtained by Holm’s procedure. The columns,andcontain the number of times that the control algorithm wins, ties and loses with respect to the row-wise algorithm. The-values for the non-rejected null hypothesis are boldfaced. As can be observed, there is no statistically significant difference between ODTE and most of the algorithms, despite ODTE having a larger number of wins. The only algorithms that are clearly outperformed areand.

In the hyperparameter tuning setting, Friedman test reports a p-value of 7,4815371e-16, thus rejecting the null hypothesis that all the algorithms are equivalent. The results of the post hoc Holm’s tests are shown in Tableby usingas control, as it is the top ranked algorithms. As can be observed, in this case there is statistically significant difference bewteenand the rest of studied algorithms.

To complete our analysis we also studied the complexity (size) of the obtained trees and also the training time required by the different oblique decision tree algorithms. For the sake of brevityin Tablewe only show the values on average over the 49 datasets once we normalized them by using ODTE as control.

With respect towe compute the average size (number of nodes) in the trees included in the ensemble, then we normalize by using ODTE. As all the numbers in the row are greater than one, it is clear that ODTE obtains the more compact trees. On the other hand, with respect towe must be careful with the analysis because the tested algorithms are in different programming languages. However, from the data in Tablewe could obtain a similar conclusion, with ODTE being the faster algorithm.

In summary, considering all the evaluated dimensions (accuracy, size, and time), there is no doubt that ODTE emerges as the outstanding algorithm in the comparison, a fact that is even more pronounced when we take the hyperparameter tuning phase into account.

SECTION: Conclusion
A new ensemble algorithm, ODTE, which incorporates a novel oblique decision tree model (STree) capable of directly handling a multi-class target variable, has been introduced. The STree algorithm generates a binary oblique decision tree that learns several SVM at each split, though only one is retained for inference. The experiments demonstrate that this approach performs well across a broad range of domains (49 datasets), surpassing competing ensembles of oblique decision trees and other state-of-the-art algorithms.
It has also been observed that tuning the hyperparameters of the base algorithm and the ensemble for each dataset is crucial for achieving even better results, leading to a version of ODTE that significantly outperforms all other tested methods.

For future research, we plan to go deeper in the use of (random/informed) sub-spaces, selecting features randomly as in random forest () or using univariate or multivariate filter feature selection algorithms. Additionally, the benefits of fine-tuning hyperparameters have proven essential for the performance of the proposed algorithms, albeit at the cost of high computational CPU time. In future studies, we aim to investigate a type of lightweight auto-tuning integrated into the ODTE/STree algorithm(s).

SECTION: Acknowledgements
First of all, we are indebted to the authors of,andfor providing us with the code for their implementations.
This work has been partially funded by the Government of Castilla-La Mancha and “ERDF A way of making Europe” under project SBPLY/21/180225/000062; by MCIU/AEI/10.13039/501100011033 and “ESF Investing your future” through project PID2022-139293NB-C32; and by the Universidad de Castilla-La Mancha and “ERDF A Way of Making Europe” under project 2022-GRIN-34437.

SECTION: CRediT author statement
: Data curation, Software, Visualization, Validation, Conceptualization, Methodology, Writing- Original draft preparation, Writing- Reviewing and Editing.: Supervision, Funding acquisition, Conceptualization, Validation, Writing- Original draft preparation, Writing- Reviewing and Editing.

SECTION: Declaration of generative AI and AI-assisted technologies in the writing process
During the preparation of this work, the author(s) used CHATGPT and GRAMMARLY in order to improve language and readability. After using these tools/services, the author(s) reviewed and edited the content as needed and take(s) full responsibility for the content of the publication.

SECTION: References