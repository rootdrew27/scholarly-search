SECTION: Rethinking Transformer-Based Blind-Spot Networkfor Self-Supervised Image Denoising
Blind-spot networks (BSN) have been prevalent neural architectures in self-supervised image denoising (SSID).
However, most existing BSNs are conducted with convolution layers.
Although transformers have shown the potential to overcome the limitations of convolutions in many image restoration tasks, the attention mechanisms may violate the blind-spot requirement, thereby restricting their applicability in BSN.
To this end, we propose to analyze and redesign the channel and spatial attentions to meet the blind-spot requirement.
Specifically, channel self-attention may leak the blind-spot information in multi-scale architectures, since the downsampling shuffles the spatial feature into channel dimensions.
To alleviate this problem, we divide the channel into several groups and perform channel attention separately.
For spatial self-attention, we apply an elaborate mask to the attention matrix to restrict and mimic the receptive field of dilated convolution.
Based on the redesigned channel and window attentions, we build aransformer-basedlind-potetwork (TBSN), which shows strong local fitting and global perspective abilities.
Furthermore, we introduce a knowledge distillation strategy that distills TBSN into smaller denoisers to improve computational efficiency while maintaining performance. Extensive experiments on real-world image denoising datasets show that TBSN largely extends the receptive field and exhibits favorable performance against state-of-the-art SSID methods. The code and pre-trained models are available at.

SECTION: Introduction
Image denoising is a fundamental low-level vision task that aims to recover implicit clean images from their noisy observations.
With the development of convolutional neural networks, learning-based methodshave shown significant improvements against traditional ones.
In order to facilitate network training, it is common to synthesize noisy-clean image pairs with additive white Gaussian noise (AWGN) for supervised learning.
Since the distribution gap between AWGN and camera noise, they exhibit degraded denoising performance in real-world scenarios.
One feasible solution is to capture datasetswith strictly aligned noisy-clean pairs for network training.
However, the data collection process requires rigorously controlled environments and much human labor, which is less practical.

Recently, self-supervised image denoising (SSID)has been introduced to circumvent the requirement of the paired datasets.
The pioneering work Noise2Void randomly masks some locations of a noisy input and trains the network to reconstruct them from their surrounding ones.
In the case of pixel-wise independent noise, the network learns to predict the masked pixels without random noise,i.e., the clean pixels, thereby achieving self-supervised denoising.
Blind-spot networks (BSN)take a step further to implement the mask schema with dedicated designed network architectures that exclude the corresponding input pixel from the receptive field of each output location, which shows superiority in both performance and training efficiency.
For spatially correlated noise removal in real-world scenarios, some workssuggest first breaking the noise correlation with pixel-shuffle downsampling (PD), and then denoising with BSN.
In particular, asymmetric PD strategyfor training and inference has shown a better trade-off between noise removal and detail preserving.

Existing BSN architectures are mostly convolutional neural networks (CNNs).
However, convolution operations have limited capability in capturing long-range dependencies and the static weights of convolution filters could not flexibly adapt to the input content.
As highlighted by the success of the image restoration models, such limitations could be mitigated with transformer models.
Nonetheless, transformer operators may violate the blind-spot requirement and lead to overfitting to the noisy inputs.
Despite the difficulty, few attempts have been made to apply transformers into BSNs.
For instance, LG-BPNincorporates channel-wise self-attentionfor global feature enhancement but still uses convolution layers for local information integration.
SwinIAimplements a swin-transformerbased BSN with modified window attention. However, limited by the requirement for blind spots, it can only exploit shallow features of the noisy inputs in the attention layer, thus showing inferior performance.
It can be seen that it is very challenging to bring out the effective capabilities of transformers in BSN.

In this paper, we propose to analyze the spatial and channel self-attention mechanisms and redesign them to meet the blind-spot requirement.
For channel-wise self-attention, we observe that simply applying it may leak the blind-spot information, especially in multi-scale architectures.
The deep features of such architectures have been downsampled multiple times and the spatial information is shuffled to the channel dimension.
The interaction between channels may leak spatial information at the blind-spot, leading to overfitting to the noisy input.
We empirically find that this effect appears when the channel dimension is larger than the spatial resolution.
To eliminate the undesirable effect, we divide the channels into groups and perform channel attention on each group separately, where the group channel number is controlled less than spatial resolution.
For spatial self-attention, we elaborately redesign window attention by restricting its receptive field to maintain the blind-spot requirement.
Specifically, a fixed mask is applied to the attention matrix so that each pixel can only attend to pixels at even coordinates.
Combining the designed spatial and channel self-attention mechanisms, we propose a dilated transformer attention block (DTAB).
We embed DTAB into the encoder-decoder based U-Net architecture, thus presenting a transformer-based blind-spot network (TBSN).

Additionally, BSN architectures are mostly computationally inefficient due to the additional design for satisfying the blind-spot requirement.
It becomes even worse with increasing model size and complicated post-refinement process.
However, some simple and efficient supervised denoisers have the potential to reach the performance of state-of-the-art SSID methods.
In this work, we take advantage of this property to explore a knowledge distillation strategy for reducing the computation cost during inference.
Specifically, we regard the results of pre-trained TBSN as pseudo ground-truths, and take them as supervision to train a plain U-Net, namely TBSN2UNet.

Extensive experiments are conducted on real-world denoising datasetsto assess the effectiveness of TBSN and TBSN2UNet.
As shown in Fig., benefiting from proposed spatial and channel self-attention mechanisms, TBSN enhances the local adaptivity and largely expands the receptive field.
TBSN behaves favorably against state-of-the-art SSID methods in terms of both quantitative metrics and perceptual quality.
Moreover, TBSN2UNet maintains the performance of TBSN while significantly reducing inference costs.

Our main contributions can be summarized as follows:

We propose a transformer-based blind-spot network (TBSN) that contains spatial and channel self-attentions for self-supervised image denoising.

For channel self-attention, we find it may leak the blind-spot information when the channel number becomes large, we thus perform it on each divided channel group separately to eliminate this adverse effect. For spatial self-attention, we introduce masked window attention where an elaborate mask is applied to the attention matrix to maintain the blind-spot requirement.

Extensive experiments demonstrate that TBSN achieves state-of-the-art performance on real-world image denoising datasets, while our U-Net distilled from TBSN effectively reduces the computation cost during inference.

SECTION: Related Work
SECTION: Deep Image Denoising
The development of learning-based methodshas shown superior performance against traditional patch-based oneson synthetic Gaussian denoising.
More advanced deep neural architecturesare further proposed to improve the denoising ability.
NBNetproposes a noise basis network by learning a set of reconstruction basis in the feature space.
InvDNproposes a lightweight denoising network based on normalizing flow architectures.
Recently, transformers that were first introduced for sequence modeling in natural language processinghave been successfully applied to vision tasks.
For image denoising, transformers are studied with large-scale image pre-trainingand Swin Transformer architectures.
Restormerand Uformerpropose multi-scale hierarchical network designs, which achieve better trade-offs between performance and efficiency.
However, there are limited efforts for adapting transformers into self-supervised image denoisingdue to the blind-spot requirement.

SECTION: Self-Supervised Image Denoising
Self-supervised image denoising (SSID) seeks to utilize the information from the noisy images themselves as supervision.
In order to prevent trivial solutions such as over-fitting to the identity mapping, blind-spot networks (BSN)exclude the corresponding noisy pixel from the receptive field at every spatial location.
Probabilistic inferenceand regular loss functionsare further introduced to recover the missing information at the blind-spot.
For real-world RGB image denoising, the noise is spatially correlated due to the demosaic operation in image signal processing (ISP) pipeline.
It will easily fit the input noise when deploying BSN designed for spatially independent noise removal.
One feasible solution is to break the noisy correlation with pixel-shuffle downsampling, then apply BSN to the downsampled images.
In addition, CVF-SIDlearns a cyclic function to decompose the noisy image into clean and noisy components.
SASLdetects flat and textured areas then constructs supervisions for them separately.
Although much effort has been made for developing SSID algorithms, there is still a lack of further exploration for BSN architectures.
In this work, we adapt the transformer mechanism to BSN to further unleash the potential of blind-spot manners.

SECTION: Method
SECTION: Overview of the Network Architecture
As shown in Fig., TBSN follows dilated BSNto apply 33 centrally masked convolution at the first layer and dilated transformer attention blocks (DTABs) in the remaining layers.
The network architecture is U-Net and adopts patch-unshuffle/shufflebased downsampling/upsampling operations to maintain the blind-spot requirement.
The building block,i.e., DTAB, is formed with dilated counterparts of grouped channel-wise self-attention (G-CSA), masked window-based self-attention (M-WSA), and feed-forward network (FFN), respectively.
Thus, TBSN benefits from both the global interaction of channel attention and the local fitting ability of window attention.
We will provide detailed illustrations of the network design in the following subsections.

SECTION: Grouped Channel-Wise Self-Attention (G-CSA)
Channel attentionrecalibrates the channel-wise feature responses by explicitly modeling the interdependencies between channels.
Given an input feature, channel attention can be formalized as,

where functionaggregates the spatial information in each channel, andis channel-wise multiplication operation.
For instance, NAFNetachievesby global average pooling, while Restormerapplies transposed matrix multiplication in the channel dimension.
However, in SSID task, channel attention may leak blind-spot information asaggregates the content of all the spatial locations, which is ignored in previous methods.

In this work, we systematically analyze the effects of channel attention (CA) in BSN and empirically find it depends on the channel number versus spatial resolution.
For single-level architectures, spatial information is largely compressed by global average pooling, thus CA is beneficial for performance.
For multi-scale architectures, the spatial information is shuffled to various channels by downsampling operations. Thus, CA may be partially equivalent to spatial interaction, leaking the blind-spot values.
To this end, we propose to control the channel number smaller than spatial resolution.
Specifically, we introduce grouped channel-wise self-attention (G-CSA) to divide the deep feature into multiple channel groups and perform CA separately.
Our G-CSA could be formulated as,

where,is the group number.
We set the channel number of each group,i.e.,, to be small enough to avoid the leakage of spatial information.
In the implementation, we adapt MDTAto our G-CSA with Eq. () for global interaction.
We also replace thedepth convolutions with their dilated counterparts to achieve blind-spot requirement, as shown in Fig.(b).

SECTION: Masked Window-Based Self-Attention (M-WSA)
Window-based self-attentionhas been wildly applied in image restoration.
In this work, we mimic the behavior of dilated convolutionsto propose a masked window-based self-attention (M-WSA) for SSID, which can be plug-and-played into any layer and exploit current deep features as.
As shown in Fig.(d), we ingeniously design a fixed attention mask adding to the attention matrix to restrict the interactions betweenandtokens.
From Fig.(a), in original window attention, eachtoken interacts withtokens at all spatial locations within the window.
In our M-WSA, thetoken attends to the spatial locations at even coordinates (see Fig.(b)).
Therefore, M-WSA exhibits the same functionality as dilated convolutions for building BSN, but with a larger receptive field and stronger local fitting capability.

Here we formally illustrate our attention mask. In window attention, within a local window of size, the current feature is first projected to,andtokens as, respectively.
Then the original window attention can be formulated as,

whereis the feature dimension.
In our M-WSA, our attention maskis applied to the attention matrix that restricts eachonly attends toat even coordinates, as shown in Fig.(d).
Thus, Eq. () can be modified as,

Specifically,is a two-valued matrix that masks out certain locations according to the relative position of(at) and(at) tokens.
Whenandare with even distance on both axes,, the attention value is unchanged.
Otherwise,and the attention value becomesafter the softmax operation, thereby being masked out.
Inspired by relative position embedding,can be calculated from a smaller-sized binary matrixaccording to the relative position ofandto improve the efficiency,i.e.,

In the implementation perspective, we adopt overlapping cross-attentionthat calculatestokens from a larger field to further expand receptive field.

. The proposed G-CSA and M-WSA are distinct from the transformer operators in previous BSNs.
As shown in Fig.(a)(b), channel attention in LGBPNis risked to leak blind-spot information when applied in multi-scale architectures, while our G-CSA performs channel attention in separate groups to alleviate this problem.
From Fig.(c)(d), window attention in SwinIAmasks the main diagonal of the attention matrix to maintain the blind-spot requirement.
Itstokens are limited to be from pixel-wise shallow features of the noisy input, thus showing inferior results.
In contrast, our M-WSA applies a dedicated designed mask to mimic the behavior of dilated convolution, which can be flexibly performed on the deep features.

SECTION: Knowledge Distillation for Efficient Inference
Self-supervised image denoising methods usually require high computational cost due to complicated network designs, increased network size, and post-processing operation.
The computation burden largely limits their applicability in certain situations,e.g., on mobile devices.
Nonetheless, the performance of SSID methods still falls behind the corresponding supervised ones.
Even lightweight supervised methods may achieve better performance than complex self-supervised ones.
In other words, the lightweight network may be fully sufficient to fit the results from some complex self-supervised methods.
Taking advantage of this, we suggest a knowledge distillation strategy to reduce the inference cost while maintaining the performance.

Specifically, we adopt the efficient U-Netarchitecture as our student network, which is distilled from the self-supervised learned TBSN (namely TBSN2UNet),

whereis the noisy image,is the stop gradient operation.
Note that we aim to reduce the computation cost during inference.
It is different from the methods that apply knowledge distillation for better performance.

SECTION: Experiments
SECTION: Implementation Details
.
We conduct experiments on two wildly used real-world image denoising datasets,i.e., SIDDand DND.
The noisy-clean pairs of SIDD dataset are collected from five smartphone cameras, where each noisy image is captured multiple times and the average image serves as ground truth.
It contains 320 training images, 1280 validation patches and 1280 benchmark patches, respectively.
We train our networks on the noisy images of train split, and test on the benchmark split.
DND is a benchmark dataset collected from DSLR cameras.
The noisy images are captured with a short exposure time while the corresponding clean images are captured with a long exposure time.
It contains 50 pairs for test only.
We train and test our networks on the test images in a fully self-supervised manner.

.
For self-supervised training of TBSN, we follow AP-BSNto apply pixel-shuffle downsampling (PD) to break the noise correlation, and adopt asymmetric PD factors during training and inference to trade-off the denoising effect and detail preserving.
We also improve the denoising results with random replacement refinement (R3) strategy.
The batch size and patch size are set to 4 and, respectively.
We adoptloss and AdamWoptimizer to train the network.
The learning rate is initially set to, and is decreased by 10 every 40k iterations with total 100k training iterations.
For knowledge distillation, the training settings are the same as self-supervised learning.
All the experiments are conducted on PyTorchframework and Nvidia RTX2080Ti GPUs.

SECTION: Comparison with State-of-the-Art Methods
.
Tab.shows the quantitative results of proposed TBSN and state-of-the-art self-supervised methods: Noise2Void, Noise2Self, NAC, R2R, CVF-SID, AP-BSN, SASL, LG-BPN, and PUCA.
Among these, blind-spot techniques designed for spatial independent noise (Noise2Void, Noise2Self, and R2R) exhibit little denoising effect on real-world noisy images.
Although pixel-shuffle downsampling (PD) breaks the noise correlation and successfully removes the noise, the performance is still limited by its plain convolutional BSN architecture.
Some recent works tackle this problem by searching for advanced BSN architectures.
For instance, LG-BPN incorporates transformer blockinto BSN for global information and brings 0.37dB improvement over AP-BSN baseline.
PUCA designs a multi-scale BSN with channel attention and achieves 0.63dB improvement.
Nonetheless, benefitting from channel and window attention mechanisms, our TBSN boosts the improvement to 0.87dB on SIDD benchmark dataset.

.
The qualitative results of self-supervised image denoising methods are shown in Fig.and Fig..
The denoising of the color chart in Fig.depends on the global information, where former methods fail to wipe the noise completely.
Benefiting from the channel-wise self-attention, our TBSN removes the spatially correlated noise smoothly.
The cup and the wall in Fig.and Fig.show that TBSN could maintain the details due to its local fitting capability of window attention.

SECTION: Comparison of Model Complexity
The channel-wiseand window-basedself-attentions in TBSN are efficient transformer modules designed for image restoration task.
In addition, TBSN adopts hierarchical multi-scale architecture to further improve its efficiency.
As shown in Tab., TBSN maintains similar computation complexity as the convolutional counterparts PUCA, and is more efficient than LG-BPN.
In addition, SASL shows attractive #Param and #FLOPs results due to its U-Net architecture.
For a fair comparison, the U-Net distilled from our TBSN exhibits the same computation complexity as SASL but with higher performance, which demonstrates the superiority of our knowledge distillation strategy.

SECTION: Ablation Study
SECTION: Visualization of the Receptive Field
The expansion of receptive field is a major factor towards the success of transformers.
In this subsection, we plot the input pixels for recovering the center pixel of output to access the effective receptive field of TBSN.
Specifically, we calculate the gradient of the input with respect to the output’s center pixel.
Such gradient indicates how much the output pixel changes with a disturbance on each input pixel.
We sum the absolute value of the gradient along the channel axis for visualization.
As shown in Fig., our dilated M-WSA performs longer-range interaction compared to the convolution operators while our dilated G-CSA aggregates all the spatial locations for global perception.
From Fig, TBSN shows a significantly wider receptive field than former BSNs, which is a possible explanation for the appealing performance of TBSN.

SECTION: Analysis on DTAB
Tab.analyzes the effectiveness of the components in our dilated transformer attention block (DTAB).
We begin with a base model (1) that degenerates the window and channel attentions to dilated convolutions.
In contrast, our dilated M-WSA (2) enhances the base model with local fitting capability and provides 0.27dB improvement.
Our dilated G-CSA (3) exhibits global interaction and shows 0.65dB improvement.
TBSN achieves a total improvement of 0.81 with combined channel and window attentions, which demonstrates the complementarity of the local and global operations.
In addition, we assess the effects of other window and channel attention implementations in Fig..
Replacing M-WSA with SwinIA (4) leads to 0.59dB performance drop while replacing G-CSA with LG-BPN (6) leads to 0.05dB performance drop.
The other attention mechanisms, Swin Transformer(5), SE(7) and SCA(8) also shows inferior performance.
To summarize, the ablation study results suggest that DTAB is the optimal network choice.

SECTION: Analysis on Channel Attention
As illustrated in the method, channel-wise self-attention (CSA)may leak blind-spot information in multi-scale architectures.
Tab.analyzes the effect of CSA with downsampling scales from 1 to 5.
In correspondence, the channel number at the deepest layers grows from 48 to 768, and the spatial resolution reduces fromto, respectively.
From the middle lines of Tab., the plain dilated CSA provides positive effects at the scales less equal to 3, but leads to obvious performance drop at the 4- and 5-scales.
This is owing to the channel dimension being larger than the spatial resolution at 4- and 5-scales so it leaks the blind-spot information.
Instead, our grouped dilated G-CSA divides the channels into several groups and performs channel attention separately.
As the channel dimension within each group is controlled smaller than the spatial resolution, dilated G-CSA provides constant improvement at all scales.

SECTION: Analysis on Knowledge Distillation
As shown in Tab., we conduct experiments with U-Net architecture to assess the effectiveness of knowledge distillation.
Despite fewer parameters and FLOPs, U-Net trained in a supervised manner achieves 1.21dB higher performance than TBSN on SIDD validation dataset. It demonstrates lightweight U-Net has enough learning capacity to receive the denoising performance of TBSN.
Consequently, the distilled student U-Net achieves comparable results as the teacher TBSN.
The results show that knowledge distillation is a feasible way to reduce the model size and computation cost during inference in SSID.

SECTION: Conclusion
In this paper, we propose a transformer-based blind-spot network, namely TBSN, for self-supervised image denoising.
Key designs are introduced to adapt the spatial and channel self-attention operators for constructing BSN.
For spatial attention, an elaborate mask is applied to the window attention matrix, thus restricting its receptive field to mimic the dilated convolutions.
For the spatial information leakage problem of channel attention, we propose to perform channel attention in separate groups to eliminate its harmful effects.
Moreover, a knowledge distillation strategy is introduced to reduce the computation cost during inference.
Extensive experiments on real-world denoising datasets demonstrate that TBSN largely expands the effective receptive field and achieves state-of-the-art performance.

SECTION: References