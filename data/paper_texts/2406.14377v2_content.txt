SECTION: CE-SSL: Computation-Efficient Semi-Supervised Learning for ECG-based Cardiovascular Diseases Detection

The label scarcity problem is the main challenge that hinders the wide application of deep learning systems in automatic cardiovascular diseases (CVDs) detection using electrocardiography (ECG). Tuning pre-trained models alleviates this problem by transferring knowledge learned from large datasets to downstream small datasets. However, bottlenecks in computational efficiency and detection performance limit its clinical applications. It is difficult to improve the detection performance without significantly sacrificing the computational efficiency during model training. Here, we propose a computation-efficient semi-supervised learning paradigm (CE-SSL) for robust and computation-efficient CVDs detection using ECG. It enables a robust adaptation of pre-trained models on downstream datasets with limited supervision and high computational efficiency. First, a random-deactivation technique is developed to achieve robust and fast low-rank adaptation of pre-trained weights. Subsequently, we propose a one-shot rank allocation module to determine the optimal ranks for the update matrices of the pre-trained weights. Finally, a lightweight semi-supervised learning pipeline is introduced to enhance model performance by leveraging labeled and unlabeled data with high computational efficiency. Extensive experiments on four downstream datasets demonstrate that CE-SSL not only outperforms the state-of-the-art methods in multi-label CVDs detection but also consumes fewer GPU footprints, training time, and parameter storage space. As such, this paradigm provides an effective solution for achieving high computational efficiency and robust detection performance in the clinical applications of pre-trained models under limited supervision. Code and Supplementary Materials are available athttps://github.com/KAZABANA/CE-SSL

SECTION: 1Introduction

Cardiovascular diseases have become the deadliest ’killer’ of human health in recent years[1]. As a non-invasive and low-cost tool, ECG provides a visual representation of the electrical activity of the heart and is widely used in the detection of various CVDs[2,3]. Benefiting from recent progress in computing hardware, ECG-based deep learning systems have achieved notable success in automatic CVDs detection[4,5,6,7]. However, previous deep learning models required sufficient labeled samples to achieve satisfactory performance when trained on new application scenarios with unseen CVDs[8,9]. Unfortunately, collecting well-labeled ECG recordings requires physicians’ expertise and their laborious manual annotation, and therefore is expensive and time-consuming in clinical practice[10,11]. Recent advancements in pre-trained models have enhanced the performance of deep learning models on the downstream datasets without large-scale labeled data[12,13,14]. A commonly used pipeline consists of pre-training over-parameterized backbone models on large-scale datasets and then fine-tuning them on small downstream datasets in a supervised manner. However, two bottlenecks still greatly limit the clinical application of CVDs detection systems based on pre-trained models under limited supervision.

(1) The bottleneck in CVDs detection performance.Fine-tuning of pre-trained models is currently conducted in a purely supervised manner. When the labeled data is very scarce in the downstream datasets, model performance may drop due to over-fitting[15]. Fortunately, a large number of unlabeled data in the medical domain are relatively easy to collect. Semi-supervised learning (SSL) is able to extract sufficient information from the unlabeled data and outperform the supervised models trained with the same amount of labeled data[16,9,17,18,19]. For example, self-tuning integrates the exploration of unlabeled data and the knowledge transfer of pre-trained models into a united framework, which significantly outperforms supervised fine-tuning on five downstream tasks[15]. Despite their robust performance, existing SSL methods are mainly built on pseudo-label techniques and the weak-strong consistency training on unlabeled samples[8,20,9,18,21], which greatly increase the GPU memory footprint and computation time during model training. This drawback results in the bottleneck of computational efficiency during the performance enhancement of pre-trained models using semi-supervised learning.

(2) The bottleneck in computational efficiency for parameter optimization.Nowadays, many studies have introduced large-scale foundation models to achieve better CVDs detection performance using ECG[22,23,24,25,26], greatly increasing the computation costs of modifying them for downstream applications. SSL methods and fine-tuning both update all the model parameters. Despite their effectiveness, both methods have the main drawback that they require saving the gradients of all the parameters and even the momentum parameters, resulting in large GPU memory footprints when tuning large pre-trained models[27]. Additionally, each tuned model can be regarded as a full copy of the original models, therefore leading to high storage consumption when simultaneously tuned on multiple datasets[28]. To address this, parameter-efficient fine-tuning (PEFT) methods have been introduced to reduce the trainable parameters during model training and thus decrease the computational costs during model training[29,30,31]. For example, Low-rank adaptation (LoRA) achieves this goal by updating the pre-trained weights with low-rank decomposition matrices. AdaLoRA and IncreLoRA overcome the performance bottleneck of LoRA by allocating different ranks to different pre-trained weights based on their importance[28,32]. However, the above performance improvement is achieved at the cost of increased training time for iterative importance estimation.

Therefore, a dilemma is encountered: model performance improvement often comes at the expense of a large sacrifice of computational efficiency during model training. Specifically, semi-supervised learning enhances CVDs detection performance under limited supervision but at significantly increased computational costs. Conversely, methods that prioritize computational efficiency may compromise model performance[33]. Consequently, achieving a superior detection performance with high computation efficiency poses a great challenge to the clinical application of pre-trained models in ECG-based CVDs detection. To the best of our knowledge, no prior study has designed and evaluated a framework to escape the dilemma.

Here, we propose a united paradigm capable of addressing the above two bottlenecks simultaneously. It is a computation-efficient semi-supervised learning paradigm (CE-SSL) for adapting pre-trained models on downstream datasets with high computational efficiency under limited supervision. Our method enables robust and low-cost detection of CVDs in clinical practice using ECG recordings. As shown in Fig.1, first, a base backbone is pre-trained on a large-scale 12-lead ECG dataset in a supervised manner, which provides a foundation for downstream datasets. We also provide medium and large backbones for performance enhancement by increasing the backbone’s depth and width. Second, a random-deactivation low-rank adaptation (RD-LoRA) method formulates a low-cost and robust pipeline for updating the pre-trained backbone on downstream datasets. Specifically, it stochastically activates or deactivates low-rank adaptation in each trainable layer of the backbone with a probability. To reduce GPU memory footprint, the pre-trained weights in each layer are always frozen. Theoretical analysis indicates that the random deactivation operation integrates various sub-networks generated during model training, thus overcoming the performance bottleneck in tuning pre-trained models. Additionally, deactivating low-rank adaptation in some layers reduces computation costs and speeds up the training process, especially when the backbone model size is large. Third, a one-shot rank allocation module allocates the optimal ranks for the low-rank matrices in each layer. In contrast to AdaLoRA[28]and IncreLoRA[32], the proposed method can determine the optimal ranks using only one gradient backward iteration, improving the adaptation performance at low computational costs.

Additionally, a lightweight semi-supervised learning module is developed to leverage the abundant information within unlabeled data. This module uses unlabeled data to stabilize the statistics estimation process in batch normalization layers, enhancing their generalization performance on unseen data distributions. Compared to the pseudo-labeling and the weak-strong consistency training methods[20,9,21], our lightweight module alleviates the label scarcity problem with significantly higher computational efficiency.

Finally, extensive experiments on four downstream datasets demonstrate the superior CVDs detection performance of the proposed CE-SSL against various state-of-the-art models under very limited supervision. Most importantly, our method only requires 66.5% training time, 70.7% GPU memory footprint, and 1.8%-5.8% trainable parameters of the state-of-the-art SSL methods. Furthermore, its computational costs can be minimized to adapt to resource-limited environments without a significant accuracy loss. In conclusion, our proposed computation-efficient semi-supervised learning paradigm provides an effective solution to overcome the two bottlenecks that limit the clinical applications of pre-trained models in ECG-based CVDs detection. We summarize the major contributions as follows:

Pre-trained backbones with various sizes are provided to serve as a foundation for ECG-based CVDs detection on downstream datasets.

A random deactivation low-rank adaptation method is proposed to update the backbones with high computational efficiency and robust performance. A one-shot rank allocation module is present to determine the optimal rank distribution during low-rank adaptation at minimal costs.

A lightweight semi-supervised method is proposed to leverage large-scale unlabeled data without greatly sacrificing the computational efficiency.

A computation-efficient semi-supervised framework for low-cost and accurate CVDs detection is proposed, which is the first one to escape the dilemma between model performance and computational efficiency.

SECTION: 2Related Work

SECTION: 2.1AI-Enabled CVDs Prediction using ECG

Benefiting from the development of deep learning, AI-enabled systems have shed light on automatic ECG screening and cardiovascular disease diagnosis[34,4,5,35,36,37,22,23,24]. Tracing the development of the systems, it can be observed that the prediction models they used are continuously scaling up. In the first stage, small-scale models demonstrated promising diagnosis performance in ECG analysis and CVDs detection. For example, Pourbabaeeet al. designed a deep convolutional neural network to extract features from ECG signals and utilize standard classifiers for screening paroxysmal
atrial fibrillation[34]. Hannunet al. proposed an end-to-end deep convolutional neural network to achieve automatic single-lead ECG screening[4]. The results demonstrated that the network achieved similar diagnosis performance compared with common cardiologists. In the second stage, pre-trained models with a prohibitive number of parameters were introduced, which demonstrated better transferability than previous networks. This advantage reduces their requirement for supervision information on downstream datasets. For instance, Vaidet al. pre-trained a large-scale vision transformer (HeartBEiT) on a huge ECG dataset and fine-tuned it on downstream datasets. The experiment results demonstrated the superiority of HeartBEiT in CVDs detection compared with traditional CNN architectures. In the current stage, many studies have proposed various kinds of foundation models for more advanced ECG screening and cardiac healthcare, inspired by their success in natural language processing[23,24]. However, pre-trained models might experience a performance drop on downstream datasets when the labeled samples are very scarce there. Additionally, the computational costs of adapting them to various tasks significantly increase as their sizes scale up.

SECTION: 2.2Semi-Supervised Learning for Performance Enhancement under Limited Supervision.

Semi-supervised learning offers an effective solution to address the label scarcity problem by leveraging unlabeled samples[15,8,9,18,21]. For example, Sohnet al. combined consistency regularization and pseudo-labeling to formulate a powerful algorithm (FixMatch)[9]. Extensive experiments demonstrate the superiority of FixMatch against the supervised baselines under the label scarcity condition. Subsequently, Zhanget al. proposed curriculum pseudo labeling (CPL) to flexibly adjust the thresholds for pseudo label selection, aiming at utilizing unlabeled data based on the model’s training progress[18]. Using a truncated Gaussian function, Chenet al. designed a soft threshold to weight unlabeled samples according to their prediction confidence, which achieved a balance between pseudo labels quality and quantity[21]. Compared with FixMatch, FlexMatch and SoftMatch both demonstrate better performance in various datasets. However, Wanget al. pointed out that the performance of semi-supervised models will be influenced by inaccurate pseudo labels, especially in large label space[15]. Hence, they proposed a self-tuning technique to explore the potential of the transfer of pre-trained models and a pseudo-label group contrast mechanism to increase the model’s tolerance to inaccurate labels. Experiments on five tasks demonstrated the superiority of the proposed framework against previous semi-supervised and supervised methods. In summary, massive unlabeled data and powerful pre-trained models led to the success of semi-supervised methods. However, high computation burdens are the side effects of leveraging them, greatly limiting their applications in resource-limited settings.

SECTION: 2.3Parameter-Effcient Methods for Higher Computational Efficiency.

Parameter-efficient training has demonstrated great potential in decreasing the computational costs of fine-tuning pre-trained models[30,27,38,28]. For example, Zakenet al. proposed BitFit to fine-tune the bias terms of the pre-trained models and freeze the other parameters, greatly reducing the computational costs. However, BitFit sacrifices the performance of the fine-tuned models because most of their parameters are not well adapted to downstream tasks. Huet al. designed a low-rank adaptation method (LoRA) to inject trainable low-rank matrices into the transformer architecture, decreasing the performance gap between parameter-efficient methods and full fine-tuning[27]. However, Zhanget al. pointed out that LoRA ignored the varying importance of different pre-trained weights and allocated the same rank for all the trainable matrices, which led to suboptimal fine-tuning performance[28]. Consequently, they designed AdaLoRA to address this problem, which dynamically allocates different ranks to the low-rank matrices according to their importance during fine-tuning. During this process, the trainable parameters of the matrices with low importance are pruned. Different from AdaLoRA, IncreLoRA adaptively adds trainable parameters to the low-rank matrices with high importance[32]. As a non-pruning method, its performance is not limited by the preset parameter budget. Although IncreLoRA and AdaLoRA surpass LoRA in some scenarios, they result in high computation costs for weight importance estimation. Consequently, advancing fine-tuning performance without sacrificing computational efficiency remains challenging when designing parameter-efficient methods.

SECTION: 3Methodology

SECTION: 3.1Backbone Model Pre-Training

The base backbone model is pre-trained on a public 12-lead ECG dataset (CODE-15%[39,5]), where 345779 ECG recordings from 233770 patients are provided. The medium and large backbones are pre-trained on a restricted dataset with 2,322,513 ECG recordings from 1,558,772 patients (CODE-full[39,40]). The specific settings of the backbone models with different sizes are shown in TableI. Note that multiple abnormalities could be identified from one ECG recording simultaneously, which indicates that a multi-label classification model should be implemented for ECG-based CVDs detection. As shown in Fig.1, The backbone modelconsists of three parts: (1) Convolution blocks, (2) Self-attention blocks, and (3) Classification blocks. Specifically, the convolution blocks comprise multiple convolution layers (Conv) and batch normalization layers. The Leaky-Relu function is used as the activation function and skip-connection is implemented[41]. In addition, a simple but efficient self-attention pipeline is employed in the self-attention blocks[13]and two successive fully-connected layers with sigmoid activation are used for label prediction in the classification block. A multi-label binary cross-entropy function is employed for model training, defined as,

whereare the ECG recordings in the current mini-batch,is the signal length andis the corresponding ground truths.is the model prediction on classandis the number of categories. During model training, a held-out validation set is used for early-stop model validation. The best-performing model on the validation set is used for downstream tasks on small-scale datasets.

SECTION: 3.2Random-Deactivation Low-Rank Adaptation

Recent studies have demonstrated that low-rank adaptation (LoRA) can drastically decrease computation and storage costs in large-scale neural network fine-tuning while achieving promising performance on downstream tasks[27,28,33]. The LoRA method models the incremental update of the pre-trained weights by the matrix multiplication of two low-rank matrices. For a hidden layer output, the LoRA forward process is defined as,

where,and, and the rank. The LoRA freezes the pre-trained weightduring model training and only optimizes the low-rank matricesand, which greatly reduces the number of trainable parameters during model training[27]. However, the incremental updates of low-rank matrices are inadequate for achieving optimal performance on downstream datasets[42,32]. To bridge the performance gap efficiently, we propose a novel random-deactivation low-rank adaptation (RD-LoRA) method, which randomly activates or deactivates the low-rank matrices in each trainable layer with a given probability. To be specific, the forward process of the proposed RD-LoRA can be defined as,

wherecan be regarded as a binary gate controlled by a random variablefollowing a uniform distribution. In the training stage, the multi-label binary cross-entropy loss defined in Eq.1is employed for parameter optimization. In the testing stage, for input dataand the pre-trained weight, the expectation of the output of the RD-LoRA can be given as,

Considering that the test datais independent from the network parameters, the covariance matrixequals zero, and thus,

Similar to LoRA, the low-rank matrices are merged into the pre-trained weightin the testing stage to avoid extra inference costs, and the random-drop operation is deactivated. According to Eq.(5), to ensure the expected output will be the same as the output with RD-LoRA, the merged matrix should be computed as,

After merging the low-rank matrices into the pre-trained weights of different layers, the final network can be viewed as an ensemble of all possible sub-networks during model training, which improves its stability and generalization performance on unseen test datasets. Additionally, randomly deactivating some low-rank matrices avoids the computation of update matrices in some layers, which improves training speed in the low-rank adaptation of large-scale models.

SECTION: 3.3Ensemble Optimization Properties of the RD-LoRA

In this section, we briefly analyze the ensemble properties of the proposed RD-LoRA. Here, we simply consider a networkwithfully-connected layers, defined as, whereis the input data andis the pre-trained weight matrix at the-th layer. During model training, a convex loss functionis employed for parameter optimization. When the RD-LoRA is activated, the expectation of the loss functioncan be given as,

where the low-rank matricesandare trainable while the pre-trained weightsare frozen. Eq.(7) can be regarded as a weighted mean of the losses of different sub-networks, which are minimized during model training. The number of activated low-rank matrices of the sub-networks is lower than the entire network. Consequently, the training costs of the sub-networks are lower than those of the entire network. In the testing stage, all the low-rank matrices are merged into the pre-trained weights, which generates an ensemble model combing all the possible sub-networks. The testing loss can be estimated as

In this paper, the multi-label binary cross-entropy loss with sigmoid activationis convex according to the second-order condition of convexity, whereis the number of categories. Specifically, the Hessian matrix ofis diagonal and the-th element of the main diagonal can be given as,

where,and. According to Eq (9), the Hessian matrix ofis positive semidefinite, demonstrating the convexity of the loss function. Based on Jensen’s inequality, the loss of any
ensemble average is smaller than the average loss of the ensemble components,

In the training stage, the proposed RD-LoRA optimizes the parameters of multiple sub-networks and generates an ensemble network in the testing stage, improving the model performance on the testing data.

SECTION: 3.4Efficient One-Shot Rank Allocation

Another limitation of LoRA is that it prespecifies the same rank for all low-rank incremental matrices, neglecting that their importance in model training varies across layers. In response to this limitation, AdaLoRA[32]and IncreLoRA[28]proposed to dynamically adjust the ranks of different incremental matrices during model training based on their importance, which improved the low-rank adaptation performance. However, these dynamic methods require continuous calculation of the importance of all low-rank matrices in each iteration, significantly increasing the computation time. Additionally, their rank allocation processes are based on the singular value decomposition (SVD) theory and thus require an extra regularization loss to force the orthogonality of the low-rank matrices. This property introduces extra hyper-parameters and computation costs. Here, we propose an efficient one-shot rank allocation method to overcome the computation inefficiency of the existing dynamic methods.

Above all, we introduce some preliminaries about how to estimate the importance of weights in neural networks. Based on the first-order Taylor expansion, the importance of a weight matrix can be computed by the error induced by removing it from the network[43], defined as,

whereis the-th element in the weight matrix,is the number of elements inandis the Hadamard product. However, the gradient matrixcan not be obtained becauseis frozen during the low-rank training process. To solve this problem, we approximate it using its incremental update, which can be computed by low-rank matricesandusing the Eq.(2).

whereandare the low-rank matrices at training round, constantis the learning rate andis the pre-trained weight. Although Eq.(12) enables importance score estimation during model training, iterative matrix multiplication induces a heavy computation burden. Hence, we propose to simplify the estimation function Eq.(12) and compute the importance score in a ’one-shot’ manner. Specifically, we only use the first gradient-backpropagation process to achieve the entire rank allocation process and fix the ranks of different low-rank matrices during the remaining training iterations. In the first backpropagation process, the low-rank matricesare initialized from a normal distributionandare initialized to zero. Consequently, the gradient ofat the-th (first) iteration is zero according to Eq.(2). Based on the above initialization conditions, Eq.(12) at the-th iteration can be rewritten as,

whereare the pre-trained weight matrices in the backbone model. Then, the importance score of the pre-trained weightcan be approximated as,

Then, we sort the importanceof all pre-trained matrices in descending order and allocate different ranks for their low-rank matrices. Here, we assume the ranks of the incremental matrices corresponding to the important weights should be higher than those of the incremental matrices associated with the unimportant weights. The allocated rankof the incremental matrices of the pre-trained weightis defined as,

whereis an initial rank, andis a hyper-parameter that controls the number of important weight matrices.
Note that the allocated ranksare fixed during the remaining iterations, and the low-rank matrices (,) are reset based on their allocated ranks. Eq.(14) is only computed at the 0-th iteration, which avoids numerous matrix multiplication. In addition, the proposed rank allocation process does not require constraint on the orthogonality of low-rank matrices. In summary, the above advantages allow the proposed method to have a faster training speed compared to existing dynamic methods, such as AdaLoRA[32]and IncreLoRA[28].

SECTION: 3.5Lightweight Semi-Supervised Learning

Semi-supervised learning (SSL) is an efficient tool for model performance enhancement when large-scale unlabeled data is available[44,8]. Recently, many studies utilized label guessing and consistency regularization to further improve the model performance in SSL tasks, such as FixMatch[9], FlexMatch[18]and SoftMatch[21]. However, the above two techniques require the output predictions of the weak and strong-augmented unlabeled samples, which induces extra computation costs. Consequently, traditional SSL methods usually exhibit much higher memory costs and longer training time than naive supervised models. Here, we introduce a lightweight but effective SSL method without extensive consistency training and pseudo-label guessing.

The main drawback of fully-supervised learning on small datasets is over-fitting, while semi-supervised learning alleviates this problem by utilizing large-scale unlabeled data. However, the heavy computation burden limits their application in real-world scenarios. Consequently, a natural question is: How can we alleviate the over-fitting problem in a lightweight but effective manner? Our solution is to update the batch normalization (BN) layers in a semi-supervised manner using both labeled and unlabeled data. Subsequently, the unlabeled data is released, and only the labeled data is forwarded to the self-attention and classification blocks for loss computation.For labeled inputsand unlabeled inputs, the mean valueand the varianceof the semi-supervised BN layers in the convolution blocks can be updated as,

whereandare the numbers of labeled and unlabeled samples in the current mini-batch, and. Note thatequalsin this study, thus. With only limited labeled data, the estimated meanand variancein traditional BN are prone to be influenced by the over-fitting problem according to the law of large numbers. On the contrary, the proposed semi-supervised BN alleviates the problem by utilizing large-scale unlabeled datafor parameter estimation, which improves the model performance on unseen distributions.Since the BN layers do not exist in the self-attention and classification blocks, we only forward the labeled features to them to reduce memory cost and training time. Compared with the SOTA methods in semi-supervised learning, the proposed CE-SSL discards the label guessing and the consistency regularization modules. However, the results demonstrate that it achieves similar or even better performance on four datasets. More importantly, its computation costs are much lower than those of the SOTA methods, including less memory consumption and faster training speed.

SECTION: 3.6Signal Pre-Processing and Data Augmentation

Artifact removal and data augmentation are two factors that play important roles in model performance. Firstly, we introduce the signal pre-processing pipeline employed in the proposed framework. The ECG recordings from the CODE-15% and CODE-full databases are first resampled to a 400Hz sampling rate following the configuration of the dataset provider[5]. The sampling rate of the recordings from the four downstream databases remains unchanged. Firstly, the length of all recordings is normalized into 6144 samples by zero-padding. Subsequently, a band-pass filter (1-47Hz) is applied to remove the power-line interference and baseline drift. Then, the pre-processed signals are normalized using z-score normalization. Secondly, CutMix[45]is employed for labeled data augmentation. Since the sample generation process of CutMix requires true labels that are absent in the unlabeled data, we employed the ECGAugment[11]for unlabeled data augmentation, which generates new samples by randomly selecting a transformation to perturb the pre-processed signals. Note that only the weak-augmentation module in the ECGAugment is employed.

SECTION: 4Experiments and Datasets

In this section, we utilized two large-scale datasets to pre-train the backbone and evaluated the performance of our CE-SSL on four downstream datasets. First, an openly available ECG dataset (CODE-15%) collected by the Telehealth Center of the Universidade Federal de Minas Gerais[39,5]is used for the base backbone pre-training. The CODE-15% dataset contains 15% of the ECG data from the restricted CODE-full dataset[5,40], which are used for pre-training the medium and large backbones. Specifically, there are 345779 ECG recordings from 233770 patients in the CODE-15% dataset, alongside six CVD labels: 1st degree AV block (1dAVb), left bundle branch block (LBBB), right bundle branch block (RBBB), sinus tachycardia (ST), atrial fibrillation (AF) and sinus bradycardia (SB). Each ECG recording lasts 7-10s, and the sampling rate is 300-600 Hz. Subsequently, we use four small datasets for downstream model retraining and evaluation: the Georgia 12-lead ECG Challenge (G12EC) database[46], the Chapman-Shaoxing database[47], the Ningbo database[48], and the Physikalisch-Technische Bundesanstalt (PTB-XL) database[49]. Specifically, the G12EC database contains 10344 ECG recordings from 10,344 people, and the PTB-XL database comprises 21837 recordings from 18885 patients. The Chapman database contains 10,646 recordings from 10646 patients, and the Ningbo database encompasses 40258 recordings from 40258 patients. Only 34,905 recordings in the Ningbo database are publicly available[46]. The recordings from the four downstream databases are around 10 seconds, and the sampling rate is 500 Hz. Additionally, each database contains over 17 different CVDs, and multiple CVDs can be identified from one ECG segment simultaneously.

The base backbone model is first pre-trained on the CODE-15% dataset using the ECG recordings and the corresponding multi-label ground truths. The architecture
of the backbone is shown in Fig.1. AdawW optimizer[50]is used for the pre-training process with a learning rate of 1e-3 and a batch size of 1024. Then, the pre-trained backbone is retrained on the four downstream datasets using different methods under limited supervision. Taking the G12EC database as an example, the ECG recordings are split into a training set and a held-out test set in a ratio of 0.9: 0.1. Then, the training set is divided into a labeled training set and an unlabeled training set in a ratio of 0.05: 0.95. A validation set is randomly sampled from the labeled training set and accounts for 20% of it, which is used for selecting the best-performing model during training. For model comparisons, we reproduce several baseline models in semi-supervised learning: ReMixMatch[20], FixMatch[9], FlexMatch[18], SoftMatch[21], MixedTeacher[10], Adsh[51], SAW[52]. Additionally, we integrate the state-of-the-art parameter-efficient methods (LoRA[27], DyLoRA[38], AdaLoRA[28], IncreLoRA[32]) with FixMatch for comparisons.

We comprehensively evaluate the model performance of various methods using multiple metrics and training costs. Since multiple CVDs can be detected from one recording simultaneously, we used metrics on multi-label classification. Our metrics include ranking loss, coverage, mean average precision (MAP), macro AUC, macroscore, and macroscore. We set thevalue to be 2 for all the corresponding experiments following the configurations provided in ref.[35]. Lower values indicate better model performance in the first two metrics (ranking loss and coverage), whereas the inverse holds for the remaining metrics. A detailed computation process for the metrics can be found in Supplementary Materials Section 1. Additionally, we report the training costs of different models. Specifically, the peak GPU memory footprint during model training (Mem), the number of trainable parameters (Params), and the average training time for each optimization iteration (Time/iter) are presented. The higher the number of trainable parameters, the higher the parameter storage consumption. Note that the number of trainable parameters of CE-SSL can be adjusted by the initial rank. Lower ranks indicate fewer trainable parameters. The AdamW optimizer[50]is used under a learning rate of 1e-3 and a batch size of 64 for all the compared methods. All the experiments are conducted in a single NVIDIA A6000 graphics processing unit using the Pytorch library.

SECTION: 5Results and Discussion

SECTION: 5.1Analysis of the CVDs Detection Results

TableIIshows that our proposed CE-SSL achieved superior detection performance on four datasets with the lowest computational costs compared with the baseline models. For example, in the G12EC dataset, CE-SSL withachieves a macroof 0.5510.017, which is 4.1% larger than the second-best model’s (FixMatch) performance. In Supplementary Materials Section 2, we present the detection performance of different models on each CVD. The results demonstrate that CE-SSL ranks the best in some CVDs, such as atrial fibrillation and first-degree AV block. It also achieves comparable performance to the compared methods in the remaining CVDs. Regarding computational costs, it requires 33.5% less training time than MixedTeacher, occupies 29.3% less GPU memory than Adsh, and has only 5.8% of the trainable parameters found in them. When the initial rankdecreases to 4, CE-SSL shows a slight performance drop in four datasets, but the number of trainable parameters further decreases to 1.8% of the baseline models. This observation indicates the stability and robustness of the CE-SSL under extremely low parameter budgets.

We further compare the proposed CE-SSL with the parameter-efficient methods, which are integrated with FixMatch for parameter-efficient semi-supervised learning. For example, FixMatch with low-rank adaptation (LoRA) is denoted as ’FixMatch+LoRA’. Similar to the CE-SSL, their budgets for the number of trainable parameters are controlled by the initial rank. As illustrated in Fig.2, we report their macro-scores, macro-scores, and Time/iter on four datasets at sufficient () and limited () budget levels. The macro-and macro-scores of the FixMatch without parameter-efficient training (Params: 9.505M) are denoted as gray dotted lines. The experiment results indicate that CE-SSL consistently outperforms the other methods on four datasets at different budget levels. Under a sufficient parameter budget (), CE-SSL achieves a macro-score of 0.3070.016 on the G12EC dataset, which is 2.8% higher than the FixMatch with LoRA. When the parameter budget is limited (), CE-SSL still outperforms it by 1.5%. Additionally, CE-SSL achieves the highest training speed and the best performance with the least trainable parameters compared to other parameter-efficient semi-supervised learning frameworks. On the four datasets, CE-SSL requires 50% less training time compared to other methods. This phenomenon demonstrates the computational efficiency of the proposed CE-SSL. In Supplementary Materials Table S8, we present detailed comparison results on more evaluation metrics, which provide supplementary evidence on the efficiency of the proposed CE-SSL in CVDs detection. Paired t-tests are conducted to evaluate the significance levels of the performance difference between CE-SSL and the aforementioned SOTA methods (Supplementary Materials Section 3 Fig. S1). Based on the calculated two-sided-value, it can be observed that CE-SSL outperforms the baselines at a 0.05 significance level in most datasets and evaluation metrics, which indicates a significant superiority for the proposed CE-SSL framework.

In summary, our experiment results demonstrate the robustness and computational efficiency of the CE-SSL in semi-supervised cardiovascular disease detection at different parameter budget levels. In other words, CE-SSL can enhance the detection performance of ECG-based CVDs detection models without introducing heavy computation burdens.

SECTION: 5.2Effect of the Ratio of Labeled Samples

Here, we compare the proposed CE-SSL and baseline models under various ratios of labeled samples in the datasets. Specifically, we adjust the ratio of the labeled samples in the dataset from 5% to 15% and present the averaged performance of different models on the four datasets in Fig.3. The experiment results demonstrate the superiority of the proposed CE-SSL compared with FixMatch and FixMatch with LoRA under various ratios of the labeled data, especially when the ratio is low. As the ratio decreases from 15% to 5%, the performance advantage of CE-SSL over other models becomes more significant. When using 15% labeled data, CE-SSL achieves improvements of 1.3% on the macrocompared to FixMatch with LoRA. In contrast, CE-SSL outperforms it by 1.9% on the macrousing 5% labeled data. In Supplementary Materials Fig. S4, we also compare CE-SSL with other baseline models, where CE-SSL consistently outperforms them in CVDs detection under various labeled ratios.

SECTION: 5.3Rank Initialization in the One-Shot Rank Allocation

Rank initialization is an important component in low-rank adaptation, which controls the number of trainable parameters during model training. In this section, we adjust the initial rank from 4 to 32 and present the averaged model performance on the four datasets in Fig.4. Note that the labeled ratio is set to 5%. The results indicate that CE-SSL with high initial ranks () achieves better performance than that with low initial ranks (). This is because the model with higher ranks has more trainable parameters and thus demonstrates a larger capacity during training.

SECTION: 5.4Effect of the Number of Important Weight Matrices

Based on the proposed one-shot rank allocation, CE-SSL allocates a rankto the incremental matrices with high importance and a rankto the matrices with low importance. The ratio of the important matrices to the total number of pre-trained matrices is defined as the coefficient. The higher the coefficient is, the higher the ratio of the important matrices. In Fig.5, we adjust the coefficient from 0.2 to 0.8 and report the averaged model performance across four datasets. Note that the labeled ratio is set to 5%, and the initial ranksfor all the low-rank matrices are set to 16. It can be observed that the performance of the proposed model is relatively insensitive to the changes in the. In Supplementary Materials Fig. S5, we visualize the rank distribution generated by the proposed method under various coefficients. When the ratio of important matrices decreases from 0.8 to 0.2, the proposed method allocates more ranks to the self-attention and classification blocks than the convolution blocks. This phenomenon indicates that the deep modules exhibit higher importance than the shallow modules during model training, which aligns with the conclusions made by previous studies[53,28].

SECTION: 5.5Effect of the Deactivation Probability

For each pre-trained weightin the CE-SSL, the proposed RD-LoRA deactivates its low-rank matrices () in the current iteration at a probability of, which produces multiple sub-networks during model training. All the low-rank matrices are activated in the testing stage, generating an ensemble network that combines all the sub-networks. Consequently, the probabilityis an important parameter that controls the training time and the final performance of the proposed CE-SSL. In Fig.6, we adjustfrom 0.1 to 0.5 and present the averaged model performance across four datasets, including the training time for each iteration. Note that the labeled ratio is set to 5%, and the initial ranks for all the low-rank matrices are set to 16. The results show that the CE-SSL withdemonstrates the best detection performance compared with the model with other settings. In addition, it can be observed that the training time of the CE-SSL decreases asincreases. The reason is that the larger theis, the more low-rank matrices are deactivated during model training, which speeds up the forward-backward propagation.

SECTION: 5.6Performance Comparisons under Various Backbone Sizes

In the previous sections, we have already proved the robustness and computation efficiency of the proposed CE-SSL under a base backbone with 9.505 million parameters. Here, we compare its performance with other baseline models under medium and large backbones, which share the same architecture as the base backbone but have more parameters (TableI). Specifically, the medium backbone has 50.494 million parameters, and the large backbone has 113.490 million parameters. They are pre-trained on the CODE-full dataset, a huge but restricted ECG dataset with 2,322,513 ECG recordings from 1,558,772 patients[39,5]. In Supplementary Materials Table S9 and Table S10, we report the performance of CE-SSL and semi-supervised baselines on the medium and the large backbones, respectively. The results demonstrate that CE-SSL achieves similar and even better CVDs detection performance than the semi-supervised baselines and exhibits the lowest computation costs. For example, using the medium backbone, CE-SSL achieves a macroof 0.5990.010, which is 3.7% larger than the second-best model’s (SAW) performance in the PTB-XL dataset. Using the large backbone, CE-SSL achieves a macroof 0.5650.010 in the G12EC dataset, outperforming SAW by 3.1%. Regarding the computational costs, the number of trainable parameters of CE-SSL is 0.9% to 3.1% of the other baselines on the medium backbone and 0.6% to 2.1% on the large backbone. In addition, CE-SSL demonstrates the lowest GPU memory consumption and the highest training speed compared to the other semi-supervised baselines. For the memory footprint, CE-SSL achieves an average GPU memory usage of 6.16 GB using the medium backbone and 9.22 GB using the large backbone, 3.09 GB and 4.59 GB less than the second-best model (Adsh). Furthermore, CE-SSL achieves an average training time per iteration of 259.25 ms using the medium backbone and 485.5 ms using the large backbone, 162.5 ms and 289.75 ms faster than the second-best model (MixedTeacher). These phenomenons demonstrate that as the number of model parameters increases, the computational efficiency advantage of CE-SSL over other models becomes increasingly apparent.
In Supplementary Materials Table S11 and Table S12, we present the performance of CE-SSL and parameter-efficient semi-supervised methods on the medium and large backbones, respectively. It can be observed that CE-SSL outperforms the other models in CVDs detection on both medium and large backbones. Additionally, CE-SSL demonstrates the fastest training speed across four datasets compared with other parameter-efficient methods. In Supplementary Materials Section 3 Fig. S2-S3, we provide the paired t-test results of the model performance on the two backbones. The statistical results indicate that CE-SSL outperforms the above baselines in ECG-based CVDs detection at a 0.05 significance level in most conditions.

SECTION: 5.7Toward Higher Computational Efficiency in Clinical Practices

Although deploying the CE-SSL paradigm with the base backbone on low-level devices (4-6 GB GPU memory) is easy, implementing the paradigm with the medium and large backbones is still challenging. To overcome this limitation, we adopt a simple but effective approach to boost the computational efficiency of the CE-SSL. Specifically, we freeze the first two convolution blocks in the backbones during the CE-SSL training process. The new paradigm is denoted as ’CE-SSL-F’ in the following analysis. We present the CVDs detection performance and the computational efficiency of CE-SSL-F, CE-SSL, and the SOTA methods in semi-supervised learning in Fig.7. Note that the batch sizes for all the compared methods are set to 64.

First, freezing the convolution blocks greatly reduces the cached activation during the forward pass, significantly decreasing the GPU memory footprints. As shown in Fig.7a, it can be observed that the CE-SSL-F requires nearly 50% less GPU memory footprints compared to the CE-SSL, generalizing its applications in low-level devices (NVIDIA RTX 3050 laptops and RTX 4060 GPU cards). Specifically, CE-SSL-F is deployable on RTX 3050 laptops with both base and medium backbones, and it is the only method that can be implemented on the RTX 4060 GPU cards with a large backbone. In contrast, deploying the CE-SSL with a large backbone requires medium-level devices (NVIDIA RTX 4070 GPU cards), while other semi-supervised methods require high-level devices with GPU memory larger than 12 GB. Second, the parameters of the frozen blocks are not updated during the backward pass, which increases the training speed of CE-SSL-F. The larger the backbone is, the more parameters are frozen, and thus the more gradient backward time is saved. As shown in Fig.7b, CE-SSL-F demonstrates the fastest training speed compared with other models, and its advantages become more significant along with the increase in backbone sizes. Third, CE-SSL-F only sacrifices 1-2% CVDs detection performance compared with CE-SSL. More importantly, it consistently outperforms the other semi-supervised methods across different backbones (Fig.7c), demonstrating its effectiveness in CVDs detection. This phenomenon can be explained by the strong transferability of the pre-trained convolution blocks located in the first few layers of the backbone[54,55]. Specifically, they mainly contain domain-invariant knowledge for CVDs detection, and their parameters will not be changed significantly during the fine-tuning process. Therefore, freezing them does not greatly decrease the model performance.

In summary, the experiment results illustrate that the computational efficiency of the CE-SSL can be increased to adapt to low-level devices without losing its superior CVDs detection performance compared to other semi-supervised methods. This advantage demonstrates CE-SSL’s flexibility in different clinical application scenarios with various computational resources.

SECTION: 5.8Ablation Study

We conducted an ablation study to evaluate the contribution of the modules implemented in CE-SSL. Specifically, we remove each of the proposed three modules from the CE-SSL and record the corresponding model performance on the four datasets. Note that the initial rankis 16 for all the compared models.(1) The random-deactivation low-rank adaptation improves model performance and computational efficiency.It randomly deactivates the low-rank matrices in each trainable layer with a given probability, which generates multiple sub-networks during model training. All the low-rank matrices are activated in the testing stage, and the sub-networks are merged into a robust ensemble network. In the G12EC dataset, the CE-SSL with the random deactivation module increases the macrofrom 0.5360.021 to 0.5510.017 and decreases the ranking loss from 0.0950.004 to 0.0920.002. Additionally, the Time/iter is reduced from 104 ms to 98 ms. This observation indicates that the computation cost of training a sub-network with fewer low-rank matrices is lower than optimizing the entire network.(2) The one-shot rank allocation method improves low-rank adaptation performance with high computational efficiency.One drawback of LoRA[27]is its inability to allocate an optimal rank for each incremental matrix[28,32]. Here, the proposed rank allocation method determines the optimal ranks using only one gradient-backward iteration with high computational efficiency. As shown in TableIII, removing the proposed module decreases the detection performance of CE-SSL in the four datasets. For example, the macrodecreases from 0.5300.008 to 0.5140.018, and the coverage increases from 2.4830.055 to 2.5030.040 on the Chapman dataset. Additionally, it can be observed that the one-shot rank allocation module does not introduce heavy computational burdens (Time/iter only increases by 1-2ms), demonstrating its high computational efficiency. (3)The proposed lightweight semi-supervised learning benefits model performance under limited supervision without greatly increasing the training time.It utilizes the unlabeled data to stabilize the statistics within the BN layers in the convolution blocks, preventing them from over-fitting to small amounts of labeled data. When the module is removed, the performance of the CE-SSL decreases on all the datasets. For example, the macroscore decreases from 0.5510.017 to 0.5360.029 on the G12EC dataset. Compared to other semi-supervised baselines (TableII), the extra computation costs (Time/iter only increases by 20ms) caused by the proposed module are much lower. In Supplementary Materials Section 3, ablation studies on the medium and large backbones are provided, consistently demonstrating the notable contribution of the proposed three techniques.

SECTION: 6Conclusion

Bottlenecks in model performance and computational efficiency have become great challenges in the clinical application of CVDs detection systems based on pre-trained models, especially when the supervised information is scarce in the downstream ECG datasets. Previous studies usually overcome the performance bottleneck at the cost of a large drop in computational efficiency[15,28,32,19]. In this paper, we propose a computationally efficient semi-supervised learning paradigm (CE-SSL) for adapting pre-trained models on downstream datasets with limited supervision and high computational efficiency. Experiment results on four downstream ECG datasets and three backbone settings indicate that CE-SSL achieves superior CVDs detection performance and computational efficiency compared to state-of-the-art methods. In conclusion, our study offers a fast and robust semi-supervised learning paradigm for ECG-based CVDs detection under limited supervision. It provides a feasible solution for efficiently adapting pre-trained models on downstream ECG datasets. We hope this learning paradigm will pave the way for the application of automatic CVDs detection systems and broaden its applicability to various ECG-based tasks.

SECTION: 7Limitation and Future Work

Apart from the label scarcity problem, the class imbalance problem also limits the model performance on ECG recordings with rare CVDs. Specifically, it tends to make negative predictions on the rare CVDs to achieve minimal training loss, which might result in sub-optimal classification performance on the test data with different class distributions. As such, developing a robust method for multi-label CVDs classification under imbalance class distributions is still an ongoing issue that deserves more attention from future studies.

SECTION: Acknowledgments

This work was supported by an RAEng Research Chair, the InnoHK Hong Kong projects under the Hong Kong Center for Cerebro-cardiovascular Health Engineering (COCHE), the National Natural Science Foundation of China (22322816) and the City University of Hong Kong Project (9610640).

SECTION: References