SECTION: Context-Aware Aerial Object Detection: Leveraging Inter-Objectand Background Relationships

In most modern object detection pipelines, the detection proposals are processed independently given the feature map. Therefore, they overlook the underlying relationships between objects and the surrounding background, which could have provided additional context for accurate detection. Because aerial imagery is almost orthographic, the spatial relations in image space closely align with those in the physical world, and inter-object and object-background relationships become particularly significant. To address this oversight, we propose a framework that leverages the strengths of Transformer-based models and Contrastive Language-Image Pre-training (CLIP) features to capture such relationships. Specifically, Building on two-stage detectors, we treat Region of Interest (RoI) proposals as tokens, accompanied by CLIP Tokens obtained from multi-level image segments. These tokens are then passed through a Transformer encoder, where specific spatial and geometric relations are incorporated into the attention weights, which are adaptively modulated and regularized. Additionally, we introduce self-supervised constraints on CLIP Tokens to ensure consistency. Extensive experiments on three benchmark datasets demonstrate that our approach achieves consistent improvements, setting new state-of-the-art results with increases of 1.37 mAP50on DOTA-v1.0, 5.30 mAP50on DOTA-v1.5, 2.30 mAP50on DOTA-v2.0 and 3.23 mAP50on DIOR-R.

SECTION: 1Introduction

Object detection has been one of the most studied problems in computer vision due to its great value in practical applications ranging from surveillance and autonomous driving to natural disaster management. The field has seen impressive advancements due to novel models and training techniques developed in the past few years[19,25,1]. Among various domains, object detection in aerial images stands out with characteristics and challenges different from those presented in natural images: objects are distributed with drastically varying scales, orientations, and spatial densities.

To tackle this challenge, prior works proposed to improve the detection performance from different perspectives, achieving various degrees of success. Many efforts have focused on learning more appropriate features by exploiting the geometric properties, e.g., symmetry and rotational invariance, leading to novel architectures and data augmentation techniques[11,34,38,39,23]. Others have developed metrics and objectives[41,40,42]that better capture the nuances of aerial object detection.

Nevertheless, despite these advancements, most present-day detection models classify and localize objects independently[11,34,38], possibly due to the lack of an effective tool for modeling the co-presence of an arbitrary number of objects in an image. In other words, the spatial and semantic relationships among objects are not fully captured, often leading to false detections that overlook surrounding contextual dependencies and inter-object dynamics. As a motivating example,Fig.1illustrates the challenge of detecting each object instance based solely on its features, without considering these critical relationships. Aerial images, in particular, offer a unique setting where objects generally share the same plane, with little occlusion and perspective distortion, and therefore have stable inter-object and surrounding context relationships. Meanwhile, we posit that knowledge of an object’s background context can provide useful information and therefore significantly improve detection. For instance, an area that appears as a green field might be presumed a playground; however, if adjacent to an airport, such an assumption would be reconsidered. Unfortunately, most datasets lack annotations for background information. The semantics of the background are complex and difficult to annotate due to the highly irregular spatial distribution.

In this paper, we propose a Transformer-based model on top of two-stage detectors to effectively capture and leverage the inter-object relationships and semantic background context. Concretely, we organize the Region of Interest (RoI)[9,26]feature maps proposed in the first stage and the independent detection results on them into embeddings. The embeddings are then fed into a transformer where the features of candidate detections interact and aggregate. However, the self-attention module in ordinary Transformers, which computes the pairwise attention weights as dot products of embeddings, does not capture the spatial and geometric relationship directly. To overcome this, we design and incorporate additional encodings and attention functions, weighing the mutual influence between objects according to distances. The attention functions are adaptive to the scales and densities of the object distribution, which is crucial for the model to generalize across different image scenarios.

To further incorporate object-background relationships, we leverage CLIP[24], a powerful multimodal model renowned for its cross-modal understanding capabilities, to integrate background information into detectors. Utilizing the image and text encoders of a pre-trained CLIP model, we divide the image into patches to be queried by pre-specified descriptions and then cast them as tokens alongside the RoI tokens.

Aerial images offer complex scenes with numerous objects on a single plane, where spatial and inter-object relationships are more explicit. The richer background context in such scenarios further supports the strengths of our approach.We validate the effectiveness of our method through comprehensive experiments on DOTA-v1.0, DOTA-v1.5, DOTA-v2.0[33], and DIOR-R[6], achieving an improvement of 1.37, 5.30, 2.30 and 3.23 mAP50over the baseline.

Our main contribution can be summarized as follows:

We introduce a novel Transformer-based model that extends the capability of two-stage detectors, enabling the effective encapsulation and utilization of inter-object relationships in aerial image detection.

We propose to use CLIP for integrating background context into the detection pipeline. We introduce multi-scale, hierarchical CLIP patches, generating CLIP Tokens and facilitating the flow of semantic information across different levels, thereby improving information fusion.

Our model innovatively incorporates additional encodings and attention mechanisms that directly address spatial and geometric relationships, enhancing adaptability to the varying scales and densities in object distribution, a critical step forward for generalization in diverse aerial scenarios.

SECTION: 2Related Works

SECTION: 2.1Aerial Object Detection

In the realm of aerial object detection, extensive research has been conducted to tackle the unique challenges posed by the diverse characteristics of aerial imagery. Numerous studies have explored both single-stage and two-stage methodologies. Notable two-stage methods include ReDet[11], which focuses on handling scale, orientation, and aspect ratio variations, Oriented RCNN[34]that introduces improved representations of oriented bounding boxes, and SCRDet[38]designed for addressing the challenges of dense clusters of small objects. Additionally, SASM[14], Gliding vertex[37], andRegion of Interest(RoI) Transformer[7]have contributed to the advancement of two-stage approaches. On the other hand, single-stage methods such as R3Det[39], S2ANet[10], and DAL[22]have been developed, demonstrating the diversity of strategies employed in the pursuit of efficient aerial object detection. These methodologies often incorporate modifications to convolution layers, novel loss functions like GWD[40], KLD[41], and KFIoU[42], as well as multi-scale training and testing strategies to enhance the robustness of object detection in aerial imagery. The evolving landscape of aerial object detection research reflects the ongoing efforts to address the complex challenges inherent in this field. In addition, ReDet[11]and ARC[23]modified the convolution layers to explicitly cope with rotation.

SECTION: 2.2Capturing Inter-Object Relationships

Common detection systems handle candidate object instances individually. They implicitly assume that the distribution of objects in an image is conditionally independent, which is generally false in reality. Transformer-based architecture[28,8]has shown impressive capability in relational modeling across multiple domains. To address the oversight mentioned above,[15]introduced an object relation module that computes attention[28]weights from both geometric and appearance features of object proposal. The module is also responsible for learning duplicate removal in place of Non-Maximum Suppression (NMS), leading to an end-to-end detector. More recently, DETR[2,13]formulates detection as a set-prediction problem and sets up object queries that interact with each other in a Transformer-decoder[28]. Its successors[4,27]improved the framework’s efficiency by operating directly on features instead of object queries. Graph Neural Networks have also been explored as a powerful alternative in relation modeling for object detection and scene understanding. Typically, one constructs the graph with the objects being the nodes and the spatial relations as edges[16,43,5].[36]instead models region-to-region relations with learned edges. They also differ in how edges are obtained. In comparison to prior works, our method focuses on aerial images where the inter-object relationships are stable, with a more explicit design.

SECTION: 2.3CLIP Features

The CLIP (Contrastive Language-Image Pretraining)[24]model is trained on a large corpus of image-text pairs and could be used to extract semantic information from images. CLIP has promoted the field of computer vision and is widely applicated in traditional vision tasks[24]. For example, the model and concepts of CLIP have been applied to a variety of other visual tasks, such as object detection, video action recognition and scene graph generation, showcasing its broad applicability and adaptability[32,44,30,29]. Its zero-shot classification capability allows it to accurately categorize images without additional fine-tuning on specific datasets. This feature is particularly useful in tasks and fields where labeled data is scarce.
Building upon the CLIP model, RegionCLIP[45]focuses on specific image regions for detailed semantic analysis. By focusing on distinct image regions, RegionCLIP can provide more precise and contextually relevant semantic information, which is critical for tasks that require the understanding of spatial relationships and localized features.
In addition to CLIP and RegionCLIP, uniDetector[31]shows the advancement of leveraging the rich information from both visual and language modalities.
As an object detection model, uniDetector combines the global context of transformers with the local feature extraction of CNNs, which is beneficial for visual tasks that need to handle objects of varying orientations and scales. Besides, the rich information from both visual and language modalities endows uniDetector with the ability to recognize open-vocabulary objects.

SECTION: 3Methodology

We build our method on the two-stage object detection framework presented in[11]. We start by following the original pipeline to obtain features and preliminary detections, which are then transformed intoRoI tokens. Then we segment images into multi-scale patches and use CLIP to generate multi-level CLIP features, and then transform them intoCLIP tokens. These tokens are input into the Transformer with additional encodings. To better leverage the Transformer, we introduce a novel attention function on top of the common scaled dot product and a set of spatial relations. It aims to reflect the degree of correlation between objects based on distances in the image, emphasizing neighboring detections while being aware of object scale and density. Moreover, we introduce self-supervised constraints on CLIP Tokens, providing additional supervised signals. Eventually, we perform another detection on the features given by the Transformer to obtain the final results. The overview of our model is shown inFig.2.

SECTION: 3.1RoI Tokens

In a two-stage detector, the Region Proposal Network (RPN) proposes for each imageRoIs from which we extract features. We apply the standard detection objective, namely classification and bounding box regression, on the features to obtain for each RoI a class labeland a bounding box poserepresenting the center coordinates, width, height and orientation, respectively.

Subsequently, we mapthrough linear layers to high-dimensional embeddings. They are then concatenated with the logits of the class distributionto form the RoI Token:

wheredenotes concatenation and the position encodingis computed as in[8]and added to enforce spatial information.

We will show in the experiments that, havingtwo detection phases (preliminary and final)is vital to the success of our model. This also distinguishes our work from prior works[15].

SECTION: 3.2CLIP Tokens

Besides RoI tokens, we additionally introduce CLIP Tokens to capture and articulate the multi-scale semantic context offered by the background. We use a CLIP model fine-tuned on the RSCID dataset fine-tuned on the RSCID dataset[21]. We divide each image into patches of size,,, and, with strides 0,,,, respectively. This results in 1,,, andpatches. The patches are resized and passed through the CLIP image encoder to get their image embeddings. We also compute text embeddings using a set of pre-defined descriptions of the format "Aerial photograph of [object]", whereobjectis selected from a range of natural and human landscapes such as forest, ocean, farmland, road, and airport. This results infrom 36 descriptions. Incorporating the semantic information from text embeddings, CLIP tokens analogous toEq.1withand,defined similarly according to patch sizes and locations. Moreover, to better combine information from patches of different sizes, we fuse them in a way similar to FPN. This part is depicted inFig.2(a).

SECTION: 3.3Spatial and Geometric Relations

Our method uses an encoder-only Transformer to capture the relationships between objects. However, the cosine distance self-attention computed between tokens in Transformers associates more closely to their semantic similarity but not spatial relations. Therefore, we introduce a series ofrelationsaccounting for the relative position and geometry between the preliminary detections as listed inTab.1. Similar to self-attention, each relation is computed in a pair-wise manner, i.e.,. We concatenate them into atensor and aggregate them toby passing through a linear layer:

SECTION: 3.4Adaptive Attention Weights

An inherent challenge in aerial images is that objects in a scene can vary drastically in size, orientation, and aspect ratio. Also, certain object types tend to cluster densely (like cars in parking lots) or align in specific patterns (such as parallel tennis courts). Thus the relationship between an object and others should be highly specific to the object instance and the contextual information around it. Based on this observation, we devise a novel scheme to adaptively adjust the attention weights with the following considerations.

It is a natural intuition that the influence of one object on another relates to the distance between them and the relative scales (sizes) of the object. For example, closer objects are assumed to have a stronger correlation than distant pairs, and smaller objects tend to be more influenced by nearby objects, whereas larger objects need to capture the impact at longer ranges. The density around a proposed detection is another important factor. We assume that when there are fewer other RoIs around a detection (i.e., lower density), it should capture the influence of RoIs from further away.
To qualitatively model these factors, we computeas:

whereis the pair-wise distance,the element-wise product, andthe indicator function.is detailed next.is a hyperparameter, andis a global (dataset-wide) scale factor determined by the input image.

To account for the density around an RoI, we first calculate for the-th RoI:

then image-wise normalizeand map them into:

In addition to the aforementioned aspects, it is also necessary to mitigate the self-influence among multiple overlapping RoIs corresponding to the same object. Specifically, if we do not exclude these closely overlapping RoIs, their proximity to each other could lead to them being overly emphasized in the attention calculation while neglecting the interactions between RoIs of different objects. Therefore, we mask the attention weights to only consider RoIs with IoU below a certain threshold.

The overall attention weights are calculated as

whereis the aggregated spatial and geometric relations computed inEq.2.

SECTION: 3.5Loss Function

We utilized a preliminary stage classification loss, and final stage detection lossesand. Furthermore, to constrain CLIP tokens, we employed a self-supervised loss. The overall loss function is given by:

Whereis the standard bounding box regression loss, andis the cross-entropy loss used in both stages.is the MSE loss between the background classification outputand its ground truth.andare hyperparameters.

SECTION: 4Experiment

SECTION: 4.1Dataset

DOTA-v1.0contains 2,806 images, with sizes ranging fromtopixels. It includes 188,282 instances across 15 categories, annotated as:Plane(PL),Baseball Diamond(BD),Bridge(BR),Ground Track Field(GTF),Small Vehicle(SV),Large Vehicle(LV),Ship(SH),Tennis Court(TC),Basketball Court(BC),Storage Tank(ST),Soccer Ball Field(SBF),Roundabout(RA),Harbor(HA),Swimming Pool(SP), andHelicopter(HC). Following the common practice[11], we use both the training and validation sets for training and the test set for testing. We report mAP in PASCAL VOC2007 format and submit the testing result on the official dataset server.

DOTA-v1.5uses the same image set but with increased annotations. This version features 402,089 instances and introduces an additional category,Container Crane(CC), broadening the dataset’s applicability. It also includes annotations for a greater number of small objects, some of which have areas smaller than 10 pixels, further enhancing the dataset’s complexity.

DOTA-v2.0further expands the datasets to 11,268 images and 1,793,658 instances, with two additional categories,Airport(AP) andHelipad(HP).

DIOR-Ris a refined version of the original DIOR dataset, specifically re-annotated with rotated bounding boxes to enhance the detection of object orientation and shape in aerial images. It consists of 23,463 high-resolution images and 190,288 annotated instances, covering 20 diverse object categories, including vehicles, airplanes, ships, and more.

HRSC2016focuses on ship detection in aerial images, containing 1,061 images with a total of 2,976 instances. Image sizes in this dataset range from 300×300 to 1500×900 pixels. The dataset is divided into training, validation, and test sets with 436, 181, and 444 images, respectively.

SECTION: 4.2Implementation Details

Our implementation is based on the MMRotate[46]library and adopts ReDet’s framework and hyperparameter settings. We train our model forepochs using the AdamW[20]optimizer with an initial learning rate of, reduced toandat epochs 8 and 11. We also use a weight decay of 0.05. The experiments were conducted using two RTX 3090 GPUs.

The Transformer module consists of 6 encoder layers, similar to the ViT structure, and integrates sinusoidal two-dimensional absolute position encoding, hyperparametersis set to 4. A dropout rate of 0.1 is employed during the training phase of the Transformer. In the loss function, we setas 1, andas 10.

SECTION: 4.3Comparison with Baselines

First, we evaluate our model against the baselines on DOTA-v1.0, DOTA-v1.5, DOTA-v2.0, DIOR-R and HRSC2016 to demonstrate the efficacy of the proposed method. The results are shown inTab.3, andTab.4, respectively.
These results demonstrate that our method consistently outperforms the baselines across different datasets. Notably, however, the improvement achieved in HRSC2016 is marginal compared to that on DOTA-v1.5 and DOTA-v2.0. This is possibly due to the number of instances in a single image being much fewer in HRSC (typically less than 4), thus there are limited opportunities to leverage the inter-object relationships. These findings suggest that our model’s strengths are most pronounced in scenarios rich in object interactions and contextual dynamics, aligning with our design’s focus on capturing and utilizing inter-object relationships.

SECTION: 4.4Ablation Study

Compared to the standard detection pipeline, our model incorporates two detection heads - placed before and after the Transformer module. The output from the initial detection phase, dubbedpreliminary detection, includes a classification result (parameterized as a softmax distribution) from the first head, which forms a component of the RoI token. We posit that knowing the class information with uncertainties would help with reasoning about the inter-object relationships. To empirically validate this hypothesis, we compared the performance of our model with and without training the first detection head. AsTab.5shows, although solely incorporating the Transformer offers an improvement of mAP50to the baseline, omitting the preliminary detection leads to a notable decline in performance. This suggests that relying only on the Transformer for RoIs to interact lacks efficacy. In contrast, the explicit inclusion of preliminary classification data, despite its potential inaccuracies, enhances the model’s ability to reason about semantical and contextual relationships. The results underscore the value of early classification cues in guiding the relational reasoning process within our proposed architecture.

The different terms presented inTab.1characterize various aspects of the spatial and geometric relationships among objects (RoI Tokens) within an image. In this section, we aim to empirically evaluate the individual and collective contributions of these spatial and geometric relational terms to the overall performance of our detection model. As shown inTab.7, IoU and rel. area contribute the most. Intuitively, they are particularly helpful when reasoning about the co-occurrence and spatial arrangement of objects. For example, IoU helps to disambiguate the overlapping, potentially duplicate or conflicting detections. Similarly, relative area aids in discerning the size relationship between objects. Consequently, our method can effectively solve the problem in the motivation example. SeeSec.5.1for details.

As mentioned inSec.3.4, making the attention weights adaptive to specific RoI Tokens is essential to cope with the diversity and complexities in a scene. We evaluate our density- and scale-aware attention weighting scheme which is designed to augment the scaled-dot-product self-attention and allow the model to dynamically adjust its focus based on the scale of objects and their surrounding density. Findings inTab.7indicate that masking the influence of overlapping RoIs plays a crucial role. This observation aligns with our initial understanding that indiscriminately emphasizing neighboring RoIs, without considering overlap, could lead to skewed attention distributions and potentially impair the model’s ability to accurately discern between distinct objects.

SECTION: 4.5Self-supervised and Multi-level Fusion

We show the additional improvements achieved by incorporating the self-supervised loss and multi-level fusion for CLIP tokens inTab.5. Self-supervised loss effectively regularizes CLIP tokens to prevent representation collapse. And multi-level fusion helps capture information at different spatial scales.

SECTION: 5Analysis

To gain insights into how inter-object relationships have improved detection performance, we collect and analyze dataset-wise statistics and specific examples.

SECTION: 5.1Evaluation Statistics

By examining the data we found that many false detections deviate far from the typical scales associated with their respective categories. To investigate this observation, we compute for each category the mean and standard deviation of object scaleusing detections with confidenceon the test set. We then identified outliers as those detections deviating from the mean by more than three times the standard deviation. This method provides a rough measure of the frequency of incorrect scale detections. As shown inFig.4, the detections produced by our methods have substantially fewer outliers compared to the baseline. This result suggests that our model better maintains scale consistency across different object categories. This improvement is particularly vital in aerial image analysis, where scale variance is substantial and often indicative of the detection model’s reliability and robustness.

Additionally, our visual analysis revealed a common misclassification of many land-based objects asShip. To quantify this observation, we compute the average chamfer distance between certain categoriesandin an image:

As Table9shows, the results are in line with the logical expectation thatShipinstances should be found in water, nearHarbor, but distant fromSmall Vehicle. This finding underscores our model’s effectiveness in accurately understanding the spatial arrangement of objects, further validating the benefits of our approach in handling complex aerial imagery.

SECTION: 5.2Limitations

While our method effectively captures inter-object relationships to improve detection accuracy, there are cases where this approach can lead to undesirable results. Specifically, when a wrong detection occurs for one object, it can propagate errors to nearby objects, particularly if those objects share similar spatial or semantic characteristics. As shown inFig.5, the false detection of ships in the ReDet model leads to an increased number of false positives for other ship instances when inter-object relationships are captured. This demonstrates that while our method enhances the overall detection performance, incorrect understanding or misidentification of one object may negatively influence the detection of surrounding objects, especially in cases where the objects are spatially or contextually similar.

SECTION: 6Conclusion

In this work, we propose a Transformer-based framework to enhance object detection by effectively capturing inter-object and object-background relationships. By integrating the strengths of Transformer models with the cross-modal capabilities of CLIP, our approach not only improves the interaction between Region of Interest (RoI) proposals but also leverages background context for more accurate detections. Extensive experiments on several benchmark datasets demonstrate the effectiveness of our method, yielding consistent improvements over existing detectors. Our analysis further shows that the model reduces scale inconsistency and improves spatial and geometric understanding.

SECTION: References