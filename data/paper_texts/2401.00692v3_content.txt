SECTION: Self-supervised learning for skin cancer diagnosis with limited training data

Early cancer detection is crucial for prognosis, but many cancer types lack large labelled datasets required for developing deep learning models. This paper investigates self-supervised learning (SSL) as an alternative to the standard supervised pre-training on ImageNet for scenarios with limited training data using a deep learning model (ResNet-50). We first demonstrate that SSL pre-training on ImageNet (via the Barlow Twins SSL algorithm) outperforms supervised pre-training (SL) using a skin lesion dataset with limited training samples. We then considerfurtherSSL pre-training (of the two ImageNet pre-trained models) on task-specific datasets, where our implementation is motivated by supervised transfer learning. This approach significantly enhances initially SL pre-trained models, closing the performance gap with initially SSL pre-trained ones. Surprisingly, further pre-training on just the limited fine-tuning data achieves this performance equivalence.
Linear probe experiments reveal that improvement stems from enhanced feature extraction. Hence, we find that minimal further SSL pre-training on task-specific data can be as effective as large-scale SSL pre-training on ImageNet for medical image classification tasks with limited labelled data.
We validate these results on an oral cancer histopathology dataset, suggesting broader applicability across medical imaging domains facing labelled data scarcity.

SECTION: 1Introduction

Cancer is the second leading cause of death worldwide, with almost 10 million deaths estimated in 2020[18]. In several types of cancer (e.g. skin, oral, pancreatic), early diagnosis is the major determining factor in prognosis[33,19,38]. If cancer is detected sufficiently early, survival rates may be above 90%[55,35,74]. On the other hand, the prevalence of several cancer types is increasing[74], and this is particularly true in poorer communities in the developing world[63]. This is of particular concern since individuals in such communities have lower access to the expert clinicians traditionally needed to diagnose such cancer. There is thus an urgent need to develop cheaper and more data-efficient methods of diagnosis that can be deployed globally. In this context, machine learning models that can effectively utilise limited datasets are particularly crucial, as they could enable the development of artificial intelligence-assisted diagnostic tools[37,65,66]in areas where large, labelled datasets are not available.

In recent years, there has been significant interest in applying machine learning to cancer image diagnosis[44]. This includes lung, breast and several other forms of cancer, and involves classifying clinical images into categories (‘malignant’ or ‘benign’) or more fine-grained classification[42,13,76].
There have been some challenges in the use of deep learning models, since they require large and class-balanced datasets to function properly. The availability of timely data with proper organisation has been a challenge in the medical domain due to restrictions in archive access given patient confidentiality and ethical approval concerns[70].
In general, assembling labelled datasets is a complex and resource-intensive process regarding collection and annotation[11]. This is even more pronounced for medical images, which require expert knowledge or even medical testing (e.g. via biopsy) to classify, dramatically increasing the cost[28,10]. Therefore, for several types of cancer, we lack image-based datasets that are suitable for training machine learning models.

Sengupta et al.[57]highlighted the scarcity of publicly available oral cancer datasets. Although there has been research published on oral cancer image classification, the data used is closed source. Our attempt to obtain data by contacting primary authors was not successful, which further highlights the problem of data availability and unreproducible research[32,59]. Oral cancer is diagnosed through biopsy, but the decision to biopsy is made by visual inspection of the mouth[5], and early detection is crucial for prognosis. In the United States, the 5-year survival with early detection is 85%, but only 28% of cases are detected at this stage. In later stages, when cancer has spread, 5-year survival drops to 40%[7]. The lack of available datasets is concerning given that the prevalence of oral cancer is increasing,[74], particularly among poorer parts of the developing world, where expert medical care is less available. As an additional point, certain kinds of cancer have extremely low prevalence. For example, Chordoma has a prevalence of only 0.18-0.84 per million[3], so constructing a large labelled dataset in such cases is very difficult. Developing data-efficient techniques is therefore essential since factors such as disease rarity, privacy concerns, and labelling costs result in the absence of labelled datasets for several cancer types. In this study, we address this challenge by investigating self-supervised learning approaches and propose an efficient technique for enhancing supervised pre-trained models.

Supervised training of deep learning models involves initialising the weights of the network, classically via random initialisation, and then iteratively updating the weights through variants of the backpropagation algorithm[21]. A well-known problem with random weight initialisation is the requirement for large amounts of labelled data to achieve good performance. Transfer learning is a machine learning technique that attempts to address this problem by instead initialising the weights with those of a network that has already been trained on a related task - typically through supervised learning[80]. A common strategy in computer vision applications is to use supervised pre-training on ImageNet[80], a large-scale dataset of natural images spanning a diverse range of object categories. It has been demonstrated that transfer learning can lead to significant performance gains compared to training from random initialisation[67].

Self-supervised learning (SSL)[31]is an alternative approach to pre-training models, also known as self-supervised pre-training. Unlike supervised pre-training, which requires labelled datasets, SSL utilisesunlabelleddata. This data is used to create a ‘surrogate task’ or ‘supervisory signal’ for learning. For example, SSL has achieved remarkable success in NLP through the pre-training of large language models[72,49], where the task is typically to predict the next word (or token). It has been more challenging to apply SSL to computer vision tasks since visual images are inherently of higher dimension[43], and developing surrogate tasks has been more difficult than in NLP. However, recently SSL has had success in computer vision tasks as well[31,43], and may be particularly applicable in medical image domains where collection of labelled data poses challenges.

The use of transfer learning is common in the medical diagnosis literature for image data[80], such as skin cancer[40]and lung cancer diagnosis[73]. Usually, the models involved have been pre-trained on ImageNet in a supervised fashion. Recent work has also explored the use of SSL pre-training[27]; however, in the low data regime, it is still unclear which pre-training methodology is superior. In this study, we investigate the efficacy of different pre-training strategies for the downstream task of cancer diagnosis in limited-data scenarios, mainly focusing on skin lesion classification as a model. Specifically:

We compare the standard supervised pre-training on ImageNet with self-supervised pre-training, (via Barlow Twins) where both networks are fine-tuned on a small labelled skin lesion dataset[68], with limited training samples compared to test samples.

We investigatefurtherSSL pre-training of each of the ImageNet pre-trained models onunlabelleddata that is from a similar distribution to the data used for fine-tuning. These doubly pre-trained networks are fine-tuned and evaluated in the same manner as the initial (pre-trained once) networks.

SECTION: 2Related work

We review the deep learning literature on medical image classification, and then give an overview of self-supervised learning and Barlow Twins.

SECTION: 2.1Skin cancer detection

Conventional deep learning models using supervised learning have had enormous success in skin lesion classification[13], which has been facilitated by large curated datasets, such as the International Skin Imaging Collaboration (ISIC) database[29], and other large datasets. The ISIC2018 training dataset is a subset of ISIC which features 10,015 image samples, distributed among 7 categories[1]. Guo and Ashour[23]proposed a Multiple Convolutional Neural Network (MCNN) model for skin lesion classification using this dataset. The training strategy focuses each subsequent CNN on samples poorly classified by previous models, aiming to improve overall performance on challenging cases. Majtner et al.[47]fine-tuned two models on 10,015 labelled ISIC samples using deep learning (VGG16 and GoogLeNet), both of which had been pre-trained via supervised learning on ImageNet, and found the ensemble accuracy was higher than individual accuracy. Maron et al.[48]utilised 11,444 ISIC skin images using a ResNet-50 pre-trained on ImageNet for the classification of benign and malignant samples that outperformed Dermatologists.

Esteva et al.[15]trained an Inception-v3 Convolutional Neural Network (CNN) on a large dataset of 129,450 lesion images with 2,032 disease categories (i.e. instead of a label of ‘melanoma’ there is a more fine-grained subclassification of amelanotic melanoma, lentigo melanoma etc) achieving performance on-par with Dermatologists. Brinker et al.[6]considered a balanced training set of 4,204 melanoma and nevi mole images, where the classification had been determined by biopsy. While this training set is smaller than in other studies, note that it involves a binary classification problem, resulting in a larger number of samples per class. They also utilised a transfer learning approach, via a ResNet-50 pre-trained on ImageNet with supervised learning. On the metrics of sensitivity and specificity, they found that the trained network outperformed Dermatologists.

Our review of skin cancer studies has shown that all the mentioned studies use supervised pre-training on ImageNet and the training sets used are relatively large to achieve high performance accuracy. The high performance is made possible by the existence of large curated datasets, such as ISIC. In our study, we consider skin lesion classification on a much smaller dataset to demonstrate model development for situations where large labelled datasets do not exist, such as oral cancer or rare cancers. We also propose a technique that can enhance the fine-tuning performance of supervised pre-trained models.

Song et al.[63]studied the problem of oral cancer image classification on an imbalanced training dataset with a CNN pre-trained on ImageNet with supervised learning. They reported that oversampling of minority class during training can effectively address the problem of imbalanced and limited data in this context. The utilisation of ImageNet supervised pre-training using deep learning models has also been applied to
Alzheimer’s disease multiclass classification from MRI images[39], pancreatic cancer diagnosis[4], diabetic retinopathy classification from images[30], and other areas of medicine. In other words, supervised pre-training on ImageNet, typically employing CNN-based architectures, is a ubiquitous transfer learning technique not just for cancer image classification, but across various areas of medical image classification. Recent work has also explored using SSL pre-trained models for medical image diagnosis[27]that is promising in leveraging large amounts of unlabelled data. However, the comparative effectiveness of SSL versus traditional supervised pre-training in medical imaging tasks with limited labelled data remains an open question that our study aims to address.

SECTION: 2.2Self-supervised learning

Self-supervised learning (SSL) is a method of pre-training models using unlabelled data. The goal is still to learn an initialisation of a deep network, similar to standard supervised pre-training. One approach to SSL is the joint embedding architecture (JEA)[43], illustrated in Figure1. The intuition of JEA is that random distortions (e.g., blur) to an image, does not change its semantic content. Supposeandare distinct images andare blurry versions of- calledpositive samplessince they arise from the same image. Conversely,is anegative sample. Then the network should preserve semantic structure:. However, it’s crucial to avoid the trivial solution of collapse to a constant function, which also satisfies this relation. Hence, an additional requirement is thatis non-trivial; or that distinct images produce distinct representations:. There are two main approaches to prevent this collapse. The contrastive approach involves an explicit comparison between negative samples, with an example algorithm being SimCLR[9](Simple Framework for Contrastive Learning of Visual Representations). The non-contrastive
approach avoids collapse implicitly through regularisation on the outputs, without requiring comparison via negative samples. Barlow Twins[78]is an example of this approach, and we focus on the non-contrastive approach in this study.

The JEA (as in Figure1) requires designated networks(encoder) and(projector), a loss function, and an unlabelled image dataset. Two distributions of data augmentationsandare also required. The training process involves sampling a batch of data and computing two distorted views:,, whereandare random data augmentations (e.g., cropping, blur). These distorted batches are then passed through the encoder and projector:and. Finally the loss function is computed, which is used to compute gradients for backpropagation. We use the following terminology: the "representation" refers to the output of the encoder:, the "projected representation" or "projection" is the output of the projector:. On downstream tasks, these "representations" can be considered with the parameters frozen or unfrozen, i.e. full network fine-tuning or linear probing.

The loss functionimposes a similarity constraint onand, which arise from the same batch of data under different augmentations. Barlow Twins[78]is one method of specifying this loss function in SSL. After training, we remove the projector and retain the encoder as the pre-trained model.

One might wonder about the role of the projectorin Figure1; since only the encoderis retained for downstream tasks - why is it necessary to include? Historically, the loss function was computed in encoder space until Chen et al.[9]found that adding a projector to the end of the network during training led to superior representations. The superior performance also arises when the projector has lower dimensionality than the encoder[78]for most algorithms based on the joint embedding architecture. Barlow Twins is an exception, and benefits from having an extremely high projector dimension, as presented in the following section.

SECTION: 3Methodology

SECTION: 3.1Barlow Twins

Barlow Twins is an SSL algorithm based on the general joint embedding framework we have outlined. It provides a way of specifying the loss function:. At a high level, the computation of the loss function is extremely simple. The two branch outputsandare normalised, and then multiplied together, yielding a cross correlation matrix, which is then equated to the identity matrix.

In more detail, if the batch size isand the projector dimension is, thenandwill each be. We first normalise the branchesandalong the batch dimension and compute the mean, (for the first branch)and the standard deviation:. We then compute the normalisation as,which is performed analogously for. Next, we compute the cross-correlation between the branches with a simple matrix multiplication:The Barlow Twins loss function then setsto the identity matrix:

whereis a trade-off hyperparameter. One can think of the loss function as being composed of an invariance term and a redundancy reduction term. The invariance term trains the network to ignore the image distortions, and the redundancy reduction term ensures that this is done in a non-trivial way, such that the components of an output vector contain non-redundant information. The hyperparameterbalances these two objectives.

Next we describe a simple modification of SSL and Barlow Twins framework that we use in the current study. Specifically, recent work[64,26,34]has found that SSL methods, including Barlow Twins, can suffer from dimensional collapse, where learned embeddings span a low-dimensional space. The projector head partially ameliorates this problem, but it cannot be entirely avoided. Song et al.[64]proposed imposing a sparsity constraint on the projector to improve generalisation and reported downstream performance gains. The method takes thenorm of the weight matrix of the final layer, which is scaled by a hyperparameterand added to the original Barlow Twins loss in Equation1:

The sparsity constraint on the projector is particularly applicable to the current setting where we consider further pre-training on smaller datasets, where the risk of overfitting will be greater which is further magnified by the large dimensionality of the projector network. Hence we use the loss in Equation2in the curent work.

The Barlow Twins algorithm has several properties that make it potentially more applicable in the low data regime. For example, Barlow Twins achieves excellent performance when fine-tuning with limited data on ImageNet[78]. As an additional point, Barlow Twins can be trained with batches as low as 128, in contrast to other self-supervised methods which may need batches above 4000[9]. This is helpful in the case that a limited computing budget is available, e.g. using very large batches may not be possible due to memory limitations.

SECTION: 3.2Framework

We next provide the details of our SSL framework, outlining the major components in Figure3. We utilise two backbone neural networks, each having a ResNet-50 architecture and pre-trained on ImageNet, either in a supervised or self-supervised fashion. We fine-tune both backbone networks on a small labelled skin lesion dataset and compare their performance. We also train linear probes against the frozen backbones, to understand the source of fine-tuning performance differences. We then perform further self-supervised pre-training, on several different datasets. We fine-tune and linear probe the doubly pre-trained models in the same manner as originally performed on the (pre-trained once) backbones. This enables comparison across all model weight initialisation, as shown in pink in Figure3.

ImageNet is a large annotated database of natural images[69]that has been prominent as a test suite for computer vision and deep learning models. It has also been popular for pre-training deep learning models111https://nnabla.readthedocs.io/en/v1.39.0/python/api/models/imagenet.htmlsuch as VGG[60]and ResNet[75]. In our framework, we use an instance of ImageNet containing approximately 1.2 million images distributed across 1000 categories[56,11]. Residual CNNs are deep neural networks that employ residual skip connections[24]. This is an architectural design that adds the input of a neural network module to its output:, wheredenotes the function performed by one or more layers. This enables the training of much deeper networks, asis easier to optimise. For instance, if an identity mapping is optimal, the network can simply learn.

Our framework (Figure3) utilises two base networks, both pre-trained on ImageNet but using different approaches. Both networks have a ResNet-50 backbone architecture[24], which we denote as the encoder, whererepresents the network parameters. ResNet-50 is a residual CNN with 50 layers and an output dimensionality of 2048. We refer to the output of the encoderas the ‘encoder representation’, where the parameters may be frozen in which caseacts purely as a feature extractor, or unfrozen where we can assess how well the pre-training adapts to the new task.

The two pre-training approaches are:

Supervised Learning (SL): where the pre-training architecture is:, withthe final linear layer[52].

Self-Supervised Learning (SSL): where the pre-training architecture is:, whererepresents the projector layers, using the Barlow Twins algorithm[54].

More formally, letbe a dataset used for pre-training (in our case, ImageNet). We denote the pre-training processes as follows:for supervised learning andfor self-supervised learning. Note that SSL requires only the unlabelled data, while SL uses bothand. Our base networks can thus be represented as:and, highlighting their shared architecture but different weights.

SECTION: 3.3Data for supervised learning

Skin lesion classification is a well-studied problem using deep learning models[46,79,45]. Brinker et al.[6]demonstrated that it is possible to train models that outperform human experts, which is primarily due to the existence of large labelled datasets. Our framework is motivated by cases where such datasets do not exist, such as oral cancer. We present the data used for fine-tuning in Table1where the training set only has 2,554 samples which make less than 12% percent of total data. Note that over 90% of oral cancer cases are squamous cell carcinomas[74].

We obtain labelled skin lesion data from ISIC2019[68]which is a subset of the ISIC database[29]. We call our dataset ISIC2019*, since we use only 2,554 samples for training. There are 8 lesion categories, 4 of which are benign, that are not cancerous (benign keratosis, dermatofibroma, nevus, vascular lesion) with the remainder being malignant, that is cancerous or pre-cancerous. The category actinic keratosis is precancer, whereas basal cell carcinoma, melanoma, and squamous cell carcinoma are all cancerous. There are only 171 squamous cell carcinoma samples, making this a very challenging class imbalanced dataset.

SECTION: 3.4Fine-tuning

In our framework (Figure3), we subsequently fine-tune the pre-trained networks using the training data in Table1. Therefore, we implement supervised learning on, whereis a pre-trained encoder andis a randomly initialised linear layer mapping to the lesion categories. First, the pre-trained encoder is frozen, and the linear head is fit for 1 epoch against the frozen encoder. In this phase, we use the Adam optimiser[41]with. Next, we unfreeze the encoder and run a learning rate search to find a good maximum learning rate:. Lastly, we train the whole network (for 40 epochs) using the 1cycle learning rate policy and maximum learning rate. The 1cycle policy (Smith et al.[62,61]) is a learning rate scheduler, that works particularly well in low data settings. It linearly increases the learning rate during training to a maximum value, and then decreases to a minimum. We provide a full discussion in the appendixA.1. We use the Adam optimiser in all stages of training and present the full fine-tuning implementation details in AppendixA.1.

SECTION: 3.5Linear Probe

Linear probing (also known as linear evaluation) is a method used to assess the quality of the representations learned by a neural network[50]. In our framework (Figure3), linear probing involves freezing the pre-trained encoderand only updating the parameters in the final linear layer. In linear probing, we use the same training procedure as during full network fine-tuning, except the encoder is frozen throughout.

SECTION: 3.6Evaluation Metrics

After performing supervised learning (either full network fine-tuning or linear probing), we evaluate model performance on the test dataset as described in Table1. We conduct 35 independent model training and test runs using different initial random initialisation of the final linear layer. We report two primary evaluation metrics, including classification accuracy and weighted-average F1 score for the test dataset.

We compute the weighted-average F1 score as follows:,
whereis the number of categories,is the frequency of categoryin the test dataset, andis the F1 score for categoryin the model training run.
This approach allows us to account for class imbalance in the test set while providing a robust measure of model performance across multiple runs.

SECTION: 3.7Further pre-training procedure

We describe our methodology for implementing further self-supervised pre-training, which requires pre-training an (already pre-trained) encoder. In Section3.2.1, we noted that self-supervised pre-training can be viewed as a function of an unlabelled image dataset, and an initialised network with parameters, i.e., whereis randomly initialised. We specify an approach to extending the definition offor the case whenhas already been pre-trained, which in our case will mean. This is analogous to supervised transfer learning, whereby the training process may be different under the knowledge that the network is pre-trained, vs. being randomly initialised.

Hence, to perform further SSL pre-training, we require a pre-trained encoderand an unlabelled image dataset. Recall that when training Barlow Twins the architecture is:whereis a projector network that is discarded after training. Figure4presents an overview for further pre-training, with further details as follows.

Projector Reinitialisation: We initialised a new projector headcomprising three layers, each with 8192 units. This fully connected network employs ReLU activation functions and batch normalisation, consistent with the original ImageNet pre-training architecture[78].

Training Strategy:

Initial Epoch: We freeze the encoderand update only the projector’s parameters for the first epoch using the Adam optimiser, with fixed. This step prevents the random projector from immediately degrading the pre-trained features in the encoder.

Subsequent Training: After training the projector for the first epoch, we can execute two approaches:

Full encoder training: We unfreeze the entire encoder and train the entire network:, for 100 epochs.

Partial encoder training: We unfreeze just the final 3 layers of the encoder while keeping the earlier layers frozen. These final 3 layers consist of a,,convolution sequence, forming a bottleneck block[24,52]. This approach aims to preserve low-level features while allowing the adaptation of higher-level features to the new dataset. We denote partial encoder training as SSLp.

Both approaches employ the 1cycle policy[62,61]as the learning rate schedule, consistent with the fine-tuning process described in sections3.4andA.1. This policy is particularly effective in limited data scenarios, making it well-suited for our SSL framework to leverage relatively small datasets; SSL typically involves very large datasets[43].

Hence, we have an approach that extends the definition ofas into work whenis pre-trained rather than randomly initialised. The approach to further pre-training is inspired by supervised transfer learning techniques that aim to leverage the pre-trained ResNet-50 ImageNet features while adapting the model to the new dataset. Essentially, we use a discriminative learning rate scheme, whereby the encoder has a learning rate of zero for the first epoch and a non-zero learning rate thereafter. For partial encoder training SSLp, the learning rate remains zero throughout except for the final bottleneck block.

By comparing full and partial encoder fine-tuning, we can assess the trade-off between preserving pre-trained features and adapting to the new data distribution.
We set the hyperparameterand the regularisation hyperparameter(Equation2). We adopt the data augmentation strategy from previous papers[78,22], and present a representative sample in Figure2. The full implementation details are in AppendixA.2and on GitHub222https://github.com/hamish-haggerty/base-rbt

SECTION: 3.8Data for further self-supervised pre-training

We utilise 3 base datasets for further self-supervised pre-training: DermNet[12,20], UFES20[51], and a subset of ISIC2019 data itself[29,68]as summarised in Table2. The base datasets feature different types of skin conditions and provide a similar data distribution to the fine-tuning data in Table1.

We combined these base datasets to form 3 datasets for further pre-training: I, IU, IUD, as summarised in Table3. We note that I refers to the fine-tuning data as shown in Table1and IU refers to all the base datasets combined - except for DermNet. IUD is IU plus DermNet (i.e., all the base datasets together).

SECTION: 4Results

We use the skin lesion dataset in Table1for experiments (whether full network fine-tuning or linear probing). We use a ResNet-50 encoderas the backbone architecture in all the experiments, with variations in how the weightshave been pre-trained. In the final section we present results for an oral cancer histopathology dataset.

SECTION: 4.1Pretrained once on ImageNet: supervised vs. self-supervised pre-training

We begin by comparing the fine-tuning results of the backbone ResNet-50 encoders that have been pre-trained on ImageNet as defined in Section3.2.1. We recall that these are the base models that have been pre-trained once on ImageNet with either SL or SSL. Hence, the models to be fine-tuned have the form:, whereandis a randomly initialised linear layer. We use the dataset in Table1for fine-tuning and present the results in Table4. Note that the randomness (weight initialisation) across the independent model training runs comes entirely from, i.e. the linear layer.

The main observation is that SSL pre-training outperforms SL pre-training in terms of both accuracy and average weighted F1 score. Table14demonstrates that this difference is highly statistically significant. In other words, pre-training on ImageNet using Barlow Twins yields better fine-tuning performance compared to supervised pre-training on ImageNet. We can denote this result as, where the subscript ‘fine-tune’ indicates that the comparison is based on fine-tuning performance.333Formally,defines a partial order on models. For a set of pre-trained models, we fine-tune each model and record its mean test accuracy (say). Then,holds if and only if the mean test accuracy of(i.e. after fine-tuning the model with initial weights) is significantly higher than that of, also post fine-tuning.This result raises a question: why does self-supervised pre-training lead to superior performance? We explore two possible hypotheses in the following section.

SECTION: 4.2Linear probe analysis of base models

We employ linear probing for assessing representation quality by training only the final linear layer on frozen features, in order to evaluate the fine-tuning performance difference between SL and SSL pre-trained networks. Hence, the experimental setup is exactly the same as Section4.1, except that we freeze the backbone encodersand only update the parameters of the linear layerduring training. We consider two hypotheses to explain the superior fine-tuning performance of SSL:

Thefast adaptation hypothesisposits that there is no significant difference in linear probe performance between SSL and SL. This would suggest that the SSL weights are more amenable to rapid learning on new data during fine-tuning.

The feature reuse hypothesisproposes that SSL outperforms SL in linear probe performance. This would indicate that the self-supervised pre-training produces more reusable features, explaining the superior fine-tuning performance. We adapted the terminology ‘rapid learning’ and ‘feature reuse’ from Raghu et al.[53].

The results in Table6strongly support the feature reuse hypothesis, where the frozen SSL representations significantly outperform their SL counterparts, with accuracies of 66.3% and 59.3%, respectively. Thelinear probeperformance of SSL is comparable to thefine-tuningperformance of SL. To make this clear we include a Table7with a side-by-side comparison of linear probe and fine-tuning performance. Hence, we can summarise this result as.

In order to better understand the (pre-trained once) results in Table6we recall how each base ResNet-50 encoder is pre-trained. The SL model is trained on a supervised classification task with 1000 categories from ImageNet. This pre-training process can be represented as, whereis the encoder (feature extractor) andis a linear layer that maps the encoder’s output to 1000 class scores. Specifically, for an input image, the encoder produces a feature vector. The linear layerthen computes scores for each of the 1000 ImageNet categories as linear combinations of this feature vector. We can represent this computation as, whereis the vector of scores for all categories. Suppose the-th category corresponds to "dog", and let the-th row ofbe, then the score for "dog" would be computed as. After pre-training,is discarded, leaving us with the pre-trained encoder:. When we perform linear probing on our new task (skin lesion classification), we have, whereis a new linear layer that maps to our 8 skin lesion categories, and the parameters inare frozen.

The difficulty arises because the features learned byare optimised to discriminate between the 1000 ImageNet categories. These features are trained such that linear combinations of them can effectively distinguish between categories such as "dog", "cat", "car", etc. However, such features may not be as effective for distinguishing between different types of skin lesions. It is perhaps not surprising that linear combinations of such features may not easily translate to our task of skin lesion classification. In contrast, Barlow Twins SSL pre-training does not use any class-specific information. Instead, it learns to create similar representations for different augmented views of the same image. This approach likely produces more abstract and generalisable features that are not tied to specific ImageNet categories. This fundamental difference in the nature of learned features appears to be the primary reason for the superior performance of, in both linear probe and fine-tuning. Our results suggest that SSL pre-training on ImageNet via Barlow Twins yields a more versatile and transferable feature representation when compared to supervised pre-training, particularly when adapting to new tasks like skin lesion classification.

These considerations motivate the question of whether it is possible to perform a minimal amount of further self-supervised pre-training to enhance the SL encoder representation. In the following section, we show that minimal further pre-training ofboosts both linear probe and fine-tuning performance.

SECTION: 4.3Fine-tuning results after further pre-training

We present the fine-tuning results following further pre-training in Table4. Further pre-training on the IUD dataset boosts performance for both (initially) SL and SSL pre-trained models. This improvement is particularly noticeable for SL, where we observe a substantial performance gain in terms of accuracy and F1 scores. In the case of SSL, although the improvement is small, it remains statistically significant, as shown in Table14.
We found no significant difference in performance between pre-training the entire network on IUD (SSL) and pre-training only the final three layers (SSLp). Hence, for subsequent pre-training experiments, we only considered the training of the final bottleneck block.

Another noteworthy observation is the comparable performance achieved when pre-training on datasets of vastly different sizes. Specifically, pre-training on the IUD dataset (containing 25,691 images), the IU dataset, and the I dataset alone (with only 2,554 images) yielded similar results. This finding suggests that the relevance of the pre-training data may be more crucial than its quantity. Perhaps most surprisingly, after further self-supervised pre-training of each of the base models, we observed no subsequent performance difference. In order to understand this better, we recall the initial result that SSL pre-training outperformed SL pre-training, i.e.. However, if we further pre-train each base model (i.e.and) this inequality becomes an equality:. This holds even when using the fine-tuning data for further pre-training, which in our case is just 2,554 samples.

SECTION: 4.4Linear probe analysis after further pre-training

Our earlier linear probe experiments demonstrated that feature reuse largely explained the fine-tuning performance difference between SL and SSL pre-training. To investigate this phenomenon further, we conducted similar linear probe experiments on the frozen representations after further pre-training. The results in Table6reveal a significant increase in linear probe performance following further pre-training of the SL encoder. Surprisingly, after further pre-training of, the representation outperformed therepresentation. In general, for any choice of dataset for further pre-training, we found that:.
We recall our prior result that:. Despite this initial gap in encoder representation, it appears that even minimal further pre-training of the SL encoder on the fine-tuning data is sufficient to boost its representation capacity to that of the SSL encoder. Therefore, although there is a gap in the feature extraction capacity of the base encoders (as measured by linear probe performance), further pre-training the worse base model (SL) just on the fine-tuning data is enough for this gap to close, and even reverse.

Further pre-training of the SSL encoder also improved its linear probe performance. For instance, further pre-training of just the final 3 layers on the fine-tuning data increased linear probe accuracy from 66.3% to 69.4%, with a similar improvement in average weighted F1 score. After further pre-training the SSL encoder, we observed no significant difference between linear probe and fine-tuning performance (summarised in Table8). This indicates that further self-supervised pre-training enhances the encoder’s representation to such an extent that a simple linear classifier performs comparably to full network fine-tuning. These findings together provide strong evidence that the general trend of increased performance after further pre-training is primarily due to enhanced feature reuse in the encoder representation.

SECTION: 4.5Validation on oral cancer dataset

We finally consider an oral cancer histopathology dataset, which can be seen in Table9[16]. This is a relatively small binary classification dataset, consisting of only 122 labelled samples for fine-tuning: 60 normal samples and 62 oral squamous cell carcinoma (OSCC). As earlier, we first compare the fine-tuning performance of the SL and SSL pre-trained encoders. We observe the same pattern of results as for ISIC2019* data, where SSL outperforms SL which can be seen in Table10. Next we consider further pre-training of the SL encoder. In the case of ISIC2019*, we could reach maximum relative performance by just pre-training on the available fine-tuning data, which was 2,554 samples. The fine-tuning data available for oral cancer histopathology classification is only 122 samples, which is very low for self-supervised learning. We increase the dataset for pre-training by adding an additional 2374 ‘normal’ samples, as shown in Table9. Note that the collection of additional normal samples compared to OSCC is significantly easier (i.e. no biopsy diagnosis is needed). We oversample the OSCC samples at a 10:1 ratio when pre-training.

Despite only having 62 OSCC samples available for pre-training, the results are similar to the ISIC2019* data, withhaving a similar performance when compared to, and substantially higher than. In other words, while there is a substantial (fine-tuning) performance gap between the SL and SSL encoders, after further pre-training the SL encoder, this gap becomes much smaller. It is surprising this is possible even with an extremely low number (62) of malignant samples.

SECTION: 5Discussion

In our study, we compared two ResNet-50 base networks that had been pre-trained on ImageNet using SL and SSL (via Barlow Twins). We fine-tuned both networks on a small skin lesion dataset and found that SSL pre-training led to superior performance. Then we performed linear probe experiments, training just the final linear layer, with the SSL network similarly superior. These results suggest that SSL pre-training on natural image datasets such as ImageNet produces more transferable features out of the box, while the final few layers of the SL pre-trained network may overfit to the pre-training task of classification of 1000 ImageNet categories. These considerations motivated the strategy of further SSL pre-training, particularly to enhance the SL pre-trained model.

We further pre-trained both base networks on several task-specific datasets of different sizes, including only the available fine-tuning data. We found that doing so generally enhanced performance, and using the fine-tuning data (and further pre-training the final 3 layers of the network) was equivalent to using larger datasets. Significantly, after further pre-training, the performance gap between the base networks disappeared. The SL encoder was significantly enhanced and the SSL encoder was mildly enhanced, after being further pre-trained. This was somewhat surprising as there was a substantial initial performance gap, resulting from large-scale pre-training on ImageNet. Although SL pre-training overfits the final few layers to the pre-training task, we found that minimal further pre-training is enough to undo this.

One can view further pre-training as a kind of ‘pre-processing step’ before fine-tuning, except the pre-processing is on the model weights, instead of the data itself. More formally, given a training dataset for fine-tuning, it is common to first normalise the features ine.g. to have mean 0 and standard deviation of 1; note that this is a function just of, i.e..
Similarly, given a pre-trained network(whereis to be fine-tuned on), one can instead perform a ‘pre-processing’ step by further pre-trainingon. This is now a function of both the model weights and the unlabelled data, i.e.but can still be viewed as a pre-processing step since the labelsare not used. We note this and recall the initial results comparing the base models:, where SSL pre-training outperformed SL pre-training. However, after further pre-training both base models on, we have:. Hence, the initial performance gap between SL and SSL pre-trained models closes after targeted, further pre-training, even whenis small. We performed additional linear probe experiments to understand this result. After further pre-training of both base networks, the linear probe performance was significantly higher for each. In fact, after further pre-training the SSL encoder, linear probe and full network fine-tuning gave equivalent results. This was somewhat surprising since the network had already been pre-trained with SSL via Barlow Twins using the entire ImageNet dataset, which features about 1.2 million samples. Apparently, further pre-training only the final 3 layers using only 2,554 samples is enough to enhance the encoder representation such that linear probe and full network fine-tuning give similar results.

Our findings present a simple method for enhancing supervised pre-trained models. Since supervised pre-training on ImageNet is a ubiquitous technique in medical image classification2.1, the findings are broadly applicable. The results also challenge the necessity of large-scale self-supervised pre-training on datasets like ImageNet - at least in the setting, where there is limited medical image data available for downstream fine-tuning. This approach opens up new possibilities for efficient model development in resource-constrained environments, which is especially relevant in medical imaging where large labelled datasets are often unavailable or inaccessible due to privacy concerns, high collection costs, or the rarity of certain conditions. Our approach also offers a streamlined method for evaluating new SSL techniques. Instead of extensive pre-training on ImageNet, researchers can assess a new SSL method’s effectiveness by performing minimal further pre-training on a small dataset, using ImageNet SL weights as initialisation. This approach could substantially reduce the computational resources and time needed to evaluate new SSL techniques, potentially accelerating innovation in SSL pre-training for downstream medical image classification.

A limitation of this work is it only involved the ResNet-50 architecture, due to the availability of ImageNet pre-trained models. Future work could investigate SSL pre-training for other architectures, such as vision transformers[14]. This is of particular interest since recent work has found that on some medical classification problems, vision transformers can outperform CNNs when both are pre-trained on ImageNet[2]. Moreover, vision transformers appear to benefit particularly from scale. Dosovitskiy et al.[14]found that vision transformers can match or exceed ResNet transfer performance when pre-trained on datasets larger than ImageNet. Investigating how our findings extend to these more recent architectures, could provide further insights into efficient pre-training strategies for medical image analysis.

Given the proven benefit of model scale in many contexts[36,14], it’s natural to wonder if using a larger ResNet architecture for ImageNet SSL pre-training leads to performance gains. Unfortunately, at present there are no open-source (ImageNet pre-trained) Barlow Twins models larger than a ResNet-50. However, there exist larger models pre-trained with VICReg[8]. VICReg is a self-supervised learning algorithm that is very similar to Barlow Twins and performs comparably on ImageNet transfer[8]. We fine-tuned a ResNet-50 on ISIC2019* dataset that had been pre-trained with VICReg and found it performed identically to Barlow Twins, as expected. To explore the effect of increasing model size, we then considered fine-tuning a VICReg pre-trained ResNet-200 (x2)[8]. This network has an encoder dimension of 4096 and about 250 million parameters, compared to 23 million parameters for a ResNet-50, making it an order of magnitude larger in parameter count. Interestingly, the performance was the same as for the ResNet-50. This suggests that the performance of such SSL pre-training methods may be saturated with respect to the ResNet architecture and ImageNet - at least in limited data scenarios. Future work could explore whether this saturation holds for other architectures or when conducting pre-training on datasets other than ImageNet.

SECTION: 6Conclusions

We compared two ResNet-50 base networks that had been pre-trained on ImageNet using supervised learning and self-supervised learning on a small labelled skin lesion dataset. We found that self-supervised pre-training led to better fine-tuning and linear probe performance. We demonstrated that further self-supervised pre-training on small, task-specific datasets can enhance initially supervised pre-trained models to match the performance of those initially pre-trained via self-supervised methods on large datasets such as ImageNet. This approach could significantly reduce
computational requirements for developing effective models in limited-data scenarios.

Our study contributes to the growing body of research on transfer learning and self-supervised learning, applied
to medical image classification. As the field of artificial intelligence for medical applications continues to evolve, techniques that can maximize performance with limited data will be crucial in expanding the reach and impact of these technologies, particularly in resource-constrained healthcare settings.

SECTION: Code and Data

Our code is open source on GitHub444https://github.com/hamish-haggerty/base_rbt.

SECTION: References

SECTION: Appendix AAppendix

SECTION: A.1Fine-tuning implementation details

The ISIC2019 dataset consists of images of varying dimensions, but deep learning models such as ResNet-50 require an input of homogeneous dimensionality. Therefore, we resize the data to. A batch size of 64 is used to suit our deep learning models. During training, we apply random data augmentation before passing a mini-batch through the network where each element of the batch is randomly cropped, rotated and flipped. The rotation is by a random angle indegrees, and the resize scale and resize ratio for cropping areand, respectively. Data augmentation applied in this way during supervised learning in a standard strategy to prevent overfitting. The models observe several slightly different views of each image during training which can be seen in FigureLABEL:fig:cancer_linear. Each column represents a mini-batch of size 2, with data augmentation. This is an example of slightly different views of the same data which is presented to the model during training.

Test time augmentation is another strategy to improve generalisation, closely related to the above technique[58]. The standard way of computing probabilities at test time is to take the test data, pass it through the neural network, and return an output probability through a softmax layer. Test time augmentation simply repeats this process several times, for augmented views of the test data. This will return several probabilities for each test input, which are then averaged to get the final probability. For example, in FigureLABEL:fig:cancer_linear, test time augmentation would compute the probabilities for each column the standard way, and then average the probabilities. This is because the columns represent the same data but under different augmentation. We use exactly the same augmentations as during training, i.e. as in Table11, and compute the average across three probabilities for each test input. Predictions are then made by taking the argmax of these probabilities, as usual.

The 1cycle training policy is a learning rate and momentum scheduler[62,61]. It involves starting training with a very low learning ratewhich is then increased up to a maximal learning value. Next, the learning rate is slowly decreased to, and for the last several epochs, to a much lower final learning rate, where. In other words, the learning rate has an increasing period, fromup to, followed by a decreasing period down to. The momentum is also scheduled, but inversely to the learning rate. Momentum decreases at the start of training, to a minimum, then is increased to a maximum. The scheme is most easily understood pictorially and can be seen in Figure6.

The 1cycle schedule diverges from standard approaches, which predominantly involve learning rate decay - starting at the maximal learning rate and decreasing to a minimum[77]. Smith[61]showed that increasing the learning rate rapidly at the start of training has a regularising effect. It has also been found that when using this policy other forms of regularisation must be reduced - hence we do not consider dropout and similar techniques in this work. Moreover, the policy has a particular advantage when the amount of training data is limited, making it especially suited to our purposes.
An important hyperparameter in the 1cycle policy is the maximal learning rate (i.e. the peak in6). In fact, this hyperparameter can be automatically inferred. A learning rate finder launches a mock training session and trains the model for several iterations, increasing the learning rate each time and recording the loss. The learning rate starts at a very low value and increases to a high value. A representative plot of this procedure can be seen in7. The loss will decrease at the start, before eventually increasing (or perhaps oscillating). A maximalis chosen that is somewhere in between a sharp valley and the minimum[25].

SECTION: A.2Barlow Twins implementation details

When further pre-training with Barlow Twins, we applied the data augmentation strategy with the same probabilities and order as described in the literature[78,22]and depicted in Table12. We generally maintained the same level of data augmentation procedures, such as blur intensity and color jitter, with the following exceptions:

Solarisation: We used the defaults from theKornialibrary, which allows greater flexibility, whereas Zbontar et al.[78]used the default fromPIL.ImageOps. Specifically, we used a solarisation threshold (sol_t) of 0.1 and an addition value (sol_a) of 0.1. In our implementation, pixels with values above the threshold are inverted, and then the addition value is added to the affected pixels. In contrast, the implementation by Zbontar et al. usesImageOps.solarize()without arguments, which implicitly uses a threshold of 128 (equivalent to 0.5 in the normalized [0,1] range) and does not apply an addition step.

Batch-wise augmentation: Several augmentations had a ‘same_on_batch’ parameter set to ‘False’, allowing for variation across the batch (e.g., varying blur intensity for each image in a batch). This is a minor improvement over other implementations which will add the same blur intensity to all elements of the batch.

Watermark handling: Since the open source DermNet images contain watermarks, we implemented a random dropout augmentation (cutout) to cover the text. This was applied as the second augmentation after cropping, only in the case DermNet is included in the pre-training data.555This was implemented using a custom RandomCenterDropout class. It randomly drops out (sets to zero) a rectangular region in the center of the image with a 33% probability. The region’s dimensions vary randomly: width from 50 to 100 pixels and height from 185 to 190 pixels, designed to approximately cover the watermark’s variable position.

Due to memory constraints, we reduced the batch size from 1024 (used when training on ImageNet) to 128[78]. We utilised PyTorch and FastAI frameworks, along with the self_supervised library[71]. Our complete code implementation is available on GitHub666https://github.com/hamish-haggerty/base-rbt.

SECTION: A.3Statistical significance

In this section we display statistical significance tables for fine-tuning results from Table4and linear probe results from Table6.