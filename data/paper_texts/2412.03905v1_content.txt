SECTION: Integrating Various Software Artifacts for Better LLM-based Bug Localization and Program Repair

LLMs have garnered considerable attention for their potential to streamline Automated Program Repair (APR). LLM-based approaches can either insert the correct code using an infilling-style technique or directly generate patches when provided with buggy methods, aiming for plausible patches to pass all tests. However, most of LLM-based APR methods rely on a single type of software information, such as issue descriptions or error stack traces, without fully leveraging a combination of diverse software artifacts. Human developers, in contrast, often use a range of information — such as debugging data, issue discussions, and error stack traces — to diagnose and fix bugs. Despite this, many LLM-based approaches do not explore which specific types of software information best assist in localizing and repairing software bugs. Addressing this gap is crucial for advancing LLM-based APR techniques.

To investigate this and mimic the way human developers fix bugs, we proposeDEVLoRe(short forDEVeloperLocalization andRepair). In this framework, LLMs first use issue content (description and discussion) and stack error traces to localize buggy methods, then rely on debug information in buggy methods and issue content and stack error to localize buggy lines and generate valid patches. We evaluated the effectiveness of issue content, error stack traces, and debugging information in bug localization and automatic program repair. Our results show that while issue content and error stack is particularly effective in assisting LLMs with fault localization and program repair respectively, different types of software artifacts complement each other in addressing various bugs. By incorporating these three types of artifacts and using the Defects4J v2.0 dataset for evaluation,DEVLoResuccessfully localizes 49.3% of single-method bugs and generates 56.0% plausible patches. Additionally,DEVLoRecan localize 47.6% of non-single-method bugs and generates 14.5% plausible patches. Moreover, our framework streamlines the end-to-end process from buggy source code to a complete repair, and achieves a 39.7% and 17.1% of single-method and non-single-method bug repair rate, outperforming current state-of-the-art APR methods. The source code and experimental results of this work for replication are available athttps://github.com/XYZboom/DEVLoRe.

SECTION: 1.Introduction

Automatic Program Repair (APR) streamlines the process of identifying and correcting code defects, significantly reducing the time and effort required for manual bug fixing(Le Goues et al.,2019). Traditional APR techniques employ various methods, including template-based approaches(Ghanbari et al.,2019; Le Goues et al.,2011; Le et al.,2016; Liu et al.,2019; Long and Rinard,2015; Mechtaev et al.,2016)and neural machine translation (NMT)(Chen et al.,2019; Jiang et al.,2021; Li et al.,2020; Zhu et al.,2021), to generate potential patches that are both syntactically valid and semantically meaningful. Although these methods can produce correct patches for certain bugs, they also have notable limitations. NMT models rely heavily on bug-fixing training data, making them unable to generate patches for new or unseen types of bugs. On the other hand, template-based approaches suffer from a limited set of templates and struggle to address more complex, nontrivial bug fixes(Xia et al.,2023b).

Recent studies have explored the use of LLMs for APR, either by having LLMs fill in the correct code in buggy methods using an infilling-style technique or by directly generating patches when provided with buggy methods(Xia and Zhang,2023; Xia et al.,2024a,b; Li et al.,2024; Xia et al.,2023b,a; Jiang et al.,2023; Wei et al.,2023; Zhang et al.,2023; Silva et al.,2023). The initial results demonstrate the ability of LLMs to correctly repair real-world bugs, including those that were previously unrepairable by existing APR approaches. One of the best-performing frameworks isAgentless(Xia et al.,2024a), which feeds an LLM with only issue description and can automatically solve GitHub issues. These promising outcomes highlight the potential of LLMs to develop more effective APR methods.

However, there are still two majorlimitationsthat need to be addressed:

Current LLM-based approaches do not fully incorporate various types of software artifacts. Most LLMs rely on just one or two kinds of software artifacts, such as issue descriptions or code structure, while other important artifacts are underutilized. For instance, debugging information, which is a critical tool for human developers in diagnosing and resolving bugs, is often not fully leveraged.

While various LLM-based approaches make use of different software artifacts, such as issue descriptions and error stack traces, it remains unclear which specific type of information most effectively aids LLMs in localizing and automatically repairing software bugs.

To address these two limitations and further explore the ability of LLMs to localize and fix software bugs (bug, defect, and fault are used interchangeably in this paper), we propose feeding LLMs different types of software artifacts to determine which information best leverages their capabilities in bug localizing and fixing. The rationale behind this approach is that human developers typically do not rely on a single type of information when localizing and fixing software bugs. Instead, they combine various sources of information in software development, such as issue descriptions, proof of concept (PoC), stack traces, discussions in issues, and more. Based on developers’ experience, having more information helps to better understand the root cause of bugs, ultimately leading to more effective bug localization and fixes.

To achieve this, we propose theDEVLoReframework, which asks LLMs to mimic human developers for bug localization and program repair. Along with this framework, we design two tools to extract executed methods in failed test cases and debugging information of buggy methods. In this framework, we feed the chosen LLM with three types of software artifacts: issue content (including issue description and discussion), error stack trace, and debug information. Then, using the well-known Defects4J dataset(Just et al.,2014), we evaluate how these different types of software artifacts contribute to effectively localizing and fixing bugs. Our experiment results demonstrate that issue content is the most effective indicator for buggy method’s localization, achieving 43.6% for localizing single-method bugs and 40.6% for localizing non-single-method bugs. Stack error trace proves to be the best indicator for single-method bug fixing, with a precision of 27.0%. Additionally, our findings highlight that different types of software information can complement each other in the process of localizing and fixing bugs. By combining issue content and error stack trace, we achieve a state-of-the-art performance of 49.3% in localizing the single-method bugs. Furthermore, incorporating issue content, error stack trace, and debugging information results in a fix rate of 43.1% for single-method bugs.

As ourDEVLoReframework does not specify that the localization and fix of buggy methods should be singular, it can localize and fix bugs across multiple methods (non-single-method bugs). For example, by combining all three software artifacts,DEVLoRecan fix 9.4% of non-single-method bugs with provided buggy locations. Moreover, similar toAgentless(Xia et al.,2024a),DEVLoRerelies on LLMs throughout the entire end-to-end process of fault localization and program repair. It can successfully fix 28.0% of single-method bugs and 11.2% of non-single-method bugs when combining all three artifacts. To the best of our knowledge, our approach outperforms current state-of-the-art methods.

To summarize, ourcontributionsin this paper are as follows:

To the best of our knowledge, this work is the first to compare different software artifacts in assisting LLMs’ ability to perform fault localization and program repair. The results can help developers understand LLMs’ potential when provided with a variety of software artifacts.

We propose a simple and lightweight framework that leverages LLMs to conduct an end-to-end process for bug localization and program repair. Accompanying this framework we design a strict input/output prompt. TheDEVLoReframework demonstrates a strong ability to localize and fix more software bugs in less time and at a lower cost, compared to current state-of-the-art methods.

The paper is structured as follows: Section2discusses the motivation of using various software artifacts, Section3outlines the proposed approach, Section4introduces the experiment setup and the research questions, Sections5and6present and discuss the results respectively, Section7reviews the related work, and Section8concludes this work with future directions.

SECTION: 2.Three Motivating Examples

Our assumption is that LLM-based program repair tools have the potential to be much more effective if they can leverage a variety of software artifacts, such as code snippets, version history, documentation, and even testing outputs. By providing LLMs with a rich set of contextual data, they can understand and localize bugs more accurately and offer more precise fixes. This assumption is based on a human software engineer’s daily practice. When a human developer tries to fix a bug, they would examine various resources such as the issue descriptions, error stack, debugging information, and test cases, until a solution is identified. However, currently most LLM-based repair approaches underutilize or use limited software artifacts. Here, we provide three examples to demonstrate the motivation of this work and the ability of LLMs to localize and fix bugs when given access to different types of software artifacts.

SECTION: 2.1.Debugging Information to the Rescue

The Lang 1b bug involves thecreateNumbermethod, which takes a string as its parameter and returns a number represented by that string. While studying the Lang 1b bug, we noticed that the newly added failing test case is the input “0x80000000” for thecreateNumbermethod. When this test case is provided, the buggycreateNumbermethod incorrectly treats the input string as an integer, when it should be a long integer instead. The code snippet in Figure1shows a portion of the buggycreateNumbermethod in Lang 1b. The local variablehexDigitsrepresents the valid hexadecimal digits. As human software developers, we can quickly recognize that at Line 471, the condition should be “hexDigits > 8 || (hexDigits == 8 && firstSigDigit > ’7’)” (firstSigDigitrefers to the first valid hexadecimal digit).

For this bug, we provided the LLM with Lang 1b’s issue content (https://issues.apache.org/jira/browse/LANG-747) and the stack error message shown in Figure2, but both could not help the LLM generate a plausible fix or near-to-correct patch. This is because the leap required in reasoning is significant, and the buggy method is quite long. Neither of the information provided above could pinpoint the buggy line precisely, let alone generate a plausible fix. To address this, we introduced debugging information to help the LLM understand the necessary changes. One piece of debug information we provided was a series of variable-value pairs,commons.lang3.math.NumberUtils:createNumber:468{hexDigits:8}. This indicates that the code is about to execute Line 468 and that the value ofhexDigitsis 8 for the given test case. This enables the LLM to understand why the code on Line 474 was executed instead of Line 471. In one of the responses, the LLM suggested changing Line 468 toif (hexDigits > 16 || (hexDigits == 8 && str.charAt(2) >= ’8’)), which, although not completely accurate, demonstrates that the provided debug information has a positive impact on fixing the bug.

SECTION: 2.2.Issue Content to the Rescue

Sometimes, the discussions and Proof of Concept (PoC) included in the issue content can help the LLM better understand the expectations for bug repair. Figure3shows a developer’s comment in the issue content, which states the expected behavior of the program (https://issues.apache.org/jira/browse/LANG-432). The original code attempted to convert both input strings to uppercase usingString.toUpperCase()and then return the result of invoking thecontainsmethod. However,String.toUpperCase()is locale-sensitive, which makes it unsuitable for case-insensitive comparisons. For example, the character0x00DFrepresents “ß” in Unicode. If we applyString.toUpperCase()to this character, it becomes “SS” in the Turkey locale. Consequently, comparing “ss” with “ß” would result in an equal match. Therefore, in this case, we should not useString.toUpperCase()but instead compare the characters individually.

The debug information,org.apache.commons.lang.StringUtils:containsIgnoreCase:1045{“str”:“ß”,“searchStr”:“SS”}, and the error stack trace shown in Figure3merely re-display the test cases and do not clearly highlight the relationship between the character0x00DFand ’SS’. As a result, neither the error stack trace nor the debug information provides any useful clues about the bug, and LLMs cannot generate a plausible fix when provided with either the stack message or the debug information alone. However, with the description and expected behavior outlined in the issue content, GPT-4o is able to generate a correct patch.

SECTION: 2.3.Error Stack to the Rescue

When a bug triggers an exception, as opposed to merely being an unexpected behavior, the error stack trace generated at the point where the exception occurs can be helpful in diagnosing and fixing the error. Stack trace provides a detailed record of the sequence of method calls that led to an exception, making it easier to pinpoint the exact location in the code where the bug arises.

For example, in Lang 39b shown in Figure4, the developer forgot to check fornullvalues in the elements of the input parametersreplacementListandsearchList, which can lead to a NullPointerException (NPE). However, if we do not inform the LLM about the occurrence of an NPE in the error stack, the LLM will not be able to identify wherenulldetection is necessary. The stack trace clearly indicates that the NPE occurs on Line 3676 in the classStringUtils. By providing this stack information to the LLM, it is able to generate a repair, as shown in Figure5.

SECTION: 2.4.Observations From the Above Three Examples

From these three examples, we can see that issue content, error stack traces, and debugging information can complement each other in bug detection and fixing. Each type of artifact provides a different level of context, and when combined, they create a more comprehensive understanding of the bug. Issue content offers the high-level description and expectations for the fix, error stack traces pinpoint the exact location of the failure, and debugging information provides granular details about the variable states and flow of execution. By incorporating these diverse artifacts, the LLM’s repair process becomes more holistic.

SECTION: 3.Approach

As shown in Figure6, our approach utilizes LLMs in two distinct steps: bug localization and program repair. Along with these two steps, we developed two dynamic tools —MethodRecorderandDebugRecorder— to assist LLMs in handling bug localization and fix.MethodRecordertracks the methods executed during the failing test case(s), whileDebugRecorderis designed to extract debug information from the buggy method(s). Debug information is stored in a list, where each entry includes the method name, line number, variable names, and the JSON-formatted content of these variables. Table1shows the prompts used in our process. The first, second, and third rows correspond to the prompts used for the tasks of localizing buggy methods, localizing buggy lines, and generating patches, respectively. The first two rows of prompts are used for the bug localization task and the last row of prompt is used for the program repair task. For each task, we provide the LLM with <General Task Prompt> + <Input Prompt> + <Expected Output Prompt> to receive the response from the LLM.

SECTION: 3.1.Localize Buggy Method(s)

We emulate the behavior of human developers, who use errors in stack traces and proof of concept (PoC) from issue content to localize buggy methods. To this end, we applyMethodRecorderto trace the methods invoked during the execution of failing test cases. We then collect the signatures of these invoked methods and prompt the LLM to identify the buggy method(s). In this step, we also provide the LLM with any available error stack traces and issue content, including general description of the issue and developers’ discussion/comment under this issue. We choose not to use debugging information because it is typically gathered at runtime and involves instrumentation (seejava.lang.instrument.Instrumentation(jav,[n. d.])), which can be computationally expensive. In developers’ daily practice, they set just a few of breakpoints and collect important information about variables. But setting breakpoints automatically is not practical. Therefore, before localizing buggy method(s), minimizing instrumentation and debugging information is more efficient.

SECTION: 3.2.Localize Buggy Line(s)

Once the buggy method(s) have been localized in the previous step, we utilizeDebugRecorderto capture detailed information about variable names and their values within the identified buggy method(s). The rationale for introducing debugging information at this stage is that it provides a more granular view, which is particularly beneficial for pinpointing the specific lines responsible for the bug and for understanding the internal state of variables. The debugging information with variables’ value is often unnecessary during the initial localization step, but becomes crucial when the focus shifts to identifying precise faults within a known problematic method.

In this process, we gather debugging information as a list of variable-value pairs that reflect the state at specific lines of code within the buggy method(s). This allows the LLM to understand the conditions and data flows that may contribute to the bug. Combined with the issue content, such as descriptions and discussions, along with stack traces, this debugging information is fed into the LLM along with the body of the buggy method(s). Together, these resources assist the LLM in localizing the exact line(s) within the method(s) where the bug manifests, providing the context needed to understand and resolve the bug effectively.

SECTION: 3.3.Patch Generation

For patch generation, we provide all relevant information to maximize the LLM’s ability to produce an accurate repair. This includes the localized buggy buggy line(s) and method(s) identified in previous steps, as well as the complete body of the buggy method(s). Additionally, we provide supplementary materials such as the issue content, which may contain descriptions and discussions about the bug, the stack trace from the error, and detailed debugging information captured at runtime. By supplying this comprehensive context, we enable the LLM to better understand the specific code behavior that led to the error and to use this knowledge to generate a more targeted and effective patch for the bug. For each bug, we use the LLM to generate multiple potential patches in.diffformat, which can be directly applied to the buggy code.

SECTION: 3.4.Patch Validation

We apply the patches generated in the previous step to the buggy code and test them to evaluate their effectiveness. If the patch passes the initial failed tests, we proceed it to pass all tests in case that it incur any regression. If it passed all the tests, then it is considered a plausible fix. To facilitate this process, we utilize the Defects4J framework to run the tests and verify the patches.

To be consistent with existing studies(Zhu et al.,2023; Li et al.,2024), if the patch is semantically equivalent to the original patch provided by developers, it is considered a correct patch. As part of this validation process, the patch undergoes manual inspection and cross-checking by two experienced developers to ensure its correctness. This step is crucial to verify that the patch not only passes all automated tests but also aligns with the intended behavior from a developer’s perspective. The manual inspection by developers serves as a final quality control step to ensure that the patch addresses the bug correctly and does not introduce new bugs.

SECTION: 4.Experiment Setup

SECTION: 4.1.Dataset

We used the well-established Defects4J benchmark(Just et al.,2014)for our experiments, specifically leveraging both version 1.2 and version 2.0. Defects4J v1.2 contains 391 bugs from six real-world projects (note that there are 395 bugs in v1.2, but due to the update to Java 8, four bugs can no longer be reproduced), while v2.0 includes an additional 444 bugs from 11 real-world projects. We chose Defects4J benchmark for two main reasons. First, it offers a diverse set of software artifacts — such as issue URLs, error stacks, and test cases — that are well-suited for our proposed ARP framework and help maximize its capabilities. Second, Defects4J benchmark has been widely used in prior research, which facilitates a fair and meaningful comparison of our approach.

SECTION: 4.2.Parameters in Experiments

SECTION: 5.Results

SECTION: 5.1.RQ1. Buggy Method and Buggy Lines Localization from Code Repository

We first use ourMethodRecordertool, which runs the failing tests and records the signatures of the methods that have been executed to narrow down the scope of buggy methods. Then we provide the LLM with the signatures of the executed methods, along with issue content (including the issue description and discussion, which we crawled from each bug’s issue URL) and the error stack trace corresponding to the failing test case, as supplied by the Defects4J framework. In this step, we choose not to generate the debugging information. The main reason is that the debugging information can be too long for LLMs to process effectively before the buggy method(s) have been localized. Using the prompt specified in Table1, we ask the LLM to output the signatures of buggy method(s). To evaluate what kinds of software artifacts can better assist the LLM in localizing buggy methods, we conduct four separate experiments by feeding the LLM with (1) only executed method signatures, (2) executed method signatures and issue content, (3) executed method signatures and error stack trace, and (4) executed method signatures, issue content and error stack trace.

Table2shows the effectiveness of buggy method localization in these four experiments. The first row, labeled “—”, indicates that only the related (executed) methods are fed to the LLM for buggy method localization. The second, third and fourth rows represent scenarios when, in addition to method localization, issue content, error stack, or both are provided. Following the approach described in LLMAO(Yang et al.,2024a), we consider a buggy method correctly localized if at least one match in the LLM’s response corresponds to the buggy method (single-method bugs) or one of the buggy methods (non-single-method bugs) in the ground truth. It is worth mentioning that there exist bug cases where certain software artifacts can not be extracted, and in this case we calculate the ratio by dividing only the number of bug cases with certain software artifacts information. Table2shows that, when issue content or error stack is provided, the LLM is able to localize more buggy methods than nothing is provided. When issue content is provided, 15.8% more single buggy methods and 16.5% more non-single buggy methods are correctly localized. When error stack is included, 16.6% more single buggy methods and 14.3% more non-single buggy methods are correctly localized. Furthermore, combining issue content and error stack achieves the best performance, which results in 49.3% in localizing single buggy methods and 47.6% of non-single buggy methods.

Figure7shows the overlap of results of buggy method localization with different artifacts in Defects4J v2.0. We can see that when provided with issue content, the LLM can localize 19 extra bugs correctly. Similarly, when provided with error stack, the LLM can localize 37 extra bugs correctly. This finding verifies our assumption in Section2that different kinds of software artifacts can complement each other in buggy method localization.

As existing SOTA fault localization approaches use Defects4J v1.2 dataset and adopt Top-n(the buggy method is in the toplist of the LLM’s response) to evaluate the performance of fault localization, we compare the performance ofDEVLoRewith the SOTA approaches in the same dataset and adopt Top-napproach. Table3shows the comparison results where AgentFL(Qin et al.,2024)is LLM-based, GRACE(Lou et al.,2021)and FLUCCS(Sohn and Yoo,2017)are learning-based. As we can see,DEVLoReoutperforms another LLM-based approach AgentFL(Qin et al.,2024)in Top-1, Top-3 and Top-5 metrics. As discussed in(Qin et al.,2024), the Closure project contains many bugs with similar code, making it more suitable for learning-based approaches. This characteristic allows GRACE(Lou et al.,2021)and FLUCCS(Sohn and Yoo,2017)to achieve an overall high localization rate. The last row shows that, without the Closure project,DEVLoReoutperforms other approaches in Top-1 and achieves similar performance in Top-3 and Top-5, demonstrating its ability to precisely localize buggy methods.

To evaluate LLMs’ ability in localizing buggy lines, we provide the LLM with the buggy method(s) baseline (i.e., the ground truth which can be extracted from human developer’s correct patch) and ask the LLM to predict which lines are buggy. If the line number generated by the LLM exactly matches the first line added in the human developer’s correct patch in Defects4J, we consider it an exact match. If the predicted line number falls within a range oflines from the first added line in the human developer’s patch (for example, if the correct patch starts at Line 368, and the LLM’s prediction falls within the range fromto), we classify it as a Range-nmatch.

Table4shows the effectiveness of buggy line localization with different artifacts. We can see that when the LLM is fed with issue content, it performs the best among all single software artifacts, achieving 49.5% in exact matches, 74.4% in Range-3, and 77.5% in Range-5 for single methods. When adding more software artifacts, the performance can be further increased. For example, when adding issue content to error stack, the exact, Range-3, and Range-5 buggy line match for single methods can be increased from 45.0% to 49.1%, 66.9% to 73.2%, and 71.0% to 77.5%. The last row indicates that, when combining all artifacts, 68.9% of buggy lines in single-method bugs and 9% of buggy lines in non-single-method bugs are correctly localized. Similar to Figure7, Figure8demonstrates that, while most of the correctly localized buggy lines across different artifacts overlap, different combinations of artifacts each have their own advantages (since only five areas can be displayed in the overlap figure, we randomly select five categories for visualization). For example, using only the issue content can correctly localize 7 more buggy lines, while using only the debug information can localize 15 more.

SECTION: 5.2.RQ2. Program Repair Based on Provided Method-level Fault Localization

To evaluate the effectiveness of program repair, we first extract buggy methods from human’s correct patches and used these buggy methods as the baseline. Then we useDebugRecorderto get the debug information in these buggy methods. Finally, we provide the LLM with different artifacts and check how many plausible patches (passing all tests) it could generate.

Table5shows that, error stack achieves the best performance in generating plausible patches for single-method bugs (27.0%) among all three software artifacts, while issue content performs the best for non-single buggy methods. If feeding the LLM with all three types of software artifacts, we can get the best performance, which is 43.1% and 9.4% in patching single-method bugs and non-single-method bugs, respectively. Figure9shows that different artifacts complement each other in program repair. Error stack can help the LLM fix 17 extra bugs while issue content can fix 13 extra bugs. Combining issue content with debug info can assist the LLM in fixing 23 extra bugs.

Table6comparesDEVLoRewith other SOTA program repair approaches. As these approaches focus on repairing single-method bugs, we compare our tool with these approaches on the same dataset. As shown in Table6, 142 out of 259 bugs in Defects4J v1.2 and 132 out of 230 bugs in Defects4J v2.0 can be repaired byDEVLoRe, outperforming the other five approaches. In total,DEVLoRecan fix 274 buggy methods, which is 60.2% more than GiantRepair(Li et al.,2024). Furthermore, Figure10shows that 105 bugs can only be fixed byDEVLoRe. This result demonstratesDEVLoRe’s ability in fixing more and extra bugs by leveraging different types of software artifacts.

SECTION: 5.3.RQ3. End-to-end Performance from Code Repository to Program Repair

Currently, most of LLM-based approaches focus on either fault localization or program repair based on the already-localized buggy methods. We investigated RQ1 and RQ2 to compare with these approaches. In contrast, ourDEVLoRefully relies on LLMs for both fault localization and program repair. Furthermore, from fault localization to program repair, we not only provide the LLM with the localized buggy method signatures and lines, but also supply it with the complete method body. When the LLM is asked for the program repair task, it may reconsider and repair other parts of the code beyond the specified buggy location. Therefore, the overall end-to-end performance from the buggy code without localization to a complete program repair cannot be simply calculated by multiplying the fault localization rate by the program repair rate. Conducting a comprehensive end-to-end assessment can offer a better understanding ofDEVLoRe’s performance throughout the entire fault localization and program repair process.

Table7showsDEVLoRe’s end-to-end performance with different software artifacts. When using issue content alone, the LLM can repair 21.8% of bugs, which is the highest among all single software artifacts. This result is also consistent withAgentless(Xia et al.,2024a), one of the best approaches in SWE-bench lite (with Python projects)(swe,2024), which relies solely on issue descriptions to assist LLMs in resolving GitHub issues.

Furthermore, by combining issue content with debugging information and the error stack, the LLM can fix an additional 1.2% and 4.0% of single-method bugs, respectively. When all three software artifacts are combined, the LLM can repair 28.0% of single-method bugs and 11.2% of non-single-method bugs. Furthermore, the combination of different artifacts achieves an end-to-end 39.7% fix rate for single-method bugs and 17.1% for non-single-method bugs. Figure11shows similar observations as other two RQs, that different combinations of software artifacts can complement each other in the end-to-end process of bug localization and program repair.

We divide the time spent on each process by the number of fixed bugs to calculate the average time required to fix a bug. Table8shows the average time spent to fix a bug in each process ofDEVLoReusing our hardware device (Intel(R) Xeon(R) Platinum 8352V CPU  2.10GHz, 120GB RAM). We observe that the entire localization and repair process takes approximately 15 seconds, with the evaluation of patches being the most time-consuming step, as it involves running all relevant tests. Additionally, since we select the most cost-efficient GPT-4o-mini model, the average cost to fix a bug is$0.057. We calculate this by dividing the total cost of using the LLM when issue content, error stack, and debug information are provided by the number of plausible patches generated, which is 147.

Using ourDEVLoReframework for this end-to-end software maintenance activity, 253 (194 for single buggy methods and 59 for non-single buggy methods) plausible patches can be generated. To the best of our knowledge,DEVLoReoutperforms all current SOTA end-to-end approaches in the Defects4J dataset (the plausible rate ofToggle(Hossain et al.,2024)is 24.2% for 58/240 single-hunk bugs and the plausible rate ofFixAgent(Lee et al.,2024)without Web search engine is 245/835=29.3% at a cost of $0.364 per bug).

SECTION: 6.Discussion

SECTION: 6.1.Analysis of Results

Unlike approaches that rely on patch skeletons(Li et al.,2024), fix templates(Zhang et al.,2023), or type checking(Zhu et al.,2023),DEVLoReadopts a straightforward framework that allows LLMs to handle both fault localization and program repair with the aid of two lightweight tools we implementedMethodRecorderandDebugRecorder, making it both simple and efficient. The findings of RQ1, RQ2, and RQ3 demonstrate that different software artifacts may lead to different performance in bug localization or program repair. Also, because there is no constraints such as fill-in-the-blank templates or code skeletons for generating patches(Li et al.,2024; Zhang et al.,2023),DEVLoRecan fix bugs that other tools cannot, especially some non-single method bugs. Our results in Table7show that by combining issue content, stack error, and debug information, our approach can fix 11.2% of non-single method bugs. More importantly, combinations of software artifacts can achieve the best performance in all experiments: bug localization, program repair, and the overall streamlined process. One explanation for the strong performance ofDEVLoReis that by integrating multiple sources of information, the noise inherent in any single source can be significantly reduced(Meng et al.,2024). This allows the LLM to focus on the consistent and complementary aspects of the different artifacts, enabling it to perform more effective reasoning and making better use of the available data. While it may be argued that the good results ofDEVLoReare due to the use of GPT-4 models, a recent study shows that directly using the GPT-4 model does not improve the fix rate(Li et al.,2024). Therefore, we argue that our proposed framework is the key to achieving the high rate in fault localization and program repair.

Table1presents the prompts used in ourDEVLoReframework. In thegeneral task prompt, the LLM is asked to act as a software engineer and conduct the review process, helping the LLM form a clear understanding of the overall task. Theinput promptincludes various types of software artifacts, with clear symbols denoting different hierarchy levels and structures. For example, the{related_methods}in the input prompt wraps the class names in ### symbols, and the method signatures are separated by line breaks. Also, the first line of{debugging_info}in the input prompt represents the currently executed method line, and the second line represents the names and values of the variables in current context (e.g.,commons.lang3.math.NumberUtils:createNumber:468{hexDigits:8} represents that the code is about to execute Line 468 and the value of the local variablehexDigitsis 8). These strict formatting specifications helps the LLM “understand” the structure of the various information from different software artifacts. Theexpected output promptis very strict inDEVLoRe. When localizing buggy methods, we ask the LLM to return a set of buggy method or field locations in the formatpath.to.ClassA::methodA. For buggy lines, we request the LLM to return a set of buggy line locations in the formatpath.to.ClassA line:20. During program repair, we employ the well-knownSEARCH/REPLACEmethod, which is commonly used in many state-of-the-art program repair approaches(Xia et al.,2024a; Ouyang et al.,2024; Zhang et al.,2024; Liu et al.,2024). By enforcing this strict output format, it significantly reduces the likelihood of hallucinations from the LLM. We believe that the clear and strict prompt design in ourDEVLoReframework has helped the LLM achieve strong performance in fault localization and program repair.

X/Y denotes X correct patches and Y plausible patches.

Most state-of-the-art (SOTA) program repair approaches use plausible patches that pass all unit tests as an important evaluation metric, since generating plausible patches within limited time and resources is crucial for practical applications. To facilitate a better comparison, we also used plausible patches in RQ2 (Which software artifacts can better assist LLMs in generating valid patches when provided with buggy methods?) and RQ3 (What is the overall performance ofDEVLoRein bug localization and program repair?). Some may argue that a high number of plausible patches does not necessarily translate to a high number of semantically correct patches. To address this, we manually inspected the plausible patches, using the approach described in Section3.4. Table9compares the end-to-end repair results on Defects4J v1.2, as all the baselines consistently used this benchmark. OurDEVLoReframework uses the LLM to localize buggy methods and lines, whereas all three baselines — GiantRepair(Li et al.,2024), Tare(Zhu et al.,2023), and TBar(Liu et al.,2019)— employ the spectrum-based algorithm Ochiai(Abreu et al.,2007), implemented by GZoltar(Campos et al.,2012), to localize buggy methods. As shown in Table9, the ratio of correct to plausible patches (54.9%) produced by ourDEVLoReis lower than that from Tare (60.0%) and from GiantRepair (57.7%). This difference is understandable, as GiantRepair enforces an AST patch skeleton and Tare uses a typing-checking mechanism. Notably, 137 out of 253 plausible patches generated byDEVLoReare evaluated as correct patches, which is significantly higher than the 64 in GiantRepair, 57 in Tare, and 31 in TBar. This finding demonstratesDEVLoRe’s ability to generate candidate patches that not only pass all unit tests but are also semantically correct.

SECTION: 6.2.Threats to Validity

The first threat to validity is the accuracy of the two tools we implemented:MethodRecorderandDebugRecorder. Both tools rely on mature Java agent technology(jav,[n. d.]). We randomly selected several projects and manually verified the outputs of both tools. The manual inspection showed that the outputs were accurate. However, we did not inspect all projects, which may pose a threat to the construct validity.

Another potential threat is that the ChatGPT-4o-mini model used in this work may have been trained on open-source projects from GitHub, which could overlap with the Defects4J dataset, leading to possible data leakage. To mitigate this, we also randomly selected 100 bugs from another dataset, GrowingBugs(Jiang et al.,2022), and found the plausible fixing rate to be 39%, which may help alleviate this concern. Additionally, the debugging information requires dynamic analysis, which is unlikely to have been used during the model’s training. Also as found in(Li et al.,2024), directly using the GPT-4 model can not improve the fix rate.

The final threat to the external validity is that our experiments were conducted on Java projects, and the findings may not be generalized to projects written in other programming languages. To address this, we plan to designMethodRecorderandDebugRecorderon other programming languages and evaluateDEVLoReon additional datasets across multiple programming languages in our future work.

SECTION: 7.Related Work

SECTION: 7.1.Large Language Models for Fault Localization

Recently, there has been significant interest in using LLMs for fault localization.Toggleincorporated additional contextual information, such as the buggy line number or code review comments, and greatly enhances the accuracy of predicting both the starting and ending buggy tokens(Hossain et al.,2024).AGENTFLemploys a multi-agent system based on ChatGPT and frames the fault localization task as a three-step process: comprehension, navigation, and confirmation. In each step,AGENTFLdeploys specialized agents, each with unique expertise, and uses different tools to address specific tasks(Qin et al.,2024).CrashTrackerconducts static analysis to map each crash to the corresponding exception instance and identify potential buggy candidates. It then utilizes LLMs to enhance the explainability of the localization results(Yan et al.,2024). Jiang et al. assessed the performance of recent commercial closed-source general-purpose LLMs, such as ChatGPT 3.5, ERNIE Bot 3.5, and IFlytek Spark 2.0, on line-level fault localization with the provided buggy method(Jiang et al.,2024).LLMAOfine-tunes LLMs with 350M, 6B, and 16B parameters on small, curated corpora like Defects4J, improving Top-1 fault localization by 2.3%-54.4% and Top-5 results by 14.4%-35.6%, compared to the state-of-the-art machine learning fault localization(Yang et al.,2024a).AutoFLprompts an LLM to use function calls for navigating a repository, enabling effective fault localization in large codebases while overcoming the LLM context length limit. It also generates an explanation of the bug and suggests a fault location(Kang et al.,2024).LLM4FLcombines traditional spectrum-based fault localization with prompt chaining to divide large coverage data into manageable groups. By employing multiple LLM agents, it navigates the codebase more effectively to localize faults(Rafi et al.,2024). LikeLLM4FLandAutoFLwhich combines transitional fault localization tools,DEVLoReincorporates a static code analysis tool,MethodRecorder, to identify relevant buggy methods in a lightweight manner by invoking failing tests. However, unlike the above approaches, we provide the LLM with a combination of software artifacts, including issue content, error stack traces, and debugging information, which are commonly used by human developers for fault localization. This enablesDEVLoReto outperform state-of-the-art fault localization methods with greater efficiency and lower cost.

SECTION: 7.2.Large Language Models for Program Repair

Recent studies have extensively explored the use of LLMs for program repair.ChatRepairinitially provides the LLM with relevant test failure information and then learns from both the failures and successes of previous patching attempts for the same bug, enhancing its ability for more effective APR(Xia and Zhang,2023).GAMMAconverts various fix templates into mask patterns and leverages a pre-trained language model to predict the correct code for the masked portions, treating APR as a fill-in-the-blank task(Zhang et al.,2023).Repilotgenerates a candidate patch by combining LLM suggestions with a Completion Engine, removing infeasible tokens and filling in gaps proactively(Wei et al.,2023).FitRepairintegrates the plastic surgery hypothesis into LLM-based APR, combining the direct use of LLMs with two domain-specific fine-tuning strategies and one prompting strategy to enhance its repair capabilities. It can directly generate the correct code in context, effectively “filling in the blanks” of missing code lines or hunks(Xia et al.,2023a).Tareincorporated type checking into neural program repair model and can successfully repair 62 and 32 bugs from Defects4J v1.2 and Defects4J v2.0(Zhu et al.,2023).GiantRepaircreates patch skeletons from LLM-generated patches to narrow the patch space, then generates context-aware, high-quality patches by instantiating these skeletons for specific programs(Li et al.,2024).FixAgentunifies debugging through multi-agent collaboration and achieves strong performance with a three-layer hierarchical structure, where the final layer involves the use of a Web search engine(Lee et al.,2024).MORepairfine-tunes LLMs for program repair by adapting both to the syntactic nuances of code transformation and the underlying logic of code changes, enabling the generation of high-quality patches(Yang et al.,2024c). To leverage LLMs’ capabilities and augmented information,CREFis a semi-automatic repair framework for programming tutors, highlighting the potential for enhancing LLMs’ repair capabilities through tutor interactions and historical conversations(Yang et al.,2024b).RepairLLaMAis an innovative program repair method that finds optimal code representations for APR using fine-tuned models, and introduces a state-of-the-art parameter-efficient fine-tuning technique (PEFT) for program repair(Silva et al.,2023). Our approach differs from the aforementioned approaches in two key ways. First, we do not rely on fill-in-the-blank templates or skeletons for generating patches, which helps avoid patch overfitting problems in program repair(Smith et al.,2015; Le et al.,2018; Fei et al.,2024). Second, our fault localization step allows for the identification of multiple buggy methods, which are then fed into the LLM’s repair process along with various software artifacts. This provides flexibility to address not only single-method bugs but also bugs spanning across different methods.
As seen in Table7, the end-to-end process from localization to repair can generate plausible patches for 17.1% of non-single-method bugs, while most existing program repair approaches primarily target single-method bugs.

SECTION: 8.Conclusions and Future Work

This paper presents an LLM-based framework,DEVLoRe, for streamlining fault localization and program repair. By mimicking human developers in addressing bug problems and integrating three different software artifacts,DEVLoRedemonstrates strong performance in both fault localization and program repair, outperforming current state-of-the-art approaches in terms of bug fixing rate, time, and cost. In addition, unlike more rigid approaches that ask LLMs to fill in the blank within a single buggy method or use the fix template,DEVLoRefeed the LLM with only different software artifacts and there are no constraints on theDEVLoReframework regarding how it repairs bugs,DEVLoRehas shown significant potential in handling bugs that span across multiple methods.

Our future work will focus on the following directions: first, expandingDEVLoReto support additional programming languages, such as Python and C/C++, and evaluating its effectiveness on projects written in these languages; and second, testing theDEVLoReframework with a broader range of software artifacts, such as commit history, code review comments, and user documentation, to assess how these additional data sources can further enhance the bug localization and program repair process.

SECTION: Data Availability

The source code and experimental results of this work for replication are available athttps://github.com/XYZboom/DEVLoRe.

SECTION: References