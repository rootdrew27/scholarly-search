SECTION: Delegating Data Collection in Decentralized Machine Learning

Motivated by the emergence of decentralized machine learning (ML) ecosystems, we study the delegation of data collection. Taking the field of contract theory as our starting point, we design optimal and near-optimal contracts that deal with two fundamental information asymmetries that arise in decentralized ML: uncertainty in the assessment of model quality and uncertainty regarding the optimal performance of any model. We show that a principal can cope with such asymmetry via simple linear contracts that achievefraction of the optimal utility.
To address the lack of a priori knowledge regarding the optimal performance, we give a convex program that can adaptively and efficiently compute the optimal contract. We also study linear contracts and derive the optimal utility in the more complex setting of multiple interactions.

SECTION: 1Introduction

The design of machine learning pipelines is increasingly a cooperative, distributed endeavor, in which the expertise needed for the design of various components of an overall pipeline is spread across many stakeholders. Such
expertise pertains in part to classical design choices such as how much and what kind of data to use for training, how much test data to use for verification,
how to train a model, and how to tune hyper-parameters, but, more broadly, expertise may reflect experience, access to certain resources, or knowledge of local conditions. To the extent that there is a central designer, their role may in large part be that of setting requirements, developing coordination mechanisms, and providing incentives.

Overall, we are seeing a flourishing new industry at the intersection of
ML and operations which makes use of specialization and decentralization to
achieve high performance and operational efficiency. Such an ML ecosystem creates
a need for new design tools and insights that are not focused merely on how the
designer could perform a task in this pipeline, but rather how she should delegate
it to agents who are willing and capable of performing the task on her behalf.How should the designer interact with this ecosystem? How should she evaluate and compensate other agents for their work? How does the outcome of the delegated pipeline compare with the outcome if the designer were to perform the task by herself?In this work, we initiate the study of delegating machine learning pipelines through the lens ofcontract theoryand take a step towards answering these questions.

Contract theory provides a principal-agent perspective, where the principal—who is the designer interested in the outcome of the learning pipeline—can create a contractual arrangement—a menu of services and compensations—with an agent. At the heart of the issue is creating contracts that incentivize the agents, who may be more knowledgeable and skilled than the principal, to take the appropriate actions. The uncertain and data-centric nature of machine learning tasks brings to light interesting sources of knowledge asymmetry between the principal and the agent and requires extensions of classical contract theory.

Consider a scenario where a firm delegates a predictive task to an ML service provider. In this context, the service provider may offer the firm either a dataset for learning or a pre-trained predictive model based on that dataset. To ensure aligned incentives, the firm needs to assess the dataset or predictive model and design the payment structure for the service provider accordingly. Since the accuracy of the model is crucial to the firm as it directly influences revenue, a natural evaluation approach involves directly measuring the accuracy of the model that the service provider produces for the firm. Several challenges arise during this evaluation process. Firstly, the firm generally only has limited data in the form of historical data or data acquired shortly after deploying the model for evaluation. So there is inherent noise in the evaluation. Secondly, the firm lacks knowledge about the baseline accuracy that is realistically achievable. This makes it harder for the firm to reward the service provider in a way that yields accuracy close to the optimal accuracy. These challenges are due to two sources of uncertainty and asymmetry that we study in this work:

Hidden actions(aka Moral Hazard): Contracts must compensate the agent for his effort towards creating a good outcome for the principal. These contracts therefore must depend on observable and verifiable outcome quality, such as the true accuracy of a classifier.
This is particularly challenging in machine learning pipelines, where the accuracy of the learned model is unknown a priori and random.
The principal may be able to invest in resources, such as large test sets, that reduce this uncertainty at a cost and better evaluate the agent’s effort.
An important consideration here is determining
whether the principal must accurately verify the outcome or instead incentivize the agent in the first place to ensure a high-quality outcome.

Hidden state(aka Adverse Selection): Effective contracts use the knowledge of the best achievable outcome to incentivize the agents to work towards such outcomes.
This is challenging in machine learning pipelines where the true error of the best model is unknown. Furthermore, generic methods that estimate the optimal accuracy
tend to use almost as many resources as are needed to learn a model of that accuracy. Here again, we must ask whether contracts exist that appropriately incentivize the agent to perform his best while knowing very little about the optimal possible accuracy.

SECTION: 1.1Our results

We consider performance-based contracts where the agent is compensated as a function of the estimated accuracy of the learned model. The principal’s utility is the true accuracy of the learned classifier minus the monetary transfer she makes to the agent. We model the agent in two delegation settings. In each we contrast the principal’s utility through contracting with and without information asymmetry. Borrowing terminology from the contract theory literature, we refer to the hypothetical scenario without information asymmetry as thefirst-best scenarioand the resulting optimal utility as thefirst-best utility, which we use as a benchmark.

We address both types of information asymmetry—hidden state and hidden action—creating contracts specific to each while also evaluating their efficacy when both asymmetries coexist. For hidden actions, our linear contract based on a single test point (Proposition2) ensures at leastfraction of the first-best utility. This guarantee continues to hold even with hidden state if the agent’s sampling cost is low (Theorem1).

For the hidden state challenge withpossible states, we derive an optimal contract by solving a convex optimization problem withconstraints. Section4describes how this contract’s optimality improves as the principal’s test set size increases.

In Section5, we analyze a multi-round delegation setting where the agent is uncertain about the delegated task and uses feedback over rounds to learn the principal’s requirements and collect relevant samples. We define the principal’s regret and establish a tightbound on this regret through repeated linear contracts overrounds. 
This shows that linear contracts are also powerful approximations of optimal utility in multi-round settings.
In comparison, we obtain a strictly better regret offor multi-round first-best contracts.

SECTION: 1.2Related work

There is a rich literature on contract theory in economics(see, e.g., Laffont and Martimort,2009; Bolton and Dewatripont,2004).
More recently, there has been work on algorithmic and statistical aspects of contract theory(Carroll,2015; Dütting et al.,2019;2020; Bates et al.,2022; Alon et al.,2022)which include results on approximation by simple contracts. These results hold for either finite actions or outcomes, and thus are not directly applicable to our setting, which involves infinite actions and a continuous space of outcomes. Working in such spaces requires utilizing the structure of our problem, and specifically exploiting fundamental results on statistical minimax rates.

Thepricingof data has been considered for various purposes and considerations(Bergemann and Bonatti,2019; Acemoglu et al.,2022; Cai et al.,2015; Ho et al.,2016)including in learning problems(Agarwal et al.,2019; Chen et al.,2022). The latter study the pricing of previously collected data to incentivize the seller and buyer to be forthright about the valuation and quality of their data, respectively. We are interested instead in pricing for the purpose of incentivizing the data collecting agent to exert effort to collect data. Some of these papers also consider incentivizing high-quality data labelling by relying on multiple labellers who can be compared. We study delegation of learning in the setting of a single agent.

An adversarial perspective on the delegation problem has been considered for machine learning from the lens of interactive proofs. In this line of work(Goldwasser et al.,2021; Chiesa and Gur,2018), the principal wants to fully verify the effort of an agent who may be an adversary that is interested in getting his effort verified. While they deal with similar challenges, such as not knowing the optimal achievable error, they do not consider incentivizing the agent (via contracts and compensations) to improve the outcome.

Concurrent work bySaig et al. (2023)studies a similar setting of incentivizing data collection for classification. They characterize the optimal contract for a given test set size, under the hidden action challenge, as a threshold contract when the agent has two choices for actions. They provide conditions which make the threshold contract optimal even for additional actions. They study empirically the effect of the hidden state challenge. We provide results for an arbitrary number of actions and propose a contract that is based on a single test sample that is optimal relative to what is achievable without the hidden action and hidden state challenges. We show that this contract is robust to hidden state challenges in many cases and describe other approaches of dealing with hidden state challenges outside of these cases.

SECTION: 2Model

We have a task distributionrepresenting the joint distribution over the domain and label set. The principal aims to learn a classifierthat achieves high accuracy on, denoted by. To accomplish this, the principal delegates the task to an agent who selects the number of samples to collect and trains a classifier. We prioritize the collection of samples as the primary effort, considering it more significant than classifier training. The principal’s primary objective is to incentivize high-quality data collection, leading to the development of an accurate classifier. To evaluate the performance of the model obtained through delegation, the principal possesses an independent test set consisting of independently and identically distributed (i.i.d.) points drawn from the distribution. The principal utilizes this test set to evaluate the learned classifier’s accuracy.

As in many other delegation settings, the principal faces the hidden state and hidden action challenges when delegating learning. While the principal desires to construct a contract based on the true accuracy of the learned model,, they can only obtain a noisy estimate of this value using test data. Our focus is on scenarios where the size of the test dataset is not excessively large. If the test dataset is too large, it becomes more beneficial for the principal to learn a model using their own test data rather than delegating the data collection process. Even when the estimate of the learned model’s accuracy has negligible noise, the principal still faces the hidden state challenge, i.e., the principal does not know how to value the accuracy since she does not know the optimal error achievable. We useto indicate the optimal accuracy achievable on.
Assigning a low payment for the model’s accuracy when the optimal error,, is high would result in negative agent utility, discouraging agent participation. Conversely, assigning a high payment when the optimal error is low might incentivize the agent to collect a smaller dataset than is optimal for the principal.

The delegation process begins with the principal publishing a contract which is a mapping from test accuracy to payment for the agent. Seeing the contract, the agent collects data and provides a classifier to the principal. The principal then executes the contract by evaluating the classifier on her own test set. The principal pays the agent the amount specified by the contract for the measured test accuracy. We assume that the principal can commit to a test set in advance and that this test set is not accessible to the agent until the contract is executed after the agent’s data collection.

Utilities.Upon receiving a classifier with accuracyfor the task distributionand paying the agent, the principal gets utilityfor some constant. The agent exerts effortper sample it collects. So the utility for the agent receiving paymentby collectingsamples is.

Outcome as a function of the agent’s action.We assume that when the agent collectssamples,111We will consider agent’s action as continuous and the true sample size is a rounding of the action.the classifier’s observed accuracy on the principal’s test set drawn fromis drawn from a distribution with meanand variance that is determined by the size of the test set.
The constantdepends on the complexity of the training algorithm and the constantdescribes the rate of decay of the excess error. These rates are motivated in part by minimax statistical rates and scaling laws.

Even though minimax rates are typically upper bounds, we treat them as exact rates in the main body and defer the discussion on the implications of treating them as upper bounds to AppendixB.1.

An algorithm that PAC-learns a function classwith VC dimensionusingi.i.d. samples drawn fromand returns a classifiersatisfying, where. This is minimax-optimal as there is a distributionsuch that.

In a-dimensional linear model with covariatesand outcomes, wherefor, the Ordinary Least Squares (OLS) estimatorsatisfies the property.

First-best contracts.As a benchmark for the best performance we can hope to achieve, we first consider the problem in an idealized setting without the hidden state and hidden action challenges. This is when the principal knows the optimal errorand the mapping between the agent’s actionand the test accuracy of the resulting model is deterministic (i.e., is exactly). The optimal contract in this idealized setting is called thefirst-best contract. The next proposition provides a closed form for this contract.

For any set of problem parameters, the first-best contract offers paymentwhen the test accuracy is at least, where.

One way to interpret the first-best contract is that it asks the agent to collectsamples and compensates the agent exactly forsamples.
Without hidden state or hidden action, the first-best contract yields zero utility to the agent. In this idealized scenario, the principal’s utility due to the first-best contract is called thefirst-best utilityand serves as a benchmark for comparison in our analysis of delegation. While first-best utility is used as a benchmark, the first-best contract itself may not be optimal due to existing randomness in test accuracy (hidden action).
Additionally, each optimal error valueleads to a different first-best contract, which is not implementable when the principal doesn’t know theparameter exactly (hidden state).
When dealing only with hidden action but known, the principal’s goal is to set up a contract specified forthat deals with the randomness in the test accuracy to recover some fraction of the first-best utility. However, when both actions and states are unknown (uncertainty inand test accuracy) the contract must ensure good principal utility for a range of possible states.

Linear contracts.As opposed to first-best contracts that can be quite complex,linear contractsare simple contracts that compensate an agent by a linear function of the test accuracy. That is, a-linear contract for parameterassigns paymentwhen the test accuracy is.

Linear contracts must have non-negative parameter, since the principal cannot make negative payments to the agent.

SECTION: 3Optimality of Linear Contracts

In this section, we aim to find near-optimal contracts in the realistic scenario with hidden state and hidden actions, recognizing that the first-best contract may not be optimal. Our main result is that
a linear contract compensating the agent based on the test (and not true) accuracy is approximately optimal across all possible contracts for the principal.
Moreover, the slope of the linear contract has an explicit value that is the same across a wide range ofmaking it possible to deal with both hidden state and hidden state challenges.

A crucial advantage of our linear contract is that it works with any unbiased estimator of the accuracy of the learned model. Therefore, even a test set of size one suffices to enact this contract. We state our main results in this section and defer their formal proofs to AppendixA.

Consider the hidden action (but known state) challenge where the principal knowsbut not the random mapping from the agent’s actionto test accuracy. This mapping has a mean) and a variance dependent on the test set size. The variance is finite but possibly arbitrarily large. This setting includes the delegation problem where the principal has as little as just a single samplein her test set. Furthermore, we assume that beyond knowing the mean
of the distribution of the test accuracy, the distribution can be arbitrary and unknown to the principal.

Our main result is that we can design an approximately optimal linear contract.
Furthermore, under a wide range of problem parameters, the principal does not even need to know the optimal error to construct this contract. This allows us to deal with both hidden state and hidden action challenges.
Our results in fact show a stronger comparison, that linear contracts approximate not just the optimal utility but also the first-best utility. This is quite a strong guarantee as there is often no contract that can achieve the first-best utility in presence of the hidden action challenge.

Before we state our main theorem, we start with the following proposition which deals only with the hidden action challenge while assuming that optimal erroris known to the principal.
Our main result in Theorem1follows from this proposition and shows that the linear contract in this proposition is also a good choice in more general settings.

For any set of problem parameters, if the principal knows(but not the distribution of the test error) she can construct a linear contract that brings an expected utility that is at leasttimes the first-best utility. Furthermore this contract only requires a single test sample.

The linear contractthat achieves this approximately optimal utility is the following:

At a finer level, this linear contract approximates the first-best utility by a factor of

Let us first note that previous work(Alon et al.,2022; Dütting et al.,2019)has provided constant approximation guarantees
but is limited to settings where the agent’s action set is a finite set or where the ratio of the maximum and minimum reward for the principal is bounded by. In the former case, an approximation ratio ofis obtained and in the latter the ratio is. Neither of these conditions hold in our settings, as the action is the number of samples collected and is unbounded and the reward can take any value in. Instead, we use the structure of first-best contract (Proposition1), the linearity of contracts, and the structure of the utility functions to obtain thisapproximation guarantees.

The full details are deferred to the appendices; here we provide some intuition and a proof sketch.
Underlying the proof is the linearity of expectation and the fact that the agent is expectation-maximizing. Under a linear contract, the expectation-maximizing agent aims to maximize, whereis the test-set accuracy of a model trained onsamples drawn from an unknown distribution with mean. The only distribution-dependent quantity in this maximizing objective is the expected accuracy. So the agent’s action and hence the principal’s contract design only depends on the expectation of the test accuracy and not on the exact distribution of the test accuracy. Next we sketch a proof for the approximation result and use the structured way the expected accuracy depends on the number of samples drawn.

Note thatis the maximum of two terms. Let us denote these terms by.
Given a linear contract with parameter, the agent’s best response is to chooseso as to maximize. The maximizing value is. By settinglarge enough, we havewhereis the threshold above which this holds. So the value ofis set to ensure the agent gets non-negative utility from participating.

When,satisfies the participation constraint. By computing the principal’s utility from the linear contractusing the expression for the agent’s best response, we see that it istimes the first-best utility. Moreover, we have. It turns out the same upper bound holds for the approximation ratio of the linear contractto the first-best utility when. This upper bound is decreasing inand the limit asis.
∎

Importantly, by inspecting the contract in Proposition2, we see that in many cases it does not depend on problem-specific parameters like the optimum error. This makesdeployable in practice.

The optimal-error-parameter-agnostic linear contract is appropriate when the cost per sample collection is small enough and when the optimal error is low enough. As a result, whenis small, we can relax the assumption that the principal knows the exact optimum errorto that the principal knows thatlies in a certain range. Moreover, even under this relaxation, linear contracts are still approximately optimal. This is stated as the following theorem.

For any, consider the linear contract. For any, suppose the optimum erroris any value inand that. Then,has utility at leasttimes the first-best utility.

Note thatis constructed based on(error decay rate) and(how the principal values accuracy relative to payment). The principal knows these quantities. In contrast, the optimal contract requires additional knowledge, such as(optimum error) and(agent’s cost per sample). The theorem demonstrates a simple contract that requires less knowledge but remains approximately optimal in utility.

SECTION: 4Extensions

Medium test set regime.In our analysis, we have examined the impact of the hidden action challenge when dealing with a small test set size. The significance of the hidden action challenge diminishes as the test set size increases, as the principal can obtain highly accurate estimates of the model’s accuracy. However, when the test set becomes too large, delegation loses its value since the principal can independently learn an accurate model without delegation. Is there a regime in which the test set size is large enough for hidden action to not be significant while also being small enough for the principal to benefit from delegating data collection? In this section, we demonstrate the existence of such a regime, referred to as the “medium test set regime.” Later, we outline how we can capitalize on the larger size of the test set to achieve stronger results.

The sample complexity for learning an-optimal model is. In particular, this bound is linear in the training algorithm’s complexity which can be problematic when using highly complex training algorithms. We say that the medium test set regime exists, if the sample complexity for hidden action is significantly smaller than,
wherecaptures the significance level of hidden action which we will make precise in the following definition.

In a finite test set setting with hidden action, for any optimal error parameter, letdenote the optimal expected utility of contracting. We say that hidden action is insignificant at level, for any, if the expected utility of the first-best contract based onin this setting is at least.

We next state a theorem giving the sample complexity of the principal’s test set to achieve insignificance of the hidden action.
The sample complexity stated in the theorem is logarithmic inwhile learning would have required a number of samples linear in. This demonstrates the existence of a medium test set regime where it is possible to employ delegation without considering hidden action.

For any, if the principal has a test set of size, then hidden action is insignificant at the level.

SECTION: 4.1Optimal contracts for hidden state

By ignoring hidden action in the medium test set regime, we can hope to design contracts with stronger guarantees. Previously we were able to design contracts with high utility when the optimal error lies in a particular range given in Theorem1. By ignoring hidden action, we can design contracts with utility guarantees for when the optimal error lies in any arbitrary set. When the principal holds a finitely supported prior belief over the optimal error value, we show how to compute the optimal contract by setting up a convex optimization problem. We also describe some qualitative properties of the optimal contract in this setting.

When we ignore the hidden action challenge, we can assume that the observed accuracy is deterministic in the agent’s action. That is, when the agent collectssamples, the observed accuracy is. We assume that the principal holds a prior belief on the optimal error but does not know the exact value. The agent knows more about the optimal error since he collects data that informs him more about the optimal error. We assume that the agent knows the exact optimal error. We start by analyzing the optimal contract in this setting. Later in Section4.2, we discuss how to design contracts in the more realistic setting of the agent learning the optimal error instead of knowing this value exactly. And we show that the utility guarantees by making the perfectly aware agent assumption still hold approximately in the more realistic case with a learning agent.

Let us analyze the optimization problem for computing the optimal contract. Let the finite support of the prior over optimal error be. The principal puts forth a contract of accuracy-payment pairswith the pairintended for when the optimal error is.222This is implied by the revelation principle that states that, with hidden state, any delegation mechanism is equivalent to anincentive compatiblemechanism where all agents inform their private information to a planner who then recommends actions.Let us denote the expected accuracy from collectingwhen optimal error isby. Hereis the number of samples the agent would collect to achieve accuracywhen optimal error is. The principal optimizes over. The constraints of the optimization problem for the principal’s contract design for hidden state are one of two types. The first type of constraint is the participation constraint, which ensures that the agent is adequately compensated for his effort when he chooses the contract intended for the optimal error. For each, the participation constraintcan be expressed as, whererepresents the compensation rate.

The second type of constraint is the incentive compatibility constraint to ensure that the agent chooses the option intended in the contract for the optimal error. For any, the corresponding incentive compatibility constraint is that when the optimal error is, the utility of choosingis worse for the agent than choosing. The number of samples the agent would choose to achieveaccuracy under optimal errorissuch that.333Note that all accuracies cannot be achieved for all optimal errors. If no suchexists, an incentive compatibility constraint is not needed.The constraintis. Due to the structure of, the IC constraints are convex.
The principal’s expected utility which it maximizes is. So the contract design problem is the following optimization problem:

Qualitative insights on the optimal contract.We derive the following insights (see Figure1) when there are two values for the optimal error,, in the AppendixB.3. These properties also hold more generally for finitely supported beliefs and have been studied for classical contract design for many other delegation problemsLaffont and Martimort (2009).

Decreased utility. The principal gets lower utility than the first-best utility and this utility decreases asincreases.

Information rent.In the first-best contract, the agent gets no more payment than to compensate his effort. That is,. Under hidden state, for problems with lower optimal error, the agent gets positive utility. This information rent is to incentivize the agent to not pretend the problem is harder and exert lower effort to achieve an accuracy that requires more effort if the problem was harder.

Downward distortion.The first-best contract calls for the agent to collect a particular number of samples regardless of the optimal error. Under hidden state, when the problem is harder, agents are asked to collect fewer samples compared to the first-best contract. When the problem is the easiest in the support, the agent is asked to collect the same number of samples as in the first contract.

SECTION: 4.2Designing contracts against state-learning agents

In Section4.1and particularlyOpt, we assumed perfect knowledge of the hidden state () by the agent. However, in reality, the agent does not know the optimal error beforehand. Instead, as the agent executes the contract, he learns more about the optimal error and adapts his actions accordingly. To design a contract for such a state-learning agent, the principal would need to predict the agent’s response to the contract. However, this is challenging for arbitrary contracts since the principal would require knowledge of the agent’s exact learning strategy, which is often unreasonable. Therefore, we focus on analyzing simple contracts for which we can easily derive the agent’s response. We demonstrate numerically that the utility achieved with these simple contracts is close to the utility we previously derived for state-aware agents, which we refer to as “state-aware utility.”
This provides evidence that qualitative insights we derived about the state-aware utility in Section2and Section3are applicable in the more realistic case of a state-learning agent.
We focus on the case where the optimal error can take one of two possible values,, but these design principles also extend to more possible values of the optimal error. The simple contract we consider, which we call thestate-learning contract, is the best of two simple contracts: optimalpoolingandseparatingcontracts.

Separating contract.Separating contracts allow the agent to perfectly infer the hidden state while executing the contract. These are incentive-compatible contracts that ask agents to collectsamples under optimal errorsrespectively. Additionally,are such that the agent can successfully infer the optimal error after collectingsamples. The agent’s response to this contract would be to first collectsamples and decide whether to collect more depending on the inferred optimal error. The agent’s successful inference of the optimal error makes computing optimal separating contracts similar to the contract design problem against a state-aware agent, which was solving the optimization problemOpt. The new optimization problem that yields optimal separating contracts has the same objective and constraints asOptwith the added constraint thatfor somethat we will describe soon. The additional constraint ensures that the agent knows the optimal error (with high probability) after collectingsamples.

To determine the value of, we rely on assumptions about the agent’s learning strategy. We assume that the agent can distinguish betweenwith high probability usingsamples. Hereandis a constant reflecting the degree of assumption made about the agent’s efficiency. A lower value ofis a stronger assumption, assuming a more efficient agent. This assumption on the agent’s learning strategy is more reasonable compared to assuming precise knowledge of the agent’s learning strategy.

Whenis small enough that the added constraintis not active, the state-learning agent is behaving exactly as the state-aware agent, so our results from Section4.1apply. On the other hand, ifis large (which happens whenis small) the additional constraint becomes too restrictive and the utility becomes low. In this case, another approach works well.

Pooling contract.In pooling contracts, the agent has no incentive to learn the optimal error. The pooling contract asks the agent to achieve one accuracy levelregardless of the optimal error. The payment for this accuracy is set to ensure the agent can get nonnegative utility regardless of the optimal error. It is again straightforward to understand the agent’s response to this contract. Supposecan be achieved by collectingsamples under optimal errorsrespectively. To execute this contract, the agent starts collectingand sees if it achievesaccuracy. If it does not, he collectsmore samples since this action is guaranteed to yield nonnegative utility. Furthermore, collecting fewer or no additional samples results in less thanexpected accuracy and hence zero payment even though the agent exerted effort.

A pooling contract does not let agents differentiate actions for different optimal errors and would be sub-optimal for this reason. However, when the difference in both problems is not significant i.e.,is low, the benefit to the principal for distinguishing the agents is low. In summary, the separating contract has good utility whenis large and the pooling contract has good utility whenis small. By deploying the contract of the two with the higher utility, we can hope to have good utility for all values of.

Numerical results.We compute the utility difference between the state-aware contract and the state-learning contract, varying problem parametersand. We highlight a few observations (see Fig2), that reflect the intuition we used to design the approach for state-learning contracts.

Figure2ashows that the state-learning contract is pooling whenis less than some threshold and is separating otherwise. For small and large values of, the state-learning contract has utility close to the state-aware utility.

Figure2bshows that when it is more difficult to distinguish between, the pooling contract is better than the separating contract for more values of.

Figure2cshows that the worst-case sub-optimality over allvalues of the state-learning contract compared to the state-aware utility increases asincreases. When the agent can test more efficiently, the state-learning contract has greater utility for the principal.

SECTION: 5Multi-round Delegation

So far, we analyzed delegated learning that occurs through a single round of interaction between the principal and the agent. However, delegation often occurs over multiple rounds to allow the agent to learn more about the principal’s requirements. Here we model such a scenario and analyze what happens when the principal uses a linear contract in each round. We introduce a notion of regret and show that repeated linear contracting overrounds results inregret for the principal which is worse than theregret achievable without delegation i.e., thefirst-best regret.
We provide proof sketches of our results in the main body and provide the full proofs in the appendices.

To model uncertainty about the principal’s requirements, we assume that the target distributionbelongs to a class. The agent knows the classbut does not knowapriori. The principal contracts and deploys classifiers forrounds.

For each round:

The principal announces payment rulewhich is a randomized mapping from a classifier to a positive, real-valued payment to the agent.

The agent chooses a target number of i.i.d. samples to collect from each distribution. Denote this number bywhereis the number of samples the agent draws from distributionin round. This choice is not observed by the principal.

The agent provides classifierto the principal.

The principal deploys a classifier.

The principal paysto the agent according to the announced payment rule.

The principal’s and agent’s action in each round can be chosen adaptively depending on the actions and outcomes of previous rounds.

Over theserounds, the agent’s utility is the sum of payments minus sample collection costs:. The principal’s utility is the sum of accuracies minus payments:.

Our results deal with contracts based on test accuracy of the deployed classifier. In roundwhere the principal deploys the classifier, the payment is a function of the test accuracy, whereis a mean-zero, random variable resulting in the principal’s randomness of testing. A linear contractin this round offers paymenttimes this test accuracy in this round.

In these repeated interactions, the payments serve as feedback to the agent to learn the principal’s requirements as long as the principal’s deployed classifiers depend on the classifiers provided by the agent. We focus onfeedback-providingcontracts which satisfy the following property. A contract is feedback-providing if for every roundwith, the principal deploys the agent-provided classifier, and the payment for the round is thereforetimes the test accuracy of the agent-provided classifier.

We prove positive results on the utility the principal can achieve when contracting with a rational agent.

We introduce a notion of regret for the principal and the agent in this online setting. The regret notion is defined relative to a class of classifiers. It compares utility to the utility obtained by deploying the best classifier inin every round, without any sample collection or payments.

Letbe any class of classifiers. Letbe the sequence of actions by the principal and agent.
The principal’s-regret () is the difference in the utility of the sequence and the utility of deploying the most accurate classifier inwithout payments:

The agent’s-regret () is the difference in the utility of the sequence and the utility of deploying the highest expected payment yielding classifier inand collecting no samples:

We consider a model of rationality for the agent in repeated linear contracting. This is an assumption on the agent’s-Regret rate.

Letbe the sequence of linear contracts employed by the principal. A rational agent achieves-regret sub-linear in.

We discuss the implications of contracting with a rational agent first and then justify the rationality assumption by providing an algorithm for the agent to achieve the rationality criteria.

In the following proposition, we show how contracting with such a rational agent provides the principal a sub-linear-regret. The proof of the proposition constructs a contract that the principal can use to achieve sub-linear-regret. This contract makes use of an upper bound on the agent’s regret rate.

Suppose the agent achievesagent’s-regret with, for any sequence of contracts. Then, the principal can achieve-regret.

The contract that allows this regret for the principal is described here. The principal contracts withfor the firstrounds. In these rounds, the principal deploys the agent-provided classifier. The principal shuts down contracting for the remaining rounds. That is,for. In rounds, the principal deploys a classifierthat is uniformly, at random picked from the classifiersdeployed in the firstrounds.

In the firstrounds, the principal’s-regret is bounded bytrivially. In the remainingrounds, the principal’s-regret is bounded above by just the suboptimality of the deployed classifiers since no payments are offered. The regret in the lastrounds is at most

where the last step follows from the rationality assumption that the agent achieves a-regret of.
The cumulative principal’s-regret overall allrounds is therefore. Since, principal’s-regret is.

∎

The above proposition shows how the principal can achieve sublinear-regret under the agent’s rationality assumption. Now we show that the agent’s rationality can be achieved. In our proof, we provide an algorithm for the agent to achieve-regret sublinear in.

For any sequence of feedback-providing contracts, the agent can achieve-regret.

We will describe the agent’s algorithm here and defer the full analysis of this algorithm to the appendices. For any round, let us denoteby. Since the agent incurs cost linear in the number of samples collected, the algorithm must limit the number of samples to. To do this, at round, the agents collectssamples. As a result, for any, the number of samples collected up to roundis.To select the distribution to sample from, the agent picks one out ofuniformly at random. This fully describes how the agent’s algorithm collects samples by describing the number of samples and distributions to sample from.

The remaining component of the agent’s algorithm is classifier selection. The classifier selection includes exploration and exploitation where the exploration is to determine which ofprovides samples to train an accurate classifier. Since we knowis one of thesedistributions, we are guaranteed that there exists a distribution providing relevant samples.

We assign rounds of sample collection to be rounds of exploration. These rounds are suitable for exploration for two reasons. The first is because the principal deploys the agent-provided classifiers in these rounds. By the algorithm’s design, the agent only collects samples in rounds with, and in feedback-providing contracts, agent-provided classifiers are deployed in these rounds. Secondly, this choice also leads to the right number of exploration rounds for the optimal exploration-exploitation tradeoff. Letdenote the indices of rounds of exploration. This set’s size is at most. A phased exploration is done in these rounds.

The phased exploration divides the indicesinto phases where phaseis of length. So at leastsamples are collected in phase. The number of phases is therefore.

Each phaseuniformly explores classifiers learned based on samples from each distribution, collected up to phase. Letbe the set of samples collected fromin phases. And let. In each round of phase, the agent picks one ofuniformly at random. Letbe the classifier of phasewith the highest sum of test accuracies. The agent can compute the test accuracy by dividing payment by the contract coefficient (which is non-zero in exploration phases).

The agent treats all rounds other thanas exploitation rounds. For an exploitation round, suppose the last collected sample was in phase. Then in round, the agent selects the best classifier in phase:. Due to the completion of theexploration phase,is guaranteed to have some optimality guarantees.

The regret analysis for this algorithm is presented in AppendixA.3.

∎

Propositions3and4together imply that the principal can achieve-Regret through linear contracting with a rational agent. We next show that the principal cannot achieve better-regret rates when the agent’s-regret rate is.

For any sequence of linear contracts, if the principal contracts with an agent with-regret on all problem instances, the principal’s-regret isfor some problem instance.

Let us denoteby. An agent with-regret collectssamples since the agent incurs cost linear in the number of samples collected. This upper bound on the number of samples provides a lower bound on the excess errors of the deployed classifiers and therefore a lower bound on the principal’s regret.

Usual sample complexity lower bounds provide lower bounds on the error of learning using a number of i.i.d samples. However, in our setting, the agent has more than just the samples he collects to learn classifiers. Through the linear payments he receives, he also has access to estimates of the accuracy of classifiers he provides in each round. This is a form of (noisy) query access to the distribution.

We provide a min-max lower bound on learning using both i.i.d samples and queries of the form answering the expected error onof a classifier in the following proposition. We show that in a min-max sense, the queries do not allow for more accurate classifiers compared to using just the i.i.d samples.

Consider a learning algorithm that usesi.i.d samples andqueries of accuracies of classifiers. Then there exists a distributionfor which the expected error of the learned classifier ismore than the optimal error of a one-dimensional halfspace on.

We prove this lemma in AppendixA.4. The main intuition for this lower bound is that without any structure on the distribution, it is hard to know what classifiers are useful to query. Therefore the queries are not useful.

Due to the above proposition, there is a problem instance for which the classifiers the principal deploys have an average excess error ofresulting in regret.

The principal also incurs a cost of orderdue to the contracting. Due to the sub-linearity of agent’s regret in, the payments have to beresulting in this term of the principal’s regret.

The optimal choice ofto balance these two terms isresulting in regret of order.

∎

Comparison with non-delegation benchmark.We have shown that the principal’s-regret through delegating with an-regret agent is. Since there is an algorithm for the agent to achieve-regret, a rational agent is likely to achieve this regret upper bound.

If the principal instead performs this learning without delegation, the achievable regret is the same as the agent’s achievable regret when the contract coefficient isin every round. By Proposition4,regret is achievable. Note that this means the-regret rate is strictly worse for the principal through delegation compared to without delegation.

SECTION: 6Conclusions

Different parts of the learning pipeline are increasingly being delegated to autoML services and firms. Focusing on the delegation of data collection in such settings, we developed a principal-agent model capturing the practical challenges of hidden action and hidden state arising due to information asymmetry. We hope our work inspires future research exploring and addressing other incentive-theoretic obstacles that arise in this domain and in the delegation of other parts of the learning pipeline.

We identified the practicality of linear contracts under many problem parameters. We also obtained insights that hold for many other delegation settings, including the decreased utility and information rent the principal faces due to not knowing the hidden state. We also addressed a more realistic form of hidden state information asymmetry where the agent gradually learns the hidden state while executing the contract.

Many of our results rely on a specific structure for the dependence of error rates on number of samples used for training. These rates are exact for some learning tasks (see Remark2) but are generally upper bounds. From the principal’s perspective, the contracts we designed continue to have good accuracy guarantees even if the error rates are just upper bounds and not exact. The agent’s perspective is more complicated. Since the agent can learn more about the true error curve during learning and not rely on the upper bound, analyzing how the error curve learning occurs is needed to allow us to design truly incentive-compatible contracts in this more general setting. However, learning the error curve shape is learning from a much broader class and is likely to be more challenging. How this challenge impacts the utility of contracts would be an interesting direction for future work.

SECTION: Acknowledgements

Funded in part by the European Union (ERC-2022-SYG-OCEAN-101071601). Views and opinions expressed
are however those of the authors only and do not necessarily reflect those of the European
Union or the European Research Council.

This work was supported in part by the National Science Foundation under grant CCF-2145898, by the Office of Naval Research under grant N00014-24-1-2159, a C3.AI Digital Transformation Institute grant, and Alfred P. Sloan fellowship, and a Schmidt Science AI2050 fellowship

SECTION: References

SECTION: Appendix AOmitted proofs

SECTION: A.1Proof of Proposition2

The linear contractthat achieves this approximately optimal utility is the following:

We first show that this contract satisfies the participation constraint for the agent. When the linear contract istimes the accuracy, the agent the number of samplesto maximize the agent’s utility. The number of samples the agent chooses as a function ofis. The contractsatisfies the participation constraint if the utility from choosing this number of samples is non-negative. This utility is:

This utility is non-negative when. By the definition of, it is greater thanand sosatisfies the participation constraint.

When the principal chooses a linear contract, it achieves a utility

We can provide an upper bound on the optimum utility using the optimum utility of the principal when there is no noise in the observed accuracy. In this case, the principal gets the agent to collectand pays the agenttimes this amount. So the optimum utilitysatisfies

To show thatachieves the approximation guaranteed in the theorem, we consider two cases.

The first case is when. In this case the utility ofis

So in the first case, we have shown the required bound on the approximation ratio of the contract.

The other case is when.
In this case,

is increasing in. The condition of this case 2 occurring turns out to be a condition on. This condition for case 2 occurring is the following:

Case 2 occurs whenand in this caseis increasing in. So the smallest value ofin this case occurs when. Plugging in this value, we get that for this case,

∎

SECTION: A.2Proof of Theorem2

Recall that the first-best contract has a threshold form. The contract offers paymentwhen the test error is less than or equal toand offers payment zero otherwise. Let us denote the sample complexity to get expected loss at mostby. That is,

The optimal contract offerswhereis the cost per sample for the agent. Let us denoteby. And.

We will show that the best response for the agent against this contract is never to collect samples less thanwhen the test set has size. We show this by showing that the agent’s utility in choosingis less than the agent’s utility in collectingfor all.

The number of samples the agent would collect to get expected erroris such that:

Similarly, the number of samples needed to get expected erroris

For any action of the agent, the probability that the observed loss isor more away from the expected loss is less than. This is by applying Hoeffding’s inequality on the observed loss random variable which is bounded between 0 and 1. As a result, for, the expected payment when collectingandsamples isandrespectively. The agent’s utility due tois less than the utility due towhen

Note thatis polynomial in bothand.

∎

SECTION: A.3Proof of Proposition4

We start by restating the agent’s algorithm. For any round, let us denoteby. Since the agent incurs cost linear in the number of samples collected, the algorithm must limit the number of samples to. To do this, at round, the agents collectssamples. As a result, for any, the number of samples collected up to roundis.To select the distribution to sample from, the agent picks one out ofuniformly at random. This fully describes how the agent’s algorithm collects samples by describing the number of samples and distributions to sample from.

The remaining component of the agent’s algorithm is classifier selection. The classifier selection includes exploration and exploitation where the exploration is to determine which ofprovides samples to train an accurate classifier. Since we knowis one of thesedistributions, we are guaranteed that there exists a distribution providing relevant samples.

We assign rounds of sample collection to be rounds of exploration. These rounds are suitable for exploration for two reasons. The first is because the principal deploys the agent-provided classifiers in these rounds. By the algorithm’s design, the agent only collects samples in rounds with, and in feedback-providing contracts, agent-provided classifiers are deployed in these rounds. Secondly, this choice also leads to the right number of exploration rounds for the optimal exploration-exploitation tradeoff. Letdenote the indices of rounds of exploration. This set’s size is at most. A phased exploration is done in these rounds.

The phased exploration divides the indicesinto phases where phaseis of length. So at leastsamples are collected in phase. The number of phases is therefore.

Each phaseuniformly explores classifiers learned based on samples from each distribution, collected up to phase. Letbe the set of samples collected fromin phases. And let. In each round of phase, the agent picks one ofuniformly at random. Letbe the classifier of phasewith the highest sum of test accuracies. The agent can compute the test accuracy by dividing payment by the contract coefficient (which is non-zero in exploration phases).

The agent treats all rounds other thanas exploitation rounds. For an exploitation round, suppose the last collected sample was in phase. Then in round, the agent selects the best classifier in phase:. Due to the completion of theexploration phase,is guaranteed to have some optimality guarantees.

Now we analyze the regret incurred due to this algorithm. The regret due to cost of sampling is. We will analyze the regret due to the sub-optimality of the classifiers deployed. Let. Since the number of exploration rounds is, the regret in these rounds is trivially bounded by. We will focus on sub-optimality of classifiers deployed in the exploitation rounds.

Consider the classifieryielding the highest accuracy in phase. Letindicate the average test loss of deploying classifierin phase. Each of the candidate classifiers of phase:is exploredtimes since we perform uniform exploration. Thereforeand.

Letbe the last non-sampling round thatis selected. This means thatis a sampling round and of phase. The number of samples drawn up to phaseand hence up to roundis. Due to this,being a sampling round implies that. So we can bound the regret due to deployingin non-sampling rounds to be at most

Summing over regret due to non-sampling rounds over all phasesfrom 1 to, total regret in non-sampling rounds is at most:.

Therefore the total regret over sampling and non-sampling rounds is.

∎

SECTION: A.4Proof of Lemma3

For each, we will construct a class of distributions such that for any learning algorithm with access toi.i.d. samples andqueries, there is a distributionin the class for which the learning algorithm will have excess error at leastwhereis the optimal error achieved by the class of one-dimensional half-spaces on. This is the classwhich we will refer by

As a construction for the lower bound, consider the class of distributions over a domain ofpoints, each having a uniform marginal distribution supported onof those points. The labelling distributionis. We later describe how to chooseso that the lower bound holds for.

We will show that for any set ofqueries, there are two distributionssuch that

All query values are the same for.

No algorithm can distinguish betweenwith probability more thanusingsamples drawn.

Any classifier with erroronnecessarily has erroron.

The above properties suffice to show the required lower bound. This is because with the above properties, a learning algorithm that achievesexpected excess error necessarily distinguishes betweenandwith probability at least. Distinguishing betweenshould not be possible withsamples andqueries if the above properties hold.

Now let us show how to chooseand constructto make the above properties hold.

Each query assigns a value of 0 or 1 to each point. So there is a sequence of lengthindicating the labels the queries assign for each point. We can partition the domain into points having the same sequence of query labels. By constructing,, so that the number of points in each partition with labelling functionis the same forand, we can guarantee that all query values are the same forand

Let us setso that. Since there arequery sequences, at least one of the partition sets has size. Consider thepoints distributionsare supported on to be thepoints of samples drawn and the remaining points are points from the partition of size. There are at leastpoints of the support from the partition of size.

Let the points in the support that are in the query partition of sizebewhere.has probability of +1 labelfor pointsand probability of +1 labelfor points.has probability oflabelfor pointsand probability of +1 labelfor points.

Outside of this partition, let the labelling distribution of any other point in the support be the same for. By this construction, property (1) is satisfied.

Restricted to the query partition, the errors on distributionssum up to. By choosing, the query partition makes up at least half the fraction of the support. Therefore any classifierhas. and due to this property (3) holds.

The KL divergence between the sampling distributions fromis at most, and therefore with probability, we cannot distinguish betweenandusingsamples. This shows property (2) holds.
∎

SECTION: Appendix BMiscellaneous results and discussions

SECTION: B.1Treating error curves as upper bounds

For most of our results we have made use of the structured form of error curves reflecting how expected error of a learned model is assumed to vary with the number of samples used for training. This structure is inspired by statistical minimax bounds and are upper bounds rather the true error curves. We designed contracts assuming the bounds to be actual error curves. Here we discuss what we can say about these contracts without assuming the bounds to be exact error curves.

From the principal’s perspective, these contracts result in accuracy that is just as good as that of learned models. However, the principal would end up paying the agent more than it could have if the principal knew the exact error curve. We can view the shape of the true error curve as another piece of information the principal is unaware of in addition to the optimal error. This hidden information results in more information rent but does not impact the accuracy of the model obtained from delegation.

The agent’s perspective of what changes is more complicated. Our contracts assumed that the agent responded assuming that the upper bound was the true error curve. It may be reasonable that before starting the delegation process, the agent believes the upper bounds to be the true curves having no other frame of reference. However after starting to collect data, it is possible that the agent will learn more about the form of the true error curve and respond differently. This is similar to how the agent can learn the optimal error while executing the contract. Analyzing how this error curve learning occurs will allow us to design truly incentive-compatible contracts. However, learning the error curve shape is learning from a much broader class and is likely to be more challenging.

SECTION: B.2Variable label quality model

The setting above models the scenario where the agent does not have the option to choose the quality of the data is collects. However, the agent might be able to control the quality of the data as a function of the cost per sample. We study a model of quality of data where the quality corresponds to the quality of the labels of the data. The quality parametercaptures the likelihood of the labels being correct. Here,is the probability of the label being incorrect. In this setting, the expected accuracy on the principal’s test set from the agent collectingsamples at quality levelwhen the optimal error isisfor some. We assume that the cost of collecting a single sample at quality levelfor the agent is given bya function increasing inand convex. So the cost for the agent of collectingsamples at quality levelis.

In this section, we provide results forfor. We can think ofas the cost of collecting an unlabelled sample andas the cost of labelling an unlabelled point. The main message of this section is that even though the quality of labels is an action that the agent chooses, effectively, this choice is not information that is private from the principal. It turns out that whatever the contract is, the utility-maximizing agent executes the contract by choosing a single quality value. The principal can also computeso the quality is not a reflection of information asymmetry. Therefore, this regime is essentially the same as the one studied in the previous section.

For any problem parameters, when, for any expected accuracythe agent wishes to achieve, the agent chooses a constantthat only depends onas the quality level.

If the agent aims to achieve an expected accuracy of at least, then the agent chooses the number of samples and quality level by solving the following optimization problem:

The solution of this optimization problem can be calculated as follows:

Ifis not in, thenoris non-zero andis either 0 or 1.
∎

SECTION: B.3Closed-form solution for the two optimal error, hidden state problem

Here we solve the state-aware optimization problemOptwhen there are two optimal errors,, with a prior probability offor. Let. We solve the following optimization problem and show that the solution is the optimal solution we are looking for. Note that this problem omits the incentive-compatibility constraint for the problemand the participation constraint for the easy problem.

First note that this is a convex optimization problem, where the objective is convex by the convexity of the loss and the PC constraint is a linear constraint. All that is left is to check that the IC constraint is convex. This is the sum of linear terms and the term. The second derivative of this term is. Since the second derivative is positive, the IC constraint is convex.

Consider the Lagrangian

which has the following gradients:

To choose valuesthat satisfy the KKT conditions, first we set the gradients to zero:

The contract described bysatisfies the properties of the second-best contract in the classical contract theory setting. We list these properties here:

No output distortion for the easy problem:is the solution ofwhich is also the value of. So for the easy problem, the agent gathers the same number of samples as in the full information case.

Downward distortion for the hard problem:

So. For the hard problem, the agent gathers fewer samples than in the full information case.

When the problem is easy, the agent gets positive information rent:

We now check that the contract that is the solution to the above optimization problem also satisfies the omitted constraints. First we start with the participation constraint for the easy problem. By the positive information rent property (P3) we know that. Next consider the incentive-compatibility constraint for the hard problem. We only need to check when. Otherwise, the IC constraint automatically holds. The difference in agent’s utility between choosing theoption and theoption is:

This solution finds the optimal contract under hidden state.

To be able to compute any separating contract, it suffices to solve the above optimization problem with the additional constraintfor some. The new optimizersare as follows:

The optimal pooling contract optimizes over..is chosen such that

Thus the optimization problem is choosingto be the minima of

SECTION: B.4Tightness of linear contracts approximation

Our main result (Theorem1) gave a linear contract that provably approximates the optimal contract up to a constant factor. This approximation factor stated in Proposition2is also tight as stated in the following theorem, which shows that there is a problem instance for which no linear contract can do better than a given approximation factor. The problem instance for which the approximation ratio is tight is one that has deterministic test error distribution, which arises when the size of the test set tends to infinity.

For every, there are problem parameterssuch that for the problem instance with these parameters, all linear contracts have at mosttimes the optimal utility.

We will show that there existsuch that the contractis the optimal contract chosen by the principal. This contract will also satisfy the participation constraint for our chosen values of. Recall that the participation constraint is

The principal chooses the contract that sets the derivative of the above quantity to zero as long as that contract satisfies the participation constraint. If settingyields a zero derivative and it satisfies the participation constraint, then it is the optimal linear contract. The derivative relative tois

Setting, the derivative is

We can chooseto set this derivative to zero by choosingsatisfying:

This shows that there are problem parameters that makethe optimal linear contract. In the proof of Proposition2, we showed that this linear contract achieves at leasttimes the optimum utility. When the problem involves a deterministic mapping between the number of samples and the observed accuracy, this ratio is exact.
∎