SECTION: Retrieval-Augmented Decision Transformer: External Memory for In-context RL
In-context learning (ICL) is the ability of a model to learn a new task by observing a few exemplars in its context.
While prevalent in NLP, this capability has recently also been observed in Reinforcement Learning (RL) settings.
Prior in-context RL methods, however, require entire episodes in the agent’s context.
Given that complex environments typically lead to
long episodes with sparse rewards, these methods are constrained to simple environments with short episodes.
To address these challenges, we introduce Retrieval-Augmented Decision Transformer (RA-DT).
RA-DT employs an external memory mechanism to store past experiences from which it retrieves only sub-trajectories relevant for the current situation.
The retrieval component in RA-DT does not require training and can be entirely domain-agnostic.
We evaluate the capabilities of RA-DT on grid-world environments, robotics simulations, and procedurally-generated video games.
On grid-worlds, RA-DT outperforms baselines, while using only a fraction of their context length.
Furthermore, we illuminate the limitations of current in-context RL methods on complex environments and discuss future directions. To facilitate future research, we release datasets for four of the considered environments.

SECTION: Introduction
In-context Learning (ICL) is the ability of a model to learn new tasks by leveraging a few exemplars in its context.
Large Language Models (LLMs) exhibit this capability after pre-training on large amounts of data crawled from the web.
A similar trend has emerged in the field of RL, where agents are pre-trained on datasets with an increasing number of tasks.
After training, such an agent is capable of learning new tasks by observing previous trials in its context.
Consequently, ICL is a promising direction for generalist agents to acquire new tasks without the need for re-training, fine-tuning, or providing expert-demonstrations.

Existing methods for in-context RL rely on keeping entire episodes in their context.
Consequently, these methods face challenges in complex environments, as complex environments are usually characterized by long episodes and sparse rewards.
Episodes in RL may consist of thousands of interaction steps, and processing them is computationally expensive, especially for network architectures such as the Transformer.
Furthermore, not all information an agent encountered in the past may be necessary to solve the new task.
Therefore, we address the question of how to facilitate ICL for environments with long episodes and sparse rewards.

We introduce Retrieval-Augmented Decision Transformer (RA-DT), which incorporates an external memory into the Decision Transformerarchitecture (see Figure).
Our external memory enables efficient storage and retrieval of past experiences, that are relevant for the current situation.
We achieve this by leveraging a vector index populated with sub-trajectories, in combination with maximum inner product search; akin to Retrieval-augmented Generation (RAG) in LLMs.
To encode retrieved sub-trajectories, RA-DT relies on a pre-trained embedding model, which can either be domain-specific, such as a DT trained on the same domain, or a domain-agnostic language model (LM) (see Section).
Subsequently, RA-DT uses cross-attention to leverage the retrieved sub-trajectories and predict the next action.
This way, RA-DT does not rely on a long context and can deal with sparse reward settings.

We evaluate the effectiveness of RA-DT on grid-world environments used in prior work with sparse rewards and increasing grid-sizes (Dark-Room, Dark Key-Door, Maze-Runner), robotics environments (Meta-World, DMControl) and procedurally-generated video games (Procgen).
On grid-worlds, RA-DT considerably outperforms previous in-context RL methods, while only using a fraction of their context length.
Further, we show that our domain-agnostic trajectory embedding model reaches performance close to a domain-specific one.
On the remaining more complex environments, we observe consistent improvements for RA-DT on hold-out tasks, but no in-context improvement for any method.
Therefore, we discuss the current limitations of RA-DT and other in-context RL methods and elaborate on potential remedies and future directions for in-context RL.

We make the following:

We introduce Retrieval-augmented Decision Transformers (RA-DT) and evaluate its effectiveness on a number of diverse domains.

We show that a domain-agnostic embedding model can be utilized for retrieval in RL without requiring any pre-training, and reaches performance close to a domain-specific model.

We release datasets for Dark-Room, Dark Key-Door, Maze-Runner, and Procgen to foster future research on in-context decision-making that leverages offline pre-training..

SECTION: Related Work
ICL is a form of Meta-learning, also referred to as learning-to-learn.
Typically, meta-learning istargetedand learned through a meta-training phase, for example in supervised-learningor in RL.
In contrast, ICLemergesas a result of pre-training on a certain data distribution.
This ability was first observed invia LSTMsand later re-discovered in LLMs.found that every memory-based architecture may exhibit such capabilities.
Another crucial factor is a training distribution comprising a vast amount of tasks.
Recent works combined these properties to induce ICL in RL.
While promising, they require keeping entire episodes in context, which is difficult in environments with long episodes.consider an in-context imitation learning setting given expert demonstrations.
In contrast, RA-DT can handle long episodes and does not rely on expert demonstrations.

The aim of retrieval-augmentation is to provide a model access to an external memory.
This alleviates the need to store the training data in the parameters of a model and allows to condition on new data without re-training.
RAG is successfully applied in the realm of LLMs, multi-modal language generation, and for chemical reaction prediction.
In RL, the access to an external memory is often referred to as episodic memory.investigate the effect of different data sources in the external memory of an online RL agent.provide access to millions of expert demonstrations via RAG in the game of Go.
In contrast, RA-DT does not rely on expert demonstrations, but leverages RAG to learn new tasks entirely in-context without the need for weight updates.
Further, RA-DT doesnotrely on a pre-trained domain-specific embedding model, as we demonstrate that the embedding model can be entirely domain-agnostic.

Most prior works have explored the utility of an external memory to cope with partially observable environments, in which the agent must remember past events to approximate the true state of the environment.
This is difficult, especially for complex tasks with sparse rewardsand long episodes.
To cope with this problem, Neural Turing Machines, which rely on a neural controller to read from and write to an external memory, were applied to RL.
Memory networksleverage an external memory for reasoning.propose a memory architecture with read/write access to learn what information to store based on a world model.
In contrast, RA-DT only retrieves pieces of past information similar to the current encountered situation.propose an attention-based external memory, where queries, keys, and values are represented by different modalities.
Similarly, our domain-agnostic embedding model extends the idea of history compression via LLMsto retrieval, where queries and keys are encoded in the language space, while values comprise raw sub-trajectories.

SECTION: Method
SECTION: Background
We formulate our problem setting as a Markov Decision Process (MDP) that is represented by a 4-tuple of.anddenote state and action spaces, respectively.
At timestepthe agent observes stateand issues action.
For each executed action, the agent receives a scalar reward, which is given by the reward function.constitutes a probability distribution over next stateswhen issuing actionin state.
RL aims at learning a policythat predicts actionin statethat maximizes.

Decision Transformerlearns a policy from offline data by conditioning on future rewards.
This allows rephrasing RL as a sequence modelling problem, where the agent is trained in a supervised manner to map future rewards to actions, often referred to as upside-down RL.
To train the DT, we assume access to a pre-collected datasetoftrajectoriesthat are sampled from the environment via a behavioural policy.
Each trajectoryconsists of state, action, reward, and return-to-go (RTG) quadruplets, whererepresents the length of trajectory, and.
The DTis trained to predict the ground truth actionconditioned on sub-trajectories via cross-entropy or mean-squared error loss, depending on the domain:

whereis the context length.
During inference, the DT is conditioned on a high RTG to produce a likely sequence of actions that yields high reward behaviour.

SECTION: Retrieval-augmented Decision Transformer (RA-DT)
Processing long sequences with DTs is computationally expensive due to the quadratic complexity of the Transformer architecture.
To address this challenge, we introduce RA-DT, which equips the DT with an external memory that relies on a vector index for retrieval.
Consequently, RA-DT consists of a parametric and a non-parametric component, reminiscent of complementary learning systems.
The former is represented by the DT and learns to predict actions conditioned on the future return.
The latter is the retrieval component that searches for relevant experiences, similar to(see Figure).

We aim at augmenting the DT with a vector index (external memory) that allows for retrieval of relevant experiences.
To this end, we build our vector index by leveraging an embedding modelthat takes a trajectoryand returns a vector of size.
Given a datasetof trajectories, we obtain a set of key-value pairs of our vector index by embedding all sub-trajectoriesviato obtain.
Note that values contain sub-trajectories ranging fromto, while keys use sub-trajectoriesfor a fixed, wheregoes over trajectory length in increments of(see Appendixfor more details).
The reason for this choice is that during inference, the model does not have access to future states.

In RAG applications for Natural Language Processing (NLP), a common choice foris a pre-trained LM.
While pre-trained models in NLP are ubiquitous, they are rarely available in RL.
A natural choice to instantiateis to train a DT on the pre-collected dataset, as they exhibit a well-separated embedding space after pre-training.
Therefore, they are well suited for retrieval since a new task can be matched to similar tasks in the vector index.
As a domain-agnostic alternative, we propose to utilize the FrozenHopfield (FH) mechanismto map trajectories to the embedding space of a pre-trained LM.
This enables instantiatingwith a pre-trained language encoder.
The FH mechanism is parameterized by an embedding matrixof a pretrained LM with vocabulary sizeand hidden dimension, a random matrixwith entries sampled from, and a scaling factorand performs:

We denoteas the input token and apply the FH position-wise to every state/action/reward token in a sub-trajectoryseparately.
Finally, we apply a LM on top of the FH to obtain the keys of our vector index by setting.
Utilizing the FH enables leveraging the expressive power of pre-trained LMs as trajectory encoders for RL.
This sidesteps the need for pre-training a domain-specific model and can be incorporated in any existing retrieval-augmentation pipeline.

Given an input sub-trajectory, we first construct a query, using our embedding model(see Appendixfor details).
Then, we use maximum inner product search (MIPS) betweenand all keysand select the corresponding top-sub-trajectoriesby:

whereis the cosine similarity.
Consequently,contains the set of retrieved sub-trajectories and their keys.
Providing too similar experiences to the model may hinder learningand we apply retrieval regularization during training (see Appendix).

Following, we characterize the usefulness of retrieved sub-trajectories inalong two dimensions:and.
The relevance of a keyis defined by its cosine similarity to the query.
While a retrieved experience may be relevant, it might not be important.
Determining the utility of a sequence in general is hard.
Thus, we experiment with two heuristics that follow different definitions of utility.
The first assigns more utility to sub-trajectories with high return, and is utilizedat inferenceonly.
The second assigns utility to sub-trajectories that originate from the same task as the query and is usedat trainingonly.
Then, we reweight a retrieved experience according to:

whereandmeasures the utility of a retrieved sub-trajectory weighted by.
Note that we instantiatedifferently depending on whether the agent is in training or inference mode.
Attrainingtime, a pre-collected set of trajectories that contains multiple tasks is stored in the vector index (Figure, left). Trajectories can be obtained from human demonstrations or RL agents.
Therefore, we encourage the agent to retrieve sub-trajectories of the same task. During training, we use:wheretakes a sub-trajectory and returns its task index.

Duringinference, we evaluate the ICL capabilities of the agent.
Starting from anemptyvector index, we store experiences of the agent while it interacts with the environment (see Figure, right).
Thus, during inference, the agent can only retrieve experiences from the same task.
Therefore, we steer the agent to produce high reward behaviour on the new task by reweighting a retrieved sub-trajectory by the total return achieved over the episode it appears in, i.e.,.
We apply this reweighting to the retrieved experiences inand select the top-elements by:

where we normalize both scores to be in the range, such that they contribute equally to the final weight.
Our reweighting mechanism is illustrated in Figure.

After reweighting, the setcontains sub-trajectories that are both important and relevant for the current inputto the DT.
To incorporate the retrieved experiences in the DT, we interleave it with cross-attention layers (CA) after every self-attention (SA) layer.
The retrieved sub-trajectories are encoded by separate embedding layers for each token type (state/action/reward/RTG) and then passed to the CA layers.
Thus, our RA-DT predicts actionsgiven input trajectory and retrieved trajectory by:

In Algorithm, we show the pseudocode for in-context RL with RA-DT atinferencetime.
In addition, we show RA-DT attrainingtime in Algorithmof Appendix.

SECTION: Experiments
We evaluate the ICL abilities of RA-DT on grid-world environments used in prior works, namely Dark-Room (see Section), Dark Key-Door (Section), and MazeRunner (Section), with increasingly larger grid-sizes, resulting in longer episodes.
Moreover, we evaluate RA-DT on two robotic benchmarks (Meta-World and DMControl, Section) and procedurally-generated video games (Procgen, Section).

Across experiments, we report performances for two variants of.
The first variant leverages a domain-specific embedding model for retrieval, specifically a DT trained on the same domain.
The second variant () makes use of the FH mechanism in combination with BERTas the pre-trained LM.
Consequently, this variant of RA-DT does not require any domain-specific pre-training of the embedding model.
We compare RA-DT against the vanillaand two established in-context RL methods, namely Algorithm Distillationand Decision Pre-trained Transformer.
Following,we report the mean across tasks and 95% confidence intervals over 3 seeds.
We use a context length equivalent to two episodes (from 200 up to 2000 timesteps) for AD, DPT and DT.
For RA-DT, we use a considerably shorter context length of 50 transitions, unless mentioned otherwise.
On grid-worlds, we train all methods for 100K steps and evaluate after every 25K steps.
Similarly, we train for 200K steps and evaluate after every 50K steps for Meta-World, DMControl and Procgen.
All grid-worlds and Procgen exhibit discrete actions and consequently, we train all methods via the cross-entropy loss to predict the next actions.
On Meta-World and DMControl, we train all method using the mean-squared error loss to predict continuous actions.
Followingand, our primary evaluation criterion is performance improvement during ICL trials.
After training, the agent interacts with the environment for a fixed amount of episodes, each of which is considered a single trial.
Upon completion of an ICL trial, the respective episode is stored in the vector index.
We provide further training and implementation details in Appendix.

SECTION: Dark-Room
Dark-Room is commonly used in prior work on in-context RL.
The agent is located in an empty room, observes only its x-y coordinates, and has to navigate to an invisible goal state (,, see Figure).
A reward of +1 is obtained in every step the agent is located in the goal state.
Because of partial observability, it must leverage memory of previous episodes to find the goal.
We conduct experiments on three different grid sizes, namely 1010, 2020, and 4020, and corresponding episode lengths of 100, 200 and 800, respectively.
We designate 80 and 20 randomly assigned goals as train and evaluation locations, respectively, as in.
We use Proximal Policy Optimization (PPO)to generate 100K transitions per goal for 1010 and 2020 grids and 200K for 4020 (see Figurefor single task expert scores).
During evaluation, the agent interacts with the environment for 40 ICL trials, and we report the scores at the last evaluation step (100K).
We provide additional details on the environment, the generated data, and the training procedure in Appendixand.

In Figure, we show the ICL performances on the 20 hold-out tasks for all considered methods on Dark-Room1010,2020, and4020.
In addition, we present the ICL curves on the training tasks and the learning curves across the entire training period in Figuresandin Appendix.
Overall, we observe that RA-DT attains the highest average rewards on all 3 grid-sizes at the end of the 40 ICL-trials.
On 1010, RA-DT obtains near-optimal performance scores both with the domain-specific and domain-agnostic embedding model.
The vanilla DT does not exhibit any performance improvement across trials.
This indicates the improvement in performance for RA-DT can be attributed to the retrieval component.
Furthermore, RA-DT outperforms AD and DPT without keeping entire episodes in its context window.
Similarly, RA-DT outperforms all baselines on the 2020 and 4020 grids.
While RA-DT successfully improves in-context, the baselines exhibit only little learning progress over the ICL trials, especially for larger grid sizes.
However, the final performance scores for 2020 and 4020 are not optimal.
With increasing grid size, discovering the goal requires systematic exploration in combination with targeted exploitation.
Therefore, we conduct a qualitative analysis on the exploration behaviour of RA-DT.
We find that RA-DT develops strategies to imitate a given successful context (see Figure), and avoids low-reward routes given an unsuccessful context (see Figure).

SECTION: Dark Key-Door
In Dark Key-Door, the agent is located in a room with two invisible objects: a key and a door.
The agent has to pick up the invisible key, then navigate to the door.
Because of the presence of two key events, the task-space is combinatorial in the number of grid-cells (possible tasks for) and is therefore considered more difficult.
A reward of +1 is obtained once for picking up the key and for every step the agent stands on the door grid-cell after it collected the key.
We retain the same experiment setup as in Sectionand provide further details in Appendix(also see Figurefor single-task expert scores).

Onand, RA-DT outperforms baselines, with the performance ranking remaining the same as on Dark-Room (see Figure).
Surprisingly, domain-agnostic RA-DT outperforms its domain-specific counterpart on, which demonstrates that the domain-agnostic embedding model is a promising alternative.
This result indicates that RA-DT can successfully handle environments with more than one key event, even with shorter observed context.

SECTION: Maze-Runner
Maze-Runner was introduced byand inspired by.
The agent is located in a procedurally-generatedmaze (see Figure), observes continuous Lidar-like depth representations of states, and has to navigate to one, two, or three goal locations in the correct order (,).
A reward of +1 is obtained when reaching a goal location.
Episodes last for a maximum of 400 steps, or terminate early if all goal locations have been visited.
Similar to Dark-Room, we use PPO to generate 100K environment interactions for 100 procedurally-generated mazes.
We train all methods on a multi-task dataset that comprises trajectories from 100 mazes, evaluate on 20 unseen mazes, and report performance over 30 ICL trials.
We give further details on the environment, the dataset, and the experiment setup in Appendixand.

We find that RA-DT considerably outperforms all baselines in terms of final performance (see Figure).
Surprisingly, RA-DT is the only method to improve over the course of the 30 ICL trials.
However, we observe a considerable performance gap between train mazes and test mazes (0.65 vs. 0.4 reward, see Figure), indicating that solving unseen mazes requires an enhanced ability to generalize and learn from previous trials.

SECTION: Meta-World & DMControl
Next, we evaluate RA-DT on two multi-task robotics benchmarks, Meta-Worldand DMControl.
States and actions in both benchmarks are multidimensional continuous vectors.
While the state and action space in Meta-World remain constant across all tasks (,), they vary considerably in DMControl (,).
Episodes last for 200 and 1000 steps in Meta-World and DMControl, respectively.We leverage the datasets released by.
For Meta-World, we pre-train a multi-task policy on 45 of the 50 tasks (ML45, 90M transitions in total) and evaluate on the 5 remaining tasks (ML5).
Similarly, on DMControl, we pre-train on 11 tasks (DMC11, 11M transitions in total) and evaluate on 5 unseen tasks (DMC5).
We provide further details on the environments, datasets, and experiment setup in Appendicesand, andandfor Meta-World and DMControl, respectively.

We present the learning curves and corresponding ICL curves for Meta-World and DMControl in Figureand, and Figuresandin Appendix, respectively.
In addition, we provide the raw and data-normalized scores in Tablesand, respectively.
On both benchmarks, we find that RA-DT attains considerably higher scores on unseen evaluation tasks, but slightly lower average scores across training tasks compared to DT.
However, these performance gains on evaluation tasks are not reflected in improved ICL performance.
In fact, we only observe slight in-context improvement on training tasks, but not on holdout tasks for any of the considered methods.

SECTION: Procgen
Finally, we conduct experiments on Procgen, a benchmark consisting
of 16 procedurally-generated video games, designed to test the generalization abilities of RL agents.
The procedural generation in Procgen is controlled by setting an environment seed, which results in visually diverse observations for the same underlying task (see-example in Figure).
In Procgen, the agent receives image-based inputs (36464).
All 16 tasks share a discrete action space ().
Rewards are either dense or sparse depending on the environment.

We followand use 12 tasks for training (PG12) and 4 tasks for evaluation (PG4). First, we generate datasets by training task-specific PPO agents for 25M timesteps on 200 environment seeds per task indifficulty.
Then, we pre-train a multi-task policy on the PG12 datasets (24M transitions in total, 2M per task).
We leverage the procedural generation of Procgen and evaluate all models in three settings:training tasks - seen(PG12-Seen),training tasks - unseen(PG12-Unseen), andevaluation tasks - unseen(PG4).
Additional details on the generated datasets and our environment setup are available in Appendicesand.

Similar to our results on Meta-World and DMControl, we find that RA-DT improves average performance scores across all three settings compared to the baselines (see Figureand Tables,,in Appendix), but no method exhibits in-context improvement during evaluation (Figure).
We further discuss our negative results on Procgen, Meta-World, and DMControl in Section.

SECTION: Ablations
To better understand the effect of learning with retrieval, we present a number of ablation studies on essential components in RA-DT conducted on Dark-Room(more details in Appendix).

To investigate the effect of learning with retrieved context, we substitute retrieval with random sampling, either over all tasks, or from the same task (see Figurea).
We find that training with retrieval outperforms both sampling variants, highlighting the benefit of training with retrieval to improve ICL abilities.
We hypothesise this is because retrieval constructs bursty sequences, which was found to be important for ICL.

(a)

(b)

(c)

RA-DT reweights a sub-trajectory by itsandscore.
By default, we use task-based reweighting during training.
In Figure, we compare against alternatives, such as reweighting by return.
Indeed, we find that task-based reweighting is critical for high performance, because it ensures that retrieved experiences are useful for predicting the next action.

We conduct a sensitivity analysis onused in the reweighting mechanism (see Equation).
In Figureb, we find that RA-DT performs well for a range of values forused during training, but performance declines if no re-weighting is employed ().
We perform the same analysis forduring evaluation in Figure.

We evaluate with three retrieval regularization strategies to mitigate the effect of copying the context: deduplication, similarity cut-off, and query dropout.
To evaluate their impact on ICL performance, we systematically removed each one from RA-DT (see Figure).
We found the combination of all three to be effective and add them to our pipeline.

Finally, we investigate how strongly domain-agnostic RA-DT is influenced by the choice of pre-trained LM for the embedding model.
We compare our default choice BERT against other smaller/larger LMs (see Figure).
We found that BERT performs best and performance decreases with smaller models.

For additional ablations on RA-DT and on our baselines, we refer to Appendix.

SECTION: Discussion
In this section, we highlight current challenges of RA-DT and other offline in-context RL methods.

Currentofflinein-context RL methods are predominantly evaluated on contextual bandits or grid-worlds, such as Dark-Room, which can only be solved by leveraging the context.
However, it remains unclear to what extent the agent learns to learn in-context or simply copies from its context.
Further, in our experiments on fully-observable environments (MetaWorld, DMControl, and Procgen), we did not observe ICL behaviour (see Appendices,,).
Therefore, it is necessary that future research on in-context RL disentangles the effects of memory and meta-learning abilities, similar to memory and credit-assignment, potentially on recent benchmarks.

Most in-context RL methods learn from offline datasets via next-action prediction and causal sequence modelling objectives.
As such, they cannot learn to infer the utility of an action, and thus, distinguish between positive and negative examples. This can induce delusions, which lead to repetitions of suboptimal actions and copying behaviour(see Figurefor examples on Dark-Room).
In contrast,onlinein-context RL methods have shown promising adaptation abilities.Consequently, a potential remedy to this problem is to train a value function to learn the utility of an action.

In LLMs, applying sophisticated conditioning strategies is important to improve ICL abilities.
Even though RTG-conditioning, and chain-of-hindsighthave shown promise for generating high reward behaviour in DTs, the broader landscape for conditioning strategies for in-context RL remains under-explored.
Therefore, we believe that systematically investigating conditioning methods for in-context RL is a fruitful direction for future research.

The diversity and scale of the pre-training dataset may significantly affect the emergence of ICL.
In our experiments, we pre-train on a relatively small set of tasks.
Our results on gridworlds suggest that this is sufficient for ICL to emerge on simple environments.
However, on more complex environments, the unseen tasks can be considered out-of-distribution and higher pre-training diversity may be necessary for ICL to emerge.
It remains unclear how much diversity is required to elicit in-context RL, and if existing large-scale agents exhibit ICL.
One promising approach is to expand the pre-training diversity through learned interactive simulations.

SECTION: Conclusion
Existing in-context RL methods keep entire episodes in their context window, which is challenging as RL environments are typically characterized by long episodes and sparse rewards.
To address this challenge, we introduce RA-DT, which employs an external memory mechanism to store past experiences and to retrieve experiences relevant for the current situation.
RA-DT outperforms baselines on grid-worlds, while using only a fraction of their context length.
While RA-DT improves average performance on holdout tasks on complex environments, it struggles to exhibit ICL, along with other in-context RL methods.
Consequently, we illuminate the current limitations of in-context RL methods and discuss future directions.
Finally, we release our datasets for Dark-Room, Dark Key-Door, MazeRunner, and Procgen, to facilitate future research on in-context RL.

Besides the general directions discussed in Section, we highlight a number of concrete approaches to extend RA-DT.
While we focus on ICL without relying on expert demonstrations, pre-filling the external memory with demonstrations may enable RA-DT to perform more complex tasks.
This may be effective for robotics applications, where expert demonstrations are easy to obtain.
Furthermore, end-to-end training of the retrieval component in RA-DT, similar to, may result in more precise context retrieval and enhanced down-stream performance.
Finally, we envision that modern recurrent architecturesas policy backbones may benefit RA-DT by maintaining hidden states across many episodes.

We acknowledge EuroHPC Joint Undertaking for awarding us access to Karolina at IT4Innovations, Czech Republic, MeluXina at LuxProvide, Luxembourg, and Leonardo at CINECA, Italy.
The ELLIS Unit Linz, the LIT AI Lab, the Institute for Machine Learning, are supported by the Federal State Upper Austria. We thank the projects Medical Cognitive Computing Center (MC3), INCONTROL-RL (FFG-881064), PRIMAL (FFG-873979), S3AI (FFG-872172), DL for GranularFlow (FFG-871302), EPILEPSIA (FFG-892171), AIRI FG 9-N (FWF-36284, FWF-36235), AI4GreenHeatingGrids (FFG- 899943), INTEGRATE (FFG-892418), ELISE (H2020-ICT-2019-3 ID: 951847), Stars4Waters (HORIZON-CL6-2021-CLIMATE-01-01). We thank NXAI GmbH, Audi.JKU Deep Learning Center, TGW LOGISTICS GROUP GMBH, Silicon Austria Labs (SAL), FILL Gesellschaft mbH, Anyline GmbH, Google, ZF Friedrichshafen AG, Robert Bosch GmbH, UCB Biopharma SRL, Merck Healthcare KGaA, Verbund AG, GLS (Univ. Waterloo), Software Competence Center Hagenberg GmbH, Borealis AG, TÜV Austria, Frauscher Sensonic, TRUMPF and the NVIDIA Corporation.

SECTION: References
SECTION: Appendix
SECTION: Ethics Statement & Reproducibility
In recent years, there has been a trend in RL towards large-scale multi-task models that leverage offline pre-training.
In this work, we broadly aim at building agents that can learn new tasks via ICL without the need for re-training or fine-tuning.
Our goal is to reduce the need to provide entire past episodes in the agent’s context, by augmenting the agent with an external memory in combination with a retrieval component, similar to RAG in LLMs.
We believe that multi-task agents of the near future will be able to perform a broad range of tasks, and that these agents will greatly benefit from RAG as used in RA-DT.
The external memory component can enable agents to leverage information from in its own distant past or experiences from other agents.
Such agents could have an immense impact on the global economy (e.g., as a source of inexpensive labour).
As such, they do not come without risks and the potential for misuse.
While we believe that our work can significantly impact the positive use of future agents, it is essential to ensure responsible deployment of future technologies.

We open-source the code-base used for our experiments, and release the datasets we generated.
Both are available at:.
In addition, we provide further information on the environments/datasets, implementation including hyperparameter tables, and on our experiments in Appendices,,, respectively.

SECTION: Environments & Datasets
SECTION: Dark-Room and Dark Key-Door
The Dark-Room environment is modelled after Morris-Watermaze, a classic experiment in behavioural neuroscience for studying spatial memory and learning in animals.
We design our Dark-Room and Dark Key-Door environments in Minihack, which is based on the NetHack Learning Environment.
We construct grids of dimensions,and, as depicted in Figure.
With increasing grid sizes, the task of locating the goal becomes harder as the number of possible positions in the grid grows (100, 400, 800).
Therefore, we set the number of interaction steps per environment equal to the number of grid cells.
Consequently, larger grids results in longer episodes and thus context lengths (e.g., 2400 for AD).
The agent observes its own x-y position on the grid and can perform one of 5 actions at every interaction step (up, down, left, right, stay).
Episodes start in the top left corner (0,0) and the agent is reset to the start position after every episode.

In, the agent has to navigate to a randomly placed and invisible goal position.
Therefore, the task space in Dark-Room environments is equal to the number of grid-cells (i.e.,for).
The agent receives a reward for +1 for every step in the episode it is located in the goal position and 0 otherwise.
As there are as many grid-cells as episode steps, the optimal strategy for solving the Dark-Room task is to use the first episode to visit every cell to find the hidden goal location.
Once found, this knowledge can be exploited in upcoming trials.

In contrast, in, there are two objects: a key and a goal state.
Similar to Dark-Room, the key and goal position are randomly placed on the grid.
The agent has to first pick up the invisible key and then find the invisible goal.
Due to the presence of the two key events (picking up the key, finding the goal), the task space is combinatorial in the number of grid-cells (i.e.,for).
This makes the Dark Key-Door more challenging than the Dark-Room task, especially as the grid-size becomes larger.

For both Dark-Room and Dark Key-Door, we generate training datasets for 80 randomly assigned goals or key-goal combinations.
We use PPOto generate 100K environment transitions per goal location forandgrids and 200K environment transitions for the largest grid.
Therefore, the total number of transitions across datasets is 8M forandgrids and 16M for.

We train PPO with standard hyperparameter settings inusing a learning rate of, batch size of, number of steps between updates of, number of update epochsand entropy coefficient of.
Forandgrids, we increase the number of update epochs toand the entropy coefficient of tofor. We store all generated transitions of PPO for our datasets. Consequently, the final datasets contain a mixture of suboptimal or exploratory, and optimal or exploitative behaviour.

We show average learning curves across all task-specific PPO agents on the 80 training tasks for all grid-sizes in Figuresandfor Dark-Room and Dark Key-Door, respectively.
For thegrids, the average performance converges towards optimal performance.
However, on the larger grid sizes, the performances are below the optimum.
This is because it takes the agent longer to discover and collect successful episodes by initially random environment interaction as the grids become larger.

SECTION: MazeRunner
MazeRunner was introduced byand inspired by the Memory Maze environment.
The agent is located in a 1515 procedurally-generated maze and has to navigate to a sequence of one, two, or three goal locations in the right order (see Figure).
Similar to Dark-Room environments, MazeRunner is partially observable and exhibits sparse rewards.
The agent observes a Lidar-like 6-dimensional representation of the state that contains 4 continuous values that measure the distance from the agent’s location to the nearest wall, and the x-y coordinates of the agent’s position in the grid.
The action-space is 4-dimensional (up, down, left, right).
A reward of +1 is obtained when reaching the currently active goal state in the goal sequence.
Therefore, the total achievable reward is equal to the number of goal states.
Episodes last for a maximum of 400 steps or terminate early, if all goal locations have been reached.
After every episode, the agent (gray box in Figure) is reset to the origin location.
During evaluation, we allow for 30 ICL trials, which amounts to 12K environment steps in total.

The procedural-generation of the maze and selection of the number of goals is controlled by setting the environment seed.
We use PPO to generate 100K environment interactions for 100 procedurally-generated mazes, and record the entire replay buffer, which amounts to 10M transitions in total.
We found it necessary, to equip the task-specific PPO agents with an LSTMpolicy.
Without the LSTM, agents hardly make progress for some mazes, especially if the maze contains two or three goal locations.
For this reason, we first generate data for more than 100 mazes and select the first 100 seeds, where the average reward at the end of training is.
This results in a set of seeds inOtherwise, we use standard hyperparameter settings as provided in.

We show the average learning curves over all 100 task-specific PPO agents in Figure.
On average, the agents receive a reward ofover all mazes.
This average include environments with one, two or three goals.
We provide further dataset statistics for MazeRunner with the corresponding dataset release.

SECTION: Meta-World
The Meta-World benchmarkconsists of 50 challenging robotics tasks, such as opening/closing a window, using a hammer, or pressing buttons.
All tasks in Meta-World use a Sawyer robotic arm simulated using the MuJoCo physics engine.
The observations and actions are 39-dimensional and 6-dimensional continuous vectors, respectively.
As all tasks share the robotic arm, the state, and action spaces remain constant across tasks.
All actions are in range.
The reward functions are dense and based on distances to the goal locations (exact reward-definitions are provided in).
Similar toand, we limit the episode lengths to 200 interactions.
We followand split the 50 Meta-World tasks into 45 training tasks (ML45) and 5 evaluation tasks (ML5).
During evaluation, we use deterministic environment resets after episodes, i.e., objects and goal positions are reset to their original state.
Furthermore, we mask-out the goal positions in the state vector, which forces agents to adapt during environment interaction.
Agents are given 30 ICL trials during evaluation.
The 5 evaluation tasks are:

,,,,

.
For our Meta-World experiments, we leverage the datasets released by.
The datasets contain 2M transitions per task, which amounts to 90M transitions across all ML45 training tasks.
The data was generated with randomized object and goal positions after every episode.

SECTION: DMControl
DMControl contains 30 different robotic tasks with different robot morphologies.
Similar to prior work, we select 16 of these 30 tasks and split them into 11 training (DMC11) and 5 evaluation tasks (DMC5).
The DMC11 training tasks are:

,,,,,,,,,,

The DMC5 evaluation tasks are:

,,,,

States and actions in DMControl are continuous vectors.
As DMControl contains different robot morphologies, the state, and action spaces vary considerably across tasks (,).
All actions in DMControl are bounded by.
Episodes last for 1000 environment steps and per time-step a maximum reward of +1 can be achieved, which results in a maximum reward of 1000 per episode.
Agents are given 30 ICL trials per task during evaluation, which results in 30K steps for a single evaluation run.

.
As for Meta-World, we leverage the datasets released by.
The datasets contain 1M transitions per task, which amounts to 11M transitions used for training across all DMC11 tasks. We refer tofor further dataset statistics on DMControl and Meta-World.

SECTION: Procgen
The Procgen benchmark consists of 16 procedurally-generated video games and was designed to test the generalization abilities of RL agents.
Unlike other environments considered in this work, Procgen environments emitsimages as observations.
All 16 environments share a common action space of 15 discrete actions.
The procedural generation in Procgen is controlled by setting an environment seed.
The environments seed randomizes the background and colour of the environment, but retains the same game dynamics.
This results in visually diverse observations for the same underlying task, as illustrated in Figurefor three seeds on the game.
The rewards in Procgen can be dense or sparse depending on the environment.

We followand use 12 tasks for training and 4 tasks for evaluation, which we refer to as PG12 and PG4, respectively.
The PG12 tasks are:

,,,,,,,,,,,

The PG4 tasks are:,,,

We exploit the procedural generation of Procgen and evaluate all models in three settings: (1) training tasks - seen seed (PG12-Seen), (2) training tasks - unseen seed (PG12-Unseen), and (3) evaluation tasks - unseen seed (PG4).
In particular, the agents observe data from 200 different training seeds.
To enable ICL to the same environment, we always keep the same seed during evaluation (seed=1 for PG12-seen, seed=200 for PG12-Unseen and PG4).
During evaluation, we limit the episode lengths to 400 steps.

We generate datasets by training task-specific PPO agents for 25M timesteps on 200 environment seeds per task indifficulty, as proposed in by.
We train PPO using the same hyperparameter settings as, using a learning rate of, batch size 2048, number of update epochs of 3, entropy coefficient of, GAE, and with reward normalization.
We use 256 timesteps per rollout over 64 parallel environments, which results in 16384 environment steps per rollout in total.
Furthermore, we found it useful to decrease the discount factor to 0.99.

As in previous experiments, we record the entire replay buffer and consequently, the datasets contain mixed-quality behaviour.
We subsample the 25M transitions per task, by storing only the observations of the first 5 parallel environments, which results in approximately 2M transitions per task.
To ensure disk-space efficiency, all trajectories are stored in separatefiles in the lowest compression level files, with all image-observations encoded in.
Consequently, the datasets for all 16 tasks (32M transitions) take up only 70GB of disk space, and theirformat enables targeted reading from disk, without loading an entire trajectory into RAM.
We release two versions of our datasets: a smaller one containing 2M transitions per task as used in our experiments, and a larger one containing 20M transitions per task.

We show the individual learning curves for all tasks in Figure, and the aggregate statistics over all 16 datasets in Table.

SECTION: Experimental & Implementation Details
SECTION: General
We compare RA-DT against DT, AD, and DPT on all environments.
On grid-world environments, we train all methods for 100K steps and evaluate after every 25K steps.
For Meta-World, DMControl and Procgen, we train for 200K steps and evaluate after every 50K steps.
During evaluation, the agent is given 40 interaction episodes for ICL on Dark-Room and Dark Key-Door, and 30 episodes on MazeRunner, Meta-World, DMControl, and Procgen.
We use the ICL curves as the primary evaluation mechanism, and report the scores at the last evaluation step (100K or 200K).
Following,we report the mean and 95% confidence intervals across tasks and over 3 seeds in all experiments.

Across experiments, we keep most parameters fixed, unless mentioned otherwise.
We train with a batch size of 128 on all environments, except forgrids, where we use a batch size of 32.
We use a constant learning rate ofand 4000 linear warm-up steps followed by a cosine decay toand train using the AdamW optimizer.
Furthermore, we employ gradient clipping of 0.25, weight decay of 0.01, and a dropout rate of 0.2 for all methods.

On grid-worlds, we use a context lengthequivalent to two 2 episodes for AD, DPT and DT.
For example, ongrids, this results in a sequence length of 6400 (for state/action/reward/RTG) for the DT and a sequence length of 4800 for AD.
On Meta-World, DMControl and Procgen, we reduce the sequence context length to 50 steps for DT.
For RA-DT, we use a shorter context length oftransitions across environments, except forandgrids, where we increase the context length to 100.
We want to highlight, that the context length for RA-DT applies to both the input context and the retrieved context.
The retrieved context contains the past, and future context, as described in Section.
Consequently, the effective context length of RA-DT isand is independent of the episode length.

For all environments, except for Procgen, we use a GPT2-like network architecturewith 4 Transformer layers, 8 head and hidden dimension of 512, which results in 16M parameters.
On Procgen, we use a larger model with 6 Transformer blocks, 12 heads and hidden dimension of 768.
States, actions, rewards and RTGs are embedded using separate embedding layers per modality, as proposed by.
For all modalities and environments, we use standard linear layers to embed the inputs.
Procgen is again an exception, where we use the convolutional architecture proposed byand adopted in prior works.
Processing image-sequences is computationally demanding.
Therefore, we first pre-train the vision-encoder using a separate DT and embed all images in the dataset using the learned vision encoder.
Therefore, the data-loading is not bottlenecked by loading entire images into memory, but only their compact representations.

Furthermore, we use global positional embeddings.
We also experimented with the Transformer++ recipe (RoPE, SwiGLU, RMSNorm), but only observed minimal performance gains for our problem setting.
To speed-up training, we use mixed-precision, model compilation as supported in PyTorch, and FlashAttention.

.
Our implementation of the DT is based on thelibraryand.
We integrated AD, DPT, and RA-DT on top of this implementation.

.
We run all our experiments on a server equipped with 4 A100 GPUs.
For most of our experiments, we only use a single A100.
Depending on the environment and method used, training times range from one hour (Dark-Room, DT) to 20 hours (DMControl, AD) for a single training run.

SECTION: Decision Transformer
For Dark-Room and Dark Key-Door, we sample the target return for RTG conditioning before every episode,, andfor grid sizes,, and, respectively.
On grid-worlds, we found that sampling the target return performs better than using a fixed target return per grid size.
We assume this is, because specifying a particular target return biases the DT towards particular goal locations.
For MazeRunner, we use a constant target return of 3.
For Meta-World, DMControl, and Procgen, we set the target return the maximum return achieved for a particular task in the training datasets.
However, we also found that constant target returns per domain work decently.

SECTION: Algorithm Distillation
AD obtains a context trajectory and learns to predict actions of an input trajectory takenepisodes later.
Therefore, we tuneper domain.
On grid-worlds, we foundto perform the best, similar to.
For MazeRunner and Meta-World, we set, and for DMControl and Procgen, we set.

SECTION: Retrieval-Augmented Decision Transformer
For the embedding model, we either use a DT pre-trained on the same environment with the same hyperparameters as listed in Section, or a pre-trained and frozen LM.
For the pre-trained LM, we usefrom thelibrary by default.
BERT is an encoder-only LM with 110M parameters, vocabulary size, and embedding dimension of.
We apply FrozenHopfield withto state, action, reward and RTG tokens (see Equation).
To achieve this, we one-hot encode all discrete input tokens, such as actions in Dark-Room/MazeRunner/Procgen or states in Dark-Room, and rewards/RTGs in the sequence before applying the FH.
For other tokens, such as continuous states/actions as in Meta-World/DMControl, we directly apply the FH.
We evaluate other alternatives for the LM in Appendix.

Regardless of whetheris domain-specific or domain-agnostic, we obtainembedded tokens after applyingto the input trajectory.
Subsequently, we apply mean aggregation over the context lengthto obtain the-dimensional query representation.
We experimented with aggregating over all tokens or only tokens of a particular modality (state/action/reward/RTG), and found aggregation over states-only to be most effective (see Appendix).
As described in Section, we construct the key-value pairs in our retrieval index by embedding all sub-trajectories in the datasetusing our embedding model,.
To avoid redundancy, in practice we constructkey-value pairs for a given trajectorywith episode lengthand sub-sequence length, instead of constructing the key and values for every step.
Note that the values, we store, contain both the sub-trajectory itself () and its continuation ().
Similar to, we found this choice important for high performance in RA-DT, because it allows the model to observe how the trajectory may evolve if it predicts a certain action (given that the retrieved context is similar enough).

We use Faissto instantiate our vector index.
This allows us to search our vector index intime using Hierarchical Navigable Small World (HNSW) graphs.
However, in practice we found it faster to use a Flat index on the GPU as provided by Faiss instead of using HNSW, because our retrieval datasets are small enough.
We use retrieval both during training and during inference.
It is, however, possible to pre-compute the retrieved trajectories forprior to the training phase to limit the computational demand of retrieval, as suggested by.
During evaluation, we can retrieve after every environment step or only after everyenvironment steps.
Here,represents a trade-off between inference time and final performance.
We usefor Dark-Room and Dark Key-Door, andfor all other environments (see Appendixfor an ablation on this design choice).
For all environments, except for Meta-World and DMControl, we provide a single retrieved sub-trajectory in the agent’s context.
For Meta-World and DMControl, we found that providing more than one retrieved sub-trajectory benefits the agent’s performance.
Therefore, for these two environments, we retrieve the top-4 sub-trajectories, order them by return achieved in that trajectory, and provide their concatenation as retrieved context for RA-DT.

To implement the reweighting mechanism, as described in Section, we first retrieve the topexperiences and the select the top-experiences according to their reweighted scores.
We setin all our experiments.

After the most similar trajectories have been retrieved, we embed the state/action/reward/RTG tokens with a separate embedding layers (as is done for the regular input sequence) before incorporating them via the CA layers.
We also experimented with sharing/detaching the regular embedding layers, but found it most effective to maintain separate ones.
Furthermore, we experimented with an additional Transformer-based encoder for the retrieved sequences, as proposed by, but did not observe substantial performance gains despite increased computational cost.

For all our experiments, we use the same dataset for retrievalas is used for training, that is.
Therefore, we prevent retrieving sub-sequences from the same trajectory as the query.

We found it advantageous to regularize the k-NN retrieval in RA-DT throughout the training phase.
In RL datasets, there is often a substantial overlap between trajectories, leading to many similar sub-trajectories.
This poses a significant challenge, as retrieving only similar sub-trajectories encourages the agent to adopt copying behaviour, which renders the DT unable to produce high-reward actions during inference.

One simple strategy to mitigate this issue is, i.e., to discard duplicate experiences before the training phase of RA-DT.
To achieve this, we first construct our index as described in Section.
For every key, we retrieve the top-neighbours (excluding experiences from the same episode as).
If the similarity score is above a cosine similarity of, we discard the experience.
This substantially reduces the number of experiences in the index and speeds-up retrieval.

Two other strategies for regularizing retrieval during the training phase, areand.
Similarity cut-off first retrieves the topexperiences, discards the experiences with a similarity score above a threshold (e.g.,), and retains only the remaining experiences.
If used in combination with reweighting, we set.
Query dropout randomly drops-out tokens (e.g., 20%) of the embedded sub-trajectory, which leads to more diverse retrieved experiences.
We found both strategies effective for RA-DT.
We use query dropout of 0.2, similarity cut-off of 0.98, and deduplication by default.
Furthermore, for Meta-World and DMControl, we founduseful.
Query-blending interpolates between then actual query and a randomly selected key from the retrieval index,.
For Meta-World and DMControl we additionally set.

On Dark-Room and Dark Key-Door environments, we found it useful to replace retrieved experiences with experiences randomly sampled from the same task, if the query sub-sequence is from the beginning of the episode (i.e., smaller than timestep 10).
This is because on these two environments, retrieving appropriate experience can be difficult if the given query sub-sequence is too short.

Finally, we use the same RTG-conditioning strategy as the vanilla DT, as described in Appendix.

SECTION: Additional Results
SECTION: Dark-Room
Analogous to the ICL curves on the 20 evaluation tasks in Figure, we present ICL curves on the 80 train tasks in Figure.
In general, we observe a similar learning behaviour on the train tasks as on the evaluation tasks, with slightly higher scores on average.
Interestingly, the domain-agnostic variant of RA-DT slightly outperforms its domain-specific counterpart on the training tasks.

In addition, we also show the learning curves on Dark-Roomover the entire training phase in Figure.
We evaluate after every 25K updates and observe a steady improvement in the average performances with every evaluation.

We conduct a qualitative analysis on Dark-Roomto better understand how RA-DT leverages the retrieved context sub-sequences.
First, we analyse the attention maps for different Dark-Roomgoal locations.

In Figure, we showcase this example.
The goal location is located at grid cell (4,6).
The attention maps exhibit high attention scores for the state and the RTG at the end of the retrieved trajectory.
We also observe high attention scores for the state similar to the current state and the action selected in that state.
The agent initially imitates the actions in the context trajectory, but deviates further into the episode.
Once the agent reaches the goal state, the attention scores for states and RTGs at the end of the trajectory reduce considerably, because the agent need not pay attention to the retrieved context any more.

Similarly, we show the corresponding example in Figure.
The goal location is again in grid cell (4,6).
The retrieved context trajectory reaches the final state (9,5).
Similar to Figure, the attention maps exhibit high attention scores for the last state and RTG for that state, as well as for a state at a similar timestep.
Previously, RA-DT imitated the action, but in this situation the agent
picks a different route, as the context trajectory does not lead to a successful outcome.

This analysis suggests, that RA-DT can develop capabilities to either imitate a given positive experience or to behave differently than a given negative experience.

In Section, we found that RA-DT learned to either copy or avoid behaviours given positive or negative context trajectories.
Therefore, we further analyse the exploration behaviour of RA-DT by visualizing the state-visitation frequencies on Dark-Roomacross the 40 ICL trials for three different goal locations:,, and(see Figure).
The agent visits nearly all states at least once at test time, as visualized in Figure(a) and (b).
Once the agent finds the goal location, it starts to imitate and stops exploring, as illustrated in Figure(c).

Furthermore, we find that in some unsuccessful trials, the agent repeatedly performs the same suboptimal action sequences.refer to such behaviour as delusions.
In Figure, we illustrate two examples in which the agent suffers from delusions and does not recover until the end of the episode.

SECTION: Maze-Runner
In Figuresand, we report the average performances at the end of the training (100K) for both the 100 train and 20 evaluation mazes, as well as the corresponding ICL curves, respectively.

While RA-DT outperforms competitors, we observe a considerable performance gap between train mazes and test mazes (0.65 vs. 0.4 reward, see Figure).
This indicates that RA-DT struggles to solve difficult, unseen mazes.
We believe that this gap is an artifact of the small pre-training distribution of 100 mazes, and be closed by increasing the number of pre-training mazes.
Furthermore, increasing the number of ICL trials may also enhance the performance.

SECTION: Meta-World
In Figuresand, we show the training curves across the entire training period (200K steps), and the corresponding ICL curves at the end of training for both ML45 and ML5.

Generally, we observe that RA-DT outperforms competitors on the evaluation tasks in terms of average performance.
However, on training task, the average performance of RA-DT is lower than of the vanilla DT.
AD and DPT lack behind both methods.
One potential reason is the RTG conditioning, which biases DT and RA-DT towards higher quality behaviour.

Nevertheless, we do not observe improved ICL performance of RA-DT on evaluation tasks.
While all in-context RL methods exhibit in-context improvement on the training tasks (ML45), neither RA-DT nor other methods show signs of improvement on the evaluation tasks (MT5).

In addition, we provide the average rewards and data-normalized scores in for the MT5 evaluation tasks in Table.

SECTION: DMControl
In Figuresand, we show the training curves across the entire training period (200K steps), and the corresponding ICL curves at the end of training for both DMC11 and DMC5.

Similar to our results on Meta-World, we observe that RA-DT outperforms competitors on average.
However, we do not observe in-context improvement on the evaluation tasks.

In addition, we show the average rewards obtained and corresponding data-normalized scores for all DMC5 evaluation tasks in Table.

SECTION: Procgen
In Figuresand, we show the training curves across the entire training period (200K steps), and the corresponding ICL curves at the end of training for PG12-Seen, PG12-Unseen, and PG4.
While we observe slightly better average performance of RA-DT compared to competitors, we do not find any in-context improvement.

.
Building on work by,identified trajectory burstiness as one important property for ICL to emerge on the Procgen benchmark.
A given sequence is considered bursty, if it contains at least two trajectories from the same seed (or level).
Consequently, the agent obtains relevant information that it can leverage to predict the next action.
Therefore, we followand always provide a trajectory from the same seed in the context of AD and DPT.
Indeed, we observed that this improves performance, compared to not taking trajectory burstiness into account.
Interestingly, we found that RA-DT retrieves trajectories from the same or similar seeds (seed accuracy of%), that is, RA-DT automatically constructs bursty sequences.
This intuitively makes sense, as retrieval directly searches for the most relevant experiences (see Section).
Therefore, for RA-DT, we do not provide additional information that indicates with which environment seed the trajectory was generated.

SECTION: Ablation Studies
To better understand the effect of learning with retrieval, we presented a number of ablation studies on critical components in RA-DT (Section).
We conduct all ablations on Dark-Roomand otherwise retain the same experiment design choices, as reported in Section.

SECTION: Retrieval outperforms sampling of experiences
RA-DT is conditioned on sub-trajectories via cross-attention.
By default, RA-DT leverages retrieval to search for relevant sub-trajectories for a given input sequence.
Instead of retrieval, sub-trajectories can be sampled at random from the external memory. Therefore, we conduct an ablation in which we swap the retrieval mechanism with random sampling of sub-trajectories during training.
This is to investigate the effect of relevance of retrieved sub-trajectories on learning performance.
We apply random sampling only during training and use our regular retrieval during inference.

In Figurea,
we show the ICL curves for training RA-DT with retrieved sub-trajectories, sub-trajectories sampled from the same task as the input sequence, and sub-trajectories sampled uniformly across all tasks.
We find that training with retrieval outperforms both sampling variants.
Uniform sampling results in poor ICL performance.
A reason for this, is that context trajectories from a different goal location, are not relevant for predicting actions in the current sequences.
As a result, the model ignores the given context during the training phase, and subsequently is unable to leverage it during inference.
In contrast, sampling sub-trajectories from the same task as the input sequence results in better ICL performance, as the model learns to make use of the context trajectories.
Nevertheless, using retrieval results in even better ICL performance, as sub-trajectories are not only relevant for the current task, but also similar to the current situation.

SECTION: Reweighting Mechanism
Next, we evaluate how our reweighting mechanism affects the ICL abilities of RA-DT.
RA-DT reweights a sub-trajectory by itsandscore (see Section).
During training, we set, if theis from the same task as, and 0 otherwise.
Instead of reweighting by task ID, alternatives are to reweight aby its return achieved or by its position in the training dataset.
When reweighting by position, we assignifwas generated beforeby the PPO agent that generated the data.
Reweighting by position makes it likely that RA-DT observes the improvement steps in its context.

We find that task-based reweighting is essential for achieving the highest performance scores (see Figure).
Using no reweighting at all results in a considerable drop in ICL performance.
However, using retrieval with no task reweighting still compares favourably to uniform sampling across all tasks.
This result suggests that retrieval can play an important role in environments without a clear task separation or in scenarios where no task IDs are available.

In addition, we conduct a sensitivity analysis on theparameter used in the re-weighting mechanism that determines how strongly the utility scores influences the final retrieval score.is used both during training for task-based reweighing and during evaluation for return-based reweighting (see Section).
In Figure, we varyduring training, orduring evaluation, while keeping the other fixed.
We find that RA-DT perform well for a range of values, but performance declines if no re-weighting is employed ().

SECTION: Retrieval Regularization
Providing the agent with too similar trajectories, can encourage it to adopt copying behaviour instead of generating high-reward actions.
To mitigate this, we found it useful to regularize the retrieval using three strategies: deduplication, similarity cut-off, and query dropout.
To evaluate their individual impact on ICL performance, we systematically removed each one from RA-DT in Figure.

We find that deduplication plays the most significant role in enhancing performance.
One reason, why deduplication is effective, is because RL datasets contain many very similar trajectories.
Removing overlapping trajectories altogether is therefore beneficial for learning.
Notably, deduplication also reduces the index size, thereby speeding-up the search process.
The effect of deduplication may vary depending on dataset characteristics, such as state-action coverage.

SECTION: Query Construction & Sequence Aggregation
In RA-DT, we aggregate the hidden states of an input trajectory using mean aggregation of state tokens over the context lengthto obtain the-dimensional query representation.
It is, however, possible to use the hidden states of other tokens to construct the query.
Therefore, we provide empirical evidence for this design choice in Figurea.
We compare aggregating states, rewards, actions, returns-to-gos, all tokens, or only using the very last hidden state.
Indeed, we find that aggregating state tokens gives the best results.

(a)

(b)

(c)

SECTION: Placement of Cross-Attention Layers
Next, we investigate the effect of the placement of the cross-attention layers in RA-DT.
In Figureb, we therefore vary the placement of cross-attention layers in RA-DT.
By default, we use cross-attention after every self-attention layer.
We find that other choice also provide good results.
While placing the cross-attention at bottom layers tends to be beneficial, placing them only upper level layers tends to hurt performance.

SECTION: Interaction steps between context retrieval
As mentioned in Section, we perform context retrieval after everyenvironment steps.
Here,represents a trade-off between inference time and final performance.
For grid-worlds, we useby default.
To better understand the effect of this design choice, we conduct an ablation in which we vary(see Figurec).
Indeed, we find that higher values forresult in a slight decrease in performance, but faster inference.

SECTION: Effect of retrieval-augmentation on Training efficiency
SECTION: Effect of retrieval-augmentation on Inference efficiency
SECTION: Pre-trained Language Model
We investigate how strongly the ICL performance of RA-DT is influenced by the pre-trained LM used in our domain-agnostic embedding model.
In Figure, we compare our default choice BERTagainst four alternative encoder and decoder backbones, namely RoBERTa, DistilRoBERTa, DistilBERTand DistilGPT2.
We find that RA-DT maintains decent performance across all pre-trained LMs, indicating robust retrieval performance across different LMs.
Generally, the non-distilled variants outperform their distilled counterparts.
Moreover, this experiment suggests a clear advantage of encoder-only models over the decoder-only LM, DistilGPT2.
This suggests that the encoder-only LMs are better able to capture the relations between tokens within the token sequence, which leads to more precise retrieval of sub-trajectories and higher down-stream performance.

SECTION: Effect ofon Algorithm Distillation
Finally, we investigate the effect ofon the performance of AD.determines the number of episodes that have passed between the current and the context trajectory, which are provided to AD as the context.
Consequently,specifies the extent of improvement observed between subsequent episodes.
By default, we usefor our experiments on Dark-Room.
Therefore, we conduct an ablation study, in which we very(see Figure.
We find that too small values for(e.g., 1 and 10) result in slow ICL behavior.
In contrast, too high values for(e.g., 500) lead to fast initial progress but suboptimal performance in the long term.
Onlyleads to steady improvement across all interaction episodes.
Consequently, AD requires careful tuning of.

SECTION: Convergence of Baselines