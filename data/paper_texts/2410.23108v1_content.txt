SECTION: Controllable Game Level Generation: Assessing the Effect of Negative Examples in GAN Models

Generative Adversarial Networks (GANs) are unsupervised models designed to learn and replicate a target distribution. The vanilla versions of these models can be extended to more controllable models. Conditional Generative Adversarial Networks (CGANs) extend vanilla GANs by conditioning both the generator and discriminator on some additional information (labels). Controllable models based on complementary learning, such as Rumi-GAN, have been introduced. Rumi-GANs leverage negative examples to enhance the generator’s ability to learn positive examples. We evaluate the performance of two controllable GAN variants, CGAN and Rumi-GAN, in generating game levels targeting specific constraints of interest: playability and controllability. This evaluation is conducted under two scenarios: with and without the inclusion of negative examples. The goal is to determine whether incorporating negative examples helps the GAN models avoid generating undesirable outputs. Our findings highlight the strengths and weaknesses of each method in enforcing the generation of specific conditions when generating outputs based on given positive and negative examples.

SECTION: Introduction

In traditional generative models without conditioning, there is limited control over the attributes of the generated data. This limitation is effectively addressed by conditional models, where additional information can condition the data generation process towards specific outcomes.
Recent research byAsokan and Seelamantula (2020)introduces a method where GAN models are trained using both positive and negative examples to guide and avoid specific outcomes (Rumi-GAN). Positive examples are data samples that align well with the desired outcome or target distribution. These examples represent the ideal or goal that the GAN is trying to achieve. Negative examples, on the other hand, are data samples that do not meet the desired outcome or diverge from the target distribution.

This study investigates the performance of two controllable GAN generators (CGAN and Rumi-GAN) through three sets of experiments conducted in the context of two 2D tile-based games. In each experiment, our goal is to enforce the generation of specific constraints within each game level. The first experiment aims to generate playable levels, emphasizing playability as the targeted constraint. The second experiment builds on the first by controlling a game-related feature, such as the specific number of pipes or treasures within the levels, while maintaining the playability constraint.

The first controllable model, a Conditional GAN (CGAN), is trained with levels containing both desired and undesired conditions (positive and negative examples), incorporating additional label information about the level conditions into the data. The second model, Rumi-GAN, also receives levels with both desired and undesired conditions and employs a loss function to encourage the generation of level segments with desired conditions while discouraging those with undesired conditions. Additionally, we utilize a baseline control model, a vanilla GAN, which cannot incorporate negative examples and, therefore, only receives levels with the desired condition for training.

Our results show that incorporating negative examples in training GAN models can help enforce some constraints of interest like playability.
To our knowledge, this study is the first attempt to systematically compare various controllable GAN models and evaluate their effectiveness in conditioning generated outputs. Additionally, the integration of negative examples in GAN training (Rumi-GAN) has previously been only utilized on state-of-the-art computer vision datasets like MNIST, CelebA, and CIFAR-10, and this work is the first to apply this new approach to game-level segments.

The codebase of the project (including the training database, trained models, and generated artifacts) is available on GitHub111https://github.com/MahsaBazzaz/NegativeExamples

SECTION: Related Work

A range of studies have explored the use of GAN models for game level generation.Kumaran, Mott, and Lester (2020)developed a GAN-based architecture capable of generating levels for multiple distinct games from a common gameplay action sequence.Volz et al. (2018)trained a GAN to generate Super Mario Bros levels, using a variety of fitness functions to guide the search for levels with specific properties.Giacomello, Lanzi, and Loiacono (2018)applied GANs to learn a model of DOOM levels, finding that the inclusion of topological features improved the similarity of generated levels to human-designed ones.Capps and Schrum (2021)focused on generating whole Mega Man levels by using multiple GANs to model levels with different types of segments.Schrum et al. (2020)used interactive exploration of the GAN latent space to generate Mario levels and dungeons.Awiszus, Schubert, and Rosenhahn (2020)proposed TOAD-GAN that expands SinGAN architecture, enabling it to create token-based levels while being trained on a single example.

Conditional GANs have also been utilized in image generation and more recently in game level generation.Torrado et al. (2020)proposed a new GAN architecture, CESAGAN, for video game level generation, which incorporated an embedding feature vector input to condition the training of the discriminator and generator. This approach also introduced a bootstrapping mechanism to reduce the number of levels necessary for training and generate a larger number of playable levels with fewer duplicates.Kelvin and Anand (2020)applied conditional generative adversarial networks (CGANs) to create road maps, providing a balance between user control and automatic generation.Hald et al. (2020)furthered the application of GANs by using parameterized GANs to produce levels for a puzzle game, although they encountered challenges in approximating certain conditions.

Outside the domain of game level generation,Asokan and Seelamantula (2020)introduces a novel method for training GANs called “Rumi-GAN”. This approach incorporates the concept of negative samples, which the GAN learns to avoid. The Rumi-GAN enhances the discriminator’s capability to accurately model the target distribution and expedites the learning process of the generator. This method has been validated through various experiments, demonstrating its effectiveness, especially in scenarios involving imbalanced datasets where certain classes are under-represented.

Inspired by this approach, we aim to apply negative examples to the generation of game levels. Specifically, we intend to compare the performance of vanilla GAN, CGAN, and Rumi-GAN when provided with both positive and negative examples of game levels.

SECTION: Domains

This work uses the Sturgeon(Cooper2022b)constraint-based level generator to create a corpus of Super Mario Bros(Nintendo1985)level segments. These segments arein size and are based on the level 1-1 from the VGLC(Summerville et al.2016). We have also created a corpus of a custom game calledCavewhich is asimple top-down cave map and it was first introduced byCooper (2022a)using tiles from Kenney(Kenney2022).

In this work, we used two of Sturgeon’s constraint-based features to generate our corpus. First is the ability to add constraints on the number of specific tiles. Second is (in addition to generating playable levels) the ability to create unplayable levels using an unreachability constraint(Cooper and Bazzaz2024)that are similar to playable levels in local tile patterns, but are not possible to play.

We createdeach of playable and unplayable Mario segments with exactly,, andpipes in them, resulting inplayable segments andunplayable segments of Mario. We also createdeach of playable and unplayable Cave segments with exactly,, andtreasures in them, resulting in alsoplayable segments andunplayable segments of Cave.

Table1shows different tile types and corresponding symbols in each game. The “start” and “end” tiles are specifically kept in each level as the minimum requirement for level playability. We use one-hot encoding of these level segments during GAN training.

SECTION: System Overview

All three controllable GANs adopt the Deep Convolutional GAN architecture as used byVolz et al. (2018), which is based on the original work ofArjovsky, Chintala, and Bottou (2017). This architecture employs batch normalization in both the generator and discriminator after each layer, ReLU activation functions for all layers in the generator (instead of Tanh), and LeakyReLU activation in all layers of the discriminator.

These models are trained using the WGAN algorithm. Wasserstein Generative Adversarial Networks offer an alternative to traditional GAN training, providing more stable training as demonstrated byArjovsky, Chintala, and Bottou (2017). Both the generator and discriminator are trained with RMSprop, using a batch size of 32 and a default learning rate of 0.00005 foriterations.

The following sections detail each controllable model, describing the classes of data used for training and the objective function of the training process.

SECTION: Vanilla Generative Adversarial Nets

The vanilla GANs as introduced by(Goodfellow et al.2014), consist of two models: a generative model (G) that captures the data distribution, and a discriminative model (D) that estimates the probability of a sample being real (from the training data) or fake (from the generative model). The entire training process is framed as a minimax game, where the discriminator tries to maximize the objective function, as described in Equation1(Goodfellow et al.2014), while the generator tries to minimize it. Here,represents samples from the real data distribution (positive examples), andrepresents samples from the generator’s distribution.

In this model, controllability is achieved by training a separate model for each set of desired constraints. For instance, with the goal of generating Mario levels with a specific number of pipes, a vanilla GAN is trained specifically to generate Mario level segments containing only one pipe. This means that the model is trained exclusively on level segments that feature only a single pipe.

SECTION: Conditional Generative Adversarial Networks

CGANs(Mirza and Osindero2014)apply extra informationto both the discriminator and generator. As shown in Equation2(Mirza and Osindero2014)The loss function of a CGAN is an extension of the vanilla GAN loss function, incorporating this conditional information. This again results in in a minimax game in which the generator tries to generate realistic data conditioned on, while the discriminator tries to distinguish between real and generated data, also conditioned on. Hereincludes both positive and negative samples, distinguished by label.

Since positive and negative examples vary depending on the training target, a specific CGAN must be trained for each desired condition in each experiment. The only difference between these models is the label of the examples, which changes based on the model’s objective.

SECTION: Rumi Generative Adversarial Nets

Rumi-GAN(Asokan and Seelamantula2020)is a specialized type of GAN inspired by the Sufi poet Rumi’s philosophy of learning from both positive and negative experiences. Equation3(Asokan and Seelamantula2020)shows how in this approach data distributionis split into the target distribution that the GAN is required to learn (positive samples,) and the distribution of samples that it must avoid (negative samples,). The fake distribution which are the samples drawn from the generatoris there as before.andis a weighting factor for the positive and negative real data distribution term. We settoandto.

Again, since positive and negative examples vary depending on the training target, we need to train a specific Rumi-GAN for each desired constraint.

SECTION: Experiments

We conducted two different experiments to examine two sets of constraints we would like to see in the generated levels: playability and controllability. A playable level is a level such that there exists a path between the level’s start and end locations. Controllability is derived from the correctness of the number of controlled features (eg. pipes or treasures) at each level. A level is considered correct if it has the desired number of features. Figure1shows the Venn diagram of the sample space. The sample space is divided intoplayable-correct,playable-incorrect,unplayable-correct, andunplayable-incorrectsubspaces. Positive and negative samples are chosen from these subspaces according to the model’s objective. In the experiments, negative examples are added in addition to the positive data that models utilize. To ensure an unbiased distribution of training data for models that incorporate negative examples, an equal number of positive and negative examples are sampled for training. Table2the models trained in each experiment and the exact input data of each model.

SECTION: Experiment One: Playability

In this experiment, the goal is to ensure playability as the primary constraint for the models to meet. This means that we take the positive samples() from theplayablesubspace and negative samples () from theunplayablesubspace. Both Conditional GAN and Rumi-GAN use this negative distribution as additional information.

SECTION: Experiment Two: Playability and Controllability

This experiment’s goal is to enforce both the number of some features in the level (number of pipes in Mario and number of treasures in Cave) and playability as a constraint for the models to satisfy. This approach gets positive samples () from theplayable-correctsubspace and negative samples() from theplayable-incorrect,unplayable-correct, andunplayable-incorrectsubspaces. This means Rumi-GAN, and Conditional GAN get the playable samples with the desired condition as the positive examples to learn, and playable samples without the condition (other playable classes) and all unplayable classes as negative examples.

SECTION: Evaluation

After the training models in each experiment, we generatedlevels with each trained model and we evaluated each model based on the criteria of that experiment.

SECTION: Experiment One: Playability

To evaluate Experiment One, we measure the percentage of playable levels as the metric. We use Sturgeon to find (if available) the shortest path between the start and goal of the level segments. Generated level segments that don’t have a start or end, or have multiple starts and ends count as unplayable levels immediately. Table3shows the results of this experiment. In Mario both models using negative examples show better performance than vanilla GAN. In Cave, only Rumi-GAN takes advantage of the negative examples resulting in better performance compared to the other models.

SECTION: Experiment Two: Playability and Controllability

To evaluate Experiment Two, we measure the percentage of levels that have the correct number of pipes/treasures while being playable. This means unplayable levels with the correct number of pipes/treasures, or playable levels with an incorrect number of pipes/treasures count as failures in this experiment as they only achieved half of the objective. Note that for the purposes of evaluating the number of treasures in Cave, it is only the presence of the treasures that matters, not whether they are reachable. Tables4and5present the results of this experiment. Incorporating both the playability constraint and the number of pipes/treasures, as expected, makes generating correct outputs more challenging than in Experiment One. This increased difficulty leads to fewerplayablelevels being produced by each model compared to Experiment One. As a result, there are also fewerplayable-correctoutputs. Overall, while the inclusion of negative examples slightly improved performance in CGAN for Mario levels, it did not provide any benefit for Cave levels.

correctplayableplayable correct123Avg123Avg123AvgVanilla44.841.219.035.067.665.064.465.630.428.812.824.0Rumi41.241.00.627.665.271.256.864.425.431.00.018.8Conditional44.431.234.836.665.469.862.465.829.422.423.425.0

correctplayableplayable correct123Avg123Avg123AvgVanilla24.420.64.616.585.079.081.881.920.616.24.013.6Rumi18.424.61.814.983.684.082.283.216.221.41.813.1Conditional38.630.019.629.441.731.932.235.316.09.612.812.8

It’s important to note that the results of Experiment Two reinforce the findings from Experiment One, with the same models—CGAN and RUMI-GAN—showing superior performance in enforcing playability constraints. Specifically, CGAN emerged as the leading model in terms of constraint correctness. However, the results did not indicate a clear winning model for enforcing both playability and correctness constraints. We believe this suggests that when combining different constraints, the model must be able to distinguish between the negative examples of each constraint. Otherwise, as shown in Experiment Two, the model may not derive significant benefits from the combination.

SECTION: Discussion

The results of Experiment One demonstrate that incorporating negative examples in training GAN models can enhance their ability to generate more playable levels. In future work, we aim to reinforce this approach using levels annotated with players’ paths. The surprising decrease in the performance of GAN models using negative examples in Experiment Two, may suggest the importance of the quality of the negative examples. In future work, we would like to explore multi-stage training approaches with high-quality negative examples in fine-tuning steps. We believe this approach could be effective when combined with bootstrapping methods(Torrado et al.2020), or active learning methods with minimal training levels(Bazzaz and Cooper2023). These approaches could make the training easier, with the only price for the additional controllability being the number of models trained on the minimal data.

SECTION: Conclusion

This study explores the potential advantages of integrating negative examples into Generative Adversarial Networks (GANs) to enhance the generation of game levels. Inspired by the work ofAsokan and Seelamantula (2020)on Rumi-GAN, the primary focus lies in leveraging positive examples to guide GANs toward producing desired outputs and negative examples to avoid undesirable outputs. Through comparative analyses involving Conditional GANs (CGANs), Rumi-GANs, and a baseline vanilla GAN with and without negative examples, it was observed that incorporating negative examples improves the capability of models to generate more playable outputs. However, this enhancement does not necessarily aid in enforcing constraints related to the controllability of specific features, such as the number of features in game levels.

SECTION: Acknowledgments

Support provided by Research Computing at Northeastern University (https://rc.northeastern.edu/).

SECTION: References