SECTION: ISMRNN: An Implicitly Segmented RNN Method with Mamba for Long-Term Time Series Forecasting

Long time series forecasting aims to utilize historical information to forecast future states over extended horizons. Traditional RNN-based series forecasting methods struggle to effectively address long-term dependencies and gradient issues in long time series problems. Recently, SegRNN has emerged as a leading RNN-based model tailored for long-term series forecasting, demonstrating state-of-the-art performance while maintaining a streamlined architecture through innovative segmentation and parallel decoding techniques. Nevertheless, SegRNN has several limitations: its fixed segmentation disrupts data continuity and fails to effectively leverage information across different segments, the segmentation strategy employed by SegRNN does not fundamentally address the issue of information loss within the recurrent structure. To address these issues, we propose the ISMRNN method with three key enhancements: we introduce an implicit segmentation structure to decompose the time series and map it to segmented hidden states, resulting in denser information exchange during the segmentation phase. Additionally, we incorporate residual structures in the encoding layer to mitigate information loss within the recurrent structure. To extract information more effectively, we further integrate the Mamba architecture to enhance time series information extraction. Experiments on several real-world long time series forecasting datasets demonstrate that our model surpasses the performance of current state-of-the-art models.

KeywordsLong Time Series AnalysisRNNMambaImplicit Segment

SECTION: 1Introduction

Long-term series forecasting is the process of predicting the future values of a sequence over an extended period. This task primarily aims to forecast long-term trends, which hold significant importance in domains such as climate prediction(Aghelpour et al.,2019), decision support(Nooruldeen et al.,2022), and policy formulation(Zhang et al.,2022). Long-term sequence data often exhibit higher levels of uncertainty, which results in reduced prediction accuracy. Additionally, longer forecasting horizons require models to incorporate a broader historical context for accurate predictions, thereby increasing the complexity of modeling. Recent advancesments in long-term sequence analysis have shifted towards deep learning methods, highlighting the crucial need to develop diverse and effective time-series methodologies.

Long-term time series analysis involves identifying trends to understand past and predict future changes, managing uncertainty and noise to enhance prediction accuracy, and capturing the sequential relationships within the data. Commonly adopted feature representation methods in time series analysis include Convolutional Neural Networks(CNNs)(Wang et al.,2022; Wu et al.,2023; Foumani et al.,2021), Multilayer Perceptrons(MLPs)(Chen et al.,2023; Zeng et al.,2023; Das et al.,2023; Li et al.,2023), attention-based Transformer models(Nie et al.,2023; Zhang and Yan,2022; Zhou et al.,2021,2022; Wu et al.,2021; Liu et al.,2021), and Recurrent Neural Networks(RNNs) based on sequence structure(Lin et al.,2023; Salinas et al.,2020). CNN-based methods use convolutional kernels to capture temporal and spatial sequences(Foumani et al.,2021); however, the limited size of these kernels prevent the model from effectively capturing long-range dependencies in time series, thus limiting the expressiveness of the feature representation. Transformer-based methods, leveraging attention mechanisms to improve the capture of spatiotemporal sequences, thereby enhancing feature representation capabilities. MLPs have been exrensively applied in time series prediction, with models such as DLinear(Zeng et al.,2023), which use simple linear layers and channel-independent methods, achieving superior performance compared to the advanced Transformer architectures of its time, prompting a reevaluation of the need for complex Transformer structures in time series prediction.

RNN-based methods have achieved remarkable success in long-term time series analysis tasks. The sequential structure of RNNs allows them to effectively handle short-term time series prediction tasks. However, as the length of the prediction sequence increases, traditional RNN structures gradually become ineffective due to their inability to handle long-term dependencies and gradient issues(Yunpeng et al.,2017). To address this limitation, Shengsheng Lin et al.(Lin et al.,2023)proposed the SegRNN model. By segmenting the time series and employing a parallel decoding strategy, SegRNN minimizes the number of RNN recurrent iterations while retaining sequential information to the greatest extent. This approach has led to significant success in both the accuracy and computational efficiency of long-term time series forecasting tasks. Nevertheless, SegRNN has several limitations: The segmentation process disrupts the continuity of the sequences, and information across different segments is not fully utilized. Additionally, the segmentation strategy effectively balances sequence information with iteration count; however, information loss within the recurrent structure increases as the prediction horizon extends, thereby limiting the model’s capability.

In this work, we propose a novel model named ISMRNN to address the issues associated with SegRNN. Specifically, ISMRNN introduces an implicit segmentation structure that decomposes and maps the time series into encoded vectors through two linear transformations. This method facilitates more continuous processing during segmentation and enhances the utilization of information between different segments. Additionally, ISMRNN incorporates a residual structure with a linear layer, allowing some information to bypass the recurrent encoding structure, thus reducing information loss within the recurrent framework. Furthermore, we employ the Mamba structure(Gu and Dao,2023)for preprocessing the time series, which aids in capturing long-term dependencies more effectively. The main contributions of ISMRNN can be summarized as follows:

Utilizing implicit segmentation for denser information exchange during the segmentation phase.

Incorporating the Mamba structure to improve information preprocessing.

The residual structure reduces information loss within the recurrent structure.

SECTION: 2Problem Statement

The analysis of long-term time series involves addressing the challenge of predicting future values of multivariate time series based on historical data. Typically, the problem of time series analysis can be formulated as follows:

in whichrepresents multivariable time sequences with historical context,represents the trained neural network structure,represents the future time sequences to be predicted,indicates the historical lookback window,indicates the number of variables, andindicates the time step to be predicted. Long-term time series analysis aims to expand the potential of predicting time steps, which poses significant challenges for training neural networks.

Long-term time series analysis and forecasting comprise two critical components: (1) Trend identification and analysis, which entail the recognition of long-term directional changes within the data. This component is crucial for understanding the behavior of time series, enabling the comprehension of past patterns and the prediction of potential future developments. (2) Uncertainty and noise management: Due to the extended time range of long-term time series data, they often exhibit increased levels of uncertainty and noise. Effectively managing and mitigating these uncertainties and noises is vital for improving the accuracy of predictions.

A significant amount of work has been dedicated to advancing the development of deep learning in time series analysis. These methods can be broadly divided into the following categories:

SECTION: 2.1Transformer Models

The core of applying Transformer models to long-term time series tasks lies in their ability to capture long-term dependencies using the attention mechanism(Hao and Cao,2020). The general form of the self-attention mechanism(Vaswani et al.,2017)is given by:

Significant research efforts have advanced the application of Transformer architectures in long-term time series analysis. For instance, PatchTST(Nie et al.,2023)introduces a method of segmenting time series into patches, reducing computational complexity and improving the model’s ability to capture local semantic information. It also employs self-supervised learning, masking random patches and training the model to reconstruct them, enhancing forecasting accuracy. iTransformer(Liu et al.,2024)addresses scalability by integrating hierarchical attention mechanisms and temporal convolutional networks, capturing both short-term and long-term dependencies more effectively. This lightweight architecture balances accuracy and efficiency, making it suitable for real-world applications. Crossformer(Zhang and Yan,2022)employs a Dimension-segment-wise structure to embed time series into a two-dimensional vector, effectively capturing dependencies across time and dimensions through its Two-stage-attention structure. The segmentation technique utilized in Crossformer is particularly advantageous for long-term time series forecasting.

SECTION: 2.2MLP Models

MLP methods have garnered significant attention in long-term time series forecasting due to their robust performance. Dlinear(Zeng et al.,2023)decomposes time series through the application of moving average kernels and remainder components, yielding two feature variables. These variables are then mapped to the prediction results via linear layers. The Dlinear model can be described as:

whererepresents the trend component obtained by the moving average kernel,represents the remainder component, and,are the weights of the linear layers.

TiDE(Das et al.,2023)introduces a residual structure within the linear layer to encode past time series and covariates, subsequently decoding the encoded time series with future covariates to obtain prediction results. The core operation of TiDE involves projecting features and encoding them through residual blocks, which can be described as:

whereanddenote the weights and biases of the projection layer, respectively. TheResBlockoperation incorporates residual connections to preserve the integrity of the encoded features while integrating the covariates.

MLP-based methods for long-term time series analysis have demonstrated outstanding predictive accuracy while maintaining low time-space complexity, thus significantly promoting progress in the field of long-term time series forecasting.

SECTION: 2.3CNN Models

The core of time series analysis based on CNN architectures lies in utilizing convolutional kernels to extract temporal and channel features. MICN(Wang et al.,2022)aims to fully exploit the latent information in time series by designing a multi-scale hybrid decomposition module to decompose the complex patterns of input time series. Each module within this design leverages subsampling convolution and equidistant convolution to respectively extract local characteristics and global correlations. TimesNet(Wu et al.,2023)transforms one-dimensional time series into a set of two-dimensional tensors based on multiple periodicities. It further introduces the TimesBlock to adaptively uncover these multi-periodic patterns and effectively extracts intricate temporal dynamics from the transformed tensors through parameter-efficient initializations. These methodologies have attained remarkable results in the domain of long-term time series forecasting.

SECTION: 2.4RNN Models

The RNN structure is specifically designed for sequence tasks due to its ability to manage temporal dependencies through the recursive updating of hidden states. This mechanism enables the RNN to retain prior information and dynamically adjust internal states to capture short-term dependencies effectively. However, due to gradient issues, RNNs face challenges in capturing long-term dependencies. As the only RNN-based model specifically designed for long-term time series tasks, SegRNN has demonstrated exceptional performance. SegRNN addresses the challenge of retaining sequence information within the recurrent structure while minimizing information loss through two key methods:

SegRNN replaces point-wise iterations with segment-wise iterations. This approach balances the trade-off between retaining sequence information and the number of recurrent iterations, leading to improved performance.

SegRNN substitutes recurrent multi-step forecasting with parallel multi-step forecasting. This parallel decoding approach enhances the model’s predictive capability and inference speed.

By combining these two strategies, SegRNN achieves outstanding performance within a lightweight model structure and exhibits superior inference speed. However, the fixed segmentation approach of SegRNN limits the exchange of information between different segments and disrupts data continuity. Additionally, segmentation methods still face the challenge of information loss within the recurrent structure.

SECTION: 3Method

SECTION: 3.1Overview

In this paper, we propose the ISMRNN method to overcome these limitations through three strategies, as illustrated in Figure1. We first introduce implicit segmentation, which allows for dense information exchange during the segmentation stage. Subsequently, we incorporate a residual structure to enable partial information bypassing the recurrent structure, thereby reducing information loss in recurrent structure. We also integrate the Mamba architecture to selectively process time series information, further enhancing the model’s capability in handling temporal data.

SECTION: 3.2Implicit Segmentation

The segmentation method employed by SegRNN is implemented by truncating the time dimension. For a given time series, selecting a segmentation lengthresults in segmented vectors of size, whererepresents the number of segments.

Subsequently, we use linear mapping to transform the dimensioninto, obtaining the embedded hidden state of the segmented vectors. This segmentation method retains the sequence information while reducing the iteration count within the recurrent structure. However, this approach has some drawbacks. The simple truncation of data disrupts continuity, and there is no effective exchange of information between different segments, thus limiting the model’s capability.

To address these issues, we propose an implicit segmentation strategy, as illustrated in Figure2. This strategy initially decomposes the time series intothrough linear mapping and subsequently transforms the information into the embedded hidden state. Notably, after segmentation, each segment includes both the information of that segment and some additional information, ensuring more continuous information processing and thereby enhancing the model’s performance.

Furthermore, our proposed method does not explicitly define the segmentation length. Instead, it achieves data segmentation and dimensionality transformation through dual linear projections. The incorporation of linear fully connected layers facilitates a more continuous and dense transformation of information. It is importanct to emphasize that this implicit segmentation strategy is not confined to a singe model but rather represents a general and straightforward technique for information enhancement.

SECTION: 3.3Residual Structure

Extensive experiments have demonstrated that increasing the number of iterations within the recurrent structure can negatively impact the model’s information retention. The use of segmentation techniques can reduce the number of iterations in the recurrent structure fromto.
Specifically, for the vector obtained through segmentation in SegRNN,, we transform it in the RNN structure with a hidden layer dimensionusing the following formula:

whererepresents the hidden state at step,represents the input vector at step,denotes the RNN’s weight parameters, andis the RNN’s activation function.

However, relying solely on segmentation does not fundamentally address the issue of information loss within the recurrent structure, which may become more pronounced as the prediction length increases. To address this issue, we incorporate a residual structure into the model. For the vectorobtained through the decomposition by the first linear layer , we apply a linear mapping to, resulting in. Finally, we addto the corresponding position of the RNN encoder’s output vector to obtain the final encoder output vector.

Introducing the residual structure allows some information to skip the recurrent structure and be directly mapped to the encoder output. This approach minimizes information loss within the recurrent structure, providing a beneficial enhancement for time series data.

SECTION: 3.4Mamba structure

The Mamba architecture excels in various fields, including large language models and long-term time series forecasting, due to its selective state space models (SSMs) that enable content-based parameter adjustment. This feature allows the architecture to dynamically select and process relevant information, making it highly effective for distinguishing signals from noise, particularly in time series forecasting.

We incorporate the Mamba architecture at the beginning of our workflow to preprocess the time series data. Specifically, we use the SSM component, which adjusts parameters based on the input signal. This setup enhances the model’s ability to selectively propagate and forget information along the sequence, focusing on relevant data points to improve forecasting accuracy.

Notably, we deviate from the standard Mamba architecture by excluding the convolutional layer, except for weather datasets. This choice is based on performance considerations, as we believe segmentation techniques effectively capture local information without the added computational load of convolutional processing.

SECTION: 4Experiments

Our experiments are structured as follows. We first introduce the datasets used for long-term time series analysis. Subsequently, we provide a detailed description of the experimental setup and baseline models for comparison. Then we presents and analyzes the performance metrics. We further conducted ablation studies to investigate the effectiveness of the model. Finally, we evaluated the model’s efficiency to demonstrate its high spatiotemporal efficiency.
All experiments in this section were conducted on two NVIDIA T4 GPU.

SECTION: 4.1Datasets

The evaluation is conducted across six real-world datasets spanning various domains, including power transformer temperature, weather, electricity load, with channel ranging from 7 to 321 and frequencies from every 10 minutes to hourly recordings. Detailed dataset information is provided in Table1.

SECTION: 4.2Experimental setup and Baselines

The unified configuration of the model is substantially aligned with the SegRNN approach. The look-back window is set to be 96, a single layer of Mamba is utilized for preprocessing the data and a single GRU layer is used for sequence processing. The dimensionality of hidden layer with GPU structure is set to be 512, and the training epochs are set to be 30. Dropout rate, learning rate, and batch size vary with the data and the scale of the model.

As baselines, we have selected state-of-the-art and representative models in the long-term time series forecasting domain, comprising the following categories: (i) RNNs: SegRNN; (ii) MLPs: Dlinear; (iii) CNNs: TimesNet; (iv) Transformers: iTransformer, Crossformer, PatchTST.

SECTION: 4.3Main Result

We select two evaluation metrics, MSE and MAE, with prediction steps of 96, 192, 336, and 720 time steps. The resulting forecasts are presented in Table2. Note that all data, except for our method, originate from
official code repositories and original papers.

Our method achieved top positions in 36 out of 48 metrics, while the SegRNN method and the iTransformer, each achieved top position in 7 and 6 metrics, respectively. This demonstrates the powerful capabilities of our model.

We subsequently conducted experiments to observe how the model’s capability varies with changes in the look-back window. The hyperparameter settings of the method were kept consistent with look-back window 96, and the horizon was uniformly set to 336. The evaluation was performed on the ETTh2 dataset, with the MSE and MAE losses of the method shown in the Figure3:

Our model exhibited smaller losses across various look-back windows, demonstrating the superiority of our method. Notably, the advantage of our method is more pronounced with smaller look-back windows. As the look-back window increases, this advantage diminishes, possibly due to noise introduced by segmentation affecting the model. Nevertheless, our model maintained a considerable advantage.

SECTION: 4.4Ablation study

We further conduct an ablation study to verify the effectiveness of incorporating the Mamba structure, implicit segmentation, and residual structures into our model. We performed experiments excluding the Mamba structure and implicit segmentation structure respectivly. For clarity, in the table below, "M" denotes preprocessing using the Mamba structure, and "LR" represents the addition of implicit segmentation and residual structure. The results are shown in Table3.

The ablation study reveals that the model incorporating the Mamba structure, along with implicit segmentation and
residual structures performs the best overall. This is followed by the model that includes only implicit segmentation and
residual structures. Interestingly, merely adding the Mamba structure does not significantly enhance model performance.
This may be due to a synergistic effect between the two structures. Consequently, the ablation study confirms the
superiority of our proposed structure.

SECTION: 4.5Model efficiency

This section is dedicated to evaluating the model’s time and memory efficiency to substantiate its lightweight nature. The experiments are conducted on weather datasets because the other datasets utilize the Mamba architecture are implemented without convolutional layers and hardware optimization. The lightweight characteristic is primarily due to the use of a small number of parameters in our Mamba structure experiments. Using the Weather dataset as an example, the time and memory usage comparison with batch size 8 conducted on a single NVIDIA T4 GPU is shown in Figure4.

Compared to the SegRNN approach, our method introduces a slight increase in memory usage and training time, primarily due to the additional linear layers and the integration of the Mamba structure during the encoding phase. However, when compared to mainstream transformer-based models such as iTransformer and PatchTST, our model still demonstrates significant spatiotemporal advantages.

SECTION: 5Conclusion

In this study, we introduce the ISMRNN model, an implicitly segmented RNN method that integrates the Mamba structure, implicit segmentation, and residual structure. The Mamba structure aids the model in capturing long-term dependencies, addressing the gradient vanishing and long-term dependency issues inherent in RNNs. Implicit segmentation resolves the fixed segmentation problem of SegRNN by maintaining sequence continuity, while the residual structure in the model reduces information loss associated with the recurrent structure. These enhancements significantly improve performance in long-term time series forecasting while keeping memory usage and training time relatively low. Experiments on six real-world benchmark datasets validate our findings. Notably, the proposed implicit segmentation method is adaptable to various scenarios beyond the specific long-term time series forecasting problem. Future work will explore how these structures enhance model performance and identify the specific attributes of time series data where the ISMRNN method excels.

SECTION: References

SECTION: Appendix

SECTION: Appendix ADatasets

For our Long-term Time Series Forecasting (LTSF) study, we employed several widely-used multivariate datasets, including:

ETTs: This collection consists of electricity transformer data from two counties in China, spanning two years. The datasets include various subsets: ETTh1 and ETTh2 provide hourly data, and ETTm1 and ETTm2 offer 15-minute interval data. Each entry encompasses the target variable "oil temperature" alongside six power load indicators.

Electricity: This dataset contains hourly electricity consumption records for 321 clients from 2012 to 2014, offering insights into consumer energy usage patterns over time.

Weather: This dataset captures 21 meteorological variables, such as temperature and humidity, at 10-minute intervals throughout 2020. It serves as a comprehensive resource for weather prediction and climate studies.

These datasets are instrumental for benchmarking and advancing forecasting models in the context of long-term time series analysis.

SECTION: Appendix BCode Framework

The implementation of our model framework is primarily derived from the official SegRNN code and the implementation of Mamba. This includes data preprocessing and evaluation, model parameter settings and device selection, as well as result output. We sincerely thank the authors of these open-source frameworks and methods for their contributions to our experiments.

The official SegRNN source code is available at:
https://github.com/lss-1138/SegRNN.

The official minimal implementation of Mamba can be found at:https://github.com/johnma2006/mamba-minimal.

The official implementation of Mamba can be found at:https://github.com/state-spaces/mamba.git

SECTION: Appendix CDetail Configuration

The main hyperparameter settings of our model are shown in Table4. The parameteris a hyperparameter in the Mamba architecture, which is detailed in Mamba[Gu and Dao,2023]. The parameter settings and results for other methods are taken from their original papers or official code repositories.

We also employed a different learning rate adjustment strategy. The learning rate remained constant for the first 15 epochs and then gradually multiplied by 0.9. This approach actually increases the learning rate compared to the adjustment method used in SegRNN.

SECTION: Appendix DComparison with extra methods

We observed that some methods also utilized Mamba for information augmentation, we selected several long-term time series analysis models based on the Mamba architecture:
Time-Machine[Ahamed and Cheng,2024], Bi-Mamba4TS[Liang et al.,2024], and DT-Mamba[Wu et al.,2024], for comparison. The results are from official paper, respectivly . The result is shown in Table5.

It can be observed that our method performs notably well on the weather, ETTh2, and ETTm2 datasets, while the TimeMachine method shows significant advantages on the ETTh1 and Electricity datasets. Additionally, the Bi-Mamba4TS method also demonstrates a clear advantage on the Electricity dataset.

SECTION: Appendix EPrediction Result

We subsequently observed the partial prediction results of the model using the weather dataset, with a look-back window of 96 and a prediction horizon of 336. The results are shown in Figure5:

SECTION: Appendix FAblation study on using convolution layers

In the main text’s model structure, we removed the convolutional layer from the Mamba structure due to performance issues. We hypothesized that local features might have been redundantly captured. To investigate the impact of the convolutional layer on the model performence, we conducted experiments. The results are shown in Table6.

It can be observed that except for the Weather dataset in our experiments, the convolutional layer may interfere with the model’s predictions in most scenarios, thereby limiting the model’s capability.