SECTION: Dissecting the Interplay of Attention Paths in a Statistical Mechanics Theory of Transformers
Despite the remarkable empirical performance of transformers, their
theoretical understanding remains elusive. Here, we consider a deep
multi-head self-attention network, that is closely related to transformers
yet analytically tractable. We develop a statistical mechanics theory
of Bayesian learning in this model, deriving exact equations for the
network’s predictor statistics under the finite-width thermodynamic
limit, i.e.,,, whereis the network width andis the number of training examples.
Our theory shows that the predictor statistics are expressed as a
sum of independent kernels, each one pairing different, defined as information pathways through different attention
heads across layers. The kernels are weighted according to amechanism that aligns the total kernel with
the task labels. As a consequence, this interplay between attention
paths enhances generalization performance. Experiments confirm our
findings on both synthetic and real-world sequence classification
tasks. Finally, our theory explicitly relates the kernel combination
mechanism to properties of the learned weights, allowing for a qualitative
transfer of its insights to models trained via gradient descent. As
an illustration, we demonstrate an efficient size reduction of the network, by pruning those attention heads
that are deemed less relevant by our theory.

SECTION: Introduction
In recent years, transformer models based on multi-head self-attention
layershave achieved remarkable performance at natural language processing
and vision tasks. Yet, theoretical characterizations accounting for
the success of these architectures remain sparse.
Two fundamental
questions remain to a large extent unsolved: First, interpretability—how
can we discern task-relevant structures within the learned weights?
Second, generalization—what specific aspects of the transformer
architecture are responsible for their effective learning?
We posit that one important feature of transformers is the combination
of layer-wise multi-head organization with depth.
This provides the network
with a large number of, defined as specific sequences
of heads through the attention layers.
Theiris still
poorly understood by deep learning theory.

In most cases, theoretical characterizations of transformers’ expressivity, inductive bias, generalizationand training dynamicsrely on simplifying assumptions on the network architecture. A characterization of attention paths is inaccessible in these models,
either because attention paths are not defined in the first place, as in models consisting of a single-head, a single-layer, or both, or because the interplay between paths cannot be fully described due to constraints imposed on the learnable weights. A few works consider a multi-head, multi-layer architecture, but address different questions than the present study, such as expressivity, generalization bounds, or phenomenological models. Further details on these and analogous works are discussed in Appendix.

One characterization of the complete transformer architecture has been obtained in the Bayesian framework under theinfinite-widththermodynamic limit(and infinite number of heads), an actively studied regime in which neural networks become equivalent to Gaussian processes (GP).
However, the attention
paths interplay is lost in this limit because the network’s hidden weights
remain statistically independent after learning. This limitation can be overcome by considering
thefinite-widththermodynamic limit, where also the number of examplessuch that. In this
regime, for example, multi-gated deep networks showcase task-relevant
interplay between gates, mediated by the learned weights.

In this work, we apply the statistical mechanics theory of finite-width networks
to a deep multi-head self-attention model, which closely mimics the
attention paths interplay in transformers, while remaining analytically
tractable. Our main contributions can be summarized as follows:

We derive exact equations for the predictor statistics under Bayesian
learning of the network’s value weights, at fixed query
and key weights.

We shed light on the interplay between attention paths by uncovering
atask-relevant kernel combinationmechanism, emerging beyond
the GP limit (). This constructs the network’s mean predictor as
an optimally weighted sum of many “path-path kernels”, defined as similarity matrices between pairs of attention
paths, thereby improving generalization.

We provide interpretability to this mechanism, by directly relating
it to the magnitude and correlations developed by the learned weights.
This allows our insights to be transferred outside the Bayesian framework, to networks trained with gradient descent.
As an application, we show that a trained network
can be reduced in size with minimal performance loss, by pruning those attention paths
that are deemed less relevant by our theory.

We corroborate our findings on both synthetic and real-world sequence
classification tasks, illustrating the two main benefits of kernel
combination: task-relevant weighting and correlation of the attention
paths, respectively.

SECTION: Model
We consider a transformer-likearchitecture consisting of a linear input projection layer;multi-head self-attention (MHA) layers, each havingattention heads;
and a linear readout layer. The network inputis a sequence oftokens,
with token index, and dimension.
The input projection layer performs the transformation

whereis the hidden layers’ width. With the operator "" we denote matrix-matrix or matrix-vector multiplication. The-th MHA layer with indexperforms the transformation

where, for each head, we define the attention matrixwith matrix elements

Hereis the softmax function, applied along the direction
of the token index, whileis the dimension of the query-key feature space. The
linear readout returns the scalar output

Herecan stand for different options for reducing the token dimension
at readout, namely reading from a specific tokenor averaging over all tokens ().
The network’s learnable parameters are the input projection weights; the value, query and key weights;
and the readout weights.

The above architecture presents two main simplifications w.r.t. the standard transformer. First, the network is linear in the value
weights, while the standard transformer has a nonlinear feedforward block
after each MHA layer. Second, in any layer, the attention
(Eq.) is always computed as a direct function of the
bare input, rather than the processed input.
These simplifications allow us to apply back-propagating kernel renormalization (BPKR) techniques, enabling the characterization of the network beyond the GP limit.
Despite these simplifications, the insights gained by
going beyond the GP limit are substantial: we will show that, in the
finite-width regime, an important mechanism——emerges, accounting for a considerable improvement
in generalization performance.

Note that, despite the linearization in the value weights, the network
is still highly nonlinear in the input, thanks to the attention operation
(Eq.). This can be seen by the following
equivalent description of the network (Fig.(a)). We introduce the concept of, by defining a path “index”, where, which
uniquely identifies each possible combination of the head indices across layers,
i.e., each possible path through the attention heads. The network output
can be rewritten as

whereis the set of all possible paths, and we define the “” as

and the “” as

In Eq. (), the network can be seen as a
deep linear network applied to a nonlinearly expanded input—the
attentioned input. Through Eq. (), we
can see that the bare inputis nonlinearly expanded from
an-dimensional space to an-dimensional space, by means ofnonlinear operations: one for each attention path.

The goal of our theory is to understand how the network learns to
combine these different attention paths, by means of the effective
weights (Eq.).
Note that the network also has other learnable parameters:
the query and key weights, which parameterize the nonlinear expansion of the input to.
The learning of these parameters is not described by our theory.
As we will see in Sec., our theory characterizes the learned effective weights (Eq.) for a given, fixed realization of the query and key weights.

SECTION: Theory
A fundamental quest of deep learning theory is to understand how deep
neural networks, which are often overparameterized, manage to avoid
overfitting, achieving good generalization performance. One important
role is played by the specific choice of network architecture, which
can impose an inductive bias towards better generalizing configurations
of parameters, among the many that fit the training data. To study this problem, we adopt the Bayesian framework. Given a dataset ofexample-label pairs, we seek to characterize the Bayesian posterior, or,
over the parameters

Hereis the network output (Eq.) corresponding to the input, where we emphasize its dependence on,is the Frobenius norm,is the variance of the weights’ Gaussian prior
(set tothroughout this paper), andis the error variance, or(not to be confused with the number of tokens). Characterizing the Gibbs distribution allows to gain insights into the inductive bias imposed by the network architecture. Indeed, note that, in overparameterized networks, the Gibbs distribution fordescribes the statistics
of those parameter configurations that perfectly fit the training
data, with a bias towards small weights induced by the Gaussian prior. These statistics depend on the choice of network architecture, which can therefore bias the distribution towards better generalizing parameter configurations.
For, parameter configurations that do not achieve
perfect fitting are also allowed, which can help to prevent overfitting.

Note that, as discussed at the end of Sec., we characterize
the statistics of the weights(the linear projection, value, and readout weights) for a fixed realization of
the query and key weights. The fixed query and key weights can be given, for example, by pre-training the network with gradient descent, or by some task-informed initialization. In Sec.we will show that the insights gained by our theory on the weightscan also be applied to the network trained with gradient descent on all of its learnable parameters, including the query and key weights.

The main theoretical result of this work is an expression for the expectationof the network’s output on a new test example, under the Gibbs distribution (Eq.). In Sec.below, we provide this formal result, accompanied by a sketch of its derivation and a discussion of the significance of the infinite-dimensional—or thermodynamic— limit under which our result is derived.
In Sec.we discuss the result’s interpretation and its insights into the network’s generalization capabilities.

. Consider a training dataset consisting ofinputsand associated labels, where. Callthe set of training inputs andthe vector of training labels with-th component. Consider a network defined by Eqs. (-) and in particular callthe network output (Eq.) corresponding to a test input.

. Assume the query and key weightsare fixed, while all other weightsare distributed according to the Bayesian posterior distribution defined in Eq. (). Assume the “thermodynamic limit”, withand, where,as well as other size parametersare finite.

. The mean predictor under the posterior distribution (Eq.) is given by

The vectorand the matrix, called training kernel, are defined in terms of a kernel functionasand, for. The kernel function is given by

whereis the “attentioned input” corresponding to an input, along path, as defined in Eq. (). The kernel function depends on a positive semi-definite matrix, called, which is given by

The scalar function, called the, consists of an “” term, and an “” term

whereis the training kernel matrix. The expression for the entropyis lengthy and is given in Appendix. In the special case of,is a scalar, and. For general, the entropyis always maximized by, which therefore is the solution of Eq. () in the GP limit defined by.

. The matrixobeys the following relation

whereare the effective weights along path, defined in Eq. ().

See Appendix.

The derivation, which uses the BPKR technique, can be sketched as follows. Computingunder the posterior distributioninvolves evaluating a high-dimensional integral in the weights. The idea is to first reduce this computation into an integration over a lower-dimensional, ‘macroscopic’ variable. Importantly, whilebecomes infinite-dimensional as,remains finite-dimensional. The reduced integral is an expectation of the r.h.s. of Eq. (), treated as a function of, under the distribution, whereis the action defined in Eq. (). Then, this integral can be solved in the thermodynamic limit, using the saddle-point method, which implies evaluating Eq. () at thethat minimizes the action (cf. Eq.). Crucially, the end result is fully characterized by this low-dimensional quantity, commonly calledorder parameterin physics, which has a direct interpretation in terms of the network weights,
given by Eq. ().

In practice, the results obtained in the thermodynamic limit represent a good approximation also for the case of large but finite. In this regard, the scaling of other hyperparameters withis of particular importance, especially the number of training examples. In the GP limit, one considersfinite. This is also called thethermodynamic limit because in practice, for a given and typically large, it is a good approximation only for very wide networks, when. In contrast, here we consider thelimit in which(which includes the GP limit for). As can be seen from Eq. (), the action gains a new term for, which, as we shall discuss below, is fundamental to account for the learning of an attention paths interplay. Finally, we note that in our numerical experiments (Sec.) we will consider Bayesian networks which are overparameterized, i.e., which is the network capacity at fixed query and key weights.

Eq. () is a commonly found expression in thermodynamic theories of Bayesian learning, relating the network’s mean predictor to kernel regression. In particular, the theory of kernel regressionsuggests that generalization improves when the training kernelis well aligned with the task, meaning its largest principal components (PCs) are well aligned with the vector of training labels.

Our result for the transformer’s kernel (Eq.) enables insights into how the transformer architecture favors this kernel-task alignment (Fig.(d)). The kernel consists of the sum, weighted by the order parameter, of many, each computing the similarity between the attentioned
input on two attention pathsand. A notable property of the
multi-head architecture is that, despite the number of attention heads
growing only linearly with the depth, the number of attention
paths grows exponentially. Therefore, the network
has at its disposal an exponentially large number of path-path kernels,
which it can learn, through, to optimally combine into a total
kernel with improved task alignment.

This phenomenon, which we term, is indeed predicted by our results Eqs. (-). These state that the learnedminimizes a function(Eq.), which, through the energy term, favors kernel-task alignment. This can be seen by interpreting the energy term (Eq.) as the negative log-likelihood of the training labelsunder a centered Gaussian distribution, whose covariance matrix is the training kernel. This negative log-likelihood can be minimized by aligning the largest PCs of the covariance (i.e. the kernel) as much as possible with(Fig.(c)).

In contrast, in the GP limit, the action(Eq.) consists only of the entropy term, which does not contain any task relevant information.
Its only effect is to attracttowards the GP limit solution.
Note that, in this limit, the benefits of kernel combination  are lost (Fig.(d), bottom line):
First, out of all the path-path kernels, only the() are used, while the() are discarded; Second, all same-path kernels are
weighted equally, without making use of any task-specific information.
Note that this is true not only for our simplified model, but also
for the full transformer architecture under its known GP limit. A task-relevant kernel combination can therefore only emerge beyond the GP limit, in the finite-width regimestudied in this work.

Finally, our result Eq. () relates the order parameter to a macroscopic measure of the network weights, allowing for a direct interpretation of the
kernel combination mechanism: correlating the effective weights across paths allows the
network to make use of cross-path kernels, while controlling their
magnitude allows to weigh the different path-path kernels in a task-relevant
manner.

SECTION: Experiments
To corroborate our theoretical results, we “train” our model (Eqs.-) by sampling its weights(i.e. all weights except the fixed query and key weights) from the posterior distribution Eq. (), using Hamiltonian Monte Carlo sampling (see Appendixfor details). We consider the following two tasks:
hidden Markov chain (HMC) classification,
and one-shot image classification by in-context learning. The first
task is defined on a synthetic dataset. Its purpose is to have a
minimal, controllable setting to illustrate the effects of
task-relevant kernel combination.
In the second task, we will proceed to show analogous effects on classic image datasets (Omniglot, MNIST, and FashionMNIST), and compare these results with those obtained from the same network trained with standard gradient descent on all of its parameters (i.e. including the query and key weights).

The HMC classification task is defined as follows (Fig.(a)).
The-th example in the dataset corresponds to an hidden Markov chainof length, alternating between two hidden states,.
The probability of transition to the opposite state () is. The-th chain can belong to one of two classes, labeled, depending on whetheror, respectively.
The input tokens are a noisy, higher dimensional
representation of the hidden states. These are given by,
whereare two orthogonal feature vectors
corresponding to the states “”, withentries, whileis a zero-mean Gaussian noise, with,
whereandare the projectors along the
subspace parallel or perpendicular to the plane spanned byand. Unless specified,.
The separate parameterization of the parallel ()
and perpendicular () noise strengths is motivated
by their distinct effect on task performance: while the first corrupts
information about the underlying hidden states, inevitably putting
an upper bound on the classification accuracy, the second can always
be filtered out by learning appropriate weights.
We useexamples for training.
We test the network performance in terms of the classification accuracy,
where the sum is over a numberof test examples. Additional task details are given in Appendix.

We consider a network oflayers andheads per layer,
with readout from the first token. The network has a total
ofattention paths, schematically depicted in Fig.(b).
For this synthetic task, we design the fixed query and key weights, and therefore the network’s attention paths, to clearly
illustrate the effects of task-relevant kernel combination (for details, see Appendix).

We design the first head of each layer to give rise to a “good” attention path (green path) such that a network consisting of
this good path alone achieves a high classification accuracy,. Along this path, the first head makes use of the Markov nature of the task by attending exclusively to nearby tokens, and only if they correspond to the same hidden state; the second head performs uniform attention, effectively counting how many times the first head detected the same-state transition. In contrast, each layer’s second head is initialized randomly. This results in the three remaining paths having chance-level
classification accuracy, when considered in isolation.
However, these paths have very different effects, when combined with
the good path. We term two of these paths “adversarial” (red and
purple paths) because they deteriorate the network performance, while
we term the remaining path “denoising” (blue path) because it
can be effectively combined with the good path to improve robustness
to noisy data.

In Fig.(c, top) we show the network’s classification accuracy
as a function of the width(blue, solid curve), compared to the
GP limit (red, solid line). At lower, well into
the finite-width regime, we observe a considerable improvement in performance with respect to the GP limit. This can be understood in terms of an improved kernel-task alignment, as shown in Fig.(d).

This improved alignment is ensured by the order parameter, plotted
in Fig.(c, bottom) for varying. For, well into the
finite-width regime, the order parameter clearly implements the two
main benefits of kernel combination: the possibility to weigh the
path-path kernels differently, and the ability to make use of the
cross-path kernels. The first benefit is particularly apparent in
the suppression of all kernels associated with the adversarial paths.
In contrast, whenand the order parameter is very close
to its GP limit, these paths
are not suppressed, causing a deterioration in performance compared
to that of the good path alone (red, dashed line in Fig.(c, top)).
The second benefit is apparent in the strong off-diagonals of,
anti-correlating the good and denoising paths. We can see that, while
also in the GP limit the denoising and good paths combined (dotted,
red line in Fig.(c, top)) have a better performance than the
good path alone (dashed, red line), the performance boost is even
higher in the renormalized regime, which makes use of the cross-path
kernels. This additional improvement in performance becomes more apparent
with noisier data. This is shown in Fig.(e), where we
plot the classification accuracy of the network consisting of only
the good and denoising paths, on data with stronger
perpendicular noise.

The one-shot image classification task (Fig.(a)) is formulated in an in-context learning setting. The network is presented with a sequence of three image-label
pairs. The first two images belong to two distinct classes of a categorized
dataset (Omniglot, FashionMNIST or MNIST in our case). They are assigned
the label “” or “” in no particular order. The third
image is assigned the label “”, and belongs to one of the
classes of the first two images. The network has to outputaccording to the label of the matching image. The sequence is fed
to the network as follows. Following the idea of the vision transformer
(ViT), each image is divided intopatches. The patchof imagecorresponds to the token, for
a total oftokens.
We encode the labels,,using three fixed random vectors,
which we directly add to each patch (i.e., token) of the corresponding image.
We also encode the token position with
additive sinusoidal positional encoding. The network is trained on the Omniglot dataset, while we test its classification accuracy on both in-distribution (ID) unseen classes of Omniglot, and out-of-distribution (OOD) FashionMNIST dataset (we also report results on MNIST in Appendix).

We consider a network ofattention layers andheads
per layer, with average pooling readout, trained on a subset ofexamples from Omniglot (analogous results for a deeper network with,are also reported in Appendix).
For the fixed query and key weights required by our Bayesian network, we use the query and key weights obtained from training the same network using gradient descent, with,,
and(i.e., the entire training set from Omniglot).
We refer to Appendixfor further details on this process.

The plots shown in Fig.are analogous to those for the
HMC task (Fig.), and illustrate analogous kernel combination
phenomena. Fig.(b) shows the classification
accuracy for varying. Again, we observe a performance gap between
the finite-width and GP regimes. Interestingly, this improvement in
performance is preserved also OOD, on FashionMNIST. Again, Fig.(d) shows that the performance gap can be understood in terms of an improved
kernel-task alignment: PCs that are well aligned withare of
higher rank, and have a larger overlap than in the GP limit.

The order parameter (Fig.(c), “theory” and “sampled”)
foris clearly far from its GP limit, accounting for the improvement
in performance observed in the finite-width regime. We observe similar
kernel combination phenomena as in the HMC task, with strong off-diagonal
elements, and a stronger weighting of certain paths w.r.t. others.
Interestingly, the block diagonal structure of the order parameter allows
for a simple interpretation of the interplay between paths: correlations mostly occur between paths sharing the same headin the first layer, which also determines which paths are overall enhanced () or suppressed ().

This structure of the order parameter transfers qualitatively well
also to the network trained with gradient descent. In Fig.(c,
“gradient descent”) we show an empirical order parameter, obtained
by computing Eq.using a single realization
of the network’s weights trained with gradient descent. Both the order
parameter’s block structure and matrix element signs are qualitatively
preserved in this empirical estimate. We emphasize that the network
is trained with the full set of training examples () rather
than the restricted one used for the Bayesian network (), and
on all learnable parameters including the query and key weights, making
this qualitative agreement more relevant to potential applications.
One example application is provided below.

Our theory allows us to prune certain heads in the
model trained with gradient descent (leading to a model size and compute reduction), with marginal performance loss.
This is achieved by using the order parameter to assign a score to
each attention head, according to its contribution to kernel combination.
The lowest-scoring heads are then pruned from the model. The headat layeris assigned the score,
whereis the set of all paths passing
through that head, andis the order parameter derived from theory. Fig.(e) shows the score of each head, normalized
by the largest one, compared against the drop in classification accuracy
caused by pruning that head. Note that the network is not retrained after
pruning, but only reevaluated on the test examples. We observe a performance
drop qualitatively in line with the head scores. Most importantly,
the two lowest scoring heads only cause a marginal drop in performance.
In Fig.(f) we report the classification accuracy after
pruning an increasing number of heads, in order of their score. Up
until the first two heads (amounting toof the total number of network parameters), the in-distribution classification accuracy
is only marginally worsen. Interestingly, the OOD classification accuracy
is even improved, possibly indicating an overspecialization of the
pruned heads in solving only the in-distribution task.
In Appendixwe achieve an analogous size reduction ofon a larger model withheads.

. Finally, we note that both figures(c,e) and(b,c) show good agreement of our
theory with the mean predictor and order parameter sampled from
Eq.. While our theory
becomes exact in thelimit, the agreement holds even for small. In particular, in figures(b,c), it holds even if, the number of independent entries in the order parameter, which is supposed to be a finite quantity in our theory.

SECTION: Conclusion and Discussion
We introduce a transformer-like model featuring deep multi-head self-attention, amenable to theoretical characterization within the Bayesian framework. Our results unveil the important role of attention paths in accounting for transformers’ remarkable performance. We demonstrate that, in scenarios involving the interplay of attention paths at finite widths, generalization consistently improves compared to the GP regime, where such interplay is absent. Our theory explains this paths interplay in terms of a task-relevant kernel combination mechanism, where the network’s total kernel results from the sum of many kernels, specific to pairs of paths, and optimally weighted to improve generalization. This mechanism is confirmed by experiments on both synthetic and real-world sequence classification tasks.
More broadly, our results are relevant to the theory of deep learning, as they provide an example of non-scalar kernel renormalizationin a widely adopted architecture such as the transformer, illustrating its importance in accounting for network performance. Non-scalar, as opposed to scalar, renormalization can affect the network’s mean predictor and therefore lead to improved generalization. Our work provides a novel interpretation of its benefits in terms of an optimized kernel-task alignment.

We provide interpretability to the kernel combination mechanism, by relating it to observable structures in the network weights, specifically to their magnitude and correlations. These predicted structures transfer well outside the Bayesian framework, to the weights of networks trained with gradient descent, broadening the applicability of our theory. As an example, we show that a trained network can be reduced in size with minimal performance loss, by pruning those heads that are deemed less relevant by our theory. The large size reduction achieved () appears in line with observations that a few specialized heads are responsible for most of the network performance, and the proposal of head pruning schemes during training. Our theoretical insights may therefore be relevant to the quest for minimalistic and resource-efficient models.

The above results are enabled by our theory’s ability to go beyond the GP limit, as well as incorporating a multi-head, multi-layer architecture—a prerequisite for the very existence of attention paths. However, various limitations could still be addressed, opening for exciting research directions. For example, attention in our model is only a function of the bare input, rather than the previous layer’s postactivation, as in standard transformers. In this case, the theoretical challenge would be to disentangle the learning of attention paths interplay from the learning of the attention paths themselves, since now also the attention matrix would depend on the value weights. Another limitation of our model is its linearity in the value weights. It may be possible to heuristically extend the theory to include nonlinear MLP blocks in between attention layers, by replacing the GP path-path kernels appearing in Eq. () with the corresponding GP kernels for the nonlinear case—an approach which has proven successful in deep ReLU networks for certain regimes. Introducing nonlinearities, strong feature learning may also emerge.
Note that, instead, our theory is readily extendable to the case ofMLP blocks, as well as multiple outputs, following. Here we chose a minimal setting focusing only on those renormalization phenomena specific to the transformer architecture. Indeed, the presence of multiple outputs causes the same kind of renormalization independently of the network architecture (i.e., adding two new output indices to the order parameter), while deeper linear blocks would not alter the essence of attention paths interplay, only affecting details in the entropy part of the action. Extending the theory to include skip connections also seems viable. A very open challenge, instead, is to characterize the learning of the query and key weights, which relates to the more general challenge of extending the BPKR technique to nonlinear deep networks. Finally, our approach characterizes the inductive bias imposed by the network architecture on the parameter configurations that fit the training data, but not the bias imposed by a learning algorithm. It would therefore be interesting to import our theory to methods characterizing deep neural networks’ training dynamics.

SECTION: Acknowledgements
We acknowledge support of the Swartz Foundation, the Kempner Institute for the Study of Natural and Artificial Intelligence at Harvard University, the Office of Naval Research (ONR) grant No. N0014-23-1-2051, and the Gatsby Charitable Foundation. This research was supported in part by grant NSF PHY-2309135 to the Kavli Institute for Theoretical Physics (KITP). FM was supported by the Simons Foundation (Award Number: 1141576). We have benefitted from helpful discussions with
Alexander van Meegen, Haozhe Shan and Qianyi Li.

SECTION: References
[appendices][appendices]l0

SECTION: Appendices
SECTION: Further discussion on related theory works
The theoretical properties of attention-based models have been investigated from various perspectives in recent years. Different lines of works have studied the expressivity, the inductive biasand the training dynamicsof attention layers. In particular,shows that multi-layer attention networks are universal approximators for certain classes of functions, such as equivariant sequence-to-sequence functions, whilehighlights their computational limitations, demonstrating that self-attention cannot model periodic finite-state languages, nor hierarchical structure, unless the number of layers or heads increases with input length.derive error bounds for non-linear models, with trainable queries and keys. As in our work,focus on the training of value matrices. Inquery and key matrices are fixed and untrainable, whilesets them equal to the identity. Several theoretical studies have focused on in-context learning. However, the works mentioned above derive generalization bounds and do not provide a tight characterization of the learning curves. A first tight analysis was done in, considering a single-layer model with factored self-attention. The authors ofprovide a tight analysis of a single-layer attention model with trainable tied queries and keys and value weights fixed to the identity. Previous theoretical works have mainly focused on a single transformer block, where attention paths are not defined. For instance,finds that one transformer block can learn different
linguistic tasks according to the position of the self-attention layer.shows that a transformer block can
learn to encode topical models.shows that one transformer block can encode patch associations. A different line of works has studied attention layers in the infinite-width limit. In particular,establishes an equivalence between Gaussian processes and infinitely-wide multi-layer attention models with infinitely many attention heads.leverages this framework and studies the inductive bias of infinitely-wide transformers towards permutation symmetric functions in sequence space.

SECTION: Theory
SECTION: Definitions
We recall the definitions for our transformer model and theory, Sec.andof the main text.

The hyperparameters are

The network weights are

where, and.

The network input is

where.

We append an additional indexto quantities that
refer to a specific example in the training set, such as, e.g.

We further define

The network performs the following sequence of input transformations (Fig.).
The input projection layer is defined as

The output from each attention layeris defined
recursively as:

where, for each head, we define the attention matrixwith matrix elements

The functiondenotes the softmax applied along the direction
of the token indexed by. The linear readout is defined as

where we adopt the unified notation

in order to consider different options of token readout, adopted in
different tasks.

We are interested in computing the posterior distribution over the
network’s weights.
This is analogous to the Gibbs distribution in statistical physics
and is defined as

whereis the Frobenius norm, andindicates the normalization, also called partition function.

In the main text, we introduce the concept ofattention paths,
by defining a path index

which uniquely identifies each possible combination of the head indices,
i.e. each possible path through the attention heads (Fig.). The network output
can be written as

whereis the set of all possible paths, and we define theeffective
weightsas

and theattentioned input

In order to present our theoretical results and their derivation,
it is useful to also introduce the following notation.

We indicate with

a collection of head indices, from layerup to layer.
For,is the path index defined above,
Eq., and in the main text. For,corresponds more generally to apartial path, starting from
layer. We call the set of such paths. For,is the collection of all complete paths defined
above and in the main text.

We define theattentioned input at layer, for,
as

were we remind the reader of the compact notationdefined
in Eq.. The attentioned inputs at subsequent
layers are related by

Recall that,
according to the notation defined in Eq..

We also give specific definitions for the casesand.
For, we define,
corresponding to the attentioned input defined above, Eq.,
and in the main text. The following relation holds

Forwe define

SECTION: Results enunciation
Here we enunciate our theoretical results, for which a derivation
is given in Appendixand Appendix.

We derive (Appendix) the statistics of the network predictionon a new test example. These are given by

wheredenotes the expectation
under the posterior distribution Eq., and. The
quantities,, andare defined in terms of a kernel
function.
Forwe define,,
and.

We derive the following form for the kernel

whereis the attentioned input corresponding to an input, while, calledorder parameter, is a path-by-path matrix
of size, wheredenotes the size of the set. The value of the order parameter
is determinedself-consistentlyby minimizing a scalar function, calledactionin the physics literature. This is defined as

where, for any matrix, we define

and

Note thatis simply the kernel defined above, through Eq., where we have explicitly written its dependence onand.
Here we have defined a collection of matrix order parametersfor, of size,
weredenotes the size of the
set. We called. The order
parameteris instead a scalar. We also definedforas
the matrix of sizewith elements

whereandare the partial-path indices defined in Eq.,
while.

The action must be minimized w.r.t. all of the order parameters. Note
that in the main text we definedas a function ofalone.
By this we meanevaluated at its minimum for a fixed, so
that only the minimization w.r.t.is left to perform.

The self-consistent solution for the order parameters is obtained numerically by minimizing
Eq.with gradient descent methods. Details on the procedure are given in Appendix.

We derive (Appendix) the following expression for the order parameter in terms
of the network weights

wheredenotes statistical averaging
over the posterior distribution Eq..

SECTION: Derivation of the predictor statistics
Here we provide the derivation of our result for the predictor statistics,
reported in Sec..

Let us recall the posterior distribution (Eq.)

whereindicates the normalization, also called partition function.

It is useful to introduce a vectorofauxiliary
variables in order to linearize the squared error in Eq..
The partition function then reads

whereis the imaginary unit,is the-th component of, whileis shorthand for, and
we use the symbol “” to indicate that we are neglecting
multiplicative constants. We also defined the Gaussian integration
measuresand.

Calling, we add asource termto the exponential
in the partition function, corresponding to the test example,
obtaining

In what follows we writeas, without
writing its argument explicitly. The partition function Eq.allows us to obtain the predictor statistics on the test example by
differentiation

where,
anddenotes statistical averaging
over the posterior distribution (Eq.). In what follows, we will neglect any multiplicative constants in the partition function, that are independent of, since they are irrelevant for the computation of Eq.and Eq..

We proceed to compute the partition function (Eq.)
by integrating all of the network weights, and finally the
auxiliary variable. We use the back-propagating kernel renormalization
(BPKR) method, which consists in the successive integration
of the weights, starting from the last layer down to the first layer.

Let us write Eq.as

where we define

Below we focus on computing. In what follows,
we will writeas, without writing its argument
explicitly.

We start by integrating over the readout weights. In Eq.,
we substitute, obtaining

In what follows, it is useful to defineand in general

We compute the Gaussian integral in the readout weights, obtaining

Withwe indicate the collection of weights,
i.e. all weights up to layer.

Plugginginto Eq.we have

We perform the integral over the value weights,
by noticing that it is given by the product ofidentical integrals
of the form

where we defined

withand. Hereindicates the-th element of the vector.
We may consideras anmatrix, whose two indices
run over the pairsand.
With this notation, the result of the value weights integration is

Using the matrix determinant lemma, we find

where we defined the scalar

We now enforce the identity of Eq.by
Fourier representation of the Dirac delta function, introducing the
auxiliary scalar variable.
We obtain

In the statistical physics language,andare called order parameters. Since, we can solve the
integral inwith the saddle point method.
The value ofat the saddle point is

Therefore, we obtain

where we defined the “entropy” term

an we introduced thematrix, with matrix elements

Hereare the partial-path indices defined
in Eq., which in this case coincide with. While the definition ofmay appear superfluous here, it is useful to perform the proof by
induction in Sec..

Next, we perform the integration over the weights at layer.
The steps are almost identical to those taken in Sec.,
but will lead to the introduction of a matrix of order parameters.
After this layer, we will be able to provide the results for the integration
of the weights at subsequent layers by induction.

Plugginginto Eq., we get

Once again we see that the integral in the value weightsis given by the product ofidentical integrals of the form

where we defined

withand.

Again, we may consideras anmatrix, whose two
indices run over the pairsand.
With this notation, the result of the value weights integration is

Using the matrix determinant lemma, we find

where we introduced thematrix, with matrix elements defined as

With the same procedure as in Sec., we
introduce the order parameterand its conjugate, also amatrix. We have

Again, we solve the integral inwith the saddle
point method. The value ofat the saddle point
is

Therefore we obtain

where we give a more general definition of the entropy Eq.,
such that it can take a matrix argument

and we introduced thematrix, with matrix elements

Hereandand are the partial-path indices defined in Eq.,
while.

We can now compute the integration over the remaining value weights
by induction. We claim that, after integration of the weights at layer,will have the form

Here we defined a collection of matrix order parameters, one for each
integrated layer. The order parameteris
a partial-path-by-partial-path matrix of size,
weredenotes the size of the
set. We also definedas the matrix of sizewith elements

whereandare the partial-path indices defined in Eq.,
while.

Eq.is verified for layer,
which we derived in Sec.. The induction
step, integrating over the weights at layer, is done by
plugginginto Eq.and applying exactly the same steps
presented in Sec..

After integrating all of the network weights, we have

with

plugginginto Eq., we see that we need to perform
the following integral in

where for convenience we report here the kernel definitions given
in Sec.. The quantities,, andare defined
in terms of a kernel function.
Forwe define,,
and.
The form of the kernel is

with

Computing the Gaussian integral (Eq.) we obtain

where we report here for convenience the definition of the actiongiven in Sec.

with

In the limit,,
we solve the integrals inwith the saddle point method. The partition function
therefore takes the final form

where we recall that,, andall depend
on, which must be evaluated at the minimum of
the action Eq.with respect to all of its arguments.

Differentiating bythe partition function Eq.(see Eq.and Eq.),
we obtain the results for the predictor mean (Eq.)
and variance (Eq.) presented in Sec..

SECTION: Derivation of the order parameter interpretation
Here we provide the derivation of our result on the order parameter
interpretation, exposed in Sec..
The derivation is almost identical to that for the predictor statistics
given in Sec.. What follows below should be
considered as a continuation of Sec., to which
we refer for definitions.

For convenience, we report here the result we want to derive

whereare the path indices defined in
Eq., whiledenotes statistical averaging over the posterior distribution Eq..
We also recall the definition of the network effective weights

As in Sec., we start from the partition
function

To the partition function, we add asource term,
with

such that differentiating byallows
us to obtain

whereindicates the-th component
of the vector, whileis the matrix
whose-th component is.

As in Sec., we proceed to compute the partition
function (Eq.) by integrating all
of the network weights, and finally the auxiliary variable. We make the following observation. In Eq.,
we can write explicitly.
Furthermore, as in Sec.we can
defineand in general

Then Eq.takes the form

Renaming,
we see that the steps for computing Eq.are
identical to those in Sec., until the integration
over the input projection weights.

After integrating all of the network weights except the input projectionwe have

with

We now substitutein Eq., as well asobtaining

The integral inin Eq.has the form

where we defined

where, while, and with the
notationandwe indicate the-th component of the vectorsandrespectively.

We may consideras the elements of anmatrix. With this notation, the partition function after performing
the integral Eq.is

We now differentiate the partition function in Eq.by, as specified by Eq.,
obtaining

The remaining steps are the same as in Sec..
We use the matrix determinant lemmaand the Woodbury matrix identityrespectively to expressandin
terms of amatrixwith elements

whose identity we enforce by Fourier representation of the Dirac delta
function, introducing the auxiliarymatrix. The result of these
operations is

As in Sec., we solve the integral inwith the saddle point method. The value ofat
the saddle point is

Plugging this back into Eq.we obtain

The calculation of the integral infollows that in Sec..
We obtain

wheredenotes the expectation under the distribution

where the actionis the same defined in Sec.,
Eq.. Exactly as in Sec., we compute
the expectation using the saddle point method, under the limit,. We therefore arrive at the
final result

whereis the value at the saddle point of.

SECTION: Experiments
SECTION: Numerical evaluation of the order parameter
The order parameter in our theory is defined self-consistently as
the minimum of an action, Eq.. We determine this order parameter numerically,
by minimizing the action using the Adam optimizer.
As we describe in Appendix, for each(out of 10 choices among), we search for the optimal temperature among 10 choices,
resulting in 100 total configurations.
Each such run takes less than 12 hours on a single A100-40GB GPU.
The learning rate is optimized for each configuration by sweeping amongfor 10 iterations at the beginning of each run, and by selecting the one that achieves the lowest energy term (averaged over these 10 first optimization steps).

SECTION: Hamiltonian Monte Carlo sampling
We sample the network weights from the posterior distribution (Eq.)
using Hamiltonian Monte Carlo sampling. Specifically,
we use the NumPyro implementation of the No U-Turn Sampler (NUTS).

For, we runindependent chains, consisting ofwarm-up steps andsampling steps. We keep one sample every, for a total ofsamples. Due to limited computational
resources, the number of samples is smaller and varies for,
while we did not take samples at all for very large. Note
however, that the largeregime is the least relevant to sample,
since the network is approaching the GP limit. The most important validation
of our theory is performed for the smaller values of.

Each run sampling a model at a giventakes less than 12 hours on a single A100-40GB GPU. The number of runs to sample all model widths for the hidden Markov chain classification task and the one-shot image classification task is 12.

Regarding the temperatureof the Bayesian posterior Eq., this is set differently depending on the task. For the HMC task, we find the temperatureto
not be relevant for improving the network’s classification accuracy. We therefore
set it to a small, but finite value of. In contrast, for the one-shot image classification task,
we find tuning the temperature to be particularly important to prevent overfitting. Therefore, we always tune the temperature to the value giving the optimal classification accuracy for the given network depth. We refer to Appendixfor details on the temperature values and its optimization process.

SECTION: Hidden Markov chain classification task
Here we give additional details on the hidden Markov chain (HMC) classification
task.

Here we recall the task definition, providing a few additional details.

The-th example in the dataset corresponds to an hidden Markov
chainof length, alternating
between two hidden states,.
The transition probability to the opposite state ()
is. In other words, describing the “” states
as one hot vectorsand, the transition probability matrix of the hidden Matkov chain is

The-th chain can belong to one of two classes, labeled,
depending on whetherorrespectively.

The network is presented with visible states - the input tokens -
which are a noisy, higher dimensional representation of the hidden
states. These are given by

Hereare two orthogonal feature vectors
corresponding to the states “”. We setand

The termis a zero-mean Gaussian noise, with,
whereandare the projectors along the
subspace parallel or perpendicular to the plane spanned byand.

We also preprocess the dataset in two ways: adding to each chain a
beginning-of-sentence (bos) token of zeros at position, and
concatenating to each token a one-hot positional encoding vector of
size(i.e. the number of tokens including the bos token).

We useexamples for training andexamples for
testing. For this task, we find the temperatureto
not be relevant for improving the network performance. We therefore
set it to a small, but finite value of. A finite
value ofis required to sample the predictor statistics,
hence comparing the theoretical results with samples.

Here we give the details of the initialization of the fixed query
and key weights.

We recall that we consider a network oflayers andheads
per layer, with readout from the first token (i.e.,). The
network has a total ofattention paths (Fig.(a)). For the first head of
each layer, we make a good choice of the fixed query and key weights,
which defines a “good” attention path, achieving a good classification
accuracy (cf. Sec.in the main text). The remaining heads are initialized
at random. Below we give the initialization details of these good
and random heads.

Let us recall here the definition of the attention matrixfor headat layer. Its matrix elements are defined as

where, whileis the softmax function, applied
along the direction of the token index, andis the dimension
of the query-key feature space. We directly initialize the query-key
matrix product.
Note thatis anmatrix, because we have appended a one-hot positional encoding vector
to the-dimensional input tokens (see Sec.).
In order to define the heads, it is convenient to decomposeinto the block structure

whereacts only on the one-hot positional encoding subspace,acts only on the subspace of the tokens’ “features”, andmix these two subspaces. The scalaris a parameter
controlling the “hardness” of the softmax function (for,
the softmax becomes a hardmax). We set it tofor all heads.

Let us define the good heads, i.e.forand. Note that the goal here is not to define heads that
are provably good at solving the task, but rather to make a good guess
for their initialization, based on our knowledge of the nature nature of the task.

For the head,we define

and

whereis the component
ofat indices,.

The head,implements uniform attention,. It is defined by,
and.

As discussed in the main text, the attention path defined by the good
heads achieves a good classification accuracy. We can give an intuition
as to why this is the case. Intuitively speaking, in the limit of
a hardmax attentionand no noise,
the attention path is “counting” the number of times a token has
remained in the same state after a new step in the Markov chain. Indeed,
the first head “detects” when there has not been a change of state
between adjacent tokens. It does so by either attending nearby tokens
if and only if they are in the same state, or attending “nothing”
(in the sense of the zero beginning-of-sentence token). Then, the
second head sums over the tokens attended by the first head,
thereby “counting” the number of times a token has not changed
state. More generally, outside the above mentioned limit, we can say
that the good attention path focuses on the two most relevant pieces
of information needed to solve the task: First, it preferentially
pays attention to nearby tokens, which is important because of the
memoryless nature of the Markov process; Second, it is able to detect
the type of transition occurrying between nearby tokens (i.e. remaining in the same state,
or changing state), which is important to distinguish between the
two classes, since they differ by their transition probability.

Let us define the random heads, i.e.forand. These are initialized with Gaussian
identically and independently distributed entries

and. Note
that we take care of proper normalization of the above matrices, depending
on which subspaces they act upon (i.e. the “features” or the “one-hot
positions” subspaces).

As mentioned in the main text (Sec.), the random heads introduce
three additional paths: two adversarial paths, deteriorating the performance
of the good path, and one “denoising” path, improving the good
path performance. We can get an intuition of why this is so by looking
at their associated same-path kernels, Fig.(b). We can see that
both the good-path and the two adversarial-path kernels appear very
structured, with sharp excursions in their values for different pairs
of examples. However, while the good-path kernel structure appears
to be aligned with the task, well distinguishing the two classes,
the adversarial-path kernels structure appears random w.r.t. to the
task. We can expect that adding these adversarial kernels to the good
one would destroy it’s task-relevant structure, as can be visually
understood from the total GP kernel. In contrast, the total renormalized
kernel, in which the adversarial-path kernels do not contribute, preserves
the task-relevant structure. Differently, the denoising-path kernel
appears less structured and more uniform, with weaker noisy excursions.
In fact, what we suspect is that there is nothing special about the
specific realization of the random head involved in the denoising
path, which is just implementing a noisy version of uniform attention.
We verify this by substituting the random head with one implementing
uniform attention and repeating the same experiment shown in Fig.(f) in the main text. This is shown in Fig.(c), were we plot
the classification accuracy of the network consisting of the good
and denoising paths alone, for the case ofand. We can see that the results are completely
analogous to those shown in Fig.(f) in the main text.

SECTION: One-shot image classification task
Here we provide details of gradient descent training of our transformer-like model (Sec.) for the one-shot image classification task.
We use the Omniglot dataset with the standard 1028/172/432-splits for the train/validation/test class splitsas implemented in.
We use the Adam optimizerusing an initial learning rate ofand a batch size of 128 for 10 epochs.
We check the validation accuracy every 1000 steps and select the final model as the one that achieves the best validation accuracy.
We use the binary regression loss as in the theory.
Unlike in the theory, here we train all the model parameters including the key and query projection weight matrices.
We set.
All the models considered in this work can be trained on a single A100-40GB GPU within less than 2 hours.

Here we report further results on the one-shot image classification
task.

For the Bayesian model, we find tuning the Gibbs
temperatureto be particularly important to optimally
perform the task. All results for the network’s classification accuracy presented in Sec.and below
are therefore shown at the optimal temperature for the given,
obtained by scanning the set of temperatures, whereand.

Note that temperature is optimized only for the in-distribution classification
accuracy. In particular, we do not optimize for temperature when testing
out-of-distribution, but rather keep the optimal temperature determined
by evaluating the network in-distribution.

For the network considered in the main text, we find the following optimal temperatures:for;for;for;for.
Note that the optimal
temperature grows consistently asbecomes smaller. This can be
readily understood by inspecting the equation for the mean predictor
(Eq.), which we report here for convenience

where we recall that

whereis a path-path kernel. For decreasing, we
typically observe the order parameter growing in overall magnitude,
which in turn affects the magnitude of the kernel. As a consequence,
also the optimal temperature needs to be rescaled. The
fact thatgrows in magnitude for smallercan be understood
from the energy term (Eq.) in the action (Eq.). We discussed
in Sec.that this can be seen as the negative log-likelihood of
the labels vectorunder a centered Gaussian distribution, whose
covariance matrix is the kernel. While the most effective way
to minimize the energy term is that described in the main text (Sec.),
i.e. aligning the kernel with the task, one more trivial way is to
increase the log-likelihood variance in all directions (i.e. increasing
the kernel’s overall magnitude). In all of our experiments, we always
observe this phenomenon of growing magnitude to a certain extent.

We perform the same experiments on the classification accuracy for
a deeper network (,). The results are shown in Fig.. Note that
here we also report the test accuracy on MNIST, which was not shown
in the main text. We can see that the results discussed in the main
text are confirmed also for the deeper network. In particular, for
the in-distribution classification accuracy, we consistently observe
a performance improvement in the renormalized regime, with respect
to the GP limit. When testing out of distribution, we can see that
in the best cases (fashionMNIST for (,); MNIST for (,)), the performance improvement is preserved, or at the very
worst (MNIST for (,); fashionMNIST for (,))
the GP and renormalized regime show comparable performance.

We repeat the head pruning experiment for a network with more heads
per layer (,). We find that we can prune with marginal performance loss a similar percentage of heads
as in the smaller network. The results are shown in Fig.. Again, we see that the
head scores determined by our theory are qualitatively in line with
the performance loss caused by pruning the corresponding head (Fig.(c)).
Note that we do not expect a perfect alignment between the two quantities.
The most important fact to verify is that the low scoring heads correspond
to a small drop in performance. In Fig.(d) we show the classification
accuracy after pruning an increasing number of heads, in order of
their score. Up to 4 heads (of the network size) the in-distribution
performance has only a marginal drop, identical to that obtained after pruning
2 heads in the smaller network considered in the main text (also accounting toof its
size, Fig.(b)). We can also see that pruning up to three heads improves the
out-of-ditribution performance on fashionMNIST, as we also observed
for the smaller network.