SECTION: Protecting Federated Learning from Extreme Model Poisoning Attacks via Multidimensional Time Series Anomaly Detection

Current defense mechanisms against model poisoning attacks in federated learning (FL) systems have proven effective up to a certain threshold of malicious clients.In this work, we introduce FLANDERS, a novel pre-aggregation filter for FL resilient to large-scale model poisoning attacks, i.e., when malicious clients far exceed legitimate participants.
FLANDERS treats the sequence of local models sent by clients in each FL round as a matrix-valued time series. Then, it identifies malicious client updates as outliers in this time series by comparing actual observations with estimates generated by a matrix autoregressive forecasting model maintained by the server.
Experiments conducted in several non-iid FL setups show that FLANDERS significantly improves robustness across a wide spectrum of attacks when paired with standard and robust existing aggregation methods.

SECTION: IIntroduction

Recently,federated learning(FL) has emerged as the leading paradigm for training distributed, large-scale, and privacy-preserving machine learning (ML) systems[33,32].
The core idea of FL is to allow multiple edge clients to collaboratively train a shared, global model without disclosing their local private training data.
A typical FL round involves the following steps:(i)the server randomly picks some clients and sends them the current, global model;(ii)each selected client locally trains its model with its own private data; then, it sends the resulting local model to the server;111Whenever we refer to global/local model, we mean global/local modelparameters.(iii)the server updates the global model by computing anaggregation function, usually the average (FedAvg), on the local models received from clients.
This process goes on until the global model converges.Although its advantages over standard ML, FL also raises security concerns[17]. Here, we focus onuntargeted model poisoningattacks[9], where an adversary attempts to tweak the global model weights by directly perturbing the local model’s parameters of some infected clients before these are sent to the central server for aggregation.
In doing so, the adversary aims to jeopardize the global modelindiscriminatelyat inference time.
Such model poisoning attacks severely impact standard FedAvg; therefore, more robust aggregation functions must be designed to secure FL systems.Unfortunately, existing defense mechanisms either rely on simple heuristics (e.g., Trimmed Mean and FedMedian by[44]) or need strong and unrealistic assumptions to work effectively (e.g., foreknowledge or estimation of the number of malicious clients in the FL system, as for Krum/Multi-Krum[13]and Bulyan[34], which, however, cannot exceed a fixed threshold).
Furthermore, outlier detection methods using K-means clustering[39]or spectral analysis like DnC[37]do not directly consider the temporal evolution of local model updates received.
Finally, strategies like FLTrust[14]require the server to collect its own dataset and act as a proper client, thereby altering the standard FL protocol.This work introduces a novel pre-aggregationfilterrobust to untargeted model poisoning attacks. Notably, this filteroperates without requiring prior knowledge or constraints on the number of malicious clients andinherently integrates temporal dependencies.
The FL server can employ this filter as a preprocessing step before applyinganyaggregation function, be it standard like FedAvg or robust like Krum or Bulyan.
Specifically, we formulate the problem of identifying corrupted updates as a multidimensional (i.e., matrix-valued) time series anomaly detection task.
The key idea is that legitimate local updates, resulting from well-calibrated iterative procedures like stochastic gradient descent (SGD) with an appropriate learning rate, showhigher predictabilitycompared to malicious updates. This hypothesis stems from the fact that the sequence of gradients (thus, model parameters) observed during legitimate training exhibit regular patterns, as validated in SectionIV-B.Inspired by the matrix autoregressive (MAR) framework for multidimensional time series forecasting[15], we propose the FLANDERS (FederatedLearning meetsANomalyDEtection for aRobust andSecure) filter.
The main advantages of FLANDERS over existing strategies like FLDetector[46]are its resilience to large-scale attacks, whereor more FL participants are hostile, and the capability of working under realistic non-iid scenarios.
We attribute such a capability to two key factors:FLANDERS works without knowing a priori the ratio of corrupted clients, andit embodies temporal dependencies between intra- and inter-client updates, quickly recognizing local model drifts caused by evil players. Below, we summarize our main contributions:

We provide empirical evidence that the sequence of models sent by legitimate clients is more predictable than those of malicious participants performing untargeted model poisoning attacks.

We introduce FLANDERS, the first pre-aggregation filter for FL robust to untargeted model poisoning based on multidimensional time series anomaly detection.

We integrate FLANDERS into Flower,222https://flower.dev/a popular FL simulation framework for reproducibility.

We show that FLANDERS improves the robustness of the existing aggregation methods under multiple settings: different datasets, client’s data distribution (non-iid), models, and attack scenarios.

We publicly release all the implementation code of FLANDERS along with our experiments.333https://anonymous.4open.science/r/flanders_exp-7EEB

The remainder of the paper is structured as follows. SectionIIcovers background and preliminaries.
In SectionIII, we discuss related work.
SectionIVand SectionVdescribe the problem formulation and the method proposed. SectionVIgathers experimental results. Finally, we conclude in SectionVIII.

SECTION: IIBackground and Preliminaries

SECTION: II-AFederated Learning

We consider a typical supervised learning task under a standard FL setting, which consists of a central serverand a set of distributed clients, such that.
Each clienthas its own private training set, namely the set of itslocal labeled examples, i.e.,.The goal of FL is to train a global predictive model whose architecture and parametersare shared across all clients by solvingwhereis the local objective function for client. Usually, this is defined as the empirical risk calculated over the training setsampled from the client’s local data distribution:whereis an instance-level loss, e.g., cross-entropy (classification) or squared error (regression).
Eachspecifies the relative contribution of each client.
Since it must hold that, two possible settings are:or, where.

The generic federated round at each timeis decomposed into the following steps and iteratively performed until convergence, i.e., for each:

randomly selects a subset of clients, so that, and sends them the current, global model. At,is randomly initalized.

Each selected clienttrains its local modelon its own private databy optimizing the following objective, starting from:

The value ofis computed via gradient-based methods (e.g., stochastic gradient descent) and sent to.

computesas the updated global model, whereis anaggregation function; for example,, i.e., FedAvg or alike[31].

SECTION: II-BThe Attack Model: Federated Aggregation under Model Poisoning

The most straightforward aggregation functionthe server can implement is FedAvg, which computes the global model as the average of the local model weights received from clients.
FedAvg is effective when all the FL participants behave honestly[18,27,32].
Instead, this work assumes that an attacker controls a fractionof theclients, i.e.,, known as malicious. This is in contrast to previous works, where.
Below, we describe our attack model.

Attacker’s Goal.Inspired by many studies on poisoning attacks against ML[36,11,10,41,29,43,26], we consider the attacker’s goal is to jeopardize the jointly learned global modelindiscriminatelyat inference time withanytest example.
Such attacks are known asuntargeted[21], as opposed totargetedpoisoning attacks, where instead, the goal is to induce prediction errors only for some specific test inputs (e.g., via so-calledbackdoor triggersas shown by[6]).

Attacker’s Capability.Like Sybil attacks to distributed systems[20], the attacker can injectfake clients into the FL system or compromisehonest clients.
The attacker can arbitrarily manipulate the local models sent by malicious clients to the server.
More formally, letbe one of thecorrupted clients selected by the server on the generic-th FL round; it first computes its legitimate local modelwithout modifying its private data; then it findsby applying apost hocperturbationto. For example,, whereis a Gaussian noise vector.
More advanced attack strategies have been designed, as discussed in SectionVI-B.

Attacker’s Knowledge.We assume the attacker knows its controlled clients’ code, local training datasets, and local models.
Moreover, we consider the worst-case scenario, where the attacker isomniscient, i.e., it has full knowledge of the parameters sent by honest parties. This allows the attacker to choose the best parameters to inject into the FL protocol, e.g., by crafting malicious local models that are close to legitimate ones, thus increasing the probability of being chosen by the server for the aggregation.

Attacker’s Behavior.In each FL round, the attacker, similar to the server, randomly selectsclients to corrupt out of theavailable. Any of thesemalicious clients that happen to be among those selected by the server will poison their local models using one of the strategies outlined in SectionVI-B.
Note that, unlike earlier studies, we do not assume that malicious clients are chosen in the initial round and remain constant throughout the training. We argue that this approach is more realistic, as legitimate clients can be compromised at any point in actual deployments. Therefore, an effective defense in FL should ideally exclude these clients as soon as they submit their first malicious model.

This category ofuntargeted model poisoningattacks has been extensively explored in previous works[13,9,21].
It has been shown that including updates even from a single malicious client can wildly disrupt the global model if the server runs standard FedAvg[13,44].

SECTION: IIIRelated Work

Below, we describe the most popular defenses against model poisoning attacks on FL systems, which will serve as the baselines for our comparison.
A more comprehensive discussion is in[25,35].

FedMedian[44].The central server sorts the-th parameters received from all thelocal models and takes the median of those as the value of the-th parameter of the global model. This process is applied for all the model parameters, i.e.,.Trimmed Mean[42].This rule computes a model as FedMedian does, and then it averages thenearest parameters to the median.
Ifclients are compromised at most, this aggregation rule achieves an order-optimal error rate when.Krum[13].It selects one of thelocal models received from the clients, which is most similar toallother models, as the global model. The rationale behind this approach is that even if the chosen local model is poisoned, its influence would be restricted because it resembles other local models, potentially from benign clients.
A variant called Multi-Krum mixes Krum with standard FedAvg.Bulyan[34].Since a single component can largely impact the Euclidean distance between two high-dimensional vectors, Krum may lose effectiveness in complex, high-dimensional parameter spaces due to the influence of a few abnormal local model weights. Bulyan iteratively applies Krum to selectlocal models and aggregates them with a Trimmed Mean variant to mitigate this issue.DnC[37].This robust aggregation uses spectral analysis to detect and filter outliers as proposed by[19].
Similarly, DnC computes the principal component of the set of local updates sent by clients. Then, it projects each local update onto this principal component. Finally, it removes a constant fraction of the submitted model updates with the largest projections.FLDetector[45].This method is the closest to our approach. It filters out malicious clients by measuring their consistency acrossrounds using an approximation of the integrated Hessian to compute thesuspicious scoresfor all clients. However, unlike our method, FLDetector struggles with large numbers of malicious clients, and cannot work with highly heterogeneous data.

SECTION: IVProblem Formulation

SECTION: IV-ATime Series of Local Models

At the end of each FL round, the central servercollects the updated local modelssent by the subset of selected clients.444A similar reasoning would apply if clients sent their local displacement vectorsor local gradientsrather than local model parameters.Without loss of generality, we assume that the number of clients picked at each round is constant and fixed, i.e.,.
Hence, the server arranges the local models received at roundinto amatrix, whose-th column corresponds to the-dimensional vector of updated parameterssent by client.
However, the subset of selected clients may be different at each FL round, i.e.,for, although we have assumed their size () is the same.
More generally, to track the local models sent byallclients chosen acrossrounds, we can extend eachinto amatrix, where, such that, withbeing the total number of clients.
Notice thatwhen, whereasif the server selects allclients at least once over therounds considered.
At every round,containscolumns corresponding to the local models sent by the clientsactuallyselected for that round. In contrast, the remaining“fictitious” columns refer to the unselected clients.
We fill these columns with the current global model at FL round, i.e.,, as if this were sent by theunselected clients.
Note that this strategy is neutral and will not impact the aggregated global model computed by the server for the next round, as this is calculated only from the updates received by theclients previously selected.

SECTION: IV-BPredictability of Local Models Evolution: Legitimate vs. Malicious Clients

We claim that legitimate local updates showhigher predictabilitycompared to malicious updates.
This hypothesis stems from the fact that the sequence of gradients (hence, model parameters) observed during legitimate training should exhibit “regular” patterns until convergence.To demonstrate this behavior, we consider two clients. Clientis assumed to be a legitimate participant, while clientacts maliciously. Both are selected by the server over a sequence of consecutiveFL rounds to train a global model on theMNISTdataset.
Therefore, we examine the local models sent to the server at each roundby clientsand. Specifically, these are-dimensional vectors of real-valued parameters, such thatand, respectively.
Next, we calculate the average time-delayed mutual information (TDMI) for each pair of observed local modelsandacrossrounds, where, for both clientand. This analysis aims to discern the time-dependent nonlinear correlation and the level of predictability between observations.
We expect the TDMI calculated between pairs of models sent by the legitimate clientto be higher than that computed between pairs sent by the malicious client. Thus, the temporal sequence of local models observed from clientis more predictable than that of client.Letdenote the local model sent by the generic FL clientas an instance of a-dimensional vector-valued process.
We defineas follows:

whereis the probability density function and. If each realizationin the vectorisindependentfrom each other, we can compute the average TDMI as follows[5]:

where TDMIcalculates TDMI for the univariate case.
Indeed, themodel parametersshould reasonably be independent. Under this assumption, the joint probability density function of the parameters factors into a product of individual probability density functions, forming a product measure on-dimensional Euclidean space. Thus, according to Fubini’s theorem[12], the integral of each parameter will be independent of the others because the parameters are independent.To mimic the behavior of a hypothetical optimal FLANDERS filter, and therefore avoid the propagation of poisoned local models sent by clientacross therounds, we assume that, at each round, the global model from which both clients start their local training process is polished from any malicious updates received at the previous round.
Next, we calculate the average time-delayed mutual information (TDMI) for each pair of observed local modelsandacrossrounds, where, for both clientand.
Firstly, we consider the special case whereand compute the average TDMI between each pair ofconsecutivelocal models sent by the legitimate client and the malicious client, when this runs one of the four attacks considered in this work, namely GAUSS, LIE, OPT, and AGR-MM presented in SectionVI-B.
In Figure1, we plot the empirical distributions of the observed average TDMI for the legitimate and malicious clients.

To validate our claim that legitimate models are more predictable than malicious ones, we compute the mean of the empirical distributions for the legitimate and malicious client,and, respectively, whereGAUSS, LIE, OPT, AGR-MM. Then, we run a one-tailed-test against the null hypothesis, where the alternative hypothesis is.
The results of these statistical tests are illustrated in TableI, showing that, in all four cases, there is enough evidence to reject the null hypothesis at a confidence level.

SECTION: IV-CPoisoned Local Models as Matrix-Based Time Series Outliers

We, therefore, formulate our problem as a multidimensional time series anomaly detection task.
At a high level, we want to equip the central server with an anomaly scoring function that estimates the degree of each client picked at the current round being malicious, i.e., itsanomaly score, based on the historical observations of model updates seen so far from the clients selected.
Such a score will be used to restrict the set of trustworthy candidate clients for the downstream aggregation method.
Note that unselected clients willnotcontribute to the aggregation; thus, there is no need to compute their anomaly score even if they were marked as suspicious in some previous rounds.
On the other hand, we must design a fallback strategy for “cold start” clients – i.e., FL participants who send their local updates for the first time or have not been selected in any of the previous rounds considered – whether honest or malicious.More formally, letbe the matrix of local model updatespredictedby the central serverat the generic FL round.
The forecasting modeldepends on a set of parametersestimated using the pastmodel updates observed, i.e.,, where.
Also, the server sees the actual matrix.The anomaly scoreof the generic clientat roundcan thus be defined as follows:

The first condition refers to a client selected for round, which also appeared at least once in the history.
In this case,measures the distance between the observed vector of weights sent to the server () and the predicted vector of weights () output by the forecasting model.
The second condition, instead, occurs when a client is selected for the first time at roundor does not appear in the previoushistorical matrices of observations used to generate predictions. Here, due to the cold start problem, we cannot rely onand, therefore, the most sensible strategy is to compute the distance between the current global model and the local update received by the new client.
Finally, the anomaly score is undefined () for any client not selected for round.The server will thus rank all theselected clients according to their anomaly scores (e.g., from the lowest to the highest).
Several strategies can be adopted to choose which model updates should be aggregated in preparation for the next round, i.e., to restrict from the initial setto another (possibly smaller) setof trusted clients.
For example,may consider only the model updates received from the top-clients () with the smallest anomaly score, i.e.,, whereindicates the-th smallest anomaly score at round.
Alternatively, the raw anomaly scores computed by the server can be converted into well-calibrated probability estimates.
Here, the server sets a thresholdand aggregates the weights only of those clients whose anomaly score is below, i.e.,.
In the former case, the number of considered clients () is bound apriori,555This may not be true if anomaly scores are not unique; in that case, we can simply enforce.whereas the latter does not put any constraint on the size of final candidates.
Eventually,will compute the updated global model, whereis any aggregation function, e.g., FedAvg, Bulyan, or any other strategy.

Below, we describe how we use the matrix autoregressive (MAR) framework proposed by[15]to implement our multidimensional time series forecasting model, hence the anomaly score.

SECTION: VProposed Method: FLANDERS

SECTION: V-AMatrix Autoregressive Model (MAR)

We assume the temporal evolution of the local models sent by FL clients at each round is captured by a matrix autoregressive model (MAR).
In its most generic form, MAR() is a-order autoregressive model defined as follows:

whereis thematrix of observations at time,areandautoregressive coefficient matrices, andis awhite noise matrix.
In this work, we consider the simplest MAR()666Unless otherwise specified, whenever we refer to MAR, we assume MAR().Markovian forecasting model, i.e.,, where the matrix of local updates at timedepends only on the matrix observed at time step, namely.

Letbe thepredictedmatrix of observations at time, according to a modelparameterized by, i.e.,.
If we have access tohistorical observations, we can estimate the best coefficientsby solving the following objective:

whereindicates the Frobenius norm.
Note thatimpacts only the size of the training set used to estimate the optimal coefficientsand; it does not affect the order of the autoregressive model, which will remain MAR() andnotMAR().
The optimal coefficientsandcan thus be estimated via alternating least squares (ALS) optimization[28].
Further details are provided in AppendixXI.

SECTION: V-BMAR-based Anomaly Score

Our approach consists of two primary steps:MAR estimationandanomaly score computation.MAR Estimation.At the first FL round (), the server sends the initial global modelto the set ofselected clientsand collects from them thematrix of updated models. Hence, it computes thenewglobal model, whereor any other existing robust aggregation heuristic.
For any other FL round, the server can use the pasthistorical observationsto estimate the best MAR coefficientsaccording to (6).
In general,; however, if we assumefixed at each round, the server will considerpast observations.
Again, independently of the value of, MAR will learn to predict the current matrix of weightsonlyfrom the previously observed matrix.
Therefore, each matrix used for traininghas the same size, as it contains exclusively theupdates received at the round.
Of course, employing a higher order MAR() model withwould require extending each observed matrix to(, as detailed in SectionIV-A. This is to track the local updates sent by selected clients across multiple historical rounds.Anomaly Score Computation.At the generic FL round, we compute the anomaly score using the estimated MAR forecasting model as follows.
Letbe the matrix of observed weights.
This matrix may contain one or more corrupted local models from malicious clients.
Then, we compute the-dimensional anomaly score vector, where, as in (4).
A critical choice concerns the functionused to measure the distance between the observed vector of weights sent by each selected client and the vector of weights predicted by MAR.
In this work, we set, whereis the squared-norm. Other functions can be used (e.g.,cosine distance), especially in high dimensional spaces, where-norm withhas proven effective[4].
Choosing the bestis outside the scope of this work, and we leave it to future study.According to one of the filtering strategies discussed in SectionIV-C, we retain only theclients with the smallest anomaly scores. The remainingclients are considered malicious; thus, they
are discarded and do not contribute to the aggregation run by the server as if they were never selected.At the next round, we may want to refresh our estimation of the MAR model, i.e., to update the coefficient matricesand.
We do so by considering the latest observedand the otherprevious matrices of local updates, using the same sliding window of size.
Since the observed matrixcontainspotentially malicious clients, we cannot use it as-is. Otherwise,if any of the spotted malicious clients is selected again at round, we may alter the estimation ofandwith possibly corrupted matrix columns.
To overcome this problem, we replace the originalwithbefore, feeding it to train the new MAR model.
Specifically,is obtained fromby substituting theanomalous columns either with the parameter vectors from the same clients observed at time, which are supposed to be still legitimateorthe current global model.
The advantage of this solution is twofold.
On the one hand, a client labeled as malicious at FL roundwould likely still be considered so atif it keeps perturbing its local weights, thus improving robustness.
On the other hand, our solution allows malicious clients to alternate legitimate behaviors without being banned, speeding up model convergence.
Notice that the two considerations above might not be valid if we updated the MAR model using the original, partially corrupted.
Indeed, in the first case, the
distance between two successive poisoned models by the same client would reasonably be small. So, the client’s anomaly score will likely drop to non-alarming values, thereby increasing the number of false negatives.
In the second case, the
distance between a corrupted and a legitimate model would likely be large.
Thus, a malicious client will maintain its anomaly score high even if it acts honestly, impacting the number of false positives.

SECTION: V-CA Step-by-Step Example

In Figure2, we depict how FLANDERS computes the anomaly score vectorat the generic FL round. Specifically, the serveruses its current MAR() forecasting modelwhose best parametersare estimated fromprevious historical observations of local models received in the previous rounds. It appliesto the previously observed matrixto get the next predicted matrix of local models, i.e.,. Then, it compares this predicted matrixwith the actual matrix of local updates received.
The final anomaly score vector is calculated by measuring the distancebetween each column of those two matrices, according to Eq.4.

It is worth remarking that, in general, only some clients are selected at every round. In particular, if a client– whether it is honest or malicious – is selected by the server for the first time at round, the MAR forecasting modelwill not be able to make any prediction for it due to a cold start problem (i.e., the predicted matrixwill not contain a column corresponding to).
In such a case, we must adopt a fallback strategy. Without any historical information for a client, the most sensible thing to do is to compute the distancebetween the local model update it sent and the current global model.

To better clarify how FLANDERS works, consider the following practical example.Suppose an FL system consists of a centralized server andclients; furthermore, at each round,of those clients are randomly chosen for training.
At the very first round (), letbe the set ofclients selected by the server. Hence, the server sends the current global modelto each of those clients and collects thematrix of updated local models.
Therefore, it computes thenewglobal model.
Notice that, at this stage, no anomaly score can be computed as FLANDERS cannot take advantage of any historical observations of local model updates. As such, if one (or more) selected clients in the very first round are malicious, plain FedAvg may not detect those. To overcome this problem, FLANDERS should be paired with one of the existing robust aggregation heuristics at. For example,.

At the next round (), FLANDERS can start using past observations (i.e., only) to estimate the best MAR coefficientsaccording to Eq. (9) above.
Supposeis the set ofclients selected by the server at the second round.
Letbe the local models sent by the clients to the server.
Moreover,are the local models predicted by MAR, using the previous set of observations.

It is worth noting that; the other two clients,and, are considered “cold-start” since they are selected for the first time or, in any case, they do not appear in the historical observations used by MAR for predictions (i.e., in the previousmatrices).
Thus, we calculate the following anomaly scores, according to Eq.5:

and, i.e., we measure the distance between the models actually sent byandand those predicted by MAR using the previous observations (first condition);

and, i.e., we measure the distance between the models actually sent byandand the current global model at time, as those clients were never picked before (second condition);

, i.e., these clients were not selected at round, and therefore they will not contribute to computing the new global modelanyway (third condition).

Let us assume thatis a malicious client controlled by an attacker. Moreover, suppose that this client started sending poisoned models since the very first round, i.e.,was already corrupted.
In such a case, it is evident that if we had used plain FedAvg at, this would have likely polluted the global modeland, therefore, FLANDERS might fail to recognize this as a malicious client due to a relatively low anomaly score. As stated before, the consequences of this edge situation in which one or more malicious clients are picked at the beginning of the FL training can be mitigated by replacing FedAvg with one of the robust aggregation strategies available in the literature, such as Trimmed Mean, Krum, or Bulyan.

However, supposeis correctly spotted as malicious at the end of rounddue to its high anomaly score. Therefore,(actually,) will be discarded from the aggregation at the server’s end. Hence, FedAvg can now safely be used; more generally, the server can restart running FedAvg fromon, i.e., once FLANDERS can computevalidanomaly scores.
The server can now compute the updated global model as, wherecontains the clients with thesmallest anomaly scores.
For instance, ifand,.

At the next round (), FLANDERS can use the previous two observationsandto refine the estimation of the best MAR coefficients, again solving Eq. (9).777In general, FLANDERS usespast observations.However,cannot be fed as-is to re-train MAR since one of its components – i.e., the local modelsent by client– has been flagged as malicious.
Otherwise, the resulting updated MAR coefficients could be unreliable due to the propagation of local poisoned models.

To overcome this problem, FLANDERS replaces the local model marked as suspiciouswith either one of the previously observed models from the same client that is supposedly legitimateorthe current global model.
Since, in this example, we assumehas been malicious from the first round, we use the latter approach.
Specifically, we changewith, wheresubstitutes.
As discussed at the end of SectionV-B, this fix allows FLANDERS to work even when a malicious client is picked for two or more rounds consecutively.

We can now computeusing the updated MAR, leveraging the previous set of amended observations.
Letdenote the set of clients selected at roundandbe the local models sent by the clients to the server.
Thus,. The remaining three clients –,, and– are treated as “cold-start.” Specifically,andare selected for the first time, while, even though previously selected in the first round, was not chosen at the previous round (). Aswas not part of the historical observations used by MAR() to make predictions, we need to treat it as if it were a cold-start client.

Therefore, anomaly scores are updated as follows:

, i.e., we measure the distance between the models actually sent byand those predicted by MAR using the previous observations (first condition);

,, andi.e., we measure the distance between the models actually sent by,, andand the current global model at time, as those clients were never picked before or did not appear in the previous observations used to make predictions (second condition);

, i.e., these clients were not selected at round, and therefore they will not contribute to computing the new global modelanyway (third condition).

Generally, the process above continues until the global model converges.

SECTION: V-DFLANDERS’ Pseudocode

We present the pseudocode of a hypothetical FL server that integrates FLANDERS into the model aggregation stage.
The main server-side loop is shown in Algorithm1, whereas Algorithm2details the computation of anomaly scores, which is at the heart of the FLANDERS filter.

SECTION: V-EComputational Complexity Analysis

To train our MAR forecasting model, we first need to run the ALS algorithm, which estimates the coefficient matricesand.
ALS iteratively performs two steps to computeand, respectively.
At the generic-th iteration, computing the-thmatrixcosts; similarly, computing the-thmatrix888It is worth remarking that, using MAR(),and thushas size.costs.
Thus, a single iteration costs, namelyor,999orusing the Coppersmith-Winograd algorithm[16].depending what term dominates between(the number of weights) and(the number of clients).
ALS performs the two steps above foriterations (e.g., in this work, we set).
Second, the computation of the anomaly score costsas it measures the distance between observed and predicted local updates from all theselected clients.
Third, every time the MAR model is refreshed, we need to rerun ALS; in the worst-case scenario, we might want to re-estimateandat every single FL round.It is worth noticing that performing ALS directly on high dimensional parameter space () in standard FL settings with loads of clients () may be unfeasible (e.g., whenandrange fromto).
To keep the computational cost tractable (and limit the impact of the curse of dimensionality), in our experiments, where, we reduce dimensionality via random sampling on the parameter space, as proposed by[37]. Specifically, we samplemodel parameters for ALS, withset to.

SECTION: VIExperiments

SECTION: VI-AExperimental Setup

Datasets, Tasks, and FL Models.We consider four public datasets for image classification:MNIST,Fashion-MNIST,CIFAR-10, andCIFAR-100. All datasets are randomly shuffled and partitioned into two disjoint sets: 80% is used for training and 20% for testing.
We train a Multilayer Perceptron (MLP) onMNISTandFashion-MNISTand a Convolutional Neural Network (CNN) onCIFAR-10andCIFAR-100(MobileNet[23]).
All models are trained by minimizing cross-entropy loss.The full details are available in AppendixXII.

FL Simulation Environment.To simulate a realistic FL environment, we integrate FLANDERS into Flower[8]. Moreover, we implement in Flower any defense baseline considered that the framework does not natively provide.
In TableIIwe synthetically report the hyperparameters.We set the numberof FL clients to, and we assume that the server selectsallthese clients in every FL round (). In AppendixXV-B, we test when the serverrandomlychoosesclients. We suppose the attack starts at; then, we monitor the performance of the global model until. Recall that we select the malicious clients randomly across all the participating ones; this implies that a single client can alternate between legitimate and malicious behavior over successive FL rounds.

Non-IID Local Training Data.We simulate non-iid clients’ training data distributions following[24]. We assume every client training example is drawn independently with class labels following a categorical distribution overclasses parameterized by a vectorsuch thatand.
To synthesize a population of non-identical clients, we drawfrom a Dirichlet distribution, wherecharacterizes a prior class distribution overclasses, andis aconcentrationparameter controlling the identicalness among clients.
With, all clients have identical label distributions; on the other extreme, with, each client holds examples from only one class chosen at random.
In our experiments, we set.

SECTION: VI-BAttacks

We assess the robustness of FLANDERS under the following well-known attacks.
For each attack, we vary the numberof malicious clients, where. Ultimately, we analyze an attacker’s performance embedded with a MAR model.

Gaussian Noise Attack.This attack randomly crafts the local models on the compromised clients.
Specifically, the attacker samples a random value from a Gaussian distributionand sums it to all thelearned parameters.
We refer to this attack as GAUSS.

“A Little Is Enough” Attack[7].This attack shifts the aggregation gradient by carefully crafting malicious values that deviate from the correct ones as far as possible.
We call this attack LIE.

Optimization-based Attack[21].This attack is framed as an optimization task, aiming to maximize the distance between the poisoned aggregated gradient and the aggregated gradient under no attack.
By using a halving search, one can obtain a crafted malicious gradient.
We refer to this attack as OPT.

AGR Attack Series[37].This improves the optimization program above by introducing perturbation vectors and scaling factors. Then, three instances are proposed: AGR-tailored, AGR-agnostic Min-Max, and Min-Sum, which maximize the deviation between benign and malicious gradients.
In this work, we experiment with AGR Min-Max, which we call AGR-MM.

Below, we describe the critical parameters for
each attack considered, which are also summarized in TableIII.

GAUSS.This attack has only one parameter: the magnitudeof the perturbation to apply. We setfor all the experiments.

LIE.This method has no parameters to set.

OPT.The parameterrepresents the minimum value thatcan assume. Below this threshold, the halving search stops. As suggested by the authors, we set.

AGR-MM.In addition to the threshold, AGR-MM uses the perturbation vectorsin combination with the scaling coefficientto optimize. We setand, which is the vector obtained by computing the parameters’ inverse of the standard deviation. For Krum, we set, which is the inverse unit vector perturbation.

SECTION: VI-CEvaluation

We evaluate four key aspects of FLANDERS.
Firstly, we test its ability to detect malicious clients against the best-competing filtering strategy, FLDetector.
Secondly, we measure the accuracy improvement of the global model when FLANDERS is paired with “vanilla” FedAvg and the most popular robust aggregation baselines: FedMedian, Trimmed Mean, Multi-Krum, Bulyan, DnC.
Thirdly, we analyze the cost-benefit trade-off of FLANDERS.
Lastly, we test the robustness of our method against adaptive attacks.

Malicious Detection Accuracy.TableIVshows the precision () and recall () of FLDetector and FLANDERS in filtering out malicious clients across different datasets and attacks, with(20% evil participants in the FL systems).
Remarkably, FLANDERS successfully detectsall and onlymalicious clients across every attack setting except for OPT, outperforming its main competitor, FLDetector.
This is further confirmed by TableV, which shows that our method generally provides much higher protection than FLDetector when combined with standard FedAvg.

Better results are observed for extreme attack settings () in TablesVI, andVII, where OPT maximizes the distance between the legitimate models and the malicious ones by erroneously thinking that the high number of its controlled clients pierces through the aggregation functions. Results forin AppendixXV-Care consistent with these findings.

FLDetector fails to distinguish malicious updates from legitimate ones because(i)the anomaly score is based on approximating a single Hessian matrix that must be close to all legitimate clients, making it inadequate for highly non-iid settings, where malicious updates can be similar to legitimate ones; and(ii)the suspicious scores are computed as the average normalized Euclidean distance of the past iterations. Although this is coherent and works well with a threat model where the malicious clients are always the same across all rounds, FLANDERS does not make such an assumption, making FLDetector unable to recognize malicious updates arriving for the first time from an ex-legitimate client.

Aggregation Robustness Lift.We proceed to evaluate the enhancement that FLANDERS provides to the robustness of the global model. Specifically, we measure the best accuracy of the global model under several attack strengths using all the baselineswithoutandwithFLANDERS as a pre-filtering strategy.
TableVIIIshows that FLANDERS keeps high accuracy for the best global model under extreme attacks () when used beforeeveryaggregation method, including Multi-Krum and Bulyan, which would otherwise be inapplicable in such strong attack scenarios.
This is further emphasized in TableXX(see AppendixXV-D), where Multi-Krum, when paired with FLANDERS, can effectively operate withoutanyperformance degradation, even in the presence ofmalicious clients.

In Figure3we compare the accuracy of the global model when using FLANDERS (left) and when using FedAvg (right) across the whole FL training process. The left figure shows how the evolution of the accuracy over multiple rounds remains stable and similar to the one without any attack (dashed line), while on the right the accuracy drops irremediably.

For weaker attacks, e.g.,, FLANDERS still generally improves the accuracy of the global model, except when combined with Bulyan, which alone appears already robust enough to counter these mild attacks (see TableXVIIIin AppendixXV-D). The complete results are in AppendixXV-D.
However, it is worth remarking that current robust aggregation methods like Multi-Krum and Bulyan are not designed to handle stronger attack settings.
Nevertheless, when these methods are combined with FLANDERS, they can be deployed successfully even under extremely severe attacks due to FLANDERS’ capability to filter out every malicious clientbeforethe aggregation process takes place.
This is illustrated with an example in TableXX, where it is demonstrated that Multi-Krum, when paired with FLANDERS, can effectively operate withoutanyperformance degradation even when the proportion of malicious clients reaches.
Similarly, as shown in Fig.3, the performance remains robust across all FL rounds even when FLANDERS is paired with a simple aggregation function, such as FedAvg. Instead, when our filter is removed, the accuracy drops as soon as the attack begins.

Cost-Benefit Analysis.Obviously, the robustness guaranteed by FLANDERS under extreme attack scenarios comes with costs, especially due to the MAR estimation stage.
Fig.4depicts two scatter plots for theMNISTandCIFAR-10datasets, focusing on a specific attack scenario (AGR-MM). Each data point on a scatter plot represents a method under one of two attack strengths considered (and). These data points are specified by two coordinates: the overall training time on the-axis and the maximum accuracy of the global model obtained afterFL rounds on the-axis.

Overall, the take-home message is as follows. In scenarios with low attack strength (), Bulyan demonstrates superior accuracy, while FLANDERS + FedAvg offers comparable performance with notably shorter training times. However, as the attack strength increases (), Bulyan becomes impractical, FedAvg alone proves ineffective, and FLANDERS emerges as the optimal choice for achieving the best accuracy vs. cost trade-off.

Robustness against Adaptive Attacks.In this section, we further validate the robustness of FLANDERS against adaptive attacks. We consider a scenario where malicious clients are aware that the FL server uses our method as a pre-aggregation filter.
Specifically, we focus on two levels of knowledge. The first scenario assumes that malicious clients tentatively guess the subset of parameters () used by the FL server to estimate the MAR forecasting model. We refer to this setting asnon-omniscient.
The second, more challenging as well as unrealistic scenario assumes that malicious clients knowexactlywhich parameters are used by the FL server. We call this second scenarioomniscient. Obviously, the latter penalizes FLANDERS way more than the former.Specifically, we perform our experiments overrounds withclients, of which() are malicious. The attacker constructs a matrixusing the local models generated by the corrupted clients. This matrixis then passed as input to the same forecasting model, MAR, that the server uses to determine the legitimacy of local models. The attacker, instead, substitutes the legitimate parameters with those estimated by MAR, exploiting the fact that these estimations do not perform like a legitimate local model. This substitution ultimately hurts the accuracy of the global model once the parameters are aggregated.As introduced above, we first assume that the attacker isnon-omniscient, meaning it does not know which parameters the server has selected for the MAR estimation. Instead, the attacker selects the last layer as.
Afterward, we consider anomniscientattacker who exploits the knowledge of the parameters selected by the server.
In TableIX, we show the results of the non-omniscient scenario, where FLANDERS + Multi-Krum outperforms all other baselines on all three datasets. TableX, on the other hand, refers to the omniscient scenario and demonstrates a different pattern, where FedAvg and Multi-Krum alone perform better than when coupled with FLANDERS.
This may be because FLANDERS consistently filters out legitimate local models in favor of corrupted ones. When using FedAvg, the impact of corrupted parameters is mitigated by averaging a larger number of legitimate models and because the corrupted models’ parameter values are not too different, unlike in methods like OPT. On the other hand, Multi-Krum selects parameters with more nearby neighbors, and with onlycorrupted clients, legitimate models likely still have more and closer neighbors, effectively defending against our adaptive attack.

SECTION: VIILimitations and Future Work

SECTION: VII-AEfficiency/Feasibility

As we discussed in Section 5.3 and AppendixXIV, FLANDERS may suffer from a high computational cost that could limit its deployment in practice.
This concern holds particularly true forcross-deviceFL configurations encompassing millions of edge devices. Conversely, the impact oncross-siloFL scenarios would be notably less pronounced.
However, as we have introduced FLANDERS as a versatile and robust aggregation approach applicable to diverse FL setups (cross-silo and cross-device), there are implementation techniques available to mitigate its complexity. For instance, methods like random parameter sampling[37]can be employed, and we have already incorporated them.
Still, we plan to enhance the scalability of FLANDERS further in future work.
For example, we could replace the standard matrix inversion algorithm with the more efficient Coppersmith-Winograd algorithm[16]and find the optimal frequency for re-estimating FLANDERS’ parameters, instead of performing it during each FL round.

In an extended version of this work, we plan to include a parameter sensitivity analysis, where the performance of FLANDERS is evaluated based on the numberof sampled parameters. This analysis will hopefully provide insights into the optimal trade-off between robustness and efficiency. Additionally, more sophisticated parameter selection strategies beyond pure random sampling could be explored to focus on the most informative neurons, such as Neuron Shapley[22].

SECTION: VII-BPotential Privacy Leakage

In the standard FL setup, the central server must access the local model updates sent by each client (e.g., even to perform a simple FedAvg).
Therefore, our approach, FLANDERS, does not need additional permission nor violate any privacy constraints beyond what any other FL server could already do.
Indeed, the most effective robust aggregation schemes, such as Krum and Bulyan, like FLANDERS, assume that the central server is atrustedentity.
However, if this assumption does not hold, the scenario will change. For instance, if the server operates as an “honest but curious” entity, thoroughly examining the local model parameters received for training the outlier detection model (such as MAR) could unveil sensitive details that the server might exploit, potentially inferring information about each client’s local data distribution.

SECTION: VII-CCross-Device Setting

The cross-device setting (thousands to millions of clients) penalizes FLANDERS, as the server cannot select them all, and the probability of choosing the same client in consecutive rounds is low. However, FLANDERS is more appropriate in cross-silo FL (tens to hundreds of clients) than in cross-device settings. In fact, large attacks involving more than 50% of clients are less feasible when the total number of participants grows to the order of thousands[38]. In such cases, since selected clients have little or no history, FLANDERS computes distances between local updates and the last global model, turning it into a heuristic similar to (Multi-)Krum. Thus, FLANDERS will be comparable to any other heuristic.

SECTION: VII-DBenchmarking

We extensively validate FLANDERS with an exhaustive set of experiments. We compared it against six robust baselines amongst the most powerful at the time of writing, along with standard FedAvg.In addition, robust federated aggregation is a hot research topic; keeping pace with the massive body of work that has been flourishing is challenging.
Hence, we might have missed considering some other methods.
However, we believe that the value of our work still stands. In this regard, our contribution is clear: We are the first to frame the problem of detecting untargeted model poisoning attacks on FL as a matrix-valued time series anomaly detection task and to propose a method effective under severe attack settings, as opposed to existing baselines.

SECTION: VIIIConclusion

We introduced FLANDERS, a novel FL filter robust to extreme untargeted model poisoning attacks, i.e., when malicious clients far exceed legitimate participants.
FLANDERS serves as a pre-processing step before applying aggregation rules, enhancing robustness across diverse hostile settings.FLANDERS treats the sequence of local model updates sent by clients in each FL round as a matrix-valued time series.
Then, it identifies malicious client updates as outliers in this time series using a matrix autoregressive forecasting model.Experiments conducted in several non-iid FL setups demonstrated that existing (secure) aggregation methods further improve their robustness when paired with FLANDERS.
Moreover, FLANDERS allows these methods to operate even under extremely severe attack scenarios thanks to its ability to accurately filter out every malicious clientbeforethe aggregation process takes place.In the future, we will address the primary limitations of this work, as discussed in SectionVII.

SECTION: References

SECTION: Appendix

The (anonymous) GitHub repository with the code and the data to replicate the results discussed in this work is accessible at the following link:https://anonymous.4open.science/r/flanders_exp-7EEB

SECTION: IXA Note on the Terminology Used

The type of model poisoning attacks we consider here are often referred to asByzantineattacks in the literature ([13,21,35]).
Although, in this work, we adhere to the taxonomy proposed by[35], the research community has yet to reach a unanimous consensus on the terminology.
In fact, some authors use the word “Byzantine” as an umbrella term to broadly indicateanyattack involving malicious clients (e.g., targeted data poisoning like backdoor attacksanduntargeted model poisoning as in[25]).
Therefore, to avoid confusion and hurting the feelings of some readers who have already debated on that and found the term inappropriate or disrespectful,101010https://openreview.net/forum?id=pfuqQQCB34&noteId=5KAMwoI2cCwe have decidednotto use the word “Byzantine” to refer to our attack model.

SECTION: XThe Impact of Malicious Clients at each FL Round

Under our assumptions, the FL system containsclients, whereof them are malicious and controlled by an attacker ().
In addition, at each FL round,clients () are selected, and thus, some of themodel updates received by the central server may be corrupted.
The probability of this event can actually be computed by noticing that the outcome of the client selection at each round can be represented by a random variable, whose probability mass function is:

The chance that, at a single round,at least oneof themalicious clients ends up in the list ofclients randomly picked by the server is equal to:

For example, if the total number of clients is,of them are malicious, andmust be drawn at each round, then.
In other words, there are about two out of three chances that at least one malicious client is selected ateveryFL round.

In our FL simulation environment, Flower, we can set a fixed proportion of malicious clients in the system (e.g.,). However, it is important to note that these clients may not remain constant across different FL rounds. In other words, a client who is selected in one round and acts legitimately could become malicious in another round of the FL process.

SECTION: XIMatrix Autoregressive Model (MAR)

This work assumes the temporal evolution of the local models sent by clients at each FL round exhibits a bilinear structure captured by amatrix autoregressive modelof order, i.e., a Markovian forecasting model denoted by MAR() and defined as follows:

whereis a white noise matrix, i.e., its entries are iid normal with zero-mean and constant variance.
To approximate such a behavior, we consider a parametric forecasting model, in the form:

whereis thepredictedmatrix of observations at timeaccording towhen parametrized by coefficient matrices.
Thus, the key question is how to estimate the best model, namely the best coefficient matrices.
For starters, we define an instance-level loss function that measures the cost of approximating the true (yet unknown) data generation process with our modelas follows:

whereindicates the Frobenius norm of a matrix.
More generally, if we have access tohistorical matrix observations, we can compute the overall loss function below:

Notice thathere affects only the size of the training setnotthe order of the autoregressive model. In other words, the forecasting modelwill still be MAR() and not MAR(), i.e., the matrix of local updates at time() dependsonlyon the previously observed matrix at time step().

Eventually, the best estimatesandcan be found as the solutions to the following objective:

Before solving the optimization task defined in Eq. (9) above, we replacewithandwithto ease the reading. Hence, we observe the following.
A closed-form solution to findcan be computed by taking the partial derivative of the loss w.r.t., setting it to, and solving it for. In other words, we search for, such that:

Using Eq. (8) and Eq. (7), the left-hand side of Eq. (10) can be rewritten as follows:

If we set Eq. (11) toand solve it for, we find:

Hence:

Notice that Eq. (14) is obtained by multiplying both sides of Eq. (13) by.

If we apply the same reasoning, we can also find a closed-form solution to compute. That is, we take the partial derivative of the loss w.r.t., set it to, and solve it for:

Eventually, we obtain the following:

We now have two closed-form solutions; one for(see Eq. (14)) and one for(see Eq. (16)).
However, the solution toinvolves, and the solution toinvolves. In other words, we must knowto computeand vice versa.

We can use the standard Alternating Least Squares (ALS) algorithm ([28]) to solve such a problem.
The fundamental idea of ALS is to iteratively update the least squares closed-form solution of each variable alternately, keeping the other fixed. At the generic-th iteration, we compute:

ALS repeats the two steps above until some convergence criterion is met, e.g., after a specific number of iterationsor when the distance between the values of the variables computed in two consecutive iterations is smaller than a given positive threshold, i.e.,and, whereis any suitable matrix distance function and.

Eventually, ifandare the parameters of the MAR model upon convergence, we setandas the best coefficient matrices.

SECTION: XIIDatasets, Tasks, and FL Models

In TableXI, we report the full details of our experimental setup concerning the datasets used, their associated tasks, and the models (along with their hyperparameters) trained on the simulated FL environment, i.e., Flower111111https://flower.dev/(see AppendixXIII). TableXIIshows the description of the hyperparameters.

The MLP for theMNISTdataset is a-layer fully connected feed-forward neural network, whereas the MLP for theFashion-MNISTdataset is a-layer fully connected feed-forward neural network. Both MLPs are trained by minimizing multiclass cross-entropy loss using Adam optimizer with batch size equal to[30].
The CNN used forCIFAR-10is a-layer convolutional neural network, while the CNN used forCIFAR-100is the well-known MobileNet architecture[23]. Both CNNs are trained by minimizing multiclass cross-entropy loss via stochastic gradient descent (SGD) with batch size equal to[3].

At each FL round, every client performs one training epoch of Adam/SGD, which corresponds to the number of iterations needed to “see” all the training instances once, when divided into batches of size.
In any case, the updated local model is sent to the central server for aggregation.

We run our experiments on a machine equipped with an AMD Ryzen 9, 64 GB RAM, and an NVIDIA 4090 GPU with 24 GB VRAM.

SECTION: XIIIFL Simulation Environment

To simulate a realistic FL environment, we integrate FLANDERS into Flower. Moreover, we implement in Flower every other defense baseline considered in this work that the framework does not natively provide.
Other valid FL frameworks are available (e.g., TensorFlow Federated121212https://www.tensorflow.org/federatedand PySyft131313https://github.com/OpenMined/PySyft), but Flower turned out the most flexible.

We want to remark that within Flower, only thenumberof malicious clients in each FL round remains constant, while the framework manages their selection. This implies that a single client can alternate between legitimate and malicious behavior over successive FL rounds.

We report the main properties of our FL environment simulated on Flower in TableII.

SECTION: XIVDefense Settings

Below, we describe the critical parameters for
each non-trivial baseline considered.

Trimmed Mean.The key parameter of this defense strategy is, which is used to cut the parameters on the edges. In this work, we set.

FedMedian.This method has no parameters to set.

Multi-Krum.We set the number of local models to keep for the aggregation (FedAvg) after the Krum filtering as

Bulyan.The two crucial parameters of this hybrid robust aggregation rule areand. The former determines the number of times Krum is applied to generatelocal models; the latter is used to determine the number of parameters to select closer to the median.
In this work, we set, where.

DnC.This is an iterative algorithm that has three parameters: we setas suggested by the authors, and the filtering fractionto keep exactly one model (i.e., the one with the best anomaly score). Furthermore, we setonCIFAR-100to samplelocal model weights.

FLDetector.We set the window size, and we let FLDetector know how many local models to keep, i.e.,, as we did for the other baselines.

FLANDERS.For a fair comparison with other baselines, we set the sliding window size, and the number of clients to keep at every round, whereis the number of clients selected at each round andthe total number of malicious clients in the FL system. Furthermore, we use a sampling valuethat indicates how many parameters we store in the history, the numberof ALS iterations, andandwhich are regularization factors. Random sampling is used to select the subset of parameters considered. This is a common strategy proposed in the literature[37]to lower the model size to train and save the server’s memory. On the other hand, the regularization factors are needed when the model parameters, the number of clients selected, orcause numerical problems in the ALS algorithm, whose number of iterations is set to. We always setand, meaning that there is no regularization since, in our experience, it reduces the capability of predicting the right model. For this reason, we set.
Finally, we set the distance functionused to measure the difference between the observed vector of weights sent to the server and the predicted vector of weights output by MAR to squared-norm.
In future work, we plan to investigate other distance measures, such as cosine or mutual information distance.

TableXIIIsummarizes the values of the key parameters discussed above and those characterizing our method FLANDERS.

SECTION: XVAdditional Results

SECTION: XV-AImpact on Attack-Free scenarios

First of all, in TableXIV, we assess the impact of FLANDERS in an attack-free scenario (i.e., when). In this setting, no clear winning strategy emerges. Sometimes, FLANDERS has a detrimental effect on the global model’s accuracy with a standard aggregation mechanism (e.g., see FedAvg with theMNISTdataset). In other instances, however, FLANDERS improves the global model’s accuracy when paired with robust aggregation schemes (e.g., see Bulyan with theMNISTdataset).

SECTION: XV-BRandom Client Selection

So far, we have assumed that the FL server selectsallavailable clients at each round, i.e.,.
In this section, we investigate the scenario where the number of clients selected at each round remains fixed (), but now the FL server chooses arandomsubset of the available clients, i.e.,.We experiment withclients, of whichare malicious (running the AGR-MM attack). Each client has a non-iid sample of theMNISTdataset, where we fix. At each FL round,clients are randomly chosen ().

The results in TableXVshow that the accuracy of the global model improves as the ratio of sampled clients increases, while FLANDERS remains robust to AGR-MM attacks.

SECTION: XV-CMalicious Detection Accuracy

We evaluate the capability of FLANDERS to detect malicious clients accurately.
Specifically, letbe the set of malicious clients selected at roundby the FL server.
Furthermore, letbe the set of malicious clients identified by FLANDERS at round. Thus, we measurePrecision() andRecall() as usual, i.e.,and, wherestand fortrue positives,false positives, andfalse negatives.
Specifically,,, and.

TableXVIillustrates the values for the precision () and recall () values of FLANDERS under extreme attack scenarios, integrating the findings already reported in TablesIV, andVIin the main body.

SECTION: XV-DAggregation Robustness Lift

In this section, we report the full results on the improved robustness of the global model against malicious attacks when FLANDERS is paired with existing aggregation strategies.

TableXVIIillustrates the comparison between FLANDERS and its main competitor, FLDetector, when both are paired with standard FedAvg, under a severe () attack scenario. These results complete those shown in TableV, andXVIIof the main submission.

Finally, TablesXVIII, andXIXillustrate the impact of FLANDERS on the global model’s accuracy under light () and severe () attack settings.