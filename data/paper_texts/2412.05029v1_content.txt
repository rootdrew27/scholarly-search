SECTION: Mixed Blessing: Class-Wise Embedding guided Instance-Dependent Partial Label Learning

In partial label learning (PLL), every sample is associated with a candidate label set comprising the ground-truth label and several noisy labels. The conventional PLL assumes the noisy labels are randomly generated (instance-independent), while in practical scenarios, the noisy labels are always instance-dependent and are highly related to the sample features, leading to the instance-dependent partial label learning (IDPLL) problem. Instance-dependent noisy label is a double-edged sword. On one side, it may promote model training as the noisy labels can depict the sample to some extent. On the other side, it brings high label ambiguity as the noisy labels are quite undistinguishable from the ground-truth label. To leverage the nuances of IDPLL effectively, for the first time we create class-wise embeddings for each sample, which allow us to explore the relationship of instance-dependent noisy labels, i.e., the class-wise embeddings in the candidate label set should have high similarity, while the class-wise embeddings between the candidate label set and the non-candidate label set should have high dissimilarity. Moreover, to reduce the high label ambiguity, we introduce the concept of class prototypes containing global feature information to disambiguate the candidate label set. Extensive experimental comparisons with twelve methods on six benchmark data sets, including four fine-grained data sets, demonstrate the effectiveness of the proposed method. The code implementation
is publicly available athttps://github.com/Yangfc-ML/CEL.

SECTION: 1.Introduction

Partial label learning (PLL)(Tian et al.,2023; Lv et al.,2024; Jiang et al.,2024; Jia et al.,2024)has garnered significant attention over the past decade as a form of weakly supervised learning. In PLL, each sample is associated with a candidate label set, concealing the ground-truth label and several noisy labels within it. In the training phase, the ground-truth label is inaccessible. PLL does not rely on precise labeling, leading to substantial reductions in time and resource costs associated with sample annotation. Consequently, PLL finds extensive applications across various real-world domains, including automatic image annotation(Cour et al.,2011), web mining(Luo and Orabona,2010), ecoinformatics(Liu and Dietterich,2012), and multimedia content analysis(Zeng et al.,2013). The crux of addressing the PLL problem lies in label disambiguation, involving the identification of the ground-truth label while mitigating the impact of noisy labels within the candidate label set. Conventional label disambiguation techniques(Lv et al.,2020; Wang et al.,2022a; Wu et al.,2022)have demonstrated strong efficacy in the typical PLL scenarios where noisy labels are randomly generated (instance-independent)(Wang et al.,2022b; Jia et al.,2023).

Recently, a more realistic PLL framework called instance-dependent partial label learning (IDPLL)(Xu et al.,2021; He et al.,2023; Wu et al.,2024)was proposed, where noisy labels are re-tailored to individual samples (instance-dependent). As shown in Fig.1, in the conventional PLL, the noisy labels in the candidate label set may have significant differences with the ground-truth label because the noisy labels are randomly generated. However, in IDPLL, the noisy labels are very similar to the ground-truth label. To be more specific, these birds all have yellow feathers, which is more easily to cause label ambiguity. Conventional PLL methods often struggle in IDPLL since they ignore the characteristics of IDPLL.

IDPLL is a mixed blessing. On the positive side, as shown in Fig.2, the model achieves better classification performance in the early stage and converges faster in the IDPLL setting compared with the PLL setting. This is because in IDPLL the noisy labels in the candidate label set are related to the sample, which can describe the sample to some extent. Although the noisy labels in the candidate label set are incorrect, they can still act as supervision to promote model training. On the negative side, the model only achieves quite inferior classification performance in the later stage. The reason is that given the instance, the instance-dependent noisy labels in the candidate label set are highly similar to the ground-truth label, bringing more label ambiguity and making the candidate label disambiguation process more challenging. The current IDPLL methods have made different attempts to achieve better label disambiguation. For example, NEPLL(He et al.,2023)proposed a well-disambiguated sample selection method based on normalized cross-entropy and trained the model progressively according to the selected samples. POP(Xu et al.,2023)proposed to purify the candidate label set during the training phase to gradually reduce the difficulty of label disambiguation. DIRK(Wu et al.,2024)proposed a label rectification strategy that ensured the model output on the candidate label set was always higher than that on the non-candidate label set. However, these methods did not comprehensively exploit both the positive side and negative side of IDPLL, limiting their performance.

To address this double-edged sword challenge in IDPLL, we propose a novel method calledCEL(Mixed Blessing:Class-WiseEmbedding guided Instance-Dependent PartialLabel Learning). Specifically, unlike previous PLL and IDPLL methods, where each sample corresponds to a single embedding, our method introduces the class-wise embedding for each sample, i.e., each sample has multiple embeddings corresponding to different classes, to comprehensively exploit the mixed blessing in IDPLL. First, we propose a class associative loss (CAL) to leverage the relationships among different classes of each sample to guide the learning of class-wise embeddings. In IDPLL, the class-wise embeddings within the candidate label set should exhibit high similarity to each other as the candidate labels can describe the instance to some extent. In contrast, the class-wise embeddings between the candidate label set and the non-candidate label set should display stark differences. In this way, we can obtain class-wise embeddings that are more suitable for IDPLL. Second, we propose a prototype discriminative loss (PDL) by constructing class prototype for each class which containts global feature information to guide the label disambiguation process. To be specific, we select the most high-confidence label for each sample based on the model output, and then we ensure the class-wise embedding of this particular high-confidence label is aligned with its corresponding class prototype while being distanced from other class prototypes, thereby enhancing the model’s discriminative ability. By employing the above two losses, we can obtain embeddings that are tailor-made for IDPLL and enhance the model’s label disambiguation performance simultaneously, thus addressing the mixed blessing issue in IDPLL. Extensive experiments on 6 benchmark
data sets demonstrate the effectiveness of
the proposed method when compared with 12 state-of-the-art methods.

The contributions of our work are summarized as follows:

To the best of our knowledge, we are the first to introduce class-wise embedding in IDPLL. Class-wise embedding enable the model to explore the nuances relationships of classes in each sample, thereby better addressing the mixed blessing problem inherent in IDPLL.

We comprehensively consider the positive and negative sides of IDPLL. To leverage the positive side, we utilize the class associative loss to exploit the relationships among the labels (including both candidate labels and non-candidate labels) of each sample. To address the negative side, we apply the prototype discriminative loss to utilize the relationships between high-confidence class and class prototypes.

Extensive experiments on benchmark data sets demonstrate that our method achieves significantly superior performance when comparing with both PLL and IDPLL methods.

SECTION: 2.RELATED WORK

SECTION: 2.1.Partial Label Learning

Conventional PLL methods can be roughly divided into two categories based on the used information. The first category uses the information in the feature space to guide label disambiguation(Hüllermeier and Beringer,2006; Zhang and Yu,2015; Zhang et al.,2016; Feng and An,2018; Wang et al.,2022b). The second category leverages the information of label space to achieve label disambiguation(Feng and An,2019; Li et al.,2020; Jia et al.,2023). These methods often rely on linear models and are difficult to apply to large-scale data sets. Deep PLL has received wide attention in recent years and it adopts deep neural networks to process large-scale data sets(Lv et al.,2020; Wu et al.,2022; Xia et al.,2023). PRODEN(Lv et al.,2020)proposed to progressively identify the ground-truth label during the self-training procedure. RC and CC(Feng et al.,2020)proposed provably risk-consistent and classifier-consistent label disambiguation methods. LWS(Wen et al.,2021)proposed a family of leveraged weighted loss functions. PICO(Wang et al.,2022a)introduced the widely used contrastive loss(He et al.,2020)to the PLL, which became the foundation for a large number of follow-up works. CR-DPLL(Wu et al.,2022)employed consistency regularization to reduce the impact of noisy labels. PAPI(Xia et al.,2023)constructed the similarity score between feature prototypes and sample embeddings, and improves the model performances by aligning the similarity score with the model output. CROSEL(Tian et al.,2024)used two models to cross-select trustworthy samples from the data set for the training phase. The above methods have achieved superior results in the conventional PLL setting, where the noisy labels in the candidate label set are randomly generated. However, in practice, noisy labels are always instance-dependent(Xu et al.,2021; Xia et al.,2022; Qiao et al.,2023). For example, in crowdsourced annotation tasks, the annotations from many annotators constitute the candidate label set of each sample, and the noisy labels within it are all instance-dependent. Consequently, the performances of conventional PLL methods are often limited due to the lack of consideration of the characteristics of IDPLL.

SECTION: 2.2.Instance-Dependent Partial Label Learning

IDPLL is a PLL framework that is closer to real-world scenarios. VALEN(Xu et al.,2021)was the first work to introduce the concept of IDPLL, with a two-stage disambiguation process. The first stage involves recovering samples’ latent label distribution, and then training the model using the recovered label distribution in the second stage. ABLE(Xia et al.,2022)proposed an ambiguity-induced positive selection contrastive learning framework for IDPLL to achieve label disambiguation. NEPLL(He et al.,2023)introduced a sample selection method based on normalized entropy, intending to separate well-disambiguated samples and under-disambiguated samples. It also established a dynamic candidate-aware thresholding for the further refinement of sample selection. POP(Xu et al.,2023)presented a method that progressively purified the candidate label set and optimized the classifier. IDGP(Qiao et al.,2023)modeled the candidate label generation process for IDPLL, simulating the ground-truth label and noisy labels generation processes with Categorical distribution and Bernoulli distribution respectively. DIRK(Wu et al.,2024)proposed a self-distillation-based label disambiguation method, i.e., the training of the student model is directed by the output of the teacher model. It also developed a label confidence rectification method that meets the prior knowledge of IDPLL. However, the aforementioned IDPLL methods did not consider that the IDPLL setting is a mixed blessing, i.e., on the positive side, noisy labels can describe the sample to some extent and help the model training, while on the negative side, identifying the ground-truth label within the candidate label set becomes more challenging. They only focused on developing better disambiguation methods while ignoring the noisy label information, thereby limiting their performance.

SECTION: 3.Proposed Method

SECTION: 3.1.Class-Wise Embedding

Denote bythe-dimensional feature space andthe corresponding label space withclasses. Letdenote a partial label data set comprisingsamples, whereis the candidate label set of sampleand the ground-truth label of sample is concealed in. The objective of IDPLL is to induce a multi-class classifier that maps elements fromto.
Previous models in the realm of PLL and IDPLL typically consist of two modules.
The first module is the backbone responsible for deriving the feature mapfor each sample, where,andare the height, width, and channel respectively. Afterward,is processed through average global pooling to obtain the low dimensional embedding. The second module encompasses a linear layer that translates this low-dimensional embedding into the final prediction.

In this paper, we use a different paradigm that consists of three modules. As shown in Fig.3, the first module is as same as other models, i.e., a backbone, which extracts the feature mapof each samplethrough the deep neural network. The second module is aclass-wise embeddingencoder(Lanchantin et al.,2021; Liu et al.,2021; Ridnik et al.,2023)which produces class-wise embeddings, whereis the length of each class-wise embedding. Note, theindicates the representation of the-th class of sample. The third module is a group of linear layersthat output the predicted probabilities.

In conventional PLL and IDPLL representation methods, the features of each sample are extracted as a single embedding, so they can only consider relationships at the sample level. However, in IDPLL, the relationships between each sample’s candidate labels and non-candidate labels are valuable and worth leveraging. By employing class-wise encoder, we can represent each sample’s embeddings on different labels. This allows us to explore the internal relationships among different classes and fully utilize the prior knowledge of IDPLL, making it a more suitable representation method for IDPLL.

SECTION: 3.2.Label Disambiguation Loss

As aforementioned, the pivotal process in addressing PLL is label disambiguation, which mitigates the impact of noisy labels within the candidate label set. Here, we adopt a widely used deep PLL disambiguation strategy(Lv et al.,2020), which constructs sample label confidences based on model outputs during training. Initially, the label confidence vectorof sampleis initialized as, ifand, otherwise, wherereturns the number of candidate labels of sample. Throughout the training, we update the label confidence according to the model output:

whererepresents the model prediction on-th class of sample. The goal of Eq. (1) is to eliminate the influence of non-candidate labels, so that the model can focus on the candidate labels. Based on the disambiguated confidence, we can obtain the following classification loss to guide the model training:

whereindicates the cross-entropy loss. Eq. (2) is usually viewed as a self-learning process and has demonstrated efficacy across various PLL scenarios.

SECTION: 3.3.Class Associative Loss based on Class-Wise Embedding

As previous analyzed, IDPLL is a double-edged sword. On the positive side, the noisy labels in the candidate label set are very similar to the ground-truth label because they often share common features and they can depict the sample to some extent, which is also the fundamental reason leading to label ambiguity.
Therefore, an important characteristic of IDPLL is that the labels within the candidate label set should be very similar, while the labels in the candidate label set should exhibit significant differences from the labels in the non-candidate label set. To fully leverage this prior knowledge, we can measure the relationships among classes of each sample through class-wise embeddings. To be specific, the class-wise embeddings corresponding to each label in the candidate label set should be similar to each other, while the class-wise embeddings between the candidate label set and the non-candidate label set should display stark differences. Therefore, we can construct the following class-wise relationships:

whereandrepresent the-th class-wise embedding and the candidate label set of samplerespectively.returns the cosine similarity of two vectors. Note that the class-wise embeddings have been normalized before calculating the cosine similarity.in Eq. (3) denotes the average similarity of classes in the candidate label set, whilein Eq. (4) indicates the average similarity of classes between the candidate label set and the non-candidate label set.

By considering the similarities of Eqs. (3) and (4) simultaneously, we can obtain the following class associative loss (CAL) function:

whereis a trade-off parameter that balances two different terms.is the absolute value operator given. The first term in Eq. (5) means that for each sample, the class-wise embeddings in its candidate label set should be pulled as close as possible. In the meanwhile, the second term implies that the class-wise embeddings between the candidate label set and the non-candidate label set should be pushed as far away as possible, and in the ideal situation, their class-wise embeddings should be orthogonal, i.e.. By minimizing the loss function, we can obtain a model that is more suitable for the IDPLL setting.

SECTION: 3.4.Prototype Discriminative Loss based on Class-Wise Embedding

In IDPLL, we can distinguish the labels between the candidate label set and the non-candidate label set easily, however, it becomes more difficult to identify which label in the candidate label set is the ground-truth label, as the candidate labels are similar to each other, bringing more label ambiguity. Therefore, to tackle this negative side of IDPLL and identify the ground-truth label, we use the global information of samples to guide the model’s disambiguation. Therefore, we first construct prototypes for each class,

whereis the-th class prototypes anddenotes thenormalization operator. To ensure the quality of the class prototypes, we only select the class with the highest model output probability in the candidate label set as the high-confidence label of each sample and add the corresponding class-wise embedding into the class prototype.

Similar to Eq. (3) and Eq. (4), we construct the similarity relationships between class-wise embeddings and class prototypes as follows:

wherein Eq. (7) represents the similarity between the class-wise embedding of the class with the highest model prediction in the candidate label set and the corresponding class prototype. Meanwhile,in Eq. (8) denotes the average similarity between this selected class-wise embedding and all other class prototypes. By combing Eq. (7) and Eq. (8), we have the following prototype discriminative loss (PDL) function:

whereis the trade-off parameter that balances the two different prototype level terms. By minimizing Eq. (9), we can guide the model training through class prototypes, i.e., the most reliable class predicted by the model should be as close as possible to the corresponding class prototype, while this reliable class should be far away from other class prototypes, and in the most ideal case, their similarity relationship should be 0. By utilizing the global information of class prototypes, we can effectively improve the discriminative performance of the model and select the ground-truth label from the candidate label set.

75.51±0.28%

75.77±0.09%

78.36±0.19%

78.18±0.12%

86.22±0.08%

68.60±0.10%

SECTION: 3.5.Overall Objective

Considering the low quality of class prototypes obtained in the early stages of model training, it is difficult to ensure the effectiveness of label disambiguation. Therefore, we divide the model training into two stages. In the first stage, our training uses the classification loss and class associative loss which aims to learn a model more suitable for IDPLL. The training objective of the first stage can be written as follows:

After training forepochs, we add the prototype discriminative loss into the training objective to further improve the model’s disambiguation performance. The objective function of the second stage can be summarized as follows:

whereandare two trade-off parameters. The overall pseudo-code of our method is summarized inAlgorithm 1.

SECTION: 4.EXPERIMENTS

SECTION: 4.1.Experimental Setting

To demonstrate the effectiveness of our method, we conducted comparisons on several challenging data sets with at least 100 classes. To be specific, we conducted experiments on two common image data sets including CIFAR-100(Krizhevsky,2009), CIFAR-100H(Krizhevsky,2009)and four fine-grained(Wei et al.,2022; Zhao et al.,2017)image data sets including CUB-200-2011(Wah et al.,2011), Stanford Cars(Krause et al.,2013), FGVC Aircraft(Maji et al.,2013), Stanford Dogs(Khosla et al.,2011)which are more easily to cause label ambiguity. Table1records the detailed information of all the data sets, where Avg. CLs represent the average number of candidate labels per sample. For data sets CIFAR-100 and CIFAR-100H, the image size was set to 3232, while for fine-grained data sets, we resized the images to 224224. We employed the IDPLL noisy label generation method proposed by VALEN(Xu et al.,2021)to generate instance-dependent noisy labels. Note that for CIFAR-100H, noisy labels only appear in other subclasses that belong to the same superclass as the ground-truth label.

Total

To demonstrate the effectiveness of the proposed method, we compared our method with 12 methods including 6 IDPLL methods and 6 PLL methods.IDPLL methods: (i) DIRK(Wu et al.,2024), a self-distillation based label disambiguation method. (ii) NEPLL(He et al.,2023), a normalized entropy guided sample selection method. (iii) POP(Xu et al.,2023), a method that progressive purifies candidate label set and refines classifier. (iv) IDGP(Qiao et al.,2023), a generation method that models the candidate label generation process. (v) ABLE(Xia et al.,2022), a contrastive learning-based framework that uses ambiguity-induced positives selection method. (vi) VALEN(Xu et al.,2021), a label enhancement guided latent label distribution recovery method.PLL methods: (i) PICO(Wang et al.,2022a), a method that combines PLL and contrastive learning for the first time. (ii) CR-DPLL(Wu et al.,2022), a consistency regularization label disambiguation method. (iii) LWS(Wen et al.,2021), a method that uses leveraged weighted loss to balance candidate label set and non-candidate label set. (iv) PRODEN(Lv et al.,2020), a method that progressively identifies the
ground-truth label during the self-training procedure. (v) RC(Feng et al.,2020), a risk-consistent weighting method. (vi) CC(Feng et al.,2020), a classifier-consistent that uses a transition matrix. The parameters of all methods were set according to their original papers.

For fair comparisons, we employed ResNet-18(He et al.,2016)as the backboneon data sets CIFAR-100 and CIFAR-100H, while using a ResNet-34(He et al.,2016)pre-trained on ImageNet(Deng et al.,2009)as the backboneon fine-grained data sets for all methods. We used ML-Decoder(Ridnik et al.,2023)as our class-wise encoder. Stochastic gradient descent (SGD) was used as the optimizer with a momentum of 0.9 and all methods were trained for 500 epochs. We selected learning rate in {0.5, 0.1, 0.05, 0.01, 0.005, 0.001} and weight decay in {0.001, 0.0005, 0.0001} respectively. Additionally, we applied the cosine annealing learning rate schedule for all methods. The batch sizes of all data sets were set to 128. We repeated the experiments three times under the same random seeds and recorded the mean accuracy and standard deviation. As for our method, parametersandwere selected from, whileandwere selected from. Thewas set to 250, which is half of the total training epochs. The length of each class-wise embeddingwas set to 512 on all data sets. Following DIRK(Wu et al.,2024), we used the same weak augmentation and strong augmentation for our method.

SECTION: 4.2.Experimental Results

Table2reports the classification accuracies of all methods on two common data sets and four fine-grained data sets. According to Table2, our method ranks first in all benchmark data sets when compared with 6 PLL methods and 6 IDPLL methods. It is worth noting that there is only minor difference in classification accuracies between the previous PLL and IDPLL methods on common data sets like CIFAR-100 and CIFAR-100H. While our method improves classification accuracy from 74.40±0.18% to 75.51±0.28% on the CIFAR-100 data set compared to the best previous method. On fine-grained data sets, the performance gaps between PLL and IDPLL methods are significant. PLL methods generally fail to achieve satisfactory accuracy because all labels in the fine-grained data sets belong to the same superclass, which is more challenging. Consequently, IDPLL methods tailored to this setting often outperform the conventional PLL methods. Our method CEL constantly excels on these fine-grained data sets, improving classification accuracy from 66.60±1.07% to 68.60±0.10% on the data set CUB200 and from 75.97±0.29% to 78.18±0.12% on the data set DOGS120, compared to the best previous method.

Table3reports win/tie/loss counts of our method CEL against each comparing method based on the pairwise t-test at 0.05 significance level. As shown in Table3, on common data sets, our method CEL significantly outperforms the comparing methods in 83.3% (20/24). While on fine-grained data sets, our method significantly outperforms the comparing methods in 95.8% (46/48). Furthermore, our method achieves superior performance than all comparing methods except DIRK on fine-grained data sets. Considering all benchmark data sets, our method significantly outperformed the comparing methods in 91.7% (66/72), demonstrating the effectiveness of our method.

Fig.4shows the classification accuracy curves of all methods on data set CUB200. As shown in Fig.4, compared to other methods, our method CEL maintains a relatively fast learning speed in the early stages, only slightly slower than POP, demonstrating that our class associative loss can enhance the model’s learning speed. Moreover, CEL achieves the best classification accuracy in the later stages of training, proving that our prototype discriminative loss further improves the model’s disambiguation performance.

AVERAGE

75.51±0.28%

75.77±0.09%

78.36±0.19%

78.18±0.12%

86.22±0.08%

68.60±0.10%

77.11%

SECTION: 4.3.Further Analysis

In Table4, we conduct ablation studies on all benchmark data sets to demonstrate the necessity of each component in our method. The first row represents our method with only a classification loss, the second row represents our method with the classification loss and our proposed class associative loss, i.e.,, and the third row represents our method with the classification loss, class associative loss, and our prototype discriminative loss, i.e.,. According to the results in Table4, both the class associative lossand prototype discriminative losscan improve the model’s classification performance. To be specific, the class associative loss improves the classification accuracy by an average of 1.22% on six data sets, and the prototype discriminative lossloss also promotes the classification accuracy by an average of 0.71%. Therefore, incorporating both terms into the model is the optimal choice.

Fig.5shows the classification accuracy of our method CEL on benchmark data sets CIFAR-100 and CUB200 under different parameter settings. Fig.5(a), (b), (c) and (d) correspond to the parameters,,, and, respectively. To be specific,andcontrol the weights of the class associative loss and the prototype discriminative loss, whileandcontrol the relative importance of the pull close and push away components within each loss. As illustrated in Fig.5, whenis set to 0.5,to 1, andto 1, the model achieves the best classification performance. Specifically, whenis set to 1 and 2, our method achieves the highest classification accuracy on data sets CUB200 and CIFAR-100, respectively. Therefore, setting,, andto 0.5, 1, and 1, and choosingfrom {1, 2} are the suggested parameters for our method.

We conduct experiments on the data sets CIFAR-100 and CUB200 to verify the impact of different lengths of class-wise embeddingon model classification performance. To be specific, we selectin. As shown in Fig.6, for data set CIFAR-100, which has smaller image sizes (), the classification accuracy of the model is higher when the length of the class-wise embedding is less than or equal to 512. This is because excessively large embeddings can dilute important features in data sets with smaller feature dimensions, which reduces classification performance. Conversely, for the data set CUB200, which has larger image sizes (), the classification accuracy is higher when the length of the class-wise embedding is greater than or equal to 512, this is because for data sets with larger feature dimensions, excessively small embeddings can compress important feature information, leading to a decline in performance. Therefore, considering both cases, setting the class-wise embedding length to 512 is a good choice.

SECTION: 5.CONCLUSION

In this paper, we have presented a novel method named CEL to address the IDPLL problem. For the first time, we realize that IDPLL is a mixed blessing with both positive side and negative side. We, therefore, propose to construct the class-wise embeddings to explore the relationships among the candidate labels and the non-candidate labels. To leverage the positive side of IDPLL, we introduced the class associative loss to learn representations that are more suitable for IDPLL. This is achieved by leveraging the similarity among labels within the candidate label set and the differences between the candidate label set and the non-candidate label set through class-wise embeddings. To mitigate the negative side of IDPLL, i.e., identifying the ground-truth label in the candidate set becomes more challenging, we constructed prototype discriminative loss to guide the model’s disambiguation process using class prototypes which include global sample information. Extensive experiments on both common and fine-grained data sets demonstrate that our method significantly outperforms twelve state-of-the-art PLL and IDPLL methods. Moreover, compared with previous methods, our method converges faster in the early stages of model training, while produces the highest classification accuracy in the later training stages.

SECTION: References