SECTION: CreatiLayout: Siamese Multimodal Diffusion Transformer forve-to-Image Generation
Diffusion models have been recognized for their ability to generate images that are not only visually appealing but also of high artistic quality.
As a result, Layout-to-Image (L2I) generation has been proposed to leverage region-specific positions and descriptions to enable more precise and controllable generation.
However, previous methods primarily focus on UNet-based models (e.g., SD1.5 and SDXL), and limited effort has explored Multimodal Diffusion Transformers (MM-DiTs), which have demonstrated powerful image generation capabilities.
Enabling MM-DiT for layout-to-image generation seems straightforward but is challenging due to the complexity of how layout is introduced, integrated, and balanced among multiple modalities.
To this end, we explore various network variants to efficiently incorporate layout guidance into MM-DiT, and ultimately present SiamLayout.
To Inherit the advantages of MM-DiT, we use a separate set of network weights to process the layout, treating it as equally important as the image and text modalities.
Meanwhile, to alleviate the competition among modalities, we decouple the image-layout interaction into a siamese branch alongside the image-text one and fuse them in the later stage.
Moreover, we contribute a large-scale layout dataset, named LayoutSAM, which includes 2.7 million image-text pairs and 10.7 million entities. Each entity is annotated with a bounding box and a detailed description.
We further construct the LayoutSAM-Eval benchmark as a comprehensive tool for evaluating the L2I generation quality.
Finally, we introduce the Layout Designer, which taps into the potential of large language models in layout planning, transforming them into experts in layout generation and optimization.
Our code, model, and dataset will be available at.

SECTION: Introduction
Text-to-image (T2I) generationhas been widely applied and deeply ingrained in various fields thanks to the rapid advancement of diffusion models.
To achieve more controllable generation, Layout-to-image (L2I) has been proposed to generate images based on layout conditions consisting of spatial location and description of entities.

Recently, multimodal diffusion transformers (MM-DiTs)have taken text-to-image generation to the next level. These models treat text as an independent modality equally important as the image and utilize MM-Attentioninstead of cross-attention for interaction between modalities, thus enhancing prompt following.
However, previous layout-to-image methodsmainly fall into UNet-based architecturesand achieve layout control by introducing extra image-layout fusion modules between the image’s self-attention and the image-text cross-attention.
While enabling MM-DiT for layout-to-image generation seems straightforward, it is challenging due to the complexity of how layout is introduced, integrated, and balanced among multiple modalities.
To this end,, fully unleashing their capabilities for high-quality and precisely controllable generation.

To address this issue, we explore various network variants and ultimately propose SiamLayout.
Firstly,, equally important as the image and text modalities. More specifically, we employ a separate set of transformer parameters to process the layout modality. During the forward process, the layout modality interacts with other modalities via MM-Attention and maintains self-updates.
Secondly,. By independently guiding the image with text and layout and then fusing them at a later stage, we alleviate competition among modalities and strengthen the guidance from the layout.

To this end, a high-quality layout dataset composed of image-text pairs and entity annotations is crucial for training the layout-to-image model.
As shown in, the closed-set and coarse-grained nature of existing layout datasets may limit the model’s ability to generate complex attributes (e.g.color, shape, texture).
Thus, we construct an automated annotation pipeline and contribute a large-scale layout dataset derived from the SAM dataset, named LayoutSAM. It includes 2.7M image-text pairs and 10.7M entities. Each entity includes a spatial position (i.e.bounding box) and a region description.
The descriptions of images and entities are fine-grained, with an average length of 95.41 tokens and 15.07 tokens, respectively.
We further introduce the LayoutSAM-Eval benchmark to provide a comprehensive tool for evaluating layout-to-image generation quality.

To support diverse user inputs rather than just bounding boxes of entities, we turn a large language model into a layout planner named LayoutDesigner.
This model can convert and optimize various user inputs such as center points, masks, scribbles, or even a rough idea, into a harmonious and aesthetically pleasing layout.

Through comprehensive evaluations on LayoutSAM-Eval and COCO benchmarks, SiamLayout outperforms other variants and previous SOTA models by a clear margin, especially in generating entities with complex attributes, as illustrated in. For layout planning, LayoutDesigner shows more comprehensive and specialized capabilities compared to baseline LLMs.

SECTION: Related Work
Text-to-image generationhas emerged as a promising application due to its impressive capabilities.
Recently, studies such as SD3, SD3.5, FLUX.1, and Playground-v3have advanced the multimodal Diffusion Transformer architecture (MM-DiT), elevating text-to-image generation to the next level. MM-DiT significantly enhances text understanding by treating text as an independent modality, equally important as the image, and replacing traditional cross-attention with MM-Attention for modal interaction.

To achieve more precise and controllable generation, layout-to-image generationhas been proposed to generate images based on layout guidance, which includes several entities. Each entity comprises a spatial location and a region description.
However, previous methods primarily focused on UNet-based architectures, and enabling MM-DiT for layout-to-image generation is challenging due to the complexity of introducing, integrating, and balancing the layout among multiple modalities.
In this paper, we focus on exploring winner solutions for incorporating layout into MM-DiT, thereby fully unleashing its capabilities and achieving precisely controllable generation.

Layout datasets typically consist of image-text pairs with entity annotations.
A common typeoriginates from COCO, featuring images with global descriptions and entities marked by bounding boxes and brief descriptions.
Although some effortexpands descriptions using Large Language Models (LLMs) or Vision-Language Models (VLMs), they are still limited due to the close-set nature. Rannicollects large-scale text-image pairs from LAIONand WebVision, moving towards an open-set layout dataset.
However, entity descriptions remain coarse-grained and lack complex attributes.
In this paper, we present an annotation pipeline and introduce a large-scale layout dataset containing 2.7M image-text pairs and 10.7M detailed entity annotations.

Layout generationrefers to the multimodal task of creating layouts for flyers, magazines, UI interfaces, or natural images.
Some studieshave explored using LLMsto generate layouts based on textual descriptions, which then guide the generation of images. In this paper, we further enhance the capabilities of LLMs for generation and optimization as well as supporting user input of different granularities.

SECTION: Methodology
SECTION: Preliminaries
Latent diffusion models perform the diffusion process in the latent space, which consists of a VAE, text encoders, and either a UNet-based or transformer-based noise prediction model. The VAE encoderencodes imagesinto the latent space, while the VAE decoderreconstructs the latent back into images. The text encoders, such as CLIPand T5, project tokenized text prompts into text embeddings. The training objective is to minimize the following LDM loss:

whereis time step uniformly sampled from. The latentis obtained by adding noise to, with the noisesampled from the standard normal distribution.

SD3/3.5, FLUX.1, and Playground-v3instantiate noise prediction using MM-DiT, which uses two independent transformers to handle text and image embeddings separately. Unlike previous diffusion models that process different modalities through cross-attention, MM-DiT concatenates the embeddings of the image and text for the self-attention operation, referred to as MM-Attention:

MM-DiT treats image and text as equally important modalities to improve prompt following.
In this paper, we explore incorporating layout into MM-DiTs, unleashing their potential for high-quality and precise L2I generation.

SECTION: Layout-to-Image Generation
Layout-to-image generation aims at precise and controllable image generation based on the instruction, which consists of a global-wise prompt conditionand a region-wise layout condition, denoted as:

The layout condition includes information forentities, each consisting of two parts: region captionand spatial location, denoted as:

In this work, we use bounding boxes to represent spatial locations, consisting of the coordinates of the top-left and bottom-right corners.

Image tokensare derived by patchifying the latent, and tokensof global captionare obtained from the text encoder, denoted as.
We denote the layout tokens as. Inspired by GLIGEN, eachis obtained from the layout encoder in:

Fourier refers to the Fourier embedding, [·, ·] denotes concatenation across the feature dimension, and MLP is a multi-layer perception.

MM-DiTallows the two modalities to interact through the following MM-Attention:

where [·, ·] denotes concatenation across the tokens dimension,,,; and,,are the weight matrices for the query, key, and value linear projection layers for image tokens, respectively.
Tokensof the global captionare handled by the same paradigm asbut with their own weights.andare the image and caption tokens after interaction.
To this end, the next critical step is to incorporate the layout tokens.
We explore three variants of network designs that incorporate layout tokens, as shown in.

Based on the fundamental idea of previous L2I methodsintroducing layout conditions in UNet-based architectures, we design extra image-layout cross-attention to incorporate layout into MM-DiT, defined as the Layout Adapter. Formally, it is represented as:

whereandare the key and values matrices from the layout tokens.andare the corresponding weight matrices.is the identical query as in.
In this way, guidance from the layout is introduced intowith fewer parameters. However, due to layout conditions not playing an equal role to the global caption condition, it may diminish the impact of layout guidance on the generation process and lead to suboptimal results.

To emphasize the importance of layout conditions, we extend the core philosophy of MM-DiT, treating layout as an independent modality equally important as the image and global caption. We employ a separate set of transformer parameters to process layout tokens and design a novel-Attention for the interaction among these three modalities:

where,andare the query, key and values matrices from the layout tokens.,andare the corresponding independent weight matrices.
We concatenate these three modalities in the token dimension and then facilitate interaction among them through self-attention. During the generation process, the layout condition acts as an independent modality, constantly interacting with other modalities and maintaining self-updates.
Although this design seems promising, we found that in the attention map of-Attention, the similarity between layout and image is much lower than that between the caption and image (as shown in(a)), resulting in the layout having much less influence on the image compared to the global caption. We refer to this phenomenon as “modality competition”, which can be attributed to the fact that layout, as a new modality, has not been pre-trained and aligned on large-scale paired datasets like the global caption and image.

Finally, in order to retain the advantages of MM-Attention in multimodal interaction while mitigating competition among them, we propose a new layout fusion network named SiamLayout. As shown in, we decouple-Attention into two isomorphic MM-Attention branches,i.e.siamese branches, to handle image-text and image-layout interactions independently and simultaneously.
The MM-Attention between image and layout can be formally denoted as:

where,andare the new query, key, and value matrices obtained from the image token, with weight matrices different from those in.
The final image tokens are the fusion of text-guided and layout-guided image tokens:.
Since the guidance from the layout and global caption has been decoupled—being independent in space and parallel in time—the issue of modality competition is significantly mitigated. As shown in(b), in the attention map of image-layout MM-Attention, the similarity between layout and image increases continually during training as layout takes a dominant role here.

In this paper, SiamLayout is chosen as the network that incorporates layout into MM-DiT, and our primary experiments are conducted on it.

We freeze the pre-trained model and only train the newly introduced parametersusing the following loss function:

Here, we employ two strategies to accelerate the convergence of the model:i@) Biased sampling of time steps: Since layout pertains to the structural content of images, which is primarily generated during the larger time steps, we sample time steps with a 70% probability from a normal distributionand with a 30% probability from.ii@) Region-aware loss: We enhance the model’s focus on areas specified by the layout by assigning greater weight to the region lossassociated with the regions localized by the bounding boxes in the latent space. The updated loss is:

wheremodulates the importance of.
During the inference phase, we perform layout-conditioned denoising only in the first 30% of the steps.

SECTION: Layout Dataset and Benchmark
As there is no large-scale and fine-grained layout dataset explicitly designed for layout-to-image generation, we collect 2.7 million image-text pairs with 10.7 million regional spatial-caption pairs derived from the SAM Dataset, named LayoutSAM.
We design automatic schemes and strict filtering rules to annotate layout and clean noisy data, with the following five parts:

i@) Image Filtering: We employ the LAION-Aesthetics predictorto curate a high visual quality subset from SAM, selecting images in the top 50% of aesthetic scores.

ii@) Global Caption Annotation: As the SAM dataset does not
provide descriptions for each image, we generate detailed descriptions using a Vision-Language Model (VLM). The average length of the captions is 95.41 tokens.

iii@) Entity Extraction: Existing SoTA open-set grounding modelsprefer to detect entities through a list of short phrases rather than dense captions. Thus, we utilize a Large Language Modelto derive brief descriptions of main entities from dense captions via in-context learning. The average length of the brief descriptions is 2.08 tokens.

iv@) Entity Spatial Annotation: We use Grounding DINOto annotate bounding boxes of entities and design filtering rules to clean noisy data. Following previous work, we first filter out bounding boxes that occupy less than 2% of the total image area, then only retain images with 3 to 10 bounding boxes.

v@) Region Caption Recaptioning: At this point, we have the spatial locations and brief captions for each entity. We use a VLMto generate fine-grained descriptions with complex attributes for each entity based on its visual content and brief description. The average length of these detailed descriptions is 15.07 tokens.

The LayouSAM-Eval benchmark serves as a comprehensive tool for evaluating L2I generation quality collected from a subset of LayouSAM. It comprises a total of 5,000 layout data.
We evaluate L2I generation quality using LayouSAM-Eval from two aspects:

This aspect is evaluated for adherence to spatial and attribute accuracy via VLM’sVisual Question Answering (VQA). For each entity, spatially, the VLM evaluates whether the entity exists within the bounding box; for attributes, the VLM assesses whether the entity matches the color, text, and shape mentioned in the detailed descriptions.

This aspect scores based on visual quality and global caption following, across multiple metrics including recently proposed scoring models like IR scoreand Pick score, as well as traditional metrics such as CLIP, FIDand ISscores.
For more details on the proposed dataset and benchmark, please refer to the supplementary materials.

SECTION: Layout Designer
Recent studieshave revealed that LLMsexhibit expertise in layout planning due to their extensive training on multiple domains. Inspired by this, we further tame a LLM into a more comprehensive and professional layout designer.
As shown in,is capable of executing two types of layout planning based on inputs of varying granularity:

For user inputs with only a global caption,designs from scratch based on the caption to determine which entities compose the layout and generates appropriate bounding boxes for these entities; for cases where the description and centroid coordinates (,) of each entity are provided,designs harmonious bounding boxes based on this information.

For entities provided with detailed spatial information, such as masks and scribbles, we first transform them into bounding boxes according to predefined rules, then employto further optimize the layout based on the global caption and the descriptions of the entities, including additions, deletions, and modifications of the bounding boxes.

To enhance the expertise of, we construct 180,000 paired layout design data from LayoutSAM and fine-tune the pre-trained LLMusing LoRAwith cross-entropy loss.

SECTION: Experiments
SECTION: Experimental Details
We conduct experiments on two types of datasets: the fine-grained open-set LayoutSAM and the coarse-grained closed-set COCO 2017.
For LayoutSAM, we train on 2.7 million image-text pairs with 10.7 million entities and conduct evaluations on LayoutSAM-Eval, which includes 5,000 prompts with detailed entity annotations. We measure generation quality using the metrics outlined in.
For COCO, following previous work, we filter out bounding boxes smaller than 2% of the total image area and images containing dense bounding boxes and crowds, resulting in 61,002 training images and 2,565 validation images.
We use the YOLO-v11-xto validate the model’s layout adherence by detecting objects in the generated images and then calculating AP,, and AR against the ground truth. We use FID, CLIP score, and IS to measure the global image quality.

We conduct experiments on the T2I-CompBenchand evaluate image quality from five aspects: spatial, color, shape, texture, and numeracy.

We construct 180,000 training sets based on the LayoutSAM training set for three types of user input and layout pairs: caption-layout pairs, center point-layout pairs, and suboptimal layout-layout pairs. Similarly, we construct 1,000 validation sets for each of these tasks from LayoutSAM-Eval.

We employ experiments on SD3-medium, and the extra parameters introduced by SiamLayout amount to 1.28B. The training resolution for the LayoutSAM dataset is, andfor COCO. We utilize the AdamW optimizer with a fixed learning rate of 5e-5 and train the model for 600,000 iterations with a batch size of 16. We train SiamLayout with 8 A800-40G GPUs for 7 days. The value ofis set to 2. LayoutDesigner is fine-tuned on Llama-3.1-8B-Instructfor one day using one A800-40G GPU.

SECTION: Evaluation on Layout-to-Image Generation
presents the quantitative results of SiamLayout on the fine-grained open-set LayoutSAM-Eval, including metrics of region-wise quality and global-wise quality. SiamLayout not only surpasses the current SOTA in terms of spatial response but also exhibits more precise responses in attributes such as color, texture, and shape. By fully unleashing the power of MM-DiT, SiamLayout also demonstrates a dominant advantage in overall image quality.
This is further confirmed by the qualitative results in, showing that SiamLayout achieves more accurate and aesthetically appealing attribute rendering in the regions localized by the bounding boxes, including the rendering of shapes, colors, textures, text, and portraits.

We train and evaluate SiamLayout on COCO to confirm its generalization in coarse-grained closed-set layout-to-image generation, as shown in. In terms of image quality, SiamLayout outperforms previous methods by a clear margin on CLIP, FID, and IS metrics, thanks to the tailored framework that unleashes the capabilities of MM-DiT. In terms of spatial positioning response, SiamLayout is slightly inferior to InstanceDiff. We attribute this to two factors:i@) The training dataset of InstanceDiff is a more fine-grained COCO dataset, with per-entity fine-grained attribute annotations;ii@) InstanceDiff generates each entity separately and then combines them, achieving more precise control at the cost of increased time and computational resources.

SECTION: Evaluation on Text-to-Image Generation
To further validate the impact of incorporating layout on text-to-image generation, we conduct experiments on the T2I-CompBench. For the prompts, we first use LLM to plan the layout, which is then used to generate images via SiamLayout.reveals that, by introducing layout to provide further guidance signals for image generation, SD3 has seen a significant improvement in spatial adherence (from 32.00 to 47.36). Additionally, the benefits of layout are also reflected in the improved adherence to prompts regarding color, shape, texture, and the number of objects.

SECTION: Evaluation on Layout Planning
To validate the capability of layout generation and optimization, we compare LayoutDesigner fine-tuned on Llama3.1 with the latest LLMs, as presented in.
Accuracy measures the correctness of the generated bounding boxes, including ensuring that the coordinates of the top-left corner are less than those of the bottom-right corner and that the bounding box does not exceed the image boundaries. Quality refers to the IR score of the image generated according to the planned layout, which is used to reflect the rationality and harmony of the layout.
In terms of format accuracy, LayoutDesigner shows significant improvement over the vanilla Llama and clearly outperforms the previous SOTA. Additionally, as the LayoutDesigner contributes more aesthetically pleasing layouts, the generated images possess higher quality.further confirms this, showing that layouts generated by Llama often fail to meet formatting standards or miss key elements, while those generated by GPT4-Turbo often violate fundamental physical laws (e.g.overly small objects).
In contrast, images generated from layouts designed by LayoutDesigner exhibit better quality as the layouts are more harmonious and aesthetically pleasing.

SECTION: Ablation Study
We explore three network variants aimed at integrating layout guidance into MM-DiT.illustrates the accuracy of layout adherence of these designs. Compared to the vanilla SD3, Layout Adapter enhances the model’s adherence to spatial locations and attributes by introducing cross-attention between the image and layout. However, as shown in, it falls short when dealing with complex and fine-grained color, quantity, and texture requirements. We attribute this to the fact that the layout is not considered an independent modality equally important as the global caption and image, which limits the potential of layout guidance.
The initial intention behind designing-Attention is to make layout, image, and caption play an equal role.
However, due to the competition among these modalities in the attention map of-Attention, layout modality consistently is at a disadvantage, which is reflected in both quantitative and qualitative results as lower layout responsiveness.
SiamLayout decouples-Attention into two independent and parallel MM-Attention branches: image-text and image-layout. This design allows each branch to play a significant role without interfering with each other, jointly contributing to precise responses to spatial locations and complex attributes.

In, we explore the impact of two strategies introduced during training on SiamLayout: biased time step sampling and region-aware loss. With the region-aware loss, the model focuses more on the areas localized by the layout, accelerating the model’s convergence.
In addition, layout predominantly guides structural content, which is mainly generated at larger time steps. Thus, sampling larger time steps with a higher probability (i.e.biased time step sampling) also effectively speeds up the model’s convergence.

SECTION: Conclusion
We presented SiamLayout, which treats layout as an independent modality and guides image generation through an image-layout branch that is siamese to the image-text branch. SiamLayout unleashes the power of MM-DiT to achieve high-quality, accurate layout-to-image generation, and significantly outperforms previous work in generating complex attributes such as color, shape, and text.
Additionally, we introduced LayoutSAM, a large-scale layout dataset with 2.7M image-text pairs and 10.7M entities, along with LayoutSAM-Eval to assess generation quality.
Finally, we proposed LayoutDesigner, which tames a large language model into a professional layout planner capable of handling user inputs of varying granularity.

We introduce a large language model for layout planning, which brings extra computation costs. Integrating layout planning with layout-to-image generation into an end-to-end model is an important direction for future research. Additionally, the automatic annotation pipeline introduces noisy data mainly caused by the object detection model, so its impact on the performance of the layout-to-image model needs to be further studied.

SECTION: References
SECTION: More details on Datasets and Benchmarks
SECTION: LayoutSAM Dataset and Benchmark
SECTION: Layout Planning Dataset and Benchmark
SECTION: More analysis on modal competition
SECTION: More qualitative results