SECTION: Heterogeneous Random Forest

Random forest (RF) stands out as a highly favored machine learning approach for classification problems. The effectiveness of RF hinges on two key factors: the accuracy of individual trees and the diversity among them. In this study, we introduce a novel approach called heterogeneous RF (HRF), designed to enhance tree diversity in a meaningful way. This diversification is achieved by deliberately introducing heterogeneity during the tree construction. Specifically, features used for splitting near the root node of previous trees are assigned lower weights when constructing the feature sub-space of the subsequent trees. As a result, dominant features in the prior trees are less likely to be employed in the next iteration, leading to a more diverse set of splitting features at the nodes.
Through simulation studies, it was confirmed that the HRF method effectively mitigates the selection bias of trees within the ensemble, increases the diversity of the ensemble, and demonstrates superior performance on datasets with fewer noise features.
To assess the comparative performance of HRF against other widely adopted ensemble methods, we conducted tests on 52 datasets, comprising both real-world and synthetic data. HRF consistently outperformed other ensemble methods in terms of accuracy across the majority of datasets.

Keywordsâ€”Ensemble, Random forest, Feature sub-space, Diversity, Noise feature, Selection bias

SECTION: 1Introduction

Random forest (RF,[4]) is one of the most prevalent machine learning techniques for classification and regression problems. It is a tree-based ensemble method that aggregates the results of numerous decision trees to generate prediction outcomes. RF employs a strategy to induce diversity among classifiers. This is accomplished by choosing a random subset of candidate features from the pool of available features and then splitting a node of the tree based on the most optimal variable within this subset.

The performance of ensemble methods is influenced by two primary factors: the individual strength of the base learners and the correlation between them. In other words, if the trees themselves are effective classifiers, the forest will also demonstrate superior performance. Additionally, if the trees are more heterogeneous and diverse, the forest is likely to exhibit enhanced performance. Consequently, numerous studies have explored modifications to RF, with the objective of improving accuracy, diversity, or a combination of both.

Several studies have enhanced the performance of ensemble models by developing more sophisticated trees. Specifically, some approaches have focused on selecting only the most relevant variables or eliminating redundant ones. This can be seen as a feature selection technique aimed at improving the accuracy of individual trees([24],[17],[6],[32],[9],[31]). In contrast, other research has explored the construction of oblique trees to simultaneously boost accuracy and diversity. These methods involve transforming features, such as through linear combinations, to create splits that are not aligned with the original feature axes[30]. For instance,[8]and[28]proposed rotated trees based on techniques like LDA (Linear Discriminant Analysis), CLDA (Canonical LDA), and CCA (Canonical Correlation Analysis). However, these rotation-based methods often sacrifice the interpretability that is one of the key strengths of tree-based models.

Instead of focusing solely on the performance of individual trees, some approaches emphasize increasing the diversity among trees. A common strategy involves introducing additional randomness to reduce the correlation between trees. For instance,[13]demonstrated improved classification performance by enhancing diversity through bootstrapping at the node level. Similarly,[12]employed a method that selects candidate split points randomly, further promoting diversity across the ensemble.

This paper proposes a novel ensemble method called heterogeneous random forest (HRF). Tree diversity was induced by regulating the variables employed for tree splitting during the training phase. To accomplish this, we employed a weighted sampling approach to choose candidate features. Notably, weighted sampling has also been applied in other previous works, such as those by[34],[5],[19]and[25].

The remainder of the article is organized as follows: Section2.1provides an overview of random forest (RF). Sections2.2and2.3provide an introduction to the fundamental concepts of HRF.
The HRF algorithm is outlined in Section2.4with the aid of a simplified illustrative example.
In Section2.5, a measure for assessing the diversity of trees in the ensemble was devised and introduced.
In Section3, we conduct an experiment on a simulation dataset comprising various input features to gain insight into the characteristics of HRF. Section4presents the empirical results, including the accuracy assessment. The results are then compared with those obtained from other ensemble methods. The following ensemble methods were considered: bagging[3], random forest[4], extremely randomized trees[12], gradient boosting[11], XGBoost[7], and CatBoost[10].
Section5concludes this paper.

SECTION: 2Method

SECTION: 2.1Random forest

Random forest (RF) is a type of bagging method that employs bootstrap sampling for each tree. The distinction between bagging and RF lies in the random selection ofcandidate features for splitting. At each node of the tree, a new random feature subset is constructed, and the optimal split is selected based on the goodness of split within the subset. RF employs this strategy to induce diversity among individual trees, introducing additional randomness. The detailed algorithm is described in Algorithm1. In general, the most commonly used value for hyper-parameterareandfor classification and regression, respectively.

: training set withinstances,features and a target variable

: the number of trees in an ensemble

: the number of candidate features to be selected at each node

: the trained forest

SECTION: 2.2Basic idea of new method

The new method employs a memory system that retains the structural characteristics of previous trees. This stored information is then leveraged to generate more diverse future trees.

Because of the hierarchical nature of decision trees, splits at shallow depths (near root node) have broad implications for the entire space, while splits at deeper depths exert more localized effects and have a less impact on the overall tree structure[2].

Therefore, to effectively enhance diversity, it is more crucial to manage the features selected for splitting near the root node than those at the lower levels.
We intend to ensure that features picked close to the root node in earlier trees are less frequently selected in subsequent trees.
This can be achieved through weighted sampling, incorporating information from previous trees when selecting the feature subset for the next tree.
This procedure differs from RF, which apply simple random sampling without considering the results of previous trees.

SECTION: 2.3Feature depth

Let us consider an individual tree within an ensemble. We define a feature depth as the earliest level a feature is used. Formally, the feature depthis defined as

where,,represents the depth of, andis a hyper-parameter for unused features typically set to 1.

To illustrate, consider the case where the first decision tree, denoted by, was built following the structure shown in Figure1(a). Thefeature is selected as the splitting criterion at the root node, thus it is assigned a value of 0 for the feature depth. In the case of features that appear more than once, the smallest number is recorded, that is, the number closest to the root node. It can be observed that thefeature is present in both the first and the third levels of. Consequently, it is assigned a value of.
In this example,is equal to. In the event that bothandare not selected, they will each have a value ofif the parameteris set to. The same process can be applied to calculate the feature depth of the second tree.

SECTION: 2.4Heterogeneous random forest

A shallower feature depth indicates a more significant feature in the previous tree. Consequently, feature depth can be employed as a selection weight when identifying potential feature sets. This implies that features that were used as influential factors in the preceding tree will be employed with diminished frequency in the subsequent trees.

The example illustrated in Figure1will continue to be used for explanatory purposes.
In Table1(a), the feature depth of the first tree,, is recorded. The next step is to calculate the cumulative depthfor each variable from all of the previous trees, but since this tree is the first one, the cumulative depths as given in Table1(b) are just equal to the tree depths. The feature weights for the second tree,, are determined by dividing the cumulative depth of each feature by the total sum of all cumulative depths of the first tree as in Table1(c).

Moving on to constructing the second tree in Figure1(b), the feature subset for each node of the second tree is sampled by using the weightsin Table1(c). Note that the feature, which was used at the root node of the first tree, does not appear in the second tree at all. In contrast,and, which were not selected in the first tree, both appear near the root node of the second tree.

The next step is to recalculate and update the weights for each variable. As previously indicated, the depths for each variable in the second tree should be recorded (denoted as), and the cumulative depths (denoted as) should then be calculated. However, we intend to use a more refined method rather than simply adding the depths of each tree to obtain the cumulative depth. The depth of the preceding tree is multiplied by the hyper-parameterand then added to the current tree depth. For example, when,receives a cumulative depth of, calculated as.

The hyper-parametercan be interpreted as the memory parameter. It controls the influence of the previous trees, or how much the next tree remembers its predecessors. Ifis equal to, then all information is fully retained. The-th tree is influenced to the same extent by the first tree as it is by the immediate predecessor. As the value ofapproaches, the influence of the preceding trees is rapidly diminished. Only the more recent trees will exert any influence on future ones. The cumulative depth resulting from the-th construction can be expressed as follows:

whereis a vector of feature depths.

Following the calculation of, the updated weights are obtained by Algorithm2, which are then used for the feature sampling weights of the subsequent tree. The hyper-parametercontrols the degree of advantage that non-used features will have. If the value ofis large, features that were not selected in previous trees will have higher weights in subsequent trees.

Finally, the proposed heterogeneous random forest (HRF) algorithm is summarized in Algorithm3.

: Cumulative feature depths

: feature depths

: How much will previous trees be reflected in

: The updated weight

: training set withinstances,features and a target variable

: the number of trees in an ensemble

: the number of candidate features to be selected at each node

:
The degree of influence of past trees on future trees

: The degree of advantage given to the non-used features

SECTION: 2.5New measure of diversity

Our proposed method, HRF, seeks to improve the diversity of trees within an ensemble. In this section, we introduce a new metric to measure tree dissimilarity, aimed at evaluating the diversity of HRF. Specifically, we devised a method to quantify the level of heterogeneity among the various tree structures generated in the ensemble.

The characteristics of a decision tree can be summarized by the feature depth,, which denotes the variable used for splitting at a given depth. We utilize this feature depth to define feature dominance, which can reflect the differences between two trees.

The dominance of a-th feature in-th classifier is defined as

wheredenotes the maximum value of the feature depth of split variables. Unused variables have a value of 0, whereas variables that are employed in proximity to the root node are assigned a relatively high value.
By concatenating the dominance of the-th and-th trees by row,, a table consisting of 2 rows andcolumns is created. We can perform atest of homogeneity with this table.
A largetest statistic means that the feature depths of each variable in the two trees are very different, and a small value means that they are similar.

Figure2presents examples to illustrate the dissimilarity between trees. Trees 1 and 2 exhibit similarity, as do trees 3 and 4.
Table2(a) presents the dominance of each feature, while Table2(b) displays the results of thetest statistic, which measures the similarity between trees.

To facilitate comparison in cases where there are different degrees of freedom, one can transform thetest statistic to a standard normal variable using the Wilsonâ€“Hilferty[33]method. Ultimately, the dissimilarity betweenandis given by the following equation:

whererepresents thetest statistic with degree of freedom.
Finally, Table2(c) shows the dissimilarity values developed in this paper. The dissimilarity values between Trees 1 and 2, as well as between Trees 3 and 4, are relatively small, whereas the values for other pairs are notably larger.

SECTION: 3Simulation Study

The objective of this simulation study is to investigate three key issues.

First, we aim to assess the extent to which HRF has a feature selection bias, as it has been reported that the bagging and RF method exhibits feature selection bias problem, due to the greedy search property of decision trees[21].
Secondly, we seek to confirm whether HRF truly enhances diversity in comparison to bagging and RF.
Thirdly, as HRF assigns feature weights based on feature selection information from preceding trees, there is a potential risk of assigning higher selection weights to noise features. We intend to assess the severity of this problem.

Taking these factors into account, two simulation data were generated. The first simulation generated a dataset containing many features with varying numbers of unique values. The second simulation produced a dataset that included noise features.

The simulation data pertains to binary classification, comprising a total of 1,000 samples. Of these, 70% were employed as the training data set, while the remaining 30% constituted the evaluation data set. The ensemble consisted of 100 individual trees, and the hyper-parameters for tree creation were set to their default values. Theandparameters of HRF were set to 0.5 and 1, respectively. To ensure comprehensive comparison, the aforementioned process was repeated 100 times.

SECTION: 3.1Selection bias

In this experiment, five input features were generated. Thefeature takes integer values between 0 and 128,between 0 and 64,between 0 and 32,between 0 and 16, andbetween 0 and 8, all following discrete uniform distributions. The binary target variable is derived as, whererepresents the median of,denotes the sigmoid function, andrepresent the binary random variable with the given probability. The variableis defined as, indicating that the five features are of equal importance in the classification of.

The feature depthdenotes the feature depth vector at-th tree in an ensemble. If there is a selection bias for features with a large number of unique values, the feature depth of, i.e., will be smallest as it is likely to be selected earlier in the tree. Conversely, features with fewer unique values will be larger. In the event that the ensemble consists of a total of 100 trees, then 100 feature depth values will be accumulated.

Figure3shows box-plots representing the feature depth values for each feature across different ensemble methods.
It can be observed that bagging and RF tends to select features with a high number of unique values in the vicinity of the root node, a phenomenon that can be attributed to selection bias. Especially in the case of bagging, a significant bias issue was observed, as it almost always prioritized the selection of thefeature.
In contrast, this bias has been considerably reduced in the case of HRF, as the feature depth values for HRF became more similar across features.

SECTION: 3.2Dissimilarity

We will discuss the results of tree dissimilarity using the same data employed in Section3.1.
In this experiment, we calculated the dissimilarity for the trees within the ensemble in a pairwise manner using equationÂ (5). The final dissimilarity was derived as the average of all possible pairwise dissimilarity values.
This process was repeated 100 times, and the resulting line graph is presented.
As shown in Figure4, we found that, as expected, the dissimilarity of bagging was the lowest, followed by RF and HRF, indicating that the diversity of HRF has improved the most.

SECTION: 3.3Noise features

To investigate the impact of noise features on the HRF method, a case with a total of 30 features was considered. Letn_informativedenotes the number of informative features. The input featureswere generated from discrete uniform distribution between 0 and 4, with the size of 1,000. The target variable is generated by, where.
In the simulation data, the noise feature ratio was increased in increments of 0.1, starting from 0 and reaching 0.9.

Figure5presents box plots showing the accuracy differences between HRF and bagging, as well as between HRF and RF.
A positive value indicates that HRF exhibits higher accuracy.
HRF outperforms bagging when the proportion of noise features is less than 60% of the total features. Conversely, when the proportion of noise features exceeds 60%, HRF experiences a decline in performance. Similarly, HRF outperforms RF when the proportion of noise features is below 40% of the total features; however, when the proportion exceeds 50%, HRF also sees a decrease in performance.

SECTION: 4Empirical Evaluation

In order to investigate the performance of HRF, a series of experiments were conducted. The experiments were based on 52 real or artificial datasets used in other studies or obtained from the UCI data repository (UCI)[22].
TableLABEL:tab:data_description_clfprovides a concise overview of the data. Any missing values were filled using the mean value for numerical features and the mode value for categorical features. As an encoding method for categorical inputs,TargetEncoderwas employed for binary classification, whilePolynomialWrapperwas utilized for multi-class classification to convert them into numerical ones[23].

SECTION: 4.1Classification accuracy

We compared the proposed method to several ensemble methods. The candidate hyper-parameters and their sources are listed up in Table4. The first 3 methods are based on bagging, while the remaining ones are based on boosting. To assess the classification performance, we evaluated the classification accuracy using the results of ensemble voting.

The experimental design is as follows: the data set was randomly split into an 80% training set for fitting and a 20% test set for evaluation. All methods were executed with 100 trees, with each method utilizing its optimal hyper-parameters, as determined through a 5-fold cross-validation process. HRF used the default values associated with RF for tree growth and identified the optimalandthrough a 5-fold cross-validation process. In this process,ranged from 0.0 to 0.9, with an increment of 0.1, whileranged from 1 to the minimum of 10 and(the number of features), with an increment of 1. To ensure statistical significance, the experiments were repeated 50 times and then results are assessed using the Wilcoxon signed-rank test for statistical comparison[29].

TableLABEL:tab:empirical_accuracypresents a comparison of the accuracy between HRF and other ensemble methods. The â€™â€™ symbol indicates that HRF showed statistically more accurate results than the corresponding method, whereas the â€™â€™ symbol indicates less accurate results. The â€™W/T/Lâ€™ at the bottom row represents the number of datasets where HRF was better/equal/worse compared to the respective method.

Table6displays pairwise comparisons among the methods. The values in the table show the number of instances where the method listed vertically is more accurate than the method listed horizontally, with the number in parentheses denoting statistical significance. To illustrate, a value of 35(22) in the first row and second column indicates that RF outperforms the Bagging method in 35 out of 52 datasets, with 22 of those instances being statistically significant. In cases where the accuracy values are the same, a value of 0.5 was assigned.

Table7summarises the ranking of methods according to the results presented in Table6.
The dominance rank is calculated as the difference between the number of significant wins and significant losses. For example, HRF achives a dominance rank of 151, derived from a total of 195 significant wins and 44 significant losses. The number of significant wins for HRF is equal to the cumulative sum of the values within parentheses in the HRF columns of Table6. Similarly, the number of significant losses for HRF is the sum of the values within parentheses in the HRF row. Tables6and7collectively demonstrate that the HRF method exhibits significant superiority over other methods across a dataset pool comprising 52 instances.

SECTION: 4.2Noise feature effect

In Section3.3, we discovered the effects of noise features on HRF. Specifically, if there are too many noise features, the accuracy becomes worse than that of bagging and RF. In this section, we examined whether a similar pattern exists in experiments with real data. To estimate the number of noise features in a dataset, a deep decision tree was constructed and the feature importance for each feature was calculated. Letâ€™s denote the feature importance vector as. We normalizesuch that. If the scaled feature importance is less than, i.e., we designate that feature as noise. Finally, the proportion of noise features (p_noise) is summarized in Table8.

We divided the entire dataset into two groups based on a noise ratio of 0.8.
Then, using the accuracy differences between HRF and bagging, as well as between HRF and RF within each group, we created box plots, which are shown in Figure6.
The accuracy difference between HRF and other ensemble methods was greater in the group with a lower proportion of noise features.

SECTION: 5Conclusion

In this paper, we proposed an ensemble method designed to modify RF to enhance diversity in decision tree structures. Our method demonstrated superior performance compared to other ensemble methods across most benchmark datasets.
The idea is to reduce the probability that key features used in the previous trees will be selected again in the next tree.
Unlike RF, which tends to favor features with many unique values, we found that HRF also helps mitigate this selection bias.
However, excessive noise features can have a negative impact on HRF performance. As demonstrated by the simulations and empirical results, HRF performs well on datasets with low noise and a diverse range of unique feature values.

Additionally, this paper introduces a methodology to quantify diversity within an ensemble by analyzing feature dominance, allowing for a comparison of the diversity across ensemble methods. Overall, it was found that the HRF ensemble method comprised more diverse trees than traditional methods, leading to more accurate classification predictions.

SECTION: Declarations

FundingHyunjoong Kimâ€™s work was supported by the IITP(Institute of Information & Coummunications Technology Planning & Evaluation)-ICAN(ICT Challenge and Advanced Network of HRD) grant funded by the Korea government(Ministry of Science and ICT)(IITP-2023-00259934) and by the National Research Foundation of Korea (NRF) grant funded by the Korean government (No. 2016R1D1A1B02011696).

SECTION: References