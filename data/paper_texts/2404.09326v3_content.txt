SECTION: Weight Copy and Low-Rank Adaptation for Few-Shot Distillationof Vision Transformers
Few-shot knowledge distillation recently emerged as a viable approach to harness the knowledge of large-scale pre-trained models, using limited data and computational resources. In this paper, we propose a novel few-shot feature distillation approach for vision transformers. Our approach is based on two key steps. Leveraging the fact that vision transformers have a consistent depth-wise structure, we first copy the weights from intermittent layers of existing pre-trained vision transformers (teachers) into shallower architectures (students), where the intermittence factor controls the complexity of the student transformer with respect to its teacher. Next, we employ an enhanced version of Low-Rank Adaptation (LoRA) to distill knowledge into the student in a few-shot scenario, aiming to recover the information processing carried out by the skipped teacher layers. We present comprehensive experiments with supervised and self-supervised transformers as teachers, on six data sets from various domains (natural, medical and satellite images) and tasks (classification and segmentation). The empirical results confirm the superiority of our approach over state-of-the-art competitors. Moreover, the ablation results demonstrate the usefulness of each component of the proposed pipeline. We release our code
at

SECTION: Introduction
Vision transformershave revolutionized computer vision research in the past few years, reaching state-of-the-art performance across a broad range of tasks, such as object recognition, object detection, image segmentation, image translation, among many others. Since the accuracy tends to grow as the model gets larger, most of the attention has been dedicated to building larger and more powerful models. However, the typically large size and slow inference speed of transformer-based architectures hinders the deployment of such models on environments with limited computational resources,e.g.on edge or mobile devices.

The success of vision transformers lies in the “pre-training then fine-tuning” paradigm, which originates from the natural language processing domain. The multi-head attention layers inside transformers can capture long-range spatial relationships among tokens, but this flexibility has a significant downside: transformers can easily overfit small training sets and experience poor generalization capabilities. Unlike convolutional nets, which benefit from the inductive bias of convolutional filters with small receptive fields, transformers requires huge amounts of data in the pre-training stage to avoid overfitting, regardless of the chosen supervisedor self-supervisedtraining setup. Therefore, to efficiently train large-scale transformer models, powerful and expensive computers are necessary, which are not widely available to researchers. Moreover, large amounts of data are not always available in some domains,e.g.hyperspectral image segmentation.

To mitigate the challenges of training large-scale models on large amounts of data on machines with limited computational resources, researchers have proposed the few-shot knowledge distillation (FSKD) paradigm, which was explored in both languageand visiondomains. This paradigm allows the trained model (called student) to benefit from the knowledge learned by large-scale transformers (called teachers), while significantly reducing the training time and data set size.

We emphasize that FSKD has not been extensively explored in the vision domain, with even less studies focused on the pre-training stage of vision transformers. To this end, we propose a novel few-shot feature distillation approach for vision transformers based on intermittentightpying andw-ankdaptation (WeCoLoRA), as illustrated in Figure. Our approach is divided into two steps. For the first step, we leverage the fact that vision transformers have a consistent depth-wise structure,i.e.the input and output dimensions are compatible across transformer blocks. This allows us to directly copy the weights from intermittent layers of existing pre-trained vision transformers (teachers) into shallower architectures (students). Here, we use the intermittence factor to control the complexity (size) of the student transformer with respect to its teacher. In the second step, we employ an enhanced version of Low-Rank Adaptation (LoRA)to distill knowledge into the student in a few-shot scenario, aiming to recover the information processing carried out by the teacher layers that were skipped in the first step. We hereby note that standard LoRA is usually employed over queries and values, which is insufficient to fully replicate the behavior of the skipped blocks. To mitigate this issue and better complement the weight copying mechanism, we apply enhanced LoRA, a variant of LoRA that approximates all layers in the skipped transformer block.

We perform the pre-training stage of efficient student transformers via few-shot knowledge distillation on various subsets of ImageNet-1K. Then, we carry out linear probing experiments on five downstream data sets from different image domains, namely ImageNet-1K, CIFAR-100, ChestX-ray14, iNaturalistand RESISC45. We compare WeCoLoRA with state-of-the-art competitors, namely DeiT, DMAE, MiniViTand DoRA, showing that our approach leads to superior results. Furthermore, we conduct an ablation study to demonstrate that each proposed component brings significant performance gains. Additionally, we perform an analysis of the distilled features, which explains why WeCoLoRA produces more robust features than competing methods. In the supplementary, we demonstrate the versatility of WeCoLoRA by applying it on a convolutional network for image segmentation on the MSLesSegdata set.

In summary, our contribution is threefold:

We propose a novel unsupervised few-shot feature distillation approach for vision transformers based onintermittent weight copying andenhanced low-rank adaption, called WeCoLoRA.

We present few-shot and linear probing experiments on six benchmark data sets comprising natural, medical and satellite images, demonstrating the utility of our training pipeline across different domains and tasks.

We analyze the features learned by our distilled models in comparison with those of the strongest competitor, showing that our approach generates more robust and discriminative features.

SECTION: Related Work
Knowledge distillation (KD), a.k.a. teacher-student training, is an efficiency boosting technique meant to reduce the computational load that comes with large models. It emergedfrom the union of model compressionand learning under privileged information. Our study is only preoccupied with the former task, being aimed at proposing an approach that involves transferring expertise from a heavy teacher model to a lighter student model, with the latter finally being able to learn more discriminative features than through a conventional training procedure.

KD was originally applied to vision transformers by Touvronet al., who proposed Data-Efficient Image Transformers (DeiT) based on leveraging attention as knowledge and enhancing the student’s capabilities through a learned distillation token. Since then, several other studies used KD on vision transformers. Close to our work, MiniViTuses a parameter reduction technique that involves copying weights (and applying slight perturbations) between the teacher and a smaller student transformer, followed by distillation through self-attention. In comparison, after copying the weights, we employ an enhanced version of LoRAduring the information recovery process, which leads to significantly better results in the few-shot scenario. Recently, distilling Masked Autoencodershas gathered some traction, with most approaches achieving data efficiency through masking very large percentages (up to 98%) of the input. Our data reduction strategy comes solely from a reduction of the number of unlabeled samples, being more fit for cases where data scarcity is an issue.

Zero-shot knowledge distillationimplies that the teacher and student do not share the same data sources. As a consequence, the latter should obtain its knowledge through synthetic examples. Black-box KDinvolves generating synthetic images from the teacher’s training distribution, along with their corresponding labels, offering the dual benefit of privacy preservation and effective learning. Zero-shot KD emerged as a solution to the lack of knowledge about the training data used by the teacher, but it suffers from the problem of collapse, caused by similar training samples. A less strict alternative that mitigates this issue is to employ few-shot KD. To perform FSKD, Liet al.introduced an additionalconvolutional layer at the end of each block of a CNN to support recovering the abilities of a teacher, using a small set of unlabeled examples. To the best of our knowledge, we are the first to employ few-shot KD in the pre-training stage of lightweight vision transformers.

LoRA and its variationsrepresent parallel performance improvement methods, enabling the use of powerful large models through lightweight fine-tuning. Consequently, LoRA reduces the inference cost and the requirement to use large data sets. Following other lines of work, the authors of LoRAshowed that over-parameterized models rely on a very low internal dimension when making their decisions. The proven hypothesis is that the change in weights during domain adaptation or fine-tuning should also be happening at a low intrinsic rank. To boost training efficiency, we thus combine Low-Rank Adaptationand KDin our study. To the best of our knowledge, there is no prior work that employs weight copying and LoRA as a method for few-shot feature distillation in an unsupervised setting.

SECTION: Method
SECTION: WeCoLoRA Architecture
Our knowledge distillation framework, WeCoLoRA, is formally described in Algorithm. Our method follows the knowledge distillation paradigm of compressing a larger model into a smaller one. Therefore, our goal is to obtain a studentwhich runs faster than the teacherduring inference. However, we also use significantly less training data during the knowledge distillation process, since it is not always feasible to obtain the same amount of data as for (pre-)training the teacher. Our knowledge distillation method is designed for vision transformers.
Given a pre-trained transformer-based teacherwithlayers, we create the studentthat haslayers, whereis the reduction ratio of the number of layers andis the floor approximation function.
The first step of our algorithm, which is described in steps-of Algorithm, is to copy the pre-trained weights ofto. Since the studenthas fewer layers than the teacher, due to the reduction ratio, the weights from the layer at indexof the teacher are copied to the layer at indexof the student, where.

After the intermittent weight copying step, we apply an adaptor layer, such that the student model is able to recover the weights which are not transferred from the teacher. In this way, we aim to minimize the performance gap between the teacher and the student.
We employ the LoRA frameworkas the adaptor, due to its competitive performance obtained in model adaptation.
LoRA can be applied to any fully-connected layer, which makes it a great choice for the transformer architecture.
Therefore, for a pre-trained fully-connected layer, with the weight matrix, LoRA learns a matrixand a matrix, whereis the matrix rank,, andandare the matrix dimensions.
During the knowledge distillation process,is kept frozen, so only the parametersandare updated. The output of the layerbecomes, whereis the input and.

Luoet al.proposed to apply LoRA only to the projection matrices corresponding to the query and value tokensand, respectively. However, in our setup, after applying the intermittent weight copying mechanism, we need to replicate the functionality of the skipped transformer blocks. Since LoRA only adapts the projection matrices of query and value tokens, it does not provide the means to fully replicate the skipped blocks. In order to mitigate this issue, we conjecture that it is better to apply LoRA to each component of the transformer block. Therefore, for each head,of the multi-head self-attention (MSA) layer, the outputs of the query, valueand keyapplied to the inputbecome:

where,andare the query, key and value projection matrices, whileandare the matrices learned by LoRA, where.

The formula to compute the output of the headremains unaltered, being equal to,, where:

andis the dimension of.

We also add adaptation parameters to the output projection layer in the multi-head attention, resulting in:

where [] is the concatenation operation,is the output projection matrix,andare the parameters introduced by LoRA, andis the number of heads.

Further, we also integrate LoRA into the feed-forward networks (FFNs). Therefore, the output of an FFN from a transformer block is:

where FFL is the output of the first layer in the FFN model,andare the parameters of the feed-forward network,is the activation function, andandare the matrices learned by LoRA, where. The biases in the FFN network were omitted to enhance the clarity of the presentation.
By introducing LoRA in every component of the transformer block, we obtain an enhanced version of LoRA-based transformer blocks. Enhanced LoRA is able to better complement the weight copying mechanism by enabling a more faithful replication of the skipped teacher blocks. This statement is confirmed by our ablation study.

SECTION: Knowledge Distillation with WeCoLoRA
Our distillation method does not require training labels, therefore, the knowledge distillation data set is, whereis an image sample andis the number of samples in the data set. This is because the knowledge distillation procedure is performed in the latent space. Moreover, we choseto be typically small, resulting in a few-shot feature distillation training setup.

The goal of applying knowledge distillation is to transfer the knowledge of the pre-trained teacherinto the LoRA-enhanced student.
Following Luoet al., during optimization, we only update the newly added weights of the student, denoted as, where,. The operation of inserting trainable weights into the student is formally described in steps-of Algorithm.

In order to optimize the parametersof the student, we minimize the absolute difference between the embeddingreturned by the teacherfor image, and the embeddingreturned by the studentfor the same image, whereis the number of tokens andis their hidden dimension. Formally, the proposed feature distillation is expressed as follows:

whereis the number of training images. This procedure is described in steps-of Algorithm.

After optimizing the parameters, we integrate them into the pre-trained parameters(copied from the teacher) in step 16, as described by Luoet al., in order to avoid affecting the running time during inference.

SECTION: Experiments
We use ImageNet-1Kto perform knowledge distillation in various few-shot setups, considering subsets ranging betweenandof the number of samples in the original training set. We test the models via linear probing, using the official ImageNet-1K evaluation set. We also perform linear probing experiments on five downstream data sets: ImageNet-1K, CIFAR-100, ChestX-ray14, iNaturalistand RESISC45.

SECTION: Data Sets
. In our self-supervised knowledge distillation process, we operate on ImageNet-1K, a large-scale data set with 1,281,167 color images from various sources, each having an average shape ofpixels. The collection covers 1,000 object categories, such as plants, vehicles, and everyday items.

. CIFAR-100is a balanced data set of 60,000 images representing various visual entities, grouped into 100 classes. The categories are arranged in a hierarchical structure, under 20 superclasses. The official test set contains 10,000 pictures. Each image haspixels.

. The CXR14data set is composed of 112,120 frontal-view X-Ray images from 30,805 patients and depicts a collection of 14 common pathologies, as well as normal lung scans. Its annotations are binary multi-labels, indicating whether each of 14 lung diseases is present or not. The labels are based on radiological reports. The resolution of each image is. To evaluate our models, we use the test set comprising 20% of the images, on which we perform multi-label classification.

. iNaturalistis a heavily imbalanced data set, proposing the fine-grained classification of 1,010 plant and animal species. Comprising a total size of 268,243 images with varying resolutions, its 2019 version offers an evaluation set of 3,030 images.

. NWPU-RESISC45incorporates 31,500 3-channel images from the aerial domain, which can be used for remote sensing classification. The data set comprises 45 scene classes and various spatial resolutions, where one pixel can represent a surface ranging from 20cm to 30m. In our evaluation setting, we use 80% of the data as the test set.

SECTION: Implementation Details
We implement our method in Pytorch. We create the enhanced version of LoRA starting from the official code of LoRAand extending it, in order to be applied to each component of the transformer layer. To demonstrate the generalization of our method to different teachers, we perform experiments with a supervised and a self-supervised teacher, respectively. As the supervised teacher backbone, we employ the ViT-B modeltrained on ImageNet, available in thelibrary. The selected self-supervised model is also based on ViT-B, but pre-trained using the Masked Autoencoder framework. Since the goal is to create a model which is faster and lighter, we generally refrain from using larger teacher models, which would be inefficient for our purpose.

To show that our method generalizes to different depth reduction factors, we perform experiments with,and. Since the number of layersin the teacher architecture (ViT-B) is, the number of layers in the student architecture becomes(when),(when), or(when), respectively.
We select the rank of the low-rank matrices from, which controls the number of parameters used to recover the skipped layers from the teacher.

We perform the knowledge distillation procedure for onlyepochs. We set the learning rate towhen the compression factorequals, and towhenis larger. The batch size is set to, with a gradient accumulation factor of. We perform weak data augmentation, similar to Heet al..

To demonstrate that our method transfers strong features, we only perform linear probing on the downstream tasks. For each downstream data set, we train the linear classifier using the best available linear probing training recipe. We also provide the code for an easier reproduction of the experiments. For a fair comparison, we use the same linear probing training setting for all the models included in the comparison.

SECTION: Results
We compare our method with various ablated versions and report the results in Table. We start by comparing our method to the conventional KD method based on training a student vision transformer from scratch, using knowledge distillation (first and fifth rows). This approach, which is well-known and widely-employed, does not perform well in the few-shot setting, since there is not enough data to learn the underlying distribution. Next, we perform experiments using an ablation version of our method, which copies the weights from the teacher (second and sixth rows). This approach, which we refer to as Weight Copying + Knowledge Distillation (WeCo+KD), updates all the weights of the student. Even though this is a very strong baseline, reaching an accuracy ofin the linear probing setting on ImageNet whenof the initial training data is used during knowledge distillation, it is stillbehind our approach. Nevertheless, weight copying brings significant performance boosts over the standard distillation, confirming its practical utility.

The third ablated model uses weight copying and adds LoRA to the projection matrices corresponding to the query and value tokensand, following Luoet al.. This approach performs poorly, reaching onlyin terms of accuracy on the ImageNet downstream task, whenof the initial training data is employed. We conjecture that LoRA adds an insufficient number of learnable weights to properly learn the skipped layers of the teacher. Our full framework, WeCoLoRA, replaces standard LoRA with our enhanced version of LoRA, which is based on adding LoRA layers to each component of the transformer blocks. Our enhanced LoRA (fourth and eight rows in Table) outperforms both LoRA and WeCo+KD on all data sets. In summary, the ablation study demonstrates the utility of our novel components.

In Table, we compare WeCoLoRA and WeCo+KD with four state-of-the-art techniques: DeiT, MiniViT, DMAEand DoRA. We also report the performance obtained by the self-supervisedand supervisedteachers (both based on ViT-B) on each downstream task, as an upper bound for the few-shot methods. Note that MiniViT and DoRA are not directly applicable in conjunction with self-supervised teachers.

Interestingly, the ablated version of WeCoLoRA, namely WeCo+KD, generally outperforms all state-of-the-art methods. This indicates that current methods are not able to handle the limited amount of data that is typical to the few-shot setting. WeCoLoRA surpasses all four state-of-the-art KD methodsby significant margins,e.g.the differences are betweenandon ImageNet-1K. Moreover, on the iNauralist data set, our method obtains an accuracy that is onlybelow the supervised teacher, while using half the number of layers and only usingof the training data during distillation. Since WeCo+KD, the ablated version of our method, generally surpasses the state-of-the-art methods, we choose it as a strong baseline for the subsequent experiments.

We report additional results with WeCoLoRA and its strong ablated version, WeCo+KD, in Tablesand. We alternatively employ two teachers in these experiments, a supervised ViT-Band a self-supervised ViT-B. We also consider two compression factors, namelyand, on five downstream tasks.

In Table, we report results when onlyof the ImageNet-1K training setis employed during the knowledge distillation procedure. In this scenario, WeCoLoRA outperforms the baseline in most cases. Remarkably, for, WeCoLoRA achieves an accuracy ofon ImageNet using onlyof the training data, surpassing the accuracy of WeCo+KD by.

We present results forof the distillation data in Table. Once again, WeCoLoRA outperforms the baseline in most settings. However, the gains in favor of our method when usingof the data (Table) are generally lower than distilling onof the data (Table). We consider thatof ImageNet-1K,i.e.about 100K training images, is at the upper end of the scenarios that can still be regarded asfew-shot. The higher number of trainable parameters in WeCo+KD, combined with the large training set, allow it to recover the gap with respect to WeCoLoRA. To further demonstrate the superiority of WeCoLoRA in other few-shot scenarios, we present additional results when using,andof the data during distillation, in the supplementary.

In Figure, we illustrate t-SNE visualizations of the embeddings learned with the proposed method (WeCoLoRA) versus its ablated version denoted as WeCo+KD. The embeddings are obtained on the RESISC45test set. We illustrate the embeddings of the student obtained through distilling the supervised ViT-Bteacher, with a compression ratio of. We notice that our WeCoLoRA is able to learn a discriminative feature distribution during distillation, disentangling the out-of-domain (downstream) data samples from RESISC45 into clearly delineated clusters, even though there was no supervision involved, other than the teacher features. However, WeCo+KD, which is based on complete weight adaptation during distillation, creates clusters which are spread across the feature space. In summary, the t-SNE visualizations emphasize that WeCoLoRA is an effective few-shot knowledge distillation approach, explaining the superior results on downstream linear probing tasks through the robustness of the learned latent space.

To visualize the attention, we employ Attention Rollout, a technique that enables the visualization of the attention flow throughout ViT. We fuse the attention heads by taking the maximum response (top) and discard pixels with lower values. In Figure, we present three test images that are randomly taken from ImageNet, which correspond to “Daisy”, “Persian Cat” and
“Goldfish” classes, respectively. The attention is extracted from student models trained with WeCo+KD and WeCoLoRA. We observe that WeCoLoRA makes the model focus more on discriminative features such as petals, fur, and fish scales. This helps the model based on WeCoLoRA in taking informed decisions.

In Figure, we present results obtained by WeCoLoRA on the CIFAR-100 data set, when changing the matrix rank. The teacher is the supervised ViT-B, the compression factor is, and the distillation process is based onof the original training data. We observe that WeCoLoRA obtains stable performance whenand. Ifis too small (), the performance drops, showing that the model needs more parameters to recover the information of the skipped teacher layers. However, the performance also drops whenis too large (), but this happens because the model starts to overfit the small data set. Hence, we observe thatis a hyperparameter that can control the bias-variance trade-off of WeCoLoRA. In our experiments, we found thatandperform generally well in the few-shot distillation scenarios.

We first compare the running time of the teacher ViT-B and the student based on a compression ratio. As expected, the student is twice as fast as its teacher, evaluatingsamples in, while the teacher model requiresfor the same mini-batch size. Next, we compare WeCoLoRA and WeCo+KD in terms of the number of trainable parameters. WeCo+KD updatesM parameters, while WeCOLoRA updates onlyM parameters (when). Because the new weights added by WeCoLoRA are not integrated into the network during training, WeCoLoRA usesGFLOPs, while WeCo+KD utilizesGFLOPs. During inference, the GFLOPs are the same for both WeCoLoRA and WeCo+KD. The reported running times are measured on a GeForce RTX 3090 GPU with 24G of VRAM.

SECTION: Conclusion
In this paper, we proposed a novel few-shot unsupervised feature distillation method that can be used to train vision transformers on a few unlabeled images. Our approach, termed WeCoLoRA, combines weight copying and low-rank adaptation in an efficient and effective training pipeline. We conducted experiments in multiple few-shot scenarios, using both supervised and self-supervised teachers, and considering various model compression factors. The results show that our method outperforms state-of-the-art competitors, as well as ablated versions of our approach. Moreover, we present feature visualizations that clearly indicate that WeCoLoRA produces more robust embeddings, which are able to better disentangle the classes, even on downstream data sets.

SECTION: Acknowledgments
This work was supported by a grant of the Ministry of Research, Innovation and Digitization, CCCDI - UEFISCDI, project number PN-IV-P6-6.3-SOL-2024-2-0227, within PNCDI IV. The research leading to these results has also received funding from the NO Grants 2014-2021, under project ELO-Hyp contract no. 24/2020.

SECTION: References
SECTION: Additional Results
In Tablesand, we report results on five downstream tasks, when the students use onlyandof the ImageNet data during distillation, respectively. As noted in the main manuscript, we perform linear probing to demonstrate that our method transfers strong features.
We notice that our method, WeCoLoRA, attains higher performance than the WeCo+KD method, especially when the distillation data is scarce. We observe a substantial improvement (of at least) on the ImageNet downstream task, regardless of the reduction ratio or the distillation training size, when the teacher is the supervised ViT-Bmodel. We observe the same trend on the other data sets employed in the evaluation. We further note that the features learned by our distillation method also transfer to out-of-distribution data sets, such as ChestX-ray14. We consider ChestX-ray14 as out-of-distribution because it contains medical images, while the pre-training data set, ImageNet, contains natural images.

We conclude that the proposed distillation method, WeCoLoRA, is robust and obtains improved performance on multiple downstream tasks, especially when the pre-training data set is small. We also emphasize that our method does not require labeled data, and is able to compress both supervised and self-supervised models.

To better assess the performance trends on various downstream tasks when the number of samples increases fromto, we further illustrate the performance levels obtained by WeCoLoRA vs. WeCo+KD on ImageNet-1K, iNaturalist, NWPU-RESISC45, CIFAR-100and ChestX-ray14in Figures,,,, and, respectively. We observe that WeCoLoRA obtains significantly higher performance than WeCo+KD when there is less data involved in the knowledge distillation process (andof the original training set). Moreover, in most of the cases, WeCoLoRA also outperforms WeCo+KD whenof the original training set in used during knowledge distillation. We also conducted experiments on the full scale ImageNet and observed marginal differences between WeCo+KD and WeCoLoRA. We therefore conclude that WeCoLoRA is particularly useful in the few-shot KD setting.

We would like to mention thatof ImageNet corresponds to 12 samples per class. However, models are often evaluated on even fewer shots. To this end, we perform extra experiments forof ImageNet,i.e.3 samples per class. In this setting, we also employ a more aggressive reduction ratio (), which leads to a very light student model.

When the number of shots is very small, the model can collapse due to a low diversity of training samples. Although this is not the particular focus of our method, there is no obvious reason for WeCoLoRA not to be compatible with orthogonal methods that deal with the issue of collapse. To mitigate the issue of collapse, we combine WeCoLoRA with k-means++ init to select the training samples.

The results of WeCo+KD and WeCoLoRA forof ImageNet andare shown in Table. WeCoLoRA obtains superior results on all three data sets (ImageNet, iNaturalist and CIFAR-100). When the training samples are chosen via k-means++, we obtain slightly improved results on two datasets (see the last row in Table).

A promising approach to create lighter models without much effort is layer pruning. For transformer models, it was recently found that TopPruning, a method that drops the top-layers, obtains surprisingly good results. To this end, we compare WeCoLoRA with TopPruning, using the same reduction factor offor both methods. The results, which are reported in Table, clearly indicate that WeCoLoRA outperforms TopPruning.

One question that arises when applying WeCoLoRA is if the student is indeed learning features similar to the skipped teacher layers. To address this point, we compute the mean cosine similarities between the skipped layers and the corresponding student layers (from 1 to 4), before and after applying our enhanced LoRA. As shown in Table, the similarities increase after distillation, indicating that the student learns features similar to the skipped teacher layers. This confirms that enhanced LoRA has the intended effect.

To showcase the versatility of our approach, we test WeCoLoRA on a medical image segmentation task, by integrating it into the U-Net architecture. The segmentation model employs ResNet-18as backbone. Since the segmentation model is based on convolutional layers, we replace LoRAwith ConvLoRA. The experiments are performed on the Multiple Sclerosis Lesion Segmentation benchmark (MSLesSeg 2024). We report results in terms of the Dice coefficient in Table, where we compare our WeCoLoRA with the strongest baseline, namely WeCo+KD. The results demonstrate that WeCoLoRA obtains higher performance than WeCo+KD, when a convolutional backbone is employed on a segmentation task. This demonstrates the compatibility of WeCoLoRA with both transformer and convolutional architectures, as well as its applicability to diverse tasks, namely classification and segmentation.

To demonstrate the applicability of WeCoLoRA to deeper teachers, and its robustness to higher compression factors, we perform additional experiments with the ViT-L teacher based on supervised pre-training, considering compression factors ofand. In Table, we report the results of WeCo+KD and WeCoLoRA on CIFAR-100, RESISC-45 and ChestX-ray14. WeCoLoRA outperforms WeCo+KD for all compression factors, thus showcasing consistent performance gains across various compression factors and teacher models.

The feature distillation performed by WeCoLoRA is unsupervised,i.e.our framework does not require classification labels during distillation. An alternative approach is to employ supervised fine-tuning instead of unsupervised feature distillation. As shown in Table, the fine-tuning combined with WeCoLoRA produces worse results on CIFAR-100, while leading to similar results on RESISC45 and ChestX-ray14. We thus conclude that the supervised fine-tuning is not always beneficial.

One way to potentially boost the performance of WeCoLoRA is to transfer the classification head from the corresponding teacher model, instead of initializing the classification head of the student model from scratch. This idea is explored in Table(last row), where it exhibits performance boosts on CIFAR-100. The results on RESISC45 and ChestX-ray14 do not clearly show the benefit of transferring the classification head.

SECTION: Limitations
The main limitation of our method is its applicability to architectures that use multiple consecutive blocks with the same configuration,e.g.vision transformers and ResNets. This restriction is imposed by our weight copying mechanism. Our ablation results indicate that the weight copying step is very useful in the few-shot distillation scenario, as it significantly boosts performance (see Table 1 from the main article). Simply removing the weight copying step is not a viable option, since the performance would drastically degrade. To make our framework applicable to any architecture, the weight copying mechanism could be enhanced with adaptor blocks, which would be able to reshape the copied weights to the appropriate size. However, the adaptor blocks need to be tailored for each specific pair of teacher and student models. This will increase the complexity of the hyperparameter tuning stage, which, in the current form, is quite straightforward,i.e.aside from typical hyperparameters, such as the learning rate and the mini-batch size, WeCoLoRA only adds the compression ratioand the rank of the low-rank matricesas extra hyperparameters.