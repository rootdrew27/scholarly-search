SECTION: Exploring the Precise Dynamics of Single-Layer GAN Models: Leveraging Multi-Feature Discriminators for High-Dimensional Subspace Learning

Subspace learning is a critical endeavor in contemporary machine learning, particularly given the vast dimensions of modern datasets. In this study, we delve into the training dynamics of a single-layer GAN model from the perspective of subspace learning, framing these GANs as a novel approach to this fundamental task. Through a rigorous scaling limit analysis, we offer insights into the behavior of this model. Extending beyond prior research that primarily focused on sequential feature learning, we investigate the non-sequential scenario, emphasizing the pivotal role of inter-feature interactions in expediting training and enhancing performance, particularly with an uninformed initialization strategy. Our investigation encompasses both synthetic and real-world datasets, such as MNIST and Olivetti Faces, demonstrating the robustness and applicability of our findings to practical scenarios. By bridging our analysis to the realm of subspace learning, we systematically compare the efficacy of GAN-based methods against conventional approaches, both theoretically and empirically. Notably, our results unveil that while all methodologies successfully capture the underlying subspace, GANs exhibit a remarkable capability to acquire a more informative basis, owing to their intrinsic ability to generate new data samples. This elucidates the unique advantage of GAN-based approaches in subspace learning tasks.

SECTION: 1Introduction

Subspace learning is a widely explored task, especially with the growth of dimensionality in modern datasets. It is important to identify meaningful subspaces within the data, such as those determined by principal component analysis (PCA). However, due to the high dimensionality of the data, it is common to employ online methods such as Oja’s method[1]and GROUSE[2]. Meanwhile, Generative Adversarial Networks (GANs)[3], primarily used as generative models, have also demonstrated the ability to learn meaningful representations of data[4,5]. Inspired by this, we explore how single-layer GAN models can be viewed as a form of subspace learning.

We seek to improve the understanding of GAN training by relaxing some common assumptions made in previous analysis of GANs[6]. Specifically, we focus on the training dynamics of the gradient-based learning algorithms, which can be converted into a continuous-time stochastic process characterized by an ordinary differential equation (ODE). Furthermore, the dynamics of the model weights form a stochastic process modeled by a stochastic differential equation (SDE). Understanding these two equations provides the relevant information to understand convergence behaviour of the training. We extend previous work to discriminators with same dimensionality as the generator. Finally, we discuss the new training outcomes that can arise in the higher-dimensionality case.

Our work explores what happens when the training switches from sequential learning in the single-feature discriminator case[6], to the non-sequential (multi-feature) learning of our discriminator. We show that the non-sequential learning of features not only allows for faster learning and convergence, but also a higher maximum similarity with the true subspace compared to the sequential case, when everything else is kept the same. This shows that contrary to the general approach of making discriminators much weaker than generators, it is still possible to use a powerful discriminator. In fact, doing so can lead to much faster training with better performance, through careful choice of learning rates.

We further show that our new framework can be used to analyze the cases where we assume different dimensionalities between the true subspace, fake subspace, and discriminator. Through the use of a simple uplifting trick on the relevant Grassmannians, we are able to extend our analysis to arbitrary dimensionalities. To understand how GANs compare with existing subspace learning algorithms, we provide both theoretical and empirical comparisons with existing such algorithms. We see that the features learned by a GAN model are more meaningful and represent the data better as compared to Oja’s method, due to the requirement of being able to generate new data from the underlying data distribution.

Finally, we test our approach using two prominent real-world datasets: MNIST and Olivetti Faces, comparing our method against a sequential discriminator, and show that all of the key insights gained through the theoretical analysis are visible in training on this dataset as well. This shows that our analysis has very practical applications, and can lead to interesting research directions exploring these ideas in more powerful GAN architectures. This testing is additionally done on the case where we assume different dimensionalities for each component of the model, showing that the results are as expected. We release all our code athttps://github.com/KU-MLIP/SolvableMultiFeatureGAN.

Overall, our contributions are as follows:

We investigate the distinctions between multi-feature and single-feature discriminators and fully characterize the learning dynamics through a rigorous scaling limit analysis.

We introduce a novel method for analyzing cases where the true feature dimensionality is unknown, enabling broader future analyses under uncertain conditions.

We position the multi-feature GAN models as a new type of subspace learning algorithm, and compare against existing algorithms, both theoretically and empirically.

We further validate our findings on image datasets (MNIST and Olivetti Faces), highlighting the practical implications of our insights for real-world applications.

SECTION: 2Related Work

SECTION: 2.1Training dynamics for GANs

Much of this work is inspired byWang et al. [6], which was one of the first to undertake this task. However, in this case, they use a single-feature discriminator, and show that the choice of learning rates relative to the strength of noise is what determines the outcome of the training process: convergence, oscillations, or mode-collapse.

There have been further attempts to understand the convergence of GANs through other approaches, not just the dynamics of the gradient-based optimization.Heusel et al. [7]showed that under reasonable assumptions on the training process and hyperparameters, using a specific update rule will guarantee that the GAN converges to a local Nash equilibrium.Mazumdar et al. [8]introduced a new learning algorithm under which the local Nash equilibria are the only attracting fixed points, so that the training algorithm tends to move towards these points. This type of analysis is very similar to our approach, focused on understanding fixed points. Other types of models have also been analyzed in the high-dimensional regime, such as linear VAEs[9], two-layer neural networks solving a regression task[10], and two-layer autoencoders[11]. A review of these methods can be found in[12].

SECTION: 2.2Subspace learning

Subspace learning is a heavily explored field, with many algorithms. However, when the noise of the data has a non-zero variance, most approaches fail, and the general technique used to solve the problem is some type of online PCA-based method. InWang et al. [13], a similar analysis of dynamics through ODEs is performed for multiple algorithms which learn subspaces with non-zero variance of noise. This analysis allows for steady-state and phase transition analysis of these algorithms.Balzano et al. [14]presents a survey of different online PCA algorithms used for subspace learning, in the case where only some of the data is visible at each timestep, and discuss how it is possible to find a unique subspace of a given rank which matches all the provided data.

SECTION: 3Background and problem formulation

SECTION: 3.1True data model

Our datais drawn from the following generative model, known as aspiked covariance model[15]:

Here,is the true subspace we wish to learn represented as an orthonormal basis,is a zero-mean random vector with covariance matrix,is a standard Gaussian vector, andrepresents the noise level.

The spiked covariance model is very widely studied, due to the non-triviality of learningUwhenever. The key property of this model is that the topeigenvectors of the data covarianceare given by the columns ofU. If there exists a strict eigengap between the topcorresponding eigenvalues and the other eigenvalues, then the reconstruction loss function is proven to haveUas a global minima[16].

SECTION: 3.2Online subspace learning algorithms

Subspace learning is a very important task in machine learning, most commonly performed by algorithms such as PCA or ICA. However, these approaches involve costly operations such as calculating covariance matrices or calculating matrix inverses, infeasible in high dimensions. Therefore, it is very common to use an online version of these algorithms, processing samples one at a time.

Online subspace learning algorithms typically fall into two categories: algebraic methods and geometric methods. Algebraic methods are based on computing the top eigenvectors of some representation of a sample covariance matrix. Assuming a strict eigengap, the top eigenvectors will yield the true subspace. Meanwhile, geometric methods optimize a certain loss function over some geometric space (Euclidean space or a Grassmannian manifold). We review two subspace learning algorithms here, Oja’s Method[1]and GROUSE[2]. While GROUSE was introduced for the missing data case, it can be used for full data too. For further details about these algorithms and their categorization, we direct the reader to[17].

However, we suggest a third category of online subspace learning algorithms, which we call the generative methods. Such methods, including single-layer GANs, do not have information about the specific task, and instead aim to learn the data simply by seeing the data and attempting to generate data from the same distribution.

Oja’s method[1]is a classical algebraic approach to online subspace learning. Given an orthonormalized initial matrix, we perform the following update at every timestep given a data sample:

Here,is an orthonormalization operator, andis the learning rate.

GROUSE[2]performs gradient descent on the Grassmannian manifold, which guarantees orthonormality of the updates. In the full data case, we again start with an orthonormal initial matrix, and at each timestep, given a data sample, our update is:

Here,,,,, andis our learning rate.

SECTION: 3.3Generative Models

Here, we focus specifically on GANs. A GAN model seeks to learn a representation of the underlying subspace through the use of two components: a generator and a discriminator. The generator learns the subspace by trying to generate new samples from the subspace, while the discriminator acts as a classifier, attempting to distinguish data from the true subspace from data produced by the generator.

Note that measuring performance through cosine similarity can actually be viewed as a way to measure the generalization performance of the generator, as it doesn’t depend on any specific instance of generated data and instead provides a concrete measure of how similar the generated data will be.

We assume that the generator also follows a spiked covariance model:

However, we do not assume that, or that the covariance of,, is the same as. The goal of the generator is to learn.

The learning in the GAN model critically depends on the choice of discriminator, which aims to separate the data from the true and generated subspaces.

The most common approach when training GANs is to use a discriminator that is weaker than the generator. If the discriminator is too strong, then it will easily learn to distinguish between true and generated samples, leading to vanishing gradients for the generator and thus preventing learning. However, a weak discriminator results in sequential learning, where the generator is only able to learn a subset of the features at a time. In multi-feature cases, this will lead to very slow learning.

Motivated by this, we seek to analyze a model in which the discriminator has the same strength as the generator. Thus, we let, and define the discriminator as

whereis some function (see the assumptions below). Since this discriminator is able to focus on all the features at once, this means the generator is also able to learn every feature at once. This is in contrast to the single-feature case (where) analyzed previously. While this is a strong assumption on the discriminator, we show below how this assumption can be relaxed.

GAN training is modeled as a two-player minimax game, where the discriminator attempts to maximize some loss function and the generator attempts to minimize it. This is used as a way to learn a "surrogate" subspace which represents the true subspace. Therefore, the GAN model can be seen as a form of subspace learning, except that the focus is on generating new samples from the subspace.

Specifically, letbe a loss function depending on the discriminator weights, and true and fake samples. Ifdenotes the true distribution anddenotes the generator distribution, the minimax game can be represented as

Following the approach ofWang et al. [6], and in order to compare the sequential and multi-feature cases, we use the following loss function:

Here,are functions affecting the outputs of the discriminator,is an element-wise function used for regularizing the weights of the generator and discriminator, andcontrols the strength of the regularization. As, the matriceswill become orthonormal.

The standard approach to solve this minimax game is using stochastic gradient descent (SGD). At timestep, given a samplefrom the true subspace and a samplefrom the generator subspace, we perform the following updates:

Here,denotes the learning rate of the discriminator, anddenotes the learning rate of the generator. Note that while it is common to use a batch of data at a time when using SGD, we focus on a single element at a time in order to simplify all the analysis.

SECTION: 4Development of ODE

Similar to[6], we make the following definitions:

is called themicroscopic stateof the training process at time.

The tupleis called themacroscopic stateofat time, where,,,, and. The macroscopic state can be written in matrix notation as, in which we get

SECTION: 4.1Macroscopic dynamics

To analyze the macroscopic dynamics, we reduce to a special case, which leads to a slightly modified set of the assumptions fromWang et al. [6].

The sequencesare i.i.d. random variables with bounded moments of all orders, andis independent of.

The sequencesare both independent Gaussian vectors with zero mean and covariance matrix.

,, and. We note that the first derivative ofexists, the first four derivatives ofexist, and all the derivatives are uniformly bounded. Thus, our choices satisfy the conditions of assumption (A.3) fromWang et al. [6].

Letbe the initial microscopic state. For, we have, whereis some constant not depending on.

The initial macroscopic statesatisfies, whereis a deterministic matrix andis some constant not depending on.

The columns of the discriminator matrixWare orthonormal, so that.

Assumptions (A1) and (A2) are the usual i.i.d assumptions common in machine learning. (A3) is important for deriving the update equations. (A4) and (A5) are used to guarantee that the macroscopic state can converge. Our assumption (A6) of orthonormal discriminator matrix allows us to simplify the equations since theZmatrix of the macroscopic state is always just.

Under these assumptions, as well as letting, we obtain a modified Theorem 1 fromWang et al. [6], specifically considering the reduced case of equation (13). Note that our choice ofmeans that our equations become an arbitrary-dimensional version of the original equations.

Fix. Under Assumptions (A.1) - (A.6), it holds that

whereis some constant depending onbut not, andis a deterministic function. Moreover,is the unique solution of the following ODE:

with the initial condition, where

A sketch of the proof of this theorem can be found in AppendixB. The proof closely mirrors the proof of the original theorem in[6].

SECTION: 4.2Microscopic dynamics

The microscopic dynamics are concerned with how the termschange over time. Following previous work, we consider the empirical measure

whereis the delta measure. This is a discrete-time stochastic process, which can be embedded in continuous time as, with. Then, as, this process converges to a deterministic process, which is the measure of the solution of the SDE

whereis some diffusion term, negligable due to our assumption on the discriminator (A.6).

From this equation and the convergence of the measure, we can obtain the following weak PDE

whereis a bounded, smooth test function.
The ODE in the main theorem can be derived from this weak PDE.

SECTION: 5Simulations

In order to demonstrate that the ODE properly represents the training dynamics of the GAN model, we first perform simulations and show that the empirical results match the ODE, seen in Figure1. To understand how the training dynamics change based on the generator learning rate, we fix the discriminator learning rate asand fix the generator learning rate. We show the results on 4 different noise levels. In all cases, we let.

We set, and we ensure that the empirical setup is initialized with exactly matchingandvalues. We note that the ODE will never learn when the initialization is exactly, and so we must provide some level of similarity to start training. However, this is not very restrictive, as our experiments show that even random matrices will have approximatelyfor bothand, which is sufficient to escape the fixed point around.

SECTION: 5.1Off-diagonal simulations

A key insight found from the multi-feature discriminator is that the interaction between different features can help learning. When the macroscopic states are initialized to non-diagonal matrices, we see that the dimension with smaller covariance is actually able to attain better results and reach a similar cosine similarity to the dimension with higher covariance. Such an outcome is not possible in the sequential learning regime, due to the lack of interaction between features. In sequential learning, features are learned one at a time, and once a feature has been learned, the training will focus on a different feature instead. This phenomenon can be seen in Figure2, showing that the off-diagonal initialization allows for not only faster training (which also happens in the diagonal initialization case), but also higher steady-state values compared to the sequential learning case. We are unable to provide a detailed characterization of these fixed points, as a neat closed-form solution cannot be obtained.

SECTION: 6Unknown number of features

While this type of analysis can provide interesting insights, it has a very restrictive assumption that we know the number of features. This is done so that the macroscopic states are well-defined. However, we now seek to extend this analysis to the case where the true subspace hasfeatures, the generator subspace hasfeatures, and the discriminator learnsfeatures, where we do not assume that. While this analysis can be performed under any assumptions on the relative size of,, and, we focus on the single case.

To simplify the demonstration of this approach, we make the assumption that, so thatUcontains the firststandard basis vectors. We introduce the idea of uplifting (inspired by the work in[18]) the matricesto the dimensionality ofV.

First, sinceUis an orthonormal matrix, it lives in the Grassmannianof-dimensional subspaces of. Similarly,. Our goal is to embedUandWinto. Once we do this, we can again calculate the macroscopic states we are interested in. To do this, we use the following map:

This produces a new matrix. We can perform a similar trick withWto obtain a matrix. The important details about this uplifting trick are the following: (1) Due to the construction, we preserve orthonormality of all the matrices, (2) the subspaces of interest are found as the firstcolumns of the matrixand the firstcolumns of the matrix, and (3) the analysis of the diagonal case is unchanged under this uplifting (In the diagonal case, there is no interaction between the different dimensions, so we ignore the other dimensions. In the non-diagonal case, these additional dimensions only provide minor noise, and so don’t affect the training at all).

SECTION: 7Real image subspace learning

In order to demonstrate the practicality of this analysis, we test our approach on the MNIST[19]and Olivetti Faces[20]dataset, and compare our approach with the single-feature discriminator fromWang et al. [6]. Here, we include some qualitative results regarding the learned features, and provide a quantitative analysis on the performance differences between the multi-feature and single-feature discriminators. We include the Olivetti Faces results in Figure4, and the MNIST results can be found in AppendixA.

Epoch 1

Epoch 200

Final Epoch

To perform these visualizations and measure performance, we first perform PCA on the entire dataset and extract the top(16 or 36) features. We then use this as an approximation of the true subspaceU, which allows us to compare the distances. We then track the Grassmann distance between the true and learned subspaces for both the multi-feature and single-feature approaches. The Grassmann distance between two-dimensional subspaces of an-dimensional space is given by

where theare the principal angles between the subspaces. Here, a lower distance means a better similarity between the subspaces. If the two matrices are orthonormal, the principal angles are the singular values of the cosine similarity matrix, explicitly connected with the macroscopic states.Figure3shows the Grassmann distances for the sequential and multi-feature learning cases on the Olivetti Faces dataset. This provides empirical justification on a real dataset, showing first that the phenomenon of faster training identified by the ODE in Figure1applies to practical settings as well. Furthermore, due to having no restrictions on off-diagonal entries of the macroscopic states, we see that the results in Figure2also apply to practical datasets, since our multi-feature discriminator attains better performance even in less time.

SECTION: 8GANs as a subspace learning algorithm

In the linear setting, GANs attempt to perform subspace learning. However, GANs do not fall into either of the categories introduced earlier. The other subspace learning algorithms all seek to minimize the following loss function

known as the reconstruction error. This is because the global optima of this loss function is the true subspace itself, and so, we can view this as a prior included in the subspace learning algorithms. GANs do not have such information, and instead seeks to learn the subspace simply through seeing the datapoints. Therefore, we can consider GANs to be a third type of subspace learning algorithm, which we call the generative algorithms. We seek to understand how well the GAN model is able to learn a subspace compared to the existing subspace learning algorithms. We compare both analytically using the derived ODEs, as well as empirically on synthetic and the MNIST dataset, in order to see under what circumstances GANs learn a subspace at a comparable rate.

SECTION: 8.1Learned features

Figure6in the Appendix compares the features learned by the GAN model to the features learned by Oja’s method. Both models are initialized to exactly the same weights, and trained on the same data at the same time, for a single epoch. For the GAN model, we use the same hyperparameters as the previous experiments above. For Oja’s method, we used a learning rate of, which experimentally we found to produce the best results. We can clearly see that the features learned by the GAN model are more meaningful and more clearly resemble the true data, while most of the features that Oja’s method learns aren’t very interpretable. This suggests that because the GAN needs to be able to generate the images, this acts as a form of regularization on what types of features are learned.

SECTION: 9Conclusion

Our investigation into single-layer GAN models through the lenses of online subspace learning and scaling limit analysis has provided valuable insights into their data subspace learning dynamics. By extending our analysis to include multi-feature discriminators, we’ve unearthed novel phenomena pertaining to the interactions among different features, significantly enhancing learning efficiency. This advantage is particularly pronounced in scenarios of near-zero initialization, where the generator achieves higher maximum and steady-state performances compared to the sequential discriminator. Moreover, the interaction between dimensions enables the generator to closely match variances across dimensions, a feat unattainable in the sequential scenario. In the context of subspace learning, we see that in higher noise levels, the GAN is able to more consistently outperform Oja’s method on a wide range of generator, discriminator, and Oja learning rates.

Introducing an uplifting method for analysis in arbitrary dimensionalities enables us to better model uncertainties inherent in real-world subspace modeling. Practical validation on the MNIST and Olivetti Faces datasets reaffirms the applicability of our theoretical findings, underscoring the superiority of overparametrization in single-layer GANs over data availability. This prompts intriguing avenues for research in multi-layer GANs, probing whether similar phenomena persist in more complex architectures. Exploring these directions holds promise for further advancements in the field. Finally, we observe that GAN models excel in acquiring a more meaningful feature basis compared to Oja’s method when applied to the real-world datasets, which we attribute to their ability to generate new data samples.

SECTION: Acknowledgements

We acknowledge that this work was supported in part by TUBITAK 2232 International Fellowship for Outstanding Researchers Award (No. 118C337) and an AI Fellowship provided by Koç University & İş Bank Artificial Intelligence (KUIS AI) Research Center.

SECTION: References

SECTION: Appendix AMNIST results

SECTION: A.1Comparisons with Single-Feature Discriminator and Oja’s Method

In order to demonstrate that our approach works in more practical settings, we train our model on the MNIST dataset. Then, in order to understand what the model has learned about the dataset, we compute the SVD of the generator weightsV, and plot the left singular vectors. Each of these vectors correspond to a single feature learned by the model, and so viewing these will help understand the model performance. Finally, we perform the same tests using the single-feature discriminator, to demonstrate the effects of sequential vs non-sequential learning of features.Our theory and development in the paper operated under the assumption that we knew everything about the true subspace. While this is not possible for these image datasets (since we cannot determine the true subspaceUor the distribution ofc), we can still use the same assumptions and model structure. Therefore, the generator still samples afrom a standard Gaussian distribution, and the choice of covariance and noise levels are determined through testing.The dataset is flattened into avector, so our ambient dimension. For our multi-feature model, we train for a single epoch. For the sequential discriminator, we train forepochs. We focus on thecase, although it can be further scaled up as necessary. Through testing, we fix the covariance matrixand. We use a generator learning rate ofand a discriminator learning rate of.
While the multi-feature discriminator is able to learn good representations of all 36 basis elements as seen in Figure5, the sequential discriminator is unable to learn even half of them in theepochs. As can be seen, the last 18 basis elements are just noise.This scaling becomes very problematic as the number of features increases. Even with justfeatures, a small amount given modern datasets, such a model requires significantly more training and is still unable to perform as well as the multi-feature model.

Finally, we provide a comparison of the GAN learned features with the Oja’s learned features in Figure6. It can be seen that most of the GAN features are more visually representative of the dataset compared to Oja’s method.

SECTION: A.2Grassmann distances

Figure7contains a comparison of the Grassmann distances for the multi-feature and single-feature cases. Even after 5 epochs, the sequential discriminator still has a much higher Grassmann distance than the multi-feature model, even though it has seentimes as much data. Specifically, after one epoch of training, the multifeature discriminator has a distance of, while the single-feature discriminator finishes with a distance of, showing a significant gap. We tested with up toepochs, but saw no improvements for the sequential discriminator pastepochs.

We also see an example of the training outcomes predicted by the ODE in Figure7. Specifically, our choice of learning ratesand noise levelis seen in Row 1, Column 2 of Figure1, and we see the expected result of the generator oscillating around its steady state.

SECTION: Appendix BProof of main theorem

This proof mirrors the proof of Theorem 1 in[6]. However, for completeness, we re-state the key results and sketch the proof here.

The proof relies on the following result, found in[13]:

Consider a sequence of stochastic processeswith some constant. Ifcan be decomposed into three parts

such that

The processis a martingale, andfor some;

for some;

is a Lipschitz function, i.e.,;

for all;

for someand deterministic vector,

then we have

whereis the solution of the ODE

The relevant stochastic process is the macroscopic states introduced in Section4. The macroscopic states are decomposed as

Note that our macroscopic state can just be written as andimensional vector, and it is equivalent to using the Frobenius norm in the conditions above. We seek to show that this decomposition satisfies the conditions (C.1) - (C.5).

Immediately, Condition (C.5) is satisfied by the assumption (A.5) for the theorem. Additionally, (C.3) is satisfied by assumption (A.3) and Lemma 4 in the supplementary material of[6].

Next, we slightly modify Lemmas 2 and 7 in the supplementary material of[6]for our case.

Under the assumptions (A.1) - (A.6), given, we have

The proof of this follows exactly from Lemma 7 of[6].

Under the assumptions (A.1) - (A.6), given, we have

We show that this holds for a fixed.First, we know that

whenever, due to boundedness of. Additionally, we can write

Combining both of these, we get that

which follows from assumption (A.4). Similarly, we get

Combining these for both terms and iteratively applying it, we get

Then, due to assumption (A.4), we get the required result.
∎

Once we have Lemmas B.2 and B.3, we can show that condition (C.4) is satisfied.

Condition (C.4) is satisfied for our macroscopic state stochastic process.

We show that the expected norm squared of each macroscopic state is less than some. The cases ofandare proven in Lemma 3 of[6], and require no changes. Additionally, by our assumption (A.6) that the matrixis orthonormalized, we know that, and so the requirement is trivially satisfied for. Thus, it remains to show this forand. We show this for, andfollows similarly.

as required.
∎

Condition (C.2) above is satisfied, meaning that for all, and for a given, we have

To prove this, we can split it into five parts, one for each of the macroscopic states. For the macroscopic state, this just requires showing that

But the left side is just zero, sincefor all. Thus, this is trivially satisfied.For the macroscopic state, we want to show that

However, from the gradient of our update equation for, averaging overwe see that

Multiplying both sides byon the left, we get

But then, the left side of the equation we wanted to show is just zero, and so the inequality is satisfied.Applying a similar process to the update equation for, we want to show that

and by averaging overand multiplying byon the left, we get

Again, this results in the left side of the expression we want to show just being zero.Finally, we show the result for. The case forfollows similarly to the previous results.Using the property that

and averaging over, we get

The second term in the sum above has expected norm

This concludes the proof.
∎

For condition (C.1), the requirement of being a martingale is automatically satisfied by construction. To show that the remainder of condition (C.1) is satisfied, it suffices to prove that

We can break this up into each of the 5 macroscopic states separately. As before, doing this foris trivial. We show this forand, and the rest follow similarly. For, we get

This finishes the proof for.For, we have

where in the last line, we used the previously calculated values forand. The valuesandare the values ofandevaluated on the corresponding inputs.The conditions for the rest of the macroscopic states can be shown in the same way.
∎

Given the previous lemmas, the proof of the theorem then follows immediately from LemmaB.1.