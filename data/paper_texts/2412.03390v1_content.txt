SECTION: Enhancing Supply Chain Visibility with Generative AI: An Exploratory Case Study on Relationship Prediction in Knowledge Graphs

A key stumbling block in effective supply chain risk management for companies and policymakers is a lack of visibility on interdependent supply network relationships.
Relationship prediction, also called link prediction is an emergent area of supply chain surveillance research that aims to increase the visibility of supply chains using data-driven techniques.
Existing methods have been successful for predicting relationships but struggle to extract the context in which these relationships are embedded - such as the products being supplied or locations they are supplied from.
Lack of context prevents practitioners from distinguishing transactional relations from established supply chain relations, hindering accurate estimations of risk.
In this work, we develop a new Generative Artificial Intelligence (Gen AI) enhanced machine learning framework that leverages pre-trained language models as embedding models combined with machine learning models to predict supply chain relationships within knowledge graphs.
By integrating Generative AI techniques, our approach captures the nuanced semantic relationships between entities, thereby improving supply chain visibility and facilitating more precise risk management.
Using data from a real case study, we show that GenAI-enhanced link prediction surpasses all benchmarks, and demonstrate how GenAI models can be explored and effectively used in supply chain risk management.SDG 9: Industry, innovation and infrastructure

[inst1]organization=Institute for Manufacturing, University of Cambridge,country=United Kingdom\affiliation[inst2]organization=Alan Turing Institute, London,country=United Kingdom

SECTION: 1Introduction

Global supply chains emerge as companies buy products from one another to produce and deliver their own(Bellamy and Basole,2013).
They play a critical role in almost every aspect of our daily lives.
80% of global trade flows through multinational corporations, and one in five jobs worldwide is tied to global supply chains.
Increased volatility and geopolitical tension in recent years have shown how vulnerable we are to supply chain disruptions, with major shortages impacting our food, medicines and supply of electric batteries.
In tandem there is rising awareness on the exposure of global supply chains to human rights violations and unsustainable environmental practices, with US and European policy makers proposing legislative measures that demand comprehensive supply chain traceability(Küblböck,2013).

One of the key stumbling blocks to begin addressing these concerns is a lack of knowledge on interdependent supply chain connections.
Most companies have limited visibility beyond their direct connections.
Increasing visibility in supply chains have been a rich area of research in the past decade, with multiple technical innovations having been proposed, such as electronic product codes, radio frequency identification, and blockchain technologies.
While these have been successful to some extent, their reach is typically limited to one or two tiers at most.
That is because in order for tracking technology to be adopted, companies need to be willing to share data.
There is little incentive for companies to share data on whom they buy from, for various reasons.
Companies typically view their own supply chains as a competitive advantage, and fear that disclosing information could result in their buyers working directly with their suppliers, reveal their pricing structure, or they may simply be not wish their manufacturing and purchasing practices to be known to the buyer.

More recently, researchers have proposed a new solution to this problem, which is to use data driven methods to “estimate” who supplies whom, rather than rely on the willingness of companies to share data.
Termed as “Digital Supply Chain Surveillance”(Brintrup et al.,2023), these methods include network reconstruction(Mungo et al.,2023), web scraping to recognise supply relationships in text obtained from news articles and company annual reports(Wichmann et al.,2018), and machine learning methods for predicting relationships (formally, link prediction).

Most current methods focus on a single type of relationship, such as firm-level networks that map supply or buy relationships between firms(Brintrup et al.,2018,Mungo et al.,2023).
While these approaches provide valuable insights, they offer a limited understanding of supply chains due to not considering the multifaceted interactions and dependencies that exist between different entities, thereby restricting a comprehensive understanding of the entire network.

Considering the supply chain as an interconnected network of entities and relationships, we can construct supply chain relevant data into a supply chain knowledge graph that can capture complex relationships and attributes associated with supply chain entities.
The multiple types of relationships in the supply chain knowledge graph such as manufacturing processes required to produce a product, product flows, and types of partnerships can contribute to supply chain visibility for a comprehensive understanding of supply chain dynamics.
It also can reveal hidden patterns, identify potential bottlenecks, and support the development of strategies to enhance network resilience.

Generative Artificial Intelligence (GenAI), a branch of machine learning, is designed to create new content, ideas, or data (known as synthetic data), by learning patterns from existing data.
Unlike traditional Artificial Intelligence (AI) where the output depends on the given inputs, GenAIs can generate novel outputs.
Examples could be generative machine learning models used for synthetic data generation(Zhang et al.,2018)and large language models like ChatGPT(Open AI,2024)and Copilot(Microsoft,2023), Gemini(Google DeepMind,2023)and LLaMA(Touvron et al.,2023)used for the generations of human-like text, images, audio and even videos.

GenAI as a powerful tool has gained tremendous attention in recent years and been used in various fields.
For instance,(Zholus et al.,2024)explored how a language model can be used to accurately create molecular structures, facilitating the drug discovery process.(Zhao et al.,2024)introduced self-attention Generative Adversarial Networks (SAGANs) model to generate the synthetic data for solving data imbalanced issue in financial transaction data and then used it for credit card fraud detection.(Gayam,2023)investigated how GenAIs contribute to the creation of music and visual art.

In the context of supply chain operation management, GenAIs have been hypothesised to empower the human workforce, improve project management processes, and help optimize manufacturing and supply chain procedure(Mohammed and Skibniewski,2023).Early adopters Walmart and Maersk integrated GenAIs into their operations to optimize pricing negotiations(Jackson et al.,2024).
A logistics company, C.H. Robinson is exploring GenAIs for automating the freight shipment(Business Wire,2024)while(Ryder System,2024)leveraged GenAIs to power chatbots to handle customer inquiries.

These early reports on GenAIs inspire us to explore its potential in enhancing supply chain visibility, particularly by predicting relationships within supply chain knowledge graphs.
GenAIs, including pre-trained language models but not limited to, are trained on extensive datasets containing diverse tex-based information which enables them to find relevant patterns in unstructured data such as emails, reports, contracts and social media posts.
This ability might allow it to elicit the complex structure and patterns of relationships within networks.
GenAIs employ sophisticated neural network architectures, such as transformers, which allows the models to handle complex, non-linear relationships.
These architectures also use mechanisms like self-attention to capture dependencies and interactions between different parts of the data.
In supply chain networks, where relationships between entities (e.g., suppliers, manufacturers, distributors) are intricate and multifaceted, GenAIs’ ability to model these complexities has great potential to enhance supply chain visibility.

However, one area of concern of applying GenAIs into industrial applications has been so called “hallucinations”, where the model might perceive patterns that are non-existent, creating outputs that are not factual or inaccurate(Huang et al.,2023). We cannot simply ask GenAI such as an LLM whether a relationship exists between Jaguar Land Rover and Aisin as we cannot guarantee that the answer to this question is not hallucinated.
This issue can undermine the reliability of the insights gained, posing risks in critical applications like relationship modelling in supply chain networks.
To address this challenge, a hybrid approach can be adopted: using a GenAI model as an embedding model to encode data into vectors, followed by applying machine learning models for relationship prediction.
This strategy leverages the strengths of the GenAI model in capturing intricate patterns and semantic contexts while relying on machine learning algorithms to avoid the risks of hallucinations.

In this paper, we explore the potential of GenAIs in enhancing supply chain visibility by integrating pre-trained language models with machine learning models to predict relationships within supply chain knowledge graphs.
We also introduce a new term, “quintuplet”, to represent more intricate relationships within the knowledge graph.
Unlike traditional triplets that capture a single relationship between two entities, quintuplets condense multiple triplets to provide a deeper understanding of the supply chain network.
After transferring regular triplets based knowledge graph into quintuplets based one, we are able to generate textual descriptions passed onto a pre-trained language model to retrieve vectorised relational knowledge learned by the pre-trained language model from large amount of website information, which are then further learned using a machine learning model to predict quintuplets.
The process thus allows us to combine structured knowledge representation by a knowledge graph, with the general knowledge base that can be used to augment the graph to make additional inference.

Combining GenAIs and Knowledge Graphs is powerful and goes beyond the state of the art in the supply chain domain, because we mitigate hallucination effects of GenAIs by restricting its use to augment structured prior data, and also allow additional contextual knowledge to arise from it, which would not have arisen by merely using knowledge graph completion methods.
Thus our contribution extends the current state of the art in supply chain relationship prediction for visibility and also allows us to provide a use case in the application of GenAIs to supply chain management research.
We compare our method to existing benchmarks with a use case in electronic vehicle battery supply chains, and present experimental results validated with a range of pre-trained language models and machine learning methods.
Our method surpasses all existing benchmarks in accuracy, and also yields better information.

The rest of this paper is organised as follows.
Section2reviews relevant existing works in the literature, including GenAIs, and existing link prediction methods in supply chain networks.
Section3presents our proposed approach for multiple connected relationship prediction in supply chain networks, including problem definition, preliminaries, and framework explanation of the proposed approach.
Section4uses a case study to evaluate the proposed approach while Section5concludes this work and explains its managerial implications and potential future work directions.

SECTION: 2Related Works

Generative artificial intelligence and link prediction in supply chain networks are two main topics relevant to the proposed approach in this work, reviewed below.

SECTION: 2.1Generative Artificial Intelligence

Generative Artificial Intelligence (GenAI) refers to a class of machine learning designed to generate new content including text, images, music and even videos, by learning patterns from existing data(Ooi et al.,2023).
GenAI models include Generative Adversarial Networks (GANs)(Goodfellow et al.,2014), Variational Autoencoders (VAEs)(Kingma et al.,2019), Transformer-based ChatGPTs(Open AI,2024), LLaMA(Touvron et al.,2023), DALL-E(Open AI,2022)and so on.
These models have been applied into various fields, leading to significant achievements and efficiencies.
One of the most significant capabilities of GenAI is natural language generation.
For example, Transformer-based models like ChatGPTs(Open AI,2024)have demonstrated remarkable abilities in tasks ranging from drafting emails to writing code, showcasing the versatility of GenAI in understanding and generating natural language.(Noy and Zhang,2023)reported that ChatGPT can substantially raise productivity by decreasing the average time consuming on mid-level professional writing tasks by 40% and increasing the output quality by 18%.

Apart from natural language generation, GenAI models have been used for generating realistic images.
For instance, GANs have been used to create high-resolution, photorealistic images that are indistinguishable from real photographs(Karras et al.,2019).
Such capabilities have enabled applications in fields like art generation(Louie et al.,2020), virtual reality(Hashim et al.,2023), and data augmentation used for other AI model trainings(Gowda et al.,2024).

As with many other domains, the capabilities of GenAI have recently been explored in the field of supply chain management, albeit very few studies exist. Those which does exist, do not yet report technical performance, but rather explore potential benefits and applications.(Fosso Wamba et al.,2023)explored the benefits, challenges and trends associated with GenAI technologies like ChatGPT in Supply Chain and Operation Management (SCOM) by surveying practitioners in the United Kingdom and the United States.
This study reveals an increased efficiency from GenAI adopters compared to non-adopters and highlights that the integration of GenAI can significantly enhance overall supply chain performance.
A subsequent study(Fosso Wamba et al.,2024)extended the exploration by additionally mapping the maturity levels of GenAI projects across supply chains and identified the specific operational benefits and challenges that organizations need to overcome.
Meanwhile,(Jackson et al.,2024)provided a comprehensive understanding of both AI and GenAI functionalities and applications in the SCOM context.
It also offers a practical framework for both practitioners and researchers to identify where and how AI and GenAI can be applied in SCOM to enhance decision-making processes, optimize operations, prioritize investments, and develop necessary skills.

In the industry, companies early adopters of GenAI have reported enhancement of their supply chain task performance.
Mars applied an GenAI platform offered by Celonis(Celonis,2024)to optimize truck loads and reduce manual efforts by 80% and improve delivery efficiency(Derek du Preez,2023).
Amazon leveraged GenAI to streamline and improve the delivery process(Meiyappan and Bales,2021)while FedEx applied GenAI to generate more precise package arrival estimates(CNBC Evolve Global Summit,2023).
Shein in the fast fashion sector leveraged GenAI to understand the changes in customer demand and interest, allowing it to adjust its supply chain in real time(Astha Rajvanshi,2023).
A report from McKinsey(Holger et al.,2023)shows that GenAI could add up to $275 billion to the operating profits of apparel, fashion, and luxury sectors in the next three to five years.

Inspired by such achievements of GenAI in both academic and industry, we explore how GenAI models can enhance supply chain visibility which is a crucial challenge in supply chains(Pichler et al.,2023).

Language models as a type of GenAI models have been pointed out the great potential of improving supply chain performance(Srivastava et al.,2024,Aguero and Nelson,2024).
One of the earlier applications in the supply chain domain has been supply chain mapping wherein(Wichmann et al.,2018)automatically extracted structured supply chain information from unstructured natural text to answer questions such as “who supplies whom with what from where?”, indicating that language models can help extract relationships of entities in supply chain networks.

Various researchers showed that relational knowledge that has been learnt in pretrained language models (pretrained LMs) can allow the recovery of factual knowledge from them(Petroni et al.,2019,Bouraoui et al.,2020,Safavi and Koutra,2021).
This points to the potential opportunity to retrieve relational knowledge of entities in supply chain networks from pretrained LMs to predict hidden dependencies.

As pretrained LMs are trained on massive data from diverse sources, the learned knowledge in these models is general and not capable of a task that requires specialised domain knowledge.
To solve this problem, two solutions can be used.
One is training a language model for a specific task and the other one is fine-tuning a pretrained LM for a specific task.
The former requires large computational resources and data, and is also time-consuming, prompting researchers to advocate the latter option of fine-tuning(Fichtel et al.,2021,Yasunaga et al.,2022).
Relevant examples include(Yasunaga et al.,2022), who fined-tuned a BERT model to predict links among documents and(Tan et al.,2024)developed a uniform framework to fine-tune language models on multiple tasks including link prediction.
Results from both works showed that fine-tuning pretrained LMs can accurately predict links. However, fine-tuning pretrained LMs requires expensive expertise in NLP, and most companies in supply chains are small and midsize enterprises (SMEs) limited by the budget to employ such expensive experts.
Additionally, SMEs typically lack access to sufficient high-quality labelled data for effective fine-tuning and are constrained by limited computational sources.

In addition, another concern for using pretrained LMs is the issue of “hallucination” leading to the generated outputs being not factual or inaccurate(Huang et al.,2023).
The phenomena of “hallucination” results from the biases and inaccuracies in massive amounts of training data from various sources.
In the supply chain context, such hallucinations can lead to erroneous demand forecasts or misinterpretation of supply chain relationships, potentially resulting in operational disruptions and financial losses.
For instance, a generative model might incorrectly predict a surge in demand based on fabricated trends, leading to overproduction and increased inventory costs.

(Agrawal et al.,2023,Martino et al.,2023,Guan et al.,2024)found that knowledge graphs that organise information into entities and relationships, creating a network of interconnected facts, can help mitigate the hallucination issue by providing a structured, verifiable source of information that the language models can reference during text generation. Contextual information represented by linkages in knowledge graphs can enhance the language models’ understanding of the relationships of different concepts to ensure the generated outputs being consistent with the established facts within knowledge graphs.

Building on prior research works that highlight the potential of relational knowledge learned in pretrained LMs(Petroni et al.,2019,Bouraoui et al.,2020,Safavi and Koutra,2021)and addressing the hallucination issue that knowledge graphs can alleviate, we develop a new approach that combines pretrained LMs with traditional machine learning techniques to predict multiple interconnected relationships within supply chain networks represented by knowledge graphs.
This new approach does not require fine-tuning.
The pre-trained language models are used to convert textual data into high-dimensional vector embeddings that capture semantic meanings and contextual nuances.
These embeddings are rich in linguistic information but may lack domain-specific factual accuracy when used in isolation, leading to hallucinations.
The machine learning models learn to map the semantic embeddings to the factual relationships represented in the knowledge graph.
This integration ensures that the predictions are not solely based on language patterns but also aligned with the actual data from the knowledge graph.
The knowledge graph acts as a factual anchor, constraining the model to produce outputs consistent with known supply chain relationships.
It can reduce the risk of hallucinations by providing a factual basis for relationship predictions, enhancing the reliability and accuracy of the model, and also leverage the strengths of pre-trained language models in understanding and encoding contextual information while counteracting their tendency to generate incorrect information when used alone.

SECTION: 2.2Link Prediction in Supply Networks

Supply chains are complex networks that exhibit non-linear interactions and inter-dependencies among various entities, processes and resources(Choi et al.,2001).
The large-scale and non-linear nature of these networks often hinder their visibility, which, in return, makes the identification of potential risks challenging.
Researchers have shown that predicting relationships of entities within a supply chain network can contribute to improved visibility(Brintrup et al.,2018).

Formally, researchers framed the problem of identifying relationships in a supply chain as alink predictionproblem on a graph.
Link prediction is widely used to solve problems in various domains such as social networks, where it predicts potential connections between individuals based on existing ties, interests, or behaviours(Hasan and Zaki,2011); recommendation systems, where it identifies associations among product sales(Su et al.,2020); and biological networks, where it predicts interactions among genes, proteins, and other biological entities(Coşkun and Koyutürk,2021).
The approaches used in the majority of these works are similarity-based and learning-based.
Similarity-based approaches predict the connection of two nodes by leveraging the similarity of characteristics between two nodes(Zareie and Sakellariou,2020), while learning-based approaches involve training machine learning models to infer the likelihood of connections between nodes based on node and network level features(Ahmed et al.,2016). Supply chain researchers have argued that similarity-based approaches are unsuitable for predicting links in this domain as similar companies usually do not connect due to competition, advocating for learning-based approaches.

(Brintrup et al.,2018)used two machine learning models to develop a link prediction approach to predict supplier interdependencies in a manufacturing supply network.(Kosasih and Brintrup,2022)developed a machine learning model using Graph Neural Network (GNN) to detect potential links that are unknown to the buyer in an automotive supply chain network.
Later on,(Kosasih et al.,2022)proposed a neurosymbolic machine learning method using a combination of GNN and knowledge graph reasoning to predict multiple types of links in two supply chain networks.
In the same year,(Brockmann et al.,2022)also performed link prediction using GNN but on an uncertain supply chain knowledge graph.(Brintrup et al.,2018)and(Kosasih and Brintrup,2022)predicted one type of relationship.(Kosasih et al.,2022)and(Brockmann et al.,2022)predicted multiple types of relationships on knowledge graphs.
Furthermore,(Mungo et al.,2023)considered production network reconstruction as link prediction and used Gradient Boosting to predict hidden relationships to reconstruct the production network.
Another work that focuses on the firm-level network reconstruction is from(Ialongo et al.,2022), in which a generalised maximum-entropy reconstruction method is introduced to reconstruct the firm-level network based on partial information.

While these approaches have been very valuable, the mere identification of a transactional relationship between companies does not allow sufficient contextual information for actionable insights.
Considering the case that Toyota and Hewlett Packard (HP) are predicted to share a transactional relationship, it might be that HP has sold a large number of office printing equipment to Toyota and it might also be that Toyota uses HP’s 3D printing material in its production.
For an analyst looking to identify whether a disruption at HP would cause an issue to the automotive industry, the two types of relationships would have different implications on actual production output. Similar issues arise when we consider other contextual information such as production locations. In this work, we will focus on adding context to identified relationships by a type of GenAI models, pretrained LMs, as a knowledge base(Srivastava et al.,2024).

Our hypothesis is that doing so will allow us to combine the structured factual knowledge that can be obtained from an ontological presentations afforded by knowledge graphs, with the general unstructured knowledge base that can be obtained from pretrained LM, thereby mitigating the hallucination issue caused by using pretrained LMs alone.
Besides, it will allow us to enhance the supply chain visibility by a complete supply chain knowledge graph for a comprehensive understanding of the supply chain dynamics.

SECTION: 3Combining Pretrained Language Models and Knowledge Graphs

SECTION: 3.1Preliminaries: From triplets to quintuplets

As mentioned earlier, the first set of studies in literature solved the supply chain link prediction problem on a graph with links representing companies(Brintrup et al.,2018,Kosasih and Brintrup,2022,Cai et al.,2021,Kazemi and Poole,2018).
This approach aimed at learning connection patterns surrounding two companies, estimating a connection between them (Figure1).
The second set of studies represent supply chain information as a knowledge graph (KG) with multiple types of links(Rossi et al.,2021,Kosasih et al.,2022).

A KG is represented by a heterogeneous graph G and its ontology O, the former being the actual data and the latter its schema.
KG can also be seen as a collection of facts represented by predicate logic statements.
A KG is based on an ontology, that defines data types and attributes, with a relational taxonomy.
Each item in the data is an entity (or a node in a graph), and the relationships between data are edges, or links.
In previous works, KGs have been used to model edges such as who-produces-what, who-has-what-certification, in addition to buyer-supplier links (Figure1), the structure of which then inform one another.

Both of the above approaches introduce triplets to describe and predict relationships.
A triplet, also known as a triple or a statement, consists of three components: subject, predicate, and object, and is used to define the relationship between a subject and an object (Figure1).
Singular links are analysed at a time - where we can predict“what a company produces”, and“to whom it sells”, but not contextual information such as“which product a company sells to its buyer in which location”.
Understanding context in supply chains is important to accurately predict risks.

To define context, we propose a new term, “quintuplet”, where information that is represented by three triplets,(Company A, has_product, Product 1), (Product 1, purchased_by, Company B)and(Company A, supplies_to, Company B), can be condensed into(Company A, supplies, Product 1, to, Company B).

Given a knowledge graph,is the set of entities andis a set of relationships.
The relationship between entityand entity, in a knowledge graph can be represented by a triplet, in whichand.

In contrast, a quintuplet would inform,and, which becomes the target of the prediction.
Compared to a triplet representing one relationship, a quintuplet describes multiple connected relationships.
The multiple connected relationships and the connected entities in a quintuplet can represent a small subgraph of the knowledge graph, leading to contextual information for an entity or relationship.

SECTION: 3.2The pretrained LM-based Machine Learning Framework

We propose a pretrained LM-based machine learning framework which transfers a knowledge graph described by triplets into quintuplets to generate composed texts, and then sends these composed snippets of text to a pretrained LM to retrieve the relational knowledge that has been learned a priori (Figure2).
The retrieved relational knowledge is represented by vectors of fixed length, that are further learned by a machine learning model to predict the multiple connected relationships of entities represented by quintuplets in supply chain networks.

We begin by constructing a knowledge graph from already known data that characterises the supply chain.
This may involve but is not limited to a priori known supply-buy relationships, products and certifications, and to a large extent determined by data that is available to the researcher.
The original data used to construct the supply chain knowledge graphs are commonly collected from various sources such as Enterprise Resource Planning (ERP) systems, transaction records, market reports, and social media (seeBrintrup et al.(2023)for a review).
The supply chain data used in this work was collected by a third-party data provider.
To construct a supply chain knowledge graph, we need to define the ontology using the data that can character supply chain information.
In this case, the ontology includes the definitions of entities, i.e.companies,productsandcertifications, and the relationships between these entities, such ashas_product,purchased_by,has_cert, andsupplies_to, shown in Figure2.
The knowledge defined by the ontology is structured into triplets, and each triplet is an instance of the relationships and entities defined by the ontology.

We define the quintuplets to represent the contextual knowledge that we aim to predict.
As a case example, we use product flow on supply-buy links, however a quintuplet can also represent contextual information on locations, types and volumes of transactions, depending on the question at hand.

We then reconstruct relationships originally represented by triplets, into quintuplets.
For example three triplets(Company A, has_product, Product 1), (Product 1, purchased_by, Company B)and(Company A, supplies_to, Company B)can be used to generate a quintuplet of the sort:(Company A, supplies, Product 1, to, Company B).
Another example is:(Company A, with, Certificate 1, has, Product 1)generated by two triplets,(Company A, has_cert, Certificate 1)and(Company A, has_product, Product 1).

The next step involves transferring quintuplets into composed snippets of text with a user-defined schema.
For example, a quintuplet of(Company A, supplies, Product 1, to, Company B)can be transferred intoCompany A supplies product 1 to company BorCompany A has product 1 and supplies it to company B.
The text used to represent a quintuplet can include different types of sentences but needs to be contextually accurate and consistent.

The composed text is then sent to a pre-trained language model for embedding and and retrieved hidden relational knowledge previously learned in the pre-trained language model.

The embeddings of quintuplets with retrieved relational knowledge are used to train a suitable machine learning model for quintuplet prediction.
As pretrained LMs cannot directly predict factual relationships in a supply chain, we use a machine learning model for it.
The resulting trained model can then be used for quintuplet prediction.

We select five general open-source pre-trained language models for experimentation with the following considerations:

Diversity: We test several pretrained LMs to investigate whether our approach is applicable across the state of the art.

Model size: Many existing works in the field of NLP(Shoeybi et al.,2019,Shin et al.,2020,Narayanan et al.,2021)suggest that larger model size can lead to improved performance. Therefore, pretrained LMs with different model sizes are selected for experimentation.

Output dimensions: Increasing the dimension of a language model can potentially capture more complex patterns and nuances in the data, however, cannot guarantee better representation by default(Kenton and Toutanova,2019).
Thus, selected pretrained LMs have different dimensions of their representations for evaluation.

Based on the considerations above, we select five pretrained LMs that all were developed on the basis of Transformer(Vaswani et al.,2017)but have different model sizes and output dimensions (Table3.2.1).

tableSelected pretrained language modelsModel NameModel SizeDimensionsMax Sequence LengthTraining Data SizeReferencedistiluse-base-multilingual-cased-v2480MB1 million sentence pairs (15 languages)512128(Reimers and Gurevych,2019)all-distilroberta-v1290MBover 1 billion pairs768512(Liu et al.,2019)all-MiniLM-L12-v2120MBover 1 billion pairs384256(Wang et al.,2020)all-MiniLM-L6-v280MBover 1 billion pairs384256(Wang et al.,2020)paraphrase-albert-small-v243MB16GB of uncompressed text768256(Lan et al.,2019)

Among the selected pretrained LMs, “paraphrase-albert-small-v2” is the smallest with 43 MB and a six-layer version of “albert-base-v2” that originates from(Lan et al.,2019)aiming to solve the problems of GPU/TPU memory limitations and longer training times by lowering model size.
Compared to the original BERT, “albert-base-v2” introduces two parameter reduction techniques to reduce memory consumption and increase training speed.

The second smallest model is “all-MiniLM-L6-v2”, a six-layer version of(Wang et al.,2020), developed by compressing the large Transformer-based pre-trained model using a simple but effective approach called deep self-attention distillation(Wang et al.,2020).
It introduces the conceptions of the student model and the teacher model.
“all-MiniLM-L6-v2” referred to as the student model in(Wang et al.,2020)is trained by mimicking the self-attention module, in Transformer networks, of the large language model referred to as the teacher model and also by distilling the self-attention module of the last Transformer layer of the teacher model.
In addition, “all-MiniLM-L6-v2” only keeps 50% of parameters of the teacher model but can retain more than 99% of accuracy on several benchmark tasks(Wang et al.,2020).
“all-MiniLM-L12-v2” is similar to “all-MiniLM-L6-v2” but is a 12-layer version of(Wang et al.,2020), leading to big model size with more parameters.

“all-distilroberta-v1” is a distilled version of the BERT base model in(Kenton and Toutanova,2019).
It is smaller and faster than BERT but developed using the BERT base model as a teacher.
Compared to “paraphrase-albert-small-v2”, “all-MiniLM-L6-v2” and “all-MiniLM-L12-v2”, this model size is larger but smaller than “distiluse-base-multilingual-cased-v2”.
“distiluse-base-multilingual-cased-v2” is also a modification of the pretrained BERT network, but trained on the data in 15 languages(Reimers and Gurevych,2019), compared to “all-distilroberta-v1” being trained in English.

Five machine learning models are selected based on the suitability of the model for application to link prediction and past works.
These include Artificial Neural Network (ANN)(Yegnanarayana,2009), Convolutional Neural Network (CNN)(Albawi et al.,2017), Logistic Regression (LogReg)(Kleinbaum et al.,2002), Long Short-Term Memory (LSTM)(Hochreiter and Schmidhuber,1997), and AutoEncoder(Wang et al.,2014).
Their architectures and parameter settings are presented in Table1.

ANN is composed of three linear layers with 300 neurons each and each linear layer is followed by a Batch Normalization layer (BatchNorm) and a Rectified Linear Unit (“ReLU”) activation function.
BatchNorm aims to normalise the output of each linear layer to ensure a more stable training process while the function “ReLU” contributes to the acceleration of the training phase and the mitigation of the problem of vanishing gradients.
The output from the last “ReLU” activation function is then sent to a fully-connected (FC) layer before being coupled with a “Softmax” function to achieve the final prediction.

The CNN model consists of three convolutional layers, each followed by the “ReLU” function, an Average Pooling layer (AvgPooling), and a BatchNorm layer.
“ReLU” and “BatchNorm” function the same as them in ANN while AvgPooling layer serves to decrease the dimensionality of the features outputted by the “ReLU”.
Similar to ANN, a FC layer followed by a Softmax function is used to output the final prediction.
Considering the computation cost and the performance, the first Conv1D layer is configured with 32 kernels of size 7 and of stride 2 while the second and third Conv1D layers use 64 kernels of size 7 and of stride 1.

The LogReg model has two linear layers, followed by a logistic function “Sigmoid”.
The first linear layer with 200 neurons and the second with 2 neurons are used to analyse the relationship between output and input features.
The LSTM model contains two LSTM layers with bi-directions, followed by the FC layer.
The input and hidden sizes in each layer are set as 16. The AutoEncoder model consists of an encoder for transforming the input to a compressed representation, a decoder for reconstructing the original input from the encoded representation, and an FC layer followed by the Softmax function for the final link prediction.
Both encoder and decoder are composed of two linear layers, each followed by the “ReLU” function.
To encode input to a compressed representation, the number of neurons in two linear layers are respectively 96 and 48.
In the decoder, two linear layers with 48 and 96 respectively are used to reconstruct the original input.

Since our task considers link prediction as a binary classification problem, all models excluding LogReg use a FC layer with 2 neurons.

SECTION: 4Case Study

The case study used to evaluate the proposed approach from the automotive sector where companies produce car parts, such as engines, front axles, fuel tanks and sell them to car manufacturing companies around the world.
The dataset has been used as a benchmark for link prediction problem within supply chain networks in previous works and therefore offers potential for cross-comparison(Brintrup et al.,2018,Kosasih and Brintrup,2022,Kosasih et al.,2022).
The dataset comprises 43,131 companies spanning 72 countries and producing 927 distinct products, each company associated with one or more of 5 certification types, along with the relationships among these entities (Table4).

tableBasic descriptions of Marklines dataEntityExampleUnique NumberCompanyHamenz For German Tech. Ind. (S.A.E.)43,131CountryEgypt79CertificateIS09001, QS9000, ISO/TS169495ProductPiston Ring Machining927

We separated the data at the country level so as to evaluate our approach over multiple heterogeneous datasets.
As shown in Table4each partition has different numbers of companies, products, certificates and relationships.

[before reading=,
tabular=—p2.5cm—p1.4cm—p1.4cm—p1.4cm—p1.4cm—,
table head =Country NameNum-CompanyNum-ProductNum-CertificateNum-Relations,
late after line=,
late after last line=]tables/data_description.csvcountry=\country, company= product=\product,certification=\certification, numberOfRelations=\numberOfRelations\country\product\certification\numberOfRelations

27 datasets have thus been generated.
As a starting point, we use the same knowledge graph ontology as previous works with three types of entities: companies, products, and certificates, and four types of relationships (triplets):(company, has_product, product),(company, has_cert, certificate),(company, supplies_to, company)and(product, purchased_by, company)(see Figure1c).

Four triplets are used to generate two quintuplets for the evaluation of the proposed approach.
Two quintuplets are(company, supplies, product, to, company)and(company, with, certificate, has, product).
The prediction problem thus is the existence of a given quintuplet.
Therefore, we consider the relationship prediction in a quintuplet as a binary classification problem.

Next, we explain how we generate positive and negative relationships based on quintuplets to train the models, followed by experimental settings and results.

SECTION: 4.1Generating training data

We refer to actual relationships in a quintuplet as positive relationships and non-existing relationships as negative relationships.
To train a machine learning model both positive relationships and negative relationships are needed.

Negative quintuplets are generated by the same triplets that were used to produce positive quintuplets.
Consider the following positive quintuplet,(company A, supplies, product 1, to, company B), three negative quintuplets can be generated by replacing any one of the three entities:(company A, has_product, product 1),(product 1, purchased_by, company B)and(company A, supplies_to, company B), using the one that does not connect the other two.

Given two lists of unique entities i.e. the list of unique companies and the list of unique products,(company A, supplies, product 1, to, company B), we randomly select a company named company C, that does not connect both company A and company B, from the list of unique companies to replace company A or company B.
Alternatively, we can randomly select a product named product 2, that does not connect to either company A and company B, and replace it with product 1.
Therefore, we can generate three negative quintuplets:(company C, supplies, product 1, to, company B),(company A, supplies, product 1, to, company C)and(company A, supplies, product 2, to, company B).
In addition, incorrect relationship direction is also considered as negative quintuplets, such as(company B, supplies, product 1, to, company A).

Based on the above method each positive quintuplet can be used to produce several negative quintuplets.
Since negative training data would far outweigh positive training data, we randomly select one negative quintuplet with the related positive quintuplet to train the model in order to have a balanced dataset.

SECTION: 4.2Experimental Settings

Experimental settings in this work include benchmark settings and model training settings.
The benchmark settings aim to evaluate whether machine learning models with the help of pretrained LMs can provide more accurate relationship predictions in supply chain networks while the model training settings aim to set the optimal parameters during the model training phase.

To evaluate the effectiveness of our proposed approach, we set machine learning models without the help of pretrained LMs as benchmarks.
The proposed approach is designed to power machine learning models by pretrained LMs so we also select five pretrained LMs (cf. Section3) to test the approach.

As the language models used in this work are pre-trained models, we only need to set parameters to train machine learning models, which are shown in Table1.
The parameters during the training phase include the number of epochs,, batch size,, and learning rate,.
To ensure the uniformity of experiments and follow common guidelines in machine learning training(Yang and Shami,2020), we setas 64 andas 0.001 for all five machine learning models.
For the number of epochs,, we use thestrategy to stop the training process if the training loss decreases but validating loss increases in 10 continuous epochs for the determination ofand also prevents overfitting.

We consider the problem of relationship prediction in supply chain networks as a binary classification problem mentioned earlier.
We thus use theCross-Entropy Lossas the loss function for all machine learning model training.Adam(Kingma and Ba,2014)is selected as the optimiser for all machine learning models.

In addition, following the common rules for splitting dataset into training, validating and testing, we use 70%, 10% and 20% of relationships present in each data partition.
All experiments are run on a desktop with ani9-9900K CPU and a GeForce RTX 2080 Ti GPU card with 11 GB physical memory.
PyTorch is used to develop and train all models.

Common metrics, includingaccuracy,precision,recallandf-score, used to evaluate the performance of a classification approach are also used to evaluate our proposed approach.
Table4.2.3shows a confusion matrix used to calculate the four metrics.
In our case,TPandTNrespectively represent positive and negative relationships that are predicted correctly whileFPandFNrespectively describe positive and negative relationships that are predicted incorrectly.
Based on Table4.2.3, the four common metrics can be calculated as:

accuracydescribes the ratio of correct relationship predictions to the total number of relationships.

precisionstands for the ratio of accurate predictions of positive relationships to the total number of predicted positive relationships.

recallrepresents the ratio of accurate predictions of positive relationships to the total number of positive relationships.

f-scoreshows the equilibrium between the precision and the recall,.

tableConfusion MatrixPredicted PositivePredicted NegativeReal PositiveTrue Positive(TP)False Negative(FN)Real NegativeFalse Positive(FP)True Negative(TN)

When we split the dataset into training, validating and testing, we randomly shuffle all positive and negative relationships for fairness.
This process may lead to an imbalanced testing dataset even though the overall dataset is balanced.
Therefore, to truly reflect the performance of our approach, we useweighted f-scoreshown in Equation (1) to replace the commonly usedf-score.

whereis the ratio of relationships for classover all relationships and is equal to.is the total number of relationships whileis the number of relationships in class.

In addition, we expect the developed approach to equally consider the importance of positive and negative relationships.
Thus, we follow(Grandini et al.,2020), who compared metrics for multi-class classification, and usebalanced accuracy weightedwhereandrespectively represent the ratio of positive relationships and the ratio negative relationships in the testing dataset (notes that), instead of the commonly used accuracy.balanced accuracy weighteddoes not only show the ability of the model to predict positive relationships but also reflect its ability to predict negative relationships.

SECTION: 4.3Experimental Results and Discussions

tableResults for the quintuplet of(company, supplies, product, to, company)Country NameLogReg(/Pre/Rec)LSTM(/Pre/Rec)CNN(/Pre/Rec)AutoEncoder(/Pre/Rec)ANN(/Pre/Rec)AUSTRALIA0.7953/0.7971/0.79360.8106/0.8106/0.81030.7872/0.7876/0.78730.8653/0.8675/0.86400.9037/0.9063/0.9024AUSTRIA0.7615/0.7619/0.76160.8853/0.8853/0.88530.9073/0.9075/0.90730.9199/0.9225/0.92000.9393/0.9418/0.9393BELGIUM0.8379/0.8375/0.83750.8748/0.8749/0.87560.8969/0.8968/0.89720.9251/0.9264/0.92670.9391/0.9392/0.9400BRAZIL0.8285/0.8292/0.82850.8672/0.8679/0.86720.9410/0.9414/0.94100.8899/0.8915/0.88980.9144/0.9152/0.9144CANADA0.8822/0.8857/0.88210.9245/0.9266/0.92460.9449/0.9458/0.94490.9702/0.9703/0.97020.9758/0.9759/0.9758CHINA0.8558/0.8558/0.85580.8698/0.8703/0.86970.8849/0.8852/0.88480.8841/0.8846/0.88390.8908/0.8909/0.8907FRANCE0.8418/0.8427/0.84170.9083/0.9093/0.90820.9265/0.9272/0.92650.9240/0.9261/0.92390.9325/0.9347/0.9324GERMANY0.8437/0.8466/0.84420.9062/0.9079/0.90600.9187/0.9189/0.91870.9012/0.9014/0.90120.9145/0.9146/0.9145HUNGARY0.8321/0.8323/0.83290.8650/0.8649/0.86560.8989/0.8988/0.89950.8820/0.8828/0.88290.8992/0.9000/0.9000INDIA0.8236/0.8256/0.82370.8749/0.8752/0.87490.8939/0.8940/0.89390.8736/0.8745/0.87360.9037/0.9039/0.9037INDONESIA0.8514/0.8522/0.85120.8686/0.8705/0.86840.9209/0.9217/0.92070.9130/0.9141/0.91280.9230/0.9240/0.9228ITALY0.8548/0.8554/0.85470.9138/0.9153/0.91370.9212/0.9233/0.92100.9242/0.9259/0.92410.9331/0.9347/0.9330JAPAN0.8469/0.8533/0.84690.8643/0.8662/0.86430.9112/0.9115/0.91120.8986/0.8990/0.89870.9080/0.9083/0.9080KOREA0.8782/0.8784/0.87820.9243/0.9249/0.92430.9434/0.9435/0.94340.9195/0.9195/0.91950.9311/0.9311/0.9311MALAYSIA0.8187/0.8206/0.81970.8645/0.8665/0.86540.9057/0.9061/0.90620.8950/0.8959/0.89560.9211/0.9225/0.9219MEXICO0.8513/0.8522/0.85140.9198/0.9204/0.91980.9267/0.9274/0.92660.9129/0.9132/0.91280.9361/0.9363/0.9361POLAND0.7763/0.7765/0.77410.8461/0.8458/0.84570.8510/0.8514/0.84970.8615/0.8619/0.86080.8760/0.8834/0.8737RUSSIA0.8276/0.8277/0.82720.8512/0.8523/0.85050.8726/0.8730/0.87220.8823/0.8834/0.88170.8937/0.8978/0.8935SPAIN0.8310/0.8351/0.83110.9030/0.9050/0.90310.9324/0.9341/0.93240.9365/0.9383/0.93650.9528/0.9542/0.9528SWEDEN0.8456/0.8459/0.84540.9160/0.9169/0.91570.9127/0.9157/0.91220.9543/0.9549/0.95420.9667/0.9674/0.9665TAIWAN0.8408/0.8408/0.84090.9219/0.9223/0.92170.9424/0.9424/0.94240.9091/0.9094/0.90890.9310/0.9311/0.9310THAILAND0.8614/0.8618/0.86170.8731/0.8745/0.87270.9141/0.9145/0.91390.8927/0.8928/0.89260.9055/0.9060/0.9053TURKEY0.8446/0.8449/0.84440.8725/0.8752/0.87180.9062/0.9075/0.90570.8975/0.8992/0.89700.9183/0.9192/0.9180U.S.A.0.8585/0.8603/0.85820.8859/0.8891/0.88620.9306/0.9306/0.93060.9139/0.9145/0.91380.9351/0.9352/0.9351UK0.8558/0.8559/0.85600.8673/0.8717/0.86800.9147/0.9148/0.91480.9073/0.9081/0.90750.9188/0.9191/0.9190

tableResults for the quintuplet of(company, with, certificate, has, product)Country NameLogReg(/Pre/Rec)LSTM(/Pre/Rec)CNN(/Pre/Rec)AutoEncoder(/Pre/Rec)ANN(/Pre/Rec)AUSTRALIA0.7052/0.7063/0.70520.6742/0.6750/0.67420.7052/0.7057/0.70520.7768/0.7780/0.77680.7987/0.8029/0.7987AUSTRIA0.6891/0.6901/0.68940.6698/0.6704/0.66950.6583/0.6622/0.65760.7708/0.7721/0.77100.8068/0.8072/0.8067BELGIUM0.7148/0.7162/0.71380.6784/0.6787/0.67780.7107/0.7115/0.70990.7651/0.7677/0.76420.7956/0.7981/0.7946BRAZIL0.7878/0.8012/0.78290.7908/0.8097/0.78520.8348/0.8361/0.83350.8185/0.8293/0.81470.8406/0.8440/0.8384CANADA0.7766/0.7801/0.76990.7684/0.7672/0.76830.7924/0.7911/0.79130.8225/0.8269/0.81760.8572/0.8585/0.8541CHINA0.7752/0.7844/0.78120.7961/0.7958/0.79710.8103/0.8097/0.81000.8058/0.8059/0.80380.8044/0.8050/0.8043FRANCE0.7743/0.7795/0.77160.8011/0.8032/0.79960.8183/0.8205/0.81680.8121/0.8177/0.80980.8262/0.8279/0.8249GERMANY0.7247/0.7248/0.72460.7610/0.7612/0.76090.7702/0.7708/0.77030.7787/0.7816/0.77810.7897/0.7904/0.7893HUNGARY0.6435/0.6450/0.64540.6658/0.6648/0.66490.7969/0.7967/0.79560.7353/0.7351/0.73440.7661/0.7654/0.7657INDIA0.7354/0.7413/0.73800.7716/0.7715/0.77120.8135/0.8143/0.81420.8067/0.8080/0.80550.8130/0.8135/0.8121INDONESIA0.7364/0.7409/0.73680.7278/0.7308/0.72810.7695/0.7710/0.76970.7705/0.7911/0.77130.8177/0.8220/0.8180ITALY0.7499/0.7546/0.75160.7283/0.7285/0.72860.8036/0.8050/0.80450.8257/0.8332/0.82780.8419/0.8451/0.8433JAPAN0.8413/0.8412/0.83990.8506/0.8529/0.84780.8492/0.8499/0.84740.8563/0.8584/0.85370.8590/0.8615/0.8562KOREA0.7466/0.7469/0.74660.7799/0.7804/0.78000.8173/0.8186/0.81730.7990/0.8046/0.79910.8039/0.8045/0.8039MALAYSIA0.6856/0.6864/0.68610.6937/0.6980/0.69530.7219/0.7253/0.72320.7358/0.7449/0.73770.7552/0.7619/0.7570MEXICO0.7342/0.7368/0.73400.6884/0.6896/0.68820.7729/0.7736/0.77290.7917/0.7982/0.79140.8214/0.8222/0.8213POLAND0.7339/0.7328/0.73390.7301/0.7285/0.72690.7842/0.7828/0.78280.7729/0.7725/0.77080.7954/0.7947/0.7926RUSSIA0.8536/0.8577/0.84880.7651/0.7640/0.76350.6365/0.6359/0.63530.8568/0.8569/0.85530.8693/0.8711/0.8663SPAIN0.7017/0.7037/0.70330.7592/0.7603/0.75740.7868/0.7875/0.78620.8234/0.8244/0.82320.8394/0.8406/0.8383SWEDEN0.6786/0.6784/0.67870.6828/0.6825/0.68170.6984/0.7007/0.69550.7531/0.7587/0.75000.8000/0.8057/0.7970TAIWAN0.7218/0.7221/0.72120.7639/0.7674/0.76470.8301/0.8309/0.83050.7963/0.7983/0.79700.8037/0.8043/0.8040THAILAND0.7064/0.7151/0.70970.7056/0.7056/0.70520.7483/0.7502/0.74960.7659/0.7666/0.76580.7824/0.7833/0.7810TURKEY0.7273/0.7279/0.72790.7047/0.7048/0.70390.8175/0.8212/0.81610.7796/0.7850/0.77790.8058/0.8066/0.8051U.S.A.0.7795/0.7863/0.78210.8148/0.8146/0.81470.8330/0.8333/0.83230.8378/0.8384/0.83720.8400/0.8409/0.8392UK0.7590/0.7612/0.76020.7932/0.7938/0.79340.8342/0.8356/0.83350.8117/0.8138/0.81060.8269/0.8277/0.8262

tablePretrained LM-enhanced machine learning, “all-MiniLM-L12-v2”, for different machine learning models for quintuplet(company, supplies, product, to, company)Country NameLogReg(/Pre/Rec)LSTM(/Pre/Rec)CNN(/Pre/Rec)AutoEncoder(/Pre/Rec)ANN(/Pre/Rec)AUSTRALIA1.0000/1.0000/1.00001.0000/1.0000/1.00001.0000/1.0000/1.00001.0000/1.0000/1.00000.9991/0.9990/0.9991AUSTRIA1.0000/1.0000/1.00001.0000/1.0000/1.00001.0000/1.0000/1.00001.0000/1.0000/1.00000.9614/0.9658/0.9615BELGIUM1.0000/1.0000/1.00001.0000/1.0000/1.00001.0000/1.0000/1.00001.0000/1.0000/1.00000.9982/0.9981/0.9982BRAZIL1.0000/1.0000/1.00000.9990/0.9990/0.99900.9996/0.9996/0.99960.9997/0.9997/0.99970.9994/0.9994/0.9994CANADA1.0000/1.0000/1.00001.0000/1.0000/1.00001.0000/1.0000/1.00001.0000/1.0000/1.00000.9998/0.9998/0.9998CHINA1.0000/1.0000/1.00000.9999/0.9999/0.99990.9998/0.9998/0.99980.9486/0.9239/0.94920.9999/0.9999/0.9999FRANCE1.0000/1.0000/1.00001.0000/1.0000/1.00000.9998/0.9998/0.99980.9999/0.9999/0.99990.9989/0.9989/0.9989GERMANY1.0000/1.0000/1.00001.0000/1.0000/1.00001.0000/1.0000/1.00000.9999/0.9999/0.99990.9998/0.9998/0.9998HUNGARY1.0000/1.0000/1.00000.9985/0.9985/0.99860.9998/0.9998/0.99981.0000/1.0000/1.00000.9989/0.9988/0.9989INDIA1.0000/1.0000/1.00001.0000/1.0000/1.00001.0000/1.0000/1.00000.9993/0.9993/0.99930.9999/0.9999/0.9999INDONESIA1.0000/1.0000/1.00001.0000/1.0000/1.00001.0000/1.0000/1.00001.0000/1.0000/1.00000.9983/0.9983/0.9983ITALY1.0000/1.0000/1.00001.0000/1.0000/1.00000.9998/0.9998/0.99981.0000/1.0000/1.00000.9999/0.9999/0.9999JAPAN1.0000/1.0000/1.00000.9999/0.9999/0.99990.9999/0.9999/0.99990.9980/0.9980/0.99800.9998/0.9998/0.9998KOREA1.0000/1.0000/1.00001.0000/1.0000/1.00001.0000/1.0000/1.00001.0000/1.0000/1.00000.9999/0.9999/0.9999MALAYSIA1.0000/1.0000/1.00001.0000/1.0000/1.00000.9997/0.9997/0.99970.9999/0.9999/0.99990.9959/0.9960/0.9958MEXICO1.0000/1.0000/1.00001.0000/1.0000/1.00001.0000/1.0000/1.00001.0000/1.0000/1.00000.9999/0.9999/0.9999POLAND1.0000/1.0000/1.00001.0000/1.0000/1.00001.0000/1.0000/1.00001.0000/1.0000/1.00000.9979/0.9979/0.9979RUSSIA1.0000/1.0000/1.00001.0000/1.0000/1.00000.9997/0.9997/0.99961.0000/1.0000/1.00000.9852/0.9881/0.9856SPAIN1.0000/1.0000/1.00001.0000/1.0000/1.00001.0000/1.0000/1.00001.0000/1.0000/1.00000.9994/0.9994/0.9994SWEDEN1.0000/1.0000/1.00001.0000/1.0000/1.00001.0000/1.0000/1.00001.0000/1.0000/1.00000.9999/0.9999/0.9999TAIWAN1.0000/1.0000/1.00000.9995/0.9995/0.99951.0000/1.0000/1.00001.0000/1.0000/1.00000.9992/0.9992/0.9992THAILAND1.0000/1.0000/1.00001.0000/1.0000/1.00001.0000/1.0000/1.00001.0000/1.0000/1.00000.9999/0.9999/0.9999TURKEY1.0000/1.0000/1.00000.9999/0.9999/0.99991.0000/1.0000/1.00001.0000/1.0000/1.00000.9999/0.9999/0.9999U.S.A.1.0000/1.0000/1.00000.9998/0.9998/0.99980.9997/0.9997/0.99970.9998/0.9998/0.99980.9998/0.9998/0.9998UK1.0000/1.0000/1.00000.9995/0.9995/0.99951.0000/1.0000/1.00000.9999/0.9999/0.99990.9994/0.9994/0.9994

tablePretrained LM-enhanced machine learning, “all-MiniLM-L12-v2”, for different machine learning models to predict relationships in the quintuplet of(company, with, certificate, has, product)Country NameLogReg(/Pre/Rec)LSTM(/Pre/Rec)CNN(/Pre/Rec)AutoEncoder(/Pre/Rec)ANN(/Pre/Rec)AUSTRALIA1.0000/1.0000/1.00001.0000/1.0000/1.00001.0000/1.0000/1.00001.0000/1.0000/1.00001.0000/1.0000/1.0000AUSTRIA1.0000/1.0000/1.00000.9990/0.9990/0.99890.9990/0.9990/0.99891.0000/1.0000/1.00001.0000/1.0000/1.0000BELGIUM1.0000/1.0000/1.00001.0000/1.0000/1.00001.0000/1.0000/1.00000.9992/0.9992/0.99920.9984/0.9984/0.9985BRAZIL1.0000/1.0000/1.00000.9899/0.9899/0.99000.9984/0.9984/0.99830.9992/0.9992/0.99920.9995/0.9995/0.9995CANADA1.0000/1.0000/1.00001.0000/1.0000/1.00001.0000/1.0000/1.00000.9998/0.9998/0.99980.9990/0.9990/0.9990CHINA1.0000/1.0000/1.00000.9996/0.9996/0.99960.9992/0.9992/0.99920.9992/0.9992/0.99920.9998/0.9998/0.9998FRANCE1.0000/1.0000/1.00000.9995/0.9995/0.99951.0000/1.0000/1.00000.9998/0.9998/0.99980.9996/0.9996/0.9996GERMANY1.0000/1.0000/1.00000.9994/0.9994/0.99940.9996/0.9996/0.99960.9998/0.9998/0.99980.9994/0.9994/0.9994HUNGARY1.0000/1.0000/1.00001.0000/1.0000/1.00001.0000/1.0000/1.00001.0000/1.0000/1.00001.0000/1.0000/1.0000INDIA1.0000/1.0000/1.00001.0000/1.0000/1.00001.0000/1.0000/1.00000.9997/0.9997/0.99970.9997/0.9996/0.9997INDONESIA1.0000/1.0000/1.00001.0000/1.0000/1.00001.0000/1.0000/1.00001.0000/1.0000/1.00001.0000/1.0000/1.0000ITALY1.0000/1.0000/1.00000.9993/0.9993/0.99930.9999/0.9999/0.99991.0000/1.0000/1.00000.9987/0.9987/0.9987JAPAN0.9998/0.9998/0.99990.9998/0.9998/0.99990.9999/0.9999/0.99990.9993/0.9993/0.99940.9992/0.9992/0.9993KOREA1.0000/1.0000/1.00001.0000/1.0000/1.00001.0000/1.0000/1.00001.0000/1.0000/1.00001.0000/1.0000/1.0000MALAYSIA0.9984/0.9985/0.99840.9984/0.9984/0.99851.0000/1.0000/1.00000.9984/0.9985/0.99840.9970/0.9971/0.9970MEXICO0.9983/0.9983/0.99831.0000/1.0000/1.00000.9998/0.9998/0.99980.9988/0.9988/0.99880.9979/0.9979/0.9979POLAND1.0000/1.0000/1.00001.0000/1.0000/1.00000.9983/0.9982/0.99850.9993/0.9993/0.99940.9992/0.9992/0.9992RUSSIA1.0000/1.0000/1.00001.0000/1.0000/1.00000.9990/0.9990/0.99891.0000/1.0000/1.00001.0000/1.0000/1.0000SPAIN1.0000/1.0000/1.00001.0000/1.0000/1.00000.9991/0.9991/0.99920.9993/0.9993/0.99930.9995/0.9995/0.9995SWEDEN0.9984/0.9984/0.99850.9984/0.9984/0.99850.9995/0.9995/0.99950.9979/0.9980/0.99790.9984/0.9984/0.9985TAIWAN1.0000/1.0000/1.00000.9991/0.9991/0.99910.9997/0.9997/0.99971.0000/1.0000/1.00000.9995/0.9995/0.9995THAILAND1.0000/1.0000/1.00000.9999/0.9999/0.99991.0000/1.0000/1.00001.0000/1.0000/1.00000.9994/0.9994/0.9994TURKEY1.0000/1.0000/1.00001.0000/1.0000/1.00001.0000/1.0000/1.00001.0000/1.0000/1.00000.9999/0.9999/0.9999U.S.A.1.0000/1.0000/1.00000.9994/0.9994/0.99940.9998/0.9998/0.99980.9998/0.9998/0.99980.9995/0.9995/0.9995UK1.0000/1.0000/1.00000.9989/0.9989/0.99890.9993/0.9993/0.99931.0000/1.0000/1.00000.9997/0.9997/0.9998

Table4.3.1and Table4.3.1respectively show results achieved by the machine learning models to predict the quintuplets of(company, supplies, product, to, company)and(company, with, certificate, has, product), while Table4.3.1and Table4.3.1present results achieved by the pretrained LM-enhanced approach.

Based on balanced accuracy weighted (), precision (Pre) and recall (Rec) in these tables, an obvious finding is that the pretrained LM-enhanced link prediction outperforms all general machine learning models for both quintuplets on all datasets.
This finding can also be observed in Figure3and Figure4that show theachieved by our proposed approach and general machine learning models for the link predictions in(company, supplies, product, to, company)and(company, with, certificate, has, product).
These results indicate that pretrained LMs indeed can help general machine learning models achieve better link predictions in supply chain networks, confirming observation that machine learning models benefit from the relational knowledge learned in pretrained LMs(Bouraoui et al.,2020,Petroni et al.,2019,Safavi and Koutra,2021).

In addition, the pretrained LM-enhanced approach presents more consistent results across different quintuplets, compared to results obtained by direct application of machine learning models, which yield poor performance in predicting quintuplets of(company, with, certificate, has, product)compared to quintuplets of(company, supplies, product, to, company).

Based on results achieved by machine learning models shown in Table4.3.1, Table4.3.1, Figure3(a) and Figure4(a), we find that, on all datasets, all machine learning models perform better for predicting relationships in the quintuplet of(company, supplies, product, to, company)than in(company, with, certificate, has, product).
This is because the number of(company, supplies, product, to, company)samples is more than the samples of(company, with, certificate, has, product)in each country’s dataset, which can be indirectly observed by a large number of companies and a small number of certificates and products shown in Table4.

Among the different machine learning models, ANN, CNN1D and AutoEncoder predict relationships more accurately in both types of quintuplets than LSTM and LogReg.
This finding matches a common conclusion from many existing works(Zheng et al.,2023,Caruana and Niculescu-Mizil,2006,Abu-Nimeh et al.,2007)that show ANN and CNN are better than LogReg and LSTM in binary classification tasks.

Figure5shows that results achieved by the pretrained LMs to enhance the CNN1D model provide higher prediction accuracy on both types of quintuplets.
Although all five pretrained LMs improve relationship prediction accuracy, the performances of pretrained LMs have subtle differences.

For example, notice that in Figure5(a) , “distiluse-base-multilingual-cased-v2” outperforms all other four in predicting(company, supplies, product, to, company).
This can also be observed in Figure5(b) for predicting quintuplet(company, with, certificate, has, product).

The performance of “distiluse-base-multilingual-cased-v2” is more consistent across all tasks, compared to the other four models.
This pretrained LM is trained on data in 15 languages(Reimers and Gurevych,2019)providing richer knowledge hidden in different languages.
Particularly for using language models as knowledge bases, language models trained on multilingual data can learn better representations than being trained on monolingual data(Pratap et al.,2020,Kassner et al.,2021).

Regarding the relationship predictions in different types of quintuplets, four of five pretrained LMs excluding “distiluse-base-multilingual-cased-v2” perform better predictions on(company, supplies, product, to, company)than on(company, with, certificate, has, product).
This is because relationships in the former describe a type of network-level information in supply chains while relationships in the latter present a type of internal information in a company.
Network-level information in supply chains, such as relationships between companies, tends to be more accessible and observable compared to internal information of a specific company, due to network-level information that can be inferred from public sources and industry publications.
Internal information about a company such as specific product certificates or product processes is more sensitive and not readily available to external observers.

Our findings can be summarised as follows:

Enhancing link prediction models with pretrained LMs outperforms all five benchmarks, indicating that pretrained LMs indeed can help common machine learning models achieve better relationship predictions in supply chain networks due to learned relational knowledge in these pretrained LMs.

Pretrained LM-enhanced machine learning model on prediction tasks is more consistent than using machine learning model alone, and is less affected by differences in dataset size.

Pretrained LM-enhanced models are better in predicting relationships that rely on network-level information, compared to relationships that rely on internal-company information.
This is because network-level information in supply chains tends to be more accessible and observable from public sources, compared to internal information of a specific entity in the network.
As such, pretrained LM-enhancement works better in cases where we predict who supplies, which product to whom, compared to for example, which quality certification a company may have for which product.

ANN, CNN1D and AutoEncoder that are commonly good at solving binary classification problems can predict relationships in supply chain networks more accurately than LSTM and LogReg in our case.

Pretrained multilingual LMs benefit common machine learning models better than monolingual LMs due to being trained on multilingual data to learn better representations.

SECTION: 5Conclusions, Managerial Implications, and Future Works

Relationship prediction, also called link prediction, or supply network reconstruction, is an emergent area of “digital supply chain surveillance” research that aims to increase visibility of supply networks using data-driven techniques without having to rely on the willingness of supply chain actors to share information.
Although many of the proposed methods have been very successful in reconstructing supply-buy relationships the context in which these relationships are embedded has thus far lacked attention.
This hinders researchers and practitioners to take full advantage of these methods, as they cannot accurately differentiate between a transactional relationship and established supply relationships that characterise physical resources needed to produce a product.
As such, estimations of resilience, distance to malicious actors and harmful practices based remain inaccurate.

Recently, Generative AI (GenAI) methods such as LLMs have become popular in eliciting information patterns from natural language data. There is also much hype in their potential in SCM. However we cannot simply ask an LLM whether a supply relationship exists, due to their hallucination problem. Hence we need methods to combine the power of GenAI methods with structured, guaranteeable methods when it comes to supply network surveillance. To date, there have been no studies on the use of LLMs for supply network surveillance.

In this work, we developed a framework that used GenAI and machine learning for predicting complex relationships of entities in supply chain networks.
We defined a new term, “quintuplet”, to describe the complex relationships that consist of multiple connected relationships and present a complete information flow, instead of commonly used triplets in the literature describing partial information.
Quintuplets provide contextual information to relationships, such as the flow of products a relationship is set for.
We define the link prediction problem as a binary classification problem, aiming to predict whether a quintuplet exists or not.
We then develop a machine learning-based approach enhanced by a type of GenAI models, pretrained LLMs, used as a knowledge base. This allows us to tap into the benefits of LLMs while at the same time, mitigate hallucinations.

A practical case study is used to evaluate the proposed approach with comparative benchmarks that use machine learning methods without pretained LM enhancement.
Pretrained LM-enhanced quintuplet prediction surpasses all benchmarks and provides consistent performance across all datasets with the advantage of providing contextual information, allowing stakeholders to be able to track the movements of products in a global network.

A secondary contribution of our research is a practical use case demonstrating the potential of GenAI models, particularly pretrained LMs, in supply chain management.
Most works in the literature regarding the use of GenAI models like large language models in supply chains are limited to theoretical discussion.
Therefore, our work here, as a practical study, helps bridge the gap between academic discourse and real-world application, and opens a door for more practical works in solving supply chain challenges with the use of language models.

Lastly, we have shows that pretrained LMs contain currently untapped relational supply chain knowledge presenting opportunities(Petroni et al.,2019,Bouraoui et al.,2020).
This finding should encourage researchers to explore further uses of pretrained LMs for solving other types of supply chain challenges.
In future work, we aim to test the developed link prediction method to other use case in industries, and explore other types of contextual knowledge that can be gained from language models for supply chain management.

SECTION: Data Availability Statement

Due to the commercially sensitive nature of this research, supporting data is not available.

SECTION: References