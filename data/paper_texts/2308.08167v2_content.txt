SECTION: A Quantum Approximation Scheme for-Means

We give a quantum approximation scheme (i.e.,-approximation for every) for the classical-means clustering problem in the QRAM model with a running time that has only polylogarithmic dependence on the number of data points. More specifically, given a datasetwithpoints instored in QRAM data structure, our quantum algorithm runs in timeand with high probability outputs a setofcenters such that.
Heredenotes the optimal-centers,denotes the standard-means cost function (i.e., the sum of squared distance of points to the closest center), andis the aspect ratio (i.e., the ratio of maximum distance to minimum distance).
This is the first quantum algorithm with a polylogarithmic running time that gives a provable approximation guarantee offor the-means problem.
Also, unlike previous works on unsupervised learning, our quantum algorithm does not require quantum linear algebra subroutines and has a running time independent of parameters (e.g., condition number) that appear in such procedures.

SECTION: 1Introduction

Data clustering and the-means problem, in particular, have many applications in data processing.
The-means problem is defined as: given a set ofpoints, and a positive integer, find a setofcenters such that the cost function,

is minimised. Here,is the Euclidean distance between pointsand.
Partitioning the points based on the closest center in the center setgives a natural clustering of the data points.
Due to its applications in data processing, a lot of work goes into designing algorithms from theoretical and practical standpoints.
The-means problem is known to be-hard, so it is unlikely to have a polynomial time algorithm.
Much research has been done on designing polynomial timeapproximationalgorithms for the-means problem.
However, the algorithm used in practice to solve-means instances is a heuristic, popularly known as the-means algorithm (not to be confused with the-means problem).
This heuristic, also known as Lloyd’s iterations[Llo82], iteratively improves the solution in several rounds.
The heuristic starts with an arbitrarily chosen set ofcenters. In every iteration, it (i) partitions the points based on the nearest center and (ii) updates the center set to the centroids of thepartitions.
In the classical computational model, it is easy to see that every Lloyd iteration coststime.
This hill-climbing approach may get stuck in a local minimum or take a huge amount of time to converge and hence does not give provable guarantees on the quality of the final solution or the running time.
In practice, Lloyd’s iterations are usually preceded by the-means++ algorithm[AV07], a fast sampling-based approach for picking the initialcenters that also gives an approximation guarantee.
So, Lloyd’s iterations, preceded by the-means++ algorithm, give the best of both worlds, theory, and practice.
Hence, it is unsurprising that a lot of work has been done on these two algorithms. This ranges from efficiency improvements in specific settings to implementations in distributed and parallel models.
With the quantum computing revolution imminent, it is natural to talk about quantum versions of these algorithms and quantum algorithms for the-means problem in general.

Early work on the-means problem within the quantum setting involved efficiency gains from quantizing Lloyd’s iterations.
In particular, Aimeur, Brassard, and Gambs[ABG13]gave antime algorithm for executing a single Lloyd’s iteration for the Metric-median clustering problem that is similar to the-means problem.
This was using the quantum minimum finding algorithm of Durr and Hoyer[DH99].
Using quantum distance estimation techniques assuming quantum data access, Lloyd, Mohseni, and Rebentrost[LMR13]gave antime algorithm for the execution of a single Lloyd’s iteration for the-means problem.
More recently,[KLLP19]gave an approximate quantization of the-means++ method and Lloyd’s iteration assumingQRAM data structure[kp17]access to the data.
Interestingly, the running time has only polylogarithmic dependence on the sizeof the dataset.
The algorithm uses quantum linear algebra procedures, and hence there is dependence on certain parameters that appear in such procedures, such as the condition number.
Since Lloyd’s iterations do not give an approximation guarantee, its quantum version is also a heuristic without a provable approximation guarantee.111Even though[KLLP19]gives a quantum version of the-means++ algorithm that has anapproximation guarantee, the guarantee for the quantum version (which has errors) is not shown explicitly.Our work on the-means problem builds upon the techniques developed in all the above and other works on quantum unsupervised learning to design algorithms with provable approximation guarantees.
Specifically, we want to design anapproximation schemefor the-means problem with a running time that has only a polylogarithmic dependence on the data sizeas in the algorithm of[KLLP19].
An approximation scheme is an algorithm that, in addition to the dataset and, takes an error parameteras input and outputs a solution with a cost withinfactor of the optimal.
We do this by quantizing the highly parallel, sampling-based approximation scheme of[BGJK20].
The tradeoff in obtaining this fine-grained approximation is that the running time of our algorithm has an exponential dependence onand error parameter.
In the classical setting, such algorithms are categorized as Fixed Parameter Approximation Schemes (fpt-AS).
Such-approximation algorithms can have exponential running time dependence on theparameter(e.g., the number of clustersin our setting).
The practical motivation for studying Fixed-Parameter Tractability for computationally hard problems is that when the parameter is small (e.g., number of clusters), the running time is not prohibitively large.
We state our main result as the following theorem, which we will prove in the remainder of the paper.

Letbe the error parameter.
There is a quantum algorithm that, when given QRAM data structure access to a dataset, runs in timeand outputs acenter setsuch that with high probability. Here,is the aspect ratio, i.e., the ratio of the maximum to the minimum distance between two given points in.222Thenotation hides logarithmic factors in. Thein the exponent hides logarithmic factors inand.

SECTION: 1.1An approximation scheme in the classical setting

We convert the-sampling-based approximation scheme of[BGJK20]to a Quantum version. The approximation scheme is simple and highly parallel, which can be described in the following few lines:

Input: Dataset, integer, and error

Output: A center setwith

(Constant approximation) Find a center setthat is a constant factor approximate solution. Anpseudo-approximate solution, for constants, also works.

(-sampling) Pick a setofpoints independently from the dataset using-sampling with respect to the center set.

(All subsets) Out of all-tuplesof (multi)subsets of, eachof size, returnthat gives the least-means cost. Here,denotes the centroid of points in.

We will discuss the quantization of the above three steps of the approximation scheme of[BGJK20], thus obtaining a quantum approximation scheme.333Steps (2) and (3) in the algorithm are within a loop for probability amplification. This loop is skipped in this high-level description for simplicity.

The first step requires finding a constant factor approximate solution for the-means problem.
Even though several constant factor approximation algorithms are known, we need one with a quantum counterpart that runs in time that is polylogarithmic in the input size.
One such algorithm is the-means++ seeding algorithm[AV07]that pickscenters in a sequence with thecenter picked using-sampling444-sampling: Given a center set,-sampling picks a datapoint with probability proportional to the squared distance of the point to the closest center in.with respect to the previously chosencenters.[KLLP19]give an approximate quantum version of-sampling.
The approximation guarantee of the-means++ algorithm isinstead of the constant approximation required in the approximation scheme of[BGJK20].
It is known from the work of[ADK09]that if the-sampling in-means++ is continued forsteps instead of stopping after samplingcenters, then we obtain a center set of sizethat is a-pseudo approximate solution.
This means that this-size center set has a-means cost that is some constant times the optimal.
Such a pseudo-approximate solution is sufficient for the approximation scheme of[BGJK20]to work.
We show that the pseudo-approximation guarantee of[ADK09]also holds when using the approximate quantum version of the-sampling procedure.

The second step of[BGJK20]involves-sampling, which we already discussed how to quantize.
This is no different than the-sampling involved in the-means++ algorithm of the previous step.
The sampling in this step is simpler since the center setwith respect to which the-sampling is performed, does not change (as is the case with the-means++ algorithm.)

Since the number of points sampled in the previous step is, we need to consider a list oftuples of subsets, each giving a-center set (a tupledefines).
We need to compute the-means cost for eachcenter sets in the list and then pick the one with the least cost.
We give quantization of the above steps.555Note that when picking the center set with the least cost, we can get quadratic improvement in the search for the best-center set using quantum search.
Given that the search space is of size, this results only in a constant factor improvement in the exponent.
So, we leave out the quantum search from the discussion for simplicity.

Note that the quantization of the classical steps of[BGJK20]will incur precision errors.
So, we first need to ensure that the approximation guarantee of[BGJK20]is robust against small errors in distance estimates,-sampling probabilities, and-means cost estimates.
We must carefully account for errors and ensure that the quantum algorithm retains theapproximation guarantee of the robust version of[BGJK20].

We begin the technical discussions in the next section by showing that the approximation scheme of[BGJK20]is robust against errors. We will also show the robustness of the-means++ procedure.
In the subsequent section, we give the quantization of the steps of[BGJK20].
First, we briefly discuss the related work.

SECTION: 1.2Related work

We have already discussed past research works on quantum versions of the-means algorithm (i.e., Lloyd’s iterations).
This includes[ABG13],[LMR13], and[KLLP19].
All these have been built using various quantum tools and techniques developed for various problems in quantum unsupervised learning, such as coherent amplitude and median estimation, distance estimation, minimum finding, etc. See[WKS15]for examples of several such tools.
Other directions on quantum-means includesadiabaticalgorithms (e.g.,[LMR13]) and algorithms using theQAOAframework (e.g.,[OMA+17,FGG14]). However, these are without provable guarantees.
A line of work has also suggested that quantum algorithms can outperform classical ones because of the QRAM data structure access.
A more level playing field is to assume that a similarsample and querydata access is available in the classical setting.
Under this assumption, several “dequantization” results for unsupervised machine learning algorithms have been given.
This includes[Tan19,CGL+20,Tan21].
It will be interesting to see if similar dequantization is possible for the quantum algorithms presented in this work since the main ingredient of our algorithm and the dequantization results is length-squared sampling.

SECTION: 2A Robust Approximation Scheme

We start the discussion with the-sampling method.
In particular, we would like to check the robustness of the approximation guarantee provided by the-sampling method against errors in estimating the distances between points.
We will show that the-sampling method gives a constant pseudo-approximation even under sampling errors.

SECTION: 2.1Pseudoapproximation using-sampling

Let the matrixdenote the dataset, where rowcontains thedata point.
Let the matrixany-center set, where rowcontains thecenterout of thecenters.
Sampling a data point using thedistribution w.r.t. (short for with respect to) a center setmeans that the datapointgets sampled with probability proportional to the squared distance to its nearest center in the center set.
This is also known assampling w.r.t. center set.
More formally, data points are sampled using the distribution, where.
For the special case,sampling is the same as uniform sampling.
The-means++ seeding algorithm starts with an empty center setand, overiterations, adds a center toin every iteration bysampling w.r.t. the current center set.
It is known from the result of[AV07]that this-means++ algorithm above gives anapproximation in expectation.
It is also known from the result of[ADK09]that ifcenters are sampled, instead of(i.e., the for-loop runs fromto), the cost with respect to thesecenters is at most some constant times the optimal-means cost.
Such an algorithm is called apseudo approximationalgorithm.
Such a pseudo approximation algorithm is sufficient for the approximation scheme of[BGJK20].
So, we will quantize the following constant factor pseudo-approximation algorithm.

In the quantum simulation of the above sampling procedure, there will be small errors in the sampling probabilities in each iteration. We need to ensure that the constant approximation guarantee of the above procedure is robust against small errors in the sampling probabilities owing to errors in distance estimation.
We will work with a relative error offor small.
Following is a crucial lemma from[AV07]needed to show the pseudo-approximation property of Algorithm 1.

Letbe an arbitrary optimal cluster, and letbe an arbitrary set of centers. Letbe a center chosen fromwith-sampling with respect to.
Then.

The above lemma is used as a black box in the analysis of Algorithm 1 in[ADK09].
The following version of the lemma holds for distance estimates with a relative error ofand gives a constant factor approximation guarantee.
Since Lemma1is used as a black box in the analysis of Algorithm 1, replacing this lemma with Lemma2also gives a constant factor approximation to the-means objective. We will use the following notion of the closeness of two distance functions.

A distance functionis said to be-close to distance function, denoted by, if for every pair of points,.666We use the notation that for positive reals,if.

Let.
Letbe an arbitrary optimal cluster andbe an arbitrary set of centers. Letbe a center chosen fromwith-sampling with respect to, where.
Then.

Letdenote the distance of the pointfrom the nearest center inand letdenote the estimated distance. We have. The following expression gives the expectation:

Note that for all,. This gives, which further givesand.
We use this to obtain the following upper bound on the expectation:

This completes the proof of the lemma.∎

We will use this lemma in the approximation scheme of[BGJK20].
However, this lemma may be of independent interest as this gives a quantum pseudo approximation algorithm with a constant factor approximation that runs in time that is polylogarithmic in the data size and linear inand. We will discuss this quantum algorithm in the next Section.

SECTION: 2.2Approximation scheme of[BGJK20]

A high-level description of the approximation scheme of[BGJK20]was given in the introduction. We give a more detailed pseudocode in Algorithm2.

In addition to the input instanceand error parameter, the algorithm is also given a constant approximate solution, which is used for-sampling. A pseudoapproximate solutionis sufficient for the analysis in[BGJK20].
The discussion from the previous subsection gives a robust algorithm that outputs a pseudoapproximate solution even under errors in distance estimates.
So, the input requirement of Algorithm2can be met.
Now, the main ingredient being-sampling, we need to ensure that errors in distance estimate do not seriously impact the approximation analysis of Algorithm2.
We state the main theorem of[BGJK20]before giving the analogous statement for the modified algorithm whereis replaced withthat is-close to.

Letbe the error parameter,be the dataset,be a positive integer, and letbe a constant approximate solution for dataset.
Letbe the list returned by Algorithm2on inputusing the Euclidean distance function. Then with probability at least,contains a center setsuch that.
Moreover,and the running time of the algorithm is.

We give the analogous theorem with access to the Euclidean distance functionreplaced with a functionthat is-close to.

Letbe the error parameter,be the closeness parameter,be the dataset,be a positive integer, and letbe a constant approximate solution for dataset.
Letbe the list returned by Algorithm2on inputusing the distance functionthat is-close to the Euclidean distance function. Then with probability at least,contains a center setsuch that.
Moreover,and the running time of the algorithm is.

The proof of the above theorem closely follows the proof of Theorem2.1of[BGJK20].
This is similar to the proof of Theorem2that we saw earlier, closely following the proof of Lemma1.
The minor changes are related to approximate distance estimates usinginstead of real estimates using.
The statement of Theorem2.2is not surprising in this light.
Instead of repeating the entire proof of[BGJK20], we point out the one change in their argument caused by usinginstead ofas the distance function.
The analysis of[BGJK20]works by partitioning the points in any optimal clusterinto those that are close toand those that are far.
For the far points, it is shown that when doing-sampling, a far point will be sampled with probability at leasttimes the uniform sampling probability (see Lemma 21 in[GJK20], which is a full version of[BGJK20]).
It then argues that a reasonable size set of-sampled points will contain a uniform sub-sample. A combination of the uniform sub-sample along with copies of points ingives a good center for this optimal cluster.
Replacingwithdecrease the value ofby a multiplicative factor of.
This means that the number of points sampled should increase by a factor of.
This means that the list size increases to.
Note that when, the list size and running time retains the same form as that in[BGJK20](i.e.,and time).

SECTION: 3Quantum Algorithms

We will work under the assumption that the minimum distance between two data points is, which can be acheived using scaling. This makes the aspect ratiosimply the maximum distance between two data points.
We will usefor an index into the rows of the data matrix, andfor an index into the rows of the center matrix.
We would ideally like to design a quantum algorithm that performs the transformation:

Let us call the state on the right.
This is an ideal quantum state for us sincehelps to perform-sampling and to find the-means cost of clustering, which are the main components of the approximation scheme of[BGJK20]that we intend to use.
One caveat is that we will only be able to perform the following transformation (instead of the abovementioned transformation)

whereis an approximation forin a sense that we will make precise below.
We will useto denote the state.
This state is prepared using tools such asswap testfollowed bycoherent amplitude estimation, andmedian estimation.
Since these tools and techniques are known from previous works[WKS15,LMR13,KLLP19], we summarise the discussion (see Section 4.1 and 4.2 in[KLLP19]) in the following lemma.

Assume for a data matrixand a center set matrixthat the following unitaries: (i), (ii)can be performed in timeand the norms of the vectors are known. For any, there is a quantum algorithm that in timecomputes:

wheresatisfies the following two conditions for everyand:

, and

For every,.

In the subsequent discussions, we will useas the time to access theQRAM data structure[kp17], i.e., for the transitionsandas given in the above lemma.
This is known to be. Moreover, the time to update each entry in this data structure is also.
This is the logarithmic factor that is hidden in thenotation.
In the following subsections, we discuss the utilities offor the various components of the approximation scheme of[BGJK20].
During these discussions, it will be easier to see the utility first with the ideal statebefore the real statethat can actually be prepared.
We will see howis sufficient within a reasonable error bound.

SECTION: 3.1Finding distance to closest center

Let us see how we can estimate the distance of any point to its closest center in a center setwithcenters.
We can use the transformationto prepare the following state for any:

We can then iteratively compare and swap pairs of registers to prepare the state.
If we apply the same procedure to, then with probability at least, the resulting state will be. So, the contents of the second register will be an estimate of the distance of thepoint to its closest center in the center set. This further means that the following state can be prepared with probability at least:777The state prepared is actuallywith. However, instead of working with this state, subsequent discussions become much simpler if we assume thatis prepared with probability.

This quantum state can be used to find the approximate clustering cost of the center set, which we discuss in the following subsection.
However, before we do that, let us summarise the main ideas of this subsection in the following lemma.

There is a quantum algorithm that, with probability at least, prepares the quantum statein time.

SECTION: 3.2Computing cost of clustering

Suppose we want to compute the-means cost,, of the clustering given by acenter set.
We can preparecopies of the stateand
then estimate the cost of clustering by measuringcopies of this quantum state and summing the squares of the second registers.
Ifis sufficiently large, we obtain a close estimate of.
To show this formally, we will use the following Hoeffding tail inequality.

Letbe independent, bounded random variables such that. Let. Then for any, we have:

Letdenotes the square of the measured value of the second register in.
These are random values in the range, where.
First, we note the expectation of these random variables equals, where.
We define the variableand apply the Hoeffding bound on these bounded random variables to get a concentration result that can then be used.

Letand. If, then we have:

We know thatFrom the Hoeffding tail inequality, we get the following:

This implies that:

This completes the proof of the lemma.∎

So, conditioned on havingcopies of the state, we get an estimate of the clustering cost within a relative error ofwith probability at least. Removing the conditioning, we get the same with probability at least.
We want to use the above cost estimation technique to calculate the cost for alistof center sets, and then pick the center set from the list with the least cost. We must apply the union bound appropriately to do this with high probability. We summarise these results in the following lemma.
Let us first set some of the parameters with values that we will use to implement the approximation scheme of[BGJK20].

denotes the size of the list of-center sets we will iterate over to find the one with the least cost. This quantity is bounded as.

is the number of copies of the statemade to estimate the cost of the center set. This, as given is Lemma5is, where.

Let,, and.
Given a point setand a list of center setsin the QRAM model, there is a quantum algorithm that runs in timeand outputs an indexsuch thatwith probbaility at least.

The algorithm estimates the cost ofusingcopies each ofand picks the index with the minimum value in time.
Plugging the values of, andwe get the running time stated in the lemma.

Let us bound the error probability of this procedure.
By Lemma4, the probability that we do not have the correctcopies each ofis bounded by. Conditioned of having, the probability that there exists an index, where the estimate is off by more than afactor is upper bounded byby the union bound. So, the probability that the algorithm will find an indexsuch thatis upper bounded by. This probability is at mostsince. This completes the proof of the lemma. ∎

SECTION: 3.3-sampling

-sampling from the point setwith respect to a center setwithcenters, sampleswith probability proportional to.
Let us see if we can use our stateis useful to perform this sampling.
If we can pull out the value of the second register as the amplitude, then measurement will give us close to-sampling.
This is possible since we have an estimate of the clustering cost from the previous subsection.
We can use controlled rotations on an ancilla qubit to prepare the state:

where.
So, the probability of measurement ofis. Since we do rejection sampling (ignoring’s that are sampled with probability), we end up sampling with a distribution where the probability of samplingis.
This means that points get sampled with a probability close to the actual-sampling probability. As we have mentioned earlier, this is sufficient for the approximation guarantees of[BGJK20]to hold.
We summarise the observations of this section in the next lemma.
We will need the following notion of the relative similarity of two distributions.

Let.
For two distributionsandover a finite set, we say thatif for every,.

Given a datasetand a center setin the QRAM model, there is a quantum algorithm that runs in timeand with probability at leastoutputsindependent samples with distributionsuch that, wheredenotes the-sampling distribution.

The proof follows from Lemma4and the preceding discussion.∎

The above lemma says that for, we obtain the required samples with high probability.
We can now give proof of Theorem1.1assembling the quantum tools of this section.

The first requirement for executing the algorithm of[BGJK20]is a constant pseudo approximation algorithm using which we obtain the initial center set.
By Lemma2, we know thatpoints sampled using-sampling gives such a center set. From Lemma7, this can be done quantumly in time, which also includes the timeto set up the QRAM data structure for alliterations.
The algorithm of[BGJK20]has an outer repeat loop for probability amplification.
Within the outer loop,points are-sampled with respect to the center set(line 6).
This can again be done quantumly using Lemma7in time.
We can then classically process the point set(see line 7 in Algorithm2) and create the QRAM data structure for the listof-center sets that correspond to all possible disjoint subsets of(see line 8 in Algorithm2). This takes time, where.
Theorem2.2shows that at least one center set in the list gives-approximation.
We use this fact in conjunction with the result of Lemma6to get that the underlying quantum algorithm runs in timeand with high probability outputs a center setsuch that.888We needed, but gotinstead. However, this can be handled with.∎

SECTION: 4Discussion and Open Problems

We give a quantum algorithm for the-means problem with a provable approximation guarantee offor arbitrarywith a polylogarithmic running time dependence on the data sizeand an exponential dependence on.
In the classical setting, there are FPT (fixed-parameter tractable) algorithms that have polynomial running time dependence on the input sizebut are allowed to have exponential dependence on theparameters(e.g.in the-means problem, which is typically a small number).
In this paper, we witnessed a case where we were able to take such a classical FPT algorithm into the quantum setting and lower the dependency onfrom linear in the classical setting[BGJK20]to polylogarithmic (this paper) while keeping the dependence on the parameters () intact. The aspect ratiocan be considered an additional parameter.
It would be interesting to see if there are other problems where such quantization is possible.
If so, discussing Quantum FPT (QFPT) algorithms with polylogarithmic dependence on the input size and possibly exponential dependence on the parameters would make sense.
Another future direction is to check whether thesample and query accessdefined by[Tan19]is sufficient to obtain comparable results in the classical setting.

SECTION: References