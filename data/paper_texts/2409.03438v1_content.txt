SECTION: Shuffle Vision Transformer: Lightweight, Fast and Efficient Recognition of Driver’s Facial Expression
Existing methods for driver’s facial expression recognition (DFER) are often computationally intensive, rendering them unsuitable for real-time applications. In this work, we introduce a novel transfer learning-based dual architecture, named ”ShuffViT-DFER,” which elegantly combines computational efficiency and accuracy. This is achieved by harnessing the strengths of two lightweight and efficient models using convolutional neural network (CNN) and vision transformers (ViT). We efficiently fuse the extracted features to enhance the performance of the model in accurately recognizing the facial expressions of the driver. Our experimental results on two benchmarking and public datasets, KMU-FED and KDEF, highlight the validity of our proposed method for real-time application with superior performance when compared to state-of-the-art methods.

SECTION: 
Human factors are responsible for a significant percentage of traffic road accidents. For this reason, there has been an increasing interest on driver’s facial expression recognition as a potential solution to improve road safety. Autonomous vehicles and Advanced Driver Assistance Systems (ADAS) both incorporate this feature, which enable recognizing and comprehending the emotional state of the driver. As a result, the systems are able to make well-informed decisions, which help to create a road environment that is safer and more effective.

In this context, several research works have focused on the development of techniques for the recognition of driver’s facial expressions as for example,,, and. However, most of these attempts were faced with the challenge of operating in real-time while accurately recognizing the driver’s emotional state in a real-world driving environment. This challenge usually involves a range of factors, such as non-frontal driver-head position, occlusions, and variation in lighting condition. While some approaches, especially those based on deep learning (e.g.,) perform better compared to traditional machine learning-based techniques (e.g.,), they require large amounts of data and significant computational resources for model training, making them less attractive in real-time applications. Nevertheless, some recent works have attempted to overcome the computational challenge by utilizing lightweight models designed to efficiently operate and with low latency at the cost of lower performance, as observed in,.

Our work introduces a novel transfer learning-based approach that combines performance and computational efficiency by leveraging lightweight and efficient models, making it suitable for embedded systems and real-time applications. The main contributions of our present work can be summarized as follows:

We introduce ShuffViT-DFER, a novel lightweight, and efficient approach for driver’s facial expression recognition.

We leverage the strengths of ShuffleNet V2and EfficientViTarchitectures, exploiting features from both models with a new classification scheme, achieving accurate and fast recognition.

We demonstrate that dual-architecture transfer learning allows to efficiently capture subtle facial cues and expressions with limited data while maintaining real-time processing.

We explore the use of Efficient ViT approach in drivers’ facial expression recognition by utilizing Grid Search to find the optimal hyper-parameters.

We conduct extensive experiments and evaluations on two benchmarking and publicly available databases, obtaining interesting performance compared to the state-of-the-art.

To support the principle of reproducible research, we share the code with the research community for comparison and future extensions at:.

SECTION: 
The pipeline of our proposed architecture is illustrated in Figure. The input to our model consists of a cropped face, detected using Multi-Task Cascaded Convolutional Networks (MTCNN). Subsequently, the detected faces undergo data augmentation for enhancing the size of the training set. Features are extracted using two pretrained models namely ShuffleNet V2and EfficientViT-M2. The extracted features are fed to a classifier for accurate expression recognition.

SECTION: 
The faces are first detected using MTCNN. Then, they are cropped and resized into 224224 pixels. Then, data augmentation is used by applying Random Horizontal Flip, Random Rotation, ColorJitter, Random Affine, and Gaussian Blur. The goal is to alleviate the overfitting issues related with small FER datasets. Finally, face normalization is performed.

SECTION: 
We extract features from both ShuffleNet V2 and EfficientViT-M2 models, employing transfer learning to address the limited data constraints and harness the strengths of both models for effective feature extraction, thereby enhancing classification accuracy. ShuffleNet V2, a lightweight CNN architecture, is known for its high computational efficiency and remarkable accuracy. However, it has limitations in capturing complex hierarchical features compared to deeper networks, which are needed for distinguishing subtle facial expressions. The outputs of ShuffleNet V2 model can be written as:. To enrich the extracted features, we also consider the High-Speed ViT family, specifically EfficientViT-M2. EfficientViT-M2 has indeed shown to be effective in encoding both local and global information, including more complex features, while utilizing limited computational resources. Its outputs can be written as:. We fuse the features extracted from both models into a single feature vector to enhance recognition accuracy and maintain trade-off between speed and accuracy. As a result we obtain:which can be expressed as:.

SECTION: 
For accurate classification, we consider three fully connected layers, facilitating linear transformations to capture additional features. The inclusion of two batch normalization layers serves to stabilize and accelerate the training process. Additionally, two ReLU activation functions are applied to introduce non-linearity and capture complex relationships within the data. To mitigate overfitting, we integrate two dropout layers, randomly disabling a portion of input units during training.

SECTION: 
Our model was implemented using the open-source PyTorch framework, on an NVIDIA GPU device, specifically the Quadro RTX 5000 with 16 GB of RAM. We conducted the experiments using two publicly available datasets namely KMU-FED and KDEF (See Figurefor some face samples).

SECTION: 
To evaluate the efficiency of our approach in real-world driving scenarios, we first used KMU-FED (Keimyung University Facial Expression of Drivers)dataset, which is captured in real driving environment. The database has a total of 1106 images from 12 subjects, with labels for the six basic emotions. The dataset includes different lighting variations and partial occlusions caused by hair or sunglasses. For comprehensive evaluation, we considered both 10-fold and 5-fold cross-validation protocol and a train–test ratio of 80%–20%.

In addition to KMU-FED dataset, we also considered the Karolinska Directed Emotional Face (KDEF) dataset, comprising 4900 images of human emotional facial expressions captured from 35 male and 35 female subjects at five different angles:,,,, and, and without any accessories, makeup, or glasses. It includes seven different emotions (afraid, angry, disgust, happy, neutral, sad, and surprise). For a fair comparison with previous works, we divided the KDEF dataset using a train–test ratio of 80%–20%.

SECTION: 
We utilized the Grid Search technique to determine the optimal hyper-parameters on both datasets. For the KMU-FED dataset, we used a batch size of 128 and the model was optimized using the Adaptive Moment Estimation (Adam) optimizer with a fixed learning rate of 0.001. The training process extended over 90 epochs, employing the cross-entropy loss function. For the KDEF dataset, a batch size of 32 was employed, and a learning rate of 0.0001 was utilized. Similar to the KMU-FED dataset, the Adam optimizer and cross-entropy loss function were applied. The training lasted 400 epochs.

SECTION: 
The results in terms of confusion matrices on KMU-FED and KDEF datasets are depicted in Figure, showing detailed overview of the recognition performance of our proposed model across different facial expressions. In the case of the KMU-FED dataset, we conducted experiments with varying data splits to investigate the generalization ability of our model. The results showed good performance for most expression categories across different split strategies. When 10-fold cross-validation (Figurea) is used, our model performed very well at identifying surprise, sadness, happiness, and fear. However, the performance decreased at differentiating between the disgust and anger emotions, due to the similarities in their appearance. In 5-fold cross-validation scenario, excellent performances were obtained by our model, successfully classifying the expressions of fear and surprise with no error rate. Moreover, our model showed very good accuracy in categorizing all expressions while using an 80%–20% data split, demonstrating the effectiveness of our proposed model on the KMU-FED dataset. For the KDEF dataset (Figureb), our model performed well, especially in recognizing neutral and happy expressions. However, a slight decrease in performance was observed for expressions of afraid, indicating that it can be challenging to distinguish between expressions of afraid, sad or surprise.

SECTION: 
To better gain insight into the performance of our model, we carried out an ablation analysis by comparing the performance of each individual module in our architecture before and after fusion. We considered a variety of factors to ensure a comprehensive comparison including the number of parameters, the processing time of single image, the accuracy, the precision, the recall, and the F1-score. The results are shown in Table. As expected, our model has slightly more parameters and uses longer processing time than the individual modules, while maintaining a relatively low computational cost and providing the best performances, yielding in an average accuracy of 97% and a processing time of 3.3 ms per a single image after face detection and cropping. The original ShuffleNet V2, either with its original classifier or with our proposed classifier, has less parameters, making it suitable for real-time applications but at the cost of lower performances. When EfficientViT-M2 is considered, the performances are increasing but still are less accurate than our proposed architecture. Based on these results, we can conclude that our proposed method does enhance the performance by combining the strengths of ShuffleNet V2 and EfficientViT-M2 alongside with the proposed classifier.

SECTION: 
We also performed a thorough comparison with state-of-the-art and some recently proposed methods. The results of the comparison are summarized in Tablesand. Our approach showed an average accuracy of 97.3% on the KMU-FED dataset which is comparable to the results published inand consistently outperforming all other approaches across different data splits. Our proposed method also outperformed all other methods on the KDEF dataset with an accuracy of 92.44%. These results assess the validity of our proposed architecture when it comes to the recognition of driver’s facial expressions.

SECTION: 
This paper introduced ShuffViT-DFER, an efficient and fast method for recognizing driver’s facial expressions. The approach adopted a transfer learning-based technique, utilizing two lightweight and efficient pre-trained models, ShuffleNet V2 and EfficientViT-M2. The method combines the strengths of the two approaches using a specialized classification scheme. Extensive experiments are conducted on two distinct and challenging datasets simulating real-world scenarios. The obtained results demonstrated the effectiveness of the proposed approach in improving the accuracy while maintaining low processing time as needed in real-time driving environments.

In future work, we plan to explore the integration of multi-modal information, such as combining facial expressions with audio cues. It is also of interest to consider driver’s facial expression recognition from multiple cameras to further enhance the performance and cope with the multi-view face angles.

SECTION: Acknowledgment
We wish to convey our deep appreciation to the EUNICE Alliance for the financial support.

SECTION: References