SECTION: A Comparative Study on Code Generation
with Transformers
In an era of widespread influence of Natural Language Processing (NLP), there have been multiple research efforts to supplant traditional manual coding techniques with automated systems capable of generating solutions autonomously. With rapid research for code generation and a sole focus on large language models, there emerges a need to compare and evaluate the performance of transformer architectures based on several complexities of the model. This paper introduces the concept of a ”A Comparative Study on Code Generation with Transformers,” a model based on Transformer architecture, and NLP methodologies to automatically generate C++ source code for different varieties of problems. Here, a comparative study is performed to evaluate the robustness of transformer-based models on the basis of their architecture complexities and their capability to handle diverse problem sets, from basic arithmetic to complex computations.

SECTION: 
Computer programming has become one of the foundations of technological evolution. With the rapid progress in machine learning and natural language processing, computer programming has become way easier with the help of numerous models capable of generating functional and executable code on the fly. With this research, we attempt to compare the performances and analyze the code execution capability of the generated output of transformer-based code generation models with different architectural complexities. The main objectives of the research are to train a base transformer for generating C++ source code via pseudo-code of problems related to arithmetic, array, string, and sorting operations; to perform transfer learning on a pretrained transformer model for better performance on pseudo-code to C++ source code conversion; to compare the results obtained from the base transformer and pretrained transformer model and analyze the performance. It must be noted that the comparison of performance also centers on the computational resources required by different models with different architectural designs. Additionally, previous works were primarily focused on converting one line of pseudocode into corresponding C++ code statements. The major issue they suffered was the initialization error. This problem was tackled in this project by using complete program pseudocode as input to the model.

SECTION: 
There have been numerous endeavors for automated source code generation. Before natural language processing and large language models, most code generation tasks were carried out using UML statecharts. The utilization of machine learning algorithms were limited to compiler heuristics optimization and parallelization. Initial implementations of natural language processing models for code generation can be seen in the work of Barone et al. where they attempted code generation using Python docstrings. Similarly, HTML code generation using wireframes was conducted by Aşiroğlu et al.. Later on, transformer based models primarily dominated the code generation domain. Different models were introduced which were customized for tasks such as code completion, code understanding, code generation and also to support multilingual tasks. Meanwhile, comparison studies were also conducted between different neural network architectures for code generation and filling mask tasks. Alphacode is one of the popular examples of transformer based code generation model for competitive programming.With the code generation, a number of metrics for code evaluation were also introduced, CodeBLEU being most popular among them. It is a modified version of popular machine translation evaluation metrics BLEU score. CodexGLUE, a benchmark for code generation was established by OpenAI to encourage research in multiple code related tasks including clone detection, defect detection, cloze test and code summarization. Also, MCoNala, a benchmark for code generation from multiple natural languages was introduced adding more diversity to the feat of automated source code generation. Recently, numerous code generation researches has been carried out introducing extremely versatile and multitasking models like CodeBERT, CodeT5 and GPT-based models which are gaining popularity for implementation and integration in varied set of use cases of code generation, code completion, code correction, code summarization and so on.The SPoC dataset used in this project was introduced and implemented by Kulal et al. using LSTM encoder and decoder. Their work was further analyzed and attempted by Kaan et al. using transformer architecture. It must be noted that previous works were primarily focused to convert one line of pseudocode into corresponding C++ code statements. The major issue they suffered was the initialization error. This problem was tackled in this project by using the complete program pseudocodes as input to the model.

SECTION: 
SPoC (Search-based Pseudocode to Code) dataset with 18,356 C++ programs for 677 programming problems having human-authored programs, pseudocodes and test cases, has been used for this project. Each problem has roughly 27 programs, which are likely to have similar semantics yet different code syntax. The variation in the problem set of the SPoC dataset is shown in Figure.

In the previous works, the approach taken was to convert one line of pseudocode into corresponding C++ code statement. This introduced initialization errors. To tackle this issue and to tap into the competency of transformer based models for large sequence of sentences, the dataset has been modified such that the all the code and pseudocode statements belonging to any one program are aggregated together to one input and one reference output to the model. Along with it, code for basic header file imports are also added such that the generated output can be directly used for code generation. The modification brought to the dataset is illustrated in Figureand Figure.

SECTION: 
This system implements a structured code generation workflow. The user-entered pseudocode undergoes UTF-8 encoding and is tokenized using a dedicated tokenizer. The parsed and tokenized input text is fed into an encoder, where queries gain similarity by referencing previously stored memory. Employing an attention-weighted mean, the weights are mapped into values, representing stored information. These values are passed through a decoder, detokenized, and then concatenated. The concatenated output tokens form the generated code, which is UTF-8 decoded and postprocessed for optimal UI display. The final result is a C++ output program derived from the user’s original pseudocode input.

SECTION: 
A comprehensive process was undertaken to enable the translation of pseudocode to C++ code through a custom-designed Transformer model and a pretrained transformer model fine-tuned on a new dataset and integrated into a web application. For the base transformer model, the BERT tokenizer was fine-tuned and customized to differentiate between reserved and input tokens. This facilitated the conversion of sentences into token IDs, essential for subsequent model input. Upon analyzing the pseudocode and C++ token vocabularies, the study identified 2285 unique tokens for pseudocode and 1989 for C++ programs. Positional encoding vectors with dimensions of 512 were established for 2048 positions.

Figuredepicts positional encoding vectors with a dimension of 512 for 2048 positions. Blue points indicate positive values, reaching a maximum of +1; white denotes zero; and red points depict negative values, reaching a minimum of -1.Additionally, masking was employed to disregard padding and look-ahead tokens. Hyperparameters were optimized through 5 iterations of random search, exploring layer counts (4-6), dmodel values (128-256), and dropout rates (0.1-0.2), with selected values of dmodel 128, dropout rate 0.1, and 4 layers for subsequent model training. Upon conducting training for 50, 80, and 30 epochs consecutively, it was noted that the model exhibited signs of overfitting during the 50 and 80 epoch training runs. Consequently, a training duration of 30 epochs with a batch size of 16 was selected as the optimal choice.Figureshows random search performed to optimize hyperparameters in the base transformer model, including the number of layers, dmodel, and dropout rate. Five iterations were conducted with varying values, and the lowest validation loss occurred with parameters (128, 0.1, 4), resulting in a loss of 2.403 that was selected for model training.

Comparing the base transformer model with pretrained counterparts, the SPoC dataset was fine-tuned on 6 layers of CodeT5-small, consisting of 60.5 million parameters, for pseudocode-to-C++ translation. CodeT5 had been trained for code translation tasks between Java and Csharp. On the grounds of semantic and syntactic similarity between Java and C++, the model was fine-tuned on the SPoC dataset. This further expanded the scope of the evaluation of the code generation capabilities of large language models. Tokenization of text utilized RobertaTokenizer, producing ’input_ids’ and ’attention_mask’. Dataset preprocessing led to the creation of a data dictionary with batch size of 8 for training and 4 for validation and test sets respectively.To optimize performance, a random search explored hyperparameters including learning rate (1e-3 to 5e-5) and warmup steps (500, 1000, 1500). Among five instances, the best results emerged at a learning rate of 8e-4 and 1000 warmup steps, with validation losses ranging from 0.0857 to 0.1116.

Figureshows the random search conducted to optimize hyperparameters in the CodeT5 transformer model, focusing on warmup steps, number of epochs, and learning rate. With the number of epochs fixed at one, five iterations were conducted with varying values of learning rate and warmup steps. The lowest validation loss was observed for the third iteration, with a learning rate of 8e-4 and 1000 warmup steps. Consequently, the model was trained for 5 epochs using a learning rate of 8e-4 and 1000 warmup steps.The implementation leveraged Google Colab’s GPU (12.68 GB of RAM) for model training, which took approximately 2 hours. A disk space of 78.19 GB was accessed during GPU. The BERT encoder model facilitated text tokenization. Keras from TensorFlow handled sequential processing, positional encoding, and detokenization in the base transformer model. PyTorch was utilized to load the CodeT5 model into Django and display output in the user interface.To leverage the advantage of reducing memory and computing time by only using a single integer for a class as opposed to a whole vector, these models are trained using the Sparse Categorical Cross Entropy Loss function, which measures the difference between the predicted probability distribution and the actual distribution (ground truth). Cross-entropy is defined as:

where,

Figureshows training and validation loss for the base transformer model trained for 30 epochs. Figureand shows training and validation loss respectively, for the CodeT5-small model trained for 5 epochs.

SECTION: 
In comparison, the results obtained from the CodeT5 small model showed better performance than the base transformer for complex problem sets. However, much difference was not seen for simpler arithmetic programs.

Models’ performance is evaluated using various metrics that capture different aspects of code generation quality. Let’s delve into the explanations of these metrics:

i. BLEU Score: BLEU (Bilingual Evaluation Understudy), a metric commonly used in NLP (Natural Language Processing) tasks, calculates the precision of n-grams in the generated output concerning the reference output. A higher BLEU score indicates better similarity.

where,

ii. CodeBLEU: Extending the BLEU score to code generation tasks, considering not only token precision but also the structural and syntactic correctness of the generated code, CodeBLEU provides a more tailored evaluation for code-related tasks.

iii. N-gram Match: This metric measures the proportion of matched n-grams between the generated code and the reference.

iv. Weighted N-gram Match: Considering not only the proportion of matched n-grams but also the significance of each match, this metric assigns weights to each matched n-gram based on their importance.

where,

v. Syntax Match Score: It evaluates the syntactic correctness of the generated code in comparison to the reference. The syntactic match score assesses how well the generated code adheres to the syntactic structures of the target programming language, C++, in this case.

vi. Dataflow Match: It assesses the alignment of data flow structures between the generated and reference code, accounting for the information passed and manipulated within the code. A higher dataflow match score indicates a closer correspondence in the data flow structures between the generated and reference code.

In Table III, additional white spaces can be observed in the output of the base transformer model, owing to punctuation marks treated as separate tokens by the model. The white spaces hinder the generated output from being used directly for execution. The ’return 0’ statement was not found in the output of CodeT5.

SECTION: 
From the study, it was found that, though the number of layers and training parameters heavily affect the code generation performance, acceptable results can be obtained by transformers with very few layers and training parameters. The model was able to overcome the initialization errors found in previous works of code generation with the SPoC dataset when a complete program pseudocode and its corresponding C++ source code were input as pairs in the model instead of statement-wise input. Also, the models trained on different programming languages can generate executable source codes for an unseen programming language due to syntactic similarities. Here, in the case of CodeT5, the model was originally trained for translation between Csharp and Java. The model was fine-tuned on a dataset of pseudocodes and C++ code pairs, and the results obtained were directly executable in most cases.The code generation ability of the base transformer model for complicated problems with multi-step logic execution was not found to be executable and had multiple syntactic errors. The reason can be considered to be the small size and low variation of code problems in the training dataset. The base transformer model’s performance seemed to decline with an increase in the complexity of problems, whereas CodeT5’s performance was found to be robust enough to handle a variety of problems. The reason can be attributed to the multi-tasking capability of the model with training on a huge dataset of CodeSearchNetwith multiple programming languages. It is worth mentioning that the performance of models built on transformer architecture when trained on a smaller dataset requires training for multiple epochs to obtain reasonable output. This results in overfitting of the model on the training set and degrades its ability to handle unseen data as inputs. Hence, dataset sizes can be seen as the bottleneck for transformer models. The model’s performance can be improved if the dataset is increased in size and variations in the complexity of problems are introduced.It was found that the robustness of multiple code generation models to generate code with few sentence commands is a result of their huge architectural design with multiple layers and huge training data corpus. Along with design, the resources required to train such a large language model, result in significant computational costs. The computational requirements are not limited to training and validation stages, but the operational costs of models during their inference stages are also high. The average time taken to generate code for any input varied from 5 to 15 seconds for CodeT5 and 15 to 60 seconds for the base transformer model, respectively. Time requirements were found to heavily depend on the device used for computation, with faster results on GPUs. For faster results, GPUs with higher computing capabilities are required, which again leads to increased costs. Hence, the integration of large language models into the deployed systems is an expensive approach to improve the systems.

SECTION: 
It can be concluded that the corpus on which language models are trained heavily affects their performance in generative tasks. Also, with increasing number of layers and parameters, the language models become more robust to handle unseen problems. It was observed that the CodeT5 model, though trained for translation between Csharp and Java, handled code generation tasks for the C++ language quite well when fine-tuned on pseudocode to code translation. The performance was found to be better than the four-layered base transformer model specifically trained for code generation from pseudocodes. The robustness of CodeT5 can be attributed to its large architecture with a higher number of layers than the base transformer and the large corpus of data that it has been trained on to carry out a number of coding tasks. In contrast, the resources and time required to train a few layered base transformer model was quite low in comparison to the resources required for training a large language model with billions of parameters. Hence, cost and performance trade-offs can significantly affect training a large language model for specific tasks.

SECTION: 
We would like to extend our deepest gratitude to our project supervisor Er. Dinesh Baniya Kshatri for providing us much needed guidance during this project. We are grateful to the Department of Electronics and Computer Engineering, Thapathali Campus for their help and suggestions in the selection and hopeful execution of this project. Our deepest gratitude to the widely growing machine learning community that provides so many resources for study and research available to learners.
At last we would like to thank everyone who were directly or indirectly involved for the successful execution of this research.

SECTION: References