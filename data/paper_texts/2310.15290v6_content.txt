SECTION: Reliable Generation of Privacy-preservingSynthetic Electronic Health RecordTime Series via Diffusion Models

Objective:Electronic Health Records (EHRs) are rich sources of patient-level data, offering valuable resources for medical data analysis. However, privacy concerns often restrict access to EHRs, hindering downstream analysis.Current EHR de-identification methods are flawed and can lead to potential privacy leakage. Additionally, existing publicly available EHR databases are limited, preventing the advancement of medical research using EHR. This study aims to overcome these challenges by generating realistic and privacy-preserving synthetic electronic health records (EHRs) time series efficiently.

Materials and Methods:We introduce a new method for generating diverse and realistic synthetic EHR time series data using Denoising Diffusion Probabilistic Models (DDPM). We conducted experiments on six databases: Medical Information Mart for Intensive Care III and IV (MIMIC-III/IV), the eICU Collaborative Research Database (eICU), and non-EHR datasets on Stocks and Energy. We compared our proposed method with nine existing methods.

Results:Our results demonstrate that our approach significantly outperforms all existing methods in terms of datafidelitywhile requiring less training effort.Additionally, data generated by our method yields a lower discriminative accuracy compared to other baseline methods, indicating the proposed method can generate data with less privacy risk.

Conclusion:The proposed diffusion-model-based method can reliably and efficiently generate synthetic EHR time series, which facilitates the downstream medical data analysis. Our numerical results show the superiority of the proposed method over all other existing methods.

Key words:electronic health records, time series generation, diffusion models.

SECTION: 1Introduction

The Electronic Health Record (EHR) is a digital version of the patient’s medical history maintained by healthcare providers.
It includes information such as demographic attributes, vital signals, and lab measurements that are sensitive and important for clinical research.
Researchers have been utilizing statistical and machine learning (ML) methods to analyze EHR for a variety of downstream tasks such as disease diagnosis, in-hospital mortality prediction, and disease phenotyping[Shickel18DeepEHR,Goldstein2017OpportunitiesAC].
However, due to privacy concerns, EHR data is strictly regulated, and thus the availability of EHR datafor research and educationis often limited, creating barriers to the development of computational models in the field of healthcare.
Widely used EHR de-identification methods to preserve patient information privacy are criticized for having high risks of re-identification of the individuals[Benitez2010EvaluatingRR].

Instead of applyingtraditional de-identificationmethods that can adversely affect EHR data utility[janmey2018re], EHR synthetic data generation is one promising solution to protect patient privacy.
Realistic synthetic data preserves crucial clinical information in real data while preventing patient information leakage[Yan2022AMB,Yoon2023EHRSafeGH].
Synthetic data also has the added benefit of providing a larger sample size for downstream analysis than de-identifying real samples[gonzales2023synthetic].
As a result, more research initiatives have begun to consider synthetic data sharing, such as the National COVID Cohort Collaborative supported by the U.S. National Institutes of Health and the Clinical Practice Research Datalink sponsored by the U.K. National Institute for Health and Care Research[Haendel2020TheNC,Herrett2015DataRP].
With the advancement in machine learning techniques, applying generative models to synthesize high-fidelity EHR data is popular research of interest[Yan2022AMB].
Recent advances in generative models have shown significant success in generating realistic high-dimensional data like images, audio, and texts[Gui2020ARO,Yi2018GenerativeAN], suggesting the potential for these models to handle EHR data with complex statistical characteristics.

Some representative work utilizing generative models for EHR data synthesis includes medGAN[Choi2017GeneratingMD], medBGAN[Baowaly2018SynthesizingEH], and EHR-Safe[Yoon2023EHRSafeGH].111We could not obtain code implementation for this work even after reaching out to the authors. Therefore, we are unable to compareTimeDiffwith this work’s proposed methods.However, most approaches to EHR data synthesis are GAN-based, and GANs are known for their difficulties in model training and deployments due to training instability and mode collapse[Saxena21GAN].
Recently, diffusion probabilistic models have shown superb ability over GANs in generating high-fidelity image data[ho2020denoising,Nichol2021ImprovedDD,rombach2022high].
A few studies thus propose to generate synthetic EHR data via diffusion models given their remarkable data generation performance[he2023meddiff,yuan2023ehrdiff].
However, most EHR data synthesis methods, either GAN-based or diffusion-based, focus on binary or categorical variables such as the International Classification of Diseases (ICD) codes.
Additionally, there is limited prior work on generating EHR data with temporal information, and most state-of-the-art time series generative models are GAN-based.\citeauthorKuo2023SyntheticHL[Kuo2023SyntheticHL]studied the diffusion models for EHR time series generation with focus only on continuous-valued time series.1It resorts to Gaussian diffusion for generating discrete sequences, treating them similarly to real-valued sequences but with further post-processing of the model output.
These observations motivate us to bridge the gap by introducing a novel direct diffusion-based method to generate realistic EHR time series data with mixed variable types.

Specifically, we make the following contributions in this paper:

We proposeTimeDiff, a new diffusion probabilistic model that uses a bidirectional recurrent neural network (BRNN) architecture for realistic privacy-preserving EHR time series generation.

To our best knowledge,TimeDiffis the first work introducing a mixed diffusion approach that combines multinomial and Gaussian diffusion for EHR time series generation.TimeDiffcan simultaneously generate bothcontinuousand discrete-valued time series.

We demonstrate thatTimeDiffoutperforms state-of-the-art methods for time series data generation by a big margin in terms of data fidelity and privacy. Additionally, our model requires less training effort than GAN-based methods.

SECTION: 2Background and Significance

Prior sequential generation methods using GANs rely primarily on binary adversarial feedback[mogren2016crnngan,esteban2017realvalued], and supervised sequence models mainly focus on tasks such as prediction[dai2015semisupervised], forecasting[lyu2018improving], and classification[srivastava2016unsupervised].
TimeGAN[yoon2019timegan]was one of the first methods to preserve temporal dynamics in time series synthesis.
The architecture comprises an embedding layer, recovery mechanism, generator, and discriminator, trained using both supervised and unsupervised losses.
GT-GAN[jeon2022gtgan]considers the generation of both regular and irregular time series data using a neural controlled differential equation (NCDE) encoder[kidger2020neural]and GRU-ODE decoder[de2019gru].
This framework, combined with a continuous time flow processes (CTFPs) generator[deng2021modeling]and a GRU-ODE discriminator, outperformed existing methods in general-purpose time series generation.
Recently,\citeauthorpmlr-v202-bilos23a[pmlr-v202-bilos23a]proposed to generate time series data for forecasting and imputation using discrete or continuous stochastic process diffusion (DSPD/CSPD).
Their proposed method views time series as discrete realizations of an underlying continuous function.
Both DSPD and CSPD use either the Gaussian or Ornstein-Uhlenbec process to model noise and apply it to the entire time series.
The learned distribution over continuous functions is then used to generate synthetic time series samples.

Diffusion models[sohl2015deep]have been proposed and achieved excellent performance in the field of computer vision and natural language processing.\citeauthorho2020denoising[ho2020denoising]proposed denoising diffusion probabilistic models (DDPM) that generate high-quality images by recovering from white latent noise.\citeauthorGu2021VectorQD[Gu2021VectorQD]proposed a vector-quantized diffusion model on text-to-image synthesis with significant improvement over GANs regarding scene complexity and diversity of the generated images.\citeauthorDhariwal2021DiffusionMB[Dhariwal2021DiffusionMB]suggested that the diffusion models with optimized architecture outperform GANs on image synthesis tasks.\citeauthorSaharia2022PhotorealisticTD[Saharia2022PhotorealisticTD]proposed a diffusion model, Imagen, incorporated with a language model for text-to-image synthesis with state-of-the-art results.\citeauthorKotelnikov2022TabDDPMMT[Kotelnikov2022TabDDPMMT]introduced TabDDPM, an extension of DDPM for heterogeneous tabular data generation, outperforming GAN-based models.\citeauthorDas2023ChiroDiffMC[Das2023ChiroDiffMC]proposed ChiroDiff, a diffusion model that considers temporal information and generates chirographic data.
Besides advancements in practical applications, some recent developments in theory for diffusion models demonstrate the effectiveness of this model class. Theoretical foundations explaining the empirical success of diffusion or score-based generative models have been established[song2019generative,song2020improved,chen2022sampling].

There exists a considerable amount of prior work on generating EHR data.\citeauthorChoi2017GeneratingMD[Choi2017GeneratingMD]proposed medGAN that generates EHR discrete variables.
Built upon medGAN,\citeauthorBaowaly2018SynthesizingEH[Baowaly2018SynthesizingEH]suggested two models, medBGAN and medWGAN, that synthesize EHR binary or discrete variables on International Classification of Diseases (ICD) codes.\citeauthorYan2020GeneratingEH[Yan2020GeneratingEH]developed a GAN that can generate high-utility EHR with both discrete and continuous data.\citeauthorBiswal2020EVAGL[Biswal2020EVAGL]proposed the EHR Variational Autoencoder that synthesizes sequences of EHR discrete variables (i.e., diagnosis, medications, and procedures).\citeauthorhe2023meddiff[he2023meddiff]developed MedDiff, a diffusion model that generates user-conditioned EHR discrete variables.\citeauthoryuan2023ehrdiff[yuan2023ehrdiff]created EHRDiff by utilizing the diffusion model to generate a collection of ICD diagnosis codes.\citeauthornaseer2023scoehr[naseer2023scoehr]used continuous-time diffusion models to generate synthetic EHR tabular data.\citeauthorceritli2023synthesizing[ceritli2023synthesizing]applied TabDDPM to synthesize tabular healthcare data.

However, most existing work focuses on discrete or tabular data generation.
There is limited literature on EHR time series data generation, and this area of research has not yet received much attention[Koo2023ACS].
Back in 2017, RCGAN[esteban2017realvalued]was created for generating multivariate medical time series data by employing RNNs as the generator and discriminator.
Until recently,\citeauthorYoon2023EHRSafeGH[Yoon2023EHRSafeGH]proposed EHR-Safe that consists of a GAN and an encoder-decoder module.
EHR-Safe can generate realistic time series and static variables in EHR with mixed data types.\citeauthorli2023generating[li2023generating]developed EHR-M-GAN that generates mixed-type time series in EHR using separate encoders for each data type.\citeauthorTheodorou2023SynthesizeHL[Theodorou2023SynthesizeHL]suggested generating longitudinal continuous EHR variables using an autoregressive language model.
Moreover,\citeauthorKuo2023SyntheticHL[Kuo2023SyntheticHL]suggested utilizing diffusion models to synthesize discrete and continuous EHR time series.
However, their approach mainly relies on Gaussian diffusion and adopts a U-Net architecture[ronneberger2015u].
The generation of discrete time series is achieved by taking argmax of softmax over real-valued one-hot representations.
By contrast, our proposed method considers multinomial diffusion for discrete time series generation, allowing the generation of discrete variables directly.\citeauthorhe2024flexible[he2024flexible], a concurrent work to ours, introduces FLEXGEN-EHR for synthesizing heterogeneous longitudinal EHR data through a latent diffusion method.
It also addresses missing modalities by formulating an optimal transport problem to create meaningful latent embedding pairs.
In comparison, our work introduces a direct diffusion model to generate heterogeneous EHR data, effectively handling potential missingness directly within the generation process.

SECTION: 3Materials and Methods

We use four publicly available EHR datasets to evaluateTimeDiff: Medical Information Mart for Intensive Care III and IV (MIMIC-III/IV)[johnson2016mimic,Johnson2023MIMICIVAF]and the eICU Collaborative Research Database (eICU)[Pollard2018TheEC]. Additionally, to evaluateTimeDiffwith state-of-the-art methods for time series generation on non-EHR datasets, we include Stocks and Energy datasets from studies that proposed TimeGAN[yoon2019timegan]and GT-GAN[jeon2022gtgan].

We evaluate our methods and make comparisons on a series of metrics, both qualitative and quantitative, characterizing the authenticity of the synthesized data, theperformancefor downstream analysis – in-hospital mortality prediction, and the preservation of privacy:

t-SNE visualization: We flatten the feature dimension and use t-SNE dimension reduction visualization[van2008visualizing]on synthetic, real training, and real testing samples.
This qualitative metric provides visual guidance on the similarity of the synthetic and real samples in two-dimensional space. Details are described inA.5.2.

UMAP visualization: We follow the same procedure as using t-SNE for visualization of distribution similarity between synthetic, real training, and real testing samples.
UMAP preserves a better global structure compared to t-SNE[McInnes2018UMAPUM], and thus we provide it as a complementary metric.

Discriminative and Predictive Scores: A GRU-based discriminator is trained to distinguish between the synthetic and real samples.
For the predictive score, a GRU-based predictor is trained using synthetic samples and evaluated on real samples for next-step vector prediction based on mean absolute error over each sequence.
Details of the score computations are described inA.5.2.

Train on Synthetic, Test on Real (TSTR): We train ML models using synthetic data and evaluate them on real test data based on the area under the receiver operating characteristic curve (AUC) for in-hospital mortality prediction.
We compare the TSTR score to the Train on Real, Test on Real (TRTR) score, which is the AUC obtained from the model trained on real training data and evaluated on real test data.

Train on Synthetic and Real, Test on Real (TSRTR): Similar to the TSTR, we train ML models and evaluate them on real test data using AUC.
We use 2,000 real training data in combination with different proportions of synthetic samples to train ML models.
This metric evaluates the impact of synthetic data for training on ML model performance.Note that we use 2,000 real training samples to simulate the real-world scenario where clinical researchers struggle to obtain limited real EHR data.
In this case, we evaluate the viability of usingTimeDiffto generate realistic samples as a data augmentation technique.

Nearest Neighbor Adversarial Accuracy Risk (NNAA): This score measures the degree to which a generative model overfits the real training data[yale2020generation].NNAA is an important metric for evaluating the privacy of synthetic data as it quantifies the risk of re-identification by measuring how easily an adversary can distinguish between real and synthetic data points. Thus, this metric effectively indicates the potency of anonymization techniques in protecting sensitive information within the synthetic dataset.

Membership Inference Risk (MIR): An F1 score is computed based on whether an adversary can correctly identify the membership of a synthetic data sample[liu2019socinf].MIR provides a precise measurement of the security of synthetic datasets, particularly in assessing the likelihood that individual data points can be traced back to the original dataset, thereby evaluating the robustness of data anonymization techniques.

For all the experiments, we split each dataset into training and testing sets and used the training set to develop generative models.
The synthetic samples obtained from trained generative models are then used for evaluation.
We repeat each experiment over 10 times and report the mean and standard deviation of each quantitative metric.
Further details for our experiments and evaluation metrics are discussed inAppendixA.

We compareTimeDiffwithninemethods:HALO[Theodorou2023SynthesizeHL], EHR-M-GAN[li2023generating], GT-GAN[jeon2022gtgan], TimeGAN[yoon2019timegan], RCGAN[esteban2017realvalued], C-RNN-GAN[mogren2016crnngan], RNNs trained with teacher forcing (T-Forcing)[graves2013generating,sutskever2011generating]and professor forcing (P-Forcing)[lamb2016professor], and discrete or continuous stochastic process diffusion (DSPD/CSPD) with Gaussian (GP) or Ornstein-Uhlenbeck (OU) processes[pmlr-v202-bilos23a].222We also hoped to include comparison with EHR-Safe[Yoon2023EHRSafeGH]. However, despite attempts, we were unable to obtain the code implementation.In addition we compare with standard GRU and LSTM approach, with results inTable1.

We first introduce our notations for the generation of bothcontinuous-valued anddiscrete-valued time series in our framework, as both are present in EHR.
Specifically, letdenote our EHR time series dataset.
Each patient inhas continuous-valued anddiscrete-valued multivariate time seriesand, respectively.is the number of time steps, andandare the number of variables for continuous and discrete data types.

To generate bothcontinuous-valued anddiscrete-valued time series, we consider a “mixed sequence diffusion” approach by adding Gaussian and multinomial noises.
For continuous-valued time series, we perform Gaussian diffusion by adding independent Gaussian noise similar to DDPM.
The forward process is thus defined as:

whereandis theobservation of the continuous-valued time series. In a similar fashion asEquation12, we define the reverse process for continuous-valued features as, where

To model discrete-valued time series, we use multinomial diffusion[NEURIPS2021_67d96d45]. The forward process is defined as:

whereis a categorical distribution,is a one-hot encoded representation of333We perform one-hot encoding on the discrete-valued time series across the feature dimension. For example, if our time series is, its one-hot representation becomes., and the addition and subtraction between scalars and vectors are performed element-wise.
The forward process posterior distribution is defined as follows, whererepresents the Hadamard product that returns a matrix with each element being the product of the corresponding elements from the original two matrices:

The reverse processis parameterized as.
We train our neural network,, using both Gaussian and multinomial diffusion processes:

whereandare the losses for continuous-valued and discrete-valued multivariate time series, respectively.
The training of the neural network is performed by minimizing the following loss:

whereis a hyperparameter for creating a balance between the two losses. We investigate the effects ofinSectionB.6.

In medical applications, missing data and variable measurement times play a crucial role as they could provide additional information and indicate a patient’s health status[zhou2023missing].
We thus derive a missing indicator mask444Or alternatively,if the time series is discrete-valued.for each555For simplicity in writing, we refer toonly, but this procedure can also be applied on.:

Thenencodes the measurement time points of.
Ifcontains missing values, we impute them in the initial value of the forward process, i.e.,, using the corresponding sample mean.666Using the sample mean for imputation is a straightforward and computationally efficient method. It helps maintain the central tendencies and distributional characteristics of the original data, minimizing the introduction of biases that might occur with more complex methods[little2019statistical,enders2022applied].Nevertheless,retains the information regarding the positions of missing values.
Our method generates discrete and continuous-valued time series, allowing us to seamlessly represent and generateas a discrete time series.

In this section, we describe our architecture for the diffusion model.
A commonly used architecture in DDPM is U-Net[ronneberger2015u].
However, most U-Net-based models are tailored to image generation tasks, requiring the neural network to process pixel-based data rather than sequential information[song2020score,ho2020denoising,rombach2022high].
Even its one-dimensional variant, 1D-U-Net, comes with limitations such as restriction on the input sequence length (which must be a multiple of U-Net multipliers) and a tendency to lose temporal dynamics information during down-sampling.
On the other hand, TabDDPM[Kotelnikov2022TabDDPMMT]proposed a mixed diffusion approach for tabular data generation but relied on a multilayer perceptron architecture, making it improper for multivariate time series generation.

To address this challenge of handling EHR time series, we need an architecture capable of encoding sequential information while being flexible to the input sequence length.
The time-conditional bidirectional RNN (BRNN) or neural controlled differential equation (NCDE)[kidger2020neural]can be possible options.
After careful evaluation, we found that BRNN without attention mechanism offers superior computational efficiency and have chosen it as the neural backbonefor all of our experiments.
A more detailed discussion of NCDE is provided in Supplementary MaterialA.4.1.

To inform the model about the current diffusion time step, we use sinusoidal positional embedding[vaswani2017attention].
The embedding vector output from the embedding layer then goes through two fully connected (FC) layers with GeLU activation in between[hendrycks2016gaussian].
The embedding vector is then fed to a SiLU activation[hendrycks2016gaussian]and another FC layer.
The purpose of this additional FC layer is to adjust the dimensionality of the embedding vector to match the stacked hidden states from BRNN.
Specifically, we set the dimensionality of the output to be two times the size of the hidden dimension from BRNN.
We denote the transformed embedding vector as.
This vector is then split into two vectors, each with half of the current size, namelyand.
Both vectors share the same dimensionality as BRNN’s hidden states and serve to inform the network about the current diffusion time step.

In practice, BRNN can be implemented with either LSTM or GRU units.
To condition BRNN on time, we follow these steps.
We first obtain noisy samples from Gaussian (for continuous-valued data) and multinomial (for discrete-valued data) diffusion.
The two samples are concatenated and fed to our BRNN, which returns a sequence of hidden statesthat stores the temporal dynamics information about the time series.
To stabilize learning and enable proper utilization of, we apply layernorm[ba2016layer]on.
The normalized sequence of hidden states,, is then scaled and shifted using.
These scaled hidden states contain information about the current diffusion step, which is then passed through an FC layer to produce the final output.
The output contains predictions for both multinomial and Gaussian diffusions, which are extracted correspondingly and used to calculateinEquation9.A visual demonstration of our architecture is shown inFigure1, where the use of BRNN allows the denoising of noisy time series samples of arbitrary length, and the diffusion step embedding is utilized to inform the model about the stage of the reverse diffusion process.

SECTION: 4Results

MetricMethodStocksEnergyMIMIC-IIIMIMIC-IVeICUTimeDiff.048.028.088.018.028.023.030.022.015.007EHR-M-GAN.483.027.497.006.499.002.499.001.488.022DSPD-GP.081.034.416.016.491.002.478.020.327.020DSPD-OU.098.030.290.010.456.014.444.037.367.018CSPD-GP.313.061.392.007.498.001.488.010.489.010CSPD-OU.283.039.384.012.494.002.479.005.479.017DiscriminativeGT-GAN.077.031.221.068.488.026.472.014.448.043ScoreTimeGAN.102.021.236.012.473.019.452.027.434.061()RCGAN.196.027.336.017.498.001.490.003.490.023C-RNN-GAN.399.028.499.001.500.000.499.000.493.010T-Forcing.226.035.483.004.499.001.497.002.479.011P-Forcing.257.026.412.006.494.006.498.002.367.047HALO.491.006.500.000.497.003.494.004.370.074Real Data.019.016.016.006.012.006.014.011.004.003TimeDiff.037.000.251.000.469.003.432.002.309.019EHR-M-GAN.120.047.254.001.861.072.880.079.913.179DSPD-GP.038.000.260.001.509.014.586.026.320.018DSPD-OU.039.000.252.000.497.006.474.023.317.023CSPD-GP.041.000.257.0011.083.002.496.034.624.066CSPD-OU.044.000.253.000.566.006.516.051.382.026PredictiveGT-GAN.040.000.312.002.584.010.517.016.487.033ScoreTimeGAN.038.001.273.004.727.010.548.022.367.025()RCGAN.040.001.292.005.837.040.700.014.890.017C-RNN-GAN.038.000.483.005.933.046.811.048.769.045T-Forcing.038.001.315.005.840.013.641.017.547.069P-Forcing.043.001.303.006.683.031.557.030.345.021HALO.042.006.299.053.816.020.767.012.378.038Real Data.036.001.250.003.467.005.433.001.304.017

We evaluate the authenticity of the generatedsyntheticEHR time series both qualitatively and quantitatively.
We provide a visualization of the distributions of the synthetic and real data using t-SNE, following[Yoon2023EHRSafeGH], shown inFigure2.Additionally, we present another visualization metric using UMAP inFigure3.
Both visualization methods indicate the synthesized data generated fromTimeDiffoverlaps with real training and test data, suggestingTimeDiffcan generate more realistic data compared to other baselines.Visualizations of the raw synthetic and real data per feature are presented inSectionB.1.Note that t-SNE and UMAP are only for qualitative evaluation and are not precise. We next present quantitative metrics for precise evaluation.By comparing the predictive and discriminative scores inTable1, we observe thatTimeDiffyields significantly lower scores than all the baseline methods across six datasets.
For instance,TimeDiffyields a 95.4% lower mean discriminative score compared to DSPD-GP and obtains a 1.6% higher mean predictive score than real testing data on the eICU dataset.
For non-EHR datasets,TimeDiffachieves a 37.7% lower and a 60.2% lower mean discriminative scores on the Stocks and Energy datasets than GT-GAN while having similar mean predictive scores as using real testing data.

We evaluate the dataperformanceof the generatedsyntheticEHR time series on one common downstream task: in-hospital mortality prediction[sadeghi2018early,sheikhalishahi2019benchmarking].
We use six ML algorithms: XGBoost (XGB)[chen2016xgboost], Random Forest (RF)[breiman2001random], AdaBoost (AB)[freund1997decision], andandregularized Logistic Regression (LR L1/L2)[friedman2010regularization].
Additionally, to simulate the practical scenario where synthetic samples are used for data augmentation, we compute the TSRTR score for each ML model.
The prediction models are trained using synthetic samples fromTimeDiffand assessed on real test data.

FromFigure4, we observe that the TSTR scores obtained from models trained using synthetic EHR time series are close to the TRTR scores yielded from models trained using real data. We also notice a non-decreasing trend in the TSRTR scores as the percentage of synthetic EHR data increases for ML model training.Additional TSTR and TSRTR evaluations for all baseline generative models can be found in Supplementary MaterialB.4.2.

Note that in addition to the six ML classifiers mentioned above, we utilize GRU and LSTM for prediction due to their ability in handling sequential data. We present the TSTR and TRTR scores obtained from RNNs in Supplementary MaterialB.4.3,Table10. We observe that they achieve lower scores compared to the conventional classifiers.

We also assess the risks of the generatedsyntheticEHR time series being attacked by malicious entities using the NNAA and the MIR scores.These metrics allow us to evaluate whether our approach can produce privacy-preserving synthetic EHR samples.As presented inTable2, we observe thatTimeDiffyields theandscores around 0.5 across all four EHR datasets.TimeDiffalso obtains low NNAA and MIR scores compared to baseline methods.
Note that the full results are presented in Supplementary MaterialB.4.

DatasetTimeDiffEHR-M-GANTimeGANGT-GANMIMIC-III2.718.910.821.8MIMIC-IV2.728.829.547.3eICU8.787.111059.1

We compare the number of hours to trainTimeDiffwith EHR-M-GAN, TimeGAN, and GT-GAN presented inTable3.
As shown inTable3,TimeDiffrequires less training time compared to GAN-based approaches.

Note that in our experiments, to demonstrate that our proposed approach outperforms GANs in terms of training time, we primarily compared the training time of our proposed diffusion model with GANs. Most of the GANs primarily involve pre-training the embedding layer and subsequently training with adversarial feedback. This staged procedure made GANs more computationally heavy to train than diffusion models (which only requires optimizing one loss function for one neural network in our proposed approach).

We further investigate the effect of utilizing multinomial diffusion inTimeDiffon missing indicators for EHR discrete sequence generation.
We compare it withTimeDiffusing Gaussian diffusion, with the following two methods applied to the resulting output as transformations to discrete sequences:
(1) direct rounding; (2) applying argmax to the softmax output of real-valued, one-hot encoded representations.777The synthetic one-hot encoding is not discrete since we use Gaussian diffusion. This method is also adopted by[Kuo2023SyntheticHL]for generating discrete time series with diffusion models.

We present the discriminative and predictive scores obtained using the aforementioned methods on the MIMIC-III/IV and the eICU datasets inTable4.
We notice thatTimeDiffusing multinomial diffusion obtains lower discriminative and predictive scores across all three datasets.

SECTION: 5Discussion

The synthetic samples generated byTimeDiffexhibit remarkable overlap with real training and testing data (seeFigure2), indicating that the generated samples preserve similar data distribution to real data.
Note that we obtain the same observation across all datasets, with the rest of the visualizations presented in Supplementary MaterialB.2.

The discriminative and predictive scores inTable1suggest that the synthetic time series generated byTimeDiffhas the closest data distribution to real data compared to samples generated by other baseline methods.
We notice that the DSPD/CSPD baseline method from a recent work does not yield good performance on EHR datasets.
This observation can be attributed to its temporal modeling, which treats time series as discrete realizations of an underlying continuous process.
This continuity assumption may not hold for EHR time series data, which are highly discontinuous.

Additionally,Table4from the ablation study indicates that the synthesized EHR time series is more realistic when using multinomial diffusion inTimeDiff.Evaluation using TSTR and TSRTR metrics is also performed to compareTimeDiffwith the “with Gaussian and softmax” alternative, where we observe thatTimeDiffoutperforms the alternative.
The result is presented in Supplementary MaterialB.4.2,Figure26andFigure35.

We observe fromFigure4that models trained using synthetic time series yield similar AUC scores compared to those trained on the real data, indicating that the synthetic EHR time series obtained fromTimeDiffmaintains high data utility for performing downstream tasks.
Additionally, we notice that most of the ML models yield increasing AUC scores with the increase in the number of synthetic samples added to model training.
This observation is consistent with our previous findings, indicating the high utility of our synthetic data.

The close to 0.5 scores ofandcomputed fromTimeDiffshown inTable2suggest thatTimeDiffgenerates high-fidelity synthetic time series and does not overfit its training data.
By contrast, although most of the baseline methods have low NNAA and MIR scores, they all have higherandscores, which implies that there may be overfitting on the training data for baseline methods.

Lastly, to assess the effects of EHR time series generation, most features selected in our study are frequent measurements such as vital signs.
This design choice enables us to evaluate the ability ofTimeDiffto generate sequential measurements without interference from measurement frequencies.
Thus, our study does not focus on infrequent time series measurements or static features.
Nevertheless, we acknowledge that this is a limitation in our study and have conducted additional experiments on the ability ofTimeDiffto generate static and infrequent measurements.
The results can be found in Supplementary MaterialB.7.

SECTION: 6Conclusion

We proposeTimeDiffforsyntheticEHR time series generation by using mixed sequence diffusion and demonstrate its superior performance compared with all state-of-the-art time series generation methods in terms of data utility.
We also demonstrate thatTimeDiffcan facilitate downstream analysis in healthcare while protect patient privacy.Thus, we believeTimeDiffcould be a useful tool to support medical data analysis by producing realistic, synthetic, and privacy-preserving EHR data to tackle data scarcity issues in healthcare.However, it is important to acknowledge the limitations of our study.
While our results suggest thatTimeDiffoffers some degree of patient privacy protection, it should not be seen as a replacement for official audits, which may still be necessary prior to data sharing.
It is also interesting to investigateTimeDiffwithin established privacy frameworks, e.g., differential privacy. Additionally, to provide better interpretability and explainability ofTimeDiff, subgroup analysis and theoretical analysis are to be developed.While we utilized sample mean imputation for computational efficiency, more advanced missing value imputation techniques could be considered to further evaluateTimeDiff’s behavior.Lastly, it would also be meaningful to investigate the modeling of highly sparse and irregular temporal data, such as lab tests and medications.
We leave the above potential improvements ofTimeDifffor future work.

SECTION: Competing Interests

All authors declare no financial or non-financial competing interests.

SECTION: Data Availability

The EHR datasets utilized during the current study are available in the following repositories:the MIMIC repositoryandthe eICU Collaborative Research Database.

We also consider the following non-EHR time-series dataset for comparisons.
Stocks: the dataset is available online and can be accessed from thehistorical Google stock price on Yahoo; Energy: this dataset can be obtained fromUCI machine learning repository.

SECTION: Code Availability

Our code is available athttps://github.com/MuhangTian/TimeDiff.

SECTION: Funding Statement

This work was supported by CS+ program in 2023 at the Department of Computer Science, Duke University. S. J. and A. R. Z were also partially supported by NSF Grant CAREER-2203741 and NIH Grants R01HL169347 and R01HL168940.

SECTION: Contributorship Statement

Muhang Tian, Bernie Chen, and Allan Guo were primarily responsible for the design and execution of the experiments and for writing the manuscript.
Shiyi Jiang and Anru Zhang were responsible for overseeing the project and writing the manuscript.

SECTION: Competing Interests Statement

The authors have no competing interests to declare.

SECTION: Supplementary Text to “Reliable Generation ofPrivacy-preserving SyntheticEHR Time Series via Diffusion Models”

SECTION: Appendix AExperiment Details

In this section, we provide further information on the datasets used in this study and the corresponding data processing procedures.
Unless specified otherwise, all datasets are normalized by min-max scaling for model training, and the minimums and maximums are calculated feature-wise, i.e., we normalize each feature by its corresponding sample minimum and maximum, and this procedure is applied across all the features.
For all EHR datasets, we extract the in-hospital mortality status as our class labels for TSTR and TSRTR evaluations.

DatasetSample SizeNumber of FeaturesSequence LengthMissing (%)Mortality Rate (%)Stocks3,7736240—Energy19,71128240—MIMIC-III26,150152517.97.98MIMIC-IV21,59311727.923.67eICU62,453927610.510.63

As mentioned earlier in the Results section, we use the Stocks and Energy datasets for a fair comparison betweenTimeDiffand the existing GAN-based time-series generation methods.

Stocks:The Stocks dataset contains daily Google stock data recorded between 2004 and 2019.
It contains features such as volume, high, low, opening, closing, and adjusted closing prices.
Each data point represents the value of those six features on a single day.

Energy:The Energy dataset is from the UCI Appliances energy prediction data. It contains multivariate continuous-valued time series and has high-dimensional, correlated, and periodic features.

To prepare both datasets for training and ensure consistency with previous approaches for a fair comparison, we use the same procedure as TimeGAN.
We then apply training and testing splits for both datasets.
For the Stocks dataset, we use 80% for training and 20% for testing.
For the Energy dataset, we use 75% for training and 25% for testing.

TheMedical Information Mart for Intensive Care-III(MIMIC-III) is a single-center database consisting of a large collection of EHR data for patients admitted to critical care units at Beth Israel Deaconess Medical Center between 2001 and 2012.
The dataset contains information such as demographics, lab results, vital measurements, procedures, caregiver notes, and patient outcomes.
It contains data for 38,597 distinct adult patients and 49,785 hospital admissions.

Variable Selection:In our study, we use the following vital sign measurements from MIMIC-III: heart rate (beats per minute), systolic blood pressure (mm Hg), diastolic blood pressure (mm Hg), mean blood pressure (mm Hg), respiratory rate (breaths per minute), body temperature (Celsius), and oxygen saturation (%).
To ensure consistency and reproducibility, we adopt the scripts inofficial MIMIC-III repositoryfor data pre-processing that selects the aforementioned features based onitemidand filters potential outliers888For reproducibility, the thresholds for the outliers are defined by the official repository..
We then extract records of the selected variables within the first 24 hours of a patient’s unit stay at one-hour intervals, where the initial measurement is treated as time step 0.
This procedure gives us a multivariate time series of length 25 for each patient.

Cohort Selection:We select our MIMIC-III study cohort by applying the outlier filter criteria adopted by the official MIMIC-III repository.
The filtering rules can be accessedhere.
We select patients based on the unit stay level usingicustay_id.
We only include patients who have spent at least 24 hours in their ICU stay.
We use 80% of the dataset for training and 20% for testing while ensuring a similar class ratio between the splits.

TheMedical Information Mart for Intensive Care-IV(MIMIC-IV) is a large collection of data for over 40,000 patients at intensive care units at the Beth Israel Deaconess Medical Center.
It contains retrospectively collected medical data for 299,712 patients, 431,231 admissions, and 73,181 ICU stays.
It improves upon the MIMIC-III dataset, incorporating more up-to-date medical data with an optimized data storage structure.
In our study, we use vital signs for time-series generation.
To simplify the data-cleaning process, we adopt scripts from theMIMIC Code Repository.

Variable Selection:We extracted five vital signs for each patient from MIMIC-IV.
The selected variables are heart rate (beats per minute), systolic blood pressure (mm Hg), diastolic blood pressure (mm Hg), respiratory rate (breaths per minute), and oxygen saturation (%).
We extract all measurements of each feature within the first 72 hours of each patient’s ICU admission.
Similar to MIMIC-III, we encode the features using the method described in the Missing value representation section for model training.

Cohort Selection:Similar to MIMIC-III, we select our MIMIC-IV study cohort by applying filtering criteria provided by the official MIMIC-IV repository.
The criteria can be accessedhere.
We also select patients at the unit stay level and include those who stayed for at least 72 hours in the ICU.
We use 75% for training and 25% for testing, and the class ratio is kept similar across the training and testing data.

TheeICU Collaborative Research Databaseis a multi-center database with 200,859 admissions to intensive care units monitored by the eICU programs across the United States.
It includes various information for the patients, such as vital sign measurements, care plan documentation, severity of illness measures, diagnosis information, and treatment information.
The database contains 139,367 patients admitted to critical care units between 2014 and 2015.

Variable Selection:We select four vital sign variables from thevitalPeriodictable in our study: heart rate (beats per minute), respiratory rate (breaths per minute), oxygen saturation (%), and mean blood pressure (mm Hg).
The measurements are recorded as one-minute averages and are then stored as five-minute medians.
We extract values between each patient’s first hour of the ICU stay and the next 24 hours for the selected variables.
Since the measurements are recorded at 5-minute intervals, we obtain a multivariate time series of length 276 for each patient in our study cohort.

Cohort Selection:We select patients for our eICU study cohort by filtering the time interval.
Specifically, we include patients who stay for at least 24 hours in their ICU stay, and the time series measurements are extracted.
We did not use filtering criteria for time series in eICU.
This is a design choice that allows us to evaluateTimeDiffwhen unfiltered time series are used as the input.
We also select patients at the unit stay level.
We use 75% for training and 25% for testing while ensuring the class ratio is similar between the two data splits.

We reference the following source code for implementations of our baselines.

In this section, we explain diffusion models following the work of[sohl2015deep]and[ho2020denoising].
Diffusion models belong to a class of latent variable models formulated as, whereis a sample following the data distributionandare latent variables with the same dimensionality as.

Theforward processis defined as a Markov chain that gradually adds Gaussian noise tovia a sequence of variances:

The process successively converts datato white latent noise.
The noisy samplecan be obtained directly from the original sampleby sampling from, whereand.

Thereverse processis the joint distribution, which is a Markov chain that starts from white latent noise and gradually denoises noisy samples to generate synthetic samples:

Under a specific parameterization described in[ho2020denoising], the training objective can be expressed as follows:

whereis a constant that is not trainable.
Empirically, a neural networkis trained to approximate.
This-prediction objective resembles denoising score matching, and the sampling procedure resembles Langevin dynamics usingas an estimator of the gradient of the data distribution[song2019generative,song2020improved].

We attempted to use neural controlled differential equation (NCDE)[kidger2020neural]as our architecture for.
We expect the continuous property of the NCDE to yield better results for time-series generation.
NCDE is formally defined as the following:

Suppose we have a time-seriesandis the dimensionality of the series.
Letbe the natural cubic spline with knots atsuch that.is often assumed to be a discretization of an underlying process that is approximated by.
Letandbe any neural networks, whereis the size of hidden states. Let

The NCDE model is then defined to be the solution to the following CDE:

where the integral is a Riemann–Stieltjes integral.

However, we find that this approach suffers from high computational cost since it needs to calculate cubic Hermite spline and solve the CDE for every noisy sample input during training.
It thus has low scalability for generating time-series data with long sequences.
Nevertheless, we believe this direction is worth exploring for future research.

The diffusion model is trained usinginEquation9.
We setto 0.01.
We use cosine scheduling[nichol2021improved]for the variances.
We apply the exponential moving average to model parameters with a decay rate of 0.995. We use Adam optimizer[KingBa15]with a learning rate of 0.00008,, and.
We set the total diffusion stepto be 1000, accumulate the gradient for every 2 steps, use 2 layers for the BRNN, and use a batch size of 32 across all our experiments.

For a fair comparison, we use a 2-layer RNN with a hidden dimension size of four times the number of input features.
We utilize the LSTM as our architecture whenever applicable.
We use a hidden dimension size of 256 for the eICU dataset.

For deterministic models such as the T-Forcing and P-Forcing, we uniformly sample the initial data vector from the real training data.
We subsequently use the initial data vector as an input to the deterministic models to generate the synthetic sequence by unrolling.

For stochastic process diffusion, we setgp_sigmato be 0.1 for Gaussian process (GP) andou_thetato be 0.5 for Ornstein-Uhlenbeck (OU) process.
For discrete diffusion, we set the total diffusion step at 1000.
We use Adam optimizer with a learning rate of 0.00001 and batch size of 32 across all the experiments.

We set the seed to 2023 and used the following software for our experiments (Table7).

We note that t-SNE is qualitative in nature and therefore subject to observer bias, but despite this limitation it can still provide a general visual guide to the usefulness of a method, and is used in related papers[Yoon2023EHRSafeGH]. We perform the hyperparameter search on the number of iterations, learning rate, and perplexity to optimize the performance of t-SNE[wattenberg2016use]. We use 300 iterations, perplexity 30, and scaled learning rate[belkina2019automated].
We flatten the input data along the feature dimension, perform standardization, and then apply t-SNE directly to the data without using any summary statistics.
We uniformly randomly select 2000 samples from the synthetic, real training, and real testing data for t-SNE visualizations on the eICU, MIMIC-III, MIMIC-IV, and Energy datasets.
For the Stocks dataset, we use 1000 and 700 samples, respectively, due to the limited size of real testing data.

To ensure consistency with results obtained from TimeGAN and GT-GAN, we adopt the same source code from TimeGAN for calculating discriminative scores.
We train a GRU time-series classification model to distinguish between real and synthetic samples, andis used as the score.

For predictive scores, we use the implementation from GT-GAN, which computes the mean absolute error based on the next stepvectorprediction (see Appendix D of the GT-GAN paper[jeon2022gtgan]).
For consistency, we compute the predictive scores for the Stocks and Energy datasets by employing the implementation from TimeGAN that calculates the error for the next stepscalarprediction.
We apply standardization to the inputs of the discriminator and predictor and use linear activation for the predictor for all EHR datasets.

Train on Synthetic, Test on Real (TSTR):We use the default hyperparameters for the six ML models described in the Results section using the scikit-learn software package. The models are trained using two input formats: (1) raw multivariate time-series data flattened along the feature dimension; (2) summary statistics for each feature (the first record since ICU admission, minimum, maximum, record range, mean, standard deviation, mode, and skewness).
After training, the models are evaluated on real testing data in terms of AUC.

Train on Synthetic and Real, Test on Real (TSRTR):To evaluate the effect of the increased proportion of the synthetic samples for training on model performance, we uniformly randomly sample 2,000 real training data from our training set and use this subset to trainTimeDiff.
After the training ofTimeDiffis complete, we subsequently add different amounts of the synthetic samples to the 2,000 real samples to train ML models for in-hospital mortality prediction.
We set the synthetic percentages to be 0.1, 0.3, 0.5, 0.7, 0.9.
In other words, the ML models are trained with at most 20,000 samples (18,000 synthetic and 2,000 real).
This evaluation also simulates the scenario where synthetic samples fromTimeDiffare used for data augmentation tasks in medical applications.
Similar to computing the TSTR score, we train the ML models using either raw time-series data or summary statistics of each feature as the input.
Results obtained using summary statistics as the input are presented inSectionB.4.

We calculate the NNAA risk score[yale2020generation]by using the implementation fromthis repository.
Similar to performing t-SNE, we flatten the data along the feature dimension and apply standardization for preprocessing.
The scaled datasets are then used to calculate the NNAA risk score.

For reference, we describe the components of the NNAA score below.

Let,andbe data samples with sizefrom real training, real testing, and synthetic datasets, respectively. The NNAA risk is the difference between two accuracies:

whereis the indicator function, and

In our experiments, there are instances where. To consistently obtain positive values, we usefor our evaluations.

Our implementation of the MIR score[liu2019socinf]follows the source code inthis repository.
To keep a similar scale of the distance across different datasets, we apply normalization on the computed distances so that they are in the [0,1] range.
We use a threshold of 0.08 for the MIMIC-IV and MIMIC-III datasets.
We set the decision threshold to 0.005 for eICU.
All the input data is normalized to the [0,1] range.

SECTION: Appendix BAdditional Experiments

In this section, we present our visualizations for all the baselines in our experiments.
For all the figures, synthetic samples are inblue, real samples in train split are inred, and real samples in test split are inorange.
We discuss our procedure for t-SNE visualizations in Supplementary MaterialA.5.1.

In this section, we present our visualizations for all the baselines in our experiments.
For all the figures, synthetic samples are inblue, real samples in train split are inred, and real samples in test split are inorange.
We follow the same procedure as t-SNE for data preprocessing.

This section provides additional results for TSTR and TSRTR scores across all four EHR datasets we considered in this study.
We train ML models using one of two methods: flattening the feature dimension of raw time series data, or using summary statistics such as initial measurement, minimum, maximum, range, mean, standard deviation, mode, and skewness.

Next, we present the TSTR results for baseline generative models.
We observe that the patient labels produced by some generative models contain values outside of(an example is), and some only produce one label across all patients.
In these cases, we cannot calculate TSTR and TSRTR scores, and thus, we leave them blank.
Specifically, we found that DSPD-GP, DSPD-OU, CSPD-GP, and CSPD-OU have these issues, so we do not include results for them in this section.

As an additional evaluation, we train RNN classifiers on synthetic time series data generated fromTimeDiffand evaluate their performance on real testing data.
We use bidirectional RNNs with a hidden dimension of 64 for this experiment.

We also further calculated the TSRTR ofTimeDiffby starting using the entire real training dataset and adding different percentages of synthetic samples to it.

In this section, we present additional runtime comparisons across all EHR datasets.
We consider EHR-M-GAN, stochastic process diffusion models, TimeGAN, and GT-GAN.
We use Intel Xeon Gold 6226 Processor and Nvidia GeForce 2080 RTX Ti to train all the models for a fair comparison.

DatasetTimeDiffEHR-M-GANTimeGANGT-GANDSPD-GPDSPD-OUCSPD-GPCSPD-OUMIMIC-III2.718.910.821.82.52.52.52.5MIMIC-IV2.728.829.547.32.62.62.62.6eICU8.787.111059.17.07.07.07.0

In this section, we investigate the effect of hyperparameteronTimeDiff.
We trainedTimeDiffusingwhile keeping the other hyperparameters identical as those described inSectionA.4.2.

In this section, we include the results of generating infrequent time series as well as other dynamic and static data. Specifically, we added channels for glucose, ICD codes, administered antibiotics, and the age at patients’ initial visits. Experiments were conducted using the MIMIC-IV dataset. Specifics on data extraction and formatting are detailed below.

For infrequent time series generation, we used glucose measurements of each patient across a 72-hour interval. Since glucose measurements are extremely sparse and never occur at fixed time intervals, sampling every hour from original time stamps without rounding (our approach with vital signs) would result in nearly no other measurements apart from the initial one. Therefore, we rounded each measurement timestamp to the nearest 20 minutes. We then resampled the data at every hour in the same way as vital signs.
We represented missing measurements with -1 within the glucose channel. After generating samples, we replaced all generated values outside of the range of possible measurement values (predetermined from the training and testing dataset) with -1. We empirically found that preprocessing the data in such a way to add skewness to the distribution works better with our architecture for infrequent or sparse time series. We suspect that the added skewness makes the occurrence of a measurement more prominent for the BRNN. Further investigations into reasons for this behavior would be an interesting direction for future work.

The medGAN repository:https://github.com/mp2893/medganwas referenced for preprocessing and extracting patient ICD codes from the MIMIC-IV database. The codes were selected based on their frequency in the entire MIMIC-IV database. The top 72 most frequent ICD codes were selected to maintain a length consistent with the 72-hour time series. Each patient’s ICD code channel is represented by a binary vector where 1 denotes the presence of a specific ICD code and 0 denotes the absence of the ICD code.

The 72 most frequent ICD codes are detailed below in order from the most frequent to the least frequent:ICD9: 4019
, ICD9: 2724
, ICD10: I10
, ICD10: E785
, ICD9: 53081
, ICD9: 25000
, ICD10: Z87891
, ICD9: 42731
, ICD9: 311
, ICD9: 4280
, ICD9: 41401
, ICD10: K219
, ICD9: V1582
, ICD10: F329
, ICD9: 5849
, ICD9: 2449
, ICD10: I2510
, ICD9: 3051
, ICD9: 2859
, ICD9: 40390
, ICD10: F419
, ICD9: V5861
, ICD9: 30000
, ICD10: N179
, ICD9: 5990
, ICD9: 2720
, ICD9: 49390
, ICD9: V5867
, ICD10: Z794
, ICD10: E039
, ICD9: 5859
, ICD10: Z7901
, ICD10: E119
, ICD9: 32723
, ICD10: F17210
, ICD10: Y929
, ICD9: V4582
, ICD9: 412
, ICD9: V5866
, ICD10: G4733
, ICD9: 496
, ICD9: 27800
, ICD10: E669
, ICD10: I4891
, ICD10: D649
, ICD10: J45909
, ICD10: Z7902
, ICD9: V4581
, ICD9: 2761
, ICD9: 41400
, ICD9: 73300
, ICD9: 30500
, ICD10: J449
, ICD9: 33829
, ICD9: V1251
, ICD10: Z66
, ICD9: 486
, ICD10: N390
, ICD9: 2749
, ICD9: 27651
, ICD10: D62
, ICD10: I129
, ICD9: V1254
, ICD9: V4986
, ICD9: V270
, ICD10: E1122
, ICD9: 78650
, ICD10: I252
, ICD9: 2851
, ICD10: N189
, ICD9: 60000
, ICD9: 56400.

Similar to formatting the ICD codes, the top 72 most frequent antibiotic codes were selected to be represented in the channel. Each patient’s antibiotic channel is represented by a binary vector where 1 denotes the presence of an administered antibiotic code and 0 denotes the absence of the code.

The 72 most frequent antibiotics are detailed below in order from the most frequent to the least frequent:’Vancomycin’, ’CefazoLIN’, ’Ciprofloxacin HCl’, ’CefePIME’,
’MetRONIDAZOLE (FLagyl)’, ’CeftriaXONE’, ’Levofloxacin’,
’Piperacillin-Tazobactam’, ’Ciprofloxacin IV’, ’CefTRIAXone’,
’CeFAZolin’, ’Azithromycin’, ’MetroNIDAZOLE’,
’Sulfameth/Trimethoprim DS’, ’Ampicillin-Sulbactam’, ’Clindamycin’,
’Amoxicillin-Clavulanic Acid’, ’Cephalexin’, ’Meropenem’,
’Doxycycline Hyclate’, ’Vancomycin Oral Liquid’,
’Sulfameth/Trimethoprim SS’, ’Cefpodoxime Proxetil’,
’Ampicillin Sodium’, ’CefTAZidime’, ’Mupirocin Ointment 2%’,
’Azithromycin’, ’Linezolid’, ’Gentamicin Sulfate’, ’Gentamicin’,
’Nitrofurantoin Monohyd (MacroBID)’, ’Piperacillin-Tazobactam Na’,
’Nafcillin’, ’Amoxicillin’, ’Mupirocin Nasal Ointment 2%’,
’Erythromycin’, ’Aztreonam’, ’Tobramycin Sulfate’,
’Sulfameth/Trimethoprim Suspension’, ’Penicillin G Potassium’,
’LevoFLOXacin’, ’Clarithromycin’, ’Ampicillin’,
’Sulfamethoxazole-Trimethoprim’, ’Rifampin’,
’Nitrofurantoin (Macrodantin)’, ’CeftazIDIME’, ’DiCLOXacillin’,
’Vancomycin Enema’, ’Minocycline’, ’Ciprofloxacin’,
’Amoxicillin-Clavulanate Susp.’, ’Neomycin Sulfate’,
’Penicillin V Potassium’, ’Sulfameth/Trimethoprim’,
’Vancomycin Antibiotic Lock’, ’Amikacin’, ’Ceftaroline’,
’vancomycin’, ’Erythromycin Ethylsuccinate Suspension’,
’Moxifloxacin’, ’Tetracycline HCl’, ’Tobramycin Inhalation Soln’,
’Tetracycline’, ’moxifloxacin’, ’AMOXicillin Oral Susp.’,
’Neomycin/Polymyxin B Sulfate’, ’Penicillin G Benzathine’,
’Trimethoprim’, ’CefTRIAXone Graded Challenge’,
’Cefepime Graded Challenge’, ’Meropenem Graded Challenge’.

The MIMIC-IV database contains patients with ages ranging from 18 to 89 inclusive. All ages above 89 are replaced with 91.
The age channel for each patient is represented by a numerical time series of length 72 containing the same age value. After sampling, we obtained the age value by computing the rounded mean of the age time series.