SECTION: Pattern Integration and Enhancement Vision Transformer for Self-supervised Learning in Remote Sensing

Recent self-supervised learning (SSL) methods have demonstrated impressive results in learning visual representations from unlabeled remote sensing images. However, most remote sensing images predominantly consist of scenographic scenes containing multiple ground objects without explicit foreground targets, which limits the performance of existing SSL methods that focus on foreground targets. This raises the question: Is there a method that can automatically aggregate similar objects within scenographic remote sensing images, thereby enabling models to differentiate knowledge embedded in various geospatial patterns for improved feature representation? In this work, we present the Pattern Integration and Enhancement Vision Transformer (PIEViT), a novel self-supervised learning framework designed specifically for remote sensing imagery. PIEViT utilizes a teacher-student architecture to address both image-level and patch-level tasks. It employs a proposed, Geospatial Pattern Cohesion (GPC) module to explore the natural clustering of patches, enhancing the differentiation of individual features. A Feature Integration Projection (FIP) module is employed to further refine masked token reconstruction using geospatially clustered patches. We validated PIEViT across multiple downstream tasks, including object detection, semantic segmentation, and change detection. Experiments demonstrated that PIEViT enhances the representation of internal patch features, providing significant improvements over existing self-supervised baselines. It achieves excellent results in object detection, land cover classification, and change detection, underscoring its robustness, generalization, and transferability for remote sensing image interpretation tasks.

SECTION: IIntroduction

Remote sensing technology, employing aerial or satellite platforms, facilitates the acquisition of earth observation data from afar, marking its utility across diverse sectors such as agriculture monitoring, environmental surveillance evaluation, urban planning, and disaster management. At the heart of these applications lies the interpretation of remote sensing imagery, a task that has been considerably challenging[1]. The advent of deep learning technology[2]has significantly enhanced the precision and automation of remote sensing image analysis, encompassing object detection, land cover classification, and change detection. Nonetheless, the intrinsic complexity of remote sensing scenarios—attributable to variations in sensor technology, atmospheric conditions, and image resolution—results in the performance disparity of analytical models across different contexts. The prohibitive costs associated with manual annotation further exacerbate this issue, rendering the re-labeling of samples for specific scenarios impractical. Consequently, there is a growing demand for models endowed with superior generalization capabilities, capable of adeptly navigating the multifaceted landscape of remote sensing applications.

In Self-Supervised Learning (SSL), algorithms learn useful features by generating labels or targets from the input data itself, facilitating an understanding of the data. A key advantage of this approach is its independence from manually annotated data, enabling more efficient utilization of large volumes of unlabeled data. In the realm of remote sensing, employing SSL for pre-training on a wide array of scene images enhances feature representation capabilities and generalizability across different scenarios. Through transfer learning, outstanding results can be achieved across various imagery-based, downstream tasks. Presently, contrastive learning[3,4]and masked learning[5,6]stand as two widely employed SSL methods in both computer vision and remote sensing fields. In contrastive learning, models are trained to distinguish between a candidate sample and positive (similar) and negative (dissimilar) samples; in masked learning, the objective is to reconstruct obscured portions based on contextual information. Contrastive learning focuses on learning representations that differentiate between samples, while masked learning emphasizes the reconstruction or prediction of missing data based on partial information. Both methods have demonstrated significant potential in remote sensing applications. However, unlike natural images, remote sensing images are predominantly scenographic, often featuring multiple ground objects without a clear foreground target. This characteristic imposes certain limitations on the methods above. For instance, in contrastive learning, random cropping to generate positive sample pairs may result in two images with completely dissimilar features due to the scenographic nature of remote sensing images, potentially leading to semantic confusion in the model. Pixel-level masked learning, on the other hand, entirely disregards semantic information.

Seeking a method that discerns both the semantic information of different scene images and the semantic details within each area of the images is essential for effectively processing scenographic remote sensing imagery. It has been observed that scenographic images in remote sensing typically exhibit specific distribution patterns, where natural elements (such as mountains, bodies of water, forests, lakes, and grasslands) and man-made elements (such as buildings, farmlands, plowed lands, and gardens) often display pronounced clustering. This tendency for feature aggregation leads to the formation of discernible geographic pattern clusters among individual image patches and their neighbors, thereby establishing intricate spatial relationships across the varying patches and their adjacent areas.

Building on these observations, we introduce a novel SSL feature learning algorithm named the Pattern Integration and Enhancement Vision Transformer. PIEViT consists of a student network and a teacher network, simultaneously addressing image-level and patch-level tasks. For the image-level task, we compute the cross-entropy loss between the class tokens extracted by the student and teacher networks from randomly cropped versions of the same image. For the patch-level task, we randomly mask some patch input to the student network and then calculate the cross-entropy loss between each masked patch token and the corresponding patch token in the teacher network. By integrating image-level loss with patch-level loss and backpropagating through the student network’s parameters, the teacher network is updated using the Exponential Moving Average (EMA) strategy[7]. To better differentiate the features of each patch within images, we propose the Geospatial Pattern Cohesion (GPC) module to explore and leverage the natural clustering of landscape elements observed in remote sensing imagery, substantially augmenting the model’s proficiency in decoding intricate spatial distributions. Furthermore, we present the Feature Integration Projection (FIP) module, which utilizes the Geospatial Pattern Cohesion scores generated by the teacher network to guide and refine the reconstruction of masked tokens in the student network. This approach not only increases the complexity of the learning task but also bolsters the learning of nuanced differences in internal features across the images.
Overall, the primary contributions of this work can be summarized as follows:

We introduce the Pattern Integration and Enhancement Vision Transformer (PIEViT), which supervises the generation of masked patches in the student branch with features aggregated from geospatially clustered patches in the teacher branch. PIEViT enhances the distinctiveness of internal area features within images, achieving superior performance in downstream tasks.

Unlike the simple Multilayer Perceptron (MLP)-based projection head, our method computes the score from the Geospatial Pattern Cohesion (GPC) module between each Query Patch and its neighboring patches, selects the toppatches based on their scores, and then processes these patches through a Feature Integration Projection (FIP) module. Compared to MLP, FIP is more adept at capturing a broader range of patch-level semantic information.

After pretraining on the unlabeled dataset, we validated our approach across multiple downstream tasks. PIEViT possesses a strong capability to represent rich semantic and local information, achieving SOTA results in object detection, land cover classification, and change detection, in comparison with classical models with similar size. This demonstrates its generalization and transferability for remote sensing image interpretation tasks.

SECTION: IIRelated Works

SECTION: II-AContrastive Learning

Self-supervised contrastive learning methods train a model by contrasting pairs of inputs derived from the same image. By design, these methods usually employ Siamese architectures. In detail, they usually utilize data augmentation to generate various views of the same image. These augmented views are then fed into different network branches to obtain corresponding representations. By training the model to differentiate between these representations, contrastive learning approaches can facilitate network training and improve the model’s ability to capture higher-level features that can be generalized to downstream tasks[8,9,10,11]. Most contrastive methods originate from computer vision tasks that concentrate on small areas. Therefore, they are primarily developed based on image-level objectives[12,13,14,15]. However, for remote sensing imagery, in addition to image-level information, there is a critical need for local patch-level information, due to the reason that remote sensing images cover large-area (e.g., city-scale) regions.

Recent works in contrastive learning have focused on the excavation of local feature information. For instance, dense contrastive learning (DenseCL)[16]achieves self-supervised learning by optimizing a pairwise contrastive (dis)similarity loss at the pixel level between two views of an input image. Selecting suitable negative samples at the pixel level presents a challenge for DenseCL due to the limited information contained in individual pixels. Object-level representation learning (ORL)[17]applies unsupervised region proposals and kNN-based object similarity to diversify both intra-image and inter-image with more variance, aiding in object-level representation learning. However, it requires three stages of pre-training, leading to less stable model performance. DetCo[18]aims to enhance the performance of general contrastive learning methods by augmenting multiple global and local views simultaneously, yet it is primarily optimized for object detection tasks. For non-detection tasks (e.g., segmentation), it may not be as effective as other contrastive learning methods. Region similarity representation learning (ReSim)[19]leverages contrastive learning to obtain the similarity of different feature regions from varying perspectives through RoI Pooling, but the model’s performance is highly sensitive to the selection of regions. If the chosen regions are not representative or insufficient to capture the image’s key features, it could adversely affect the learning outcomes. Considering that a set of pixel-wise features contains more semantic and structure information, set similarity (SetSim)[20]generalizes pixel-wise similarity learning to set-wise similarity. This method effectively improves robustness through set similarity between views, using attention features to establish corresponding sets, filtering out noise backgrounds, and addressing misleading pixel-level features and semantic inconsistencies. However, pixel-level correspondences based on similarity or geometric information are prone to noise, a problem that is pronounced in remote sensing images.

In short, while these methods represent advancements in the extraction of pixel-level and region-level features, they encounter specific challenges when applied to remote sensing imagery, such as selecting effective negative samples, achieving stable model performance through multi-stage training, optimizing for non-detection tasks, sensitivity to region selection, and handling noise in pixel-level correspondences. This underscores the complexity of addressing spectral anomalies and spectral homogeneity across different objects in remote sensing images.

SECTION: II-BMasked Image Modeling (MIM)

Inspired by the BERT[21]from the natural language processing domain, the MIM approach learns visual representations through the reconstruction of masked patches. Based on the reconstruction target, it can be divided into patch-level, e.g., BeiT[5]and pixel-level, e.g., MAE[6]approaches, where the former predicts the semantic tokens of masked patches, and the latter predicts the RGB values of masked patches. BeiT is the first self-supervised method in the visual domain to use patch-level MIM. It first tokenizes the image based on dVAE[22], then obtains hidden layer features after passing the masked patches through an encoder, and finally completes token reconstruction by calculating the loss with softmax against the tokenized values. MAE[6]and SimMIM[23]are the first two self-supervised algorithms in the visual domain to use pixel-level MIM. MAE[6]employs an encoder-decoder architecture, where the encoder processes unmasked image blocks, and the decoder reconstructs the entire image. SimMIM[23]uses a more diversified architectural design, utilizing just an encoder or combining an encoder-decoder, and finally reconstructs the original pixel values of masked patches using a lightweight prediction head. In addition, CAE[24]introduces a cross-attention module to learn latent space knowledge between visible and masked patches, while MaskFeat[25]proposes using the hand-crafted HOG features of masked patches as the prediction target in self-supervised pertaining.

The above self-supervised methods based on patch-level MIM primarily focus on the semantic recovery of local features but lack the acquisition of global semantic information. Pixel-level MIM methods focus on learning high-frequency spatial information, lacking in low-frequency semantic information, which leads to significant semantic confusion when transitioning to downstream tasks.

SECTION: II-CSelf-supervised Learning in Remote Sensing Domain

The application of SSL techniques is increasingly widespread in the remote sensing field, aiming to leverage unlabeled satellite imagery to learn robust and informative feature representations. These methods can also be broadly classified into two main categories: contrastive learning-based and MIM-based methods, similar to those in computer vision. Studies like[26,27,28]have applied contrastive SSL techniques to satellite imagery feature representation learning. Although these methods have achieved commendable results, they focus on temporal and spatial global self-supervised learning, neglecting the local semantic information of remote sensing images.

The MIM-based approaches are also extensive in the remote sensing domain. For instance, RingMo[29]proposes the PIMask Strategy based on MAE[6], which retains some pixels in the masked parts to better recover small objects. RVSA[30], leveraging MAE[6]for self-supervised training, introduces rotated varied-size attention based on ViT[31]during the fine-tuning stage for downstream tasks to adapt to multi-directional remote sensing images. GFM[32]builds on SimMIM[23]by using a frozen ImageNet22k[33]pre-trained model as a teacher network to obtain image features, which are then compared for similarity with features in the student network. SatMAE[34]extends MAE technology to temporal and multi-spectral satellite imagery, with an innovative approach of using extra tokens to maintain temporal consistency. Building on this, SatMAE++[35]further advances the pre-training of multispectral and RGB images at multiple scales. Scale-MAE[36]introduces the concept of scale into MAE, adhering to the principle that more refined ground objects require more pixels for description, by using scale-variant positional encoding, which scales the original positional encoding by 1/GSD. In addition, some methods, such as SpectralGPT[37]and S2HM2[38], focus on SSL for hyperspectral images and design networks that cater to the unique spatial-spectral structure. These MIM methods proficiently capture the spatial texture information of remote sensing image features but lack semantic information, posing challenges for downstream remote sensing tasks. To address these limitations, we introduced PIEViT, which employs a teacher-student architecture and uses neighborhood similarity aggregation of patch features to predict masked patch tokens, while also retaining the ability to learn image-level global semantic information.

SECTION: IIIMethodology

SECTION: III-AApproach Overview

To facilitate spatiotemporal semantic learning in remote sensing images, we introduce PIEViT, a visual transformer designed for patch geospatial pattern representation learning that is specially crafted for the complexities of remote sensing data. PIEViT incorporates a dual-stream encoder as its backbone, exploiting the comprehensive feature extraction process of Vision Transformers (ViT) alongside self-distillation[7]methodologies to enhance self-supervised learning. Moreover, we introduce two innovative modules: the Geospatial Pattern Cohesion (GPC) Module and the Feature Integration Projection (FIP). The former is engineered to enhance the learning of representations for internal scene components within remote sensing images, while the latter employs the inherent clustering pattern to supervise and refine the masked tokens in the student network, thus deepening the discrimination of complex spatial distributions. Consequently, PIEViT not only intensifies the learning task’s complexity but also focuses on internal feature variations more acutely, resulting in a pre-trained model optimally configured for downstream tasks in remote sensing. The architecture of PIEViT is delineated in Figure1.

The architecture encompasses both a student and a teacher network, establishing a dual-stream feature learning framework. Despite sharing the same structural components, including an encoder and a projection head, each network operates with unique parameters. The teacher network’s role is to set learning objectives for the student network. To this end, a stop-gradient (SG) mechanism is applied to the teacher network, inhibiting any gradient backpropagation through it and ensuring that only the student network is subject to backpropagation. The teacher network’s parameters are dynamically updated through an EMA derived from the student network’s parameters. In pursuit of greater input patch diversity, initial cropping is applied to the input images (sized). Subsequently, for a specific image, two randomly augmented versions,and, are created, with selective patch masking yielding their masked counterparts,and. These masked versions are then input into the student network, while their unmasked originals are directed to the teacher network. After being processed through the ViT encoder, a sequence of query patch embeddings is generated, representing the feature representation of each query patch, which encapsulates the rich semantic information extracted from the remote sensing imagery.

SECTION: III-BThe Geospatial Pattern Cohesion (GPC)

In the realm of remote sensing imagery, captured from an overhead perspective, the landscape unfolds in distinct patterns of spatial distribution. This perspective brings into focus a complex interplay of natural features such as mountains, rivers, forests, and grasslands, alongside human-made elements like buildings, agricultural fields, and urban layouts. These features tend to form discernible clusters, creating a unique spatial relationship between different patches and their surrounding areas. To effectively leverage these spatial patterns and enhance the student network’s masked token optimization with a nuanced understanding of geographical contexts, we introduce the GPC Module, as shown in Figure2. This module is designed to delve into and exploit the inherent clustering of landscape elements within remote sensing images, thereby significantly improving the model’s ability to interpret complex spatial distributions.

Specifically, the GPC Module is applied to query patch embeddings derived from image patches. This module quantifies the GPC score by evaluating the similarity between the embedding of a central patch, denoted as, and the embeddings of its adjacent neighboring patches, represented as. The cohesion score between the central patch and its neighbors is computed to produce a series of cohesion scores:

we select the toppatches (, where) with the highest scores as the output representing geospatial pattern aggregation in the neighborhood, which then guides the generation of masked patches in the subsequent student network.

This method of neighborhood feature aggregation allows us to effectively highlight features that are prominent or representative within local areas, achieving a pattern of GPC. This, in turn, enables a more precise analysis and understanding of the complexity inherent in remote sensing imagery. Such an approach is crucial for guiding and refining the process of generating masked tokens within the student network.

SECTION: III-CThe Feature Integration Projection (FIP)

The Feature Integration Projection (FIP) module is designed to harness the Geospatial Pattern Cohesion scores, derived within the teacher network, to guide the generation of masked patches in the student network. Given that the conventional projection head, denoted as, comprises 3-layer MLPs with an-normalized bottleneck, it falls short in addressing the complexities of multi-patch integration. To overcome this limitation, we introduce the FIP Module, specifically tailored to process patch embeddings subsequent to our ViT transformation.

The FIP structure consists of two primary blocks and an MLP head. Each block is composed of a cross-attention mechanism and a Feed Forward Network (FFN). The cross-attention mechanism is pivotal for distilling pertinent information from an array of patch features. It shares a structural resemblance with self-attention; however, its distinctiveness lies in leveraging attention between a targeted set of embeddings, either classification(cls) or patch, and a series of static patch embeddings, thereby facilitating a focused integration of feature information. For conciseness, we useto denote the whole process of the FIP module parameterized byas:

where CA denotes the cross-attention operation,denotes the embeddings and the architecture of the cross-attention mechanism is defined by multiple projection matrices. This formulation sets the stage for the computation within the CA residual blocks, enabling a structured and efficient integration of features across the patch embeddings. In terms of computational efficiency, the FIP module consists of a set of 2 blocks of layers (2 CA and 2 FFN). The parameter count of each block is the same as that of SA + FFN layers, both being 1.1M. However, CA is more resource-efficient in terms of memory and computation compared to SA because CA computes attention between the class embedding and the patch embeddings, while SA computes attention between the patch embeddings. Since the patch embeddings have a higher dimension, SA requires higher computational overhead.

SECTION: III-DThe Dual-Stream Feature Learning Network

As outlined in SectionIII-A, the overarching network is a parallel architecture consisting of two neural networks, commonly referred to as the student network and the teacher network, forming a dual-stream framework. The student network is responsible for learning and updating its weights through standard gradient descent (backpropagation). In contrast, the teacher network’s parameters are not updated directly through backpropagation. Instead, they are updated using EMA of the student network’s parameters. This means that during subsequent error propagation, the teacher network does not receive gradient updates from the loss function. This stop-gradient mechanism ensures that the teacher network’s parameters are updated smoothly and remain relatively stable, which helps prevent oscillations during the training process. The specific formula for EMA is as follows:, whereandrepresent the weights of the teacher network and the student network, respectively.is the EMA decay factor, which we have set to 0.99.

Within this structure, the teacher network’s architecture adopts tailored strategies at the image and patch levels, as illustrated in the accompanying Figure3. For image-level processing, a CLS embeddingis initialized to function as the query features. This embedding is then concatenated with the static patch embeddings, derived from the ViT, to function as the key and value features. This configuration aligns the CA operation more closely with a Self-Attention (SA) layer by ensuring a corresponding key for every query. At the patch level, an aggregated patch embeddingis designated as the query features, which is then concatenated with similar, static patch embeddingsto serve as key and value features. Consequently, the residual output vectors for CLS and patch embeddings can be articulated as follows:

and their respective biasesrepresent the weight matrices and bias vectors used in the calculation of the cross-attention mechanism.denotes the embedding size. While the block modules at both the image and patch levels share parameters, the parameters in the final MLP layer remain distinct to differentiate the weights associated with the two targets, thereby mitigating the risk of underfitting at the patch level and overfitting at the image level. Empirical evidence suggests that this approach yields superior outcomes compared to the utilization of shared heads[39].

In the student network’s branch, the approach at the image level mirrors that of the teacher network, where a CLS embedding is derived from the static masked patch embeddings produced by the ViT and then introduced as a query into the FIP module. At the patch level, aligning with the objective of Masked Image Modeling (MIM), the masked patch embeddings are fed directly into the MLP. Consistent with the teacher network’s design, the student network maintains distinct parameters in the terminal MLP layer at both the image and patch levels, ensuring specificity in parameter application across different levels of processing.

This strategy of aggregating neighborhood features serves to underscore features that are either prominent or emblematic within localized regions, facilitating a more refined analysis and comprehension of the intricacies embedded within remote sensing imagery. Such a methodology is instrumental in steering and refining the process of masked token generation within the student network, optimizing the network’s learning efficiency and efficacy in handling complex spatial data.

SECTION: III-EOptimization Function of the Network

During training, PIEViT is designed to synergize with theandobjectives, which encompass: 1) a global objective function applied across two global views to encapsulate global-discriminative information; and 2) a patch-level contrastive objective within the same view to encapsulate local masked information. Given an image, upon which two random augmentations are applied, we derive two distorted viewsand. Subsequent random block masking yields their masked counterparts,and. These distorted views are processed through a teacher-student framework to obtain predictive categorical distributions from the CLS token and patch tokens. The training objectives for CLS and MIM within PIEViT are delineated as follows:

wheredenotes the loss function at the global level, capturing class-level predictive accuracy, whilefocuses on the patch level, emphasizing the learning of local features through masked image modeling.represents the aggregate loss, combining both global and local learning objectives to foster a comprehensive understanding of the image content at multiple scales.

SECTION: IVExperiments

In this section, we conducted extensive experiments to evaluate the effectiveness of the proposed self-supervised learning model PIEViT. We compared PIEViT with other self-supervised learning methods based on models with similar sizes across various remote sensing downstream tasks, including object detection, land cover classification, and change detection. All methods were implemented in PyTorch[40].

SECTION: IV-APretraining

The pretraining of PIEViT was based on the Million-AID dataset[41]. Million-AID is a new large-scale benchmark dataset comprising nearly one million remote sensing (RS) scene classification images. It encompasses a wide range of semantic classes, totaling 51 scene categories. PIEViT was trained from scratch for 100 epochs with a batch size of 256, distributed across two A800 GPUs using ViT-B/16 as the backbone.

We used the AdamW optimizer with hyperparametersand. The learning rate was warmed up linearly to a base value of 1e-4 during the first 10 epochs, followed by a decay according to a cosine schedule. Weight decay also followed a cosine schedule from 0.04 to 0.4. The initial LayerScale value was set to 1e-5, and the teacher momentum followed a cosine schedule from 0.994 to 1. We utilized several data augmentation techniques, including color jittering, Gaussian blur, and multi-crop. Additionally, we performed random MIM when training PIEViT, with the mask ratio randomly chosen between 0.1 and 0.5. After pretraining the PIEViT, we use its weight parameters as the backbone and combine it with downstream task-specific heads for fine-tuning.

We compile the number of parameters and FLOPs (floating point operations) for each model during pre-training. FLOPs are computed for a given size of 224×224. TableIprovides a detailed computational complexity comparison of the pre-training models.

SECTION: IV-BObject Detection

We used the DIOR dataset[42]to conduct experiments for the object detection task. DIOR is a large-scale, publicly available benchmark for object detection in optical remote sensing images, containing 23,463 images and 192,472 instances across 20 object classes. The training, validation, and testing sets comprise 5,862, 5,863, and 11,738 images, respectively.

We evaluated our model using the popular Faster-RCNN[43]framework with the ViT backbone pretrained by PIEViT. The ViT backbone was adapted for use with adapter[44]. We trained all models for 12 epochs using an AdamW optimizer with a batch size of 6, a learning rate of 1e-3, and a weight decay of 0.05.
TableIIshows that after fine-tuning Faster-RCNN for 12 epochs, the proposed PIEViT achieved significant improvements in mAP50 over DINO, MAE, and iBOT, which are also self-supervised trained based on Million-AID, increasing by 4.96%, 5.18%, and 4.35%, respectively. Compared to ViT models supervised on ImageNet, PIEViT led by 2.22% in mAP50. Additionally, PIEViT demonstrated higher accuracy than other self-supervised algorithms in the remote sensing field, achieving a mAP50 score of 76.92% on the DIOR test datasets, despite its smaller parameter count.

SECTION: IV-CLand Cover Classification

We conducted experiments for the land cover classification task using the Potsdam dataset[45], which includes 38 satellite images at a 0.05-meter resolution, each with a size of 6000 × 6000 pixels. According to the official splitting, 24 images are used for training, and 14 are used for testing. We cropped the original images to patches with 512 × 512 pixels, resulting in 3,456 training and 2,016 testing samples.

We utilized the ViT backbone pretrained by PIEViT and validated the model using the UperNet network[46]. The ViT backbone was adapted to work with adapter. All models were trained for 20,000 iterations using an AdamW optimizer with a batch size of 8, a learning rate of 1e-6, and a weight decay of 0.01.

As shown in TableIII, PIEViT achieved significant improvement. With the same number of parameters and computational resources, it outperformed DINO, MAE, and iBOT in the mF1 evaluation metric and even surpassed the ImageNet-supervised ViT by 2.76%. Furthermore, PIEViT outperforms some self-supervised algorithms using hierarchical ViT structures. For example, GFM uses the Swin Transformer[47]as its backbone network, but its mF1 accuracy is 0.87% lower than that of PIEViT.

SECTION: IV-DChange Detection

We used the LevirCD dataset[48]for change detection experiments. LEVIR-CD is a widely used building change detection dataset containing 637 very high-resolution (0.5 m) Google Earth image pairs, each with a size of 1024 × 1024 pixels. The training, validation, and testing sets consist of 445, 64, and 128 pairs, respectively. We cropped the images to 512 × 512 pixels, resulting in 1,189 training, 168 validation, and 353 testing samples.

We used the ViT backbone pretrained by PIEViT and validated the model using the BIT[49]. The ViT backbone was adapted for use with adapter. All models were trained for 100 epochs with an AdamW optimizer using a batch size of 8, a learning rate of 1e-4, and a weight decay of 0.05.

TableIVshows the change detection performance on the LevirCD test datasets. Under similar parameter counts and computational resources, PIEViT outperformed DINO, iBOT, and MAE in the mF1 evaluation metric. Compared to the ViT supervised on ImageNet, PIEViT achieved an additional 1.72% improvement in mF1. PIEViT’s performance was only slightly lower than that of Scale-MAE, which has more than three times the parameters of PIEViT.

SECTION: IV-EQualitative Analysis

To assess the quality of PIEViT’s feature representation, we conducted several qualitative analyses, including examinations of object detection, semantic segmentation, and change detection in downstream tasks, as well as an analysis of the patch features extracted by PIEViT.

Figure4provides a visual comparison of different pretraining methods on the DIOR test set. The first row shows an airport scene where models like DINO, iBOT, and MAE missed detecting many airplanes. SatMAE and Scale-MAE could effectively reduce these errors, but still had difficulty when the airplanes closely matched the background in color. Our method detected all airplanes with high confidence. The second row features an overpass scene where the task is to detect both the overpass and vehicles of varying sizes. SatMAE and Scale-MAE had cases of missed and erroneous detections, with SatMAE missing a black car and Scale-MAE misidentifying a white traffic marker as a car. MAE performed well, but its bounding boxes for the overpass and some cars were not accurate. Our method provided bounding boxes that accurately fit the targets’ boundaries. The third row shows a complex dock with ships of varying sizes densely arranged. Our method detected every target without redundant bounding boxes, a problem seen with other methods. These results clearly show that PIEViT surpasses other methods, demonstrating the transferability of its feature representation in object detection tasks.

Figure5illustrates the results of different pretraining methods on the Potsdam validation set. The labels represent impervious surfaces in white, buildings in blue, low vegetation in cyan, trees in green, cars in yellow, and clutter/background in red. Other methods, including SatMAE and Scale-MAE, struggled to distinguish similar features, such as the confusion between impervious surfaces and low vegetation in the top-left corner of the first row. Furthermore, other methods could not effectively separate land cover from the background, misidentifying the background as trees or buildings or mistaking impervious surfaces for the background. In contrast, our method accurately identified these features, avoiding confusion between land covers or the background, demonstrating PIEViT’s superior transferability for land cover classification.

Figure6compares different pretraining methods on the LevirCD test dataset, showing changes or additions in buildings. Overall, our method and Scale-MAE produced the best visual results, while other methods struggled with issues such as holes or broken boundaries. Compared to Scale-MAE, PIEViT’s boundaries align better with the ground truth, providing the most accurate change map. These results indicate that PIEViT’s feature representation is highly transferable and generalized for change detection tasks.

To further verify the feature representation capabilities of PIEViT, we selected four images representing the same category from the Million AID test dataset, DIOR test dataset, Potsdam validation dataset, and LevirCD test dataset, and conducted a Principal Component Analysis (PCA)[50]on the patch features extracted by PIEViT. First, we computed the first PCA on all patches, applying a 0.5 threshold to filter out the first component, thus separating the foreground from the background and retaining the primary objects. Next, we calculated a second PCA on the remaining patches and visualized the top three components with different colors, as shown in Figure7. Notably, PIEViT demonstrated a strong ability to capture features of the same type of objects. For example, in the far-left image, PIEViT accurately extracted buildings situated within the forest, with highly precise boundaries. Furthermore, the local features of objects within the same category were well-matched, as seen in the football field section of the athletics stadium and the water body in the pond, indicating that the model had been trained to interpret parts of objects. This ability is attributed to PIEViT’s focus on Geospatial Pattern Cohesion, which is crucial for its outstanding performance in downstream dense tasks, particularly in complex remote sensing scenarios where objects are often obstructed or scattered.

SECTION: VAblation Study

SECTION: V-AFIP vs AVG

We used the FIP module for multi-patch integration, producing comprehensive features that supervise the generation of masked patches. To verify the effectiveness of the FIP module, we replaced it with an average module (AVG), which simply sums and averages multiple patches to supervise masked patch generation. As shown in TableV, PIEViT-FIP outperformed PIEViT-AVG in all three downstream tasks—object detection, semantic segmentation, and change detection—by 3.84% (mAP50), 3.21% (mF1), and 1.92% (mF1), respectively. The results demonstrate that the FIP module, based on the attention mechanism, handles multi-patch features more effectively and achieves accurate neighborhood representations.

SECTION: V-BNeighborhood Range

To investigate the impact of neighborhood range on model performance, we evaluated three neighborhood ranges (33, 55, and 77) when selecting the most similar patches based on cosine similarity. As shown in TableVI, the model’s performance declines as the neighborhood range expands. When the neighborhood range is 77, accuracy significantly drops, such as a decrease in mAP50 from 76.92 to 66.41 on the object detection task. This result indicates that using nearby patches is more suitable for remote sensing scenarios than employing patches at greater distances.

SECTION: V-CTop-k

We selected the toppatches based on cosine similarity among neighboring patches. To verify the effectiveness of this step, we tested various patch counts with. For a 3×3 neighborhood range, each patch can have up to 8 neighboring patches. For patches located at the corners of the image, we use the 3 surrounding patches. For patches on the edges, we select the most similar 3 patches from the 5 neighboring patches. When, for patches at the corners, the fourth patch was replaced by the one with the highest geospatial pattern cohesion score within the neighborhood range. As shown in TableVII, selecting 3 patches from the neighborhood resulted in the best performance compared to choosing 2 or 4 patches. This result indicates that the number of patches needs to be moderate. Too few patches lead to inaccurate comprehensive features, while too many patches introduce noise into the aggregated features.

SECTION: V-DGPC

The GPC module exploits the inherent clustering of landscape elements within remote sensing images. To investigate its significance, we conducted an experiment where the GPC module was removed, and the embeddings from the vision encoder were fed directly into the FIP module. As shown in TableVIII, the removal of the GPC module resulted in decreases in performance across three downstream tasks: mAP50 by 5.05%, mF1 by 6.63%, and mF1 by 2.06%. These results indicate that the GPC module is crucial for the model’s transfer performance. Additionally, the GPC module’s computations involve only dot products and L2 norms, which are computationally efficient and do not significantly affect overall training efficiency.

SECTION: VIConclusion

In this work, we presented PIEViT, a Pattern Integration and Enhancement Vision Transformer specifically designed for remote sensing applications. Our method uses geospatial pattern cohesion to aggregate patch tokens, which supervises the masked token reconstruction in the student network. This approach makes the task more challenging and effectively enhances the model’s ability to learn internal feature distinctions. Our extensive experiments validated PIEViT’s efficacy across multiple downstream tasks, including object detection, semantic segmentation, and change detection. The Geospatial Pattern Cohesion (GPC) and Feature Integration Projection (FIP) modules allowed PIEViT to capture complex semantic relationships and distinguish subtle differences within and across images. These results affirm PIEViT’s ability to learn rich semantic and local information, demonstrating exceptional generalization and transferability in various remote sensing image interpretation tasks. In the future, we would like to explore more remote sensing data modalities and downstream tasks. For example, we will try to use more types of data during the pre-training process under limited computing power, and we will also try to design self-supervised methods suitable for backbones with larger parameters.

SECTION: Acknowledgment

The authors are graceful to the authors who provided the publicly available datasets.

SECTION: References