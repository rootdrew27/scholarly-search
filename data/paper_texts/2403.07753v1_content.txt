SECTION: A robust SVM-based approach with feature selection and outliers detection for classification problems

This paper proposes a robust classification model, based on support vector machine (SVM), which simultaneously deals with outliers detection and feature selection.
The classifier is built considering the ramp loss margin error and it includes a budget constraint to limit the number of selected features. The search of this classifier is modeled using a mixed-integer formulation with big M parameters. Two different approaches (exact and heuristic) are proposed to solve the model.
The heuristic approach is validated by comparing the quality of the solutions provided by this approach with the exact approach.
In addition, the classifiers obtained with the heuristic method
are tested and compared with existing SVM-based models to demonstrate their efficiency.

Keywords:Data Science; Classification; Support Vector Machine; Outliers Detection; Feature Selection; Mixed Integer Programming.

SECTION: 1Introduction

Support vector machine (SVM) is a frequently used mathematical programming approach in supervised classification. It was introduced byCortes and Vapnik(1995)andVapnik(1998). It has been applied to different fields such as finance(Gavrishchaka and Banerjee,2006;Maldonado et al.,2017;Min and Lee,2005;Ghoddusi et al.,2019); machine vision(Decoste and Schölkopf,2002;Rehman et al.,2019); biology(Zhang et al.,2006;Guerrero et al.,2012;Joloudari et al.,2019); or sustainability(Mrówczyńska et al.,2019;Kim et al.,2019). More applications can be found inCervantes et al.(2020).

Given a setof individuals partitioned into two classes, each individualis associated with a pair, whereis the number of features analyzed over each individual of,contains the feature values, andprovides the class membership, 1 or -1. The aim of SVM is to find a separating hyperplaneto classify new individuals by finding the balance between maximizing the distance (margin) between the two parallel hyperplanes supporting individuals of each class and minimizing the error of misclassified data. SeeBradley et al.(1998);Blanco et al.(2020c)for the use of SVM model considering different norms to measure the margin.

Although classical SVM models have high predictive power in comparison with other state-of-the-art classifying methods, some drawbacks also arise from their use. As stated inHastie et al.(2009), classical SVMs are not robust against outliers and they do not perform efficiently when the analyzed dataset contains a large number of irrelevant features. In this paper we introduce a model which deals with both aspects: outliers detection and feature selection.

In this context, an outlier is an individual whose feature values are not similar from the corresponding ones of most individuals that belong to the same class. The presence of outliers in real-life data is common and is caused by several factors which include but are not limited to: transcription errors, measurement errors, mislabelling, data processing errors, intentional instances (e.g. spam). There are many papers in the literature that use an SVM based model to address these issues, see for instance,Xu et al.(2017);Zhang et al.(2009);Lukashevich et al.(2009);Brooks(2011);Blanco et al.(2020b).

Besides,Karami(2018)also deals with outliers detection. Specifically, the mentioned paper considers a novel anomaly-based intrusion detection system in presence of long-range independence data called benign outliers and also provides visualized information and insights to
end users. This type of methodology is particularly suitable in the abnormal network traffic analysis which is an important research topic to protect computer networks from intruders. Some recent papers are focused on this topic, seeBao and Wang(2016);Kabir et al.(2018);Karami(2015);Karami and Guerrero-Zapata(2015a,b);Karami and Johansson(2014).

We will focus on the approach introduced byBrooks(2011).
This specific model is known as SVM with ramp loss and its formulation for the-norm case is given by

where- and-variables are the hyperplane coefficients. Furthermore,is a non-negative constant, andrepresents the-norm of vector.
In addition,determines penalization if the individualis within the strip defined by the two parallel hyperplanes (and) anddetermines whetheris a misclassified object outside this strip or not, taking the value 1 or 0, respectively. Observe that in this model, penalization in the objective function of each misclassified individual is bounded by two. This model has been analyzed in the literature, see for instanceBrooks(2011);Carrizosa et al.(2014);Belotti et al.(2016);Blanco et al.(2020a);Baldomero-Naranjo et al.(2020).
In this paper, we propose an SVM model with ramp loss derived from the aforementioned model, which includes feature selection and it is formulated as a mixed integer linear program.

As previously mentioned, a weakness of the classical SVM model is that its mathematical formulation does not limit the number of-variables that are not zero in the optimal solution. It may result in classifiers that require information from many features in order to determine an individual’s class. In real-life, analyzing many features could imply high costs. For example, if we want to classify whether or not a patient has a disease, each feature usually represents a medical analysis and each of these has a cost in money and time. Therefore, our aim is to use the least number of medical tests getting the maximum accuracy when classifying the patient. Usually, it feels that the more tests are performed, the better the diagnosis. However, the performance of a large number of medical tests can lead to high costs for the health system or the patient, saturation of the health system, inconvenience for the patient (invasive tests, numerous visits to the clinic, etc.), and longer waiting time to obtain the diagnosis. For example, in the United States, there is a 62% of personal bankruptcies incurred in medical bills, and 90% of patients are in debt, as reportedLee et al.(2020);Jacoby and Holman(2010). Many of the aforementioned drawbacks could be avoided if only the tests that are truly meaningful were carried out. Moreover, classifiers are easier to interpret when the number of features is reduced rather than the ones that contain many features.
Furthermore, the knowledge of the features that have a relevant influence in a medical diagnosis is highly applicable in real-life because it can help in opening new lines of medical research as new treatments or earlier diagnosis.

In general, feature selection techniques have been divided into three different groups: filter, wrapper and embedded methods. Filter methods are based on a preliminary study of each feature’s relevance and only features with significant importance are considered for the classification method, seeGuyon et al.(2006). In contrast, wrapper methods interact with the classification method to select the set of relevant features, seeKohavi and John(1997);Alazzam et al.(2020). Finally, embedded methods study the feature selection and the classification simultaneously in the same model. Many recent papers in the literature propose different techniques to deal with feature selection in support vector machine. These models can be considered as embedded methods, see for exampleMaldonado et al.(2014);Aytug(2015);Gaudioso et al.(2017);Ghaddar and Naoum-Sawaya(2018);Jiménez-Cordero et al.(2020);Kunapuli et al.(2008);Labbé et al.(2019);Maldonado et al.(2020);Gaudioso et al.(2020);Nguyen and de la Torre(2010);Cura(2020);Lee et al.(2020).

In this paper, we introduce an SVM with ramp loss limiting the number of selected features including a budget constraint. In this model we use the-norm, the norm commonly used in the literature for feature selection in SVM based models. Adopting this norm results in classifiers where the number of selected features is lower than using-norm due to its sparse property. Besides, the fact that the proposed model determines the outliers and selects the features simultaneously have some advantages in practical environments with respect to other approaches that do these processes independently.

One advantage of the proposed model is that it prevents the loss of valuable information due to the incorrect removal of elements of the sample. In many cases, several of the features of an individual in the sample may take anomalous values. However, it does not make sense
to exclude this individual if those features
do not affect the classification. This is particularly striking when
dealing with medical data, where usually a large sample is not available.

Another advantage is that the developed model allows us to identify as outliers data that were not initially classified as such.
Indeed, an individual might not be identified as outlier for a general outlier detection procedure because most of its features take similar values to the ones of their class and only some of them take very different values. However, if after the feature selection, these features with very different values are the most relevant in the classification process, this individual should be classified as outlier. Precisely, in the proposed model, feature selection and outliers detection processes are carried out simultaneously.

In addition to the introduction of a new model, we develop some strategies to improve the solution time. Furthermore, we provide a heuristic method that allows us to obtain good quality solutions very quickly. Concretely, we adapt a heuristic approach that has been previously used in different mixed integer linear models(seeGuastaroba and Speranza,2012;Angelelli et al.,2010;Guastaroba et al.,2017). This heuristic is known as Kernel Search (KS) and its main idea is to iteratively provide a better feasible solution to the problem by solving a sequence of restricted MILPs obtained from the original model.

The remainder of this paper is structured as follows. Section2introduces the model, presents a valid inequality and develops a procedure to tighten the big M parameters of the formulation. Moreover, in this section, valid values for the big M parameters are computed.
Section3presents a heuristic based on the Kernel Search for solving the proposed model. Section4contains computational experiments carried out on real-life datasets. In this section, we validate the heuristic algorithm analyzing the best solutions reported by this algorithm and the exact approach. We also test the efficiency of the proposed classifier on real-life datasets comparing its predictive power with other SVM-based models existing in the literature.
Our conclusions and some future research topics are included in Section5.

SECTION: 2The model

In this section, we introduce a model based on SVM that tries to eliminate the adverse effects of outliers using a small number of relevant features. Regarding feature selection, this model
includes a budget constraint in the formulation to limit the maximum number of features required for classification.
This budget constraint has also been used inMaldonado et al.(2014)andLabbé et al.(2019). The model could be slightly modified associating a cost with each feature. Thus, the resulting budget constraint would restrict the cost of classifying a new individual.
With respect to the adverse effects of outliers, this model
uses the ramp loss margin error introduced byBrooks(2011)to avoid outliers influence.
Finally, this model considers the-norm to measure the distances and it is formulated as a mixed integer program with conditional constraints:

where-variables represent the absolute value of the hyperplane coefficientsand-variables are binary variables that indicate whether the associated feature is selected or not. Observe that constraint (6) limits the number of selected features.

This model can be reformulated by expressing each-variable foras the difference between twonon-negative variablesand. Sinceforis part of the objective function, thenand, at most, one of the two variables for anyis non-zero in an optimal solution, see for instanceLabbé et al.(2019)orBaldomero-Naranjo et al.(2020). Hence, the model above can be equivalently reformulated as,

Moreover,(RL-FS)can be linearized substituting (9) by:

We will refer to the resulting model as(RL-FS-M).
The choice of an appropriate value forforis essential to provide efficient solution approaches for the model; i.e.,should be big enough so that both models are equivalent, but it should be also as small as possible to provide good linear relaxation and to reduce the computational time for solving this model.
In the following proposition, we give a result that establishes a relationship between-variables and-variables.

An optimal solution of(RL-FS),satisfies the following condition:

The proof can be obtained by contradiction in an easy way, for this reason, it has been omitted.
As a consequence of the previous result, the linearized version of condition (14) given by

will be used to strengthen the formulations(RL-FS-M). Henceforth, we will refer to(RL-FS-M)as(RL-FS-M), unless stated otherwise.

In order to solve the model, it is necessary to provide valid values of big M parameters. Model(RL-FS-M)contains two sets of big M parameters: the ones associated with the families of constraints (10) and (11), and also the ones associated with the set of constraints (13). The following subsections will focus on initializing and improving these big M parameters in order to efficiently solve the model.

SECTION: 2.1Initial bounds for the big M parameters

InBrooks(2011), a mixed integer linear model for the classical SVM with ramp loss was introduced. In addition, inBaldomero-Naranjo et al.(2020)a result establishing theoretical bounds for the big M parameters appearing in the ramp loss model was proposed. In fact, this result could be adapted for(RL-FS-M)obtaining the following proposition,

Taking the valuesforsuch that,

whererepresents the-norm andits dual, the problems(RL-FS)and(RL-FS-M)are equivalent.

This proposition can be proven analogously to the one presented inBaldomero-Naranjo et al.(2020). This result bounds the big M parameters by a value composed by two terms. The first term,, is easily computed for any norm, but computing the second term,, is not that easy.
However, note that if we establishand we have an upper bound (UB) on the optimal objective value of(RL-FS-M), then. Consequently, initial big M parameters for the proposed model can be calculated.

The first step is to compute an upper bound (UB) on the optimal objective value. To do so, we solve the classical SVM model using-norm that will be denoted as (SVM-) and formulated as follows:

From its optimal solution, (), we buildas follows:

If constraint (6) is fulfilled, a feasible solution of problem(RL-FS-M)is obtained. If not, we sort the components of-vector in non-decreasing order and we fix to zero the first-variables. Fixing these variables to zero, we solve the model again (SVM-). From its optimal solution, we build a feasible solution of(RL-FS-M)following the procedure described before, i.e., we update.
This feasible solution could be improved using the information given byandvalues to obtain the model below. Note that constraints associated with a-value equal to 1 are not considered:

A feasible solution of(RL-FS-M),, can be obtained considering
the solution of the above linear problemtogether withand.From this feasible solution, we compute an upper bound of the model, named UB. Then, we can establish initial bounds of big M parameters as follows:

SECTION: 2.2Improving big M parameters for(RL-FS-M)

In order to tighten the big M parameters for(RL-FS-M), some strategies have been developed. In fact, these strategies are based on solving linear models derived from the original model. We will now detail how each of these parameters could be improved.

By solving the next linear model, we are maximizing the value that the sum ofand-variable solutions can have in the feasible region of the original model.

The optimal objective value of the previous model allows us to obtain an upper bound on. We denote it as UB. Then, the following valid inequalities are obtained and added to the formulation:

Observe that this upper bound tightens the big M parameters, by establishing,

In a similar way,-variable can be tightened by solving

The solution of this model allows us to obtain a valid lower and upper bound on the-variable of the model. We include them in the formulation of the problem as the following constraint:

This subsection is focused on tightening the bounds offor. Observe that for,, constraint (13) is equivalent to,

We introduce the following model in which the right-hand side of (23) is maximized over the feasible region of the original model.

For each, the objective value of this model provides a tightened upper bound on. However, in datasets that contain a huge number of individuals, this strategy is computationally demanding. For this reason, we propose the following variant.

For allwhenwe establishas the optimal objective value of the following linear problem:

whereandfor.

For allwhenwe establishas the optimal objective value of the following linear problem:

whereand, for.

The strategy to initialize and tighten the bounds on the big M parameters of the model is summarized in Algorithm1which is described as a pseudocode.

To summarize, in this section we have introduced a model based on SVM using the-norm that includes feature selection and avoids the effect of outliers. For this model, we have developed a mixed integer linear formulation(RL-FS-M). In addition, we have introduced a methodology for initializing and tightening the big M parameters appearing in the model. In the next section, we present a heuristic approach to obtain adequate feasible solutions for the model.

SECTION: 3Heuristic approach

In the previous section, some strategies to tighten big M parameters of(RL-FS-M)have been presented. Consequently, the formulation is strengthened by the obtained bounds easing its exact solution approach. However, the exact solution of this model cannot be obtained in reasonable times for large datasets. For this reason, it is necessary to develop a heuristic solution method to deal with such datasets.

In this section, we develop a procedure for obtaining good quality solutions. It is based on the Adaptative Kernel Search (AKS) proposed byGuastaroba et al.(2017). The idea of the AKS is to solve a sequence of mixed integer problems derived from the original one, where the majority of the binary variables are fixed and only some of them are considered as integer – this set of integer variables is called Kernel.

The heuristic approach that we propose for(RL-FS-M)has two main characteristics. On the one hand, we introduce an AKS for-variables, seeLabbé et al.(2019). On the other hand, the-variables are dynamically fixed using the solution of the previous sub-problems. This strategy considers sub-problems where the number of binary variables and constraints containing the big M parameter are reduced with respect to the
original problem. As a consequence, these sub-problems have a shorter solution time.
This heuristic method will be referred to as Dynamic Adaptative Kernel Search (DAKS). DAKS allows us to obtain adequate feasible solutions of(RL-FS-M)in datasets of large size, as can be appreciated in Section4. The improvement is justified by the reduction in the number of integer variables and big M constraints in the sub-problems considered in this procedure.

SECTION: 3.1Dynamic Adaptative Kernel Search (DAKS)

The DAKS algorithm has three phases.

The first one computes an initial feasible solution to the problem. Using this solution, the big M parameters appearing in the model are initialized and some-variables are fixed.

The second one determines the initial set of integer-variables; i.e., the initial kernel. Furthermore, in this step, the remaining-variables are ranked. Note that the most promising variables to take a value of 1 in the optimal solution are considered first. The chosen order has a huge influence on the solution obtained by the heuristic, so this order should be adapted to the dataset under study. Observe that although in the original algorithm several sets of integer variables can be considered, in this version, we only apply the Adaptative Kernel Search over-variables, i.e.,-variables are adjusted using a different methodology during the procedure.

In the third phase, a sequence of restricted MILP derived from the original problem are solved, including constraints that ensure that a progressively better bound on the solution is obtained. The solution and the computational effort required to solve the previously restricted MILP, guide the construction of the subsequent Kernels. In each iteration, the
Kernel is updated to test other promising variables and to remove useless ones. Moreover, taking into account the solution of the previous sub-problem, each-variable is fixed to zero, fixed to one or considered as a binary variable, i.e., this set of variables is updated dynamically. After some iterations, the second phase is repeated with the objective of reordering-variables using the information of the current-variables values.

Giventhe restricted MILP derived from the original problem will be referred to aswhere if-variable is considered a binary variable, it is otherwise fixed to zero. Furthermore, with the objective of simplifying the notation of these sub-problems, we include an auxiliary vectorwhich indicates the corresponding-variables values. Each element of vectorcan take one of the following values with the following meanings:

Therefore,model is formulated as described below.

In the next subsections, the three main phases of DAKS procedure are described in detail and a pseudocode of this heuristic approach is also presented in Algorithm2.

In the first phase, Algorithm1is applied with the aim of obtaining an initial upper bound of the problem and valid values for the big M parameters. We can set a time limit for the algorithm or establish a fixed number of iterations. The modeler should decide the best strategy to follow for the dataset under study and choose one of the variants of this algorithm, taking into account the number of individuals and features considered in the dataset.

Furthermore, in this phase,-variables are fixed. Letbe the initial solution built in Algorithm1. Then, vectoris determined as follows,

for. Note that ifis established as two, thenwill be a binary variable in.

The idea of this procedure is to discard as outliers the well-classified individuals in the initial solution, i.e.,is fixed to zero. Furthermore, the individuals
whose corresponding-values are one in the initial solution are set as outliers, i.e.,is fixed to one. Finally, we establish as “possible outliers” (considering the associated-variable as binary)
the wrong-classified individuals
corresponding to-values in the interval [1,2].

In the second phase, we will create an initial kernel on-variables and the remaining variables will be ordered according to how likely these variables
will take a value of one in the optimal solution. This stage is essential for the success of the algorithm.

We propose the following strategy for ordering the variables. First, we solve the problemconsidering-variables as continuous. We will denote the previously described model as. Observe that in this relaxed problem, only the-variables whose correspondingtake value 2 are binary (recall thatwas computed in the initial phase). The optimal objective value of this problem will be called. Again, we solve it by fixing the values of the binary variables in order to obtain the reduced costs.

Letandbe
the optimal values of variablesandandandthe corresponding reduced cost of these variables.
Hence, the variables are ordered in non-decreasing order with respect to vector, which is computed as:

The initial kernel,, is composed bysuch thatorvariable takes a positive value in the solution ofmodel considering-variables fixed to their optimal values in.
Moreover, the first time that this phase is executed,includes the indexessuch thatorvariables take a positive value in the initial solution built in the first step of Algorithm1.

Once this initial kernelis defined, the modelis solved. Its solution is an upper bound (UB) on the problem(RL-FS-M). Letbe the solution of the problem, i.e., the solution of the current upper bound. After this step, vectoris updated as follows:

Ifis equal to zero and, for, (i.e., the individual was discarded as outlier, but in the current solution it is wrongly classified),will be fixed to two (i.e., it will be set as a “possible outlier”).

Ifis equal to one and, for, (i.e., the individual was set as an outlier, but in the current solution it is well classified),is updated to two (i.e., it will be set as a “possible outlier”).

Note that the criterion of being a “possible outlier” can be adjusted to the dataset, imposing tighter conditions, e.g., or relaxing them, e.g..

An iterative procedure starts in this phase. In each iteration, (), a new set of indexes, namedis added to the kernel. These indexes are included in the order determined by vector, i.e., the most promising-variables to take value one in the optimal solution are considered first. Note that each index is included at most once. The initial size ofis the size of, seeGuastaroba and Speranza(2012);Labbé et al.(2019). In the subsequent iterations, the size ofwill be adjusted.

The modelis solved adding the following constraints:

Constraint (24) ensures that a better upper bound of the problem is obtained, and constraint (25) ensures that at least one variable is selected for the newset. If the previous iteration found a feasible solution but the optimality of this solution was not proved because the time limit was reached, constraint (25) will be replaced by:

whererepresents the solution associated with the current upper bound.

A feasible solution of this sub-problem, i.e.,if the previous solution was optimal orif the previous solution was feasible, will improve the upper bound. However, this problem may not be feasible and for this reason, a time limitis imposed. Furthermore, we also add a time limitfor finding a feasible solution, i.e, the sub-problem is stopped if no feasible solution is found within. In addition, as the big M parameters affect the lower bounds of the problem, proving optimality is hard. Consequently, if the incumbent solution is not improved after a fixed time, the resolution of the sub-problem is stopped.

After this, the kernel is updated: the selected variables of the setare added, denoted as, and the variables of the kernel that were not selected in the lastiterations are removed from the kernel, denoted asMoreover, vectoris updated as follows:

The idea of the previous procedure is: if the individual was discarded as outlier but in the current solution it is wrongly classified, it will be set as a “possible outlier”. If the individual was set as outlier, but in the current solution it is well classified, it will be set as a “possible outlier”. Finally, if the individual was set as “possible outlier” but it has the same value in the lastsolutions, the variable will be fixed to this value. Observe that the criterion of being a “possible outlier” can be adjusted to the dataset, imposing tighter conditions or relaxing them, as explained at the end of Section3.1.2.

In the following iterations, the problemis solved. The time employed in solving the previous iteration determines the size of,. If the problem related to an iteration is easily solved, i.e., if, the number of-variables that are binary in the next step is incremented, i.e.,, where

This iterative process (phase 3) will be stopped if any of the following situations occur:

Ifandremains unchanged, the algorithm finishes. If, we know that the obtained solution cannot be improved having fixed-variables to these values. Since different-values to the current ones are not proposed for the next iteration, the algorithm finishes. Note thatis an upper bound of the lower bound (LB) of the problem. Hence, the solution could be a local minimum or the optimal solution of the problem.

Ifthe currentis the same as the one used to compute, and different values are proposed for the next iteration to updatein the procedure described in Table1; return to phase two. If this situation happens, our solution cannot be improved if-variables are fixed to these values. However, it may improve if-variables are fixed to different values. In order to recompute the initial kernel and sort the-variables considering the updated values of, the algorithm returns to phase two.

After a criterion defined by the user (e.g. doing a fixed number of iterations, establishing a time limit or fixing a percentage of-variables that should be analyzed), the algorithm returns to phase two. The objective is to sort the-variable taking into account the current.

Note that the number of iterations that the algorithm does in Phase 2 and 3 is a criterion determined by the user that should be adapted to the dataset. Similarly, the proposed heuristic has many parametersthat the user should adapt to the dataset and their needs, finding the desired balance between time employed and required precision.
In Algorithm2, this procedure is summarized and described as a pseudocode.

In conclusion, in this section we have proposed a new algorithm to obtain accurate feasible solutions of our model based on the kernel search algorithm. In particular, we have developed a different strategy to seek the most promising values of-variables with the objective of avoiding using the reduced costs of these variables due to the high influence that the big M parameters have over them.

SECTION: 4Computational Experiments

In this section, we set out the results of several computational experiments. Firstly, we analyze the performance of Algorithm2comparing the solution provided using this algorithm to the solution obtained applying an exact resolution method. The results demonstrate that the proposed algorithm computes high-quality solutions. Secondly, we present a comparison of the classification done by several models over real-life datasets showing the efficiency of the proposed classifier.

The experiments were conducted on an Intel(R) Xeon(R) W-2135 CPU 3.70 GHz 32 GB RAM, using CPLEX 12.9.0. in Concert Technology C++. As seen inBelotti et al.(2016)due to the presence of big M parameters, the relative MIP tolerance and the integrality tolerance were fixed to zero. The remaining parameters were left to their default values unless stated otherwise.

SECTION: 4.1Data

The computational experiments were carried out on real-life datasets. They are specified in Table2, whereis the number of individuals,is the number of features, and the last column reports the percentage of elements in each class. Leukemia dataset is fromGolub et al.(1999), Colon dataset is fromAlon et al.(1999), and DLBCL dataset is fromShipp et al.(2002). All other datasets are from the UCI repository(seeLichman,2013). Observe that these datasets were previously used to analyze the performance of models based on SVM(see, for instance,Brooks,2011;Labbé et al.,2019;Maldonado et al.,2014).

In these datasets, the following values for the parameterwill be analyzed,as inBrooks(2011). We tested five different values for the parameter, which is different for each dataset. To choose it, we solved(RL-FS-M)on ten randomly generated samples containing 90% of individuals from the original dataset beingequal to the number of features. Note that in each sample, the percentage of elements from each class is the same as in the original dataset. We compute the average from the number of selected features for each. The maximum average is the greatestanalyzed, the other four are proportional to it, beingandof the greatest value of. In Table3, these values are depicted. Moreover, since
the two greatest values offor the Arrhythmia dataset (and) and Mfeat dataset (and) computed by the above description are too big, we have removed them and we have included the following values of:andfor Arrhythmia dataset;andfor Mfeat dataset.

SECTION: 4.2Validation of DAKS algorithm

In this section, we validate the efficiency of Algorithm2, i.e, we compare the solution obtained by the exact method with the one provided by Algorithm2. The exact method consists of applying Algorithm1and solving the resulting formulation using CPLEX. The resolution of this model is quite hard, and for this reason, a time limit of 7200 seconds has been set.

We solved(RL-FS-M)for all datasets for allvalues () and the smallest value oftested, i.e., Colon, Leukemia, DLBCL, SONAR, Iono, Arrhythmia, Wdbc, Mfeat, and Lepiota. For the Mfeat and Lepiota datasets, we applied Variant 2 of Algorithm1to initialize the big M parameters, while Variant 1 was used in the rest of the cases.

As done inGuastaroba et al.(2017)for the AKS algorithm, parameterwas set to 0.35 in DAKS. Moreover, we establishedand. Furthermore, in the restricted problems, we limited the computational time () to 400 seconds. In order to improve the behavior, as inGuastaroba et al.(2017), we used CPLEX with default values, except for the following parameters: BndStrenInd=1, MIPEmphasis=HiddenFeas, FPHeur=1. The parameters previously mentioned to avoid the negative effect of big M values were also used.

The results are depicted in Table4. The first column indicates the name of the dataset and the second column the value of parameter. The next column reports the total time of the exact solution method in seconds, i.e., the strategy time plus the solving time. The following column contains information about the MIP relative GAP reported by CPLEX. The fifth column indicates the time in seconds that the heuristic took. Finally, the sixth column reports the percentage difference between the best solution found by heuristic algorithm () the best solution found by the exact solution method (). This difference is computed as follows:

As can be appreciated in Table4, the proposed heuristic resolution method reported the same solution as the exact solution method in most cases, with the advantage that the heuristic algorithm is much less time-consuming than the exact resolution method. Note also, that in some cases, Algorithm2found a better solution than the exact solution method.
For instance, in the case of Leukemia dataset with, Algorithm2found a 1.9% better solution in one minute and a half than the exact solution method found in over two hours. Besides, in the case of IONO dataset with, Algorithm2found a 8.55% better solution in less than two minutes than the exact solution method found in over two hours. Similarly, in the case of Arrhythmia dataset withand, Algorithm2found 10% and 8.91% better solutions respectively than the exact solution method found in over two hours.

In summary, we compared the performance of Algorithm2with an exact solution approach for a challengingparameter value. We proved that DAKS provides quite good solutions in the 45 cases analyzed for differentparameter values. On average, the difference between the best solution found by the exact solution method and the heuristic is -0.17%, while the heuristic has significantly reduced the amount of time required.

SECTION: 4.3Model validation

The aim of this section is to compare the performance of the proposed classifier(RL-FS-M)with other well-known methods based on SVM in the literature. We focus our attention on efficient models that deal with outliers detectionor feature selection:,(Fisher-SVM), and (RFE-SVM). More concretely, the purpose ofis to avoid the influence of outliers, seeBrooks(2011). We used the methodology proposed inBaldomero-Naranjo et al.(2020)to solve this model, imposing 1800 seconds of time limit. On the other hand, the objective ofis to limit the number of features selected by the classifier and it was solved by applying the heuristic algorithm proposed inLabbé et al.(2019). Similarly, the goal of Fisher Criterion Score with SVM (Fisher-SVM) and Recursive Feature Elimination algorithm with SVM (RFE-SVM) is also restricting the number of features selected. They were introduced inGuyon et al.(2006,2002), respectively. Finally,(RL-FS-M)was also compared with the classical(SVM-).

To compare the classifiers, we used the recognized classification performance metrics: the accuracy (ACC) and the area under the curve (AUC). The accuracy is computed as

where TP are true positives, TN are true negatives, FP false positives and FN false negatives. The area under the curve is computed as

We followed the Ten Fold Cross Validation procedure (TFCV) to obtain these metrics. It consists in randomly partitioning the dataset into 10 subsets. In each iteration, nine of these subsets constitute the training set and the remaining partition is the test set. The performance of the classifier is evaluated by the independent test set.

Although many of the tested dataset are known to contain outliers, in order to check the robustness of the classifiers, we perturbed the original data including label noise and SVM outliers. The label noise can come from several real situations which produce (intentionally or unintentionally) errors in the class of the individuals by adding outliers to the dataset, seeSalgado et al.(2016). For this purpose, we randomly changed the class ofof the individuals in the training sample. On the other hand, for adding SVM outliers, we solved problem(SVM-)in the training sample. We sorted the individuals of each class in non-increasing order according to the result ofwhereNote thatandare the optimal solution values of (SVM-). We changed the class of the firstranked individuals.

The tables in which this information is depicted are structured as follows: in the first column the name of the classifier is shown and in the second column the value of parameteris reported. The following five columns describe information about the best case ofwhen 5% of the dataset contains label noise. The best case ofis the value of parameterthat reported the highest accuracy. Note that in the event of a tie, the best case is the one that provided a larger area under the curve. If this value is also the same, the best case is the one that took the least time. More concretely, the third column of the tables reports the above described value of parameter, the next one shows the average time of the ten iterations, the following column depicts the average number of selected features for each classifier. Finally, the penultimate and last column of this group report the average ACC and AUC obtained for the ten iterations. The following five columns depict information about the best case ofwhen 5% of the dataset are SVM outliers. These columns are structured as the previous ones.

In Table6, the computational experiments on the Colon dataset are depicted. We can observe that the best classification performance when 5% of the dataset contains label noise
is obtained by (Fisher-SVM), with the following metrics: 88.75% of ACC and 88.75% of AUC.
On the other hand, the best classifier is(RL--M)when 5% of the dataset are SVM outliers, providing 88.75% ACC and 87.50% AUC. Slightly smaller classification metrics are obtained by(RL-FS-M)selecting only eleven features on average in contrast with the 38.20 selected by(RL--M).

The classification performance for the different formulations in the Leukemia dataset is depicted in Table6. Note that the best classifier when 5% of the dataset contains label noise
is(RL-FS-M), providing 92.86% ACC and 93.75% AUC. Moreover, observe that our model is able to classify a new individual by analyzing only 9.9 features on average. Similarly, the largest classification metrics (94.60% ACC and 95.54% AUC) are obtained by(RL-FS-M)when 5% of the dataset are SVM outliers.

The results from the DLBCL dataset are reported in Table8. As can be appreciated, when 5% of the dataset contains label noise
the best classifier is(RL--M), providing 91.25% ACC and 85.83% AUC. However, this classifier used 51.1 features on average. Slightly smaller classification metrics are obtained by(RL-FS-M)and(FS-SVM)selecting fewer than half of the features on average. On the other hand, when 5% of the dataset are SVM outliers,(RL-FS-M)is the classifier that provides the largest ACC (95%) and AUC (95%) with nearly a point and a half difference in percentage compared to the rest of the models. Observe that this classifier selected only fifteen features on average from the 7129 provided by the data.

The computational results for the Sonar dataset are shown in Table8. When 5% of the dataset contains label noise,
the largest ACC (77.52%) and AUC (77.03%) are obtained by(RL-FS-M). Likewise, in the case of having a 5% of SVM outliers, the best classification performance (76.94% ACC and 76.90% AUC) is reported by(RL-FS-M).

In Table10, the classification performance of the analyzed formulations on the Iono dataset is provided. Although the highest performance metrics when 5% of the dataset contains label noise
are reported by(FS-SVM)whose average ACC isand average AUC is, slightly smaller accuracy and area under the curve (88.06% and 84.34% respectively) are provided by(RL-FS-M)and(RL--M). On the contrary, when 5% of the dataset are SVM outliers, the best classifier is (Fisher-SVM).

The experiments carried out on the Arrhythmia dataset are depicted in Table10. Observe that when 5% of the dataset contains label noise,(FS-SVM)is the classifier that provides the largest ACC (76.67%) and AUC (74.74%) on average. However, when 5% of the dataset are SVM outliers,(RL--M)reported the largest classification metrics: 75.95% ACC and 74.56% AUC. Slightly smaller classification metrics are obtained by(FS-SVM)selecting only 16 features on average instead of 79.30.

The classification performance comparison for the Wdbc dataset is shown in Table12. In the first case, i.e. 5% of label noise,
almost all tested formulations provided nearly the same accuracy and area under the curve with the exception of(RL-FS-M), which reported the largest classification metrics, selecting only 16.4 features on average. In the second case, the best classifiers are(RL-FS-M)and(RL--M)reporting 97.54% ACC and 97.06% AUC.

The computational results of Mfeat dataset are depicted in Table12. As can be observed, the classification metrics are almost the same for all tested formulations when 5% of the dataset contains label noise.
The one that provided the highest ACC (100%) and AUC (100%) is(FS-SVM). On the other hand, when 5% of the dataset are SVM outliers, the best classifier is(RL-FS-M)reporting 99.90% ACC and 99.50% AUC.

Finally, the computational experiments carried out on the Lepiota dataset are reported in Table13. We can appreciate that the best classification performance
is obtained by(RL-FS-M)when 5% of the dataset contains label noise, with the following metrics: 100% ACC and 100% AUC on average. Moreover, this classifier analyzed only 10.7 features on average. In the same manner, when 5% of the dataset are SVM outliers, the best classifier is(RL-FS-M)with more than one and a half unit difference in percentage compared to the rest of the models. It reported 99.73% ACC and 99.72% AUC. Observe that this classifier selected only 6.4 features on average.

Therefore, in this subsection we have demonstrated the efficiency of the proposed classifier. We have observed that in the majority of cases,(RL-FS-M)provided the highest accuracy and area under the curve using less information to classify a new individual than(RL--M). Note also that the performance of(SVM-)is significantly influenced by outliers, providing in several cases the worst results despite selecting many features. Although(FS-SVM)reported quite good results with label noise, its efficiency is decreased when the dataset contains SVM outliers. (Fisher-SVM) and (RFE-SVM) presents good results for some dataset but their behaviour is not as robust as our model.

In summary, choosing the parameters which provide the best accuracy in each dataset when contains 5% of label noise, the average (maximum) percentages of improvement for ACC and AUC of our model with respect the others in the studied datasets are 2.63% (24.54%) and 2.94% (22.78%), respectively. Similarly, when the dataset contains 5% of SVM outliers, the average (maximum) percentages of improvement for ACC and AUC are 2.06% (14.46%) and 2.36% (14.58%), respectively. Choosing the parameters which provide the best ACC in each dataset, the average (maximum) percentages of improvement for ACC and AUC with respect to each model is depicted in Table14.
The aforementioned results show that(RL-FS-M)is a very robust classification method improving the existing ones or being among the best in terms of ACC and AUC. Moreover model(RL-FS-M)involves a reduced number of features in the obtained classifier in contrast to the models (RL-) and (SVM-).

Av. ACC impr. (Max. ACC impr.)

Av. AUC impr. (Max. AUC impr.)

Av. ACC impr. (Max. ACC impr.)

Av. AUC impr. (Max. AUC impr.)

SECTION: 5Conclusions

In this paper, we have developed a new model based on support vector machines especially designed for datasets with a large number of features that may contain outliers. We have formulated the model as a mixed-integer problem and proposed an exact strategy for computing the big M parameters of the formulation. Moreover, we have developed a heuristic algorithm to solve it efficiently. We have also validated the heuristic, proving that the obtained upper bound is of high quality.

Furthermore, we have compared the performance of the proposed classifier with other classifiers based on support vector machines that deal with feature selection or with outlier detection. We have showed the efficiency of our model, whose competitive advantage is that it deals simultaneously with both aspects. Finally, we think that analyzing the proposed model using other-norms would be interesting for future research.

SECTION: Declaration of interest

The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.

SECTION: Credit authorship contribution statement

Marta Baldomero-Naranjo:Conceptualization, Methodology, Software, Writing.Luisa I. Martínez Merino:Conceptualization, Methodology, Software, Writing.Antonio M. Rodríguez-Chía:Conceptualization, Methodology, Software, Writing.

SECTION: Acknowledgements

The authors thank theAgencia Estatal de
Investigación (AEI) and the European Regional Development’s funds (ERDF): project MTM2016-74983-C2-2-R, 2014-2020 ERDF Operational Programme and the Department of Economy, Knowledge, Business and University of the Regional Government of Andalusia: projects FEDER-UCA18-106895 and P18-FR-1422, Fundación BBVA: project NetmeetData (Ayudas Fundación BBVA a equipos de investigación científica 2019), andUniversidad de Cádiz: PhD grant UCA/REC01VI/2017 and Programa de Fomento e Impulso de la actividad Investigadora UCA (2018). The authors would like to thank the anonymous reviewers for their valuable comments and suggestions.

SECTION: References