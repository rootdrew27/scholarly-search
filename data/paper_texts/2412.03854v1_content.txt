SECTION: What Do Machine Learning Researchers Mean by “Reproducible”?

The concern that Artificial Intelligence (AI) and Machine Learning (ML) are entering a “reproducibility crisis” has spurred significant research in the past few years. Yet with each paper, it is often unclear what someone means by “reproducibility”.
Our work attempts to clarify the scope of “reproducibility” as displayed by the community at large. In doing so, we propose to refine the research to eight general topic areas. In this light, we see that each of these areas contains many works that do not advertise themselves as being about “reproducibility”, in part because they go back decades before the matter came to broader attention.

SECTION: 1Introduction

The Artificial Intelligence (AI) and Machine Learning (ML) communities are increasingly concerned with the “reproducibility” of their fields. This has come on the heels of a reproducibility crisis noted in many others. We will refer to this overarching concern, that the science of research is being done with some error rate, as a genericscientific rigorconcern. This concern is justified, and
it is increasingly challenging to evaluate the state of research around scientific rigor due to confused and incompatible usage of the same few terms like “reproducibility”(Plesser2018).

Due to confusing and often inconsistently used terminology in the
literature, it is challenging to understand precisely what issues of
scientific rigor the community is tackling. In light of these issues,
we propose a new formulation of
current scientific rigor research by surveying the current articles by the
topics they cover. In doing so, we observe that many historical works tackled these very issues – with different motivations and no particular thematic name like “reproducibility” as it was not an urgent concern at the time.

In this article, we will expand the ACM’s proposed terminology of
Repeatability, Reproducibility, and Replicability which we find useful,
although still insufficient to capture the breadth of work done to
date. Our contribution classified current AI / ML research in scientific rigor into eight aspects we label asrepeatability,reproducibility, replicability,adaptability,model
selection,label/data quality,meta & incentive, andmaintainability. These eight aspects are defined inTable 1. We
propose these aspects based on our review of 101 papers published since
2017 and reflect the focus of the community at large.Table 1also shows
for each aspect the proportion of papers focused primarily on that
aspect (though many papers touch on multiple aspects).

The rest of this article is organized as follows.
We will summarize the eight main topic areas of scientific rigor insection 2, with sub-areas included based on our literature review. Based on this survey of the literature, we propose relationships for how these rigors interact insection 3, which we find informative as a macro-level picture of the scope of scientific rigor.
Finally, we conclude insection 4.

SECTION: 2The Current Scope of Work

Our literature survey identifies at least eight primary aspects of
scientific rigor studied in the AI/ML literature. Each major sub-section will repeat one of the eight rigors defined inTable 1, and include further delineation for nuanced sub-categories that are present or noteworthy in the literature. A key criterion for being included inTable 1is that the paper must self-identify itself as being about “repeatability, reproducibility, or replicability” since those are the three preexisting terminologies used (interchangeably) in the prior literature. These delimitations reflect the current scope of what researchers actively consider “reproducibility” consider worthy of study and effort.
As our bibliography will show though, many more papers exist in these topical areas that were published before 2017, and thus before the AI/ML communities started to put renewed effort into the issue of scientific rigor. We include such articles in the discussion of each section to establish the full scope of available work and to connect the current reproducibility-themed motivation to its historical precedents, as the historical literature is often unknown to existing researchers on this topic.

Before we detail these aspects, it is worth noting that many existing articles are best summarized as opinion pieces with varying degrees of formalization of their arguments. Most of these articles propose strategies or arguments on how to obtain “reproducibility”, without evidence of effect(Gundersen and Kjensmo2018; Matsui and Goya2022; Publio, Esteves, and Zafar2018; Tatman, Vanderplas, and Dane2018; Sculley et al.2018; Vollmer et al.2020; Drummond2009,2018; Raff and Farris2022; Lin2022).
The contents of our article are focused on works that study issues, incentives, or interventions to rigor issues in AI/ML — and thus go beyond opinion or thought pieces on the topic, of which there are many. These thought pieces are valuable in spring motivation and growth in the field, but most are disconnected from the long literature on the topic, so we prefer not to focus on their opinions which may well change with new literature111Indeed, our own understanding have evolved in the discovery of the wide and deep literature on scientific rigor..

Since there is no canonically accepted “home” for AI/ML papers on reproducibility, we find that such published literature is scattered across various subfields and specialized conferences. In many cases, we find common themes in the nature of issues that occur across fields and domains, and in some aspects, the literature on issues impacting scientific rigor directly goes back to the 1990s. Our categorization is based on a review of all literature we are aware of that tackles scientific rigor issues, even if they did not use terms like “reproducible” as they often pre-date the larger academic concern itself. One recommendation that we would put forth forall major AI/ML conferences is to create a track for scientific rigor studying all eight proposed rigor topics to further incentivize and organize this important work.

SECTION: 2.1Repeatability

Repeatability concerns the authors who obtain the same results using the original source code and data.
Interesting questions in repeatability include how to develop code and
systems that make it easy for the developer to keep track of how they
came to their experimental results from an experimental design
perspective(Gardner, Brooks, and Baker2018; Paganini and Forde2020).
In Human-Computer Interaction (HCI) research, there has been significant
study on the iterative development nature of computational notebooks
(e.g., Jupyter) that are widely used in AI/ML development processes.
These notebooks can be prone to many subtle code errors/issues due to
their fluidity and out-of-order execution. Enhanced tools can ensure the
exact execution sequence to generate a result(Head et al.2019; Kery and Myers2018; Courtès et al.2024). Many simple factors, like using a random-number seed (i.e., for a pseudo-random-number generator (PRNG)), are important for obtaining instantaneous repeatability. Furthermore, many mathematical operations are not guaranteed to produce identical numerical results due to floating point errors and differences in numerical stability of different implementations and hardware(Arteaga, Fuhrer, and Hoefler2014; Schlögl, Hofer, and Böhme2023).

Other factors, such as software version conflicts, are often thought to be factors of repeatability but often lead to conflation. For example, does capturing software versions via a container system lead to repeatability or reproducibility? We argue that it would be reproducibility as a higher-level concept in our categorization, which we will detail further insection 3. A second distinction we make is that of instantaneous repeatability vs. repeatability over time. In this immediate section, we consider instantaneous repeatability, where the question is how to ensure repeatable results as the software/algorithm is being developed, and we find that there is surprisingly little beyond the work noted in the prior paragraph. When time is added as a factor, we consider this to be distinguishable as the maintainability rigor that we will detail insubsection 2.8.

SECTION: 2.2Reproducibility

Reproducibility alters repeatability by requiring that a different individual/team be able to produce the same results using the original source code and data. This is a high focus of the AI/ML community and incentivization of Open Source Software (OSS) by major conferences and paper submission questionnaires/guidelines.
Current work can be divided into those that explore surface-level issues
such as unquantified proposals or exact procedure reproductions, vs
those that attempt to quantify or better understand why a reproduction
does(not) work.

Surface-level studies of reproducibility report on the scale of the
reproducibility challenge without examining whether their attempts at
improving reproducibility work. The only large-scale study we are aware of found that 74%
of the code released by the broader scientific community (beyond AI/ML) ran
without issue(Trisovic et al.2022). Toward remediating this in machine
learning, many have proposed techniques like Docker to try and capture
the exact conditions to re-run the experiments(Forde et al.2018a,b).(Gardner et al.2018)looked at enhancing reproducibility by standardizing data access and execution environments for MOOCs. However, the project appears to be abandoned and stresses the importance of repeatability/reproducibility over time, which we note forms the aspect of maintainability we discuss later insubsection 2.8.

A major factor in Reproducibility, and the discovery of non-reproducible work, is errors in the original comparisons being made. There are cases where reproducibility may be strictly achievable but meaningless due to an error in the fundamental approach being taken or experimental setup. In a seminal example of metric learning, it was found that papers had multiple changes occurring simultaneously in comparison to prior baselines (new layers like Batch-Norm, optimizers, etc.) beyond just the proposed metric learning changes, which produced misleadingly large effect sizes(Musgrave, Belongie, and Lim2020). In general, many other works have identified similar issues nuanced to the subdomain being studied(Lu, Raff, and Holt2023; Liu et al.2020; Chen, Belouadi, and Eger2022; Ito et al.2023). More serious instances have determined a subfield of research being constructed around unsound methodologies(Lin et al.2022; Kapoor and Narayanan2023; Hullman et al.2022; Raff and Holt2023).

Thematically similar to(Musgrave, Belongie, and Lim2020)are the multiple realizations of insufficient baseline evaluation that have occurred in many works since. Such work includes studies that use similar baseline errors/lack of adjustment(Rao et al.2022), studies that expand the set of baselines against an overly broad prior conclusion(Huang et al.2022; Wang et al.2022), and studies that demonstrate that decades-old methods are still competitive when given the chance to run on larger modern datasets(Liu, Hu, and Lin2022). Another example is the effectiveness of linear models in natural language processing tasks, which are orders of magnitude faster and capable of comparable results(Lin et al.2023). A unique aspect shown by(Chen et al.2018)is that many improvements prescribed to one family of algorithms are actually applicable to prior approaches and would perform just as well using an “out-of-date” method. They showed this by applying improvements from seq2seq modeling to Recurrent Neural Networks and found that the improvements were still effective, allowing a Pareto improvement in combined approaches.

SECTION: 2.3Replicability

Replicability concerns the ability of a different person/team to produce qualitatively similar results from the original article by writing their own code and potentially different data.
The aspect of replicability is highly understudied, likely due to the challenges this aspect presents.
Replicability can be subdivided into empirical replicability and theoretical replicability.

Empirical Replicability requires re-implementing a target method’s code from scratch, which is a labor-intensive process. Notable work in this direction was done by(Raff2019), who attempted to reimplement 255 papers, and computed features to quantify what properties correlated with a replicable paper. Smaller scale replications have also been performed(Belz et al.2022), including a volunteer effort by ReproducedPapers.org collecting some (most are reproduction attempts) Replicability attempts(Yildiz et al.2021)based on which a thorough study in IR has been performed(Wang et al.2022).(Chen et al.2022b; Ganesan et al.2021)identified issues with a specific common baseline method XML-CNN in multi-label learning. Famously,(Henderson et al.2018)replicated recent reinforcement learning results and discovered various aspects, such as the seed and scale of rewards, that significantly altered the perception of improvement.

(Johnson, Pollard, and Mark2017)Replicate studies in mortality prediction in a healthcare context, highlighting the difficulty of producing comparable results when replication also requires collecting new data of the same intrinsic nature (that is, patient data in this context). Textual descriptions presented in the original studies were found to be insufficient for collecting new data that would replicate.(Hegselmann et al.2018)extended this observation by showing how to produce replicable data collection schemes for survival analysis against medical repositories such as SEER.

We are not aware of other work within AI/ML on empirical replicability. This state of affairs is common to other (relatively) code-free disciplines such as medicine(Ioannidis2005)economics(Camerer et al.2016)social sciences(Camerer et al.2018). In these disciplines, replication studies are necessary and representative because they are the least costly way to evaluate a result. Other aspects of scientific rigor have a significantly lower barrier to entry, largely because AI/ML has a large open-source culture.

More recent work has advanced a theoretical definition of replicability in terms of constraints on the output distribution as a function of the input distribution.(Impagliazzo et al.2022; Kalavasis et al.2023; Bun et al.2023)have developed much of this foundation by showing various desirable statistical properties such as that Total Variation (TV) between outputs drawn from the same distribution using the same algorithm (i.e., a congruent definition of replicability) is equivalent to results in approximate differential privacy and robust statistics. This idea has since been expanded to bandits(Esfandiari et al.2023b), optimization(Zhang et al.2023), clustering(Esfandiari et al.2023b), and reinforcement learning (at an exponential increase in runtime)(Karbasi et al.2023).

Lastly, a unique approach to the question of replicability was studied by(Ahn et al.2022), which focuses on the difference between the computational precision of floating points and the underlying symbolic math. From this perspective, they are able to suggest conditions about what statements can be rigorously tested and concluded about the math based on floating-point errors that would accumulate and cause issues otherwise.

SECTION: 2.4Model Selection

Model selection deals with the common task of AI/ML papers: given two competing methods (one of which may be the paper’s own proposal), how do we conclude which method is better?
As the AI/ML literature has advanced significantly through the presentation of empirically “better” algorithms, it is not surprising that most historical and current work has focused on the question of model selection. This includes how to pick and evaluate criteria to decide “better”, how to build benchmarks for a problem, and the process to determine “better” given criteria in a statistically sound way. There are also multiple resurgences of this issue as ML is incorporated into other fields, and comparisons that may be invalid in a new field occur as both communities begin to merge and discover what is/is not acceptable(Hoefler2022).

Before selecting the “better” of one or more methods, it is necessary first to determine how the quality of a method is determined. The scope of evaluation metrics and scores is larger than that of scientific rigor, and this article is concerned with cases where an invalid or errant procedure was identified and remediated. The literature in this direction is old, starting in the late 1990s on the various pros and cons of metrics like Area Under the Curve (AUC) for evaluation(Bradley1997; Hand2009; Lobo, Jiménez-Valverde, and Real2008). Likewise, work has addressed issues in scoring from leaderboards(Blum and Hardt2015), and subtle issues in using cross-validation to produce test scores(Varma and Simon2006; Bergmeir, Hyndman, and Koo2018; Varoquaux2018; Bates, Hastie, and Tibshirani2021; Mathieu and Preux2024). Niche examples of the evaluation concern also exist. For example, three decades of malware detection performed subtle train/test leakage by adjusting for a target false-positive rate incorrectly(Nguyen et al.2021)and time series anomaly detection scores being overly generous to “near hits”(Kim et al.2022).

It is becoming increasingly popular to build benchmarks of multiple datasets, pre-prepared evaluation code, and methodology for specific problem domains(Blalock et al.2020; Eggensperger et al.2021; Sun et al.2020; Saul et al.2024; Liu et al.2024; Ordun et al.2021; Kebe et al.2021). Such a benchmark construction is popular, although it has yet to evolve into a science of how to build benchmarks, with limited study at a macro level(Koch et al.2021). Some domains may require additional thought to how methods are compared, especially when they are measuring a non-stationary objective like human preferences in Information Retreival(Breuer and Maistro2024).

Much of the ML literature presents raw results and makes a nonscientifically rigorous statement of being “better” by some metric (i.e.,boldnumbers in a table are better, and our method has more bold numbers in the table). There are two approaches to developing improved comparisons.

One is to devise better statistical tests to compare two methods when a single test set is available, first seriously studied byDietterich (1998)with many follow-up works shortly after(Alpaydin1999; Bouckaert2003; Bouckaert and Frank2004). Different perspectives on this include using one test run to make a conclusion(Dror, Shlomov, and Reichart2019), or including sources of variation in model performance (e.g., hyperparameter values) and comparing the distribution of model results(Bouthillier, Laurent, and Vincent2019; Bouthillier et al.2021; Cooper et al.2021). Others have introduced computational budget for training and parameter tuning as a conditional factor that impacts the conclusion of “best”(Dodge et al.2019).

The second option is to use multiple datasets to perform a single test of whether one algorithm is better than another(Guerrero Vázquez et al.2001; Hull1994; Pizarro, Guerrero, and Galindo2002). The use of a nonparametric Wilcoxon test has been found to be effective in multiple studies(Demšar2006; Benavoli, Corani, and Mangili2016).(Dror et al.2017)extended this to make a conclusion about how many datasets and which one method performs better. Other recent work has proposed using meta-analysis methods to draw conclusions about a single method tested under multiple conditions(Soboroff2018). Notably, work using multiple datasets to make decisions based on a single evaluation metric implicitly contributes to the Adaptability question, which we explore next. Interestingly, we note the field of programming languages has also proposed quantile regression as a better method of analyzing results(de Oliveira et al.2013).

SECTION: 2.5Adaptability and its Second-Class Status

Adaptability is the study of a different person/team, using the original code but applying it to their own and different data. Very little work on scientific rigor in AI/ML focuses on Adaptability. To be clear, many prior works have studied the question of generalization in machine learning, of which there is recent evolution due to the advance of deep learning(Zhang et al.2017). However, generalization assumes some form of intrinsic relationship (usually I.I.D.) between the training and testing distribution. Under Adaptability, there is no direct train/test split to compare. Instead, it is a question of the methodology’s effectiveness on an entirely different statistical distribution at training and test time. Thus, our concern is more focused on the practical, real-world issues that enable or inhibit amethodto generalize. Our contention is the lack of study on adaptability is one of the most glaring shortfalls in the current scientific rigor literature, with significant room for researchers to define and develop new ways of studying the problem222Anecdotally, we have had significant trouble publishing work attempting to tackle adaptability problems in other sub-domains when we were trying to use it for real-world needs(Raff, McLean, and Holt2023)..

The work we have found can broadly be described as including adaptability to new datasets or specialized subsets to better understand the overall behavior and utility of a set of algorithms(Marchesin, Purpura, and Silvello2020; Rahmani et al.2022). The other work that tackles adaptability is from an HCI perspective in validating a method’s utility as population preferences evolve(Roy, Maxwell, and Hauff2022).

Though it has not been presented as a part of the literature on scientific rigor, considerable effort in the Adaptability question has been advanced by Decision Tree-based literature. In particular, the long-standing effectiveness of tree ensembles has led to numerous studies investigating the persistent efficacy of tree ensembles(Grinsztajn, Oyallon, and Varoquaux2022; Wainberg, Alipanahi, and Frey2016; Bagnall et al.2020). Despite little work on the adaptability question, we note that many works in Model Selection make use of the adaptability argument as a component of their study or an otherwise latent concern.

One way that others could seek to understand adaptability is to see if they adapt to crossing a small “chasm” of change in the problem. This can be done by taking methods developed in one context and applying them to a highly related problem (ideally with a minor to no modification necessary). Two prior works that we are aware of have demonstrated surprising failures of methods that fail to cross these small chasms.(Riquelme, Tucker, and Snoek2018)studied extensions of Thompson Sampling for reinforcement learning that work well in supervised settings, but a modest adaption to sequential decision making causes simple Thompson Sampling to outperform the various previous improvements.(Liu et al.2021)found that the original hyperparameters for a multi-label prediction algorithm were kept when the method was adapted to a new task. Subsequent works compared to this original parameterized version rather than re-tuning to the new task. When properly accounted for, all subsequent methods failed to improve on the original method.

SECTION: 2.6Label & Data Quality

Label & Data Quality is focused on the reliability of data and label acquisition, error rates, and working to understand how they occur, detect them, or work around them. The distinction we make from research in inferring a single label from labelers is that scientific rigor is concerned with the process of how labels are collected, defined, and have impacted research conclusions (e.g., inferring a 99% accurate model when labels have a 5% noise level would imply a failure in process).
Many works today identify these issues long after dataset construction, in part due to the high accuracies now being achieved, making the errors more pronounced.
For example, the process for deriving labels of ImageNet had rules incongruent with the nature of the data (e.g. assuming that only one class is present) and error-prone steps in the labeling pipeline(Beyer et al.2020). Label quality issues also include leakage from the train / test set(Barz and Denzler2020).

Some of the most insightful research results have come from replicating dataset construction and labeling processes for prior datasets and then characterizing and discovering why differences in results occur. This includes detection cases where recreation is implicitly made more challenging than the original dataset(Engstrom et al.2020).
Although there is a long history of research inferring a single correct label from multiple labelers(Whitehill et al.2009; Lin, Mausam, and Weld2014; Ratner et al.2016; Yoshimura, Baba, and Kashima2017; Ratner et al.2020), this literature is generally not framed as a scientific rigor issue. While these methods have been utilized in work from a rigor perspective(Beyer et al.2020), we are not aware of work that bridges a longitudinal study of the replicability of these various label inference procedures.

SECTION: 2.7Meta and Incentives

Very few papers have studied incentives for scientific rigor. The study of the scientific process itself is often termed metascience and, when applied to AI/ML research, would fall into this category. Such research could include basic studies of incentives, drivers of scientific rigor, and surveys across various AI/ML research domains.
Sample studies focused on drivers such as the rate of data and code sharing in computational linguistics(Wieling, Rawee, and van Noord2018)and the use of statistical testing(Dror et al.2018). Related work has found that code sharing and replica research are correlated with higher citations(Raff2022; Obadage, Rajtmajer, and Wu2024), although most meta-studies have looked at the rate of code sharing in their subdisciplines(McDermott et al.2021; Olszewski et al.2023; Arvan, Pina, and
Parde2022a; Arvan, Doğruöz, and Parde2023; Cavenaghi et al.2023). A unique aspect of code availability is studied by(Storks et al.2023), who perform a user study with students on the time and difficulty factors for students to reproduce the results of three NLP papers. Another study focuses on how evaluation and comparison practices evolve throughout the Machine Translation community(Marie, Fujita, and Rubino2021). The last work we are aware of challenged the treatment of replicability as a binary “yes/no” question and instead suggested a survival model, where replicability is a function of time/effort(Raff2021)and quantifying a reproducibility score(Belz, Popovic, and Mille2022).

SECTION: 2.8Maintainability

Maintainability is similar to Repeatability, in that we are concerned with producing the same results with the original authors (though new users could also occur) using the original code and data. The key difference that distinguishes maintainability is that time is a factor, as the ability to repeat results degrades over time as nuances of labels(Inel, Draws, and Aroyo2023)or dependency versions change(Connolly et al.2023)333In software development, this notion is often termed “bit rot”.. Maintainability can also deal with the code itself changing over time.
The focus on the aspect of maintainability within AI/ML was started by the seminal work of(Sculley et al.2015). A key area of maintainability deals with adapting known “code smells” while considering ML-specific concerns and factors that practitioner surveys consider most important(Gesi et al.2022). Another key area of maintainability is the quality of the results as the code itself changes. It is well known that scientific algorithms may produce different results by different (but supposedly equivalent) implementations(Hatton1993). Multiple studies have found that AI/ML is no exception to this history, with large and statistically significant changes in accuracy when using allegedly equivalent algorithms and changing just the implementation or the runtime platform (e.g. GPU hardware)(Coakley, Kirkpatrick, and Gundersen2022; Gundersen, Shamsaliei, and Isdahl2022; Pham et al.2020; Zhuang et al.2021).(Zhou, Chen, and Lipton2023)found that in many medical time series tasks, it may be beneficial to train on all historical data in some cases vs. training a sliding window of recent data. They also looked at models that experienced “shocks” of sudden degradation in time.

The study of maintainability is surprisingly minimal in our community despite the rapid adoption, abandonment, and evolution of frameworks used within the field. Torch, Tensorflow, JAX, Theano, and many more frameworks have come and gone through major revisions over time. These changes and re-implementation of algorithms are fertile ground for maintenance issues and, thus, their study, which directly impacts researchers and the developers of these frameworks. Studying how to build maintinable code in AI/ML is still nascent(Gilbertson et al.2024; Papi et al.2024)

SECTION: 3Connections between Rigor Types

Having defined a set of eight rigor types that are being worked on, we further elaborate on our perception of connections between these rigors. In particular, there are direct and indirect relationships, which are summarized inFigure 1with solid and dashed lines, respectively.

RepeatabilityReproducibilityReplicabilityAdaptabilityModel SelectionData QualityMeta/Incentives: influence all other parts.MaintainabilityInteract With EachotherIs a precodition for the targetStrongly Influences

SECTION: 3.1Direct Relationships

The most obvious, and intuitive connections are from repeatability to reproducibility to repeatability, as each requires a progressive step of
difficulty from the prior. If a single person/team cannot repeat their
own experiments, there is no reason to believe that a different person with
the same code would be able to reproduce those results. Extended
further, if they cannot reproduce the results with the original code,
there is no special reason to believe that by writing their own code or
using different data, they would be able to replicate the results.

Less obvious are the interactions between maintainability, repeatability, and replicability. The first is the two-way relationship between repeatability and maintainability. If an AI/ML system is not repeatable, it cannot be maintainable, as repeatability is the property that we want to maintain. Similarly, if it cannot be maintained, it may not be repeatableover time. A simple case is the use of Docker to gain repeatability, which is predicated on the repeatability of Docker containers. This assumption is true on short time horizons, but changes in software, hardware, and eventually deprecation of tools like Docker itself do not make it true in perpetuity. The time-based evolution that maintainability requires then directly implies the replicability of a method. If a system is replicable, meaning that the code or data can change as well as the people, it satisfies the requirement of maintainability over a single point in time. Thus, maintainability involves iterated replicability over time and instantaneous repeatability at any point in time.

SECTION: 3.2Indirect Relationships

Beyond the general influence of meta- and incentives-based rigor having a relationship to all parts of scientific rigor, we can further draw other connections that are of particular note. The most straightforward of these is that of model selection on repeatability, reproducibility, and replicability, each of which will often incorporate the model selection task as part of the motivation for why the proposed work should be used (i.e., it was demonstrated to “be better” than something prior). Thus, by its nature, different approaches to model selection will influence each. For example, the use of random search as a hyperparameter tuning method(Bergstra and Bengio2012)is potentially a hindrance to replicability due to higher variance, even if it is easily repeatable and reproducible given the original code with initial seed values for the pseudorandom number generator.

Upstream from this concern is then label and data quality, which will influence what features are selected. This is particularly notable as many datasets reach high accuracies where “errors” in the model’s predictions are discovered to be either 1) correct and that the test data were mislabeled or 2) that the test instance was inherently ambiguous(Barz and Denzler2020). This creates a new kind of noise in the selection process, and can thus alter conclusions on the merits of what is considered. This is particularly true for the eventual selection of the downstream model under replicability, where the data in use may be different.

Finally, we note that a method that is adaptable is more likely to be maintainable. The nature of one method being effective in many others is the observation that many small details on the implementation can vary, while still producing quantitatively similar results, an often observed phenomenon in decision tree literature(Quinlan1993; Breiman et al.1984; Quinlan2006; Raff2017). This provides some inherent “robustness” to issues that often cause maintainability problems, such as changes in low-level libraries like BLAS/LAPACK or new hardware.

SECTION: 4Conclusions

We have synthesized eight current directions in the literature of scientific rigor for machine learning, disentangling them from the commonly repeated moniker of “reproducibility” and thus quantified the proportion of each type as studied today. These rigor types have been further characterized by their interactions/dependencies with each other.

SECTION: References

SECTION: Appendix APapers Used

In the following table, we cite all the papers (101) we categorized as being significantly related to AI/ML and self-identify as being about “reproducibility” in some sense to buildTable 1. Articles are listed in no particular order. Articles that did not self-identify as being about “reproducibility” are excluded as the purpose was to determine what researchers currently identify, though we found many more articles that discuss the same themes/issues in both historical and current literature (as the long bibliography demonstrates). The category assigned is our subjective call as to the most important/prominent theme of the paper, though many papers discussed more than one issue.