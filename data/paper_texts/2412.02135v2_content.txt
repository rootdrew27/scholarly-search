SECTION: Unsupervised learning-based calibration scheme for Rough Bergomi model††thanks:Submitted to the editors Dec 2024.\fundingThis work was funded by GRF (project number: 17317122) and Early Career Scheme (Project number: 27301921), RGC, Hong Kong.

Current deep learning-based calibration schemes for rough volatility models are based on the supervised learning framework, which can be costly due to a large amount of training data being generated. In this work, we propose a novel unsupervised learning-based scheme for the rough Bergomi (rBergomi) model which does not require accessing training data. The main idea is to use the backward stochastic differential equation (BSDE) derived in [Bayer, Qiu and Yao,SIAM J. Financial Math., 2022] and simultaneously learn the BSDE solutions with the model parameters. We establish that the mean squares error between the option prices under the learned model parameters and the historical data is bounded by the loss function. Moreover, the loss can be made arbitrarily small under suitable conditions on the fitting ability of the rBergomi model to the market and the universal approximation capability of neural networks. Numerical experiments for both simulated and historical data confirm the efficiency of scheme.

91G20, 60H15, 91G60, 60H35, 91G80.

SECTION: 1Introduction

Since the pioneering work[17], there has been enormous and growing interest in studying rough volatility models[3,5,6,14,15,16,23,26,27]. The rough Bergomi (rBergomi) model, first proposed in[4], is one representative example, and can effectively capture several stylized facts from both statistics and option pricing points of view. Under a filtered probability spacewithbeing the risk-neutral measure, the dynamics of the rBergomi model is given by

whereandis the terminal time. The constantdenotes the interest rate.andare independent standard Brownian motions. We denote bythe augmented filtration generated byand, and bythe augmented filtration generated by. Note thatin (2) has continuous, non-negative trajectories and is adapted to. Furthermore,is integrable, i.e.,

There are several scalar parameters and a time-varying parameter function in the model (1)-(2), which we denote as.is the so-called initial forward variance curve, defined by[4]. The Hurst indexis the origin of the term “rough”, since the sample paths of the variance processare-Hölder continuous for anywhich is rougher than the samples of standard Brownian motions.is the correlation between the Brownian motions that drive the underlying asset processand the variance. Finally, the parameteris defined by

whereis the ratio between the increment ofand the fractional Brownian motion (fBm) with Hurst indexover[4, Equation (2.1)].

It is widely recognized that the practical utility of a model heavily relies on the availability of efficient calibration methods. Calibration involves finding the model parametersso that the option prices generated by the model align closely with the market data. Specifically, letdenote the strike and maturity, representing the contract details. We establish the pricing mapso that the model parameters and market information fully determine the option price. The calibration objective is to solve the following optimization problem:

whererepresents the number of market data points. Addressing the calibration task typically necessitates multiple iterations of the model, underscoring the importance of an efficient pricing methodology. However, with the inclusion of the Riemann-Liouville type of fBm in the variance processgiven by

the joint processloses the Markovian property and semi-martingale structure. This feature precludes the application of common PDE-based methods and all related techniques relying on the Feynman-Kac representation theorem. Therefore, pricing methods based on Monte Carlo (MC) simulations are predominant for the rBergomi model. Despite the recent advancements in this field[9,30](notably Teng et al.[30]show linear complexity), a substantial number of samples are still needed during the online calibration phase. This can lead to a slow calibration process, highlighting the need for further enhancements in efficiency and speed.

Over the past few years, data-driven concepts and deep learning approaches based on neural networks (NNs) have been extensively explored to accelerate calibration. Existing NN-based calibration schemes can be categorized into two classes. One is calledone-step approachor direct inverse mapping[21]: an NN is trained to learn the following map

with historical data and model parametersas the input and output, respectively. The traditional calibration routine is performed on a set of historical data to get the following labeled training pairs

where the number of training pairsis intrinsically limited to the amount of reliable market data. Once the NN is trained, it can directly output the model parameters for any given set of market data, which is the most efficient in calibration. The idea is direct, and the NN can fit observations very well, but as noted in[21]and pointed out by[7], the learned maplacks control and behaves unsatisfactorily when exposed to unseen data, which suggests its restricted generalization ability. The other is calledtwo-step approach(first ”Learn a model” then ”Calibrate to data”)[7,22,29,28,25]. Here an NN is trained as an approximator of the pricing function, with a choice of model parameters as input, and the corresponding vanilla option prices (or implied volatilities) for a range of prespecified maturities and strikes as output, i.e., to learn a map

The training pairs are artificially synthesized, and not limited to the available market data. As was proved recently that NN can learn the pricing function up to an arbitrarily small errorand the network size only grows sub-polynomially inunder some suitable assumptions[10]. With this deterministic approximative pricing function at hand, the calibration problem (4) can be reformulated as

This approach also shifts the time-consuming numerical approximation of the pricing function to the offline stage, which leads to faster online calibration processes.

These two approaches both follow the framework of supervised learning, requiring many labeled data pairs for the NN training. Despite their promising results, they have drawbacks in three key aspects. First, training data generation for such NNs heavily relies on MC-based methods, which incurs significant computational cost and storage requirement. Second, once the NN is trained, market data used for calibration must align with a pre-specified strike and maturity grid. While it is possible to choose a fine grid, this can lead to increased complexity in preparing training pairs[22]. Finally, financial markets are complex and dynamic, making it challenging to have a complete mathematical model that captures all nuances. The model parameters to be identified may follow an unknown, application-specific distribution that could be inferred from market data. For example, the scalar parametermay vary with time to reflect the changing local regularity of the volatility[12]. Since these approaches presuppose certain forms of model parameters to get the training pairs, they may struggle to adapt to more general cases and lack the flexibility to capture the complete information from the market.

The main objective of this work is to propose an unsupervised learning-based calibration scheme to address these challenges. Since the vanilla option price for classical diffusive stochastic volatility models can be expressed as the solution of a nonlinear parabolic partial differential equation (PDE), model calibration can be viewed as a parameter identification (inverse problem) for such parameterized PDEs. The well-known nonlinear Feynman-Kac formula implies that the solution of such PDE corresponds to the solution of a backward stochastic differential equation (BSDE), which can be efficiently solved using deep learning-based methods[19,20,24]. For the rBergomi model, where the framework is non-Markovian, the corresponding option price follows a Backward Stochastic Partial Differential Equation (BSPDE) rather than a deterministic PDE. A stochastic Feynman-Kac formula has been established to represent the weak solution of the BSPDE in terms of a BSDE coupled with the forward SDE[8], and subsequently, a deep learning-based scheme is developed to price options backwardly. Our proposed scheme, namely the deep BSDE scheme, treats the model parameters as tunable weights or approximates them by NNs if necessary. Then these tunable weights or NNs are optimized alongside the BSDE solutions during training. Inspired by[20], our scheme implements the forward Euler method starting from the market data and matching the terminal condition to solve the BSDE. The obtained numerical solution is adapted to the filtration automatically, which avoids the calculation of conditional expectation for the backward scheme.

The contribution of this work is threefold. First, we propose an unsupervised learning-based calibration scheme for the rBergomi model to tackle the challenge of training pair generation. In contrast to purely data-driven methods like the one-step approach, our scheme incorporates PDE knowledge to reduce the reliance on extensive market data while maintaining high accuracy. Further, it is adaptable for increasing market data and allows a more general form of model parameters. Second, during the NN training process, the model parameters and the numerical solutions of BSDE are learned simultaneously. The training procedure can be terminated early when a good set of model parameters is achieved to save computational costs.
Third and last, we demonstrate inSection4that the discrepancy in option prices resulting from model parameters (4) can be bounded if the loss is kept sufficiently small. Moreover, the loss converges to zero, given the calibration power of the rBergomi model to the historical data and the universal approximation capability of NNs. This analysis provides insights into the relationship between parameter estimation accuracy and option price discrepancies, leading to a better understanding of the calibration process.

MethodsPDE usageTraining datausageStrike and MaturitygridForms of modelparametersOne-step approachNoYesFixedFixedTwo-step approachNoYesFixedFixedDeep BSDEYesNoGrids freeFree to set

The rest of the paper is structured as follows. We recall the BSPDE developed in[8]for model (1)-(2) inSection2. We present the new calibration scheme inSection3, and give the convergence analysis inSection4. Extensive numerical experiments are provided inSection5using synthetic and historical data. The algorithm and all the presented numerical examples are available online at the GitHub linkhttps://github.com/evergreen1002/Calibration-BSDE-rBergomi. We end with several remarks and future research inSection6.

SECTION: 2BSPDE model

We recap in this section the European option pricing theory of the rBergomi model (1)-(2) by a BSPDE[8]. Define the processwith the initial statefor. Thencan be reformulated as

The price of a European option at timebased onwith payoff functionis given by

which is a random field for. For the European call option,. Due to the lack of Markov property for the pair, it is impossible to representby a conventional deterministic PDE. It is established in[8]thattogether with another random fieldsatisfies the following BSPDE (or the so-called stochastic Black-Scholes equation):

whereand the pairis unknown. We give one condition onto obtain the stochastic Feynman-Kac formula in[8], which shows the connection between the BSPDEand a BSDE.{assumption}The functionsatisfies for some

UnderEq.7, letbe a weak solution of the BSPDEin the sense of[8, Definition 2.1]such that there is a constantsatisfying for each

Then the following holds a.s.,

forand, whereis the unique solution of the following BSDE in the sense of[11, Definition 2.1]

Without loss of generality, we set the initial timeand. Then,andgive the following decoupled forward and backward SDE (FBSDE),

Here and in the sequel, we drop the superscripts to simplify the notation. Note that this stochastic Feynman-Kac formula ensures the uniqueness and existence of a weak solution to the BSPDE (7)[8, Theorem 2.5]. By the first equation in, the solution of BSPDE (7) corresponds to one of the solutions of BSDE (9). Hence, the option pricing problem (6) can be reformulated as solving the associated BSDE (9).

Motivated by (8) and the fact that[8, Theorem 2.2], we can regardas a function depending on, similarly forand.

It is well-known that the interest ratevaries in long-term financial data. Socan be regarded as a function of time, i.e.,. Then we define,
which followsas well. The corresponding European option price with payoff functionis given by

and the associated BSDE is

SECTION: 3Unsupervised Learning-based Calibration method

In this section, we present our unsupervised learning-based calibration scheme for the rBergomi model (1)-(2). Letbe the model parameters, and the interested set of market parameterbe

with each pairdenoting the strike and expiry for one European option. We also assumeis listed in ascending order. We solve the BSDE in a forward manner using deep learning-based methods for the following two reasons:

The market prices of European options are commonly used for calibration, and the forward solver is suitable for pricing European options, while the optimal stopping problem generally requires solving the BSDE backwardly[24].

The forward solver ensures that the numerical solution is adapted to the filtration. For the backward Euler scheme of BSDE[31, Sec 5.3.2], the key is to compute the conditional expectation to guarantee adaptiveness, which is generally computationally costly.

We only discuss the case thatare all scalars. The calibration scheme can be easily extended to function parameters, cf.Section5.

SECTION: 3.1Deep learning based scheme

First we discretize the temporal domainwithusing an equidistant temporal gridwith stepping sizeand. We takewhereis a strictly increasing function. Letandforbe the Brownian motion increment. We approximatein (10) by the Euler-Maruyama scheme:

whereis obtained by the mSOE scheme proposed in[30]. Here, we explicitly express the dependence ofandon the model parameter.

Next, we solve the BSDEunder the given market parameter. By (9), the strike and maturity characterize the terminal condition of the BSDE. Letbe the-valued stochastic process with the expiry, which satisfies the following BSDE

withfor. We write the payoff functionexplicitly to show its dependence on the strike and restrict ourselves to call options. Moreover, we use the notationfor,for,forandforbelow. For, we utilize the Euler-Maruyama scheme to approximateusing the historical data as the initial values

Finally, we use NNs as surrogates forandwith. ByRemark2.2, we viewandas-valued functions of. A similar idea has been implemented in[8]. Letsuch that, we take

whereare NNs with input dimension, output dimensionandrepresents all the tunable weights. Plugging the ansatz expression (14) into (13), we obtain

Since we treat the historical data as the initial condition (12), the proposed scheme has no data loss if it converges to the BSDE (9). To guarantee the convergence of the deep learning-based scheme, we match the terminal condition to determine all the parameters involved and take the loss function to be

with

SECTION: 3.2Calibration task

The calibration task is to find the model parameterthat minimizes the discrepancy between the true option priceand the market prices, which is quantified by the following function

whereanddenotes the Frobenius norm of a matrix. According to (12),is identical with, and we can reformulateas

Hence, the calibration task (4) is equivalent to

Instead of providing an explicit pricing map, we state inTheorem4.1thatis bounded by the loss, regardless of the specific choice of the parametric function space, to justify the use of the loss. Hence, we solve the optimization problem

We summarize our calibration scheme inAlgorithm1.

SECTION: 4Convergence analysis

In this section, we derive estimates for the targetand the loss. For simplicity, we considerand. Recall that, (12), (13) andlead to the following discrete scheme for,

Clearly,. In the sequel, we denoteas a generic constant whose value is independent ofand may vary from line to line. For any, we have[8, Remark A.1]

wherefor some constantdue to the path properties of the Volterra processes with a fractional kernel. Based on,and the path properties of the variance process, the strong solution to the SDEsatisfies

and its Euler-Maruyama approximation given byhas the following error estimate

for a constant[8, Equation A.1, A.2]. The BSDE theory[11],Eq.7and the path properties ofimply the existence and uniqueness of an adapted-solutionof BSDE, which together withgives-regularity result on:

for a constant. Now, we can bound the numerical error of the BSDE solutions (13), together with the target(16) in terms of the sum of temporal discretization error and the loss function.

LetEq.7hold. Then there exists, depending on,,,andbut independent ofsuch that

where,andfor.

For simplicity, we remove the dependence on,andin the proof. Note that the third equation inis equivalent to the integral form

and the last equation ofgives

Subtraction these two equations gives

By plugging it into the identity, we obtain

Taking conditional expectationof the identity yields

Then, by Young’s inequality of the form

Cauchy-Schwarz inequality and the-regularity of, we derive

By taking, we derive

By induction we obtain, for,

By combining the estimate with the-regularity ofinand the triangle inequality, we obtain

Next we bound theandcomponents in. From, we have

By squaring both sides and taking conditional expectation, we have

Taking expectation and using tower property gives

Together with the Young’s inequality (26) and (27), we derive for any,

We takeand add the inequalities for:

Finally, noting thatand decomposing the terminal misfit give

where the second inequality follows fromEq.7and. We complete the proof by combining estimatesand.

Next we prove that the loss (15) can be small if we can find the model parametersuch that the corresponding option price is close to the market price and the approximation capability of NNs is high. NNs are suitable to approximate random functions; See[8, Proposition 4.2]for details. First we recall the-regularity of the pair:

Sinceis an-projection of,andconverge to zero when.

Let Assumptionhold. Then there existsdepending onand, independent of, such that for sufficiently small,

For simplicity, we remove the dependence on,andin the proof. We first decompose the loss as

where the second line follows similarly to. Next we estimate. Byand using Young’s inequality of the form

we have

Then Cauchy’s inequality andindicate that

By discrete Grönwall inequality[13, Proposition 3.2], for sufficiently small, we have

Note that

Similarly, we have

Combining these estimates and taking infimum on both sides give the desired result.

SECTION: 5Numerical experiments

Now we showcase the performance of the proposed calibration scheme. First, we evaluate the performance using simulated data. Since the synthetic data is noise-free and unaffected by market imperfections, we can assess the calibration performance independently of the rBergomi model’s modeling capabilities. Then we conduct calibrations using historical market data. The model parameters can be either scalars or functions, and we analyze their learning effectiveness for the market data. Since the original mSOE scheme is unsuitable for the case of time-varying function parameters, we provide an adapted version inAppendixAto get the samples ofrequired byAlgorithm1.

First we describe the general experimental settings. We employ TensorFlow[2]as an autodifferentiable framework and basically follow the implementation ofDeep BSDE Solver[18].
All the experiments are carried out on the following computing infrastructure:
CPU: Dual Intel Xeon 6226R (16 Core); GPU: NVIDIA Tesla V100 32GB SXM2, OS: Rocky Linux 8 (x86-64). The NN Architecture is chosen as follows. For, the sub-networksandare fully connected NNs with two hidden layers, 32 neurons per layer and leaky ReLU activation function with negative slope 0.3. We implement batch normalization right after each linear transformation and before activation, which is initialized to a normal or uniform distribution. All the trainable weights of dense layers involved inandare initialized via Xavier uniform distribution and zero bias without pre-training.

SECTION: 5.1Numerical accuracy

In this part, we test the calibration performance of the deep BSDE scheme using synthetic data. We take the ground truth model parameterslisted inTable2with,to compute.

Here, all the model parameters are treated as constants, i.e.,are scalar trainable variables in the BSDE model. The market parameteris set to be

with.is then obtained by the mSOE scheme with step sizeandMC repetitions. For the deep BSDE scheme,trajectories are generated at each iteration.

We present the error variation of the deep BSDE scheme against the number of iterations inFig.1.is obtained using the mSOE scheme withtime discretization size andMC repetitions, using the model parametersoutputed by the algorithm after each iteration.is then computed by. The relative error is defined as

wheredenotes the absolute value, and the division is implemented element-wise. Average relative error takes the mean value among this matrix’s entries, and Maximum relative error is the matrix’s largest element. It is observed that all three error metrics decreased at first with a smaller standard deviation (SD) and then dramatically increased. This observation motivates implementing early stopping so that the training process terminates when the targetdoes not decrease over a certain number of training steps, and we refer to this number as ”Patience”.Fig.1indicates that the turning point comes only after 7 iterations.

Training(s/Iter)ItersSDAvg Rel.errorSDMax Rel.errorSD19.92083.525e-31.270e-31.708e-33.118e-46.551e-32.020e-3334.65356.32483.637e-31.254e-31.737e-33.019e-46.695e-31.941e-3330.167149.28183.628e-31.271e-31.732e-33.108e-46.659e-31.930e-3295.667491.66083.403e-31.129e-31.730e-33.073e-46.103e-31.929e-3243.517

SECTION: 5.2Calibration with historical data

In this part, we perform calibration to the S&P 500 index (with the ticker SPX) European call option data of Feb 28th, 2023, sourced from OptionMetricswww.optionmetrics.comvia Wharton Research Data Services (WRDS). We take the underlying priceas the official close of the S&P 500 index and proxy the spot varianceby the square of at the money implied volatility with the shortest maturity. The option price used is an average between max Bid and min Ask. We take the market parameter gridto be

with varyings. Since the raw option data are not provided in a grid-like format over the same maturities listed above, we selectso that the corresponding maturitiesare close toelement-wise. To get, which is supposed to fall on the given maturity grid, we apply cubic spline interpolation and constant extrapolation tofor each,using thetf-quant-financelibrary[1]. To compute the loss, we apply interpolation toso that it lies on the maturities associated withfor consistency.

Below we allow the interest rateto be time-varying. Since the dataset contains current interest rate information at various maturities, cubic spline interpolation is applied to makeavailable for each. GivenRemark2.3, the simulation scheme for the forward SDE will not be affected. For the backward SDE, equationis modified to

It is well known that calibration is not necessarily a convex optimization problem, and the targetexhibits multiple local minima. Hence, a proper initial guess of the model parameter is important for the deep BSDE scheme. Motivated by the idea that NN can be utilized as a warm-start tool[32], we can train the following inverse mapping using synthetic data

and regardas an initial guess. In the implementation, we take the output ofAlgorithm1with a small amount of market data as input to get a quick initialization. To perform calibration for days in the future, one may use the calibrated result of the latest day as a reasonable initial guess since the market does not change dramatically during a short period. More elaborate methods for finding the initial guess are left for future research. All the numerical experiments in this part share the same initial guess of model parameters.

First we consider the case that all model parameters are scalars. We present the error of calibration using different numbers of market data inTable4and list the learned model parametersinTable5. The number of market data is, i.e., the number of nodes in the market parameter grid.

The comparison between the first rows ofTable3andTable4indicates that the deep BSDE scheme for the historical data can achieve a similar level of relative errors as for synthetic data. It yields aroundaverage relative error andmaximum relative error for the noisy data, which is remarkably good for the calibration problem. When comparing the results listed in different rows ofTable4, we observe that the usage of more market data deteriorates the accuracy, especially for the maximum relative error, which may be attributed to the more noise included.

To further enhance the model learning capability, we replace the initial forward variance curve with a NN surrogate whose input is, i.e.,wheredenotes all the trainable weights. We set it to be a fully connected NN (FCNN) with two hidden layers, 8 neurons, and leaky ReLU activation with a negative slope of 0.2. A more careful design of the NN architecture using prior knowledge of the model parameters is likely to achieve better results, which is however not pursued here. All the trainable weights and biases of the FCNN are initialized with a constant close to zero except for the bias term at the output layer, which is set to be the initial guess for. The error variation against the number of market data is shown inTable6. The learned initial forward variance curve is plotted inFig.2with the rest of the calibrated model parameters presented inTable7.

The results in the 2nd and 3rd columns ofTable4andTable6indicate that the enhanced model obtains significantly better calibration results than the classical one in terms ofand average relative error, but with larger maximum relative errors. This might indicate thatmoderately influences the modeling accuracy.

Finally, we consider all the model parameters to be time-dependent functions and replace them with NNs withas input, i.e.,,,andwhererepresent all the trainable parameters of the NNs. The NNs are structured in the same as(as above). We present the error variation inTable8and plot the learned parameter functions inFig.3. The result inTable8is similar to that inTable6and different numbers of market data give roughly the same calibration result forandbut with varyingbyFig.3.

SECTION: 6Conclusion

In this work, we have proposed a novel unsupervised learning-based scheme to calibrate the rough Bergomi model without generating labeled data. This scheme takes historical data as the initial condition and simulates the corresponding BSDE in a forward manner. Neural networks (NNs) approximate unknown solutions of the BSDE with increasing input dimensions, and the model parameters are regarded as tunable variables or formulated as NNs, all of which are trained simultaneously by matching the terminal condition. We provided rigorous upper bounds on the discrepancy between historical data and the price from the learned model parameters in terms of the loss. Further, the loss can be made small given fitting capability of the rough Bergomi model and universal approximation properties of NNs.

There are several lines of future work. First, we aim to propose a dimension stationary numerical scheme for the BSPDE. The current approach connects the BSPDE solution with a BSDE and uses NNs with increasing input dimensions to approximate the BSDE solutions, following the implementation in[8]. Thus the maximal dimension grows linearly to the reciprocal of the time stepping size, which is undesirable in traditional numerical methods and significantly increases the complexity of the neural network.
Second, we can explore more delicate NN architectures for the model parameters with more advanced training techniques. Parameterizing the model parameters by NNs without any pretraining gives the model too much freedom to achieve desirable results. Thus, regarding all model parameters as NNs may not perform better than just viewingas NN.

SECTION: Appendix AmSOE scheme for time-dependent rBergomi model

In this section, we generalize the mSOE scheme proposed in[30]to the rBergomi model with time-dependent parameters, i.e., model parameters are functions of time. The dynamics are

under a risk-neutral probability space. Consider an equidistant temporal gridwith the time discretization sizeand. The major difficulty of samplingforarises from the stochastic integral inside the exponential function, denoted by

The essential idea of the mSOE scheme is that we keep the kernelexact near its singularity atand approximate it by a sum of exponentials elsewhere. More precisely, letbe the kernel approximation

whereis a set of nodes andare the corresponding weights, both implicitly depend on. Here, we assume the number of summation termsfor each,is identical for the sake of simplicity. Based on this approximation, the resulted approximationcan be split into a local part and a history part, which we denote asand, respectively

whereis called theth historical factor for, whose weighted sum is the history part. By direct computation,

The first stochastic integral in the bracket is Gaussian and fully correlated withso it is a scalar multiple of. Hence,

where

for. We simply takeasby definition. Then we obtain the following recursive formula for each historical factor

With this recursive relation, we simulate a centered-dimensional Gaussian random vector at timefor

whereis the Brownian motion increment to the aim of simulatingandjointly. At the terminal time, we only need to simulate a 2D Gaussian vector. We give the covariance matrixof the Gaussian vector:

for, whereis the lower incomplete gamma function. Sincevaries w.r.t., we must implement Cholesky decomposition at each time step. The total offline cost is. It is direct to see fromthat the overall computation complexity foriswith storage cost.
With, the samples ofare naturally obtained by

We adopt the Euler-Maruyama method for the log-stock processwhich followsand retrieve samples ofby

This numerical scheme is summarized inAlgorithm2.

SECTION: References