SECTION: Histogram-Based Federated XGBoost using Minimal Variance Sampling for Federated Tabular Data

Federated Learning (FL) has gained considerable traction, yet, for tabular data, FL has received less attention. Most FL research has focused on Neural Networks while Tree-Based Models (TBMs) such as XGBoost have historically performed better on tabular data. It has been shown that subsampling of training data when building trees can improve performance but it is an open problem whether such subsampling can improve performance in FL. In this paper, we evaluate a histogram-based federated XGBoost that uses Minimal Variance Sampling (MVS). We demonstrate the underlying algorithm and show that our model using MVS can improve performance in terms of accuracy and regression error in a federated setting. In our evaluation, our model using MVS performs better than uniform (random) sampling and no sampling at all. It achieves both outstanding local and global performance on a new set of federated tabular datasets. Federated XGBoost using MVS also outperforms centralized XGBoost in half of the studied cases.

SECTION: IIntroduction

Federated Learning (FL)[1]has gained traction, mainly due to enhanced privacy and ability to exploit distributed datasets. Generally, FL offers low communication cost , privacy aware, Machine Learning (ML) functions and research suggest that its performance can be similar to that of centralized ML[2].

FL was initially designed to fitparametricmodels, models characterized by a simplified input-output function of a know form e.g. (Deep) Neural Networks (DNNs). DNNs show great promise but struggle with tabular data, a common data type[3]. Researchers have designed parametric models for tabular data[4], suitable for FL[5], yet they do not provide state-of-the-art performance[6].Non-parametricmodels have performed well on tabular data. They do not make strong assumptions about the form of the input-output function and can learn the form in training. Tree-Based Models (TBMs) e.g. Random Forests and boosted trees[7]have shown great performance on tabular data. However, federated implementation is non-trivial and researchers have only recently designed federated TBMs.

Extreme Gradient Boosting (XGBoost)[8]can be considered a suitable model for such data, often outperforming related models[3], yet few XGBoosts for FL are available.

Recent work has shown that subsampling for TBMs can improve performance in a centralized ML[9]. Subsampling is the process of taking a fraction of data to train on. It can help select useful data. We refer to subsampling assampling. Sampling has received little attention in FL and initial works do not show increase in performance, compared to centralized learning[10]. In FL, each client gets a subset of the whole dataset for training, and with additional sampling we further reduce the local datasets with different techniques.

To such end, we propose histogram-based Federated Learning with XGBoost using sampling of training data when building the trees. We evaluate our model using uniform sampling and Minimal Variance Sampling (MVS)[9], as MVS shows performance improvements in centralized settings. We do not only study the global performance but also local performance on given client test sets. We also present a set of federated tabular datasets that reflect the natural properties of data in FL. We focus on developing models for horizontal FL and from our extensive evaluation, our main contributions are:

Federated XGBoost using MVS improves performance in terms of accuracy and regression error when compared with federated XGBoost using no- or uniform sampling.

Federated XGBoost using MVS performs similarly as centralized, and even outperforms it in half of the cases.

FedTab - A selected set of federated tabular datasets that can serve future benchmark studies.

The paper is structured as related work, method and results before a discussion and conclusion.

SECTION: IIRelated Work

Tabular data are observations that are represented by rows and columns, see Figure1. FL was initially designed for parametric models yet research suggests that TBMs perform better on tabular data[6]. Researchers have started to develop federated TBMs and few provide sophisticated open-source packages: FATE[11], Flower[12]and NVFlare[13].

In their paper,[14]present a non-parametric federated representation of a random forest. Building their federated forest requires clients to train partial tree models based on a subsets of features and data, and aggregation of these on server side. Other researchers have thereafter implemented their versions of federated random forests, focusing on various aspects such as decentralization[15]and blockchain-based[16]forests. Yet, the performance of a federated random forest is similar or slightly lower than for centralized learning. In centralized learning, there is reason to believe that the strongest model is still XGBoost[6]. In a federated setting,[17]found that one of the stronger performing models are federated Gradient Boosted Decision Trees (GBDTs). Much research has thereafter focused on developing federated GBDTs.

SECTION: II-AFederated Gradient Boosted Decision Trees

Researchers started to investigate federated GBDTs in 2019.[18]and[19]proposed federated GBDTs, mainly for classification tasks. They explore features such as secure aggregation and discuss model efficiency. A similar study was made by[20]in which they argue that a secure federated LightGBM can be used for tabular data. FederBoost[21]and SecureBoost[22]extended their work by introducing more complete federated XGBoost frameworks. The authors of SecureBoost also demonstrate that their model can be as accurate as non-federated TBMs.[23]created FedTree, an open-source framework for federated forests and GBDTs that can be used for horizontal- and vertical FL.[24]created a federated XGBoost that could adapt to local data. Party Adaptive XGBoost (PAX) includes a quantile sketch approximation hyperparameterwhich is roughly the inverse ofbin size of histogram. Given a global, they define clients’ approximation hyperparameter as

for whichis the size of the client dataset, a subset of total dataset. The inversegives the bin size for each client. Their PAX model has thereafter been used in other studies[25]in which it has been applied to non-independent and identically (non-IID) distributions.[10]extended SecureBoost to include Gradient One-Side Sampling (GOSS), a sampling technique for selecting training data[26]. Sampling training data can improve performance in centralized learning[27]yet it is an open problem whether the effect is similar for FL. Their model SecureBoost+ reduced tree building time and provided similar performance to that of XGBoost.[28]presentedimportance samplingfor FL. Their algorithm ISFedAvg not only samples specific data instances from mini-batches but also incorporates a dynamic client selection.[29]extended their work to address non-IID data. Their ISFL framework improves the original FedAvg algorithm but still does not outperform centralized learning.

In a centralized setting,[9]proposed Minimal Variance Sampling (MVS), a sampling technique that they argue outperforms GOSS, similarly concluded by[30]. MVS uses the regularized gradientsfor whichandare the gradients and hessian, to calculate the probability of selecting a specific data point. It outperforms other methods e.g. GOSS on small and large datasets, can improve computation time, and generalize better than other methods. Summarized, MVS bases its sample selection criteria on low variance output from previous predictions and selects a predefined proportion of samples to help grow the tree. This approach is similar to[31], with less computation as we do not include any corrective fine-tuning. To such end, we evaluate whether federated XGBoost using MVS can improve performance in terms of accuracy (and other related scores) in a federated setting.

SECTION: IIIMethod

This section includes basics of XGBoost, our federated implementation, and a set of compiled datasets. The datasets are carefully selected to fit our comparisons and a federated setting.

SECTION: III-AXGBoost Model

The model architecture we opt to use in this study is XGBoost and we hereafter present some preliminaries to the model. Given a datasetwherewithfeatures,, and an arbitrary task, predictions are calculated fortrees andlearning rate as:

whereis the prediction made by theth tree. XGBoost’s objective is minimizing sum the total loss for all samples and regularization parameter.is training loss andis regularization term.

The Taylor expansion of the objective function is:

for which gradientand hessianare calculated as:

for whichis previous prediction made by tree andis loss function.

In summary, XGBoost works by combining the predictions of several individual models, called ”weak learners”, into a final prediction. Each weak learner is typically a simple decision tree. XGBoost trains weak learners iteratively and adjusts their predictions based on how well they perform on the data. In each iteration, the algorithm adds a new weak learner to the ensemble and adjusts the weights of the previous learners to account for any mistakes they made, see Figure2for reference. Using MVS, the training samples with a higher gradient and hessian are more likely to be selected for training.

SECTION: III-BFederated XGBoost using Sampling

Instead of looking at framework e.g. PAX[24], we propose federated XGBoost using MVS. The core part of MVS is calculating the regularized gradientsfor whichandare the gradient and hessian. MVS bases its selection criteria on low variance output from previous predictions and selects a proportion of samples to grow the tree. Selecting instances with low variance can provide more stable and informative training examples. Moreover, we opt to use histogram-based XGBoost framework as it can improve efficiency and robustness to outliers[8]even though there might be a loss of information due to approximation. Histograms are constructed based on distribution of feature values.

The process of building a federated XGBoost for horizontal is explained in Algorithm1and we show our contribution to the algorithm. The data are split and distributed to clients (Line 1). Gradient boosted decision trees then usequantile based approximationto reduce the search space of finding optimal splits, we use the work proposed in[30].

Input:, Data;, Aggregator;, Number of Clients;Parameter:, Sampling Fraction;, Number of Training Rounds;, Early Stopping Rounds,, Learning Rate;, L2 Regularization;, Max. number of Bins;, Tree DepthOutput: Federated XGBoost Model

Each client constructs local histogramusing specified histogram bin sizeand local datasetand sends them to the aggregator where they are merged into a single histogram(Line 3-7). Then, the iterative FL process starts. For each round, we reset the aggregated parametersto 0. Then, for all clients in parallel, the aggregatortransmits global modelto each client which is assigned to client’s local model(Line 12). We sample training data using either MVS or uniform sampling (Line 13). The integration of the sampling technique comes at a crucial point before deriving the gradients and hessians. The locally sampled dataset is derived from thesampled training data. Predictions(Line 14) are generated and we use them for computing the local gradientand hessian, before transmitting them to the Aggregator (Line 15-17). After transmission, they form aggregated gradients and hessians(Line 19). They are used with the aggregated histogram representationto continue to grow the tree (Line 20).
Figure3shows the system overview of construction our federated XGBoost. For clients, histograms are computed, samples are selected and weak learners are created. For the server, gathered parameters are merged and later evaluated as a model. Final model is created when termination criteria are satisfied. Federated XGBoost implementation can be made public upon accepted publication.

SECTION: IVResults

We call our model F-XGB and present its performance in comparison with other federated XGBoost frameworks. We include results using MVS- and uniform sampling on federated tabular datasets. We define federated datasets as data with (1) natural keyed generation process (keys refers to unique users), and (2) distribution skew across users/devices[32]. We argue that it is desirable to test federated models on such datasets as it is a natural and realistic approach to evaluate FL due to non-IID properties in distributed data. We also include 3 non-federated datasets to compare our models to relevant work. Thereafter, we presentFedTab, a collection of federated tabular datasets covering many different tasks, see TableI. To our knowledge, no open set of federated tabular datasets have been collected.
We use 8GB NVIDIA GeForce RTX 3080 GPU for experiments.
See TableIIfor hyperparameter search space.

SECTION: IV-ARelated Model Comparison

First, we compare F-XGB to PAX[24]on the Airline Delay dataset.[24]tested their model using mentioned dataset and we follow the same pre-processing steps and use 3 clients with 1000 training instances each, and 1000 for testing. We compare F-XGB using MVS to PAX on this dataset as their model is not open-source. Random and balanced setting refers to the dataset sampling used, selecting samples based on uniform distribution or balanced in which each client has the same amount of labels for all classes. From TableIII, we read that F-XGB can nearly predict all samples correctly.

We thereafter compare F-XGB to the proposed federated XGBoost by[25](denoted FX) which is based upon the work of[24]. Their study provides a federated XGBoost on Non-IID data, a prominent challenge in FL, and we investigate whether F-XGB can handle such data partitions. We compare the models on 2 of the datasets included in their study: (1) Credit Card and (2) Firewall datasets as they are used in many other related studies, and use their hyperparameter setting. We complete the same partitions to form non-IID datasets and show it in TableIV. F-XGB outperforms FX on both datasets in (almost) all partitions, except for one in which they perform equally good. We show the best F-XGB results using MVS and sampling fraction%.

As shown in[10], a GOSS sampling technique in a federated setting does not improve performance in terms of accuracy. It is important to highlight, as sampling in centralized learning can improve performance. They evaluate their model in a vertical FL setting which prevents a F-XGB comparison. To our knowledge, there is no other paper that has investigated either GOSS or MVS in federated XGBoost. Thus, we evaluate F-XGB on federated tabular datasets in SubsectionIV-B.

SECTION: IV-BFederated Tabular Datasets

We study the performance of F-XGB on 1 regression-, 2 multiclass-, and 3 binary datasets and include results from centralized XGBoost. Scores presented are mean scores over 5 runs. We initially split the dataset into 80% training data and 20% validation data. We train our model for 200 rounds.

From TableV, we read that F-XGB using MVS outperforms other variants of the model in almost all cases. On datasets Lumpy Skin Disease and Insurance Premium Prediction, the scores are the most similar. For lumpy skin classification, the scores are almost perfect for ano sampling(NS) F-XGB, thus sampling has a marginal effect. We notice that F-XGB using MVS with 50% sampling fraction is the best performing model on larger- and multiclass datasets. We do not see similar behavior for uniform sampling. F-XGB using MVS and 10% sampling fraction shows better performance on smaller and binary classification datasets. For regression task, a sampling fraction of 20% performs the best for F-XGB using MVS.

We include other metrics than accuracy and Root Mean Squared Error (RMSE), namely F1, AUC and R2 scores to mitigate the risk of only analyzing non-representative scores for unbalanced datasets. As shown in TableV, F-XGB using MVS achieves good F1 and AUC scores and for Insurance Premium dataset it is almost able to explain 90% of the variance in the target using the features (R2 score), similarly for centralized XGBoost. As seen in TableI, datasets like Heart Disease, Insurance Premium Prediction, and Machine Failure include only 3-4 unique splits. Thus, we use 3-4 clients where applicable. For the remaining datasets, we sample 22 clients each run due to computational limitations.

Interestingly, F-XGB using MVS outperforms F-XGB when no sampling is applied. Uniform sampling can increase performance but performs similarly to no-sampling in most cases. We find that F-XGB using MVS outperforms centralized XGBoost. It does so in 3 cases and perform similarly to it in the remaining cases. This is important, as the general consensus is that FL can enhance privacy and reduce communication but oftentimes comes with a loss in performance. We include the results from centralized XGBoost in TableV, using the same hyperparameters for both federated and centralized models.

The standard deviation for all sampling methods for Insurance Premium Prediction regression task is also worth highlighting. The average standard deviation is approx. 10% of mean value, in comparison to second largest standard deviation 2.5% for FEMNIST. Moreover, we show in Figure4how F-XGB using MVS and a sampling fraction of 50% achieves better performance in terms of smaller standard deviation. The figure clearly illustrates how variance in predictions is reduced using F-XGB and MVS compared to uniform sampling. Uniform sampling can in certain cases perform similarly to MVS but over several runs, the performance is significantly less. For Lumpy Skin and Machine Failure dataset, the predictions over several runs are fairly similar, resulting in very small standard deviation. F-XGB using MVS does not show signs of varied predictive performance over various tasks e.g. binary- and multiclass classification, nor for small and large datasets e.g. Heart Disease and FEMNIST. The average standard deviation for MVS vs. uniform sampling on selected datasets in Figure4is 0.009 and 0.013 respectively.

Furthermore, we evaluate what the local vs. global performance in for F-XGB. We use a train-validation-test split of 70-20-10%, the same hyperparameter search space as described in TableII, and compare the aggregated test scores for each client in relation with global evaluation performance. In Figure5, we demonstrate F-XGB’s performance using uniform sampling and MVS. Figure5illustrates the change in accuracy (or RSME) on local vs. global datasets used for prediction. Importantly, as datasets Heart Disease, Machine Failure, and Insurance Premium Prediction only include 4, 3, and 4 unique splits based on their federated characteristics we only include results from 3 and 4 clients. For the other datasets, we include scores from 5 clients. We limit client selection tofor illustrative purposes. 5 unique clients are randomly sampled each round. Since insurance premium prediction is a regression task, it is desirable that a local test score is lower than global evaluation since we use RMSE as metric. F-XGB using MVS outperforms uniform sampling in all classification tasks. In almost all cases, it boosts performance compared to uniform sampling for which performance mostly decreases or remains unchanged. For Lumpy Skin dataset, both sampling techniques demonstrate similar capabilities. This can be explained by that both already perform very well, almost 100% global evaluation accuracy (see TableV). Moreover, for Insurance Premium Prediction dataset, we see that MVS can help decrease the error while uniform sampling shows no clear signs of this behavior. For FEMNIST, Synthetic, and Lumpy Skin datasets which includes many unique users, the change in accuracy is similar among clients since we sample new clients each run.

SECTION: VDiscussion

We have presented results from our model F-XGB using sampling techniques, and both global and local performance.

SECTION: V-AGlobal Performance

From the results, we read that F-XGB using MVS as a sampling technique, can increase global performance in a federated setting, which we argue is not the case for related studies that include other sampling techniques[10]. The results from F-XGB using MVS are compared with no sampling and uniform sampling, see TableV. In most cases, a sampling fraction of 50% is recommendable to use. Presented in TableV, MVS50 (which is F-XGB using MVS and a sampling fraction of 50%) outperforms other model configurations. This seems reasonable as there might be a significant proportion of data that are redundant and do not have much impact on model training, thus MVS does not select them. Nevertheless, we are surprised to see that a sampling fraction of 10 or 20% can be a more suitable choice. This could be due to data quality or many outliers in distributed datasets. Moreover, F-XGB using MVS is compared with centralized XGBoost and outperforms it in half of the studied cases. This is important as FL is often seen to improve privacy yet lower accuracy. We also show that F-XGB using MVS is able to outperform related and highly sophisticated models e.g. PAX on various datasets. F-XGB demonstrate signs of robustness on non-IID data (see TableIV) which is a desirable trait in FL.

SECTION: V-BLocal Performance

In Figure5, we show that F-XGB using MVS can increase performance locally when compared to global performance. In other words, in the final round of training, we evaluate both local and global model on either respective local test data or global evaluation data. From the figure, we read that F-XGB’s predictive performance on local datasets can be higher than global performance, which we argue is a method of locally optimizing for data. By finding the data instances that minimize variance (i.e. MVS), F-XGB trains on the mostinformativeinstances, avoiding redundancy. This is also argued by[9]. For the top 3 graphs in this Figure, we see that the performance improvement is quite client specific while for the remaining graphs, the effect is similar for each client because we sample 5 clients out of a larger pool.

SECTION: V-CSampling in Federated Learning

As presented in SectionII, sampling has shown that performance can increase in a centralized setting, and we have now demonstrated that it can also apply to FL when using federated XGBoost and MVS. Based on the presented results, research should focus on studying sampling as a technique in FL for tabular data. As an example, will a federated model that can use an arbitrary sampling technique other than uniform sampling, select the same data instances as in a centralized setting. If so, then why? If not, what are the implications? To illustrate what we refer to Figure6. Our example includes a centralized case in which 1 (-) sign is selected, 4 (+) signs and 4 () signs. For the contrary federated case, and arbitrary model samplesof the data from each ”classes”, thus not the same samples for a training round. MVS seeks to minimize variance, in our case on client side. Researchers may look to explainable FL[39]to easier visualize and give intuition to why a decentralized model in FL acts a certain way.

SECTION: VIConclusion

FL as a technology is gaining traction yet FL for tabular data has received less attention. XGBoost is a suitable model for such data and in a centralized setting, its performance can be improved by sampling training data when building the trees. Such sampling in a federated setting has not been concluded to improve scores e.g. accuracy, even though it is commonly used in a centralized setting. We propose a federated XGBoost that can use MVS, on federated tabular dataset that reflect the non-IID properties of natural federated data. We study the behavior of our federated XGBoost using varying dataset tasks and sampling fraction. Our integration of Federated XGBoost with MVS significantly enhances accuracy and reduces regression error in a federated setting compared to traditional XGBoost approaches without sampling. Additionally, our federated XGBoost using MVS has similar performance to that of centralized models, outperforming them in half of the cases. Lastly, we introduce ’FedTab,’ a curated collection of federated tabular datasets designed for future benchmarking studies, providing a standardized basis for evaluating federated learning methods.

SECTION: Acknowledgement

This work was partly supported by the TRANSACT project. TRANSACT (https://transact-ecsel.eu/) has received funding from the Electronic Component Systems for European Leadership Joint Under-taking under grant agreement no. 101007260. This joint undertaking receives support from the European Union’s Horizon 2020 research and innovation programme and Austria, Belgium, Denmark, Finland, Germany, Poland, Netherlands, Norway, and Spain.

SECTION: References