SECTION: Towards virtual painting recolouring using Vision Transformer on X-Ray Fluorescence datacubes
In this contribution, we define (and test) a pipeline to perform virtual painting recolouring using raw data of X-Ray Fluorescence (XRF) analysis on pictorial artworks. To circumvent the small dataset size, we generate a synthetic dataset, starting from a database of XRF spectra; furthermore, to ensure a better generalisation capacity (and to tackle the issue of in-memory size and inference time), we define a Deep Variational Embedding network to embed the XRF spectra into a lower dimensional, K-Means friendly, metric space.
We thus train a set of models to assign coloured images to embedded XRF images. We report here the devised pipeline performances in terms of visual quality metrics, and we close on a discussion on the results.

October 11, 2024

: Computer Vision and Cultural HeritageVision TransformersMA-XRFHeritage ScienceComputer Vision

SECTION: Introduction
The rise of Artificial Intelligence in the last decade had a tremendous impact on all academic fields. Statistical and deep learning methods have been ubiquitously employed to help researchers in their tasks, from the easier ones to the most complex ones. Yet, in many fields, there is still room to find suitable applications of such technologies.

In the field of nuclear physics for Cultural Heritage analysis and imaging (for a non-exhaustive list of relevant reviews to the subject, see, e.g.and references therein), the use of deep learning methods has begun and is spreading; for example, deep learning can be used to help researcher analyse data from non-destructive testing techniques, such as neutron imaging, gamma-ray spectroscopy, and/or X-ray based imaging techniques. These algorithms can help to identify patterns and correlations in the
data, providing valuable insights into the artwork’s composition, age, and condition(for other Machine Learning approaches in Cultural Heritage, see, and references therein).

A relevant technique in the field of nuclear technologies applied to Cultural Heritage is X-Ray Fluorescence (XRF) spectrometry, especially in its Macro mapping version (MA-XRF). The XRF technique is employed in Cultural Heritage applications, since no sample pretreatment is required. Furthermore, XRF allows for non-invasive, non-destructive measurements, which is crucial in maintaining the integrity of the sample and the repeatably of the measurements (see, and references therein).

In Macro X-Ray Fluorescence mapping (MA-XRF), the imaging apparatus produces a data cube of a scanned area of an artwork; each datacube pixel is formed by a spectrum containing fluorescence lines associated with the element composition of the pigment present in the pictorial layers. From there, it is possible to extract elemental distribution maps of the scanned area of the artwork, proving to be extremely useful for material characterisation, and also for understanding an artist’s painting and production techniques.

One possible application of Computer Vision techniques on data obtained form such techniques is,i.e.the extraction of a RGB image out of a MA-XRF data cube of a pictorial artwork analysis. This task may be relevant in conservation science, since techniques such as MA-XRF are capable of detecting spectral signal (thus containing elemental information on pictorial pigments) even from degraded surfaces (either paintings or frescoes), and also to study a series of subsurface features such as over-painted compositions, pentimenti, covered paint losses, retouching and other physical changes. Thus, such pipeline may assist conservation scientist and heritage science experts in the interpretation of the MA-XRF data, granting an additional visual feedback (i.e., the reconstructed RGB image).

Unfortunately, the availability of quality dataset for many of the learning task is usually hampered by various causes, such as slow data creation due to intrinsic apparatus speed limits, legal/Intellectual Property strict limits, difficulties in accessing study objects, limited availability and high cost of experimental apparatus, the necessity of high skilled and qualified personnel to use it, etc. Furthermore, data standardisation is far from the usual in other fields, due to the different, custom nature of the various measuring apparatus, as well as different measurement conditions, different detectors, etc.

To tackle this issue, we define a pipeline to be applied use case by use case, capable of working without huge dataset. To do so, we resort to synthetic MA-XRF data generation, starting from a tabulated database of pigments’ XRF signal, and Deep Variational Embedding, to (a) reduce the disk size of the generated synthetic dataset, and (b) to extract relevant features and reduce the role of different measuring apparatus, conditions, and environmental impacts. Afterwards, we train a Vision Transformer to map synthetic, embedded MA-XRF maps to RGB images.

This study is the first step towards the applicability of such pipeline in real-case scenario, where a domain adaptation learning technique will be added to apply, in an almost self-supervising manner, the whole pipeline to a real MA-XRF image obtained from an analysis conducted on a use case.

Inwe describe the building blocks of the aforementioned virtual recolouring pipeline; inwe report the results of the training, and the inference of the pipeline on the test set. We close withwith a discussion. Finally, inwe report the link to the git repository containing the code used to produce the results presented in this contribution.

SECTION: Methods: the virtual colouring pipeline
In this section we describe the pipeline architecture design and implementation. The goal of the whole process is to assign a coloured image to a MA-XRF map, obtained using the movable apparatus developed at the INFN-CHNet node facility in Florence, LABEC (Laboratorio di tecniche nucleari per l’Ambiente e i BEni Culturali).

The pipeline relies on the following steps:

Generate a synthetic dataset of Spectral signal;

train a Deep Embedding model to map spectra into lower dimensional metric space;

Starting from RGB images, build a synthetic dataset of embedded MA-XRF images using the python package;

Train a Computer Vision model to assign RGB images to (embedded) MA-XRF images.

This pipeline is visually reported in.

SECTION: Motivating the pipeline architecture
X-ray Fluorescence (XRF) is a non-invasive, non-destructive analytical technique widely used in the study of cultural heritage, and it is well suited for analysis of pictorial artworks (for a nice introduction to the subject, see,e.g.and references therein).

XRF employs X-ray beams emitted by radiogenic tube to excite atoms within the material composing the study object. As a result of the matter-radiation interaction, these atoms emit characteristic X-ray fluorescence radiation, which is collected in appropriate detectors, defining a spectrogram (i.e., a histogram of counts in the X-ray range); by looking as such spectrogram, researchers are able to identify the chemical elements present in the material.

Due to material inhomogeneities (always present in the CH study objects), standard single spot XRF analysis may produce false results concerning the chemical composition of the substance under investigation. Thus, it has been developed the XRF macro-mapping imaging technique, known as Macro X-Ray Fluorescence mapping (MA-XRF), with scanning mode acquisition systems. MA-XRF allows to gather data on the sample’s material composition and the distribution of the distinctive components within the scanned area, which allows for the creation of elemental distribution maps.

The MA-XRF raw data can then be arranged into,i.e.a 3-D array; the first two indeces are the (discretised) pixel positions, while the third index is the discretised energy (in KeV)/wavelength of the emitted fluorescence radiation. These datacubes may be of different sizes, depending on themotors range used in the analysis set up, but usually have a fixed Energy depth, due to the Analog-to-Digital converter (ADC) used. Nevertheless, to maintain spectra readability, the number of energy bins cannot be too low,i.e., no less thatbins; this means thatMA-XRF imagehas a large disk occupation size (approximately, each MA-XRF image, once rebinned to, let us say, 512 bins, has a size which istimesthan an RGB image of the same height and width).

Furthermore, due to instrumental limitations, copyright and data management issues, as well as study objects availability, it is almost impossible to have a dataset of standardised MA-XRF images whose size is really suited for Deep Learning applications.

Additionally, the obtained spectra posses a set of features which are not related to the analysed object, but on the apparatus(for more information about the physics of the XRF, see; while, for a more detailed description of difficulties experienced while applying learning techniques on imaging data of Cultural Heritage object, see, and references therein).

To address all these issue, we resort to

Synthetic dataset generation;

Deep Embedding in a low dimensional metric feature space;

The deep embedding is useful either to extract relevant, apparatus-independent features of the material/pigment composition of the target object, and to reduce the in-memory size of MA-XRF images.

Furthermore, it is possible to add an,step in the Deep Embedding model training, which may help in the inference of the whole recolouring pipeline onMA-XRF images.

SECTION: Preparing the synthetic dataset - part I: spectra
To generate a meaningful Spectral Dataset, we started from a database of pigments’ XRF signal. We extracted from there a subset of relevant pigments (a), comprising different pigments (Red pigments: Red Ochre, Cinnabar; Blue pigments: Cobalt Blue, Smaltino; Yellow pigments: Gold Ochre, Dark Ochre; Green pigments: Aegirine, Green Earth; Dark pigments: Caput Mortum, Ivory Black, Carbon Black; White pigment: Titanium white). This palette have been chosen due to its similarity with the palette used to create a mock-up, prepared in our laboratory: a replica of an Etruscan
wall painting from IV century BC from the “Tomba della Quadriga Infernale”in Sarteano (Siena, IT), obtained following the historical buon fresco technique.

From the aforementioned palette of couples [pigment XRF signal, pigment RGB], we have built a synthetic dataset of approximatelyimages (we split them into 70% training set, 20% validation set, and 10% test set) using the Python package.

As a seed, we have used all the images of Paintings and Frescoes available on wikidata, and freely downloadable from there through the wikidata SPARQL query service.

The MA-XRF generation system works as in; starting from the aforementioned palette, the data generation algorithm comprises two main parts:

A RGB clustering of the input image(s), performed with an iterative K-Means () algorithm, to reduce the colour levels;

A random extraction (Monte Carlo method) to generate a XRF spectra for each pixel, after having unsupervisedly associated a (set of) pigment(s) to each RGB pixel.

To perform step 2., the algorithm computes, pixel-by-pixel, a similarity measure in RGB space (based on the Delta CIEDE 2000 distance), between the (clustered) RGB and the palette indexed pigments’ characteristic RGB. Furthermore, to (unsupervisedly) reduce the number of pigments for each pixel, an hard thresholding is performed on the similarity values (values below the threshold are set to zero). This process defines, for each pixel, a linear combination of XRF signals, seen as a probability distribution defining the probability that a certain signal is detected by the detector.

The full pseudocode of the MA-XRF data generation is reported in(from).

From the dataset of MA-XRF images we randomly extracted a subset of 5 million spectra (by selecting random pixels spectra). We divided this set with a 70/20/10 % split for Training, Validation and Test, so that we have

Train set shape:

Validation set shape:

Test set shape:

SECTION: Training the Deep Embedding Model
The idea of this part of the pipeline is to train a Deep Variational Embedding modelto dimensionally reduce the signal via relevant feature extraction; The whole process is self-supervised, since only spectra are used during training. The whole architecture comprises:

An Encoder,;

A clustering algorithm in latent space,; in this latent space, we can compute

a self-supervised clustering loss, based on the;

a variational loss, inspired by the infoVAE model, which is the Maximum-Mean Discrepancy (MMD);

A Decoder,;

Compute a self-supervised reconstruction loss;

Thus, the full loss we use, inspired byVAEs, is

whereis theth training epoch, and where.

This architecture is based on the Deep Clustering Network (DCN) architecture ofand the Variational Deep Embedding (VaDE) model of.

The model hyperparameters were obtained using a grid-search method. The Encoder has 4+1 layer, while the decoder has 4 layers; the latent space has dimension, the Multi-Layer Perceptrons (MLPs) are Self-normalising Neural Networks, and their sizes arefor the encoder, andfor the decoder. Furthermore,, while.

A visual representation of the Deep Variational Embedding model is reported in.

SECTION: Preparing the synthetic dataset - part II: embedded MA-XRF images
After having obtained a trained model following the procedure described in, we can use it on the MA-XRF image dataset of, and obtain a (disk occupation size reduced) dataset in embedded space, subbed embedded MA-XRF dataset.

This grants us a dataset of size (we used theordering)

Train set shape:

Validation set shape:

Test set shape:

Please notice that the embedded MA-XRF images have three channels, but areRGB images; it was a mere coincidence that the embedded space have the same dimensions as the target RGB space. Nevertheless, the approach described here would still work even if the two spaces have different dimensions.

Inwe report an example of a couple input-output. Notice that, due to the coincidence that the embedded space has 3 dimension, we can use false colours to represent the embedded MA-XRF.

SECTION: Training the virtual recolouring model
We are now in the position to define a virtual recolouring model, whose goal is to map embedded MA-XRF images obtained into RGB images.

As a model, we use a Vision Transformer (ViT)(for a nice survey, review, and introduction on Vision Transformers and their applications, see, and references therein). In particular, due to the limited size of the training dataset, we use a model based on the one presented in;i.e., we use(SPT) embedding, and(LSA), whose aim is to solve the lack of locality inductive bias, and enable ViT models to learn from scratch on reduced-size datasets.

Furthermore, we employed long skip connections to form a U-ViT inner architecture. Each transformer block in the backbone has thus a LSA multi-head part, and a Feed Forward (FF) part; the Feed Forward part is comprised of a sequential model of Linear Layer, Dropout layer, GELU activation, followed again by a Linear layer and a dropout layer.

After the U-shaped transformer layers, we have a Linear layer followed by a rearranging layer of the elaborated visual tokens (done using thepackage). The reconstructed tensor of shapeis thus passed to a final convolutional layer withkernel.

For simplicity, we will refer in thefollowing to this architecture as(U-Net based Vision Transformer for Small dataset tasks).

We usedduring training(in an implementation inspired to the(timm) library, as done in the UViT paper, following the recipe of the DeiT paper). We have also used data augmentations during training using the standardtransformations.

During training, the learning rate is changed dynamically via thenativefunction by monitoring the selected metric, which is the(MS-SSIM)in the Lightning AIimplementation.

We arrived at the formulation of this architecture by trial-and-error on multiple architectures, starting from the plain ViT, the UViT, the SWIN transformer, and the SEgmentation TRansformer (SETR) with progressive up-sampling design (SETR-PUP). We trained various implementation of such architecture (and mixture of them) on a reduced dataset (sampled from the full one), in a plain grid-search approach, finding the most performing one at (almost) fixed number of parameters,i.e.the aforementioned SmallUViT.

To train the SmallUViT model we have used an ad hoc distance for the loss, to take in account perceptual differences in colors; due to its simple implementation, we used the CompuPhasesRGB distance approximation:

where,, and similarly for the other channels.

Thus the loss is

whereis the model prediction, whileis the ground truth.

SECTION: Results: from MA-XRF to RGB
Among the possible hyperparameter set selection at fixed VRAM occupation size we could handle with our set-up at this stage, we selected the best model architecture via a plain grid search.

The selected model has 3 485 076 trainable parameters, obtained by 3+1+3 Transformer layers in the in/mid/out U-ViT blocks, apatch size, 9 heads per MultiHead block, 192 embed dimension size, 2 MLP-to-Dim factor, and a head dimension of 32. The dropout rate and the stochastic depth rate are both set to.

We trained the SmallUViT model for 100 epochs, using the schemes described in;

The result of the training is reported in; the discrepancy between validation and train loss/metric may be traced back to the presence of Dropout and Stochastic Depth during training.

Applied on theimages in the test set, we get a sRGB loss of, and an MS-SSIM metric value of.

SECTION: Application on some test set cases
We now report few examples extracted from the test dataset. For each example, we show a triplet of figures: the leftmost one, is the embedded synthetic MA-XRF image; the middle one, is the output of the SmallUViT; the rightmost one, is the true RGB image. On top of the image in the middle are reported two metric scores, computed between the prediction and the target: the Multi-Scale Similarity Index, and the(UiQi). The four examples presented in,,andare selected because they present relevant feature of the inference of the model (pros and cons); they are not the ones with the higher MS-SSIM nor UiQi scores in the test dataset. Furthermore, we choose to show the comparison of the inferred RGB with the actual (resized) image, and not with itsclustered version employed in the synthetic dataset generating algorithm (we refer to).

From the presented images, we already seen pros and cons of the trained model. Even if the model was limited, the pipeline cumbersome, the task complex and the computing power limited, we were able to extrapolate a set of RGB images out of (embedded, synthetic) MA-XRF data cubes. This is quite comforting.

Nevertheless, even if the MS-SSIM score is quite high (around), and, overall, the network was capable of assigning colors to semantic regions of the images, we may spot few issues, as partially signalled by the low UiQi scores.

Interesting examples are,; the network was able of identifying the dark colours (the blueish black and the dark blue of the background, as well as the black of the jacket), the light colours (the white and the light blue of the man’s Ruff), and, quite surprisingly, the red, which is given (in the example) by the embedding of signals formed (in part) by a red ochre, which is, in composition, quite similar to the Gold Ochre. Yet, the incarnate has been not properly coloured, at least in(instead, for the peculiar visual aspect of, theclustering performed on the RGB has to be blamed, see). The poor performance on such incarnate may also be attributed to the complex pigment selection during this image generation - fact that can be hinted from the false rgb representation of the embedded synthetic MA-XRF.

In, the model correctly coloured the frame’s yellow (a difficult pigment), the leaves’ greens are quite correctly identified and, apart from a slight colour deviation, so are the ground and horses colours. Yet, no clouds appears in the sky. This is due to the aforementioned RGB clustering (see).

Finally,presents an interesting artefact. While the background greens, the incarnate, the beard, and, partly, the uniform colours have been partly identified, the turban red posed a difficult task to the network, and it creates a patching artefact.

SECTION: Conclusions
In this contribution we report the design and a first test of a pipeline aimed at virtual recolour MA-XRF data cubes. This task is intended to furnish restoration scientist and heritage scientist a visual aid to help them elaborate the raw data obtained from nuclear imaging performed on pictorial artworks, especially in those situation where the study object suffers from loss of visual readability.

The goal of the pipeline design is to perform the task starting from synthetic data, to overcome the lack of availability of data. Furthermore, the pipeline has an intermediate step to (a) address the high per-MA-XRF image disk size, and (b) extract relevant spectral features, regardless of experimental/environmental conditions.

Thus, the designed pipeline comprises two trained models:

A Deep Embedding model working on single spectra;

A Vision Transformer to elaborate embedded.

We have shown that, a part from some difficulties inherited by the synthetic dataset generation algorithm, we have a pipeline capable of performing the end-to-end task.

SECTION: Next steps
After having motivated the pipeline design and architecture, the next step is apply it on a recolouring use case task. To do so, the crucial part is devising an appropriatetechnique to handle the domain shift between the synthetic dataset and the real MA-XRF data. The fact that the pipeline relies on two networks allows us to perform the domain adaption unsupervisingly on one training only,i.e.add the domain adaptation (e.g., as an adversarial step as shown in), while also giving us the opportunity the tackle each issues arising from the network disjointly and in parallel.

The first and foremost issue it is possible to trace back, is the limited size of the dataset. We plan to enlarge it by removing the restriction of scraped images from WikiData to be either paintings or frescoes. This, of course, alters the domain of the dataset, including scenes not usually represented in the domain of application of the network (i.e., pictorial artworks). Nevertheless, the goal is to split the train into two parts:

A first self-supervised step, to force the ViT to learn the semantic aspects of images (using the enlarged dataset);

A second step, where the pretrained network should learn how to assign colours (using the pictorial artwork dataset).

Furthermore, inwe have reported few limitations shown by the network; mainly, its inability to reconstruct visual appearances lost during the rgb clustering step in the synthetic dataset generation algorithm. One way to tackle this issue would be to add apart to the model, in order to infer something not properly encoded in the model’s input. To do so, we plan to implement aapproach, using SmallUViT as backbone (for a review on GAN approaches with ViT, seeand references therein).

SECTION: Code and Data Availability
The code used in this project can be found at the ICSC Spoke 2 repository.

SECTION: Acknowledgements
To conduct the work presented in this contribution, we resorted to the cloud computing Software-as-a-Service infrastructure made available by the ML_INFN initiative (“Machine Learning at INFN”), aim to foster Machine Learning activities at the Italian National Institute for Nuclear Physics (INFN), as well as some servers made available by INFN-CHNet, the network devoted to the application of nuclear techniques to Cultural Heritage.

We would also like to thank the authors of the site, and WikiDataand WikiMedia commons.

SECTION: Funding
This work is supported by ICSC – Centro Nazionale di Ricerca in High Performance Computing, Big Data and Quantum Computing, funded by European Union – NextGenerationEU, by the European Commission within the Framework Programme Horizon 2020 with the project “4CH - Competence Centre for the Conservation of Cultural Heritage” (GA n.101004468 – 4CH) and by the project AIRES–CH - Artificial Intelligence for digital REStoration of Cultural Heritage jointly funded by the Tuscany Region (Progetto Giovani Sì) and INFN.

The work of AB was funded by Progetto ICSC - Spoke 2 - Codice CN00000013 - CUP I53C21000340006 - Missione 4 Istruzione e ricerca - Componente 2 Dalla ricerca all’impresa – Investimento 1.4.

The work of FGAB was funded by the research grant titled “Artificial Intelligence and Big Data” and funded by the AIRES-CH Project cod. 291514 (CUP I95F21001120008).

SECTION: References
SECTION: References