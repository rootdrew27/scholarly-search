SECTION: Magnetic Resonance Imaging Feature-Based Subtyping and Model Ensemble for Enhanced Brain Tumor Segmentation

Accurate and automatic segmentation of brain tumors in multi-parametric magnetic resonance imaging (mpMRI) is essential for quantitative measurements, which play an increasingly important role in clinical diagnosis and prognosis. The International Brain Tumor Segmentation (BraTS) Challenge 2024 offers a unique benchmarking opportunity, including various types of brain tumors in both adult and pediatric populations, such as pediatric brain tumors (PED), meningiomas (MEN-RT) and brain metastases (MET), among others. Compared to previous editions, BraTS 2024 has implemented changes to substantially increase clinical relevance, such as refined tumor regions for evaluation. We propose a deep learning-based ensemble approach that integrates state-of-the-art segmentation models. Additionally, we introduce innovative, adaptive pre- and post-processing techniques that employ MRI-based radiomic analyses to differentiate tumor subtypes. Given the heterogeneous nature of the tumors present in the BraTS datasets, this approach enhances the precision and generalizability of segmentation models. On the final testing sets, our method achieved mean lesion-wise Dice similarity coefficients of 0.926, 0.801, and 0.688 for the whole tumor in PED, MEN-RT, and MET, respectively. These results demonstrate the effectiveness of our approach in improving segmentation performance and generalizability for various brain tumor types.

* These authors contributed equally.

SECTION: 1Introduction

Currently, brain cancer is the 10th leading cause of cancer death across all age groups for both males and females in the United States, with an estimated 18,760 deaths from malignant brain tumors[24]. Early and accurate diagnosis is crucial for effective treatment planning, which can significantly impact patient outcomes. The variability in tumor appearance across different imaging modalities further complicates assessing and managing these conditions, underscoring the critical need for precise and reliable diagnostic tools.

In this context, segmentation of brain tumors using deep learning techniques has emerged as a transformative approach. Traditional manual segmentation methods are labor-intensive, time-consuming, and subject to inter-observer variability, leading to inconsistencies in clinical decision-making. Deep learning models, particularly those leveraging convolutional neural networks (CNNs), offer a robust and automated solution for brain tumor segmentation by learning complex patterns from large datasets[6]. These models can achieve high accuracy and consistency, facilitating early detection, precise localization, and detailed characterization of tumors. By incorporating various MRI-based scanning protocols, deep learning models can successfully account for diverse morphological and pathological brain tumor features thereby enhancing diagnostic accuracy and improving treatment planning[26]. Ultimately, deep learning for automatic brain tumor segmentation promises to improve patient outcomes by enabling more personalized and data-driven therapeutic interventions[15,26].

This paper presents a methodology developed primarily for the segmentation of pediatric brain tumors (PED); the method was adapted for the segmentation of meningioma radiotherapy (MEN-RT) and brain metastases (MET).

SECTION: 2Segmentation Challenges

Since 2012, the international brain tumor segmentation (BraTS) challenge[2,3,4,5,16,20], held in conjunction with the international conference on Medical Image Computing and Computer Assisted Intervention (MICCAI), has generated a benchmark dataset for the segmentation of adult brain gliomas. The BraTS 2024[2]is expanded to a cluster of challenges, encompassing a variety of tumor types alongside histopathology and augmentation tasks. Herein, we propose a generalized segmentation technique for PED, MEN-RT, and MET.

PED:The BraTS-PEDs 2024 challenge[9,17]aims to address the unique challenges of pediatric brain tumors, including diffuse midline gliomas and high-grade astrocytoma. The data (N=464) were collected from institutions including the Children’s Brain Tumor Network (CBTN, N=120), DMG/DIPG Registry (N=256), Boston’s Children Hospital (N=61), and Yale University (N=27).

Currently, this challenge consists of 261 training, 91 validation, and 86 testing cases. Each case includes pre-contrast T1-weighted (T1), constrast-enhanced T1-weighted (T1CE), T2-weighted (T2), and T2-weighted fluid-attenuated inversion recovery (T2-FLAIR) MRI sequences. The task focuses on the segmentation of the whole tumor (WT), tumor core (TC), enhancing tumor (ET), cystic components (CC), non-enhancing tumor core (NET), and edema (ED).

MEN-RT:The BraTS 2024 MEN-RT Challenge[18,19]aims to advance automated segmentation of the gross tumor volume (GTV) for meningiomas using pre-radiation therapy MRI scans. Meningiomas, the most common primary intracranial tumors, vary in grade and higher grades are associated with greater morbidity and frequent recurrence[18]. Accurate GTV segmentation is crucial for effective radiation therapy but remains complex and necessitates intensive expert delineation.

Unlike other sub-challenges that focus on pre-operative cases, this challenge uses one 3D T1CE MRI sequence in native acquisition space. Faces are automatically removed for patient anonymity and the skull is left skull intact, unlike traditional BraTS data that is typically skull-stripped[18]. This challenge consists of 500 training, 70 validation, and 180 testing cases focusing on benchmark algorithms for precise GTV delineation, aiding radiotherapy planning and supporting future research on tumor progression and recurrence.

MET:The BraTS 2024 MET challenge[21]addresses the labor-intensive and error-prone process of monitoring metastatic brain disease, particularly when lesions are small and hard to segment. Traditional methods often focus on measuring the largest diameter of metastases, but accurate volumetric estimates are crucial for clinical decision-making and treatment prediction.

This challenge aims to benchmark algorithms for automatically segmenting brain metastases (WT, TC, and ET) and surrounding ED, improving efficiency and consistency. The dataset consists of 651 training, 88 validation, and 119 testing cases with T1, T1CE, T2, and T2-FLAIR sequences.

SECTION: 3Methods

Our methodology utilizes a common segmentation framework, wherein each component is tailored to specific BraTS challenges, which present different types of brain tumors. Consequently, the methodology is generalizable and can be easily adapted to new types of brain tumors to optimize their segmentation. The framework (Figure1) includes data pre-processing, model training, model ensembling, and post-processing of predicted labels. It also incorporates MRI radiomic feature-based subtype clustering for both training and post-processing.

SECTION: 3.1MRI Radiomic Features for Tumor Subtypes

We defined tumor subtypes for each task using MRI radiomic features on the segmented tumor regions of interest in each input MRI sequence. We utilized the PyRadiomics package[1]and its implementation in[13]to extract these features. Specifically, radiomic features were computed on the largest lesion area (connected component of the WT) for each case. These features were categorized into two groups: 14 shape- and 93 intensity-based features per MRI sequence. Since some radiomic features are sensitive to image spatial resolution, input images were resampled to an isotropic spacing (0.9375 mm for MEN-RT and 1.0 mm for MET, respectively) before applying radiomic measurements.

Principal component analysis was used to select the most relevant features, explaining 99% of the variance, resulting in 9 features for PED, 3 for MEN-RT, and 2 for MET. The k-means clustering algorithm grouped lesions into different clusters (subtypes) of tumors based on the most relevant radiomic features. An optimal number of clusters was determined using grid search and silhouette analysis on the k-means clustering results. This k-means algorithm was trained on the training set with their corresponding ground-truth WT for tumor subtype analysis during pre-processing, as well as the cross-validated predicted WT during post-processing.

SECTION: 3.2Model Training

is based on the U-Net architecture[22]and is a self-configuring deep learning framework for semantic segmentation. According to the specific imaging modality and unique attributes of each dataset, the framework autonomously adjusts its internal configurations, resulting in an improved segmentation performance and generalization when compared to other state-of-the-art methods for biomedical image segmentation[12].

We trained a full-resolution 3D nnU-Net (v2) model on the stratified five-fold created for the tasks of PED, MEN-RT, and MET. A preprocessing of zero mean unit variance normalization was applied to the input images. Each input image was divided into patches ofvoxels. For each task, PED, MEN-RT, or MET, we trained models to predict the respective number of task-dependent output labels, i.e., 4 labels were predicted for PED, 1 label was predicted for MEN-RT, and 3 labels were predicted for MET.

We used a class-weighted loss function combining Dice loss and cross-entropy loss. To optimize this loss function, we used the stochastic gradient descent (SGD) optimizer with Nesterov momentum, using an initial learning rate of 0.01, momentum of 0.99, and weight decay of 3e-05. Each of the five folds was trained for 200 epochs on NVIDIA A100 (40 GB) and NVIDIA V100 (16GB) GPUs. During inference, images were predicted using a sliding window approach, with the window size matching the patch size used during training.
The nnU-Net implementation is available in an open-source repository:https://github.com/MIC-DKFZ/nnUNet.

[23]leverages a hybrid approach combining convolutional neural networks and attention mechanisms for medical image analysis. The architecture integrates convolutional layers for feature extraction with attention modules that enhance the model’s focus on relevant regions within the images[23]. Based on 3D nnU-Net (v2)[12]strategies the framework autonomously adjusts its internal configurations to give better performances.

As with 3D nnU-Net, MedNeXt was trained in a label-respective manner for each task. For all tasks, we trained MedNeXt-M (k=3, 17.6M parameters, 248 GFlops) with a class-weighted loss function combining Dice loss and cross-entropy loss. To optimize this loss function, we used the stochastic gradient descent (SGD) optimizer with Nesterov momentum, using an initial learning rate of 0.01, momentum of 0.99, and weight decay of 3e-05. Each of the five folds was trained for 200 epochs on NVIDIA A100 (40 GB) and NVIDIA V100 (16GB) GPUs. During inference, images were predicted using a sliding window approach, with the window size matching the patch size used during training. The MedNeXt implementation is available in an open-source repository:https://github.com/MIC-DKFZ/MedNeXt.

is a vision transformer-based[8]hierarchical structure for localized self-attention using shifted windows[10,11,25]. We trained a 3D SwinUNETR model using five-fold cross-validation for each task.

Each input image was sampled four times using patches ofvoxels and batch size of 1 was used to fully utilize the GPU’s memory. The model output waschannels corresponding to thenon-overlapping labels and background. Softmax activations were used at the output layer. We used a class-weight loss function that combined Dice loss and focal loss. To optimize the loss function, we used the AdamW optimizer with an initial learning rate of 0.0001, momentum of 0.99, and weight decay of 3e-05. Each of the folds was trained on an NVIDIA H100 (80 GB) GPU and an NVIDIA A6000 (48GB) GPU. The number of epochs varies across tasks: 650 epochs for PED, 250 epochs for MEN-RT and MET. The Swin UNETR implementation is part of the PyTorch-based framework MONAI:https://monai.io.

SECTION: 3.3Model Ensemble

We used a model ensemble strategy to enhance the accuracy and robustness of the segmentation outcome[7]. This approach involves harnessing the complementary strengths of the models described, nnU-Net, MedNeXt, and SwinUNETR, to improve the probability prediction for the segmentation task.

The weightsfor nnUNet,for MedNeXt, andfor SwinUNETR are estimated using the individual model performance on the training set using five-fold cross validation. For the PED, we obtained,, and. While for the MEN-RT, we obtained,, and. Finally, for the MET, given the significantly lower performance of SwinUNETR, we employed,, and.

SECTION: 3.4Post-processing

Following the strategy outlined in[14], we utilized an adaptive post-processing technique on the cross-validated WT predictions computed by the ensemble of the models trained on each fold.
After the k-means algorithm was trained on the training set images (Section3.1), we performed an optimal threshold search to eliminate small, disconnected components, thereby reducing the number of false positives in the segmentation maps. These thresholds were determined adaptively within each tumor subtype and for each label separately. Finally, a second threshold search was conducted on these refined segmentation maps to redefine labels, i.e., labels which ratio with respect to WT fell below certain threshold, the label would be redefined to other label.

Details regarding thresholds and weights for model ensemble can be found in the source code of our implementation available athttps://github.com/Precision-Medical-Imaging-Group/HOPE-Segmenter-Kids. Additionally, an open-source web-based application for demonstration purposes is accessible athttps://segmenter.hope4kids.io/, allowing users to obtain segmentation and volumetric measurements by uploading MRI sequences.

SECTION: 4Results

SECTION: 4.1Metrics

The evaluation of the model prediction on the validation set was done on the BraTS pipeline on the Synapse platform. The models were assessed for each of the regions using the lesion-wise Dice score and thepercentile lesion-wise Hausdorff distance.

SECTION: 4.2Segmentation Performance

Quantitative results of our models across the validation and testing datasets for each challenge are shown in Tables1,2, and3for PED, MEN-RT, and MET, respectively. These evaluations were performed automatically by the challenge’s digital platform, with no access to the validation ground truth data and no access to any testing data including images and labels. Additionally, Figures2,3, and4illustrated qualitative results on validation cases for PED, MEN-RT, and MET, respectively.

SECTION: 5Discussion

Our observations indicated that ensemble techniques generally enhance the precision and generalizability of segmentation models. Specifically, we found that weighted ensembles outperformed simple averaged ensembles of probabilities. By assigning different weights to the outputs of individual models based on their validation performance, the weighted ensemble method effectively prioritizes more accurate predictions, leading to improved overall segmentation results. However, the improvement is limited in some tasks.

Despite these improvements, the variability of tumor regions across subjects remains a significant challenge. The heterogeneous nature of brain tumors, characterized by differences in size, shape, and location, complicates the segmentation task. This variability suggests that a one-size-fits-all post-processing approach may not be optimal.

To address this issue, we propose incorporating cluster-wise post-processing techniques. By clustering similar tumor subtypes based on radiomic features, we can tailor the post-processing steps to the specific characteristics of each cluster. This approach allows for more precise adjustments to the segmentation results, potentially reducing false positives and improving the overall accuracy of tumor segmentation.

SECTION: 6Conclusion

This year’s challenge introduced changes to enhance clinical relevance, such as refined tumor region evaluations. We developed a deep learning-based ensemble method that integrates advanced segmentation models, complemented by an innovative adaptive pre- and post-processing technique utilizing MRI-based radiomic analysis to distinguish tumor subtypes. This approach addresses the heterogeneous nature of tumors in clinical datasets, improving the precision and generalizability of segmentation models. Our method is ready to be tested on other clinical datasets to investigate the efficacy and robustness of our approach in enhancing segmentation accuracy and generalizability across various brain tumor types.

Partial support for this work was provided by the National Cancer Institute (UG3 CA236536) and by the Spanish Ministerio de Ciencia e Innovación, the Agencia Estatal de Investigación, NextGenerationEU funds, under grants PDC2022-133865-I00 and PID2022-141493OB-I00, and EUCAIM project co-funded by the European Union (Grant Agreement #101100633). The authors gratefully acknowledge the Universidad Politécnica de Madrid (www.upm.es) for providing computing resources on the Magerit Supercomputer.

The authors have no competing interests to declare that are
relevant to the content of this article.

SECTION: References