SECTION: ParFormer: A Vision Transformer with Parallel Mixer and Sparse Channel Attention Patch Embedding
In recent years, both Convolutional Neural Networks (CNNs) and Transformers have achieved remarkable success in computer vision tasks. However, their deep architectures often lead to high computational redundancy, making them less suitable for resource-constrained environments, such as edge devices. This paper introduces ParFormer, a novel vision transformer that addresses this challenge by incorporating a Parallel Mixer and a Sparse Channel Attention Patch Embedding (SCAPE). By combining convolutional and attention mechanisms, ParFormer improves feature extraction. This makes spatial feature extraction more efficient and cuts down on unnecessary computation. The SCAPE module further reduces computational redundancy while preserving essential feature information during down-sampling. Experimental results on the ImageNet-1K dataset show that ParFormer-T achieves 78.9% Top-1 accuracy with a high throughput on a GPU that outperforms other small models such as MobileViT-S, FasterNet-T2, and EdgeNeXt-S. Specifically, ParFormer-T achieves 2.56higher throughput than MobileViT-S, 0.24% faster than FasterNet-T2, and 1.79higher than EdgeNeXt-S. For edge device deployment, ParFormer-T excels with a throughput of 278.1 images/sec, which is 1.38higher than EdgeNeXt-S and 2.36higher than MobileViT-S, making it highly suitable for real-time applications in resource-constrained settings. The larger variant, ParFormer-L, reaches 83.5% Top-1 accuracy, offering a balanced trade-off between accuracy and efficiency, surpassing many state-of-the-art models. In COCO object detection, ParFormer-M achieves 40.7 AP for object detection and 37.6 AP for instance segmentation, surpassing models like ResNet-50, PVT-S and PoolFormer-S24 with significantly higher efficiency. These results validate ParFormer as a highly efficient and scalable model for both high-performance and resource-constrained scenarios, making it an ideal solution for edge-based AI applications.

SECTION: 
In recent years, deep learning models, particularly Convolution Neural Network (CNN) and Transformers, have garnered significant interest and demonstrated remarkable achievement in computer vision and natural language processing. There has been a clear observation in the domain of deep learning models that as the network grows deeper, it becomes possible to acquire more sophisticated features to enhance the performance. In classical CNN, ResNet50and VGG-16have around 25 million and 138 million parameters, respectively, assigned to image feature extraction and classification tasks. Moreover, the vanilla Vision Transformer (ViT)needs 85 million to 632 million parameters to do the similar ImageNet classification task. Their forward propagation requires a significant amount of computation, which is slow inference and not easily accessible in many particular fields of application. Application of CNN models in low-cost settings, such as mobile and edge devices, is highly difficult because of the restricted memory, processing power, and battery life. Therefore, our work is to construct a lightweight and efficient deep learning model that minimizes the amount of computation while nonetheless ensuring fast inference and high performance.

Numerous studiescontend that feature redundancy may be the root of CNN or Transformer models’ inefficiency. For instance, the intermediate layer of ResNet50 exhibits significant redundancy in feature maps across the channels. It indicates that elevated parameters in ResNet50 do not enhance the model’s accuracy or generalization capability. Not only in CNN, different approaches, such as transformer models, may lead to redundancy. The tokenization of image patches and subsequent attention mechanisms such as multi-head self attention (MHSA) in DeiTand Swin Transformersalso suffer feature redundancy across the head. Each patch in a transformer may represent visual patterns that are too similar to neighboring patches, causing the MHSA to focus on repetitive information rather than diverse. Inshows that there are 64.8% feature similarities when using 6 heads and get lower when the number of heads is reduced. It shows inefficiency using many heads in MHSA that used much memory to token folding or reshaping during MHSA. Therefore, there are several ongoing research projects on network design to deal with it.

Several studies have been done to eliminate computational redundancy by designing efficient modules in networks. In Fasternetproposed partial convolution (PConv) to process a quarter feature channel only. Usingconventional convolution, process spatial information along with a quarter channel followed by FFN as a channel mixer to construct a Fasternet block. Along with that, InceptionNeXtuses 3 depth-wise convolutions (DWConv) with different kernel sizes as a spatial mixer to process thechannel. A set of, andDWConv was proposed to make a high-throughput network and a high-receptive field to deal with segmentation tasks. Inproposed single head attention (SHA) to learn spatial information for a quarter feature channel. The purpose of SHA in SHViT to keep the other channel untouched is to avoid feature redundancy such as in MHSA.

The presence of abundant and redundant information in the feature maps of deep neural networks frequently ensures a thorough comprehension of the input data. Redundancy in feature maps may be a crucial attribute for an effective deep neural network. Even though several works try to avoid feature redundancy by processing several channels only and keeping others untouched, they need a large number of channels to keep the accuracy maintained, which leads to slower inference and high parameter. Rather than avoiding unnecessary feature maps, we prefer to incorporate them in a cost-effective manner by proposing a ParFormer as shown in Figure. To reduce feature and computational redundancy, we proposed a novel Parallel Mixer and Sparse Channel Attention Patch Embedding (SCAPE). The contributions of ParFormer are summarized as follows:

A parallel mixer is proposed by combining two different spatial mixer to generate multi-receptive field features. Combining self-attention, which can capture long-range sequence information, with DWConv, the local feature extractor, to construct a parallel mixer. Then project both features to ensure two different pieces of information will propagate across the channel.

Integrating the Sparse Channel Attention Module (SCAM) into the Patch Embedding Module to create the down-sampling module known as Sparse Channel Attention Patch Embedding (SCAPE). The SCAPE module is engineered to prevent information loss during down-sampling across the channel. Implementing linear operations in SCAM transforms the interconnections between channels from a dense configuration to a sparse one, thereby conserving parameters and computational resources.

Based on MetaFormer architecture with the Parallel Mixer and SCAPE module, we design a novel basic architectural block ParFormer Encoder. We also design a network structure called ParFormer, which is constructed by stacking multiple ParFormer Encoder.

The subsequent sections of this article are structured as follows. Section II presents an overview of pertinent research on efficient network architectures for CNNs and Transformers. In Section III, we delineate the proposed network architectures, which encompass the SCAPE module alongside SCAM, Parallel Mixer, and ParFormer. Section IV presents numerous comparative experiments to validate the efficacy of the proposed network model. Finally, conclusions and expectations are drawn in Section V.

SECTION: 
SECTION: 
The integration of transformers from natural language processing (NLP) into the field of computer vision was first proposed by. Since then, this idea has garnered considerable attention and has been used to various activities within the domain. Numerous transformer topology have been proposedwith the aim of enhancing precision and effectiveness in diverse applications. Generally, these techniques prioritize the attention mechanism of the transformer architecture due to its computational complexity and lack of continuity being identified as the primary challenge. For example, the Pyramid Vision Transformer (PVT)introduced a modification known as Spatial Reduction Attention (SRA), which replaces the Vanilla Attention mechanism. SRA achieves a reduction in computational complexity by employing key and value reductions, while still preserving the global receptive field characteristic of the ViT. On the other hand, the transformer model exhibits a deficiency in terms of local continuity. To address this issue, alternative local approaches, such as the Swin Transformer, adopt a strategy of using non-overlapping windows. These windows are gradually shifted to increase the receptive field, thereby capturing interactions across several phases. Therefore, the self-attention mechanism has constraints in its ability to efficiently capture information that spans across great distances.

SECTION: 
In numerous works, the attention-based model transformer is combined with a convolution network or another type of attention model in order to reduce computational complexity and address the issue of discontinuity. In a general context, the process of combining or hybridising can be categorised into two distinct groups: serial combination and parallel combination. Within the framework of a sequential methodology, the integration of the attention module and convolution can yield a serial design. In the initial phase of its architecture, the convolution network performs local field extraction, but in the later stage, it employs the attention module to obtain global receptive capabilities. The purpose of this approach is to ensure that the architecture remains lightweight while still being able to retrieve and store both local and global information. On the other hand, this approach involves implementing parallelism between the local and global extractors within the attention module. The PLG-ViTmodel exhibits parallelism in its attention mechanisms, specifically in the W-MSA of Swin Transformer as a local extractor and the SRA of PVT as a global extractor. Another approach in Lite Transformer, combine vanilla attention as a long range feature capturing with convolution network as local feature capturing in transformer encoder. Next approach, using combination transformer encoder block from ViTand inverted bottleneck block from MobilenetV2as a fusion block in Human Pose Estimation and Running Movement Recognition. In, use transformer block and dilated convolution block and combined using fusion score as and ensemble network for sketch recognition using vector images. However, most of them combine two spatial mixer separately without any mechanism to allow propagation across the channel between them.

SECTION: 
In the initial implementation of the transformer, the image is minimally processed by linear projection, as opposed to the CNN-based model which utilizes convolution with stride in the block to reduce the spatial size of the image. In the upcoming iteration of Transformer, the vision transformer architecture, which is employed in PVTand CVT, incorporates convolution patch embedding . In order to enhance the local coherence, the technique of overlap patch embedding is employed in PVTv2and CVT. Subsequently, it has been demonstrated that the ”patchify” technique may effectively improve the performance of the CNN architecture in ConvNeXt. This enhancement enables the ConvNeXt design to attain comparable results to the Swin Transformer. In our methodology, we draw inspiration from the Squeeze-and-Excitation techniqueto incorporate sparse channel attention into the patch embedding process. This is done with the aim of improving the learnability in image patching and reducing loss of information while downsampling.

SECTION: 
The main idea of a novel ParFormer architecture for an efficient vision model is reducing the redundancy in image features. The ParFormer model incorporates the SCAPE module and parallel mixer technique, which could generate low-redundancy feature maps. The Sparse Channel Attention Patch Embedding (SCAPE) is constructed by integrating the sparse channel attention module into the patch embedding to reduce feature information loss while downsampling. The parallel mixer, combination of attention and convolution mixer, can extract the feature map information efficiently with different receptive field.

SECTION: 
Transformer-based vision modelsoften use patch embedding as a layer for downsampling space and increasing the number of channels. In the original Vision Transformer (ViT), to transform a 2D image into a 1D token sequence, patch embedding utilized convolution and folded the 2D feature into a 1D sequence. In the recent lightweights ViT, an overlap convolution layer is used, keeping the feature in 2D shape since the feature folding process or tensor reshaping consumes more memory. Along with that, ParFormer architecture used convolution to process the given input image or the feature map from the previous stagewith the size.

Using the patch embedding in equation, the inputwill be patched intowith convolution stride, kernel size, and padding sizefor the overlapped spatial downsizing andfor the number of feature in the stage-i. Theandkernel size are chosen for patch size.

Upon patch embedding, we integrate the SCAM to minimize channel redundancy and information loss caused by the patching process. The Sparse Channel Attention Module, or SCAM, is derived from the classical Squeeze-and-Excitation module, which can be integrated into the existing model. It has the ability to re-calibrate the feature map in each channel to maintain the interdependence between the channels of feature maps to enhance their representation ability.

As depicted in Figure, The SCAM obtains the channel saliency through global average pooling (GAP) to squeeze space dependency. Then, instead of using two fully connected (FC) layers, a single linear operator () is applied to make it efficient. Finally, a nonlinear sigmoid function () could scale the input features to highlight the useful channels as defined in.

SECTION: 
The information of the patched image will be extracted into the ParFormer block with repeated sequence and two residual sub-blocks: an Feed Forward Network (FFN) block and a parallel mixer block. The parallel mixer comprises two different spatial operations that enable effective extraction of token information. We proposed combination of single-head attention (SHA) and depth-wise convolution (dwconv) to construct a parallel mixer block. Supposed thatis the feature map from SCAPE block, the parallel mixer will projectedandusing convolution operation withis theconvolution weight andis the attention channel number ratio. Unlikethat using two different spatial mixer separately, we proposed convolution projection to ensure efficient propagation of the attention features with dwconv feature as detailed in equation.

The global attention scoreand local convolution feature mapprocessed previously from the following equation

where,,, andare query, key, attention value, and convolution value. The query and key channel dimensionsandare set to 32 as a the maximum following vanilla attention. The channel number of attention value and convolution valueandfollowing the channel ratiowithfor attention channel ratio andfor dwconv channel as illustrated in Figure. The optimal channel number ratio between attention and convolution is set, as detailed in the ablation study. To expand the input feature channel dimensioninto parallel mixer,convolution withweight is used as input projection. The projected input will be divided with the splitting array that corresponds to the channel as described in equation.

Finally, following, the residual block withas the layer scale parameter applied after parallel mixer block and followed by residual FFN block as detailed in equation bellow

where. This paper’s FFN block is similar to the original MLP block used in the Vision Transformer (ViT) with point-wise convolution as linear operation replacement to minimize tensor reshaping. It encompasses a singular activation function, which may be mathematically represented by the subsequent equation.

Theandare learn-able weight withexpansion ratio. Thedenotes the activation function that can be configured as,, or another non-linear activation function (i.e).

SECTION: 
In the preceding section, we explain the novel parallel mixer in the ParFormer approach. This enabled us to assess the efficacy of the model by employing two different token mixers at each stage of the encoder. The ParFormer design as a general-purpose vision transformer utilized four-stage pyramid architecture. The SCAPE is used to downsampling the feature with spatial reduction rates 4, 8, 16, 32 with embedding layer usingpatch size for the first embedding layer or stem layer and the other embedding or merging layer using. For each stage contain a stack of ParFormer encoder that employ a parallel mixer with a different ratioin every stage and ParFormer variant. For example, parallel mixer with ratio ofmeans all feature channels will be learned from DWConv mixer. If parallel mixer with ratio ofmeans 1/4 feature channel will learned from attention based mixer and others in DWConv mixer. We observe that in the early stage will consume higher memory access since then we employ parallel mixer ratioto use DWConv mixer as less complexity then attention for all variant.

Alongside the aforementioned layer, normalization and activation layers are essential for optimal neural network performance. In contrast to the conventional transformer model that use Layer Normalization, we utilize batch normalization (BN) to reduce tensor reshaping or folding. Furthermore The advantage of Batch Normalization is its ability to be integrated with neighboring Convolution layers, resulting in expedited inference without compromising efficacy. We empirically select GELUfor the activation layers in all variations of ParFormer, taking into account both execution time and efficacy. The final layers, namely global average pooling, a fully connected layer with 1280 number of hidden, are employed collectively for feature modification and classification.

The overall architecture variant of the ParFormer is shown in Table. There are four ParFormer variants: tiny (T), small (S), medium (M), and large(L). The blocks in each stage and the feature dimension is manually chosen to format a several models in different size, whose the computational complexity ranges from 0.82G(T) to 6.2G(L). The number of block ratio is following the recipe from ConvNeXtand MetaFormer.

SECTION: 
In order to assess the effectiveness of ParFormer, two thorough tests are conducted. The tests involve ImageNet-1K datasetsfor classification and COCO Datasetsfor object detection and instance segmentation.

SECTION: 
Image classification experiments are conducted using the ImageNet-1K dataset, consisting of 1.28 million training images and 50,000 validation images across 1,000 categories. Our study used a training recipe similar to those of DeiTand Swin Transformer. The models undergo training for a total of 300 epochs, with a resolution of 224224. Data augmentation and regularization approaches encompass several methods such as RandAugment, Mixup, CutMix, Random Erasing, weight decay, Label Smoothing, and Stochastic Depth. The hyper-parameters of DeiTare primarily adhered to in our approach. We employ the AdamW optimizerwith 256 batch size on 3A6000 GPUs for most ParFormer models. The performance evaluation was conducted without any pre-training on larger datasets like ImageNet-21K or knowledge distillation.

To evaluate the throughput, we select three representative processors that encompass a broad spectrum of computational capabilities: GPU (RTX-3090), CPU (Intel i5-13500), and Edge Device utilizing Jetson Orin Nano (NVIDIA Ampere 1024 CUDA and 32 tensor cores). We present their throughput for inputs with a batch size of 256 for both GPU and CPU. For Edge Device utilizing a batch size of 64 with ONNX Runtime. During inference, the batch normalization layers of the ParFormer model are integrated with their adjacent layers when appropriate.

The performance comparison of various models on the ImageNet-1K dataset, as detailed in Table, underscores several important findings related to throughput, model complexity, and accuracy, especially for edge deployment on the NVIDIA Jetson Orin Nano using the ONNX format. To show the effectiveness of ParFormer, the results are grouped based on accuracy. Our proposed method, ParFormer, demonstrates exceptional balance across these metrics, making it a strong candidate for edge-based AI tasks. When evaluating throughput on GPUs, ParFormer-T stands out with an impressive 4961 images per second, outperforming other CNN or Transformer based models such as ConvNeXtV2-F, Fasternet-T2and MobileViT seriesin terms of GPU computation efficiency. This indicates that ParFormer is well-suited for real-time image processing on GPU-equipped devices. Even in CPU scenarios, where throughput tends to be significantly lower, ParFormer-T maintains competitive performance with 106.6 img/s, which positions it as a versatile model for environments where GPU resources may be limited.

For edge deployment, the Edge-ONNX throughput is particularly critical, and here, ParFormer-T excels with 278.1 img/s, making it the highest-performing model in this category. This highlights the efficiency of our method in real-world edge applications, where computational resources are constrained. Similarly, ParFormer-S achieves a notable 193.7 img/s in Edge-ONNX throughput compare to other lightweight model such as MobileViTV2-1.5and FastViT-SA12, further reinforcing the versatility and scalability of our proposed architecture across various edge scenarios.

In terms of model complexity, ParFormer maintains a relatively low computational cost. For instance, ParFormer-T has just 7.4 million parameters and 0.82 GFLOPs, offering a lightweight solution that balances performance and efficiency. Even ParFormer-M and ParFormer-L, which have larger parameter counts (24.2M and 42.3M, respectively), still perform exceptionally well in edge settings while offering strong accuracy, with ParFormer-L achieving 83.5% top-1 accuracy. While models like CAFormer-S18(83.5%) and Swin-L(83.6%) slightly edge out in accuracy, these models require significantly more parameters and computational resources. In contrast, ParFormer offers highly competitive accuracy while maintaining lower computational complexity, particularly in resource-limited edge scenarios. Lastly, unlike some larger models such as Swin-Land ConvNeXtV2-T, which face out-of-memory (OOM) issues during edge deployment, ParFormer models exhibit no such limitations, confirming their suitability for memory-constrained environments.

The results in Tableclearly show that ParFormer is an excellent solution for an edge-based deep learning vision model, offering an optimal balance between throughput, accuracy, and computational efficiency. Our proposed ParFormer architecture not only excels in high-throughput scenarios but also addresses the practical constraints of edge deployment, making it a highly viable option for real-world applications.

SECTION: 
In this part, we examine the unique architectural decisions. Tablepresents an ablation study on ParFormer-S, analyzing the effects of key design elements, specifically the Parallel Mixer (PM) ratio and the SCAPE module, which incorporates the SCAM. SCAM is crucial for minimizing information loss across feature channels during downsampling. The study investigates how varying these components affects the model’s parameters, computational complexity (GFLOPs), throughput (in images per second on a GPU), and Top-1 accuracy.

The baseline configuration of ParFormer-S includes 11.9 million parameters, 1.47 GFLOPs, a throughput of 3362 images per second, and achieves a Top-1 accuracy of 80.9%. This serves as the reference point for comparing the effects of modifications in both the Parallel Mixer and SCAM designs.

The SCAPE module, which combines SCAM and Patch Embedding (PE), was assessed in three configurations to evaluate its impact on performance optimization: SCAM Before PE, SCAM Following PE and PE without SCAM module. Positioning SCAM prior to the patch embedding decreases the parameter count to 11.6 million and marginally elevates GFLOPs to 1.48 compared to the Baseline. Throughput increases marginally to 3368 images per second, while Top-1 accuracy diminishes slightly to 80.7%. This indicates that positioning SCAM prior to PE results in inefficiencies in processing feature channels, causing a minor decrease in accuracy. Positioning SCAM subsequent to patch embedding as the baseline (11.9M and 1.48) yields a throughput of 3362 images per second. The Top-1 accuracy is maintained at 80.9%, signifying that this configuration successfully elaborates channel-wise attention and information preservation. The placement of SCAM subsequent to PE guarantees the retention of essential features during downsampling, effectively preserving both global and local information. Eliminating SCAM entirely while preserving patch embedding reduces the parameter count to 11.5 million and enhances throughput to 3390 images per second. Nevertheless, Top-1 accuracy declines to 80.4%, underscoring SCAM’s critical function in reducing information loss across feature channels during downsampling. The lack of SCAM diminishes the model’s capacity to capture and prioritize essential channel information, leading to diminished accuracy.

The upcoming ablation procedure is the parallel token mixer. The Parallel Mixer (PM) ratio defines how feature channels are processed by two different spatial mixing operations: depthwise convolution (DWConv) and Single Head Attention (SHA). A ratio of 0 means that all channels are processed via DWConv, while a ratio of 1/4 implies that one-quarter of the channels are handled through SHA, with the remaining 3/4 processed by DWConv. The purpose of the Parallel Mixer is to combine these two spatial mixing operations to reduce feature redundancy by leveraging DWConv for local feature extraction and SHA for global information aggregation.

The ablation study evaluates three configurations of the PM ratio in four stage architecture. The first configuration is. In this configuration, all channels are processed entirely by DWConv across all stages, increasing the parameter count slightly to 12.0M and reducing GFLOPs to 1.45. Throughput improves to 3435 img/s, but Top-1 accuracy drops to 80.4%. The reliance solely on DWConv improves computational efficiency but sacrifices accuracy due to the absence of global attention provided by SHA, leading to less effective feature interaction across the entire image.
The second is [0, 0, 1/4, 1/4] that introducing SHA for 1/4 of the channels in the last two stages balances local and global feature processing. The parameter count remains at 11.9M with a slight increase in GFLOPs to 1.48. Throughput is consistent with the baseline at 3362 img/s, while Top-1 accuracy is maintained at 80.9%. This configuration demonstrates that using SHA on a small portion of the channels in later stages helps improve global context capture without sacrificing efficiency or accuracy, making it a well-balanced approach. Finally, increasing the SHA allocation to 1/2 of the channels in the final two stages [0, 0, 1/2, 1/2] reduces the parameter count to 11.3M and GFLOPs to 1.41, while throughput improves to 3428 img/s. However, Top-1 accuracy drops to 80.2%, indicating that while higher PM ratios increase computational efficiency and reduce model complexity, excessive reliance on SHA for global attention at the expense of DWConv’s local processing reduces the model’s ability to capture fine-grained details, leading to diminished performance.

SECTION: 
We assess the efficacy of ParFormer on downstream computer vision tasks. We utilize the Mask R-CNNalgorithm to train our method on the COCO2017 train split. Subsequently, we assess the performance of the models on the val split. The training schedule is set at 1× in order to maintain a consistent comparison with earlier approaches. For the 1× schedule, we conduct training for 12 epochs using a single size on the 3A6000 GPUs. AdamWOptimizer is used with an initial learning rate ofwith batch size 16.The training images are scaled to have a shorter side of 800 pixels and a longer side that does not exceed 1,333 pixels. The implementation is derived from the mmdetectioncode-base. As part of the testing process, the dimension of the images is adjusted to a size ofpixels.

Due to computation resource constraints, we have chosen to test the ParFormer-T model for smallest model and ParFormer-M as the larger model size. Tablecompares the object detection and instance segmentation performance of various backbone networks including ParFormer on the COCO val2017 dataset using Mask R-CNN. The results are presented in terms of bounding box average precision () and mask average precision () at different IoU thresholds, alongside model parameters, computational complexity (FLOPs), and inference speed (FPS). The ParFormer models, particularly ParFormer-T and ParFormer-M, demonstrate superior performance across most metrics while maintaining competitive computational efficiency. Figureshows the qualitative result of ParFormer as the backbone of Mask-RCNN detector for object detection and instance segmentation task in COCO dataset.

In terms of object detection, ParFormer-T achieves an () of 37.3, matching the performance of ResNet-50 and PoolFormer-S12, while significantly outperforming lighter models such as ResNet-18 (34.0). ParFormer-M leads in bounding box detection, with an impressive () of 40.7, surpassing other models like PVT-T (36.7), PoolFormer-S24 (40.1), and even ResNet-50 (37.3). This trend holds for higher IoU thresholds (), where ParFormer-M achieves the highest score of 40.1, again outperforming all other backbones.

For instance segmentation, ParFormer-T delivers strong results with an () of 35.6, outperforming the baseline models such as ResNet-18 (31.2) and approaching the performance of larger backbones like PoolFormer-S12 (34.1) and ResNet-50 (35.6). ParFormer-M further excels with an ()of 37.6, leading over both PVT-S (37.0) and ResNet-50 (35.6), showing the efficacy of the ParFormer backbone in capturing fine-grained mask details. Notably, at the more challenging () metric, ParFormer-M achieves a standout score of 40.1, the highest in the table, showcasing its ability to handle complex segmentation tasks with high precision.

SECTION: 
In this paper, we introduced ParFormer, a novel vision transformer architecture designed to address the computational challenges faced by deep learning models, particularly in resource-constrained environments. By integrating a Parallel Mixer and a Sparse Channel Attention Patch Embedding (SCAPE) module, ParFormer effectively balances local and global feature extraction while reducing computational redundancy. Our approach outperforms many state-of-the-art models in terms of both accuracy and efficiency, making it highly suitable for edge-based AI applications.

Experimental results on the ImageNet-1K dataset demonstrate that ParFormer-T achieves a competitive Top-1 accuracy of 78.9% with superior throughput performance, especially on edge devices, where it significantly outpaces competitors like MobileViT and EdgeNeXt. Additionally, ParFormer-L offers a higher Top-1 accuracy of 83.5%, showing its scalability and suitability for high-performance tasks. On COCO object detection and segmentation, ParFormer-M achieves 40.7 AP and 37.6 AP, respectively, surpassing models like PVT-S, ResNet-50, and PoolFormer-S24 while maintaining higher efficiency.

The Parallel Mixer and SCAPE module, which are the main contributions of this work, show how powerful it can be to combine attention-based and convolutional mechanisms to get around the problems with current models. The outcomes demonstrate that ParFormer is not only a high-performance architecture but also a highly effective solution for real-time vision tasks on edge devices, which are resource- and computational-constrained. While ParFormer has proven to be an efficient and scalable model, future work could explore further optimizations, such as dynamic pruning techniques to reduce computational costs during inference. Additionally, applying ParFormer to other tasks like video classification and natural language processing could reveal its broader applicability. Lastly, investigating how this architecture can be extended to even smaller, ultra-efficient models would further enhance its utility in edge AI applications.

SECTION: References